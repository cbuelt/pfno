INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/fno_laplace.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10, 'ood': True}
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.20203428, Validation loss: 0.03550202, Gradient norm: 1.69279581
INFO:root:[    2] Training loss: 0.04482121, Validation loss: 0.03032968, Gradient norm: 1.19221373
INFO:root:[    3] Training loss: 0.03911649, Validation loss: 0.03229536, Gradient norm: 0.95552305
INFO:root:[    4] Training loss: 0.03741056, Validation loss: 0.02231133, Gradient norm: 1.35686246
INFO:root:[    5] Training loss: 0.03279382, Validation loss: 0.02465249, Gradient norm: 1.09396775
INFO:root:[    6] Training loss: 0.03149692, Validation loss: 0.01815338, Gradient norm: 1.20528996
INFO:root:[    7] Training loss: 0.02912409, Validation loss: 0.02955316, Gradient norm: 1.03380082
INFO:root:[    8] Training loss: 0.02923074, Validation loss: 0.02875466, Gradient norm: 1.15068012
INFO:root:[    9] Training loss: 0.02806936, Validation loss: 0.02255263, Gradient norm: 1.17333378
INFO:root:[   10] Training loss: 0.02658608, Validation loss: 0.02137981, Gradient norm: 1.03389466
INFO:root:[   11] Training loss: 0.02692900, Validation loss: 0.01841646, Gradient norm: 1.08809268
INFO:root:[   12] Training loss: 0.02542401, Validation loss: 0.01951631, Gradient norm: 1.00065795
INFO:root:[   13] Training loss: 0.02459394, Validation loss: 0.02812851, Gradient norm: 0.95533538
INFO:root:[   14] Training loss: 0.02483671, Validation loss: 0.02852240, Gradient norm: 1.03402935
INFO:root:[   15] Training loss: 0.02345009, Validation loss: 0.01451062, Gradient norm: 1.01374820
INFO:root:[   16] Training loss: 0.02306613, Validation loss: 0.01551685, Gradient norm: 0.99609437
INFO:root:[   17] Training loss: 0.02257676, Validation loss: 0.01606967, Gradient norm: 1.00959078
INFO:root:[   18] Training loss: 0.02344239, Validation loss: 0.01867051, Gradient norm: 0.98788968
INFO:root:[   19] Training loss: 0.02244114, Validation loss: 0.01286121, Gradient norm: 0.94393000
INFO:root:[   20] Training loss: 0.02211980, Validation loss: 0.01323998, Gradient norm: 0.97127974
INFO:root:[   21] Training loss: 0.02104331, Validation loss: 0.01773791, Gradient norm: 0.89332342
INFO:root:[   22] Training loss: 0.02099582, Validation loss: 0.02395769, Gradient norm: 0.89900340
INFO:root:[   23] Training loss: 0.02129860, Validation loss: 0.02569669, Gradient norm: 0.95709120
INFO:root:[   24] Training loss: 0.02123362, Validation loss: 0.01595046, Gradient norm: 0.97253294
INFO:root:[   25] Training loss: 0.02056307, Validation loss: 0.01078069, Gradient norm: 0.94293960
INFO:root:[   26] Training loss: 0.02046355, Validation loss: 0.01104231, Gradient norm: 0.90558427
INFO:root:[   27] Training loss: 0.01977127, Validation loss: 0.01113304, Gradient norm: 0.90764388
INFO:root:[   28] Training loss: 0.02017268, Validation loss: 0.01238247, Gradient norm: 0.88057222
INFO:root:[   29] Training loss: 0.01955726, Validation loss: 0.01268971, Gradient norm: 0.88447156
INFO:root:[   30] Training loss: 0.01898910, Validation loss: 0.01236667, Gradient norm: 0.89288979
INFO:root:[   31] Training loss: 0.01873030, Validation loss: 0.01291147, Gradient norm: 0.88783098
INFO:root:[   32] Training loss: 0.01896668, Validation loss: 0.01103772, Gradient norm: 0.79239887
INFO:root:[   33] Training loss: 0.01906899, Validation loss: 0.01234474, Gradient norm: 0.78029943
INFO:root:[   34] Training loss: 0.01868622, Validation loss: 0.00870196, Gradient norm: 0.83464318
INFO:root:[   35] Training loss: 0.01828731, Validation loss: 0.00923722, Gradient norm: 0.85015211
INFO:root:[   36] Training loss: 0.01797320, Validation loss: 0.00779108, Gradient norm: 0.85025990
INFO:root:[   37] Training loss: 0.01831126, Validation loss: 0.01245470, Gradient norm: 0.84354786
INFO:root:[   38] Training loss: 0.01839127, Validation loss: 0.00832176, Gradient norm: 0.88816337
INFO:root:[   39] Training loss: 0.01758730, Validation loss: 0.01020527, Gradient norm: 0.86454740
INFO:root:[   40] Training loss: 0.01847225, Validation loss: 0.02103926, Gradient norm: 0.85457564
INFO:root:[   41] Training loss: 0.01736633, Validation loss: 0.02162315, Gradient norm: 0.84300865
INFO:root:[   42] Training loss: 0.01810958, Validation loss: 0.01493420, Gradient norm: 0.79355261
INFO:root:[   43] Training loss: 0.01763497, Validation loss: 0.00764964, Gradient norm: 0.79753190
INFO:root:[   44] Training loss: 0.01721867, Validation loss: 0.00864040, Gradient norm: 0.79649785
INFO:root:[   45] Training loss: 0.01759917, Validation loss: 0.00952501, Gradient norm: 0.87570582
INFO:root:[   46] Training loss: 0.01746336, Validation loss: 0.01682002, Gradient norm: 0.85568683
INFO:root:[   47] Training loss: 0.01659871, Validation loss: 0.01833112, Gradient norm: 0.78935145
INFO:root:[   48] Training loss: 0.01674990, Validation loss: 0.01949370, Gradient norm: 0.80154118
INFO:root:[   49] Training loss: 0.01643186, Validation loss: 0.01609636, Gradient norm: 0.79603701
INFO:root:[   50] Training loss: 0.01614205, Validation loss: 0.01487919, Gradient norm: 0.78381751
INFO:root:[   51] Training loss: 0.01643430, Validation loss: 0.01499244, Gradient norm: 0.76272556
INFO:root:[   52] Training loss: 0.01644705, Validation loss: 0.00829269, Gradient norm: 0.81383230
INFO:root:[   53] Training loss: 0.01622190, Validation loss: 0.00702731, Gradient norm: 0.79084596
INFO:root:[   54] Training loss: 0.01641059, Validation loss: 0.00939180, Gradient norm: 0.79262971
INFO:root:[   55] Training loss: 0.01623788, Validation loss: 0.00729378, Gradient norm: 0.79763555
INFO:root:[   56] Training loss: 0.01654756, Validation loss: 0.01340151, Gradient norm: 0.84303582
INFO:root:[   57] Training loss: 0.01629808, Validation loss: 0.01979909, Gradient norm: 0.80722384
INFO:root:[   58] Training loss: 0.01591707, Validation loss: 0.01712871, Gradient norm: 0.75624570
INFO:root:[   59] Training loss: 0.01578355, Validation loss: 0.01767446, Gradient norm: 0.69886347
INFO:root:[   60] Training loss: 0.01577487, Validation loss: 0.01750322, Gradient norm: 0.74869786
INFO:root:[   61] Training loss: 0.01582607, Validation loss: 0.02006648, Gradient norm: 0.72261818
INFO:root:[   62] Training loss: 0.01566592, Validation loss: 0.01859384, Gradient norm: 0.73485863
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1300.801s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.13033
INFO:root:EnergyScoreTrain: 0.06195
INFO:root:CoverageTrain: 0.76511
INFO:root:IntervalWidthTrain: 0.43768
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.15702
INFO:root:EnergyScoreValidation: 0.0842
INFO:root:CoverageValidation: 0.64623
INFO:root:IntervalWidthValidation: 0.44532
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.14626
INFO:root:EnergyScoreTest: 0.08097
INFO:root:CoverageTest: 0.65741
INFO:root:IntervalWidthTest: 0.35129
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1935671296
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.19119391, Validation loss: 0.04245126, Gradient norm: 1.92349482
INFO:root:[    2] Training loss: 0.04905889, Validation loss: 0.03585220, Gradient norm: 1.49888458
INFO:root:[    3] Training loss: 0.04011651, Validation loss: 0.02802459, Gradient norm: 1.17590918
INFO:root:[    4] Training loss: 0.03669513, Validation loss: 0.03565873, Gradient norm: 1.18734790
INFO:root:[    5] Training loss: 0.03431549, Validation loss: 0.02201647, Gradient norm: 1.25699970
INFO:root:[    6] Training loss: 0.03256099, Validation loss: 0.03002158, Gradient norm: 1.24763304
INFO:root:[    7] Training loss: 0.02961684, Validation loss: 0.01846788, Gradient norm: 1.12309182
INFO:root:[    8] Training loss: 0.02865731, Validation loss: 0.02783521, Gradient norm: 1.06182426
INFO:root:[    9] Training loss: 0.02838901, Validation loss: 0.02436654, Gradient norm: 1.13129353
INFO:root:[   10] Training loss: 0.02644192, Validation loss: 0.01791358, Gradient norm: 0.97953121
INFO:root:[   11] Training loss: 0.02619423, Validation loss: 0.02341016, Gradient norm: 0.94400472
INFO:root:[   12] Training loss: 0.02689684, Validation loss: 0.01793521, Gradient norm: 1.08178410
INFO:root:[   13] Training loss: 0.02677149, Validation loss: 0.02755273, Gradient norm: 1.08640232
INFO:root:[   14] Training loss: 0.02381320, Validation loss: 0.01434603, Gradient norm: 0.90911673
INFO:root:[   15] Training loss: 0.02510391, Validation loss: 0.01463060, Gradient norm: 0.99420912
INFO:root:[   16] Training loss: 0.02370326, Validation loss: 0.02137510, Gradient norm: 0.94809113
INFO:root:[   17] Training loss: 0.02518583, Validation loss: 0.01599517, Gradient norm: 1.03299278
INFO:root:[   18] Training loss: 0.02310051, Validation loss: 0.01551218, Gradient norm: 0.95919416
INFO:root:[   19] Training loss: 0.02375557, Validation loss: 0.01830444, Gradient norm: 0.93401179
INFO:root:[   20] Training loss: 0.02370882, Validation loss: 0.02377331, Gradient norm: 0.95528357
INFO:root:[   21] Training loss: 0.02258660, Validation loss: 0.02355002, Gradient norm: 0.83383708
INFO:root:[   22] Training loss: 0.02270461, Validation loss: 0.03001594, Gradient norm: 0.88771193
INFO:root:[   23] Training loss: 0.02214853, Validation loss: 0.02067378, Gradient norm: 0.93338907
INFO:root:[   24] Training loss: 0.02215660, Validation loss: 0.01648189, Gradient norm: 0.95239454
INFO:root:[   25] Training loss: 0.02156515, Validation loss: 0.01153154, Gradient norm: 0.89888123
INFO:root:[   26] Training loss: 0.02136762, Validation loss: 0.01424754, Gradient norm: 0.88141483
INFO:root:[   27] Training loss: 0.02129095, Validation loss: 0.01480314, Gradient norm: 0.91756890
INFO:root:[   28] Training loss: 0.02058609, Validation loss: 0.01357996, Gradient norm: 0.84662223
INFO:root:[   29] Training loss: 0.02050727, Validation loss: 0.01356210, Gradient norm: 0.89743161
INFO:root:[   30] Training loss: 0.02093342, Validation loss: 0.01148916, Gradient norm: 0.90437572
INFO:root:[   31] Training loss: 0.02034030, Validation loss: 0.02140443, Gradient norm: 0.90133641
INFO:root:[   32] Training loss: 0.01993044, Validation loss: 0.02428595, Gradient norm: 0.87189497
INFO:root:[   33] Training loss: 0.02146790, Validation loss: 0.01474576, Gradient norm: 0.87231148
INFO:root:[   34] Training loss: 0.02039756, Validation loss: 0.01436323, Gradient norm: 0.76166340
INFO:root:[   35] Training loss: 0.01952016, Validation loss: 0.01131373, Gradient norm: 0.84640561
INFO:root:[   36] Training loss: 0.01936476, Validation loss: 0.00985686, Gradient norm: 0.87882364
INFO:root:[   37] Training loss: 0.01904063, Validation loss: 0.01803637, Gradient norm: 0.87877334
INFO:root:[   38] Training loss: 0.01927067, Validation loss: 0.02144643, Gradient norm: 0.83722061
INFO:root:[   39] Training loss: 0.01877576, Validation loss: 0.01975246, Gradient norm: 0.84157834
INFO:root:[   40] Training loss: 0.01813959, Validation loss: 0.02102865, Gradient norm: 0.79422170
INFO:root:[   41] Training loss: 0.01868396, Validation loss: 0.02285456, Gradient norm: 0.83029192
INFO:root:[   42] Training loss: 0.01859874, Validation loss: 0.01380297, Gradient norm: 0.89046085
INFO:root:[   43] Training loss: 0.01873743, Validation loss: 0.00734333, Gradient norm: 0.84482683
INFO:root:[   44] Training loss: 0.01797423, Validation loss: 0.00731947, Gradient norm: 0.81984065
INFO:root:[   45] Training loss: 0.01781389, Validation loss: 0.00984096, Gradient norm: 0.85569237
INFO:root:[   46] Training loss: 0.01854525, Validation loss: 0.01299268, Gradient norm: 0.79180209
INFO:root:[   47] Training loss: 0.01798401, Validation loss: 0.01070684, Gradient norm: 0.75538752
INFO:root:[   48] Training loss: 0.01777353, Validation loss: 0.00786675, Gradient norm: 0.81458950
INFO:root:[   49] Training loss: 0.01726143, Validation loss: 0.00878637, Gradient norm: 0.79718842
INFO:root:[   50] Training loss: 0.01789061, Validation loss: 0.01513217, Gradient norm: 0.85456930
INFO:root:[   51] Training loss: 0.01774040, Validation loss: 0.01999303, Gradient norm: 0.79696511
INFO:root:[   52] Training loss: 0.01749045, Validation loss: 0.02167241, Gradient norm: 0.74660464
INFO:root:[   53] Training loss: 0.01713093, Validation loss: 0.02105182, Gradient norm: 0.79165300
INFO:root:[   54] Training loss: 0.01707894, Validation loss: 0.01561155, Gradient norm: 0.83591619
INFO:root:[   55] Training loss: 0.01731811, Validation loss: 0.00656016, Gradient norm: 0.84938629
INFO:root:[   56] Training loss: 0.01660354, Validation loss: 0.00801062, Gradient norm: 0.81318555
INFO:root:[   57] Training loss: 0.01692030, Validation loss: 0.00882721, Gradient norm: 0.76970031
INFO:root:[   58] Training loss: 0.01688246, Validation loss: 0.00786016, Gradient norm: 0.71565162
INFO:root:[   59] Training loss: 0.01663388, Validation loss: 0.00873646, Gradient norm: 0.77615036
INFO:root:[   60] Training loss: 0.01651459, Validation loss: 0.00957996, Gradient norm: 0.83653706
INFO:root:[   61] Training loss: 0.01642689, Validation loss: 0.00660217, Gradient norm: 0.82998318
INFO:root:[   62] Training loss: 0.01642799, Validation loss: 0.00866120, Gradient norm: 0.79998510
INFO:root:[   63] Training loss: 0.01626450, Validation loss: 0.01159097, Gradient norm: 0.77827411
INFO:root:[   64] Training loss: 0.01692201, Validation loss: 0.02195940, Gradient norm: 0.80173494
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 981.394s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.14611
INFO:root:EnergyScoreTrain: 0.08045
INFO:root:CoverageTrain: 0.75861
INFO:root:IntervalWidthTrain: 0.51267
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.13619
INFO:root:EnergyScoreValidation: 0.06054
INFO:root:CoverageValidation: 0.82382
INFO:root:IntervalWidthValidation: 0.51004
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.18317
INFO:root:EnergyScoreTest: 0.09509
INFO:root:CoverageTest: 0.45907
INFO:root:IntervalWidthTest: 0.34059
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1897922560
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.21698322, Validation loss: 0.03552774, Gradient norm: 1.93168577
INFO:root:[    2] Training loss: 0.04394248, Validation loss: 0.02952267, Gradient norm: 1.10336859
INFO:root:[    3] Training loss: 0.04389719, Validation loss: 0.02930704, Gradient norm: 1.47900756
INFO:root:[    4] Training loss: 0.03919916, Validation loss: 0.02475969, Gradient norm: 1.38189153
INFO:root:[    5] Training loss: 0.03595648, Validation loss: 0.02465719, Gradient norm: 1.35220501
INFO:root:[    6] Training loss: 0.03268619, Validation loss: 0.03214157, Gradient norm: 1.22425326
INFO:root:[    7] Training loss: 0.03178103, Validation loss: 0.02321767, Gradient norm: 1.27884101
INFO:root:[    8] Training loss: 0.02854673, Validation loss: 0.01874361, Gradient norm: 1.02029083
INFO:root:[    9] Training loss: 0.03014012, Validation loss: 0.02777072, Gradient norm: 1.12720852
INFO:root:[   10] Training loss: 0.02768738, Validation loss: 0.01606057, Gradient norm: 1.07891767
INFO:root:[   11] Training loss: 0.02758557, Validation loss: 0.02350182, Gradient norm: 1.13982521
INFO:root:[   12] Training loss: 0.02720502, Validation loss: 0.02063388, Gradient norm: 1.15125381
INFO:root:[   13] Training loss: 0.02726648, Validation loss: 0.02110577, Gradient norm: 1.08315225
INFO:root:[   14] Training loss: 0.02674417, Validation loss: 0.01945392, Gradient norm: 1.00369809
INFO:root:[   15] Training loss: 0.02482150, Validation loss: 0.01436710, Gradient norm: 1.04666110
INFO:root:[   16] Training loss: 0.02442505, Validation loss: 0.02243126, Gradient norm: 0.99730335
INFO:root:[   17] Training loss: 0.02491769, Validation loss: 0.02234218, Gradient norm: 1.02058671
INFO:root:[   18] Training loss: 0.02393288, Validation loss: 0.02149009, Gradient norm: 1.01184466
INFO:root:[   19] Training loss: 0.02301420, Validation loss: 0.02286854, Gradient norm: 0.98056385
INFO:root:[   20] Training loss: 0.02258314, Validation loss: 0.01922015, Gradient norm: 0.95768414
INFO:root:[   21] Training loss: 0.02346244, Validation loss: 0.01431965, Gradient norm: 0.94127766
INFO:root:[   22] Training loss: 0.02217653, Validation loss: 0.01597853, Gradient norm: 0.93446395
INFO:root:[   23] Training loss: 0.02190518, Validation loss: 0.01432139, Gradient norm: 0.91130618
INFO:root:[   24] Training loss: 0.02243873, Validation loss: 0.01221024, Gradient norm: 0.94135728
INFO:root:[   25] Training loss: 0.02115407, Validation loss: 0.01721379, Gradient norm: 0.87136830
INFO:root:[   26] Training loss: 0.02163783, Validation loss: 0.01783101, Gradient norm: 0.92875009
INFO:root:[   27] Training loss: 0.02007703, Validation loss: 0.02332813, Gradient norm: 0.84860273
INFO:root:[   28] Training loss: 0.02033336, Validation loss: 0.01767944, Gradient norm: 0.92787821
INFO:root:[   29] Training loss: 0.02006942, Validation loss: 0.01462046, Gradient norm: 0.91741290
INFO:root:[   30] Training loss: 0.02054361, Validation loss: 0.02153992, Gradient norm: 0.93792794
INFO:root:[   31] Training loss: 0.01991222, Validation loss: 0.02047256, Gradient norm: 0.86342613
INFO:root:[   32] Training loss: 0.02003941, Validation loss: 0.02032192, Gradient norm: 0.89675740
INFO:root:[   33] Training loss: 0.02086600, Validation loss: 0.01087624, Gradient norm: 0.88438272
INFO:root:[   34] Training loss: 0.01989552, Validation loss: 0.01399434, Gradient norm: 0.88361878
INFO:root:[   35] Training loss: 0.01955003, Validation loss: 0.01205726, Gradient norm: 0.86257427
INFO:root:[   36] Training loss: 0.01958851, Validation loss: 0.01025975, Gradient norm: 0.87219240
INFO:root:[   37] Training loss: 0.01933427, Validation loss: 0.01734566, Gradient norm: 0.87769059
INFO:root:[   38] Training loss: 0.01855082, Validation loss: 0.01495352, Gradient norm: 0.83199222
INFO:root:[   39] Training loss: 0.01842830, Validation loss: 0.01685530, Gradient norm: 0.84029358
INFO:root:[   40] Training loss: 0.01860933, Validation loss: 0.02152233, Gradient norm: 0.85151231
INFO:root:[   41] Training loss: 0.01828467, Validation loss: 0.02119538, Gradient norm: 0.83284420
INFO:root:[   42] Training loss: 0.01874633, Validation loss: 0.02000885, Gradient norm: 0.78672479
INFO:root:[   43] Training loss: 0.01816765, Validation loss: 0.02025820, Gradient norm: 0.79151926
INFO:root:[   44] Training loss: 0.01807620, Validation loss: 0.02172416, Gradient norm: 0.82268306
INFO:root:[   45] Training loss: 0.01837244, Validation loss: 0.01669868, Gradient norm: 0.85442129
INFO:root:[   46] Training loss: 0.01800662, Validation loss: 0.01286871, Gradient norm: 0.83040948
INFO:root:[   47] Training loss: 0.01794794, Validation loss: 0.01125280, Gradient norm: 0.79367311
INFO:root:[   48] Training loss: 0.01791597, Validation loss: 0.00828442, Gradient norm: 0.81412970
INFO:root:[   49] Training loss: 0.01790286, Validation loss: 0.00996865, Gradient norm: 0.82679017
INFO:root:[   50] Training loss: 0.01767606, Validation loss: 0.00968484, Gradient norm: 0.83505807
INFO:root:[   51] Training loss: 0.01716723, Validation loss: 0.00822441, Gradient norm: 0.77783413
INFO:root:[   52] Training loss: 0.01726711, Validation loss: 0.01468669, Gradient norm: 0.82851620
INFO:root:[   53] Training loss: 0.01721970, Validation loss: 0.02346328, Gradient norm: 0.80556909
INFO:root:[   54] Training loss: 0.01733088, Validation loss: 0.01008564, Gradient norm: 0.85735902
INFO:root:[   55] Training loss: 0.01738852, Validation loss: 0.00681660, Gradient norm: 0.77348392
INFO:root:[   56] Training loss: 0.01759455, Validation loss: 0.00876178, Gradient norm: 0.73853892
INFO:root:[   57] Training loss: 0.01696030, Validation loss: 0.00974614, Gradient norm: 0.77249980
INFO:root:[   58] Training loss: 0.01674353, Validation loss: 0.00685915, Gradient norm: 0.79347641
INFO:root:[   59] Training loss: 0.01692152, Validation loss: 0.01506110, Gradient norm: 0.83506558
INFO:root:[   60] Training loss: 0.01627715, Validation loss: 0.01886561, Gradient norm: 0.78731805
INFO:root:[   61] Training loss: 0.01671822, Validation loss: 0.01640810, Gradient norm: 0.80395410
INFO:root:[   62] Training loss: 0.01670378, Validation loss: 0.01536150, Gradient norm: 0.69554525
INFO:root:[   63] Training loss: 0.01603658, Validation loss: 0.02039708, Gradient norm: 0.69752614
INFO:root:[   64] Training loss: 0.01653283, Validation loss: 0.01804373, Gradient norm: 0.77493058
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 985.279s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.14106
INFO:root:EnergyScoreTrain: 0.0724
INFO:root:CoverageTrain: 0.74961
INFO:root:IntervalWidthTrain: 0.46145
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.13102
INFO:root:EnergyScoreValidation: 0.07183
INFO:root:CoverageValidation: 0.6887
INFO:root:IntervalWidthValidation: 0.41047
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.1532
INFO:root:EnergyScoreTest: 0.06779
INFO:root:CoverageTest: 0.69986
INFO:root:IntervalWidthTest: 0.44856
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1927282688
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.21310195, Validation loss: 0.03408694, Gradient norm: 2.07505276
INFO:root:[    2] Training loss: 0.04690539, Validation loss: 0.02941872, Gradient norm: 1.45010745
INFO:root:[    3] Training loss: 0.04718973, Validation loss: 0.04414770, Gradient norm: 1.85104821
INFO:root:[    4] Training loss: 0.04031867, Validation loss: 0.04258310, Gradient norm: 1.55765629
INFO:root:[    5] Training loss: 0.03751079, Validation loss: 0.03859356, Gradient norm: 1.64349589
INFO:root:[    6] Training loss: 0.03409549, Validation loss: 0.03512286, Gradient norm: 1.52010436
INFO:root:[    7] Training loss: 0.03212925, Validation loss: 0.02865092, Gradient norm: 1.45904171
INFO:root:[    8] Training loss: 0.03083727, Validation loss: 0.02675878, Gradient norm: 1.25286678
INFO:root:[    9] Training loss: 0.02902200, Validation loss: 0.01972557, Gradient norm: 1.25783895
INFO:root:[   10] Training loss: 0.02757654, Validation loss: 0.02822600, Gradient norm: 1.07275253
INFO:root:[   11] Training loss: 0.02678405, Validation loss: 0.01878771, Gradient norm: 1.14800653
INFO:root:[   12] Training loss: 0.02607745, Validation loss: 0.02305704, Gradient norm: 1.14890346
INFO:root:[   13] Training loss: 0.02641912, Validation loss: 0.02077825, Gradient norm: 1.16947356
INFO:root:[   14] Training loss: 0.02548826, Validation loss: 0.02334184, Gradient norm: 1.11530285
INFO:root:[   15] Training loss: 0.02521368, Validation loss: 0.02199868, Gradient norm: 1.04091001
INFO:root:[   16] Training loss: 0.02417413, Validation loss: 0.01892066, Gradient norm: 1.01112076
INFO:root:[   17] Training loss: 0.02459722, Validation loss: 0.01720431, Gradient norm: 1.05812423
INFO:root:[   18] Training loss: 0.02384501, Validation loss: 0.03142259, Gradient norm: 1.02231704
INFO:root:[   19] Training loss: 0.02392148, Validation loss: 0.02701406, Gradient norm: 1.04940038
INFO:root:[   20] Training loss: 0.02290659, Validation loss: 0.01371475, Gradient norm: 1.01618402
INFO:root:[   21] Training loss: 0.02359310, Validation loss: 0.01717283, Gradient norm: 0.97897312
INFO:root:[   22] Training loss: 0.02302783, Validation loss: 0.01367514, Gradient norm: 0.91037165
INFO:root:[   23] Training loss: 0.02222702, Validation loss: 0.02841090, Gradient norm: 0.92297268
INFO:root:[   24] Training loss: 0.02143897, Validation loss: 0.01026263, Gradient norm: 0.96586160
INFO:root:[   25] Training loss: 0.02120376, Validation loss: 0.01214517, Gradient norm: 0.97723611
INFO:root:[   26] Training loss: 0.02162044, Validation loss: 0.01009006, Gradient norm: 0.95693673
INFO:root:[   27] Training loss: 0.02166769, Validation loss: 0.01395582, Gradient norm: 0.99437212
INFO:root:[   28] Training loss: 0.02134309, Validation loss: 0.01412000, Gradient norm: 0.98586455
INFO:root:[   29] Training loss: 0.02127448, Validation loss: 0.01258787, Gradient norm: 0.94718861
INFO:root:[   30] Training loss: 0.02119096, Validation loss: 0.01064813, Gradient norm: 0.91161260
INFO:root:[   31] Training loss: 0.02045774, Validation loss: 0.01291875, Gradient norm: 0.89624488
INFO:root:[   32] Training loss: 0.01993948, Validation loss: 0.00884979, Gradient norm: 0.83133189
INFO:root:[   33] Training loss: 0.02020346, Validation loss: 0.01134811, Gradient norm: 0.90856901
INFO:root:[   34] Training loss: 0.02038099, Validation loss: 0.01303253, Gradient norm: 0.88075672
INFO:root:[   35] Training loss: 0.02017726, Validation loss: 0.00815201, Gradient norm: 0.91376461
INFO:root:[   36] Training loss: 0.01996736, Validation loss: 0.01507904, Gradient norm: 0.91704665
INFO:root:[   37] Training loss: 0.01917720, Validation loss: 0.01913801, Gradient norm: 0.88460686
INFO:root:[   38] Training loss: 0.01962218, Validation loss: 0.02217116, Gradient norm: 0.83893838
INFO:root:[   39] Training loss: 0.01957157, Validation loss: 0.02427771, Gradient norm: 0.82842681
INFO:root:[   40] Training loss: 0.01923541, Validation loss: 0.02318451, Gradient norm: 0.80378822
INFO:root:[   41] Training loss: 0.01907900, Validation loss: 0.02411683, Gradient norm: 0.85848720
INFO:root:[   42] Training loss: 0.01892673, Validation loss: 0.02254150, Gradient norm: 0.84833006
INFO:root:[   43] Training loss: 0.01925564, Validation loss: 0.02216944, Gradient norm: 0.85994450
INFO:root:[   44] Training loss: 0.01918471, Validation loss: 0.01113226, Gradient norm: 0.89515170
INFO:root:[   45] Training loss: 0.01865841, Validation loss: 0.00802197, Gradient norm: 0.85075550
INFO:root:[   46] Training loss: 0.01862549, Validation loss: 0.00860196, Gradient norm: 0.82534730
INFO:root:[   47] Training loss: 0.01878372, Validation loss: 0.01296720, Gradient norm: 0.86482524
INFO:root:[   48] Training loss: 0.01832534, Validation loss: 0.01851576, Gradient norm: 0.82801686
INFO:root:[   49] Training loss: 0.01868627, Validation loss: 0.02163695, Gradient norm: 0.84477417
INFO:root:[   50] Training loss: 0.01862525, Validation loss: 0.01539351, Gradient norm: 0.83805690
INFO:root:[   51] Training loss: 0.01856319, Validation loss: 0.01408291, Gradient norm: 0.84080812
INFO:root:[   52] Training loss: 0.01819832, Validation loss: 0.00671030, Gradient norm: 0.81220253
INFO:root:[   53] Training loss: 0.01823344, Validation loss: 0.02030182, Gradient norm: 0.84116228
INFO:root:[   54] Training loss: 0.01785628, Validation loss: 0.01879258, Gradient norm: 0.81399913
INFO:root:[   55] Training loss: 0.01711583, Validation loss: 0.02000745, Gradient norm: 0.77762597
INFO:root:[   56] Training loss: 0.01790984, Validation loss: 0.01327941, Gradient norm: 0.79821906
INFO:root:[   57] Training loss: 0.01802983, Validation loss: 0.00797187, Gradient norm: 0.71337827
INFO:root:[   58] Training loss: 0.01770413, Validation loss: 0.01090953, Gradient norm: 0.78526970
INFO:root:[   59] Training loss: 0.01788516, Validation loss: 0.01163934, Gradient norm: 0.85489997
INFO:root:[   60] Training loss: 0.01739393, Validation loss: 0.02101419, Gradient norm: 0.79168319
INFO:root:[   61] Training loss: 0.01747971, Validation loss: 0.01900124, Gradient norm: 0.77510198
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 937.125s.
INFO:root:Emptying the cuda cache took 0.053s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.13675
INFO:root:EnergyScoreTrain: 0.06939
INFO:root:CoverageTrain: 0.80981
INFO:root:IntervalWidthTrain: 0.50587
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.1878
INFO:root:EnergyScoreValidation: 0.09133
INFO:root:CoverageValidation: 0.66776
INFO:root:IntervalWidthValidation: 0.53123
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.16981
INFO:root:EnergyScoreTest: 0.0913
INFO:root:CoverageTest: 0.60909
INFO:root:IntervalWidthTest: 0.34644
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1855979520
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.19635898, Validation loss: 0.04111380, Gradient norm: 1.79584580
INFO:root:[    2] Training loss: 0.04645172, Validation loss: 0.02918160, Gradient norm: 1.45265312
INFO:root:[    3] Training loss: 0.04152287, Validation loss: 0.04361815, Gradient norm: 1.29910820
INFO:root:[    4] Training loss: 0.03871264, Validation loss: 0.02732758, Gradient norm: 1.35122453
INFO:root:[    5] Training loss: 0.03300192, Validation loss: 0.02676863, Gradient norm: 1.18487053
INFO:root:[    6] Training loss: 0.03079798, Validation loss: 0.02489591, Gradient norm: 1.19408243
INFO:root:[    7] Training loss: 0.02911076, Validation loss: 0.02418063, Gradient norm: 1.08264940
INFO:root:[    8] Training loss: 0.02809738, Validation loss: 0.02521310, Gradient norm: 1.07275976
INFO:root:[    9] Training loss: 0.02704042, Validation loss: 0.01574242, Gradient norm: 1.04182644
INFO:root:[   10] Training loss: 0.02454166, Validation loss: 0.02999127, Gradient norm: 0.89396822
INFO:root:[   11] Training loss: 0.02569769, Validation loss: 0.02342509, Gradient norm: 0.97066991
INFO:root:[   12] Training loss: 0.02398417, Validation loss: 0.01850229, Gradient norm: 0.89827182
INFO:root:[   13] Training loss: 0.02481761, Validation loss: 0.01700214, Gradient norm: 0.97349216
INFO:root:[   14] Training loss: 0.02492095, Validation loss: 0.02048264, Gradient norm: 1.02545871
INFO:root:[   15] Training loss: 0.02455057, Validation loss: 0.02046730, Gradient norm: 0.93566631
INFO:root:[   16] Training loss: 0.02378185, Validation loss: 0.01836597, Gradient norm: 0.90865550
INFO:root:[   17] Training loss: 0.02369709, Validation loss: 0.01775969, Gradient norm: 1.01520879
INFO:root:[   18] Training loss: 0.02219677, Validation loss: 0.02166107, Gradient norm: 0.96271398
INFO:root:[   19] Training loss: 0.02273163, Validation loss: 0.02395541, Gradient norm: 0.85918150
INFO:root:[   20] Training loss: 0.02127421, Validation loss: 0.02611429, Gradient norm: 0.86394038
INFO:root:[   21] Training loss: 0.02125975, Validation loss: 0.02691488, Gradient norm: 0.90114602
INFO:root:[   22] Training loss: 0.02145293, Validation loss: 0.02227483, Gradient norm: 0.88677704
INFO:root:[   23] Training loss: 0.02198920, Validation loss: 0.01377082, Gradient norm: 0.88376468
INFO:root:[   24] Training loss: 0.02120523, Validation loss: 0.01368514, Gradient norm: 0.81847746
INFO:root:[   25] Training loss: 0.02044097, Validation loss: 0.01382692, Gradient norm: 0.85648834
INFO:root:[   26] Training loss: 0.01956567, Validation loss: 0.01228299, Gradient norm: 0.85319299
INFO:root:[   27] Training loss: 0.01939830, Validation loss: 0.01350704, Gradient norm: 0.86358018
INFO:root:[   28] Training loss: 0.02031362, Validation loss: 0.01508998, Gradient norm: 0.89562441
INFO:root:[   29] Training loss: 0.01944544, Validation loss: 0.01790722, Gradient norm: 0.85478455
INFO:root:[   30] Training loss: 0.01956518, Validation loss: 0.02324266, Gradient norm: 0.79335472
INFO:root:[   31] Training loss: 0.01882117, Validation loss: 0.02013898, Gradient norm: 0.82257767
INFO:root:[   32] Training loss: 0.01856503, Validation loss: 0.02088148, Gradient norm: 0.83750343
INFO:root:[   33] Training loss: 0.01902873, Validation loss: 0.01406289, Gradient norm: 0.82637809
INFO:root:[   34] Training loss: 0.01852338, Validation loss: 0.00886305, Gradient norm: 0.81189469
INFO:root:[   35] Training loss: 0.01816716, Validation loss: 0.00987406, Gradient norm: 0.82399258
INFO:root:[   36] Training loss: 0.01820480, Validation loss: 0.01229351, Gradient norm: 0.80613656
INFO:root:[   37] Training loss: 0.01827287, Validation loss: 0.00918210, Gradient norm: 0.86230318
INFO:root:[   38] Training loss: 0.01761266, Validation loss: 0.02085037, Gradient norm: 0.82929771
INFO:root:[   39] Training loss: 0.01784091, Validation loss: 0.01679034, Gradient norm: 0.78598333
INFO:root:[   40] Training loss: 0.01748652, Validation loss: 0.01425630, Gradient norm: 0.77405802
INFO:root:[   41] Training loss: 0.01799427, Validation loss: 0.01295103, Gradient norm: 0.78394486
INFO:root:[   42] Training loss: 0.01749721, Validation loss: 0.01103972, Gradient norm: 0.76843141
INFO:root:[   43] Training loss: 0.01742647, Validation loss: 0.00716706, Gradient norm: 0.84017907
INFO:root:[   44] Training loss: 0.01759471, Validation loss: 0.01767341, Gradient norm: 0.85003869
INFO:root:[   45] Training loss: 0.01675089, Validation loss: 0.01329200, Gradient norm: 0.79331682
INFO:root:[   46] Training loss: 0.01731275, Validation loss: 0.00924996, Gradient norm: 0.82114767
INFO:root:[   47] Training loss: 0.01707262, Validation loss: 0.01164944, Gradient norm: 0.81918713
INFO:root:[   48] Training loss: 0.01704827, Validation loss: 0.01899885, Gradient norm: 0.77764435
INFO:root:[   49] Training loss: 0.01686836, Validation loss: 0.01021233, Gradient norm: 0.79395261
INFO:root:[   50] Training loss: 0.01680783, Validation loss: 0.00830564, Gradient norm: 0.77968401
INFO:root:[   51] Training loss: 0.01645273, Validation loss: 0.01087234, Gradient norm: 0.77313155
INFO:root:[   52] Training loss: 0.01605399, Validation loss: 0.00722660, Gradient norm: 0.67368413
INFO:root:[   53] Training loss: 0.01599109, Validation loss: 0.00639128, Gradient norm: 0.70390462
INFO:root:[   54] Training loss: 0.01616351, Validation loss: 0.01500216, Gradient norm: 0.76434769
INFO:root:[   55] Training loss: 0.01585968, Validation loss: 0.00839652, Gradient norm: 0.67289104
INFO:root:[   56] Training loss: 0.01616543, Validation loss: 0.01176484, Gradient norm: 0.75054932
INFO:root:[   57] Training loss: 0.01607367, Validation loss: 0.01768793, Gradient norm: 0.77363517
INFO:root:[   58] Training loss: 0.01652471, Validation loss: 0.01604221, Gradient norm: 0.77597388
INFO:root:[   59] Training loss: 0.01588388, Validation loss: 0.00738051, Gradient norm: 0.77366661
INFO:root:[   60] Training loss: 0.01565105, Validation loss: 0.00905918, Gradient norm: 0.78469707
INFO:root:[   61] Training loss: 0.01556227, Validation loss: 0.00655046, Gradient norm: 0.78643230
INFO:root:[   62] Training loss: 0.01539849, Validation loss: 0.01806103, Gradient norm: 0.77273131
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 954.226s.
INFO:root:Emptying the cuda cache took 0.036s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.15945
INFO:root:EnergyScoreTrain: 0.07258
INFO:root:CoverageTrain: 0.75766
INFO:root:IntervalWidthTrain: 0.50071
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.14142
INFO:root:EnergyScoreValidation: 0.07673
INFO:root:CoverageValidation: 0.81452
INFO:root:IntervalWidthValidation: 0.49745
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.12437
INFO:root:EnergyScoreTest: 0.06911
INFO:root:CoverageTest: 0.75924
INFO:root:IntervalWidthTest: 0.45873
INFO:root:###6 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1939865600
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.24685601, Validation loss: 0.09218144, Gradient norm: 2.27468545
INFO:root:[    2] Training loss: 0.05829464, Validation loss: 0.03504114, Gradient norm: 1.54507367
INFO:root:[    3] Training loss: 0.04443913, Validation loss: 0.02852328, Gradient norm: 1.37405887
INFO:root:[    4] Training loss: 0.03971782, Validation loss: 0.03587523, Gradient norm: 1.34526861
INFO:root:[    5] Training loss: 0.03512303, Validation loss: 0.02673439, Gradient norm: 1.28584192
INFO:root:[    6] Training loss: 0.03247811, Validation loss: 0.02222005, Gradient norm: 1.33285419
INFO:root:[    7] Training loss: 0.03092534, Validation loss: 0.02964368, Gradient norm: 1.24077378
INFO:root:[    8] Training loss: 0.02891375, Validation loss: 0.02623546, Gradient norm: 1.16391510
INFO:root:[    9] Training loss: 0.02792741, Validation loss: 0.01934889, Gradient norm: 1.09735985
INFO:root:[   10] Training loss: 0.02759559, Validation loss: 0.02211899, Gradient norm: 1.09946147
INFO:root:[   11] Training loss: 0.02683204, Validation loss: 0.01886613, Gradient norm: 1.10673169
INFO:root:[   12] Training loss: 0.02616455, Validation loss: 0.01533336, Gradient norm: 1.09857212
INFO:root:[   13] Training loss: 0.02569255, Validation loss: 0.02156016, Gradient norm: 1.06271519
INFO:root:[   14] Training loss: 0.02516031, Validation loss: 0.01948888, Gradient norm: 0.94023179
INFO:root:[   15] Training loss: 0.02368299, Validation loss: 0.01676286, Gradient norm: 0.97453951
INFO:root:[   16] Training loss: 0.02401131, Validation loss: 0.01371649, Gradient norm: 1.03852976
INFO:root:[   17] Training loss: 0.02354896, Validation loss: 0.01833876, Gradient norm: 1.00870624
INFO:root:[   18] Training loss: 0.02373237, Validation loss: 0.01678886, Gradient norm: 0.94868669
INFO:root:[   19] Training loss: 0.02346534, Validation loss: 0.02304379, Gradient norm: 0.83321966
INFO:root:[   20] Training loss: 0.02259952, Validation loss: 0.01357254, Gradient norm: 0.85334480
INFO:root:[   21] Training loss: 0.02215751, Validation loss: 0.01499409, Gradient norm: 0.98161414
INFO:root:[   22] Training loss: 0.02241030, Validation loss: 0.01448062, Gradient norm: 1.03152481
INFO:root:[   23] Training loss: 0.02237990, Validation loss: 0.01958759, Gradient norm: 0.98953950
INFO:root:[   24] Training loss: 0.02336537, Validation loss: 0.01586285, Gradient norm: 0.96279500
INFO:root:[   25] Training loss: 0.02124013, Validation loss: 0.01609309, Gradient norm: 0.80451544
INFO:root:[   26] Training loss: 0.02141288, Validation loss: 0.01731377, Gradient norm: 0.98047898
INFO:root:[   27] Training loss: 0.02123640, Validation loss: 0.01449778, Gradient norm: 0.98148050
INFO:root:[   28] Training loss: 0.02094296, Validation loss: 0.01643437, Gradient norm: 0.90785189
INFO:root:[   29] Training loss: 0.02058672, Validation loss: 0.01454105, Gradient norm: 0.92839781
INFO:root:[   30] Training loss: 0.02063783, Validation loss: 0.01559970, Gradient norm: 0.90498732
INFO:root:[   31] Training loss: 0.02013830, Validation loss: 0.01288554, Gradient norm: 0.90507583
INFO:root:[   32] Training loss: 0.01963414, Validation loss: 0.01388474, Gradient norm: 0.87615325
INFO:root:[   33] Training loss: 0.02009022, Validation loss: 0.01201439, Gradient norm: 0.90196591
INFO:root:[   34] Training loss: 0.02003883, Validation loss: 0.01258682, Gradient norm: 0.88924140
INFO:root:[   35] Training loss: 0.02017689, Validation loss: 0.01415041, Gradient norm: 0.93545390
INFO:root:[   36] Training loss: 0.01958893, Validation loss: 0.01237341, Gradient norm: 0.89999891
INFO:root:[   37] Training loss: 0.01900392, Validation loss: 0.01353678, Gradient norm: 0.89911195
INFO:root:[   38] Training loss: 0.01889840, Validation loss: 0.01228770, Gradient norm: 0.88437764
INFO:root:[   39] Training loss: 0.01940884, Validation loss: 0.01166893, Gradient norm: 0.85819434
INFO:root:[   40] Training loss: 0.01918812, Validation loss: 0.01430906, Gradient norm: 0.90070557
INFO:root:[   41] Training loss: 0.01951071, Validation loss: 0.01675951, Gradient norm: 0.87326352
INFO:root:[   42] Training loss: 0.01868675, Validation loss: 0.01503167, Gradient norm: 0.84139352
INFO:root:[   43] Training loss: 0.01820891, Validation loss: 0.01725836, Gradient norm: 0.88649305
INFO:root:[   44] Training loss: 0.01819721, Validation loss: 0.02364285, Gradient norm: 0.89259531
INFO:root:[   45] Training loss: 0.01814156, Validation loss: 0.02157938, Gradient norm: 0.87816576
INFO:root:[   46] Training loss: 0.01807124, Validation loss: 0.01913318, Gradient norm: 0.80359870
INFO:root:[   47] Training loss: 0.01760476, Validation loss: 0.02171372, Gradient norm: 0.82589161
INFO:root:[   48] Training loss: 0.01794717, Validation loss: 0.01879520, Gradient norm: 0.89139273
INFO:root:[   49] Training loss: 0.01770728, Validation loss: 0.01202144, Gradient norm: 0.86962322
INFO:root:[   50] Training loss: 0.01741784, Validation loss: 0.01406301, Gradient norm: 0.83933409
INFO:root:[   51] Training loss: 0.01711723, Validation loss: 0.01660667, Gradient norm: 0.86446946
INFO:root:[   52] Training loss: 0.01759912, Validation loss: 0.01342476, Gradient norm: 0.82822984
INFO:root:[   53] Training loss: 0.01739448, Validation loss: 0.01278663, Gradient norm: 0.85144789
INFO:root:[   54] Training loss: 0.01738868, Validation loss: 0.00622624, Gradient norm: 0.85667048
INFO:root:[   55] Training loss: 0.01702293, Validation loss: 0.00570688, Gradient norm: 0.81673778
INFO:root:[   56] Training loss: 0.01738746, Validation loss: 0.00942555, Gradient norm: 0.85270900
INFO:root:[   57] Training loss: 0.01709251, Validation loss: 0.01067312, Gradient norm: 0.79345434
INFO:root:[   58] Training loss: 0.01731803, Validation loss: 0.01173063, Gradient norm: 0.78107173
INFO:root:[   59] Training loss: 0.01657144, Validation loss: 0.00868206, Gradient norm: 0.78128070
INFO:root:[   60] Training loss: 0.01687029, Validation loss: 0.00733013, Gradient norm: 0.84048468
INFO:root:[   61] Training loss: 0.01671960, Validation loss: 0.00633191, Gradient norm: 0.86717020
INFO:root:[   62] Training loss: 0.01625854, Validation loss: 0.00908862, Gradient norm: 0.82981174
INFO:root:[   63] Training loss: 0.01696251, Validation loss: 0.02014890, Gradient norm: 0.85029450
INFO:root:[   64] Training loss: 0.01645709, Validation loss: 0.01789034, Gradient norm: 0.84089576
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 983.007s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.15929
INFO:root:EnergyScoreTrain: 0.07563
INFO:root:CoverageTrain: 0.70315
INFO:root:IntervalWidthTrain: 0.47441
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.18255
INFO:root:EnergyScoreValidation: 0.09726
INFO:root:CoverageValidation: 0.67512
INFO:root:IntervalWidthValidation: 0.51052
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.1748
INFO:root:EnergyScoreTest: 0.09234
INFO:root:CoverageTest: 0.60077
INFO:root:IntervalWidthTest: 0.45491
INFO:root:###7 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234567, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1900019712
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.22695526, Validation loss: 0.03529861, Gradient norm: 1.44015676
INFO:root:[    2] Training loss: 0.03983030, Validation loss: 0.02962035, Gradient norm: 0.95818694
INFO:root:[    3] Training loss: 0.03869775, Validation loss: 0.03159652, Gradient norm: 1.10113941
INFO:root:[    4] Training loss: 0.03538450, Validation loss: 0.02720860, Gradient norm: 1.11770886
INFO:root:[    5] Training loss: 0.03413370, Validation loss: 0.02153770, Gradient norm: 1.09784438
INFO:root:[    6] Training loss: 0.03049042, Validation loss: 0.02660563, Gradient norm: 1.02362903
INFO:root:[    7] Training loss: 0.02776432, Validation loss: 0.01878754, Gradient norm: 1.02793676
INFO:root:[    8] Training loss: 0.02769835, Validation loss: 0.01823032, Gradient norm: 1.06622565
INFO:root:[    9] Training loss: 0.02673549, Validation loss: 0.02303712, Gradient norm: 0.98704750
INFO:root:[   10] Training loss: 0.02675974, Validation loss: 0.01448149, Gradient norm: 1.02653360
INFO:root:[   11] Training loss: 0.02491334, Validation loss: 0.02095184, Gradient norm: 0.93726053
INFO:root:[   12] Training loss: 0.02475119, Validation loss: 0.01602865, Gradient norm: 0.89602370
INFO:root:[   13] Training loss: 0.02349313, Validation loss: 0.02333779, Gradient norm: 0.88879812
INFO:root:[   14] Training loss: 0.02342037, Validation loss: 0.01448679, Gradient norm: 0.92238572
INFO:root:[   15] Training loss: 0.02311103, Validation loss: 0.01965907, Gradient norm: 0.89557448
INFO:root:[   16] Training loss: 0.02338473, Validation loss: 0.02168279, Gradient norm: 0.98637732
INFO:root:[   17] Training loss: 0.02423789, Validation loss: 0.01474988, Gradient norm: 0.98866651
INFO:root:[   18] Training loss: 0.02237447, Validation loss: 0.01304554, Gradient norm: 0.93499518
INFO:root:[   19] Training loss: 0.02232932, Validation loss: 0.01500852, Gradient norm: 0.90302367
INFO:root:[   20] Training loss: 0.02235373, Validation loss: 0.01463783, Gradient norm: 0.90546661
INFO:root:[   21] Training loss: 0.02171378, Validation loss: 0.01465227, Gradient norm: 0.80582823
INFO:root:[   22] Training loss: 0.02182893, Validation loss: 0.01424541, Gradient norm: 0.86014794
INFO:root:[   23] Training loss: 0.02148075, Validation loss: 0.01667887, Gradient norm: 0.96238026
INFO:root:[   24] Training loss: 0.02056580, Validation loss: 0.02306889, Gradient norm: 0.91640126
INFO:root:[   25] Training loss: 0.02011733, Validation loss: 0.02258739, Gradient norm: 0.87081315
INFO:root:[   26] Training loss: 0.01967004, Validation loss: 0.01589298, Gradient norm: 0.86217384
INFO:root:[   27] Training loss: 0.02034155, Validation loss: 0.02407147, Gradient norm: 0.87368648
INFO:root:[   28] Training loss: 0.02069219, Validation loss: 0.01608413, Gradient norm: 0.91303810
INFO:root:[   29] Training loss: 0.01984886, Validation loss: 0.00855746, Gradient norm: 0.86831756
INFO:root:[   30] Training loss: 0.01955850, Validation loss: 0.01092419, Gradient norm: 0.87888588
INFO:root:[   31] Training loss: 0.01903753, Validation loss: 0.00983527, Gradient norm: 0.79791600
INFO:root:[   32] Training loss: 0.01914646, Validation loss: 0.01231619, Gradient norm: 0.84907071
INFO:root:[   33] Training loss: 0.01873732, Validation loss: 0.01388502, Gradient norm: 0.86922372
INFO:root:[   34] Training loss: 0.01845744, Validation loss: 0.01259461, Gradient norm: 0.88490042
INFO:root:[   35] Training loss: 0.01813734, Validation loss: 0.01037155, Gradient norm: 0.87366153
INFO:root:[   36] Training loss: 0.01871669, Validation loss: 0.01029782, Gradient norm: 0.86473220
INFO:root:[   37] Training loss: 0.01829035, Validation loss: 0.01933223, Gradient norm: 0.85575310
INFO:root:[   38] Training loss: 0.01804100, Validation loss: 0.01932554, Gradient norm: 0.85258115
INFO:root:[   39] Training loss: 0.01774904, Validation loss: 0.01415341, Gradient norm: 0.86934774
INFO:root:[   40] Training loss: 0.01770590, Validation loss: 0.00865381, Gradient norm: 0.86238531
INFO:root:[   41] Training loss: 0.01768573, Validation loss: 0.01126465, Gradient norm: 0.84675143
INFO:root:[   42] Training loss: 0.01754761, Validation loss: 0.01289586, Gradient norm: 0.84442064
INFO:root:[   43] Training loss: 0.01762845, Validation loss: 0.00791460, Gradient norm: 0.82487451
INFO:root:[   44] Training loss: 0.01756255, Validation loss: 0.01953624, Gradient norm: 0.88433474
INFO:root:[   45] Training loss: 0.01729419, Validation loss: 0.02071462, Gradient norm: 0.79349831
INFO:root:[   46] Training loss: 0.01754986, Validation loss: 0.00978335, Gradient norm: 0.82229899
INFO:root:[   47] Training loss: 0.01696759, Validation loss: 0.00624499, Gradient norm: 0.79808614
INFO:root:[   48] Training loss: 0.01677882, Validation loss: 0.01148734, Gradient norm: 0.82439911
INFO:root:[   49] Training loss: 0.01689410, Validation loss: 0.00608392, Gradient norm: 0.88525280
INFO:root:[   50] Training loss: 0.01677314, Validation loss: 0.02085313, Gradient norm: 0.84964045
INFO:root:[   51] Training loss: 0.01696113, Validation loss: 0.00877597, Gradient norm: 0.84599306
INFO:root:[   52] Training loss: 0.01655464, Validation loss: 0.00826921, Gradient norm: 0.81442271
INFO:root:[   53] Training loss: 0.01658372, Validation loss: 0.00973326, Gradient norm: 0.81597894
INFO:root:[   54] Training loss: 0.01615957, Validation loss: 0.00789073, Gradient norm: 0.78583480
INFO:root:[   55] Training loss: 0.01637897, Validation loss: 0.01465527, Gradient norm: 0.84159774
INFO:root:[   56] Training loss: 0.01627743, Validation loss: 0.01947224, Gradient norm: 0.80364291
INFO:root:[   57] Training loss: 0.01603953, Validation loss: 0.01559863, Gradient norm: 0.78435791
INFO:root:[   58] Training loss: 0.01646131, Validation loss: 0.00813961, Gradient norm: 0.81283716
INFO:root:[   59] Training loss: 0.01586185, Validation loss: 0.00733837, Gradient norm: 0.79171562
INFO:root:[   60] Training loss: 0.01584392, Validation loss: 0.00710106, Gradient norm: 0.81150257
INFO:root:[   61] Training loss: 0.01567570, Validation loss: 0.00467116, Gradient norm: 0.79066896
INFO:root:[   62] Training loss: 0.01536669, Validation loss: 0.00854178, Gradient norm: 0.79884311
INFO:root:[   63] Training loss: 0.01547297, Validation loss: 0.01671185, Gradient norm: 0.80650269
INFO:root:[   64] Training loss: 0.01546065, Validation loss: 0.01809606, Gradient norm: 0.77657192
INFO:root:[   65] Training loss: 0.01605664, Validation loss: 0.01260912, Gradient norm: 0.78055883
INFO:root:[   66] Training loss: 0.01580379, Validation loss: 0.00806788, Gradient norm: 0.80772630
INFO:root:[   67] Training loss: 0.01538285, Validation loss: 0.00609696, Gradient norm: 0.80236071
INFO:root:[   68] Training loss: 0.01560220, Validation loss: 0.01454130, Gradient norm: 0.81999939
INFO:root:[   69] Training loss: 0.01508079, Validation loss: 0.01707324, Gradient norm: 0.76915487
INFO:root:[   70] Training loss: 0.01545357, Validation loss: 0.01466169, Gradient norm: 0.76740160
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1073.397s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.16009
INFO:root:EnergyScoreTrain: 0.08743
INFO:root:CoverageTrain: 0.68782
INFO:root:IntervalWidthTrain: 0.4657
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.18459
INFO:root:EnergyScoreValidation: 0.08729
INFO:root:CoverageValidation: 0.77914
INFO:root:IntervalWidthValidation: 0.64684
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.19375
INFO:root:EnergyScoreTest: 0.10212
INFO:root:CoverageTest: 0.67684
INFO:root:IntervalWidthTest: 0.5518
INFO:root:###8 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345678, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1929379840
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.18860661, Validation loss: 0.03410922, Gradient norm: 1.87265660
INFO:root:[    2] Training loss: 0.05012868, Validation loss: 0.04818195, Gradient norm: 1.01172875
INFO:root:[    3] Training loss: 0.04007516, Validation loss: 0.03772712, Gradient norm: 1.30991730
INFO:root:[    4] Training loss: 0.03434487, Validation loss: 0.02145788, Gradient norm: 1.06943829
INFO:root:[    5] Training loss: 0.03284590, Validation loss: 0.02269373, Gradient norm: 1.14934574
INFO:root:[    6] Training loss: 0.02917965, Validation loss: 0.03110356, Gradient norm: 1.01172323
INFO:root:[    7] Training loss: 0.02810710, Validation loss: 0.01832690, Gradient norm: 0.99547699
INFO:root:[    8] Training loss: 0.02664831, Validation loss: 0.01824438, Gradient norm: 0.88159341
INFO:root:[    9] Training loss: 0.02680760, Validation loss: 0.02623282, Gradient norm: 0.95362065
INFO:root:[   10] Training loss: 0.02635177, Validation loss: 0.01681968, Gradient norm: 1.02868639
INFO:root:[   11] Training loss: 0.02477558, Validation loss: 0.02257139, Gradient norm: 0.90821474
INFO:root:[   12] Training loss: 0.02550017, Validation loss: 0.01475439, Gradient norm: 0.98961512
INFO:root:[   13] Training loss: 0.02402012, Validation loss: 0.01457642, Gradient norm: 0.89207491
INFO:root:[   14] Training loss: 0.02341948, Validation loss: 0.01947276, Gradient norm: 0.87160697
INFO:root:[   15] Training loss: 0.02307274, Validation loss: 0.02155526, Gradient norm: 0.87819702
INFO:root:[   16] Training loss: 0.02367954, Validation loss: 0.01504608, Gradient norm: 0.90791669
INFO:root:[   17] Training loss: 0.02245678, Validation loss: 0.02279338, Gradient norm: 0.78320088
INFO:root:[   18] Training loss: 0.02299259, Validation loss: 0.02007397, Gradient norm: 0.88700583
INFO:root:[   19] Training loss: 0.02266567, Validation loss: 0.01367770, Gradient norm: 0.93462800
INFO:root:[   20] Training loss: 0.02119525, Validation loss: 0.01720252, Gradient norm: 0.88890015
INFO:root:[   21] Training loss: 0.02130262, Validation loss: 0.01696374, Gradient norm: 0.88625601
INFO:root:[   22] Training loss: 0.02084153, Validation loss: 0.01571296, Gradient norm: 0.87014400
INFO:root:[   23] Training loss: 0.02054838, Validation loss: 0.01360542, Gradient norm: 0.87906718
INFO:root:[   24] Training loss: 0.02065174, Validation loss: 0.01504164, Gradient norm: 0.86282185
INFO:root:[   25] Training loss: 0.02060656, Validation loss: 0.01368113, Gradient norm: 0.79099229
INFO:root:[   26] Training loss: 0.02031567, Validation loss: 0.02406008, Gradient norm: 0.87906972
INFO:root:[   27] Training loss: 0.01985891, Validation loss: 0.01816631, Gradient norm: 0.88359173
INFO:root:[   28] Training loss: 0.01972038, Validation loss: 0.01348009, Gradient norm: 0.90907679
INFO:root:[   29] Training loss: 0.01954912, Validation loss: 0.01083081, Gradient norm: 0.86930894
INFO:root:[   30] Training loss: 0.01914159, Validation loss: 0.01744051, Gradient norm: 0.89445121
INFO:root:[   31] Training loss: 0.01881613, Validation loss: 0.01983917, Gradient norm: 0.85006978
INFO:root:[   32] Training loss: 0.01868787, Validation loss: 0.01482739, Gradient norm: 0.87130172
INFO:root:[   33] Training loss: 0.01892179, Validation loss: 0.01038837, Gradient norm: 0.78949594
INFO:root:[   34] Training loss: 0.01828652, Validation loss: 0.01488058, Gradient norm: 0.82751770
INFO:root:[   35] Training loss: 0.01841309, Validation loss: 0.00824941, Gradient norm: 0.80835746
INFO:root:[   36] Training loss: 0.01835041, Validation loss: 0.01660528, Gradient norm: 0.79828375
INFO:root:[   37] Training loss: 0.01762956, Validation loss: 0.02054021, Gradient norm: 0.81075626
INFO:root:[   38] Training loss: 0.01727607, Validation loss: 0.01949816, Gradient norm: 0.81431607
INFO:root:[   39] Training loss: 0.01768034, Validation loss: 0.00939194, Gradient norm: 0.85593654
INFO:root:[   40] Training loss: 0.01777352, Validation loss: 0.01204780, Gradient norm: 0.83562286
INFO:root:[   41] Training loss: 0.01770029, Validation loss: 0.01406449, Gradient norm: 0.87551596
INFO:root:[   42] Training loss: 0.01700855, Validation loss: 0.02159399, Gradient norm: 0.81031662
INFO:root:[   43] Training loss: 0.01726825, Validation loss: 0.01282512, Gradient norm: 0.84106784
INFO:root:[   44] Training loss: 0.01716211, Validation loss: 0.00976275, Gradient norm: 0.79666999
INFO:root:[   45] Training loss: 0.01704413, Validation loss: 0.00635942, Gradient norm: 0.81346839
INFO:root:[   46] Training loss: 0.01701218, Validation loss: 0.02059232, Gradient norm: 0.79241978
INFO:root:[   47] Training loss: 0.01723128, Validation loss: 0.01320586, Gradient norm: 0.79756680
INFO:root:[   48] Training loss: 0.01622051, Validation loss: 0.00939232, Gradient norm: 0.75940824
INFO:root:[   49] Training loss: 0.01636314, Validation loss: 0.00686051, Gradient norm: 0.78269076
INFO:root:[   50] Training loss: 0.01639781, Validation loss: 0.00998697, Gradient norm: 0.81819913
INFO:root:[   51] Training loss: 0.01651516, Validation loss: 0.00731992, Gradient norm: 0.81360471
INFO:root:[   52] Training loss: 0.01631328, Validation loss: 0.01679569, Gradient norm: 0.76992872
INFO:root:[   53] Training loss: 0.01630062, Validation loss: 0.01716474, Gradient norm: 0.74288306
INFO:root:[   54] Training loss: 0.01571704, Validation loss: 0.01851514, Gradient norm: 0.74225968
INFO:root:[   55] Training loss: 0.01597888, Validation loss: 0.01252228, Gradient norm: 0.79724362
INFO:root:[   56] Training loss: 0.01595650, Validation loss: 0.00947970, Gradient norm: 0.76402998
INFO:root:[   57] Training loss: 0.01574802, Validation loss: 0.00892066, Gradient norm: 0.76147595
INFO:root:[   58] Training loss: 0.01600307, Validation loss: 0.00715302, Gradient norm: 0.79897789
INFO:root:[   59] Training loss: 0.01568258, Validation loss: 0.01231752, Gradient norm: 0.82450189
INFO:root:[   60] Training loss: 0.01553391, Validation loss: 0.01908031, Gradient norm: 0.76688884
INFO:root:[   61] Training loss: 0.01574606, Validation loss: 0.01219015, Gradient norm: 0.78740133
INFO:root:[   62] Training loss: 0.01600758, Validation loss: 0.00699483, Gradient norm: 0.76331757
INFO:root:[   63] Training loss: 0.01525826, Validation loss: 0.00817519, Gradient norm: 0.75645676
INFO:root:[   64] Training loss: 0.01533505, Validation loss: 0.00559937, Gradient norm: 0.81117312
INFO:root:[   65] Training loss: 0.01512773, Validation loss: 0.01338647, Gradient norm: 0.78513572
INFO:root:[   66] Training loss: 0.01509175, Validation loss: 0.01817575, Gradient norm: 0.80372062
INFO:root:[   67] Training loss: 0.01535750, Validation loss: 0.00560404, Gradient norm: 0.77468868
INFO:root:[   68] Training loss: 0.01520476, Validation loss: 0.00728328, Gradient norm: 0.75169170
INFO:root:[   69] Training loss: 0.01504931, Validation loss: 0.01110492, Gradient norm: 0.74002034
INFO:root:[   70] Training loss: 0.01515661, Validation loss: 0.00773615, Gradient norm: 0.69572289
INFO:root:[   71] Training loss: 0.01510460, Validation loss: 0.00997743, Gradient norm: 0.74901125
INFO:root:[   72] Training loss: 0.01512806, Validation loss: 0.01634959, Gradient norm: 0.80416898
INFO:root:[   73] Training loss: 0.01477896, Validation loss: 0.00474677, Gradient norm: 0.79682919
INFO:root:[   74] Training loss: 0.01455867, Validation loss: 0.00807429, Gradient norm: 0.77863008
INFO:root:[   75] Training loss: 0.01474986, Validation loss: 0.00669079, Gradient norm: 0.75996245
INFO:root:[   76] Training loss: 0.01419788, Validation loss: 0.00859837, Gradient norm: 0.68040064
INFO:root:[   77] Training loss: 0.01429152, Validation loss: 0.00638719, Gradient norm: 0.68782318
INFO:root:[   78] Training loss: 0.01450001, Validation loss: 0.00748653, Gradient norm: 0.70509910
INFO:root:[   79] Training loss: 0.01496924, Validation loss: 0.01386457, Gradient norm: 0.82603187
INFO:root:[   80] Training loss: 0.01456752, Validation loss: 0.01760029, Gradient norm: 0.78561967
INFO:root:[   81] Training loss: 0.01452824, Validation loss: 0.00561902, Gradient norm: 0.80567185
INFO:root:[   82] Training loss: 0.01426992, Validation loss: 0.00630835, Gradient norm: 0.75570161
INFO:root:EP 82: Early stopping
INFO:root:Training the model took 1252.961s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.1455
INFO:root:EnergyScoreTrain: 0.07139
INFO:root:CoverageTrain: 0.73806
INFO:root:IntervalWidthTrain: 0.46389
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.10426
INFO:root:EnergyScoreValidation: 0.04986
INFO:root:CoverageValidation: 0.87355
INFO:root:IntervalWidthValidation: 0.43343
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.18093
INFO:root:EnergyScoreTest: 0.10132
INFO:root:CoverageTest: 0.58343
INFO:root:IntervalWidthTest: 0.52006
INFO:root:###9 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456789, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1900019712
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.24481988, Validation loss: 0.06870933, Gradient norm: 1.80226959
INFO:root:[    2] Training loss: 0.04443985, Validation loss: 0.04155120, Gradient norm: 0.98344946
INFO:root:[    3] Training loss: 0.04224184, Validation loss: 0.02986998, Gradient norm: 1.15833079
INFO:root:[    4] Training loss: 0.03617211, Validation loss: 0.03030661, Gradient norm: 1.06441397
INFO:root:[    5] Training loss: 0.03590898, Validation loss: 0.02586163, Gradient norm: 1.23000397
INFO:root:[    6] Training loss: 0.03379182, Validation loss: 0.02018198, Gradient norm: 1.14841335
INFO:root:[    7] Training loss: 0.02997963, Validation loss: 0.03091313, Gradient norm: 1.04566570
INFO:root:[    8] Training loss: 0.02798881, Validation loss: 0.02349272, Gradient norm: 0.91869290
INFO:root:[    9] Training loss: 0.02828008, Validation loss: 0.01608858, Gradient norm: 1.01185215
INFO:root:[   10] Training loss: 0.02702999, Validation loss: 0.02481696, Gradient norm: 0.96211937
INFO:root:[   11] Training loss: 0.02686302, Validation loss: 0.01976381, Gradient norm: 0.95788577
INFO:root:[   12] Training loss: 0.02715083, Validation loss: 0.01697290, Gradient norm: 1.04577317
INFO:root:[   13] Training loss: 0.02622913, Validation loss: 0.01715589, Gradient norm: 0.99537853
INFO:root:[   14] Training loss: 0.02467918, Validation loss: 0.01837750, Gradient norm: 0.92778946
INFO:root:[   15] Training loss: 0.02504254, Validation loss: 0.01785202, Gradient norm: 0.98782064
INFO:root:[   16] Training loss: 0.02434630, Validation loss: 0.01585363, Gradient norm: 0.94468706
INFO:root:[   17] Training loss: 0.02456015, Validation loss: 0.01352218, Gradient norm: 0.92316501
INFO:root:[   18] Training loss: 0.02322174, Validation loss: 0.02663632, Gradient norm: 0.82983082
INFO:root:[   19] Training loss: 0.02403440, Validation loss: 0.02129430, Gradient norm: 0.98181525
INFO:root:[   20] Training loss: 0.02276226, Validation loss: 0.02593531, Gradient norm: 0.90576656
INFO:root:[   21] Training loss: 0.02294578, Validation loss: 0.02860234, Gradient norm: 0.93164328
INFO:root:[   22] Training loss: 0.02313417, Validation loss: 0.01350630, Gradient norm: 0.95465534
INFO:root:[   23] Training loss: 0.02219317, Validation loss: 0.01146048, Gradient norm: 0.89394940
INFO:root:[   24] Training loss: 0.02186134, Validation loss: 0.01497330, Gradient norm: 0.90673235
INFO:root:[   25] Training loss: 0.02146842, Validation loss: 0.01178661, Gradient norm: 0.91051529
INFO:root:[   26] Training loss: 0.02082972, Validation loss: 0.00976420, Gradient norm: 0.90845596
INFO:root:[   27] Training loss: 0.02062968, Validation loss: 0.01456197, Gradient norm: 0.90837657
INFO:root:[   28] Training loss: 0.02097004, Validation loss: 0.01148476, Gradient norm: 0.86865959
INFO:root:[   29] Training loss: 0.02095582, Validation loss: 0.01163286, Gradient norm: 0.88314315
INFO:root:[   30] Training loss: 0.01983933, Validation loss: 0.01289760, Gradient norm: 0.84533073
INFO:root:[   31] Training loss: 0.01981686, Validation loss: 0.00896579, Gradient norm: 0.92601835
INFO:root:[   32] Training loss: 0.01948678, Validation loss: 0.01215142, Gradient norm: 0.86404079
INFO:root:[   33] Training loss: 0.01902411, Validation loss: 0.00906035, Gradient norm: 0.84206055
INFO:root:[   34] Training loss: 0.01955430, Validation loss: 0.00961916, Gradient norm: 0.87807288
INFO:root:[   35] Training loss: 0.01908956, Validation loss: 0.00972826, Gradient norm: 0.82289396
INFO:root:[   36] Training loss: 0.01868060, Validation loss: 0.00965766, Gradient norm: 0.83877806
INFO:root:[   37] Training loss: 0.01932585, Validation loss: 0.01395240, Gradient norm: 0.89471430
INFO:root:[   38] Training loss: 0.01930326, Validation loss: 0.01058953, Gradient norm: 0.89310234
INFO:root:[   39] Training loss: 0.01893756, Validation loss: 0.01717870, Gradient norm: 0.85470828
INFO:root:[   40] Training loss: 0.01834491, Validation loss: 0.01913508, Gradient norm: 0.85509616
INFO:root:[   41] Training loss: 0.01858189, Validation loss: 0.02229394, Gradient norm: 0.84562267
INFO:root:[   42] Training loss: 0.01778115, Validation loss: 0.02036956, Gradient norm: 0.84507400
INFO:root:[   43] Training loss: 0.01780840, Validation loss: 0.02367352, Gradient norm: 0.85451937
INFO:root:[   44] Training loss: 0.01842569, Validation loss: 0.01466176, Gradient norm: 0.90750523
INFO:root:[   45] Training loss: 0.01789689, Validation loss: 0.00731639, Gradient norm: 0.83577779
INFO:root:[   46] Training loss: 0.01802025, Validation loss: 0.00978505, Gradient norm: 0.81550626
INFO:root:[   47] Training loss: 0.01760397, Validation loss: 0.00998673, Gradient norm: 0.83677292
INFO:root:[   48] Training loss: 0.01814081, Validation loss: 0.00887612, Gradient norm: 0.87025867
INFO:root:[   49] Training loss: 0.01745138, Validation loss: 0.02132035, Gradient norm: 0.82214754
INFO:root:[   50] Training loss: 0.01730625, Validation loss: 0.02199637, Gradient norm: 0.82903253
INFO:root:[   51] Training loss: 0.01740776, Validation loss: 0.01211210, Gradient norm: 0.85662153
INFO:root:[   52] Training loss: 0.01678700, Validation loss: 0.00554513, Gradient norm: 0.83922494
INFO:root:[   53] Training loss: 0.01671703, Validation loss: 0.00659455, Gradient norm: 0.81367267
INFO:root:[   54] Training loss: 0.01717540, Validation loss: 0.00836279, Gradient norm: 0.78614263
INFO:root:[   55] Training loss: 0.01704061, Validation loss: 0.00809448, Gradient norm: 0.77882916
INFO:root:[   56] Training loss: 0.01701299, Validation loss: 0.00759381, Gradient norm: 0.81247715
INFO:root:[   57] Training loss: 0.01677584, Validation loss: 0.00578820, Gradient norm: 0.78023782
INFO:root:[   58] Training loss: 0.01708961, Validation loss: 0.01380921, Gradient norm: 0.84911047
INFO:root:[   59] Training loss: 0.01673146, Validation loss: 0.02234289, Gradient norm: 0.82130800
INFO:root:[   60] Training loss: 0.01663528, Validation loss: 0.01729879, Gradient norm: 0.82078222
INFO:root:[   61] Training loss: 0.01697943, Validation loss: 0.00489777, Gradient norm: 0.82835467
INFO:root:[   62] Training loss: 0.01632908, Validation loss: 0.00727078, Gradient norm: 0.79663272
INFO:root:[   63] Training loss: 0.01616390, Validation loss: 0.00681244, Gradient norm: 0.79672846
INFO:root:[   64] Training loss: 0.01634589, Validation loss: 0.00739800, Gradient norm: 0.79340122
INFO:root:[   65] Training loss: 0.01612189, Validation loss: 0.00784096, Gradient norm: 0.78159497
INFO:root:[   66] Training loss: 0.01593546, Validation loss: 0.00672316, Gradient norm: 0.74965362
INFO:root:[   67] Training loss: 0.01668459, Validation loss: 0.01786961, Gradient norm: 0.82851181
INFO:root:[   68] Training loss: 0.01608015, Validation loss: 0.02051917, Gradient norm: 0.81147731
INFO:root:[   69] Training loss: 0.01607902, Validation loss: 0.01502404, Gradient norm: 0.80594454
INFO:root:[   70] Training loss: 0.01564054, Validation loss: 0.01204979, Gradient norm: 0.79411408
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1070.257s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.19887
INFO:root:EnergyScoreTrain: 0.10387
INFO:root:CoverageTrain: 0.71163
INFO:root:IntervalWidthTrain: 0.59736
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.16022
INFO:root:EnergyScoreValidation: 0.06405
INFO:root:CoverageValidation: 0.79592
INFO:root:IntervalWidthValidation: 0.55816
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.16678
INFO:root:EnergyScoreTest: 0.06524
INFO:root:CoverageTest: 0.82128
INFO:root:IntervalWidthTest: 0.63783
