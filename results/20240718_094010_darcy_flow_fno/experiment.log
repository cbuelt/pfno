INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05084766, Validation loss: 0.02845966, Gradient norm: 0.43342038
INFO:root:[    2] Training loss: 0.02418541, Validation loss: 0.02128927, Gradient norm: 0.50895263
INFO:root:[    3] Training loss: 0.01928774, Validation loss: 0.01724608, Gradient norm: 0.58926951
INFO:root:[    4] Training loss: 0.01730834, Validation loss: 0.01781742, Gradient norm: 0.55928390
INFO:root:[    5] Training loss: 0.01624553, Validation loss: 0.01803300, Gradient norm: 0.54241027
INFO:root:[    6] Training loss: 0.01533042, Validation loss: 0.01768691, Gradient norm: 0.53024697
INFO:root:[    7] Training loss: 0.01495336, Validation loss: 0.01527290, Gradient norm: 0.59494221
INFO:root:[    8] Training loss: 0.01371073, Validation loss: 0.01273230, Gradient norm: 0.48595577
INFO:root:[    9] Training loss: 0.01290644, Validation loss: 0.01479808, Gradient norm: 0.46507570
INFO:root:[   10] Training loss: 0.01267475, Validation loss: 0.01225574, Gradient norm: 0.55588719
INFO:root:[   11] Training loss: 0.01173269, Validation loss: 0.01195427, Gradient norm: 0.43008273
INFO:root:[   12] Training loss: 0.01134502, Validation loss: 0.01331352, Gradient norm: 0.48514155
INFO:root:[   13] Training loss: 0.01123301, Validation loss: 0.01059888, Gradient norm: 0.52255072
INFO:root:[   14] Training loss: 0.01027325, Validation loss: 0.01015294, Gradient norm: 0.36495412
INFO:root:[   15] Training loss: 0.01012907, Validation loss: 0.01017214, Gradient norm: 0.40972291
INFO:root:[   16] Training loss: 0.01047264, Validation loss: 0.01056122, Gradient norm: 0.51818884
INFO:root:[   17] Training loss: 0.01031869, Validation loss: 0.00969222, Gradient norm: 0.49759055
INFO:root:[   18] Training loss: 0.00982112, Validation loss: 0.00954527, Gradient norm: 0.44016362
INFO:root:[   19] Training loss: 0.00973213, Validation loss: 0.01074341, Gradient norm: 0.45719122
INFO:root:[   20] Training loss: 0.00952431, Validation loss: 0.00925952, Gradient norm: 0.44139573
INFO:root:[   21] Training loss: 0.00958869, Validation loss: 0.01029311, Gradient norm: 0.47372007
INFO:root:[   22] Training loss: 0.00920637, Validation loss: 0.01152606, Gradient norm: 0.40383155
INFO:root:[   23] Training loss: 0.00921524, Validation loss: 0.01050266, Gradient norm: 0.41486769
INFO:root:[   24] Training loss: 0.00931594, Validation loss: 0.00914933, Gradient norm: 0.46175812
INFO:root:[   25] Training loss: 0.00927229, Validation loss: 0.00931289, Gradient norm: 0.50174021
INFO:root:[   26] Training loss: 0.00874706, Validation loss: 0.00882300, Gradient norm: 0.39004739
INFO:root:[   27] Training loss: 0.00863328, Validation loss: 0.01006812, Gradient norm: 0.38781654
INFO:root:[   28] Training loss: 0.00859840, Validation loss: 0.00874171, Gradient norm: 0.40505418
INFO:root:[   29] Training loss: 0.00870947, Validation loss: 0.01009910, Gradient norm: 0.45182111
INFO:root:[   30] Training loss: 0.00838404, Validation loss: 0.00843378, Gradient norm: 0.35402461
INFO:root:[   31] Training loss: 0.00840905, Validation loss: 0.00938848, Gradient norm: 0.39367960
INFO:root:[   32] Training loss: 0.00815859, Validation loss: 0.00894011, Gradient norm: 0.33286803
INFO:root:[   33] Training loss: 0.00809739, Validation loss: 0.00926574, Gradient norm: 0.38782175
INFO:root:[   34] Training loss: 0.00797260, Validation loss: 0.00825347, Gradient norm: 0.32179757
INFO:root:[   35] Training loss: 0.00816995, Validation loss: 0.00852531, Gradient norm: 0.37964716
INFO:root:[   36] Training loss: 0.00809228, Validation loss: 0.00825437, Gradient norm: 0.39891837
INFO:root:[   37] Training loss: 0.00808672, Validation loss: 0.00887723, Gradient norm: 0.35723976
INFO:root:[   38] Training loss: 0.00786700, Validation loss: 0.00884616, Gradient norm: 0.37618378
INFO:root:[   39] Training loss: 0.00810141, Validation loss: 0.00955918, Gradient norm: 0.41135589
INFO:root:[   40] Training loss: 0.00801087, Validation loss: 0.00873913, Gradient norm: 0.42063947
INFO:root:[   41] Training loss: 0.00767835, Validation loss: 0.00830947, Gradient norm: 0.32662598
INFO:root:[   42] Training loss: 0.00788635, Validation loss: 0.00926752, Gradient norm: 0.39429878
INFO:root:[   43] Training loss: 0.00745314, Validation loss: 0.00815930, Gradient norm: 0.34480499
INFO:root:[   44] Training loss: 0.00800691, Validation loss: 0.00887299, Gradient norm: 0.40397322
INFO:root:[   45] Training loss: 0.00729847, Validation loss: 0.00936542, Gradient norm: 0.32287134
INFO:root:[   46] Training loss: 0.00748082, Validation loss: 0.00856284, Gradient norm: 0.32484699
INFO:root:[   47] Training loss: 0.00750248, Validation loss: 0.00815698, Gradient norm: 0.37056593
INFO:root:[   48] Training loss: 0.00748791, Validation loss: 0.00819764, Gradient norm: 0.40079512
INFO:root:[   49] Training loss: 0.00735787, Validation loss: 0.00814608, Gradient norm: 0.36950253
INFO:root:[   50] Training loss: 0.00750419, Validation loss: 0.00801959, Gradient norm: 0.40170414
INFO:root:[   51] Training loss: 0.00707821, Validation loss: 0.00818398, Gradient norm: 0.32427092
INFO:root:[   52] Training loss: 0.00727176, Validation loss: 0.00952440, Gradient norm: 0.35646993
INFO:root:[   53] Training loss: 0.00716958, Validation loss: 0.00859972, Gradient norm: 0.35559347
INFO:root:[   54] Training loss: 0.00695008, Validation loss: 0.00892982, Gradient norm: 0.32180635
INFO:root:[   55] Training loss: 0.00713394, Validation loss: 0.00866442, Gradient norm: 0.36205942
INFO:root:[   56] Training loss: 0.00712683, Validation loss: 0.00860967, Gradient norm: 0.33915699
INFO:root:[   57] Training loss: 0.00679408, Validation loss: 0.00834990, Gradient norm: 0.31275594
INFO:root:[   58] Training loss: 0.00693094, Validation loss: 0.00825911, Gradient norm: 0.35043171
INFO:root:[   59] Training loss: 0.00706894, Validation loss: 0.00905590, Gradient norm: 0.38578257
INFO:root:[   60] Training loss: 0.00689750, Validation loss: 0.00814236, Gradient norm: 0.33812453
INFO:root:[   61] Training loss: 0.00699340, Validation loss: 0.00899857, Gradient norm: 0.37922579
INFO:root:[   62] Training loss: 0.00649420, Validation loss: 0.00793897, Gradient norm: 0.28323079
INFO:root:[   63] Training loss: 0.00686321, Validation loss: 0.00861231, Gradient norm: 0.35062867
INFO:root:[   64] Training loss: 0.00662109, Validation loss: 0.00852913, Gradient norm: 0.34298294
INFO:root:[   65] Training loss: 0.00693738, Validation loss: 0.00810073, Gradient norm: 0.39756855
INFO:root:[   66] Training loss: 0.00651924, Validation loss: 0.00806014, Gradient norm: 0.34015256
INFO:root:[   67] Training loss: 0.00635979, Validation loss: 0.00785175, Gradient norm: 0.28930690
INFO:root:[   68] Training loss: 0.00667802, Validation loss: 0.00895960, Gradient norm: 0.32527677
INFO:root:[   69] Training loss: 0.00640094, Validation loss: 0.00889900, Gradient norm: 0.30560983
INFO:root:[   70] Training loss: 0.00641897, Validation loss: 0.00831254, Gradient norm: 0.32363671
INFO:root:[   71] Training loss: 0.00664953, Validation loss: 0.00920419, Gradient norm: 0.36348254
INFO:root:[   72] Training loss: 0.00604424, Validation loss: 0.00786741, Gradient norm: 0.28024610
INFO:root:[   73] Training loss: 0.00627959, Validation loss: 0.00814143, Gradient norm: 0.35190239
INFO:root:[   74] Training loss: 0.00636565, Validation loss: 0.00837385, Gradient norm: 0.32302447
INFO:root:[   75] Training loss: 0.00611009, Validation loss: 0.00821217, Gradient norm: 0.30668326
INFO:root:[   76] Training loss: 0.00625222, Validation loss: 0.00931354, Gradient norm: 0.32958387
INFO:root:EP 76: Early stopping
INFO:root:Training the model took 1021.673s.
INFO:root:Emptying the cuda cache took 0.027s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
