INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 2097152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.82859726, Validation loss: 0.72999800, Gradient norm: 8.21265833
INFO:root:[    2] Training loss: 0.72652325, Validation loss: 0.70573837, Gradient norm: 12.32497959
INFO:root:[    3] Training loss: 0.71688756, Validation loss: 0.71153655, Gradient norm: 10.06743662
INFO:root:[    4] Training loss: 0.71616248, Validation loss: 0.72025358, Gradient norm: 10.44340068
INFO:root:[    5] Training loss: 0.71376069, Validation loss: 0.74055903, Gradient norm: 10.24762729
INFO:root:[    6] Training loss: 0.71310346, Validation loss: 0.75789035, Gradient norm: 9.87424124
INFO:root:[    7] Training loss: 0.71356102, Validation loss: 0.72382746, Gradient norm: 10.47319533
INFO:root:[    8] Training loss: 0.70934012, Validation loss: 0.71277113, Gradient norm: 8.19179596
INFO:root:[    9] Training loss: 0.70962801, Validation loss: 0.70341153, Gradient norm: 8.39282460
INFO:root:[   10] Training loss: 0.70876415, Validation loss: 0.71304819, Gradient norm: 8.28251576
INFO:root:[   11] Training loss: 0.70760263, Validation loss: 0.70631365, Gradient norm: 7.82500916
INFO:root:[   12] Training loss: 0.70831826, Validation loss: 0.70520150, Gradient norm: 8.36615882
INFO:root:[   13] Training loss: 0.70388799, Validation loss: 0.69638458, Gradient norm: 7.53015093
INFO:root:[   14] Training loss: 0.69682427, Validation loss: 0.69347992, Gradient norm: 6.91977515
INFO:root:[   15] Training loss: 0.68718343, Validation loss: 0.68995915, Gradient norm: 5.76318310
INFO:root:[   16] Training loss: 0.68184832, Validation loss: 0.67695889, Gradient norm: 7.20652363
INFO:root:[   17] Training loss: 0.67407510, Validation loss: 0.67141332, Gradient norm: 5.82047765
INFO:root:[   18] Training loss: 0.66928543, Validation loss: 0.66342588, Gradient norm: 6.17705349
INFO:root:[   19] Training loss: 0.66636610, Validation loss: 0.66003842, Gradient norm: 6.93989533
INFO:root:[   20] Training loss: 0.66259926, Validation loss: 0.66320724, Gradient norm: 6.72053838
INFO:root:[   21] Training loss: 0.65652259, Validation loss: 0.65522107, Gradient norm: 4.38644647
INFO:root:[   22] Training loss: 0.65639135, Validation loss: 0.65336048, Gradient norm: 5.87455975
INFO:root:[   23] Training loss: 0.65345138, Validation loss: 0.64984819, Gradient norm: 5.81509191
INFO:root:[   24] Training loss: 0.65222563, Validation loss: 0.64773731, Gradient norm: 6.09471278
INFO:root:[   25] Training loss: 0.64840294, Validation loss: 0.64747958, Gradient norm: 4.45205543
INFO:root:[   26] Training loss: 0.64758630, Validation loss: 0.64724804, Gradient norm: 5.38783924
INFO:root:[   27] Training loss: 0.64517217, Validation loss: 0.64732262, Gradient norm: 4.58253293
INFO:root:[   28] Training loss: 0.64570066, Validation loss: 0.64310518, Gradient norm: 5.75414309
INFO:root:[   29] Training loss: 0.64310201, Validation loss: 0.64160595, Gradient norm: 5.31856623
INFO:root:[   30] Training loss: 0.64192174, Validation loss: 0.64716632, Gradient norm: 4.83723345
INFO:root:[   31] Training loss: 0.63991528, Validation loss: 0.63774253, Gradient norm: 5.07754537
INFO:root:[   32] Training loss: 0.63814709, Validation loss: 0.64788850, Gradient norm: 3.74659586
INFO:root:[   33] Training loss: 0.63699246, Validation loss: 0.64077882, Gradient norm: 4.65531505
INFO:root:[   34] Training loss: 0.63698598, Validation loss: 0.63870919, Gradient norm: 5.05025232
INFO:root:[   35] Training loss: 0.63517291, Validation loss: 0.63760450, Gradient norm: 4.82280231
INFO:root:[   36] Training loss: 0.63424709, Validation loss: 0.63781772, Gradient norm: 4.84639696
INFO:root:[   37] Training loss: 0.63283044, Validation loss: 0.63818607, Gradient norm: 4.79519938
INFO:root:[   38] Training loss: 0.63218657, Validation loss: 0.63286705, Gradient norm: 5.09239569
INFO:root:[   39] Training loss: 0.63059662, Validation loss: 0.63373956, Gradient norm: 4.57433188
INFO:root:[   40] Training loss: 0.62950730, Validation loss: 0.62946114, Gradient norm: 4.27420782
INFO:root:[   41] Training loss: 0.62823942, Validation loss: 0.63640464, Gradient norm: 4.26948650
INFO:root:[   42] Training loss: 0.62690216, Validation loss: 0.63256119, Gradient norm: 4.22751014
INFO:root:[   43] Training loss: 0.62735630, Validation loss: 0.63618521, Gradient norm: 4.07773075
INFO:root:[   44] Training loss: 0.62577096, Validation loss: 0.63020755, Gradient norm: 4.37925565
INFO:root:[   45] Training loss: 0.62379576, Validation loss: 0.62645723, Gradient norm: 3.97117808
INFO:root:[   46] Training loss: 0.62415079, Validation loss: 0.62870715, Gradient norm: 4.67824415
INFO:root:[   47] Training loss: 0.62267462, Validation loss: 0.62403357, Gradient norm: 3.81394372
INFO:root:[   48] Training loss: 0.62191821, Validation loss: 0.63083943, Gradient norm: 4.36456739
INFO:root:[   49] Training loss: 0.62140833, Validation loss: 0.62539841, Gradient norm: 4.16322899
INFO:root:[   50] Training loss: 0.61996806, Validation loss: 0.63076795, Gradient norm: 3.96403049
INFO:root:[   51] Training loss: 0.61916723, Validation loss: 0.62414454, Gradient norm: 3.87875039
INFO:root:[   52] Training loss: 0.61868447, Validation loss: 0.62685129, Gradient norm: 4.00884326
INFO:root:[   53] Training loss: 0.61770877, Validation loss: 0.62011846, Gradient norm: 4.03744609
INFO:root:[   54] Training loss: 0.61664752, Validation loss: 0.62207493, Gradient norm: 3.98716013
INFO:root:[   55] Training loss: 0.61731505, Validation loss: 0.61986583, Gradient norm: 4.19997803
INFO:root:[   56] Training loss: 0.61613863, Validation loss: 0.62094590, Gradient norm: 4.29666976
INFO:root:[   57] Training loss: 0.61448531, Validation loss: 0.62340444, Gradient norm: 4.82436340
INFO:root:[   58] Training loss: 0.61344608, Validation loss: 0.61999773, Gradient norm: 4.46965919
INFO:root:[   59] Training loss: 0.61228093, Validation loss: 0.62202224, Gradient norm: 4.28287540
INFO:root:[   60] Training loss: 0.61240212, Validation loss: 0.62163607, Gradient norm: 4.37105327
INFO:root:[   61] Training loss: 0.61085698, Validation loss: 0.62048648, Gradient norm: 4.10398030
INFO:root:[   62] Training loss: 0.61041258, Validation loss: 0.62121332, Gradient norm: 4.07866219
INFO:root:[   63] Training loss: 0.60984540, Validation loss: 0.61912950, Gradient norm: 4.02580970
INFO:root:[   64] Training loss: 0.60935330, Validation loss: 0.61827336, Gradient norm: 3.99802985
INFO:root:[   65] Training loss: 0.60836106, Validation loss: 0.61835886, Gradient norm: 3.85579488
INFO:root:[   66] Training loss: 0.60745306, Validation loss: 0.61624476, Gradient norm: 3.69463295
INFO:root:[   67] Training loss: 0.60730227, Validation loss: 0.61736550, Gradient norm: 3.77014870
INFO:root:[   68] Training loss: 0.60620097, Validation loss: 0.61635591, Gradient norm: 3.57285477
INFO:root:[   69] Training loss: 0.60573879, Validation loss: 0.61673489, Gradient norm: 3.65775824
INFO:root:[   70] Training loss: 0.60496527, Validation loss: 0.61780758, Gradient norm: 3.49928508
INFO:root:[   71] Training loss: 0.60445298, Validation loss: 0.61645058, Gradient norm: 3.45923034
INFO:root:[   72] Training loss: 0.60427437, Validation loss: 0.61822928, Gradient norm: 3.38635424
INFO:root:[   73] Training loss: 0.60341262, Validation loss: 0.61607911, Gradient norm: 3.35272339
INFO:root:[   74] Training loss: 0.60240004, Validation loss: 0.61658694, Gradient norm: 3.24417333
INFO:root:[   75] Training loss: 0.60264286, Validation loss: 0.61605329, Gradient norm: 3.21346709
INFO:root:[   76] Training loss: 0.60143647, Validation loss: 0.61622902, Gradient norm: 3.19019026
INFO:root:[   77] Training loss: 0.60085913, Validation loss: 0.61315427, Gradient norm: 3.17458937
INFO:root:[   78] Training loss: 0.60043132, Validation loss: 0.61584416, Gradient norm: 3.08592431
INFO:root:[   79] Training loss: 0.59944023, Validation loss: 0.61609811, Gradient norm: 3.00732681
INFO:root:[   80] Training loss: 0.59917284, Validation loss: 0.61424509, Gradient norm: 2.95223138
INFO:root:[   81] Training loss: 0.59954072, Validation loss: 0.61397927, Gradient norm: 2.95552326
INFO:root:[   82] Training loss: 0.59800473, Validation loss: 0.61617185, Gradient norm: 2.87652029
INFO:root:[   83] Training loss: 0.59786164, Validation loss: 0.61359850, Gradient norm: 2.87246436
INFO:root:[   84] Training loss: 0.59687751, Validation loss: 0.61747330, Gradient norm: 2.76075426
INFO:root:[   85] Training loss: 0.59723860, Validation loss: 0.61269125, Gradient norm: 2.74389931
INFO:root:[   86] Training loss: 0.59624581, Validation loss: 0.61530639, Gradient norm: 2.69249711
INFO:root:[   87] Training loss: 0.59564239, Validation loss: 0.61405122, Gradient norm: 2.65302238
INFO:root:[   88] Training loss: 0.59475734, Validation loss: 0.61504744, Gradient norm: 2.64554364
INFO:root:[   89] Training loss: 0.59496648, Validation loss: 0.61362369, Gradient norm: 2.56991453
INFO:root:[   90] Training loss: 0.59422244, Validation loss: 0.61489128, Gradient norm: 2.55758108
INFO:root:[   91] Training loss: 0.59362679, Validation loss: 0.61353970, Gradient norm: 2.49714320
INFO:root:[   92] Training loss: 0.59258053, Validation loss: 0.61302639, Gradient norm: 2.46823518
INFO:root:[   93] Training loss: 0.59327448, Validation loss: 0.61243346, Gradient norm: 2.39103743
INFO:root:[   94] Training loss: 0.59271429, Validation loss: 0.61143404, Gradient norm: 2.26897100
INFO:root:[   95] Training loss: 0.59098256, Validation loss: 0.61449732, Gradient norm: 2.36356642
INFO:root:[   96] Training loss: 0.59056626, Validation loss: 0.61294077, Gradient norm: 2.33736506
INFO:root:[   97] Training loss: 0.59220229, Validation loss: 0.61555075, Gradient norm: 2.10780691
INFO:root:[   98] Training loss: 0.59025273, Validation loss: 0.61065797, Gradient norm: 2.16129920
INFO:root:[   99] Training loss: 0.58882044, Validation loss: 0.61306544, Gradient norm: 2.18516235
INFO:root:[  100] Training loss: 0.58920257, Validation loss: 0.61119421, Gradient norm: 2.18439300
INFO:root:[  101] Training loss: 0.58904767, Validation loss: 0.61529335, Gradient norm: 2.08558143
INFO:root:[  102] Training loss: 0.58848811, Validation loss: 0.61700695, Gradient norm: 2.54918262
INFO:root:[  103] Training loss: 0.58784143, Validation loss: 0.61759931, Gradient norm: 2.33712606
INFO:root:[  104] Training loss: 0.58740609, Validation loss: 0.61580758, Gradient norm: 2.32147810
INFO:root:[  105] Training loss: 0.58761212, Validation loss: 0.61666451, Gradient norm: 2.31647095
INFO:root:[  106] Training loss: 0.58644250, Validation loss: 0.61295899, Gradient norm: 2.22859679
INFO:root:[  107] Training loss: 0.58658703, Validation loss: 0.61715558, Gradient norm: 2.28217101
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 4558.046s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.83136
INFO:root:EnergyScoreTrain: 0.58524
INFO:root:CoverageTrain: 0.71817
INFO:root:IntervalWidthTrain: 6.41591
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86772
INFO:root:EnergyScoreValidation: 0.61131
INFO:root:CoverageValidation: 0.70261
INFO:root:IntervalWidthValidation: 6.41536
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86874
INFO:root:EnergyScoreTest: 0.61205
INFO:root:CoverageTest: 0.70079
INFO:root:IntervalWidthTest: 6.40825
