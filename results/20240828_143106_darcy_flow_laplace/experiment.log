INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno_laplace.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.38803506, Validation loss: 0.18295942, Gradient norm: 7.55143371
INFO:root:[    2] Training loss: 0.19270685, Validation loss: 0.21920444, Gradient norm: 4.44559614
INFO:root:[    3] Training loss: 0.16826331, Validation loss: 0.14167607, Gradient norm: 3.44168785
INFO:root:[    4] Training loss: 0.16082417, Validation loss: 0.13500752, Gradient norm: 4.27147404
INFO:root:[    5] Training loss: 0.15123539, Validation loss: 0.14654428, Gradient norm: 3.74616055
INFO:root:[    6] Training loss: 0.14641395, Validation loss: 0.16091959, Gradient norm: 3.70105921
INFO:root:[    7] Training loss: 0.13873831, Validation loss: 0.14106738, Gradient norm: 3.44445115
INFO:root:[    8] Training loss: 0.13687520, Validation loss: 0.14796995, Gradient norm: 3.36626656
INFO:root:[    9] Training loss: 0.13380098, Validation loss: 0.12953908, Gradient norm: 3.54198472
INFO:root:[   10] Training loss: 0.13313853, Validation loss: 0.16207254, Gradient norm: 3.40664630
INFO:root:[   11] Training loss: 0.12765815, Validation loss: 0.14870428, Gradient norm: 3.35926271
INFO:root:[   12] Training loss: 0.12226439, Validation loss: 0.16001229, Gradient norm: 3.35579205
INFO:root:[   13] Training loss: 0.12237446, Validation loss: 0.12203095, Gradient norm: 3.50430779
INFO:root:[   14] Training loss: 0.11501894, Validation loss: 0.12931296, Gradient norm: 3.39822353
INFO:root:[   15] Training loss: 0.11459542, Validation loss: 0.13334937, Gradient norm: 2.95957970
INFO:root:[   16] Training loss: 0.10808290, Validation loss: 0.11552937, Gradient norm: 3.43126459
INFO:root:[   17] Training loss: 0.10811610, Validation loss: 0.12614003, Gradient norm: 3.20174357
INFO:root:[   18] Training loss: 0.10565485, Validation loss: 0.13726294, Gradient norm: 3.26012249
INFO:root:[   19] Training loss: 0.10136084, Validation loss: 0.13628584, Gradient norm: 3.12243516
INFO:root:[   20] Training loss: 0.09750690, Validation loss: 0.18278793, Gradient norm: 2.89721069
INFO:root:[   21] Training loss: 0.09876492, Validation loss: 0.18260333, Gradient norm: 3.32263513
INFO:root:[   22] Training loss: 0.09723611, Validation loss: 0.12722299, Gradient norm: 2.78621721
INFO:root:[   23] Training loss: 0.09450680, Validation loss: 0.13018921, Gradient norm: 3.24511696
INFO:root:[   24] Training loss: 0.09252181, Validation loss: 0.15386680, Gradient norm: 3.02250008
INFO:root:[   25] Training loss: 0.09095307, Validation loss: 0.15618097, Gradient norm: 2.66830970
INFO:root:[   26] Training loss: 0.09000039, Validation loss: 0.14743714, Gradient norm: 2.87132344
INFO:root:[   27] Training loss: 0.08833492, Validation loss: 0.16394906, Gradient norm: 3.26359820
INFO:root:[   28] Training loss: 0.08499256, Validation loss: 0.15197023, Gradient norm: 2.82404163
INFO:root:[   29] Training loss: 0.08866117, Validation loss: 0.18587716, Gradient norm: 2.61054150
INFO:root:[   30] Training loss: 0.08347389, Validation loss: 0.13873077, Gradient norm: 3.00672328
INFO:root:[   31] Training loss: 0.08197943, Validation loss: 0.13027281, Gradient norm: 2.72743879
INFO:root:[   32] Training loss: 0.08193570, Validation loss: 0.15281110, Gradient norm: 3.13614608
INFO:root:[   33] Training loss: 0.08007048, Validation loss: 0.16932863, Gradient norm: 2.98307358
INFO:root:[   34] Training loss: 0.07953065, Validation loss: 0.17050652, Gradient norm: 2.77297054
INFO:root:[   35] Training loss: 0.07948407, Validation loss: 0.14587680, Gradient norm: 2.92419539
INFO:root:[   36] Training loss: 0.07847454, Validation loss: 0.13535770, Gradient norm: 2.79737310
INFO:root:[   37] Training loss: 0.07742633, Validation loss: 0.16542973, Gradient norm: 2.58667910
INFO:root:[   38] Training loss: 0.07753610, Validation loss: 0.14711716, Gradient norm: 2.76146038
INFO:root:[   39] Training loss: 0.07905632, Validation loss: 0.18206405, Gradient norm: 3.03189878
INFO:root:[   40] Training loss: 0.07724183, Validation loss: 0.15942604, Gradient norm: 2.95944466
INFO:root:[   41] Training loss: 0.07578733, Validation loss: 0.15816055, Gradient norm: 2.72306148
INFO:root:[   42] Training loss: 0.07514114, Validation loss: 0.16884007, Gradient norm: 2.43999424
INFO:root:[   43] Training loss: 0.07685970, Validation loss: 0.16328727, Gradient norm: 3.24057084
INFO:root:[   44] Training loss: 0.07366657, Validation loss: 0.14529735, Gradient norm: 2.42533174
INFO:root:[   45] Training loss: 0.07589028, Validation loss: 0.17288676, Gradient norm: 2.90083986
INFO:root:[   46] Training loss: 0.07432915, Validation loss: 0.17181303, Gradient norm: 2.70845755
INFO:root:[   47] Training loss: 0.07372368, Validation loss: 0.16583246, Gradient norm: 2.56856519
INFO:root:[   48] Training loss: 0.07214315, Validation loss: 0.16414910, Gradient norm: 2.26480065
INFO:root:[   49] Training loss: 0.07367127, Validation loss: 0.15969021, Gradient norm: 2.96684901
INFO:root:[   50] Training loss: 0.07089440, Validation loss: 0.16106877, Gradient norm: 2.46897682
INFO:root:[   51] Training loss: 0.07168716, Validation loss: 0.16767930, Gradient norm: 2.94280861
INFO:root:[   52] Training loss: 0.07044787, Validation loss: 0.16635419, Gradient norm: 2.35232830
INFO:root:[   53] Training loss: 0.07139437, Validation loss: 0.16125197, Gradient norm: 2.73217387
INFO:root:[   54] Training loss: 0.07198252, Validation loss: 0.16849773, Gradient norm: 2.62642358
INFO:root:[   55] Training loss: 0.07108483, Validation loss: 0.15463707, Gradient norm: 2.91897406
INFO:root:[   56] Training loss: 0.07116042, Validation loss: 0.15607118, Gradient norm: 2.66256583
INFO:root:[   57] Training loss: 0.06998590, Validation loss: 0.15729434, Gradient norm: 2.51198081
INFO:root:[   58] Training loss: 0.06939653, Validation loss: 0.16364470, Gradient norm: 2.97503561
INFO:root:[   59] Training loss: 0.06895337, Validation loss: 0.16689032, Gradient norm: 2.76969800
INFO:root:[   60] Training loss: 0.06884152, Validation loss: 0.14925138, Gradient norm: 2.36997546
INFO:root:[   61] Training loss: 0.06867767, Validation loss: 0.15356428, Gradient norm: 2.52806317
INFO:root:[   62] Training loss: 0.06745004, Validation loss: 0.15466966, Gradient norm: 2.51694711
INFO:root:[   63] Training loss: 0.06787702, Validation loss: 0.19078985, Gradient norm: 2.53595538
INFO:root:[   64] Training loss: 0.06719120, Validation loss: 0.17249435, Gradient norm: 2.84245919
INFO:root:[   65] Training loss: 0.06710189, Validation loss: 0.17735612, Gradient norm: 2.43676881
INFO:root:[   66] Training loss: 0.06578312, Validation loss: 0.15791496, Gradient norm: 2.29869890
INFO:root:[   67] Training loss: 0.06755120, Validation loss: 0.16995119, Gradient norm: 3.08993058
INFO:root:[   68] Training loss: 0.06663560, Validation loss: 0.16144004, Gradient norm: 2.45978834
INFO:root:[   69] Training loss: 0.06779000, Validation loss: 0.17760727, Gradient norm: 2.64828434
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 3422.058s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.27438
INFO:root:EnergyScoreTrain: 0.14021
INFO:root:CoverageTrain: 0.72079
INFO:root:IntervalWidthTrain: 0.12088
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.25773
INFO:root:EnergyScoreValidation: 0.13707
INFO:root:CoverageValidation: 0.70443
INFO:root:IntervalWidthValidation: 0.10815
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.28065
INFO:root:EnergyScoreTest: 0.14531
INFO:root:CoverageTest: 0.6665
INFO:root:IntervalWidthTest: 0.10549
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 494927872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.33932292, Validation loss: 0.18386599, Gradient norm: 6.35586662
INFO:root:[    2] Training loss: 0.18056785, Validation loss: 0.16201844, Gradient norm: 3.31443861
INFO:root:[    3] Training loss: 0.16614321, Validation loss: 0.15695979, Gradient norm: 4.25666643
INFO:root:[    4] Training loss: 0.14967797, Validation loss: 0.13529980, Gradient norm: 4.33807263
INFO:root:[    5] Training loss: 0.14525130, Validation loss: 0.13385897, Gradient norm: 4.30603247
INFO:root:[    6] Training loss: 0.13157011, Validation loss: 0.12610801, Gradient norm: 3.69927887
INFO:root:[    7] Training loss: 0.12880929, Validation loss: 0.13480281, Gradient norm: 3.82717378
INFO:root:[    8] Training loss: 0.12391699, Validation loss: 0.17029050, Gradient norm: 3.58207046
INFO:root:[    9] Training loss: 0.12303495, Validation loss: 0.12840487, Gradient norm: 3.70394888
INFO:root:[   10] Training loss: 0.11482029, Validation loss: 0.13589524, Gradient norm: 3.27950457
INFO:root:[   11] Training loss: 0.11641348, Validation loss: 0.11984647, Gradient norm: 3.66892247
INFO:root:[   12] Training loss: 0.11193189, Validation loss: 0.15511040, Gradient norm: 3.54981378
INFO:root:[   13] Training loss: 0.11263822, Validation loss: 0.13350316, Gradient norm: 3.47099869
INFO:root:[   14] Training loss: 0.10557696, Validation loss: 0.15934637, Gradient norm: 3.11792390
INFO:root:[   15] Training loss: 0.10440881, Validation loss: 0.13620894, Gradient norm: 3.07544665
INFO:root:[   16] Training loss: 0.10010082, Validation loss: 0.14602107, Gradient norm: 3.04093010
INFO:root:[   17] Training loss: 0.10131308, Validation loss: 0.12467672, Gradient norm: 2.66736938
INFO:root:[   18] Training loss: 0.09870745, Validation loss: 0.13170128, Gradient norm: 3.22675212
INFO:root:[   19] Training loss: 0.09719394, Validation loss: 0.16205483, Gradient norm: 3.34953186
INFO:root:[   20] Training loss: 0.09398803, Validation loss: 0.18106894, Gradient norm: 3.20949191
INFO:root:[   21] Training loss: 0.09487833, Validation loss: 0.17248285, Gradient norm: 3.03029168
INFO:root:[   22] Training loss: 0.09004842, Validation loss: 0.12696406, Gradient norm: 2.98346820
INFO:root:[   23] Training loss: 0.09137529, Validation loss: 0.17008066, Gradient norm: 3.03517110
INFO:root:[   24] Training loss: 0.09084786, Validation loss: 0.14649323, Gradient norm: 2.99179338
INFO:root:[   25] Training loss: 0.08635024, Validation loss: 0.16040956, Gradient norm: 2.89999015
INFO:root:[   26] Training loss: 0.08420704, Validation loss: 0.17529011, Gradient norm: 2.49588041
INFO:root:[   27] Training loss: 0.08392940, Validation loss: 0.17079414, Gradient norm: 3.05910324
INFO:root:[   28] Training loss: 0.08236189, Validation loss: 0.14178101, Gradient norm: 2.83377654
INFO:root:[   29] Training loss: 0.08311029, Validation loss: 0.12587366, Gradient norm: 3.13778401
INFO:root:[   30] Training loss: 0.08348374, Validation loss: 0.17318810, Gradient norm: 2.55597255
INFO:root:[   31] Training loss: 0.07969405, Validation loss: 0.15359505, Gradient norm: 3.01650685
INFO:root:[   32] Training loss: 0.08170371, Validation loss: 0.14297434, Gradient norm: 2.72536481
INFO:root:[   33] Training loss: 0.07875701, Validation loss: 0.16788378, Gradient norm: 2.74123808
INFO:root:[   34] Training loss: 0.07800492, Validation loss: 0.13011068, Gradient norm: 2.91517330
INFO:root:[   35] Training loss: 0.08020958, Validation loss: 0.17227850, Gradient norm: 2.59714650
INFO:root:[   36] Training loss: 0.07922740, Validation loss: 0.17845005, Gradient norm: 2.73182873
INFO:root:[   37] Training loss: 0.07750872, Validation loss: 0.15028613, Gradient norm: 2.79774312
INFO:root:[   38] Training loss: 0.07432799, Validation loss: 0.15987804, Gradient norm: 2.40731811
INFO:root:[   39] Training loss: 0.07642009, Validation loss: 0.17597222, Gradient norm: 2.93033700
INFO:root:[   40] Training loss: 0.07448185, Validation loss: 0.17830086, Gradient norm: 2.67165372
INFO:root:[   41] Training loss: 0.07420464, Validation loss: 0.16968445, Gradient norm: 2.73939758
INFO:root:[   42] Training loss: 0.07477042, Validation loss: 0.16233006, Gradient norm: 2.88564419
INFO:root:[   43] Training loss: 0.07276288, Validation loss: 0.16146163, Gradient norm: 2.64637681
INFO:root:[   44] Training loss: 0.07234714, Validation loss: 0.15789004, Gradient norm: 2.66039531
INFO:root:[   45] Training loss: 0.07190643, Validation loss: 0.15070966, Gradient norm: 2.71071727
INFO:root:[   46] Training loss: 0.07141490, Validation loss: 0.16339400, Gradient norm: 2.67116501
INFO:root:[   47] Training loss: 0.06912177, Validation loss: 0.14851343, Gradient norm: 2.34862309
INFO:root:[   48] Training loss: 0.07076147, Validation loss: 0.18531854, Gradient norm: 2.82271258
INFO:root:[   49] Training loss: 0.06952726, Validation loss: 0.14710586, Gradient norm: 2.57489852
INFO:root:[   50] Training loss: 0.07015661, Validation loss: 0.16598734, Gradient norm: 2.70103227
INFO:root:[   51] Training loss: 0.07249240, Validation loss: 0.16541529, Gradient norm: 2.82569381
INFO:root:[   52] Training loss: 0.06904758, Validation loss: 0.17405571, Gradient norm: 2.65954894
INFO:root:[   53] Training loss: 0.06951571, Validation loss: 0.15934652, Gradient norm: 2.71440665
INFO:root:[   54] Training loss: 0.06933597, Validation loss: 0.15176924, Gradient norm: 2.59641894
INFO:root:[   55] Training loss: 0.06907077, Validation loss: 0.15660518, Gradient norm: 2.47895318
INFO:root:[   56] Training loss: 0.06755686, Validation loss: 0.16176669, Gradient norm: 2.67290354
INFO:root:[   57] Training loss: 0.06803418, Validation loss: 0.17528990, Gradient norm: 2.58749688
INFO:root:[   58] Training loss: 0.06765318, Validation loss: 0.15704486, Gradient norm: 2.57694077
INFO:root:[   59] Training loss: 0.06655466, Validation loss: 0.16827540, Gradient norm: 2.39311091
INFO:root:[   60] Training loss: 0.06674275, Validation loss: 0.15745933, Gradient norm: 2.43548490
INFO:root:[   61] Training loss: 0.06701356, Validation loss: 0.15999678, Gradient norm: 2.61323878
INFO:root:[   62] Training loss: 0.06788002, Validation loss: 0.16246515, Gradient norm: 2.44384831
INFO:root:[   63] Training loss: 0.06898362, Validation loss: 0.18599225, Gradient norm: 2.60991304
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 3055.13s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.28015
INFO:root:EnergyScoreTrain: 0.14414
INFO:root:CoverageTrain: 0.70045
INFO:root:IntervalWidthTrain: 0.11595
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.24376
INFO:root:EnergyScoreValidation: 0.12458
INFO:root:CoverageValidation: 0.72657
INFO:root:IntervalWidthValidation: 0.107
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.28016
INFO:root:EnergyScoreTest: 0.1478
INFO:root:CoverageTest: 0.65211
INFO:root:IntervalWidthTest: 0.10371
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 268435456
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.29088749, Validation loss: 0.17537165, Gradient norm: 4.30315831
INFO:root:[    2] Training loss: 0.17723015, Validation loss: 0.15311160, Gradient norm: 4.03223766
INFO:root:[    3] Training loss: 0.15504496, Validation loss: 0.16308305, Gradient norm: 3.67568151
INFO:root:[    4] Training loss: 0.14218348, Validation loss: 0.15242686, Gradient norm: 3.30739391
INFO:root:[    5] Training loss: 0.12916224, Validation loss: 0.19225001, Gradient norm: 3.05497245
INFO:root:[    6] Training loss: 0.12870796, Validation loss: 0.12119816, Gradient norm: 4.02073395
INFO:root:[    7] Training loss: 0.12106552, Validation loss: 0.14386404, Gradient norm: 3.45024582
INFO:root:[    8] Training loss: 0.11746318, Validation loss: 0.15475890, Gradient norm: 3.12162806
INFO:root:[    9] Training loss: 0.11258227, Validation loss: 0.15240269, Gradient norm: 3.12537362
INFO:root:[   10] Training loss: 0.10924259, Validation loss: 0.15981648, Gradient norm: 3.14700271
INFO:root:[   11] Training loss: 0.11046673, Validation loss: 0.14947047, Gradient norm: 2.81748077
INFO:root:[   12] Training loss: 0.11190561, Validation loss: 0.12396028, Gradient norm: 3.16858198
INFO:root:[   13] Training loss: 0.10395970, Validation loss: 0.12626292, Gradient norm: 2.92988613
INFO:root:[   14] Training loss: 0.10350495, Validation loss: 0.12399267, Gradient norm: 3.03133731
INFO:root:[   15] Training loss: 0.10485871, Validation loss: 0.12177551, Gradient norm: 3.40555488
INFO:root:[   16] Training loss: 0.09778578, Validation loss: 0.12677048, Gradient norm: 3.09237678
INFO:root:[   17] Training loss: 0.09750256, Validation loss: 0.17147826, Gradient norm: 2.93955605
INFO:root:[   18] Training loss: 0.09510592, Validation loss: 0.13718627, Gradient norm: 2.97920408
INFO:root:[   19] Training loss: 0.09464236, Validation loss: 0.17940720, Gradient norm: 2.94650578
INFO:root:[   20] Training loss: 0.09581854, Validation loss: 0.13940974, Gradient norm: 3.13507893
INFO:root:[   21] Training loss: 0.09268858, Validation loss: 0.16018921, Gradient norm: 2.87160651
INFO:root:[   22] Training loss: 0.09188234, Validation loss: 0.17952279, Gradient norm: 2.99672842
INFO:root:[   23] Training loss: 0.09248958, Validation loss: 0.13124750, Gradient norm: 3.33304861
INFO:root:[   24] Training loss: 0.08809130, Validation loss: 0.17208274, Gradient norm: 2.96939373
INFO:root:[   25] Training loss: 0.08721218, Validation loss: 0.19809581, Gradient norm: 2.81102780
INFO:root:[   26] Training loss: 0.08764134, Validation loss: 0.16045217, Gradient norm: 2.90763014
INFO:root:[   27] Training loss: 0.08655014, Validation loss: 0.18258462, Gradient norm: 2.84379893
INFO:root:[   28] Training loss: 0.08763927, Validation loss: 0.14683782, Gradient norm: 3.35902754
INFO:root:[   29] Training loss: 0.08511302, Validation loss: 0.16442444, Gradient norm: 2.66925409
INFO:root:[   30] Training loss: 0.08269543, Validation loss: 0.17096738, Gradient norm: 2.51569161
INFO:root:[   31] Training loss: 0.08040483, Validation loss: 0.15710871, Gradient norm: 2.84174588
INFO:root:[   32] Training loss: 0.08292940, Validation loss: 0.15470098, Gradient norm: 3.30887331
INFO:root:[   33] Training loss: 0.07924594, Validation loss: 0.17227302, Gradient norm: 2.81979020
INFO:root:[   34] Training loss: 0.07959628, Validation loss: 0.18200780, Gradient norm: 2.72957105
INFO:root:[   35] Training loss: 0.07947254, Validation loss: 0.15297173, Gradient norm: 2.68299862
INFO:root:[   36] Training loss: 0.07665431, Validation loss: 0.17663007, Gradient norm: 2.62823193
INFO:root:[   37] Training loss: 0.07909796, Validation loss: 0.18213218, Gradient norm: 2.98424063
INFO:root:[   38] Training loss: 0.07810131, Validation loss: 0.18438430, Gradient norm: 2.76958851
INFO:root:[   39] Training loss: 0.07840244, Validation loss: 0.19104115, Gradient norm: 2.92265993
INFO:root:[   40] Training loss: 0.07696266, Validation loss: 0.16605375, Gradient norm: 2.92703085
INFO:root:[   41] Training loss: 0.07572501, Validation loss: 0.15047034, Gradient norm: 2.53735329
INFO:root:[   42] Training loss: 0.07712300, Validation loss: 0.15424743, Gradient norm: 3.26959987
INFO:root:[   43] Training loss: 0.07677240, Validation loss: 0.15866393, Gradient norm: 2.65970944
INFO:root:[   44] Training loss: 0.07405739, Validation loss: 0.18310253, Gradient norm: 2.59113921
INFO:root:[   45] Training loss: 0.07484311, Validation loss: 0.17390498, Gradient norm: 2.61558885
INFO:root:[   46] Training loss: 0.07535945, Validation loss: 0.15119689, Gradient norm: 3.08752304
INFO:root:[   47] Training loss: 0.07306397, Validation loss: 0.17497391, Gradient norm: 2.81702871
INFO:root:[   48] Training loss: 0.07415884, Validation loss: 0.17312357, Gradient norm: 2.83986200
INFO:root:[   49] Training loss: 0.07093385, Validation loss: 0.15731987, Gradient norm: 2.39915531
INFO:root:[   50] Training loss: 0.07261436, Validation loss: 0.16346275, Gradient norm: 2.90740363
INFO:root:[   51] Training loss: 0.07228095, Validation loss: 0.16543963, Gradient norm: 2.68020132
INFO:root:[   52] Training loss: 0.07060563, Validation loss: 0.18926065, Gradient norm: 2.62442438
INFO:root:[   53] Training loss: 0.07057385, Validation loss: 0.16947369, Gradient norm: 2.72559741
INFO:root:[   54] Training loss: 0.07062747, Validation loss: 0.17365996, Gradient norm: 2.91187491
INFO:root:[   55] Training loss: 0.06898787, Validation loss: 0.17237037, Gradient norm: 2.68754189
INFO:root:[   56] Training loss: 0.06823348, Validation loss: 0.15985759, Gradient norm: 2.48661474
INFO:root:[   57] Training loss: 0.06820222, Validation loss: 0.16505328, Gradient norm: 2.61336062
INFO:root:[   58] Training loss: 0.07042323, Validation loss: 0.17040629, Gradient norm: 2.82160020
INFO:root:[   59] Training loss: 0.06825767, Validation loss: 0.16071254, Gradient norm: 2.73425729
INFO:root:[   60] Training loss: 0.06876665, Validation loss: 0.17255139, Gradient norm: 2.54692340
INFO:root:[   61] Training loss: 0.06682580, Validation loss: 0.15230541, Gradient norm: 2.59712176
INFO:root:[   62] Training loss: 0.06760673, Validation loss: 0.17069916, Gradient norm: 2.71911641
INFO:root:[   63] Training loss: 0.06716258, Validation loss: 0.16487911, Gradient norm: 2.86599964
INFO:root:[   64] Training loss: 0.06657351, Validation loss: 0.15956527, Gradient norm: 2.34932880
INFO:root:[   65] Training loss: 0.06710558, Validation loss: 0.16183886, Gradient norm: 2.61452920
INFO:root:[   66] Training loss: 0.06579268, Validation loss: 0.16695115, Gradient norm: 2.50613072
INFO:root:[   67] Training loss: 0.06640415, Validation loss: 0.17438766, Gradient norm: 2.53411812
INFO:root:[   68] Training loss: 0.06825028, Validation loss: 0.18139563, Gradient norm: 3.07360875
INFO:root:[   69] Training loss: 0.06506540, Validation loss: 0.17801092, Gradient norm: 2.51662994
INFO:root:[   70] Training loss: 0.06579052, Validation loss: 0.17263395, Gradient norm: 2.31370535
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 3335.649s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.2616
INFO:root:EnergyScoreTrain: 0.13823
INFO:root:CoverageTrain: 0.66818
INFO:root:IntervalWidthTrain: 0.0988
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.25355
INFO:root:EnergyScoreValidation: 0.13566
INFO:root:CoverageValidation: 0.63559
INFO:root:IntervalWidthValidation: 0.08753
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.25482
INFO:root:EnergyScoreTest: 0.13446
INFO:root:CoverageTest: 0.67085
INFO:root:IntervalWidthTest: 0.09611
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 436207616
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.32353047, Validation loss: 0.18570804, Gradient norm: 4.83239741
INFO:root:[    2] Training loss: 0.19231676, Validation loss: 0.15724501, Gradient norm: 4.12753558
INFO:root:[    3] Training loss: 0.16504398, Validation loss: 0.15491506, Gradient norm: 3.38345768
INFO:root:[    4] Training loss: 0.15067983, Validation loss: 0.14733391, Gradient norm: 4.33037689
INFO:root:[    5] Training loss: 0.13764316, Validation loss: 0.12702120, Gradient norm: 3.47887810
INFO:root:[    6] Training loss: 0.13461117, Validation loss: 0.12163278, Gradient norm: 3.10272076
INFO:root:[    7] Training loss: 0.12696962, Validation loss: 0.14302714, Gradient norm: 3.41125016
INFO:root:[    8] Training loss: 0.12343101, Validation loss: 0.12264350, Gradient norm: 3.47797295
INFO:root:[    9] Training loss: 0.11901882, Validation loss: 0.12654254, Gradient norm: 3.01193614
INFO:root:[   10] Training loss: 0.11610193, Validation loss: 0.11985724, Gradient norm: 3.38191705
INFO:root:[   11] Training loss: 0.11306562, Validation loss: 0.12124079, Gradient norm: 2.95506014
INFO:root:[   12] Training loss: 0.11108812, Validation loss: 0.14071502, Gradient norm: 3.18429687
INFO:root:[   13] Training loss: 0.10756518, Validation loss: 0.13258130, Gradient norm: 2.86335663
INFO:root:[   14] Training loss: 0.10432927, Validation loss: 0.14210664, Gradient norm: 2.97493543
INFO:root:[   15] Training loss: 0.10322301, Validation loss: 0.18337194, Gradient norm: 2.58088598
INFO:root:[   16] Training loss: 0.10368002, Validation loss: 0.15844912, Gradient norm: 3.03318308
INFO:root:[   17] Training loss: 0.10207254, Validation loss: 0.13116696, Gradient norm: 2.99186962
INFO:root:[   18] Training loss: 0.09796575, Validation loss: 0.13738839, Gradient norm: 2.69985047
INFO:root:[   19] Training loss: 0.09622722, Validation loss: 0.14139544, Gradient norm: 2.80942491
INFO:root:[   20] Training loss: 0.09423533, Validation loss: 0.13926546, Gradient norm: 2.87293065
INFO:root:[   21] Training loss: 0.09211136, Validation loss: 0.17428892, Gradient norm: 2.73052816
INFO:root:[   22] Training loss: 0.08825374, Validation loss: 0.13734332, Gradient norm: 2.47277296
INFO:root:[   23] Training loss: 0.09068156, Validation loss: 0.13757060, Gradient norm: 2.77928935
INFO:root:[   24] Training loss: 0.09151314, Validation loss: 0.16446076, Gradient norm: 3.18279705
INFO:root:[   25] Training loss: 0.08847999, Validation loss: 0.17418347, Gradient norm: 2.88164729
INFO:root:[   26] Training loss: 0.08683496, Validation loss: 0.16225750, Gradient norm: 3.04206487
INFO:root:[   27] Training loss: 0.08469555, Validation loss: 0.19629748, Gradient norm: 2.58284248
INFO:root:[   28] Training loss: 0.08488225, Validation loss: 0.17949562, Gradient norm: 2.56313889
INFO:root:[   29] Training loss: 0.08062840, Validation loss: 0.16022657, Gradient norm: 2.56498903
INFO:root:[   30] Training loss: 0.08128539, Validation loss: 0.16627205, Gradient norm: 2.60010288
INFO:root:[   31] Training loss: 0.08120207, Validation loss: 0.14565829, Gradient norm: 2.54140050
INFO:root:[   32] Training loss: 0.07689958, Validation loss: 0.15501072, Gradient norm: 2.56730193
INFO:root:[   33] Training loss: 0.07817209, Validation loss: 0.14807836, Gradient norm: 2.89712681
INFO:root:[   34] Training loss: 0.07891757, Validation loss: 0.17646138, Gradient norm: 2.90471161
INFO:root:[   35] Training loss: 0.07781060, Validation loss: 0.15227407, Gradient norm: 2.52907470
INFO:root:[   36] Training loss: 0.07578630, Validation loss: 0.16287545, Gradient norm: 2.83169385
INFO:root:[   37] Training loss: 0.07855451, Validation loss: 0.14679817, Gradient norm: 2.91671569
INFO:root:[   38] Training loss: 0.07493681, Validation loss: 0.17521782, Gradient norm: 2.63979779
INFO:root:[   39] Training loss: 0.07571811, Validation loss: 0.15713981, Gradient norm: 2.92774357
INFO:root:[   40] Training loss: 0.07378177, Validation loss: 0.15508223, Gradient norm: 2.78060131
INFO:root:[   41] Training loss: 0.07198342, Validation loss: 0.18519390, Gradient norm: 2.84063519
INFO:root:[   42] Training loss: 0.07433353, Validation loss: 0.15637350, Gradient norm: 2.84085634
INFO:root:[   43] Training loss: 0.07009487, Validation loss: 0.15751857, Gradient norm: 2.40149850
INFO:root:[   44] Training loss: 0.07093442, Validation loss: 0.16183317, Gradient norm: 2.38504603
INFO:root:[   45] Training loss: 0.07239519, Validation loss: 0.15887216, Gradient norm: 2.62489493
INFO:root:[   46] Training loss: 0.07061316, Validation loss: 0.16715940, Gradient norm: 2.49491180
INFO:root:[   47] Training loss: 0.07229068, Validation loss: 0.17249266, Gradient norm: 2.81747502
INFO:root:[   48] Training loss: 0.07019984, Validation loss: 0.15470293, Gradient norm: 2.83679071
INFO:root:[   49] Training loss: 0.06886177, Validation loss: 0.15426339, Gradient norm: 2.51162310
INFO:root:[   50] Training loss: 0.06784192, Validation loss: 0.16091694, Gradient norm: 2.52331446
INFO:root:[   51] Training loss: 0.07103100, Validation loss: 0.14341897, Gradient norm: 2.50517373
INFO:root:[   52] Training loss: 0.06922046, Validation loss: 0.14487685, Gradient norm: 2.61681094
INFO:root:[   53] Training loss: 0.07039896, Validation loss: 0.17626390, Gradient norm: 2.93812300
INFO:root:[   54] Training loss: 0.06563578, Validation loss: 0.16542994, Gradient norm: 2.35393538
INFO:root:[   55] Training loss: 0.06815941, Validation loss: 0.18161686, Gradient norm: 2.47009844
INFO:root:[   56] Training loss: 0.06869345, Validation loss: 0.15227357, Gradient norm: 2.69413484
INFO:root:[   57] Training loss: 0.06643970, Validation loss: 0.14782160, Gradient norm: 2.81065914
INFO:root:[   58] Training loss: 0.06626301, Validation loss: 0.15329711, Gradient norm: 2.41974296
INFO:root:[   59] Training loss: 0.06782510, Validation loss: 0.17555599, Gradient norm: 2.55444471
INFO:root:[   60] Training loss: 0.06654573, Validation loss: 0.16381667, Gradient norm: 2.51187895
INFO:root:[   61] Training loss: 0.06352757, Validation loss: 0.16569360, Gradient norm: 2.04053414
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 2919.66s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.24623
INFO:root:EnergyScoreTrain: 0.12864
INFO:root:CoverageTrain: 0.6789
INFO:root:IntervalWidthTrain: 0.09711
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.23591
INFO:root:EnergyScoreValidation: 0.1259
INFO:root:CoverageValidation: 0.64912
INFO:root:IntervalWidthValidation: 0.08801
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.22054
INFO:root:EnergyScoreTest: 0.11626
INFO:root:CoverageTest: 0.68336
INFO:root:IntervalWidthTest: 0.08881
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 469762048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28306375, Validation loss: 0.18484421, Gradient norm: 3.49462013
INFO:root:[    2] Training loss: 0.17313155, Validation loss: 0.14085840, Gradient norm: 3.35349136
INFO:root:[    3] Training loss: 0.15171042, Validation loss: 0.17643802, Gradient norm: 3.36362513
INFO:root:[    4] Training loss: 0.13867610, Validation loss: 0.14035901, Gradient norm: 3.12349786
INFO:root:[    5] Training loss: 0.12876117, Validation loss: 0.15548698, Gradient norm: 3.04551301
INFO:root:[    6] Training loss: 0.12304833, Validation loss: 0.12787385, Gradient norm: 3.15207935
INFO:root:[    7] Training loss: 0.11877190, Validation loss: 0.15028143, Gradient norm: 2.94465818
INFO:root:[    8] Training loss: 0.11441187, Validation loss: 0.13391308, Gradient norm: 2.89428283
INFO:root:[    9] Training loss: 0.11458865, Validation loss: 0.15831783, Gradient norm: 3.11711189
INFO:root:[   10] Training loss: 0.11000647, Validation loss: 0.12494580, Gradient norm: 2.62937692
INFO:root:[   11] Training loss: 0.11086774, Validation loss: 0.16202200, Gradient norm: 3.22070509
INFO:root:[   12] Training loss: 0.10759297, Validation loss: 0.14699508, Gradient norm: 2.97374483
INFO:root:[   13] Training loss: 0.10556369, Validation loss: 0.16630203, Gradient norm: 2.91022119
INFO:root:[   14] Training loss: 0.09921122, Validation loss: 0.16865807, Gradient norm: 2.70226008
INFO:root:[   15] Training loss: 0.10164871, Validation loss: 0.15453297, Gradient norm: 2.80090896
INFO:root:[   16] Training loss: 0.09840292, Validation loss: 0.15928003, Gradient norm: 2.70483288
INFO:root:[   17] Training loss: 0.09485196, Validation loss: 0.14104852, Gradient norm: 2.64563354
INFO:root:[   18] Training loss: 0.09621476, Validation loss: 0.12594563, Gradient norm: 2.84382162
INFO:root:[   19] Training loss: 0.09548242, Validation loss: 0.15046082, Gradient norm: 2.82156424
INFO:root:[   20] Training loss: 0.09428747, Validation loss: 0.16044530, Gradient norm: 2.65032963
INFO:root:[   21] Training loss: 0.09319700, Validation loss: 0.16632828, Gradient norm: 2.82193425
INFO:root:[   22] Training loss: 0.08913442, Validation loss: 0.16046237, Gradient norm: 2.71495396
INFO:root:[   23] Training loss: 0.09029825, Validation loss: 0.16064653, Gradient norm: 2.56154958
INFO:root:[   24] Training loss: 0.08472883, Validation loss: 0.18193161, Gradient norm: 2.32470019
INFO:root:[   25] Training loss: 0.08712647, Validation loss: 0.16196683, Gradient norm: 3.03547320
INFO:root:[   26] Training loss: 0.08743902, Validation loss: 0.20108842, Gradient norm: 3.01615229
INFO:root:[   27] Training loss: 0.08651958, Validation loss: 0.16050234, Gradient norm: 2.92299149
INFO:root:[   28] Training loss: 0.08254814, Validation loss: 0.19202162, Gradient norm: 2.58571154
INFO:root:[   29] Training loss: 0.08105727, Validation loss: 0.17072787, Gradient norm: 3.02296282
INFO:root:[   30] Training loss: 0.08355127, Validation loss: 0.19292749, Gradient norm: 3.09993736
INFO:root:[   31] Training loss: 0.08003268, Validation loss: 0.17454219, Gradient norm: 2.87435134
INFO:root:[   32] Training loss: 0.07606250, Validation loss: 0.14731421, Gradient norm: 2.72161969
INFO:root:[   33] Training loss: 0.07953168, Validation loss: 0.18602737, Gradient norm: 3.03121191
INFO:root:[   34] Training loss: 0.07935848, Validation loss: 0.15483042, Gradient norm: 3.13370618
INFO:root:[   35] Training loss: 0.07599358, Validation loss: 0.16472561, Gradient norm: 3.07878934
INFO:root:[   36] Training loss: 0.07660403, Validation loss: 0.17116980, Gradient norm: 3.25823488
INFO:root:[   37] Training loss: 0.07572199, Validation loss: 0.17955711, Gradient norm: 2.97404060
INFO:root:[   38] Training loss: 0.07350983, Validation loss: 0.16868763, Gradient norm: 3.16942593
INFO:root:[   39] Training loss: 0.07291039, Validation loss: 0.14862756, Gradient norm: 2.47577186
INFO:root:[   40] Training loss: 0.07468105, Validation loss: 0.15656632, Gradient norm: 2.81457076
INFO:root:[   41] Training loss: 0.07158080, Validation loss: 0.17714407, Gradient norm: 2.67900552
INFO:root:[   42] Training loss: 0.07103062, Validation loss: 0.16419680, Gradient norm: 2.57005432
INFO:root:[   43] Training loss: 0.07409950, Validation loss: 0.16589691, Gradient norm: 2.57829805
INFO:root:[   44] Training loss: 0.07210266, Validation loss: 0.17204704, Gradient norm: 2.83080733
INFO:root:[   45] Training loss: 0.07200824, Validation loss: 0.14898827, Gradient norm: 3.27666224
INFO:root:[   46] Training loss: 0.07184576, Validation loss: 0.16199810, Gradient norm: 3.11957963
INFO:root:[   47] Training loss: 0.06943156, Validation loss: 0.15706221, Gradient norm: 2.57440032
INFO:root:[   48] Training loss: 0.07298800, Validation loss: 0.15701589, Gradient norm: 3.03741475
INFO:root:[   49] Training loss: 0.06958365, Validation loss: 0.17705756, Gradient norm: 2.82421451
INFO:root:[   50] Training loss: 0.07246334, Validation loss: 0.17017301, Gradient norm: 3.17767877
INFO:root:[   51] Training loss: 0.07008422, Validation loss: 0.16302726, Gradient norm: 2.89057073
INFO:root:[   52] Training loss: 0.07117300, Validation loss: 0.17685321, Gradient norm: 3.08851612
INFO:root:[   53] Training loss: 0.06875003, Validation loss: 0.16549217, Gradient norm: 2.84174996
INFO:root:[   54] Training loss: 0.06996960, Validation loss: 0.17256648, Gradient norm: 2.87044952
INFO:root:[   55] Training loss: 0.06682813, Validation loss: 0.16139087, Gradient norm: 2.57631359
INFO:root:[   56] Training loss: 0.06704880, Validation loss: 0.15042043, Gradient norm: 2.85964580
INFO:root:[   57] Training loss: 0.06922732, Validation loss: 0.17623418, Gradient norm: 2.49149104
INFO:root:[   58] Training loss: 0.06702804, Validation loss: 0.15805587, Gradient norm: 2.65793274
INFO:root:[   59] Training loss: 0.06786508, Validation loss: 0.15430685, Gradient norm: 2.74244072
INFO:root:[   60] Training loss: 0.06782050, Validation loss: 0.15755054, Gradient norm: 2.41799456
INFO:root:[   61] Training loss: 0.06629453, Validation loss: 0.16319323, Gradient norm: 2.65390632
INFO:root:[   62] Training loss: 0.06687801, Validation loss: 0.16863055, Gradient norm: 2.73855813
INFO:root:[   63] Training loss: 0.06660219, Validation loss: 0.16210724, Gradient norm: 2.68480318
INFO:root:[   64] Training loss: 0.06619346, Validation loss: 0.14736743, Gradient norm: 2.59690036
INFO:root:[   65] Training loss: 0.06830057, Validation loss: 0.17193733, Gradient norm: 2.99994669
INFO:root:[   66] Training loss: 0.06577782, Validation loss: 0.15663270, Gradient norm: 2.42661457
INFO:root:[   67] Training loss: 0.06540522, Validation loss: 0.16092124, Gradient norm: 2.68264407
INFO:root:[   68] Training loss: 0.06521432, Validation loss: 0.16331039, Gradient norm: 2.31681190
INFO:root:[   69] Training loss: 0.06547138, Validation loss: 0.15814833, Gradient norm: 2.61153375
INFO:root:[   70] Training loss: 0.06690489, Validation loss: 0.16286201, Gradient norm: 2.79233177
INFO:root:[   71] Training loss: 0.06773145, Validation loss: 0.16306098, Gradient norm: 2.86962723
INFO:root:[   72] Training loss: 0.06564700, Validation loss: 0.15454853, Gradient norm: 2.99587567
INFO:root:[   73] Training loss: 0.06591777, Validation loss: 0.17431165, Gradient norm: 2.36996458
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 3493.274s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.23449
INFO:root:EnergyScoreTrain: 0.12307
INFO:root:CoverageTrain: 0.67644
INFO:root:IntervalWidthTrain: 0.08938
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.21979
INFO:root:EnergyScoreValidation: 0.11733
INFO:root:CoverageValidation: 0.66891
INFO:root:IntervalWidthValidation: 0.08278
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.22824
INFO:root:EnergyScoreTest: 0.12445
INFO:root:CoverageTest: 0.64739
INFO:root:IntervalWidthTest: 0.08139
INFO:root:###6 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 436207616
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.30329552, Validation loss: 0.18654896, Gradient norm: 4.89560684
INFO:root:[    2] Training loss: 0.17799841, Validation loss: 0.15023133, Gradient norm: 3.87555608
INFO:root:[    3] Training loss: 0.16134792, Validation loss: 0.14966107, Gradient norm: 3.63475572
INFO:root:[    4] Training loss: 0.15059428, Validation loss: 0.13974623, Gradient norm: 3.10491952
INFO:root:[    5] Training loss: 0.14173504, Validation loss: 0.14982917, Gradient norm: 3.40224537
INFO:root:[    6] Training loss: 0.13501652, Validation loss: 0.14325608, Gradient norm: 2.82595745
INFO:root:[    7] Training loss: 0.12601227, Validation loss: 0.14169496, Gradient norm: 2.83100050
INFO:root:[    8] Training loss: 0.12582803, Validation loss: 0.16880252, Gradient norm: 3.74041362
INFO:root:[    9] Training loss: 0.11586310, Validation loss: 0.13179025, Gradient norm: 3.33764672
INFO:root:[   10] Training loss: 0.11412761, Validation loss: 0.16138312, Gradient norm: 2.69657399
INFO:root:[   11] Training loss: 0.11499924, Validation loss: 0.13019699, Gradient norm: 3.05305655
INFO:root:[   12] Training loss: 0.11041790, Validation loss: 0.12490408, Gradient norm: 3.42335740
INFO:root:[   13] Training loss: 0.10728360, Validation loss: 0.15270122, Gradient norm: 3.21290186
INFO:root:[   14] Training loss: 0.10560903, Validation loss: 0.14715878, Gradient norm: 2.70485096
INFO:root:[   15] Training loss: 0.10014992, Validation loss: 0.14390626, Gradient norm: 2.58750508
INFO:root:[   16] Training loss: 0.10117475, Validation loss: 0.15653212, Gradient norm: 2.63086254
INFO:root:[   17] Training loss: 0.10068656, Validation loss: 0.15746577, Gradient norm: 2.99620514
INFO:root:[   18] Training loss: 0.09701280, Validation loss: 0.14788114, Gradient norm: 2.87730730
INFO:root:[   19] Training loss: 0.09451423, Validation loss: 0.13614135, Gradient norm: 2.61329341
INFO:root:[   20] Training loss: 0.09432103, Validation loss: 0.15132250, Gradient norm: 2.81852201
INFO:root:[   21] Training loss: 0.09344944, Validation loss: 0.13229854, Gradient norm: 2.75900826
INFO:root:[   22] Training loss: 0.09137571, Validation loss: 0.17670834, Gradient norm: 2.97129767
INFO:root:[   23] Training loss: 0.08739310, Validation loss: 0.17081072, Gradient norm: 2.43525197
INFO:root:[   24] Training loss: 0.08859688, Validation loss: 0.14860426, Gradient norm: 2.79142621
INFO:root:[   25] Training loss: 0.08910795, Validation loss: 0.15176132, Gradient norm: 3.08951550
INFO:root:[   26] Training loss: 0.08860862, Validation loss: 0.14744294, Gradient norm: 3.12758477
INFO:root:[   27] Training loss: 0.08307120, Validation loss: 0.15322393, Gradient norm: 2.59924397
INFO:root:[   28] Training loss: 0.08422061, Validation loss: 0.14236562, Gradient norm: 2.94902243
INFO:root:[   29] Training loss: 0.08207226, Validation loss: 0.17190855, Gradient norm: 2.43316881
INFO:root:[   30] Training loss: 0.08410964, Validation loss: 0.16778379, Gradient norm: 2.74615981
INFO:root:[   31] Training loss: 0.08023276, Validation loss: 0.17258720, Gradient norm: 2.95755872
INFO:root:[   32] Training loss: 0.08083863, Validation loss: 0.16699872, Gradient norm: 2.74253196
INFO:root:[   33] Training loss: 0.08062672, Validation loss: 0.15973914, Gradient norm: 2.99613758
INFO:root:[   34] Training loss: 0.07852328, Validation loss: 0.15656648, Gradient norm: 2.82629833
INFO:root:[   35] Training loss: 0.07806141, Validation loss: 0.16714538, Gradient norm: 2.43189840
INFO:root:[   36] Training loss: 0.07628873, Validation loss: 0.15225820, Gradient norm: 2.81390289
INFO:root:[   37] Training loss: 0.07369736, Validation loss: 0.15784710, Gradient norm: 2.40449224
INFO:root:[   38] Training loss: 0.07643461, Validation loss: 0.15575651, Gradient norm: 2.86953672
INFO:root:[   39] Training loss: 0.07144816, Validation loss: 0.17336475, Gradient norm: 2.44586621
INFO:root:[   40] Training loss: 0.07323734, Validation loss: 0.16189516, Gradient norm: 2.70005229
INFO:root:[   41] Training loss: 0.07418340, Validation loss: 0.16362781, Gradient norm: 2.54759794
INFO:root:[   42] Training loss: 0.07286086, Validation loss: 0.15718031, Gradient norm: 2.80025163
INFO:root:[   43] Training loss: 0.07267952, Validation loss: 0.15556980, Gradient norm: 2.40925560
INFO:root:[   44] Training loss: 0.07653657, Validation loss: 0.15895814, Gradient norm: 3.24834752
INFO:root:[   45] Training loss: 0.07136823, Validation loss: 0.16918592, Gradient norm: 2.71917375
INFO:root:[   46] Training loss: 0.07107308, Validation loss: 0.16219547, Gradient norm: 2.72261400
INFO:root:[   47] Training loss: 0.07021219, Validation loss: 0.16839962, Gradient norm: 2.46224571
INFO:root:[   48] Training loss: 0.07069757, Validation loss: 0.15569897, Gradient norm: 2.39076145
INFO:root:[   49] Training loss: 0.07058105, Validation loss: 0.15908575, Gradient norm: 2.45691471
INFO:root:[   50] Training loss: 0.07005009, Validation loss: 0.18439522, Gradient norm: 2.98585204
INFO:root:[   51] Training loss: 0.07047122, Validation loss: 0.18614347, Gradient norm: 2.29298338
INFO:root:[   52] Training loss: 0.06848638, Validation loss: 0.18412642, Gradient norm: 2.51396083
INFO:root:[   53] Training loss: 0.06941440, Validation loss: 0.17267100, Gradient norm: 2.82799906
INFO:root:[   54] Training loss: 0.06748271, Validation loss: 0.16316911, Gradient norm: 2.67532810
INFO:root:[   55] Training loss: 0.06745918, Validation loss: 0.16141837, Gradient norm: 2.61975316
INFO:root:[   56] Training loss: 0.06720841, Validation loss: 0.18536218, Gradient norm: 2.38093971
INFO:root:[   57] Training loss: 0.06708107, Validation loss: 0.17472518, Gradient norm: 2.55536720
INFO:root:[   58] Training loss: 0.06686043, Validation loss: 0.15857016, Gradient norm: 2.36719438
INFO:root:[   59] Training loss: 0.06797751, Validation loss: 0.19789647, Gradient norm: 2.70936450
INFO:root:[   60] Training loss: 0.06919644, Validation loss: 0.16569727, Gradient norm: 2.88784236
INFO:root:[   61] Training loss: 0.06526489, Validation loss: 0.15891225, Gradient norm: 2.26572938
INFO:root:[   62] Training loss: 0.06462650, Validation loss: 0.15126957, Gradient norm: 2.54748574
INFO:root:[   63] Training loss: 0.06728273, Validation loss: 0.16518347, Gradient norm: 3.02994731
INFO:root:[   64] Training loss: 0.06590719, Validation loss: 0.17830541, Gradient norm: 2.86146470
INFO:root:[   65] Training loss: 0.06536944, Validation loss: 0.16173896, Gradient norm: 2.68118514
INFO:root:[   66] Training loss: 0.06592846, Validation loss: 0.17292458, Gradient norm: 2.30662575
INFO:root:[   67] Training loss: 0.06492178, Validation loss: 0.17493313, Gradient norm: 2.22766925
INFO:root:[   68] Training loss: 0.06681956, Validation loss: 0.16520049, Gradient norm: 2.67204561
INFO:root:[   69] Training loss: 0.06488285, Validation loss: 0.17595272, Gradient norm: 2.59396438
INFO:root:[   70] Training loss: 0.06668853, Validation loss: 0.17394017, Gradient norm: 2.72181145
INFO:root:[   71] Training loss: 0.06747926, Validation loss: 0.17071991, Gradient norm: 2.72128649
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 3373.588s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.24592
INFO:root:EnergyScoreTrain: 0.12857
INFO:root:CoverageTrain: 0.68083
INFO:root:IntervalWidthTrain: 0.10137
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.25283
INFO:root:EnergyScoreValidation: 0.13471
INFO:root:CoverageValidation: 0.6626
INFO:root:IntervalWidthValidation: 0.09684
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.25369
INFO:root:EnergyScoreTest: 0.13466
INFO:root:CoverageTest: 0.67414
INFO:root:IntervalWidthTest: 0.10185
INFO:root:###7 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234567, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 469762048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27331218, Validation loss: 0.18063800, Gradient norm: 3.86355734
INFO:root:[    2] Training loss: 0.17629067, Validation loss: 0.17424665, Gradient norm: 2.91549559
INFO:root:[    3] Training loss: 0.15955797, Validation loss: 0.17132911, Gradient norm: 2.57552866
INFO:root:[    4] Training loss: 0.15081403, Validation loss: 0.16023917, Gradient norm: 2.99000006
INFO:root:[    5] Training loss: 0.14611891, Validation loss: 0.18555855, Gradient norm: 3.26831324
INFO:root:[    6] Training loss: 0.13682861, Validation loss: 0.14385825, Gradient norm: 2.73576057
INFO:root:[    7] Training loss: 0.13301664, Validation loss: 0.15059463, Gradient norm: 2.87822206
INFO:root:[    8] Training loss: 0.12718175, Validation loss: 0.13326597, Gradient norm: 2.86777447
INFO:root:[    9] Training loss: 0.12193865, Validation loss: 0.13417034, Gradient norm: 3.20758898
INFO:root:[   10] Training loss: 0.11714835, Validation loss: 0.14353696, Gradient norm: 3.22971802
INFO:root:[   11] Training loss: 0.11233157, Validation loss: 0.17114496, Gradient norm: 3.03059563
INFO:root:[   12] Training loss: 0.10670495, Validation loss: 0.15316576, Gradient norm: 2.44873622
INFO:root:[   13] Training loss: 0.10942862, Validation loss: 0.13296276, Gradient norm: 3.04855360
INFO:root:[   14] Training loss: 0.10471304, Validation loss: 0.18600468, Gradient norm: 2.69203582
INFO:root:[   15] Training loss: 0.10307295, Validation loss: 0.16918614, Gradient norm: 2.81155095
INFO:root:[   16] Training loss: 0.09984032, Validation loss: 0.14080110, Gradient norm: 2.50611908
INFO:root:[   17] Training loss: 0.09958084, Validation loss: 0.19662258, Gradient norm: 2.70542787
INFO:root:[   18] Training loss: 0.10038377, Validation loss: 0.15904555, Gradient norm: 2.99659413
INFO:root:[   19] Training loss: 0.09868404, Validation loss: 0.16712281, Gradient norm: 2.82625008
INFO:root:[   20] Training loss: 0.09437734, Validation loss: 0.14172086, Gradient norm: 2.60966703
INFO:root:[   21] Training loss: 0.09325826, Validation loss: 0.14401370, Gradient norm: 2.64912021
INFO:root:[   22] Training loss: 0.09028353, Validation loss: 0.15468524, Gradient norm: 2.62241431
INFO:root:[   23] Training loss: 0.09073895, Validation loss: 0.16409655, Gradient norm: 2.86389723
INFO:root:[   24] Training loss: 0.09161769, Validation loss: 0.14384783, Gradient norm: 2.88543429
INFO:root:[   25] Training loss: 0.08646613, Validation loss: 0.18784786, Gradient norm: 2.39903884
INFO:root:[   26] Training loss: 0.08719091, Validation loss: 0.16269687, Gradient norm: 2.61596826
INFO:root:[   27] Training loss: 0.08822586, Validation loss: 0.14917297, Gradient norm: 3.10162430
INFO:root:[   28] Training loss: 0.08637261, Validation loss: 0.18708896, Gradient norm: 2.74114521
INFO:root:[   29] Training loss: 0.08188389, Validation loss: 0.18748261, Gradient norm: 2.59236865
INFO:root:[   30] Training loss: 0.08500749, Validation loss: 0.19220585, Gradient norm: 2.71945141
INFO:root:[   31] Training loss: 0.08130346, Validation loss: 0.16204449, Gradient norm: 2.83094110
INFO:root:[   32] Training loss: 0.08457156, Validation loss: 0.17460962, Gradient norm: 3.19593957
INFO:root:[   33] Training loss: 0.08078123, Validation loss: 0.18140946, Gradient norm: 3.03809253
INFO:root:[   34] Training loss: 0.08003832, Validation loss: 0.17141237, Gradient norm: 2.56132564
INFO:root:[   35] Training loss: 0.07974079, Validation loss: 0.17497609, Gradient norm: 2.52027081
INFO:root:[   36] Training loss: 0.07902116, Validation loss: 0.18569657, Gradient norm: 2.87500493
INFO:root:[   37] Training loss: 0.07788856, Validation loss: 0.16567852, Gradient norm: 2.77576518
INFO:root:[   38] Training loss: 0.07500071, Validation loss: 0.17868891, Gradient norm: 2.92958475
INFO:root:[   39] Training loss: 0.07825438, Validation loss: 0.17754493, Gradient norm: 2.67101476
INFO:root:[   40] Training loss: 0.07634760, Validation loss: 0.16697703, Gradient norm: 2.65915960
INFO:root:[   41] Training loss: 0.07769476, Validation loss: 0.17186727, Gradient norm: 3.03266895
INFO:root:[   42] Training loss: 0.07564217, Validation loss: 0.16295070, Gradient norm: 2.85893173
INFO:root:[   43] Training loss: 0.07448087, Validation loss: 0.17606231, Gradient norm: 2.94140375
INFO:root:[   44] Training loss: 0.07556504, Validation loss: 0.17785066, Gradient norm: 2.67646628
INFO:root:[   45] Training loss: 0.07103860, Validation loss: 0.18385353, Gradient norm: 2.52955017
INFO:root:[   46] Training loss: 0.07329952, Validation loss: 0.15409283, Gradient norm: 2.76103552
INFO:root:[   47] Training loss: 0.07229799, Validation loss: 0.17981849, Gradient norm: 2.48239035
INFO:root:[   48] Training loss: 0.07079756, Validation loss: 0.17381774, Gradient norm: 2.65433825
INFO:root:[   49] Training loss: 0.07035131, Validation loss: 0.16558265, Gradient norm: 2.47348185
INFO:root:[   50] Training loss: 0.07077526, Validation loss: 0.18033736, Gradient norm: 2.97651812
INFO:root:[   51] Training loss: 0.07033581, Validation loss: 0.17624584, Gradient norm: 2.61525839
INFO:root:[   52] Training loss: 0.07322210, Validation loss: 0.18850198, Gradient norm: 3.00234398
INFO:root:[   53] Training loss: 0.07055753, Validation loss: 0.17340413, Gradient norm: 3.04384218
INFO:root:[   54] Training loss: 0.06846389, Validation loss: 0.17636220, Gradient norm: 2.58914442
INFO:root:[   55] Training loss: 0.07235329, Validation loss: 0.17736152, Gradient norm: 2.67866882
INFO:root:[   56] Training loss: 0.06996343, Validation loss: 0.18221955, Gradient norm: 3.12337456
INFO:root:[   57] Training loss: 0.06929397, Validation loss: 0.18833660, Gradient norm: 2.90128614
INFO:root:[   58] Training loss: 0.06795120, Validation loss: 0.16753996, Gradient norm: 2.64879662
INFO:root:[   59] Training loss: 0.06632448, Validation loss: 0.17417557, Gradient norm: 2.28126932
INFO:root:[   60] Training loss: 0.06843756, Validation loss: 0.16180499, Gradient norm: 2.16305854
INFO:root:[   61] Training loss: 0.07000747, Validation loss: 0.16245225, Gradient norm: 3.22933157
INFO:root:[   62] Training loss: 0.06636360, Validation loss: 0.17117163, Gradient norm: 2.77730838
INFO:root:[   63] Training loss: 0.06662075, Validation loss: 0.16272491, Gradient norm: 2.61819057
INFO:root:[   64] Training loss: 0.06862332, Validation loss: 0.18658616, Gradient norm: 2.98966193
INFO:root:[   65] Training loss: 0.06582501, Validation loss: 0.16989822, Gradient norm: 2.75766361
INFO:root:[   66] Training loss: 0.06779440, Validation loss: 0.17136231, Gradient norm: 2.90795484
INFO:root:[   67] Training loss: 0.07308357, Validation loss: 0.18766599, Gradient norm: 2.58123496
INFO:root:[   68] Training loss: 0.06695792, Validation loss: 0.18052391, Gradient norm: 2.59914341
INFO:root:[   69] Training loss: 0.06606479, Validation loss: 0.18898855, Gradient norm: 2.79341468
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 3264.499s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.22124
INFO:root:EnergyScoreTrain: 0.11614
INFO:root:CoverageTrain: 0.68061
INFO:root:IntervalWidthTrain: 0.08727
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.22196
INFO:root:EnergyScoreValidation: 0.12189
INFO:root:CoverageValidation: 0.63138
INFO:root:IntervalWidthValidation: 0.077
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.22101
INFO:root:EnergyScoreTest: 0.12286
INFO:root:CoverageTest: 0.61957
INFO:root:IntervalWidthTest: 0.07419
INFO:root:###8 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345678, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 436207616
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.30361082, Validation loss: 0.21038132, Gradient norm: 4.13309208
INFO:root:[    2] Training loss: 0.18481318, Validation loss: 0.18016245, Gradient norm: 3.44277330
INFO:root:[    3] Training loss: 0.16374649, Validation loss: 0.14261324, Gradient norm: 3.52722800
INFO:root:[    4] Training loss: 0.15525147, Validation loss: 0.14882409, Gradient norm: 2.96540553
INFO:root:[    5] Training loss: 0.14314869, Validation loss: 0.16328108, Gradient norm: 3.50304044
INFO:root:[    6] Training loss: 0.13697218, Validation loss: 0.13326847, Gradient norm: 3.25906102
INFO:root:[    7] Training loss: 0.13119726, Validation loss: 0.13606756, Gradient norm: 3.41076671
INFO:root:[    8] Training loss: 0.12852104, Validation loss: 0.16733620, Gradient norm: 3.60909821
INFO:root:[    9] Training loss: 0.12329351, Validation loss: 0.14541359, Gradient norm: 3.16386213
INFO:root:[   10] Training loss: 0.11749563, Validation loss: 0.14506506, Gradient norm: 3.31392850
INFO:root:[   11] Training loss: 0.11225684, Validation loss: 0.13736147, Gradient norm: 3.18480265
INFO:root:[   12] Training loss: 0.10853416, Validation loss: 0.12674914, Gradient norm: 3.20050932
INFO:root:[   13] Training loss: 0.10736963, Validation loss: 0.15636026, Gradient norm: 3.10786198
INFO:root:[   14] Training loss: 0.10451610, Validation loss: 0.16175642, Gradient norm: 3.16435842
INFO:root:[   15] Training loss: 0.10236792, Validation loss: 0.12490311, Gradient norm: 3.24073517
INFO:root:[   16] Training loss: 0.10263809, Validation loss: 0.13213868, Gradient norm: 3.06987013
INFO:root:[   17] Training loss: 0.09801800, Validation loss: 0.15237475, Gradient norm: 2.96083281
INFO:root:[   18] Training loss: 0.09673891, Validation loss: 0.14402033, Gradient norm: 3.11261143
INFO:root:[   19] Training loss: 0.09863824, Validation loss: 0.14656035, Gradient norm: 2.99692775
INFO:root:[   20] Training loss: 0.09577660, Validation loss: 0.14704747, Gradient norm: 2.97179515
INFO:root:[   21] Training loss: 0.09050730, Validation loss: 0.14792580, Gradient norm: 2.97136310
INFO:root:[   22] Training loss: 0.09026879, Validation loss: 0.15357890, Gradient norm: 2.53667927
INFO:root:[   23] Training loss: 0.09249032, Validation loss: 0.15054081, Gradient norm: 2.96051925
INFO:root:[   24] Training loss: 0.08653703, Validation loss: 0.17530420, Gradient norm: 2.45856077
INFO:root:[   25] Training loss: 0.08787220, Validation loss: 0.13097331, Gradient norm: 3.14079401
INFO:root:[   26] Training loss: 0.08786014, Validation loss: 0.16220439, Gradient norm: 2.65388219
INFO:root:[   27] Training loss: 0.08565922, Validation loss: 0.15515067, Gradient norm: 3.09186997
INFO:root:[   28] Training loss: 0.08211548, Validation loss: 0.17169645, Gradient norm: 2.41626360
INFO:root:[   29] Training loss: 0.08437228, Validation loss: 0.16173570, Gradient norm: 3.06423308
INFO:root:[   30] Training loss: 0.08174456, Validation loss: 0.14883602, Gradient norm: 3.04088193
INFO:root:[   31] Training loss: 0.07996497, Validation loss: 0.13647604, Gradient norm: 2.92095313
INFO:root:[   32] Training loss: 0.08084330, Validation loss: 0.16858514, Gradient norm: 2.56776699
INFO:root:[   33] Training loss: 0.08003887, Validation loss: 0.16115733, Gradient norm: 2.96003532
INFO:root:[   34] Training loss: 0.08011211, Validation loss: 0.15651348, Gradient norm: 2.61366099
INFO:root:[   35] Training loss: 0.07731984, Validation loss: 0.15016413, Gradient norm: 2.68691868
INFO:root:[   36] Training loss: 0.07619340, Validation loss: 0.16371562, Gradient norm: 2.66992292
INFO:root:[   37] Training loss: 0.07473977, Validation loss: 0.17065093, Gradient norm: 2.72122210
INFO:root:[   38] Training loss: 0.07705414, Validation loss: 0.15692055, Gradient norm: 2.60494863
INFO:root:[   39] Training loss: 0.07438003, Validation loss: 0.15245663, Gradient norm: 2.53915866
INFO:root:[   40] Training loss: 0.07470404, Validation loss: 0.17411813, Gradient norm: 2.46755209
INFO:root:[   41] Training loss: 0.07574075, Validation loss: 0.16460978, Gradient norm: 2.82571027
INFO:root:[   42] Training loss: 0.07370860, Validation loss: 0.16176131, Gradient norm: 2.74447704
INFO:root:[   43] Training loss: 0.07394920, Validation loss: 0.16360407, Gradient norm: 2.65424654
INFO:root:[   44] Training loss: 0.07270365, Validation loss: 0.16253907, Gradient norm: 2.60155516
INFO:root:[   45] Training loss: 0.07202874, Validation loss: 0.17119065, Gradient norm: 2.57419355
INFO:root:[   46] Training loss: 0.07166218, Validation loss: 0.18777714, Gradient norm: 2.47376723
INFO:root:[   47] Training loss: 0.07301881, Validation loss: 0.17401936, Gradient norm: 2.63188477
INFO:root:[   48] Training loss: 0.06927826, Validation loss: 0.17649622, Gradient norm: 2.29145791
INFO:root:[   49] Training loss: 0.06932013, Validation loss: 0.15335370, Gradient norm: 2.78097402
INFO:root:[   50] Training loss: 0.07029227, Validation loss: 0.17794154, Gradient norm: 2.76580778
INFO:root:[   51] Training loss: 0.07056307, Validation loss: 0.16928868, Gradient norm: 2.80232582
INFO:root:[   52] Training loss: 0.07291189, Validation loss: 0.16730376, Gradient norm: 2.97079924
INFO:root:[   53] Training loss: 0.06726636, Validation loss: 0.16355956, Gradient norm: 2.37347601
INFO:root:[   54] Training loss: 0.06597852, Validation loss: 0.17074278, Gradient norm: 2.28382895
INFO:root:[   55] Training loss: 0.06787702, Validation loss: 0.16651734, Gradient norm: 2.25227139
INFO:root:[   56] Training loss: 0.06856744, Validation loss: 0.17992277, Gradient norm: 2.46677479
INFO:root:[   57] Training loss: 0.06776911, Validation loss: 0.18689207, Gradient norm: 2.81205752
INFO:root:[   58] Training loss: 0.06762478, Validation loss: 0.19524316, Gradient norm: 2.86050288
INFO:root:[   59] Training loss: 0.06949147, Validation loss: 0.18337611, Gradient norm: 3.14990167
INFO:root:[   60] Training loss: 0.06688541, Validation loss: 0.16267863, Gradient norm: 2.54649581
INFO:root:[   61] Training loss: 0.06799520, Validation loss: 0.16352671, Gradient norm: 2.84863304
INFO:root:[   62] Training loss: 0.06633783, Validation loss: 0.15430021, Gradient norm: 2.61280059
INFO:root:[   63] Training loss: 0.06656552, Validation loss: 0.16734620, Gradient norm: 2.45305624
INFO:root:[   64] Training loss: 0.06462371, Validation loss: 0.16084530, Gradient norm: 2.61964219
INFO:root:[   65] Training loss: 0.06468230, Validation loss: 0.17614869, Gradient norm: 2.59917639
INFO:root:[   66] Training loss: 0.06421751, Validation loss: 0.18489012, Gradient norm: 2.49324813
INFO:root:[   67] Training loss: 0.06528826, Validation loss: 0.17917499, Gradient norm: 2.62277729
INFO:root:[   68] Training loss: 0.06363136, Validation loss: 0.18131936, Gradient norm: 2.39091517
INFO:root:[   69] Training loss: 0.06425114, Validation loss: 0.16999436, Gradient norm: 2.56875894
INFO:root:[   70] Training loss: 0.06430661, Validation loss: 0.16033036, Gradient norm: 2.26498211
INFO:root:[   71] Training loss: 0.06597245, Validation loss: 0.16679523, Gradient norm: 2.98188231
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 3368.754s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.23187
INFO:root:EnergyScoreTrain: 0.12173
INFO:root:CoverageTrain: 0.68266
INFO:root:IntervalWidthTrain: 0.09256
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.23682
INFO:root:EnergyScoreValidation: 0.12499
INFO:root:CoverageValidation: 0.64601
INFO:root:IntervalWidthValidation: 0.08476
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.22694
INFO:root:EnergyScoreTest: 0.11896
INFO:root:CoverageTest: 0.67099
INFO:root:IntervalWidthTest: 0.09012
INFO:root:###9 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456789, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 469762048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.32245702, Validation loss: 0.16748917, Gradient norm: 5.12277524
INFO:root:[    2] Training loss: 0.18180246, Validation loss: 0.20701478, Gradient norm: 3.66206173
INFO:root:[    3] Training loss: 0.16073185, Validation loss: 0.18638331, Gradient norm: 3.52561435
INFO:root:[    4] Training loss: 0.14919803, Validation loss: 0.13925155, Gradient norm: 3.86002037
INFO:root:[    5] Training loss: 0.13796682, Validation loss: 0.12959713, Gradient norm: 3.49382815
INFO:root:[    6] Training loss: 0.12953593, Validation loss: 0.14581386, Gradient norm: 3.42406157
INFO:root:[    7] Training loss: 0.12487182, Validation loss: 0.14940066, Gradient norm: 3.45126878
INFO:root:[    8] Training loss: 0.12354102, Validation loss: 0.16073015, Gradient norm: 3.37401791
INFO:root:[    9] Training loss: 0.11577211, Validation loss: 0.12164580, Gradient norm: 3.13428382
INFO:root:[   10] Training loss: 0.11197012, Validation loss: 0.12157562, Gradient norm: 3.32624605
INFO:root:[   11] Training loss: 0.10943316, Validation loss: 0.12953276, Gradient norm: 3.03119774
INFO:root:[   12] Training loss: 0.10485928, Validation loss: 0.14826936, Gradient norm: 2.79014657
INFO:root:[   13] Training loss: 0.10441615, Validation loss: 0.12070031, Gradient norm: 2.91246856
INFO:root:[   14] Training loss: 0.10261227, Validation loss: 0.18574426, Gradient norm: 3.20294376
INFO:root:[   15] Training loss: 0.10222067, Validation loss: 0.13443915, Gradient norm: 2.93254033
INFO:root:[   16] Training loss: 0.10094127, Validation loss: 0.15987912, Gradient norm: 2.79766562
INFO:root:[   17] Training loss: 0.09697486, Validation loss: 0.16325335, Gradient norm: 2.78341463
INFO:root:[   18] Training loss: 0.09478350, Validation loss: 0.13576372, Gradient norm: 2.91716449
INFO:root:[   19] Training loss: 0.09488118, Validation loss: 0.14286748, Gradient norm: 2.83805748
INFO:root:[   20] Training loss: 0.09418600, Validation loss: 0.14653114, Gradient norm: 2.85973650
INFO:root:[   21] Training loss: 0.09124617, Validation loss: 0.13595472, Gradient norm: 2.95225633
INFO:root:[   22] Training loss: 0.09169245, Validation loss: 0.14968013, Gradient norm: 2.94745215
INFO:root:[   23] Training loss: 0.09023473, Validation loss: 0.16099253, Gradient norm: 2.67582575
INFO:root:[   24] Training loss: 0.08757486, Validation loss: 0.15212419, Gradient norm: 2.50503148
INFO:root:[   25] Training loss: 0.08795425, Validation loss: 0.18563581, Gradient norm: 2.90382357
INFO:root:[   26] Training loss: 0.08822777, Validation loss: 0.14074036, Gradient norm: 2.85386661
INFO:root:[   27] Training loss: 0.08435215, Validation loss: 0.15405935, Gradient norm: 2.87105512
INFO:root:[   28] Training loss: 0.08618553, Validation loss: 0.15228736, Gradient norm: 3.00870149
INFO:root:[   29] Training loss: 0.08147504, Validation loss: 0.16516757, Gradient norm: 2.67395399
INFO:root:[   30] Training loss: 0.08209018, Validation loss: 0.17275069, Gradient norm: 2.58878332
INFO:root:[   31] Training loss: 0.08091653, Validation loss: 0.16427625, Gradient norm: 2.64091130
INFO:root:[   32] Training loss: 0.08237640, Validation loss: 0.18263937, Gradient norm: 2.87759885
INFO:root:[   33] Training loss: 0.08011274, Validation loss: 0.15430157, Gradient norm: 3.09708519
INFO:root:[   34] Training loss: 0.07897481, Validation loss: 0.18220089, Gradient norm: 2.68592798
INFO:root:[   35] Training loss: 0.07805209, Validation loss: 0.14793015, Gradient norm: 2.66177802
INFO:root:[   36] Training loss: 0.07772299, Validation loss: 0.14517177, Gradient norm: 3.09616998
INFO:root:[   37] Training loss: 0.07662559, Validation loss: 0.16908296, Gradient norm: 2.45608969
INFO:root:[   38] Training loss: 0.07768475, Validation loss: 0.16484808, Gradient norm: 2.68740378
INFO:root:[   39] Training loss: 0.07520520, Validation loss: 0.14602542, Gradient norm: 2.50026627
INFO:root:[   40] Training loss: 0.07391522, Validation loss: 0.16059697, Gradient norm: 2.29704361
INFO:root:[   41] Training loss: 0.07360342, Validation loss: 0.16531602, Gradient norm: 2.69377019
INFO:root:[   42] Training loss: 0.07369257, Validation loss: 0.15632481, Gradient norm: 2.68902849
INFO:root:[   43] Training loss: 0.07238077, Validation loss: 0.15461681, Gradient norm: 2.70614272
INFO:root:[   44] Training loss: 0.07332479, Validation loss: 0.16803640, Gradient norm: 2.90226036
INFO:root:[   45] Training loss: 0.07358847, Validation loss: 0.15943988, Gradient norm: 3.01699071
INFO:root:[   46] Training loss: 0.07140745, Validation loss: 0.15808615, Gradient norm: 2.82063831
INFO:root:[   47] Training loss: 0.06969438, Validation loss: 0.16833598, Gradient norm: 2.74150141
INFO:root:[   48] Training loss: 0.07096412, Validation loss: 0.15500451, Gradient norm: 2.60484173
INFO:root:[   49] Training loss: 0.06987833, Validation loss: 0.16577457, Gradient norm: 2.46138703
INFO:root:[   50] Training loss: 0.07061014, Validation loss: 0.16088130, Gradient norm: 2.75525108
INFO:root:[   51] Training loss: 0.07067768, Validation loss: 0.16032452, Gradient norm: 2.77506756
INFO:root:[   52] Training loss: 0.07138092, Validation loss: 0.17111090, Gradient norm: 2.63110968
INFO:root:[   53] Training loss: 0.06754595, Validation loss: 0.14790473, Gradient norm: 2.43938874
INFO:root:[   54] Training loss: 0.06853167, Validation loss: 0.16958359, Gradient norm: 2.53157270
INFO:root:[   55] Training loss: 0.06802036, Validation loss: 0.16285014, Gradient norm: 2.74856854
INFO:root:[   56] Training loss: 0.06774141, Validation loss: 0.15130267, Gradient norm: 2.48672895
INFO:root:[   57] Training loss: 0.06840832, Validation loss: 0.15482076, Gradient norm: 2.76037814
INFO:root:[   58] Training loss: 0.06709556, Validation loss: 0.16696534, Gradient norm: 2.48360378
INFO:root:[   59] Training loss: 0.06530236, Validation loss: 0.17375282, Gradient norm: 2.53214575
INFO:root:[   60] Training loss: 0.06542947, Validation loss: 0.16760752, Gradient norm: 2.41855279
INFO:root:[   61] Training loss: 0.06608814, Validation loss: 0.16263330, Gradient norm: 2.50763776
INFO:root:[   62] Training loss: 0.06595628, Validation loss: 0.16055250, Gradient norm: 2.65150989
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 2954.975s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.23264
INFO:root:EnergyScoreTrain: 0.12123
INFO:root:CoverageTrain: 0.6817
INFO:root:IntervalWidthTrain: 0.09495
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.22946
INFO:root:EnergyScoreValidation: 0.12248
INFO:root:CoverageValidation: 0.63993
INFO:root:IntervalWidthValidation: 0.08453
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.22774
INFO:root:EnergyScoreTest: 0.11969
INFO:root:CoverageTest: 0.67394
INFO:root:IntervalWidthTest: 0.09162
