INFO:root:Starting the logger.
INFO:root:Using cpu.
INFO:root:############### Starting experiment with config file debug.ini ###############
INFO:root:###1 out of 2 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 10000, 'downscaling_factor': 2, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 10, 'pred_horizon': 5}
INFO:root:###1 out of 3 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 100, 'early_stopping': 100, 'dropout': 0.0, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'alpha_label_smoothing': 0.0, 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'fourier_dropout': 0.0, 'hidden_channels': 32, 'projection_channels': 64, 'n_samples': 3, 'lifting_channels': 128, 'n_modes': (16, 16), 'uno_out_channels': [32, 64, 64, 32], 'uno_scalings': [[1.0, 1.0], [0.5, 0.5], [1, 1], [2, 2]], 'uno_n_modes': [[16, 16], [8, 8], [8, 8], [16, 16]]}
INFO:root:Number parameters: 1192101
INFO:root:Memory allocated: 0
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 5.13409756, Validation loss: 29.49923420, Gradient norm: 2.90629071
INFO:root:[    2] Training loss: 4.32178643, Validation loss: 17.63393140, Gradient norm: 5.24413721
INFO:root:[    3] Training loss: 1.22291697, Validation loss: 2.54587102, Gradient norm: 12.31628938
INFO:root:[    4] Training loss: 0.24367053, Validation loss: 1.04523455, Gradient norm: 5.01606907
INFO:root:[    5] Training loss: 0.15943234, Validation loss: 0.90624471, Gradient norm: 2.17496158
INFO:root:[    6] Training loss: 0.14486879, Validation loss: 0.84286582, Gradient norm: 2.04987283
INFO:root:[    7] Training loss: 0.13609627, Validation loss: 0.79144286, Gradient norm: 1.86636070
INFO:root:[    8] Training loss: 0.12969599, Validation loss: 0.75728966, Gradient norm: 2.44732087
INFO:root:[    9] Training loss: 0.12339767, Validation loss: 0.72921865, Gradient norm: 2.28282844
INFO:root:[   10] Training loss: 0.12006325, Validation loss: 0.71831605, Gradient norm: 4.32169443
INFO:root:[   11] Training loss: 0.11660601, Validation loss: 0.69166449, Gradient norm: 5.32753297
INFO:root:[   12] Training loss: 0.11322992, Validation loss: 0.66201955, Gradient norm: 5.59259487
INFO:root:[   13] Training loss: 0.10856127, Validation loss: 0.63680184, Gradient norm: 4.90428670
INFO:root:[   14] Training loss: 0.10626810, Validation loss: 0.65090767, Gradient norm: 5.61374722
INFO:root:[   15] Training loss: 0.10544316, Validation loss: 0.62299813, Gradient norm: 6.50322243
INFO:root:[   16] Training loss: 0.10330739, Validation loss: 0.62413378, Gradient norm: 5.73830155
INFO:root:[   17] Training loss: 0.10185947, Validation loss: 0.60020795, Gradient norm: 5.46664479
INFO:root:[   18] Training loss: 0.10138879, Validation loss: 0.61278578, Gradient norm: 5.94527818
INFO:root:[   19] Training loss: 0.10030340, Validation loss: 0.59651369, Gradient norm: 5.93000387
INFO:root:[   20] Training loss: 0.09866753, Validation loss: 0.60247178, Gradient norm: 5.40247292
INFO:root:[   21] Training loss: 0.09804120, Validation loss: 0.57859075, Gradient norm: 5.94603648
INFO:root:[   22] Training loss: 0.09671496, Validation loss: 0.59150961, Gradient norm: 5.81218333
INFO:root:[   23] Training loss: 0.09564678, Validation loss: 0.56151514, Gradient norm: 6.21465929
INFO:root:[   24] Training loss: 0.09368850, Validation loss: 0.57268043, Gradient norm: 5.82090854
INFO:root:[   25] Training loss: 0.09231746, Validation loss: 0.53827368, Gradient norm: 6.05811491
INFO:root:[   26] Training loss: 0.09026659, Validation loss: 0.55509890, Gradient norm: 5.94373652
INFO:root:[   27] Training loss: 0.08901780, Validation loss: 0.51191759, Gradient norm: 6.28680392
INFO:root:[   28] Training loss: 0.08758502, Validation loss: 0.53751571, Gradient norm: 6.87312311
INFO:root:[   29] Training loss: 0.08655781, Validation loss: 0.50185623, Gradient norm: 7.76068359
INFO:root:[   30] Training loss: 0.08307452, Validation loss: 0.51129822, Gradient norm: 6.67266661
INFO:root:[   31] Training loss: 0.08217241, Validation loss: 0.47941200, Gradient norm: 7.37117478
INFO:root:[   32] Training loss: 0.08100311, Validation loss: 0.50006940, Gradient norm: 8.38592395
INFO:root:[   33] Training loss: 0.07876301, Validation loss: 0.44968718, Gradient norm: 7.97117522
INFO:root:[   34] Training loss: 0.07661964, Validation loss: 0.48976249, Gradient norm: 7.71984187
INFO:root:[   35] Training loss: 0.07583273, Validation loss: 0.42639174, Gradient norm: 8.54810905
INFO:root:[   36] Training loss: 0.07401410, Validation loss: 0.45775744, Gradient norm: 8.45375642
INFO:root:[   37] Training loss: 0.07234790, Validation loss: 0.41419657, Gradient norm: 8.34452028
INFO:root:[   38] Training loss: 0.07024160, Validation loss: 0.44771618, Gradient norm: 7.84214542
INFO:root:[   39] Training loss: 0.07045142, Validation loss: 0.39724108, Gradient norm: 9.09366254
INFO:root:[   40] Training loss: 0.06861481, Validation loss: 0.42384119, Gradient norm: 8.83635599
INFO:root:[   41] Training loss: 0.06746992, Validation loss: 0.38813343, Gradient norm: 8.67621720
INFO:root:[   42] Training loss: 0.06634564, Validation loss: 0.41400507, Gradient norm: 8.75466582
INFO:root:[   43] Training loss: 0.06509944, Validation loss: 0.36561007, Gradient norm: 8.50714095
INFO:root:[   44] Training loss: 0.06461579, Validation loss: 0.40515596, Gradient norm: 8.60957953
INFO:root:[   45] Training loss: 0.06412160, Validation loss: 0.36680050, Gradient norm: 9.05025909
INFO:root:[   46] Training loss: 0.06383570, Validation loss: 0.39584242, Gradient norm: 9.33161096
INFO:root:[   47] Training loss: 0.06270333, Validation loss: 0.35918993, Gradient norm: 8.99658391
INFO:root:[   48] Training loss: 0.06188959, Validation loss: 0.37666755, Gradient norm: 8.78718360
INFO:root:[   49] Training loss: 0.06088681, Validation loss: 0.34769941, Gradient norm: 8.28065111
INFO:root:[   50] Training loss: 0.06084444, Validation loss: 0.38076029, Gradient norm: 8.67533666
INFO:root:[   51] Training loss: 0.06091112, Validation loss: 0.35186410, Gradient norm: 9.19832331
INFO:root:[   52] Training loss: 0.06019953, Validation loss: 0.37714526, Gradient norm: 9.17963494
INFO:root:[   53] Training loss: 0.05964235, Validation loss: 0.33863863, Gradient norm: 8.96272156
INFO:root:[   54] Training loss: 0.05877414, Validation loss: 0.36897240, Gradient norm: 8.39579458
INFO:root:[   55] Training loss: 0.05879952, Validation loss: 0.33451076, Gradient norm: 8.59315996
INFO:root:[   56] Training loss: 0.05818292, Validation loss: 0.37076472, Gradient norm: 8.57191521
INFO:root:[   57] Training loss: 0.05874479, Validation loss: 0.33074278, Gradient norm: 9.46826707
INFO:root:[   58] Training loss: 0.05764935, Validation loss: 0.35701821, Gradient norm: 8.70749047
INFO:root:[   59] Training loss: 0.05666025, Validation loss: 0.31918052, Gradient norm: 7.98470680
INFO:root:[   60] Training loss: 0.05683450, Validation loss: 0.36403446, Gradient norm: 8.39407280
INFO:root:[   61] Training loss: 0.05747056, Validation loss: 0.32394383, Gradient norm: 9.45448992
INFO:root:[   62] Training loss: 0.05631372, Validation loss: 0.35535532, Gradient norm: 8.70281882
INFO:root:[   63] Training loss: 0.05625010, Validation loss: 0.32077600, Gradient norm: 8.93202472
INFO:root:[   64] Training loss: 0.05547383, Validation loss: 0.35321148, Gradient norm: 8.51417132
INFO:root:[   65] Training loss: 0.05550988, Validation loss: 0.31150701, Gradient norm: 8.82529166
INFO:root:[   66] Training loss: 0.05508580, Validation loss: 0.35003724, Gradient norm: 8.67132040
INFO:root:[   67] Training loss: 0.05526195, Validation loss: 0.31305299, Gradient norm: 9.24899218
INFO:root:[   68] Training loss: 0.05512446, Validation loss: 0.34701833, Gradient norm: 9.53809692
INFO:root:[   69] Training loss: 0.05447899, Validation loss: 0.30818876, Gradient norm: 9.24036459
INFO:root:[   70] Training loss: 0.05417727, Validation loss: 0.34298565, Gradient norm: 9.30949584
INFO:root:[   71] Training loss: 0.05403556, Validation loss: 0.30351945, Gradient norm: 9.61780748
INFO:root:[   72] Training loss: 0.05363470, Validation loss: 0.34273769, Gradient norm: 9.55039159
INFO:root:[   73] Training loss: 0.05315614, Validation loss: 0.29784413, Gradient norm: 9.35084276
INFO:root:[   74] Training loss: 0.05273473, Validation loss: 0.34288608, Gradient norm: 9.42676002
INFO:root:[   75] Training loss: 0.05288517, Validation loss: 0.29376358, Gradient norm: 9.92015913
INFO:root:[   76] Training loss: 0.05194962, Validation loss: 0.33187594, Gradient norm: 9.40671167
INFO:root:[   77] Training loss: 0.05200434, Validation loss: 0.28862968, Gradient norm: 9.90533702
INFO:root:[   78] Training loss: 0.05135672, Validation loss: 0.32647483, Gradient norm: 9.57670136
INFO:root:[   79] Training loss: 0.05147118, Validation loss: 0.29542238, Gradient norm: 10.07151062
INFO:root:[   80] Training loss: 0.05055842, Validation loss: 0.32318411, Gradient norm: 9.73962885
INFO:root:[   81] Training loss: 0.05063506, Validation loss: 0.28352296, Gradient norm: 10.14555913
INFO:root:[   82] Training loss: 0.05006105, Validation loss: 0.32303730, Gradient norm: 10.03964825
INFO:root:[   83] Training loss: 0.05008653, Validation loss: 0.28160351, Gradient norm: 10.45034815
INFO:root:[   84] Training loss: 0.04911773, Validation loss: 0.31930513, Gradient norm: 10.01612579
INFO:root:[   85] Training loss: 0.04947010, Validation loss: 0.27697541, Gradient norm: 10.75025355
INFO:root:[   86] Training loss: 0.04858083, Validation loss: 0.30333359, Gradient norm: 10.30305747
INFO:root:[   87] Training loss: 0.04799278, Validation loss: 0.27464912, Gradient norm: 9.95271062
INFO:root:[   88] Training loss: 0.04750697, Validation loss: 0.30576921, Gradient norm: 10.04810702
INFO:root:[   89] Training loss: 0.04810803, Validation loss: 0.26971637, Gradient norm: 10.93357552
INFO:root:[   90] Training loss: 0.04751736, Validation loss: 0.30249143, Gradient norm: 10.92732592
INFO:root:[   91] Training loss: 0.04733793, Validation loss: 0.26697666, Gradient norm: 11.11454371
INFO:root:[   92] Training loss: 0.04675356, Validation loss: 0.29121497, Gradient norm: 10.95020830
INFO:root:[   93] Training loss: 0.04618847, Validation loss: 0.25572692, Gradient norm: 10.76068567
INFO:root:[   94] Training loss: 0.04587219, Validation loss: 0.29894029, Gradient norm: 10.95891478
INFO:root:[   95] Training loss: 0.04611601, Validation loss: 0.25647794, Gradient norm: 11.45393339
INFO:root:[   96] Training loss: 0.04543418, Validation loss: 0.28459132, Gradient norm: 11.22606776
INFO:root:[   97] Training loss: 0.04537135, Validation loss: 0.25873604, Gradient norm: 11.46782161
INFO:root:[   98] Training loss: 0.04431223, Validation loss: 0.28785810, Gradient norm: 10.97220088
INFO:root:[   99] Training loss: 0.04457488, Validation loss: 0.24562195, Gradient norm: 11.48582167
INFO:root:[  100] Training loss: 0.04384766, Validation loss: 0.28251269, Gradient norm: 11.16355201
INFO:root:Training the model took 1051.881s.
INFO:root:Emptying the cuda cache took 0.0s.
