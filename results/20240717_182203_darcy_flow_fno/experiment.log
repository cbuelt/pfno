INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05557221, Validation loss: 1.84681235, Gradient norm: 0.66333786
INFO:root:[    2] Training loss: 0.02614351, Validation loss: 1.29044240, Gradient norm: 0.82754323
INFO:root:[    3] Training loss: 0.02000679, Validation loss: 0.99048319, Gradient norm: 0.79017440
INFO:root:[    4] Training loss: 0.01750037, Validation loss: 0.93251082, Gradient norm: 0.68148831
INFO:root:[    5] Training loss: 0.01664063, Validation loss: 0.86324354, Gradient norm: 0.68032667
INFO:root:[    6] Training loss: 0.01538628, Validation loss: 1.04325121, Gradient norm: 0.62207449
INFO:root:[    7] Training loss: 0.01469396, Validation loss: 0.91714943, Gradient norm: 0.62988418
INFO:root:[    8] Training loss: 0.01369478, Validation loss: 0.76181525, Gradient norm: 0.56013712
INFO:root:[    9] Training loss: 0.01322453, Validation loss: 0.67911480, Gradient norm: 0.58490604
INFO:root:[   10] Training loss: 0.01210556, Validation loss: 0.64291437, Gradient norm: 0.47785065
INFO:root:[   11] Training loss: 0.01195275, Validation loss: 0.70253916, Gradient norm: 0.51313210
INFO:root:[   12] Training loss: 0.01139777, Validation loss: 0.60884657, Gradient norm: 0.50180248
INFO:root:[   13] Training loss: 0.01107541, Validation loss: 0.56928996, Gradient norm: 0.49183088
INFO:root:[   14] Training loss: 0.01049733, Validation loss: 0.57488591, Gradient norm: 0.37813238
INFO:root:[   15] Training loss: 0.01015295, Validation loss: 0.58508171, Gradient norm: 0.38683116
INFO:root:[   16] Training loss: 0.01119264, Validation loss: 0.58287621, Gradient norm: 0.61173237
INFO:root:[   17] Training loss: 0.00976742, Validation loss: 0.59189846, Gradient norm: 0.35565788
INFO:root:[   18] Training loss: 0.00993383, Validation loss: 0.55666190, Gradient norm: 0.46177103
INFO:root:[   19] Training loss: 0.00963344, Validation loss: 0.62080949, Gradient norm: 0.39358993
INFO:root:[   20] Training loss: 0.00970195, Validation loss: 0.56418291, Gradient norm: 0.45742895
INFO:root:[   21] Training loss: 0.00958440, Validation loss: 0.55410664, Gradient norm: 0.43635832
INFO:root:[   22] Training loss: 0.00907216, Validation loss: 0.57146861, Gradient norm: 0.34533586
INFO:root:[   23] Training loss: 0.00993773, Validation loss: 0.50773489, Gradient norm: 0.53568610
INFO:root:[   24] Training loss: 0.00904903, Validation loss: 0.49565920, Gradient norm: 0.38981592
INFO:root:[   25] Training loss: 0.00908899, Validation loss: 0.54970598, Gradient norm: 0.39192202
INFO:root:[   26] Training loss: 0.00886228, Validation loss: 0.63558348, Gradient norm: 0.40664587
INFO:root:[   27] Training loss: 0.00897410, Validation loss: 0.52248817, Gradient norm: 0.39917243
INFO:root:[   28] Training loss: 0.00913439, Validation loss: 0.49663910, Gradient norm: 0.45293242
INFO:root:[   29] Training loss: 0.00852069, Validation loss: 0.52179279, Gradient norm: 0.33801359
INFO:root:[   30] Training loss: 0.00878955, Validation loss: 0.49221869, Gradient norm: 0.40352357
INFO:root:[   31] Training loss: 0.00855085, Validation loss: 0.52038185, Gradient norm: 0.38140479
INFO:root:[   32] Training loss: 0.00855462, Validation loss: 0.51092130, Gradient norm: 0.40276935
INFO:root:[   33] Training loss: 0.00877673, Validation loss: 0.49358699, Gradient norm: 0.40409236
INFO:root:[   34] Training loss: 0.00828365, Validation loss: 0.50489650, Gradient norm: 0.34908583
INFO:root:[   35] Training loss: 0.00828267, Validation loss: 0.49687284, Gradient norm: 0.35500007
INFO:root:[   36] Training loss: 0.00832391, Validation loss: 0.52424582, Gradient norm: 0.40648674
INFO:root:[   37] Training loss: 0.00838694, Validation loss: 0.54141495, Gradient norm: 0.41469714
INFO:root:[   38] Training loss: 0.00811219, Validation loss: 0.57871518, Gradient norm: 0.36716111
INFO:root:[   39] Training loss: 0.00802060, Validation loss: 0.47081728, Gradient norm: 0.35491473
INFO:root:[   40] Training loss: 0.00781580, Validation loss: 0.48757576, Gradient norm: 0.31509380
INFO:root:[   41] Training loss: 0.00795450, Validation loss: 0.49841656, Gradient norm: 0.39406289
INFO:root:[   42] Training loss: 0.00777634, Validation loss: 0.52943494, Gradient norm: 0.33734629
INFO:root:[   43] Training loss: 0.00789606, Validation loss: 0.47552492, Gradient norm: 0.38487284
INFO:root:[   44] Training loss: 0.00753754, Validation loss: 0.47488832, Gradient norm: 0.29562313
INFO:root:[   45] Training loss: 0.00753240, Validation loss: 0.46236402, Gradient norm: 0.33393287
INFO:root:[   46] Training loss: 0.00771015, Validation loss: 0.48470824, Gradient norm: 0.35391958
INFO:root:[   47] Training loss: 0.00772623, Validation loss: 0.50981263, Gradient norm: 0.36677958
INFO:root:[   48] Training loss: 0.00741947, Validation loss: 0.45716551, Gradient norm: 0.31146840
INFO:root:[   49] Training loss: 0.00833410, Validation loss: 0.45840180, Gradient norm: 0.48340398
INFO:root:[   50] Training loss: 0.00744612, Validation loss: 0.48374131, Gradient norm: 0.32156745
INFO:root:[   51] Training loss: 0.00713413, Validation loss: 0.48347869, Gradient norm: 0.27950729
INFO:root:[   52] Training loss: 0.00728311, Validation loss: 0.46169650, Gradient norm: 0.33301282
INFO:root:[   53] Training loss: 0.00723122, Validation loss: 0.47040397, Gradient norm: 0.33308243
INFO:root:[   54] Training loss: 0.00722607, Validation loss: 0.48049062, Gradient norm: 0.33284045
INFO:root:[   55] Training loss: 0.00700182, Validation loss: 0.49214189, Gradient norm: 0.31094489
INFO:root:[   56] Training loss: 0.00700550, Validation loss: 0.45436156, Gradient norm: 0.28976533
INFO:root:[   57] Training loss: 0.00716690, Validation loss: 0.47283027, Gradient norm: 0.34702366
INFO:root:[   58] Training loss: 0.00702111, Validation loss: 0.48665018, Gradient norm: 0.29416464
INFO:root:[   59] Training loss: 0.00703600, Validation loss: 0.47293627, Gradient norm: 0.32553565
INFO:root:[   60] Training loss: 0.00719198, Validation loss: 0.48358106, Gradient norm: 0.35383330
INFO:root:[   61] Training loss: 0.00694671, Validation loss: 0.53550202, Gradient norm: 0.34239931
INFO:root:[   62] Training loss: 0.00706718, Validation loss: 0.51527791, Gradient norm: 0.34798326
INFO:root:[   63] Training loss: 0.00676183, Validation loss: 0.47180658, Gradient norm: 0.30990917
INFO:root:[   64] Training loss: 0.00684118, Validation loss: 0.48160706, Gradient norm: 0.31552157
INFO:root:[   65] Training loss: 0.00710649, Validation loss: 0.47797531, Gradient norm: 0.37269847
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 874.984s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.06041
INFO:root:EnergyScoretrain: 0.04461
INFO:root:Coveragetrain: 6.8217
INFO:root:IntervalWidthtrain: 131.69133
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.0196
INFO:root:EnergyScorevalidation: 0.01454
INFO:root:Coveragevalidation: 1.78476
INFO:root:IntervalWidthvalidation: 34.48437
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.06348
INFO:root:EnergyScoretest: 0.05619
INFO:root:Coveragetest: 0.13903
INFO:root:IntervalWidthtest: 39.11214
INFO:root:###2 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05438982, Validation loss: 1.61371051, Gradient norm: 0.38615959
INFO:root:[    2] Training loss: 0.02456999, Validation loss: 1.20051812, Gradient norm: 0.32844348
INFO:root:[    3] Training loss: 0.01902821, Validation loss: 1.05805472, Gradient norm: 0.26913005
INFO:root:[    4] Training loss: 0.01718769, Validation loss: 0.96009524, Gradient norm: 0.31774107
INFO:root:[    5] Training loss: 0.01593374, Validation loss: 1.00237019, Gradient norm: 0.38630386
INFO:root:[    6] Training loss: 0.01470516, Validation loss: 0.81311356, Gradient norm: 0.32998596
INFO:root:[    7] Training loss: 0.01354204, Validation loss: 0.72413080, Gradient norm: 0.30851741
INFO:root:[    8] Training loss: 0.01265496, Validation loss: 0.71073873, Gradient norm: 0.28655338
INFO:root:[    9] Training loss: 0.01242944, Validation loss: 0.69388755, Gradient norm: 0.35752758
INFO:root:[   10] Training loss: 0.01215029, Validation loss: 0.69566809, Gradient norm: 0.33244200
INFO:root:[   11] Training loss: 0.01134928, Validation loss: 0.63247106, Gradient norm: 0.27560256
INFO:root:[   12] Training loss: 0.01135059, Validation loss: 0.63207657, Gradient norm: 0.33690375
INFO:root:[   13] Training loss: 0.01075983, Validation loss: 0.64568626, Gradient norm: 0.28490519
INFO:root:[   14] Training loss: 0.01048177, Validation loss: 0.58651018, Gradient norm: 0.27875973
INFO:root:[   15] Training loss: 0.01036880, Validation loss: 0.64392820, Gradient norm: 0.31245389
INFO:root:[   16] Training loss: 0.01005784, Validation loss: 0.55597496, Gradient norm: 0.27011726
INFO:root:[   17] Training loss: 0.01023099, Validation loss: 0.59900870, Gradient norm: 0.32934263
INFO:root:[   18] Training loss: 0.00963629, Validation loss: 0.55049789, Gradient norm: 0.26423888
INFO:root:[   19] Training loss: 0.00971606, Validation loss: 0.52723696, Gradient norm: 0.29466955
INFO:root:[   20] Training loss: 0.00932884, Validation loss: 0.53649092, Gradient norm: 0.22676917
INFO:root:[   21] Training loss: 0.00943506, Validation loss: 0.53719729, Gradient norm: 0.27982421
INFO:root:[   22] Training loss: 0.00915735, Validation loss: 0.58520127, Gradient norm: 0.28080591
INFO:root:[   23] Training loss: 0.00945562, Validation loss: 0.52576216, Gradient norm: 0.32334964
INFO:root:[   24] Training loss: 0.00883738, Validation loss: 0.52159417, Gradient norm: 0.24575341
INFO:root:[   25] Training loss: 0.00884058, Validation loss: 0.52245652, Gradient norm: 0.28985535
INFO:root:[   26] Training loss: 0.00899650, Validation loss: 0.50512327, Gradient norm: 0.30058609
INFO:root:[   27] Training loss: 0.00872031, Validation loss: 0.50749567, Gradient norm: 0.29468152
INFO:root:[   28] Training loss: 0.00842731, Validation loss: 0.49907969, Gradient norm: 0.23826431
INFO:root:[   29] Training loss: 0.00864401, Validation loss: 0.55544242, Gradient norm: 0.28570924
INFO:root:[   30] Training loss: 0.00809069, Validation loss: 0.49817281, Gradient norm: 0.19928088
INFO:root:[   31] Training loss: 0.00825141, Validation loss: 0.55485998, Gradient norm: 0.25360307
INFO:root:[   32] Training loss: 0.00860895, Validation loss: 0.50990028, Gradient norm: 0.29670474
INFO:root:[   33] Training loss: 0.00814947, Validation loss: 0.47188317, Gradient norm: 0.26191726
INFO:root:[   34] Training loss: 0.00841587, Validation loss: 0.54250905, Gradient norm: 0.29546074
INFO:root:[   35] Training loss: 0.00819824, Validation loss: 0.56633512, Gradient norm: 0.27740731
INFO:root:[   36] Training loss: 0.00828654, Validation loss: 0.46761773, Gradient norm: 0.29193216
INFO:root:[   37] Training loss: 0.00788638, Validation loss: 0.48637399, Gradient norm: 0.23973426
INFO:root:[   38] Training loss: 0.00805996, Validation loss: 0.50366185, Gradient norm: 0.27285154
INFO:root:[   39] Training loss: 0.00769979, Validation loss: 0.50152824, Gradient norm: 0.22798559
INFO:root:[   40] Training loss: 0.00780757, Validation loss: 0.47824792, Gradient norm: 0.22885707
INFO:root:[   41] Training loss: 0.00776686, Validation loss: 0.46460573, Gradient norm: 0.26240440
INFO:root:[   42] Training loss: 0.00771617, Validation loss: 0.47516730, Gradient norm: 0.26029729
INFO:root:[   43] Training loss: 0.00775981, Validation loss: 0.47225200, Gradient norm: 0.27067847
INFO:root:[   44] Training loss: 0.00739154, Validation loss: 0.46234344, Gradient norm: 0.20363560
INFO:root:[   45] Training loss: 0.00767883, Validation loss: 0.47133571, Gradient norm: 0.28535413
INFO:root:[   46] Training loss: 0.00752106, Validation loss: 0.44766714, Gradient norm: 0.25723292
INFO:root:[   47] Training loss: 0.00737496, Validation loss: 0.48350852, Gradient norm: 0.22257120
INFO:root:[   48] Training loss: 0.00744417, Validation loss: 0.46260222, Gradient norm: 0.25629322
INFO:root:[   49] Training loss: 0.00743661, Validation loss: 0.48740651, Gradient norm: 0.25350980
INFO:root:[   50] Training loss: 0.00749773, Validation loss: 0.44938760, Gradient norm: 0.27966208
INFO:root:[   51] Training loss: 0.00715381, Validation loss: 0.47738193, Gradient norm: 0.20635922
INFO:root:[   52] Training loss: 0.00716869, Validation loss: 0.46712665, Gradient norm: 0.23776460
INFO:root:[   53] Training loss: 0.00713142, Validation loss: 0.45744248, Gradient norm: 0.23434798
INFO:root:[   54] Training loss: 0.00713724, Validation loss: 0.49697731, Gradient norm: 0.24265237
INFO:root:[   55] Training loss: 0.00717327, Validation loss: 0.47991366, Gradient norm: 0.24434081
INFO:root:[   56] Training loss: 0.00718525, Validation loss: 0.45974867, Gradient norm: 0.26677792
INFO:root:[   57] Training loss: 0.00676225, Validation loss: 0.44592565, Gradient norm: 0.19188459
INFO:root:[   58] Training loss: 0.00688097, Validation loss: 0.44802850, Gradient norm: 0.25368032
INFO:root:[   59] Training loss: 0.00678248, Validation loss: 0.47958358, Gradient norm: 0.21547442
INFO:root:[   60] Training loss: 0.00685031, Validation loss: 0.44892570, Gradient norm: 0.26048946
INFO:root:[   61] Training loss: 0.00671768, Validation loss: 0.48647859, Gradient norm: 0.23951295
INFO:root:[   62] Training loss: 0.00684968, Validation loss: 0.49210699, Gradient norm: 0.25453182
INFO:root:[   63] Training loss: 0.00673899, Validation loss: 0.51320294, Gradient norm: 0.23554254
INFO:root:[   64] Training loss: 0.00673385, Validation loss: 0.49131183, Gradient norm: 0.24125500
INFO:root:[   65] Training loss: 0.00680739, Validation loss: 0.45130923, Gradient norm: 0.27409787
INFO:root:[   66] Training loss: 0.00652998, Validation loss: 0.48136051, Gradient norm: 0.20349490
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 898.474s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.0615
INFO:root:EnergyScoretrain: 0.04657
INFO:root:Coveragetrain: 6.96107
INFO:root:IntervalWidthtrain: 163.66515
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02078
INFO:root:EnergyScorevalidation: 0.01535
INFO:root:Coveragevalidation: 1.82653
INFO:root:IntervalWidthvalidation: 42.84143
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.0232
INFO:root:EnergyScoretest: 0.01696
INFO:root:Coveragetest: 0.81876
INFO:root:IntervalWidthtest: 49.28273
INFO:root:###3 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04860209, Validation loss: 1.53615080, Gradient norm: 0.37823385
INFO:root:[    2] Training loss: 0.02433883, Validation loss: 1.22611315, Gradient norm: 0.30664325
INFO:root:[    3] Training loss: 0.01937979, Validation loss: 0.99688409, Gradient norm: 0.29019133
INFO:root:[    4] Training loss: 0.01647173, Validation loss: 0.87398897, Gradient norm: 0.27371302
INFO:root:[    5] Training loss: 0.01497496, Validation loss: 0.82516266, Gradient norm: 0.27171608
INFO:root:[    6] Training loss: 0.01407752, Validation loss: 0.78226747, Gradient norm: 0.28605419
INFO:root:[    7] Training loss: 0.01318067, Validation loss: 0.71052257, Gradient norm: 0.26879920
INFO:root:[    8] Training loss: 0.01241750, Validation loss: 0.76924383, Gradient norm: 0.26163648
INFO:root:[    9] Training loss: 0.01246722, Validation loss: 0.72542549, Gradient norm: 0.33025373
INFO:root:[   10] Training loss: 0.01168532, Validation loss: 0.64172279, Gradient norm: 0.26050575
INFO:root:[   11] Training loss: 0.01164711, Validation loss: 0.62677378, Gradient norm: 0.28912751
INFO:root:[   12] Training loss: 0.01120540, Validation loss: 0.61387998, Gradient norm: 0.28781661
INFO:root:[   13] Training loss: 0.01078619, Validation loss: 0.61761503, Gradient norm: 0.24891673
INFO:root:[   14] Training loss: 0.01065286, Validation loss: 0.61181291, Gradient norm: 0.25503900
INFO:root:[   15] Training loss: 0.01044431, Validation loss: 0.59944356, Gradient norm: 0.24228671
INFO:root:[   16] Training loss: 0.01046893, Validation loss: 0.65868083, Gradient norm: 0.29339688
INFO:root:[   17] Training loss: 0.01024550, Validation loss: 0.57400388, Gradient norm: 0.25732651
INFO:root:[   18] Training loss: 0.01000129, Validation loss: 0.58076811, Gradient norm: 0.24695831
INFO:root:[   19] Training loss: 0.01011647, Validation loss: 0.54957886, Gradient norm: 0.30115092
INFO:root:[   20] Training loss: 0.00968324, Validation loss: 0.66468279, Gradient norm: 0.23812232
INFO:root:[   21] Training loss: 0.00968839, Validation loss: 0.80417028, Gradient norm: 0.25579169
INFO:root:[   22] Training loss: 0.00982009, Validation loss: 0.55222461, Gradient norm: 0.29873815
INFO:root:[   23] Training loss: 0.00952537, Validation loss: 0.53247921, Gradient norm: 0.27054618
INFO:root:[   24] Training loss: 0.00946520, Validation loss: 0.53406732, Gradient norm: 0.27226180
INFO:root:[   25] Training loss: 0.00914310, Validation loss: 0.51861060, Gradient norm: 0.22315664
INFO:root:[   26] Training loss: 0.00918218, Validation loss: 0.77216312, Gradient norm: 0.22076857
INFO:root:[   27] Training loss: 0.00909185, Validation loss: 0.52543607, Gradient norm: 0.25238487
INFO:root:[   28] Training loss: 0.00887136, Validation loss: 0.57529113, Gradient norm: 0.21693409
INFO:root:[   29] Training loss: 0.00912250, Validation loss: 0.51466754, Gradient norm: 0.27990248
INFO:root:[   30] Training loss: 0.00873798, Validation loss: 0.50538240, Gradient norm: 0.22029306
INFO:root:[   31] Training loss: 0.00890658, Validation loss: 0.50083943, Gradient norm: 0.26562404
INFO:root:[   32] Training loss: 0.00868666, Validation loss: 0.52002928, Gradient norm: 0.23624855
INFO:root:[   33] Training loss: 0.00851029, Validation loss: 0.51133767, Gradient norm: 0.20409187
INFO:root:[   34] Training loss: 0.00867290, Validation loss: 0.50369685, Gradient norm: 0.26783136
INFO:root:[   35] Training loss: 0.00883609, Validation loss: 0.49198916, Gradient norm: 0.28854406
INFO:root:[   36] Training loss: 0.00823026, Validation loss: 0.48835168, Gradient norm: 0.19410913
INFO:root:[   37] Training loss: 0.00821990, Validation loss: 0.48653166, Gradient norm: 0.21092724
INFO:root:[   38] Training loss: 0.00813260, Validation loss: 0.47406542, Gradient norm: 0.20897875
INFO:root:[   39] Training loss: 0.00832177, Validation loss: 0.48086396, Gradient norm: 0.24651278
INFO:root:[   40] Training loss: 0.00804575, Validation loss: 0.48360240, Gradient norm: 0.20642520
INFO:root:[   41] Training loss: 0.00813541, Validation loss: 0.47075258, Gradient norm: 0.25978118
INFO:root:[   42] Training loss: 0.00832819, Validation loss: 0.48908019, Gradient norm: 0.25718256
INFO:root:[   43] Training loss: 0.00799880, Validation loss: 0.46301959, Gradient norm: 0.21912888
INFO:root:[   44] Training loss: 0.00799625, Validation loss: 0.47900903, Gradient norm: 0.21536264
INFO:root:[   45] Training loss: 0.00794136, Validation loss: 0.51216644, Gradient norm: 0.24102028
INFO:root:[   46] Training loss: 0.00799689, Validation loss: 0.47461883, Gradient norm: 0.23735482
INFO:root:[   47] Training loss: 0.00763869, Validation loss: 0.48725860, Gradient norm: 0.18871062
INFO:root:[   48] Training loss: 0.00775753, Validation loss: 0.54721585, Gradient norm: 0.22678939
INFO:root:[   49] Training loss: 0.00763268, Validation loss: 0.46983018, Gradient norm: 0.22279015
INFO:root:[   50] Training loss: 0.00764530, Validation loss: 0.51523576, Gradient norm: 0.20497408
INFO:root:[   51] Training loss: 0.00775717, Validation loss: 0.53087337, Gradient norm: 0.23239236
INFO:root:[   52] Training loss: 0.00798463, Validation loss: 0.49600634, Gradient norm: 0.26003130
INFO:root:[   53] Training loss: 0.00745221, Validation loss: 0.46516532, Gradient norm: 0.18758709
INFO:root:[   54] Training loss: 0.00748836, Validation loss: 0.57686083, Gradient norm: 0.20876086
INFO:root:[   55] Training loss: 0.00742641, Validation loss: 0.45575609, Gradient norm: 0.19569953
INFO:root:[   56] Training loss: 0.00723676, Validation loss: 0.47199855, Gradient norm: 0.18055334
INFO:root:[   57] Training loss: 0.00768398, Validation loss: 0.45325176, Gradient norm: 0.27215357
INFO:root:[   58] Training loss: 0.00747715, Validation loss: 0.45819478, Gradient norm: 0.22963558
INFO:root:[   59] Training loss: 0.00731228, Validation loss: 0.46881418, Gradient norm: 0.20444073
INFO:root:[   60] Training loss: 0.00719588, Validation loss: 0.44750903, Gradient norm: 0.18522992
INFO:root:[   61] Training loss: 0.00722128, Validation loss: 0.47650672, Gradient norm: 0.21535161
INFO:root:[   62] Training loss: 0.00724445, Validation loss: 0.47208507, Gradient norm: 0.23552529
INFO:root:[   63] Training loss: 0.00707075, Validation loss: 0.45888761, Gradient norm: 0.20100648
INFO:root:[   64] Training loss: 0.00710023, Validation loss: 0.47136627, Gradient norm: 0.19163193
INFO:root:[   65] Training loss: 0.00724337, Validation loss: 0.45347079, Gradient norm: 0.25732235
INFO:root:[   66] Training loss: 0.00709712, Validation loss: 0.44673924, Gradient norm: 0.19725027
INFO:root:[   67] Training loss: 0.00701122, Validation loss: 0.46188258, Gradient norm: 0.20378017
INFO:root:[   68] Training loss: 0.00706782, Validation loss: 0.48867374, Gradient norm: 0.20546266
INFO:root:[   69] Training loss: 0.00710771, Validation loss: 0.45527243, Gradient norm: 0.22935382
INFO:root:[   70] Training loss: 0.00691812, Validation loss: 0.45978564, Gradient norm: 0.19220409
INFO:root:[   71] Training loss: 0.00695642, Validation loss: 0.51329270, Gradient norm: 0.18454061
INFO:root:[   72] Training loss: 0.00702453, Validation loss: 0.45989168, Gradient norm: 0.22335565
INFO:root:[   73] Training loss: 0.00689056, Validation loss: 0.46119373, Gradient norm: 0.20593313
INFO:root:[   74] Training loss: 0.00674515, Validation loss: 0.45043038, Gradient norm: 0.20517818
INFO:root:[   75] Training loss: 0.00704003, Validation loss: 0.44395013, Gradient norm: 0.23196455
INFO:root:[   76] Training loss: 0.00677582, Validation loss: 0.46138006, Gradient norm: 0.19909630
INFO:root:[   77] Training loss: 0.00684844, Validation loss: 0.44974764, Gradient norm: 0.20132136
INFO:root:[   78] Training loss: 0.00672797, Validation loss: 0.43526493, Gradient norm: 0.20186735
INFO:root:[   79] Training loss: 0.00675392, Validation loss: 0.43655766, Gradient norm: 0.22588958
INFO:root:[   80] Training loss: 0.00687127, Validation loss: 0.54659797, Gradient norm: 0.21025588
INFO:root:[   81] Training loss: 0.00653950, Validation loss: 0.44612219, Gradient norm: 0.15867503
INFO:root:[   82] Training loss: 0.00668051, Validation loss: 0.45733389, Gradient norm: 0.20919888
INFO:root:[   83] Training loss: 0.00676260, Validation loss: 0.43407559, Gradient norm: 0.22158084
INFO:root:[   84] Training loss: 0.00648036, Validation loss: 0.43483440, Gradient norm: 0.16999056
INFO:root:[   85] Training loss: 0.00647960, Validation loss: 0.48410678, Gradient norm: 0.19780617
INFO:root:[   86] Training loss: 0.00648821, Validation loss: 0.45850130, Gradient norm: 0.19893409
INFO:root:[   87] Training loss: 0.00644501, Validation loss: 0.45843930, Gradient norm: 0.19883056
INFO:root:[   88] Training loss: 0.00647261, Validation loss: 0.42731291, Gradient norm: 0.19971159
INFO:root:[   89] Training loss: 0.00636900, Validation loss: 0.44808244, Gradient norm: 0.20937648
INFO:root:[   90] Training loss: 0.00641833, Validation loss: 0.43848041, Gradient norm: 0.20524871
INFO:root:[   91] Training loss: 0.00636088, Validation loss: 0.46047439, Gradient norm: 0.19165071
INFO:root:[   92] Training loss: 0.00610880, Validation loss: 0.48968962, Gradient norm: 0.17586355
INFO:root:[   93] Training loss: 0.00636025, Validation loss: 0.45723124, Gradient norm: 0.19881664
INFO:root:[   94] Training loss: 0.00601710, Validation loss: 0.42625129, Gradient norm: 0.15852817
INFO:root:[   95] Training loss: 0.00632585, Validation loss: 0.44460280, Gradient norm: 0.19749812
INFO:root:[   96] Training loss: 0.00622157, Validation loss: 0.61112052, Gradient norm: 0.20337345
INFO:root:[   97] Training loss: 0.00652601, Validation loss: 0.45508316, Gradient norm: 0.22956230
INFO:root:[   98] Training loss: 0.00612213, Validation loss: 0.47019615, Gradient norm: 0.18109806
INFO:root:[   99] Training loss: 0.00607642, Validation loss: 0.45209706, Gradient norm: 0.17432152
INFO:root:[  100] Training loss: 0.00612798, Validation loss: 0.46879700, Gradient norm: 0.19340856
INFO:root:[  101] Training loss: 0.00601160, Validation loss: 0.44740052, Gradient norm: 0.18830078
INFO:root:[  102] Training loss: 0.00604495, Validation loss: 0.45837043, Gradient norm: 0.19023206
INFO:root:[  103] Training loss: 0.00600374, Validation loss: 0.46020473, Gradient norm: 0.18763052
INFO:root:EP 103: Early stopping
INFO:root:Training the model took 1385.012s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05221
INFO:root:EnergyScoretrain: 0.04146
INFO:root:Coveragetrain: 6.99278
INFO:root:IntervalWidthtrain: 166.96897
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01824
INFO:root:EnergyScorevalidation: 0.01379
INFO:root:Coveragevalidation: 1.8421
INFO:root:IntervalWidthvalidation: 43.2497
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01667
INFO:root:EnergyScoretest: 0.01182
INFO:root:Coveragetest: 0.96488
INFO:root:IntervalWidthtest: 50.51669
INFO:root:###4 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05768128, Validation loss: 1.76450867, Gradient norm: 0.33518005
INFO:root:[    2] Training loss: 0.02629430, Validation loss: 1.31428871, Gradient norm: 0.23310886
INFO:root:[    3] Training loss: 0.02141694, Validation loss: 1.13533217, Gradient norm: 0.28734133
INFO:root:[    4] Training loss: 0.01870168, Validation loss: 0.99945174, Gradient norm: 0.28802372
INFO:root:[    5] Training loss: 0.01749488, Validation loss: 0.98963469, Gradient norm: 0.34754903
INFO:root:[    6] Training loss: 0.01567323, Validation loss: 0.87650444, Gradient norm: 0.24548012
INFO:root:[    7] Training loss: 0.01469988, Validation loss: 0.82347237, Gradient norm: 0.24922651
INFO:root:[    8] Training loss: 0.01410243, Validation loss: 0.82388062, Gradient norm: 0.26408453
INFO:root:[    9] Training loss: 0.01335548, Validation loss: 0.87143877, Gradient norm: 0.24249128
INFO:root:[   10] Training loss: 0.01326671, Validation loss: 0.74258070, Gradient norm: 0.29897423
INFO:root:[   11] Training loss: 0.01290936, Validation loss: 0.68928268, Gradient norm: 0.30192083
INFO:root:[   12] Training loss: 0.01214664, Validation loss: 0.66922057, Gradient norm: 0.25047932
INFO:root:[   13] Training loss: 0.01206243, Validation loss: 0.68115063, Gradient norm: 0.30395084
INFO:root:[   14] Training loss: 0.01152837, Validation loss: 0.67723849, Gradient norm: 0.25736822
INFO:root:[   15] Training loss: 0.01155586, Validation loss: 0.62895466, Gradient norm: 0.26784345
INFO:root:[   16] Training loss: 0.01109783, Validation loss: 0.63795942, Gradient norm: 0.23497457
INFO:root:[   17] Training loss: 0.01102929, Validation loss: 0.62033205, Gradient norm: 0.26064751
INFO:root:[   18] Training loss: 0.01099277, Validation loss: 0.61004792, Gradient norm: 0.25681357
INFO:root:[   19] Training loss: 0.01059212, Validation loss: 0.59171682, Gradient norm: 0.23941686
INFO:root:[   20] Training loss: 0.01094677, Validation loss: 0.61022989, Gradient norm: 0.28810042
INFO:root:[   21] Training loss: 0.01052082, Validation loss: 0.59818726, Gradient norm: 0.25482830
INFO:root:[   22] Training loss: 0.01050360, Validation loss: 0.62590986, Gradient norm: 0.28381394
INFO:root:[   23] Training loss: 0.01005108, Validation loss: 0.60276940, Gradient norm: 0.23577149
INFO:root:[   24] Training loss: 0.01018542, Validation loss: 0.56479493, Gradient norm: 0.28014704
INFO:root:[   25] Training loss: 0.00970879, Validation loss: 0.57547876, Gradient norm: 0.21057138
INFO:root:[   26] Training loss: 0.00977698, Validation loss: 0.54829836, Gradient norm: 0.25901949
INFO:root:[   27] Training loss: 0.00962127, Validation loss: 0.56233042, Gradient norm: 0.20601291
INFO:root:[   28] Training loss: 0.00950256, Validation loss: 0.54818373, Gradient norm: 0.21954009
INFO:root:[   29] Training loss: 0.00950747, Validation loss: 0.56390543, Gradient norm: 0.20762279
INFO:root:[   30] Training loss: 0.00957170, Validation loss: 0.54410643, Gradient norm: 0.28376780
INFO:root:[   31] Training loss: 0.00954329, Validation loss: 0.54704631, Gradient norm: 0.23174050
INFO:root:[   32] Training loss: 0.00928609, Validation loss: 0.53761684, Gradient norm: 0.24022155
INFO:root:[   33] Training loss: 0.00941193, Validation loss: 0.52850078, Gradient norm: 0.26895087
INFO:root:[   34] Training loss: 0.00916656, Validation loss: 0.55311954, Gradient norm: 0.23210749
INFO:root:[   35] Training loss: 0.00900005, Validation loss: 0.52929609, Gradient norm: 0.21592656
INFO:root:[   36] Training loss: 0.00903538, Validation loss: 0.51654402, Gradient norm: 0.22770149
INFO:root:[   37] Training loss: 0.00892911, Validation loss: 0.50814911, Gradient norm: 0.24475448
INFO:root:[   38] Training loss: 0.00880532, Validation loss: 0.56674165, Gradient norm: 0.22142267
INFO:root:[   39] Training loss: 0.00912996, Validation loss: 0.51474193, Gradient norm: 0.29461941
INFO:root:[   40] Training loss: 0.00887016, Validation loss: 0.54499339, Gradient norm: 0.23894822
INFO:root:[   41] Training loss: 0.00866245, Validation loss: 0.55544080, Gradient norm: 0.25108263
INFO:root:[   42] Training loss: 0.00863650, Validation loss: 0.60804587, Gradient norm: 0.23452185
INFO:root:[   43] Training loss: 0.00874790, Validation loss: 0.52189966, Gradient norm: 0.24703501
INFO:root:[   44] Training loss: 0.00873042, Validation loss: 0.57256602, Gradient norm: 0.25597615
INFO:root:[   45] Training loss: 0.00835125, Validation loss: 0.57965658, Gradient norm: 0.21917613
INFO:root:[   46] Training loss: 0.00859481, Validation loss: 0.50724193, Gradient norm: 0.24620485
INFO:root:[   47] Training loss: 0.00810088, Validation loss: 0.50383343, Gradient norm: 0.17727185
INFO:root:[   48] Training loss: 0.00851043, Validation loss: 0.50062944, Gradient norm: 0.25541968
INFO:root:[   49] Training loss: 0.00826120, Validation loss: 0.50802712, Gradient norm: 0.23438652
INFO:root:[   50] Training loss: 0.00836084, Validation loss: 0.53475719, Gradient norm: 0.24563160
INFO:root:[   51] Training loss: 0.00822585, Validation loss: 0.53841687, Gradient norm: 0.23894968
INFO:root:[   52] Training loss: 0.00838760, Validation loss: 0.49496209, Gradient norm: 0.24358169
INFO:root:[   53] Training loss: 0.00795982, Validation loss: 0.52029117, Gradient norm: 0.19576372
INFO:root:[   54] Training loss: 0.00808224, Validation loss: 0.51193569, Gradient norm: 0.23873681
INFO:root:[   55] Training loss: 0.00817977, Validation loss: 0.48592585, Gradient norm: 0.24601927
INFO:root:[   56] Training loss: 0.00791268, Validation loss: 0.48519640, Gradient norm: 0.19352418
INFO:root:[   57] Training loss: 0.00787989, Validation loss: 0.47553035, Gradient norm: 0.21566165
INFO:root:[   58] Training loss: 0.00815559, Validation loss: 0.51803415, Gradient norm: 0.26277531
INFO:root:[   59] Training loss: 0.00793547, Validation loss: 0.46799780, Gradient norm: 0.21146553
INFO:root:[   60] Training loss: 0.00785946, Validation loss: 0.54558289, Gradient norm: 0.20559511
INFO:root:[   61] Training loss: 0.00790260, Validation loss: 0.47764969, Gradient norm: 0.21705857
INFO:root:[   62] Training loss: 0.00790694, Validation loss: 0.48210880, Gradient norm: 0.23954470
INFO:root:[   63] Training loss: 0.00770077, Validation loss: 0.48453271, Gradient norm: 0.20943443
INFO:root:[   64] Training loss: 0.00805506, Validation loss: 0.50864748, Gradient norm: 0.26624114
INFO:root:[   65] Training loss: 0.00774889, Validation loss: 0.47899420, Gradient norm: 0.22463103
INFO:root:[   66] Training loss: 0.00768723, Validation loss: 0.47626071, Gradient norm: 0.21108612
INFO:root:[   67] Training loss: 0.00764928, Validation loss: 0.49509986, Gradient norm: 0.20062015
INFO:root:[   68] Training loss: 0.00746959, Validation loss: 0.51385191, Gradient norm: 0.20839902
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 907.176s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.06452
INFO:root:EnergyScoretrain: 0.05281
INFO:root:Coveragetrain: 7.00386
INFO:root:IntervalWidthtrain: 216.40021
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01946
INFO:root:EnergyScorevalidation: 0.01538
INFO:root:Coveragevalidation: 1.86214
INFO:root:IntervalWidthvalidation: 56.64581
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01724
INFO:root:EnergyScoretest: 0.01226
INFO:root:Coveragetest: 1.06451
INFO:root:IntervalWidthtest: 68.70968
INFO:root:###5 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06100878, Validation loss: 1.55102897, Gradient norm: 0.42380451
INFO:root:[    2] Training loss: 0.02287271, Validation loss: 1.26115655, Gradient norm: 0.49020425
INFO:root:[    3] Training loss: 0.01793027, Validation loss: 0.96266599, Gradient norm: 0.38631686
INFO:root:[    4] Training loss: 0.01649085, Validation loss: 0.99602240, Gradient norm: 0.38579345
INFO:root:[    5] Training loss: 0.01583304, Validation loss: 1.04750378, Gradient norm: 0.48670855
INFO:root:[    6] Training loss: 0.01456141, Validation loss: 0.96635525, Gradient norm: 0.40395418
INFO:root:[    7] Training loss: 0.01369438, Validation loss: 0.75942089, Gradient norm: 0.36975250
INFO:root:[    8] Training loss: 0.01315233, Validation loss: 0.76545741, Gradient norm: 0.42570601
INFO:root:[    9] Training loss: 0.01232807, Validation loss: 0.69637979, Gradient norm: 0.35791076
INFO:root:[   10] Training loss: 0.01215709, Validation loss: 0.66138157, Gradient norm: 0.42372993
INFO:root:[   11] Training loss: 0.01114376, Validation loss: 0.69417627, Gradient norm: 0.31539622
INFO:root:[   12] Training loss: 0.01117835, Validation loss: 0.64174555, Gradient norm: 0.38911407
INFO:root:[   13] Training loss: 0.01090616, Validation loss: 0.58796836, Gradient norm: 0.36784433
INFO:root:[   14] Training loss: 0.01041978, Validation loss: 0.57614116, Gradient norm: 0.35849767
INFO:root:[   15] Training loss: 0.01028373, Validation loss: 0.57059305, Gradient norm: 0.35630342
INFO:root:[   16] Training loss: 0.01005382, Validation loss: 0.64737759, Gradient norm: 0.35407589
INFO:root:[   17] Training loss: 0.01041504, Validation loss: 0.57110521, Gradient norm: 0.44497641
INFO:root:[   18] Training loss: 0.00967798, Validation loss: 0.57526683, Gradient norm: 0.32298845
INFO:root:[   19] Training loss: 0.00972914, Validation loss: 0.53442666, Gradient norm: 0.37530033
INFO:root:[   20] Training loss: 0.00940638, Validation loss: 0.52520663, Gradient norm: 0.33919959
INFO:root:[   21] Training loss: 0.00894944, Validation loss: 0.57428616, Gradient norm: 0.24788782
INFO:root:[   22] Training loss: 0.00888901, Validation loss: 0.54523483, Gradient norm: 0.25933216
INFO:root:[   23] Training loss: 0.00926080, Validation loss: 0.55568701, Gradient norm: 0.39485092
INFO:root:[   24] Training loss: 0.00913601, Validation loss: 0.51432333, Gradient norm: 0.36735658
INFO:root:[   25] Training loss: 0.00847469, Validation loss: 0.64306749, Gradient norm: 0.24529130
INFO:root:[   26] Training loss: 0.00881470, Validation loss: 0.55881297, Gradient norm: 0.33815939
INFO:root:[   27] Training loss: 0.00910474, Validation loss: 0.57812911, Gradient norm: 0.40930129
INFO:root:[   28] Training loss: 0.00861306, Validation loss: 0.52836666, Gradient norm: 0.33968640
INFO:root:[   29] Training loss: 0.00847935, Validation loss: 0.51926770, Gradient norm: 0.33513523
INFO:root:[   30] Training loss: 0.00834010, Validation loss: 0.55124216, Gradient norm: 0.32334628
INFO:root:[   31] Training loss: 0.00839605, Validation loss: 0.53022632, Gradient norm: 0.32657954
INFO:root:[   32] Training loss: 0.00826977, Validation loss: 0.50446166, Gradient norm: 0.33550222
INFO:root:[   33] Training loss: 0.00836282, Validation loss: 0.57815696, Gradient norm: 0.35149482
INFO:root:[   34] Training loss: 0.00788661, Validation loss: 0.50293431, Gradient norm: 0.26619236
INFO:root:[   35] Training loss: 0.00802754, Validation loss: 0.57634413, Gradient norm: 0.30947540
INFO:root:[   36] Training loss: 0.00791616, Validation loss: 0.47727882, Gradient norm: 0.31955117
INFO:root:[   37] Training loss: 0.00786888, Validation loss: 0.53471609, Gradient norm: 0.31564036
INFO:root:[   38] Training loss: 0.00789408, Validation loss: 0.61323604, Gradient norm: 0.33966351
INFO:root:[   39] Training loss: 0.00800270, Validation loss: 0.49928532, Gradient norm: 0.36553853
INFO:root:[   40] Training loss: 0.00760037, Validation loss: 0.49821761, Gradient norm: 0.28745535
INFO:root:[   41] Training loss: 0.00739041, Validation loss: 0.54641246, Gradient norm: 0.24475363
INFO:root:[   42] Training loss: 0.00751316, Validation loss: 0.47192233, Gradient norm: 0.30532899
INFO:root:[   43] Training loss: 0.00746968, Validation loss: 0.51883775, Gradient norm: 0.30800351
INFO:root:[   44] Training loss: 0.00739949, Validation loss: 0.48333613, Gradient norm: 0.28410904
INFO:root:[   45] Training loss: 0.00772020, Validation loss: 0.49518514, Gradient norm: 0.36921965
INFO:root:[   46] Training loss: 0.00699949, Validation loss: 0.49445617, Gradient norm: 0.19777651
INFO:root:[   47] Training loss: 0.00771305, Validation loss: 0.47417593, Gradient norm: 0.35793780
INFO:root:[   48] Training loss: 0.00717717, Validation loss: 0.51624584, Gradient norm: 0.26161751
INFO:root:[   49] Training loss: 0.00743452, Validation loss: 0.45205796, Gradient norm: 0.34252519
INFO:root:[   50] Training loss: 0.00719255, Validation loss: 0.48081898, Gradient norm: 0.31000822
INFO:root:[   51] Training loss: 0.00704830, Validation loss: 0.49466807, Gradient norm: 0.26266474
INFO:root:[   52] Training loss: 0.00726341, Validation loss: 0.49463104, Gradient norm: 0.34051297
INFO:root:[   53] Training loss: 0.00717453, Validation loss: 0.52043381, Gradient norm: 0.34075147
INFO:root:[   54] Training loss: 0.00686010, Validation loss: 0.48717696, Gradient norm: 0.27511501
INFO:root:[   55] Training loss: 0.00683780, Validation loss: 0.47317679, Gradient norm: 0.27835562
INFO:root:[   56] Training loss: 0.00684000, Validation loss: 0.54133541, Gradient norm: 0.28502124
INFO:root:[   57] Training loss: 0.00661236, Validation loss: 0.48845322, Gradient norm: 0.26094244
INFO:root:[   58] Training loss: 0.00692572, Validation loss: 0.46849367, Gradient norm: 0.31272263
INFO:root:[   59] Training loss: 0.00667700, Validation loss: 0.55953407, Gradient norm: 0.26401869
INFO:root:[   60] Training loss: 0.00667591, Validation loss: 0.48564003, Gradient norm: 0.28981663
INFO:root:[   61] Training loss: 0.00673262, Validation loss: 0.53528105, Gradient norm: 0.31968337
INFO:root:[   62] Training loss: 0.00713762, Validation loss: 0.46396093, Gradient norm: 0.40502973
INFO:root:[   63] Training loss: 0.00651132, Validation loss: 0.48376399, Gradient norm: 0.30131496
INFO:root:[   64] Training loss: 0.00645683, Validation loss: 0.47967662, Gradient norm: 0.26854969
INFO:root:[   65] Training loss: 0.00675021, Validation loss: 0.44631144, Gradient norm: 0.33280084
INFO:root:[   66] Training loss: 0.00629954, Validation loss: 0.46422316, Gradient norm: 0.23882342
INFO:root:[   67] Training loss: 0.00655169, Validation loss: 0.47604303, Gradient norm: 0.30425846
INFO:root:[   68] Training loss: 0.00625120, Validation loss: 0.48393999, Gradient norm: 0.25969114
INFO:root:[   69] Training loss: 0.00652013, Validation loss: 0.45358140, Gradient norm: 0.32662995
INFO:root:[   70] Training loss: 0.00624942, Validation loss: 0.46182964, Gradient norm: 0.28304029
INFO:root:[   71] Training loss: 0.00601819, Validation loss: 0.46540348, Gradient norm: 0.22069961
INFO:root:[   72] Training loss: 0.00641354, Validation loss: 0.46713438, Gradient norm: 0.32292060
INFO:root:[   73] Training loss: 0.00610987, Validation loss: 0.47531593, Gradient norm: 0.26651675
INFO:root:[   74] Training loss: 0.00622551, Validation loss: 0.44565657, Gradient norm: 0.32395273
INFO:root:[   75] Training loss: 0.00593477, Validation loss: 0.46206342, Gradient norm: 0.24192430
INFO:root:[   76] Training loss: 0.00615948, Validation loss: 0.46874110, Gradient norm: 0.31291082
INFO:root:[   77] Training loss: 0.00600272, Validation loss: 0.51756452, Gradient norm: 0.28093857
INFO:root:[   78] Training loss: 0.00588903, Validation loss: 0.50332936, Gradient norm: 0.27556404
INFO:root:[   79] Training loss: 0.00586981, Validation loss: 0.45947017, Gradient norm: 0.26424966
INFO:root:[   80] Training loss: 0.00596598, Validation loss: 0.47097721, Gradient norm: 0.28920462
INFO:root:[   81] Training loss: 0.00600940, Validation loss: 0.44777449, Gradient norm: 0.31051231
INFO:root:[   82] Training loss: 0.00616753, Validation loss: 0.46164241, Gradient norm: 0.35422513
INFO:root:[   83] Training loss: 0.00577933, Validation loss: 0.50282366, Gradient norm: 0.29092840
INFO:root:EP 83: Early stopping
INFO:root:Training the model took 1089.596s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05178
INFO:root:EnergyScoretrain: 0.03931
INFO:root:Coveragetrain: 6.89912
INFO:root:IntervalWidthtrain: 119.47613
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02028
INFO:root:EnergyScorevalidation: 0.01538
INFO:root:Coveragevalidation: 1.77719
INFO:root:IntervalWidthvalidation: 31.31065
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02454
INFO:root:EnergyScoretest: 0.01911
INFO:root:Coveragetest: 0.7006
INFO:root:IntervalWidthtest: 34.781
INFO:root:###6 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04942675, Validation loss: 1.53259857, Gradient norm: 0.34163246
INFO:root:[    2] Training loss: 0.02356190, Validation loss: 1.13958828, Gradient norm: 0.30291217
INFO:root:[    3] Training loss: 0.01876906, Validation loss: 0.92941891, Gradient norm: 0.30282972
INFO:root:[    4] Training loss: 0.01654973, Validation loss: 0.85078055, Gradient norm: 0.36268329
INFO:root:[    5] Training loss: 0.01462110, Validation loss: 0.80744234, Gradient norm: 0.26365307
INFO:root:[    6] Training loss: 0.01408040, Validation loss: 0.75689914, Gradient norm: 0.33016263
INFO:root:[    7] Training loss: 0.01336107, Validation loss: 0.74148989, Gradient norm: 0.29644402
INFO:root:[    8] Training loss: 0.01258612, Validation loss: 0.75858126, Gradient norm: 0.28390567
INFO:root:[    9] Training loss: 0.01220063, Validation loss: 0.67849286, Gradient norm: 0.30464134
INFO:root:[   10] Training loss: 0.01167981, Validation loss: 0.71792699, Gradient norm: 0.27388033
INFO:root:[   11] Training loss: 0.01110328, Validation loss: 0.61289552, Gradient norm: 0.26677843
INFO:root:[   12] Training loss: 0.01112685, Validation loss: 0.63270463, Gradient norm: 0.28564543
INFO:root:[   13] Training loss: 0.01086614, Validation loss: 0.73171716, Gradient norm: 0.31600797
INFO:root:[   14] Training loss: 0.01045797, Validation loss: 0.58554872, Gradient norm: 0.27149268
INFO:root:[   15] Training loss: 0.01028562, Validation loss: 0.56544987, Gradient norm: 0.25355125
INFO:root:[   16] Training loss: 0.01061386, Validation loss: 0.58412784, Gradient norm: 0.32650767
INFO:root:[   17] Training loss: 0.00990277, Validation loss: 0.64200264, Gradient norm: 0.24813199
INFO:root:[   18] Training loss: 0.00952920, Validation loss: 0.54907802, Gradient norm: 0.22414368
INFO:root:[   19] Training loss: 0.00947046, Validation loss: 0.61331687, Gradient norm: 0.24265424
INFO:root:[   20] Training loss: 0.00941758, Validation loss: 0.53656654, Gradient norm: 0.26698581
INFO:root:[   21] Training loss: 0.00940310, Validation loss: 0.53100648, Gradient norm: 0.26344137
INFO:root:[   22] Training loss: 0.00934633, Validation loss: 0.53708737, Gradient norm: 0.24887212
INFO:root:[   23] Training loss: 0.00928450, Validation loss: 0.54632636, Gradient norm: 0.26050173
INFO:root:[   24] Training loss: 0.00916250, Validation loss: 0.57001001, Gradient norm: 0.27297017
INFO:root:[   25] Training loss: 0.00922260, Validation loss: 0.51791475, Gradient norm: 0.29796022
INFO:root:[   26] Training loss: 0.00876120, Validation loss: 0.54377514, Gradient norm: 0.22814455
INFO:root:[   27] Training loss: 0.00884990, Validation loss: 0.52300017, Gradient norm: 0.28074110
INFO:root:[   28] Training loss: 0.00860254, Validation loss: 0.50853023, Gradient norm: 0.24286623
INFO:root:[   29] Training loss: 0.00888189, Validation loss: 0.50299075, Gradient norm: 0.27868585
INFO:root:[   30] Training loss: 0.00839593, Validation loss: 0.53925616, Gradient norm: 0.24079562
INFO:root:[   31] Training loss: 0.00847819, Validation loss: 0.50947530, Gradient norm: 0.24899402
INFO:root:[   32] Training loss: 0.00860238, Validation loss: 0.59022528, Gradient norm: 0.27031739
INFO:root:[   33] Training loss: 0.00827028, Validation loss: 0.58281289, Gradient norm: 0.25848718
INFO:root:[   34] Training loss: 0.00854195, Validation loss: 0.49139743, Gradient norm: 0.28343697
INFO:root:[   35] Training loss: 0.00832990, Validation loss: 0.49098280, Gradient norm: 0.26616059
INFO:root:[   36] Training loss: 0.00822067, Validation loss: 0.47138109, Gradient norm: 0.25080122
INFO:root:[   37] Training loss: 0.00819804, Validation loss: 0.54298732, Gradient norm: 0.26324387
INFO:root:[   38] Training loss: 0.00793180, Validation loss: 0.49490562, Gradient norm: 0.22061701
INFO:root:[   39] Training loss: 0.00821569, Validation loss: 0.50054799, Gradient norm: 0.28157078
INFO:root:[   40] Training loss: 0.00774962, Validation loss: 0.49626101, Gradient norm: 0.21671079
INFO:root:[   41] Training loss: 0.00804887, Validation loss: 0.47680461, Gradient norm: 0.27597103
INFO:root:[   42] Training loss: 0.00780387, Validation loss: 0.47449061, Gradient norm: 0.22488201
INFO:root:[   43] Training loss: 0.00787194, Validation loss: 0.52348036, Gradient norm: 0.25891223
INFO:root:[   44] Training loss: 0.00801235, Validation loss: 0.48159961, Gradient norm: 0.29821867
INFO:root:[   45] Training loss: 0.00768212, Validation loss: 0.48546807, Gradient norm: 0.25658435
INFO:root:[   46] Training loss: 0.00760313, Validation loss: 0.48181158, Gradient norm: 0.25080893
INFO:root:[   47] Training loss: 0.00775911, Validation loss: 0.46346426, Gradient norm: 0.25230476
INFO:root:[   48] Training loss: 0.00767004, Validation loss: 0.48634528, Gradient norm: 0.25678672
INFO:root:[   49] Training loss: 0.00746813, Validation loss: 0.47898568, Gradient norm: 0.22896206
INFO:root:[   50] Training loss: 0.00754914, Validation loss: 0.46564814, Gradient norm: 0.25354594
INFO:root:[   51] Training loss: 0.00759228, Validation loss: 0.48880087, Gradient norm: 0.26452433
INFO:root:[   52] Training loss: 0.00745974, Validation loss: 0.50913376, Gradient norm: 0.27143539
INFO:root:[   53] Training loss: 0.00734988, Validation loss: 0.45791504, Gradient norm: 0.22984930
INFO:root:[   54] Training loss: 0.00731895, Validation loss: 0.46243457, Gradient norm: 0.22459028
INFO:root:[   55] Training loss: 0.00716066, Validation loss: 0.45149152, Gradient norm: 0.24351163
INFO:root:[   56] Training loss: 0.00721159, Validation loss: 0.47731585, Gradient norm: 0.23889259
INFO:root:[   57] Training loss: 0.00713402, Validation loss: 0.64944249, Gradient norm: 0.22378070
INFO:root:[   58] Training loss: 0.00707860, Validation loss: 0.46657621, Gradient norm: 0.22114270
INFO:root:[   59] Training loss: 0.00705772, Validation loss: 0.55586724, Gradient norm: 0.24495612
INFO:root:[   60] Training loss: 0.00705829, Validation loss: 0.51458338, Gradient norm: 0.22895330
INFO:root:[   61] Training loss: 0.00702368, Validation loss: 0.46679885, Gradient norm: 0.22509942
INFO:root:[   62] Training loss: 0.00699430, Validation loss: 0.47310991, Gradient norm: 0.21617080
INFO:root:[   63] Training loss: 0.00694516, Validation loss: 0.47132553, Gradient norm: 0.22412776
INFO:root:[   64] Training loss: 0.00697320, Validation loss: 0.47911171, Gradient norm: 0.24559720
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 851.618s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.06131
INFO:root:EnergyScoretrain: 0.0473
INFO:root:Coveragetrain: 6.96041
INFO:root:IntervalWidthtrain: 165.63655
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01979
INFO:root:EnergyScorevalidation: 0.0148
INFO:root:Coveragevalidation: 1.838
INFO:root:IntervalWidthvalidation: 43.27552
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02052
INFO:root:EnergyScoretest: 0.0145
INFO:root:Coveragetest: 0.92254
INFO:root:IntervalWidthtest: 52.33199
INFO:root:###7 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05183335, Validation loss: 1.52029528, Gradient norm: 0.34464821
INFO:root:[    2] Training loss: 0.02400832, Validation loss: 1.18236801, Gradient norm: 0.25267157
INFO:root:[    3] Training loss: 0.01919876, Validation loss: 1.00773012, Gradient norm: 0.21944498
INFO:root:[    4] Training loss: 0.01713638, Validation loss: 0.90579832, Gradient norm: 0.24434583
INFO:root:[    5] Training loss: 0.01548857, Validation loss: 0.89768067, Gradient norm: 0.26074334
INFO:root:[    6] Training loss: 0.01464314, Validation loss: 0.79991037, Gradient norm: 0.26288611
INFO:root:[    7] Training loss: 0.01379121, Validation loss: 0.75645463, Gradient norm: 0.26172674
INFO:root:[    8] Training loss: 0.01291361, Validation loss: 0.73487889, Gradient norm: 0.23232588
INFO:root:[    9] Training loss: 0.01242210, Validation loss: 0.67752845, Gradient norm: 0.24789592
INFO:root:[   10] Training loss: 0.01178262, Validation loss: 0.67156584, Gradient norm: 0.21691630
INFO:root:[   11] Training loss: 0.01179658, Validation loss: 0.66355129, Gradient norm: 0.25820784
INFO:root:[   12] Training loss: 0.01118045, Validation loss: 0.64228356, Gradient norm: 0.21415588
INFO:root:[   13] Training loss: 0.01107622, Validation loss: 0.61808823, Gradient norm: 0.23780028
INFO:root:[   14] Training loss: 0.01075720, Validation loss: 0.65519959, Gradient norm: 0.22907973
INFO:root:[   15] Training loss: 0.01057562, Validation loss: 0.60712363, Gradient norm: 0.22974576
INFO:root:[   16] Training loss: 0.01043743, Validation loss: 0.64710961, Gradient norm: 0.23395709
INFO:root:[   17] Training loss: 0.01016232, Validation loss: 0.58222270, Gradient norm: 0.22321292
INFO:root:[   18] Training loss: 0.01001956, Validation loss: 0.70616267, Gradient norm: 0.22761946
INFO:root:[   19] Training loss: 0.00960025, Validation loss: 0.56699197, Gradient norm: 0.20356473
INFO:root:[   20] Training loss: 0.00989510, Validation loss: 0.57253279, Gradient norm: 0.23369196
INFO:root:[   21] Training loss: 0.00975617, Validation loss: 0.58349649, Gradient norm: 0.25448238
INFO:root:[   22] Training loss: 0.00946466, Validation loss: 0.54735887, Gradient norm: 0.22018292
INFO:root:[   23] Training loss: 0.00936042, Validation loss: 0.52551772, Gradient norm: 0.19947929
INFO:root:[   24] Training loss: 0.00950965, Validation loss: 0.52916447, Gradient norm: 0.23425224
INFO:root:[   25] Training loss: 0.00901492, Validation loss: 0.52305071, Gradient norm: 0.19941148
INFO:root:[   26] Training loss: 0.00908835, Validation loss: 0.63964874, Gradient norm: 0.22192876
INFO:root:[   27] Training loss: 0.00890397, Validation loss: 0.51037611, Gradient norm: 0.20789084
INFO:root:[   28] Training loss: 0.00894878, Validation loss: 0.53264846, Gradient norm: 0.22420372
INFO:root:[   29] Training loss: 0.00899686, Validation loss: 0.57691828, Gradient norm: 0.23367282
INFO:root:[   30] Training loss: 0.00884122, Validation loss: 0.51033229, Gradient norm: 0.24028390
INFO:root:[   31] Training loss: 0.00900732, Validation loss: 0.53105397, Gradient norm: 0.27917047
INFO:root:[   32] Training loss: 0.00859597, Validation loss: 0.53382947, Gradient norm: 0.20862412
INFO:root:[   33] Training loss: 0.00853307, Validation loss: 0.51351729, Gradient norm: 0.22451610
INFO:root:[   34] Training loss: 0.00847026, Validation loss: 0.49832040, Gradient norm: 0.22851936
INFO:root:[   35] Training loss: 0.00859750, Validation loss: 0.49612488, Gradient norm: 0.27188421
INFO:root:[   36] Training loss: 0.00820597, Validation loss: 0.49047490, Gradient norm: 0.19395114
INFO:root:[   37] Training loss: 0.00874975, Validation loss: 0.49482460, Gradient norm: 0.28701426
INFO:root:[   38] Training loss: 0.00825253, Validation loss: 0.52072274, Gradient norm: 0.19025776
INFO:root:[   39] Training loss: 0.00822472, Validation loss: 0.49768443, Gradient norm: 0.23426554
INFO:root:[   40] Training loss: 0.00795107, Validation loss: 0.49727816, Gradient norm: 0.19092458
INFO:root:[   41] Training loss: 0.00829160, Validation loss: 0.48436711, Gradient norm: 0.26422162
INFO:root:[   42] Training loss: 0.00817118, Validation loss: 0.48128887, Gradient norm: 0.22138371
INFO:root:[   43] Training loss: 0.00802122, Validation loss: 0.48141099, Gradient norm: 0.22834369
INFO:root:[   44] Training loss: 0.00792469, Validation loss: 0.48830987, Gradient norm: 0.21275803
INFO:root:[   45] Training loss: 0.00778104, Validation loss: 0.47781803, Gradient norm: 0.19253789
INFO:root:[   46] Training loss: 0.00775701, Validation loss: 0.48151192, Gradient norm: 0.21829054
INFO:root:[   47] Training loss: 0.00801530, Validation loss: 0.50498819, Gradient norm: 0.26073067
INFO:root:[   48] Training loss: 0.00750812, Validation loss: 0.46886479, Gradient norm: 0.16930432
INFO:root:[   49] Training loss: 0.00782673, Validation loss: 0.50374049, Gradient norm: 0.22208112
INFO:root:[   50] Training loss: 0.00760834, Validation loss: 0.47680675, Gradient norm: 0.19984140
INFO:root:[   51] Training loss: 0.00790394, Validation loss: 0.50571709, Gradient norm: 0.26220686
INFO:root:[   52] Training loss: 0.00754782, Validation loss: 0.47172969, Gradient norm: 0.22550612
INFO:root:[   53] Training loss: 0.00745114, Validation loss: 0.58180069, Gradient norm: 0.20850273
INFO:root:[   54] Training loss: 0.00763264, Validation loss: 0.45773291, Gradient norm: 0.22893813
INFO:root:[   55] Training loss: 0.00753357, Validation loss: 0.48310355, Gradient norm: 0.22862963
INFO:root:[   56] Training loss: 0.00761474, Validation loss: 0.47425647, Gradient norm: 0.25940215
INFO:root:[   57] Training loss: 0.00723050, Validation loss: 0.45842437, Gradient norm: 0.18408939
INFO:root:[   58] Training loss: 0.00740543, Validation loss: 0.47542246, Gradient norm: 0.23950759
INFO:root:[   59] Training loss: 0.00747053, Validation loss: 0.49764144, Gradient norm: 0.22386097
INFO:root:[   60] Training loss: 0.00713778, Validation loss: 0.47546079, Gradient norm: 0.21023482
INFO:root:[   61] Training loss: 0.00721413, Validation loss: 0.46162113, Gradient norm: 0.22819932
INFO:root:[   62] Training loss: 0.00710758, Validation loss: 0.49897392, Gradient norm: 0.20908117
INFO:root:[   63] Training loss: 0.00747231, Validation loss: 0.46455376, Gradient norm: 0.26825457
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 829.813s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.063
INFO:root:EnergyScoretrain: 0.0498
INFO:root:Coveragetrain: 6.98467
INFO:root:IntervalWidthtrain: 184.04838
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02128
INFO:root:EnergyScorevalidation: 0.01631
INFO:root:Coveragevalidation: 1.84074
INFO:root:IntervalWidthvalidation: 48.20248
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02414
INFO:root:EnergyScoretest: 0.01742
INFO:root:Coveragetest: 0.83664
INFO:root:IntervalWidthtest: 56.8502
INFO:root:###8 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05161903, Validation loss: 1.62643725, Gradient norm: 0.32148298
INFO:root:[    2] Training loss: 0.02533558, Validation loss: 1.34964475, Gradient norm: 0.20720009
INFO:root:[    3] Training loss: 0.02078274, Validation loss: 1.09262330, Gradient norm: 0.22389873
INFO:root:[    4] Training loss: 0.01796605, Validation loss: 0.94658219, Gradient norm: 0.23939026
INFO:root:[    5] Training loss: 0.01628375, Validation loss: 0.98107687, Gradient norm: 0.22654698
INFO:root:[    6] Training loss: 0.01534996, Validation loss: 0.86380900, Gradient norm: 0.23375891
INFO:root:[    7] Training loss: 0.01453719, Validation loss: 0.80109167, Gradient norm: 0.21817557
INFO:root:[    8] Training loss: 0.01394912, Validation loss: 0.77112327, Gradient norm: 0.23349535
INFO:root:[    9] Training loss: 0.01354724, Validation loss: 0.79119832, Gradient norm: 0.23914908
INFO:root:[   10] Training loss: 0.01287774, Validation loss: 0.74262294, Gradient norm: 0.21090923
INFO:root:[   11] Training loss: 0.01252181, Validation loss: 0.70974756, Gradient norm: 0.22211450
INFO:root:[   12] Training loss: 0.01223095, Validation loss: 0.70671664, Gradient norm: 0.19874530
INFO:root:[   13] Training loss: 0.01263779, Validation loss: 0.87021982, Gradient norm: 0.29122748
INFO:root:[   14] Training loss: 0.01170142, Validation loss: 0.65828185, Gradient norm: 0.20253569
INFO:root:[   15] Training loss: 0.01158741, Validation loss: 0.64880527, Gradient norm: 0.20848492
INFO:root:[   16] Training loss: 0.01126841, Validation loss: 0.64399025, Gradient norm: 0.21059972
INFO:root:[   17] Training loss: 0.01126980, Validation loss: 0.64525004, Gradient norm: 0.23585476
INFO:root:[   18] Training loss: 0.01109000, Validation loss: 0.63537955, Gradient norm: 0.23602125
INFO:root:[   19] Training loss: 0.01086983, Validation loss: 0.60950108, Gradient norm: 0.22900292
INFO:root:[   20] Training loss: 0.01048094, Validation loss: 0.61461415, Gradient norm: 0.19646791
INFO:root:[   21] Training loss: 0.01051031, Validation loss: 0.59807149, Gradient norm: 0.21676367
INFO:root:[   22] Training loss: 0.01059861, Validation loss: 0.59853787, Gradient norm: 0.23555654
INFO:root:[   23] Training loss: 0.01050857, Validation loss: 0.58864697, Gradient norm: 0.23866743
INFO:root:[   24] Training loss: 0.01028779, Validation loss: 0.60355105, Gradient norm: 0.24137411
INFO:root:[   25] Training loss: 0.01011322, Validation loss: 0.56412383, Gradient norm: 0.20057345
INFO:root:[   26] Training loss: 0.00984762, Validation loss: 0.58180004, Gradient norm: 0.21333092
INFO:root:[   27] Training loss: 0.01000601, Validation loss: 0.59345963, Gradient norm: 0.21785815
INFO:root:[   28] Training loss: 0.00981817, Validation loss: 0.62108149, Gradient norm: 0.21018406
INFO:root:[   29] Training loss: 0.00962458, Validation loss: 0.56400789, Gradient norm: 0.20843836
INFO:root:[   30] Training loss: 0.00950708, Validation loss: 0.56960186, Gradient norm: 0.18145384
INFO:root:[   31] Training loss: 0.00943398, Validation loss: 0.57426437, Gradient norm: 0.17525086
INFO:root:[   32] Training loss: 0.00948345, Validation loss: 0.57623809, Gradient norm: 0.22166857
INFO:root:[   33] Training loss: 0.00963927, Validation loss: 0.56609087, Gradient norm: 0.23573194
INFO:root:[   34] Training loss: 0.00923201, Validation loss: 0.53450402, Gradient norm: 0.18852283
INFO:root:[   35] Training loss: 0.00916680, Validation loss: 0.55338180, Gradient norm: 0.20801027
INFO:root:[   36] Training loss: 0.00920115, Validation loss: 0.53981120, Gradient norm: 0.22517704
INFO:root:[   37] Training loss: 0.00900213, Validation loss: 0.59988500, Gradient norm: 0.18534862
INFO:root:[   38] Training loss: 0.00906032, Validation loss: 0.52790371, Gradient norm: 0.20905372
INFO:root:[   39] Training loss: 0.00911711, Validation loss: 0.55284121, Gradient norm: 0.24978764
INFO:root:[   40] Training loss: 0.00892346, Validation loss: 0.54849461, Gradient norm: 0.21309654
INFO:root:[   41] Training loss: 0.00866253, Validation loss: 0.51361366, Gradient norm: 0.18418856
INFO:root:[   42] Training loss: 0.00876980, Validation loss: 0.65238711, Gradient norm: 0.19979557
INFO:root:[   43] Training loss: 0.00899323, Validation loss: 0.53367582, Gradient norm: 0.26245604
INFO:root:[   44] Training loss: 0.00862578, Validation loss: 0.61590642, Gradient norm: 0.21229230
INFO:root:[   45] Training loss: 0.00852986, Validation loss: 0.52173472, Gradient norm: 0.18668867
INFO:root:[   46] Training loss: 0.00861179, Validation loss: 0.49570736, Gradient norm: 0.20671003
INFO:root:[   47] Training loss: 0.00859574, Validation loss: 0.48845900, Gradient norm: 0.21946132
INFO:root:[   48] Training loss: 0.00850924, Validation loss: 0.50284240, Gradient norm: 0.21396111
INFO:root:[   49] Training loss: 0.00841553, Validation loss: 0.51815231, Gradient norm: 0.20033460
INFO:root:[   50] Training loss: 0.00828695, Validation loss: 0.50210360, Gradient norm: 0.20875533
INFO:root:[   51] Training loss: 0.00856472, Validation loss: 0.50795952, Gradient norm: 0.24027323
INFO:root:[   52] Training loss: 0.00833383, Validation loss: 0.50844929, Gradient norm: 0.21762451
INFO:root:[   53] Training loss: 0.00854205, Validation loss: 0.49776664, Gradient norm: 0.24141461
INFO:root:[   54] Training loss: 0.00821943, Validation loss: 0.52780172, Gradient norm: 0.20056967
INFO:root:[   55] Training loss: 0.00849490, Validation loss: 0.50555109, Gradient norm: 0.24605850
INFO:root:[   56] Training loss: 0.00832986, Validation loss: 0.55960897, Gradient norm: 0.21038577
INFO:root:[   57] Training loss: 0.00803015, Validation loss: 0.48909054, Gradient norm: 0.18655816
INFO:root:[   58] Training loss: 0.00803314, Validation loss: 0.52057006, Gradient norm: 0.19075530
INFO:root:[   59] Training loss: 0.00821290, Validation loss: 0.49336905, Gradient norm: 0.24323994
INFO:root:[   60] Training loss: 0.00793051, Validation loss: 0.48878941, Gradient norm: 0.19525307
INFO:root:[   61] Training loss: 0.00810921, Validation loss: 0.48940128, Gradient norm: 0.24658121
INFO:root:[   62] Training loss: 0.00776996, Validation loss: 0.53949485, Gradient norm: 0.17125125
INFO:root:[   63] Training loss: 0.00781687, Validation loss: 0.49696190, Gradient norm: 0.17953443
INFO:root:[   64] Training loss: 0.00789941, Validation loss: 0.49173545, Gradient norm: 0.20597258
INFO:root:[   65] Training loss: 0.00808184, Validation loss: 0.54153914, Gradient norm: 0.23826871
INFO:root:[   66] Training loss: 0.00787931, Validation loss: 0.47554312, Gradient norm: 0.22448891
INFO:root:[   67] Training loss: 0.00787275, Validation loss: 0.49535669, Gradient norm: 0.22904775
INFO:root:[   68] Training loss: 0.00765674, Validation loss: 0.49869203, Gradient norm: 0.18974279
INFO:root:[   69] Training loss: 0.00793096, Validation loss: 0.47919994, Gradient norm: 0.24734894
INFO:root:[   70] Training loss: 0.00744707, Validation loss: 0.48871814, Gradient norm: 0.17589583
INFO:root:[   71] Training loss: 0.00767844, Validation loss: 0.62432850, Gradient norm: 0.21342315
INFO:root:[   72] Training loss: 0.00774977, Validation loss: 0.51234491, Gradient norm: 0.23217265
INFO:root:[   73] Training loss: 0.00747320, Validation loss: 0.53446891, Gradient norm: 0.18577086
INFO:root:[   74] Training loss: 0.00762222, Validation loss: 0.46579789, Gradient norm: 0.21531760
INFO:root:[   75] Training loss: 0.00750367, Validation loss: 0.47834785, Gradient norm: 0.20897315
INFO:root:[   76] Training loss: 0.00747137, Validation loss: 0.53097390, Gradient norm: 0.21458536
INFO:root:[   77] Training loss: 0.00762930, Validation loss: 0.51662290, Gradient norm: 0.23541555
INFO:root:[   78] Training loss: 0.00732960, Validation loss: 0.50279854, Gradient norm: 0.19626145
INFO:root:[   79] Training loss: 0.00733138, Validation loss: 0.47525984, Gradient norm: 0.20624590
INFO:root:[   80] Training loss: 0.00732811, Validation loss: 0.49236817, Gradient norm: 0.19612822
INFO:root:[   81] Training loss: 0.00715648, Validation loss: 0.48737663, Gradient norm: 0.16486239
INFO:root:[   82] Training loss: 0.00744527, Validation loss: 0.48047179, Gradient norm: 0.21844834
INFO:root:[   83] Training loss: 0.00752225, Validation loss: 0.45519698, Gradient norm: 0.22985319
INFO:root:[   84] Training loss: 0.00736908, Validation loss: 0.47156422, Gradient norm: 0.24557219
INFO:root:[   85] Training loss: 0.00730642, Validation loss: 0.46798527, Gradient norm: 0.21432477
INFO:root:[   86] Training loss: 0.00716635, Validation loss: 0.46969302, Gradient norm: 0.19756826
INFO:root:[   87] Training loss: 0.00728351, Validation loss: 0.46474718, Gradient norm: 0.20304979
INFO:root:[   88] Training loss: 0.00717824, Validation loss: 0.46042334, Gradient norm: 0.21811636
INFO:root:[   89] Training loss: 0.00716007, Validation loss: 0.46927986, Gradient norm: 0.22264740
INFO:root:[   90] Training loss: 0.00727155, Validation loss: 0.44992201, Gradient norm: 0.21775371
INFO:root:[   91] Training loss: 0.00737643, Validation loss: 0.49666445, Gradient norm: 0.24752168
INFO:root:[   92] Training loss: 0.00709850, Validation loss: 0.57902078, Gradient norm: 0.22376029
INFO:root:[   93] Training loss: 0.00725671, Validation loss: 0.49843609, Gradient norm: 0.25691689
INFO:root:[   94] Training loss: 0.00706266, Validation loss: 0.48834198, Gradient norm: 0.20871127
INFO:root:[   95] Training loss: 0.00691592, Validation loss: 0.44666435, Gradient norm: 0.19316582
INFO:root:[   96] Training loss: 0.00681501, Validation loss: 0.48292829, Gradient norm: 0.20797718
INFO:root:[   97] Training loss: 0.00687112, Validation loss: 0.47669661, Gradient norm: 0.20561907
INFO:root:[   98] Training loss: 0.00706731, Validation loss: 0.48548577, Gradient norm: 0.23143504
INFO:root:[   99] Training loss: 0.00696044, Validation loss: 0.48872771, Gradient norm: 0.22543476
INFO:root:[  100] Training loss: 0.00675048, Validation loss: 0.51154317, Gradient norm: 0.19829704
INFO:root:[  101] Training loss: 0.00693345, Validation loss: 0.47043930, Gradient norm: 0.22790243
INFO:root:[  102] Training loss: 0.00675769, Validation loss: 0.46156089, Gradient norm: 0.21377797
INFO:root:[  103] Training loss: 0.00687353, Validation loss: 0.46161607, Gradient norm: 0.21301729
INFO:root:[  104] Training loss: 0.00688774, Validation loss: 0.43821335, Gradient norm: 0.20598688
INFO:root:[  105] Training loss: 0.00672990, Validation loss: 0.45531727, Gradient norm: 0.21539645
INFO:root:[  106] Training loss: 0.00678747, Validation loss: 0.44179074, Gradient norm: 0.21311437
INFO:root:[  107] Training loss: 0.00669066, Validation loss: 0.46221477, Gradient norm: 0.20250412
INFO:root:[  108] Training loss: 0.00670737, Validation loss: 0.52393560, Gradient norm: 0.20462147
INFO:root:[  109] Training loss: 0.00682750, Validation loss: 0.45474105, Gradient norm: 0.23303122
INFO:root:[  110] Training loss: 0.00683763, Validation loss: 0.43510162, Gradient norm: 0.23186534
INFO:root:[  111] Training loss: 0.00659800, Validation loss: 0.49528762, Gradient norm: 0.20538877
INFO:root:[  112] Training loss: 0.00660694, Validation loss: 0.51501290, Gradient norm: 0.21619376
INFO:root:[  113] Training loss: 0.00661070, Validation loss: 0.50004398, Gradient norm: 0.19418622
INFO:root:[  114] Training loss: 0.00660197, Validation loss: 0.45725530, Gradient norm: 0.19976265
INFO:root:[  115] Training loss: 0.00662794, Validation loss: 0.44800212, Gradient norm: 0.22417816
INFO:root:[  116] Training loss: 0.00643967, Validation loss: 0.54744710, Gradient norm: 0.20100420
INFO:root:[  117] Training loss: 0.00658763, Validation loss: 0.47653795, Gradient norm: 0.20639012
INFO:root:[  118] Training loss: 0.00643744, Validation loss: 0.45424784, Gradient norm: 0.19731335
INFO:root:[  119] Training loss: 0.00646064, Validation loss: 0.46210710, Gradient norm: 0.21367114
INFO:root:[  120] Training loss: 0.00642328, Validation loss: 0.43196636, Gradient norm: 0.20026885
INFO:root:[  121] Training loss: 0.00657279, Validation loss: 0.45601169, Gradient norm: 0.22864287
INFO:root:[  122] Training loss: 0.00634513, Validation loss: 0.47250523, Gradient norm: 0.19391974
INFO:root:[  123] Training loss: 0.00636123, Validation loss: 0.48874951, Gradient norm: 0.20495752
INFO:root:[  124] Training loss: 0.00624110, Validation loss: 0.45096178, Gradient norm: 0.19521871
INFO:root:[  125] Training loss: 0.00642692, Validation loss: 0.44667947, Gradient norm: 0.20805980
INFO:root:[  126] Training loss: 0.00631678, Validation loss: 0.46185618, Gradient norm: 0.19264429
INFO:root:[  127] Training loss: 0.00644466, Validation loss: 0.46224879, Gradient norm: 0.23199242
INFO:root:[  128] Training loss: 0.00638598, Validation loss: 0.44283533, Gradient norm: 0.20009539
INFO:root:[  129] Training loss: 0.00640451, Validation loss: 0.46213596, Gradient norm: 0.24230960
INFO:root:EP 129: Early stopping
INFO:root:Training the model took 1685.058s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05172
INFO:root:EnergyScoretrain: 0.04311
INFO:root:Coveragetrain: 7.00662
INFO:root:IntervalWidthtrain: 179.22751
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02012
INFO:root:EnergyScorevalidation: 0.01547
INFO:root:Coveragevalidation: 1.84841
INFO:root:IntervalWidthvalidation: 47.07053
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01743
INFO:root:EnergyScoretest: 0.01259
INFO:root:Coveragetest: 1.0317
INFO:root:IntervalWidthtest: 56.64935
INFO:root:###9 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05453308, Validation loss: 1.50474040, Gradient norm: 0.43278406
INFO:root:[    2] Training loss: 0.02305236, Validation loss: 1.08088842, Gradient norm: 0.44002530
INFO:root:[    3] Training loss: 0.01792579, Validation loss: 0.95279402, Gradient norm: 0.33372958
INFO:root:[    4] Training loss: 0.01658558, Validation loss: 1.25115187, Gradient norm: 0.40274547
INFO:root:[    5] Training loss: 0.01502035, Validation loss: 0.79456156, Gradient norm: 0.32705562
INFO:root:[    6] Training loss: 0.01394158, Validation loss: 0.74778452, Gradient norm: 0.32191910
INFO:root:[    7] Training loss: 0.01342713, Validation loss: 0.71586544, Gradient norm: 0.34654011
INFO:root:[    8] Training loss: 0.01251254, Validation loss: 0.76485839, Gradient norm: 0.31487717
INFO:root:[    9] Training loss: 0.01163804, Validation loss: 0.77549889, Gradient norm: 0.24609866
INFO:root:[   10] Training loss: 0.01188433, Validation loss: 0.66690674, Gradient norm: 0.35693515
INFO:root:[   11] Training loss: 0.01105272, Validation loss: 0.63758456, Gradient norm: 0.28919628
INFO:root:[   12] Training loss: 0.01095154, Validation loss: 0.61192280, Gradient norm: 0.30909787
INFO:root:[   13] Training loss: 0.01031202, Validation loss: 0.58118718, Gradient norm: 0.27527771
INFO:root:[   14] Training loss: 0.01036757, Validation loss: 0.65648032, Gradient norm: 0.31550847
INFO:root:[   15] Training loss: 0.01026518, Validation loss: 0.57499792, Gradient norm: 0.32404192
INFO:root:[   16] Training loss: 0.01010017, Validation loss: 0.59798315, Gradient norm: 0.34735298
INFO:root:[   17] Training loss: 0.00975285, Validation loss: 0.55308843, Gradient norm: 0.29862205
INFO:root:[   18] Training loss: 0.00937104, Validation loss: 0.60558464, Gradient norm: 0.24559880
INFO:root:[   19] Training loss: 0.00941437, Validation loss: 0.58042977, Gradient norm: 0.30296907
INFO:root:[   20] Training loss: 0.00964388, Validation loss: 0.55400050, Gradient norm: 0.30902435
INFO:root:[   21] Training loss: 0.00910037, Validation loss: 0.54358693, Gradient norm: 0.26693179
INFO:root:[   22] Training loss: 0.00901074, Validation loss: 0.57397166, Gradient norm: 0.28234852
INFO:root:[   23] Training loss: 0.00915206, Validation loss: 0.51114245, Gradient norm: 0.34888603
INFO:root:[   24] Training loss: 0.00880597, Validation loss: 0.52098311, Gradient norm: 0.29861845
INFO:root:[   25] Training loss: 0.00870984, Validation loss: 0.52728103, Gradient norm: 0.28178755
INFO:root:[   26] Training loss: 0.00883020, Validation loss: 0.50191092, Gradient norm: 0.32589896
INFO:root:[   27] Training loss: 0.00885623, Validation loss: 0.50476056, Gradient norm: 0.33229034
INFO:root:[   28] Training loss: 0.00855266, Validation loss: 0.51642591, Gradient norm: 0.30661831
INFO:root:[   29] Training loss: 0.00828606, Validation loss: 0.51010062, Gradient norm: 0.26223289
INFO:root:[   30] Training loss: 0.00874657, Validation loss: 0.73746045, Gradient norm: 0.35219239
INFO:root:[   31] Training loss: 0.00822342, Validation loss: 0.49032253, Gradient norm: 0.29171979
INFO:root:[   32] Training loss: 0.00796698, Validation loss: 0.48489088, Gradient norm: 0.23580203
INFO:root:[   33] Training loss: 0.00822788, Validation loss: 0.56176181, Gradient norm: 0.29938064
INFO:root:[   34] Training loss: 0.00807696, Validation loss: 0.49001768, Gradient norm: 0.28183387
INFO:root:[   35] Training loss: 0.00784374, Validation loss: 0.54136129, Gradient norm: 0.25384351
INFO:root:[   36] Training loss: 0.00824304, Validation loss: 0.48992576, Gradient norm: 0.33593892
INFO:root:[   37] Training loss: 0.00765303, Validation loss: 0.46114256, Gradient norm: 0.24591076
INFO:root:[   38] Training loss: 0.00782916, Validation loss: 0.48972899, Gradient norm: 0.29217046
INFO:root:[   39] Training loss: 0.00773454, Validation loss: 0.54968306, Gradient norm: 0.28671788
INFO:root:[   40] Training loss: 0.00795827, Validation loss: 0.47524158, Gradient norm: 0.32032868
INFO:root:[   41] Training loss: 0.00738649, Validation loss: 0.46115656, Gradient norm: 0.22307199
INFO:root:[   42] Training loss: 0.00758190, Validation loss: 0.47761300, Gradient norm: 0.30068320
INFO:root:[   43] Training loss: 0.00738025, Validation loss: 0.50062964, Gradient norm: 0.25628083
INFO:root:[   44] Training loss: 0.00764206, Validation loss: 0.47075352, Gradient norm: 0.32002221
INFO:root:[   45] Training loss: 0.00729152, Validation loss: 0.48297941, Gradient norm: 0.25892027
INFO:root:[   46] Training loss: 0.00731873, Validation loss: 0.52017029, Gradient norm: 0.25797362
INFO:root:[   47] Training loss: 0.00728802, Validation loss: 0.50610560, Gradient norm: 0.29710313
INFO:root:[   48] Training loss: 0.00709388, Validation loss: 0.47070137, Gradient norm: 0.24597153
INFO:root:[   49] Training loss: 0.00708503, Validation loss: 0.51587937, Gradient norm: 0.25175329
INFO:root:[   50] Training loss: 0.00714147, Validation loss: 0.46761667, Gradient norm: 0.27191163
INFO:root:[   51] Training loss: 0.00694342, Validation loss: 0.45774674, Gradient norm: 0.24544393
INFO:root:[   52] Training loss: 0.00720421, Validation loss: 0.47600852, Gradient norm: 0.30769004
INFO:root:[   53] Training loss: 0.00689461, Validation loss: 0.49624592, Gradient norm: 0.25202996
INFO:root:[   54] Training loss: 0.00710779, Validation loss: 0.49846015, Gradient norm: 0.31014113
INFO:root:[   55] Training loss: 0.00685843, Validation loss: 0.46862871, Gradient norm: 0.27351189
INFO:root:[   56] Training loss: 0.00713199, Validation loss: 0.46067952, Gradient norm: 0.31535935
INFO:root:[   57] Training loss: 0.00676518, Validation loss: 0.46867765, Gradient norm: 0.26303573
INFO:root:[   58] Training loss: 0.00696571, Validation loss: 0.49031163, Gradient norm: 0.30916163
INFO:root:[   59] Training loss: 0.00680451, Validation loss: 0.47637356, Gradient norm: 0.27764592
INFO:root:[   60] Training loss: 0.00666407, Validation loss: 0.45822694, Gradient norm: 0.27965180
INFO:root:[   61] Training loss: 0.00647643, Validation loss: 0.45008585, Gradient norm: 0.22730565
INFO:root:[   62] Training loss: 0.00652127, Validation loss: 0.46006528, Gradient norm: 0.26529115
INFO:root:[   63] Training loss: 0.00654085, Validation loss: 0.46032636, Gradient norm: 0.26270109
INFO:root:[   64] Training loss: 0.00657737, Validation loss: 0.59862867, Gradient norm: 0.28093143
INFO:root:[   65] Training loss: 0.00661574, Validation loss: 0.56334652, Gradient norm: 0.29629767
INFO:root:[   66] Training loss: 0.00666636, Validation loss: 0.49621882, Gradient norm: 0.32192269
INFO:root:[   67] Training loss: 0.00635058, Validation loss: 0.54115544, Gradient norm: 0.24622665
INFO:root:[   68] Training loss: 0.00637021, Validation loss: 0.46883049, Gradient norm: 0.27704558
INFO:root:[   69] Training loss: 0.00617423, Validation loss: 0.45009949, Gradient norm: 0.24075110
INFO:root:[   70] Training loss: 0.00614570, Validation loss: 0.49966158, Gradient norm: 0.23096949
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 929.709s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05571
INFO:root:EnergyScoretrain: 0.04268
INFO:root:Coveragetrain: 6.8908
INFO:root:IntervalWidthtrain: 128.05301
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01909
INFO:root:EnergyScorevalidation: 0.01449
INFO:root:Coveragevalidation: 1.79438
INFO:root:IntervalWidthvalidation: 33.09048
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02987
INFO:root:EnergyScoretest: 0.02335
INFO:root:Coveragetest: 0.60293
INFO:root:IntervalWidthtest: 39.83853
INFO:root:###10 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05246534, Validation loss: 1.60368932, Gradient norm: 0.43049412
INFO:root:[    2] Training loss: 0.02453983, Validation loss: 1.19276505, Gradient norm: 0.31704361
INFO:root:[    3] Training loss: 0.01923576, Validation loss: 1.04923013, Gradient norm: 0.27120656
INFO:root:[    4] Training loss: 0.01741761, Validation loss: 0.96796744, Gradient norm: 0.34137902
INFO:root:[    5] Training loss: 0.01545172, Validation loss: 0.86831927, Gradient norm: 0.24912484
INFO:root:[    6] Training loss: 0.01462043, Validation loss: 0.90739092, Gradient norm: 0.28486893
INFO:root:[    7] Training loss: 0.01325830, Validation loss: 0.75168583, Gradient norm: 0.22597628
INFO:root:[    8] Training loss: 0.01313780, Validation loss: 0.73694110, Gradient norm: 0.29131336
INFO:root:[    9] Training loss: 0.01250889, Validation loss: 0.76202805, Gradient norm: 0.27229506
INFO:root:[   10] Training loss: 0.01182665, Validation loss: 0.66791120, Gradient norm: 0.26484497
INFO:root:[   11] Training loss: 0.01143761, Validation loss: 0.65631423, Gradient norm: 0.24325401
INFO:root:[   12] Training loss: 0.01112513, Validation loss: 0.65583898, Gradient norm: 0.25733739
INFO:root:[   13] Training loss: 0.01081983, Validation loss: 0.65573674, Gradient norm: 0.25790339
INFO:root:[   14] Training loss: 0.01055615, Validation loss: 0.62745476, Gradient norm: 0.24264005
INFO:root:[   15] Training loss: 0.01040821, Validation loss: 0.60625542, Gradient norm: 0.24204559
INFO:root:[   16] Training loss: 0.01000510, Validation loss: 0.65432849, Gradient norm: 0.22636400
INFO:root:[   17] Training loss: 0.01031365, Validation loss: 0.73493053, Gradient norm: 0.30246182
INFO:root:[   18] Training loss: 0.01004364, Validation loss: 0.55513588, Gradient norm: 0.27031565
INFO:root:[   19] Training loss: 0.00945313, Validation loss: 0.55114861, Gradient norm: 0.20603933
INFO:root:[   20] Training loss: 0.00943540, Validation loss: 0.54460879, Gradient norm: 0.24770138
INFO:root:[   21] Training loss: 0.00915002, Validation loss: 0.53476813, Gradient norm: 0.21044851
INFO:root:[   22] Training loss: 0.00958731, Validation loss: 0.52690480, Gradient norm: 0.30394879
INFO:root:[   23] Training loss: 0.00902823, Validation loss: 0.57033310, Gradient norm: 0.24309567
INFO:root:[   24] Training loss: 0.00886639, Validation loss: 0.57023484, Gradient norm: 0.22444398
INFO:root:[   25] Training loss: 0.00882723, Validation loss: 0.51038964, Gradient norm: 0.21676517
INFO:root:[   26] Training loss: 0.00881712, Validation loss: 0.50543904, Gradient norm: 0.24492394
INFO:root:[   27] Training loss: 0.00864594, Validation loss: 0.50733234, Gradient norm: 0.25258982
INFO:root:[   28] Training loss: 0.00837788, Validation loss: 0.56909160, Gradient norm: 0.19803058
INFO:root:[   29] Training loss: 0.00844826, Validation loss: 0.49968492, Gradient norm: 0.22165078
INFO:root:[   30] Training loss: 0.00824333, Validation loss: 0.50565080, Gradient norm: 0.19701398
INFO:root:[   31] Training loss: 0.00860245, Validation loss: 0.53057014, Gradient norm: 0.26529819
INFO:root:[   32] Training loss: 0.00822772, Validation loss: 0.50318616, Gradient norm: 0.22827538
INFO:root:[   33] Training loss: 0.00831559, Validation loss: 0.47730816, Gradient norm: 0.24972975
INFO:root:[   34] Training loss: 0.00822555, Validation loss: 0.50130798, Gradient norm: 0.22185599
INFO:root:[   35] Training loss: 0.00781599, Validation loss: 0.47595104, Gradient norm: 0.20496204
INFO:root:[   36] Training loss: 0.00815778, Validation loss: 0.48475379, Gradient norm: 0.26115388
INFO:root:[   37] Training loss: 0.00772445, Validation loss: 0.47759710, Gradient norm: 0.20732620
INFO:root:[   38] Training loss: 0.00779317, Validation loss: 0.53211623, Gradient norm: 0.20723637
INFO:root:[   39] Training loss: 0.00782256, Validation loss: 0.49101781, Gradient norm: 0.21900962
INFO:root:[   40] Training loss: 0.00768819, Validation loss: 0.47005158, Gradient norm: 0.21911964
INFO:root:[   41] Training loss: 0.00792117, Validation loss: 0.46582809, Gradient norm: 0.26645346
INFO:root:[   42] Training loss: 0.00749810, Validation loss: 0.50983287, Gradient norm: 0.20277394
INFO:root:[   43] Training loss: 0.00740054, Validation loss: 0.46254594, Gradient norm: 0.20022067
INFO:root:[   44] Training loss: 0.00762160, Validation loss: 0.48223866, Gradient norm: 0.24835523
INFO:root:[   45] Training loss: 0.00763048, Validation loss: 0.46477809, Gradient norm: 0.26053524
INFO:root:[   46] Training loss: 0.00746156, Validation loss: 0.45707334, Gradient norm: 0.22114954
INFO:root:[   47] Training loss: 0.00734630, Validation loss: 0.46804953, Gradient norm: 0.21265844
INFO:root:[   48] Training loss: 0.00761639, Validation loss: 0.54536242, Gradient norm: 0.27024505
INFO:root:[   49] Training loss: 0.00744509, Validation loss: 0.45615956, Gradient norm: 0.24448457
INFO:root:[   50] Training loss: 0.00713963, Validation loss: 0.44590347, Gradient norm: 0.21619359
INFO:root:[   51] Training loss: 0.00710291, Validation loss: 0.45065949, Gradient norm: 0.20113230
INFO:root:[   52] Training loss: 0.00723523, Validation loss: 0.48039588, Gradient norm: 0.24407599
INFO:root:[   53] Training loss: 0.00695555, Validation loss: 0.46795805, Gradient norm: 0.21472359
INFO:root:[   54] Training loss: 0.00687802, Validation loss: 0.45943576, Gradient norm: 0.20037447
INFO:root:[   55] Training loss: 0.00684047, Validation loss: 0.50634340, Gradient norm: 0.20249163
INFO:root:[   56] Training loss: 0.00729666, Validation loss: 0.46999061, Gradient norm: 0.28196755
INFO:root:[   57] Training loss: 0.00699775, Validation loss: 0.45718887, Gradient norm: 0.22471359
INFO:root:[   58] Training loss: 0.00711214, Validation loss: 0.52536905, Gradient norm: 0.25077757
INFO:root:[   59] Training loss: 0.00677063, Validation loss: 0.47975048, Gradient norm: 0.19543131
INFO:root:[   60] Training loss: 0.00674392, Validation loss: 0.44794863, Gradient norm: 0.22234200
INFO:root:[   61] Training loss: 0.00697428, Validation loss: 0.45443209, Gradient norm: 0.24970407
INFO:root:[   62] Training loss: 0.00710467, Validation loss: 0.52180207, Gradient norm: 0.27115306
INFO:root:[   63] Training loss: 0.00672207, Validation loss: 0.54447423, Gradient norm: 0.21814201
INFO:root:[   64] Training loss: 0.00673666, Validation loss: 0.44678820, Gradient norm: 0.21492615
INFO:root:[   65] Training loss: 0.00676312, Validation loss: 0.46262936, Gradient norm: 0.25425534
INFO:root:[   66] Training loss: 0.00661452, Validation loss: 0.44361815, Gradient norm: 0.21529533
INFO:root:[   67] Training loss: 0.00652151, Validation loss: 0.45392863, Gradient norm: 0.22995053
INFO:root:[   68] Training loss: 0.00667437, Validation loss: 0.48019617, Gradient norm: 0.23789027
INFO:root:[   69] Training loss: 0.00664262, Validation loss: 0.51847874, Gradient norm: 0.25197074
INFO:root:[   70] Training loss: 0.00644009, Validation loss: 0.44461871, Gradient norm: 0.21518422
INFO:root:[   71] Training loss: 0.00644992, Validation loss: 0.43483855, Gradient norm: 0.23303766
INFO:root:[   72] Training loss: 0.00653534, Validation loss: 0.44906283, Gradient norm: 0.24644859
INFO:root:[   73] Training loss: 0.00627758, Validation loss: 0.44937069, Gradient norm: 0.21796400
INFO:root:[   74] Training loss: 0.00656617, Validation loss: 0.56877591, Gradient norm: 0.26964118
INFO:root:[   75] Training loss: 0.00633078, Validation loss: 0.45525091, Gradient norm: 0.21553718
INFO:root:[   76] Training loss: 0.00627019, Validation loss: 0.46643235, Gradient norm: 0.21068567
INFO:root:[   77] Training loss: 0.00632297, Validation loss: 0.44850248, Gradient norm: 0.23769959
INFO:root:[   78] Training loss: 0.00623294, Validation loss: 0.43512989, Gradient norm: 0.21880806
INFO:root:[   79] Training loss: 0.00642519, Validation loss: 0.46283307, Gradient norm: 0.24704841
INFO:root:[   80] Training loss: 0.00609912, Validation loss: 0.43358550, Gradient norm: 0.21853360
INFO:root:[   81] Training loss: 0.00617309, Validation loss: 0.44414698, Gradient norm: 0.23195894
INFO:root:[   82] Training loss: 0.00607579, Validation loss: 0.46968760, Gradient norm: 0.21049785
INFO:root:[   83] Training loss: 0.00621386, Validation loss: 0.47516312, Gradient norm: 0.24856411
INFO:root:[   84] Training loss: 0.00620386, Validation loss: 0.53210301, Gradient norm: 0.23910643
INFO:root:[   85] Training loss: 0.00604221, Validation loss: 0.44025271, Gradient norm: 0.22683467
INFO:root:[   86] Training loss: 0.00619556, Validation loss: 0.46082376, Gradient norm: 0.23776644
INFO:root:[   87] Training loss: 0.00588214, Validation loss: 0.44785270, Gradient norm: 0.20088813
INFO:root:[   88] Training loss: 0.00594868, Validation loss: 0.46315380, Gradient norm: 0.22871280
INFO:root:[   89] Training loss: 0.00581580, Validation loss: 0.44778844, Gradient norm: 0.21912584
INFO:root:EP 89: Early stopping
INFO:root:Training the model took 1178.514s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05123
INFO:root:EnergyScoretrain: 0.04009
INFO:root:Coveragetrain: 6.97256
INFO:root:IntervalWidthtrain: 151.15192
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01839
INFO:root:EnergyScorevalidation: 0.01384
INFO:root:Coveragevalidation: 1.82641
INFO:root:IntervalWidthvalidation: 39.00666
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01755
INFO:root:EnergyScoretest: 0.01259
INFO:root:Coveragetest: 0.93625
INFO:root:IntervalWidthtest: 43.95569
INFO:root:###11 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05615717, Validation loss: 1.61329854, Gradient norm: 0.39672765
INFO:root:[    2] Training loss: 0.02412634, Validation loss: 1.22123188, Gradient norm: 0.24045648
INFO:root:[    3] Training loss: 0.01930993, Validation loss: 1.01534092, Gradient norm: 0.26021656
INFO:root:[    4] Training loss: 0.01716777, Validation loss: 0.96003267, Gradient norm: 0.25604506
INFO:root:[    5] Training loss: 0.01574428, Validation loss: 0.89783464, Gradient norm: 0.25358892
INFO:root:[    6] Training loss: 0.01491417, Validation loss: 0.94181758, Gradient norm: 0.29014464
INFO:root:[    7] Training loss: 0.01388056, Validation loss: 0.75854625, Gradient norm: 0.27124470
INFO:root:[    8] Training loss: 0.01294132, Validation loss: 0.72163965, Gradient norm: 0.22149612
INFO:root:[    9] Training loss: 0.01260502, Validation loss: 0.69107619, Gradient norm: 0.24832371
INFO:root:[   10] Training loss: 0.01200595, Validation loss: 0.70116880, Gradient norm: 0.21811927
INFO:root:[   11] Training loss: 0.01160918, Validation loss: 0.65902396, Gradient norm: 0.20392017
INFO:root:[   12] Training loss: 0.01133752, Validation loss: 0.66707953, Gradient norm: 0.23872952
INFO:root:[   13] Training loss: 0.01116364, Validation loss: 0.63794291, Gradient norm: 0.23049397
INFO:root:[   14] Training loss: 0.01080567, Validation loss: 0.60799568, Gradient norm: 0.19910613
INFO:root:[   15] Training loss: 0.01076219, Validation loss: 0.60581340, Gradient norm: 0.25236177
INFO:root:[   16] Training loss: 0.01048056, Validation loss: 0.63895690, Gradient norm: 0.23015474
INFO:root:[   17] Training loss: 0.01033449, Validation loss: 0.62736140, Gradient norm: 0.22552353
INFO:root:[   18] Training loss: 0.01007999, Validation loss: 0.57727478, Gradient norm: 0.20898030
INFO:root:[   19] Training loss: 0.00978570, Validation loss: 0.56967650, Gradient norm: 0.19989675
INFO:root:[   20] Training loss: 0.00987319, Validation loss: 0.58988851, Gradient norm: 0.24895616
INFO:root:[   21] Training loss: 0.00981843, Validation loss: 0.59796134, Gradient norm: 0.23285758
INFO:root:[   22] Training loss: 0.00963785, Validation loss: 0.54875464, Gradient norm: 0.24700696
INFO:root:[   23] Training loss: 0.00939585, Validation loss: 0.61110745, Gradient norm: 0.21427662
INFO:root:[   24] Training loss: 0.00940872, Validation loss: 0.53813325, Gradient norm: 0.23525781
INFO:root:[   25] Training loss: 0.00910683, Validation loss: 0.57056898, Gradient norm: 0.20062304
INFO:root:[   26] Training loss: 0.00916290, Validation loss: 0.54201485, Gradient norm: 0.24083231
INFO:root:[   27] Training loss: 0.00914929, Validation loss: 0.52830334, Gradient norm: 0.23736722
INFO:root:[   28] Training loss: 0.00905319, Validation loss: 0.62203559, Gradient norm: 0.24125061
INFO:root:[   29] Training loss: 0.00872711, Validation loss: 0.50725109, Gradient norm: 0.19388347
INFO:root:[   30] Training loss: 0.00897123, Validation loss: 0.50336173, Gradient norm: 0.25351572
INFO:root:[   31] Training loss: 0.00869559, Validation loss: 0.54041627, Gradient norm: 0.20618467
INFO:root:[   32] Training loss: 0.00871923, Validation loss: 0.50970594, Gradient norm: 0.22515881
INFO:root:[   33] Training loss: 0.00842745, Validation loss: 0.51000687, Gradient norm: 0.20283372
INFO:root:[   34] Training loss: 0.00851091, Validation loss: 0.52758037, Gradient norm: 0.23399522
INFO:root:[   35] Training loss: 0.00883722, Validation loss: 0.49821243, Gradient norm: 0.29161668
INFO:root:[   36] Training loss: 0.00835090, Validation loss: 0.48490796, Gradient norm: 0.20855400
INFO:root:[   37] Training loss: 0.00827594, Validation loss: 0.50240072, Gradient norm: 0.21213825
INFO:root:[   38] Training loss: 0.00821633, Validation loss: 0.52702356, Gradient norm: 0.21157221
INFO:root:[   39] Training loss: 0.00795643, Validation loss: 0.49510966, Gradient norm: 0.19314839
INFO:root:[   40] Training loss: 0.00801275, Validation loss: 0.50541145, Gradient norm: 0.22140806
INFO:root:[   41] Training loss: 0.00819750, Validation loss: 0.49534384, Gradient norm: 0.23180113
INFO:root:[   42] Training loss: 0.00781108, Validation loss: 0.48303243, Gradient norm: 0.21104841
INFO:root:[   43] Training loss: 0.00788942, Validation loss: 0.48332971, Gradient norm: 0.20104964
INFO:root:[   44] Training loss: 0.00788757, Validation loss: 0.47324591, Gradient norm: 0.22671828
INFO:root:[   45] Training loss: 0.00784996, Validation loss: 0.46682700, Gradient norm: 0.22420044
INFO:root:[   46] Training loss: 0.00777753, Validation loss: 0.47124756, Gradient norm: 0.21594847
INFO:root:[   47] Training loss: 0.00777123, Validation loss: 0.52825130, Gradient norm: 0.22255234
INFO:root:[   48] Training loss: 0.00788462, Validation loss: 0.47660073, Gradient norm: 0.25605371
INFO:root:[   49] Training loss: 0.00767929, Validation loss: 0.46864387, Gradient norm: 0.24606854
INFO:root:[   50] Training loss: 0.00747519, Validation loss: 0.47176399, Gradient norm: 0.19681076
INFO:root:[   51] Training loss: 0.00758936, Validation loss: 0.46985235, Gradient norm: 0.21370452
INFO:root:[   52] Training loss: 0.00743457, Validation loss: 0.45391040, Gradient norm: 0.21751710
INFO:root:[   53] Training loss: 0.00757852, Validation loss: 0.48376405, Gradient norm: 0.23473880
INFO:root:[   54] Training loss: 0.00745053, Validation loss: 0.49886510, Gradient norm: 0.21213479
INFO:root:[   55] Training loss: 0.00743596, Validation loss: 0.45596435, Gradient norm: 0.24151650
INFO:root:[   56] Training loss: 0.00725624, Validation loss: 0.47290742, Gradient norm: 0.21894674
INFO:root:[   57] Training loss: 0.00738048, Validation loss: 0.53806713, Gradient norm: 0.23769912
INFO:root:[   58] Training loss: 0.00742017, Validation loss: 0.52200742, Gradient norm: 0.24130256
INFO:root:[   59] Training loss: 0.00723399, Validation loss: 0.47697837, Gradient norm: 0.22881331
INFO:root:[   60] Training loss: 0.00721572, Validation loss: 0.50717457, Gradient norm: 0.22770390
INFO:root:[   61] Training loss: 0.00711391, Validation loss: 0.47169002, Gradient norm: 0.22167573
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 804.135s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.06223
INFO:root:EnergyScoretrain: 0.04957
INFO:root:Coveragetrain: 6.99187
INFO:root:IntervalWidthtrain: 190.14848
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01921
INFO:root:EnergyScorevalidation: 0.01478
INFO:root:Coveragevalidation: 1.85378
INFO:root:IntervalWidthvalidation: 49.43092
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01856
INFO:root:EnergyScoretest: 0.01309
INFO:root:Coveragetest: 1.01672
INFO:root:IntervalWidthtest: 57.5744
INFO:root:###12 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04973327, Validation loss: 1.57493880, Gradient norm: 0.30935608
INFO:root:[    2] Training loss: 0.02462918, Validation loss: 1.36959854, Gradient norm: 0.24324076
INFO:root:[    3] Training loss: 0.02042763, Validation loss: 1.05880110, Gradient norm: 0.21384961
INFO:root:[    4] Training loss: 0.01877725, Validation loss: 0.99387694, Gradient norm: 0.29167414
INFO:root:[    5] Training loss: 0.01674632, Validation loss: 0.93467765, Gradient norm: 0.19489471
INFO:root:[    6] Training loss: 0.01599549, Validation loss: 0.87622967, Gradient norm: 0.20607595
INFO:root:[    7] Training loss: 0.01530062, Validation loss: 0.85945614, Gradient norm: 0.23910693
INFO:root:[    8] Training loss: 0.01480174, Validation loss: 0.82365548, Gradient norm: 0.26093643
INFO:root:[    9] Training loss: 0.01399228, Validation loss: 0.77913105, Gradient norm: 0.21844755
INFO:root:[   10] Training loss: 0.01372198, Validation loss: 0.75157664, Gradient norm: 0.24162943
INFO:root:[   11] Training loss: 0.01308872, Validation loss: 0.72451224, Gradient norm: 0.22027118
INFO:root:[   12] Training loss: 0.01283454, Validation loss: 0.78139149, Gradient norm: 0.22929929
INFO:root:[   13] Training loss: 0.01270536, Validation loss: 0.70071671, Gradient norm: 0.22986130
INFO:root:[   14] Training loss: 0.01222052, Validation loss: 0.68278376, Gradient norm: 0.20021157
INFO:root:[   15] Training loss: 0.01194290, Validation loss: 0.73058582, Gradient norm: 0.20294220
INFO:root:[   16] Training loss: 0.01185816, Validation loss: 0.65314647, Gradient norm: 0.21419782
INFO:root:[   17] Training loss: 0.01165938, Validation loss: 0.64792205, Gradient norm: 0.22771090
INFO:root:[   18] Training loss: 0.01152853, Validation loss: 0.65752733, Gradient norm: 0.23175421
INFO:root:[   19] Training loss: 0.01143405, Validation loss: 0.62663449, Gradient norm: 0.23819816
INFO:root:[   20] Training loss: 0.01117053, Validation loss: 0.64914593, Gradient norm: 0.22906958
INFO:root:[   21] Training loss: 0.01108077, Validation loss: 0.61032825, Gradient norm: 0.21109269
INFO:root:[   22] Training loss: 0.01069091, Validation loss: 0.61735744, Gradient norm: 0.18022950
INFO:root:[   23] Training loss: 0.01051863, Validation loss: 0.69112987, Gradient norm: 0.19412318
INFO:root:[   24] Training loss: 0.01066563, Validation loss: 0.60715554, Gradient norm: 0.23177774
INFO:root:[   25] Training loss: 0.01041231, Validation loss: 0.61125091, Gradient norm: 0.18804634
INFO:root:[   26] Training loss: 0.01008156, Validation loss: 0.57093553, Gradient norm: 0.17269692
INFO:root:[   27] Training loss: 0.01036506, Validation loss: 0.58141531, Gradient norm: 0.25491369
INFO:root:[   28] Training loss: 0.01020325, Validation loss: 0.57393287, Gradient norm: 0.19074509
INFO:root:[   29] Training loss: 0.01003706, Validation loss: 0.60032447, Gradient norm: 0.21801184
INFO:root:[   30] Training loss: 0.01012307, Validation loss: 0.57244109, Gradient norm: 0.23577314
INFO:root:[   31] Training loss: 0.00999038, Validation loss: 0.59867680, Gradient norm: 0.22800325
INFO:root:[   32] Training loss: 0.00961325, Validation loss: 0.55832034, Gradient norm: 0.19239399
INFO:root:[   33] Training loss: 0.00966065, Validation loss: 0.57007017, Gradient norm: 0.21698844
INFO:root:[   34] Training loss: 0.00956271, Validation loss: 0.55927570, Gradient norm: 0.19412192
INFO:root:[   35] Training loss: 0.00952987, Validation loss: 0.57138485, Gradient norm: 0.21912116
INFO:root:[   36] Training loss: 0.00958147, Validation loss: 0.54912315, Gradient norm: 0.23549380
INFO:root:[   37] Training loss: 0.00924221, Validation loss: 0.56960874, Gradient norm: 0.20128074
INFO:root:[   38] Training loss: 0.00917557, Validation loss: 0.55924187, Gradient norm: 0.18799148
INFO:root:[   39] Training loss: 0.00933584, Validation loss: 0.56626111, Gradient norm: 0.22980284
INFO:root:[   40] Training loss: 0.00909041, Validation loss: 0.53468747, Gradient norm: 0.22259026
INFO:root:[   41] Training loss: 0.00931284, Validation loss: 0.56174007, Gradient norm: 0.23723751
INFO:root:[   42] Training loss: 0.00906192, Validation loss: 0.56425188, Gradient norm: 0.21382541
INFO:root:[   43] Training loss: 0.00887882, Validation loss: 0.51958383, Gradient norm: 0.16853328
INFO:root:[   44] Training loss: 0.00886999, Validation loss: 0.57708888, Gradient norm: 0.19487444
INFO:root:[   45] Training loss: 0.00897840, Validation loss: 0.55428423, Gradient norm: 0.22419513
INFO:root:[   46] Training loss: 0.00869203, Validation loss: 0.51294184, Gradient norm: 0.18996659
INFO:root:[   47] Training loss: 0.00891197, Validation loss: 0.51580145, Gradient norm: 0.25221002
INFO:root:[   48] Training loss: 0.00868308, Validation loss: 0.52141925, Gradient norm: 0.19648839
INFO:root:[   49] Training loss: 0.00870170, Validation loss: 0.52219246, Gradient norm: 0.22394759
INFO:root:[   50] Training loss: 0.00872453, Validation loss: 0.54641274, Gradient norm: 0.20144754
INFO:root:[   51] Training loss: 0.00893538, Validation loss: 0.50239258, Gradient norm: 0.23508137
INFO:root:[   52] Training loss: 0.00850609, Validation loss: 0.53502982, Gradient norm: 0.21169487
INFO:root:[   53] Training loss: 0.00853328, Validation loss: 0.50662633, Gradient norm: 0.21021344
INFO:root:[   54] Training loss: 0.00853562, Validation loss: 0.50186937, Gradient norm: 0.21983840
INFO:root:[   55] Training loss: 0.00861874, Validation loss: 0.50194792, Gradient norm: 0.26105752
INFO:root:[   56] Training loss: 0.00845170, Validation loss: 0.52961526, Gradient norm: 0.21468429
INFO:root:[   57] Training loss: 0.00845573, Validation loss: 0.49715701, Gradient norm: 0.23006272
INFO:root:[   58] Training loss: 0.00837090, Validation loss: 0.49501507, Gradient norm: 0.21600666
INFO:root:[   59] Training loss: 0.00847126, Validation loss: 0.49693733, Gradient norm: 0.24699537
INFO:root:[   60] Training loss: 0.00830592, Validation loss: 0.48811527, Gradient norm: 0.22407010
INFO:root:[   61] Training loss: 0.00829974, Validation loss: 0.62345705, Gradient norm: 0.22806137
INFO:root:[   62] Training loss: 0.00812692, Validation loss: 0.49575838, Gradient norm: 0.20452254
INFO:root:[   63] Training loss: 0.00821805, Validation loss: 0.53233998, Gradient norm: 0.24066480
INFO:root:[   64] Training loss: 0.00782589, Validation loss: 0.48450061, Gradient norm: 0.16628347
INFO:root:[   65] Training loss: 0.00796755, Validation loss: 0.50833254, Gradient norm: 0.19915690
INFO:root:[   66] Training loss: 0.00811619, Validation loss: 0.48251210, Gradient norm: 0.23689153
INFO:root:[   67] Training loss: 0.00832799, Validation loss: 0.47938901, Gradient norm: 0.26954751
INFO:root:[   68] Training loss: 0.00814811, Validation loss: 0.47433583, Gradient norm: 0.24554464
INFO:root:[   69] Training loss: 0.00778512, Validation loss: 0.49504424, Gradient norm: 0.20051100
INFO:root:[   70] Training loss: 0.00803969, Validation loss: 0.48796637, Gradient norm: 0.22341495
INFO:root:[   71] Training loss: 0.00767769, Validation loss: 0.49135777, Gradient norm: 0.18886875
INFO:root:[   72] Training loss: 0.00806452, Validation loss: 0.49353322, Gradient norm: 0.26252567
INFO:root:[   73] Training loss: 0.00768898, Validation loss: 0.48502143, Gradient norm: 0.21062577
INFO:root:[   74] Training loss: 0.00761180, Validation loss: 0.49681135, Gradient norm: 0.20319152
INFO:root:[   75] Training loss: 0.00791173, Validation loss: 0.47955643, Gradient norm: 0.26195107
INFO:root:[   76] Training loss: 0.00771344, Validation loss: 0.53076939, Gradient norm: 0.23615239
INFO:root:[   77] Training loss: 0.00762193, Validation loss: 0.48532672, Gradient norm: 0.18368847
INFO:root:EP 77: Early stopping
INFO:root:Training the model took 1018.72s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.06478
INFO:root:EnergyScoretrain: 0.05322
INFO:root:Coveragetrain: 7.00196
INFO:root:IntervalWidthtrain: 214.40877
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.0197
INFO:root:EnergyScorevalidation: 0.01555
INFO:root:Coveragevalidation: 1.86001
INFO:root:IntervalWidthvalidation: 55.2284
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01708
INFO:root:EnergyScoretest: 0.01203
INFO:root:Coveragetest: 1.05421
INFO:root:IntervalWidthtest: 64.10136
INFO:root:###13 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04932758, Validation loss: 1.72063638, Gradient norm: 0.36981228
INFO:root:[    2] Training loss: 0.02306691, Validation loss: 1.18450846, Gradient norm: 0.28759709
INFO:root:[    3] Training loss: 0.01810345, Validation loss: 1.00255847, Gradient norm: 0.29231831
INFO:root:[    4] Training loss: 0.01628905, Validation loss: 0.94017527, Gradient norm: 0.28596391
INFO:root:[    5] Training loss: 0.01548478, Validation loss: 0.82478644, Gradient norm: 0.34770702
INFO:root:[    6] Training loss: 0.01380512, Validation loss: 0.79170503, Gradient norm: 0.22529156
INFO:root:[    7] Training loss: 0.01314352, Validation loss: 0.73849807, Gradient norm: 0.27626870
INFO:root:[    8] Training loss: 0.01263185, Validation loss: 0.69829192, Gradient norm: 0.29917357
INFO:root:[    9] Training loss: 0.01175820, Validation loss: 0.66942492, Gradient norm: 0.25020688
INFO:root:[   10] Training loss: 0.01146477, Validation loss: 0.79802335, Gradient norm: 0.27774286
INFO:root:[   11] Training loss: 0.01084055, Validation loss: 0.60857744, Gradient norm: 0.22482730
INFO:root:[   12] Training loss: 0.01073433, Validation loss: 0.62318745, Gradient norm: 0.27544036
INFO:root:[   13] Training loss: 0.01063633, Validation loss: 0.75943975, Gradient norm: 0.29762756
INFO:root:[   14] Training loss: 0.01003201, Validation loss: 0.63792697, Gradient norm: 0.21408142
INFO:root:[   15] Training loss: 0.00997516, Validation loss: 0.62209514, Gradient norm: 0.25904345
INFO:root:[   16] Training loss: 0.01002172, Validation loss: 0.56833319, Gradient norm: 0.29754455
INFO:root:[   17] Training loss: 0.00968676, Validation loss: 0.62311098, Gradient norm: 0.27172737
INFO:root:[   18] Training loss: 0.00954456, Validation loss: 0.59455655, Gradient norm: 0.27905604
INFO:root:[   19] Training loss: 0.00930923, Validation loss: 0.57950120, Gradient norm: 0.25570097
INFO:root:[   20] Training loss: 0.00913150, Validation loss: 0.54260779, Gradient norm: 0.24414018
INFO:root:[   21] Training loss: 0.00927429, Validation loss: 0.53933441, Gradient norm: 0.30612834
INFO:root:[   22] Training loss: 0.00898941, Validation loss: 0.57961218, Gradient norm: 0.27051296
INFO:root:[   23] Training loss: 0.00879324, Validation loss: 0.54355729, Gradient norm: 0.25769941
INFO:root:[   24] Training loss: 0.00888045, Validation loss: 0.55651356, Gradient norm: 0.30169353
INFO:root:[   25] Training loss: 0.00860678, Validation loss: 0.53246828, Gradient norm: 0.24872391
INFO:root:[   26] Training loss: 0.00865430, Validation loss: 0.51421011, Gradient norm: 0.28035998
INFO:root:[   27] Training loss: 0.00866561, Validation loss: 0.56358633, Gradient norm: 0.29393033
INFO:root:[   28] Training loss: 0.00848189, Validation loss: 0.51200100, Gradient norm: 0.27648355
INFO:root:[   29] Training loss: 0.00815422, Validation loss: 0.50413930, Gradient norm: 0.21557579
INFO:root:[   30] Training loss: 0.00844977, Validation loss: 0.51605063, Gradient norm: 0.30480112
INFO:root:[   31] Training loss: 0.00815808, Validation loss: 0.58416850, Gradient norm: 0.26296262
INFO:root:[   32] Training loss: 0.00793347, Validation loss: 0.54900836, Gradient norm: 0.22616853
INFO:root:[   33] Training loss: 0.00783668, Validation loss: 0.53697236, Gradient norm: 0.22965470
INFO:root:[   34] Training loss: 0.00806518, Validation loss: 0.55736920, Gradient norm: 0.31851327
INFO:root:[   35] Training loss: 0.00798704, Validation loss: 0.50390168, Gradient norm: 0.29652375
INFO:root:[   36] Training loss: 0.00761685, Validation loss: 0.49119228, Gradient norm: 0.22727538
INFO:root:[   37] Training loss: 0.00809562, Validation loss: 0.50932214, Gradient norm: 0.32006043
INFO:root:[   38] Training loss: 0.00765984, Validation loss: 0.47073874, Gradient norm: 0.25869477
INFO:root:[   39] Training loss: 0.00755258, Validation loss: 0.54123861, Gradient norm: 0.24933986
INFO:root:[   40] Training loss: 0.00797110, Validation loss: 0.49366380, Gradient norm: 0.32384747
INFO:root:[   41] Training loss: 0.00756230, Validation loss: 0.49488913, Gradient norm: 0.25822922
INFO:root:[   42] Training loss: 0.00736628, Validation loss: 0.48119933, Gradient norm: 0.25411951
INFO:root:[   43] Training loss: 0.00761855, Validation loss: 0.50906356, Gradient norm: 0.30224795
INFO:root:[   44] Training loss: 0.00725782, Validation loss: 0.49345977, Gradient norm: 0.24238560
INFO:root:[   45] Training loss: 0.00728043, Validation loss: 0.52482034, Gradient norm: 0.26427447
INFO:root:[   46] Training loss: 0.00731949, Validation loss: 0.50059450, Gradient norm: 0.28393160
INFO:root:[   47] Training loss: 0.00710102, Validation loss: 0.49056768, Gradient norm: 0.24299093
INFO:root:[   48] Training loss: 0.00738834, Validation loss: 0.46355916, Gradient norm: 0.30633165
INFO:root:[   49] Training loss: 0.00724965, Validation loss: 0.46692208, Gradient norm: 0.28800884
INFO:root:[   50] Training loss: 0.00705909, Validation loss: 0.46849964, Gradient norm: 0.25428127
INFO:root:[   51] Training loss: 0.00712947, Validation loss: 0.49046487, Gradient norm: 0.27038986
INFO:root:[   52] Training loss: 0.00688119, Validation loss: 0.46899358, Gradient norm: 0.25166246
INFO:root:[   53] Training loss: 0.00680706, Validation loss: 0.47781534, Gradient norm: 0.24037607
INFO:root:[   54] Training loss: 0.00690999, Validation loss: 0.48944495, Gradient norm: 0.25824748
INFO:root:[   55] Training loss: 0.00682634, Validation loss: 0.47057249, Gradient norm: 0.27568242
INFO:root:[   56] Training loss: 0.00670214, Validation loss: 0.47705511, Gradient norm: 0.24605756
INFO:root:[   57] Training loss: 0.00706612, Validation loss: 0.54463475, Gradient norm: 0.34166310
INFO:root:[   58] Training loss: 0.00680064, Validation loss: 0.47900495, Gradient norm: 0.30195506
INFO:root:[   59] Training loss: 0.00670184, Validation loss: 0.48175069, Gradient norm: 0.27413835
INFO:root:[   60] Training loss: 0.00642506, Validation loss: 0.46150121, Gradient norm: 0.21766894
INFO:root:[   61] Training loss: 0.00686020, Validation loss: 0.46525404, Gradient norm: 0.30356136
INFO:root:[   62] Training loss: 0.00657393, Validation loss: 0.50547921, Gradient norm: 0.27875462
INFO:root:[   63] Training loss: 0.00644510, Validation loss: 0.50744486, Gradient norm: 0.26243040
INFO:root:[   64] Training loss: 0.00661672, Validation loss: 0.47978913, Gradient norm: 0.31457635
INFO:root:[   65] Training loss: 0.00624592, Validation loss: 0.47055638, Gradient norm: 0.22635993
INFO:root:[   66] Training loss: 0.00641209, Validation loss: 0.48672197, Gradient norm: 0.26789721
INFO:root:[   67] Training loss: 0.00655164, Validation loss: 0.48156789, Gradient norm: 0.31244420
INFO:root:[   68] Training loss: 0.00615227, Validation loss: 0.49390529, Gradient norm: 0.24391347
INFO:root:[   69] Training loss: 0.00629663, Validation loss: 0.46747457, Gradient norm: 0.27788435
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 931.204s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05765
INFO:root:EnergyScoretrain: 0.04422
INFO:root:Coveragetrain: 6.8819
INFO:root:IntervalWidthtrain: 131.64165
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01938
INFO:root:EnergyScorevalidation: 0.0147
INFO:root:Coveragevalidation: 1.79341
INFO:root:IntervalWidthvalidation: 33.7961
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02519
INFO:root:EnergyScoretest: 0.01913
INFO:root:Coveragetest: 0.72715
INFO:root:IntervalWidthtest: 41.08719
INFO:root:###14 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05716585, Validation loss: 1.75330049, Gradient norm: 0.39268341
INFO:root:[    2] Training loss: 0.02436538, Validation loss: 1.21534428, Gradient norm: 0.28213368
INFO:root:[    3] Training loss: 0.01945498, Validation loss: 1.01920246, Gradient norm: 0.26434090
INFO:root:[    4] Training loss: 0.01691099, Validation loss: 0.94912907, Gradient norm: 0.19450992
INFO:root:[    5] Training loss: 0.01580454, Validation loss: 0.89852381, Gradient norm: 0.24752068
INFO:root:[    6] Training loss: 0.01457712, Validation loss: 0.84007262, Gradient norm: 0.22404303
INFO:root:[    7] Training loss: 0.01357773, Validation loss: 0.84242067, Gradient norm: 0.20173990
INFO:root:[    8] Training loss: 0.01281688, Validation loss: 0.71271511, Gradient norm: 0.22958815
INFO:root:[    9] Training loss: 0.01232058, Validation loss: 0.70749303, Gradient norm: 0.23842999
INFO:root:[   10] Training loss: 0.01188732, Validation loss: 0.66550371, Gradient norm: 0.23130647
INFO:root:[   11] Training loss: 0.01152062, Validation loss: 0.70182997, Gradient norm: 0.22892969
INFO:root:[   12] Training loss: 0.01155702, Validation loss: 0.62617580, Gradient norm: 0.29265629
INFO:root:[   13] Training loss: 0.01060356, Validation loss: 0.61125367, Gradient norm: 0.17446971
INFO:root:[   14] Training loss: 0.01045772, Validation loss: 0.58700902, Gradient norm: 0.23306194
INFO:root:[   15] Training loss: 0.01011491, Validation loss: 0.59866070, Gradient norm: 0.19662112
INFO:root:[   16] Training loss: 0.01013664, Validation loss: 0.57423824, Gradient norm: 0.24534726
INFO:root:[   17] Training loss: 0.00982296, Validation loss: 0.64959118, Gradient norm: 0.22664831
INFO:root:[   18] Training loss: 0.00957919, Validation loss: 0.60850292, Gradient norm: 0.19499358
INFO:root:[   19] Training loss: 0.00958688, Validation loss: 0.57315819, Gradient norm: 0.23056062
INFO:root:[   20] Training loss: 0.00930803, Validation loss: 0.57136473, Gradient norm: 0.21756056
INFO:root:[   21] Training loss: 0.00926514, Validation loss: 0.57601362, Gradient norm: 0.22719209
INFO:root:[   22] Training loss: 0.00891567, Validation loss: 0.53813068, Gradient norm: 0.18350318
INFO:root:[   23] Training loss: 0.00905059, Validation loss: 0.52322602, Gradient norm: 0.22373747
INFO:root:[   24] Training loss: 0.00894133, Validation loss: 0.53405476, Gradient norm: 0.23489216
INFO:root:[   25] Training loss: 0.00895155, Validation loss: 0.51780141, Gradient norm: 0.23585170
INFO:root:[   26] Training loss: 0.00895909, Validation loss: 0.54737711, Gradient norm: 0.26007959
INFO:root:[   27] Training loss: 0.00872483, Validation loss: 0.49822042, Gradient norm: 0.24815118
INFO:root:[   28] Training loss: 0.00848376, Validation loss: 0.50375914, Gradient norm: 0.20497565
INFO:root:[   29] Training loss: 0.00818602, Validation loss: 0.50952161, Gradient norm: 0.18874384
INFO:root:[   30] Training loss: 0.00830051, Validation loss: 0.50224730, Gradient norm: 0.22278099
INFO:root:[   31] Training loss: 0.00815863, Validation loss: 0.49521607, Gradient norm: 0.20917538
INFO:root:[   32] Training loss: 0.00831262, Validation loss: 0.50691922, Gradient norm: 0.23959436
INFO:root:[   33] Training loss: 0.00809418, Validation loss: 0.48950351, Gradient norm: 0.24475089
INFO:root:[   34] Training loss: 0.00820593, Validation loss: 0.56760946, Gradient norm: 0.27046332
INFO:root:[   35] Training loss: 0.00787588, Validation loss: 0.49894813, Gradient norm: 0.20095009
INFO:root:[   36] Training loss: 0.00811069, Validation loss: 0.50598574, Gradient norm: 0.26777270
INFO:root:[   37] Training loss: 0.00791771, Validation loss: 0.47190495, Gradient norm: 0.23818351
INFO:root:[   38] Training loss: 0.00794780, Validation loss: 0.47588048, Gradient norm: 0.24599037
INFO:root:[   39] Training loss: 0.00767735, Validation loss: 0.48029102, Gradient norm: 0.21884390
INFO:root:[   40] Training loss: 0.00773676, Validation loss: 0.45777725, Gradient norm: 0.24659640
INFO:root:[   41] Training loss: 0.00761946, Validation loss: 0.46860430, Gradient norm: 0.22179430
INFO:root:[   42] Training loss: 0.00746380, Validation loss: 0.47552336, Gradient norm: 0.21358696
INFO:root:[   43] Training loss: 0.00768308, Validation loss: 0.46546046, Gradient norm: 0.25554968
INFO:root:[   44] Training loss: 0.00758880, Validation loss: 0.47200642, Gradient norm: 0.23701446
INFO:root:[   45] Training loss: 0.00746206, Validation loss: 0.46819741, Gradient norm: 0.23116974
INFO:root:[   46] Training loss: 0.00761702, Validation loss: 0.47720562, Gradient norm: 0.24883078
INFO:root:[   47] Training loss: 0.00738616, Validation loss: 0.47581843, Gradient norm: 0.23300358
INFO:root:[   48] Training loss: 0.00730252, Validation loss: 0.47890975, Gradient norm: 0.23606901
INFO:root:[   49] Training loss: 0.00726124, Validation loss: 0.49929991, Gradient norm: 0.24092768
INFO:root:[   50] Training loss: 0.00734493, Validation loss: 0.45198257, Gradient norm: 0.26101838
INFO:root:[   51] Training loss: 0.00709863, Validation loss: 0.58221956, Gradient norm: 0.21785512
INFO:root:[   52] Training loss: 0.00722446, Validation loss: 0.47422062, Gradient norm: 0.25206071
INFO:root:[   53] Training loss: 0.00701647, Validation loss: 0.45443079, Gradient norm: 0.21027703
INFO:root:[   54] Training loss: 0.00722803, Validation loss: 0.52766046, Gradient norm: 0.26598059
INFO:root:[   55] Training loss: 0.00710514, Validation loss: 0.44714457, Gradient norm: 0.24352294
INFO:root:[   56] Training loss: 0.00690371, Validation loss: 0.50876893, Gradient norm: 0.22416044
INFO:root:[   57] Training loss: 0.00700957, Validation loss: 0.49339863, Gradient norm: 0.23926712
INFO:root:[   58] Training loss: 0.00706372, Validation loss: 0.44240406, Gradient norm: 0.25318459
INFO:root:[   59] Training loss: 0.00696380, Validation loss: 0.49556396, Gradient norm: 0.23846637
INFO:root:[   60] Training loss: 0.00687068, Validation loss: 0.50903397, Gradient norm: 0.23113935
INFO:root:[   61] Training loss: 0.00675459, Validation loss: 0.43899405, Gradient norm: 0.21585274
INFO:root:[   62] Training loss: 0.00665886, Validation loss: 0.44445556, Gradient norm: 0.21808999
INFO:root:[   63] Training loss: 0.00681037, Validation loss: 0.44730900, Gradient norm: 0.23686316
INFO:root:[   64] Training loss: 0.00676527, Validation loss: 0.46222013, Gradient norm: 0.23442103
INFO:root:[   65] Training loss: 0.00688084, Validation loss: 0.49693422, Gradient norm: 0.25279575
INFO:root:[   66] Training loss: 0.00658820, Validation loss: 0.45287373, Gradient norm: 0.22291358
INFO:root:[   67] Training loss: 0.00651038, Validation loss: 0.44188104, Gradient norm: 0.21254563
INFO:root:[   68] Training loss: 0.00661574, Validation loss: 0.46832385, Gradient norm: 0.22596212
INFO:root:[   69] Training loss: 0.00651879, Validation loss: 0.46194161, Gradient norm: 0.23018941
INFO:root:[   70] Training loss: 0.00646643, Validation loss: 0.43518269, Gradient norm: 0.20693861
INFO:root:[   71] Training loss: 0.00663422, Validation loss: 0.44550361, Gradient norm: 0.26976618
INFO:root:[   72] Training loss: 0.00639626, Validation loss: 0.43959087, Gradient norm: 0.21141673
INFO:root:[   73] Training loss: 0.00658889, Validation loss: 0.46144291, Gradient norm: 0.27153953
INFO:root:[   74] Training loss: 0.00644313, Validation loss: 0.49101304, Gradient norm: 0.22055764
INFO:root:[   75] Training loss: 0.00633609, Validation loss: 0.44953371, Gradient norm: 0.24913159
INFO:root:[   76] Training loss: 0.00643508, Validation loss: 0.49472593, Gradient norm: 0.23825558
INFO:root:[   77] Training loss: 0.00644765, Validation loss: 0.44700222, Gradient norm: 0.25419769
INFO:root:[   78] Training loss: 0.00642809, Validation loss: 0.45475810, Gradient norm: 0.24535698
INFO:root:[   79] Training loss: 0.00604873, Validation loss: 0.43249153, Gradient norm: 0.19990318
INFO:root:[   80] Training loss: 0.00627521, Validation loss: 0.46009254, Gradient norm: 0.23565882
INFO:root:[   81] Training loss: 0.00636020, Validation loss: 0.46000178, Gradient norm: 0.26530983
INFO:root:[   82] Training loss: 0.00633913, Validation loss: 0.43436532, Gradient norm: 0.26199271
INFO:root:[   83] Training loss: 0.00625932, Validation loss: 0.46020448, Gradient norm: 0.26410452
INFO:root:[   84] Training loss: 0.00605414, Validation loss: 0.44460203, Gradient norm: 0.22606226
INFO:root:[   85] Training loss: 0.00598348, Validation loss: 0.43970216, Gradient norm: 0.21062668
INFO:root:[   86] Training loss: 0.00621821, Validation loss: 0.46043006, Gradient norm: 0.25842009
INFO:root:[   87] Training loss: 0.00600327, Validation loss: 0.44078263, Gradient norm: 0.20844811
INFO:root:[   88] Training loss: 0.00607398, Validation loss: 0.45165640, Gradient norm: 0.24123961
INFO:root:EP 88: Early stopping
INFO:root:Training the model took 1153.342s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05057
INFO:root:EnergyScoretrain: 0.04042
INFO:root:Coveragetrain: 6.97558
INFO:root:IntervalWidthtrain: 155.06048
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01906
INFO:root:EnergyScorevalidation: 0.01442
INFO:root:Coveragevalidation: 1.82682
INFO:root:IntervalWidthvalidation: 40.31018
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01957
INFO:root:EnergyScoretest: 0.01404
INFO:root:Coveragetest: 0.8488
INFO:root:IntervalWidthtest: 47.10824
INFO:root:###15 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05455914, Validation loss: 1.68656075, Gradient norm: 0.32212402
INFO:root:[    2] Training loss: 0.02527994, Validation loss: 1.25892277, Gradient norm: 0.22502982
INFO:root:[    3] Training loss: 0.02031219, Validation loss: 1.07780383, Gradient norm: 0.23298198
INFO:root:[    4] Training loss: 0.01765151, Validation loss: 0.98259461, Gradient norm: 0.19589421
INFO:root:[    5] Training loss: 0.01669700, Validation loss: 0.90417188, Gradient norm: 0.26295595
INFO:root:[    6] Training loss: 0.01550482, Validation loss: 0.85355729, Gradient norm: 0.25363446
INFO:root:[    7] Training loss: 0.01458357, Validation loss: 0.79395374, Gradient norm: 0.24508864
INFO:root:[    8] Training loss: 0.01365616, Validation loss: 0.75888743, Gradient norm: 0.19006230
INFO:root:[    9] Training loss: 0.01310273, Validation loss: 0.73825863, Gradient norm: 0.22635961
INFO:root:[   10] Training loss: 0.01264905, Validation loss: 0.72353716, Gradient norm: 0.22109958
INFO:root:[   11] Training loss: 0.01226920, Validation loss: 0.83682070, Gradient norm: 0.23011174
INFO:root:[   12] Training loss: 0.01193887, Validation loss: 0.71957500, Gradient norm: 0.24937690
INFO:root:[   13] Training loss: 0.01177289, Validation loss: 0.65153452, Gradient norm: 0.26019970
INFO:root:[   14] Training loss: 0.01107503, Validation loss: 0.62458634, Gradient norm: 0.19045617
INFO:root:[   15] Training loss: 0.01090899, Validation loss: 0.64165007, Gradient norm: 0.19380674
INFO:root:[   16] Training loss: 0.01084129, Validation loss: 0.62255705, Gradient norm: 0.21697143
INFO:root:[   17] Training loss: 0.01064672, Validation loss: 0.61707380, Gradient norm: 0.21761128
INFO:root:[   18] Training loss: 0.01053914, Validation loss: 0.61499214, Gradient norm: 0.23917318
INFO:root:[   19] Training loss: 0.01003797, Validation loss: 0.64432386, Gradient norm: 0.18608621
INFO:root:[   20] Training loss: 0.01033672, Validation loss: 0.62387134, Gradient norm: 0.25188192
INFO:root:[   21] Training loss: 0.01016131, Validation loss: 0.56816087, Gradient norm: 0.26550331
INFO:root:[   22] Training loss: 0.00968386, Validation loss: 0.59151032, Gradient norm: 0.20237101
INFO:root:[   23] Training loss: 0.00964883, Validation loss: 0.67474843, Gradient norm: 0.21089265
INFO:root:[   24] Training loss: 0.00966138, Validation loss: 0.55515560, Gradient norm: 0.22270446
INFO:root:[   25] Training loss: 0.00958239, Validation loss: 0.62260438, Gradient norm: 0.24034831
INFO:root:[   26] Training loss: 0.00929909, Validation loss: 0.54220860, Gradient norm: 0.19510262
INFO:root:[   27] Training loss: 0.00938329, Validation loss: 0.53320621, Gradient norm: 0.25137717
INFO:root:[   28] Training loss: 0.00918908, Validation loss: 0.52698001, Gradient norm: 0.23741964
INFO:root:[   29] Training loss: 0.00919491, Validation loss: 0.53127403, Gradient norm: 0.25397565
INFO:root:[   30] Training loss: 0.00898063, Validation loss: 0.53200364, Gradient norm: 0.22083885
INFO:root:[   31] Training loss: 0.00887884, Validation loss: 0.56197053, Gradient norm: 0.18466569
INFO:root:[   32] Training loss: 0.00904417, Validation loss: 0.52596781, Gradient norm: 0.25098245
INFO:root:[   33] Training loss: 0.00899634, Validation loss: 0.58514168, Gradient norm: 0.27187179
INFO:root:[   34] Training loss: 0.00858965, Validation loss: 0.52655710, Gradient norm: 0.21358138
INFO:root:[   35] Training loss: 0.00902566, Validation loss: 0.50109122, Gradient norm: 0.28986162
INFO:root:[   36] Training loss: 0.00872047, Validation loss: 0.59104346, Gradient norm: 0.21935395
INFO:root:[   37] Training loss: 0.00857056, Validation loss: 0.50024154, Gradient norm: 0.24329019
INFO:root:[   38] Training loss: 0.00848271, Validation loss: 0.52013082, Gradient norm: 0.24552030
INFO:root:[   39] Training loss: 0.00855269, Validation loss: 0.50040774, Gradient norm: 0.23424428
INFO:root:[   40] Training loss: 0.00829719, Validation loss: 0.48902694, Gradient norm: 0.21557795
INFO:root:[   41] Training loss: 0.00846804, Validation loss: 0.49823449, Gradient norm: 0.27309739
INFO:root:[   42] Training loss: 0.00822111, Validation loss: 0.49246478, Gradient norm: 0.22851575
INFO:root:[   43] Training loss: 0.00814072, Validation loss: 0.51157914, Gradient norm: 0.23378442
INFO:root:[   44] Training loss: 0.00831526, Validation loss: 0.47636196, Gradient norm: 0.26731208
INFO:root:[   45] Training loss: 0.00821561, Validation loss: 0.49801485, Gradient norm: 0.25660806
INFO:root:[   46] Training loss: 0.00812219, Validation loss: 0.49067803, Gradient norm: 0.24612338
INFO:root:[   47] Training loss: 0.00805747, Validation loss: 0.52893229, Gradient norm: 0.24827392
INFO:root:[   48] Training loss: 0.00807594, Validation loss: 0.49303988, Gradient norm: 0.27204495
INFO:root:[   49] Training loss: 0.00795636, Validation loss: 0.49835357, Gradient norm: 0.22678480
INFO:root:[   50] Training loss: 0.00777275, Validation loss: 0.49161909, Gradient norm: 0.21921034
INFO:root:[   51] Training loss: 0.00799441, Validation loss: 0.52725380, Gradient norm: 0.25073886
INFO:root:[   52] Training loss: 0.00785617, Validation loss: 0.50993822, Gradient norm: 0.24449810
INFO:root:[   53] Training loss: 0.00803381, Validation loss: 0.47897804, Gradient norm: 0.27476779
INFO:root:[   54] Training loss: 0.00781263, Validation loss: 0.48370088, Gradient norm: 0.25036366
INFO:root:[   55] Training loss: 0.00797141, Validation loss: 0.48854649, Gradient norm: 0.29912123
INFO:root:[   56] Training loss: 0.00757063, Validation loss: 0.47201998, Gradient norm: 0.23521329
INFO:root:[   57] Training loss: 0.00761547, Validation loss: 0.48885321, Gradient norm: 0.24896337
INFO:root:[   58] Training loss: 0.00755532, Validation loss: 0.46586614, Gradient norm: 0.23284097
INFO:root:[   59] Training loss: 0.00757745, Validation loss: 0.47841113, Gradient norm: 0.26534306
INFO:root:[   60] Training loss: 0.00733003, Validation loss: 0.48265794, Gradient norm: 0.21239011
INFO:root:[   61] Training loss: 0.00738645, Validation loss: 0.49011442, Gradient norm: 0.23476587
INFO:root:[   62] Training loss: 0.00784469, Validation loss: 0.58663914, Gradient norm: 0.32887413
INFO:root:[   63] Training loss: 0.00745364, Validation loss: 0.47566213, Gradient norm: 0.24949200
INFO:root:[   64] Training loss: 0.00741171, Validation loss: 0.46186790, Gradient norm: 0.24623656
INFO:root:[   65] Training loss: 0.00751545, Validation loss: 0.53906095, Gradient norm: 0.27644401
INFO:root:[   66] Training loss: 0.00743844, Validation loss: 0.46078729, Gradient norm: 0.27369755
INFO:root:[   67] Training loss: 0.00716926, Validation loss: 0.46298138, Gradient norm: 0.22618716
INFO:root:[   68] Training loss: 0.00758174, Validation loss: 0.62683959, Gradient norm: 0.29348292
INFO:root:[   69] Training loss: 0.00720521, Validation loss: 0.48375697, Gradient norm: 0.23162890
INFO:root:[   70] Training loss: 0.00719742, Validation loss: 0.45874672, Gradient norm: 0.25312442
INFO:root:[   71] Training loss: 0.00687047, Validation loss: 0.44928725, Gradient norm: 0.19744584
INFO:root:[   72] Training loss: 0.00738607, Validation loss: 0.49872825, Gradient norm: 0.31446377
INFO:root:[   73] Training loss: 0.00713791, Validation loss: 0.45912156, Gradient norm: 0.23427047
INFO:root:[   74] Training loss: 0.00701425, Validation loss: 0.51977128, Gradient norm: 0.24560343
INFO:root:[   75] Training loss: 0.00705868, Validation loss: 0.46836545, Gradient norm: 0.26164817
INFO:root:[   76] Training loss: 0.00702353, Validation loss: 0.48062964, Gradient norm: 0.23822816
INFO:root:[   77] Training loss: 0.00679247, Validation loss: 0.49809186, Gradient norm: 0.20862953
INFO:root:[   78] Training loss: 0.00707312, Validation loss: 0.47153327, Gradient norm: 0.25812642
INFO:root:[   79] Training loss: 0.00687553, Validation loss: 0.45829248, Gradient norm: 0.23569227
INFO:root:[   80] Training loss: 0.00684340, Validation loss: 0.47769690, Gradient norm: 0.25347945
INFO:root:EP 80: Early stopping
INFO:root:Training the model took 1060.411s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05879
INFO:root:EnergyScoretrain: 0.0475
INFO:root:Coveragetrain: 6.99441
INFO:root:IntervalWidthtrain: 188.65512
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02009
INFO:root:EnergyScorevalidation: 0.01554
INFO:root:Coveragevalidation: 1.84929
INFO:root:IntervalWidthvalidation: 50.13465
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01552
INFO:root:EnergyScoretest: 0.01103
INFO:root:Coveragetest: 1.04504
INFO:root:IntervalWidthtest: 57.30003
INFO:root:###16 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04857104, Validation loss: 1.55973685, Gradient norm: 0.28600266
INFO:root:[    2] Training loss: 0.02509389, Validation loss: 1.23313983, Gradient norm: 0.23718031
INFO:root:[    3] Training loss: 0.02045074, Validation loss: 1.07874038, Gradient norm: 0.21542917
INFO:root:[    4] Training loss: 0.01818827, Validation loss: 0.98275371, Gradient norm: 0.20559289
INFO:root:[    5] Training loss: 0.01694663, Validation loss: 0.93513290, Gradient norm: 0.20509763
INFO:root:[    6] Training loss: 0.01582128, Validation loss: 0.89255348, Gradient norm: 0.20755729
INFO:root:[    7] Training loss: 0.01502950, Validation loss: 0.88681512, Gradient norm: 0.22547702
INFO:root:[    8] Training loss: 0.01418749, Validation loss: 0.81132754, Gradient norm: 0.18344875
INFO:root:[    9] Training loss: 0.01382160, Validation loss: 0.80354482, Gradient norm: 0.22371994
INFO:root:[   10] Training loss: 0.01337299, Validation loss: 0.76527950, Gradient norm: 0.19968423
INFO:root:[   11] Training loss: 0.01291131, Validation loss: 0.73222336, Gradient norm: 0.20527221
INFO:root:[   12] Training loss: 0.01254585, Validation loss: 0.69353606, Gradient norm: 0.20586977
INFO:root:[   13] Training loss: 0.01229943, Validation loss: 0.69735625, Gradient norm: 0.21799263
INFO:root:[   14] Training loss: 0.01169597, Validation loss: 0.66954814, Gradient norm: 0.17850500
INFO:root:[   15] Training loss: 0.01176769, Validation loss: 0.70949331, Gradient norm: 0.21542218
INFO:root:[   16] Training loss: 0.01156121, Validation loss: 0.68712401, Gradient norm: 0.23320121
INFO:root:[   17] Training loss: 0.01117696, Validation loss: 0.64479082, Gradient norm: 0.18036743
INFO:root:[   18] Training loss: 0.01109252, Validation loss: 0.65487619, Gradient norm: 0.21805944
INFO:root:[   19] Training loss: 0.01080661, Validation loss: 0.62649501, Gradient norm: 0.19911330
INFO:root:[   20] Training loss: 0.01082561, Validation loss: 0.61778021, Gradient norm: 0.20585082
INFO:root:[   21] Training loss: 0.01062947, Validation loss: 0.58967703, Gradient norm: 0.21664128
INFO:root:[   22] Training loss: 0.01046128, Validation loss: 0.67711380, Gradient norm: 0.22721936
INFO:root:[   23] Training loss: 0.01019610, Validation loss: 0.59225241, Gradient norm: 0.18974680
INFO:root:[   24] Training loss: 0.01019486, Validation loss: 0.57292632, Gradient norm: 0.18774372
INFO:root:[   25] Training loss: 0.01005144, Validation loss: 0.57003241, Gradient norm: 0.19888958
INFO:root:[   26] Training loss: 0.01003467, Validation loss: 0.58131645, Gradient norm: 0.21336134
INFO:root:[   27] Training loss: 0.00978700, Validation loss: 0.60938394, Gradient norm: 0.18773709
INFO:root:[   28] Training loss: 0.00992600, Validation loss: 0.55249038, Gradient norm: 0.21465000
INFO:root:[   29] Training loss: 0.00968150, Validation loss: 0.55319084, Gradient norm: 0.21681261
INFO:root:[   30] Training loss: 0.00954817, Validation loss: 0.58067027, Gradient norm: 0.20750017
INFO:root:[   31] Training loss: 0.00951313, Validation loss: 0.57013336, Gradient norm: 0.20712502
INFO:root:[   32] Training loss: 0.00944027, Validation loss: 0.55826723, Gradient norm: 0.21396735
INFO:root:[   33] Training loss: 0.00936164, Validation loss: 0.54413693, Gradient norm: 0.20294025
INFO:root:[   34] Training loss: 0.00951886, Validation loss: 0.53672287, Gradient norm: 0.24257913
INFO:root:[   35] Training loss: 0.00914212, Validation loss: 0.55762861, Gradient norm: 0.21285252
INFO:root:[   36] Training loss: 0.00904442, Validation loss: 0.56480942, Gradient norm: 0.22436172
INFO:root:[   37] Training loss: 0.00925562, Validation loss: 0.54012713, Gradient norm: 0.25708208
INFO:root:[   38] Training loss: 0.00895233, Validation loss: 0.53838253, Gradient norm: 0.23834995
INFO:root:[   39] Training loss: 0.00889932, Validation loss: 0.53478897, Gradient norm: 0.22621350
INFO:root:[   40] Training loss: 0.00884918, Validation loss: 0.53860992, Gradient norm: 0.24429281
INFO:root:[   41] Training loss: 0.00892784, Validation loss: 0.52023421, Gradient norm: 0.22070995
INFO:root:[   42] Training loss: 0.00884983, Validation loss: 0.51982519, Gradient norm: 0.23534537
INFO:root:[   43] Training loss: 0.00866161, Validation loss: 0.52465139, Gradient norm: 0.24220189
INFO:root:[   44] Training loss: 0.00865321, Validation loss: 0.51603597, Gradient norm: 0.23346755
INFO:root:[   45] Training loss: 0.00861795, Validation loss: 0.52379982, Gradient norm: 0.23754527
INFO:root:[   46] Training loss: 0.00877109, Validation loss: 0.51760489, Gradient norm: 0.27597028
INFO:root:[   47] Training loss: 0.00847056, Validation loss: 0.51067328, Gradient norm: 0.22488075
INFO:root:[   48] Training loss: 0.00842408, Validation loss: 0.51210592, Gradient norm: 0.23264856
INFO:root:[   49] Training loss: 0.00846099, Validation loss: 0.51452284, Gradient norm: 0.22742425
INFO:root:[   50] Training loss: 0.00840055, Validation loss: 0.50117147, Gradient norm: 0.22014476
INFO:root:[   51] Training loss: 0.00844253, Validation loss: 0.50124635, Gradient norm: 0.24816800
INFO:root:[   52] Training loss: 0.00836056, Validation loss: 0.50474002, Gradient norm: 0.22534326
INFO:root:[   53] Training loss: 0.00840175, Validation loss: 0.49444639, Gradient norm: 0.23884359
INFO:root:[   54] Training loss: 0.00816914, Validation loss: 0.52787918, Gradient norm: 0.22879755
INFO:root:[   55] Training loss: 0.00807803, Validation loss: 0.51345512, Gradient norm: 0.22983328
INFO:root:[   56] Training loss: 0.00822545, Validation loss: 0.50617417, Gradient norm: 0.25727973
INFO:root:[   57] Training loss: 0.00809223, Validation loss: 0.51527408, Gradient norm: 0.23664577
INFO:root:[   58] Training loss: 0.00788231, Validation loss: 0.48390670, Gradient norm: 0.20121286
INFO:root:[   59] Training loss: 0.00794874, Validation loss: 0.47685765, Gradient norm: 0.24351175
INFO:root:[   60] Training loss: 0.00804756, Validation loss: 0.50036756, Gradient norm: 0.24237437
INFO:root:[   61] Training loss: 0.00787060, Validation loss: 0.51170520, Gradient norm: 0.22353569
INFO:root:[   62] Training loss: 0.00804792, Validation loss: 0.55037563, Gradient norm: 0.26254680
INFO:root:[   63] Training loss: 0.00796530, Validation loss: 0.48062211, Gradient norm: 0.24159342
INFO:root:[   64] Training loss: 0.00781707, Validation loss: 0.47307019, Gradient norm: 0.22269553
INFO:root:[   65] Training loss: 0.00784375, Validation loss: 0.47424568, Gradient norm: 0.24796015
INFO:root:[   66] Training loss: 0.00770442, Validation loss: 0.49767644, Gradient norm: 0.24727850
INFO:root:[   67] Training loss: 0.00774450, Validation loss: 0.47885627, Gradient norm: 0.23353327
INFO:root:[   68] Training loss: 0.00772619, Validation loss: 0.47611979, Gradient norm: 0.23570220
INFO:root:[   69] Training loss: 0.00777136, Validation loss: 0.51802467, Gradient norm: 0.23792512
INFO:root:[   70] Training loss: 0.00755092, Validation loss: 0.50235536, Gradient norm: 0.21408655
INFO:root:[   71] Training loss: 0.00770362, Validation loss: 0.46423770, Gradient norm: 0.25214918
INFO:root:[   72] Training loss: 0.00765447, Validation loss: 0.48025275, Gradient norm: 0.25917797
INFO:root:[   73] Training loss: 0.00753329, Validation loss: 0.48960400, Gradient norm: 0.22905799
INFO:root:[   74] Training loss: 0.00747665, Validation loss: 0.50963203, Gradient norm: 0.21924525
INFO:root:[   75] Training loss: 0.00741045, Validation loss: 0.48115803, Gradient norm: 0.21960688
INFO:root:[   76] Training loss: 0.00745227, Validation loss: 0.47305478, Gradient norm: 0.25205527
INFO:root:[   77] Training loss: 0.00748822, Validation loss: 0.49379823, Gradient norm: 0.25716372
INFO:root:[   78] Training loss: 0.00753843, Validation loss: 0.48871286, Gradient norm: 0.23699230
INFO:root:[   79] Training loss: 0.00721726, Validation loss: 0.48838474, Gradient norm: 0.21494354
INFO:root:[   80] Training loss: 0.00739673, Validation loss: 0.54068778, Gradient norm: 0.22947044
INFO:root:[   81] Training loss: 0.00737687, Validation loss: 0.46200985, Gradient norm: 0.22102439
INFO:root:[   82] Training loss: 0.00725314, Validation loss: 0.48515435, Gradient norm: 0.23599525
INFO:root:[   83] Training loss: 0.00739493, Validation loss: 0.48668340, Gradient norm: 0.26370580
INFO:root:[   84] Training loss: 0.00719718, Validation loss: 0.51584890, Gradient norm: 0.21550427
INFO:root:[   85] Training loss: 0.00729947, Validation loss: 0.51605712, Gradient norm: 0.23212836
INFO:root:[   86] Training loss: 0.00743225, Validation loss: 0.51159739, Gradient norm: 0.26083963
INFO:root:[   87] Training loss: 0.00716681, Validation loss: 0.47650002, Gradient norm: 0.22714673
INFO:root:[   88] Training loss: 0.00703574, Validation loss: 0.48171743, Gradient norm: 0.22261984
INFO:root:[   89] Training loss: 0.00701524, Validation loss: 0.47674748, Gradient norm: 0.21595558
INFO:root:[   90] Training loss: 0.00712289, Validation loss: 0.50641698, Gradient norm: 0.23676549
INFO:root:EP 90: Early stopping
INFO:root:Training the model took 1200.822s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.06094
INFO:root:EnergyScoretrain: 0.04969
INFO:root:Coveragetrain: 7.00334
INFO:root:IntervalWidthtrain: 202.30739
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02232
INFO:root:EnergyScorevalidation: 0.0171
INFO:root:Coveragevalidation: 1.84894
INFO:root:IntervalWidthvalidation: 53.26065
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01694
INFO:root:EnergyScoretest: 0.01202
INFO:root:Coveragetest: 1.04981
INFO:root:IntervalWidthtest: 62.21195
INFO:root:###17 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06534025, Validation loss: 2.04982894, Gradient norm: 0.35622173
INFO:root:[    2] Training loss: 0.03299643, Validation loss: 1.74013195, Gradient norm: 0.32136516
INFO:root:[    3] Training loss: 0.02732791, Validation loss: 1.29218087, Gradient norm: 0.41280332
INFO:root:[    4] Training loss: 0.02411449, Validation loss: 1.32356752, Gradient norm: 0.40079582
INFO:root:[    5] Training loss: 0.02218117, Validation loss: 1.06993334, Gradient norm: 0.39234655
INFO:root:[    6] Training loss: 0.02050084, Validation loss: 1.08712290, Gradient norm: 0.35602417
INFO:root:[    7] Training loss: 0.01978443, Validation loss: 0.92498357, Gradient norm: 0.42800334
INFO:root:[    8] Training loss: 0.01855616, Validation loss: 0.94782798, Gradient norm: 0.42133439
INFO:root:[    9] Training loss: 0.01793882, Validation loss: 0.84708060, Gradient norm: 0.44457174
INFO:root:[   10] Training loss: 0.01714139, Validation loss: 0.83142983, Gradient norm: 0.42327207
INFO:root:[   11] Training loss: 0.01647363, Validation loss: 1.04068704, Gradient norm: 0.38099827
INFO:root:[   12] Training loss: 0.01586683, Validation loss: 0.84341239, Gradient norm: 0.36292954
INFO:root:[   13] Training loss: 0.01570747, Validation loss: 0.74577043, Gradient norm: 0.38916129
INFO:root:[   14] Training loss: 0.01574543, Validation loss: 0.74553020, Gradient norm: 0.46152951
INFO:root:[   15] Training loss: 0.01490139, Validation loss: 0.73775119, Gradient norm: 0.35068120
INFO:root:[   16] Training loss: 0.01480896, Validation loss: 0.69067974, Gradient norm: 0.41826050
INFO:root:[   17] Training loss: 0.01495867, Validation loss: 0.68790352, Gradient norm: 0.46343818
INFO:root:[   18] Training loss: 0.01431777, Validation loss: 0.75494417, Gradient norm: 0.40753488
INFO:root:[   19] Training loss: 0.01393265, Validation loss: 0.69485459, Gradient norm: 0.35229628
INFO:root:[   20] Training loss: 0.01409511, Validation loss: 0.77286211, Gradient norm: 0.41043541
INFO:root:[   21] Training loss: 0.01375687, Validation loss: 0.75311699, Gradient norm: 0.37402607
INFO:root:[   22] Training loss: 0.01342725, Validation loss: 0.82654960, Gradient norm: 0.34647298
INFO:root:[   23] Training loss: 0.01379180, Validation loss: 1.01381589, Gradient norm: 0.44437593
INFO:root:[   24] Training loss: 0.01311537, Validation loss: 0.69134698, Gradient norm: 0.35458942
INFO:root:[   25] Training loss: 0.01314457, Validation loss: 0.65082853, Gradient norm: 0.38777023
INFO:root:[   26] Training loss: 0.01303041, Validation loss: 0.65906578, Gradient norm: 0.38969711
INFO:root:[   27] Training loss: 0.01300464, Validation loss: 0.65341758, Gradient norm: 0.40823593
INFO:root:[   28] Training loss: 0.01275047, Validation loss: 0.74797940, Gradient norm: 0.39538388
INFO:root:[   29] Training loss: 0.01282475, Validation loss: 0.70025779, Gradient norm: 0.42236376
INFO:root:[   30] Training loss: 0.01258146, Validation loss: 0.72689913, Gradient norm: 0.37416431
INFO:root:[   31] Training loss: 0.01244556, Validation loss: 0.66115793, Gradient norm: 0.39975513
INFO:root:[   32] Training loss: 0.01214083, Validation loss: 0.66046213, Gradient norm: 0.34985873
INFO:root:[   33] Training loss: 0.01228068, Validation loss: 0.74328496, Gradient norm: 0.39079839
INFO:root:[   34] Training loss: 0.01184715, Validation loss: 0.66900413, Gradient norm: 0.31464701
INFO:root:[   35] Training loss: 0.01248788, Validation loss: 0.68526769, Gradient norm: 0.45767271
INFO:root:[   36] Training loss: 0.01191108, Validation loss: 0.71458599, Gradient norm: 0.38573664
INFO:root:[   37] Training loss: 0.01151298, Validation loss: 0.61005103, Gradient norm: 0.28464398
INFO:root:[   38] Training loss: 0.01213188, Validation loss: 0.67262371, Gradient norm: 0.44865230
INFO:root:[   39] Training loss: 0.01142313, Validation loss: 0.59907031, Gradient norm: 0.32137434
INFO:root:[   40] Training loss: 0.01165179, Validation loss: 0.71109043, Gradient norm: 0.39457335
INFO:root:[   41] Training loss: 0.01152980, Validation loss: 0.70880845, Gradient norm: 0.38421115
INFO:root:[   42] Training loss: 0.01127953, Validation loss: 0.62226023, Gradient norm: 0.34667462
INFO:root:[   43] Training loss: 0.01121702, Validation loss: 0.62128515, Gradient norm: 0.35016708
INFO:root:[   44] Training loss: 0.01133771, Validation loss: 0.59166207, Gradient norm: 0.38528048
INFO:root:[   45] Training loss: 0.01147623, Validation loss: 0.60796025, Gradient norm: 0.41302634
INFO:root:[   46] Training loss: 0.01122368, Validation loss: 0.61810152, Gradient norm: 0.37842835
INFO:root:[   47] Training loss: 0.01091670, Validation loss: 0.63041864, Gradient norm: 0.31433091
INFO:root:[   48] Training loss: 0.01092115, Validation loss: 0.59591566, Gradient norm: 0.36058780
INFO:root:[   49] Training loss: 0.01091249, Validation loss: 0.61956870, Gradient norm: 0.37914197
INFO:root:[   50] Training loss: 0.01069777, Validation loss: 0.61810813, Gradient norm: 0.35135362
INFO:root:[   51] Training loss: 0.01099574, Validation loss: 0.65197592, Gradient norm: 0.41086700
INFO:root:[   52] Training loss: 0.01057798, Validation loss: 0.59928590, Gradient norm: 0.32244707
INFO:root:[   53] Training loss: 0.01112642, Validation loss: 0.58306944, Gradient norm: 0.47045890
INFO:root:[   54] Training loss: 0.01052860, Validation loss: 0.65732967, Gradient norm: 0.35937822
INFO:root:[   55] Training loss: 0.01022164, Validation loss: 0.60057108, Gradient norm: 0.31112056
INFO:root:[   56] Training loss: 0.01061452, Validation loss: 0.71788817, Gradient norm: 0.40106930
INFO:root:[   57] Training loss: 0.01045445, Validation loss: 0.62684256, Gradient norm: 0.37807594
INFO:root:[   58] Training loss: 0.01025516, Validation loss: 0.66748150, Gradient norm: 0.36475317
INFO:root:[   59] Training loss: 0.01005117, Validation loss: 0.63841992, Gradient norm: 0.33300186
INFO:root:[   60] Training loss: 0.01003299, Validation loss: 0.59869331, Gradient norm: 0.31818497
INFO:root:[   61] Training loss: 0.01034583, Validation loss: 0.62055194, Gradient norm: 0.40466700
INFO:root:[   62] Training loss: 0.01024793, Validation loss: 0.57048914, Gradient norm: 0.41439473
INFO:root:[   63] Training loss: 0.00990729, Validation loss: 0.67017385, Gradient norm: 0.33274845
INFO:root:[   64] Training loss: 0.01001671, Validation loss: 0.64343534, Gradient norm: 0.37544171
INFO:root:[   65] Training loss: 0.00986279, Validation loss: 0.59666946, Gradient norm: 0.34446548
INFO:root:[   66] Training loss: 0.00994198, Validation loss: 0.56635012, Gradient norm: 0.36987688
INFO:root:[   67] Training loss: 0.01025649, Validation loss: 0.75438934, Gradient norm: 0.46690144
INFO:root:[   68] Training loss: 0.00974277, Validation loss: 0.61232694, Gradient norm: 0.34846426
INFO:root:[   69] Training loss: 0.00969075, Validation loss: 0.58097284, Gradient norm: 0.33857040
INFO:root:[   70] Training loss: 0.00981184, Validation loss: 0.60047026, Gradient norm: 0.38755971
INFO:root:[   71] Training loss: 0.00934432, Validation loss: 0.58364329, Gradient norm: 0.27966096
INFO:root:[   72] Training loss: 0.00990885, Validation loss: 0.59901046, Gradient norm: 0.42267781
INFO:root:[   73] Training loss: 0.00987691, Validation loss: 0.64449073, Gradient norm: 0.42612946
INFO:root:[   74] Training loss: 0.00980531, Validation loss: 0.59828488, Gradient norm: 0.43628939
INFO:root:[   75] Training loss: 0.00931996, Validation loss: 0.57298926, Gradient norm: 0.33054244
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 380.495s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.09015
INFO:root:EnergyScoretrain: 0.04918
INFO:root:Coveragetrain: 4.85125
INFO:root:IntervalWidthtrain: 92.86209
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02676
INFO:root:EnergyScorevalidation: 0.01519
INFO:root:Coveragevalidation: 1.23381
INFO:root:IntervalWidthvalidation: 23.86102
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02246
INFO:root:EnergyScoretest: 0.0142
INFO:root:Coveragetest: 0.48854
INFO:root:IntervalWidthtest: 26.71584
INFO:root:###18 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06577923, Validation loss: 1.98907256, Gradient norm: 0.33586622
INFO:root:[    2] Training loss: 0.03485275, Validation loss: 1.64480723, Gradient norm: 0.32967462
INFO:root:[    3] Training loss: 0.02903838, Validation loss: 1.30132489, Gradient norm: 0.31747040
INFO:root:[    4] Training loss: 0.02622360, Validation loss: 1.14730001, Gradient norm: 0.35167135
INFO:root:[    5] Training loss: 0.02450973, Validation loss: 1.10695396, Gradient norm: 0.39006558
INFO:root:[    6] Training loss: 0.02305425, Validation loss: 1.02891882, Gradient norm: 0.38782541
INFO:root:[    7] Training loss: 0.02193610, Validation loss: 1.01115198, Gradient norm: 0.36666269
INFO:root:[    8] Training loss: 0.02076629, Validation loss: 1.16108213, Gradient norm: 0.32199316
INFO:root:[    9] Training loss: 0.02034148, Validation loss: 0.87556563, Gradient norm: 0.36577862
INFO:root:[   10] Training loss: 0.01981300, Validation loss: 0.86486017, Gradient norm: 0.35453000
INFO:root:[   11] Training loss: 0.01888302, Validation loss: 0.81802171, Gradient norm: 0.32389284
INFO:root:[   12] Training loss: 0.01875971, Validation loss: 0.92954199, Gradient norm: 0.37422260
INFO:root:[   13] Training loss: 0.01793436, Validation loss: 0.96984985, Gradient norm: 0.30964963
INFO:root:[   14] Training loss: 0.01816506, Validation loss: 0.82237303, Gradient norm: 0.37753391
INFO:root:[   15] Training loss: 0.01741641, Validation loss: 0.84895941, Gradient norm: 0.32486593
INFO:root:[   16] Training loss: 0.01716718, Validation loss: 0.91454267, Gradient norm: 0.32991541
INFO:root:[   17] Training loss: 0.01679151, Validation loss: 0.83625614, Gradient norm: 0.33787040
INFO:root:[   18] Training loss: 0.01673175, Validation loss: 0.75211870, Gradient norm: 0.35726094
INFO:root:[   19] Training loss: 0.01651879, Validation loss: 0.74434265, Gradient norm: 0.36651159
INFO:root:[   20] Training loss: 0.01610467, Validation loss: 0.87423972, Gradient norm: 0.30592348
INFO:root:[   21] Training loss: 0.01643011, Validation loss: 0.79902940, Gradient norm: 0.40770484
INFO:root:[   22] Training loss: 0.01564410, Validation loss: 0.85308805, Gradient norm: 0.31674530
INFO:root:[   23] Training loss: 0.01555735, Validation loss: 0.75203172, Gradient norm: 0.33366176
INFO:root:[   24] Training loss: 0.01549938, Validation loss: 0.87853736, Gradient norm: 0.35893159
INFO:root:[   25] Training loss: 0.01502611, Validation loss: 0.71210302, Gradient norm: 0.29918291
INFO:root:[   26] Training loss: 0.01515207, Validation loss: 0.71317200, Gradient norm: 0.32646731
INFO:root:[   27] Training loss: 0.01480913, Validation loss: 0.69395331, Gradient norm: 0.31312383
INFO:root:[   28] Training loss: 0.01485152, Validation loss: 0.68961676, Gradient norm: 0.33547227
INFO:root:[   29] Training loss: 0.01471252, Validation loss: 0.69624597, Gradient norm: 0.35747288
INFO:root:[   30] Training loss: 0.01443070, Validation loss: 0.66764730, Gradient norm: 0.32206993
INFO:root:[   31] Training loss: 0.01441827, Validation loss: 0.65366022, Gradient norm: 0.33249777
INFO:root:[   32] Training loss: 0.01404342, Validation loss: 0.66144718, Gradient norm: 0.25925483
INFO:root:[   33] Training loss: 0.01403779, Validation loss: 0.86590409, Gradient norm: 0.30415634
INFO:root:[   34] Training loss: 0.01392998, Validation loss: 0.68685473, Gradient norm: 0.31928157
INFO:root:[   35] Training loss: 0.01388972, Validation loss: 0.68752196, Gradient norm: 0.34366942
INFO:root:[   36] Training loss: 0.01392078, Validation loss: 0.76045444, Gradient norm: 0.33904857
INFO:root:[   37] Training loss: 0.01383274, Validation loss: 0.67765577, Gradient norm: 0.34283604
INFO:root:[   38] Training loss: 0.01355209, Validation loss: 0.66308336, Gradient norm: 0.30624618
INFO:root:[   39] Training loss: 0.01343136, Validation loss: 0.63267266, Gradient norm: 0.32161195
INFO:root:[   40] Training loss: 0.01348391, Validation loss: 0.64264004, Gradient norm: 0.31912457
INFO:root:[   41] Training loss: 0.01336813, Validation loss: 0.88150839, Gradient norm: 0.33005608
INFO:root:[   42] Training loss: 0.01337445, Validation loss: 0.61575976, Gradient norm: 0.32914499
INFO:root:[   43] Training loss: 0.01273953, Validation loss: 0.63390870, Gradient norm: 0.25342294
INFO:root:[   44] Training loss: 0.01287328, Validation loss: 0.61705098, Gradient norm: 0.29024861
INFO:root:[   45] Training loss: 0.01313002, Validation loss: 0.64355210, Gradient norm: 0.34555291
INFO:root:[   46] Training loss: 0.01284510, Validation loss: 0.72641499, Gradient norm: 0.32102357
INFO:root:[   47] Training loss: 0.01274250, Validation loss: 0.61761212, Gradient norm: 0.30352890
INFO:root:[   48] Training loss: 0.01262709, Validation loss: 0.69438566, Gradient norm: 0.29628594
INFO:root:[   49] Training loss: 0.01255698, Validation loss: 0.72470717, Gradient norm: 0.30254756
INFO:root:[   50] Training loss: 0.01261703, Validation loss: 0.64168374, Gradient norm: 0.34508885
INFO:root:[   51] Training loss: 0.01254148, Validation loss: 0.99007274, Gradient norm: 0.33782372
INFO:root:[   52] Training loss: 0.01251317, Validation loss: 0.62250357, Gradient norm: 0.31805883
INFO:root:[   53] Training loss: 0.01218335, Validation loss: 0.63352381, Gradient norm: 0.27850221
INFO:root:[   54] Training loss: 0.01244505, Validation loss: 0.60838489, Gradient norm: 0.34597395
INFO:root:[   55] Training loss: 0.01236564, Validation loss: 0.65514161, Gradient norm: 0.33191238
INFO:root:[   56] Training loss: 0.01226483, Validation loss: 0.79024666, Gradient norm: 0.33174580
INFO:root:[   57] Training loss: 0.01244196, Validation loss: 0.62343894, Gradient norm: 0.34375164
INFO:root:[   58] Training loss: 0.01192194, Validation loss: 0.64463267, Gradient norm: 0.29124763
INFO:root:[   59] Training loss: 0.01211040, Validation loss: 0.61410317, Gradient norm: 0.33881085
INFO:root:[   60] Training loss: 0.01206382, Validation loss: 0.61967826, Gradient norm: 0.31841294
INFO:root:[   61] Training loss: 0.01181781, Validation loss: 0.63898138, Gradient norm: 0.30425146
INFO:root:[   62] Training loss: 0.01220350, Validation loss: 0.68909703, Gradient norm: 0.37514203
INFO:root:[   63] Training loss: 0.01169450, Validation loss: 0.62043271, Gradient norm: 0.28452798
INFO:root:[   64] Training loss: 0.01175234, Validation loss: 0.59461194, Gradient norm: 0.33043237
INFO:root:[   65] Training loss: 0.01144815, Validation loss: 0.58604089, Gradient norm: 0.26011886
INFO:root:[   66] Training loss: 0.01143965, Validation loss: 0.61312240, Gradient norm: 0.28744663
INFO:root:[   67] Training loss: 0.01151294, Validation loss: 0.60893595, Gradient norm: 0.30447444
INFO:root:[   68] Training loss: 0.01150583, Validation loss: 0.62522689, Gradient norm: 0.33390927
INFO:root:[   69] Training loss: 0.01121581, Validation loss: 0.57289047, Gradient norm: 0.25275056
INFO:root:[   70] Training loss: 0.01136001, Validation loss: 0.66009053, Gradient norm: 0.32441945
INFO:root:[   71] Training loss: 0.01118340, Validation loss: 0.62059669, Gradient norm: 0.30519283
INFO:root:[   72] Training loss: 0.01128969, Validation loss: 0.57740590, Gradient norm: 0.31610987
INFO:root:[   73] Training loss: 0.01133683, Validation loss: 0.69000720, Gradient norm: 0.36283163
INFO:root:[   74] Training loss: 0.01127145, Validation loss: 0.60350725, Gradient norm: 0.31704643
INFO:root:[   75] Training loss: 0.01096826, Validation loss: 0.58144894, Gradient norm: 0.29602218
INFO:root:[   76] Training loss: 0.01114217, Validation loss: 0.71868239, Gradient norm: 0.29668843
INFO:root:[   77] Training loss: 0.01100181, Validation loss: 0.57667272, Gradient norm: 0.28664676
INFO:root:[   78] Training loss: 0.01100277, Validation loss: 0.57169805, Gradient norm: 0.31038672
INFO:root:[   79] Training loss: 0.01089789, Validation loss: 0.62530634, Gradient norm: 0.31035593
INFO:root:[   80] Training loss: 0.01097978, Validation loss: 0.60188174, Gradient norm: 0.33000858
INFO:root:[   81] Training loss: 0.01106285, Validation loss: 0.60820656, Gradient norm: 0.34633212
INFO:root:[   82] Training loss: 0.01095559, Validation loss: 0.63500611, Gradient norm: 0.31630751
INFO:root:[   83] Training loss: 0.01097492, Validation loss: 0.59143592, Gradient norm: 0.36044858
INFO:root:[   84] Training loss: 0.01057869, Validation loss: 0.65958404, Gradient norm: 0.27744917
INFO:root:[   85] Training loss: 0.01069926, Validation loss: 0.60268457, Gradient norm: 0.31465813
INFO:root:[   86] Training loss: 0.01067345, Validation loss: 0.58644629, Gradient norm: 0.32095077
INFO:root:[   87] Training loss: 0.01082754, Validation loss: 0.62840187, Gradient norm: 0.33028616
INFO:root:EP 87: Early stopping
INFO:root:Training the model took 429.205s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.09382
INFO:root:EnergyScoretrain: 0.05103
INFO:root:Coveragetrain: 4.71595
INFO:root:IntervalWidthtrain: 96.54725
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02838
INFO:root:EnergyScorevalidation: 0.01582
INFO:root:Coveragevalidation: 1.23061
INFO:root:IntervalWidthvalidation: 26.11577
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02398
INFO:root:EnergyScoretest: 0.01477
INFO:root:Coveragetest: 0.56432
INFO:root:IntervalWidthtest: 29.46583
INFO:root:###19 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06403439, Validation loss: 2.14694735, Gradient norm: 0.32418930
INFO:root:[    2] Training loss: 0.03833204, Validation loss: 1.68592585, Gradient norm: 0.32509799
INFO:root:[    3] Training loss: 0.03219256, Validation loss: 1.41833897, Gradient norm: 0.33360589
INFO:root:[    4] Training loss: 0.02863797, Validation loss: 1.20689988, Gradient norm: 0.33846495
INFO:root:[    5] Training loss: 0.02656654, Validation loss: 1.21291483, Gradient norm: 0.34701904
INFO:root:[    6] Training loss: 0.02524357, Validation loss: 1.11353094, Gradient norm: 0.35729014
INFO:root:[    7] Training loss: 0.02381827, Validation loss: 1.01790654, Gradient norm: 0.29504460
INFO:root:[    8] Training loss: 0.02291360, Validation loss: 1.03091519, Gradient norm: 0.32960690
INFO:root:[    9] Training loss: 0.02198330, Validation loss: 0.97558913, Gradient norm: 0.33073203
INFO:root:[   10] Training loss: 0.02120586, Validation loss: 0.87682438, Gradient norm: 0.30869080
INFO:root:[   11] Training loss: 0.02110671, Validation loss: 1.04078419, Gradient norm: 0.40831207
INFO:root:[   12] Training loss: 0.02023128, Validation loss: 0.82495088, Gradient norm: 0.29697536
INFO:root:[   13] Training loss: 0.01988426, Validation loss: 0.91937581, Gradient norm: 0.35769983
INFO:root:[   14] Training loss: 0.01928682, Validation loss: 0.83846351, Gradient norm: 0.32642941
INFO:root:[   15] Training loss: 0.01898848, Validation loss: 0.80881916, Gradient norm: 0.31124042
INFO:root:[   16] Training loss: 0.01857268, Validation loss: 0.85308493, Gradient norm: 0.29624530
INFO:root:[   17] Training loss: 0.01823400, Validation loss: 0.81384646, Gradient norm: 0.29480102
INFO:root:[   18] Training loss: 0.01816771, Validation loss: 0.79043801, Gradient norm: 0.32133378
INFO:root:[   19] Training loss: 0.01778932, Validation loss: 0.73183933, Gradient norm: 0.33524682
INFO:root:[   20] Training loss: 0.01730489, Validation loss: 0.84760778, Gradient norm: 0.28240450
INFO:root:[   21] Training loss: 0.01771955, Validation loss: 0.72855620, Gradient norm: 0.35136402
INFO:root:[   22] Training loss: 0.01671934, Validation loss: 0.72659660, Gradient norm: 0.22743746
INFO:root:[   23] Training loss: 0.01679285, Validation loss: 0.71219738, Gradient norm: 0.30617694
INFO:root:[   24] Training loss: 0.01675563, Validation loss: 0.73990138, Gradient norm: 0.32576623
INFO:root:[   25] Training loss: 0.01675145, Validation loss: 0.73998990, Gradient norm: 0.32033656
INFO:root:[   26] Training loss: 0.01607909, Validation loss: 0.68665659, Gradient norm: 0.27834163
INFO:root:[   27] Training loss: 0.01617214, Validation loss: 0.72962269, Gradient norm: 0.33068846
INFO:root:[   28] Training loss: 0.01606846, Validation loss: 0.76844209, Gradient norm: 0.32306931
INFO:root:[   29] Training loss: 0.01591576, Validation loss: 0.72220012, Gradient norm: 0.29619205
INFO:root:[   30] Training loss: 0.01574989, Validation loss: 0.66686986, Gradient norm: 0.32308750
INFO:root:[   31] Training loss: 0.01548857, Validation loss: 0.75761978, Gradient norm: 0.29216665
INFO:root:[   32] Training loss: 0.01559591, Validation loss: 0.67573067, Gradient norm: 0.32208429
INFO:root:[   33] Training loss: 0.01538579, Validation loss: 0.69479579, Gradient norm: 0.34347324
INFO:root:[   34] Training loss: 0.01513587, Validation loss: 0.66489560, Gradient norm: 0.29518850
INFO:root:[   35] Training loss: 0.01518698, Validation loss: 0.67238099, Gradient norm: 0.31561470
INFO:root:[   36] Training loss: 0.01506001, Validation loss: 0.69945982, Gradient norm: 0.31786244
INFO:root:[   37] Training loss: 0.01488720, Validation loss: 0.65847370, Gradient norm: 0.31343691
INFO:root:[   38] Training loss: 0.01470744, Validation loss: 0.66695590, Gradient norm: 0.29822777
INFO:root:[   39] Training loss: 0.01459854, Validation loss: 0.64842206, Gradient norm: 0.31255492
INFO:root:[   40] Training loss: 0.01447280, Validation loss: 0.62924332, Gradient norm: 0.29728475
INFO:root:[   41] Training loss: 0.01421022, Validation loss: 0.64281090, Gradient norm: 0.25003136
INFO:root:[   42] Training loss: 0.01454934, Validation loss: 0.71246586, Gradient norm: 0.32699770
INFO:root:[   43] Training loss: 0.01420962, Validation loss: 0.65397840, Gradient norm: 0.28756327
INFO:root:[   44] Training loss: 0.01396262, Validation loss: 0.68084250, Gradient norm: 0.25443741
INFO:root:[   45] Training loss: 0.01391711, Validation loss: 0.63680775, Gradient norm: 0.26456129
INFO:root:[   46] Training loss: 0.01406460, Validation loss: 0.63400440, Gradient norm: 0.33616960
INFO:root:[   47] Training loss: 0.01412868, Validation loss: 0.71920826, Gradient norm: 0.29517713
INFO:root:[   48] Training loss: 0.01377613, Validation loss: 0.61976682, Gradient norm: 0.27798366
INFO:root:[   49] Training loss: 0.01361165, Validation loss: 0.61469380, Gradient norm: 0.29435672
INFO:root:[   50] Training loss: 0.01347077, Validation loss: 0.63765928, Gradient norm: 0.29601322
INFO:root:[   51] Training loss: 0.01356039, Validation loss: 0.61897207, Gradient norm: 0.30105029
INFO:root:[   52] Training loss: 0.01344948, Validation loss: 0.61865020, Gradient norm: 0.28272863
INFO:root:[   53] Training loss: 0.01347992, Validation loss: 0.60827308, Gradient norm: 0.30219460
INFO:root:[   54] Training loss: 0.01342093, Validation loss: 0.66749467, Gradient norm: 0.30108566
INFO:root:[   55] Training loss: 0.01342562, Validation loss: 0.62120591, Gradient norm: 0.31819410
INFO:root:[   56] Training loss: 0.01328621, Validation loss: 0.63211518, Gradient norm: 0.29902369
INFO:root:[   57] Training loss: 0.01320764, Validation loss: 0.59913103, Gradient norm: 0.31225924
INFO:root:[   58] Training loss: 0.01293156, Validation loss: 0.72733524, Gradient norm: 0.25456030
INFO:root:[   59] Training loss: 0.01323877, Validation loss: 0.62757256, Gradient norm: 0.32448641
INFO:root:[   60] Training loss: 0.01281677, Validation loss: 0.68626332, Gradient norm: 0.28712530
INFO:root:[   61] Training loss: 0.01285859, Validation loss: 0.73543792, Gradient norm: 0.28671417
INFO:root:[   62] Training loss: 0.01284840, Validation loss: 0.60841016, Gradient norm: 0.29972501
INFO:root:[   63] Training loss: 0.01268632, Validation loss: 0.64109513, Gradient norm: 0.28563969
INFO:root:[   64] Training loss: 0.01265324, Validation loss: 0.65957588, Gradient norm: 0.27894762
INFO:root:[   65] Training loss: 0.01252608, Validation loss: 0.72085507, Gradient norm: 0.27741250
INFO:root:[   66] Training loss: 0.01277727, Validation loss: 0.61931039, Gradient norm: 0.32443483
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 327.405s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.11533
INFO:root:EnergyScoretrain: 0.06286
INFO:root:Coveragetrain: 4.72169
INFO:root:IntervalWidthtrain: 118.17802
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03102
INFO:root:EnergyScorevalidation: 0.0175
INFO:root:Coveragevalidation: 1.30514
INFO:root:IntervalWidthvalidation: 30.7565
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02329
INFO:root:EnergyScoretest: 0.01345
INFO:root:Coveragetest: 0.67769
INFO:root:IntervalWidthtest: 36.33205
INFO:root:###20 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07571615, Validation loss: 2.21439958, Gradient norm: 0.33980748
INFO:root:[    2] Training loss: 0.04232519, Validation loss: 1.88356758, Gradient norm: 0.33547412
INFO:root:[    3] Training loss: 0.03714149, Validation loss: 1.60300933, Gradient norm: 0.29486697
INFO:root:[    4] Training loss: 0.03379636, Validation loss: 1.59607146, Gradient norm: 0.33864139
INFO:root:[    5] Training loss: 0.03094398, Validation loss: 1.31001455, Gradient norm: 0.30343244
INFO:root:[    6] Training loss: 0.02920736, Validation loss: 1.25403600, Gradient norm: 0.35351019
INFO:root:[    7] Training loss: 0.02751533, Validation loss: 1.13682187, Gradient norm: 0.33785123
INFO:root:[    8] Training loss: 0.02628924, Validation loss: 1.08060032, Gradient norm: 0.30534021
INFO:root:[    9] Training loss: 0.02526199, Validation loss: 1.22600713, Gradient norm: 0.33647801
INFO:root:[   10] Training loss: 0.02491553, Validation loss: 1.04771279, Gradient norm: 0.34937243
INFO:root:[   11] Training loss: 0.02379864, Validation loss: 0.95549980, Gradient norm: 0.29249567
INFO:root:[   12] Training loss: 0.02294408, Validation loss: 1.19817390, Gradient norm: 0.32194410
INFO:root:[   13] Training loss: 0.02269009, Validation loss: 0.96730530, Gradient norm: 0.31635715
INFO:root:[   14] Training loss: 0.02239822, Validation loss: 1.04274177, Gradient norm: 0.32400312
INFO:root:[   15] Training loss: 0.02197180, Validation loss: 0.87814698, Gradient norm: 0.32479108
INFO:root:[   16] Training loss: 0.02109353, Validation loss: 0.92640462, Gradient norm: 0.27086756
INFO:root:[   17] Training loss: 0.02139980, Validation loss: 0.85934753, Gradient norm: 0.38107696
INFO:root:[   18] Training loss: 0.02056499, Validation loss: 1.02439336, Gradient norm: 0.29580981
INFO:root:[   19] Training loss: 0.02039324, Validation loss: 0.85772920, Gradient norm: 0.33218932
INFO:root:[   20] Training loss: 0.02017081, Validation loss: 0.96183073, Gradient norm: 0.32020884
INFO:root:[   21] Training loss: 0.01971977, Validation loss: 0.93894547, Gradient norm: 0.29483774
INFO:root:[   22] Training loss: 0.01992089, Validation loss: 0.80342539, Gradient norm: 0.40235039
INFO:root:[   23] Training loss: 0.01924883, Validation loss: 0.79466589, Gradient norm: 0.31792177
INFO:root:[   24] Training loss: 0.01897738, Validation loss: 0.81726991, Gradient norm: 0.30962015
INFO:root:[   25] Training loss: 0.01908649, Validation loss: 0.82371399, Gradient norm: 0.36340118
INFO:root:[   26] Training loss: 0.01883422, Validation loss: 0.91691462, Gradient norm: 0.30517991
INFO:root:[   27] Training loss: 0.01845596, Validation loss: 0.80594079, Gradient norm: 0.28652501
INFO:root:[   28] Training loss: 0.01822945, Validation loss: 0.75878947, Gradient norm: 0.32143812
INFO:root:[   29] Training loss: 0.01817966, Validation loss: 0.83257127, Gradient norm: 0.34188947
INFO:root:[   30] Training loss: 0.01776710, Validation loss: 0.95497486, Gradient norm: 0.30190280
INFO:root:[   31] Training loss: 0.01771583, Validation loss: 0.83629702, Gradient norm: 0.31755764
INFO:root:[   32] Training loss: 0.01752069, Validation loss: 0.89242325, Gradient norm: 0.33501594
INFO:root:[   33] Training loss: 0.01707939, Validation loss: 0.74826880, Gradient norm: 0.27936879
INFO:root:[   34] Training loss: 0.01735212, Validation loss: 0.76623572, Gradient norm: 0.33441745
INFO:root:[   35] Training loss: 0.01728542, Validation loss: 0.80250564, Gradient norm: 0.35561415
INFO:root:[   36] Training loss: 0.01691576, Validation loss: 0.81707643, Gradient norm: 0.31993950
INFO:root:[   37] Training loss: 0.01670636, Validation loss: 0.70412116, Gradient norm: 0.28591350
INFO:root:[   38] Training loss: 0.01681930, Validation loss: 0.76713750, Gradient norm: 0.31950062
INFO:root:[   39] Training loss: 0.01637500, Validation loss: 0.69253376, Gradient norm: 0.27633139
INFO:root:[   40] Training loss: 0.01678572, Validation loss: 0.89671067, Gradient norm: 0.36008505
INFO:root:[   41] Training loss: 0.01631350, Validation loss: 0.92562146, Gradient norm: 0.29485038
INFO:root:[   42] Training loss: 0.01606003, Validation loss: 0.78889994, Gradient norm: 0.28389170
INFO:root:[   43] Training loss: 0.01612317, Validation loss: 0.73358279, Gradient norm: 0.33259030
INFO:root:[   44] Training loss: 0.01638416, Validation loss: 0.73600102, Gradient norm: 0.34755788
INFO:root:[   45] Training loss: 0.01582359, Validation loss: 0.86716068, Gradient norm: 0.31947177
INFO:root:[   46] Training loss: 0.01559564, Validation loss: 0.66531315, Gradient norm: 0.30311449
INFO:root:[   47] Training loss: 0.01550960, Validation loss: 0.68856297, Gradient norm: 0.29787135
INFO:root:[   48] Training loss: 0.01581596, Validation loss: 0.77798660, Gradient norm: 0.35895935
INFO:root:[   49] Training loss: 0.01548030, Validation loss: 0.73326762, Gradient norm: 0.29512788
INFO:root:[   50] Training loss: 0.01523101, Validation loss: 0.74594992, Gradient norm: 0.30808777
INFO:root:[   51] Training loss: 0.01515308, Validation loss: 0.70910440, Gradient norm: 0.30888221
INFO:root:[   52] Training loss: 0.01506714, Validation loss: 0.68088335, Gradient norm: 0.30004765
INFO:root:[   53] Training loss: 0.01515306, Validation loss: 0.75018207, Gradient norm: 0.33488291
INFO:root:[   54] Training loss: 0.01482740, Validation loss: 0.79473998, Gradient norm: 0.29837098
INFO:root:[   55] Training loss: 0.01510903, Validation loss: 0.73436954, Gradient norm: 0.34769568
INFO:root:[   56] Training loss: 0.01479472, Validation loss: 0.64920419, Gradient norm: 0.31618943
INFO:root:[   57] Training loss: 0.01464479, Validation loss: 0.66210020, Gradient norm: 0.29767000
INFO:root:[   58] Training loss: 0.01448252, Validation loss: 0.73674302, Gradient norm: 0.27665624
INFO:root:[   59] Training loss: 0.01462846, Validation loss: 0.67543946, Gradient norm: 0.29327376
INFO:root:[   60] Training loss: 0.01448758, Validation loss: 0.64969501, Gradient norm: 0.30251528
INFO:root:[   61] Training loss: 0.01460837, Validation loss: 0.71716515, Gradient norm: 0.33719662
INFO:root:[   62] Training loss: 0.01444693, Validation loss: 0.73916493, Gradient norm: 0.31124058
INFO:root:[   63] Training loss: 0.01430146, Validation loss: 0.72861626, Gradient norm: 0.28783656
INFO:root:[   64] Training loss: 0.01417287, Validation loss: 0.69835189, Gradient norm: 0.26150620
INFO:root:[   65] Training loss: 0.01440273, Validation loss: 0.80190905, Gradient norm: 0.35073420
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 322.597s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.13425
INFO:root:EnergyScoretrain: 0.07287
INFO:root:Coveragetrain: 4.81863
INFO:root:IntervalWidthtrain: 137.89963
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03689
INFO:root:EnergyScorevalidation: 0.0201
INFO:root:Coveragevalidation: 1.2429
INFO:root:IntervalWidthvalidation: 35.77889
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02418
INFO:root:EnergyScoretest: 0.01307
INFO:root:Coveragetest: 0.69063
INFO:root:IntervalWidthtest: 42.35254
INFO:root:###21 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 299892736
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06665964, Validation loss: 2.13737210, Gradient norm: 0.35723343
INFO:root:[    2] Training loss: 0.03396551, Validation loss: 1.49369623, Gradient norm: 0.32372923
INFO:root:[    3] Training loss: 0.02818877, Validation loss: 1.25765530, Gradient norm: 0.38103906
INFO:root:[    4] Training loss: 0.02535167, Validation loss: 1.16933279, Gradient norm: 0.37557933
INFO:root:[    5] Training loss: 0.02354183, Validation loss: 1.11100909, Gradient norm: 0.38652568
INFO:root:[    6] Training loss: 0.02169628, Validation loss: 1.00264514, Gradient norm: 0.31103568
INFO:root:[    7] Training loss: 0.02081581, Validation loss: 1.23115995, Gradient norm: 0.35592027
INFO:root:[    8] Training loss: 0.01953370, Validation loss: 0.93512954, Gradient norm: 0.31060862
INFO:root:[    9] Training loss: 0.01894199, Validation loss: 0.96653148, Gradient norm: 0.37174720
INFO:root:[   10] Training loss: 0.01785951, Validation loss: 0.84360192, Gradient norm: 0.31730884
INFO:root:[   11] Training loss: 0.01712276, Validation loss: 0.79087185, Gradient norm: 0.28508571
INFO:root:[   12] Training loss: 0.01687395, Validation loss: 0.75472045, Gradient norm: 0.33927968
INFO:root:[   13] Training loss: 0.01615998, Validation loss: 0.76261169, Gradient norm: 0.31171884
INFO:root:[   14] Training loss: 0.01612759, Validation loss: 0.80695758, Gradient norm: 0.40155166
INFO:root:[   15] Training loss: 0.01533121, Validation loss: 0.73335065, Gradient norm: 0.29242471
INFO:root:[   16] Training loss: 0.01540876, Validation loss: 0.71281098, Gradient norm: 0.36575419
INFO:root:[   17] Training loss: 0.01501273, Validation loss: 0.69856302, Gradient norm: 0.36311145
INFO:root:[   18] Training loss: 0.01474606, Validation loss: 0.70762661, Gradient norm: 0.35221671
INFO:root:[   19] Training loss: 0.01428410, Validation loss: 0.68039925, Gradient norm: 0.35047432
INFO:root:[   20] Training loss: 0.01387950, Validation loss: 0.71282122, Gradient norm: 0.29573841
INFO:root:[   21] Training loss: 0.01446048, Validation loss: 0.68336131, Gradient norm: 0.42265686
INFO:root:[   22] Training loss: 0.01379985, Validation loss: 0.67830497, Gradient norm: 0.34655832
INFO:root:[   23] Training loss: 0.01354748, Validation loss: 0.66303943, Gradient norm: 0.33268150
INFO:root:[   24] Training loss: 0.01328420, Validation loss: 0.71522592, Gradient norm: 0.31259744
INFO:root:[   25] Training loss: 0.01341775, Validation loss: 0.64214324, Gradient norm: 0.37802079
INFO:root:[   26] Training loss: 0.01309652, Validation loss: 0.65400354, Gradient norm: 0.36048713
INFO:root:[   27] Training loss: 0.01321265, Validation loss: 0.68235652, Gradient norm: 0.39096194
INFO:root:[   28] Training loss: 0.01258942, Validation loss: 0.65959090, Gradient norm: 0.28803161
INFO:root:[   29] Training loss: 0.01280019, Validation loss: 0.65904350, Gradient norm: 0.36941835
INFO:root:[   30] Training loss: 0.01237933, Validation loss: 0.78930550, Gradient norm: 0.29801772
INFO:root:[   31] Training loss: 0.01261229, Validation loss: 0.64221218, Gradient norm: 0.39106355
INFO:root:[   32] Training loss: 0.01231305, Validation loss: 0.61619569, Gradient norm: 0.35364844
INFO:root:[   33] Training loss: 0.01244739, Validation loss: 0.63111999, Gradient norm: 0.39740559
INFO:root:[   34] Training loss: 0.01207682, Validation loss: 0.68375686, Gradient norm: 0.34204945
INFO:root:[   35] Training loss: 0.01205471, Validation loss: 0.62521031, Gradient norm: 0.34735538
INFO:root:[   36] Training loss: 0.01211485, Validation loss: 0.61442848, Gradient norm: 0.38128681
INFO:root:[   37] Training loss: 0.01209872, Validation loss: 0.69192739, Gradient norm: 0.39981539
INFO:root:[   38] Training loss: 0.01164677, Validation loss: 0.65236084, Gradient norm: 0.34938113
INFO:root:[   39] Training loss: 0.01165319, Validation loss: 0.60249046, Gradient norm: 0.33876185
INFO:root:[   40] Training loss: 0.01169717, Validation loss: 0.60760085, Gradient norm: 0.39000887
INFO:root:[   41] Training loss: 0.01141278, Validation loss: 0.60332363, Gradient norm: 0.32931798
INFO:root:[   42] Training loss: 0.01153359, Validation loss: 0.68278614, Gradient norm: 0.38872140
INFO:root:[   43] Training loss: 0.01141911, Validation loss: 0.65714651, Gradient norm: 0.38139083
INFO:root:[   44] Training loss: 0.01127604, Validation loss: 0.66605960, Gradient norm: 0.36219201
INFO:root:[   45] Training loss: 0.01120464, Validation loss: 0.64244430, Gradient norm: 0.34299778
INFO:root:[   46] Training loss: 0.01113011, Validation loss: 0.66020827, Gradient norm: 0.36709667
INFO:root:[   47] Training loss: 0.01103568, Validation loss: 0.62419053, Gradient norm: 0.36129902
INFO:root:[   48] Training loss: 0.01098467, Validation loss: 0.61544156, Gradient norm: 0.36978218
INFO:root:[   49] Training loss: 0.01083691, Validation loss: 0.58557116, Gradient norm: 0.35481884
INFO:root:[   50] Training loss: 0.01078787, Validation loss: 0.62595499, Gradient norm: 0.35117914
INFO:root:[   51] Training loss: 0.01083078, Validation loss: 0.59437432, Gradient norm: 0.35704467
INFO:root:[   52] Training loss: 0.01059377, Validation loss: 0.61590410, Gradient norm: 0.34432536
INFO:root:[   53] Training loss: 0.01089825, Validation loss: 0.58911884, Gradient norm: 0.42282247
INFO:root:[   54] Training loss: 0.01114280, Validation loss: 0.59786185, Gradient norm: 0.47047829
INFO:root:[   55] Training loss: 0.01051481, Validation loss: 0.65401994, Gradient norm: 0.32978449
INFO:root:[   56] Training loss: 0.01084683, Validation loss: 0.58333998, Gradient norm: 0.43116281
INFO:root:[   57] Training loss: 0.01030939, Validation loss: 0.57508742, Gradient norm: 0.32289120
INFO:root:[   58] Training loss: 0.01035507, Validation loss: 0.59775306, Gradient norm: 0.37280072
INFO:root:[   59] Training loss: 0.01008722, Validation loss: 0.57899803, Gradient norm: 0.33226566
INFO:root:[   60] Training loss: 0.01024649, Validation loss: 0.57432929, Gradient norm: 0.35794617
INFO:root:[   61] Training loss: 0.01040038, Validation loss: 0.63221112, Gradient norm: 0.41059182
INFO:root:[   62] Training loss: 0.01043716, Validation loss: 0.63358556, Gradient norm: 0.43456574
INFO:root:[   63] Training loss: 0.01023996, Validation loss: 0.60592255, Gradient norm: 0.39691442
INFO:root:[   64] Training loss: 0.00991963, Validation loss: 0.56564750, Gradient norm: 0.32786328
INFO:root:[   65] Training loss: 0.00977397, Validation loss: 0.57608381, Gradient norm: 0.34175061
INFO:root:[   66] Training loss: 0.01005603, Validation loss: 0.57168841, Gradient norm: 0.40019806
INFO:root:[   67] Training loss: 0.00968645, Validation loss: 0.58910349, Gradient norm: 0.33981392
INFO:root:[   68] Training loss: 0.01004905, Validation loss: 0.61677833, Gradient norm: 0.42962801
INFO:root:[   69] Training loss: 0.00989150, Validation loss: 0.64436796, Gradient norm: 0.37454853
INFO:root:[   70] Training loss: 0.00994116, Validation loss: 0.63601431, Gradient norm: 0.39435146
INFO:root:[   71] Training loss: 0.00990145, Validation loss: 0.60924348, Gradient norm: 0.42542936
INFO:root:[   72] Training loss: 0.00965660, Validation loss: 0.61314185, Gradient norm: 0.39186029
INFO:root:[   73] Training loss: 0.00978954, Validation loss: 0.57975453, Gradient norm: 0.41687838
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 361.323s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08898
INFO:root:EnergyScoretrain: 0.04783
INFO:root:Coveragetrain: 4.70951
INFO:root:IntervalWidthtrain: 100.38169
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02571
INFO:root:EnergyScorevalidation: 0.01463
INFO:root:Coveragevalidation: 1.2019
INFO:root:IntervalWidthvalidation: 24.66328
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02187
INFO:root:EnergyScoretest: 0.01353
INFO:root:Coveragetest: 0.50524
INFO:root:IntervalWidthtest: 29.22161
INFO:root:###22 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06728996, Validation loss: 2.09946517, Gradient norm: 0.34257015
INFO:root:[    2] Training loss: 0.03727359, Validation loss: 1.64435993, Gradient norm: 0.30445271
INFO:root:[    3] Training loss: 0.03139124, Validation loss: 1.41840500, Gradient norm: 0.34370056
INFO:root:[    4] Training loss: 0.02864774, Validation loss: 1.35110933, Gradient norm: 0.39728440
INFO:root:[    5] Training loss: 0.02627755, Validation loss: 1.23578329, Gradient norm: 0.30907157
INFO:root:[    6] Training loss: 0.02441878, Validation loss: 1.03654439, Gradient norm: 0.32334531
INFO:root:[    7] Training loss: 0.02313302, Validation loss: 1.01870211, Gradient norm: 0.31868119
INFO:root:[    8] Training loss: 0.02251632, Validation loss: 1.00520873, Gradient norm: 0.40420017
INFO:root:[    9] Training loss: 0.02094851, Validation loss: 0.91739240, Gradient norm: 0.29017020
INFO:root:[   10] Training loss: 0.02049770, Validation loss: 0.94815940, Gradient norm: 0.31448859
INFO:root:[   11] Training loss: 0.01959365, Validation loss: 0.82777364, Gradient norm: 0.30738742
INFO:root:[   12] Training loss: 0.01880604, Validation loss: 0.86127212, Gradient norm: 0.26216164
INFO:root:[   13] Training loss: 0.01873142, Validation loss: 0.79396855, Gradient norm: 0.37397216
INFO:root:[   14] Training loss: 0.01820780, Validation loss: 0.89255472, Gradient norm: 0.33718250
INFO:root:[   15] Training loss: 0.01769863, Validation loss: 0.80026664, Gradient norm: 0.31062676
INFO:root:[   16] Training loss: 0.01711768, Validation loss: 0.75822835, Gradient norm: 0.31830837
INFO:root:[   17] Training loss: 0.01693181, Validation loss: 0.73308065, Gradient norm: 0.31805423
INFO:root:[   18] Training loss: 0.01657208, Validation loss: 0.74041294, Gradient norm: 0.30607798
INFO:root:[   19] Training loss: 0.01632364, Validation loss: 0.81042855, Gradient norm: 0.28666920
INFO:root:[   20] Training loss: 0.01626861, Validation loss: 0.75985799, Gradient norm: 0.33446921
INFO:root:[   21] Training loss: 0.01619313, Validation loss: 0.81757183, Gradient norm: 0.38531473
INFO:root:[   22] Training loss: 0.01571679, Validation loss: 0.71017745, Gradient norm: 0.34280715
INFO:root:[   23] Training loss: 0.01545129, Validation loss: 0.69710710, Gradient norm: 0.30876279
INFO:root:[   24] Training loss: 0.01566770, Validation loss: 0.69787639, Gradient norm: 0.38859320
INFO:root:[   25] Training loss: 0.01496985, Validation loss: 0.67275215, Gradient norm: 0.30500306
INFO:root:[   26] Training loss: 0.01510935, Validation loss: 0.82022273, Gradient norm: 0.38463519
INFO:root:[   27] Training loss: 0.01504524, Validation loss: 0.66996715, Gradient norm: 0.39951645
INFO:root:[   28] Training loss: 0.01474263, Validation loss: 0.67490058, Gradient norm: 0.35905926
INFO:root:[   29] Training loss: 0.01474279, Validation loss: 0.71718262, Gradient norm: 0.38524507
INFO:root:[   30] Training loss: 0.01457384, Validation loss: 0.66485951, Gradient norm: 0.39978407
INFO:root:[   31] Training loss: 0.01407871, Validation loss: 0.63575703, Gradient norm: 0.32080989
INFO:root:[   32] Training loss: 0.01414049, Validation loss: 0.64335766, Gradient norm: 0.34036165
INFO:root:[   33] Training loss: 0.01421279, Validation loss: 0.73811150, Gradient norm: 0.37941265
INFO:root:[   34] Training loss: 0.01408526, Validation loss: 0.63571000, Gradient norm: 0.40263260
INFO:root:[   35] Training loss: 0.01360559, Validation loss: 0.82489446, Gradient norm: 0.29800790
INFO:root:[   36] Training loss: 0.01370929, Validation loss: 0.64395435, Gradient norm: 0.35918652
INFO:root:[   37] Training loss: 0.01384049, Validation loss: 0.63924611, Gradient norm: 0.39097639
INFO:root:[   38] Training loss: 0.01337952, Validation loss: 0.65424193, Gradient norm: 0.32935337
INFO:root:[   39] Training loss: 0.01392812, Validation loss: 0.65392006, Gradient norm: 0.47199961
INFO:root:[   40] Training loss: 0.01330095, Validation loss: 0.60859356, Gradient norm: 0.35928524
INFO:root:[   41] Training loss: 0.01304973, Validation loss: 0.69835814, Gradient norm: 0.34711647
INFO:root:[   42] Training loss: 0.01321509, Validation loss: 0.63050278, Gradient norm: 0.37630451
INFO:root:[   43] Training loss: 0.01330490, Validation loss: 0.74688325, Gradient norm: 0.44188246
INFO:root:[   44] Training loss: 0.01332151, Validation loss: 0.61290635, Gradient norm: 0.40572471
INFO:root:[   45] Training loss: 0.01304406, Validation loss: 0.63199325, Gradient norm: 0.40782086
INFO:root:[   46] Training loss: 0.01282041, Validation loss: 0.62638571, Gradient norm: 0.36969788
INFO:root:[   47] Training loss: 0.01297640, Validation loss: 0.77227553, Gradient norm: 0.43176589
INFO:root:[   48] Training loss: 0.01282317, Validation loss: 0.86423423, Gradient norm: 0.39031409
INFO:root:[   49] Training loss: 0.01255851, Validation loss: 0.59097756, Gradient norm: 0.37673856
INFO:root:[   50] Training loss: 0.01251734, Validation loss: 0.62610977, Gradient norm: 0.36662180
INFO:root:[   51] Training loss: 0.01268274, Validation loss: 0.61530672, Gradient norm: 0.41732362
INFO:root:[   52] Training loss: 0.01246207, Validation loss: 0.63899026, Gradient norm: 0.39943723
INFO:root:[   53] Training loss: 0.01225197, Validation loss: 0.69368720, Gradient norm: 0.35579297
INFO:root:[   54] Training loss: 0.01245113, Validation loss: 0.65848336, Gradient norm: 0.41829629
INFO:root:[   55] Training loss: 0.01235262, Validation loss: 0.66171983, Gradient norm: 0.40421153
INFO:root:[   56] Training loss: 0.01248065, Validation loss: 0.72110553, Gradient norm: 0.42473873
INFO:root:[   57] Training loss: 0.01220188, Validation loss: 0.76180251, Gradient norm: 0.39652908
INFO:root:[   58] Training loss: 0.01190847, Validation loss: 0.59957477, Gradient norm: 0.32222130
INFO:root:[   59] Training loss: 0.01184905, Validation loss: 0.57707629, Gradient norm: 0.36264562
INFO:root:[   60] Training loss: 0.01184576, Validation loss: 0.59357461, Gradient norm: 0.34681794
INFO:root:[   61] Training loss: 0.01187015, Validation loss: 0.59428827, Gradient norm: 0.39174741
INFO:root:[   62] Training loss: 0.01184224, Validation loss: 0.58539720, Gradient norm: 0.38471562
INFO:root:[   63] Training loss: 0.01192590, Validation loss: 0.61087529, Gradient norm: 0.38801605
INFO:root:[   64] Training loss: 0.01163502, Validation loss: 0.60048360, Gradient norm: 0.35782050
INFO:root:[   65] Training loss: 0.01146298, Validation loss: 0.64175898, Gradient norm: 0.34361742
INFO:root:[   66] Training loss: 0.01174262, Validation loss: 0.68575541, Gradient norm: 0.43271597
INFO:root:[   67] Training loss: 0.01164025, Validation loss: 0.57732472, Gradient norm: 0.39167831
INFO:root:[   68] Training loss: 0.01189695, Validation loss: 0.62397357, Gradient norm: 0.46457697
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 348.684s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.09394
INFO:root:EnergyScoretrain: 0.05116
INFO:root:Coveragetrain: 4.6523
INFO:root:IntervalWidthtrain: 106.87348
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02943
INFO:root:EnergyScorevalidation: 0.01686
INFO:root:Coveragevalidation: 1.15817
INFO:root:IntervalWidthvalidation: 25.55557
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01856
INFO:root:EnergyScoretest: 0.01115
INFO:root:Coveragetest: 0.59436
INFO:root:IntervalWidthtest: 28.19408
INFO:root:###23 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06637814, Validation loss: 2.12292890, Gradient norm: 0.30573709
INFO:root:[    2] Training loss: 0.03999778, Validation loss: 1.85453799, Gradient norm: 0.29959391
INFO:root:[    3] Training loss: 0.03412175, Validation loss: 1.41759237, Gradient norm: 0.28236117
INFO:root:[    4] Training loss: 0.03076538, Validation loss: 1.28254964, Gradient norm: 0.32331557
INFO:root:[    5] Training loss: 0.02858552, Validation loss: 1.20147912, Gradient norm: 0.30285605
INFO:root:[    6] Training loss: 0.02692898, Validation loss: 1.14180918, Gradient norm: 0.28196124
INFO:root:[    7] Training loss: 0.02592857, Validation loss: 1.18058094, Gradient norm: 0.31474825
INFO:root:[    8] Training loss: 0.02452443, Validation loss: 1.08788926, Gradient norm: 0.29150079
INFO:root:[    9] Training loss: 0.02357315, Validation loss: 0.95756912, Gradient norm: 0.27479918
INFO:root:[   10] Training loss: 0.02261837, Validation loss: 0.98948526, Gradient norm: 0.29778079
INFO:root:[   11] Training loss: 0.02161062, Validation loss: 0.89802745, Gradient norm: 0.28159654
INFO:root:[   12] Training loss: 0.02125227, Validation loss: 0.89485334, Gradient norm: 0.30138748
INFO:root:[   13] Training loss: 0.02051445, Validation loss: 0.86482870, Gradient norm: 0.27332599
INFO:root:[   14] Training loss: 0.02017722, Validation loss: 0.87667782, Gradient norm: 0.28972195
INFO:root:[   15] Training loss: 0.01967073, Validation loss: 0.85091287, Gradient norm: 0.28892952
INFO:root:[   16] Training loss: 0.01910170, Validation loss: 0.87638221, Gradient norm: 0.26006303
INFO:root:[   17] Training loss: 0.01887590, Validation loss: 0.95112322, Gradient norm: 0.29300065
INFO:root:[   18] Training loss: 0.01836526, Validation loss: 0.82665400, Gradient norm: 0.29500837
INFO:root:[   19] Training loss: 0.01820564, Validation loss: 0.89669528, Gradient norm: 0.28236554
INFO:root:[   20] Training loss: 0.01802452, Validation loss: 0.76947733, Gradient norm: 0.31247323
INFO:root:[   21] Training loss: 0.01755387, Validation loss: 0.77420067, Gradient norm: 0.27229158
INFO:root:[   22] Training loss: 0.01766282, Validation loss: 0.73937146, Gradient norm: 0.33728875
INFO:root:[   23] Training loss: 0.01706052, Validation loss: 0.71795076, Gradient norm: 0.26394542
INFO:root:[   24] Training loss: 0.01688009, Validation loss: 0.73017296, Gradient norm: 0.30749309
INFO:root:[   25] Training loss: 0.01684901, Validation loss: 0.85135929, Gradient norm: 0.30313033
INFO:root:[   26] Training loss: 0.01668928, Validation loss: 0.72678504, Gradient norm: 0.34019928
INFO:root:[   27] Training loss: 0.01656872, Validation loss: 0.74865649, Gradient norm: 0.34052827
INFO:root:[   28] Training loss: 0.01618822, Validation loss: 0.71727437, Gradient norm: 0.30639586
INFO:root:[   29] Training loss: 0.01616080, Validation loss: 0.71229368, Gradient norm: 0.29870564
INFO:root:[   30] Training loss: 0.01589888, Validation loss: 0.73277700, Gradient norm: 0.32060871
INFO:root:[   31] Training loss: 0.01587237, Validation loss: 0.70289790, Gradient norm: 0.31989921
INFO:root:[   32] Training loss: 0.01547088, Validation loss: 0.74358337, Gradient norm: 0.27158038
INFO:root:[   33] Training loss: 0.01545543, Validation loss: 0.66766374, Gradient norm: 0.29938657
INFO:root:[   34] Training loss: 0.01570706, Validation loss: 0.72849749, Gradient norm: 0.33338375
INFO:root:[   35] Training loss: 0.01526920, Validation loss: 0.69425919, Gradient norm: 0.32210200
INFO:root:[   36] Training loss: 0.01497330, Validation loss: 0.67126966, Gradient norm: 0.28179726
INFO:root:[   37] Training loss: 0.01511983, Validation loss: 0.72007141, Gradient norm: 0.34806868
INFO:root:[   38] Training loss: 0.01506955, Validation loss: 0.70923082, Gradient norm: 0.37343134
INFO:root:[   39] Training loss: 0.01458103, Validation loss: 0.68002158, Gradient norm: 0.28017086
INFO:root:[   40] Training loss: 0.01451860, Validation loss: 0.66542439, Gradient norm: 0.30772319
INFO:root:[   41] Training loss: 0.01461651, Validation loss: 0.66520007, Gradient norm: 0.33495560
INFO:root:[   42] Training loss: 0.01442266, Validation loss: 0.65632689, Gradient norm: 0.31596029
INFO:root:[   43] Training loss: 0.01470504, Validation loss: 0.65459961, Gradient norm: 0.35795563
INFO:root:[   44] Training loss: 0.01415624, Validation loss: 0.67319006, Gradient norm: 0.29642176
INFO:root:[   45] Training loss: 0.01415474, Validation loss: 0.66756408, Gradient norm: 0.33897822
INFO:root:[   46] Training loss: 0.01394079, Validation loss: 0.65666815, Gradient norm: 0.28758535
INFO:root:[   47] Training loss: 0.01411978, Validation loss: 0.65806483, Gradient norm: 0.33776024
INFO:root:[   48] Training loss: 0.01392034, Validation loss: 0.66736867, Gradient norm: 0.34180083
INFO:root:[   49] Training loss: 0.01392270, Validation loss: 0.62466372, Gradient norm: 0.31984656
INFO:root:[   50] Training loss: 0.01375673, Validation loss: 0.65103777, Gradient norm: 0.33166671
INFO:root:[   51] Training loss: 0.01356090, Validation loss: 0.63037300, Gradient norm: 0.29111168
INFO:root:[   52] Training loss: 0.01380314, Validation loss: 0.64835148, Gradient norm: 0.36019870
INFO:root:[   53] Training loss: 0.01372310, Validation loss: 0.63586576, Gradient norm: 0.39402360
INFO:root:[   54] Training loss: 0.01363083, Validation loss: 0.64263210, Gradient norm: 0.36146674
INFO:root:[   55] Training loss: 0.01355165, Validation loss: 0.62630510, Gradient norm: 0.34400656
INFO:root:[   56] Training loss: 0.01359909, Validation loss: 0.61450792, Gradient norm: 0.36908035
INFO:root:[   57] Training loss: 0.01333932, Validation loss: 0.60567490, Gradient norm: 0.35578068
INFO:root:[   58] Training loss: 0.01326552, Validation loss: 0.68386962, Gradient norm: 0.33113382
INFO:root:[   59] Training loss: 0.01315604, Validation loss: 0.65328491, Gradient norm: 0.31193125
INFO:root:[   60] Training loss: 0.01328895, Validation loss: 0.67375302, Gradient norm: 0.35744218
INFO:root:[   61] Training loss: 0.01304622, Validation loss: 0.63752533, Gradient norm: 0.32457537
INFO:root:[   62] Training loss: 0.01299299, Validation loss: 0.63557399, Gradient norm: 0.33707127
INFO:root:[   63] Training loss: 0.01309720, Validation loss: 0.60886530, Gradient norm: 0.35898985
INFO:root:[   64] Training loss: 0.01301479, Validation loss: 0.64636634, Gradient norm: 0.35910792
INFO:root:[   65] Training loss: 0.01299424, Validation loss: 0.60673353, Gradient norm: 0.35890238
INFO:root:[   66] Training loss: 0.01265819, Validation loss: 0.65781170, Gradient norm: 0.30813283
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 339.639s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.1081
INFO:root:EnergyScoretrain: 0.05931
INFO:root:Coveragetrain: 4.44606
INFO:root:IntervalWidthtrain: 114.61759
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03038
INFO:root:EnergyScorevalidation: 0.01639
INFO:root:Coveragevalidation: 1.1807
INFO:root:IntervalWidthvalidation: 28.80273
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02194
INFO:root:EnergyScoretest: 0.01342
INFO:root:Coveragetest: 0.54717
INFO:root:IntervalWidthtest: 31.46493
INFO:root:###24 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07860400, Validation loss: 2.27149491, Gradient norm: 0.33387167
INFO:root:[    2] Training loss: 0.04413738, Validation loss: 2.10583774, Gradient norm: 0.26545084
INFO:root:[    3] Training loss: 0.03959772, Validation loss: 1.65264542, Gradient norm: 0.26871727
INFO:root:[    4] Training loss: 0.03586675, Validation loss: 1.44935996, Gradient norm: 0.31300854
INFO:root:[    5] Training loss: 0.03308840, Validation loss: 1.42167510, Gradient norm: 0.30397978
INFO:root:[    6] Training loss: 0.03131096, Validation loss: 1.23727024, Gradient norm: 0.27930444
INFO:root:[    7] Training loss: 0.02947678, Validation loss: 1.24406868, Gradient norm: 0.25148798
INFO:root:[    8] Training loss: 0.02817641, Validation loss: 1.15451983, Gradient norm: 0.28416449
INFO:root:[    9] Training loss: 0.02729588, Validation loss: 1.22215995, Gradient norm: 0.29735856
INFO:root:[   10] Training loss: 0.02651252, Validation loss: 1.08112395, Gradient norm: 0.31773348
INFO:root:[   11] Training loss: 0.02525582, Validation loss: 1.01853508, Gradient norm: 0.27518249
INFO:root:[   12] Training loss: 0.02468962, Validation loss: 1.16897189, Gradient norm: 0.31950514
INFO:root:[   13] Training loss: 0.02407502, Validation loss: 0.98156225, Gradient norm: 0.28506440
INFO:root:[   14] Training loss: 0.02349822, Validation loss: 0.95462795, Gradient norm: 0.28464143
INFO:root:[   15] Training loss: 0.02283680, Validation loss: 0.92195392, Gradient norm: 0.25016711
INFO:root:[   16] Training loss: 0.02249756, Validation loss: 1.10580259, Gradient norm: 0.30567208
INFO:root:[   17] Training loss: 0.02208786, Validation loss: 1.09474383, Gradient norm: 0.32244657
INFO:root:[   18] Training loss: 0.02159601, Validation loss: 1.02563983, Gradient norm: 0.27490932
INFO:root:[   19] Training loss: 0.02115590, Validation loss: 0.86484137, Gradient norm: 0.26852833
INFO:root:[   20] Training loss: 0.02084460, Validation loss: 0.86250811, Gradient norm: 0.29487052
INFO:root:[   21] Training loss: 0.02062660, Validation loss: 0.87014557, Gradient norm: 0.32645729
INFO:root:[   22] Training loss: 0.02017874, Validation loss: 0.87341730, Gradient norm: 0.27677224
INFO:root:[   23] Training loss: 0.01990323, Validation loss: 0.82452567, Gradient norm: 0.31114344
INFO:root:[   24] Training loss: 0.01988651, Validation loss: 0.93851916, Gradient norm: 0.33369717
INFO:root:[   25] Training loss: 0.01968777, Validation loss: 0.92236735, Gradient norm: 0.34084446
INFO:root:[   26] Training loss: 0.01920301, Validation loss: 0.89105834, Gradient norm: 0.30706099
INFO:root:[   27] Training loss: 0.01904930, Validation loss: 0.93494032, Gradient norm: 0.30534158
INFO:root:[   28] Training loss: 0.01886374, Validation loss: 0.81213930, Gradient norm: 0.31207043
INFO:root:[   29] Training loss: 0.01849605, Validation loss: 0.90302503, Gradient norm: 0.28039642
INFO:root:[   30] Training loss: 0.01893914, Validation loss: 0.81708944, Gradient norm: 0.36193671
INFO:root:[   31] Training loss: 0.01846395, Validation loss: 0.84501451, Gradient norm: 0.32348639
INFO:root:[   32] Training loss: 0.01840302, Validation loss: 0.80032427, Gradient norm: 0.33042480
INFO:root:[   33] Training loss: 0.01805491, Validation loss: 1.03216801, Gradient norm: 0.32144315
INFO:root:[   34] Training loss: 0.01790483, Validation loss: 0.74713124, Gradient norm: 0.34790009
INFO:root:[   35] Training loss: 0.01777507, Validation loss: 0.83596742, Gradient norm: 0.32884960
INFO:root:[   36] Training loss: 0.01749575, Validation loss: 0.91011178, Gradient norm: 0.31830200
INFO:root:[   37] Training loss: 0.01765283, Validation loss: 0.81927164, Gradient norm: 0.33575612
INFO:root:[   38] Training loss: 0.01740753, Validation loss: 0.85291636, Gradient norm: 0.33114147
INFO:root:[   39] Training loss: 0.01710837, Validation loss: 0.86159193, Gradient norm: 0.29702768
INFO:root:[   40] Training loss: 0.01729505, Validation loss: 0.97165020, Gradient norm: 0.36310165
INFO:root:[   41] Training loss: 0.01726052, Validation loss: 0.77659377, Gradient norm: 0.37389678
INFO:root:[   42] Training loss: 0.01687852, Validation loss: 0.74496972, Gradient norm: 0.32927735
INFO:root:[   43] Training loss: 0.01670493, Validation loss: 0.87033806, Gradient norm: 0.32842570
INFO:root:[   44] Training loss: 0.01680614, Validation loss: 0.82908413, Gradient norm: 0.35945887
INFO:root:[   45] Training loss: 0.01644616, Validation loss: 0.84516442, Gradient norm: 0.29557100
INFO:root:[   46] Training loss: 0.01627009, Validation loss: 0.69185582, Gradient norm: 0.30438468
INFO:root:[   47] Training loss: 0.01612581, Validation loss: 0.70812896, Gradient norm: 0.30381886
INFO:root:[   48] Training loss: 0.01611442, Validation loss: 0.70630973, Gradient norm: 0.31431853
INFO:root:[   49] Training loss: 0.01622057, Validation loss: 0.74024190, Gradient norm: 0.33501374
INFO:root:[   50] Training loss: 0.01631156, Validation loss: 0.72142739, Gradient norm: 0.33485367
INFO:root:[   51] Training loss: 0.01607429, Validation loss: 0.87340812, Gradient norm: 0.34085750
INFO:root:[   52] Training loss: 0.01610176, Validation loss: 0.74311570, Gradient norm: 0.37349628
INFO:root:[   53] Training loss: 0.01573787, Validation loss: 0.74214495, Gradient norm: 0.32465554
INFO:root:[   54] Training loss: 0.01562706, Validation loss: 0.75846220, Gradient norm: 0.32349178
INFO:root:[   55] Training loss: 0.01565362, Validation loss: 0.71825025, Gradient norm: 0.32954597
INFO:root:[   56] Training loss: 0.01574364, Validation loss: 0.71680726, Gradient norm: 0.36991078
INFO:root:[   57] Training loss: 0.01563768, Validation loss: 0.72145746, Gradient norm: 0.33756503
INFO:root:[   58] Training loss: 0.01546684, Validation loss: 0.70393590, Gradient norm: 0.33315955
INFO:root:[   59] Training loss: 0.01543948, Validation loss: 0.80706239, Gradient norm: 0.37205586
INFO:root:[   60] Training loss: 0.01510046, Validation loss: 0.72659393, Gradient norm: 0.31839949
INFO:root:[   61] Training loss: 0.01525757, Validation loss: 0.78006299, Gradient norm: 0.32152020
INFO:root:[   62] Training loss: 0.01501786, Validation loss: 0.70099423, Gradient norm: 0.34684466
INFO:root:[   63] Training loss: 0.01484996, Validation loss: 0.67963412, Gradient norm: 0.30509199
INFO:root:[   64] Training loss: 0.01543656, Validation loss: 0.86080798, Gradient norm: 0.40850324
INFO:root:[   65] Training loss: 0.01485531, Validation loss: 0.65645899, Gradient norm: 0.32607369
INFO:root:[   66] Training loss: 0.01498280, Validation loss: 0.75142134, Gradient norm: 0.34149795
INFO:root:[   67] Training loss: 0.01469341, Validation loss: 0.75486585, Gradient norm: 0.30442319
INFO:root:[   68] Training loss: 0.01469672, Validation loss: 0.69432644, Gradient norm: 0.32535778
INFO:root:[   69] Training loss: 0.01465799, Validation loss: 0.66418776, Gradient norm: 0.34973548
INFO:root:[   70] Training loss: 0.01442464, Validation loss: 0.75545526, Gradient norm: 0.31526800
INFO:root:[   71] Training loss: 0.01451733, Validation loss: 0.71538563, Gradient norm: 0.37284548
INFO:root:[   72] Training loss: 0.01443382, Validation loss: 0.67105945, Gradient norm: 0.33411498
INFO:root:[   73] Training loss: 0.01445637, Validation loss: 0.64664663, Gradient norm: 0.34100797
INFO:root:[   74] Training loss: 0.01430089, Validation loss: 0.69529188, Gradient norm: 0.31489761
INFO:root:[   75] Training loss: 0.01418065, Validation loss: 0.74232625, Gradient norm: 0.33140436
INFO:root:[   76] Training loss: 0.01422993, Validation loss: 0.73875536, Gradient norm: 0.35273537
INFO:root:[   77] Training loss: 0.01432082, Validation loss: 0.65112760, Gradient norm: 0.36145119
INFO:root:[   78] Training loss: 0.01407582, Validation loss: 0.66512334, Gradient norm: 0.34654298
INFO:root:[   79] Training loss: 0.01409682, Validation loss: 0.66829317, Gradient norm: 0.37843599
INFO:root:[   80] Training loss: 0.01425118, Validation loss: 0.63576876, Gradient norm: 0.37119097
INFO:root:[   81] Training loss: 0.01391360, Validation loss: 0.77525665, Gradient norm: 0.30217339
INFO:root:[   82] Training loss: 0.01392574, Validation loss: 0.61849393, Gradient norm: 0.32521108
INFO:root:[   83] Training loss: 0.01391992, Validation loss: 0.61394215, Gradient norm: 0.33196656
INFO:root:[   84] Training loss: 0.01403340, Validation loss: 0.65254383, Gradient norm: 0.37960186
INFO:root:[   85] Training loss: 0.01351430, Validation loss: 0.69468291, Gradient norm: 0.30840878
INFO:root:[   86] Training loss: 0.01363317, Validation loss: 0.69690120, Gradient norm: 0.32224798
INFO:root:[   87] Training loss: 0.01363774, Validation loss: 0.65224726, Gradient norm: 0.35245281
INFO:root:[   88] Training loss: 0.01353991, Validation loss: 0.66313124, Gradient norm: 0.35756713
INFO:root:[   89] Training loss: 0.01358428, Validation loss: 0.66577447, Gradient norm: 0.34791760
INFO:root:[   90] Training loss: 0.01351402, Validation loss: 0.71773002, Gradient norm: 0.36172523
INFO:root:[   91] Training loss: 0.01341888, Validation loss: 0.85582669, Gradient norm: 0.30568311
INFO:root:[   92] Training loss: 0.01351653, Validation loss: 0.70237039, Gradient norm: 0.39245854
INFO:root:EP 92: Early stopping
INFO:root:Training the model took 453.745s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.11486
INFO:root:EnergyScoretrain: 0.06297
INFO:root:Coveragetrain: 4.57094
INFO:root:IntervalWidthtrain: 128.284
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03434
INFO:root:EnergyScorevalidation: 0.01862
INFO:root:Coveragevalidation: 1.23603
INFO:root:IntervalWidthvalidation: 37.78139
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02609
INFO:root:EnergyScoretest: 0.01537
INFO:root:Coveragetest: 0.54227
INFO:root:IntervalWidthtest: 41.47469
INFO:root:###25 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 299892736
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06748281, Validation loss: 2.13732111, Gradient norm: 0.39814161
INFO:root:[    2] Training loss: 0.03565478, Validation loss: 1.46268172, Gradient norm: 0.33243557
INFO:root:[    3] Training loss: 0.02966617, Validation loss: 1.31586873, Gradient norm: 0.37325864
INFO:root:[    4] Training loss: 0.02634337, Validation loss: 1.17618945, Gradient norm: 0.30310684
INFO:root:[    5] Training loss: 0.02462477, Validation loss: 1.28817970, Gradient norm: 0.36611514
INFO:root:[    6] Training loss: 0.02297289, Validation loss: 1.31797360, Gradient norm: 0.34127589
INFO:root:[    7] Training loss: 0.02124726, Validation loss: 1.14481538, Gradient norm: 0.28600588
INFO:root:[    8] Training loss: 0.02028321, Validation loss: 0.97896907, Gradient norm: 0.30936327
INFO:root:[    9] Training loss: 0.01937644, Validation loss: 0.92426903, Gradient norm: 0.32086673
INFO:root:[   10] Training loss: 0.01857076, Validation loss: 0.87494894, Gradient norm: 0.33902361
INFO:root:[   11] Training loss: 0.01781232, Validation loss: 0.87559634, Gradient norm: 0.35359508
INFO:root:[   12] Training loss: 0.01783737, Validation loss: 0.82372873, Gradient norm: 0.45388900
INFO:root:[   13] Training loss: 0.01697261, Validation loss: 0.78174988, Gradient norm: 0.37040099
INFO:root:[   14] Training loss: 0.01633690, Validation loss: 0.79736873, Gradient norm: 0.32062553
INFO:root:[   15] Training loss: 0.01620356, Validation loss: 0.74336738, Gradient norm: 0.38894114
INFO:root:[   16] Training loss: 0.01620883, Validation loss: 0.74733466, Gradient norm: 0.44928029
INFO:root:[   17] Training loss: 0.01550569, Validation loss: 0.78037594, Gradient norm: 0.36182502
INFO:root:[   18] Training loss: 0.01499298, Validation loss: 0.79436272, Gradient norm: 0.33149325
INFO:root:[   19] Training loss: 0.01505639, Validation loss: 0.71218883, Gradient norm: 0.39798529
INFO:root:[   20] Training loss: 0.01492926, Validation loss: 0.69423297, Gradient norm: 0.38219280
INFO:root:[   21] Training loss: 0.01467234, Validation loss: 0.71733580, Gradient norm: 0.36704798
INFO:root:[   22] Training loss: 0.01430329, Validation loss: 0.71915116, Gradient norm: 0.36127652
INFO:root:[   23] Training loss: 0.01435118, Validation loss: 0.83265278, Gradient norm: 0.42045705
INFO:root:[   24] Training loss: 0.01414566, Validation loss: 0.67213598, Gradient norm: 0.40235210
INFO:root:[   25] Training loss: 0.01412542, Validation loss: 0.67519753, Gradient norm: 0.41725487
INFO:root:[   26] Training loss: 0.01371152, Validation loss: 0.70176526, Gradient norm: 0.38399461
INFO:root:[   27] Training loss: 0.01358830, Validation loss: 0.69065417, Gradient norm: 0.36286834
INFO:root:[   28] Training loss: 0.01387300, Validation loss: 0.68170525, Gradient norm: 0.47398702
INFO:root:[   29] Training loss: 0.01341861, Validation loss: 0.65820683, Gradient norm: 0.42285256
INFO:root:[   30] Training loss: 0.01325376, Validation loss: 0.67628720, Gradient norm: 0.35927458
INFO:root:[   31] Training loss: 0.01294437, Validation loss: 0.64633509, Gradient norm: 0.34167313
INFO:root:[   32] Training loss: 0.01284858, Validation loss: 0.66326047, Gradient norm: 0.36771026
INFO:root:[   33] Training loss: 0.01307481, Validation loss: 0.67956519, Gradient norm: 0.42235126
INFO:root:[   34] Training loss: 0.01302495, Validation loss: 0.66369681, Gradient norm: 0.42978149
INFO:root:[   35] Training loss: 0.01252445, Validation loss: 0.64487137, Gradient norm: 0.35568762
INFO:root:[   36] Training loss: 0.01260575, Validation loss: 0.64555823, Gradient norm: 0.40126881
INFO:root:[   37] Training loss: 0.01248675, Validation loss: 0.62755500, Gradient norm: 0.39566254
INFO:root:[   38] Training loss: 0.01228163, Validation loss: 0.63279131, Gradient norm: 0.38649166
INFO:root:[   39] Training loss: 0.01220409, Validation loss: 0.61692717, Gradient norm: 0.39675231
INFO:root:[   40] Training loss: 0.01235299, Validation loss: 0.64693558, Gradient norm: 0.42344181
INFO:root:[   41] Training loss: 0.01225573, Validation loss: 0.62504095, Gradient norm: 0.43658035
INFO:root:[   42] Training loss: 0.01218431, Validation loss: 0.61520423, Gradient norm: 0.41761203
INFO:root:[   43] Training loss: 0.01201463, Validation loss: 0.64145679, Gradient norm: 0.39697139
INFO:root:[   44] Training loss: 0.01203215, Validation loss: 0.71148416, Gradient norm: 0.41785575
INFO:root:[   45] Training loss: 0.01220440, Validation loss: 0.68590121, Gradient norm: 0.47478681
INFO:root:[   46] Training loss: 0.01177066, Validation loss: 0.74723165, Gradient norm: 0.40577552
INFO:root:[   47] Training loss: 0.01174360, Validation loss: 0.60833973, Gradient norm: 0.43599076
INFO:root:[   48] Training loss: 0.01153914, Validation loss: 0.70333900, Gradient norm: 0.36961151
INFO:root:[   49] Training loss: 0.01195381, Validation loss: 0.64725463, Gradient norm: 0.49551606
INFO:root:[   50] Training loss: 0.01142950, Validation loss: 0.64600623, Gradient norm: 0.38660348
INFO:root:[   51] Training loss: 0.01166516, Validation loss: 0.64328618, Gradient norm: 0.45751104
INFO:root:[   52] Training loss: 0.01152683, Validation loss: 0.60326882, Gradient norm: 0.42130015
INFO:root:[   53] Training loss: 0.01150748, Validation loss: 0.61698216, Gradient norm: 0.45048022
INFO:root:[   54] Training loss: 0.01099418, Validation loss: 0.61727734, Gradient norm: 0.34522021
INFO:root:[   55] Training loss: 0.01141727, Validation loss: 0.62469795, Gradient norm: 0.44077875
INFO:root:[   56] Training loss: 0.01103958, Validation loss: 0.61856962, Gradient norm: 0.39506020
INFO:root:[   57] Training loss: 0.01114257, Validation loss: 0.62118817, Gradient norm: 0.42304250
INFO:root:[   58] Training loss: 0.01134224, Validation loss: 0.60340342, Gradient norm: 0.48750139
INFO:root:[   59] Training loss: 0.01080691, Validation loss: 0.71157618, Gradient norm: 0.37148505
INFO:root:[   60] Training loss: 0.01102018, Validation loss: 0.64252494, Gradient norm: 0.42810390
INFO:root:[   61] Training loss: 0.01126919, Validation loss: 0.59692503, Gradient norm: 0.48754433
INFO:root:[   62] Training loss: 0.01068159, Validation loss: 0.62003052, Gradient norm: 0.39328026
INFO:root:[   63] Training loss: 0.01069074, Validation loss: 0.65639345, Gradient norm: 0.39999085
INFO:root:[   64] Training loss: 0.01099554, Validation loss: 0.61710934, Gradient norm: 0.46898600
INFO:root:[   65] Training loss: 0.01110855, Validation loss: 0.64007442, Gradient norm: 0.51406092
INFO:root:[   66] Training loss: 0.01050368, Validation loss: 0.61973357, Gradient norm: 0.38493529
INFO:root:[   67] Training loss: 0.01025610, Validation loss: 0.60985324, Gradient norm: 0.35238722
INFO:root:[   68] Training loss: 0.01072093, Validation loss: 0.71556800, Gradient norm: 0.45421040
INFO:root:[   69] Training loss: 0.01071687, Validation loss: 0.59674426, Gradient norm: 0.48521226
INFO:root:[   70] Training loss: 0.01015996, Validation loss: 0.62433677, Gradient norm: 0.34121557
INFO:root:[   71] Training loss: 0.01067741, Validation loss: 0.61367525, Gradient norm: 0.45396962
INFO:root:[   72] Training loss: 0.01038443, Validation loss: 0.59850971, Gradient norm: 0.44873572
INFO:root:[   73] Training loss: 0.01031425, Validation loss: 0.57525175, Gradient norm: 0.40972592
INFO:root:[   74] Training loss: 0.01002751, Validation loss: 0.59381995, Gradient norm: 0.38447637
INFO:root:[   75] Training loss: 0.01042588, Validation loss: 0.63033296, Gradient norm: 0.48488687
INFO:root:[   76] Training loss: 0.01009607, Validation loss: 0.58858288, Gradient norm: 0.40252110
INFO:root:[   77] Training loss: 0.01018203, Validation loss: 0.63600345, Gradient norm: 0.44975012
INFO:root:[   78] Training loss: 0.00986001, Validation loss: 0.61792322, Gradient norm: 0.36362967
INFO:root:[   79] Training loss: 0.01004915, Validation loss: 0.60562130, Gradient norm: 0.41021880
INFO:root:[   80] Training loss: 0.00987383, Validation loss: 0.60528139, Gradient norm: 0.37785562
INFO:root:[   81] Training loss: 0.00978199, Validation loss: 0.58834839, Gradient norm: 0.37648737
INFO:root:[   82] Training loss: 0.01001804, Validation loss: 0.66225837, Gradient norm: 0.44914979
INFO:root:EP 82: Early stopping
INFO:root:Training the model took 405.206s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.10448
INFO:root:EnergyScoretrain: 0.05601
INFO:root:Coveragetrain: 4.70363
INFO:root:IntervalWidthtrain: 126.70657
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02797
INFO:root:EnergyScorevalidation: 0.01513
INFO:root:Coveragevalidation: 1.25347
INFO:root:IntervalWidthvalidation: 32.2799
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.03141
INFO:root:EnergyScoretest: 0.02016
INFO:root:Coveragetest: 0.44583
INFO:root:IntervalWidthtest: 38.39317
INFO:root:###26 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06883320, Validation loss: 2.08964813, Gradient norm: 0.34044804
INFO:root:[    2] Training loss: 0.03950296, Validation loss: 1.76313290, Gradient norm: 0.33224880
INFO:root:[    3] Training loss: 0.03288141, Validation loss: 1.36030852, Gradient norm: 0.29367548
INFO:root:[    4] Training loss: 0.02924309, Validation loss: 1.51720083, Gradient norm: 0.29250169
INFO:root:[    5] Training loss: 0.02727347, Validation loss: 1.18676797, Gradient norm: 0.30921111
INFO:root:[    6] Training loss: 0.02513941, Validation loss: 1.21661040, Gradient norm: 0.29467834
INFO:root:[    7] Training loss: 0.02397117, Validation loss: 1.00013433, Gradient norm: 0.32084967
INFO:root:[    8] Training loss: 0.02300519, Validation loss: 1.14157286, Gradient norm: 0.36444797
INFO:root:[    9] Training loss: 0.02198259, Validation loss: 0.97446742, Gradient norm: 0.34453234
INFO:root:[   10] Training loss: 0.02098504, Validation loss: 1.10774808, Gradient norm: 0.32564538
INFO:root:[   11] Training loss: 0.02058629, Validation loss: 0.86968172, Gradient norm: 0.35033709
INFO:root:[   12] Training loss: 0.01987994, Validation loss: 0.88434684, Gradient norm: 0.35122751
INFO:root:[   13] Training loss: 0.01902494, Validation loss: 0.85732787, Gradient norm: 0.27262579
INFO:root:[   14] Training loss: 0.01851132, Validation loss: 0.82397545, Gradient norm: 0.28996649
INFO:root:[   15] Training loss: 0.01843505, Validation loss: 1.02712954, Gradient norm: 0.34133996
INFO:root:[   16] Training loss: 0.01810795, Validation loss: 0.90128162, Gradient norm: 0.35027136
INFO:root:[   17] Training loss: 0.01784419, Validation loss: 0.83244029, Gradient norm: 0.38339453
INFO:root:[   18] Training loss: 0.01718779, Validation loss: 0.76433586, Gradient norm: 0.32865677
INFO:root:[   19] Training loss: 0.01710632, Validation loss: 0.75580275, Gradient norm: 0.34929223
INFO:root:[   20] Training loss: 0.01706228, Validation loss: 0.79904136, Gradient norm: 0.36890639
INFO:root:[   21] Training loss: 0.01663068, Validation loss: 0.71106815, Gradient norm: 0.34584793
INFO:root:[   22] Training loss: 0.01649411, Validation loss: 0.72966563, Gradient norm: 0.37828775
INFO:root:[   23] Training loss: 0.01602303, Validation loss: 0.72877244, Gradient norm: 0.31370116
INFO:root:[   24] Training loss: 0.01583234, Validation loss: 0.84262052, Gradient norm: 0.31768577
INFO:root:[   25] Training loss: 0.01605957, Validation loss: 0.78243195, Gradient norm: 0.41640431
INFO:root:[   26] Training loss: 0.01581593, Validation loss: 0.70346515, Gradient norm: 0.40666077
INFO:root:[   27] Training loss: 0.01560927, Validation loss: 0.73068776, Gradient norm: 0.39495034
INFO:root:[   28] Training loss: 0.01551625, Validation loss: 0.69199282, Gradient norm: 0.40404653
INFO:root:[   29] Training loss: 0.01515552, Validation loss: 0.69222825, Gradient norm: 0.34168252
INFO:root:[   30] Training loss: 0.01493445, Validation loss: 0.82925223, Gradient norm: 0.33052221
INFO:root:[   31] Training loss: 0.01504667, Validation loss: 0.83012441, Gradient norm: 0.37697051
INFO:root:[   32] Training loss: 0.01457524, Validation loss: 0.67530738, Gradient norm: 0.31255941
INFO:root:[   33] Training loss: 0.01509658, Validation loss: 0.76733954, Gradient norm: 0.43734625
INFO:root:[   34] Training loss: 0.01471390, Validation loss: 0.69754376, Gradient norm: 0.40022755
INFO:root:[   35] Training loss: 0.01440288, Validation loss: 0.67480785, Gradient norm: 0.35239018
INFO:root:[   36] Training loss: 0.01422802, Validation loss: 0.68254330, Gradient norm: 0.35671881
INFO:root:[   37] Training loss: 0.01427246, Validation loss: 0.71411032, Gradient norm: 0.36911037
INFO:root:[   38] Training loss: 0.01401795, Validation loss: 0.64196255, Gradient norm: 0.32307917
INFO:root:[   39] Training loss: 0.01413482, Validation loss: 0.62697801, Gradient norm: 0.36294351
INFO:root:[   40] Training loss: 0.01388403, Validation loss: 0.70888914, Gradient norm: 0.36450828
INFO:root:[   41] Training loss: 0.01411127, Validation loss: 0.68513208, Gradient norm: 0.42047399
INFO:root:[   42] Training loss: 0.01378945, Validation loss: 0.63365998, Gradient norm: 0.36555412
INFO:root:[   43] Training loss: 0.01368025, Validation loss: 0.75324935, Gradient norm: 0.37273778
INFO:root:[   44] Training loss: 0.01397852, Validation loss: 0.69450458, Gradient norm: 0.45924579
INFO:root:[   45] Training loss: 0.01375536, Validation loss: 0.69892042, Gradient norm: 0.45297334
INFO:root:[   46] Training loss: 0.01359509, Validation loss: 0.99922231, Gradient norm: 0.40642199
INFO:root:[   47] Training loss: 0.01355970, Validation loss: 0.64665307, Gradient norm: 0.40790944
INFO:root:[   48] Training loss: 0.01311275, Validation loss: 0.95256944, Gradient norm: 0.35512515
INFO:root:[   49] Training loss: 0.01321178, Validation loss: 0.70170246, Gradient norm: 0.39557363
INFO:root:[   50] Training loss: 0.01317311, Validation loss: 0.73874169, Gradient norm: 0.39617555
INFO:root:[   51] Training loss: 0.01315446, Validation loss: 0.65107213, Gradient norm: 0.40961536
INFO:root:[   52] Training loss: 0.01298267, Validation loss: 0.67342030, Gradient norm: 0.36951291
INFO:root:[   53] Training loss: 0.01313119, Validation loss: 0.61197550, Gradient norm: 0.39613672
INFO:root:[   54] Training loss: 0.01311182, Validation loss: 0.64668650, Gradient norm: 0.42144314
INFO:root:[   55] Training loss: 0.01293829, Validation loss: 0.62157354, Gradient norm: 0.40074241
INFO:root:[   56] Training loss: 0.01297039, Validation loss: 0.80998879, Gradient norm: 0.42495841
INFO:root:[   57] Training loss: 0.01268062, Validation loss: 0.64692364, Gradient norm: 0.36330882
INFO:root:[   58] Training loss: 0.01263867, Validation loss: 0.63115009, Gradient norm: 0.36562895
INFO:root:[   59] Training loss: 0.01287523, Validation loss: 0.61254530, Gradient norm: 0.42200021
INFO:root:[   60] Training loss: 0.01288173, Validation loss: 0.66759235, Gradient norm: 0.45000902
INFO:root:[   61] Training loss: 0.01278257, Validation loss: 0.69516107, Gradient norm: 0.46208433
INFO:root:[   62] Training loss: 0.01255758, Validation loss: 0.88250281, Gradient norm: 0.38840865
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 307.747s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.1146
INFO:root:EnergyScoretrain: 0.06084
INFO:root:Coveragetrain: 4.61543
INFO:root:IntervalWidthtrain: 126.72793
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03259
INFO:root:EnergyScorevalidation: 0.01786
INFO:root:Coveragevalidation: 1.20234
INFO:root:IntervalWidthvalidation: 34.63402
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02925
INFO:root:EnergyScoretest: 0.01749
INFO:root:Coveragetest: 0.60926
INFO:root:IntervalWidthtest: 45.56456
INFO:root:###27 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06448755, Validation loss: 2.10715730, Gradient norm: 0.29514438
INFO:root:[    2] Training loss: 0.04078465, Validation loss: 1.73294828, Gradient norm: 0.28575852
INFO:root:[    3] Training loss: 0.03513608, Validation loss: 1.49031005, Gradient norm: 0.23789884
INFO:root:[    4] Training loss: 0.03124612, Validation loss: 1.30731895, Gradient norm: 0.28752347
INFO:root:[    5] Training loss: 0.02878438, Validation loss: 1.20447054, Gradient norm: 0.28444687
INFO:root:[    6] Training loss: 0.02667979, Validation loss: 1.30344777, Gradient norm: 0.27806707
INFO:root:[    7] Training loss: 0.02509066, Validation loss: 1.18187565, Gradient norm: 0.23132879
INFO:root:[    8] Training loss: 0.02418530, Validation loss: 1.05614397, Gradient norm: 0.30741645
INFO:root:[    9] Training loss: 0.02357951, Validation loss: 0.96302655, Gradient norm: 0.33468689
INFO:root:[   10] Training loss: 0.02245208, Validation loss: 0.94408574, Gradient norm: 0.31731335
INFO:root:[   11] Training loss: 0.02125723, Validation loss: 0.90304926, Gradient norm: 0.26894490
INFO:root:[   12] Training loss: 0.02085445, Validation loss: 0.86358442, Gradient norm: 0.27416378
INFO:root:[   13] Training loss: 0.02073973, Validation loss: 0.96584237, Gradient norm: 0.36703019
INFO:root:[   14] Training loss: 0.01982598, Validation loss: 0.96555324, Gradient norm: 0.27659088
INFO:root:[   15] Training loss: 0.01960315, Validation loss: 0.82748466, Gradient norm: 0.32604125
INFO:root:[   16] Training loss: 0.01919019, Validation loss: 0.79526816, Gradient norm: 0.33332168
INFO:root:[   17] Training loss: 0.01885348, Validation loss: 0.81042644, Gradient norm: 0.28247202
INFO:root:[   18] Training loss: 0.01851014, Validation loss: 0.94192854, Gradient norm: 0.32331459
INFO:root:[   19] Training loss: 0.01843583, Validation loss: 0.95357648, Gradient norm: 0.34511641
INFO:root:[   20] Training loss: 0.01801943, Validation loss: 0.77731272, Gradient norm: 0.35484374
INFO:root:[   21] Training loss: 0.01783958, Validation loss: 0.78667921, Gradient norm: 0.36919746
INFO:root:[   22] Training loss: 0.01750719, Validation loss: 0.81257577, Gradient norm: 0.33867203
INFO:root:[   23] Training loss: 0.01737147, Validation loss: 0.74720388, Gradient norm: 0.36248258
INFO:root:[   24] Training loss: 0.01703348, Validation loss: 0.81498421, Gradient norm: 0.34113154
INFO:root:[   25] Training loss: 0.01661960, Validation loss: 0.88357458, Gradient norm: 0.29673897
INFO:root:[   26] Training loss: 0.01699423, Validation loss: 0.83314426, Gradient norm: 0.37765942
INFO:root:[   27] Training loss: 0.01655686, Validation loss: 0.78096407, Gradient norm: 0.35229990
INFO:root:[   28] Training loss: 0.01642275, Validation loss: 0.83557593, Gradient norm: 0.34257519
INFO:root:[   29] Training loss: 0.01636249, Validation loss: 0.72885460, Gradient norm: 0.37263912
INFO:root:[   30] Training loss: 0.01623771, Validation loss: 0.68851395, Gradient norm: 0.38317811
INFO:root:[   31] Training loss: 0.01599077, Validation loss: 0.71833318, Gradient norm: 0.37341094
INFO:root:[   32] Training loss: 0.01577534, Validation loss: 0.79473429, Gradient norm: 0.37967556
INFO:root:[   33] Training loss: 0.01581533, Validation loss: 0.71651468, Gradient norm: 0.37619305
INFO:root:[   34] Training loss: 0.01531511, Validation loss: 0.70760136, Gradient norm: 0.32254955
INFO:root:[   35] Training loss: 0.01540647, Validation loss: 0.74232671, Gradient norm: 0.36971186
INFO:root:[   36] Training loss: 0.01550110, Validation loss: 0.71745817, Gradient norm: 0.39618830
INFO:root:[   37] Training loss: 0.01534354, Validation loss: 0.68549000, Gradient norm: 0.40979291
INFO:root:[   38] Training loss: 0.01510662, Validation loss: 0.72473068, Gradient norm: 0.37343431
INFO:root:[   39] Training loss: 0.01485990, Validation loss: 0.67539914, Gradient norm: 0.34354399
INFO:root:[   40] Training loss: 0.01522188, Validation loss: 0.68043428, Gradient norm: 0.41285475
INFO:root:[   41] Training loss: 0.01499543, Validation loss: 0.83722820, Gradient norm: 0.37689440
INFO:root:[   42] Training loss: 0.01470751, Validation loss: 0.73213197, Gradient norm: 0.34071461
INFO:root:[   43] Training loss: 0.01473526, Validation loss: 0.69686192, Gradient norm: 0.39257593
INFO:root:[   44] Training loss: 0.01471656, Validation loss: 0.65596667, Gradient norm: 0.42867100
INFO:root:[   45] Training loss: 0.01476258, Validation loss: 0.65151982, Gradient norm: 0.43210310
INFO:root:[   46] Training loss: 0.01439358, Validation loss: 0.69782323, Gradient norm: 0.39917646
INFO:root:[   47] Training loss: 0.01453585, Validation loss: 0.65739115, Gradient norm: 0.37219896
INFO:root:[   48] Training loss: 0.01454776, Validation loss: 0.74378682, Gradient norm: 0.41032204
INFO:root:[   49] Training loss: 0.01439439, Validation loss: 0.70481934, Gradient norm: 0.36686147
INFO:root:[   50] Training loss: 0.01408313, Validation loss: 0.66400965, Gradient norm: 0.34427819
INFO:root:[   51] Training loss: 0.01412253, Validation loss: 0.65667826, Gradient norm: 0.37693677
INFO:root:[   52] Training loss: 0.01396566, Validation loss: 0.69933076, Gradient norm: 0.36757408
INFO:root:[   53] Training loss: 0.01395401, Validation loss: 0.67603714, Gradient norm: 0.37056873
INFO:root:[   54] Training loss: 0.01378364, Validation loss: 0.64574300, Gradient norm: 0.34784155
INFO:root:[   55] Training loss: 0.01382750, Validation loss: 0.82260589, Gradient norm: 0.34794264
INFO:root:[   56] Training loss: 0.01380108, Validation loss: 0.68632730, Gradient norm: 0.36895105
INFO:root:[   57] Training loss: 0.01378740, Validation loss: 0.67662340, Gradient norm: 0.37449975
INFO:root:[   58] Training loss: 0.01364973, Validation loss: 0.71222168, Gradient norm: 0.35894130
INFO:root:[   59] Training loss: 0.01385089, Validation loss: 0.75064541, Gradient norm: 0.41694727
INFO:root:[   60] Training loss: 0.01347139, Validation loss: 0.67285860, Gradient norm: 0.36499854
INFO:root:[   61] Training loss: 0.01314489, Validation loss: 0.68046208, Gradient norm: 0.34469108
INFO:root:[   62] Training loss: 0.01336274, Validation loss: 0.65405025, Gradient norm: 0.35945420
INFO:root:[   63] Training loss: 0.01329321, Validation loss: 0.70441187, Gradient norm: 0.37260035
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 312.189s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.11667
INFO:root:EnergyScoretrain: 0.06245
INFO:root:Coveragetrain: 4.61766
INFO:root:IntervalWidthtrain: 140.16648
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03341
INFO:root:EnergyScorevalidation: 0.01833
INFO:root:Coveragevalidation: 1.20211
INFO:root:IntervalWidthvalidation: 34.41924
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02938
INFO:root:EnergyScoretest: 0.01872
INFO:root:Coveragetest: 0.50196
INFO:root:IntervalWidthtest: 39.92555
INFO:root:###28 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 299892736
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07211321, Validation loss: 2.25228099, Gradient norm: 0.28396495
INFO:root:[    2] Training loss: 0.04553933, Validation loss: 1.99278763, Gradient norm: 0.25656413
INFO:root:[    3] Training loss: 0.03942413, Validation loss: 1.67007048, Gradient norm: 0.25274293
INFO:root:[    4] Training loss: 0.03501526, Validation loss: 1.48595416, Gradient norm: 0.26652260
INFO:root:[    5] Training loss: 0.03225115, Validation loss: 1.30495413, Gradient norm: 0.27482613
INFO:root:[    6] Training loss: 0.03040871, Validation loss: 1.23921987, Gradient norm: 0.30990370
INFO:root:[    7] Training loss: 0.02844839, Validation loss: 1.15483198, Gradient norm: 0.27404080
INFO:root:[    8] Training loss: 0.02723456, Validation loss: 1.16149054, Gradient norm: 0.25473369
INFO:root:[    9] Training loss: 0.02611816, Validation loss: 1.09131219, Gradient norm: 0.26446226
INFO:root:[   10] Training loss: 0.02543351, Validation loss: 1.03343292, Gradient norm: 0.27890861
INFO:root:[   11] Training loss: 0.02434283, Validation loss: 1.05183635, Gradient norm: 0.28372002
INFO:root:[   12] Training loss: 0.02376358, Validation loss: 0.97898824, Gradient norm: 0.31444463
INFO:root:[   13] Training loss: 0.02294868, Validation loss: 0.93880998, Gradient norm: 0.27508299
INFO:root:[   14] Training loss: 0.02249386, Validation loss: 0.98881692, Gradient norm: 0.26212500
INFO:root:[   15] Training loss: 0.02178039, Validation loss: 0.91211229, Gradient norm: 0.27114865
INFO:root:[   16] Training loss: 0.02124213, Validation loss: 0.87635898, Gradient norm: 0.25639076
INFO:root:[   17] Training loss: 0.02070044, Validation loss: 0.97994205, Gradient norm: 0.25893213
INFO:root:[   18] Training loss: 0.02067850, Validation loss: 0.93317868, Gradient norm: 0.32169833
INFO:root:[   19] Training loss: 0.02044338, Validation loss: 0.87380767, Gradient norm: 0.35156594
INFO:root:[   20] Training loss: 0.01983494, Validation loss: 0.83194093, Gradient norm: 0.27578487
INFO:root:[   21] Training loss: 0.01951350, Validation loss: 0.87039796, Gradient norm: 0.29892848
INFO:root:[   22] Training loss: 0.01925984, Validation loss: 0.81326653, Gradient norm: 0.28977482
INFO:root:[   23] Training loss: 0.01903385, Validation loss: 0.79206864, Gradient norm: 0.30907074
INFO:root:[   24] Training loss: 0.01883913, Validation loss: 0.79294929, Gradient norm: 0.32843000
INFO:root:[   25] Training loss: 0.01853862, Validation loss: 0.83706259, Gradient norm: 0.29968725
INFO:root:[   26] Training loss: 0.01861088, Validation loss: 0.80987284, Gradient norm: 0.39119528
INFO:root:[   27] Training loss: 0.01837933, Validation loss: 0.81836317, Gradient norm: 0.32765243
INFO:root:[   28] Training loss: 0.01793071, Validation loss: 0.82030601, Gradient norm: 0.31538972
INFO:root:[   29] Training loss: 0.01797263, Validation loss: 0.87977203, Gradient norm: 0.33830197
INFO:root:[   30] Training loss: 0.01797551, Validation loss: 0.83052460, Gradient norm: 0.36503280
INFO:root:[   31] Training loss: 0.01759752, Validation loss: 0.80287861, Gradient norm: 0.35671715
INFO:root:[   32] Training loss: 0.01739358, Validation loss: 0.81378704, Gradient norm: 0.32955060
INFO:root:[   33] Training loss: 0.01710083, Validation loss: 0.81823532, Gradient norm: 0.33551065
INFO:root:[   34] Training loss: 0.01745276, Validation loss: 0.77038422, Gradient norm: 0.38267490
INFO:root:[   35] Training loss: 0.01720257, Validation loss: 0.77943050, Gradient norm: 0.36952878
INFO:root:[   36] Training loss: 0.01666546, Validation loss: 0.80260693, Gradient norm: 0.30053491
INFO:root:[   37] Training loss: 0.01659802, Validation loss: 0.73555609, Gradient norm: 0.32138171
INFO:root:[   38] Training loss: 0.01672536, Validation loss: 0.72415819, Gradient norm: 0.34387243
INFO:root:[   39] Training loss: 0.01664955, Validation loss: 0.75842424, Gradient norm: 0.37362292
INFO:root:[   40] Training loss: 0.01636299, Validation loss: 0.79173957, Gradient norm: 0.37438697
INFO:root:[   41] Training loss: 0.01618382, Validation loss: 0.75420934, Gradient norm: 0.31645525
INFO:root:[   42] Training loss: 0.01600393, Validation loss: 0.72551160, Gradient norm: 0.32364552
INFO:root:[   43] Training loss: 0.01633019, Validation loss: 0.73452919, Gradient norm: 0.39970755
INFO:root:[   44] Training loss: 0.01593886, Validation loss: 0.69856578, Gradient norm: 0.34273887
INFO:root:[   45] Training loss: 0.01583365, Validation loss: 0.74943883, Gradient norm: 0.32205889
INFO:root:[   46] Training loss: 0.01581559, Validation loss: 0.68398927, Gradient norm: 0.38172505
INFO:root:[   47] Training loss: 0.01567979, Validation loss: 0.70559379, Gradient norm: 0.35119706
INFO:root:[   48] Training loss: 0.01521616, Validation loss: 0.65871799, Gradient norm: 0.30624513
INFO:root:[   49] Training loss: 0.01548883, Validation loss: 0.77488177, Gradient norm: 0.34311420
INFO:root:[   50] Training loss: 0.01551176, Validation loss: 0.69096411, Gradient norm: 0.39001518
INFO:root:[   51] Training loss: 0.01530502, Validation loss: 0.70194769, Gradient norm: 0.35019532
INFO:root:[   52] Training loss: 0.01515296, Validation loss: 0.73003720, Gradient norm: 0.37046780
INFO:root:[   53] Training loss: 0.01506399, Validation loss: 0.77804143, Gradient norm: 0.34944680
INFO:root:[   54] Training loss: 0.01515356, Validation loss: 0.70252150, Gradient norm: 0.37467649
INFO:root:[   55] Training loss: 0.01502070, Validation loss: 0.71968447, Gradient norm: 0.34360834
INFO:root:[   56] Training loss: 0.01478890, Validation loss: 0.73155129, Gradient norm: 0.36457959
INFO:root:[   57] Training loss: 0.01472443, Validation loss: 0.67344991, Gradient norm: 0.32288063
INFO:root:[   58] Training loss: 0.01492531, Validation loss: 0.72207290, Gradient norm: 0.37972279
INFO:root:[   59] Training loss: 0.01457437, Validation loss: 0.72049191, Gradient norm: 0.33173566
INFO:root:[   60] Training loss: 0.01459035, Validation loss: 0.67483152, Gradient norm: 0.34249159
INFO:root:[   61] Training loss: 0.01440459, Validation loss: 0.68544366, Gradient norm: 0.33497537
INFO:root:[   62] Training loss: 0.01430874, Validation loss: 0.71962488, Gradient norm: 0.33505571
INFO:root:[   63] Training loss: 0.01431158, Validation loss: 0.82163771, Gradient norm: 0.33466609
INFO:root:[   64] Training loss: 0.01464009, Validation loss: 0.69857191, Gradient norm: 0.42039529
INFO:root:[   65] Training loss: 0.01428870, Validation loss: 0.68654555, Gradient norm: 0.36310113
INFO:root:[   66] Training loss: 0.01414621, Validation loss: 0.89514915, Gradient norm: 0.33555082
INFO:root:[   67] Training loss: 0.01399654, Validation loss: 0.66407830, Gradient norm: 0.33625715
INFO:root:[   68] Training loss: 0.01397763, Validation loss: 0.88680418, Gradient norm: 0.34185228
INFO:root:[   69] Training loss: 0.01386048, Validation loss: 0.73438146, Gradient norm: 0.33122896
INFO:root:[   70] Training loss: 0.01412679, Validation loss: 0.65083537, Gradient norm: 0.38386332
INFO:root:[   71] Training loss: 0.01392352, Validation loss: 0.65638083, Gradient norm: 0.35004393
INFO:root:[   72] Training loss: 0.01379351, Validation loss: 0.73614096, Gradient norm: 0.32885166
INFO:root:[   73] Training loss: 0.01375553, Validation loss: 0.65599545, Gradient norm: 0.34236245
INFO:root:[   74] Training loss: 0.01373840, Validation loss: 0.77257550, Gradient norm: 0.33718715
INFO:root:[   75] Training loss: 0.01362359, Validation loss: 0.68300995, Gradient norm: 0.34365505
INFO:root:[   76] Training loss: 0.01361269, Validation loss: 0.65003584, Gradient norm: 0.34944826
INFO:root:[   77] Training loss: 0.01324882, Validation loss: 0.68886580, Gradient norm: 0.29081655
INFO:root:[   78] Training loss: 0.01350109, Validation loss: 0.64433347, Gradient norm: 0.34194175
INFO:root:[   79] Training loss: 0.01344106, Validation loss: 0.69230366, Gradient norm: 0.33962050
INFO:root:[   80] Training loss: 0.01339942, Validation loss: 0.69442310, Gradient norm: 0.33855617
INFO:root:[   81] Training loss: 0.01313741, Validation loss: 0.69065228, Gradient norm: 0.30055214
INFO:root:[   82] Training loss: 0.01329082, Validation loss: 0.63480325, Gradient norm: 0.34700079
INFO:root:[   83] Training loss: 0.01315896, Validation loss: 0.67921838, Gradient norm: 0.31527973
INFO:root:[   84] Training loss: 0.01302915, Validation loss: 0.70698333, Gradient norm: 0.32653903
INFO:root:[   85] Training loss: 0.01330719, Validation loss: 0.67027089, Gradient norm: 0.38064438
INFO:root:[   86] Training loss: 0.01291857, Validation loss: 0.60579887, Gradient norm: 0.32653357
INFO:root:[   87] Training loss: 0.01285017, Validation loss: 0.71643765, Gradient norm: 0.29937535
INFO:root:[   88] Training loss: 0.01289673, Validation loss: 0.64517953, Gradient norm: 0.32582134
INFO:root:[   89] Training loss: 0.01288419, Validation loss: 0.66769750, Gradient norm: 0.30322322
INFO:root:[   90] Training loss: 0.01283119, Validation loss: 0.63424022, Gradient norm: 0.34684451
INFO:root:[   91] Training loss: 0.01307161, Validation loss: 0.66476843, Gradient norm: 0.38865593
INFO:root:[   92] Training loss: 0.01285702, Validation loss: 0.62379560, Gradient norm: 0.31502228
INFO:root:[   93] Training loss: 0.01268770, Validation loss: 0.60896280, Gradient norm: 0.31889008
INFO:root:[   94] Training loss: 0.01261557, Validation loss: 0.66203174, Gradient norm: 0.29909667
INFO:root:[   95] Training loss: 0.01244613, Validation loss: 0.59062982, Gradient norm: 0.29647595
INFO:root:[   96] Training loss: 0.01273184, Validation loss: 0.67920522, Gradient norm: 0.37331424
INFO:root:[   97] Training loss: 0.01236961, Validation loss: 0.59321508, Gradient norm: 0.30423795
INFO:root:[   98] Training loss: 0.01239282, Validation loss: 0.78081709, Gradient norm: 0.31130304
INFO:root:[   99] Training loss: 0.01246523, Validation loss: 0.76493553, Gradient norm: 0.31516278
INFO:root:[  100] Training loss: 0.01247815, Validation loss: 0.64024822, Gradient norm: 0.33496071
INFO:root:[  101] Training loss: 0.01233816, Validation loss: 0.62157827, Gradient norm: 0.30983752
INFO:root:[  102] Training loss: 0.01206279, Validation loss: 0.62480283, Gradient norm: 0.28720200
INFO:root:[  103] Training loss: 0.01208769, Validation loss: 0.74117337, Gradient norm: 0.30147469
INFO:root:[  104] Training loss: 0.01235878, Validation loss: 0.63114162, Gradient norm: 0.35388595
INFO:root:EP 104: Early stopping
INFO:root:Training the model took 512.626s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.12169
INFO:root:EnergyScoretrain: 0.06167
INFO:root:Coveragetrain: 4.96522
INFO:root:IntervalWidthtrain: 158.5418
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03522
INFO:root:EnergyScorevalidation: 0.01858
INFO:root:Coveragevalidation: 1.24201
INFO:root:IntervalWidthvalidation: 37.20447
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.0277
INFO:root:EnergyScoretest: 0.01596
INFO:root:Coveragetest: 0.57127
INFO:root:IntervalWidthtest: 43.02372
INFO:root:###29 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06409215, Validation loss: 2.01558599, Gradient norm: 0.29047052
INFO:root:[    2] Training loss: 0.03541646, Validation loss: 1.53287839, Gradient norm: 0.27008956
INFO:root:[    3] Training loss: 0.02904369, Validation loss: 1.37619979, Gradient norm: 0.31147998
INFO:root:[    4] Training loss: 0.02603462, Validation loss: 1.23446279, Gradient norm: 0.34985644
INFO:root:[    5] Training loss: 0.02394663, Validation loss: 1.19428719, Gradient norm: 0.33731979
INFO:root:[    6] Training loss: 0.02234317, Validation loss: 1.07386206, Gradient norm: 0.34184233
INFO:root:[    7] Training loss: 0.02082919, Validation loss: 1.06962900, Gradient norm: 0.28886963
INFO:root:[    8] Training loss: 0.02004373, Validation loss: 0.97369036, Gradient norm: 0.35562039
INFO:root:[    9] Training loss: 0.01906156, Validation loss: 0.99190761, Gradient norm: 0.32600563
INFO:root:[   10] Training loss: 0.01848086, Validation loss: 0.87925585, Gradient norm: 0.36241582
INFO:root:[   11] Training loss: 0.01785211, Validation loss: 0.83524405, Gradient norm: 0.35328258
INFO:root:[   12] Training loss: 0.01732019, Validation loss: 0.94025138, Gradient norm: 0.37593927
INFO:root:[   13] Training loss: 0.01655949, Validation loss: 0.80404911, Gradient norm: 0.30836111
INFO:root:[   14] Training loss: 0.01641315, Validation loss: 0.93275285, Gradient norm: 0.37457107
INFO:root:[   15] Training loss: 0.01591835, Validation loss: 0.79756851, Gradient norm: 0.35704308
INFO:root:[   16] Training loss: 0.01560983, Validation loss: 0.82962703, Gradient norm: 0.33299659
INFO:root:[   17] Training loss: 0.01523336, Validation loss: 0.75847788, Gradient norm: 0.34821718
INFO:root:[   18] Training loss: 0.01513058, Validation loss: 0.72558573, Gradient norm: 0.36514920
INFO:root:[   19] Training loss: 0.01512669, Validation loss: 0.83704497, Gradient norm: 0.40564755
INFO:root:[   20] Training loss: 0.01480442, Validation loss: 0.77176729, Gradient norm: 0.38571156
INFO:root:[   21] Training loss: 0.01453750, Validation loss: 0.84596276, Gradient norm: 0.39570177
INFO:root:[   22] Training loss: 0.01402886, Validation loss: 0.71084339, Gradient norm: 0.32204324
INFO:root:[   23] Training loss: 0.01401813, Validation loss: 0.80914909, Gradient norm: 0.35861161
INFO:root:[   24] Training loss: 0.01388541, Validation loss: 0.76084495, Gradient norm: 0.36602032
INFO:root:[   25] Training loss: 0.01369709, Validation loss: 0.67850633, Gradient norm: 0.36184367
INFO:root:[   26] Training loss: 0.01370322, Validation loss: 0.68082033, Gradient norm: 0.39745797
INFO:root:[   27] Training loss: 0.01331748, Validation loss: 0.74817041, Gradient norm: 0.32285811
INFO:root:[   28] Training loss: 0.01324513, Validation loss: 0.65986328, Gradient norm: 0.34436832
INFO:root:[   29] Training loss: 0.01305574, Validation loss: 0.91691767, Gradient norm: 0.34536796
INFO:root:[   30] Training loss: 0.01295740, Validation loss: 0.68432647, Gradient norm: 0.36029573
INFO:root:[   31] Training loss: 0.01294963, Validation loss: 0.67737857, Gradient norm: 0.37572761
INFO:root:[   32] Training loss: 0.01259723, Validation loss: 0.68683749, Gradient norm: 0.32398798
INFO:root:[   33] Training loss: 0.01284090, Validation loss: 0.80056264, Gradient norm: 0.40271063
INFO:root:[   34] Training loss: 0.01290405, Validation loss: 0.73022472, Gradient norm: 0.41942430
INFO:root:[   35] Training loss: 0.01257104, Validation loss: 0.71568869, Gradient norm: 0.38087126
INFO:root:[   36] Training loss: 0.01262998, Validation loss: 0.64143361, Gradient norm: 0.40856455
INFO:root:[   37] Training loss: 0.01225186, Validation loss: 0.82848674, Gradient norm: 0.35547436
INFO:root:[   38] Training loss: 0.01241622, Validation loss: 0.68396600, Gradient norm: 0.40331699
INFO:root:[   39] Training loss: 0.01243045, Validation loss: 0.64364375, Gradient norm: 0.42062614
INFO:root:[   40] Training loss: 0.01217395, Validation loss: 0.65292822, Gradient norm: 0.36880951
INFO:root:[   41] Training loss: 0.01206906, Validation loss: 0.63121998, Gradient norm: 0.37671801
INFO:root:[   42] Training loss: 0.01184572, Validation loss: 0.63432598, Gradient norm: 0.34495705
INFO:root:[   43] Training loss: 0.01203742, Validation loss: 0.61904406, Gradient norm: 0.41367182
INFO:root:[   44] Training loss: 0.01178494, Validation loss: 0.65401328, Gradient norm: 0.34895900
INFO:root:[   45] Training loss: 0.01166900, Validation loss: 0.63810909, Gradient norm: 0.35423732
INFO:root:[   46] Training loss: 0.01165180, Validation loss: 0.67684353, Gradient norm: 0.37314875
INFO:root:[   47] Training loss: 0.01161976, Validation loss: 0.60373355, Gradient norm: 0.39224121
INFO:root:[   48] Training loss: 0.01170552, Validation loss: 0.63091596, Gradient norm: 0.39765371
INFO:root:[   49] Training loss: 0.01152150, Validation loss: 0.61679866, Gradient norm: 0.37218124
INFO:root:[   50] Training loss: 0.01158704, Validation loss: 0.60898344, Gradient norm: 0.40361709
INFO:root:[   51] Training loss: 0.01133510, Validation loss: 0.61689599, Gradient norm: 0.36280667
INFO:root:[   52] Training loss: 0.01122706, Validation loss: 0.73592276, Gradient norm: 0.34827712
INFO:root:[   53] Training loss: 0.01147355, Validation loss: 0.67648766, Gradient norm: 0.41816221
INFO:root:[   54] Training loss: 0.01142047, Validation loss: 0.66304553, Gradient norm: 0.40655572
INFO:root:[   55] Training loss: 0.01103602, Validation loss: 0.62188696, Gradient norm: 0.32923108
INFO:root:[   56] Training loss: 0.01099879, Validation loss: 0.63589124, Gradient norm: 0.34670719
INFO:root:[   57] Training loss: 0.01129454, Validation loss: 0.65253236, Gradient norm: 0.40753523
INFO:root:[   58] Training loss: 0.01086933, Validation loss: 0.61196033, Gradient norm: 0.34761576
INFO:root:[   59] Training loss: 0.01095273, Validation loss: 0.59728651, Gradient norm: 0.36683892
INFO:root:[   60] Training loss: 0.01087024, Validation loss: 0.71681259, Gradient norm: 0.34783323
INFO:root:[   61] Training loss: 0.01096464, Validation loss: 0.68568520, Gradient norm: 0.38360624
INFO:root:[   62] Training loss: 0.01074557, Validation loss: 0.60677964, Gradient norm: 0.33411735
INFO:root:[   63] Training loss: 0.01075817, Validation loss: 0.62627767, Gradient norm: 0.36772304
INFO:root:[   64] Training loss: 0.01054335, Validation loss: 0.65059283, Gradient norm: 0.32403073
INFO:root:[   65] Training loss: 0.01067850, Validation loss: 0.62186666, Gradient norm: 0.38026363
INFO:root:[   66] Training loss: 0.01085299, Validation loss: 0.66814049, Gradient norm: 0.40390145
INFO:root:[   67] Training loss: 0.01067666, Validation loss: 0.68233526, Gradient norm: 0.38563703
INFO:root:[   68] Training loss: 0.01071685, Validation loss: 0.60553634, Gradient norm: 0.39109111
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 336.989s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.11494
INFO:root:EnergyScoretrain: 0.06038
INFO:root:Coveragetrain: 4.65908
INFO:root:IntervalWidthtrain: 149.68354
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03099
INFO:root:EnergyScorevalidation: 0.01801
INFO:root:Coveragevalidation: 1.22258
INFO:root:IntervalWidthvalidation: 38.62767
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02344
INFO:root:EnergyScoretest: 0.01303
INFO:root:Coveragetest: 0.60805
INFO:root:IntervalWidthtest: 44.39975
INFO:root:###30 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07384739, Validation loss: 2.21428485, Gradient norm: 0.32773971
INFO:root:[    2] Training loss: 0.04195626, Validation loss: 1.80341824, Gradient norm: 0.30557869
INFO:root:[    3] Training loss: 0.03503980, Validation loss: 1.63675373, Gradient norm: 0.28241482
INFO:root:[    4] Training loss: 0.03081111, Validation loss: 1.29904858, Gradient norm: 0.30847580
INFO:root:[    5] Training loss: 0.02834450, Validation loss: 1.24538878, Gradient norm: 0.31937843
INFO:root:[    6] Training loss: 0.02598624, Validation loss: 1.17265357, Gradient norm: 0.27172586
INFO:root:[    7] Training loss: 0.02453259, Validation loss: 1.11593279, Gradient norm: 0.25813325
INFO:root:[    8] Training loss: 0.02341771, Validation loss: 1.04884191, Gradient norm: 0.29916267
INFO:root:[    9] Training loss: 0.02217161, Validation loss: 0.95872755, Gradient norm: 0.27349651
INFO:root:[   10] Training loss: 0.02161174, Validation loss: 1.01082471, Gradient norm: 0.35220092
INFO:root:[   11] Training loss: 0.02028544, Validation loss: 0.86906422, Gradient norm: 0.26853062
INFO:root:[   12] Training loss: 0.01986990, Validation loss: 1.04338427, Gradient norm: 0.31554440
INFO:root:[   13] Training loss: 0.01952737, Validation loss: 0.86945010, Gradient norm: 0.33828758
INFO:root:[   14] Training loss: 0.01917855, Validation loss: 0.89469156, Gradient norm: 0.37311687
INFO:root:[   15] Training loss: 0.01838151, Validation loss: 0.82426997, Gradient norm: 0.33068319
INFO:root:[   16] Training loss: 0.01834227, Validation loss: 0.85066347, Gradient norm: 0.37142367
INFO:root:[   17] Training loss: 0.01777084, Validation loss: 0.77897260, Gradient norm: 0.31177784
INFO:root:[   18] Training loss: 0.01772656, Validation loss: 0.87812522, Gradient norm: 0.37544892
INFO:root:[   19] Training loss: 0.01747232, Validation loss: 0.88591210, Gradient norm: 0.40143118
INFO:root:[   20] Training loss: 0.01736358, Validation loss: 0.82445008, Gradient norm: 0.39854849
INFO:root:[   21] Training loss: 0.01669208, Validation loss: 0.73514669, Gradient norm: 0.31688056
INFO:root:[   22] Training loss: 0.01682371, Validation loss: 0.78452549, Gradient norm: 0.40209869
INFO:root:[   23] Training loss: 0.01610490, Validation loss: 0.76510365, Gradient norm: 0.30856551
INFO:root:[   24] Training loss: 0.01652301, Validation loss: 0.79181999, Gradient norm: 0.40956618
INFO:root:[   25] Training loss: 0.01593471, Validation loss: 0.78142235, Gradient norm: 0.35284754
INFO:root:[   26] Training loss: 0.01594840, Validation loss: 0.85533388, Gradient norm: 0.39707535
INFO:root:[   27] Training loss: 0.01577237, Validation loss: 0.92196062, Gradient norm: 0.40978760
INFO:root:[   28] Training loss: 0.01554873, Validation loss: 0.69529318, Gradient norm: 0.39628482
INFO:root:[   29] Training loss: 0.01539777, Validation loss: 0.69099617, Gradient norm: 0.39336035
INFO:root:[   30] Training loss: 0.01535597, Validation loss: 0.71399787, Gradient norm: 0.40245947
INFO:root:[   31] Training loss: 0.01488782, Validation loss: 0.80782643, Gradient norm: 0.32304524
INFO:root:[   32] Training loss: 0.01509821, Validation loss: 0.79577713, Gradient norm: 0.41691208
INFO:root:[   33] Training loss: 0.01476663, Validation loss: 0.71197022, Gradient norm: 0.35956425
INFO:root:[   34] Training loss: 0.01486790, Validation loss: 0.92473523, Gradient norm: 0.42065895
INFO:root:[   35] Training loss: 0.01455031, Validation loss: 0.97707698, Gradient norm: 0.37379536
INFO:root:[   36] Training loss: 0.01456383, Validation loss: 0.79062557, Gradient norm: 0.39841193
INFO:root:[   37] Training loss: 0.01427456, Validation loss: 0.68669976, Gradient norm: 0.37350828
INFO:root:[   38] Training loss: 0.01452334, Validation loss: 0.90868491, Gradient norm: 0.44003743
INFO:root:[   39] Training loss: 0.01407228, Validation loss: 0.65091696, Gradient norm: 0.37536769
INFO:root:[   40] Training loss: 0.01426270, Validation loss: 0.72348478, Gradient norm: 0.41579525
INFO:root:[   41] Training loss: 0.01423051, Validation loss: 0.77642539, Gradient norm: 0.44051368
INFO:root:[   42] Training loss: 0.01377408, Validation loss: 0.91639797, Gradient norm: 0.37095975
INFO:root:[   43] Training loss: 0.01393407, Validation loss: 0.68971728, Gradient norm: 0.41512156
INFO:root:[   44] Training loss: 0.01401814, Validation loss: 0.73980559, Gradient norm: 0.44869740
INFO:root:[   45] Training loss: 0.01377563, Validation loss: 0.78890325, Gradient norm: 0.40385203
INFO:root:[   46] Training loss: 0.01367287, Validation loss: 0.69699455, Gradient norm: 0.40209988
INFO:root:[   47] Training loss: 0.01353336, Validation loss: 0.64406332, Gradient norm: 0.39944593
INFO:root:[   48] Training loss: 0.01339951, Validation loss: 0.75738923, Gradient norm: 0.39199800
INFO:root:[   49] Training loss: 0.01311215, Validation loss: 1.00887936, Gradient norm: 0.32945851
INFO:root:[   50] Training loss: 0.01331105, Validation loss: 0.66904021, Gradient norm: 0.40349338
INFO:root:[   51] Training loss: 0.01315463, Validation loss: 0.62779978, Gradient norm: 0.36343430
INFO:root:[   52] Training loss: 0.01298597, Validation loss: 0.72170791, Gradient norm: 0.33002536
INFO:root:[   53] Training loss: 0.01342473, Validation loss: 0.75233688, Gradient norm: 0.43472467
INFO:root:[   54] Training loss: 0.01291812, Validation loss: 0.69624239, Gradient norm: 0.35576347
INFO:root:[   55] Training loss: 0.01320300, Validation loss: 0.82471500, Gradient norm: 0.44747572
INFO:root:[   56] Training loss: 0.01288053, Validation loss: 0.63925969, Gradient norm: 0.38627962
INFO:root:[   57] Training loss: 0.01272411, Validation loss: 0.75391376, Gradient norm: 0.34961890
INFO:root:[   58] Training loss: 0.01287049, Validation loss: 0.70925996, Gradient norm: 0.39663880
INFO:root:[   59] Training loss: 0.01286396, Validation loss: 0.92648366, Gradient norm: 0.40199251
INFO:root:[   60] Training loss: 0.01284751, Validation loss: 0.64436719, Gradient norm: 0.41171933
INFO:root:[   61] Training loss: 0.01227296, Validation loss: 0.74146234, Gradient norm: 0.30741224
INFO:root:[   62] Training loss: 0.01274269, Validation loss: 0.64851983, Gradient norm: 0.40954615
INFO:root:[   63] Training loss: 0.01239851, Validation loss: 0.66656501, Gradient norm: 0.34877613
INFO:root:[   64] Training loss: 0.01232249, Validation loss: 0.63221100, Gradient norm: 0.34371703
INFO:root:[   65] Training loss: 0.01257479, Validation loss: 0.77646176, Gradient norm: 0.40644660
INFO:root:[   66] Training loss: 0.01228665, Validation loss: 0.74340143, Gradient norm: 0.36059383
INFO:root:[   67] Training loss: 0.01230406, Validation loss: 0.69482850, Gradient norm: 0.36490073
INFO:root:[   68] Training loss: 0.01242708, Validation loss: 0.81498816, Gradient norm: 0.39432324
INFO:root:[   69] Training loss: 0.01205737, Validation loss: 0.81995302, Gradient norm: 0.32917427
INFO:root:[   70] Training loss: 0.01223181, Validation loss: 0.60756350, Gradient norm: 0.39130243
INFO:root:[   71] Training loss: 0.01211094, Validation loss: 0.66715008, Gradient norm: 0.37606233
INFO:root:[   72] Training loss: 0.01190791, Validation loss: 0.59342731, Gradient norm: 0.33703305
INFO:root:[   73] Training loss: 0.01192930, Validation loss: 0.61304338, Gradient norm: 0.33907656
INFO:root:[   74] Training loss: 0.01215289, Validation loss: 0.70938610, Gradient norm: 0.42045394
INFO:root:[   75] Training loss: 0.01168087, Validation loss: 0.63068207, Gradient norm: 0.31204420
INFO:root:[   76] Training loss: 0.01179192, Validation loss: 0.77305478, Gradient norm: 0.34157238
INFO:root:[   77] Training loss: 0.01152738, Validation loss: 0.59729856, Gradient norm: 0.31221682
INFO:root:[   78] Training loss: 0.01193874, Validation loss: 0.62041158, Gradient norm: 0.37204362
INFO:root:[   79] Training loss: 0.01161494, Validation loss: 0.86869010, Gradient norm: 0.35223864
INFO:root:[   80] Training loss: 0.01176579, Validation loss: 0.66813760, Gradient norm: 0.37261987
INFO:root:[   81] Training loss: 0.01147926, Validation loss: 0.66938983, Gradient norm: 0.34005920
INFO:root:EP 81: Early stopping
INFO:root:Training the model took 400.591s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.11885
INFO:root:EnergyScoretrain: 0.06344
INFO:root:Coveragetrain: 4.65674
INFO:root:IntervalWidthtrain: 154.11468
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03268
INFO:root:EnergyScorevalidation: 0.01703
INFO:root:Coveragevalidation: 1.23184
INFO:root:IntervalWidthvalidation: 39.95234
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02686
INFO:root:EnergyScoretest: 0.01641
INFO:root:Coveragetest: 0.55801
INFO:root:IntervalWidthtest: 46.37384
INFO:root:###31 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06970093, Validation loss: 2.45360415, Gradient norm: 0.28899405
INFO:root:[    2] Training loss: 0.04298411, Validation loss: 1.85250096, Gradient norm: 0.24316992
INFO:root:[    3] Training loss: 0.03646471, Validation loss: 1.54882294, Gradient norm: 0.28006346
INFO:root:[    4] Training loss: 0.03239802, Validation loss: 1.42379461, Gradient norm: 0.25452246
INFO:root:[    5] Training loss: 0.02974588, Validation loss: 1.31339102, Gradient norm: 0.28942382
INFO:root:[    6] Training loss: 0.02790718, Validation loss: 1.27971201, Gradient norm: 0.25846158
INFO:root:[    7] Training loss: 0.02651812, Validation loss: 1.15093420, Gradient norm: 0.28758246
INFO:root:[    8] Training loss: 0.02519579, Validation loss: 1.22047530, Gradient norm: 0.28765325
INFO:root:[    9] Training loss: 0.02412846, Validation loss: 1.06674908, Gradient norm: 0.27284225
INFO:root:[   10] Training loss: 0.02335710, Validation loss: 1.02042653, Gradient norm: 0.30097645
INFO:root:[   11] Training loss: 0.02257289, Validation loss: 1.06471348, Gradient norm: 0.30461947
INFO:root:[   12] Training loss: 0.02167684, Validation loss: 0.92543242, Gradient norm: 0.26503952
INFO:root:[   13] Training loss: 0.02143505, Validation loss: 1.00814107, Gradient norm: 0.31867811
INFO:root:[   14] Training loss: 0.02064292, Validation loss: 0.87600197, Gradient norm: 0.27417621
INFO:root:[   15] Training loss: 0.02004840, Validation loss: 0.96914811, Gradient norm: 0.27844720
INFO:root:[   16] Training loss: 0.01968749, Validation loss: 1.11028418, Gradient norm: 0.28393866
INFO:root:[   17] Training loss: 0.01957984, Validation loss: 0.85986936, Gradient norm: 0.34390696
INFO:root:[   18] Training loss: 0.01920782, Validation loss: 0.88263306, Gradient norm: 0.34648581
INFO:root:[   19] Training loss: 0.01890884, Validation loss: 0.87163426, Gradient norm: 0.33724709
INFO:root:[   20] Training loss: 0.01847819, Validation loss: 0.97493674, Gradient norm: 0.33197189
INFO:root:[   21] Training loss: 0.01845209, Validation loss: 0.88988651, Gradient norm: 0.40885164
INFO:root:[   22] Training loss: 0.01782970, Validation loss: 0.80669240, Gradient norm: 0.30271229
INFO:root:[   23] Training loss: 0.01768780, Validation loss: 0.79081517, Gradient norm: 0.34780356
INFO:root:[   24] Training loss: 0.01765939, Validation loss: 0.80833599, Gradient norm: 0.37892615
INFO:root:[   25] Training loss: 0.01717732, Validation loss: 0.94914236, Gradient norm: 0.34154034
INFO:root:[   26] Training loss: 0.01712458, Validation loss: 0.78246958, Gradient norm: 0.34979080
INFO:root:[   27] Training loss: 0.01673294, Validation loss: 0.75856555, Gradient norm: 0.31357244
INFO:root:[   28] Training loss: 0.01687328, Validation loss: 0.72217543, Gradient norm: 0.41226970
INFO:root:[   29] Training loss: 0.01649457, Validation loss: 0.85121126, Gradient norm: 0.30402324
INFO:root:[   30] Training loss: 0.01639988, Validation loss: 0.73400533, Gradient norm: 0.35462867
INFO:root:[   31] Training loss: 0.01624677, Validation loss: 0.72079414, Gradient norm: 0.36310842
INFO:root:[   32] Training loss: 0.01612864, Validation loss: 0.74539928, Gradient norm: 0.38308874
INFO:root:[   33] Training loss: 0.01592567, Validation loss: 0.79853958, Gradient norm: 0.35031004
INFO:root:[   34] Training loss: 0.01585386, Validation loss: 0.70961611, Gradient norm: 0.37219179
INFO:root:[   35] Training loss: 0.01580940, Validation loss: 0.75699604, Gradient norm: 0.40410801
INFO:root:[   36] Training loss: 0.01586782, Validation loss: 0.70228634, Gradient norm: 0.42106543
INFO:root:[   37] Training loss: 0.01564934, Validation loss: 0.78651404, Gradient norm: 0.39775035
INFO:root:[   38] Training loss: 0.01526586, Validation loss: 0.72449535, Gradient norm: 0.38154345
INFO:root:[   39] Training loss: 0.01507556, Validation loss: 0.75432552, Gradient norm: 0.34022173
INFO:root:[   40] Training loss: 0.01519015, Validation loss: 0.75579554, Gradient norm: 0.37980257
INFO:root:[   41] Training loss: 0.01509569, Validation loss: 0.72618755, Gradient norm: 0.36669690
INFO:root:[   42] Training loss: 0.01527368, Validation loss: 0.78996819, Gradient norm: 0.41987581
INFO:root:[   43] Training loss: 0.01490764, Validation loss: 0.71760580, Gradient norm: 0.37746927
INFO:root:[   44] Training loss: 0.01492273, Validation loss: 0.74393194, Gradient norm: 0.38215471
INFO:root:[   45] Training loss: 0.01468712, Validation loss: 0.68435109, Gradient norm: 0.35398007
INFO:root:[   46] Training loss: 0.01461051, Validation loss: 0.68070891, Gradient norm: 0.39286511
INFO:root:[   47] Training loss: 0.01462084, Validation loss: 0.68200999, Gradient norm: 0.38690754
INFO:root:[   48] Training loss: 0.01429280, Validation loss: 0.70270515, Gradient norm: 0.33933731
INFO:root:[   49] Training loss: 0.01432136, Validation loss: 0.66158152, Gradient norm: 0.36083371
INFO:root:[   50] Training loss: 0.01431320, Validation loss: 0.69060925, Gradient norm: 0.39652391
INFO:root:[   51] Training loss: 0.01454128, Validation loss: 0.71495479, Gradient norm: 0.44170600
INFO:root:[   52] Training loss: 0.01444424, Validation loss: 0.67747521, Gradient norm: 0.42777894
INFO:root:[   53] Training loss: 0.01400383, Validation loss: 0.67025222, Gradient norm: 0.35396759
INFO:root:[   54] Training loss: 0.01388407, Validation loss: 0.67086297, Gradient norm: 0.33026239
INFO:root:[   55] Training loss: 0.01400040, Validation loss: 0.68370507, Gradient norm: 0.39634915
INFO:root:[   56] Training loss: 0.01404169, Validation loss: 0.72378536, Gradient norm: 0.40537175
INFO:root:[   57] Training loss: 0.01375166, Validation loss: 0.70510914, Gradient norm: 0.34985179
INFO:root:[   58] Training loss: 0.01359863, Validation loss: 0.69601507, Gradient norm: 0.36616285
INFO:root:[   59] Training loss: 0.01368707, Validation loss: 0.69901790, Gradient norm: 0.36001799
INFO:root:[   60] Training loss: 0.01367027, Validation loss: 0.63834159, Gradient norm: 0.35919935
INFO:root:[   61] Training loss: 0.01365472, Validation loss: 0.67891223, Gradient norm: 0.36040899
INFO:root:[   62] Training loss: 0.01360018, Validation loss: 0.65586671, Gradient norm: 0.38572262
INFO:root:[   63] Training loss: 0.01351229, Validation loss: 0.66769338, Gradient norm: 0.38686518
INFO:root:[   64] Training loss: 0.01336994, Validation loss: 0.65940977, Gradient norm: 0.34621473
INFO:root:[   65] Training loss: 0.01370316, Validation loss: 0.77664158, Gradient norm: 0.42446095
INFO:root:[   66] Training loss: 0.01326722, Validation loss: 0.69035450, Gradient norm: 0.34510024
INFO:root:[   67] Training loss: 0.01307840, Validation loss: 0.65233463, Gradient norm: 0.30876817
INFO:root:[   68] Training loss: 0.01314322, Validation loss: 0.66984518, Gradient norm: 0.34639334
INFO:root:[   69] Training loss: 0.01306567, Validation loss: 0.68097758, Gradient norm: 0.35664563
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 342.478s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.12677
INFO:root:EnergyScoretrain: 0.06745
INFO:root:Coveragetrain: 4.81596
INFO:root:IntervalWidthtrain: 171.1353
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03398
INFO:root:EnergyScorevalidation: 0.01828
INFO:root:Coveragevalidation: 1.21172
INFO:root:IntervalWidthvalidation: 39.20997
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.024
INFO:root:EnergyScoretest: 0.0136
INFO:root:Coveragetest: 0.65745
INFO:root:IntervalWidthtest: 49.28987
INFO:root:###32 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 299892736
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07422371, Validation loss: 2.30364423, Gradient norm: 0.27706941
INFO:root:[    2] Training loss: 0.04632156, Validation loss: 2.21487155, Gradient norm: 0.26723605
INFO:root:[    3] Training loss: 0.04127087, Validation loss: 1.88559610, Gradient norm: 0.27017313
INFO:root:[    4] Training loss: 0.03673994, Validation loss: 1.54166538, Gradient norm: 0.23016326
INFO:root:[    5] Training loss: 0.03375254, Validation loss: 1.42434171, Gradient norm: 0.28285766
INFO:root:[    6] Training loss: 0.03176715, Validation loss: 1.36877542, Gradient norm: 0.29111682
INFO:root:[    7] Training loss: 0.02957004, Validation loss: 1.47949246, Gradient norm: 0.26362593
INFO:root:[    8] Training loss: 0.02858826, Validation loss: 1.58671210, Gradient norm: 0.29160555
INFO:root:[    9] Training loss: 0.02753218, Validation loss: 1.33411556, Gradient norm: 0.31175276
INFO:root:[   10] Training loss: 0.02645483, Validation loss: 1.22604294, Gradient norm: 0.28996692
INFO:root:[   11] Training loss: 0.02545973, Validation loss: 1.07550033, Gradient norm: 0.29823273
INFO:root:[   12] Training loss: 0.02482563, Validation loss: 1.10409277, Gradient norm: 0.33170959
INFO:root:[   13] Training loss: 0.02410637, Validation loss: 1.03579651, Gradient norm: 0.32231805
INFO:root:[   14] Training loss: 0.02336119, Validation loss: 0.98606765, Gradient norm: 0.28972648
INFO:root:[   15] Training loss: 0.02322823, Validation loss: 0.98680286, Gradient norm: 0.36657723
INFO:root:[   16] Training loss: 0.02275267, Validation loss: 1.12240911, Gradient norm: 0.36953039
INFO:root:[   17] Training loss: 0.02179741, Validation loss: 0.98613007, Gradient norm: 0.29020479
INFO:root:[   18] Training loss: 0.02173359, Validation loss: 0.95569178, Gradient norm: 0.35078524
INFO:root:[   19] Training loss: 0.02104327, Validation loss: 1.21266999, Gradient norm: 0.29834283
INFO:root:[   20] Training loss: 0.02083895, Validation loss: 1.00492216, Gradient norm: 0.34160264
INFO:root:[   21] Training loss: 0.02089510, Validation loss: 1.03213059, Gradient norm: 0.40019434
INFO:root:[   22] Training loss: 0.02054716, Validation loss: 0.99562916, Gradient norm: 0.37815005
INFO:root:[   23] Training loss: 0.02009895, Validation loss: 1.02216317, Gradient norm: 0.33685134
INFO:root:[   24] Training loss: 0.01992116, Validation loss: 0.85808744, Gradient norm: 0.34822371
INFO:root:[   25] Training loss: 0.01933209, Validation loss: 1.00021355, Gradient norm: 0.32544351
INFO:root:[   26] Training loss: 0.01938592, Validation loss: 0.92833873, Gradient norm: 0.36155928
INFO:root:[   27] Training loss: 0.01927237, Validation loss: 0.87527714, Gradient norm: 0.39575837
INFO:root:[   28] Training loss: 0.01889435, Validation loss: 0.88998819, Gradient norm: 0.33777723
INFO:root:[   29] Training loss: 0.01885072, Validation loss: 0.85695179, Gradient norm: 0.36265284
INFO:root:[   30] Training loss: 0.01845603, Validation loss: 0.97050413, Gradient norm: 0.36017852
INFO:root:[   31] Training loss: 0.01862719, Validation loss: 0.86852278, Gradient norm: 0.45144637
INFO:root:[   32] Training loss: 0.01811676, Validation loss: 0.90500409, Gradient norm: 0.33919140
INFO:root:[   33] Training loss: 0.01822271, Validation loss: 0.81305828, Gradient norm: 0.43951786
INFO:root:[   34] Training loss: 0.01809213, Validation loss: 1.01956886, Gradient norm: 0.40373818
INFO:root:[   35] Training loss: 0.01776516, Validation loss: 0.81762632, Gradient norm: 0.38026140
INFO:root:[   36] Training loss: 0.01778245, Validation loss: 0.86102014, Gradient norm: 0.38662545
INFO:root:[   37] Training loss: 0.01728815, Validation loss: 0.87264545, Gradient norm: 0.36875776
INFO:root:[   38] Training loss: 0.01719769, Validation loss: 0.88259565, Gradient norm: 0.38738771
INFO:root:[   39] Training loss: 0.01784997, Validation loss: 0.83070792, Gradient norm: 0.48088603
INFO:root:[   40] Training loss: 0.01687411, Validation loss: 0.78831404, Gradient norm: 0.35533742
INFO:root:[   41] Training loss: 0.01707359, Validation loss: 0.84333842, Gradient norm: 0.39641846
INFO:root:[   42] Training loss: 0.01687375, Validation loss: 0.81494108, Gradient norm: 0.37709517
INFO:root:[   43] Training loss: 0.01683609, Validation loss: 0.87546597, Gradient norm: 0.39937391
INFO:root:[   44] Training loss: 0.01695523, Validation loss: 0.77054772, Gradient norm: 0.41971100
INFO:root:[   45] Training loss: 0.01653701, Validation loss: 0.79414060, Gradient norm: 0.34005986
INFO:root:[   46] Training loss: 0.01640770, Validation loss: 0.83025407, Gradient norm: 0.33054442
INFO:root:[   47] Training loss: 0.01675906, Validation loss: 0.79975059, Gradient norm: 0.43393587
INFO:root:[   48] Training loss: 0.01644542, Validation loss: 0.85814108, Gradient norm: 0.43392330
INFO:root:[   49] Training loss: 0.01602728, Validation loss: 0.77668893, Gradient norm: 0.31429442
INFO:root:[   50] Training loss: 0.01605888, Validation loss: 0.75818200, Gradient norm: 0.37258821
INFO:root:[   51] Training loss: 0.01591705, Validation loss: 0.82780290, Gradient norm: 0.36300245
INFO:root:[   52] Training loss: 0.01585571, Validation loss: 0.96954118, Gradient norm: 0.40965147
INFO:root:[   53] Training loss: 0.01588378, Validation loss: 0.77624801, Gradient norm: 0.40040090
INFO:root:[   54] Training loss: 0.01584058, Validation loss: 0.89613161, Gradient norm: 0.40561126
INFO:root:[   55] Training loss: 0.01573820, Validation loss: 0.79147875, Gradient norm: 0.38062688
INFO:root:[   56] Training loss: 0.01555653, Validation loss: 0.80772730, Gradient norm: 0.39208682
INFO:root:[   57] Training loss: 0.01559438, Validation loss: 0.74841141, Gradient norm: 0.37797066
INFO:root:[   58] Training loss: 0.01542795, Validation loss: 0.77382285, Gradient norm: 0.35841508
INFO:root:[   59] Training loss: 0.01514699, Validation loss: 0.74697967, Gradient norm: 0.36300529
INFO:root:[   60] Training loss: 0.01526442, Validation loss: 0.76704849, Gradient norm: 0.37822120
INFO:root:[   61] Training loss: 0.01509318, Validation loss: 0.77060712, Gradient norm: 0.32188989
INFO:root:[   62] Training loss: 0.01508534, Validation loss: 0.75842005, Gradient norm: 0.34629070
INFO:root:[   63] Training loss: 0.01512319, Validation loss: 0.94126030, Gradient norm: 0.38006139
INFO:root:[   64] Training loss: 0.01526829, Validation loss: 0.78782007, Gradient norm: 0.39839438
INFO:root:[   65] Training loss: 0.01493159, Validation loss: 0.74002834, Gradient norm: 0.36274711
INFO:root:[   66] Training loss: 0.01476623, Validation loss: 0.73871712, Gradient norm: 0.30762176
INFO:root:[   67] Training loss: 0.01480901, Validation loss: 0.98072642, Gradient norm: 0.35843222
INFO:root:[   68] Training loss: 0.01461581, Validation loss: 0.77189235, Gradient norm: 0.34082341
INFO:root:[   69] Training loss: 0.01445389, Validation loss: 0.76465507, Gradient norm: 0.30316091
INFO:root:[   70] Training loss: 0.01478471, Validation loss: 0.75576602, Gradient norm: 0.38541008
INFO:root:[   71] Training loss: 0.01480514, Validation loss: 0.75474182, Gradient norm: 0.39396014
INFO:root:[   72] Training loss: 0.01469010, Validation loss: 0.80104346, Gradient norm: 0.38737482
INFO:root:[   73] Training loss: 0.01417665, Validation loss: 0.72536665, Gradient norm: 0.29929992
INFO:root:[   74] Training loss: 0.01448311, Validation loss: 0.77536901, Gradient norm: 0.36515647
INFO:root:[   75] Training loss: 0.01422218, Validation loss: 0.77275007, Gradient norm: 0.33910406
INFO:root:[   76] Training loss: 0.01403063, Validation loss: 0.84606091, Gradient norm: 0.29920043
INFO:root:[   77] Training loss: 0.01448882, Validation loss: 0.91274135, Gradient norm: 0.37281276
INFO:root:[   78] Training loss: 0.01399925, Validation loss: 0.70265201, Gradient norm: 0.32286743
INFO:root:[   79] Training loss: 0.01412907, Validation loss: 0.85580300, Gradient norm: 0.35066228
INFO:root:[   80] Training loss: 0.01388654, Validation loss: 0.83722383, Gradient norm: 0.32115983
INFO:root:[   81] Training loss: 0.01422054, Validation loss: 0.75565390, Gradient norm: 0.38763885
INFO:root:[   82] Training loss: 0.01369279, Validation loss: 0.76376961, Gradient norm: 0.30016034
INFO:root:[   83] Training loss: 0.01389000, Validation loss: 0.73636045, Gradient norm: 0.33177817
INFO:root:[   84] Training loss: 0.01357262, Validation loss: 0.75567025, Gradient norm: 0.25788914
INFO:root:[   85] Training loss: 0.01368790, Validation loss: 0.75122127, Gradient norm: 0.31395881
INFO:root:[   86] Training loss: 0.01357342, Validation loss: 0.86180062, Gradient norm: 0.32402093
INFO:root:[   87] Training loss: 0.01381181, Validation loss: 0.76581263, Gradient norm: 0.36134224
INFO:root:EP 87: Early stopping
INFO:root:Training the model took 429.192s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.1388
INFO:root:EnergyScoretrain: 0.07165
INFO:root:Coveragetrain: 4.47675
INFO:root:IntervalWidthtrain: 170.33815
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.04136
INFO:root:EnergyScorevalidation: 0.02285
INFO:root:Coveragevalidation: 1.15659
INFO:root:IntervalWidthvalidation: 46.34293
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02578
INFO:root:EnergyScoretest: 0.01427
INFO:root:Coveragetest: 0.60675
INFO:root:IntervalWidthtest: 50.32888
INFO:root:###33 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 299892736
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07277803, Validation loss: 2.08145757, Gradient norm: 0.44609595
INFO:root:[    2] Training loss: 0.03228649, Validation loss: 1.64728503, Gradient norm: 0.38740913
INFO:root:[    3] Training loss: 0.02677831, Validation loss: 1.45240576, Gradient norm: 0.42508854
INFO:root:[    4] Training loss: 0.02423996, Validation loss: 1.28491152, Gradient norm: 0.42718860
INFO:root:[    5] Training loss: 0.02206286, Validation loss: 1.19734550, Gradient norm: 0.36532002
INFO:root:[    6] Training loss: 0.02044396, Validation loss: 1.14056218, Gradient norm: 0.33512073
INFO:root:[    7] Training loss: 0.01966087, Validation loss: 1.05521580, Gradient norm: 0.45756582
INFO:root:[    8] Training loss: 0.01849070, Validation loss: 1.02862575, Gradient norm: 0.39632289
INFO:root:[    9] Training loss: 0.01764024, Validation loss: 0.98707415, Gradient norm: 0.39766477
INFO:root:[   10] Training loss: 0.01736552, Validation loss: 0.95134036, Gradient norm: 0.43261993
INFO:root:[   11] Training loss: 0.01650439, Validation loss: 0.99559346, Gradient norm: 0.39358517
INFO:root:[   12] Training loss: 0.01610921, Validation loss: 0.99510388, Gradient norm: 0.39129559
INFO:root:[   13] Training loss: 0.01587485, Validation loss: 0.87038991, Gradient norm: 0.41891225
INFO:root:[   14] Training loss: 0.01551661, Validation loss: 0.90670863, Gradient norm: 0.40315343
INFO:root:[   15] Training loss: 0.01527399, Validation loss: 0.90434076, Gradient norm: 0.42212044
INFO:root:[   16] Training loss: 0.01483760, Validation loss: 0.83598463, Gradient norm: 0.40136893
INFO:root:[   17] Training loss: 0.01435200, Validation loss: 0.83123023, Gradient norm: 0.31670038
INFO:root:[   18] Training loss: 0.01416230, Validation loss: 0.81894519, Gradient norm: 0.35287766
INFO:root:[   19] Training loss: 0.01415374, Validation loss: 0.87395776, Gradient norm: 0.40399698
INFO:root:[   20] Training loss: 0.01423313, Validation loss: 0.77683220, Gradient norm: 0.42276665
INFO:root:[   21] Training loss: 0.01376227, Validation loss: 0.79753882, Gradient norm: 0.39595407
INFO:root:[   22] Training loss: 0.01395401, Validation loss: 0.78903459, Gradient norm: 0.44774288
INFO:root:[   23] Training loss: 0.01354694, Validation loss: 0.80772797, Gradient norm: 0.39431923
INFO:root:[   24] Training loss: 0.01313694, Validation loss: 0.87382838, Gradient norm: 0.34078805
INFO:root:[   25] Training loss: 0.01329408, Validation loss: 0.80078008, Gradient norm: 0.41871304
INFO:root:[   26] Training loss: 0.01311678, Validation loss: 0.74906489, Gradient norm: 0.37899113
INFO:root:[   27] Training loss: 0.01316796, Validation loss: 0.78832097, Gradient norm: 0.41633808
INFO:root:[   28] Training loss: 0.01290647, Validation loss: 0.75337104, Gradient norm: 0.41595543
INFO:root:[   29] Training loss: 0.01258457, Validation loss: 0.73836834, Gradient norm: 0.36080131
INFO:root:[   30] Training loss: 0.01239773, Validation loss: 0.83039815, Gradient norm: 0.35220179
INFO:root:[   31] Training loss: 0.01259748, Validation loss: 0.81536564, Gradient norm: 0.40660192
INFO:root:[   32] Training loss: 0.01221942, Validation loss: 0.73388620, Gradient norm: 0.37445158
INFO:root:[   33] Training loss: 0.01214095, Validation loss: 0.76292509, Gradient norm: 0.37321971
INFO:root:[   34] Training loss: 0.01214666, Validation loss: 0.73212078, Gradient norm: 0.36526093
INFO:root:[   35] Training loss: 0.01208128, Validation loss: 0.71106392, Gradient norm: 0.37547089
INFO:root:[   36] Training loss: 0.01192503, Validation loss: 0.73873881, Gradient norm: 0.38596316
INFO:root:[   37] Training loss: 0.01216309, Validation loss: 0.71303465, Gradient norm: 0.46332085
INFO:root:[   38] Training loss: 0.01161343, Validation loss: 0.76063534, Gradient norm: 0.34100087
INFO:root:[   39] Training loss: 0.01172314, Validation loss: 0.72870715, Gradient norm: 0.39561300
INFO:root:[   40] Training loss: 0.01137764, Validation loss: 0.70418827, Gradient norm: 0.34506016
INFO:root:[   41] Training loss: 0.01150970, Validation loss: 0.78894421, Gradient norm: 0.37272471
INFO:root:[   42] Training loss: 0.01149470, Validation loss: 0.68558866, Gradient norm: 0.39851435
INFO:root:[   43] Training loss: 0.01102596, Validation loss: 0.67962621, Gradient norm: 0.27429497
INFO:root:[   44] Training loss: 0.01117422, Validation loss: 0.75770132, Gradient norm: 0.38014459
INFO:root:[   45] Training loss: 0.01132184, Validation loss: 0.68044851, Gradient norm: 0.42408711
INFO:root:[   46] Training loss: 0.01112604, Validation loss: 0.68330948, Gradient norm: 0.37696087
INFO:root:[   47] Training loss: 0.01105209, Validation loss: 0.69824690, Gradient norm: 0.36270213
INFO:root:[   48] Training loss: 0.01101900, Validation loss: 0.68138247, Gradient norm: 0.38585487
INFO:root:[   49] Training loss: 0.01080379, Validation loss: 0.66535548, Gradient norm: 0.36244739
INFO:root:[   50] Training loss: 0.01078672, Validation loss: 0.68366835, Gradient norm: 0.37092593
INFO:root:[   51] Training loss: 0.01054175, Validation loss: 0.67637597, Gradient norm: 0.31455886
INFO:root:[   52] Training loss: 0.01050533, Validation loss: 0.66141247, Gradient norm: 0.33271317
INFO:root:[   53] Training loss: 0.01074793, Validation loss: 0.70111238, Gradient norm: 0.37972882
INFO:root:[   54] Training loss: 0.01045551, Validation loss: 0.66462815, Gradient norm: 0.35930853
INFO:root:[   55] Training loss: 0.01047567, Validation loss: 0.67256588, Gradient norm: 0.36000731
INFO:root:[   56] Training loss: 0.01055512, Validation loss: 0.69477474, Gradient norm: 0.38963091
INFO:root:[   57] Training loss: 0.01032488, Validation loss: 0.65503941, Gradient norm: 0.36440530
INFO:root:[   58] Training loss: 0.01041507, Validation loss: 0.75841055, Gradient norm: 0.39326924
INFO:root:[   59] Training loss: 0.01026788, Validation loss: 0.67468280, Gradient norm: 0.38190048
INFO:root:[   60] Training loss: 0.01025759, Validation loss: 0.68735091, Gradient norm: 0.37145287
INFO:root:[   61] Training loss: 0.01000725, Validation loss: 0.64163609, Gradient norm: 0.33177989
INFO:root:[   62] Training loss: 0.01021494, Validation loss: 0.68213162, Gradient norm: 0.38400105
INFO:root:[   63] Training loss: 0.00984257, Validation loss: 0.66209392, Gradient norm: 0.33265620
INFO:root:[   64] Training loss: 0.01004505, Validation loss: 0.65442164, Gradient norm: 0.37659250
INFO:root:[   65] Training loss: 0.00971174, Validation loss: 0.66798001, Gradient norm: 0.29859991
INFO:root:[   66] Training loss: 0.00997187, Validation loss: 0.76786697, Gradient norm: 0.40889036
INFO:root:[   67] Training loss: 0.01014363, Validation loss: 0.67781110, Gradient norm: 0.42088959
INFO:root:[   68] Training loss: 0.00951445, Validation loss: 0.64409448, Gradient norm: 0.29636897
INFO:root:[   69] Training loss: 0.01002193, Validation loss: 0.66622681, Gradient norm: 0.43254781
INFO:root:[   70] Training loss: 0.00983965, Validation loss: 0.64202791, Gradient norm: 0.40440060
INFO:root:[   71] Training loss: 0.00945533, Validation loss: 0.63798187, Gradient norm: 0.32996639
INFO:root:[   72] Training loss: 0.00975656, Validation loss: 0.66494754, Gradient norm: 0.40455610
INFO:root:[   73] Training loss: 0.00949808, Validation loss: 0.66448976, Gradient norm: 0.38244175
INFO:root:[   74] Training loss: 0.00949834, Validation loss: 0.64793340, Gradient norm: 0.35846290
INFO:root:[   75] Training loss: 0.00950176, Validation loss: 0.64635710, Gradient norm: 0.35758489
INFO:root:[   76] Training loss: 0.00935172, Validation loss: 0.64688698, Gradient norm: 0.34251682
INFO:root:[   77] Training loss: 0.00939359, Validation loss: 0.63628149, Gradient norm: 0.35559116
INFO:root:[   78] Training loss: 0.00914849, Validation loss: 0.64135634, Gradient norm: 0.33885567
INFO:root:[   79] Training loss: 0.00932030, Validation loss: 0.68252129, Gradient norm: 0.36532445
INFO:root:[   80] Training loss: 0.00920121, Validation loss: 0.65866418, Gradient norm: 0.38720936
INFO:root:[   81] Training loss: 0.00892485, Validation loss: 0.62989770, Gradient norm: 0.31785788
INFO:root:[   82] Training loss: 0.00890351, Validation loss: 0.68635470, Gradient norm: 0.33064270
INFO:root:[   83] Training loss: 0.00894192, Validation loss: 0.64626025, Gradient norm: 0.34064052
INFO:root:[   84] Training loss: 0.00875442, Validation loss: 0.64543738, Gradient norm: 0.32143049
INFO:root:[   85] Training loss: 0.00904631, Validation loss: 0.66642022, Gradient norm: 0.39183198
INFO:root:[   86] Training loss: 0.00874290, Validation loss: 0.67506890, Gradient norm: 0.34817207
INFO:root:[   87] Training loss: 0.00873339, Validation loss: 0.63953118, Gradient norm: 0.32752571
INFO:root:[   88] Training loss: 0.00877702, Validation loss: 0.71843671, Gradient norm: 0.36161696
INFO:root:[   89] Training loss: 0.00862134, Validation loss: 0.64506075, Gradient norm: 0.33771923
INFO:root:[   90] Training loss: 0.00882705, Validation loss: 0.69760048, Gradient norm: 0.38960352
INFO:root:EP 90: Early stopping
INFO:root:Training the model took 449.84s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.04831
INFO:root:EnergyScoretrain: 0.03676
INFO:root:Coveragetrain: 6.19567
INFO:root:IntervalWidthtrain: 54.59489
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01871
INFO:root:EnergyScorevalidation: 0.015
INFO:root:Coveragevalidation: 1.51192
INFO:root:IntervalWidthvalidation: 14.29744
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01868
INFO:root:EnergyScoretest: 0.01572
INFO:root:Coveragetest: 0.49288
INFO:root:IntervalWidthtest: 16.38214
INFO:root:###34 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06666046, Validation loss: 2.27362820, Gradient norm: 0.33499551
INFO:root:[    2] Training loss: 0.03549369, Validation loss: 1.81917148, Gradient norm: 0.32797402
INFO:root:[    3] Training loss: 0.02989218, Validation loss: 1.66923605, Gradient norm: 0.37002628
INFO:root:[    4] Training loss: 0.02709522, Validation loss: 1.56185629, Gradient norm: 0.42036996
INFO:root:[    5] Training loss: 0.02474094, Validation loss: 1.42474573, Gradient norm: 0.35830547
INFO:root:[    6] Training loss: 0.02340826, Validation loss: 1.33795643, Gradient norm: 0.35916740
INFO:root:[    7] Training loss: 0.02228883, Validation loss: 1.25808037, Gradient norm: 0.39859138
INFO:root:[    8] Training loss: 0.02113261, Validation loss: 1.17951598, Gradient norm: 0.39887782
INFO:root:[    9] Training loss: 0.01999999, Validation loss: 1.15797718, Gradient norm: 0.33232822
INFO:root:[   10] Training loss: 0.01976403, Validation loss: 1.09868395, Gradient norm: 0.39227020
INFO:root:[   11] Training loss: 0.01894257, Validation loss: 1.09950120, Gradient norm: 0.36989263
INFO:root:[   12] Training loss: 0.01862794, Validation loss: 1.04001530, Gradient norm: 0.37894555
INFO:root:[   13] Training loss: 0.01793057, Validation loss: 1.04658864, Gradient norm: 0.34790682
INFO:root:[   14] Training loss: 0.01803210, Validation loss: 0.98774406, Gradient norm: 0.39399681
INFO:root:[   15] Training loss: 0.01756928, Validation loss: 0.98022302, Gradient norm: 0.37769495
INFO:root:[   16] Training loss: 0.01673735, Validation loss: 0.96407514, Gradient norm: 0.29243103
INFO:root:[   17] Training loss: 0.01654906, Validation loss: 0.94700974, Gradient norm: 0.33644573
INFO:root:[   18] Training loss: 0.01691775, Validation loss: 0.94569427, Gradient norm: 0.42768276
INFO:root:[   19] Training loss: 0.01635315, Validation loss: 0.99023452, Gradient norm: 0.36734337
INFO:root:[   20] Training loss: 0.01604524, Validation loss: 1.00544709, Gradient norm: 0.33709758
INFO:root:[   21] Training loss: 0.01575072, Validation loss: 0.92073192, Gradient norm: 0.32442992
INFO:root:[   22] Training loss: 0.01604494, Validation loss: 0.94088146, Gradient norm: 0.42477584
INFO:root:[   23] Training loss: 0.01546772, Validation loss: 0.92871381, Gradient norm: 0.34234093
INFO:root:[   24] Training loss: 0.01534959, Validation loss: 0.87428485, Gradient norm: 0.37309195
INFO:root:[   25] Training loss: 0.01529482, Validation loss: 0.93891132, Gradient norm: 0.37938410
INFO:root:[   26] Training loss: 0.01512772, Validation loss: 0.91841388, Gradient norm: 0.35153454
INFO:root:[   27] Training loss: 0.01475494, Validation loss: 0.88090246, Gradient norm: 0.32569481
INFO:root:[   28] Training loss: 0.01458856, Validation loss: 0.84767109, Gradient norm: 0.31708562
INFO:root:[   29] Training loss: 0.01446016, Validation loss: 0.88380425, Gradient norm: 0.32373920
INFO:root:[   30] Training loss: 0.01465617, Validation loss: 0.84052602, Gradient norm: 0.38417983
INFO:root:[   31] Training loss: 0.01426721, Validation loss: 0.82473010, Gradient norm: 0.33284164
INFO:root:[   32] Training loss: 0.01409918, Validation loss: 0.82951390, Gradient norm: 0.30632260
INFO:root:[   33] Training loss: 0.01409818, Validation loss: 0.83709545, Gradient norm: 0.34631519
INFO:root:[   34] Training loss: 0.01388259, Validation loss: 0.85496263, Gradient norm: 0.35012655
INFO:root:[   35] Training loss: 0.01365578, Validation loss: 0.83009434, Gradient norm: 0.31828348
INFO:root:[   36] Training loss: 0.01366930, Validation loss: 0.82515168, Gradient norm: 0.31717347
INFO:root:[   37] Training loss: 0.01351324, Validation loss: 0.79235330, Gradient norm: 0.31291090
INFO:root:[   38] Training loss: 0.01340861, Validation loss: 0.87227015, Gradient norm: 0.32939972
INFO:root:[   39] Training loss: 0.01331981, Validation loss: 0.79462354, Gradient norm: 0.32827300
INFO:root:[   40] Training loss: 0.01322175, Validation loss: 0.78073452, Gradient norm: 0.31899261
INFO:root:[   41] Training loss: 0.01318876, Validation loss: 0.80720706, Gradient norm: 0.32572397
INFO:root:[   42] Training loss: 0.01348090, Validation loss: 0.77452615, Gradient norm: 0.40535632
INFO:root:[   43] Training loss: 0.01311789, Validation loss: 0.80457599, Gradient norm: 0.35227031
INFO:root:[   44] Training loss: 0.01301223, Validation loss: 0.77159170, Gradient norm: 0.32823394
INFO:root:[   45] Training loss: 0.01283015, Validation loss: 0.74540490, Gradient norm: 0.32941804
INFO:root:[   46] Training loss: 0.01289446, Validation loss: 0.78706754, Gradient norm: 0.33869964
INFO:root:[   47] Training loss: 0.01304915, Validation loss: 0.75584871, Gradient norm: 0.40032293
INFO:root:[   48] Training loss: 0.01241167, Validation loss: 0.80909839, Gradient norm: 0.28304861
INFO:root:[   49] Training loss: 0.01255272, Validation loss: 0.79263603, Gradient norm: 0.31051889
INFO:root:[   50] Training loss: 0.01273836, Validation loss: 0.80257911, Gradient norm: 0.38566262
INFO:root:[   51] Training loss: 0.01245371, Validation loss: 0.76961337, Gradient norm: 0.35251014
INFO:root:[   52] Training loss: 0.01268609, Validation loss: 0.82805083, Gradient norm: 0.39603109
INFO:root:[   53] Training loss: 0.01230280, Validation loss: 0.75055218, Gradient norm: 0.34521521
INFO:root:[   54] Training loss: 0.01206298, Validation loss: 0.74166874, Gradient norm: 0.30967158
INFO:root:[   55] Training loss: 0.01235605, Validation loss: 0.73386833, Gradient norm: 0.38788124
INFO:root:[   56] Training loss: 0.01200963, Validation loss: 0.78291743, Gradient norm: 0.31596557
INFO:root:[   57] Training loss: 0.01215422, Validation loss: 0.74612808, Gradient norm: 0.32122546
INFO:root:[   58] Training loss: 0.01175833, Validation loss: 0.76838162, Gradient norm: 0.28835958
INFO:root:[   59] Training loss: 0.01170286, Validation loss: 0.73267175, Gradient norm: 0.27999976
INFO:root:[   60] Training loss: 0.01200087, Validation loss: 0.75927340, Gradient norm: 0.36849281
INFO:root:[   61] Training loss: 0.01178505, Validation loss: 0.74697721, Gradient norm: 0.34406843
INFO:root:[   62] Training loss: 0.01162729, Validation loss: 0.74359661, Gradient norm: 0.30424871
INFO:root:[   63] Training loss: 0.01166352, Validation loss: 0.72664226, Gradient norm: 0.34170379
INFO:root:[   64] Training loss: 0.01170217, Validation loss: 0.74756230, Gradient norm: 0.33094788
INFO:root:[   65] Training loss: 0.01143443, Validation loss: 0.82003953, Gradient norm: 0.31151080
INFO:root:[   66] Training loss: 0.01137564, Validation loss: 0.73903990, Gradient norm: 0.31440036
INFO:root:[   67] Training loss: 0.01140781, Validation loss: 0.79580594, Gradient norm: 0.33729401
INFO:root:[   68] Training loss: 0.01159767, Validation loss: 0.74735839, Gradient norm: 0.36676452
INFO:root:[   69] Training loss: 0.01127164, Validation loss: 0.71957706, Gradient norm: 0.33242284
INFO:root:[   70] Training loss: 0.01151434, Validation loss: 0.72181762, Gradient norm: 0.37632463
INFO:root:[   71] Training loss: 0.01115637, Validation loss: 0.71667997, Gradient norm: 0.31393359
INFO:root:[   72] Training loss: 0.01103111, Validation loss: 0.70660892, Gradient norm: 0.30815897
INFO:root:[   73] Training loss: 0.01102419, Validation loss: 0.71734546, Gradient norm: 0.30392195
INFO:root:[   74] Training loss: 0.01106215, Validation loss: 0.73007653, Gradient norm: 0.33058597
INFO:root:[   75] Training loss: 0.01091466, Validation loss: 0.73496123, Gradient norm: 0.28698008
INFO:root:[   76] Training loss: 0.01076788, Validation loss: 0.77466467, Gradient norm: 0.29695404
INFO:root:[   77] Training loss: 0.01075914, Validation loss: 0.74850452, Gradient norm: 0.29647781
INFO:root:[   78] Training loss: 0.01098821, Validation loss: 0.71244032, Gradient norm: 0.36793096
INFO:root:[   79] Training loss: 0.01084191, Validation loss: 0.69524463, Gradient norm: 0.31904451
INFO:root:[   80] Training loss: 0.01062188, Validation loss: 0.72174383, Gradient norm: 0.30730454
INFO:root:[   81] Training loss: 0.01060422, Validation loss: 0.69937121, Gradient norm: 0.31381751
INFO:root:[   82] Training loss: 0.01063436, Validation loss: 0.70910167, Gradient norm: 0.31386785
INFO:root:[   83] Training loss: 0.01057427, Validation loss: 0.78189795, Gradient norm: 0.31017776
INFO:root:[   84] Training loss: 0.01060323, Validation loss: 0.79641465, Gradient norm: 0.30936531
INFO:root:[   85] Training loss: 0.01056942, Validation loss: 0.69346820, Gradient norm: 0.33731200
INFO:root:[   86] Training loss: 0.01046842, Validation loss: 0.73613547, Gradient norm: 0.32577913
INFO:root:[   87] Training loss: 0.01027842, Validation loss: 0.70048854, Gradient norm: 0.29457988
INFO:root:[   88] Training loss: 0.01024721, Validation loss: 0.69215471, Gradient norm: 0.30736397
INFO:root:[   89] Training loss: 0.01036727, Validation loss: 0.71482037, Gradient norm: 0.32635182
INFO:root:[   90] Training loss: 0.01049074, Validation loss: 0.69012681, Gradient norm: 0.36619990
INFO:root:[   91] Training loss: 0.01017179, Validation loss: 0.70017764, Gradient norm: 0.30797939
INFO:root:[   92] Training loss: 0.01028810, Validation loss: 0.73826950, Gradient norm: 0.32604466
INFO:root:[   93] Training loss: 0.01012499, Validation loss: 0.72128581, Gradient norm: 0.28952966
INFO:root:[   94] Training loss: 0.01042548, Validation loss: 0.70106854, Gradient norm: 0.37670969
INFO:root:[   95] Training loss: 0.01005921, Validation loss: 0.70397833, Gradient norm: 0.33089297
INFO:root:[   96] Training loss: 0.01012070, Validation loss: 0.69199969, Gradient norm: 0.34353059
INFO:root:[   97] Training loss: 0.01007185, Validation loss: 0.69568707, Gradient norm: 0.34980886
INFO:root:[   98] Training loss: 0.00999817, Validation loss: 0.68557272, Gradient norm: 0.31247142
INFO:root:[   99] Training loss: 0.00986985, Validation loss: 0.68612207, Gradient norm: 0.30853431
INFO:root:[  100] Training loss: 0.00990263, Validation loss: 0.71309570, Gradient norm: 0.31921623
INFO:root:[  101] Training loss: 0.00991513, Validation loss: 0.68922573, Gradient norm: 0.34692347
INFO:root:[  102] Training loss: 0.00970503, Validation loss: 0.73535757, Gradient norm: 0.30220175
INFO:root:[  103] Training loss: 0.01005975, Validation loss: 0.71389406, Gradient norm: 0.39554951
INFO:root:[  104] Training loss: 0.00957307, Validation loss: 0.69058484, Gradient norm: 0.26989787
INFO:root:[  105] Training loss: 0.00979419, Validation loss: 0.67940175, Gradient norm: 0.33820518
INFO:root:[  106] Training loss: 0.00970321, Validation loss: 0.67833730, Gradient norm: 0.30538145
INFO:root:[  107] Training loss: 0.00956583, Validation loss: 0.67945074, Gradient norm: 0.30547035
INFO:root:[  108] Training loss: 0.00962086, Validation loss: 0.72735831, Gradient norm: 0.31934998
INFO:root:[  109] Training loss: 0.00957840, Validation loss: 0.70967358, Gradient norm: 0.32906139
INFO:root:[  110] Training loss: 0.00961647, Validation loss: 0.71136509, Gradient norm: 0.36210944
INFO:root:[  111] Training loss: 0.00953193, Validation loss: 0.71317000, Gradient norm: 0.33296397
INFO:root:[  112] Training loss: 0.00930834, Validation loss: 0.69448086, Gradient norm: 0.29328912
INFO:root:[  113] Training loss: 0.00950085, Validation loss: 0.68730351, Gradient norm: 0.34390868
INFO:root:[  114] Training loss: 0.00927185, Validation loss: 0.68438631, Gradient norm: 0.28857246
INFO:root:[  115] Training loss: 0.00919809, Validation loss: 0.70528003, Gradient norm: 0.28555507
INFO:root:EP 115: Early stopping
INFO:root:Training the model took 572.761s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.04639
INFO:root:EnergyScoretrain: 0.03465
INFO:root:Coveragetrain: 6.63357
INFO:root:IntervalWidthtrain: 70.59487
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.0178
INFO:root:EnergyScorevalidation: 0.01389
INFO:root:Coveragevalidation: 1.65166
INFO:root:IntervalWidthvalidation: 18.18609
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02164
INFO:root:EnergyScoretest: 0.01752
INFO:root:Coveragetest: 0.48634
INFO:root:IntervalWidthtest: 24.07445
INFO:root:###35 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06799713, Validation loss: 2.37459343, Gradient norm: 0.30867075
INFO:root:[    2] Training loss: 0.03723568, Validation loss: 1.87920710, Gradient norm: 0.31085554
INFO:root:[    3] Training loss: 0.03164807, Validation loss: 1.70818996, Gradient norm: 0.39452025
INFO:root:[    4] Training loss: 0.02822718, Validation loss: 1.51626036, Gradient norm: 0.32301393
INFO:root:[    5] Training loss: 0.02631512, Validation loss: 1.47269177, Gradient norm: 0.38749480
INFO:root:[    6] Training loss: 0.02485227, Validation loss: 1.36349698, Gradient norm: 0.36605676
INFO:root:[    7] Training loss: 0.02349574, Validation loss: 1.39109171, Gradient norm: 0.35323727
INFO:root:[    8] Training loss: 0.02285461, Validation loss: 1.25036150, Gradient norm: 0.37042251
INFO:root:[    9] Training loss: 0.02190717, Validation loss: 1.23468753, Gradient norm: 0.32517513
INFO:root:[   10] Training loss: 0.02119471, Validation loss: 1.25870929, Gradient norm: 0.34259336
INFO:root:[   11] Training loss: 0.02068466, Validation loss: 1.16436926, Gradient norm: 0.34109692
INFO:root:[   12] Training loss: 0.02016746, Validation loss: 1.10221427, Gradient norm: 0.34008399
INFO:root:[   13] Training loss: 0.01974341, Validation loss: 1.11940491, Gradient norm: 0.35514629
INFO:root:[   14] Training loss: 0.01928228, Validation loss: 1.11458722, Gradient norm: 0.34583516
INFO:root:[   15] Training loss: 0.01851501, Validation loss: 1.05901246, Gradient norm: 0.25250152
INFO:root:[   16] Training loss: 0.01845919, Validation loss: 1.07780475, Gradient norm: 0.33908721
INFO:root:[   17] Training loss: 0.01823344, Validation loss: 1.06745101, Gradient norm: 0.35235937
INFO:root:[   18] Training loss: 0.01843858, Validation loss: 1.05986159, Gradient norm: 0.42883686
INFO:root:[   19] Training loss: 0.01757731, Validation loss: 1.01307349, Gradient norm: 0.27090802
INFO:root:[   20] Training loss: 0.01727848, Validation loss: 1.06416224, Gradient norm: 0.30801888
INFO:root:[   21] Training loss: 0.01710715, Validation loss: 1.06059071, Gradient norm: 0.33283529
INFO:root:[   22] Training loss: 0.01668270, Validation loss: 0.98500861, Gradient norm: 0.29936870
INFO:root:[   23] Training loss: 0.01663191, Validation loss: 0.95759910, Gradient norm: 0.31347575
INFO:root:[   24] Training loss: 0.01620036, Validation loss: 0.94735795, Gradient norm: 0.26308660
INFO:root:[   25] Training loss: 0.01646363, Validation loss: 0.95910285, Gradient norm: 0.32302322
INFO:root:[   26] Training loss: 0.01619546, Validation loss: 0.97290405, Gradient norm: 0.34478009
INFO:root:[   27] Training loss: 0.01618958, Validation loss: 0.91872239, Gradient norm: 0.36487368
INFO:root:[   28] Training loss: 0.01581799, Validation loss: 0.92140732, Gradient norm: 0.30499963
INFO:root:[   29] Training loss: 0.01561046, Validation loss: 0.91334050, Gradient norm: 0.32466154
INFO:root:[   30] Training loss: 0.01538208, Validation loss: 0.99708280, Gradient norm: 0.27957728
INFO:root:[   31] Training loss: 0.01547684, Validation loss: 0.89063109, Gradient norm: 0.33654177
INFO:root:[   32] Training loss: 0.01533927, Validation loss: 0.90227924, Gradient norm: 0.30609117
INFO:root:[   33] Training loss: 0.01544142, Validation loss: 0.90532863, Gradient norm: 0.35070729
INFO:root:[   34] Training loss: 0.01504102, Validation loss: 0.88903711, Gradient norm: 0.32290941
INFO:root:[   35] Training loss: 0.01480882, Validation loss: 0.87595512, Gradient norm: 0.30211068
INFO:root:[   36] Training loss: 0.01465256, Validation loss: 0.87672407, Gradient norm: 0.28916865
INFO:root:[   37] Training loss: 0.01462232, Validation loss: 0.87028752, Gradient norm: 0.31275079
INFO:root:[   38] Training loss: 0.01495947, Validation loss: 0.86954007, Gradient norm: 0.35963009
INFO:root:[   39] Training loss: 0.01429382, Validation loss: 0.86053490, Gradient norm: 0.26029943
INFO:root:[   40] Training loss: 0.01433822, Validation loss: 0.85420473, Gradient norm: 0.30490319
INFO:root:[   41] Training loss: 0.01428870, Validation loss: 0.82916352, Gradient norm: 0.32688135
INFO:root:[   42] Training loss: 0.01438310, Validation loss: 0.83506963, Gradient norm: 0.34536154
INFO:root:[   43] Training loss: 0.01403911, Validation loss: 0.86543709, Gradient norm: 0.30984315
INFO:root:[   44] Training loss: 0.01414597, Validation loss: 0.82013024, Gradient norm: 0.33749669
INFO:root:[   45] Training loss: 0.01362907, Validation loss: 0.87364077, Gradient norm: 0.24773644
INFO:root:[   46] Training loss: 0.01371250, Validation loss: 0.84185964, Gradient norm: 0.28700361
INFO:root:[   47] Training loss: 0.01364618, Validation loss: 0.84592116, Gradient norm: 0.29187684
INFO:root:[   48] Training loss: 0.01374298, Validation loss: 0.83899606, Gradient norm: 0.35266127
INFO:root:[   49] Training loss: 0.01337093, Validation loss: 0.81319097, Gradient norm: 0.25685542
INFO:root:[   50] Training loss: 0.01344076, Validation loss: 0.80357657, Gradient norm: 0.28626175
INFO:root:[   51] Training loss: 0.01338262, Validation loss: 0.81584631, Gradient norm: 0.30155188
INFO:root:[   52] Training loss: 0.01324387, Validation loss: 0.81216650, Gradient norm: 0.30783234
INFO:root:[   53] Training loss: 0.01341717, Validation loss: 0.80985324, Gradient norm: 0.34104923
INFO:root:[   54] Training loss: 0.01320446, Validation loss: 0.84982570, Gradient norm: 0.32181353
INFO:root:[   55] Training loss: 0.01300610, Validation loss: 0.82734673, Gradient norm: 0.31053927
INFO:root:[   56] Training loss: 0.01292027, Validation loss: 0.81832299, Gradient norm: 0.27907214
INFO:root:[   57] Training loss: 0.01273783, Validation loss: 0.82433038, Gradient norm: 0.27474898
INFO:root:[   58] Training loss: 0.01301637, Validation loss: 0.80252370, Gradient norm: 0.34771312
INFO:root:[   59] Training loss: 0.01272120, Validation loss: 0.77630391, Gradient norm: 0.28224096
INFO:root:[   60] Training loss: 0.01257223, Validation loss: 0.78221576, Gradient norm: 0.24779650
INFO:root:[   61] Training loss: 0.01276715, Validation loss: 0.76719987, Gradient norm: 0.31910589
INFO:root:[   62] Training loss: 0.01249420, Validation loss: 0.78216462, Gradient norm: 0.28528940
INFO:root:[   63] Training loss: 0.01242176, Validation loss: 0.77639118, Gradient norm: 0.30506798
INFO:root:[   64] Training loss: 0.01235966, Validation loss: 0.76770311, Gradient norm: 0.26296869
INFO:root:[   65] Training loss: 0.01240484, Validation loss: 0.76845469, Gradient norm: 0.29131049
INFO:root:[   66] Training loss: 0.01238996, Validation loss: 0.75223511, Gradient norm: 0.29355983
INFO:root:[   67] Training loss: 0.01242959, Validation loss: 0.77530269, Gradient norm: 0.31996830
INFO:root:[   68] Training loss: 0.01223191, Validation loss: 0.77974239, Gradient norm: 0.31789793
INFO:root:[   69] Training loss: 0.01216077, Validation loss: 0.74332911, Gradient norm: 0.28043146
INFO:root:[   70] Training loss: 0.01223893, Validation loss: 0.77847979, Gradient norm: 0.32161819
INFO:root:[   71] Training loss: 0.01202369, Validation loss: 0.76073588, Gradient norm: 0.29797842
INFO:root:[   72] Training loss: 0.01202971, Validation loss: 0.81969747, Gradient norm: 0.27919942
INFO:root:[   73] Training loss: 0.01192307, Validation loss: 0.76432803, Gradient norm: 0.30180070
INFO:root:[   74] Training loss: 0.01190666, Validation loss: 0.73525114, Gradient norm: 0.30440763
INFO:root:[   75] Training loss: 0.01185498, Validation loss: 0.82398792, Gradient norm: 0.29650884
INFO:root:[   76] Training loss: 0.01165473, Validation loss: 0.77070075, Gradient norm: 0.28633730
INFO:root:[   77] Training loss: 0.01162793, Validation loss: 0.75761298, Gradient norm: 0.25697739
INFO:root:[   78] Training loss: 0.01189392, Validation loss: 0.73998524, Gradient norm: 0.34145901
INFO:root:[   79] Training loss: 0.01160633, Validation loss: 0.74698115, Gradient norm: 0.28299668
INFO:root:[   80] Training loss: 0.01142607, Validation loss: 0.73979474, Gradient norm: 0.25996307
INFO:root:[   81] Training loss: 0.01166823, Validation loss: 0.75599074, Gradient norm: 0.31939703
INFO:root:[   82] Training loss: 0.01166719, Validation loss: 0.72861700, Gradient norm: 0.34433807
INFO:root:[   83] Training loss: 0.01140130, Validation loss: 0.75983275, Gradient norm: 0.29686262
INFO:root:[   84] Training loss: 0.01148392, Validation loss: 0.87787595, Gradient norm: 0.30830406
INFO:root:[   85] Training loss: 0.01137356, Validation loss: 0.73669836, Gradient norm: 0.30658217
INFO:root:[   86] Training loss: 0.01151158, Validation loss: 0.71533021, Gradient norm: 0.31946874
INFO:root:[   87] Training loss: 0.01131714, Validation loss: 0.76280015, Gradient norm: 0.29346596
INFO:root:[   88] Training loss: 0.01135538, Validation loss: 0.74317326, Gradient norm: 0.30823126
INFO:root:[   89] Training loss: 0.01112977, Validation loss: 0.74186929, Gradient norm: 0.28475746
INFO:root:[   90] Training loss: 0.01123619, Validation loss: 0.72210636, Gradient norm: 0.30611465
INFO:root:[   91] Training loss: 0.01116150, Validation loss: 0.73019291, Gradient norm: 0.29848832
INFO:root:[   92] Training loss: 0.01121606, Validation loss: 0.71846430, Gradient norm: 0.32950332
INFO:root:[   93] Training loss: 0.01092925, Validation loss: 0.72300313, Gradient norm: 0.27416702
INFO:root:[   94] Training loss: 0.01113048, Validation loss: 0.73869458, Gradient norm: 0.31545220
INFO:root:[   95] Training loss: 0.01101938, Validation loss: 0.73344654, Gradient norm: 0.32023160
INFO:root:[   96] Training loss: 0.01083108, Validation loss: 0.70846633, Gradient norm: 0.26325505
INFO:root:[   97] Training loss: 0.01096117, Validation loss: 0.69650595, Gradient norm: 0.30782256
INFO:root:[   98] Training loss: 0.01070026, Validation loss: 0.71632845, Gradient norm: 0.25032226
INFO:root:[   99] Training loss: 0.01085077, Validation loss: 0.71637083, Gradient norm: 0.27557792
INFO:root:[  100] Training loss: 0.01072990, Validation loss: 0.69815495, Gradient norm: 0.28166869
INFO:root:[  101] Training loss: 0.01060535, Validation loss: 0.72735544, Gradient norm: 0.27522186
INFO:root:[  102] Training loss: 0.01075676, Validation loss: 0.71513851, Gradient norm: 0.30074756
INFO:root:[  103] Training loss: 0.01064594, Validation loss: 0.71888832, Gradient norm: 0.31246486
INFO:root:[  104] Training loss: 0.01062536, Validation loss: 0.72473986, Gradient norm: 0.29894999
INFO:root:[  105] Training loss: 0.01054231, Validation loss: 0.71237613, Gradient norm: 0.29233994
INFO:root:[  106] Training loss: 0.01039582, Validation loss: 0.72709837, Gradient norm: 0.27985715
INFO:root:EP 106: Early stopping
INFO:root:Training the model took 528.854s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05152
INFO:root:EnergyScoretrain: 0.03828
INFO:root:Coveragetrain: 6.66117
INFO:root:IntervalWidthtrain: 83.32324
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01756
INFO:root:EnergyScorevalidation: 0.01342
INFO:root:Coveragevalidation: 1.70558
INFO:root:IntervalWidthvalidation: 21.34365
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02335
INFO:root:EnergyScoretest: 0.01828
INFO:root:Coveragetest: 0.72005
INFO:root:IntervalWidthtest: 31.83707
INFO:root:###36 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06638811, Validation loss: 2.53941833, Gradient norm: 0.32580348
INFO:root:[    2] Training loss: 0.04204000, Validation loss: 2.24834270, Gradient norm: 0.31870473
INFO:root:[    3] Training loss: 0.03657421, Validation loss: 1.98949525, Gradient norm: 0.26299187
INFO:root:[    4] Training loss: 0.03313676, Validation loss: 1.77754980, Gradient norm: 0.34658945
INFO:root:[    5] Training loss: 0.03079775, Validation loss: 1.66634823, Gradient norm: 0.33106272
INFO:root:[    6] Training loss: 0.02904571, Validation loss: 1.62258177, Gradient norm: 0.33392492
INFO:root:[    7] Training loss: 0.02790734, Validation loss: 1.61305769, Gradient norm: 0.34718542
INFO:root:[    8] Training loss: 0.02681493, Validation loss: 1.59844488, Gradient norm: 0.34582171
INFO:root:[    9] Training loss: 0.02573177, Validation loss: 1.48269142, Gradient norm: 0.32741736
INFO:root:[   10] Training loss: 0.02452041, Validation loss: 1.41197105, Gradient norm: 0.27348729
INFO:root:[   11] Training loss: 0.02385779, Validation loss: 1.34015806, Gradient norm: 0.26400575
INFO:root:[   12] Training loss: 0.02352650, Validation loss: 1.31791078, Gradient norm: 0.32236239
INFO:root:[   13] Training loss: 0.02283035, Validation loss: 1.41227992, Gradient norm: 0.26383696
INFO:root:[   14] Training loss: 0.02232979, Validation loss: 1.25101146, Gradient norm: 0.32036780
INFO:root:[   15] Training loss: 0.02180725, Validation loss: 1.29559986, Gradient norm: 0.26992050
INFO:root:[   16] Training loss: 0.02143228, Validation loss: 1.22408678, Gradient norm: 0.30049654
INFO:root:[   17] Training loss: 0.02123807, Validation loss: 1.21259868, Gradient norm: 0.30027938
INFO:root:[   18] Training loss: 0.02082951, Validation loss: 1.19272134, Gradient norm: 0.30070672
INFO:root:[   19] Training loss: 0.02034855, Validation loss: 1.16758296, Gradient norm: 0.29813320
INFO:root:[   20] Training loss: 0.02003914, Validation loss: 1.17388901, Gradient norm: 0.28930328
INFO:root:[   21] Training loss: 0.01975256, Validation loss: 1.11547995, Gradient norm: 0.25325061
INFO:root:[   22] Training loss: 0.01959914, Validation loss: 1.10527206, Gradient norm: 0.28534803
INFO:root:[   23] Training loss: 0.01951699, Validation loss: 1.11446252, Gradient norm: 0.31100800
INFO:root:[   24] Training loss: 0.01899424, Validation loss: 1.15645344, Gradient norm: 0.26772714
INFO:root:[   25] Training loss: 0.01899594, Validation loss: 1.09778223, Gradient norm: 0.31320721
INFO:root:[   26] Training loss: 0.01865986, Validation loss: 1.06561015, Gradient norm: 0.30858890
INFO:root:[   27] Training loss: 0.01839069, Validation loss: 1.07630470, Gradient norm: 0.26946707
INFO:root:[   28] Training loss: 0.01797083, Validation loss: 1.01711124, Gradient norm: 0.23731655
INFO:root:[   29] Training loss: 0.01810218, Validation loss: 1.03200728, Gradient norm: 0.27611115
INFO:root:[   30] Training loss: 0.01817027, Validation loss: 1.01832690, Gradient norm: 0.32595456
INFO:root:[   31] Training loss: 0.01765378, Validation loss: 1.02824239, Gradient norm: 0.25673219
INFO:root:[   32] Training loss: 0.01742821, Validation loss: 1.00403042, Gradient norm: 0.27160801
INFO:root:[   33] Training loss: 0.01760110, Validation loss: 1.02691664, Gradient norm: 0.30845629
INFO:root:[   34] Training loss: 0.01739020, Validation loss: 1.02529293, Gradient norm: 0.27159043
INFO:root:[   35] Training loss: 0.01697436, Validation loss: 0.99893533, Gradient norm: 0.27148719
INFO:root:[   36] Training loss: 0.01679606, Validation loss: 1.01070029, Gradient norm: 0.27051935
INFO:root:[   37] Training loss: 0.01688155, Validation loss: 0.98457764, Gradient norm: 0.27756767
INFO:root:[   38] Training loss: 0.01657100, Validation loss: 0.93907714, Gradient norm: 0.28116721
INFO:root:[   39] Training loss: 0.01661011, Validation loss: 0.96005140, Gradient norm: 0.28667933
INFO:root:[   40] Training loss: 0.01660063, Validation loss: 0.95273484, Gradient norm: 0.29238756
INFO:root:[   41] Training loss: 0.01610749, Validation loss: 0.95561434, Gradient norm: 0.26197870
INFO:root:[   42] Training loss: 0.01632538, Validation loss: 0.99398680, Gradient norm: 0.27946977
INFO:root:[   43] Training loss: 0.01591502, Validation loss: 0.95317534, Gradient norm: 0.23653935
INFO:root:[   44] Training loss: 0.01586441, Validation loss: 0.90401954, Gradient norm: 0.25655792
INFO:root:[   45] Training loss: 0.01595232, Validation loss: 0.93762294, Gradient norm: 0.30540166
INFO:root:[   46] Training loss: 0.01570367, Validation loss: 0.92116028, Gradient norm: 0.26759504
INFO:root:[   47] Training loss: 0.01571871, Validation loss: 1.01067071, Gradient norm: 0.29035444
INFO:root:[   48] Training loss: 0.01573070, Validation loss: 0.89214013, Gradient norm: 0.30149004
INFO:root:[   49] Training loss: 0.01529032, Validation loss: 0.88661130, Gradient norm: 0.23259188
INFO:root:[   50] Training loss: 0.01538204, Validation loss: 0.93980429, Gradient norm: 0.28737870
INFO:root:[   51] Training loss: 0.01534687, Validation loss: 0.92613584, Gradient norm: 0.29675685
INFO:root:[   52] Training loss: 0.01501420, Validation loss: 0.88785772, Gradient norm: 0.24745407
INFO:root:[   53] Training loss: 0.01515394, Validation loss: 0.92237855, Gradient norm: 0.29695858
INFO:root:[   54] Training loss: 0.01489279, Validation loss: 0.87341579, Gradient norm: 0.24316283
INFO:root:[   55] Training loss: 0.01476695, Validation loss: 0.89869618, Gradient norm: 0.27622592
INFO:root:[   56] Training loss: 0.01498632, Validation loss: 0.86083440, Gradient norm: 0.29728101
INFO:root:[   57] Training loss: 0.01472727, Validation loss: 0.88066529, Gradient norm: 0.25545215
INFO:root:[   58] Training loss: 0.01461279, Validation loss: 0.87492478, Gradient norm: 0.26210108
INFO:root:[   59] Training loss: 0.01444229, Validation loss: 0.86648955, Gradient norm: 0.25462448
INFO:root:[   60] Training loss: 0.01435123, Validation loss: 0.85615701, Gradient norm: 0.24614437
INFO:root:[   61] Training loss: 0.01437654, Validation loss: 0.85642270, Gradient norm: 0.28696313
INFO:root:[   62] Training loss: 0.01412800, Validation loss: 0.85243225, Gradient norm: 0.25658283
INFO:root:[   63] Training loss: 0.01412086, Validation loss: 0.84249741, Gradient norm: 0.26129986
INFO:root:[   64] Training loss: 0.01417880, Validation loss: 1.10121057, Gradient norm: 0.27958780
INFO:root:[   65] Training loss: 0.01424767, Validation loss: 0.82088494, Gradient norm: 0.31720379
INFO:root:[   66] Training loss: 0.01408181, Validation loss: 0.87703688, Gradient norm: 0.26474809
INFO:root:[   67] Training loss: 0.01404813, Validation loss: 0.81668677, Gradient norm: 0.27871915
INFO:root:[   68] Training loss: 0.01358004, Validation loss: 0.82057352, Gradient norm: 0.23038934
INFO:root:[   69] Training loss: 0.01397700, Validation loss: 0.80902500, Gradient norm: 0.30274165
INFO:root:[   70] Training loss: 0.01373045, Validation loss: 0.85375785, Gradient norm: 0.27363331
INFO:root:[   71] Training loss: 0.01367849, Validation loss: 0.83426285, Gradient norm: 0.27065959
INFO:root:[   72] Training loss: 0.01364242, Validation loss: 0.89728419, Gradient norm: 0.27734675
INFO:root:[   73] Training loss: 0.01345283, Validation loss: 0.80912639, Gradient norm: 0.25362455
INFO:root:[   74] Training loss: 0.01362856, Validation loss: 0.81490794, Gradient norm: 0.29464837
INFO:root:[   75] Training loss: 0.01344383, Validation loss: 0.81223212, Gradient norm: 0.25389457
INFO:root:[   76] Training loss: 0.01362816, Validation loss: 0.83637424, Gradient norm: 0.30108337
INFO:root:[   77] Training loss: 0.01328045, Validation loss: 0.78428169, Gradient norm: 0.25358406
INFO:root:[   78] Training loss: 0.01322078, Validation loss: 0.80621360, Gradient norm: 0.27826601
INFO:root:[   79] Training loss: 0.01305127, Validation loss: 0.82399347, Gradient norm: 0.24306215
INFO:root:[   80] Training loss: 0.01320512, Validation loss: 0.80850032, Gradient norm: 0.27981977
INFO:root:[   81] Training loss: 0.01300380, Validation loss: 0.79169733, Gradient norm: 0.26654190
INFO:root:[   82] Training loss: 0.01304941, Validation loss: 0.88148717, Gradient norm: 0.25943061
INFO:root:[   83] Training loss: 0.01323435, Validation loss: 0.77770683, Gradient norm: 0.31466377
INFO:root:[   84] Training loss: 0.01279829, Validation loss: 0.79326303, Gradient norm: 0.24654461
INFO:root:[   85] Training loss: 0.01286627, Validation loss: 0.77115507, Gradient norm: 0.29038799
INFO:root:[   86] Training loss: 0.01287200, Validation loss: 0.78063279, Gradient norm: 0.27388813
INFO:root:[   87] Training loss: 0.01267866, Validation loss: 0.79900490, Gradient norm: 0.23969682
INFO:root:[   88] Training loss: 0.01276648, Validation loss: 0.78245611, Gradient norm: 0.30512696
INFO:root:[   89] Training loss: 0.01267557, Validation loss: 0.77893667, Gradient norm: 0.29591807
INFO:root:[   90] Training loss: 0.01257969, Validation loss: 0.75788158, Gradient norm: 0.25755285
INFO:root:[   91] Training loss: 0.01252260, Validation loss: 0.78572247, Gradient norm: 0.25287197
INFO:root:[   92] Training loss: 0.01232764, Validation loss: 0.75680117, Gradient norm: 0.24982184
INFO:root:[   93] Training loss: 0.01258060, Validation loss: 0.77112659, Gradient norm: 0.28920015
INFO:root:[   94] Training loss: 0.01221336, Validation loss: 0.75508970, Gradient norm: 0.23034982
INFO:root:[   95] Training loss: 0.01244215, Validation loss: 0.74608361, Gradient norm: 0.28472119
INFO:root:[   96] Training loss: 0.01239740, Validation loss: 0.76583370, Gradient norm: 0.28648084
INFO:root:[   97] Training loss: 0.01210087, Validation loss: 0.78238144, Gradient norm: 0.24299418
INFO:root:[   98] Training loss: 0.01224844, Validation loss: 0.74441599, Gradient norm: 0.27995246
INFO:root:[   99] Training loss: 0.01212180, Validation loss: 0.76972758, Gradient norm: 0.26568510
INFO:root:[  100] Training loss: 0.01218188, Validation loss: 0.78737735, Gradient norm: 0.25427896
INFO:root:[  101] Training loss: 0.01211787, Validation loss: 0.75390877, Gradient norm: 0.30462600
INFO:root:[  102] Training loss: 0.01214517, Validation loss: 0.73446282, Gradient norm: 0.30098309
INFO:root:[  103] Training loss: 0.01182731, Validation loss: 0.74548545, Gradient norm: 0.26654488
INFO:root:[  104] Training loss: 0.01192872, Validation loss: 0.75912843, Gradient norm: 0.27536120
INFO:root:[  105] Training loss: 0.01194306, Validation loss: 0.73980250, Gradient norm: 0.27845521
INFO:root:[  106] Training loss: 0.01189127, Validation loss: 0.76225825, Gradient norm: 0.27777202
INFO:root:[  107] Training loss: 0.01180105, Validation loss: 0.73293014, Gradient norm: 0.26383980
INFO:root:[  108] Training loss: 0.01164951, Validation loss: 0.83271969, Gradient norm: 0.25375560
INFO:root:[  109] Training loss: 0.01179826, Validation loss: 0.73256464, Gradient norm: 0.28435298
INFO:root:[  110] Training loss: 0.01172410, Validation loss: 0.71275057, Gradient norm: 0.26293316
INFO:root:[  111] Training loss: 0.01140894, Validation loss: 0.73008306, Gradient norm: 0.22107912
INFO:root:[  112] Training loss: 0.01140533, Validation loss: 0.72272249, Gradient norm: 0.23561704
INFO:root:[  113] Training loss: 0.01139015, Validation loss: 0.72653113, Gradient norm: 0.23946444
INFO:root:[  114] Training loss: 0.01146082, Validation loss: 0.73485310, Gradient norm: 0.26044591
INFO:root:[  115] Training loss: 0.01131735, Validation loss: 0.74258123, Gradient norm: 0.25016429
INFO:root:[  116] Training loss: 0.01123081, Validation loss: 0.72528148, Gradient norm: 0.24330312
INFO:root:[  117] Training loss: 0.01144111, Validation loss: 0.71112414, Gradient norm: 0.27481940
INFO:root:[  118] Training loss: 0.01132857, Validation loss: 0.70926308, Gradient norm: 0.28403068
INFO:root:[  119] Training loss: 0.01131887, Validation loss: 0.70946012, Gradient norm: 0.25322405
INFO:root:[  120] Training loss: 0.01130892, Validation loss: 0.72454093, Gradient norm: 0.27802242
INFO:root:[  121] Training loss: 0.01140163, Validation loss: 0.69641489, Gradient norm: 0.28083251
INFO:root:[  122] Training loss: 0.01101216, Validation loss: 0.72668675, Gradient norm: 0.24859725
INFO:root:[  123] Training loss: 0.01111788, Validation loss: 0.70745640, Gradient norm: 0.28088017
INFO:root:[  124] Training loss: 0.01100235, Validation loss: 0.78670697, Gradient norm: 0.25502253
INFO:root:[  125] Training loss: 0.01124479, Validation loss: 0.79248877, Gradient norm: 0.31545426
INFO:root:[  126] Training loss: 0.01107858, Validation loss: 0.73487473, Gradient norm: 0.27892952
INFO:root:[  127] Training loss: 0.01089741, Validation loss: 0.70750976, Gradient norm: 0.23571633
INFO:root:[  128] Training loss: 0.01074730, Validation loss: 0.71747216, Gradient norm: 0.23107285
INFO:root:[  129] Training loss: 0.01100259, Validation loss: 0.70786354, Gradient norm: 0.30515043
INFO:root:[  130] Training loss: 0.01075227, Validation loss: 0.71639496, Gradient norm: 0.25891593
INFO:root:EP 130: Early stopping
INFO:root:Training the model took 654.623s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05285
INFO:root:EnergyScoretrain: 0.03962
INFO:root:Coveragetrain: 6.70104
INFO:root:IntervalWidthtrain: 87.54851
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01878
INFO:root:EnergyScorevalidation: 0.01454
INFO:root:Coveragevalidation: 1.72852
INFO:root:IntervalWidthvalidation: 22.81905
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02648
INFO:root:EnergyScoretest: 0.0201
INFO:root:Coveragetest: 0.86304
INFO:root:IntervalWidthtest: 42.44383
INFO:root:###37 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07172923, Validation loss: 2.31453643, Gradient norm: 0.43017803
INFO:root:[    2] Training loss: 0.03497886, Validation loss: 1.86558968, Gradient norm: 0.31712825
INFO:root:[    3] Training loss: 0.02867188, Validation loss: 1.60905690, Gradient norm: 0.34721317
INFO:root:[    4] Training loss: 0.02578846, Validation loss: 1.45897511, Gradient norm: 0.36854218
INFO:root:[    5] Training loss: 0.02464225, Validation loss: 1.35928438, Gradient norm: 0.45465653
INFO:root:[    6] Training loss: 0.02220111, Validation loss: 1.28633672, Gradient norm: 0.32695685
INFO:root:[    7] Training loss: 0.02118553, Validation loss: 1.16236654, Gradient norm: 0.38527489
INFO:root:[    8] Training loss: 0.01982321, Validation loss: 1.17871300, Gradient norm: 0.33366670
INFO:root:[    9] Training loss: 0.01954377, Validation loss: 1.12338454, Gradient norm: 0.44586557
INFO:root:[   10] Training loss: 0.01848115, Validation loss: 1.04357525, Gradient norm: 0.38015163
INFO:root:[   11] Training loss: 0.01788662, Validation loss: 0.97070632, Gradient norm: 0.39996779
INFO:root:[   12] Training loss: 0.01713453, Validation loss: 0.96457966, Gradient norm: 0.35622978
INFO:root:[   13] Training loss: 0.01655090, Validation loss: 0.92877216, Gradient norm: 0.35318056
INFO:root:[   14] Training loss: 0.01655812, Validation loss: 0.96180312, Gradient norm: 0.42712949
INFO:root:[   15] Training loss: 0.01588551, Validation loss: 0.88465502, Gradient norm: 0.33284593
INFO:root:[   16] Training loss: 0.01571745, Validation loss: 0.98731043, Gradient norm: 0.38960543
INFO:root:[   17] Training loss: 0.01536276, Validation loss: 0.93857931, Gradient norm: 0.40128367
INFO:root:[   18] Training loss: 0.01545105, Validation loss: 0.87291587, Gradient norm: 0.41658752
INFO:root:[   19] Training loss: 0.01503010, Validation loss: 1.04856730, Gradient norm: 0.42087413
INFO:root:[   20] Training loss: 0.01492659, Validation loss: 0.85625116, Gradient norm: 0.44467802
INFO:root:[   21] Training loss: 0.01439732, Validation loss: 0.80783147, Gradient norm: 0.37136179
INFO:root:[   22] Training loss: 0.01458729, Validation loss: 0.83860043, Gradient norm: 0.42803659
INFO:root:[   23] Training loss: 0.01398149, Validation loss: 0.80708133, Gradient norm: 0.33442943
INFO:root:[   24] Training loss: 0.01413003, Validation loss: 0.90525694, Gradient norm: 0.42373125
INFO:root:[   25] Training loss: 0.01398190, Validation loss: 0.83877474, Gradient norm: 0.42145232
INFO:root:[   26] Training loss: 0.01349525, Validation loss: 0.78072204, Gradient norm: 0.33607705
INFO:root:[   27] Training loss: 0.01354989, Validation loss: 0.81108188, Gradient norm: 0.38739639
INFO:root:[   28] Training loss: 0.01316140, Validation loss: 0.77283890, Gradient norm: 0.34947809
INFO:root:[   29] Training loss: 0.01330970, Validation loss: 0.75417852, Gradient norm: 0.38598577
INFO:root:[   30] Training loss: 0.01341872, Validation loss: 0.85449293, Gradient norm: 0.41677173
INFO:root:[   31] Training loss: 0.01332598, Validation loss: 0.77050240, Gradient norm: 0.44545132
INFO:root:[   32] Training loss: 0.01277267, Validation loss: 0.78375338, Gradient norm: 0.35611244
INFO:root:[   33] Training loss: 0.01301880, Validation loss: 0.75333036, Gradient norm: 0.42674800
INFO:root:[   34] Training loss: 0.01267733, Validation loss: 0.76050784, Gradient norm: 0.37712164
INFO:root:[   35] Training loss: 0.01287955, Validation loss: 0.77864351, Gradient norm: 0.41431667
INFO:root:[   36] Training loss: 0.01241790, Validation loss: 0.74418673, Gradient norm: 0.38747297
INFO:root:[   37] Training loss: 0.01272722, Validation loss: 0.73170124, Gradient norm: 0.46722672
INFO:root:[   38] Training loss: 0.01217710, Validation loss: 0.74022288, Gradient norm: 0.35105724
INFO:root:[   39] Training loss: 0.01276714, Validation loss: 0.82318020, Gradient norm: 0.48469753
INFO:root:[   40] Training loss: 0.01243261, Validation loss: 0.75773096, Gradient norm: 0.46268785
INFO:root:[   41] Training loss: 0.01188820, Validation loss: 0.78033673, Gradient norm: 0.34293905
INFO:root:[   42] Training loss: 0.01181797, Validation loss: 0.77453704, Gradient norm: 0.35765252
INFO:root:[   43] Training loss: 0.01188378, Validation loss: 0.71372380, Gradient norm: 0.37735393
INFO:root:[   44] Training loss: 0.01224146, Validation loss: 0.72382084, Gradient norm: 0.46254837
INFO:root:[   45] Training loss: 0.01232023, Validation loss: 0.75018329, Gradient norm: 0.47801041
INFO:root:[   46] Training loss: 0.01210259, Validation loss: 0.80199906, Gradient norm: 0.46685922
INFO:root:[   47] Training loss: 0.01165940, Validation loss: 0.72402189, Gradient norm: 0.38767485
INFO:root:[   48] Training loss: 0.01144208, Validation loss: 0.71069698, Gradient norm: 0.32684694
INFO:root:[   49] Training loss: 0.01168975, Validation loss: 0.70664180, Gradient norm: 0.42640833
INFO:root:[   50] Training loss: 0.01162946, Validation loss: 0.71790061, Gradient norm: 0.43403545
INFO:root:[   51] Training loss: 0.01142519, Validation loss: 0.68723768, Gradient norm: 0.41200686
INFO:root:[   52] Training loss: 0.01118458, Validation loss: 0.71762938, Gradient norm: 0.37558717
INFO:root:[   53] Training loss: 0.01148095, Validation loss: 0.71040557, Gradient norm: 0.42783954
INFO:root:[   54] Training loss: 0.01130766, Validation loss: 0.70171401, Gradient norm: 0.42702788
INFO:root:[   55] Training loss: 0.01100364, Validation loss: 0.68748308, Gradient norm: 0.37262896
INFO:root:[   56] Training loss: 0.01110591, Validation loss: 0.77657200, Gradient norm: 0.38742784
INFO:root:[   57] Training loss: 0.01128061, Validation loss: 0.67633465, Gradient norm: 0.45660325
INFO:root:[   58] Training loss: 0.01077919, Validation loss: 0.69411599, Gradient norm: 0.35985758
INFO:root:[   59] Training loss: 0.01121146, Validation loss: 0.73082111, Gradient norm: 0.43327718
INFO:root:[   60] Training loss: 0.01074057, Validation loss: 0.73581281, Gradient norm: 0.35771535
INFO:root:[   61] Training loss: 0.01070071, Validation loss: 0.71146537, Gradient norm: 0.36157354
INFO:root:[   62] Training loss: 0.01108202, Validation loss: 0.71804060, Gradient norm: 0.47062545
INFO:root:[   63] Training loss: 0.01046051, Validation loss: 0.72409408, Gradient norm: 0.33194182
INFO:root:[   64] Training loss: 0.01082646, Validation loss: 0.73910281, Gradient norm: 0.45883530
INFO:root:[   65] Training loss: 0.01042376, Validation loss: 0.70693653, Gradient norm: 0.34154783
INFO:root:[   66] Training loss: 0.01062749, Validation loss: 0.94065010, Gradient norm: 0.41951996
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 331.363s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05866
INFO:root:EnergyScoretrain: 0.04477
INFO:root:Coveragetrain: 6.09877
INFO:root:IntervalWidthtrain: 63.37693
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.019
INFO:root:EnergyScorevalidation: 0.01489
INFO:root:Coveragevalidation: 1.5487
INFO:root:IntervalWidthvalidation: 16.5904
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02287
INFO:root:EnergyScoretest: 0.01923
INFO:root:Coveragetest: 0.40177
INFO:root:IntervalWidthtest: 19.43523
INFO:root:###38 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06100966, Validation loss: 2.71239705, Gradient norm: 0.30417470
INFO:root:[    2] Training loss: 0.03679859, Validation loss: 1.90795242, Gradient norm: 0.28818980
INFO:root:[    3] Training loss: 0.03135915, Validation loss: 1.66839845, Gradient norm: 0.31252178
INFO:root:[    4] Training loss: 0.02807794, Validation loss: 1.55327122, Gradient norm: 0.29415073
INFO:root:[    5] Training loss: 0.02617994, Validation loss: 1.44663675, Gradient norm: 0.30266869
INFO:root:[    6] Training loss: 0.02478927, Validation loss: 1.45767383, Gradient norm: 0.33840627
INFO:root:[    7] Training loss: 0.02382816, Validation loss: 1.46364893, Gradient norm: 0.32380859
INFO:root:[    8] Training loss: 0.02213139, Validation loss: 1.23152097, Gradient norm: 0.28709166
INFO:root:[    9] Training loss: 0.02146627, Validation loss: 1.30669112, Gradient norm: 0.32907957
INFO:root:[   10] Training loss: 0.02044415, Validation loss: 1.14631002, Gradient norm: 0.31335584
INFO:root:[   11] Training loss: 0.01977640, Validation loss: 1.11079219, Gradient norm: 0.28639557
INFO:root:[   12] Training loss: 0.01930235, Validation loss: 1.18994079, Gradient norm: 0.32061835
INFO:root:[   13] Training loss: 0.01859936, Validation loss: 1.08012882, Gradient norm: 0.29321209
INFO:root:[   14] Training loss: 0.01829112, Validation loss: 1.01324628, Gradient norm: 0.31470031
INFO:root:[   15] Training loss: 0.01767916, Validation loss: 1.00882285, Gradient norm: 0.28096869
INFO:root:[   16] Training loss: 0.01735237, Validation loss: 1.02103253, Gradient norm: 0.30033033
INFO:root:[   17] Training loss: 0.01716656, Validation loss: 0.97432991, Gradient norm: 0.29594831
INFO:root:[   18] Training loss: 0.01652583, Validation loss: 0.98120917, Gradient norm: 0.25106977
INFO:root:[   19] Training loss: 0.01637572, Validation loss: 0.93972204, Gradient norm: 0.26856810
INFO:root:[   20] Training loss: 0.01627973, Validation loss: 1.01380319, Gradient norm: 0.31786071
INFO:root:[   21] Training loss: 0.01577432, Validation loss: 0.98624015, Gradient norm: 0.26594705
INFO:root:[   22] Training loss: 0.01558468, Validation loss: 1.04039916, Gradient norm: 0.27624377
INFO:root:[   23] Training loss: 0.01562893, Validation loss: 0.95121433, Gradient norm: 0.30389116
INFO:root:[   24] Training loss: 0.01550472, Validation loss: 0.88687996, Gradient norm: 0.33097587
INFO:root:[   25] Training loss: 0.01489486, Validation loss: 0.87359942, Gradient norm: 0.27428340
INFO:root:[   26] Training loss: 0.01515271, Validation loss: 0.87988560, Gradient norm: 0.33883370
INFO:root:[   27] Training loss: 0.01485488, Validation loss: 0.95883704, Gradient norm: 0.30839215
INFO:root:[   28] Training loss: 0.01464669, Validation loss: 0.93918503, Gradient norm: 0.30339159
INFO:root:[   29] Training loss: 0.01473072, Validation loss: 0.85382565, Gradient norm: 0.36216539
INFO:root:[   30] Training loss: 0.01455863, Validation loss: 0.83365824, Gradient norm: 0.33813283
INFO:root:[   31] Training loss: 0.01419050, Validation loss: 0.87502825, Gradient norm: 0.28479111
INFO:root:[   32] Training loss: 0.01403539, Validation loss: 0.83298820, Gradient norm: 0.28917010
INFO:root:[   33] Training loss: 0.01402913, Validation loss: 0.84011311, Gradient norm: 0.30006569
INFO:root:[   34] Training loss: 0.01409969, Validation loss: 0.86128410, Gradient norm: 0.35607257
INFO:root:[   35] Training loss: 0.01388070, Validation loss: 0.83221467, Gradient norm: 0.32308213
INFO:root:[   36] Training loss: 0.01387366, Validation loss: 0.83413973, Gradient norm: 0.35071402
INFO:root:[   37] Training loss: 0.01373414, Validation loss: 0.84285112, Gradient norm: 0.32691069
INFO:root:[   38] Training loss: 0.01340527, Validation loss: 0.81601732, Gradient norm: 0.29301769
INFO:root:[   39] Training loss: 0.01335413, Validation loss: 0.88790175, Gradient norm: 0.33441665
INFO:root:[   40] Training loss: 0.01336223, Validation loss: 0.78067412, Gradient norm: 0.31109315
INFO:root:[   41] Training loss: 0.01330508, Validation loss: 0.89191296, Gradient norm: 0.31532660
INFO:root:[   42] Training loss: 0.01333014, Validation loss: 0.80494075, Gradient norm: 0.34013408
INFO:root:[   43] Training loss: 0.01321728, Validation loss: 0.83434783, Gradient norm: 0.35709851
INFO:root:[   44] Training loss: 0.01337517, Validation loss: 0.80330040, Gradient norm: 0.36804422
INFO:root:[   45] Training loss: 0.01300973, Validation loss: 0.80098044, Gradient norm: 0.33940322
INFO:root:[   46] Training loss: 0.01284961, Validation loss: 0.78626470, Gradient norm: 0.34501850
INFO:root:[   47] Training loss: 0.01275205, Validation loss: 0.75927767, Gradient norm: 0.32437150
INFO:root:[   48] Training loss: 0.01262938, Validation loss: 0.77185896, Gradient norm: 0.31388353
INFO:root:[   49] Training loss: 0.01254335, Validation loss: 0.77711815, Gradient norm: 0.34770861
INFO:root:[   50] Training loss: 0.01243280, Validation loss: 0.80734515, Gradient norm: 0.28646298
INFO:root:[   51] Training loss: 0.01266576, Validation loss: 0.74960261, Gradient norm: 0.36390422
INFO:root:[   52] Training loss: 0.01240819, Validation loss: 0.77148873, Gradient norm: 0.33730042
INFO:root:[   53] Training loss: 0.01259017, Validation loss: 0.74753513, Gradient norm: 0.36474540
INFO:root:[   54] Training loss: 0.01224271, Validation loss: 0.82486243, Gradient norm: 0.32308340
INFO:root:[   55] Training loss: 0.01216811, Validation loss: 0.79107344, Gradient norm: 0.32498120
INFO:root:[   56] Training loss: 0.01229483, Validation loss: 0.74506330, Gradient norm: 0.37395695
INFO:root:[   57] Training loss: 0.01222645, Validation loss: 0.77845980, Gradient norm: 0.34122743
INFO:root:[   58] Training loss: 0.01199038, Validation loss: 0.77642163, Gradient norm: 0.30713378
INFO:root:[   59] Training loss: 0.01192645, Validation loss: 0.77506175, Gradient norm: 0.33252551
INFO:root:[   60] Training loss: 0.01200222, Validation loss: 0.74729043, Gradient norm: 0.31613243
INFO:root:[   61] Training loss: 0.01216709, Validation loss: 0.74014480, Gradient norm: 0.37138868
INFO:root:[   62] Training loss: 0.01173631, Validation loss: 0.73047765, Gradient norm: 0.32762368
INFO:root:[   63] Training loss: 0.01186539, Validation loss: 0.79865482, Gradient norm: 0.34089539
INFO:root:[   64] Training loss: 0.01171161, Validation loss: 0.74150324, Gradient norm: 0.34714636
INFO:root:[   65] Training loss: 0.01155482, Validation loss: 0.81879000, Gradient norm: 0.31260315
INFO:root:[   66] Training loss: 0.01157730, Validation loss: 0.73869062, Gradient norm: 0.30911578
INFO:root:[   67] Training loss: 0.01157639, Validation loss: 0.75624482, Gradient norm: 0.32682245
INFO:root:[   68] Training loss: 0.01139207, Validation loss: 0.74452952, Gradient norm: 0.30181852
INFO:root:[   69] Training loss: 0.01154171, Validation loss: 0.73089120, Gradient norm: 0.35295129
INFO:root:[   70] Training loss: 0.01152661, Validation loss: 0.73717155, Gradient norm: 0.35061340
INFO:root:[   71] Training loss: 0.01127992, Validation loss: 0.74536682, Gradient norm: 0.31396509
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 355.692s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.06276
INFO:root:EnergyScoretrain: 0.04658
INFO:root:Coveragetrain: 6.36541
INFO:root:IntervalWidthtrain: 82.97447
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01921
INFO:root:EnergyScorevalidation: 0.01458
INFO:root:Coveragevalidation: 1.64015
INFO:root:IntervalWidthvalidation: 21.29532
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01764
INFO:root:EnergyScoretest: 0.01371
INFO:root:Coveragetest: 0.75222
INFO:root:IntervalWidthtest: 26.09158
INFO:root:###39 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07267374, Validation loss: 2.56756812, Gradient norm: 0.36156509
INFO:root:[    2] Training loss: 0.04068614, Validation loss: 2.12436935, Gradient norm: 0.26774934
INFO:root:[    3] Training loss: 0.03491903, Validation loss: 1.87483822, Gradient norm: 0.32084548
INFO:root:[    4] Training loss: 0.03123110, Validation loss: 1.70952707, Gradient norm: 0.30284032
INFO:root:[    5] Training loss: 0.02930347, Validation loss: 1.64664229, Gradient norm: 0.32306720
INFO:root:[    6] Training loss: 0.02706082, Validation loss: 1.51192000, Gradient norm: 0.27453857
INFO:root:[    7] Training loss: 0.02617490, Validation loss: 1.46197071, Gradient norm: 0.34351653
INFO:root:[    8] Training loss: 0.02476781, Validation loss: 1.42100428, Gradient norm: 0.30487211
INFO:root:[    9] Training loss: 0.02367417, Validation loss: 1.31017148, Gradient norm: 0.31079203
INFO:root:[   10] Training loss: 0.02257916, Validation loss: 1.29306489, Gradient norm: 0.28084151
INFO:root:[   11] Training loss: 0.02205582, Validation loss: 1.22718101, Gradient norm: 0.30417224
INFO:root:[   12] Training loss: 0.02124586, Validation loss: 1.21221996, Gradient norm: 0.30165220
INFO:root:[   13] Training loss: 0.02107370, Validation loss: 1.23428794, Gradient norm: 0.33899082
INFO:root:[   14] Training loss: 0.02056431, Validation loss: 1.16472589, Gradient norm: 0.35598302
INFO:root:[   15] Training loss: 0.01965574, Validation loss: 1.14684379, Gradient norm: 0.30002112
INFO:root:[   16] Training loss: 0.01943321, Validation loss: 1.12996485, Gradient norm: 0.29632877
INFO:root:[   17] Training loss: 0.01929216, Validation loss: 1.09007184, Gradient norm: 0.33884526
INFO:root:[   18] Training loss: 0.01877410, Validation loss: 1.07783634, Gradient norm: 0.32246834
INFO:root:[   19] Training loss: 0.01857205, Validation loss: 1.04161175, Gradient norm: 0.35024072
INFO:root:[   20] Training loss: 0.01829735, Validation loss: 1.05203057, Gradient norm: 0.34681681
INFO:root:[   21] Training loss: 0.01792649, Validation loss: 1.15961701, Gradient norm: 0.31573827
INFO:root:[   22] Training loss: 0.01772693, Validation loss: 1.10633220, Gradient norm: 0.31423857
INFO:root:[   23] Training loss: 0.01741366, Validation loss: 1.03887444, Gradient norm: 0.33771184
INFO:root:[   24] Training loss: 0.01742512, Validation loss: 0.96998748, Gradient norm: 0.34558916
INFO:root:[   25] Training loss: 0.01713205, Validation loss: 0.96851902, Gradient norm: 0.35531090
INFO:root:[   26] Training loss: 0.01688908, Validation loss: 1.01006167, Gradient norm: 0.33818878
INFO:root:[   27] Training loss: 0.01689671, Validation loss: 0.97546391, Gradient norm: 0.35782213
INFO:root:[   28] Training loss: 0.01637026, Validation loss: 0.94908684, Gradient norm: 0.31441848
INFO:root:[   29] Training loss: 0.01659647, Validation loss: 0.97266463, Gradient norm: 0.37649021
INFO:root:[   30] Training loss: 0.01640180, Validation loss: 0.96179494, Gradient norm: 0.35499951
INFO:root:[   31] Training loss: 0.01596413, Validation loss: 0.92311801, Gradient norm: 0.31243975
INFO:root:[   32] Training loss: 0.01607290, Validation loss: 0.92689357, Gradient norm: 0.37659131
INFO:root:[   33] Training loss: 0.01583519, Validation loss: 0.97304636, Gradient norm: 0.33283512
INFO:root:[   34] Training loss: 0.01556183, Validation loss: 0.92744875, Gradient norm: 0.31427097
INFO:root:[   35] Training loss: 0.01565987, Validation loss: 0.96097255, Gradient norm: 0.37170163
INFO:root:[   36] Training loss: 0.01547462, Validation loss: 0.92710398, Gradient norm: 0.35027768
INFO:root:[   37] Training loss: 0.01567533, Validation loss: 0.90281628, Gradient norm: 0.41029665
INFO:root:[   38] Training loss: 0.01508293, Validation loss: 0.92322563, Gradient norm: 0.30744735
INFO:root:[   39] Training loss: 0.01577964, Validation loss: 0.93243414, Gradient norm: 0.47145758
INFO:root:[   40] Training loss: 0.01515437, Validation loss: 0.86833899, Gradient norm: 0.32880658
INFO:root:[   41] Training loss: 0.01496798, Validation loss: 0.88317144, Gradient norm: 0.37250067
INFO:root:[   42] Training loss: 0.01493257, Validation loss: 0.90234088, Gradient norm: 0.38794865
INFO:root:[   43] Training loss: 0.01506588, Validation loss: 0.85890702, Gradient norm: 0.43808967
INFO:root:[   44] Training loss: 0.01482437, Validation loss: 0.92735956, Gradient norm: 0.35755094
INFO:root:[   45] Training loss: 0.01447550, Validation loss: 0.92552709, Gradient norm: 0.33750288
INFO:root:[   46] Training loss: 0.01441737, Validation loss: 0.84640402, Gradient norm: 0.33809587
INFO:root:[   47] Training loss: 0.01445368, Validation loss: 0.90400721, Gradient norm: 0.37345622
INFO:root:[   48] Training loss: 0.01432017, Validation loss: 0.85671321, Gradient norm: 0.35185370
INFO:root:[   49] Training loss: 0.01457618, Validation loss: 0.86178456, Gradient norm: 0.44713548
INFO:root:[   50] Training loss: 0.01439237, Validation loss: 0.83774809, Gradient norm: 0.41020328
INFO:root:[   51] Training loss: 0.01424615, Validation loss: 0.82891825, Gradient norm: 0.38087613
INFO:root:[   52] Training loss: 0.01396715, Validation loss: 0.82819191, Gradient norm: 0.34070413
INFO:root:[   53] Training loss: 0.01417831, Validation loss: 1.02039707, Gradient norm: 0.40194860
INFO:root:[   54] Training loss: 0.01422297, Validation loss: 0.82638688, Gradient norm: 0.44156546
INFO:root:[   55] Training loss: 0.01376686, Validation loss: 0.83360355, Gradient norm: 0.34312445
INFO:root:[   56] Training loss: 0.01380515, Validation loss: 0.82443222, Gradient norm: 0.34623378
INFO:root:[   57] Training loss: 0.01384437, Validation loss: 0.85303577, Gradient norm: 0.40620234
INFO:root:[   58] Training loss: 0.01345290, Validation loss: 0.83627718, Gradient norm: 0.31677261
INFO:root:[   59] Training loss: 0.01370589, Validation loss: 0.79212655, Gradient norm: 0.40712347
INFO:root:[   60] Training loss: 0.01338544, Validation loss: 0.79804095, Gradient norm: 0.34293392
INFO:root:[   61] Training loss: 0.01348045, Validation loss: 0.80948089, Gradient norm: 0.35664277
INFO:root:[   62] Training loss: 0.01334851, Validation loss: 0.84432262, Gradient norm: 0.40141070
INFO:root:[   63] Training loss: 0.01347556, Validation loss: 0.85077334, Gradient norm: 0.39255032
INFO:root:[   64] Training loss: 0.01302297, Validation loss: 0.80218433, Gradient norm: 0.30620002
INFO:root:[   65] Training loss: 0.01304154, Validation loss: 0.81669137, Gradient norm: 0.35267932
INFO:root:[   66] Training loss: 0.01319630, Validation loss: 0.77625558, Gradient norm: 0.38093794
INFO:root:[   67] Training loss: 0.01295133, Validation loss: 0.79162497, Gradient norm: 0.33906722
INFO:root:[   68] Training loss: 0.01325313, Validation loss: 0.85202906, Gradient norm: 0.41791691
INFO:root:[   69] Training loss: 0.01294812, Validation loss: 0.78424922, Gradient norm: 0.37918939
INFO:root:[   70] Training loss: 0.01271402, Validation loss: 0.78855822, Gradient norm: 0.33212159
INFO:root:[   71] Training loss: 0.01287432, Validation loss: 0.84913201, Gradient norm: 0.38142488
INFO:root:[   72] Training loss: 0.01272559, Validation loss: 0.76950618, Gradient norm: 0.32613476
INFO:root:[   73] Training loss: 0.01288499, Validation loss: 0.84436511, Gradient norm: 0.39550081
INFO:root:[   74] Training loss: 0.01294734, Validation loss: 0.76211742, Gradient norm: 0.42264396
INFO:root:[   75] Training loss: 0.01262120, Validation loss: 0.78756943, Gradient norm: 0.38804338
INFO:root:[   76] Training loss: 0.01251571, Validation loss: 0.77166833, Gradient norm: 0.34925377
INFO:root:[   77] Training loss: 0.01239572, Validation loss: 0.80608850, Gradient norm: 0.34257558
INFO:root:[   78] Training loss: 0.01266153, Validation loss: 0.75337465, Gradient norm: 0.41493554
INFO:root:[   79] Training loss: 0.01233332, Validation loss: 0.76967561, Gradient norm: 0.33665344
INFO:root:[   80] Training loss: 0.01232098, Validation loss: 0.83856671, Gradient norm: 0.36430902
INFO:root:[   81] Training loss: 0.01212335, Validation loss: 0.75723111, Gradient norm: 0.33161594
INFO:root:[   82] Training loss: 0.01233766, Validation loss: 0.74455572, Gradient norm: 0.38012720
INFO:root:[   83] Training loss: 0.01234993, Validation loss: 0.82253802, Gradient norm: 0.38403122
INFO:root:[   84] Training loss: 0.01207557, Validation loss: 0.82078130, Gradient norm: 0.30737678
INFO:root:[   85] Training loss: 0.01217133, Validation loss: 0.75631089, Gradient norm: 0.37226223
INFO:root:[   86] Training loss: 0.01213790, Validation loss: 0.74591767, Gradient norm: 0.38533254
INFO:root:[   87] Training loss: 0.01197286, Validation loss: 0.74516208, Gradient norm: 0.35574804
INFO:root:[   88] Training loss: 0.01184358, Validation loss: 0.75224140, Gradient norm: 0.32989548
INFO:root:[   89] Training loss: 0.01205870, Validation loss: 0.72469945, Gradient norm: 0.38660294
INFO:root:[   90] Training loss: 0.01167388, Validation loss: 0.76063648, Gradient norm: 0.30107142
INFO:root:[   91] Training loss: 0.01183324, Validation loss: 0.79317026, Gradient norm: 0.37046829
INFO:root:[   92] Training loss: 0.01185498, Validation loss: 0.72888258, Gradient norm: 0.37055709
INFO:root:[   93] Training loss: 0.01170501, Validation loss: 0.75151372, Gradient norm: 0.33829730
INFO:root:[   94] Training loss: 0.01184602, Validation loss: 0.73232655, Gradient norm: 0.39458856
INFO:root:[   95] Training loss: 0.01158882, Validation loss: 0.75306288, Gradient norm: 0.34887334
INFO:root:[   96] Training loss: 0.01158826, Validation loss: 0.70559087, Gradient norm: 0.34921716
INFO:root:[   97] Training loss: 0.01160036, Validation loss: 0.75860905, Gradient norm: 0.36765024
INFO:root:[   98] Training loss: 0.01147378, Validation loss: 0.71688707, Gradient norm: 0.36957135
INFO:root:[   99] Training loss: 0.01171094, Validation loss: 0.81411446, Gradient norm: 0.40329688
INFO:root:[  100] Training loss: 0.01135343, Validation loss: 0.79254807, Gradient norm: 0.34528770
INFO:root:[  101] Training loss: 0.01160471, Validation loss: 0.75337924, Gradient norm: 0.36951962
INFO:root:[  102] Training loss: 0.01124465, Validation loss: 0.72939297, Gradient norm: 0.30904521
INFO:root:[  103] Training loss: 0.01128757, Validation loss: 0.71100275, Gradient norm: 0.33446749
INFO:root:[  104] Training loss: 0.01127570, Validation loss: 0.74632096, Gradient norm: 0.36081160
INFO:root:[  105] Training loss: 0.01136683, Validation loss: 0.69908166, Gradient norm: 0.38411842
INFO:root:[  106] Training loss: 0.01113559, Validation loss: 0.72279083, Gradient norm: 0.34500958
INFO:root:[  107] Training loss: 0.01113687, Validation loss: 0.70948519, Gradient norm: 0.33816113
INFO:root:[  108] Training loss: 0.01117110, Validation loss: 0.73416400, Gradient norm: 0.35598271
INFO:root:[  109] Training loss: 0.01109613, Validation loss: 0.85230124, Gradient norm: 0.35258928
INFO:root:[  110] Training loss: 0.01099659, Validation loss: 0.81330499, Gradient norm: 0.32713811
INFO:root:[  111] Training loss: 0.01106363, Validation loss: 0.77860647, Gradient norm: 0.34448506
INFO:root:[  112] Training loss: 0.01097380, Validation loss: 0.71512460, Gradient norm: 0.34429326
INFO:root:[  113] Training loss: 0.01096801, Validation loss: 0.73164305, Gradient norm: 0.32832764
INFO:root:[  114] Training loss: 0.01106354, Validation loss: 0.84391500, Gradient norm: 0.39000495
INFO:root:EP 114: Early stopping
INFO:root:Training the model took 568.239s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05303
INFO:root:EnergyScoretrain: 0.03983
INFO:root:Coveragetrain: 6.65498
INFO:root:IntervalWidthtrain: 82.78687
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01746
INFO:root:EnergyScorevalidation: 0.01345
INFO:root:Coveragevalidation: 1.71352
INFO:root:IntervalWidthvalidation: 21.25711
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02
INFO:root:EnergyScoretest: 0.01514
INFO:root:Coveragetest: 0.73766
INFO:root:IntervalWidthtest: 32.60992
INFO:root:###40 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07156568, Validation loss: 2.76474123, Gradient norm: 0.37141288
INFO:root:[    2] Training loss: 0.04400550, Validation loss: 2.44801385, Gradient norm: 0.24686725
INFO:root:[    3] Training loss: 0.03957522, Validation loss: 2.12854432, Gradient norm: 0.28504578
INFO:root:[    4] Training loss: 0.03610160, Validation loss: 2.07757150, Gradient norm: 0.28493859
INFO:root:[    5] Training loss: 0.03306305, Validation loss: 1.85125435, Gradient norm: 0.33634574
INFO:root:[    6] Training loss: 0.03060450, Validation loss: 1.69443780, Gradient norm: 0.25866898
INFO:root:[    7] Training loss: 0.02921458, Validation loss: 1.67052714, Gradient norm: 0.29066527
INFO:root:[    8] Training loss: 0.02821299, Validation loss: 1.54825033, Gradient norm: 0.33342211
INFO:root:[    9] Training loss: 0.02678398, Validation loss: 1.50445185, Gradient norm: 0.28793855
INFO:root:[   10] Training loss: 0.02624573, Validation loss: 1.44929815, Gradient norm: 0.31084214
INFO:root:[   11] Training loss: 0.02497775, Validation loss: 1.55523616, Gradient norm: 0.26940419
INFO:root:[   12] Training loss: 0.02468857, Validation loss: 1.34189565, Gradient norm: 0.32938748
INFO:root:[   13] Training loss: 0.02399200, Validation loss: 1.33518052, Gradient norm: 0.29811725
INFO:root:[   14] Training loss: 0.02312994, Validation loss: 1.51266245, Gradient norm: 0.29668950
INFO:root:[   15] Training loss: 0.02258526, Validation loss: 1.41080677, Gradient norm: 0.30633416
INFO:root:[   16] Training loss: 0.02196811, Validation loss: 1.28078993, Gradient norm: 0.31888317
INFO:root:[   17] Training loss: 0.02169453, Validation loss: 1.22339756, Gradient norm: 0.31370160
INFO:root:[   18] Training loss: 0.02097340, Validation loss: 1.19747229, Gradient norm: 0.27821594
INFO:root:[   19] Training loss: 0.02120245, Validation loss: 1.18437581, Gradient norm: 0.36492554
INFO:root:[   20] Training loss: 0.02037450, Validation loss: 1.14902198, Gradient norm: 0.30041825
INFO:root:[   21] Training loss: 0.02013572, Validation loss: 1.16608345, Gradient norm: 0.31888346
INFO:root:[   22] Training loss: 0.01933899, Validation loss: 1.11226317, Gradient norm: 0.25782155
INFO:root:[   23] Training loss: 0.01965869, Validation loss: 1.11863397, Gradient norm: 0.33062307
INFO:root:[   24] Training loss: 0.01894731, Validation loss: 1.10419213, Gradient norm: 0.27679926
INFO:root:[   25] Training loss: 0.01884750, Validation loss: 1.18292525, Gradient norm: 0.27584839
INFO:root:[   26] Training loss: 0.01856214, Validation loss: 1.05881887, Gradient norm: 0.27936457
INFO:root:[   27] Training loss: 0.01856817, Validation loss: 1.08333551, Gradient norm: 0.31843938
INFO:root:[   28] Training loss: 0.01844655, Validation loss: 1.04654697, Gradient norm: 0.33147296
INFO:root:[   29] Training loss: 0.01790893, Validation loss: 1.12692026, Gradient norm: 0.26730575
INFO:root:[   30] Training loss: 0.01790208, Validation loss: 1.10381402, Gradient norm: 0.31382417
INFO:root:[   31] Training loss: 0.01783884, Validation loss: 1.02840266, Gradient norm: 0.34517918
INFO:root:[   32] Training loss: 0.01745405, Validation loss: 1.03030954, Gradient norm: 0.29163113
INFO:root:[   33] Training loss: 0.01736819, Validation loss: 0.99921800, Gradient norm: 0.29163596
INFO:root:[   34] Training loss: 0.01707028, Validation loss: 1.00151309, Gradient norm: 0.30284521
INFO:root:[   35] Training loss: 0.01702785, Validation loss: 0.96951260, Gradient norm: 0.31066285
INFO:root:[   36] Training loss: 0.01709651, Validation loss: 0.98106682, Gradient norm: 0.34651048
INFO:root:[   37] Training loss: 0.01699743, Validation loss: 0.97890934, Gradient norm: 0.36933642
INFO:root:[   38] Training loss: 0.01642099, Validation loss: 0.98138438, Gradient norm: 0.28691410
INFO:root:[   39] Training loss: 0.01658745, Validation loss: 0.97109405, Gradient norm: 0.32148029
INFO:root:[   40] Training loss: 0.01638951, Validation loss: 0.93690122, Gradient norm: 0.31184681
INFO:root:[   41] Training loss: 0.01625162, Validation loss: 0.92145254, Gradient norm: 0.32051735
INFO:root:[   42] Training loss: 0.01584906, Validation loss: 0.93183640, Gradient norm: 0.28847035
INFO:root:[   43] Training loss: 0.01583180, Validation loss: 0.89208432, Gradient norm: 0.29483260
INFO:root:[   44] Training loss: 0.01593281, Validation loss: 0.91120984, Gradient norm: 0.33814587
INFO:root:[   45] Training loss: 0.01629062, Validation loss: 0.92409349, Gradient norm: 0.38159982
INFO:root:[   46] Training loss: 0.01574633, Validation loss: 0.91874572, Gradient norm: 0.33905928
INFO:root:[   47] Training loss: 0.01580586, Validation loss: 0.93035913, Gradient norm: 0.35756644
INFO:root:[   48] Training loss: 0.01549157, Validation loss: 0.91737366, Gradient norm: 0.32117826
INFO:root:[   49] Training loss: 0.01563493, Validation loss: 0.91343397, Gradient norm: 0.35550444
INFO:root:[   50] Training loss: 0.01505727, Validation loss: 0.90004474, Gradient norm: 0.28121364
INFO:root:[   51] Training loss: 0.01536457, Validation loss: 0.91268227, Gradient norm: 0.33783052
INFO:root:[   52] Training loss: 0.01508754, Validation loss: 0.87710390, Gradient norm: 0.30199364
INFO:root:[   53] Training loss: 0.01510946, Validation loss: 0.89997888, Gradient norm: 0.32520071
INFO:root:[   54] Training loss: 0.01500424, Validation loss: 0.87313283, Gradient norm: 0.31068356
INFO:root:[   55] Training loss: 0.01487810, Validation loss: 0.91251262, Gradient norm: 0.29642340
INFO:root:[   56] Training loss: 0.01478820, Validation loss: 0.88335239, Gradient norm: 0.32611716
INFO:root:[   57] Training loss: 0.01458762, Validation loss: 0.87541237, Gradient norm: 0.27492645
INFO:root:[   58] Training loss: 0.01478013, Validation loss: 0.95697377, Gradient norm: 0.35296578
INFO:root:[   59] Training loss: 0.01475598, Validation loss: 0.84897474, Gradient norm: 0.35194490
INFO:root:[   60] Training loss: 0.01453672, Validation loss: 0.85624729, Gradient norm: 0.33651062
INFO:root:[   61] Training loss: 0.01427157, Validation loss: 0.84617290, Gradient norm: 0.29601661
INFO:root:[   62] Training loss: 0.01427990, Validation loss: 0.86309892, Gradient norm: 0.31458610
INFO:root:[   63] Training loss: 0.01463161, Validation loss: 0.85521344, Gradient norm: 0.37618576
INFO:root:[   64] Training loss: 0.01405753, Validation loss: 0.87137679, Gradient norm: 0.29995909
INFO:root:[   65] Training loss: 0.01428321, Validation loss: 0.84633255, Gradient norm: 0.33120441
INFO:root:[   66] Training loss: 0.01422591, Validation loss: 1.05193445, Gradient norm: 0.35068058
INFO:root:[   67] Training loss: 0.01406746, Validation loss: 0.84677860, Gradient norm: 0.33150114
INFO:root:[   68] Training loss: 0.01408387, Validation loss: 0.92321992, Gradient norm: 0.33916362
INFO:root:[   69] Training loss: 0.01401474, Validation loss: 0.81897845, Gradient norm: 0.35622204
INFO:root:[   70] Training loss: 0.01392292, Validation loss: 0.82102877, Gradient norm: 0.32677019
INFO:root:[   71] Training loss: 0.01365183, Validation loss: 0.81471640, Gradient norm: 0.29800225
INFO:root:[   72] Training loss: 0.01361404, Validation loss: 0.82952483, Gradient norm: 0.31526228
INFO:root:[   73] Training loss: 0.01369695, Validation loss: 0.81661297, Gradient norm: 0.34124406
INFO:root:[   74] Training loss: 0.01354871, Validation loss: 0.81473998, Gradient norm: 0.30245417
INFO:root:[   75] Training loss: 0.01348275, Validation loss: 0.82755286, Gradient norm: 0.31331961
INFO:root:[   76] Training loss: 0.01376845, Validation loss: 0.78853561, Gradient norm: 0.37491374
INFO:root:[   77] Training loss: 0.01353010, Validation loss: 0.79462354, Gradient norm: 0.32650957
INFO:root:[   78] Training loss: 0.01344942, Validation loss: 0.80682989, Gradient norm: 0.31147384
INFO:root:[   79] Training loss: 0.01323517, Validation loss: 0.86198162, Gradient norm: 0.29304344
INFO:root:[   80] Training loss: 0.01345129, Validation loss: 0.87967573, Gradient norm: 0.33714541
INFO:root:[   81] Training loss: 0.01322146, Validation loss: 0.77319417, Gradient norm: 0.30953667
INFO:root:[   82] Training loss: 0.01312562, Validation loss: 0.81206848, Gradient norm: 0.30151167
INFO:root:[   83] Training loss: 0.01331542, Validation loss: 0.79495000, Gradient norm: 0.34081318
INFO:root:[   84] Training loss: 0.01286208, Validation loss: 0.77631025, Gradient norm: 0.30769540
INFO:root:[   85] Training loss: 0.01324477, Validation loss: 0.82221033, Gradient norm: 0.33637910
INFO:root:[   86] Training loss: 0.01298641, Validation loss: 0.77335230, Gradient norm: 0.29965330
INFO:root:[   87] Training loss: 0.01285562, Validation loss: 0.79514579, Gradient norm: 0.28273273
INFO:root:[   88] Training loss: 0.01303416, Validation loss: 0.77072474, Gradient norm: 0.32729459
INFO:root:[   89] Training loss: 0.01282944, Validation loss: 0.89002376, Gradient norm: 0.28603584
INFO:root:[   90] Training loss: 0.01295351, Validation loss: 0.86001424, Gradient norm: 0.32642140
INFO:root:[   91] Training loss: 0.01274050, Validation loss: 0.76684037, Gradient norm: 0.31468987
INFO:root:[   92] Training loss: 0.01301994, Validation loss: 0.79243170, Gradient norm: 0.36608338
INFO:root:[   93] Training loss: 0.01280464, Validation loss: 0.77482495, Gradient norm: 0.32359776
INFO:root:[   94] Training loss: 0.01244316, Validation loss: 0.77182442, Gradient norm: 0.26602012
INFO:root:[   95] Training loss: 0.01278947, Validation loss: 0.76259908, Gradient norm: 0.30926700
INFO:root:[   96] Training loss: 0.01251549, Validation loss: 0.75243710, Gradient norm: 0.29191138
INFO:root:[   97] Training loss: 0.01246165, Validation loss: 0.76129366, Gradient norm: 0.29396397
INFO:root:[   98] Training loss: 0.01223506, Validation loss: 0.75662893, Gradient norm: 0.27946602
INFO:root:[   99] Training loss: 0.01232845, Validation loss: 0.81964083, Gradient norm: 0.26813321
INFO:root:[  100] Training loss: 0.01254636, Validation loss: 0.79474435, Gradient norm: 0.32403411
INFO:root:[  101] Training loss: 0.01250723, Validation loss: 0.75704430, Gradient norm: 0.31425418
INFO:root:[  102] Training loss: 0.01240939, Validation loss: 0.76765784, Gradient norm: 0.31728521
INFO:root:[  103] Training loss: 0.01236558, Validation loss: 0.79589049, Gradient norm: 0.29323002
INFO:root:[  104] Training loss: 0.01212402, Validation loss: 0.77617041, Gradient norm: 0.27065868
INFO:root:[  105] Training loss: 0.01210249, Validation loss: 0.74407230, Gradient norm: 0.29942836
INFO:root:[  106] Training loss: 0.01238933, Validation loss: 0.76015038, Gradient norm: 0.32261582
INFO:root:[  107] Training loss: 0.01235914, Validation loss: 0.78421786, Gradient norm: 0.34004469
INFO:root:[  108] Training loss: 0.01204281, Validation loss: 0.74430935, Gradient norm: 0.30222782
INFO:root:[  109] Training loss: 0.01197956, Validation loss: 0.77644283, Gradient norm: 0.27307039
INFO:root:[  110] Training loss: 0.01191616, Validation loss: 0.75946805, Gradient norm: 0.27893740
INFO:root:[  111] Training loss: 0.01188868, Validation loss: 0.74466401, Gradient norm: 0.29532131
INFO:root:[  112] Training loss: 0.01177244, Validation loss: 0.76010957, Gradient norm: 0.26857733
INFO:root:[  113] Training loss: 0.01195537, Validation loss: 0.73316440, Gradient norm: 0.28958883
INFO:root:[  114] Training loss: 0.01188998, Validation loss: 0.76525160, Gradient norm: 0.29939904
INFO:root:[  115] Training loss: 0.01187121, Validation loss: 0.73788204, Gradient norm: 0.31470599
INFO:root:[  116] Training loss: 0.01190537, Validation loss: 0.76965485, Gradient norm: 0.30627585
INFO:root:[  117] Training loss: 0.01154554, Validation loss: 0.73718758, Gradient norm: 0.23945852
INFO:root:[  118] Training loss: 0.01170619, Validation loss: 0.75557198, Gradient norm: 0.28157461
INFO:root:[  119] Training loss: 0.01162643, Validation loss: 0.74788770, Gradient norm: 0.30016475
INFO:root:[  120] Training loss: 0.01149586, Validation loss: 0.75048055, Gradient norm: 0.27403610
INFO:root:[  121] Training loss: 0.01166195, Validation loss: 0.74134181, Gradient norm: 0.29434570
INFO:root:[  122] Training loss: 0.01157228, Validation loss: 0.74766255, Gradient norm: 0.30645139
INFO:root:EP 122: Early stopping
INFO:root:Training the model took 607.177s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05623
INFO:root:EnergyScoretrain: 0.04168
INFO:root:Coveragetrain: 6.69321
INFO:root:IntervalWidthtrain: 93.84698
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01844
INFO:root:EnergyScorevalidation: 0.01389
INFO:root:Coveragevalidation: 1.73276
INFO:root:IntervalWidthvalidation: 24.34187
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02328
INFO:root:EnergyScoretest: 0.01688
INFO:root:Coveragetest: 0.89012
INFO:root:IntervalWidthtest: 45.99683
INFO:root:###41 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06765167, Validation loss: 2.31122179, Gradient norm: 0.34748264
INFO:root:[    2] Training loss: 0.03517264, Validation loss: 1.75070870, Gradient norm: 0.37561994
INFO:root:[    3] Training loss: 0.02863178, Validation loss: 1.55856396, Gradient norm: 0.34878585
INFO:root:[    4] Training loss: 0.02589324, Validation loss: 1.40945173, Gradient norm: 0.36529565
INFO:root:[    5] Training loss: 0.02354037, Validation loss: 1.28956830, Gradient norm: 0.31475485
INFO:root:[    6] Training loss: 0.02207364, Validation loss: 1.21744277, Gradient norm: 0.35711408
INFO:root:[    7] Training loss: 0.02081956, Validation loss: 1.13002626, Gradient norm: 0.37632678
INFO:root:[    8] Training loss: 0.01930710, Validation loss: 1.08400149, Gradient norm: 0.32182039
INFO:root:[    9] Training loss: 0.01877466, Validation loss: 1.10962261, Gradient norm: 0.36664369
INFO:root:[   10] Training loss: 0.01785881, Validation loss: 1.05914546, Gradient norm: 0.33491940
INFO:root:[   11] Training loss: 0.01705868, Validation loss: 0.95645449, Gradient norm: 0.32642756
INFO:root:[   12] Training loss: 0.01676209, Validation loss: 1.05176958, Gradient norm: 0.35704169
INFO:root:[   13] Training loss: 0.01625688, Validation loss: 0.95734787, Gradient norm: 0.35717633
INFO:root:[   14] Training loss: 0.01576494, Validation loss: 0.89352298, Gradient norm: 0.33887010
INFO:root:[   15] Training loss: 0.01575008, Validation loss: 0.90836312, Gradient norm: 0.39455759
INFO:root:[   16] Training loss: 0.01542770, Validation loss: 0.91517239, Gradient norm: 0.38752528
INFO:root:[   17] Training loss: 0.01545151, Validation loss: 0.88640663, Gradient norm: 0.44338436
INFO:root:[   18] Training loss: 0.01456911, Validation loss: 0.86933383, Gradient norm: 0.31840927
INFO:root:[   19] Training loss: 0.01464213, Validation loss: 0.85055598, Gradient norm: 0.39094361
INFO:root:[   20] Training loss: 0.01423109, Validation loss: 0.88745042, Gradient norm: 0.32528652
INFO:root:[   21] Training loss: 0.01445352, Validation loss: 0.80890659, Gradient norm: 0.39589058
INFO:root:[   22] Training loss: 0.01409188, Validation loss: 0.82047108, Gradient norm: 0.37282404
INFO:root:[   23] Training loss: 0.01398288, Validation loss: 0.82751951, Gradient norm: 0.39282923
INFO:root:[   24] Training loss: 0.01378128, Validation loss: 0.77281995, Gradient norm: 0.36344808
INFO:root:[   25] Training loss: 0.01342362, Validation loss: 0.81570048, Gradient norm: 0.30574912
INFO:root:[   26] Training loss: 0.01330222, Validation loss: 0.87898128, Gradient norm: 0.35246183
INFO:root:[   27] Training loss: 0.01351130, Validation loss: 0.75975984, Gradient norm: 0.43184508
INFO:root:[   28] Training loss: 0.01305689, Validation loss: 0.76626981, Gradient norm: 0.37018429
INFO:root:[   29] Training loss: 0.01301659, Validation loss: 0.78699211, Gradient norm: 0.36648181
INFO:root:[   30] Training loss: 0.01323827, Validation loss: 0.74037351, Gradient norm: 0.41558142
INFO:root:[   31] Training loss: 0.01286344, Validation loss: 0.76885158, Gradient norm: 0.39938301
INFO:root:[   32] Training loss: 0.01278600, Validation loss: 0.76879198, Gradient norm: 0.41558787
INFO:root:[   33] Training loss: 0.01241213, Validation loss: 0.74500365, Gradient norm: 0.35458900
INFO:root:[   34] Training loss: 0.01268602, Validation loss: 0.75279285, Gradient norm: 0.38729154
INFO:root:[   35] Training loss: 0.01285326, Validation loss: 0.80030056, Gradient norm: 0.44883100
INFO:root:[   36] Training loss: 0.01240069, Validation loss: 0.75780446, Gradient norm: 0.40064666
INFO:root:[   37] Training loss: 0.01211045, Validation loss: 0.73288971, Gradient norm: 0.34399730
INFO:root:[   38] Training loss: 0.01225817, Validation loss: 0.74079413, Gradient norm: 0.41665867
INFO:root:[   39] Training loss: 0.01175421, Validation loss: 0.74461161, Gradient norm: 0.30374570
INFO:root:[   40] Training loss: 0.01262821, Validation loss: 0.71642282, Gradient norm: 0.51267633
INFO:root:[   41] Training loss: 0.01173428, Validation loss: 0.72487977, Gradient norm: 0.32353970
INFO:root:[   42] Training loss: 0.01178080, Validation loss: 0.74887360, Gradient norm: 0.36108978
INFO:root:[   43] Training loss: 0.01176731, Validation loss: 0.72625458, Gradient norm: 0.38052309
INFO:root:[   44] Training loss: 0.01209829, Validation loss: 0.71997264, Gradient norm: 0.46233727
INFO:root:[   45] Training loss: 0.01164612, Validation loss: 0.77141030, Gradient norm: 0.37714124
INFO:root:[   46] Training loss: 0.01135042, Validation loss: 0.70783511, Gradient norm: 0.35588385
INFO:root:[   47] Training loss: 0.01128135, Validation loss: 0.70404726, Gradient norm: 0.32431810
INFO:root:[   48] Training loss: 0.01171524, Validation loss: 0.73387460, Gradient norm: 0.42482656
INFO:root:[   49] Training loss: 0.01152253, Validation loss: 0.79157188, Gradient norm: 0.41897578
INFO:root:[   50] Training loss: 0.01140079, Validation loss: 0.70826519, Gradient norm: 0.38738895
INFO:root:[   51] Training loss: 0.01164207, Validation loss: 0.79135578, Gradient norm: 0.46535422
INFO:root:[   52] Training loss: 0.01117374, Validation loss: 0.69483432, Gradient norm: 0.38642494
INFO:root:[   53] Training loss: 0.01106587, Validation loss: 0.72739897, Gradient norm: 0.37175837
INFO:root:[   54] Training loss: 0.01118741, Validation loss: 0.69105910, Gradient norm: 0.41159760
INFO:root:[   55] Training loss: 0.01096519, Validation loss: 0.73743584, Gradient norm: 0.37998610
INFO:root:[   56] Training loss: 0.01104898, Validation loss: 0.69945208, Gradient norm: 0.41254109
INFO:root:[   57] Training loss: 0.01069334, Validation loss: 0.68209335, Gradient norm: 0.31058025
INFO:root:[   58] Training loss: 0.01079046, Validation loss: 0.69526756, Gradient norm: 0.37570909
INFO:root:[   59] Training loss: 0.01109531, Validation loss: 0.70711512, Gradient norm: 0.46646911
INFO:root:[   60] Training loss: 0.01062126, Validation loss: 0.67380493, Gradient norm: 0.35780800
INFO:root:[   61] Training loss: 0.01082453, Validation loss: 0.65793935, Gradient norm: 0.42932285
INFO:root:[   62] Training loss: 0.01053604, Validation loss: 0.77484199, Gradient norm: 0.36697167
INFO:root:[   63] Training loss: 0.01069295, Validation loss: 0.67724132, Gradient norm: 0.38143558
INFO:root:[   64] Training loss: 0.01079488, Validation loss: 0.66358719, Gradient norm: 0.43676227
INFO:root:[   65] Training loss: 0.01013669, Validation loss: 0.73833818, Gradient norm: 0.30183552
INFO:root:[   66] Training loss: 0.01032854, Validation loss: 0.66373291, Gradient norm: 0.36271770
INFO:root:[   67] Training loss: 0.01050339, Validation loss: 0.74460444, Gradient norm: 0.41150401
INFO:root:[   68] Training loss: 0.01024014, Validation loss: 0.67973263, Gradient norm: 0.35157535
INFO:root:[   69] Training loss: 0.01043310, Validation loss: 0.67040730, Gradient norm: 0.40072523
INFO:root:[   70] Training loss: 0.01056848, Validation loss: 0.67859294, Gradient norm: 0.43926892
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 350.743s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05594
INFO:root:EnergyScoretrain: 0.04261
INFO:root:Coveragetrain: 6.13713
INFO:root:IntervalWidthtrain: 63.59963
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01799
INFO:root:EnergyScorevalidation: 0.01409
INFO:root:Coveragevalidation: 1.55027
INFO:root:IntervalWidthvalidation: 16.39063
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02107
INFO:root:EnergyScoretest: 0.01748
INFO:root:Coveragetest: 0.41537
INFO:root:IntervalWidthtest: 19.7769
INFO:root:###42 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06128161, Validation loss: 2.32785491, Gradient norm: 0.27956004
INFO:root:[    2] Training loss: 0.03793531, Validation loss: 1.98783871, Gradient norm: 0.32514985
INFO:root:[    3] Training loss: 0.03232336, Validation loss: 1.73094764, Gradient norm: 0.31617004
INFO:root:[    4] Training loss: 0.02848592, Validation loss: 1.57467044, Gradient norm: 0.27538187
INFO:root:[    5] Training loss: 0.02672371, Validation loss: 1.50741594, Gradient norm: 0.31697945
INFO:root:[    6] Training loss: 0.02511923, Validation loss: 1.36549485, Gradient norm: 0.34902843
INFO:root:[    7] Training loss: 0.02380523, Validation loss: 1.32116489, Gradient norm: 0.33607476
INFO:root:[    8] Training loss: 0.02234809, Validation loss: 1.26201338, Gradient norm: 0.29324680
INFO:root:[    9] Training loss: 0.02132880, Validation loss: 1.20214317, Gradient norm: 0.28058000
INFO:root:[   10] Training loss: 0.02086750, Validation loss: 1.17364175, Gradient norm: 0.33811421
INFO:root:[   11] Training loss: 0.01981104, Validation loss: 1.13249450, Gradient norm: 0.28088256
INFO:root:[   12] Training loss: 0.01928441, Validation loss: 1.09344950, Gradient norm: 0.30360066
INFO:root:[   13] Training loss: 0.01895084, Validation loss: 1.07449386, Gradient norm: 0.31378145
INFO:root:[   14] Training loss: 0.01836409, Validation loss: 1.03016264, Gradient norm: 0.30240760
INFO:root:[   15] Training loss: 0.01801894, Validation loss: 1.00368934, Gradient norm: 0.32458998
INFO:root:[   16] Training loss: 0.01781337, Validation loss: 1.09850142, Gradient norm: 0.32481840
INFO:root:[   17] Training loss: 0.01738586, Validation loss: 1.18876716, Gradient norm: 0.30054744
INFO:root:[   18] Training loss: 0.01712543, Validation loss: 0.98459470, Gradient norm: 0.33559652
INFO:root:[   19] Training loss: 0.01706153, Validation loss: 0.95531292, Gradient norm: 0.36589863
INFO:root:[   20] Training loss: 0.01671811, Validation loss: 0.97913549, Gradient norm: 0.34883030
INFO:root:[   21] Training loss: 0.01633464, Validation loss: 0.93546198, Gradient norm: 0.33699086
INFO:root:[   22] Training loss: 0.01616670, Validation loss: 0.95175641, Gradient norm: 0.32735604
INFO:root:[   23] Training loss: 0.01599289, Validation loss: 0.91851623, Gradient norm: 0.32134609
INFO:root:[   24] Training loss: 0.01573162, Validation loss: 0.88768055, Gradient norm: 0.31591422
INFO:root:[   25] Training loss: 0.01541497, Validation loss: 0.91293959, Gradient norm: 0.32637344
INFO:root:[   26] Training loss: 0.01560975, Validation loss: 0.99571181, Gradient norm: 0.36261065
INFO:root:[   27] Training loss: 0.01546732, Validation loss: 0.88672375, Gradient norm: 0.36278478
INFO:root:[   28] Training loss: 0.01519696, Validation loss: 0.85708402, Gradient norm: 0.32733487
INFO:root:[   29] Training loss: 0.01511973, Validation loss: 0.86303572, Gradient norm: 0.34595140
INFO:root:[   30] Training loss: 0.01511805, Validation loss: 0.98691290, Gradient norm: 0.38382661
INFO:root:[   31] Training loss: 0.01470307, Validation loss: 0.86043760, Gradient norm: 0.34308448
INFO:root:[   32] Training loss: 0.01466133, Validation loss: 0.87138335, Gradient norm: 0.35293679
INFO:root:[   33] Training loss: 0.01459674, Validation loss: 0.88257450, Gradient norm: 0.35015619
INFO:root:[   34] Training loss: 0.01454089, Validation loss: 0.89740419, Gradient norm: 0.34531826
INFO:root:[   35] Training loss: 0.01471593, Validation loss: 0.93463575, Gradient norm: 0.40880595
INFO:root:[   36] Training loss: 0.01425661, Validation loss: 0.85907959, Gradient norm: 0.35095658
INFO:root:[   37] Training loss: 0.01407508, Validation loss: 0.86508043, Gradient norm: 0.35509497
INFO:root:[   38] Training loss: 0.01372455, Validation loss: 0.81742526, Gradient norm: 0.29208815
INFO:root:[   39] Training loss: 0.01380063, Validation loss: 0.84465716, Gradient norm: 0.32232546
INFO:root:[   40] Training loss: 0.01415728, Validation loss: 0.81360311, Gradient norm: 0.37647620
INFO:root:[   41] Training loss: 0.01378980, Validation loss: 0.97418400, Gradient norm: 0.34557651
INFO:root:[   42] Training loss: 0.01370001, Validation loss: 0.80054453, Gradient norm: 0.37235393
INFO:root:[   43] Training loss: 0.01351892, Validation loss: 0.98280466, Gradient norm: 0.34375881
INFO:root:[   44] Training loss: 0.01381340, Validation loss: 0.81523160, Gradient norm: 0.40959623
INFO:root:[   45] Training loss: 0.01347016, Validation loss: 0.80592778, Gradient norm: 0.35477480
INFO:root:[   46] Training loss: 0.01330372, Validation loss: 0.78375353, Gradient norm: 0.36771878
INFO:root:[   47] Training loss: 0.01325756, Validation loss: 0.77805124, Gradient norm: 0.34265373
INFO:root:[   48] Training loss: 0.01326263, Validation loss: 0.81219073, Gradient norm: 0.37365608
INFO:root:[   49] Training loss: 0.01329203, Validation loss: 0.78469739, Gradient norm: 0.38125979
INFO:root:[   50] Training loss: 0.01316316, Validation loss: 0.78774277, Gradient norm: 0.37166269
INFO:root:[   51] Training loss: 0.01293285, Validation loss: 0.79341472, Gradient norm: 0.34304639
INFO:root:[   52] Training loss: 0.01283947, Validation loss: 0.80118814, Gradient norm: 0.32882897
INFO:root:[   53] Training loss: 0.01280575, Validation loss: 0.77214666, Gradient norm: 0.31910288
INFO:root:[   54] Training loss: 0.01278271, Validation loss: 0.82510969, Gradient norm: 0.36545315
INFO:root:[   55] Training loss: 0.01294306, Validation loss: 0.75182715, Gradient norm: 0.37653673
INFO:root:[   56] Training loss: 0.01262691, Validation loss: 0.77567690, Gradient norm: 0.34999199
INFO:root:[   57] Training loss: 0.01259169, Validation loss: 0.88853066, Gradient norm: 0.33615354
INFO:root:[   58] Training loss: 0.01271447, Validation loss: 0.74263638, Gradient norm: 0.36264936
INFO:root:[   59] Training loss: 0.01283361, Validation loss: 0.77847022, Gradient norm: 0.39788536
INFO:root:[   60] Training loss: 0.01220959, Validation loss: 0.88198978, Gradient norm: 0.30303692
INFO:root:[   61] Training loss: 0.01251954, Validation loss: 0.76505315, Gradient norm: 0.35214586
INFO:root:[   62] Training loss: 0.01237079, Validation loss: 0.86578152, Gradient norm: 0.36732552
INFO:root:[   63] Training loss: 0.01237215, Validation loss: 0.77283251, Gradient norm: 0.36722897
INFO:root:[   64] Training loss: 0.01220765, Validation loss: 0.73631037, Gradient norm: 0.35515163
INFO:root:[   65] Training loss: 0.01237211, Validation loss: 0.78667977, Gradient norm: 0.38220892
INFO:root:[   66] Training loss: 0.01199397, Validation loss: 0.79915197, Gradient norm: 0.33762455
INFO:root:[   67] Training loss: 0.01210224, Validation loss: 0.75604655, Gradient norm: 0.33848260
INFO:root:[   68] Training loss: 0.01216279, Validation loss: 0.77518082, Gradient norm: 0.38250882
INFO:root:[   69] Training loss: 0.01180146, Validation loss: 0.74473085, Gradient norm: 0.31021107
INFO:root:[   70] Training loss: 0.01186325, Validation loss: 0.72969820, Gradient norm: 0.34221184
INFO:root:[   71] Training loss: 0.01179215, Validation loss: 0.85162532, Gradient norm: 0.33533127
INFO:root:[   72] Training loss: 0.01187464, Validation loss: 0.71303718, Gradient norm: 0.33147920
INFO:root:[   73] Training loss: 0.01176187, Validation loss: 0.72706255, Gradient norm: 0.34315465
INFO:root:[   74] Training loss: 0.01163675, Validation loss: 0.71808817, Gradient norm: 0.31752250
INFO:root:[   75] Training loss: 0.01174137, Validation loss: 0.78049954, Gradient norm: 0.36132664
INFO:root:[   76] Training loss: 0.01192384, Validation loss: 0.72875048, Gradient norm: 0.39448217
INFO:root:[   77] Training loss: 0.01146983, Validation loss: 0.74894651, Gradient norm: 0.31388569
INFO:root:[   78] Training loss: 0.01168696, Validation loss: 0.72788321, Gradient norm: 0.36195248
INFO:root:[   79] Training loss: 0.01161613, Validation loss: 0.75074823, Gradient norm: 0.36240343
INFO:root:[   80] Training loss: 0.01162141, Validation loss: 0.77027853, Gradient norm: 0.36505528
INFO:root:[   81] Training loss: 0.01137191, Validation loss: 0.80322699, Gradient norm: 0.31251121
INFO:root:EP 81: Early stopping
INFO:root:Training the model took 405.611s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05797
INFO:root:EnergyScoretrain: 0.04329
INFO:root:Coveragetrain: 6.51569
INFO:root:IntervalWidthtrain: 82.96034
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01844
INFO:root:EnergyScorevalidation: 0.01406
INFO:root:Coveragevalidation: 1.67553
INFO:root:IntervalWidthvalidation: 21.76288
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02306
INFO:root:EnergyScoretest: 0.01853
INFO:root:Coveragetest: 0.47835
INFO:root:IntervalWidthtest: 26.46444
INFO:root:###43 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06971365, Validation loss: 2.52891109, Gradient norm: 0.30372303
INFO:root:[    2] Training loss: 0.04003824, Validation loss: 2.09353082, Gradient norm: 0.26678542
INFO:root:[    3] Training loss: 0.03397100, Validation loss: 1.80170114, Gradient norm: 0.30405091
INFO:root:[    4] Training loss: 0.03042714, Validation loss: 1.64787455, Gradient norm: 0.31746104
INFO:root:[    5] Training loss: 0.02800379, Validation loss: 1.52542724, Gradient norm: 0.29906562
INFO:root:[    6] Training loss: 0.02616045, Validation loss: 1.46730578, Gradient norm: 0.30773645
INFO:root:[    7] Training loss: 0.02446136, Validation loss: 1.36485961, Gradient norm: 0.24195122
INFO:root:[    8] Training loss: 0.02358455, Validation loss: 1.30066626, Gradient norm: 0.29137998
INFO:root:[    9] Training loss: 0.02272658, Validation loss: 1.25217499, Gradient norm: 0.30562666
INFO:root:[   10] Training loss: 0.02151797, Validation loss: 1.21282800, Gradient norm: 0.24995297
INFO:root:[   11] Training loss: 0.02112094, Validation loss: 1.19468802, Gradient norm: 0.29640876
INFO:root:[   12] Training loss: 0.02047451, Validation loss: 1.15500064, Gradient norm: 0.28529392
INFO:root:[   13] Training loss: 0.02002538, Validation loss: 1.14143760, Gradient norm: 0.30627843
INFO:root:[   14] Training loss: 0.01953536, Validation loss: 1.10712233, Gradient norm: 0.28897536
INFO:root:[   15] Training loss: 0.01889268, Validation loss: 1.14841712, Gradient norm: 0.26671697
INFO:root:[   16] Training loss: 0.01919416, Validation loss: 1.10716533, Gradient norm: 0.35172137
INFO:root:[   17] Training loss: 0.01893897, Validation loss: 1.05169251, Gradient norm: 0.36488840
INFO:root:[   18] Training loss: 0.01804376, Validation loss: 1.03103425, Gradient norm: 0.27959366
INFO:root:[   19] Training loss: 0.01779802, Validation loss: 1.03336742, Gradient norm: 0.32200051
INFO:root:[   20] Training loss: 0.01757666, Validation loss: 0.99808716, Gradient norm: 0.30504311
INFO:root:[   21] Training loss: 0.01742513, Validation loss: 1.13338445, Gradient norm: 0.30573191
INFO:root:[   22] Training loss: 0.01726278, Validation loss: 1.07504537, Gradient norm: 0.32230010
INFO:root:[   23] Training loss: 0.01695828, Validation loss: 1.01060490, Gradient norm: 0.33807487
INFO:root:[   24] Training loss: 0.01665924, Validation loss: 0.95575178, Gradient norm: 0.31826443
INFO:root:[   25] Training loss: 0.01688292, Validation loss: 0.96465893, Gradient norm: 0.35735885
INFO:root:[   26] Training loss: 0.01638195, Validation loss: 1.04448915, Gradient norm: 0.33469152
INFO:root:[   27] Training loss: 0.01646983, Validation loss: 1.14128649, Gradient norm: 0.37474190
INFO:root:[   28] Training loss: 0.01617981, Validation loss: 1.02747120, Gradient norm: 0.33266357
INFO:root:[   29] Training loss: 0.01595709, Validation loss: 0.93232732, Gradient norm: 0.33666686
INFO:root:[   30] Training loss: 0.01590550, Validation loss: 0.93049959, Gradient norm: 0.34526848
INFO:root:[   31] Training loss: 0.01589107, Validation loss: 0.93597250, Gradient norm: 0.36799389
INFO:root:[   32] Training loss: 0.01551507, Validation loss: 0.89656348, Gradient norm: 0.33747243
INFO:root:[   33] Training loss: 0.01569265, Validation loss: 0.94345996, Gradient norm: 0.38245678
INFO:root:[   34] Training loss: 0.01558884, Validation loss: 0.95084308, Gradient norm: 0.37843616
INFO:root:[   35] Training loss: 0.01529884, Validation loss: 0.92206690, Gradient norm: 0.36911991
INFO:root:[   36] Training loss: 0.01511733, Validation loss: 0.92716832, Gradient norm: 0.35946346
INFO:root:[   37] Training loss: 0.01471702, Validation loss: 0.89999311, Gradient norm: 0.29995777
INFO:root:[   38] Training loss: 0.01501745, Validation loss: 0.86263523, Gradient norm: 0.37874160
INFO:root:[   39] Training loss: 0.01498061, Validation loss: 0.87003587, Gradient norm: 0.37155000
INFO:root:[   40] Training loss: 0.01472343, Validation loss: 0.88045748, Gradient norm: 0.34632606
INFO:root:[   41] Training loss: 0.01451891, Validation loss: 0.86560844, Gradient norm: 0.32806295
INFO:root:[   42] Training loss: 0.01463652, Validation loss: 0.90105596, Gradient norm: 0.37955165
INFO:root:[   43] Training loss: 0.01435487, Validation loss: 0.86259457, Gradient norm: 0.33139906
INFO:root:[   44] Training loss: 0.01479097, Validation loss: 0.94019620, Gradient norm: 0.43261179
INFO:root:[   45] Training loss: 0.01415145, Validation loss: 0.89627844, Gradient norm: 0.32001919
INFO:root:[   46] Training loss: 0.01418316, Validation loss: 0.82915838, Gradient norm: 0.34993972
INFO:root:[   47] Training loss: 0.01432456, Validation loss: 0.81881989, Gradient norm: 0.37952647
INFO:root:[   48] Training loss: 0.01391711, Validation loss: 0.82597808, Gradient norm: 0.33576359
INFO:root:[   49] Training loss: 0.01416719, Validation loss: 0.82537309, Gradient norm: 0.39372629
INFO:root:[   50] Training loss: 0.01394172, Validation loss: 0.83151240, Gradient norm: 0.36995270
INFO:root:[   51] Training loss: 0.01359721, Validation loss: 0.82320151, Gradient norm: 0.33110100
INFO:root:[   52] Training loss: 0.01383681, Validation loss: 0.82550411, Gradient norm: 0.36094591
INFO:root:[   53] Training loss: 0.01380177, Validation loss: 0.81310595, Gradient norm: 0.36183563
INFO:root:[   54] Training loss: 0.01372152, Validation loss: 0.91038042, Gradient norm: 0.40283002
INFO:root:[   55] Training loss: 0.01348684, Validation loss: 0.82749855, Gradient norm: 0.37126937
INFO:root:[   56] Training loss: 0.01399005, Validation loss: 0.83600631, Gradient norm: 0.45547315
INFO:root:[   57] Training loss: 0.01347057, Validation loss: 0.81840670, Gradient norm: 0.38691219
INFO:root:[   58] Training loss: 0.01333089, Validation loss: 0.79224033, Gradient norm: 0.36151675
INFO:root:[   59] Training loss: 0.01342773, Validation loss: 0.84868546, Gradient norm: 0.38036633
INFO:root:[   60] Training loss: 0.01324045, Validation loss: 0.81510477, Gradient norm: 0.34338435
INFO:root:[   61] Training loss: 0.01338757, Validation loss: 0.80480907, Gradient norm: 0.36457787
INFO:root:[   62] Training loss: 0.01329244, Validation loss: 0.79326707, Gradient norm: 0.37066307
INFO:root:[   63] Training loss: 0.01315537, Validation loss: 0.80714186, Gradient norm: 0.37458939
INFO:root:[   64] Training loss: 0.01288173, Validation loss: 0.77378966, Gradient norm: 0.32934716
INFO:root:[   65] Training loss: 0.01318308, Validation loss: 0.79284222, Gradient norm: 0.37955293
INFO:root:[   66] Training loss: 0.01317255, Validation loss: 0.78442043, Gradient norm: 0.41237894
INFO:root:[   67] Training loss: 0.01298216, Validation loss: 0.78869596, Gradient norm: 0.38224719
INFO:root:[   68] Training loss: 0.01281963, Validation loss: 0.83791284, Gradient norm: 0.33713039
INFO:root:[   69] Training loss: 0.01290384, Validation loss: 0.84671100, Gradient norm: 0.38339957
INFO:root:[   70] Training loss: 0.01291174, Validation loss: 0.94501756, Gradient norm: 0.36880204
INFO:root:[   71] Training loss: 0.01276927, Validation loss: 0.78080301, Gradient norm: 0.35831118
INFO:root:[   72] Training loss: 0.01295550, Validation loss: 0.79214720, Gradient norm: 0.41079614
INFO:root:[   73] Training loss: 0.01259012, Validation loss: 0.75679718, Gradient norm: 0.33484984
INFO:root:[   74] Training loss: 0.01258524, Validation loss: 0.77008047, Gradient norm: 0.35699372
INFO:root:[   75] Training loss: 0.01237297, Validation loss: 0.75658403, Gradient norm: 0.34210723
INFO:root:[   76] Training loss: 0.01234760, Validation loss: 0.76919695, Gradient norm: 0.31749961
INFO:root:[   77] Training loss: 0.01220203, Validation loss: 0.79632511, Gradient norm: 0.32713064
INFO:root:[   78] Training loss: 0.01226716, Validation loss: 0.75618484, Gradient norm: 0.33632563
INFO:root:[   79] Training loss: 0.01239304, Validation loss: 0.76536257, Gradient norm: 0.36607428
INFO:root:[   80] Training loss: 0.01239285, Validation loss: 0.78551356, Gradient norm: 0.38272205
INFO:root:[   81] Training loss: 0.01250768, Validation loss: 0.76984248, Gradient norm: 0.40995829
INFO:root:[   82] Training loss: 0.01197539, Validation loss: 0.76329828, Gradient norm: 0.30876216
INFO:root:[   83] Training loss: 0.01242972, Validation loss: 0.76875328, Gradient norm: 0.39499054
INFO:root:[   84] Training loss: 0.01201979, Validation loss: 0.80640342, Gradient norm: 0.32544376
INFO:root:[   85] Training loss: 0.01219282, Validation loss: 0.74352474, Gradient norm: 0.37840879
INFO:root:[   86] Training loss: 0.01188709, Validation loss: 0.74757767, Gradient norm: 0.32018300
INFO:root:[   87] Training loss: 0.01188703, Validation loss: 0.75831203, Gradient norm: 0.31316605
INFO:root:[   88] Training loss: 0.01196222, Validation loss: 0.75457046, Gradient norm: 0.33946140
INFO:root:[   89] Training loss: 0.01181184, Validation loss: 0.76104538, Gradient norm: 0.33324617
INFO:root:[   90] Training loss: 0.01189333, Validation loss: 0.77896046, Gradient norm: 0.33592462
INFO:root:[   91] Training loss: 0.01214981, Validation loss: 0.75148986, Gradient norm: 0.40059916
INFO:root:[   92] Training loss: 0.01152481, Validation loss: 0.77015413, Gradient norm: 0.28796054
INFO:root:[   93] Training loss: 0.01170136, Validation loss: 0.85229797, Gradient norm: 0.31871273
INFO:root:[   94] Training loss: 0.01162680, Validation loss: 0.73948705, Gradient norm: 0.33957821
INFO:root:[   95] Training loss: 0.01188838, Validation loss: 0.73419043, Gradient norm: 0.38671182
INFO:root:[   96] Training loss: 0.01140449, Validation loss: 0.73223742, Gradient norm: 0.28464401
INFO:root:[   97] Training loss: 0.01159657, Validation loss: 0.85825946, Gradient norm: 0.34000479
INFO:root:[   98] Training loss: 0.01176203, Validation loss: 0.73559442, Gradient norm: 0.37236770
INFO:root:[   99] Training loss: 0.01142904, Validation loss: 0.73716125, Gradient norm: 0.30770552
INFO:root:[  100] Training loss: 0.01132717, Validation loss: 0.74762401, Gradient norm: 0.30365833
INFO:root:[  101] Training loss: 0.01156266, Validation loss: 0.72080319, Gradient norm: 0.35731877
INFO:root:[  102] Training loss: 0.01130069, Validation loss: 0.74640961, Gradient norm: 0.31093887
INFO:root:[  103] Training loss: 0.01129119, Validation loss: 0.72905246, Gradient norm: 0.33931661
INFO:root:[  104] Training loss: 0.01121311, Validation loss: 0.84545032, Gradient norm: 0.31661479
INFO:root:[  105] Training loss: 0.01120405, Validation loss: 0.76766490, Gradient norm: 0.31244427
INFO:root:[  106] Training loss: 0.01129785, Validation loss: 0.75442826, Gradient norm: 0.34226134
INFO:root:[  107] Training loss: 0.01136275, Validation loss: 0.73019600, Gradient norm: 0.35450409
INFO:root:[  108] Training loss: 0.01121907, Validation loss: 0.76948384, Gradient norm: 0.34191199
INFO:root:[  109] Training loss: 0.01101083, Validation loss: 0.72005801, Gradient norm: 0.29187533
INFO:root:[  110] Training loss: 0.01098595, Validation loss: 0.71873770, Gradient norm: 0.30868964
INFO:root:[  111] Training loss: 0.01108426, Validation loss: 0.73044892, Gradient norm: 0.34486711
INFO:root:[  112] Training loss: 0.01110939, Validation loss: 0.73125966, Gradient norm: 0.33007170
INFO:root:[  113] Training loss: 0.01092990, Validation loss: 0.74380111, Gradient norm: 0.31315529
INFO:root:[  114] Training loss: 0.01084350, Validation loss: 0.73870675, Gradient norm: 0.31037457
INFO:root:[  115] Training loss: 0.01116122, Validation loss: 0.76539643, Gradient norm: 0.38387959
INFO:root:[  116] Training loss: 0.01071303, Validation loss: 0.71650608, Gradient norm: 0.28211194
INFO:root:[  117] Training loss: 0.01087145, Validation loss: 0.71918595, Gradient norm: 0.32894224
INFO:root:[  118] Training loss: 0.01074334, Validation loss: 0.71877745, Gradient norm: 0.28947115
INFO:root:[  119] Training loss: 0.01068064, Validation loss: 0.72998918, Gradient norm: 0.30929967
INFO:root:[  120] Training loss: 0.01060850, Validation loss: 0.70417416, Gradient norm: 0.27140106
INFO:root:[  121] Training loss: 0.01056756, Validation loss: 0.73463455, Gradient norm: 0.29749297
INFO:root:[  122] Training loss: 0.01068488, Validation loss: 0.70173059, Gradient norm: 0.32672338
INFO:root:[  123] Training loss: 0.01044880, Validation loss: 0.71561756, Gradient norm: 0.26722121
INFO:root:[  124] Training loss: 0.01077112, Validation loss: 0.73203367, Gradient norm: 0.34514587
INFO:root:[  125] Training loss: 0.01065263, Validation loss: 0.73172473, Gradient norm: 0.32755890
INFO:root:[  126] Training loss: 0.01056303, Validation loss: 0.78321499, Gradient norm: 0.31916646
INFO:root:[  127] Training loss: 0.01043946, Validation loss: 0.72639053, Gradient norm: 0.30227720
INFO:root:[  128] Training loss: 0.01067389, Validation loss: 0.71251404, Gradient norm: 0.31689031
INFO:root:[  129] Training loss: 0.01041365, Validation loss: 0.71379175, Gradient norm: 0.31426429
INFO:root:[  130] Training loss: 0.01045561, Validation loss: 0.74470357, Gradient norm: 0.31819267
INFO:root:[  131] Training loss: 0.01047339, Validation loss: 0.69517617, Gradient norm: 0.34718535
INFO:root:[  132] Training loss: 0.01027712, Validation loss: 0.71756661, Gradient norm: 0.28155536
INFO:root:[  133] Training loss: 0.01026667, Validation loss: 0.71069110, Gradient norm: 0.29484516
INFO:root:[  134] Training loss: 0.01047287, Validation loss: 0.71739410, Gradient norm: 0.33270949
INFO:root:[  135] Training loss: 0.01029934, Validation loss: 0.71100083, Gradient norm: 0.34208415
INFO:root:[  136] Training loss: 0.01026168, Validation loss: 0.74870551, Gradient norm: 0.31181226
INFO:root:[  137] Training loss: 0.01029532, Validation loss: 0.70679166, Gradient norm: 0.33190700
INFO:root:[  138] Training loss: 0.01007157, Validation loss: 0.76512273, Gradient norm: 0.26293799
INFO:root:[  139] Training loss: 0.01010206, Validation loss: 0.72240889, Gradient norm: 0.28330716
INFO:root:[  140] Training loss: 0.01025068, Validation loss: 0.68477534, Gradient norm: 0.33176152
INFO:root:[  141] Training loss: 0.01028613, Validation loss: 0.70112177, Gradient norm: 0.34711336
INFO:root:[  142] Training loss: 0.00998895, Validation loss: 0.70978233, Gradient norm: 0.28410700
INFO:root:[  143] Training loss: 0.01007769, Validation loss: 0.71301823, Gradient norm: 0.28670495
INFO:root:[  144] Training loss: 0.01005486, Validation loss: 0.69235266, Gradient norm: 0.29813306
INFO:root:[  145] Training loss: 0.01017417, Validation loss: 0.69134911, Gradient norm: 0.33643030
INFO:root:[  146] Training loss: 0.00995932, Validation loss: 0.72197738, Gradient norm: 0.29443203
INFO:root:[  147] Training loss: 0.00982388, Validation loss: 0.69611594, Gradient norm: 0.28570313
INFO:root:[  148] Training loss: 0.00989467, Validation loss: 0.69157144, Gradient norm: 0.29546042
INFO:root:[  149] Training loss: 0.01010173, Validation loss: 0.69598475, Gradient norm: 0.32801533
INFO:root:EP 149: Early stopping
INFO:root:Training the model took 774.148s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.04626
INFO:root:EnergyScoretrain: 0.03445
INFO:root:Coveragetrain: 6.72409
INFO:root:IntervalWidthtrain: 80.00742
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01738
INFO:root:EnergyScorevalidation: 0.0134
INFO:root:Coveragevalidation: 1.70137
INFO:root:IntervalWidthvalidation: 20.64371
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02018
INFO:root:EnergyScoretest: 0.0154
INFO:root:Coveragetest: 0.80573
INFO:root:IntervalWidthtest: 31.37539
INFO:root:###44 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07016821, Validation loss: 2.72736111, Gradient norm: 0.27470763
INFO:root:[    2] Training loss: 0.04415903, Validation loss: 2.36204571, Gradient norm: 0.30993542
INFO:root:[    3] Training loss: 0.03852711, Validation loss: 2.05633145, Gradient norm: 0.28426049
INFO:root:[    4] Training loss: 0.03466471, Validation loss: 1.96409298, Gradient norm: 0.25568356
INFO:root:[    5] Training loss: 0.03188159, Validation loss: 2.07048861, Gradient norm: 0.28052609
INFO:root:[    6] Training loss: 0.03030475, Validation loss: 1.68838591, Gradient norm: 0.30343588
INFO:root:[    7] Training loss: 0.02854636, Validation loss: 1.62996611, Gradient norm: 0.24673617
INFO:root:[    8] Training loss: 0.02737040, Validation loss: 1.58716993, Gradient norm: 0.28802282
INFO:root:[    9] Training loss: 0.02673896, Validation loss: 1.51463716, Gradient norm: 0.32165989
INFO:root:[   10] Training loss: 0.02534750, Validation loss: 1.51402695, Gradient norm: 0.29173239
INFO:root:[   11] Training loss: 0.02456999, Validation loss: 1.39044368, Gradient norm: 0.25892727
INFO:root:[   12] Training loss: 0.02381843, Validation loss: 1.34359214, Gradient norm: 0.26341440
INFO:root:[   13] Training loss: 0.02361117, Validation loss: 1.31366767, Gradient norm: 0.29428995
INFO:root:[   14] Training loss: 0.02259629, Validation loss: 1.36476272, Gradient norm: 0.26318507
INFO:root:[   15] Training loss: 0.02251628, Validation loss: 1.25825658, Gradient norm: 0.31466686
INFO:root:[   16] Training loss: 0.02171714, Validation loss: 1.21603617, Gradient norm: 0.29343458
INFO:root:[   17] Training loss: 0.02144627, Validation loss: 1.25920196, Gradient norm: 0.32887807
INFO:root:[   18] Training loss: 0.02100629, Validation loss: 1.25159624, Gradient norm: 0.30814811
INFO:root:[   19] Training loss: 0.02069045, Validation loss: 1.16718758, Gradient norm: 0.33275619
INFO:root:[   20] Training loss: 0.02066040, Validation loss: 1.15864115, Gradient norm: 0.33664270
INFO:root:[   21] Training loss: 0.02005469, Validation loss: 1.18774820, Gradient norm: 0.29230855
INFO:root:[   22] Training loss: 0.02001463, Validation loss: 1.13596342, Gradient norm: 0.33192438
INFO:root:[   23] Training loss: 0.01965243, Validation loss: 1.14001535, Gradient norm: 0.32226530
INFO:root:[   24] Training loss: 0.01951313, Validation loss: 1.16852637, Gradient norm: 0.33079190
INFO:root:[   25] Training loss: 0.01919254, Validation loss: 1.10048175, Gradient norm: 0.33980332
INFO:root:[   26] Training loss: 0.01877458, Validation loss: 1.08403740, Gradient norm: 0.30734460
INFO:root:[   27] Training loss: 0.01885698, Validation loss: 1.08248690, Gradient norm: 0.34651566
INFO:root:[   28] Training loss: 0.01870008, Validation loss: 1.11128344, Gradient norm: 0.38463921
INFO:root:[   29] Training loss: 0.01824218, Validation loss: 1.06253360, Gradient norm: 0.30393485
INFO:root:[   30] Training loss: 0.01847352, Validation loss: 1.06330513, Gradient norm: 0.38085126
INFO:root:[   31] Training loss: 0.01791758, Validation loss: 1.02898814, Gradient norm: 0.31541248
INFO:root:[   32] Training loss: 0.01787665, Validation loss: 1.03429700, Gradient norm: 0.33382123
INFO:root:[   33] Training loss: 0.01800426, Validation loss: 1.02700928, Gradient norm: 0.40719938
INFO:root:[   34] Training loss: 0.01756397, Validation loss: 1.04526152, Gradient norm: 0.34625390
INFO:root:[   35] Training loss: 0.01751330, Validation loss: 1.03196818, Gradient norm: 0.36203801
INFO:root:[   36] Training loss: 0.01712746, Validation loss: 1.02408979, Gradient norm: 0.32337187
INFO:root:[   37] Training loss: 0.01716747, Validation loss: 1.02001785, Gradient norm: 0.33142386
INFO:root:[   38] Training loss: 0.01703406, Validation loss: 0.99960134, Gradient norm: 0.35367036
INFO:root:[   39] Training loss: 0.01657376, Validation loss: 1.13949065, Gradient norm: 0.31225797
INFO:root:[   40] Training loss: 0.01709408, Validation loss: 0.96143233, Gradient norm: 0.40399748
INFO:root:[   41] Training loss: 0.01674705, Validation loss: 0.97307216, Gradient norm: 0.36659928
INFO:root:[   42] Training loss: 0.01655258, Validation loss: 1.09313804, Gradient norm: 0.32273117
INFO:root:[   43] Training loss: 0.01642831, Validation loss: 0.97976372, Gradient norm: 0.33928340
INFO:root:[   44] Training loss: 0.01651082, Validation loss: 0.93232411, Gradient norm: 0.40379129
INFO:root:[   45] Training loss: 0.01610100, Validation loss: 0.98344914, Gradient norm: 0.33909068
INFO:root:[   46] Training loss: 0.01595558, Validation loss: 0.93067817, Gradient norm: 0.31075412
INFO:root:[   47] Training loss: 0.01590134, Validation loss: 0.94864553, Gradient norm: 0.34587404
INFO:root:[   48] Training loss: 0.01632119, Validation loss: 0.97830301, Gradient norm: 0.43379296
INFO:root:[   49] Training loss: 0.01577367, Validation loss: 0.91658573, Gradient norm: 0.31559123
INFO:root:[   50] Training loss: 0.01566629, Validation loss: 0.93402760, Gradient norm: 0.33655840
INFO:root:[   51] Training loss: 0.01572368, Validation loss: 0.91078380, Gradient norm: 0.36479925
INFO:root:[   52] Training loss: 0.01561118, Validation loss: 0.92695561, Gradient norm: 0.36156850
INFO:root:[   53] Training loss: 0.01562055, Validation loss: 0.88866402, Gradient norm: 0.35118748
INFO:root:[   54] Training loss: 0.01556737, Validation loss: 0.95631469, Gradient norm: 0.35836803
INFO:root:[   55] Training loss: 0.01541936, Validation loss: 0.88033667, Gradient norm: 0.35509697
INFO:root:[   56] Training loss: 0.01536725, Validation loss: 0.95821467, Gradient norm: 0.34406734
INFO:root:[   57] Training loss: 0.01532056, Validation loss: 0.95712686, Gradient norm: 0.34282213
INFO:root:[   58] Training loss: 0.01523316, Validation loss: 0.89495320, Gradient norm: 0.37665926
INFO:root:[   59] Training loss: 0.01497459, Validation loss: 0.90059953, Gradient norm: 0.32492329
INFO:root:[   60] Training loss: 0.01495149, Validation loss: 0.93475991, Gradient norm: 0.34771781
INFO:root:[   61] Training loss: 0.01512254, Validation loss: 0.94074424, Gradient norm: 0.37932838
INFO:root:[   62] Training loss: 0.01500917, Validation loss: 0.89935093, Gradient norm: 0.38045973
INFO:root:[   63] Training loss: 0.01474240, Validation loss: 0.97226877, Gradient norm: 0.34063168
INFO:root:[   64] Training loss: 0.01500035, Validation loss: 0.87023131, Gradient norm: 0.37701485
INFO:root:[   65] Training loss: 0.01468056, Validation loss: 0.85866791, Gradient norm: 0.33948158
INFO:root:[   66] Training loss: 0.01456619, Validation loss: 0.88914448, Gradient norm: 0.35026124
INFO:root:[   67] Training loss: 0.01464097, Validation loss: 0.87527516, Gradient norm: 0.34136575
INFO:root:[   68] Training loss: 0.01433795, Validation loss: 0.99193904, Gradient norm: 0.29624005
INFO:root:[   69] Training loss: 0.01427873, Validation loss: 0.87272180, Gradient norm: 0.32687370
INFO:root:[   70] Training loss: 0.01427528, Validation loss: 0.88327548, Gradient norm: 0.33627388
INFO:root:[   71] Training loss: 0.01419522, Validation loss: 0.87369739, Gradient norm: 0.31635137
INFO:root:[   72] Training loss: 0.01436417, Validation loss: 0.86048038, Gradient norm: 0.37536002
INFO:root:[   73] Training loss: 0.01410031, Validation loss: 0.83725281, Gradient norm: 0.31483835
INFO:root:[   74] Training loss: 0.01411596, Validation loss: 0.82939479, Gradient norm: 0.35130998
INFO:root:[   75] Training loss: 0.01408829, Validation loss: 0.85914279, Gradient norm: 0.31029893
INFO:root:[   76] Training loss: 0.01401380, Validation loss: 0.87272469, Gradient norm: 0.36298257
INFO:root:[   77] Training loss: 0.01392203, Validation loss: 0.84765163, Gradient norm: 0.34583729
INFO:root:[   78] Training loss: 0.01371647, Validation loss: 0.89834446, Gradient norm: 0.30882781
INFO:root:[   79] Training loss: 0.01392672, Validation loss: 0.85010949, Gradient norm: 0.33736294
INFO:root:[   80] Training loss: 0.01384908, Validation loss: 0.82262435, Gradient norm: 0.36728482
INFO:root:[   81] Training loss: 0.01359011, Validation loss: 0.84898490, Gradient norm: 0.30431965
INFO:root:[   82] Training loss: 0.01368657, Validation loss: 0.82827064, Gradient norm: 0.33315591
INFO:root:[   83] Training loss: 0.01364975, Validation loss: 0.81037190, Gradient norm: 0.34300635
INFO:root:[   84] Training loss: 0.01357188, Validation loss: 0.84337549, Gradient norm: 0.33437816
INFO:root:[   85] Training loss: 0.01335615, Validation loss: 0.82234439, Gradient norm: 0.32238344
INFO:root:[   86] Training loss: 0.01323792, Validation loss: 0.78621425, Gradient norm: 0.28674104
INFO:root:[   87] Training loss: 0.01338300, Validation loss: 0.79543021, Gradient norm: 0.33066354
INFO:root:[   88] Training loss: 0.01326250, Validation loss: 0.79150294, Gradient norm: 0.32812406
INFO:root:[   89] Training loss: 0.01351728, Validation loss: 0.87730253, Gradient norm: 0.37678350
INFO:root:[   90] Training loss: 0.01313177, Validation loss: 0.78887106, Gradient norm: 0.30600910
INFO:root:[   91] Training loss: 0.01318468, Validation loss: 0.82193049, Gradient norm: 0.32208757
INFO:root:[   92] Training loss: 0.01312944, Validation loss: 0.79018538, Gradient norm: 0.30804109
INFO:root:[   93] Training loss: 0.01337765, Validation loss: 0.78868753, Gradient norm: 0.35043665
INFO:root:[   94] Training loss: 0.01319502, Validation loss: 0.78290004, Gradient norm: 0.34450316
INFO:root:[   95] Training loss: 0.01288912, Validation loss: 0.78436292, Gradient norm: 0.32486230
INFO:root:[   96] Training loss: 0.01289300, Validation loss: 0.78784970, Gradient norm: 0.32243982
INFO:root:[   97] Training loss: 0.01299158, Validation loss: 0.84480350, Gradient norm: 0.30936543
INFO:root:[   98] Training loss: 0.01286063, Validation loss: 0.76604927, Gradient norm: 0.31356943
INFO:root:[   99] Training loss: 0.01268921, Validation loss: 0.77264963, Gradient norm: 0.31269120
INFO:root:[  100] Training loss: 0.01279850, Validation loss: 0.79834515, Gradient norm: 0.32473090
INFO:root:[  101] Training loss: 0.01259784, Validation loss: 0.77797757, Gradient norm: 0.30333278
INFO:root:[  102] Training loss: 0.01279945, Validation loss: 0.77543400, Gradient norm: 0.31459368
INFO:root:[  103] Training loss: 0.01240242, Validation loss: 0.78863061, Gradient norm: 0.29587034
INFO:root:[  104] Training loss: 0.01267078, Validation loss: 0.77281319, Gradient norm: 0.32301394
INFO:root:[  105] Training loss: 0.01280403, Validation loss: 0.76592453, Gradient norm: 0.33726885
INFO:root:[  106] Training loss: 0.01247318, Validation loss: 0.79917710, Gradient norm: 0.31193997
INFO:root:[  107] Training loss: 0.01262276, Validation loss: 0.79819767, Gradient norm: 0.34831251
INFO:root:[  108] Training loss: 0.01235637, Validation loss: 0.80358003, Gradient norm: 0.31322926
INFO:root:[  109] Training loss: 0.01238688, Validation loss: 0.79714138, Gradient norm: 0.32545375
INFO:root:[  110] Training loss: 0.01216379, Validation loss: 0.78541393, Gradient norm: 0.29219990
INFO:root:[  111] Training loss: 0.01236582, Validation loss: 0.76062237, Gradient norm: 0.31146503
INFO:root:[  112] Training loss: 0.01258275, Validation loss: 0.77671361, Gradient norm: 0.34424581
INFO:root:[  113] Training loss: 0.01220506, Validation loss: 0.75915459, Gradient norm: 0.29649603
INFO:root:[  114] Training loss: 0.01212480, Validation loss: 0.75271075, Gradient norm: 0.30231546
INFO:root:[  115] Training loss: 0.01216080, Validation loss: 0.81947786, Gradient norm: 0.31804994
INFO:root:[  116] Training loss: 0.01205481, Validation loss: 0.76588330, Gradient norm: 0.30272432
INFO:root:[  117] Training loss: 0.01195570, Validation loss: 0.75718508, Gradient norm: 0.27384252
INFO:root:[  118] Training loss: 0.01204312, Validation loss: 0.73882942, Gradient norm: 0.30572407
INFO:root:[  119] Training loss: 0.01189918, Validation loss: 0.76957689, Gradient norm: 0.26771149
INFO:root:[  120] Training loss: 0.01209336, Validation loss: 0.75102072, Gradient norm: 0.31168988
INFO:root:[  121] Training loss: 0.01197001, Validation loss: 0.81451493, Gradient norm: 0.31760898
INFO:root:[  122] Training loss: 0.01204934, Validation loss: 0.75781719, Gradient norm: 0.33048485
INFO:root:[  123] Training loss: 0.01176382, Validation loss: 0.74523416, Gradient norm: 0.28479760
INFO:root:[  124] Training loss: 0.01186689, Validation loss: 0.74258848, Gradient norm: 0.30702594
INFO:root:[  125] Training loss: 0.01186400, Validation loss: 0.72528788, Gradient norm: 0.31559544
INFO:root:[  126] Training loss: 0.01165927, Validation loss: 0.81059129, Gradient norm: 0.26961519
INFO:root:[  127] Training loss: 0.01184795, Validation loss: 0.74166581, Gradient norm: 0.32942258
INFO:root:[  128] Training loss: 0.01162640, Validation loss: 0.79008877, Gradient norm: 0.28342882
INFO:root:[  129] Training loss: 0.01162622, Validation loss: 0.74605618, Gradient norm: 0.29209011
INFO:root:[  130] Training loss: 0.01153636, Validation loss: 0.74941543, Gradient norm: 0.28415959
INFO:root:[  131] Training loss: 0.01148262, Validation loss: 0.74798586, Gradient norm: 0.27622607
INFO:root:[  132] Training loss: 0.01149484, Validation loss: 0.75306737, Gradient norm: 0.31192326
INFO:root:[  133] Training loss: 0.01147776, Validation loss: 0.74715579, Gradient norm: 0.27134351
INFO:root:[  134] Training loss: 0.01173242, Validation loss: 0.73879655, Gradient norm: 0.33902772
INFO:root:EP 134: Early stopping
INFO:root:Training the model took 667.009s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05384
INFO:root:EnergyScoretrain: 0.04046
INFO:root:Coveragetrain: 6.73174
INFO:root:IntervalWidthtrain: 92.69126
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.0179
INFO:root:EnergyScorevalidation: 0.01364
INFO:root:Coveragevalidation: 1.74488
INFO:root:IntervalWidthvalidation: 23.99574
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02223
INFO:root:EnergyScoretest: 0.01611
INFO:root:Coveragetest: 0.94271
INFO:root:IntervalWidthtest: 50.2519
INFO:root:###45 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.01, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06801299, Validation loss: 2.46950047, Gradient norm: 0.33403636
INFO:root:[    2] Training loss: 0.03676245, Validation loss: 1.81611192, Gradient norm: 0.26983572
INFO:root:[    3] Training loss: 0.02984516, Validation loss: 1.60044229, Gradient norm: 0.36105417
INFO:root:[    4] Training loss: 0.02606319, Validation loss: 1.42867656, Gradient norm: 0.29967104
INFO:root:[    5] Training loss: 0.02442992, Validation loss: 1.29221179, Gradient norm: 0.39568700
INFO:root:[    6] Training loss: 0.02193374, Validation loss: 1.22300636, Gradient norm: 0.31468880
INFO:root:[    7] Training loss: 0.02108748, Validation loss: 1.21236338, Gradient norm: 0.37525448
INFO:root:[    8] Training loss: 0.02018898, Validation loss: 1.10784239, Gradient norm: 0.43164151
INFO:root:[    9] Training loss: 0.01869810, Validation loss: 1.04493122, Gradient norm: 0.36075139
INFO:root:[   10] Training loss: 0.01811192, Validation loss: 1.03769004, Gradient norm: 0.37620601
INFO:root:[   11] Training loss: 0.01719658, Validation loss: 0.98440433, Gradient norm: 0.31920641
INFO:root:[   12] Training loss: 0.01676138, Validation loss: 0.94473148, Gradient norm: 0.33542719
INFO:root:[   13] Training loss: 0.01635875, Validation loss: 0.93661093, Gradient norm: 0.33819711
INFO:root:[   14] Training loss: 0.01602037, Validation loss: 0.89477145, Gradient norm: 0.35393671
INFO:root:[   15] Training loss: 0.01565693, Validation loss: 0.88738766, Gradient norm: 0.31601285
INFO:root:[   16] Training loss: 0.01567795, Validation loss: 0.87859083, Gradient norm: 0.41742595
INFO:root:[   17] Training loss: 0.01507839, Validation loss: 0.85538885, Gradient norm: 0.34745346
INFO:root:[   18] Training loss: 0.01487109, Validation loss: 0.84234094, Gradient norm: 0.37850640
INFO:root:[   19] Training loss: 0.01493595, Validation loss: 0.89112415, Gradient norm: 0.39176204
INFO:root:[   20] Training loss: 0.01440319, Validation loss: 0.84364218, Gradient norm: 0.35819680
INFO:root:[   21] Training loss: 0.01426951, Validation loss: 0.81596777, Gradient norm: 0.37005458
INFO:root:[   22] Training loss: 0.01400998, Validation loss: 0.81311362, Gradient norm: 0.34730591
INFO:root:[   23] Training loss: 0.01433454, Validation loss: 0.86651483, Gradient norm: 0.43109509
INFO:root:[   24] Training loss: 0.01383995, Validation loss: 0.86445124, Gradient norm: 0.37809685
INFO:root:[   25] Training loss: 0.01371276, Validation loss: 0.78076755, Gradient norm: 0.38311387
INFO:root:[   26] Training loss: 0.01367830, Validation loss: 0.81066177, Gradient norm: 0.37914057
INFO:root:[   27] Training loss: 0.01346305, Validation loss: 0.78625875, Gradient norm: 0.38831168
INFO:root:[   28] Training loss: 0.01309969, Validation loss: 0.76999251, Gradient norm: 0.35102132
INFO:root:[   29] Training loss: 0.01326596, Validation loss: 0.77875286, Gradient norm: 0.40534944
INFO:root:[   30] Training loss: 0.01301855, Validation loss: 0.75460492, Gradient norm: 0.40851491
INFO:root:[   31] Training loss: 0.01286224, Validation loss: 0.77021080, Gradient norm: 0.35976072
INFO:root:[   32] Training loss: 0.01277776, Validation loss: 0.81270112, Gradient norm: 0.37641583
INFO:root:[   33] Training loss: 0.01330577, Validation loss: 0.77114482, Gradient norm: 0.46345770
INFO:root:[   34] Training loss: 0.01262013, Validation loss: 0.82950182, Gradient norm: 0.37978956
INFO:root:[   35] Training loss: 0.01235407, Validation loss: 0.73401671, Gradient norm: 0.34812131
INFO:root:[   36] Training loss: 0.01263619, Validation loss: 0.73568469, Gradient norm: 0.43788150
INFO:root:[   37] Training loss: 0.01231487, Validation loss: 0.77364013, Gradient norm: 0.36345943
INFO:root:[   38] Training loss: 0.01261739, Validation loss: 0.73557052, Gradient norm: 0.43996469
INFO:root:[   39] Training loss: 0.01199748, Validation loss: 0.72635805, Gradient norm: 0.34342986
INFO:root:[   40] Training loss: 0.01176840, Validation loss: 0.71235784, Gradient norm: 0.29595046
INFO:root:[   41] Training loss: 0.01220264, Validation loss: 0.71513723, Gradient norm: 0.41644799
INFO:root:[   42] Training loss: 0.01196966, Validation loss: 0.73159452, Gradient norm: 0.37216077
INFO:root:[   43] Training loss: 0.01208732, Validation loss: 0.75144502, Gradient norm: 0.42298104
INFO:root:[   44] Training loss: 0.01179085, Validation loss: 0.70414612, Gradient norm: 0.38367251
INFO:root:[   45] Training loss: 0.01190451, Validation loss: 0.73179750, Gradient norm: 0.41091148
INFO:root:[   46] Training loss: 0.01148218, Validation loss: 0.72040274, Gradient norm: 0.34595057
INFO:root:[   47] Training loss: 0.01172474, Validation loss: 0.76119634, Gradient norm: 0.38435927
INFO:root:[   48] Training loss: 0.01141622, Validation loss: 0.73984272, Gradient norm: 0.33939886
INFO:root:[   49] Training loss: 0.01194064, Validation loss: 0.76213401, Gradient norm: 0.42132784
INFO:root:[   50] Training loss: 0.01170401, Validation loss: 0.71243037, Gradient norm: 0.39910231
INFO:root:[   51] Training loss: 0.01170524, Validation loss: 0.69775779, Gradient norm: 0.43020373
INFO:root:[   52] Training loss: 0.01137813, Validation loss: 0.69573827, Gradient norm: 0.36816943
INFO:root:[   53] Training loss: 0.01140561, Validation loss: 0.69480563, Gradient norm: 0.38694428
INFO:root:[   54] Training loss: 0.01148987, Validation loss: 0.76277881, Gradient norm: 0.40787342
INFO:root:[   55] Training loss: 0.01115593, Validation loss: 0.68867507, Gradient norm: 0.37019713
INFO:root:[   56] Training loss: 0.01124657, Validation loss: 0.84887497, Gradient norm: 0.38535289
INFO:root:[   57] Training loss: 0.01129157, Validation loss: 0.73455555, Gradient norm: 0.40600281
INFO:root:[   58] Training loss: 0.01119066, Validation loss: 0.70460608, Gradient norm: 0.39061713
INFO:root:[   59] Training loss: 0.01083945, Validation loss: 0.72595954, Gradient norm: 0.32498699
INFO:root:[   60] Training loss: 0.01092790, Validation loss: 0.73186476, Gradient norm: 0.34941276
INFO:root:[   61] Training loss: 0.01104795, Validation loss: 0.70516052, Gradient norm: 0.39707969
INFO:root:[   62] Training loss: 0.01088647, Validation loss: 0.69913386, Gradient norm: 0.35369547
INFO:root:[   63] Training loss: 0.01072964, Validation loss: 0.72508989, Gradient norm: 0.34744991
INFO:root:[   64] Training loss: 0.01102649, Validation loss: 0.70193556, Gradient norm: 0.39303956
INFO:root:[   65] Training loss: 0.01087239, Validation loss: 0.68668555, Gradient norm: 0.35957461
INFO:root:[   66] Training loss: 0.01105425, Validation loss: 0.71206577, Gradient norm: 0.44318411
INFO:root:[   67] Training loss: 0.01051487, Validation loss: 0.70069671, Gradient norm: 0.33236203
INFO:root:[   68] Training loss: 0.01080767, Validation loss: 0.86667215, Gradient norm: 0.39925992
INFO:root:[   69] Training loss: 0.01066989, Validation loss: 0.68653168, Gradient norm: 0.37944414
INFO:root:[   70] Training loss: 0.01036537, Validation loss: 0.70430423, Gradient norm: 0.32046295
INFO:root:[   71] Training loss: 0.01051243, Validation loss: 0.68290357, Gradient norm: 0.37281018
INFO:root:[   72] Training loss: 0.01046947, Validation loss: 0.70237984, Gradient norm: 0.35311363
INFO:root:[   73] Training loss: 0.01038447, Validation loss: 0.69235242, Gradient norm: 0.36019267
INFO:root:[   74] Training loss: 0.01053640, Validation loss: 0.75036133, Gradient norm: 0.37978202
INFO:root:[   75] Training loss: 0.01031997, Validation loss: 0.68037076, Gradient norm: 0.35925037
INFO:root:[   76] Training loss: 0.01039188, Validation loss: 0.70317486, Gradient norm: 0.38033995
INFO:root:[   77] Training loss: 0.01021011, Validation loss: 0.66647440, Gradient norm: 0.36220931
INFO:root:[   78] Training loss: 0.01014183, Validation loss: 0.68710728, Gradient norm: 0.34800769
INFO:root:[   79] Training loss: 0.00999229, Validation loss: 0.67007891, Gradient norm: 0.32260639
INFO:root:[   80] Training loss: 0.00996433, Validation loss: 0.67553409, Gradient norm: 0.30706480
INFO:root:[   81] Training loss: 0.01006593, Validation loss: 0.67500387, Gradient norm: 0.34294699
INFO:root:[   82] Training loss: 0.01011169, Validation loss: 0.67326930, Gradient norm: 0.35342708
INFO:root:[   83] Training loss: 0.01019823, Validation loss: 0.74469206, Gradient norm: 0.40537280
INFO:root:[   84] Training loss: 0.01011602, Validation loss: 0.66816464, Gradient norm: 0.35230546
INFO:root:[   85] Training loss: 0.00971067, Validation loss: 0.68810425, Gradient norm: 0.28801668
INFO:root:[   86] Training loss: 0.00975841, Validation loss: 0.69894113, Gradient norm: 0.32022955
INFO:root:EP 86: Early stopping
INFO:root:Training the model took 429.688s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05359
INFO:root:EnergyScoretrain: 0.03999
INFO:root:Coveragetrain: 6.26921
INFO:root:IntervalWidthtrain: 68.26836
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01857
INFO:root:EnergyScorevalidation: 0.01437
INFO:root:Coveragevalidation: 1.56195
INFO:root:IntervalWidthvalidation: 17.76946
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02955
INFO:root:EnergyScoretest: 0.02555
INFO:root:Coveragetest: 0.32136
INFO:root:IntervalWidthtest: 20.70409
INFO:root:###46 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.05, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07554287, Validation loss: 2.64898625, Gradient norm: 0.32870932
INFO:root:[    2] Training loss: 0.04145797, Validation loss: 2.09845128, Gradient norm: 0.23177960
INFO:root:[    3] Training loss: 0.03450925, Validation loss: 1.82884071, Gradient norm: 0.30926039
INFO:root:[    4] Training loss: 0.03027023, Validation loss: 1.69642533, Gradient norm: 0.28473012
INFO:root:[    5] Training loss: 0.02775769, Validation loss: 1.56072843, Gradient norm: 0.29561494
INFO:root:[    6] Training loss: 0.02557480, Validation loss: 1.43585750, Gradient norm: 0.25078216
INFO:root:[    7] Training loss: 0.02453927, Validation loss: 1.34813986, Gradient norm: 0.32698598
INFO:root:[    8] Training loss: 0.02315196, Validation loss: 1.31156236, Gradient norm: 0.27619288
INFO:root:[    9] Training loss: 0.02202002, Validation loss: 1.34583734, Gradient norm: 0.28787117
INFO:root:[   10] Training loss: 0.02141751, Validation loss: 1.28677829, Gradient norm: 0.33136302
INFO:root:[   11] Training loss: 0.02079520, Validation loss: 1.20445908, Gradient norm: 0.31370834
INFO:root:[   12] Training loss: 0.02014553, Validation loss: 1.16386598, Gradient norm: 0.31745442
INFO:root:[   13] Training loss: 0.02003497, Validation loss: 1.13405509, Gradient norm: 0.39135335
INFO:root:[   14] Training loss: 0.01919728, Validation loss: 1.06717099, Gradient norm: 0.33189861
INFO:root:[   15] Training loss: 0.01903644, Validation loss: 1.07160751, Gradient norm: 0.35394077
INFO:root:[   16] Training loss: 0.01806994, Validation loss: 1.08495354, Gradient norm: 0.28529561
INFO:root:[   17] Training loss: 0.01800059, Validation loss: 1.15337247, Gradient norm: 0.32594378
INFO:root:[   18] Training loss: 0.01773413, Validation loss: 1.03241572, Gradient norm: 0.32543511
INFO:root:[   19] Training loss: 0.01732820, Validation loss: 1.04453644, Gradient norm: 0.31030335
INFO:root:[   20] Training loss: 0.01730501, Validation loss: 0.98939149, Gradient norm: 0.35095930
INFO:root:[   21] Training loss: 0.01691817, Validation loss: 0.95276784, Gradient norm: 0.34759726
INFO:root:[   22] Training loss: 0.01654168, Validation loss: 1.06161082, Gradient norm: 0.31491062
INFO:root:[   23] Training loss: 0.01669261, Validation loss: 1.05780009, Gradient norm: 0.39034160
INFO:root:[   24] Training loss: 0.01629650, Validation loss: 1.08375194, Gradient norm: 0.34969099
INFO:root:[   25] Training loss: 0.01607289, Validation loss: 1.01179797, Gradient norm: 0.35343803
INFO:root:[   26] Training loss: 0.01593155, Validation loss: 0.93449520, Gradient norm: 0.34357642
INFO:root:[   27] Training loss: 0.01608983, Validation loss: 0.95263563, Gradient norm: 0.40733053
INFO:root:[   28] Training loss: 0.01565465, Validation loss: 0.90454649, Gradient norm: 0.35462150
INFO:root:[   29] Training loss: 0.01543557, Validation loss: 0.89447626, Gradient norm: 0.30501791
INFO:root:[   30] Training loss: 0.01558922, Validation loss: 0.91673304, Gradient norm: 0.39757516
INFO:root:[   31] Training loss: 0.01543454, Validation loss: 0.89326405, Gradient norm: 0.38443995
INFO:root:[   32] Training loss: 0.01536357, Validation loss: 0.87783851, Gradient norm: 0.40779579
INFO:root:[   33] Training loss: 0.01499846, Validation loss: 0.88412705, Gradient norm: 0.34042685
INFO:root:[   34] Training loss: 0.01475763, Validation loss: 0.86485299, Gradient norm: 0.34823550
INFO:root:[   35] Training loss: 0.01468683, Validation loss: 0.98288772, Gradient norm: 0.32129040
INFO:root:[   36] Training loss: 0.01493010, Validation loss: 0.95258062, Gradient norm: 0.38245983
INFO:root:[   37] Training loss: 0.01474516, Validation loss: 0.85168895, Gradient norm: 0.39131200
INFO:root:[   38] Training loss: 0.01468654, Validation loss: 0.85995300, Gradient norm: 0.40882696
INFO:root:[   39] Training loss: 0.01434457, Validation loss: 0.87718100, Gradient norm: 0.36280082
INFO:root:[   40] Training loss: 0.01427164, Validation loss: 1.01582860, Gradient norm: 0.38330093
INFO:root:[   41] Training loss: 0.01418129, Validation loss: 0.90846593, Gradient norm: 0.36541191
INFO:root:[   42] Training loss: 0.01436532, Validation loss: 0.83981221, Gradient norm: 0.43269848
INFO:root:[   43] Training loss: 0.01387835, Validation loss: 0.83318429, Gradient norm: 0.33925664
INFO:root:[   44] Training loss: 0.01362974, Validation loss: 0.84460273, Gradient norm: 0.32649703
INFO:root:[   45] Training loss: 0.01421031, Validation loss: 0.84137376, Gradient norm: 0.45732778
INFO:root:[   46] Training loss: 0.01407633, Validation loss: 0.81649454, Gradient norm: 0.41854555
INFO:root:[   47] Training loss: 0.01360462, Validation loss: 1.06624655, Gradient norm: 0.34996339
INFO:root:[   48] Training loss: 0.01377394, Validation loss: 0.81748373, Gradient norm: 0.38819694
INFO:root:[   49] Training loss: 0.01349195, Validation loss: 0.79793576, Gradient norm: 0.34505668
INFO:root:[   50] Training loss: 0.01360641, Validation loss: 0.80559577, Gradient norm: 0.42060352
INFO:root:[   51] Training loss: 0.01339429, Validation loss: 0.80137991, Gradient norm: 0.38849304
INFO:root:[   52] Training loss: 0.01326463, Validation loss: 0.78857953, Gradient norm: 0.35842926
INFO:root:[   53] Training loss: 0.01330249, Validation loss: 0.82003511, Gradient norm: 0.37313537
INFO:root:[   54] Training loss: 0.01329360, Validation loss: 0.84548953, Gradient norm: 0.39659513
INFO:root:[   55] Training loss: 0.01309343, Validation loss: 0.80147915, Gradient norm: 0.38014456
INFO:root:[   56] Training loss: 0.01311922, Validation loss: 0.81556419, Gradient norm: 0.39408091
INFO:root:[   57] Training loss: 0.01315072, Validation loss: 0.77989904, Gradient norm: 0.40016723
INFO:root:[   58] Training loss: 0.01308782, Validation loss: 0.78377087, Gradient norm: 0.37982609
INFO:root:[   59] Training loss: 0.01273535, Validation loss: 0.79182880, Gradient norm: 0.34474866
INFO:root:[   60] Training loss: 0.01274816, Validation loss: 0.79931497, Gradient norm: 0.35848450
INFO:root:[   61] Training loss: 0.01312379, Validation loss: 0.78172070, Gradient norm: 0.43434765
INFO:root:[   62] Training loss: 0.01293946, Validation loss: 0.78325294, Gradient norm: 0.41081170
INFO:root:[   63] Training loss: 0.01250608, Validation loss: 0.80479542, Gradient norm: 0.30880008
INFO:root:[   64] Training loss: 0.01272100, Validation loss: 0.80089234, Gradient norm: 0.37650873
INFO:root:[   65] Training loss: 0.01263811, Validation loss: 0.78058823, Gradient norm: 0.38459497
INFO:root:[   66] Training loss: 0.01225529, Validation loss: 0.81982762, Gradient norm: 0.31749491
INFO:root:[   67] Training loss: 0.01236086, Validation loss: 0.74750989, Gradient norm: 0.34579793
INFO:root:[   68] Training loss: 0.01228679, Validation loss: 0.76598148, Gradient norm: 0.35619383
INFO:root:[   69] Training loss: 0.01211020, Validation loss: 0.76021633, Gradient norm: 0.31644251
INFO:root:[   70] Training loss: 0.01222013, Validation loss: 0.79053430, Gradient norm: 0.35196020
INFO:root:[   71] Training loss: 0.01253290, Validation loss: 0.77843083, Gradient norm: 0.42751952
INFO:root:[   72] Training loss: 0.01241717, Validation loss: 0.76466829, Gradient norm: 0.42398898
INFO:root:[   73] Training loss: 0.01188535, Validation loss: 0.75697475, Gradient norm: 0.29776922
INFO:root:[   74] Training loss: 0.01217822, Validation loss: 0.78041873, Gradient norm: 0.38151962
INFO:root:[   75] Training loss: 0.01196011, Validation loss: 0.81200574, Gradient norm: 0.34960767
INFO:root:[   76] Training loss: 0.01198424, Validation loss: 0.90684239, Gradient norm: 0.36228623
INFO:root:EP 76: Early stopping
INFO:root:Training the model took 380.265s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.05902
INFO:root:EnergyScoretrain: 0.04383
INFO:root:Coveragetrain: 6.57707
INFO:root:IntervalWidthtrain: 92.77714
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01865
INFO:root:EnergyScorevalidation: 0.01408
INFO:root:Coveragevalidation: 1.69957
INFO:root:IntervalWidthvalidation: 24.04529
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01775
INFO:root:EnergyScoretest: 0.01348
INFO:root:Coveragetest: 0.76752
INFO:root:IntervalWidthtest: 29.14764
INFO:root:###47 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.1, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06657439, Validation loss: 2.59014570, Gradient norm: 0.25930860
INFO:root:[    2] Training loss: 0.04100345, Validation loss: 2.11764859, Gradient norm: 0.26449707
INFO:root:[    3] Training loss: 0.03455204, Validation loss: 1.88030942, Gradient norm: 0.27364568
INFO:root:[    4] Training loss: 0.03112236, Validation loss: 1.70319289, Gradient norm: 0.30660136
INFO:root:[    5] Training loss: 0.02835831, Validation loss: 1.59389210, Gradient norm: 0.24292751
INFO:root:[    6] Training loss: 0.02691620, Validation loss: 1.55661520, Gradient norm: 0.31615037
INFO:root:[    7] Training loss: 0.02506479, Validation loss: 1.41913173, Gradient norm: 0.25467053
INFO:root:[    8] Training loss: 0.02388254, Validation loss: 1.36715949, Gradient norm: 0.28180695
INFO:root:[    9] Training loss: 0.02293218, Validation loss: 1.26352262, Gradient norm: 0.29748800
INFO:root:[   10] Training loss: 0.02186262, Validation loss: 1.22978491, Gradient norm: 0.29921367
INFO:root:[   11] Training loss: 0.02143036, Validation loss: 1.27159716, Gradient norm: 0.31600610
INFO:root:[   12] Training loss: 0.02072452, Validation loss: 1.17325315, Gradient norm: 0.29899667
INFO:root:[   13] Training loss: 0.02021545, Validation loss: 1.30218696, Gradient norm: 0.30629798
INFO:root:[   14] Training loss: 0.01964218, Validation loss: 1.12508054, Gradient norm: 0.29434259
INFO:root:[   15] Training loss: 0.01936910, Validation loss: 1.06087935, Gradient norm: 0.34362378
INFO:root:[   16] Training loss: 0.01871183, Validation loss: 1.05502855, Gradient norm: 0.30065926
INFO:root:[   17] Training loss: 0.01872003, Validation loss: 1.39186695, Gradient norm: 0.35154529
INFO:root:[   18] Training loss: 0.01822637, Validation loss: 1.23539858, Gradient norm: 0.32696637
INFO:root:[   19] Training loss: 0.01828872, Validation loss: 1.03710164, Gradient norm: 0.39467202
INFO:root:[   20] Training loss: 0.01767447, Validation loss: 1.03609078, Gradient norm: 0.35505559
INFO:root:[   21] Training loss: 0.01738875, Validation loss: 0.98476374, Gradient norm: 0.33780316
INFO:root:[   22] Training loss: 0.01739225, Validation loss: 1.00144804, Gradient norm: 0.41366251
INFO:root:[   23] Training loss: 0.01698098, Validation loss: 1.03168643, Gradient norm: 0.34654317
INFO:root:[   24] Training loss: 0.01686966, Validation loss: 0.94231708, Gradient norm: 0.38089413
INFO:root:[   25] Training loss: 0.01658264, Validation loss: 0.96547109, Gradient norm: 0.37189536
INFO:root:[   26] Training loss: 0.01659512, Validation loss: 0.93463347, Gradient norm: 0.42491314
INFO:root:[   27] Training loss: 0.01647137, Validation loss: 0.91949490, Gradient norm: 0.40018187
INFO:root:[   28] Training loss: 0.01619056, Validation loss: 0.99002266, Gradient norm: 0.38381162
INFO:root:[   29] Training loss: 0.01590802, Validation loss: 0.97647805, Gradient norm: 0.35454313
INFO:root:[   30] Training loss: 0.01591220, Validation loss: 0.89831889, Gradient norm: 0.38205504
INFO:root:[   31] Training loss: 0.01568872, Validation loss: 0.90115219, Gradient norm: 0.35627196
INFO:root:[   32] Training loss: 0.01552826, Validation loss: 0.91826747, Gradient norm: 0.37403894
INFO:root:[   33] Training loss: 0.01531741, Validation loss: 1.00568071, Gradient norm: 0.35778083
INFO:root:[   34] Training loss: 0.01521412, Validation loss: 0.87267883, Gradient norm: 0.37767957
INFO:root:[   35] Training loss: 0.01512938, Validation loss: 0.91396859, Gradient norm: 0.34711714
INFO:root:[   36] Training loss: 0.01497694, Validation loss: 0.89007231, Gradient norm: 0.39041593
INFO:root:[   37] Training loss: 0.01513767, Validation loss: 0.90317719, Gradient norm: 0.41564444
INFO:root:[   38] Training loss: 0.01488803, Validation loss: 0.86640263, Gradient norm: 0.39430090
INFO:root:[   39] Training loss: 0.01487422, Validation loss: 0.88864678, Gradient norm: 0.38950207
INFO:root:[   40] Training loss: 0.01487949, Validation loss: 0.85499269, Gradient norm: 0.39360630
INFO:root:[   41] Training loss: 0.01432445, Validation loss: 0.91708596, Gradient norm: 0.31792478
INFO:root:[   42] Training loss: 0.01470524, Validation loss: 0.93247513, Gradient norm: 0.43537611
INFO:root:[   43] Training loss: 0.01411012, Validation loss: 0.91924513, Gradient norm: 0.33965484
INFO:root:[   44] Training loss: 0.01454817, Validation loss: 0.82792632, Gradient norm: 0.42463316
INFO:root:[   45] Training loss: 0.01403540, Validation loss: 0.86806385, Gradient norm: 0.35346355
INFO:root:[   46] Training loss: 0.01429507, Validation loss: 0.83460750, Gradient norm: 0.41072229
INFO:root:[   47] Training loss: 0.01412847, Validation loss: 0.85598050, Gradient norm: 0.37927974
INFO:root:[   48] Training loss: 0.01374961, Validation loss: 0.86193417, Gradient norm: 0.31171392
INFO:root:[   49] Training loss: 0.01409804, Validation loss: 0.83632144, Gradient norm: 0.40951954
INFO:root:[   50] Training loss: 0.01389812, Validation loss: 0.86194297, Gradient norm: 0.38764242
INFO:root:[   51] Training loss: 0.01404943, Validation loss: 0.82611881, Gradient norm: 0.42552789
INFO:root:[   52] Training loss: 0.01371646, Validation loss: 0.83053264, Gradient norm: 0.38434741
INFO:root:[   53] Training loss: 0.01377990, Validation loss: 0.87029745, Gradient norm: 0.38000550
INFO:root:[   54] Training loss: 0.01382301, Validation loss: 0.84297192, Gradient norm: 0.39113214
INFO:root:[   55] Training loss: 0.01355527, Validation loss: 0.83270764, Gradient norm: 0.38774772
INFO:root:[   56] Training loss: 0.01348005, Validation loss: 0.81196353, Gradient norm: 0.36677621
INFO:root:[   57] Training loss: 0.01336841, Validation loss: 0.80574891, Gradient norm: 0.32933103
INFO:root:[   58] Training loss: 0.01338242, Validation loss: 0.79563068, Gradient norm: 0.35272740
INFO:root:[   59] Training loss: 0.01313132, Validation loss: 0.83156725, Gradient norm: 0.31598718
INFO:root:[   60] Training loss: 0.01331954, Validation loss: 0.80484028, Gradient norm: 0.39482940
INFO:root:[   61] Training loss: 0.01313708, Validation loss: 0.85677662, Gradient norm: 0.32273389
INFO:root:[   62] Training loss: 0.01316980, Validation loss: 0.78481053, Gradient norm: 0.37226144
INFO:root:[   63] Training loss: 0.01314990, Validation loss: 0.80229097, Gradient norm: 0.38670876
INFO:root:[   64] Training loss: 0.01309428, Validation loss: 0.81796378, Gradient norm: 0.37939920
INFO:root:[   65] Training loss: 0.01296138, Validation loss: 0.80584310, Gradient norm: 0.37807723
INFO:root:[   66] Training loss: 0.01291531, Validation loss: 0.78439998, Gradient norm: 0.37471666
INFO:root:[   67] Training loss: 0.01274203, Validation loss: 0.79107497, Gradient norm: 0.32050093
INFO:root:[   68] Training loss: 0.01292963, Validation loss: 1.09198610, Gradient norm: 0.38891070
INFO:root:[   69] Training loss: 0.01303326, Validation loss: 0.77828613, Gradient norm: 0.37575262
INFO:root:[   70] Training loss: 0.01262274, Validation loss: 0.78085798, Gradient norm: 0.34731729
INFO:root:[   71] Training loss: 0.01245864, Validation loss: 0.76746333, Gradient norm: 0.29431834
INFO:root:[   72] Training loss: 0.01264675, Validation loss: 0.85019347, Gradient norm: 0.34998849
INFO:root:[   73] Training loss: 0.01260898, Validation loss: 0.84958603, Gradient norm: 0.36385869
INFO:root:[   74] Training loss: 0.01230960, Validation loss: 0.77014378, Gradient norm: 0.31997833
INFO:root:[   75] Training loss: 0.01245705, Validation loss: 0.78910558, Gradient norm: 0.35577673
INFO:root:[   76] Training loss: 0.01227146, Validation loss: 0.76401313, Gradient norm: 0.31616421
INFO:root:[   77] Training loss: 0.01244451, Validation loss: 0.77767447, Gradient norm: 0.36573844
INFO:root:[   78] Training loss: 0.01245327, Validation loss: 0.75969663, Gradient norm: 0.36142004
INFO:root:[   79] Training loss: 0.01231758, Validation loss: 0.77454222, Gradient norm: 0.37083875
INFO:root:[   80] Training loss: 0.01214646, Validation loss: 0.78796023, Gradient norm: 0.31509493
INFO:root:[   81] Training loss: 0.01236486, Validation loss: 0.79736402, Gradient norm: 0.37555102
INFO:root:[   82] Training loss: 0.01233509, Validation loss: 0.76283831, Gradient norm: 0.34237729
INFO:root:[   83] Training loss: 0.01206932, Validation loss: 0.88600879, Gradient norm: 0.31747418
INFO:root:[   84] Training loss: 0.01217052, Validation loss: 0.75897557, Gradient norm: 0.32977620
INFO:root:[   85] Training loss: 0.01201059, Validation loss: 0.78138501, Gradient norm: 0.32791518
INFO:root:[   86] Training loss: 0.01205790, Validation loss: 0.80209672, Gradient norm: 0.35780505
INFO:root:[   87] Training loss: 0.01173809, Validation loss: 0.74494937, Gradient norm: 0.28868063
INFO:root:[   88] Training loss: 0.01191482, Validation loss: 0.73454511, Gradient norm: 0.33279995
INFO:root:[   89] Training loss: 0.01193444, Validation loss: 0.75034943, Gradient norm: 0.35985365
INFO:root:[   90] Training loss: 0.01172681, Validation loss: 0.73584735, Gradient norm: 0.29153333
INFO:root:[   91] Training loss: 0.01177070, Validation loss: 0.75038751, Gradient norm: 0.32851359
INFO:root:[   92] Training loss: 0.01157864, Validation loss: 0.78093133, Gradient norm: 0.29188429
INFO:root:[   93] Training loss: 0.01164564, Validation loss: 0.74825596, Gradient norm: 0.33008209
INFO:root:[   94] Training loss: 0.01150068, Validation loss: 0.76171309, Gradient norm: 0.31068189
INFO:root:[   95] Training loss: 0.01156546, Validation loss: 0.75377883, Gradient norm: 0.31174505
INFO:root:[   96] Training loss: 0.01149612, Validation loss: 0.76775666, Gradient norm: 0.31055209
INFO:root:[   97] Training loss: 0.01149827, Validation loss: 0.75687274, Gradient norm: 0.33081895
INFO:root:[   98] Training loss: 0.01150183, Validation loss: 0.72817285, Gradient norm: 0.36796098
INFO:root:[   99] Training loss: 0.01138743, Validation loss: 0.73287494, Gradient norm: 0.31957016
INFO:root:[  100] Training loss: 0.01132642, Validation loss: 0.77359588, Gradient norm: 0.29382513
INFO:root:[  101] Training loss: 0.01157626, Validation loss: 0.73756133, Gradient norm: 0.36911589
INFO:root:[  102] Training loss: 0.01141008, Validation loss: 0.78303551, Gradient norm: 0.35614448
INFO:root:[  103] Training loss: 0.01127841, Validation loss: 0.83492553, Gradient norm: 0.33555069
INFO:root:[  104] Training loss: 0.01133663, Validation loss: 0.72818272, Gradient norm: 0.33827508
INFO:root:[  105] Training loss: 0.01098855, Validation loss: 0.74322699, Gradient norm: 0.26080103
INFO:root:[  106] Training loss: 0.01141945, Validation loss: 0.75971123, Gradient norm: 0.35125474
INFO:root:[  107] Training loss: 0.01111568, Validation loss: 0.73581342, Gradient norm: 0.30186918
INFO:root:[  108] Training loss: 0.01111403, Validation loss: 0.72568196, Gradient norm: 0.31343219
INFO:root:[  109] Training loss: 0.01095929, Validation loss: 0.76857586, Gradient norm: 0.29764322
INFO:root:[  110] Training loss: 0.01097652, Validation loss: 0.73632997, Gradient norm: 0.29117810
INFO:root:[  111] Training loss: 0.01097373, Validation loss: 0.74814101, Gradient norm: 0.32097254
INFO:root:[  112] Training loss: 0.01131027, Validation loss: 0.77910841, Gradient norm: 0.37198347
INFO:root:[  113] Training loss: 0.01090273, Validation loss: 0.74279430, Gradient norm: 0.30898915
INFO:root:[  114] Training loss: 0.01097696, Validation loss: 0.72026485, Gradient norm: 0.30564905
INFO:root:[  115] Training loss: 0.01097777, Validation loss: 0.71468595, Gradient norm: 0.33119187
INFO:root:[  116] Training loss: 0.01101657, Validation loss: 0.74925879, Gradient norm: 0.34694791
INFO:root:[  117] Training loss: 0.01085188, Validation loss: 0.73187572, Gradient norm: 0.31207484
INFO:root:[  118] Training loss: 0.01067103, Validation loss: 0.74255240, Gradient norm: 0.28970859
INFO:root:[  119] Training loss: 0.01077373, Validation loss: 0.74276322, Gradient norm: 0.33555924
INFO:root:[  120] Training loss: 0.01084558, Validation loss: 0.80192464, Gradient norm: 0.33578171
INFO:root:[  121] Training loss: 0.01069968, Validation loss: 0.76574262, Gradient norm: 0.33425609
INFO:root:[  122] Training loss: 0.01061482, Validation loss: 0.74518850, Gradient norm: 0.30604151
INFO:root:[  123] Training loss: 0.01062255, Validation loss: 0.74220612, Gradient norm: 0.29812488
INFO:root:[  124] Training loss: 0.01052634, Validation loss: 0.73238873, Gradient norm: 0.30734949
INFO:root:EP 124: Early stopping
INFO:root:Training the model took 617.504s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.04822
INFO:root:EnergyScoretrain: 0.03603
INFO:root:Coveragetrain: 6.75944
INFO:root:IntervalWidthtrain: 87.83022
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.01777
INFO:root:EnergyScorevalidation: 0.01367
INFO:root:Coveragevalidation: 1.71885
INFO:root:IntervalWidthvalidation: 22.96589
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.02148
INFO:root:EnergyScoretest: 0.01644
INFO:root:Coveragetest: 0.7953
INFO:root:IntervalWidthtest: 32.85742
INFO:root:###48 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.2, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06990577, Validation loss: 2.88208815, Gradient norm: 0.23497702
INFO:root:[    2] Training loss: 0.04641603, Validation loss: 2.45743230, Gradient norm: 0.27777109
INFO:root:[    3] Training loss: 0.04073920, Validation loss: 2.18839311, Gradient norm: 0.25345025
INFO:root:[    4] Training loss: 0.03647824, Validation loss: 2.11393966, Gradient norm: 0.26099219
INFO:root:[    5] Training loss: 0.03406161, Validation loss: 1.86453380, Gradient norm: 0.28858346
INFO:root:[    6] Training loss: 0.03186511, Validation loss: 1.77786447, Gradient norm: 0.29162413
INFO:root:[    7] Training loss: 0.02988472, Validation loss: 1.69249687, Gradient norm: 0.22459602
INFO:root:[    8] Training loss: 0.02859462, Validation loss: 1.60722778, Gradient norm: 0.26012606
INFO:root:[    9] Training loss: 0.02733044, Validation loss: 1.52689807, Gradient norm: 0.27520918
INFO:root:[   10] Training loss: 0.02644243, Validation loss: 1.48270835, Gradient norm: 0.27709260
INFO:root:[   11] Training loss: 0.02566970, Validation loss: 1.63554819, Gradient norm: 0.30449842
INFO:root:[   12] Training loss: 0.02493243, Validation loss: 1.41732602, Gradient norm: 0.31113047
INFO:root:[   13] Training loss: 0.02404195, Validation loss: 1.36845400, Gradient norm: 0.29839666
INFO:root:[   14] Training loss: 0.02351893, Validation loss: 1.42906846, Gradient norm: 0.31090002
INFO:root:[   15] Training loss: 0.02284141, Validation loss: 1.27208129, Gradient norm: 0.29588085
INFO:root:[   16] Training loss: 0.02212941, Validation loss: 1.27825004, Gradient norm: 0.28681971
INFO:root:[   17] Training loss: 0.02232693, Validation loss: 1.25881106, Gradient norm: 0.37218373
INFO:root:[   18] Training loss: 0.02148883, Validation loss: 1.26154977, Gradient norm: 0.33138592
INFO:root:[   19] Training loss: 0.02148788, Validation loss: 1.25439297, Gradient norm: 0.38621807
INFO:root:[   20] Training loss: 0.02086067, Validation loss: 1.17727903, Gradient norm: 0.33731228
INFO:root:[   21] Training loss: 0.02041249, Validation loss: 1.15171762, Gradient norm: 0.29964200
INFO:root:[   22] Training loss: 0.02008830, Validation loss: 1.18156682, Gradient norm: 0.31490248
INFO:root:[   23] Training loss: 0.02010200, Validation loss: 1.16060118, Gradient norm: 0.37677177
INFO:root:[   24] Training loss: 0.01973925, Validation loss: 1.11096380, Gradient norm: 0.35046783
INFO:root:[   25] Training loss: 0.01951977, Validation loss: 1.12517958, Gradient norm: 0.35533798
INFO:root:[   26] Training loss: 0.01971886, Validation loss: 1.15702480, Gradient norm: 0.40035835
INFO:root:[   27] Training loss: 0.01915974, Validation loss: 1.10534597, Gradient norm: 0.35573152
INFO:root:[   28] Training loss: 0.01907565, Validation loss: 1.06785999, Gradient norm: 0.37291880
INFO:root:[   29] Training loss: 0.01851522, Validation loss: 1.15532167, Gradient norm: 0.34576822
INFO:root:[   30] Training loss: 0.01858738, Validation loss: 1.05126850, Gradient norm: 0.37283548
INFO:root:[   31] Training loss: 0.01824852, Validation loss: 1.03106384, Gradient norm: 0.35398760
INFO:root:[   32] Training loss: 0.01804181, Validation loss: 1.02205445, Gradient norm: 0.35425168
INFO:root:[   33] Training loss: 0.01823386, Validation loss: 1.06896714, Gradient norm: 0.38581929
INFO:root:[   34] Training loss: 0.01761223, Validation loss: 1.07066453, Gradient norm: 0.32492797
INFO:root:[   35] Training loss: 0.01756862, Validation loss: 1.00773382, Gradient norm: 0.37022135
INFO:root:[   36] Training loss: 0.01730173, Validation loss: 1.00492986, Gradient norm: 0.32126628
INFO:root:[   37] Training loss: 0.01747904, Validation loss: 1.05209792, Gradient norm: 0.38046738
INFO:root:[   38] Training loss: 0.01728916, Validation loss: 1.11011043, Gradient norm: 0.36975202
INFO:root:[   39] Training loss: 0.01683483, Validation loss: 0.98226662, Gradient norm: 0.30928145
INFO:root:[   40] Training loss: 0.01698210, Validation loss: 0.97718783, Gradient norm: 0.36344573
INFO:root:[   41] Training loss: 0.01677754, Validation loss: 0.94935328, Gradient norm: 0.34243743
INFO:root:[   42] Training loss: 0.01673471, Validation loss: 0.96416091, Gradient norm: 0.36524787
INFO:root:[   43] Training loss: 0.01632987, Validation loss: 0.99111733, Gradient norm: 0.31665678
INFO:root:[   44] Training loss: 0.01656006, Validation loss: 0.95256563, Gradient norm: 0.40791502
INFO:root:[   45] Training loss: 0.01632699, Validation loss: 0.95177920, Gradient norm: 0.35504842
INFO:root:[   46] Training loss: 0.01619803, Validation loss: 0.98166073, Gradient norm: 0.35225051
INFO:root:[   47] Training loss: 0.01631258, Validation loss: 0.99230811, Gradient norm: 0.41110558
INFO:root:[   48] Training loss: 0.01605742, Validation loss: 0.93567912, Gradient norm: 0.37911568
INFO:root:[   49] Training loss: 0.01566197, Validation loss: 1.03101608, Gradient norm: 0.32099570
INFO:root:[   50] Training loss: 0.01600346, Validation loss: 0.93215017, Gradient norm: 0.38791884
INFO:root:[   51] Training loss: 0.01580581, Validation loss: 0.94377639, Gradient norm: 0.36660285
INFO:root:[   52] Training loss: 0.01540267, Validation loss: 0.89056344, Gradient norm: 0.30900509
INFO:root:[   53] Training loss: 0.01546220, Validation loss: 0.90715806, Gradient norm: 0.35674446
INFO:root:[   54] Training loss: 0.01572903, Validation loss: 0.91617753, Gradient norm: 0.40379808
INFO:root:[   55] Training loss: 0.01543447, Validation loss: 0.93109058, Gradient norm: 0.36914053
INFO:root:[   56] Training loss: 0.01533148, Validation loss: 0.89916112, Gradient norm: 0.36451106
INFO:root:[   57] Training loss: 0.01510761, Validation loss: 0.89022981, Gradient norm: 0.32686404
INFO:root:[   58] Training loss: 0.01512921, Validation loss: 0.86428132, Gradient norm: 0.33649105
INFO:root:[   59] Training loss: 0.01482400, Validation loss: 0.88933126, Gradient norm: 0.29098611
INFO:root:[   60] Training loss: 0.01510757, Validation loss: 0.89122724, Gradient norm: 0.34622351
INFO:root:[   61] Training loss: 0.01519613, Validation loss: 0.93351366, Gradient norm: 0.36098823
INFO:root:[   62] Training loss: 0.01508326, Validation loss: 0.88642973, Gradient norm: 0.36771688
INFO:root:[   63] Training loss: 0.01471190, Validation loss: 0.87879036, Gradient norm: 0.32669861
INFO:root:[   64] Training loss: 0.01465704, Validation loss: 0.84783568, Gradient norm: 0.31029219
INFO:root:[   65] Training loss: 0.01468929, Validation loss: 0.84944685, Gradient norm: 0.34299252
INFO:root:[   66] Training loss: 0.01464108, Validation loss: 0.87899358, Gradient norm: 0.34935897
INFO:root:[   67] Training loss: 0.01441545, Validation loss: 0.87310505, Gradient norm: 0.31052413
INFO:root:[   68] Training loss: 0.01430787, Validation loss: 0.84259053, Gradient norm: 0.32419665
INFO:root:[   69] Training loss: 0.01432217, Validation loss: 0.83654759, Gradient norm: 0.33827263
INFO:root:[   70] Training loss: 0.01435908, Validation loss: 0.86363542, Gradient norm: 0.33895216
INFO:root:[   71] Training loss: 0.01427623, Validation loss: 0.83704187, Gradient norm: 0.33986598
INFO:root:[   72] Training loss: 0.01408293, Validation loss: 0.83785928, Gradient norm: 0.29627706
INFO:root:[   73] Training loss: 0.01430449, Validation loss: 0.84944627, Gradient norm: 0.34440755
INFO:root:[   74] Training loss: 0.01422054, Validation loss: 0.87418541, Gradient norm: 0.36981050
INFO:root:[   75] Training loss: 0.01385734, Validation loss: 0.82632752, Gradient norm: 0.30059331
INFO:root:[   76] Training loss: 0.01405922, Validation loss: 0.84831972, Gradient norm: 0.33554623
INFO:root:[   77] Training loss: 0.01404961, Validation loss: 0.86601922, Gradient norm: 0.34006467
INFO:root:[   78] Training loss: 0.01374565, Validation loss: 0.83403013, Gradient norm: 0.30327060
INFO:root:[   79] Training loss: 0.01376704, Validation loss: 0.82579228, Gradient norm: 0.31927865
INFO:root:[   80] Training loss: 0.01375329, Validation loss: 0.83641482, Gradient norm: 0.35215836
INFO:root:[   81] Training loss: 0.01368775, Validation loss: 0.83066387, Gradient norm: 0.32915156
INFO:root:[   82] Training loss: 0.01389495, Validation loss: 0.80578653, Gradient norm: 0.36424633
INFO:root:[   83] Training loss: 0.01355449, Validation loss: 0.81677494, Gradient norm: 0.29808655
INFO:root:[   84] Training loss: 0.01362040, Validation loss: 0.85227381, Gradient norm: 0.33885521
INFO:root:[   85] Training loss: 0.01353224, Validation loss: 0.81502844, Gradient norm: 0.33442772
INFO:root:[   86] Training loss: 0.01365754, Validation loss: 0.78710136, Gradient norm: 0.35519399
INFO:root:[   87] Training loss: 0.01331437, Validation loss: 0.81308644, Gradient norm: 0.27761445
INFO:root:[   88] Training loss: 0.01337695, Validation loss: 0.84191340, Gradient norm: 0.31186383
INFO:root:[   89] Training loss: 0.01349279, Validation loss: 0.81888759, Gradient norm: 0.34674759
INFO:root:[   90] Training loss: 0.01310586, Validation loss: 0.77685226, Gradient norm: 0.29357617
INFO:root:[   91] Training loss: 0.01328569, Validation loss: 0.81601574, Gradient norm: 0.33312278
INFO:root:[   92] Training loss: 0.01320455, Validation loss: 0.82168601, Gradient norm: 0.31922170
INFO:root:[   93] Training loss: 0.01347146, Validation loss: 0.85627689, Gradient norm: 0.35345625
INFO:root:[   94] Training loss: 0.01328107, Validation loss: 0.79622061, Gradient norm: 0.34522264
INFO:root:[   95] Training loss: 0.01318020, Validation loss: 0.80014815, Gradient norm: 0.32058324
INFO:root:[   96] Training loss: 0.01313258, Validation loss: 0.83488241, Gradient norm: 0.30947039
INFO:root:[   97] Training loss: 0.01288121, Validation loss: 0.81789323, Gradient norm: 0.28090931
INFO:root:[   98] Training loss: 0.01296129, Validation loss: 0.78331779, Gradient norm: 0.29528944
INFO:root:[   99] Training loss: 0.01310466, Validation loss: 0.80670018, Gradient norm: 0.33137848
INFO:root:EP 99: Early stopping
INFO:root:Training the model took 494.368s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.06057
INFO:root:EnergyScoretrain: 0.04526
INFO:root:Coveragetrain: 6.70338
INFO:root:IntervalWidthtrain: 104.03602
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02007
INFO:root:EnergyScorevalidation: 0.01527
INFO:root:Coveragevalidation: 1.73595
INFO:root:IntervalWidthvalidation: 27.35419
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.01934
INFO:root:EnergyScoretest: 0.01413
INFO:root:Coveragetest: 0.98599
INFO:root:IntervalWidthtest: 44.71111
