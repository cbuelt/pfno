INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno_dropout.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.25352554, Validation loss: 0.20769083, Gradient norm: 2.94907415
INFO:root:[    2] Training loss: 0.14865133, Validation loss: 0.16084488, Gradient norm: 3.17360064
INFO:root:[    3] Training loss: 0.12623321, Validation loss: 0.14864762, Gradient norm: 3.32555432
INFO:root:[    4] Training loss: 0.11204833, Validation loss: 0.12771085, Gradient norm: 2.94727518
INFO:root:[    5] Training loss: 0.10186073, Validation loss: 0.13817186, Gradient norm: 2.77639690
INFO:root:[    6] Training loss: 0.09653895, Validation loss: 0.12575877, Gradient norm: 3.15870775
INFO:root:[    7] Training loss: 0.08880669, Validation loss: 0.11156238, Gradient norm: 2.77251192
INFO:root:[    8] Training loss: 0.08423421, Validation loss: 0.11901858, Gradient norm: 2.75831765
INFO:root:[    9] Training loss: 0.08793274, Validation loss: 0.10673518, Gradient norm: 3.03657144
INFO:root:[   10] Training loss: 0.08143862, Validation loss: 0.10794322, Gradient norm: 2.79740808
INFO:root:[   11] Training loss: 0.08089599, Validation loss: 0.11232959, Gradient norm: 2.90676066
INFO:root:[   12] Training loss: 0.07855887, Validation loss: 0.11279100, Gradient norm: 2.85910259
INFO:root:[   13] Training loss: 0.07709289, Validation loss: 0.10593485, Gradient norm: 2.76162576
INFO:root:[   14] Training loss: 0.07582188, Validation loss: 0.13175195, Gradient norm: 2.76571290
INFO:root:[   15] Training loss: 0.07427648, Validation loss: 0.10490398, Gradient norm: 2.75383589
INFO:root:[   16] Training loss: 0.07315099, Validation loss: 0.10417760, Gradient norm: 2.74827362
INFO:root:[   17] Training loss: 0.07217108, Validation loss: 0.11515429, Gradient norm: 2.88208676
INFO:root:[   18] Training loss: 0.07015325, Validation loss: 0.12013948, Gradient norm: 2.55286529
INFO:root:[   19] Training loss: 0.06950167, Validation loss: 0.13656004, Gradient norm: 2.87603141
INFO:root:[   20] Training loss: 0.06972933, Validation loss: 0.14154856, Gradient norm: 2.76353501
INFO:root:[   21] Training loss: 0.07151530, Validation loss: 0.11192678, Gradient norm: 2.85159278
INFO:root:[   22] Training loss: 0.06554275, Validation loss: 0.11616690, Gradient norm: 2.51717442
INFO:root:[   23] Training loss: 0.06240090, Validation loss: 0.10623184, Gradient norm: 2.25334452
INFO:root:[   24] Training loss: 0.06545443, Validation loss: 0.11119265, Gradient norm: 2.60656488
INFO:root:[   25] Training loss: 0.06749544, Validation loss: 0.10728830, Gradient norm: 2.87049987
INFO:root:[   26] Training loss: 0.06389856, Validation loss: 0.12130872, Gradient norm: 2.43735092
INFO:root:[   27] Training loss: 0.05963065, Validation loss: 0.10875104, Gradient norm: 2.25982937
INFO:root:[   28] Training loss: 0.06223695, Validation loss: 0.11057883, Gradient norm: 2.55689729
INFO:root:[   29] Training loss: 0.06218747, Validation loss: 0.11618268, Gradient norm: 2.41213194
INFO:root:[   30] Training loss: 0.06078642, Validation loss: 0.11453611, Gradient norm: 2.28128959
INFO:root:[   31] Training loss: 0.06095184, Validation loss: 0.11490558, Gradient norm: 2.61799943
INFO:root:[   32] Training loss: 0.06086078, Validation loss: 0.11117585, Gradient norm: 2.42430015
INFO:root:[   33] Training loss: 0.05750523, Validation loss: 0.11381725, Gradient norm: 2.04988116
INFO:root:[   34] Training loss: 0.05820097, Validation loss: 0.12976101, Gradient norm: 2.50979309
INFO:root:[   35] Training loss: 0.05707453, Validation loss: 0.10747819, Gradient norm: 2.35929230
INFO:root:[   36] Training loss: 0.05901754, Validation loss: 0.11983401, Gradient norm: 2.45614533
INFO:root:[   37] Training loss: 0.05444052, Validation loss: 0.10973428, Gradient norm: 2.32077466
INFO:root:[   38] Training loss: 0.05571397, Validation loss: 0.13150226, Gradient norm: 2.44684042
INFO:root:[   39] Training loss: 0.05587481, Validation loss: 0.11821892, Gradient norm: 2.49173246
INFO:root:[   40] Training loss: 0.05514347, Validation loss: 0.11774503, Gradient norm: 2.29472846
INFO:root:[   41] Training loss: 0.05715836, Validation loss: 0.11542676, Gradient norm: 2.57120961
INFO:root:[   42] Training loss: 0.05436817, Validation loss: 0.12451152, Gradient norm: 2.50385564
INFO:root:[   43] Training loss: 0.05338963, Validation loss: 0.12746414, Gradient norm: 2.26802660
INFO:root:[   44] Training loss: 0.05379541, Validation loss: 0.11039498, Gradient norm: 2.46041597
INFO:root:[   45] Training loss: 0.05350613, Validation loss: 0.11475361, Gradient norm: 2.53039557
INFO:root:[   46] Training loss: 0.05452383, Validation loss: 0.11335786, Gradient norm: 2.53605314
INFO:root:[   47] Training loss: 0.05331257, Validation loss: 0.10740560, Gradient norm: 2.44729859
INFO:root:[   48] Training loss: 0.05261159, Validation loss: 0.10724354, Gradient norm: 2.54606205
INFO:root:[   49] Training loss: 0.05320520, Validation loss: 0.11309370, Gradient norm: 2.53694870
INFO:root:[   50] Training loss: 0.05303337, Validation loss: 0.11100943, Gradient norm: 2.75953582
INFO:root:[   51] Training loss: 0.05027380, Validation loss: 0.11143633, Gradient norm: 2.14079307
INFO:root:[   52] Training loss: 0.05193052, Validation loss: 0.10838832, Gradient norm: 2.24982301
INFO:root:[   53] Training loss: 0.05014279, Validation loss: 0.10881189, Gradient norm: 2.39681150
INFO:root:[   54] Training loss: 0.04896008, Validation loss: 0.11138463, Gradient norm: 2.33269921
INFO:root:[   55] Training loss: 0.05063508, Validation loss: 0.10710623, Gradient norm: 2.39990670
INFO:root:[   56] Training loss: 0.04988298, Validation loss: 0.10771913, Gradient norm: 2.41163345
INFO:root:[   57] Training loss: 0.05167048, Validation loss: 0.12641836, Gradient norm: 2.30718031
INFO:root:[   58] Training loss: 0.05036705, Validation loss: 0.11108592, Gradient norm: 2.56122120
INFO:root:[   59] Training loss: 0.04877065, Validation loss: 0.10704973, Gradient norm: 2.50423281
INFO:root:[   60] Training loss: 0.04966797, Validation loss: 0.11570698, Gradient norm: 2.42279874
INFO:root:[   61] Training loss: 0.04919450, Validation loss: 0.10949092, Gradient norm: 2.67779450
INFO:root:[   62] Training loss: 0.04897448, Validation loss: 0.11293969, Gradient norm: 2.34124563
INFO:root:[   63] Training loss: 0.04675625, Validation loss: 0.11334097, Gradient norm: 2.00832091
INFO:root:[   64] Training loss: 0.04710531, Validation loss: 0.10730819, Gradient norm: 2.54582648
INFO:root:[   65] Training loss: 0.04694538, Validation loss: 0.10093937, Gradient norm: 2.29345036
INFO:root:[   66] Training loss: 0.04769339, Validation loss: 0.11275642, Gradient norm: 2.09092984
INFO:root:[   67] Training loss: 0.04977770, Validation loss: 0.10998915, Gradient norm: 2.52264704
INFO:root:[   68] Training loss: 0.04620443, Validation loss: 0.09830641, Gradient norm: 2.27982101
INFO:root:[   69] Training loss: 0.04632434, Validation loss: 0.11935499, Gradient norm: 2.28356415
INFO:root:[   70] Training loss: 0.04642383, Validation loss: 0.11633360, Gradient norm: 2.51410375
INFO:root:[   71] Training loss: 0.04734173, Validation loss: 0.11076904, Gradient norm: 2.47864632
INFO:root:[   72] Training loss: 0.04642038, Validation loss: 0.11432785, Gradient norm: 2.56324698
INFO:root:[   73] Training loss: 0.04658205, Validation loss: 0.10630294, Gradient norm: 2.46058113
INFO:root:[   74] Training loss: 0.04439159, Validation loss: 0.11669603, Gradient norm: 2.42480967
INFO:root:[   75] Training loss: 0.04442657, Validation loss: 0.10314440, Gradient norm: 2.31309633
INFO:root:[   76] Training loss: 0.04603193, Validation loss: 0.10428427, Gradient norm: 2.39413759
INFO:root:[   77] Training loss: 0.04642519, Validation loss: 0.10027472, Gradient norm: 2.44973311
INFO:root:EP 77: Early stopping
INFO:root:Training the model took 2866.03s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03926
INFO:root:EnergyScoreTrain: 0.02857
INFO:root:CoverageTrain: 0.81533
INFO:root:IntervalWidthTrain: 0.02106
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.09139
INFO:root:EnergyScoreValidation: 0.07295
INFO:root:CoverageValidation: 0.54947
INFO:root:IntervalWidthValidation: 0.02535
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.09252
INFO:root:EnergyScoreTest: 0.07403
INFO:root:CoverageTest: 0.54294
INFO:root:IntervalWidthTest: 0.02527
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27677115, Validation loss: 0.23140456, Gradient norm: 3.71401505
INFO:root:[    2] Training loss: 0.15304543, Validation loss: 0.18366793, Gradient norm: 3.76261749
INFO:root:[    3] Training loss: 0.12830853, Validation loss: 0.17350893, Gradient norm: 3.30703748
INFO:root:[    4] Training loss: 0.11541835, Validation loss: 0.15317741, Gradient norm: 3.31707144
INFO:root:[    5] Training loss: 0.10160340, Validation loss: 0.15802373, Gradient norm: 3.18116024
INFO:root:[    6] Training loss: 0.10029005, Validation loss: 0.13639339, Gradient norm: 3.37113757
INFO:root:[    7] Training loss: 0.09037161, Validation loss: 0.13089286, Gradient norm: 2.92242557
INFO:root:[    8] Training loss: 0.08862081, Validation loss: 0.13190046, Gradient norm: 3.00479792
INFO:root:[    9] Training loss: 0.08367955, Validation loss: 0.11679203, Gradient norm: 2.89512659
INFO:root:[   10] Training loss: 0.08215386, Validation loss: 0.11602587, Gradient norm: 3.05191178
INFO:root:[   11] Training loss: 0.08124746, Validation loss: 0.11925390, Gradient norm: 2.90452910
INFO:root:[   12] Training loss: 0.07287146, Validation loss: 0.12881042, Gradient norm: 2.47992273
INFO:root:[   13] Training loss: 0.07534817, Validation loss: 0.12133631, Gradient norm: 2.83437328
INFO:root:[   14] Training loss: 0.07748077, Validation loss: 0.10744204, Gradient norm: 3.16404622
INFO:root:[   15] Training loss: 0.07138358, Validation loss: 0.10507305, Gradient norm: 2.71156561
INFO:root:[   16] Training loss: 0.07260169, Validation loss: 0.11698968, Gradient norm: 2.70581215
INFO:root:[   17] Training loss: 0.06939934, Validation loss: 0.11438295, Gradient norm: 2.68422914
INFO:root:[   18] Training loss: 0.07179451, Validation loss: 0.09676912, Gradient norm: 2.90697648
INFO:root:[   19] Training loss: 0.06648474, Validation loss: 0.10070581, Gradient norm: 2.62772912
INFO:root:[   20] Training loss: 0.06581632, Validation loss: 0.10029194, Gradient norm: 2.49905244
INFO:root:[   21] Training loss: 0.06766089, Validation loss: 0.09710567, Gradient norm: 2.83025970
INFO:root:[   22] Training loss: 0.06589081, Validation loss: 0.10449773, Gradient norm: 2.51796285
INFO:root:[   23] Training loss: 0.06528432, Validation loss: 0.09706196, Gradient norm: 2.69037528
INFO:root:[   24] Training loss: 0.06421509, Validation loss: 0.11168016, Gradient norm: 2.56176592
INFO:root:[   25] Training loss: 0.06249464, Validation loss: 0.09357254, Gradient norm: 2.56772338
INFO:root:[   26] Training loss: 0.06127272, Validation loss: 0.09336081, Gradient norm: 2.54874626
INFO:root:[   27] Training loss: 0.06225299, Validation loss: 0.09446079, Gradient norm: 2.68624017
INFO:root:[   28] Training loss: 0.06265300, Validation loss: 0.09052215, Gradient norm: 2.67296059
INFO:root:[   29] Training loss: 0.06077340, Validation loss: 0.10493846, Gradient norm: 2.71294371
INFO:root:[   30] Training loss: 0.05766543, Validation loss: 0.11031368, Gradient norm: 2.51194190
INFO:root:[   31] Training loss: 0.05746788, Validation loss: 0.11297238, Gradient norm: 2.14516288
INFO:root:[   32] Training loss: 0.05731858, Validation loss: 0.08993587, Gradient norm: 2.29466890
INFO:root:[   33] Training loss: 0.05660642, Validation loss: 0.08980854, Gradient norm: 2.42890363
INFO:root:[   34] Training loss: 0.05799390, Validation loss: 0.08438919, Gradient norm: 2.45121408
INFO:root:[   35] Training loss: 0.05815951, Validation loss: 0.09558203, Gradient norm: 2.76693438
INFO:root:[   36] Training loss: 0.05428115, Validation loss: 0.09724663, Gradient norm: 2.54969884
INFO:root:[   37] Training loss: 0.05543084, Validation loss: 0.10058211, Gradient norm: 2.41424764
INFO:root:[   38] Training loss: 0.05528276, Validation loss: 0.09217864, Gradient norm: 2.62371091
INFO:root:[   39] Training loss: 0.05465230, Validation loss: 0.10100005, Gradient norm: 2.44875042
INFO:root:[   40] Training loss: 0.05411678, Validation loss: 0.10352817, Gradient norm: 2.65068855
INFO:root:[   41] Training loss: 0.05457143, Validation loss: 0.08771421, Gradient norm: 2.71582975
INFO:root:[   42] Training loss: 0.05147467, Validation loss: 0.08531906, Gradient norm: 2.35180741
INFO:root:[   43] Training loss: 0.05647268, Validation loss: 0.08626874, Gradient norm: 2.47653376
INFO:root:[   44] Training loss: 0.05669548, Validation loss: 0.08667382, Gradient norm: 2.40046199
INFO:root:[   45] Training loss: 0.05260194, Validation loss: 0.09880005, Gradient norm: 2.56429764
INFO:root:[   46] Training loss: 0.05157831, Validation loss: 0.09064738, Gradient norm: 2.60656728
INFO:root:[   47] Training loss: 0.05017378, Validation loss: 0.09247400, Gradient norm: 2.38242592
INFO:root:[   48] Training loss: 0.05261987, Validation loss: 0.08502431, Gradient norm: 2.63595601
INFO:root:[   49] Training loss: 0.05264723, Validation loss: 0.09809966, Gradient norm: 2.79950828
INFO:root:[   50] Training loss: 0.05033177, Validation loss: 0.08270716, Gradient norm: 2.55811104
INFO:root:[   51] Training loss: 0.04983754, Validation loss: 0.09206910, Gradient norm: 2.39945953
INFO:root:[   52] Training loss: 0.05193220, Validation loss: 0.10542939, Gradient norm: 2.66205605
INFO:root:[   53] Training loss: 0.05254654, Validation loss: 0.09174411, Gradient norm: 2.71794102
INFO:root:[   54] Training loss: 0.04867126, Validation loss: 0.08226741, Gradient norm: 2.59246867
INFO:root:[   55] Training loss: 0.04932385, Validation loss: 0.09443474, Gradient norm: 2.77437905
INFO:root:[   56] Training loss: 0.04831184, Validation loss: 0.08429256, Gradient norm: 2.19937706
INFO:root:[   57] Training loss: 0.05036136, Validation loss: 0.08442136, Gradient norm: 2.60711190
INFO:root:[   58] Training loss: 0.04914116, Validation loss: 0.10030523, Gradient norm: 2.55940505
INFO:root:[   59] Training loss: 0.04746983, Validation loss: 0.08078728, Gradient norm: 2.60731176
INFO:root:[   60] Training loss: 0.04899970, Validation loss: 0.07917019, Gradient norm: 2.65311552
INFO:root:[   61] Training loss: 0.04958166, Validation loss: 0.08505814, Gradient norm: 2.63013703
INFO:root:[   62] Training loss: 0.04612483, Validation loss: 0.09920385, Gradient norm: 2.43975024
INFO:root:[   63] Training loss: 0.05016899, Validation loss: 0.09433956, Gradient norm: 2.64234720
INFO:root:[   64] Training loss: 0.04718713, Validation loss: 0.08759057, Gradient norm: 2.33255410
INFO:root:[   65] Training loss: 0.04765037, Validation loss: 0.08039894, Gradient norm: 2.76359678
INFO:root:[   66] Training loss: 0.04661115, Validation loss: 0.09186817, Gradient norm: 2.59386846
INFO:root:[   67] Training loss: 0.04698262, Validation loss: 0.08338820, Gradient norm: 2.31899977
INFO:root:[   68] Training loss: 0.04599144, Validation loss: 0.09361178, Gradient norm: 2.57174512
INFO:root:[   69] Training loss: 0.04695388, Validation loss: 0.08048386, Gradient norm: 2.81776349
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2452.016s.
INFO:root:Emptying the cuda cache took 0.023s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04364
INFO:root:EnergyScoreTrain: 0.03205
INFO:root:CoverageTrain: 0.81221
INFO:root:IntervalWidthTrain: 0.02072
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06986
INFO:root:EnergyScoreValidation: 0.05337
INFO:root:CoverageValidation: 0.66866
INFO:root:IntervalWidthValidation: 0.02512
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07066
INFO:root:EnergyScoreTest: 0.05421
INFO:root:CoverageTest: 0.66408
INFO:root:IntervalWidthTest: 0.02504
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28517147, Validation loss: 0.22015873, Gradient norm: 3.17052017
INFO:root:[    2] Training loss: 0.15508509, Validation loss: 0.16570387, Gradient norm: 3.62608578
INFO:root:[    3] Training loss: 0.12923130, Validation loss: 0.17126322, Gradient norm: 3.43884221
INFO:root:[    4] Training loss: 0.11626044, Validation loss: 0.14150370, Gradient norm: 3.43808946
INFO:root:[    5] Training loss: 0.10563548, Validation loss: 0.13348234, Gradient norm: 3.20645997
INFO:root:[    6] Training loss: 0.09715591, Validation loss: 0.12211103, Gradient norm: 3.06724754
INFO:root:[    7] Training loss: 0.09992399, Validation loss: 0.12591127, Gradient norm: 3.67138991
INFO:root:[    8] Training loss: 0.09480373, Validation loss: 0.12566354, Gradient norm: 3.37247804
INFO:root:[    9] Training loss: 0.08752722, Validation loss: 0.12699877, Gradient norm: 3.01843652
INFO:root:[   10] Training loss: 0.08391728, Validation loss: 0.11326412, Gradient norm: 3.19945775
INFO:root:[   11] Training loss: 0.08266237, Validation loss: 0.14055712, Gradient norm: 3.03868076
INFO:root:[   12] Training loss: 0.08053061, Validation loss: 0.11015938, Gradient norm: 3.17315074
INFO:root:[   13] Training loss: 0.07815074, Validation loss: 0.10507930, Gradient norm: 3.09794128
INFO:root:[   14] Training loss: 0.07613552, Validation loss: 0.10201981, Gradient norm: 2.98375291
INFO:root:[   15] Training loss: 0.07274757, Validation loss: 0.10544915, Gradient norm: 2.78967801
INFO:root:[   16] Training loss: 0.07436112, Validation loss: 0.11337956, Gradient norm: 3.05069646
INFO:root:[   17] Training loss: 0.07248492, Validation loss: 0.10702789, Gradient norm: 2.97021382
INFO:root:[   18] Training loss: 0.07443777, Validation loss: 0.11125297, Gradient norm: 3.02476716
INFO:root:[   19] Training loss: 0.07324707, Validation loss: 0.10358149, Gradient norm: 3.24367761
INFO:root:[   20] Training loss: 0.07110683, Validation loss: 0.09872228, Gradient norm: 2.85442249
INFO:root:[   21] Training loss: 0.06989566, Validation loss: 0.10826123, Gradient norm: 3.00066545
INFO:root:[   22] Training loss: 0.06844000, Validation loss: 0.10284971, Gradient norm: 3.02306800
INFO:root:[   23] Training loss: 0.07026360, Validation loss: 0.09903420, Gradient norm: 3.18048225
INFO:root:[   24] Training loss: 0.06465009, Validation loss: 0.09734640, Gradient norm: 2.56544372
INFO:root:[   25] Training loss: 0.06489592, Validation loss: 0.10082773, Gradient norm: 2.71707968
INFO:root:[   26] Training loss: 0.06361921, Validation loss: 0.09831413, Gradient norm: 2.66814809
INFO:root:[   27] Training loss: 0.06454289, Validation loss: 0.10262327, Gradient norm: 2.69512461
INFO:root:[   28] Training loss: 0.06339673, Validation loss: 0.09830181, Gradient norm: 2.79433000
INFO:root:[   29] Training loss: 0.06477006, Validation loss: 0.10592689, Gradient norm: 2.94736359
INFO:root:[   30] Training loss: 0.05910793, Validation loss: 0.09435877, Gradient norm: 2.25341215
INFO:root:[   31] Training loss: 0.06211054, Validation loss: 0.10429957, Gradient norm: 2.60603742
INFO:root:[   32] Training loss: 0.05946552, Validation loss: 0.09964534, Gradient norm: 2.60434101
INFO:root:[   33] Training loss: 0.06135850, Validation loss: 0.09597066, Gradient norm: 2.54349078
INFO:root:[   34] Training loss: 0.05959401, Validation loss: 0.11228101, Gradient norm: 2.78329319
INFO:root:[   35] Training loss: 0.05735591, Validation loss: 0.10226251, Gradient norm: 2.31985644
INFO:root:[   36] Training loss: 0.05863489, Validation loss: 0.09689864, Gradient norm: 2.81941710
INFO:root:[   37] Training loss: 0.06046824, Validation loss: 0.10521466, Gradient norm: 2.30735473
INFO:root:[   38] Training loss: 0.05701840, Validation loss: 0.11405815, Gradient norm: 2.37047969
INFO:root:[   39] Training loss: 0.05654937, Validation loss: 0.09631379, Gradient norm: 2.89270771
INFO:root:[   40] Training loss: 0.05765760, Validation loss: 0.09366155, Gradient norm: 2.71725661
INFO:root:[   41] Training loss: 0.05522941, Validation loss: 0.10394734, Gradient norm: 2.55598414
INFO:root:[   42] Training loss: 0.05432054, Validation loss: 0.10128384, Gradient norm: 2.42423030
INFO:root:[   43] Training loss: 0.05431148, Validation loss: 0.10529498, Gradient norm: 2.56255480
INFO:root:[   44] Training loss: 0.05531992, Validation loss: 0.10053462, Gradient norm: 2.89945520
INFO:root:[   45] Training loss: 0.05508336, Validation loss: 0.09947830, Gradient norm: 2.33921393
INFO:root:[   46] Training loss: 0.05432833, Validation loss: 0.10970283, Gradient norm: 2.65660009
INFO:root:[   47] Training loss: 0.05059755, Validation loss: 0.10292814, Gradient norm: 2.09868187
INFO:root:[   48] Training loss: 0.05339101, Validation loss: 0.10716922, Gradient norm: 2.31002711
INFO:root:[   49] Training loss: 0.05271335, Validation loss: 0.09337066, Gradient norm: 2.65802468
INFO:root:[   50] Training loss: 0.05326582, Validation loss: 0.09269872, Gradient norm: 2.62687417
INFO:root:[   51] Training loss: 0.05398739, Validation loss: 0.10215775, Gradient norm: 2.92635864
INFO:root:[   52] Training loss: 0.05208438, Validation loss: 0.09753379, Gradient norm: 2.61814045
INFO:root:[   53] Training loss: 0.05139543, Validation loss: 0.10207362, Gradient norm: 2.63155399
INFO:root:[   54] Training loss: 0.05055328, Validation loss: 0.08920391, Gradient norm: 2.48219797
INFO:root:[   55] Training loss: 0.05078928, Validation loss: 0.10003623, Gradient norm: 2.14564656
INFO:root:[   56] Training loss: 0.04993650, Validation loss: 0.09359414, Gradient norm: 2.46850413
INFO:root:[   57] Training loss: 0.04830868, Validation loss: 0.09373883, Gradient norm: 2.24327209
INFO:root:[   58] Training loss: 0.05011176, Validation loss: 0.09039089, Gradient norm: 2.63898353
INFO:root:[   59] Training loss: 0.04923206, Validation loss: 0.09486230, Gradient norm: 2.54536471
INFO:root:[   60] Training loss: 0.04937394, Validation loss: 0.09838203, Gradient norm: 2.37851955
INFO:root:[   61] Training loss: 0.04677431, Validation loss: 0.10595085, Gradient norm: 2.06934307
INFO:root:[   62] Training loss: 0.04837434, Validation loss: 0.09333963, Gradient norm: 2.60078562
INFO:root:[   63] Training loss: 0.04914537, Validation loss: 0.09270737, Gradient norm: 2.64377712
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 2255.892s.
INFO:root:Emptying the cuda cache took 0.024s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03025
INFO:root:EnergyScoreTrain: 0.02217
INFO:root:CoverageTrain: 0.9177
INFO:root:IntervalWidthTrain: 0.02027
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08079
INFO:root:EnergyScoreValidation: 0.06296
INFO:root:CoverageValidation: 0.60002
INFO:root:IntervalWidthValidation: 0.02492
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08225
INFO:root:EnergyScoreTest: 0.06433
INFO:root:CoverageTest: 0.59437
INFO:root:IntervalWidthTest: 0.02487
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27764282, Validation loss: 0.23165077, Gradient norm: 3.73692686
INFO:root:[    2] Training loss: 0.15379533, Validation loss: 0.20768971, Gradient norm: 3.52394803
INFO:root:[    3] Training loss: 0.12790875, Validation loss: 0.17432511, Gradient norm: 3.65997983
INFO:root:[    4] Training loss: 0.11475595, Validation loss: 0.15756086, Gradient norm: 3.65263681
INFO:root:[    5] Training loss: 0.10514645, Validation loss: 0.15979746, Gradient norm: 3.34118366
INFO:root:[    6] Training loss: 0.10370162, Validation loss: 0.14666914, Gradient norm: 3.65520801
INFO:root:[    7] Training loss: 0.09638776, Validation loss: 0.13411879, Gradient norm: 3.26639045
INFO:root:[    8] Training loss: 0.08884187, Validation loss: 0.12273217, Gradient norm: 2.97691283
INFO:root:[    9] Training loss: 0.08676690, Validation loss: 0.12360125, Gradient norm: 3.16570207
INFO:root:[   10] Training loss: 0.08265456, Validation loss: 0.12020036, Gradient norm: 3.05993148
INFO:root:[   11] Training loss: 0.08069607, Validation loss: 0.11522447, Gradient norm: 3.12021762
INFO:root:[   12] Training loss: 0.07660548, Validation loss: 0.11400597, Gradient norm: 2.74293739
INFO:root:[   13] Training loss: 0.07719740, Validation loss: 0.10273853, Gradient norm: 3.22807574
INFO:root:[   14] Training loss: 0.07440131, Validation loss: 0.11204668, Gradient norm: 2.91354291
INFO:root:[   15] Training loss: 0.07604983, Validation loss: 0.10148530, Gradient norm: 3.03309937
INFO:root:[   16] Training loss: 0.07089202, Validation loss: 0.12850585, Gradient norm: 2.81514639
INFO:root:[   17] Training loss: 0.07175062, Validation loss: 0.10678355, Gradient norm: 2.75369830
INFO:root:[   18] Training loss: 0.07077267, Validation loss: 0.10753034, Gradient norm: 2.81898214
INFO:root:[   19] Training loss: 0.06937969, Validation loss: 0.10445498, Gradient norm: 3.00540613
INFO:root:[   20] Training loss: 0.07040102, Validation loss: 0.11221287, Gradient norm: 2.91817787
INFO:root:[   21] Training loss: 0.06760831, Validation loss: 0.09288186, Gradient norm: 2.86930147
INFO:root:[   22] Training loss: 0.06725359, Validation loss: 0.09438358, Gradient norm: 2.77126827
INFO:root:[   23] Training loss: 0.06375456, Validation loss: 0.08952157, Gradient norm: 2.52743587
INFO:root:[   24] Training loss: 0.06666717, Validation loss: 0.11689358, Gradient norm: 2.88599317
INFO:root:[   25] Training loss: 0.06371409, Validation loss: 0.09447551, Gradient norm: 2.70440439
INFO:root:[   26] Training loss: 0.06610028, Validation loss: 0.09010984, Gradient norm: 2.98196737
INFO:root:[   27] Training loss: 0.06285489, Validation loss: 0.10580887, Gradient norm: 2.60714224
INFO:root:[   28] Training loss: 0.06700012, Validation loss: 0.08996137, Gradient norm: 3.06121429
INFO:root:[   29] Training loss: 0.06192049, Validation loss: 0.09284496, Gradient norm: 2.49129582
INFO:root:[   30] Training loss: 0.06054964, Validation loss: 0.09115508, Gradient norm: 2.36130657
INFO:root:[   31] Training loss: 0.05955058, Validation loss: 0.09745605, Gradient norm: 2.27644244
INFO:root:[   32] Training loss: 0.06188626, Validation loss: 0.09671101, Gradient norm: 3.00756229
INFO:root:[   33] Training loss: 0.05819657, Validation loss: 0.09371545, Gradient norm: 2.35620369
INFO:root:[   34] Training loss: 0.05923410, Validation loss: 0.09635025, Gradient norm: 2.50405320
INFO:root:[   35] Training loss: 0.05611608, Validation loss: 0.08970905, Gradient norm: 2.64665602
INFO:root:[   36] Training loss: 0.05909989, Validation loss: 0.09509359, Gradient norm: 2.60542804
INFO:root:[   37] Training loss: 0.05648620, Validation loss: 0.09734513, Gradient norm: 2.45997679
INFO:root:[   38] Training loss: 0.05865212, Validation loss: 0.09592323, Gradient norm: 2.68095823
INFO:root:[   39] Training loss: 0.05631846, Validation loss: 0.09078552, Gradient norm: 2.61060984
INFO:root:[   40] Training loss: 0.05436121, Validation loss: 0.09979743, Gradient norm: 2.28511636
INFO:root:[   41] Training loss: 0.05510679, Validation loss: 0.09159089, Gradient norm: 2.33966451
INFO:root:[   42] Training loss: 0.05323629, Validation loss: 0.10010976, Gradient norm: 2.17021812
INFO:root:[   43] Training loss: 0.05306740, Validation loss: 0.10065436, Gradient norm: 2.43288842
INFO:root:[   44] Training loss: 0.05301931, Validation loss: 0.10591222, Gradient norm: 2.28865143
INFO:root:[   45] Training loss: 0.05337460, Validation loss: 0.09145657, Gradient norm: 2.52614868
INFO:root:[   46] Training loss: 0.05358911, Validation loss: 0.09738943, Gradient norm: 2.48754199
INFO:root:[   47] Training loss: 0.05305030, Validation loss: 0.09483706, Gradient norm: 2.32157653
INFO:root:[   48] Training loss: 0.05356661, Validation loss: 0.09352260, Gradient norm: 2.34128776
INFO:root:[   49] Training loss: 0.05180309, Validation loss: 0.09749864, Gradient norm: 2.55178030
INFO:root:[   50] Training loss: 0.05206132, Validation loss: 0.09959977, Gradient norm: 2.48387143
INFO:root:[   51] Training loss: 0.05363186, Validation loss: 0.10471184, Gradient norm: 2.69199855
INFO:root:[   52] Training loss: 0.05397147, Validation loss: 0.09845481, Gradient norm: 2.94007771
INFO:root:[   53] Training loss: 0.05372265, Validation loss: 0.10045283, Gradient norm: 2.55643343
INFO:root:[   54] Training loss: 0.05308449, Validation loss: 0.09432129, Gradient norm: 2.76698253
INFO:root:[   55] Training loss: 0.04951825, Validation loss: 0.09797139, Gradient norm: 2.47185807
INFO:root:[   56] Training loss: 0.05025908, Validation loss: 0.09980871, Gradient norm: 2.58434663
INFO:root:[   57] Training loss: 0.04859958, Validation loss: 0.10925506, Gradient norm: 2.36786230
INFO:root:[   58] Training loss: 0.04941144, Validation loss: 0.10406207, Gradient norm: 2.28767523
INFO:root:[   59] Training loss: 0.04863616, Validation loss: 0.10360617, Gradient norm: 2.36564464
INFO:root:[   60] Training loss: 0.04913337, Validation loss: 0.10615753, Gradient norm: 2.41666514
INFO:root:[   61] Training loss: 0.05016583, Validation loss: 0.09502989, Gradient norm: 2.08555654
INFO:root:[   62] Training loss: 0.04789304, Validation loss: 0.09364934, Gradient norm: 2.64233406
INFO:root:[   63] Training loss: 0.04816940, Validation loss: 0.10527664, Gradient norm: 2.35442496
INFO:root:[   64] Training loss: 0.04827042, Validation loss: 0.09693226, Gradient norm: 2.65213855
INFO:root:[   65] Training loss: 0.04758637, Validation loss: 0.10887965, Gradient norm: 2.47137317
INFO:root:[   66] Training loss: 0.04654545, Validation loss: 0.10456745, Gradient norm: 2.51924689
INFO:root:[   67] Training loss: 0.04834008, Validation loss: 0.09740702, Gradient norm: 2.30621154
INFO:root:[   68] Training loss: 0.04675518, Validation loss: 0.09826094, Gradient norm: 2.55571328
INFO:root:[   69] Training loss: 0.04480713, Validation loss: 0.09938036, Gradient norm: 2.25723620
INFO:root:[   70] Training loss: 0.04649706, Validation loss: 0.10975195, Gradient norm: 2.60904747
INFO:root:[   71] Training loss: 0.04604772, Validation loss: 0.10608115, Gradient norm: 2.43909933
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 2498.579s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04957
INFO:root:EnergyScoreTrain: 0.03595
INFO:root:CoverageTrain: 0.83688
INFO:root:IntervalWidthTrain: 0.02646
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07978
INFO:root:EnergyScoreValidation: 0.06085
INFO:root:CoverageValidation: 0.65842
INFO:root:IntervalWidthValidation: 0.02837
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08014
INFO:root:EnergyScoreTest: 0.06118
INFO:root:CoverageTest: 0.65855
INFO:root:IntervalWidthTest: 0.02845
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27144255, Validation loss: 0.19852963, Gradient norm: 3.35255324
INFO:root:[    2] Training loss: 0.15239784, Validation loss: 0.18776542, Gradient norm: 3.41686062
INFO:root:[    3] Training loss: 0.12851279, Validation loss: 0.16965980, Gradient norm: 3.52642982
INFO:root:[    4] Training loss: 0.11535274, Validation loss: 0.17472740, Gradient norm: 3.38315935
INFO:root:[    5] Training loss: 0.10197281, Validation loss: 0.14655077, Gradient norm: 3.03646665
INFO:root:[    6] Training loss: 0.09572308, Validation loss: 0.13401443, Gradient norm: 3.06436291
INFO:root:[    7] Training loss: 0.09723559, Validation loss: 0.13953120, Gradient norm: 3.48241688
INFO:root:[    8] Training loss: 0.08957015, Validation loss: 0.12273300, Gradient norm: 2.99069644
INFO:root:[    9] Training loss: 0.08198939, Validation loss: 0.12586206, Gradient norm: 2.67459914
INFO:root:[   10] Training loss: 0.07747366, Validation loss: 0.11603260, Gradient norm: 2.84832143
INFO:root:[   11] Training loss: 0.07982897, Validation loss: 0.14860394, Gradient norm: 2.96154477
INFO:root:[   12] Training loss: 0.08198189, Validation loss: 0.10720080, Gradient norm: 3.27556503
INFO:root:[   13] Training loss: 0.07544558, Validation loss: 0.11758695, Gradient norm: 2.82300636
INFO:root:[   14] Training loss: 0.07490484, Validation loss: 0.10779662, Gradient norm: 2.77323285
INFO:root:[   15] Training loss: 0.07204693, Validation loss: 0.11112744, Gradient norm: 2.69923967
INFO:root:[   16] Training loss: 0.07154867, Validation loss: 0.09773593, Gradient norm: 2.86103740
INFO:root:[   17] Training loss: 0.06679028, Validation loss: 0.09636155, Gradient norm: 2.56355446
INFO:root:[   18] Training loss: 0.07027849, Validation loss: 0.11455485, Gradient norm: 2.86862514
INFO:root:[   19] Training loss: 0.06919590, Validation loss: 0.11527176, Gradient norm: 2.56207419
INFO:root:[   20] Training loss: 0.06883757, Validation loss: 0.10931581, Gradient norm: 2.78559976
INFO:root:[   21] Training loss: 0.06666525, Validation loss: 0.09541979, Gradient norm: 2.71323749
INFO:root:[   22] Training loss: 0.06558258, Validation loss: 0.09599736, Gradient norm: 2.55532128
INFO:root:[   23] Training loss: 0.06571759, Validation loss: 0.09367316, Gradient norm: 2.82577378
INFO:root:[   24] Training loss: 0.06479844, Validation loss: 0.09241749, Gradient norm: 2.59306212
INFO:root:[   25] Training loss: 0.06129604, Validation loss: 0.09153450, Gradient norm: 2.38206266
INFO:root:[   26] Training loss: 0.05803215, Validation loss: 0.09707230, Gradient norm: 2.50527620
INFO:root:[   27] Training loss: 0.05993891, Validation loss: 0.09414283, Gradient norm: 2.39567113
INFO:root:[   28] Training loss: 0.05931211, Validation loss: 0.10829660, Gradient norm: 2.57962590
INFO:root:[   29] Training loss: 0.06166590, Validation loss: 0.08977390, Gradient norm: 2.61406307
INFO:root:[   30] Training loss: 0.05937410, Validation loss: 0.08501584, Gradient norm: 2.61950374
INFO:root:[   31] Training loss: 0.05864990, Validation loss: 0.09107034, Gradient norm: 2.53082654
INFO:root:[   32] Training loss: 0.05746463, Validation loss: 0.08993991, Gradient norm: 2.43576636
INFO:root:[   33] Training loss: 0.05693919, Validation loss: 0.08972062, Gradient norm: 2.50856297
INFO:root:[   34] Training loss: 0.05530918, Validation loss: 0.09375834, Gradient norm: 2.39510714
INFO:root:[   35] Training loss: 0.05653474, Validation loss: 0.08705156, Gradient norm: 2.58191239
INFO:root:[   36] Training loss: 0.05483075, Validation loss: 0.09038347, Gradient norm: 2.13781938
INFO:root:[   37] Training loss: 0.05573655, Validation loss: 0.09061306, Gradient norm: 2.42759573
INFO:root:[   38] Training loss: 0.05279069, Validation loss: 0.09992448, Gradient norm: 2.32408046
INFO:root:[   39] Training loss: 0.05397316, Validation loss: 0.11177735, Gradient norm: 2.41529584
INFO:root:[   40] Training loss: 0.05505529, Validation loss: 0.09884244, Gradient norm: 2.50108965
INFO:root:[   41] Training loss: 0.05520587, Validation loss: 0.08908027, Gradient norm: 2.49661112
INFO:root:[   42] Training loss: 0.05364233, Validation loss: 0.08960994, Gradient norm: 2.55722964
INFO:root:[   43] Training loss: 0.05181894, Validation loss: 0.08596574, Gradient norm: 2.33062787
INFO:root:[   44] Training loss: 0.05270033, Validation loss: 0.10351913, Gradient norm: 2.63296235
INFO:root:[   45] Training loss: 0.05273132, Validation loss: 0.08377708, Gradient norm: 2.58011501
INFO:root:[   46] Training loss: 0.05357595, Validation loss: 0.10241427, Gradient norm: 2.62948443
INFO:root:[   47] Training loss: 0.05044646, Validation loss: 0.08608771, Gradient norm: 2.46517566
INFO:root:[   48] Training loss: 0.05294512, Validation loss: 0.08742121, Gradient norm: 2.71443247
INFO:root:[   49] Training loss: 0.05112270, Validation loss: 0.08833464, Gradient norm: 2.46972825
INFO:root:[   50] Training loss: 0.04914616, Validation loss: 0.09120234, Gradient norm: 2.45261341
INFO:root:[   51] Training loss: 0.05228315, Validation loss: 0.09474353, Gradient norm: 2.51854902
INFO:root:[   52] Training loss: 0.05186818, Validation loss: 0.08461563, Gradient norm: 2.52188106
INFO:root:[   53] Training loss: 0.05072153, Validation loss: 0.09674202, Gradient norm: 2.57363303
INFO:root:[   54] Training loss: 0.04913090, Validation loss: 0.10117704, Gradient norm: 2.44703311
INFO:root:[   55] Training loss: 0.04939595, Validation loss: 0.08630854, Gradient norm: 2.59345729
INFO:root:[   56] Training loss: 0.04912056, Validation loss: 0.08606376, Gradient norm: 2.52800312
INFO:root:[   57] Training loss: 0.04898551, Validation loss: 0.08713108, Gradient norm: 2.40879048
INFO:root:[   58] Training loss: 0.04666360, Validation loss: 0.08872031, Gradient norm: 2.43941894
INFO:root:[   59] Training loss: 0.04572478, Validation loss: 0.09163168, Gradient norm: 2.18699327
INFO:root:[   60] Training loss: 0.04772562, Validation loss: 0.08823713, Gradient norm: 2.56427809
INFO:root:[   61] Training loss: 0.04921292, Validation loss: 0.08981353, Gradient norm: 2.40656092
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 2123.453s.
INFO:root:Emptying the cuda cache took 0.026s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03356
INFO:root:EnergyScoreTrain: 0.02451
INFO:root:CoverageTrain: 0.89747
INFO:root:IntervalWidthTrain: 0.02062
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07703
INFO:root:EnergyScoreValidation: 0.06001
INFO:root:CoverageValidation: 0.57444
INFO:root:IntervalWidthValidation: 0.02395
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07845
INFO:root:EnergyScoreTest: 0.0614
INFO:root:CoverageTest: 0.57111
INFO:root:IntervalWidthTest: 0.02392
INFO:root:###6 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26668814, Validation loss: 0.19338703, Gradient norm: 3.34324891
INFO:root:[    2] Training loss: 0.15702028, Validation loss: 0.19258944, Gradient norm: 3.66200471
INFO:root:[    3] Training loss: 0.12579327, Validation loss: 0.13621087, Gradient norm: 3.23815150
INFO:root:[    4] Training loss: 0.11246635, Validation loss: 0.11722462, Gradient norm: 3.63644360
INFO:root:[    5] Training loss: 0.10434434, Validation loss: 0.11713118, Gradient norm: 3.40315128
INFO:root:[    6] Training loss: 0.09953585, Validation loss: 0.10443933, Gradient norm: 3.24282239
INFO:root:[    7] Training loss: 0.09536588, Validation loss: 0.11669003, Gradient norm: 3.16993632
INFO:root:[    8] Training loss: 0.09309003, Validation loss: 0.10630368, Gradient norm: 3.48898917
INFO:root:[    9] Training loss: 0.08586653, Validation loss: 0.10723926, Gradient norm: 3.01204618
INFO:root:[   10] Training loss: 0.08358644, Validation loss: 0.13119004, Gradient norm: 3.24355289
INFO:root:[   11] Training loss: 0.08269118, Validation loss: 0.11749079, Gradient norm: 2.99650348
INFO:root:[   12] Training loss: 0.08060575, Validation loss: 0.10510146, Gradient norm: 3.19760946
INFO:root:[   13] Training loss: 0.07927897, Validation loss: 0.11315047, Gradient norm: 3.18495056
INFO:root:[   14] Training loss: 0.07496367, Validation loss: 0.09353856, Gradient norm: 2.81592938
INFO:root:[   15] Training loss: 0.07457444, Validation loss: 0.09104314, Gradient norm: 2.95568576
INFO:root:[   16] Training loss: 0.07225378, Validation loss: 0.09437792, Gradient norm: 2.83255599
INFO:root:[   17] Training loss: 0.07050678, Validation loss: 0.10793326, Gradient norm: 2.74351309
INFO:root:[   18] Training loss: 0.07309405, Validation loss: 0.09608114, Gradient norm: 3.04651503
INFO:root:[   19] Training loss: 0.07118126, Validation loss: 0.09951555, Gradient norm: 2.95391580
INFO:root:[   20] Training loss: 0.06732822, Validation loss: 0.09538569, Gradient norm: 2.83315053
INFO:root:[   21] Training loss: 0.06515639, Validation loss: 0.09930786, Gradient norm: 2.66065554
INFO:root:[   22] Training loss: 0.06644949, Validation loss: 0.10229253, Gradient norm: 2.77453277
INFO:root:[   23] Training loss: 0.06270990, Validation loss: 0.09026977, Gradient norm: 2.45375219
INFO:root:[   24] Training loss: 0.06300948, Validation loss: 0.09692847, Gradient norm: 2.70411407
INFO:root:[   25] Training loss: 0.06864095, Validation loss: 0.09645066, Gradient norm: 2.98667464
INFO:root:[   26] Training loss: 0.06219973, Validation loss: 0.09889836, Gradient norm: 2.66133708
INFO:root:[   27] Training loss: 0.06312729, Validation loss: 0.10157292, Gradient norm: 2.75723422
INFO:root:[   28] Training loss: 0.06070028, Validation loss: 0.10911468, Gradient norm: 2.62629752
INFO:root:[   29] Training loss: 0.06308005, Validation loss: 0.09540598, Gradient norm: 2.77922071
INFO:root:[   30] Training loss: 0.06059179, Validation loss: 0.09802969, Gradient norm: 2.52466722
INFO:root:[   31] Training loss: 0.06130701, Validation loss: 0.09406991, Gradient norm: 3.00838094
INFO:root:[   32] Training loss: 0.05932695, Validation loss: 0.11109340, Gradient norm: 2.50585923
INFO:root:[   33] Training loss: 0.05953989, Validation loss: 0.11279312, Gradient norm: 2.67419214
INFO:root:[   34] Training loss: 0.05992981, Validation loss: 0.10688388, Gradient norm: 2.79007102
INFO:root:[   35] Training loss: 0.05697014, Validation loss: 0.09337284, Gradient norm: 2.66326187
INFO:root:[   36] Training loss: 0.05970737, Validation loss: 0.11434672, Gradient norm: 2.71500520
INFO:root:[   37] Training loss: 0.05701227, Validation loss: 0.10591281, Gradient norm: 2.71434363
INFO:root:[   38] Training loss: 0.05657482, Validation loss: 0.10493976, Gradient norm: 2.33608221
INFO:root:[   39] Training loss: 0.05558488, Validation loss: 0.10731895, Gradient norm: 2.53781958
INFO:root:[   40] Training loss: 0.05615334, Validation loss: 0.10769903, Gradient norm: 2.51022541
INFO:root:[   41] Training loss: 0.05662712, Validation loss: 0.09803142, Gradient norm: 2.84223657
INFO:root:[   42] Training loss: 0.05481471, Validation loss: 0.10248283, Gradient norm: 2.68721995
INFO:root:[   43] Training loss: 0.05417016, Validation loss: 0.10504989, Gradient norm: 2.42031091
INFO:root:[   44] Training loss: 0.05267876, Validation loss: 0.10446924, Gradient norm: 2.70091280
INFO:root:[   45] Training loss: 0.05493170, Validation loss: 0.10769622, Gradient norm: 2.71985799
INFO:root:[   46] Training loss: 0.05485823, Validation loss: 0.09995747, Gradient norm: 2.55849359
INFO:root:[   47] Training loss: 0.05216046, Validation loss: 0.10527667, Gradient norm: 2.61710338
INFO:root:[   48] Training loss: 0.05077909, Validation loss: 0.09365694, Gradient norm: 2.34055687
INFO:root:[   49] Training loss: 0.05297145, Validation loss: 0.09321977, Gradient norm: 2.44448652
INFO:root:[   50] Training loss: 0.04916455, Validation loss: 0.09966877, Gradient norm: 2.46296569
INFO:root:[   51] Training loss: 0.05282439, Validation loss: 0.09135346, Gradient norm: 2.80723242
INFO:root:[   52] Training loss: 0.05088962, Validation loss: 0.10272176, Gradient norm: 2.36314378
INFO:root:[   53] Training loss: 0.05163950, Validation loss: 0.09407265, Gradient norm: 2.68198417
INFO:root:[   54] Training loss: 0.05184013, Validation loss: 0.10781168, Gradient norm: 2.80693799
INFO:root:[   55] Training loss: 0.05155385, Validation loss: 0.10194317, Gradient norm: 2.51936641
INFO:root:[   56] Training loss: 0.05071268, Validation loss: 0.10452047, Gradient norm: 2.56623158
INFO:root:[   57] Training loss: 0.05121654, Validation loss: 0.10742840, Gradient norm: 2.69712994
