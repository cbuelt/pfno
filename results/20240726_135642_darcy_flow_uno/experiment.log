INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06512139, Validation loss: 0.03518820, Gradient norm: 1.01610049
INFO:root:[    2] Training loss: 0.03376755, Validation loss: 0.05011676, Gradient norm: 0.84470862
INFO:root:[    3] Training loss: 0.02986430, Validation loss: 0.03027089, Gradient norm: 0.81197624
INFO:root:[    4] Training loss: 0.02665337, Validation loss: 0.03023945, Gradient norm: 0.80808782
INFO:root:[    5] Training loss: 0.02256413, Validation loss: 0.02724972, Gradient norm: 0.56505811
INFO:root:[    6] Training loss: 0.02290157, Validation loss: 0.03040398, Gradient norm: 0.68049550
INFO:root:[    7] Training loss: 0.02297635, Validation loss: 0.02642562, Gradient norm: 0.68282401
INFO:root:[    8] Training loss: 0.01937842, Validation loss: 0.02674355, Gradient norm: 0.51990676
INFO:root:[    9] Training loss: 0.02095944, Validation loss: 0.03802914, Gradient norm: 0.58902579
INFO:root:[   10] Training loss: 0.01921808, Validation loss: 0.02846810, Gradient norm: 0.53707004
INFO:root:[   11] Training loss: 0.01915867, Validation loss: 0.02949327, Gradient norm: 0.58627696
INFO:root:[   12] Training loss: 0.01690791, Validation loss: 0.02821182, Gradient norm: 0.41757455
INFO:root:[   13] Training loss: 0.02043161, Validation loss: 0.02869012, Gradient norm: 0.63567940
INFO:root:[   14] Training loss: 0.01799876, Validation loss: 0.02910502, Gradient norm: 0.55003883
INFO:root:[   15] Training loss: 0.01711865, Validation loss: 0.02913832, Gradient norm: 0.49570301
INFO:root:[   16] Training loss: 0.01708239, Validation loss: 0.02974223, Gradient norm: 0.46062056
INFO:root:[   17] Training loss: 0.01685893, Validation loss: 0.02943871, Gradient norm: 0.48905225
INFO:root:[   18] Training loss: 0.01624764, Validation loss: 0.02851010, Gradient norm: 0.43435202
INFO:root:[   19] Training loss: 0.01590139, Validation loss: 0.03039963, Gradient norm: 0.51231503
INFO:root:[   20] Training loss: 0.01573706, Validation loss: 0.03507880, Gradient norm: 0.44465800
INFO:root:[   21] Training loss: 0.01633150, Validation loss: 0.02828084, Gradient norm: 0.51994415
INFO:root:[   22] Training loss: 0.01560431, Validation loss: 0.03377164, Gradient norm: 0.51849805
INFO:root:[   23] Training loss: 0.01610744, Validation loss: 0.02983446, Gradient norm: 0.49580977
INFO:root:[   24] Training loss: 0.01377454, Validation loss: 0.02980019, Gradient norm: 0.32877297
INFO:root:[   25] Training loss: 0.01441580, Validation loss: 0.03065280, Gradient norm: 0.40855399
INFO:root:[   26] Training loss: 0.01519008, Validation loss: 0.02933649, Gradient norm: 0.47463999
INFO:root:[   27] Training loss: 0.01471102, Validation loss: 0.03342907, Gradient norm: 0.45397436
INFO:root:[   28] Training loss: 0.01511658, Validation loss: 0.03012035, Gradient norm: 0.46578764
INFO:root:[   29] Training loss: 0.01439995, Validation loss: 0.03442158, Gradient norm: 0.44592810
INFO:root:[   30] Training loss: 0.01474430, Validation loss: 0.03230985, Gradient norm: 0.43921725
INFO:root:[   31] Training loss: 0.01396324, Validation loss: 0.03205695, Gradient norm: 0.38602045
INFO:root:[   32] Training loss: 0.01202487, Validation loss: 0.03830448, Gradient norm: 0.28172179
INFO:root:[   33] Training loss: 0.01454240, Validation loss: 0.03134151, Gradient norm: 0.46874669
INFO:root:[   34] Training loss: 0.01299386, Validation loss: 0.03435519, Gradient norm: 0.40837907
INFO:root:[   35] Training loss: 0.01349036, Validation loss: 0.03515188, Gradient norm: 0.41794313
INFO:root:[   36] Training loss: 0.01280935, Validation loss: 0.03067498, Gradient norm: 0.40597476
INFO:root:[   37] Training loss: 0.01306351, Validation loss: 0.03345819, Gradient norm: 0.41959606
INFO:root:[   38] Training loss: 0.01350325, Validation loss: 0.03346483, Gradient norm: 0.46643720
INFO:root:[   39] Training loss: 0.01292588, Validation loss: 0.03579029, Gradient norm: 0.42974945
INFO:root:[   40] Training loss: 0.01191228, Validation loss: 0.03637590, Gradient norm: 0.34304470
INFO:root:[   41] Training loss: 0.01330511, Validation loss: 0.03559263, Gradient norm: 0.44107023
INFO:root:[   42] Training loss: 0.01207517, Validation loss: 0.03413399, Gradient norm: 0.36333643
INFO:root:[   43] Training loss: 0.01275085, Validation loss: 0.03325758, Gradient norm: 0.39414622
INFO:root:[   44] Training loss: 0.01190568, Validation loss: 0.03350067, Gradient norm: 0.36889197
INFO:root:[   45] Training loss: 0.01165508, Validation loss: 0.03233797, Gradient norm: 0.34538901
INFO:root:[   46] Training loss: 0.01265020, Validation loss: 0.03426923, Gradient norm: 0.43603580
INFO:root:[   47] Training loss: 0.01144009, Validation loss: 0.03528323, Gradient norm: 0.37758338
INFO:root:[   48] Training loss: 0.01195389, Validation loss: 0.03372996, Gradient norm: 0.36738967
INFO:root:[   49] Training loss: 0.01123841, Validation loss: 0.03493518, Gradient norm: 0.32345188
INFO:root:[   50] Training loss: 0.01127548, Validation loss: 0.03586010, Gradient norm: 0.34968815
INFO:root:[   51] Training loss: 0.01116841, Validation loss: 0.03410058, Gradient norm: 0.31332737
INFO:root:[   52] Training loss: 0.01160250, Validation loss: 0.03585692, Gradient norm: 0.38386778
INFO:root:[   53] Training loss: 0.01065821, Validation loss: 0.03763928, Gradient norm: 0.27369306
INFO:root:[   54] Training loss: 0.01102286, Validation loss: 0.03613872, Gradient norm: 0.29261099
INFO:root:[   55] Training loss: 0.01053590, Validation loss: 0.03615135, Gradient norm: 0.30267641
INFO:root:[   56] Training loss: 0.01211237, Validation loss: 0.03286657, Gradient norm: 0.39263856
INFO:root:[   57] Training loss: 0.01097622, Validation loss: 0.03618745, Gradient norm: 0.35841970
INFO:root:[   58] Training loss: 0.01042384, Validation loss: 0.04061667, Gradient norm: 0.28223267
INFO:root:[   59] Training loss: 0.01110238, Validation loss: 0.03635413, Gradient norm: 0.32415285
INFO:root:[   60] Training loss: 0.01135472, Validation loss: 0.03450932, Gradient norm: 0.38662298
INFO:root:[   61] Training loss: 0.01059055, Validation loss: 0.03810586, Gradient norm: 0.32235683
INFO:root:[   62] Training loss: 0.01095461, Validation loss: 0.04012541, Gradient norm: 0.32580584
INFO:root:[   63] Training loss: 0.30647137, Validation loss: 0.07188978, Gradient norm: 10.35852670
INFO:root:[   64] Training loss: 0.05358960, Validation loss: 0.05323470, Gradient norm: 1.59927115
INFO:root:[   65] Training loss: 0.05442569, Validation loss: 0.05455146, Gradient norm: 4.05754390
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 3828.194s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02497
INFO:root:EnergyScoreTrain: 0.0183
INFO:root:CoverageTrain: 0.83444
INFO:root:IntervalWidthTrain: 0.07721
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03559
INFO:root:EnergyScoreValidation: 0.02676
INFO:root:CoverageValidation: 0.61516
INFO:root:IntervalWidthValidation: 0.06675
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0355
INFO:root:EnergyScoreTest: 0.02669
INFO:root:CoverageTest: 0.61469
INFO:root:IntervalWidthTest: 0.0668
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 150994944
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06072772, Validation loss: 0.03759336, Gradient norm: 0.72366608
INFO:root:[    2] Training loss: 0.03331319, Validation loss: 0.03810876, Gradient norm: 0.58446395
INFO:root:[    3] Training loss: 0.02821159, Validation loss: 0.03494136, Gradient norm: 0.53702616
INFO:root:[    4] Training loss: 0.02574692, Validation loss: 0.02900950, Gradient norm: 0.51608939
INFO:root:[    5] Training loss: 0.02286930, Validation loss: 0.03384419, Gradient norm: 0.48317503
INFO:root:[    6] Training loss: 0.02267188, Validation loss: 0.04202715, Gradient norm: 0.54147975
INFO:root:[    7] Training loss: 0.02112939, Validation loss: 0.02753904, Gradient norm: 0.45771893
INFO:root:[    8] Training loss: 0.01867348, Validation loss: 0.03033629, Gradient norm: 0.39359374
INFO:root:[    9] Training loss: 0.01891518, Validation loss: 0.02470862, Gradient norm: 0.41768111
INFO:root:[   10] Training loss: 0.01933007, Validation loss: 0.03065771, Gradient norm: 0.45929230
INFO:root:[   11] Training loss: 0.01642000, Validation loss: 0.02507285, Gradient norm: 0.32246552
INFO:root:[   12] Training loss: 0.01770044, Validation loss: 0.03447276, Gradient norm: 0.39603203
INFO:root:[   13] Training loss: 0.01843731, Validation loss: 0.03534991, Gradient norm: 0.44834047
INFO:root:[   14] Training loss: 0.01801755, Validation loss: 0.02754728, Gradient norm: 0.43837416
INFO:root:[   15] Training loss: 0.01533522, Validation loss: 0.02636606, Gradient norm: 0.30625713
INFO:root:[   16] Training loss: 0.01593066, Validation loss: 0.02846384, Gradient norm: 0.39666669
INFO:root:[   17] Training loss: 0.01557867, Validation loss: 0.03156287, Gradient norm: 0.35168301
INFO:root:[   18] Training loss: 0.01515331, Validation loss: 0.02974385, Gradient norm: 0.37486580
INFO:root:[   19] Training loss: 0.01466085, Validation loss: 0.02609934, Gradient norm: 0.30899122
INFO:root:[   20] Training loss: 0.01643791, Validation loss: 0.03408571, Gradient norm: 0.42429686
INFO:root:[   21] Training loss: 0.01516076, Validation loss: 0.02975882, Gradient norm: 0.34476426
INFO:root:[   22] Training loss: 0.01423864, Validation loss: 0.03034927, Gradient norm: 0.33219317
INFO:root:[   23] Training loss: 0.01429482, Validation loss: 0.03429587, Gradient norm: 0.33075610
INFO:root:[   24] Training loss: 0.01486712, Validation loss: 0.02921957, Gradient norm: 0.38969071
INFO:root:[   25] Training loss: 0.01419165, Validation loss: 0.03217646, Gradient norm: 0.35039114
INFO:root:[   26] Training loss: 0.01485293, Validation loss: 0.03042317, Gradient norm: 0.35760963
INFO:root:[   27] Training loss: 0.01453533, Validation loss: 0.02736390, Gradient norm: 0.37646660
INFO:root:[   28] Training loss: 0.01388872, Validation loss: 0.03002343, Gradient norm: 0.33782704
INFO:root:[   29] Training loss: 0.01228819, Validation loss: 0.03429361, Gradient norm: 0.24948321
INFO:root:[   30] Training loss: 0.01401404, Validation loss: 0.03467073, Gradient norm: 0.35970709
INFO:root:[   31] Training loss: 0.01282409, Validation loss: 0.03258786, Gradient norm: 0.31142664
INFO:root:[   32] Training loss: 0.01350616, Validation loss: 0.03558210, Gradient norm: 0.33216529
INFO:root:[   33] Training loss: 0.01381954, Validation loss: 0.03101315, Gradient norm: 0.32330811
INFO:root:[   34] Training loss: 0.01320589, Validation loss: 0.03438630, Gradient norm: 0.32383889
INFO:root:[   35] Training loss: 0.01299871, Validation loss: 0.03369249, Gradient norm: 0.32857292
INFO:root:[   36] Training loss: 0.01246824, Validation loss: 0.03525718, Gradient norm: 0.31486750
INFO:root:[   37] Training loss: 0.01189847, Validation loss: 0.03105500, Gradient norm: 0.28356398
INFO:root:[   38] Training loss: 0.01300186, Validation loss: 0.03603370, Gradient norm: 0.33961495
INFO:root:[   39] Training loss: 0.01178827, Validation loss: 0.03369309, Gradient norm: 0.27725989
INFO:root:[   40] Training loss: 0.01268907, Validation loss: 0.03379909, Gradient norm: 0.34277170
INFO:root:[   41] Training loss: 0.01162434, Validation loss: 0.03588815, Gradient norm: 0.27017383
INFO:root:[   42] Training loss: 0.01163986, Validation loss: 0.03314065, Gradient norm: 0.26477204
INFO:root:[   43] Training loss: 0.01189302, Validation loss: 0.03019447, Gradient norm: 0.33011729
INFO:root:[   44] Training loss: 0.01246921, Validation loss: 0.03516359, Gradient norm: 0.33499743
INFO:root:[   45] Training loss: 0.01235516, Validation loss: 0.03101260, Gradient norm: 0.33537385
INFO:root:[   46] Training loss: 0.01084739, Validation loss: 0.03500862, Gradient norm: 0.21121371
INFO:root:[   47] Training loss: 0.01190535, Validation loss: 0.03516443, Gradient norm: 0.30758540
INFO:root:[   48] Training loss: 0.01160066, Validation loss: 0.03308306, Gradient norm: 0.28812841
INFO:root:[   49] Training loss: 0.01183517, Validation loss: 0.03151795, Gradient norm: 0.31767901
INFO:root:[   50] Training loss: 0.01162710, Validation loss: 0.02971784, Gradient norm: 0.31070898
INFO:root:[   51] Training loss: 0.01151783, Validation loss: 0.03108789, Gradient norm: 0.26918977
INFO:root:[   52] Training loss: 0.01224639, Validation loss: 0.03711728, Gradient norm: 0.35888535
INFO:root:[   53] Training loss: 0.01067823, Validation loss: 0.03495579, Gradient norm: 0.26384776
INFO:root:[   54] Training loss: 0.01127785, Validation loss: 0.03104042, Gradient norm: 0.30779734
INFO:root:[   55] Training loss: 0.01055151, Validation loss: 0.03078963, Gradient norm: 0.22158063
INFO:root:[   56] Training loss: 0.01180277, Validation loss: 0.03172213, Gradient norm: 0.33552441
INFO:root:[   57] Training loss: 0.01127966, Validation loss: 0.03193667, Gradient norm: 0.30610193
INFO:root:[   58] Training loss: 0.01068915, Validation loss: 0.03440793, Gradient norm: 0.23383283
INFO:root:[   59] Training loss: 0.01061958, Validation loss: 0.03036251, Gradient norm: 0.23212099
INFO:root:[   60] Training loss: 0.01084819, Validation loss: 0.03180271, Gradient norm: 0.26171426
INFO:root:[   61] Training loss: 0.01066678, Validation loss: 0.03157497, Gradient norm: 0.27823287
INFO:root:[   62] Training loss: 0.01155883, Validation loss: 0.03212654, Gradient norm: 0.32032430
INFO:root:[   63] Training loss: 0.01088339, Validation loss: 0.03876630, Gradient norm: 0.28500411
INFO:root:[   64] Training loss: 0.11201381, Validation loss: 0.11171443, Gradient norm: 1.93301308
INFO:root:[   65] Training loss: 0.11369270, Validation loss: 0.11185626, Gradient norm: 0.04828175
INFO:root:[   66] Training loss: 0.11363214, Validation loss: 0.11331430, Gradient norm: 0.04967561
INFO:root:[   67] Training loss: 0.11376332, Validation loss: 0.11339703, Gradient norm: 0.05035791
INFO:root:[   68] Training loss: 0.11377403, Validation loss: 0.11235131, Gradient norm: 0.05583237
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 3898.496s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02165
INFO:root:EnergyScoreTrain: 0.01585
INFO:root:CoverageTrain: 0.97018
INFO:root:IntervalWidthTrain: 0.09867
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03349
INFO:root:EnergyScoreValidation: 0.02455
INFO:root:CoverageValidation: 0.82285
INFO:root:IntervalWidthValidation: 0.09076
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03371
INFO:root:EnergyScoreTest: 0.02477
INFO:root:CoverageTest: 0.81993
INFO:root:IntervalWidthTest: 0.09078
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 236978176
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05323615, Validation loss: 0.04020356, Gradient norm: 0.66185089
INFO:root:[    2] Training loss: 0.02923513, Validation loss: 0.03099819, Gradient norm: 0.51505154
INFO:root:[    3] Training loss: 0.02741813, Validation loss: 0.02915870, Gradient norm: 0.51588119
INFO:root:[    4] Training loss: 0.02418055, Validation loss: 0.02617919, Gradient norm: 0.47300728
INFO:root:[    5] Training loss: 0.02216422, Validation loss: 0.02962566, Gradient norm: 0.45455073
INFO:root:[    6] Training loss: 0.02115129, Validation loss: 0.02494865, Gradient norm: 0.40001613
INFO:root:[    7] Training loss: 0.01970437, Validation loss: 0.02388622, Gradient norm: 0.36017824
INFO:root:[    8] Training loss: 0.01832118, Validation loss: 0.02457770, Gradient norm: 0.33589035
INFO:root:[    9] Training loss: 0.01875481, Validation loss: 0.02816573, Gradient norm: 0.40301036
INFO:root:[   10] Training loss: 0.01713315, Validation loss: 0.02442642, Gradient norm: 0.31014862
INFO:root:[   11] Training loss: 0.01872320, Validation loss: 0.03202751, Gradient norm: 0.41369145
INFO:root:[   12] Training loss: 0.01741713, Validation loss: 0.02588529, Gradient norm: 0.35228320
INFO:root:[   13] Training loss: 0.01603771, Validation loss: 0.02310189, Gradient norm: 0.29035309
INFO:root:[   14] Training loss: 0.01795109, Validation loss: 0.02511191, Gradient norm: 0.37833845
INFO:root:[   15] Training loss: 0.01533806, Validation loss: 0.03069642, Gradient norm: 0.30521425
INFO:root:[   16] Training loss: 0.01679015, Validation loss: 0.02666990, Gradient norm: 0.39598290
INFO:root:[   17] Training loss: 0.01636357, Validation loss: 0.02926515, Gradient norm: 0.37107795
INFO:root:[   18] Training loss: 0.01513666, Validation loss: 0.02535134, Gradient norm: 0.32107278
INFO:root:[   19] Training loss: 0.01523350, Validation loss: 0.03265023, Gradient norm: 0.33395937
INFO:root:[   20] Training loss: 0.01499501, Validation loss: 0.02508419, Gradient norm: 0.35035381
INFO:root:[   21] Training loss: 0.01415747, Validation loss: 0.02813696, Gradient norm: 0.30465335
INFO:root:[   22] Training loss: 0.01486783, Validation loss: 0.03231728, Gradient norm: 0.34002131
INFO:root:[   23] Training loss: 0.01415821, Validation loss: 0.02978199, Gradient norm: 0.33465886
INFO:root:[   24] Training loss: 0.01381139, Validation loss: 0.02684050, Gradient norm: 0.31034261
INFO:root:[   25] Training loss: 0.01442126, Validation loss: 0.02505630, Gradient norm: 0.34204610
INFO:root:[   26] Training loss: 0.01405107, Validation loss: 0.02802846, Gradient norm: 0.33490578
INFO:root:[   27] Training loss: 0.01310127, Validation loss: 0.02632129, Gradient norm: 0.31891207
INFO:root:[   28] Training loss: 0.01302123, Validation loss: 0.02745339, Gradient norm: 0.31855032
INFO:root:[   29] Training loss: 0.01294441, Validation loss: 0.02791389, Gradient norm: 0.27706944
INFO:root:[   30] Training loss: 0.01351585, Validation loss: 0.03019579, Gradient norm: 0.32433418
INFO:root:[   31] Training loss: 0.01356275, Validation loss: 0.02913563, Gradient norm: 0.33755834
INFO:root:[   32] Training loss: 0.01291580, Validation loss: 0.02709495, Gradient norm: 0.31163885
INFO:root:[   33] Training loss: 0.01386445, Validation loss: 0.02943040, Gradient norm: 0.33225654
INFO:root:[   34] Training loss: 0.01342190, Validation loss: 0.03014180, Gradient norm: 0.35698839
INFO:root:[   35] Training loss: 0.01280669, Validation loss: 0.02857875, Gradient norm: 0.32816278
INFO:root:[   36] Training loss: 0.01229832, Validation loss: 0.03125168, Gradient norm: 0.25934108
INFO:root:[   37] Training loss: 0.01219352, Validation loss: 0.02712120, Gradient norm: 0.30296706
INFO:root:[   38] Training loss: 0.01281821, Validation loss: 0.02939999, Gradient norm: 0.31952067
INFO:root:[   39] Training loss: 0.01211671, Validation loss: 0.02932626, Gradient norm: 0.30030212
INFO:root:[   40] Training loss: 0.01244431, Validation loss: 0.02902476, Gradient norm: 0.30369769
INFO:root:[   41] Training loss: 0.01180047, Validation loss: 0.03135741, Gradient norm: 0.26224173
INFO:root:[   42] Training loss: 0.01203799, Validation loss: 0.02784586, Gradient norm: 0.26683639
INFO:root:[   43] Training loss: 0.01128863, Validation loss: 0.02773385, Gradient norm: 0.25244058
INFO:root:[   44] Training loss: 0.01216055, Validation loss: 0.02606884, Gradient norm: 0.28700237
INFO:root:[   45] Training loss: 0.01195796, Validation loss: 0.03017841, Gradient norm: 0.32123205
INFO:root:[   46] Training loss: 0.01201462, Validation loss: 0.03147237, Gradient norm: 0.25821065
INFO:root:[   47] Training loss: 0.01182859, Validation loss: 0.03854854, Gradient norm: 0.27024444
INFO:root:[   48] Training loss: 0.01255216, Validation loss: 0.02946767, Gradient norm: 0.33102629
INFO:root:[   49] Training loss: 0.01214489, Validation loss: 0.03314635, Gradient norm: 0.28452264
INFO:root:[   50] Training loss: 0.01160809, Validation loss: 0.03050958, Gradient norm: 0.25859957
INFO:root:[   51] Training loss: 0.01160277, Validation loss: 0.03160075, Gradient norm: 0.27420983
INFO:root:[   52] Training loss: 0.01136000, Validation loss: 0.02794211, Gradient norm: 0.28507073
INFO:root:[   53] Training loss: 0.01134054, Validation loss: 0.02981078, Gradient norm: 0.27597223
INFO:root:[   54] Training loss: 0.01179188, Validation loss: 0.03075124, Gradient norm: 0.30428567
INFO:root:[   55] Training loss: 0.01138798, Validation loss: 0.02760424, Gradient norm: 0.26679020
INFO:root:[   56] Training loss: 0.01133627, Validation loss: 0.03019160, Gradient norm: 0.26571077
INFO:root:[   57] Training loss: 0.01148376, Validation loss: 0.03040992, Gradient norm: 0.26577290
INFO:root:[   58] Training loss: 0.01192856, Validation loss: 0.03149538, Gradient norm: 0.29583747
INFO:root:[   59] Training loss: 0.01163396, Validation loss: 0.02931082, Gradient norm: 0.28402196
INFO:root:[   60] Training loss: 0.01132327, Validation loss: 0.02955846, Gradient norm: 0.22767025
INFO:root:[   61] Training loss: 0.01115049, Validation loss: 0.02828846, Gradient norm: 0.28616539
INFO:root:[   62] Training loss: 0.01153271, Validation loss: 0.03036562, Gradient norm: 0.32520926
INFO:root:[   63] Training loss: 0.01160242, Validation loss: 0.03093765, Gradient norm: 0.29286588
INFO:root:[   64] Training loss: 0.01136640, Validation loss: 0.02966877, Gradient norm: 0.26943755
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 3668.121s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0202
INFO:root:EnergyScoreTrain: 0.01467
INFO:root:CoverageTrain: 0.97936
INFO:root:IntervalWidthTrain: 0.10011
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03233
INFO:root:EnergyScoreValidation: 0.02342
INFO:root:CoverageValidation: 0.80829
INFO:root:IntervalWidthValidation: 0.09189
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03255
INFO:root:EnergyScoreTest: 0.02362
INFO:root:CoverageTest: 0.80783
INFO:root:IntervalWidthTest: 0.092
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 134217728
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04961992, Validation loss: 0.03551142, Gradient norm: 0.63676202
INFO:root:[    2] Training loss: 0.02951964, Validation loss: 0.03234305, Gradient norm: 0.45468133
INFO:root:[    3] Training loss: 0.02610205, Validation loss: 0.03508079, Gradient norm: 0.44572324
INFO:root:[    4] Training loss: 0.02258522, Validation loss: 0.02305678, Gradient norm: 0.34967899
INFO:root:[    5] Training loss: 0.02224133, Validation loss: 0.02331315, Gradient norm: 0.39945578
INFO:root:[    6] Training loss: 0.02110604, Validation loss: 0.02593396, Gradient norm: 0.38351216
INFO:root:[    7] Training loss: 0.02157812, Validation loss: 0.02285107, Gradient norm: 0.41487059
INFO:root:[    8] Training loss: 0.01819394, Validation loss: 0.03254170, Gradient norm: 0.25961210
INFO:root:[    9] Training loss: 0.01883970, Validation loss: 0.02266670, Gradient norm: 0.32385291
INFO:root:[   10] Training loss: 0.01873996, Validation loss: 0.02657121, Gradient norm: 0.32434010
INFO:root:[   11] Training loss: 0.01903367, Validation loss: 0.02683291, Gradient norm: 0.37474382
INFO:root:[   12] Training loss: 0.01744831, Validation loss: 0.03328180, Gradient norm: 0.34000895
INFO:root:[   13] Training loss: 0.01713217, Validation loss: 0.02379161, Gradient norm: 0.29830503
INFO:root:[   14] Training loss: 0.01677552, Validation loss: 0.02277796, Gradient norm: 0.32293395
INFO:root:[   15] Training loss: 0.01604158, Validation loss: 0.02867415, Gradient norm: 0.31054125
INFO:root:[   16] Training loss: 0.01619322, Validation loss: 0.02474128, Gradient norm: 0.30595731
INFO:root:[   17] Training loss: 0.01497195, Validation loss: 0.02265605, Gradient norm: 0.25772940
INFO:root:[   18] Training loss: 0.01601286, Validation loss: 0.02310478, Gradient norm: 0.30443787
INFO:root:[   19] Training loss: 0.01722088, Validation loss: 0.02358935, Gradient norm: 0.37324566
INFO:root:[   20] Training loss: 0.01559518, Validation loss: 0.02471974, Gradient norm: 0.33064786
INFO:root:[   21] Training loss: 0.01444179, Validation loss: 0.02400861, Gradient norm: 0.26780021
INFO:root:[   22] Training loss: 0.01470632, Validation loss: 0.02442101, Gradient norm: 0.27530879
INFO:root:[   23] Training loss: 0.01527398, Validation loss: 0.02392449, Gradient norm: 0.32465603
INFO:root:[   24] Training loss: 0.01508475, Validation loss: 0.02346265, Gradient norm: 0.31712469
INFO:root:[   25] Training loss: 0.01434441, Validation loss: 0.02380630, Gradient norm: 0.30008544
INFO:root:[   26] Training loss: 0.01405995, Validation loss: 0.02951332, Gradient norm: 0.28138934
INFO:root:[   27] Training loss: 0.01448890, Validation loss: 0.02613106, Gradient norm: 0.30950781
INFO:root:[   28] Training loss: 0.01429601, Validation loss: 0.03396325, Gradient norm: 0.31841770
INFO:root:[   29] Training loss: 0.01449551, Validation loss: 0.03487243, Gradient norm: 0.31229692
INFO:root:[   30] Training loss: 0.01386330, Validation loss: 0.02735084, Gradient norm: 0.27905143
INFO:root:[   31] Training loss: 0.01334457, Validation loss: 0.03016156, Gradient norm: 0.26568102
INFO:root:[   32] Training loss: 0.01378877, Validation loss: 0.02703716, Gradient norm: 0.27614982
INFO:root:[   33] Training loss: 0.01295153, Validation loss: 0.02772991, Gradient norm: 0.25835814
INFO:root:[   34] Training loss: 0.01355056, Validation loss: 0.03043965, Gradient norm: 0.28296578
INFO:root:[   35] Training loss: 0.01246716, Validation loss: 0.02999795, Gradient norm: 0.24956143
INFO:root:[   36] Training loss: 0.01325926, Validation loss: 0.02996111, Gradient norm: 0.27148956
INFO:root:[   37] Training loss: 0.01327304, Validation loss: 0.03139632, Gradient norm: 0.28433534
INFO:root:[   38] Training loss: 0.01335673, Validation loss: 0.03075024, Gradient norm: 0.29767602
INFO:root:[   39] Training loss: 0.01294041, Validation loss: 0.02594916, Gradient norm: 0.28875539
INFO:root:[   40] Training loss: 0.01305219, Validation loss: 0.03007022, Gradient norm: 0.29567080
INFO:root:[   41] Training loss: 0.01232033, Validation loss: 0.03056366, Gradient norm: 0.24775766
INFO:root:[   42] Training loss: 0.01200455, Validation loss: 0.02649682, Gradient norm: 0.22737358
INFO:root:[   43] Training loss: 0.01303633, Validation loss: 0.02651178, Gradient norm: 0.29792618
INFO:root:[   44] Training loss: 0.01181258, Validation loss: 0.02873556, Gradient norm: 0.22475987
INFO:root:[   45] Training loss: 0.01184784, Validation loss: 0.02851366, Gradient norm: 0.23689782
INFO:root:[   46] Training loss: 0.01234059, Validation loss: 0.02589597, Gradient norm: 0.25601049
INFO:root:[   47] Training loss: 0.01189335, Validation loss: 0.02694913, Gradient norm: 0.24094952
INFO:root:[   48] Training loss: 0.01228119, Validation loss: 0.02457500, Gradient norm: 0.26200523
INFO:root:[   49] Training loss: 0.08895683, Validation loss: 0.06526835, Gradient norm: 2.72780981
INFO:root:[   50] Training loss: 0.04615052, Validation loss: 0.05091895, Gradient norm: 2.88836204
INFO:root:[   51] Training loss: 0.03796940, Validation loss: 0.03865475, Gradient norm: 2.35275148
INFO:root:[   52] Training loss: 0.03583958, Validation loss: 0.03797629, Gradient norm: 1.75346052
INFO:root:[   53] Training loss: 0.02970317, Validation loss: 0.02964011, Gradient norm: 1.48447342
INFO:root:[   54] Training loss: 0.02362495, Validation loss: 0.02808404, Gradient norm: 1.12101100
INFO:root:[   55] Training loss: 0.02121761, Validation loss: 0.03679827, Gradient norm: 0.95498186
INFO:root:[   56] Training loss: 0.01990681, Validation loss: 0.03241122, Gradient norm: 0.83852827
INFO:root:[   57] Training loss: 0.02106954, Validation loss: 0.02560396, Gradient norm: 1.08875495
INFO:root:[   58] Training loss: 0.01685003, Validation loss: 0.02858317, Gradient norm: 0.69208436
INFO:root:[   59] Training loss: 0.01771498, Validation loss: 0.02940695, Gradient norm: 0.84470208
INFO:root:[   60] Training loss: 0.01544623, Validation loss: 0.02984739, Gradient norm: 0.59579846
INFO:root:[   61] Training loss: 0.01661357, Validation loss: 0.03231314, Gradient norm: 0.74126449
INFO:root:[   62] Training loss: 0.01409574, Validation loss: 0.03044520, Gradient norm: 0.41083022
INFO:root:[   63] Training loss: 0.01511406, Validation loss: 0.02692464, Gradient norm: 0.57804258
INFO:root:[   64] Training loss: 0.01453472, Validation loss: 0.02783016, Gradient norm: 0.46584624
INFO:root:[   65] Training loss: 0.01354155, Validation loss: 0.03576401, Gradient norm: 0.41086218
INFO:root:[   66] Training loss: 0.01399875, Validation loss: 0.02930589, Gradient norm: 0.46626968
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 3786.356s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02057
INFO:root:EnergyScoreTrain: 0.01498
INFO:root:CoverageTrain: 0.97641
INFO:root:IntervalWidthTrain: 0.09655
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0314
INFO:root:EnergyScoreValidation: 0.02285
INFO:root:CoverageValidation: 0.79732
INFO:root:IntervalWidthValidation: 0.08732
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03189
INFO:root:EnergyScoreTest: 0.02329
INFO:root:CoverageTest: 0.79405
INFO:root:IntervalWidthTest: 0.08721
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04495650, Validation loss: 0.03212905, Gradient norm: 0.61861151
INFO:root:[    2] Training loss: 0.02966228, Validation loss: 0.03873562, Gradient norm: 0.48801979
INFO:root:[    3] Training loss: 0.02707862, Validation loss: 0.02929830, Gradient norm: 0.43062328
INFO:root:[    4] Training loss: 0.02430312, Validation loss: 0.02542979, Gradient norm: 0.41618882
INFO:root:[    5] Training loss: 0.02212594, Validation loss: 0.02486753, Gradient norm: 0.32618704
INFO:root:[    6] Training loss: 0.02180620, Validation loss: 0.02400454, Gradient norm: 0.35696092
INFO:root:[    7] Training loss: 0.02050434, Validation loss: 0.02181799, Gradient norm: 0.32028313
INFO:root:[    8] Training loss: 0.02178635, Validation loss: 0.02466907, Gradient norm: 0.38386718
INFO:root:[    9] Training loss: 0.01977092, Validation loss: 0.02508127, Gradient norm: 0.35243031
INFO:root:[   10] Training loss: 0.01892925, Validation loss: 0.02978095, Gradient norm: 0.33527353
INFO:root:[   11] Training loss: 0.01771407, Validation loss: 0.02323570, Gradient norm: 0.30740144
INFO:root:[   12] Training loss: 0.01753232, Validation loss: 0.02407732, Gradient norm: 0.31806434
INFO:root:[   13] Training loss: 0.01684929, Validation loss: 0.02531791, Gradient norm: 0.27415428
INFO:root:[   14] Training loss: 0.01786781, Validation loss: 0.02422410, Gradient norm: 0.34588741
INFO:root:[   15] Training loss: 0.01668448, Validation loss: 0.03014726, Gradient norm: 0.33059606
INFO:root:[   16] Training loss: 0.01638833, Validation loss: 0.02293430, Gradient norm: 0.31422182
INFO:root:[   17] Training loss: 0.01553805, Validation loss: 0.02403856, Gradient norm: 0.28257778
INFO:root:[   18] Training loss: 0.01560154, Validation loss: 0.02388258, Gradient norm: 0.32842804
INFO:root:[   19] Training loss: 0.01465014, Validation loss: 0.02328644, Gradient norm: 0.27336572
INFO:root:[   20] Training loss: 0.01532022, Validation loss: 0.02510256, Gradient norm: 0.30022808
INFO:root:[   21] Training loss: 0.01413696, Validation loss: 0.02574081, Gradient norm: 0.25300278
INFO:root:[   22] Training loss: 0.01574799, Validation loss: 0.02385634, Gradient norm: 0.34846987
INFO:root:[   23] Training loss: 0.01559490, Validation loss: 0.02543011, Gradient norm: 0.37081266
INFO:root:[   24] Training loss: 0.01326014, Validation loss: 0.03030344, Gradient norm: 0.21109805
INFO:root:[   25] Training loss: 0.01374042, Validation loss: 0.02471094, Gradient norm: 0.26322983
INFO:root:[   26] Training loss: 0.01357470, Validation loss: 0.02172537, Gradient norm: 0.25497452
INFO:root:[   27] Training loss: 0.01365716, Validation loss: 0.02434781, Gradient norm: 0.27055977
INFO:root:[   28] Training loss: 0.01357742, Validation loss: 0.02586631, Gradient norm: 0.25866624
INFO:root:[   29] Training loss: 0.01467105, Validation loss: 0.02647782, Gradient norm: 0.30046432
INFO:root:[   30] Training loss: 0.01290624, Validation loss: 0.03407009, Gradient norm: 0.24685254
INFO:root:[   31] Training loss: 0.01413855, Validation loss: 0.02356198, Gradient norm: 0.30270424
INFO:root:[   32] Training loss: 0.01465170, Validation loss: 0.02613624, Gradient norm: 0.30536595
INFO:root:[   33] Training loss: 0.01301839, Validation loss: 0.02663659, Gradient norm: 0.23934770
INFO:root:[   34] Training loss: 0.01365631, Validation loss: 0.02826733, Gradient norm: 0.27416788
INFO:root:[   35] Training loss: 0.01268613, Validation loss: 0.02832683, Gradient norm: 0.20210780
INFO:root:[   36] Training loss: 0.01367948, Validation loss: 0.02612008, Gradient norm: 0.30681794
INFO:root:[   37] Training loss: 0.01318691, Validation loss: 0.02416708, Gradient norm: 0.25196255
INFO:root:[   38] Training loss: 0.01331662, Validation loss: 0.02977346, Gradient norm: 0.29629252
INFO:root:[   39] Training loss: 0.01248053, Validation loss: 0.02367291, Gradient norm: 0.23465332
INFO:root:[   40] Training loss: 0.01264517, Validation loss: 0.02600507, Gradient norm: 0.24035137
INFO:root:[   41] Training loss: 0.01307970, Validation loss: 0.02365117, Gradient norm: 0.25082594
INFO:root:[   42] Training loss: 0.01232006, Validation loss: 0.02854922, Gradient norm: 0.23927791
INFO:root:[   43] Training loss: 0.01244419, Validation loss: 0.02277749, Gradient norm: 0.24260498
INFO:root:[   44] Training loss: 0.01249211, Validation loss: 0.03498759, Gradient norm: 0.25677733
INFO:root:[   45] Training loss: 0.01313643, Validation loss: 0.02697202, Gradient norm: 0.25764795
INFO:root:[   46] Training loss: 0.01321588, Validation loss: 0.02567361, Gradient norm: 0.28688204
INFO:root:[   47] Training loss: 0.01251704, Validation loss: 0.02694775, Gradient norm: 0.24014700
INFO:root:[   48] Training loss: 0.01253866, Validation loss: 0.03007534, Gradient norm: 0.24675105
INFO:root:[   49] Training loss: 0.01233171, Validation loss: 0.02386513, Gradient norm: 0.25422894
INFO:root:[   50] Training loss: 0.01182305, Validation loss: 0.02680685, Gradient norm: 0.19567867
INFO:root:[   51] Training loss: 0.01260836, Validation loss: 0.02682004, Gradient norm: 0.26087340
INFO:root:[   52] Training loss: 0.01229603, Validation loss: 0.02286001, Gradient norm: 0.22238552
INFO:root:[   53] Training loss: 0.01219820, Validation loss: 0.02528481, Gradient norm: 0.24374991
INFO:root:[   54] Training loss: 0.01184284, Validation loss: 0.02402460, Gradient norm: 0.22324563
INFO:root:[   55] Training loss: 0.01219359, Validation loss: 0.02513090, Gradient norm: 0.23496574
INFO:root:[   56] Training loss: 0.01272515, Validation loss: 0.03068179, Gradient norm: 0.27065492
INFO:root:[   57] Training loss: 0.01242942, Validation loss: 0.02699545, Gradient norm: 0.24615106
INFO:root:[   58] Training loss: 0.01241283, Validation loss: 0.02823005, Gradient norm: 0.26261979
INFO:root:[   59] Training loss: 0.01299530, Validation loss: 0.02591479, Gradient norm: 0.29578540
INFO:root:[   60] Training loss: 0.01227215, Validation loss: 0.02779360, Gradient norm: 0.25393605
INFO:root:[   61] Training loss: 0.01170659, Validation loss: 0.02895807, Gradient norm: 0.20527513
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 108766.925s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02277
INFO:root:EnergyScoreTrain: 0.01698
INFO:root:CoverageTrain: 0.99015
INFO:root:IntervalWidthTrain: 0.10833
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03025
INFO:root:EnergyScoreValidation: 0.02172
INFO:root:CoverageValidation: 0.82058
INFO:root:IntervalWidthValidation: 0.09751
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03063
INFO:root:EnergyScoreTest: 0.02201
INFO:root:CoverageTest: 0.81751
INFO:root:IntervalWidthTest: 0.09732
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05988331, Validation loss: 0.04953222, Gradient norm: 0.78754358
INFO:root:[    2] Training loss: 0.03317634, Validation loss: 0.03184387, Gradient norm: 0.41697260
INFO:root:[    3] Training loss: 0.03058247, Validation loss: 0.02964830, Gradient norm: 0.43406082
INFO:root:[    4] Training loss: 0.02744224, Validation loss: 0.02754832, Gradient norm: 0.43869722
INFO:root:[    5] Training loss: 0.02610182, Validation loss: 0.02644764, Gradient norm: 0.39308594
INFO:root:[    6] Training loss: 0.02353030, Validation loss: 0.02525601, Gradient norm: 0.35194790
INFO:root:[    7] Training loss: 0.02365519, Validation loss: 0.02429490, Gradient norm: 0.41932219
INFO:root:[    8] Training loss: 0.02226860, Validation loss: 0.03180569, Gradient norm: 0.36136337
INFO:root:[    9] Training loss: 0.02029388, Validation loss: 0.02168947, Gradient norm: 0.31014834
INFO:root:[   10] Training loss: 0.01960283, Validation loss: 0.02289051, Gradient norm: 0.30690143
INFO:root:[   11] Training loss: 0.02057553, Validation loss: 0.02440020, Gradient norm: 0.37481511
INFO:root:[   12] Training loss: 0.01991890, Validation loss: 0.02907120, Gradient norm: 0.36309511
INFO:root:[   13] Training loss: 0.01812822, Validation loss: 0.02881679, Gradient norm: 0.27344421
INFO:root:[   14] Training loss: 0.01886155, Validation loss: 0.02567494, Gradient norm: 0.36706796
INFO:root:[   15] Training loss: 0.01711018, Validation loss: 0.02616965, Gradient norm: 0.26132415
INFO:root:[   16] Training loss: 0.01793059, Validation loss: 0.02452900, Gradient norm: 0.29732288
INFO:root:[   17] Training loss: 0.01719393, Validation loss: 0.02324845, Gradient norm: 0.29631343
INFO:root:[   18] Training loss: 0.01661987, Validation loss: 0.02289238, Gradient norm: 0.26895046
INFO:root:[   19] Training loss: 0.01684974, Validation loss: 0.02122110, Gradient norm: 0.29709677
INFO:root:[   20] Training loss: 0.01484910, Validation loss: 0.02179520, Gradient norm: 0.17653464
INFO:root:[   21] Training loss: 0.01715321, Validation loss: 0.02940140, Gradient norm: 0.31048327
INFO:root:[   22] Training loss: 0.01729431, Validation loss: 0.02931530, Gradient norm: 0.31859990
INFO:root:[   23] Training loss: 0.01738941, Validation loss: 0.02367779, Gradient norm: 0.34875630
INFO:root:[   24] Training loss: 0.01527525, Validation loss: 0.02228954, Gradient norm: 0.23861058
INFO:root:[   25] Training loss: 0.01601060, Validation loss: 0.02730308, Gradient norm: 0.28710447
INFO:root:[   26] Training loss: 0.01482115, Validation loss: 0.02393891, Gradient norm: 0.21894380
INFO:root:[   27] Training loss: 0.01563407, Validation loss: 0.02218251, Gradient norm: 0.29211344
INFO:root:[   28] Training loss: 0.01463710, Validation loss: 0.02689705, Gradient norm: 0.20892916
INFO:root:[   29] Training loss: 0.01502260, Validation loss: 0.02622077, Gradient norm: 0.23816542
INFO:root:[   30] Training loss: 0.01554723, Validation loss: 0.02357466, Gradient norm: 0.28606977
INFO:root:[   31] Training loss: 0.01556641, Validation loss: 0.03328999, Gradient norm: 0.27404827
INFO:root:[   32] Training loss: 0.01506363, Validation loss: 0.02441942, Gradient norm: 0.26897608
INFO:root:[   33] Training loss: 0.01465325, Validation loss: 0.02834508, Gradient norm: 0.22648220
INFO:root:[   34] Training loss: 0.01418774, Validation loss: 0.02692840, Gradient norm: 0.23161447
INFO:root:[   35] Training loss: 0.01401481, Validation loss: 0.02759586, Gradient norm: 0.21988476
INFO:root:[   36] Training loss: 0.01416931, Validation loss: 0.02479775, Gradient norm: 0.23265512
INFO:root:[   37] Training loss: 0.01491496, Validation loss: 0.03126836, Gradient norm: 0.27583382
INFO:root:[   38] Training loss: 0.01438953, Validation loss: 0.02484945, Gradient norm: 0.24800324
INFO:root:[   39] Training loss: 0.01508017, Validation loss: 0.03145385, Gradient norm: 0.28517585
INFO:root:[   40] Training loss: 0.01391712, Validation loss: 0.02920472, Gradient norm: 0.23418304
INFO:root:[   41] Training loss: 0.01407193, Validation loss: 0.02676837, Gradient norm: 0.24375086
INFO:root:[   42] Training loss: 0.01369661, Validation loss: 0.03519857, Gradient norm: 0.22815051
INFO:root:[   43] Training loss: 0.01453558, Validation loss: 0.03102807, Gradient norm: 0.28013819
INFO:root:[   44] Training loss: 0.01359748, Validation loss: 0.02947411, Gradient norm: 0.22762329
INFO:root:[   45] Training loss: 0.01430888, Validation loss: 0.02860578, Gradient norm: 0.26605654
INFO:root:[   46] Training loss: 0.01394051, Validation loss: 0.02689123, Gradient norm: 0.24550860
INFO:root:[   47] Training loss: 0.01370090, Validation loss: 0.02744392, Gradient norm: 0.21702837
INFO:root:[   48] Training loss: 0.01352905, Validation loss: 0.03418831, Gradient norm: 0.24508662
INFO:root:[   49] Training loss: 0.01390501, Validation loss: 0.02837957, Gradient norm: 0.25013265
INFO:root:[   50] Training loss: 0.01403897, Validation loss: 0.02266256, Gradient norm: 0.25413096
INFO:root:[   51] Training loss: 0.01322732, Validation loss: 0.02328867, Gradient norm: 0.21576471
INFO:root:[   52] Training loss: 0.01376050, Validation loss: 0.02595933, Gradient norm: 0.22486879
INFO:root:[   53] Training loss: 0.01329509, Validation loss: 0.02838690, Gradient norm: 0.21093937
INFO:root:[   54] Training loss: 0.01284429, Validation loss: 0.02422265, Gradient norm: 0.20472076
INFO:root:[   55] Training loss: 0.01300940, Validation loss: 0.02487253, Gradient norm: 0.21381540
INFO:root:[   56] Training loss: 0.01420929, Validation loss: 0.03485624, Gradient norm: 0.27799719
INFO:root:[   57] Training loss: 0.01333834, Validation loss: 0.02701888, Gradient norm: 0.23583570
INFO:root:[   58] Training loss: 0.01295805, Validation loss: 0.03030449, Gradient norm: 0.22529441
INFO:root:[   59] Training loss: 0.01309415, Validation loss: 0.02522462, Gradient norm: 0.22198047
INFO:root:[   60] Training loss: 0.01317256, Validation loss: 0.02340861, Gradient norm: 0.23918723
INFO:root:[   61] Training loss: 0.01282462, Validation loss: 0.02571628, Gradient norm: 0.21119864
INFO:root:[   62] Training loss: 0.01288295, Validation loss: 0.02223862, Gradient norm: 0.21760836
INFO:root:[   63] Training loss: 0.01325066, Validation loss: 0.02211704, Gradient norm: 0.22319390
INFO:root:[   64] Training loss: 0.01312451, Validation loss: 0.03094459, Gradient norm: 0.23340862
INFO:root:[   65] Training loss: 0.01319783, Validation loss: 0.02398133, Gradient norm: 0.24780189
INFO:root:[   66] Training loss: 0.01288844, Validation loss: 0.02346306, Gradient norm: 0.21861647
INFO:root:[   67] Training loss: 0.01282304, Validation loss: 0.02524492, Gradient norm: 0.22659158
INFO:root:[   68] Training loss: 0.01285721, Validation loss: 0.03085764, Gradient norm: 0.21855076
INFO:root:[   69] Training loss: 0.01298014, Validation loss: 0.02672116, Gradient norm: 0.22197708
INFO:root:[   70] Training loss: 0.01299139, Validation loss: 0.02529499, Gradient norm: 0.23920469
INFO:root:[   71] Training loss: 0.01284167, Validation loss: 0.03053655, Gradient norm: 0.22769480
INFO:root:[   72] Training loss: 0.01258309, Validation loss: 0.02295108, Gradient norm: 0.21137359
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 4133.372s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02277
INFO:root:EnergyScoreTrain: 0.01727
INFO:root:CoverageTrain: 0.9936
INFO:root:IntervalWidthTrain: 0.12997
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02966
INFO:root:EnergyScoreValidation: 0.02134
INFO:root:CoverageValidation: 0.92217
INFO:root:IntervalWidthValidation: 0.11446
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03008
INFO:root:EnergyScoreTest: 0.02168
INFO:root:CoverageTest: 0.91902
INFO:root:IntervalWidthTest: 0.11435
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 268435456
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06782803, Validation loss: 0.03773170, Gradient norm: 1.04224274
INFO:root:[    2] Training loss: 0.03398887, Validation loss: 0.02836620, Gradient norm: 0.68483761
INFO:root:[    3] Training loss: 0.02954775, Validation loss: 0.02948834, Gradient norm: 0.61119133
INFO:root:[    4] Training loss: 0.02538486, Validation loss: 0.02949834, Gradient norm: 0.49964306
INFO:root:[    5] Training loss: 0.02461177, Validation loss: 0.02885690, Gradient norm: 0.52216640
INFO:root:[    6] Training loss: 0.02114681, Validation loss: 0.03207173, Gradient norm: 0.36482614
INFO:root:[    7] Training loss: 0.02305888, Validation loss: 0.02538842, Gradient norm: 0.54701619
INFO:root:[    8] Training loss: 0.01998727, Validation loss: 0.02554455, Gradient norm: 0.36827156
INFO:root:[    9] Training loss: 0.02170954, Validation loss: 0.02834203, Gradient norm: 0.47971343
INFO:root:[   10] Training loss: 0.02083633, Validation loss: 0.02302892, Gradient norm: 0.46389999
INFO:root:[   11] Training loss: 0.01974809, Validation loss: 0.02708606, Gradient norm: 0.44922987
INFO:root:[   12] Training loss: 0.01744986, Validation loss: 0.02516517, Gradient norm: 0.36095480
INFO:root:[   13] Training loss: 0.01730209, Validation loss: 0.02704659, Gradient norm: 0.36756951
INFO:root:[   14] Training loss: 0.01841264, Validation loss: 0.03203200, Gradient norm: 0.37684319
INFO:root:[   15] Training loss: 0.01722964, Validation loss: 0.02561839, Gradient norm: 0.38133637
INFO:root:[   16] Training loss: 0.01575432, Validation loss: 0.02785161, Gradient norm: 0.24959502
INFO:root:[   17] Training loss: 0.01782616, Validation loss: 0.02568866, Gradient norm: 0.42403253
INFO:root:[   18] Training loss: 0.01687652, Validation loss: 0.02855591, Gradient norm: 0.32830495
INFO:root:[   19] Training loss: 0.01602863, Validation loss: 0.02533440, Gradient norm: 0.35865313
INFO:root:[   20] Training loss: 0.01659804, Validation loss: 0.02976335, Gradient norm: 0.36044168
INFO:root:[   21] Training loss: 0.01610409, Validation loss: 0.02549270, Gradient norm: 0.34502617
INFO:root:[   22] Training loss: 0.01531762, Validation loss: 0.03041844, Gradient norm: 0.35845147
INFO:root:[   23] Training loss: 0.01533285, Validation loss: 0.03671970, Gradient norm: 0.32824178
INFO:root:[   24] Training loss: 0.01453248, Validation loss: 0.02964286, Gradient norm: 0.28094350
INFO:root:[   25] Training loss: 0.01610597, Validation loss: 0.02980880, Gradient norm: 0.38377498
INFO:root:[   26] Training loss: 0.01470215, Validation loss: 0.02786893, Gradient norm: 0.33550134
INFO:root:[   27] Training loss: 0.01516554, Validation loss: 0.03209318, Gradient norm: 0.36981384
INFO:root:[   28] Training loss: 0.01518589, Validation loss: 0.02771656, Gradient norm: 0.35614639
INFO:root:[   29] Training loss: 0.01380858, Validation loss: 0.02613691, Gradient norm: 0.30869549
INFO:root:[   30] Training loss: 0.01492804, Validation loss: 0.03117371, Gradient norm: 0.36623354
INFO:root:[   31] Training loss: 0.01416182, Validation loss: 0.02790285, Gradient norm: 0.33936789
INFO:root:[   32] Training loss: 0.01372666, Validation loss: 0.03134324, Gradient norm: 0.32250834
INFO:root:[   33] Training loss: 0.01424962, Validation loss: 0.03083767, Gradient norm: 0.33841243
INFO:root:[   34] Training loss: 0.01357075, Validation loss: 0.02728256, Gradient norm: 0.31421817
INFO:root:[   35] Training loss: 0.01372364, Validation loss: 0.02850942, Gradient norm: 0.33726976
INFO:root:[   36] Training loss: 0.01343593, Validation loss: 0.02885069, Gradient norm: 0.33165552
INFO:root:[   37] Training loss: 0.01364481, Validation loss: 0.03296699, Gradient norm: 0.34016424
INFO:root:[   38] Training loss: 0.01363370, Validation loss: 0.03000841, Gradient norm: 0.31645584
INFO:root:[   39] Training loss: 0.01254390, Validation loss: 0.03042332, Gradient norm: 0.25213819
INFO:root:[   40] Training loss: 0.01341920, Validation loss: 0.03351774, Gradient norm: 0.35715582
INFO:root:[   41] Training loss: 0.01297828, Validation loss: 0.03114621, Gradient norm: 0.29969557
INFO:root:[   42] Training loss: 0.01268079, Validation loss: 0.03051042, Gradient norm: 0.31476699
INFO:root:[   43] Training loss: 0.01281151, Validation loss: 0.03660650, Gradient norm: 0.32068866
INFO:root:[   44] Training loss: 0.01226570, Validation loss: 0.03232873, Gradient norm: 0.24272279
INFO:root:[   45] Training loss: 0.01225676, Validation loss: 0.02997953, Gradient norm: 0.25931073
INFO:root:[   46] Training loss: 0.01322249, Validation loss: 0.03391482, Gradient norm: 0.33917725
INFO:root:[   47] Training loss: 0.01273312, Validation loss: 0.03509151, Gradient norm: 0.29115080
INFO:root:[   48] Training loss: 0.01242042, Validation loss: 0.03029571, Gradient norm: 0.28716521
INFO:root:[   49] Training loss: 0.01264904, Validation loss: 0.03042787, Gradient norm: 0.33111005
INFO:root:[   50] Training loss: 0.01253486, Validation loss: 0.02841581, Gradient norm: 0.31375159
INFO:root:[   51] Training loss: 0.01181357, Validation loss: 0.03104571, Gradient norm: 0.26815845
INFO:root:[   52] Training loss: 0.01194827, Validation loss: 0.02827790, Gradient norm: 0.29351475
INFO:root:[   53] Training loss: 0.01204483, Validation loss: 0.02925903, Gradient norm: 0.26761290
INFO:root:[   54] Training loss: 0.01208837, Validation loss: 0.02756411, Gradient norm: 0.31114079
INFO:root:[   55] Training loss: 0.11501009, Validation loss: 0.06124103, Gradient norm: 4.71863645
INFO:root:[   56] Training loss: 0.05214835, Validation loss: 0.05308783, Gradient norm: 3.76636485
INFO:root:[   57] Training loss: 0.11510557, Validation loss: 0.11226447, Gradient norm: 1.59384640
INFO:root:[   58] Training loss: 0.11365256, Validation loss: 0.11172725, Gradient norm: 0.05394572
INFO:root:[   59] Training loss: 0.11363112, Validation loss: 0.11315548, Gradient norm: 0.05232891
INFO:root:[   60] Training loss: 0.11377557, Validation loss: 0.11210128, Gradient norm: 0.05017540
INFO:root:[   61] Training loss: 0.11363602, Validation loss: 0.11384457, Gradient norm: 0.05129020
INFO:root:[   62] Training loss: 0.11366685, Validation loss: 0.11354283, Gradient norm: 0.06127603
INFO:root:[   63] Training loss: 0.11359949, Validation loss: 0.11319781, Gradient norm: 0.05281287
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 1860.145s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02071
INFO:root:EnergyScoreTrain: 0.01522
INFO:root:CoverageTrain: 0.82486
INFO:root:IntervalWidthTrain: 0.07466
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03187
INFO:root:EnergyScoreValidation: 0.02321
INFO:root:CoverageValidation: 0.64916
INFO:root:IntervalWidthValidation: 0.07495
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03213
INFO:root:EnergyScoreTest: 0.0234
INFO:root:CoverageTest: 0.6461
INFO:root:IntervalWidthTest: 0.07493
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06658463, Validation loss: 0.03779583, Gradient norm: 0.92334868
INFO:root:[    2] Training loss: 0.03767561, Validation loss: 0.04018064, Gradient norm: 0.62116463
INFO:root:[    3] Training loss: 0.03305969, Validation loss: 0.02647844, Gradient norm: 0.53088154
INFO:root:[    4] Training loss: 0.02879804, Validation loss: 0.02482748, Gradient norm: 0.46571760
INFO:root:[    5] Training loss: 0.02643880, Validation loss: 0.02878347, Gradient norm: 0.41261206
INFO:root:[    6] Training loss: 0.02511468, Validation loss: 0.02612864, Gradient norm: 0.39170500
INFO:root:[    7] Training loss: 0.02494559, Validation loss: 0.02549348, Gradient norm: 0.42141543
INFO:root:[    8] Training loss: 0.02317963, Validation loss: 0.02270273, Gradient norm: 0.42887811
INFO:root:[    9] Training loss: 0.02177610, Validation loss: 0.02608959, Gradient norm: 0.39506731
INFO:root:[   10] Training loss: 0.02127030, Validation loss: 0.02459808, Gradient norm: 0.36492402
INFO:root:[   11] Training loss: 0.02077129, Validation loss: 0.02431176, Gradient norm: 0.36528280
INFO:root:[   12] Training loss: 0.02041671, Validation loss: 0.02425341, Gradient norm: 0.37705054
INFO:root:[   13] Training loss: 0.01989121, Validation loss: 0.02733374, Gradient norm: 0.35400723
INFO:root:[   14] Training loss: 0.01961334, Validation loss: 0.02576397, Gradient norm: 0.35482942
INFO:root:[   15] Training loss: 0.01975170, Validation loss: 0.02427979, Gradient norm: 0.38448642
INFO:root:[   16] Training loss: 0.01951190, Validation loss: 0.02706242, Gradient norm: 0.39523495
INFO:root:[   17] Training loss: 0.01872379, Validation loss: 0.03004745, Gradient norm: 0.37369349
INFO:root:[   18] Training loss: 0.01787112, Validation loss: 0.02839775, Gradient norm: 0.33214345
INFO:root:[   19] Training loss: 0.01756055, Validation loss: 0.02596533, Gradient norm: 0.32277173
INFO:root:[   20] Training loss: 0.01872838, Validation loss: 0.02685107, Gradient norm: 0.37403905
INFO:root:[   21] Training loss: 0.01694518, Validation loss: 0.02919824, Gradient norm: 0.32253833
INFO:root:[   22] Training loss: 0.01687968, Validation loss: 0.02864359, Gradient norm: 0.30880514
INFO:root:[   23] Training loss: 0.01702354, Validation loss: 0.02645479, Gradient norm: 0.32557332
INFO:root:[   24] Training loss: 0.01739725, Validation loss: 0.03137695, Gradient norm: 0.35764608
INFO:root:[   25] Training loss: 0.01672134, Validation loss: 0.03220288, Gradient norm: 0.32985219
INFO:root:[   26] Training loss: 0.01602310, Validation loss: 0.02804981, Gradient norm: 0.28153087
INFO:root:[   27] Training loss: 0.01571974, Validation loss: 0.02996201, Gradient norm: 0.29422000
INFO:root:[   28] Training loss: 0.01542918, Validation loss: 0.02891593, Gradient norm: 0.28738230
INFO:root:[   29] Training loss: 0.01669161, Validation loss: 0.03199895, Gradient norm: 0.33689634
INFO:root:[   30] Training loss: 0.01544276, Validation loss: 0.02759036, Gradient norm: 0.29641910
INFO:root:[   31] Training loss: 0.01583569, Validation loss: 0.03322607, Gradient norm: 0.31079010
INFO:root:[   32] Training loss: 0.01524311, Validation loss: 0.03025856, Gradient norm: 0.28908332
INFO:root:[   33] Training loss: 0.01481103, Validation loss: 0.02984202, Gradient norm: 0.25893709
INFO:root:[   34] Training loss: 0.01549822, Validation loss: 0.02788108, Gradient norm: 0.30973440
INFO:root:[   35] Training loss: 0.01488001, Validation loss: 0.02956216, Gradient norm: 0.28338126
INFO:root:[   36] Training loss: 0.01602350, Validation loss: 0.02939696, Gradient norm: 0.34221017
INFO:root:[   37] Training loss: 0.01551413, Validation loss: 0.02871378, Gradient norm: 0.29139644
INFO:root:[   38] Training loss: 0.01492497, Validation loss: 0.03570676, Gradient norm: 0.29426966
INFO:root:[   39] Training loss: 0.01539808, Validation loss: 0.03059219, Gradient norm: 0.31236431
INFO:root:[   40] Training loss: 0.01465545, Validation loss: 0.02932947, Gradient norm: 0.29229449
INFO:root:[   41] Training loss: 0.01376128, Validation loss: 0.03008658, Gradient norm: 0.24830704
INFO:root:[   42] Training loss: 0.01487934, Validation loss: 0.02702046, Gradient norm: 0.29874575
INFO:root:[   43] Training loss: 0.01402531, Validation loss: 0.03303077, Gradient norm: 0.26294669
INFO:root:[   44] Training loss: 0.01403733, Validation loss: 0.03155361, Gradient norm: 0.26192304
INFO:root:[   45] Training loss: 0.01524496, Validation loss: 0.02655200, Gradient norm: 0.32051786
INFO:root:[   46] Training loss: 0.01433146, Validation loss: 0.02880155, Gradient norm: 0.27307107
INFO:root:[   47] Training loss: 0.01427551, Validation loss: 0.03187779, Gradient norm: 0.28611760
INFO:root:[   48] Training loss: 0.01400120, Validation loss: 0.02740738, Gradient norm: 0.28659713
INFO:root:[   49] Training loss: 0.01454714, Validation loss: 0.03333703, Gradient norm: 0.30800720
INFO:root:[   50] Training loss: 0.01418083, Validation loss: 0.02982667, Gradient norm: 0.29315124
INFO:root:[   51] Training loss: 0.01349580, Validation loss: 0.02755296, Gradient norm: 0.23306366
INFO:root:[   52] Training loss: 0.01364604, Validation loss: 0.02943929, Gradient norm: 0.27504008
INFO:root:[   53] Training loss: 0.01405188, Validation loss: 0.02950895, Gradient norm: 0.28580848
INFO:root:[   54] Training loss: 0.01415528, Validation loss: 0.02866225, Gradient norm: 0.29933903
INFO:root:[   55] Training loss: 0.01363374, Validation loss: 0.02666871, Gradient norm: 0.26940160
INFO:root:[   56] Training loss: 0.01361032, Validation loss: 0.02601335, Gradient norm: 0.26451612
INFO:root:[   57] Training loss: 0.01346725, Validation loss: 0.02795642, Gradient norm: 0.26432594
INFO:root:[   58] Training loss: 0.01343764, Validation loss: 0.03080699, Gradient norm: 0.27132316
INFO:root:[   59] Training loss: 0.01444288, Validation loss: 0.03227987, Gradient norm: 0.30740618
INFO:root:[   60] Training loss: 0.01323202, Validation loss: 0.03039579, Gradient norm: 0.22609552
INFO:root:[   61] Training loss: 0.01376365, Validation loss: 0.03047588, Gradient norm: 0.28812167
INFO:root:[   62] Training loss: 0.01339837, Validation loss: 0.03277050, Gradient norm: 0.25219398
INFO:root:[   63] Training loss: 0.01396772, Validation loss: 0.02817244, Gradient norm: 0.23686438
INFO:root:[   64] Training loss: 0.01315233, Validation loss: 0.03043373, Gradient norm: 0.26298304
INFO:root:[   65] Training loss: 0.01379889, Validation loss: 0.02969505, Gradient norm: 0.30715040
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1909.508s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02452
INFO:root:EnergyScoreTrain: 0.01793
INFO:root:CoverageTrain: 0.81361
INFO:root:IntervalWidthTrain: 0.08254
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03125
INFO:root:EnergyScoreValidation: 0.0226
INFO:root:CoverageValidation: 0.69374
INFO:root:IntervalWidthValidation: 0.08447
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03165
INFO:root:EnergyScoreTest: 0.02289
INFO:root:CoverageTest: 0.68745
INFO:root:IntervalWidthTest: 0.08452
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 234881024
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05437205, Validation loss: 0.02959687, Gradient norm: 0.60349121
INFO:root:[    2] Training loss: 0.03718819, Validation loss: 0.03700183, Gradient norm: 0.53350309
INFO:root:[    3] Training loss: 0.03065576, Validation loss: 0.02867865, Gradient norm: 0.45086511
INFO:root:[    4] Training loss: 0.02747448, Validation loss: 0.03011421, Gradient norm: 0.41123423
INFO:root:[    5] Training loss: 0.02481113, Validation loss: 0.02982718, Gradient norm: 0.28405259
INFO:root:[    6] Training loss: 0.02580183, Validation loss: 0.03315699, Gradient norm: 0.40509848
INFO:root:[    7] Training loss: 0.02459438, Validation loss: 0.02971704, Gradient norm: 0.39052372
INFO:root:[    8] Training loss: 0.02489817, Validation loss: 0.02785966, Gradient norm: 0.41908212
INFO:root:[    9] Training loss: 0.02293421, Validation loss: 0.02822719, Gradient norm: 0.35130340
INFO:root:[   10] Training loss: 0.02220803, Validation loss: 0.02696159, Gradient norm: 0.35479554
INFO:root:[   11] Training loss: 0.02123415, Validation loss: 0.02614129, Gradient norm: 0.32336214
INFO:root:[   12] Training loss: 0.02167821, Validation loss: 0.02870034, Gradient norm: 0.34759347
INFO:root:[   13] Training loss: 0.02187578, Validation loss: 0.02489976, Gradient norm: 0.36483030
INFO:root:[   14] Training loss: 0.01940259, Validation loss: 0.02207044, Gradient norm: 0.24532839
INFO:root:[   15] Training loss: 0.01993772, Validation loss: 0.02390133, Gradient norm: 0.26775823
INFO:root:[   16] Training loss: 0.02059681, Validation loss: 0.02639455, Gradient norm: 0.32291472
INFO:root:[   17] Training loss: 0.02014894, Validation loss: 0.02579785, Gradient norm: 0.32341163
INFO:root:[   18] Training loss: 0.01895594, Validation loss: 0.02373868, Gradient norm: 0.27143196
INFO:root:[   19] Training loss: 0.01979149, Validation loss: 0.02464172, Gradient norm: 0.32201494
INFO:root:[   20] Training loss: 0.01904451, Validation loss: 0.03303796, Gradient norm: 0.31449210
INFO:root:[   21] Training loss: 0.01912662, Validation loss: 0.02650803, Gradient norm: 0.27395487
INFO:root:[   22] Training loss: 0.01807973, Validation loss: 0.02515546, Gradient norm: 0.27912769
INFO:root:[   23] Training loss: 0.01742534, Validation loss: 0.02767170, Gradient norm: 0.23422642
INFO:root:[   24] Training loss: 0.01827742, Validation loss: 0.02876324, Gradient norm: 0.30935666
INFO:root:[   25] Training loss: 0.01870607, Validation loss: 0.02490686, Gradient norm: 0.31903364
INFO:root:[   26] Training loss: 0.01746864, Validation loss: 0.02684162, Gradient norm: 0.26039775
INFO:root:[   27] Training loss: 0.01690193, Validation loss: 0.02637474, Gradient norm: 0.24179814
INFO:root:[   28] Training loss: 0.01756444, Validation loss: 0.02607496, Gradient norm: 0.26256708
INFO:root:[   29] Training loss: 0.01717567, Validation loss: 0.02654184, Gradient norm: 0.27535534
INFO:root:[   30] Training loss: 0.01677570, Validation loss: 0.03048407, Gradient norm: 0.25228020
INFO:root:[   31] Training loss: 0.01769147, Validation loss: 0.02544245, Gradient norm: 0.30808976
INFO:root:[   32] Training loss: 0.01628529, Validation loss: 0.02945052, Gradient norm: 0.23110269
INFO:root:[   33] Training loss: 0.01654032, Validation loss: 0.02573614, Gradient norm: 0.25765138
INFO:root:[   34] Training loss: 0.01706976, Validation loss: 0.02770128, Gradient norm: 0.27955069
INFO:root:[   35] Training loss: 0.01637330, Validation loss: 0.02501427, Gradient norm: 0.25203510
INFO:root:[   36] Training loss: 0.01652927, Validation loss: 0.02954837, Gradient norm: 0.25128785
INFO:root:[   37] Training loss: 0.01666285, Validation loss: 0.02568842, Gradient norm: 0.28048517
INFO:root:[   38] Training loss: 0.01669860, Validation loss: 0.02368981, Gradient norm: 0.28364378
INFO:root:[   39] Training loss: 0.01576214, Validation loss: 0.02610299, Gradient norm: 0.23958677
INFO:root:[   40] Training loss: 0.01648306, Validation loss: 0.02591034, Gradient norm: 0.27110892
INFO:root:[   41] Training loss: 0.01611311, Validation loss: 0.02491711, Gradient norm: 0.27158796
INFO:root:[   42] Training loss: 0.01576835, Validation loss: 0.03101541, Gradient norm: 0.23933010
INFO:root:[   43] Training loss: 0.01605773, Validation loss: 0.02828241, Gradient norm: 0.26836928
INFO:root:[   44] Training loss: 0.01571688, Validation loss: 0.02770057, Gradient norm: 0.25004734
INFO:root:[   45] Training loss: 0.01627796, Validation loss: 0.02608793, Gradient norm: 0.27891764
INFO:root:[   46] Training loss: 0.01559030, Validation loss: 0.02729872, Gradient norm: 0.26533208
INFO:root:[   47] Training loss: 0.01551559, Validation loss: 0.02501387, Gradient norm: 0.23934591
INFO:root:[   48] Training loss: 0.01546331, Validation loss: 0.02979357, Gradient norm: 0.23771510
INFO:root:[   49] Training loss: 0.01576564, Validation loss: 0.03267970, Gradient norm: 0.25833056
INFO:root:[   50] Training loss: 0.01557563, Validation loss: 0.03042068, Gradient norm: 0.23851351
INFO:root:[   51] Training loss: 0.01587055, Validation loss: 0.02981198, Gradient norm: 0.25201276
INFO:root:[   52] Training loss: 0.01471573, Validation loss: 0.02588953, Gradient norm: 0.19748435
INFO:root:[   53] Training loss: 0.01576860, Validation loss: 0.02700297, Gradient norm: 0.26676431
INFO:root:[   54] Training loss: 0.01513293, Validation loss: 0.02713654, Gradient norm: 0.22738826
INFO:root:[   55] Training loss: 0.01561181, Validation loss: 0.02876958, Gradient norm: 0.26322636
INFO:root:[   56] Training loss: 0.01585976, Validation loss: 0.02477699, Gradient norm: 0.25379026
INFO:root:[   57] Training loss: 0.01501556, Validation loss: 0.02806693, Gradient norm: 0.22311666
INFO:root:[   58] Training loss: 0.01575635, Validation loss: 0.02812641, Gradient norm: 0.27738637
INFO:root:[   59] Training loss: 0.01519803, Validation loss: 0.02765210, Gradient norm: 0.25082502
INFO:root:[   60] Training loss: 0.01504847, Validation loss: 0.02705005, Gradient norm: 0.22403402
INFO:root:[   61] Training loss: 0.01533748, Validation loss: 0.02924099, Gradient norm: 0.27102776
INFO:root:[   62] Training loss: 0.01558158, Validation loss: 0.02544128, Gradient norm: 0.27813634
INFO:root:[   63] Training loss: 0.01504368, Validation loss: 0.02770541, Gradient norm: 0.25361934
INFO:root:[   64] Training loss: 0.01481365, Validation loss: 0.02816404, Gradient norm: 0.21844845
INFO:root:[   65] Training loss: 0.01532936, Validation loss: 0.02648207, Gradient norm: 0.25766353
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 112699.368s.
INFO:root:Emptying the cuda cache took 0.024s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02382
INFO:root:EnergyScoreTrain: 0.01754
INFO:root:CoverageTrain: 0.61341
INFO:root:IntervalWidthTrain: 0.05798
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02972
INFO:root:EnergyScoreValidation: 0.02227
INFO:root:CoverageValidation: 0.57332
INFO:root:IntervalWidthValidation: 0.06301
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03029
INFO:root:EnergyScoreTest: 0.02276
INFO:root:CoverageTest: 0.56834
INFO:root:IntervalWidthTest: 0.06297
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06181435, Validation loss: 0.03422389, Gradient norm: 0.69569317
INFO:root:[    2] Training loss: 0.03763131, Validation loss: 0.04024080, Gradient norm: 0.40990241
INFO:root:[    3] Training loss: 0.03354914, Validation loss: 0.03673470, Gradient norm: 0.41159859
INFO:root:[    4] Training loss: 0.03021386, Validation loss: 0.03377840, Gradient norm: 0.36598259
INFO:root:[    5] Training loss: 0.02860666, Validation loss: 0.04001621, Gradient norm: 0.38848741
INFO:root:[    6] Training loss: 0.02534673, Validation loss: 0.02791695, Gradient norm: 0.28672309
INFO:root:[    7] Training loss: 0.02761982, Validation loss: 0.02508706, Gradient norm: 0.41321687
INFO:root:[    8] Training loss: 0.02507685, Validation loss: 0.03323557, Gradient norm: 0.36200442
INFO:root:[    9] Training loss: 0.02361820, Validation loss: 0.03101733, Gradient norm: 0.32043765
INFO:root:[   10] Training loss: 0.02380716, Validation loss: 0.02944342, Gradient norm: 0.30867556
INFO:root:[   11] Training loss: 0.02312827, Validation loss: 0.02476781, Gradient norm: 0.30043444
INFO:root:[   12] Training loss: 0.02194144, Validation loss: 0.02482543, Gradient norm: 0.26561935
INFO:root:[   13] Training loss: 0.02242557, Validation loss: 0.02580795, Gradient norm: 0.31475472
INFO:root:[   14] Training loss: 0.02123769, Validation loss: 0.02553767, Gradient norm: 0.26656849
INFO:root:[   15] Training loss: 0.02062832, Validation loss: 0.02736199, Gradient norm: 0.23824586
INFO:root:[   16] Training loss: 0.02120125, Validation loss: 0.02874897, Gradient norm: 0.29009852
INFO:root:[   17] Training loss: 0.02104141, Validation loss: 0.02929201, Gradient norm: 0.28063220
INFO:root:[   18] Training loss: 0.02017462, Validation loss: 0.02710491, Gradient norm: 0.26131708
INFO:root:[   19] Training loss: 0.02103109, Validation loss: 0.02858054, Gradient norm: 0.32512590
INFO:root:[   20] Training loss: 0.02042806, Validation loss: 0.02968956, Gradient norm: 0.29774866
INFO:root:[   21] Training loss: 0.02094543, Validation loss: 0.02799785, Gradient norm: 0.32810442
INFO:root:[   22] Training loss: 0.01882720, Validation loss: 0.02520510, Gradient norm: 0.22687943
INFO:root:[   23] Training loss: 0.01961438, Validation loss: 0.03670958, Gradient norm: 0.26806258
INFO:root:[   24] Training loss: 0.01995412, Validation loss: 0.02988208, Gradient norm: 0.30302622
INFO:root:[   25] Training loss: 0.01879796, Validation loss: 0.02777368, Gradient norm: 0.27908011
INFO:root:[   26] Training loss: 0.01920548, Validation loss: 0.02714638, Gradient norm: 0.28339232
INFO:root:[   27] Training loss: 0.01969578, Validation loss: 0.02595481, Gradient norm: 0.31068114
INFO:root:[   28] Training loss: 0.01804711, Validation loss: 0.02836853, Gradient norm: 0.23730804
INFO:root:[   29] Training loss: 0.01847019, Validation loss: 0.02361922, Gradient norm: 0.26036481
INFO:root:[   30] Training loss: 0.01903153, Validation loss: 0.02599891, Gradient norm: 0.27677858
INFO:root:[   31] Training loss: 0.01799604, Validation loss: 0.02740497, Gradient norm: 0.24012078
INFO:root:[   32] Training loss: 0.01862130, Validation loss: 0.03132169, Gradient norm: 0.27620650
INFO:root:[   33] Training loss: 0.01862370, Validation loss: 0.02826001, Gradient norm: 0.27074905
INFO:root:[   34] Training loss: 0.01773746, Validation loss: 0.02626584, Gradient norm: 0.24707680
INFO:root:[   35] Training loss: 0.01884734, Validation loss: 0.02486740, Gradient norm: 0.29402388
INFO:root:[   36] Training loss: 0.01773053, Validation loss: 0.02646992, Gradient norm: 0.25800023
INFO:root:[   37] Training loss: 0.01754630, Validation loss: 0.02403790, Gradient norm: 0.24669525
INFO:root:[   38] Training loss: 0.01750862, Validation loss: 0.02616698, Gradient norm: 0.24170230
INFO:root:[   39] Training loss: 0.01791407, Validation loss: 0.02485895, Gradient norm: 0.27610864
INFO:root:[   40] Training loss: 0.01787506, Validation loss: 0.02445146, Gradient norm: 0.24950696
INFO:root:[   41] Training loss: 0.01814844, Validation loss: 0.02991298, Gradient norm: 0.28346304
INFO:root:[   42] Training loss: 0.01710079, Validation loss: 0.02526003, Gradient norm: 0.24143747
INFO:root:[   43] Training loss: 0.01801974, Validation loss: 0.02774525, Gradient norm: 0.29069826
INFO:root:[   44] Training loss: 0.01702212, Validation loss: 0.02636887, Gradient norm: 0.23717465
INFO:root:[   45] Training loss: 0.01685318, Validation loss: 0.02907302, Gradient norm: 0.22567933
INFO:root:[   46] Training loss: 0.01807361, Validation loss: 0.02882402, Gradient norm: 0.27832499
INFO:root:[   47] Training loss: 0.01652903, Validation loss: 0.02358595, Gradient norm: 0.20968818
INFO:root:[   48] Training loss: 0.01766391, Validation loss: 0.03066221, Gradient norm: 0.29090156
INFO:root:[   49] Training loss: 0.01714329, Validation loss: 0.02534957, Gradient norm: 0.24612051
INFO:root:[   50] Training loss: 0.01680659, Validation loss: 0.02774074, Gradient norm: 0.23321053
INFO:root:[   51] Training loss: 0.01689713, Validation loss: 0.02616555, Gradient norm: 0.23578488
INFO:root:[   52] Training loss: 0.01696595, Validation loss: 0.02707721, Gradient norm: 0.26235303
INFO:root:[   53] Training loss: 0.01809980, Validation loss: 0.02757843, Gradient norm: 0.27516236
INFO:root:[   54] Training loss: 0.01736630, Validation loss: 0.02824138, Gradient norm: 0.27723138
INFO:root:[   55] Training loss: 0.01653099, Validation loss: 0.02995455, Gradient norm: 0.20517254
INFO:root:[   56] Training loss: 0.01737645, Validation loss: 0.02531932, Gradient norm: 0.26727464
INFO:root:[   57] Training loss: 0.01711297, Validation loss: 0.02527394, Gradient norm: 0.25783750
INFO:root:[   58] Training loss: 0.01682755, Validation loss: 0.02961026, Gradient norm: 0.24427095
INFO:root:[   59] Training loss: 0.01626205, Validation loss: 0.02767139, Gradient norm: 0.22065465
INFO:root:[   60] Training loss: 0.01710031, Validation loss: 0.02919404, Gradient norm: 0.27497635
INFO:root:[   61] Training loss: 0.01688135, Validation loss: 0.02521939, Gradient norm: 0.25479953
INFO:root:[   62] Training loss: 0.01653700, Validation loss: 0.02621264, Gradient norm: 0.24228692
INFO:root:[   63] Training loss: 0.01718764, Validation loss: 0.02672614, Gradient norm: 0.25464312
INFO:root:[   64] Training loss: 0.01597485, Validation loss: 0.02739399, Gradient norm: 0.21958430
INFO:root:[   65] Training loss: 0.01744063, Validation loss: 0.02487277, Gradient norm: 0.28695944
INFO:root:[   66] Training loss: 0.01638336, Validation loss: 0.02439232, Gradient norm: 0.23012838
INFO:root:[   67] Training loss: 0.01650963, Validation loss: 0.02547498, Gradient norm: 0.23649308
INFO:root:[   68] Training loss: 0.01609468, Validation loss: 0.02756459, Gradient norm: 0.21802141
INFO:root:[   69] Training loss: 0.01624448, Validation loss: 0.02876907, Gradient norm: 0.25208291
INFO:root:[   70] Training loss: 0.01635219, Validation loss: 0.02563624, Gradient norm: 0.22805761
INFO:root:[   71] Training loss: 0.01652256, Validation loss: 0.02509741, Gradient norm: 0.26548426
INFO:root:[   72] Training loss: 0.01619126, Validation loss: 0.02855435, Gradient norm: 0.22663362
INFO:root:[   73] Training loss: 0.01650514, Validation loss: 0.02824932, Gradient norm: 0.25167768
INFO:root:[   74] Training loss: 0.01656517, Validation loss: 0.02784287, Gradient norm: 0.25284980
INFO:root:[   75] Training loss: 0.01587389, Validation loss: 0.02657265, Gradient norm: 0.22151314
INFO:root:[   76] Training loss: 0.01593408, Validation loss: 0.02436527, Gradient norm: 0.21237850
INFO:root:[   77] Training loss: 0.01629396, Validation loss: 0.02383538, Gradient norm: 0.24846525
INFO:root:[   78] Training loss: 0.01618240, Validation loss: 0.02633706, Gradient norm: 0.21184941
INFO:root:[   79] Training loss: 0.01604647, Validation loss: 0.02583794, Gradient norm: 0.22669418
INFO:root:[   80] Training loss: 0.01669269, Validation loss: 0.02926156, Gradient norm: 0.26132765
INFO:root:[   81] Training loss: 0.01608927, Validation loss: 0.02610989, Gradient norm: 0.20220732
INFO:root:[   82] Training loss: 0.01660637, Validation loss: 0.02546152, Gradient norm: 0.26868962
INFO:root:[   83] Training loss: 0.01584130, Validation loss: 0.02789389, Gradient norm: 0.20344185
INFO:root:[   84] Training loss: 0.01610695, Validation loss: 0.02710481, Gradient norm: 0.24992561
INFO:root:[   85] Training loss: 0.01604777, Validation loss: 0.02879927, Gradient norm: 0.24272347
INFO:root:[   86] Training loss: 0.01546657, Validation loss: 0.02651687, Gradient norm: 0.19930817
INFO:root:EP 86: Early stopping
INFO:root:Training the model took 2464.435s.
INFO:root:Emptying the cuda cache took 0.021s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0258
INFO:root:EnergyScoreTrain: 0.01963
INFO:root:CoverageTrain: 0.49732
INFO:root:IntervalWidthTrain: 0.0507
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0306
INFO:root:EnergyScoreValidation: 0.02353
INFO:root:CoverageValidation: 0.55161
INFO:root:IntervalWidthValidation: 0.05982
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03108
INFO:root:EnergyScoreTest: 0.02399
INFO:root:CoverageTest: 0.54923
INFO:root:IntervalWidthTest: 0.05969
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06022009, Validation loss: 0.06105673, Gradient norm: 0.58772234
INFO:root:[    2] Training loss: 0.03791427, Validation loss: 0.06041832, Gradient norm: 0.39983895
INFO:root:[    3] Training loss: 0.03442319, Validation loss: 0.03662580, Gradient norm: 0.40859993
INFO:root:[    4] Training loss: 0.02979555, Validation loss: 0.04155846, Gradient norm: 0.32938562
INFO:root:[    5] Training loss: 0.02848576, Validation loss: 0.03864906, Gradient norm: 0.31572348
INFO:root:[    6] Training loss: 0.02681906, Validation loss: 0.03837839, Gradient norm: 0.31659352
INFO:root:[    7] Training loss: 0.02623880, Validation loss: 0.03922043, Gradient norm: 0.28145353
INFO:root:[    8] Training loss: 0.02654556, Validation loss: 0.02857279, Gradient norm: 0.34749294
INFO:root:[    9] Training loss: 0.02408132, Validation loss: 0.02822957, Gradient norm: 0.24725744
INFO:root:[   10] Training loss: 0.02439980, Validation loss: 0.02494487, Gradient norm: 0.27061540
INFO:root:[   11] Training loss: 0.02365366, Validation loss: 0.02766313, Gradient norm: 0.25062551
INFO:root:[   12] Training loss: 0.02546633, Validation loss: 0.02991290, Gradient norm: 0.31756121
INFO:root:[   13] Training loss: 0.02253980, Validation loss: 0.02780213, Gradient norm: 0.23932132
INFO:root:[   14] Training loss: 0.02360461, Validation loss: 0.02724827, Gradient norm: 0.30881500
INFO:root:[   15] Training loss: 0.02270406, Validation loss: 0.03266606, Gradient norm: 0.25923224
INFO:root:[   16] Training loss: 0.02189448, Validation loss: 0.02592066, Gradient norm: 0.23487325
INFO:root:[   17] Training loss: 0.02176756, Validation loss: 0.03394443, Gradient norm: 0.24822846
INFO:root:[   18] Training loss: 0.02209592, Validation loss: 0.02991117, Gradient norm: 0.29610615
INFO:root:[   19] Training loss: 0.02258677, Validation loss: 0.03687349, Gradient norm: 0.30848668
INFO:root:[   20] Training loss: 0.02233443, Validation loss: 0.02849988, Gradient norm: 0.29725789
INFO:root:[   21] Training loss: 0.02089146, Validation loss: 0.02982944, Gradient norm: 0.26264880
INFO:root:[   22] Training loss: 0.02031136, Validation loss: 0.02836452, Gradient norm: 0.20330293
INFO:root:[   23] Training loss: 0.02151143, Validation loss: 0.02965664, Gradient norm: 0.28914212
INFO:root:[   24] Training loss: 0.02054511, Validation loss: 0.02782181, Gradient norm: 0.25897645
INFO:root:[   25] Training loss: 0.02115761, Validation loss: 0.02969462, Gradient norm: 0.30377710
INFO:root:[   26] Training loss: 0.02108569, Validation loss: 0.02679032, Gradient norm: 0.30311986
INFO:root:[   27] Training loss: 0.02032198, Validation loss: 0.02995016, Gradient norm: 0.27894693
INFO:root:[   28] Training loss: 0.01939210, Validation loss: 0.03418953, Gradient norm: 0.23910890
INFO:root:[   29] Training loss: 0.02029048, Validation loss: 0.02868397, Gradient norm: 0.28178768
INFO:root:[   30] Training loss: 0.02055012, Validation loss: 0.02818873, Gradient norm: 0.30547870
INFO:root:[   31] Training loss: 0.01970665, Validation loss: 0.03493249, Gradient norm: 0.26487783
INFO:root:[   32] Training loss: 0.01960974, Validation loss: 0.02737175, Gradient norm: 0.25450253
INFO:root:[   33] Training loss: 0.01882818, Validation loss: 0.03282988, Gradient norm: 0.22563078
INFO:root:[   34] Training loss: 0.01912414, Validation loss: 0.03052434, Gradient norm: 0.26042544
INFO:root:[   35] Training loss: 0.02017453, Validation loss: 0.02946783, Gradient norm: 0.26689944
INFO:root:[   36] Training loss: 0.01949586, Validation loss: 0.02953009, Gradient norm: 0.26394030
INFO:root:[   37] Training loss: 0.01826041, Validation loss: 0.02833678, Gradient norm: 0.18175051
INFO:root:[   38] Training loss: 0.01943687, Validation loss: 0.03778906, Gradient norm: 0.25825756
INFO:root:[   39] Training loss: 0.01966793, Validation loss: 0.02840587, Gradient norm: 0.28679375
INFO:root:[   40] Training loss: 0.01906929, Validation loss: 0.03054409, Gradient norm: 0.25662021
INFO:root:[   41] Training loss: 0.01841071, Validation loss: 0.02680071, Gradient norm: 0.21093725
INFO:root:[   42] Training loss: 0.01839325, Validation loss: 0.02551846, Gradient norm: 0.20424567
INFO:root:[   43] Training loss: 0.01796864, Validation loss: 0.03254785, Gradient norm: 0.20064401
INFO:root:[   44] Training loss: 0.01983821, Validation loss: 0.02918664, Gradient norm: 0.30285712
INFO:root:[   45] Training loss: 0.01895725, Validation loss: 0.03271098, Gradient norm: 0.26748569
INFO:root:[   46] Training loss: 0.01875643, Validation loss: 0.02784772, Gradient norm: 0.26222527
INFO:root:[   47] Training loss: 0.01861269, Validation loss: 0.02603537, Gradient norm: 0.24131509
INFO:root:[   48] Training loss: 0.01806719, Validation loss: 0.02722884, Gradient norm: 0.22779041
INFO:root:[   49] Training loss: 0.01796725, Validation loss: 0.02906292, Gradient norm: 0.24211371
INFO:root:[   50] Training loss: 0.01875585, Validation loss: 0.03063406, Gradient norm: 0.26809746
INFO:root:[   51] Training loss: 0.01865352, Validation loss: 0.02883625, Gradient norm: 0.25871165
INFO:root:[   52] Training loss: 0.01778642, Validation loss: 0.02899910, Gradient norm: 0.18225612
INFO:root:[   53] Training loss: 0.01863806, Validation loss: 0.03088419, Gradient norm: 0.24712098
INFO:root:[   54] Training loss: 0.01774020, Validation loss: 0.03342951, Gradient norm: 0.22589657
INFO:root:[   55] Training loss: 0.01837538, Validation loss: 0.03058056, Gradient norm: 0.24376602
INFO:root:[   56] Training loss: 0.01847569, Validation loss: 0.02885836, Gradient norm: 0.25842987
INFO:root:[   57] Training loss: 0.01836749, Validation loss: 0.02858757, Gradient norm: 0.25850970
INFO:root:[   58] Training loss: 0.01789117, Validation loss: 0.02754039, Gradient norm: 0.22291457
INFO:root:[   59] Training loss: 0.01808886, Validation loss: 0.03181662, Gradient norm: 0.24659310
INFO:root:[   60] Training loss: 0.01809865, Validation loss: 0.02712155, Gradient norm: 0.23993433
INFO:root:[   61] Training loss: 0.01764381, Validation loss: 0.03287286, Gradient norm: 0.21791240
INFO:root:[   62] Training loss: 0.01808494, Validation loss: 0.02956740, Gradient norm: 0.24155109
INFO:root:[   63] Training loss: 0.01791359, Validation loss: 0.02888920, Gradient norm: 0.22634979
INFO:root:[   64] Training loss: 0.01752277, Validation loss: 0.02884674, Gradient norm: 0.21250691
INFO:root:[   65] Training loss: 0.01807458, Validation loss: 0.02794019, Gradient norm: 0.26577339
INFO:root:[   66] Training loss: 0.01720127, Validation loss: 0.02743696, Gradient norm: 0.18378711
INFO:root:[   67] Training loss: 0.01768155, Validation loss: 0.02821258, Gradient norm: 0.23080481
INFO:root:[   68] Training loss: 0.01777209, Validation loss: 0.02964408, Gradient norm: 0.22506873
INFO:root:[   69] Training loss: 0.01757587, Validation loss: 0.02685770, Gradient norm: 0.22721166
INFO:root:[   70] Training loss: 0.01705861, Validation loss: 0.03094255, Gradient norm: 0.19867699
INFO:root:[   71] Training loss: 0.01774837, Validation loss: 0.02754728, Gradient norm: 0.24348840
INFO:root:[   72] Training loss: 0.01737778, Validation loss: 0.02656371, Gradient norm: 0.21345581
INFO:root:[   73] Training loss: 0.01729357, Validation loss: 0.02749665, Gradient norm: 0.21504142
INFO:root:[   74] Training loss: 0.01744548, Validation loss: 0.02766753, Gradient norm: 0.22929958
INFO:root:[   75] Training loss: 0.01769271, Validation loss: 0.02888802, Gradient norm: 0.24775411
INFO:root:[   76] Training loss: 0.01772032, Validation loss: 0.02605174, Gradient norm: 0.22916626
INFO:root:[   77] Training loss: 0.01744671, Validation loss: 0.02624212, Gradient norm: 0.22327724
INFO:root:[   78] Training loss: 0.01698214, Validation loss: 0.03022099, Gradient norm: 0.21255439
INFO:root:[   79] Training loss: 0.01737446, Validation loss: 0.02727972, Gradient norm: 0.22132956
INFO:root:[   80] Training loss: 0.01782735, Validation loss: 0.02956548, Gradient norm: 0.25469762
INFO:root:[   81] Training loss: 0.01722354, Validation loss: 0.03060474, Gradient norm: 0.23097978
INFO:root:[   82] Training loss: 0.01751741, Validation loss: 0.03008056, Gradient norm: 0.22692074
INFO:root:[   83] Training loss: 0.01750080, Validation loss: 0.02990655, Gradient norm: 0.24156720
INFO:root:[   84] Training loss: 0.01780744, Validation loss: 0.02916077, Gradient norm: 0.26791898
INFO:root:[   85] Training loss: 0.01692661, Validation loss: 0.02671315, Gradient norm: 0.20496762
INFO:root:EP 85: Early stopping
INFO:root:Training the model took 2435.253s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02948
INFO:root:EnergyScoreTrain: 0.02196
INFO:root:CoverageTrain: 0.61433
INFO:root:IntervalWidthTrain: 0.0727
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03378
INFO:root:EnergyScoreValidation: 0.02522
INFO:root:CoverageValidation: 0.54841
INFO:root:IntervalWidthValidation: 0.07388
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03429
INFO:root:EnergyScoreTest: 0.02562
INFO:root:CoverageTest: 0.54314
INFO:root:IntervalWidthTest: 0.07389
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05738234, Validation loss: 0.03691434, Gradient norm: 0.58682074
INFO:root:[    2] Training loss: 0.03857143, Validation loss: 0.04055145, Gradient norm: 0.46868385
INFO:root:[    3] Training loss: 0.03630988, Validation loss: 0.04799820, Gradient norm: 0.43102995
INFO:root:[    4] Training loss: 0.03324401, Validation loss: 0.05180568, Gradient norm: 0.33254193
INFO:root:[    5] Training loss: 0.03229613, Validation loss: 0.03442020, Gradient norm: 0.33423689
INFO:root:[    6] Training loss: 0.03031053, Validation loss: 0.05325474, Gradient norm: 0.27612179
INFO:root:[    7] Training loss: 0.03037285, Validation loss: 0.04123768, Gradient norm: 0.30570331
INFO:root:[    8] Training loss: 0.03011745, Validation loss: 0.03534699, Gradient norm: 0.31381143
INFO:root:[    9] Training loss: 0.02879124, Validation loss: 0.04387855, Gradient norm: 0.28727711
INFO:root:[   10] Training loss: 0.02787226, Validation loss: 0.04083311, Gradient norm: 0.28061151
INFO:root:[   11] Training loss: 0.02813920, Validation loss: 0.04838004, Gradient norm: 0.31739894
INFO:root:[   12] Training loss: 0.02646262, Validation loss: 0.03476845, Gradient norm: 0.27935922
INFO:root:[   13] Training loss: 0.02552165, Validation loss: 0.05404297, Gradient norm: 0.25676926
INFO:root:[   14] Training loss: 0.02486833, Validation loss: 0.03575989, Gradient norm: 0.24731918
INFO:root:[   15] Training loss: 0.02508204, Validation loss: 0.03626808, Gradient norm: 0.25087386
INFO:root:[   16] Training loss: 0.02483778, Validation loss: 0.05968228, Gradient norm: 0.25256707
INFO:root:[   17] Training loss: 0.02437937, Validation loss: 0.04177918, Gradient norm: 0.23942349
INFO:root:[   18] Training loss: 0.02379817, Validation loss: 0.04434737, Gradient norm: 0.22271879
INFO:root:[   19] Training loss: 0.02405003, Validation loss: 0.04279892, Gradient norm: 0.25346142
INFO:root:[   20] Training loss: 0.02413505, Validation loss: 0.05443937, Gradient norm: 0.22984145
INFO:root:[   21] Training loss: 0.02302663, Validation loss: 0.04259084, Gradient norm: 0.21960515
INFO:root:[   22] Training loss: 0.02483800, Validation loss: 0.07098238, Gradient norm: 0.30226874
INFO:root:[   23] Training loss: 0.02340005, Validation loss: 0.04056561, Gradient norm: 0.24149816
INFO:root:[   24] Training loss: 0.02302917, Validation loss: 0.04526262, Gradient norm: 0.22837380
INFO:root:[   25] Training loss: 0.02258775, Validation loss: 0.04122163, Gradient norm: 0.19975900
INFO:root:[   26] Training loss: 0.02330975, Validation loss: 0.05411253, Gradient norm: 0.25487933
INFO:root:[   27] Training loss: 0.02220140, Validation loss: 0.05656730, Gradient norm: 0.18864039
INFO:root:[   28] Training loss: 0.02302865, Validation loss: 0.04549048, Gradient norm: 0.24692480
INFO:root:[   29] Training loss: 0.02198304, Validation loss: 0.05655113, Gradient norm: 0.18142744
INFO:root:[   30] Training loss: 0.02296985, Validation loss: 0.04178490, Gradient norm: 0.26158306
INFO:root:[   31] Training loss: 0.02228562, Validation loss: 0.04537436, Gradient norm: 0.21899669
INFO:root:[   32] Training loss: 0.02174884, Validation loss: 0.05958952, Gradient norm: 0.19703505
INFO:root:[   33] Training loss: 0.02231627, Validation loss: 0.04695450, Gradient norm: 0.22051381
INFO:root:[   34] Training loss: 0.02174407, Validation loss: 0.05477966, Gradient norm: 0.18668685
INFO:root:[   35] Training loss: 0.02294057, Validation loss: 0.05427231, Gradient norm: 0.25688288
INFO:root:[   36] Training loss: 0.02172492, Validation loss: 0.04895823, Gradient norm: 0.20500078
INFO:root:[   37] Training loss: 0.02166959, Validation loss: 0.05569926, Gradient norm: 0.22355997
INFO:root:[   38] Training loss: 0.02106866, Validation loss: 0.04870559, Gradient norm: 0.16798133
INFO:root:[   39] Training loss: 0.02178097, Validation loss: 0.07047460, Gradient norm: 0.21684753
INFO:root:[   40] Training loss: 0.02179338, Validation loss: 0.06509321, Gradient norm: 0.21093153
INFO:root:[   41] Training loss: 0.02139272, Validation loss: 0.06414420, Gradient norm: 0.21318581
INFO:root:[   42] Training loss: 0.02194891, Validation loss: 0.06520233, Gradient norm: 0.22371018
INFO:root:[   43] Training loss: 0.02078850, Validation loss: 0.05940107, Gradient norm: 0.17421613
INFO:root:[   44] Training loss: 0.02093325, Validation loss: 0.05318258, Gradient norm: 0.18791134
INFO:root:[   45] Training loss: 0.02175087, Validation loss: 0.05367180, Gradient norm: 0.24263666
INFO:root:[   46] Training loss: 0.02188210, Validation loss: 0.05125984, Gradient norm: 0.23159452
INFO:root:[   47] Training loss: 0.02089215, Validation loss: 0.06454653, Gradient norm: 0.18604728
INFO:root:[   48] Training loss: 0.02189608, Validation loss: 0.06329813, Gradient norm: 0.25770244
INFO:root:[   49] Training loss: 0.02083563, Validation loss: 0.07064049, Gradient norm: 0.18673783
INFO:root:[   50] Training loss: 0.02080348, Validation loss: 0.06316531, Gradient norm: 0.20946814
INFO:root:[   51] Training loss: 0.02129240, Validation loss: 0.07498577, Gradient norm: 0.21954213
INFO:root:[   52] Training loss: 0.02079207, Validation loss: 0.05335522, Gradient norm: 0.18003795
INFO:root:[   53] Training loss: 0.02081833, Validation loss: 0.06964711, Gradient norm: 0.22194179
INFO:root:[   54] Training loss: 0.02137178, Validation loss: 0.07459804, Gradient norm: 0.23066866
INFO:root:[   55] Training loss: 0.02016795, Validation loss: 0.07222364, Gradient norm: 0.15940733
INFO:root:[   56] Training loss: 0.02087015, Validation loss: 0.06262533, Gradient norm: 0.19241958
INFO:root:[   57] Training loss: 0.02115520, Validation loss: 0.05528145, Gradient norm: 0.23244233
INFO:root:[   58] Training loss: 0.02046816, Validation loss: 0.07166938, Gradient norm: 0.17871891
INFO:root:[   59] Training loss: 0.02131212, Validation loss: 0.05496521, Gradient norm: 0.23892161
INFO:root:[   60] Training loss: 0.02058324, Validation loss: 0.06159854, Gradient norm: 0.22190954
INFO:root:[   61] Training loss: 0.02000874, Validation loss: 0.06590292, Gradient norm: 0.14996403
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1759.126s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03709
INFO:root:EnergyScoreTrain: 0.03224
INFO:root:CoverageTrain: 0.19444
INFO:root:IntervalWidthTrain: 0.0301
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03959
INFO:root:EnergyScoreValidation: 0.03457
INFO:root:CoverageValidation: 0.18396
INFO:root:IntervalWidthValidation: 0.03011
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0401
INFO:root:EnergyScoreTest: 0.03506
INFO:root:CoverageTest: 0.1818
INFO:root:IntervalWidthTest: 0.02997
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05448618, Validation loss: 0.05327703, Gradient norm: 0.58971981
INFO:root:[    2] Training loss: 0.04305527, Validation loss: 0.03998950, Gradient norm: 0.60203090
INFO:root:[    3] Training loss: 0.03501072, Validation loss: 0.03583769, Gradient norm: 0.58591113
INFO:root:[    4] Training loss: 0.03207924, Validation loss: 0.03690998, Gradient norm: 0.55817727
INFO:root:[    5] Training loss: 0.03091825, Validation loss: 0.03267665, Gradient norm: 0.57758121
INFO:root:[    6] Training loss: 0.02778938, Validation loss: 0.03740140, Gradient norm: 0.54113827
INFO:root:[    7] Training loss: 0.02600442, Validation loss: 0.03084719, Gradient norm: 0.44113075
INFO:root:[    8] Training loss: 0.02711013, Validation loss: 0.03119824, Gradient norm: 0.54194517
INFO:root:[    9] Training loss: 0.02343277, Validation loss: 0.03450100, Gradient norm: 0.39728093
INFO:root:[   10] Training loss: 0.02338107, Validation loss: 0.03044406, Gradient norm: 0.42616863
INFO:root:[   11] Training loss: 0.02396701, Validation loss: 0.03589526, Gradient norm: 0.49693312
INFO:root:[   12] Training loss: 0.02329201, Validation loss: 0.03583070, Gradient norm: 0.46371882
INFO:root:[   13] Training loss: 0.02378576, Validation loss: 0.04526345, Gradient norm: 0.51464675
INFO:root:[   14] Training loss: 0.02422175, Validation loss: 0.03435850, Gradient norm: 0.53195984
INFO:root:[   15] Training loss: 0.02126187, Validation loss: 0.03832356, Gradient norm: 0.41132844
INFO:root:[   16] Training loss: 0.02123865, Validation loss: 0.03964364, Gradient norm: 0.43378956
INFO:root:[   17] Training loss: 0.02266133, Validation loss: 0.03331424, Gradient norm: 0.48840931
INFO:root:[   18] Training loss: 0.02026683, Validation loss: 0.03401861, Gradient norm: 0.40112474
INFO:root:[   19] Training loss: 0.02114051, Validation loss: 0.03684422, Gradient norm: 0.49131093
INFO:root:[   20] Training loss: 0.01993555, Validation loss: 0.03298066, Gradient norm: 0.43440040
INFO:root:[   21] Training loss: 0.02087599, Validation loss: 0.03561274, Gradient norm: 0.53781842
INFO:root:[   22] Training loss: 0.02035251, Validation loss: 0.03559088, Gradient norm: 0.45367303
INFO:root:[   23] Training loss: 0.01972145, Validation loss: 0.03712160, Gradient norm: 0.45414063
INFO:root:[   24] Training loss: 0.01891812, Validation loss: 0.03778630, Gradient norm: 0.44429320
INFO:root:[   25] Training loss: 0.01940275, Validation loss: 0.03450921, Gradient norm: 0.44119255
INFO:root:[   26] Training loss: 0.01977272, Validation loss: 0.04011321, Gradient norm: 0.51072802
