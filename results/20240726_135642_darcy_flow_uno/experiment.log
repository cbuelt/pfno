INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06512139, Validation loss: 0.03518820, Gradient norm: 1.01610049
INFO:root:[    2] Training loss: 0.03376755, Validation loss: 0.05011676, Gradient norm: 0.84470862
INFO:root:[    3] Training loss: 0.02986430, Validation loss: 0.03027089, Gradient norm: 0.81197624
INFO:root:[    4] Training loss: 0.02665337, Validation loss: 0.03023945, Gradient norm: 0.80808782
INFO:root:[    5] Training loss: 0.02256413, Validation loss: 0.02724972, Gradient norm: 0.56505811
INFO:root:[    6] Training loss: 0.02290157, Validation loss: 0.03040398, Gradient norm: 0.68049550
INFO:root:[    7] Training loss: 0.02297635, Validation loss: 0.02642562, Gradient norm: 0.68282401
INFO:root:[    8] Training loss: 0.01937842, Validation loss: 0.02674355, Gradient norm: 0.51990676
INFO:root:[    9] Training loss: 0.02095944, Validation loss: 0.03802914, Gradient norm: 0.58902579
INFO:root:[   10] Training loss: 0.01921808, Validation loss: 0.02846810, Gradient norm: 0.53707004
INFO:root:[   11] Training loss: 0.01915867, Validation loss: 0.02949327, Gradient norm: 0.58627696
INFO:root:[   12] Training loss: 0.01690791, Validation loss: 0.02821182, Gradient norm: 0.41757455
INFO:root:[   13] Training loss: 0.02043161, Validation loss: 0.02869012, Gradient norm: 0.63567940
INFO:root:[   14] Training loss: 0.01799876, Validation loss: 0.02910502, Gradient norm: 0.55003883
INFO:root:[   15] Training loss: 0.01711865, Validation loss: 0.02913832, Gradient norm: 0.49570301
INFO:root:[   16] Training loss: 0.01708239, Validation loss: 0.02974223, Gradient norm: 0.46062056
INFO:root:[   17] Training loss: 0.01685893, Validation loss: 0.02943871, Gradient norm: 0.48905225
INFO:root:[   18] Training loss: 0.01624764, Validation loss: 0.02851010, Gradient norm: 0.43435202
INFO:root:[   19] Training loss: 0.01590139, Validation loss: 0.03039963, Gradient norm: 0.51231503
INFO:root:[   20] Training loss: 0.01573706, Validation loss: 0.03507880, Gradient norm: 0.44465800
INFO:root:[   21] Training loss: 0.01633150, Validation loss: 0.02828084, Gradient norm: 0.51994415
INFO:root:[   22] Training loss: 0.01560431, Validation loss: 0.03377164, Gradient norm: 0.51849805
INFO:root:[   23] Training loss: 0.01610744, Validation loss: 0.02983446, Gradient norm: 0.49580977
INFO:root:[   24] Training loss: 0.01377454, Validation loss: 0.02980019, Gradient norm: 0.32877297
INFO:root:[   25] Training loss: 0.01441580, Validation loss: 0.03065280, Gradient norm: 0.40855399
INFO:root:[   26] Training loss: 0.01519008, Validation loss: 0.02933649, Gradient norm: 0.47463999
INFO:root:[   27] Training loss: 0.01471102, Validation loss: 0.03342907, Gradient norm: 0.45397436
INFO:root:[   28] Training loss: 0.01511658, Validation loss: 0.03012035, Gradient norm: 0.46578764
INFO:root:[   29] Training loss: 0.01439995, Validation loss: 0.03442158, Gradient norm: 0.44592810
INFO:root:[   30] Training loss: 0.01474430, Validation loss: 0.03230985, Gradient norm: 0.43921725
INFO:root:[   31] Training loss: 0.01396324, Validation loss: 0.03205695, Gradient norm: 0.38602045
INFO:root:[   32] Training loss: 0.01202487, Validation loss: 0.03830448, Gradient norm: 0.28172179
INFO:root:[   33] Training loss: 0.01454240, Validation loss: 0.03134151, Gradient norm: 0.46874669
INFO:root:[   34] Training loss: 0.01299386, Validation loss: 0.03435519, Gradient norm: 0.40837907
INFO:root:[   35] Training loss: 0.01349036, Validation loss: 0.03515188, Gradient norm: 0.41794313
INFO:root:[   36] Training loss: 0.01280935, Validation loss: 0.03067498, Gradient norm: 0.40597476
INFO:root:[   37] Training loss: 0.01306351, Validation loss: 0.03345819, Gradient norm: 0.41959606
INFO:root:[   38] Training loss: 0.01350325, Validation loss: 0.03346483, Gradient norm: 0.46643720
INFO:root:[   39] Training loss: 0.01292588, Validation loss: 0.03579029, Gradient norm: 0.42974945
INFO:root:[   40] Training loss: 0.01191228, Validation loss: 0.03637590, Gradient norm: 0.34304470
INFO:root:[   41] Training loss: 0.01330511, Validation loss: 0.03559263, Gradient norm: 0.44107023
INFO:root:[   42] Training loss: 0.01207517, Validation loss: 0.03413399, Gradient norm: 0.36333643
INFO:root:[   43] Training loss: 0.01275085, Validation loss: 0.03325758, Gradient norm: 0.39414622
INFO:root:[   44] Training loss: 0.01190568, Validation loss: 0.03350067, Gradient norm: 0.36889197
INFO:root:[   45] Training loss: 0.01165508, Validation loss: 0.03233797, Gradient norm: 0.34538901
INFO:root:[   46] Training loss: 0.01265020, Validation loss: 0.03426923, Gradient norm: 0.43603580
INFO:root:[   47] Training loss: 0.01144009, Validation loss: 0.03528323, Gradient norm: 0.37758338
INFO:root:[   48] Training loss: 0.01195389, Validation loss: 0.03372996, Gradient norm: 0.36738967
INFO:root:[   49] Training loss: 0.01123841, Validation loss: 0.03493518, Gradient norm: 0.32345188
INFO:root:[   50] Training loss: 0.01127548, Validation loss: 0.03586010, Gradient norm: 0.34968815
INFO:root:[   51] Training loss: 0.01116841, Validation loss: 0.03410058, Gradient norm: 0.31332737
INFO:root:[   52] Training loss: 0.01160250, Validation loss: 0.03585692, Gradient norm: 0.38386778
INFO:root:[   53] Training loss: 0.01065821, Validation loss: 0.03763928, Gradient norm: 0.27369306
INFO:root:[   54] Training loss: 0.01102286, Validation loss: 0.03613872, Gradient norm: 0.29261099
INFO:root:[   55] Training loss: 0.01053590, Validation loss: 0.03615135, Gradient norm: 0.30267641
INFO:root:[   56] Training loss: 0.01211237, Validation loss: 0.03286657, Gradient norm: 0.39263856
INFO:root:[   57] Training loss: 0.01097622, Validation loss: 0.03618745, Gradient norm: 0.35841970
INFO:root:[   58] Training loss: 0.01042384, Validation loss: 0.04061667, Gradient norm: 0.28223267
INFO:root:[   59] Training loss: 0.01110238, Validation loss: 0.03635413, Gradient norm: 0.32415285
INFO:root:[   60] Training loss: 0.01135472, Validation loss: 0.03450932, Gradient norm: 0.38662298
INFO:root:[   61] Training loss: 0.01059055, Validation loss: 0.03810586, Gradient norm: 0.32235683
INFO:root:[   62] Training loss: 0.01095461, Validation loss: 0.04012541, Gradient norm: 0.32580584
INFO:root:[   63] Training loss: 0.30647137, Validation loss: 0.07188978, Gradient norm: 10.35852670
INFO:root:[   64] Training loss: 0.05358960, Validation loss: 0.05323470, Gradient norm: 1.59927115
INFO:root:[   65] Training loss: 0.05442569, Validation loss: 0.05455146, Gradient norm: 4.05754390
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 3828.194s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02497
INFO:root:EnergyScoreTrain: 0.0183
INFO:root:CoverageTrain: 0.83444
INFO:root:IntervalWidthTrain: 0.07721
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03559
INFO:root:EnergyScoreValidation: 0.02676
INFO:root:CoverageValidation: 0.61516
INFO:root:IntervalWidthValidation: 0.06675
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0355
INFO:root:EnergyScoreTest: 0.02669
INFO:root:CoverageTest: 0.61469
INFO:root:IntervalWidthTest: 0.0668
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 150994944
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06072772, Validation loss: 0.03759336, Gradient norm: 0.72366608
INFO:root:[    2] Training loss: 0.03331319, Validation loss: 0.03810876, Gradient norm: 0.58446395
INFO:root:[    3] Training loss: 0.02821159, Validation loss: 0.03494136, Gradient norm: 0.53702616
INFO:root:[    4] Training loss: 0.02574692, Validation loss: 0.02900950, Gradient norm: 0.51608939
INFO:root:[    5] Training loss: 0.02286930, Validation loss: 0.03384419, Gradient norm: 0.48317503
INFO:root:[    6] Training loss: 0.02267188, Validation loss: 0.04202715, Gradient norm: 0.54147975
INFO:root:[    7] Training loss: 0.02112939, Validation loss: 0.02753904, Gradient norm: 0.45771893
INFO:root:[    8] Training loss: 0.01867348, Validation loss: 0.03033629, Gradient norm: 0.39359374
INFO:root:[    9] Training loss: 0.01891518, Validation loss: 0.02470862, Gradient norm: 0.41768111
INFO:root:[   10] Training loss: 0.01933007, Validation loss: 0.03065771, Gradient norm: 0.45929230
INFO:root:[   11] Training loss: 0.01642000, Validation loss: 0.02507285, Gradient norm: 0.32246552
INFO:root:[   12] Training loss: 0.01770044, Validation loss: 0.03447276, Gradient norm: 0.39603203
INFO:root:[   13] Training loss: 0.01843731, Validation loss: 0.03534991, Gradient norm: 0.44834047
INFO:root:[   14] Training loss: 0.01801755, Validation loss: 0.02754728, Gradient norm: 0.43837416
INFO:root:[   15] Training loss: 0.01533522, Validation loss: 0.02636606, Gradient norm: 0.30625713
INFO:root:[   16] Training loss: 0.01593066, Validation loss: 0.02846384, Gradient norm: 0.39666669
INFO:root:[   17] Training loss: 0.01557867, Validation loss: 0.03156287, Gradient norm: 0.35168301
INFO:root:[   18] Training loss: 0.01515331, Validation loss: 0.02974385, Gradient norm: 0.37486580
INFO:root:[   19] Training loss: 0.01466085, Validation loss: 0.02609934, Gradient norm: 0.30899122
INFO:root:[   20] Training loss: 0.01643791, Validation loss: 0.03408571, Gradient norm: 0.42429686
INFO:root:[   21] Training loss: 0.01516076, Validation loss: 0.02975882, Gradient norm: 0.34476426
INFO:root:[   22] Training loss: 0.01423864, Validation loss: 0.03034927, Gradient norm: 0.33219317
INFO:root:[   23] Training loss: 0.01429482, Validation loss: 0.03429587, Gradient norm: 0.33075610
INFO:root:[   24] Training loss: 0.01486712, Validation loss: 0.02921957, Gradient norm: 0.38969071
INFO:root:[   25] Training loss: 0.01419165, Validation loss: 0.03217646, Gradient norm: 0.35039114
INFO:root:[   26] Training loss: 0.01485293, Validation loss: 0.03042317, Gradient norm: 0.35760963
INFO:root:[   27] Training loss: 0.01453533, Validation loss: 0.02736390, Gradient norm: 0.37646660
INFO:root:[   28] Training loss: 0.01388872, Validation loss: 0.03002343, Gradient norm: 0.33782704
INFO:root:[   29] Training loss: 0.01228819, Validation loss: 0.03429361, Gradient norm: 0.24948321
INFO:root:[   30] Training loss: 0.01401404, Validation loss: 0.03467073, Gradient norm: 0.35970709
INFO:root:[   31] Training loss: 0.01282409, Validation loss: 0.03258786, Gradient norm: 0.31142664
INFO:root:[   32] Training loss: 0.01350616, Validation loss: 0.03558210, Gradient norm: 0.33216529
INFO:root:[   33] Training loss: 0.01381954, Validation loss: 0.03101315, Gradient norm: 0.32330811
INFO:root:[   34] Training loss: 0.01320589, Validation loss: 0.03438630, Gradient norm: 0.32383889
INFO:root:[   35] Training loss: 0.01299871, Validation loss: 0.03369249, Gradient norm: 0.32857292
INFO:root:[   36] Training loss: 0.01246824, Validation loss: 0.03525718, Gradient norm: 0.31486750
INFO:root:[   37] Training loss: 0.01189847, Validation loss: 0.03105500, Gradient norm: 0.28356398
INFO:root:[   38] Training loss: 0.01300186, Validation loss: 0.03603370, Gradient norm: 0.33961495
INFO:root:[   39] Training loss: 0.01178827, Validation loss: 0.03369309, Gradient norm: 0.27725989
INFO:root:[   40] Training loss: 0.01268907, Validation loss: 0.03379909, Gradient norm: 0.34277170
INFO:root:[   41] Training loss: 0.01162434, Validation loss: 0.03588815, Gradient norm: 0.27017383
INFO:root:[   42] Training loss: 0.01163986, Validation loss: 0.03314065, Gradient norm: 0.26477204
INFO:root:[   43] Training loss: 0.01189302, Validation loss: 0.03019447, Gradient norm: 0.33011729
INFO:root:[   44] Training loss: 0.01246921, Validation loss: 0.03516359, Gradient norm: 0.33499743
INFO:root:[   45] Training loss: 0.01235516, Validation loss: 0.03101260, Gradient norm: 0.33537385
INFO:root:[   46] Training loss: 0.01084739, Validation loss: 0.03500862, Gradient norm: 0.21121371
INFO:root:[   47] Training loss: 0.01190535, Validation loss: 0.03516443, Gradient norm: 0.30758540
INFO:root:[   48] Training loss: 0.01160066, Validation loss: 0.03308306, Gradient norm: 0.28812841
INFO:root:[   49] Training loss: 0.01183517, Validation loss: 0.03151795, Gradient norm: 0.31767901
INFO:root:[   50] Training loss: 0.01162710, Validation loss: 0.02971784, Gradient norm: 0.31070898
INFO:root:[   51] Training loss: 0.01151783, Validation loss: 0.03108789, Gradient norm: 0.26918977
INFO:root:[   52] Training loss: 0.01224639, Validation loss: 0.03711728, Gradient norm: 0.35888535
INFO:root:[   53] Training loss: 0.01067823, Validation loss: 0.03495579, Gradient norm: 0.26384776
INFO:root:[   54] Training loss: 0.01127785, Validation loss: 0.03104042, Gradient norm: 0.30779734
INFO:root:[   55] Training loss: 0.01055151, Validation loss: 0.03078963, Gradient norm: 0.22158063
INFO:root:[   56] Training loss: 0.01180277, Validation loss: 0.03172213, Gradient norm: 0.33552441
INFO:root:[   57] Training loss: 0.01127966, Validation loss: 0.03193667, Gradient norm: 0.30610193
INFO:root:[   58] Training loss: 0.01068915, Validation loss: 0.03440793, Gradient norm: 0.23383283
INFO:root:[   59] Training loss: 0.01061958, Validation loss: 0.03036251, Gradient norm: 0.23212099
INFO:root:[   60] Training loss: 0.01084819, Validation loss: 0.03180271, Gradient norm: 0.26171426
INFO:root:[   61] Training loss: 0.01066678, Validation loss: 0.03157497, Gradient norm: 0.27823287
INFO:root:[   62] Training loss: 0.01155883, Validation loss: 0.03212654, Gradient norm: 0.32032430
INFO:root:[   63] Training loss: 0.01088339, Validation loss: 0.03876630, Gradient norm: 0.28500411
INFO:root:[   64] Training loss: 0.11201381, Validation loss: 0.11171443, Gradient norm: 1.93301308
INFO:root:[   65] Training loss: 0.11369270, Validation loss: 0.11185626, Gradient norm: 0.04828175
INFO:root:[   66] Training loss: 0.11363214, Validation loss: 0.11331430, Gradient norm: 0.04967561
INFO:root:[   67] Training loss: 0.11376332, Validation loss: 0.11339703, Gradient norm: 0.05035791
INFO:root:[   68] Training loss: 0.11377403, Validation loss: 0.11235131, Gradient norm: 0.05583237
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 3898.496s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02165
INFO:root:EnergyScoreTrain: 0.01585
INFO:root:CoverageTrain: 0.97018
INFO:root:IntervalWidthTrain: 0.09867
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03349
INFO:root:EnergyScoreValidation: 0.02455
INFO:root:CoverageValidation: 0.82285
INFO:root:IntervalWidthValidation: 0.09076
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03371
INFO:root:EnergyScoreTest: 0.02477
INFO:root:CoverageTest: 0.81993
INFO:root:IntervalWidthTest: 0.09078
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 236978176
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05323615, Validation loss: 0.04020356, Gradient norm: 0.66185089
INFO:root:[    2] Training loss: 0.02923513, Validation loss: 0.03099819, Gradient norm: 0.51505154
INFO:root:[    3] Training loss: 0.02741813, Validation loss: 0.02915870, Gradient norm: 0.51588119
INFO:root:[    4] Training loss: 0.02418055, Validation loss: 0.02617919, Gradient norm: 0.47300728
INFO:root:[    5] Training loss: 0.02216422, Validation loss: 0.02962566, Gradient norm: 0.45455073
INFO:root:[    6] Training loss: 0.02115129, Validation loss: 0.02494865, Gradient norm: 0.40001613
INFO:root:[    7] Training loss: 0.01970437, Validation loss: 0.02388622, Gradient norm: 0.36017824
INFO:root:[    8] Training loss: 0.01832118, Validation loss: 0.02457770, Gradient norm: 0.33589035
INFO:root:[    9] Training loss: 0.01875481, Validation loss: 0.02816573, Gradient norm: 0.40301036
INFO:root:[   10] Training loss: 0.01713315, Validation loss: 0.02442642, Gradient norm: 0.31014862
INFO:root:[   11] Training loss: 0.01872320, Validation loss: 0.03202751, Gradient norm: 0.41369145
INFO:root:[   12] Training loss: 0.01741713, Validation loss: 0.02588529, Gradient norm: 0.35228320
INFO:root:[   13] Training loss: 0.01603771, Validation loss: 0.02310189, Gradient norm: 0.29035309
INFO:root:[   14] Training loss: 0.01795109, Validation loss: 0.02511191, Gradient norm: 0.37833845
INFO:root:[   15] Training loss: 0.01533806, Validation loss: 0.03069642, Gradient norm: 0.30521425
INFO:root:[   16] Training loss: 0.01679015, Validation loss: 0.02666990, Gradient norm: 0.39598290
INFO:root:[   17] Training loss: 0.01636357, Validation loss: 0.02926515, Gradient norm: 0.37107795
INFO:root:[   18] Training loss: 0.01513666, Validation loss: 0.02535134, Gradient norm: 0.32107278
INFO:root:[   19] Training loss: 0.01523350, Validation loss: 0.03265023, Gradient norm: 0.33395937
INFO:root:[   20] Training loss: 0.01499501, Validation loss: 0.02508419, Gradient norm: 0.35035381
INFO:root:[   21] Training loss: 0.01415747, Validation loss: 0.02813696, Gradient norm: 0.30465335
INFO:root:[   22] Training loss: 0.01486783, Validation loss: 0.03231728, Gradient norm: 0.34002131
INFO:root:[   23] Training loss: 0.01415821, Validation loss: 0.02978199, Gradient norm: 0.33465886
INFO:root:[   24] Training loss: 0.01381139, Validation loss: 0.02684050, Gradient norm: 0.31034261
INFO:root:[   25] Training loss: 0.01442126, Validation loss: 0.02505630, Gradient norm: 0.34204610
INFO:root:[   26] Training loss: 0.01405107, Validation loss: 0.02802846, Gradient norm: 0.33490578
INFO:root:[   27] Training loss: 0.01310127, Validation loss: 0.02632129, Gradient norm: 0.31891207
INFO:root:[   28] Training loss: 0.01302123, Validation loss: 0.02745339, Gradient norm: 0.31855032
INFO:root:[   29] Training loss: 0.01294441, Validation loss: 0.02791389, Gradient norm: 0.27706944
INFO:root:[   30] Training loss: 0.01351585, Validation loss: 0.03019579, Gradient norm: 0.32433418
INFO:root:[   31] Training loss: 0.01356275, Validation loss: 0.02913563, Gradient norm: 0.33755834
INFO:root:[   32] Training loss: 0.01291580, Validation loss: 0.02709495, Gradient norm: 0.31163885
INFO:root:[   33] Training loss: 0.01386445, Validation loss: 0.02943040, Gradient norm: 0.33225654
INFO:root:[   34] Training loss: 0.01342190, Validation loss: 0.03014180, Gradient norm: 0.35698839
INFO:root:[   35] Training loss: 0.01280669, Validation loss: 0.02857875, Gradient norm: 0.32816278
INFO:root:[   36] Training loss: 0.01229832, Validation loss: 0.03125168, Gradient norm: 0.25934108
INFO:root:[   37] Training loss: 0.01219352, Validation loss: 0.02712120, Gradient norm: 0.30296706
INFO:root:[   38] Training loss: 0.01281821, Validation loss: 0.02939999, Gradient norm: 0.31952067
INFO:root:[   39] Training loss: 0.01211671, Validation loss: 0.02932626, Gradient norm: 0.30030212
INFO:root:[   40] Training loss: 0.01244431, Validation loss: 0.02902476, Gradient norm: 0.30369769
INFO:root:[   41] Training loss: 0.01180047, Validation loss: 0.03135741, Gradient norm: 0.26224173
INFO:root:[   42] Training loss: 0.01203799, Validation loss: 0.02784586, Gradient norm: 0.26683639
INFO:root:[   43] Training loss: 0.01128863, Validation loss: 0.02773385, Gradient norm: 0.25244058
INFO:root:[   44] Training loss: 0.01216055, Validation loss: 0.02606884, Gradient norm: 0.28700237
INFO:root:[   45] Training loss: 0.01195796, Validation loss: 0.03017841, Gradient norm: 0.32123205
INFO:root:[   46] Training loss: 0.01201462, Validation loss: 0.03147237, Gradient norm: 0.25821065
INFO:root:[   47] Training loss: 0.01182859, Validation loss: 0.03854854, Gradient norm: 0.27024444
INFO:root:[   48] Training loss: 0.01255216, Validation loss: 0.02946767, Gradient norm: 0.33102629
INFO:root:[   49] Training loss: 0.01214489, Validation loss: 0.03314635, Gradient norm: 0.28452264
INFO:root:[   50] Training loss: 0.01160809, Validation loss: 0.03050958, Gradient norm: 0.25859957
INFO:root:[   51] Training loss: 0.01160277, Validation loss: 0.03160075, Gradient norm: 0.27420983
INFO:root:[   52] Training loss: 0.01136000, Validation loss: 0.02794211, Gradient norm: 0.28507073
INFO:root:[   53] Training loss: 0.01134054, Validation loss: 0.02981078, Gradient norm: 0.27597223
INFO:root:[   54] Training loss: 0.01179188, Validation loss: 0.03075124, Gradient norm: 0.30428567
INFO:root:[   55] Training loss: 0.01138798, Validation loss: 0.02760424, Gradient norm: 0.26679020
INFO:root:[   56] Training loss: 0.01133627, Validation loss: 0.03019160, Gradient norm: 0.26571077
INFO:root:[   57] Training loss: 0.01148376, Validation loss: 0.03040992, Gradient norm: 0.26577290
INFO:root:[   58] Training loss: 0.01192856, Validation loss: 0.03149538, Gradient norm: 0.29583747
INFO:root:[   59] Training loss: 0.01163396, Validation loss: 0.02931082, Gradient norm: 0.28402196
INFO:root:[   60] Training loss: 0.01132327, Validation loss: 0.02955846, Gradient norm: 0.22767025
INFO:root:[   61] Training loss: 0.01115049, Validation loss: 0.02828846, Gradient norm: 0.28616539
INFO:root:[   62] Training loss: 0.01153271, Validation loss: 0.03036562, Gradient norm: 0.32520926
INFO:root:[   63] Training loss: 0.01160242, Validation loss: 0.03093765, Gradient norm: 0.29286588
INFO:root:[   64] Training loss: 0.01136640, Validation loss: 0.02966877, Gradient norm: 0.26943755
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 3668.121s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0202
INFO:root:EnergyScoreTrain: 0.01467
INFO:root:CoverageTrain: 0.97936
INFO:root:IntervalWidthTrain: 0.10011
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03233
INFO:root:EnergyScoreValidation: 0.02342
INFO:root:CoverageValidation: 0.80829
INFO:root:IntervalWidthValidation: 0.09189
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03255
INFO:root:EnergyScoreTest: 0.02362
INFO:root:CoverageTest: 0.80783
INFO:root:IntervalWidthTest: 0.092
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 134217728
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04961992, Validation loss: 0.03551142, Gradient norm: 0.63676202
INFO:root:[    2] Training loss: 0.02951964, Validation loss: 0.03234305, Gradient norm: 0.45468133
INFO:root:[    3] Training loss: 0.02610205, Validation loss: 0.03508079, Gradient norm: 0.44572324
INFO:root:[    4] Training loss: 0.02258522, Validation loss: 0.02305678, Gradient norm: 0.34967899
INFO:root:[    5] Training loss: 0.02224133, Validation loss: 0.02331315, Gradient norm: 0.39945578
INFO:root:[    6] Training loss: 0.02110604, Validation loss: 0.02593396, Gradient norm: 0.38351216
INFO:root:[    7] Training loss: 0.02157812, Validation loss: 0.02285107, Gradient norm: 0.41487059
INFO:root:[    8] Training loss: 0.01819394, Validation loss: 0.03254170, Gradient norm: 0.25961210
INFO:root:[    9] Training loss: 0.01883970, Validation loss: 0.02266670, Gradient norm: 0.32385291
INFO:root:[   10] Training loss: 0.01873996, Validation loss: 0.02657121, Gradient norm: 0.32434010
INFO:root:[   11] Training loss: 0.01903367, Validation loss: 0.02683291, Gradient norm: 0.37474382
INFO:root:[   12] Training loss: 0.01744831, Validation loss: 0.03328180, Gradient norm: 0.34000895
INFO:root:[   13] Training loss: 0.01713217, Validation loss: 0.02379161, Gradient norm: 0.29830503
INFO:root:[   14] Training loss: 0.01677552, Validation loss: 0.02277796, Gradient norm: 0.32293395
INFO:root:[   15] Training loss: 0.01604158, Validation loss: 0.02867415, Gradient norm: 0.31054125
INFO:root:[   16] Training loss: 0.01619322, Validation loss: 0.02474128, Gradient norm: 0.30595731
INFO:root:[   17] Training loss: 0.01497195, Validation loss: 0.02265605, Gradient norm: 0.25772940
INFO:root:[   18] Training loss: 0.01601286, Validation loss: 0.02310478, Gradient norm: 0.30443787
INFO:root:[   19] Training loss: 0.01722088, Validation loss: 0.02358935, Gradient norm: 0.37324566
INFO:root:[   20] Training loss: 0.01559518, Validation loss: 0.02471974, Gradient norm: 0.33064786
INFO:root:[   21] Training loss: 0.01444179, Validation loss: 0.02400861, Gradient norm: 0.26780021
INFO:root:[   22] Training loss: 0.01470632, Validation loss: 0.02442101, Gradient norm: 0.27530879
INFO:root:[   23] Training loss: 0.01527398, Validation loss: 0.02392449, Gradient norm: 0.32465603
INFO:root:[   24] Training loss: 0.01508475, Validation loss: 0.02346265, Gradient norm: 0.31712469
INFO:root:[   25] Training loss: 0.01434441, Validation loss: 0.02380630, Gradient norm: 0.30008544
INFO:root:[   26] Training loss: 0.01405995, Validation loss: 0.02951332, Gradient norm: 0.28138934
INFO:root:[   27] Training loss: 0.01448890, Validation loss: 0.02613106, Gradient norm: 0.30950781
INFO:root:[   28] Training loss: 0.01429601, Validation loss: 0.03396325, Gradient norm: 0.31841770
INFO:root:[   29] Training loss: 0.01449551, Validation loss: 0.03487243, Gradient norm: 0.31229692
INFO:root:[   30] Training loss: 0.01386330, Validation loss: 0.02735084, Gradient norm: 0.27905143
INFO:root:[   31] Training loss: 0.01334457, Validation loss: 0.03016156, Gradient norm: 0.26568102
INFO:root:[   32] Training loss: 0.01378877, Validation loss: 0.02703716, Gradient norm: 0.27614982
INFO:root:[   33] Training loss: 0.01295153, Validation loss: 0.02772991, Gradient norm: 0.25835814
INFO:root:[   34] Training loss: 0.01355056, Validation loss: 0.03043965, Gradient norm: 0.28296578
INFO:root:[   35] Training loss: 0.01246716, Validation loss: 0.02999795, Gradient norm: 0.24956143
INFO:root:[   36] Training loss: 0.01325926, Validation loss: 0.02996111, Gradient norm: 0.27148956
INFO:root:[   37] Training loss: 0.01327304, Validation loss: 0.03139632, Gradient norm: 0.28433534
INFO:root:[   38] Training loss: 0.01335673, Validation loss: 0.03075024, Gradient norm: 0.29767602
INFO:root:[   39] Training loss: 0.01294041, Validation loss: 0.02594916, Gradient norm: 0.28875539
INFO:root:[   40] Training loss: 0.01305219, Validation loss: 0.03007022, Gradient norm: 0.29567080
INFO:root:[   41] Training loss: 0.01232033, Validation loss: 0.03056366, Gradient norm: 0.24775766
INFO:root:[   42] Training loss: 0.01200455, Validation loss: 0.02649682, Gradient norm: 0.22737358
INFO:root:[   43] Training loss: 0.01303633, Validation loss: 0.02651178, Gradient norm: 0.29792618
INFO:root:[   44] Training loss: 0.01181258, Validation loss: 0.02873556, Gradient norm: 0.22475987
INFO:root:[   45] Training loss: 0.01184784, Validation loss: 0.02851366, Gradient norm: 0.23689782
INFO:root:[   46] Training loss: 0.01234059, Validation loss: 0.02589597, Gradient norm: 0.25601049
INFO:root:[   47] Training loss: 0.01189335, Validation loss: 0.02694913, Gradient norm: 0.24094952
INFO:root:[   48] Training loss: 0.01228119, Validation loss: 0.02457500, Gradient norm: 0.26200523
INFO:root:[   49] Training loss: 0.08895683, Validation loss: 0.06526835, Gradient norm: 2.72780981
INFO:root:[   50] Training loss: 0.04615052, Validation loss: 0.05091895, Gradient norm: 2.88836204
INFO:root:[   51] Training loss: 0.03796940, Validation loss: 0.03865475, Gradient norm: 2.35275148
INFO:root:[   52] Training loss: 0.03583958, Validation loss: 0.03797629, Gradient norm: 1.75346052
INFO:root:[   53] Training loss: 0.02970317, Validation loss: 0.02964011, Gradient norm: 1.48447342
INFO:root:[   54] Training loss: 0.02362495, Validation loss: 0.02808404, Gradient norm: 1.12101100
INFO:root:[   55] Training loss: 0.02121761, Validation loss: 0.03679827, Gradient norm: 0.95498186
INFO:root:[   56] Training loss: 0.01990681, Validation loss: 0.03241122, Gradient norm: 0.83852827
INFO:root:[   57] Training loss: 0.02106954, Validation loss: 0.02560396, Gradient norm: 1.08875495
INFO:root:[   58] Training loss: 0.01685003, Validation loss: 0.02858317, Gradient norm: 0.69208436
INFO:root:[   59] Training loss: 0.01771498, Validation loss: 0.02940695, Gradient norm: 0.84470208
INFO:root:[   60] Training loss: 0.01544623, Validation loss: 0.02984739, Gradient norm: 0.59579846
INFO:root:[   61] Training loss: 0.01661357, Validation loss: 0.03231314, Gradient norm: 0.74126449
INFO:root:[   62] Training loss: 0.01409574, Validation loss: 0.03044520, Gradient norm: 0.41083022
INFO:root:[   63] Training loss: 0.01511406, Validation loss: 0.02692464, Gradient norm: 0.57804258
INFO:root:[   64] Training loss: 0.01453472, Validation loss: 0.02783016, Gradient norm: 0.46584624
INFO:root:[   65] Training loss: 0.01354155, Validation loss: 0.03576401, Gradient norm: 0.41086218
INFO:root:[   66] Training loss: 0.01399875, Validation loss: 0.02930589, Gradient norm: 0.46626968
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 3786.356s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02057
INFO:root:EnergyScoreTrain: 0.01498
INFO:root:CoverageTrain: 0.97641
INFO:root:IntervalWidthTrain: 0.09655
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0314
INFO:root:EnergyScoreValidation: 0.02285
INFO:root:CoverageValidation: 0.79732
INFO:root:IntervalWidthValidation: 0.08732
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03189
INFO:root:EnergyScoreTest: 0.02329
INFO:root:CoverageTest: 0.79405
INFO:root:IntervalWidthTest: 0.08721
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04495650, Validation loss: 0.03212905, Gradient norm: 0.61861151
INFO:root:[    2] Training loss: 0.02966228, Validation loss: 0.03873562, Gradient norm: 0.48801979
INFO:root:[    3] Training loss: 0.02707862, Validation loss: 0.02929830, Gradient norm: 0.43062328
INFO:root:[    4] Training loss: 0.02430312, Validation loss: 0.02542979, Gradient norm: 0.41618882
INFO:root:[    5] Training loss: 0.02212594, Validation loss: 0.02486753, Gradient norm: 0.32618704
INFO:root:[    6] Training loss: 0.02180620, Validation loss: 0.02400454, Gradient norm: 0.35696092
INFO:root:[    7] Training loss: 0.02050434, Validation loss: 0.02181799, Gradient norm: 0.32028313
INFO:root:[    8] Training loss: 0.02178635, Validation loss: 0.02466907, Gradient norm: 0.38386718
INFO:root:[    9] Training loss: 0.01977092, Validation loss: 0.02508127, Gradient norm: 0.35243031
INFO:root:[   10] Training loss: 0.01892925, Validation loss: 0.02978095, Gradient norm: 0.33527353
INFO:root:[   11] Training loss: 0.01771407, Validation loss: 0.02323570, Gradient norm: 0.30740144
INFO:root:[   12] Training loss: 0.01753232, Validation loss: 0.02407732, Gradient norm: 0.31806434
