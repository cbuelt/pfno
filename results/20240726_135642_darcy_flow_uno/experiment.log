INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06512139, Validation loss: 0.03518820, Gradient norm: 1.01610049
INFO:root:[    2] Training loss: 0.03376755, Validation loss: 0.05011676, Gradient norm: 0.84470862
INFO:root:[    3] Training loss: 0.02986430, Validation loss: 0.03027089, Gradient norm: 0.81197624
INFO:root:[    4] Training loss: 0.02665337, Validation loss: 0.03023945, Gradient norm: 0.80808782
INFO:root:[    5] Training loss: 0.02256413, Validation loss: 0.02724972, Gradient norm: 0.56505811
INFO:root:[    6] Training loss: 0.02290157, Validation loss: 0.03040398, Gradient norm: 0.68049550
INFO:root:[    7] Training loss: 0.02297635, Validation loss: 0.02642562, Gradient norm: 0.68282401
INFO:root:[    8] Training loss: 0.01937842, Validation loss: 0.02674355, Gradient norm: 0.51990676
INFO:root:[    9] Training loss: 0.02095944, Validation loss: 0.03802914, Gradient norm: 0.58902579
INFO:root:[   10] Training loss: 0.01921808, Validation loss: 0.02846810, Gradient norm: 0.53707004
INFO:root:[   11] Training loss: 0.01915867, Validation loss: 0.02949327, Gradient norm: 0.58627696
INFO:root:[   12] Training loss: 0.01690791, Validation loss: 0.02821182, Gradient norm: 0.41757455
INFO:root:[   13] Training loss: 0.02043161, Validation loss: 0.02869012, Gradient norm: 0.63567940
INFO:root:[   14] Training loss: 0.01799876, Validation loss: 0.02910502, Gradient norm: 0.55003883
INFO:root:[   15] Training loss: 0.01711865, Validation loss: 0.02913832, Gradient norm: 0.49570301
INFO:root:[   16] Training loss: 0.01708239, Validation loss: 0.02974223, Gradient norm: 0.46062056
INFO:root:[   17] Training loss: 0.01685893, Validation loss: 0.02943871, Gradient norm: 0.48905225
INFO:root:[   18] Training loss: 0.01624764, Validation loss: 0.02851010, Gradient norm: 0.43435202
INFO:root:[   19] Training loss: 0.01590139, Validation loss: 0.03039963, Gradient norm: 0.51231503
INFO:root:[   20] Training loss: 0.01573706, Validation loss: 0.03507880, Gradient norm: 0.44465800
INFO:root:[   21] Training loss: 0.01633150, Validation loss: 0.02828084, Gradient norm: 0.51994415
INFO:root:[   22] Training loss: 0.01560431, Validation loss: 0.03377164, Gradient norm: 0.51849805
INFO:root:[   23] Training loss: 0.01610744, Validation loss: 0.02983446, Gradient norm: 0.49580977
INFO:root:[   24] Training loss: 0.01377454, Validation loss: 0.02980019, Gradient norm: 0.32877297
INFO:root:[   25] Training loss: 0.01441580, Validation loss: 0.03065280, Gradient norm: 0.40855399
INFO:root:[   26] Training loss: 0.01519008, Validation loss: 0.02933649, Gradient norm: 0.47463999
INFO:root:[   27] Training loss: 0.01471102, Validation loss: 0.03342907, Gradient norm: 0.45397436
INFO:root:[   28] Training loss: 0.01511658, Validation loss: 0.03012035, Gradient norm: 0.46578764
INFO:root:[   29] Training loss: 0.01439995, Validation loss: 0.03442158, Gradient norm: 0.44592810
INFO:root:[   30] Training loss: 0.01474430, Validation loss: 0.03230985, Gradient norm: 0.43921725
INFO:root:[   31] Training loss: 0.01396324, Validation loss: 0.03205695, Gradient norm: 0.38602045
INFO:root:[   32] Training loss: 0.01202487, Validation loss: 0.03830448, Gradient norm: 0.28172179
INFO:root:[   33] Training loss: 0.01454240, Validation loss: 0.03134151, Gradient norm: 0.46874669
INFO:root:[   34] Training loss: 0.01299386, Validation loss: 0.03435519, Gradient norm: 0.40837907
INFO:root:[   35] Training loss: 0.01349036, Validation loss: 0.03515188, Gradient norm: 0.41794313
INFO:root:[   36] Training loss: 0.01280935, Validation loss: 0.03067498, Gradient norm: 0.40597476
INFO:root:[   37] Training loss: 0.01306351, Validation loss: 0.03345819, Gradient norm: 0.41959606
INFO:root:[   38] Training loss: 0.01350325, Validation loss: 0.03346483, Gradient norm: 0.46643720
INFO:root:[   39] Training loss: 0.01292588, Validation loss: 0.03579029, Gradient norm: 0.42974945
INFO:root:[   40] Training loss: 0.01191228, Validation loss: 0.03637590, Gradient norm: 0.34304470
INFO:root:[   41] Training loss: 0.01330511, Validation loss: 0.03559263, Gradient norm: 0.44107023
INFO:root:[   42] Training loss: 0.01207517, Validation loss: 0.03413399, Gradient norm: 0.36333643
INFO:root:[   43] Training loss: 0.01275085, Validation loss: 0.03325758, Gradient norm: 0.39414622
INFO:root:[   44] Training loss: 0.01190568, Validation loss: 0.03350067, Gradient norm: 0.36889197
INFO:root:[   45] Training loss: 0.01165508, Validation loss: 0.03233797, Gradient norm: 0.34538901
INFO:root:[   46] Training loss: 0.01265020, Validation loss: 0.03426923, Gradient norm: 0.43603580
INFO:root:[   47] Training loss: 0.01144009, Validation loss: 0.03528323, Gradient norm: 0.37758338
INFO:root:[   48] Training loss: 0.01195389, Validation loss: 0.03372996, Gradient norm: 0.36738967
INFO:root:[   49] Training loss: 0.01123841, Validation loss: 0.03493518, Gradient norm: 0.32345188
INFO:root:[   50] Training loss: 0.01127548, Validation loss: 0.03586010, Gradient norm: 0.34968815
INFO:root:[   51] Training loss: 0.01116841, Validation loss: 0.03410058, Gradient norm: 0.31332737
INFO:root:[   52] Training loss: 0.01160250, Validation loss: 0.03585692, Gradient norm: 0.38386778
INFO:root:[   53] Training loss: 0.01065821, Validation loss: 0.03763928, Gradient norm: 0.27369306
INFO:root:[   54] Training loss: 0.01102286, Validation loss: 0.03613872, Gradient norm: 0.29261099
INFO:root:[   55] Training loss: 0.01053590, Validation loss: 0.03615135, Gradient norm: 0.30267641
INFO:root:[   56] Training loss: 0.01211237, Validation loss: 0.03286657, Gradient norm: 0.39263856
INFO:root:[   57] Training loss: 0.01097622, Validation loss: 0.03618745, Gradient norm: 0.35841970
INFO:root:[   58] Training loss: 0.01042384, Validation loss: 0.04061667, Gradient norm: 0.28223267
INFO:root:[   59] Training loss: 0.01110238, Validation loss: 0.03635413, Gradient norm: 0.32415285
INFO:root:[   60] Training loss: 0.01135472, Validation loss: 0.03450932, Gradient norm: 0.38662298
INFO:root:[   61] Training loss: 0.01059055, Validation loss: 0.03810586, Gradient norm: 0.32235683
INFO:root:[   62] Training loss: 0.01095461, Validation loss: 0.04012541, Gradient norm: 0.32580584
INFO:root:[   63] Training loss: 0.30647137, Validation loss: 0.07188978, Gradient norm: 10.35852670
INFO:root:[   64] Training loss: 0.05358960, Validation loss: 0.05323470, Gradient norm: 1.59927115
INFO:root:[   65] Training loss: 0.05442569, Validation loss: 0.05455146, Gradient norm: 4.05754390
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 3828.194s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02497
INFO:root:EnergyScoreTrain: 0.0183
INFO:root:CoverageTrain: 0.83444
INFO:root:IntervalWidthTrain: 0.07721
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03559
INFO:root:EnergyScoreValidation: 0.02676
INFO:root:CoverageValidation: 0.61516
INFO:root:IntervalWidthValidation: 0.06675
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0355
INFO:root:EnergyScoreTest: 0.02669
INFO:root:CoverageTest: 0.61469
INFO:root:IntervalWidthTest: 0.0668
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 150994944
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06072772, Validation loss: 0.03759336, Gradient norm: 0.72366608
INFO:root:[    2] Training loss: 0.03331319, Validation loss: 0.03810876, Gradient norm: 0.58446395
INFO:root:[    3] Training loss: 0.02821159, Validation loss: 0.03494136, Gradient norm: 0.53702616
INFO:root:[    4] Training loss: 0.02574692, Validation loss: 0.02900950, Gradient norm: 0.51608939
INFO:root:[    5] Training loss: 0.02286930, Validation loss: 0.03384419, Gradient norm: 0.48317503
INFO:root:[    6] Training loss: 0.02267188, Validation loss: 0.04202715, Gradient norm: 0.54147975
INFO:root:[    7] Training loss: 0.02112939, Validation loss: 0.02753904, Gradient norm: 0.45771893
INFO:root:[    8] Training loss: 0.01867348, Validation loss: 0.03033629, Gradient norm: 0.39359374
INFO:root:[    9] Training loss: 0.01891518, Validation loss: 0.02470862, Gradient norm: 0.41768111
INFO:root:[   10] Training loss: 0.01933007, Validation loss: 0.03065771, Gradient norm: 0.45929230
INFO:root:[   11] Training loss: 0.01642000, Validation loss: 0.02507285, Gradient norm: 0.32246552
INFO:root:[   12] Training loss: 0.01770044, Validation loss: 0.03447276, Gradient norm: 0.39603203
INFO:root:[   13] Training loss: 0.01843731, Validation loss: 0.03534991, Gradient norm: 0.44834047
INFO:root:[   14] Training loss: 0.01801755, Validation loss: 0.02754728, Gradient norm: 0.43837416
INFO:root:[   15] Training loss: 0.01533522, Validation loss: 0.02636606, Gradient norm: 0.30625713
INFO:root:[   16] Training loss: 0.01593066, Validation loss: 0.02846384, Gradient norm: 0.39666669
INFO:root:[   17] Training loss: 0.01557867, Validation loss: 0.03156287, Gradient norm: 0.35168301
INFO:root:[   18] Training loss: 0.01515331, Validation loss: 0.02974385, Gradient norm: 0.37486580
INFO:root:[   19] Training loss: 0.01466085, Validation loss: 0.02609934, Gradient norm: 0.30899122
INFO:root:[   20] Training loss: 0.01643791, Validation loss: 0.03408571, Gradient norm: 0.42429686
INFO:root:[   21] Training loss: 0.01516076, Validation loss: 0.02975882, Gradient norm: 0.34476426
INFO:root:[   22] Training loss: 0.01423864, Validation loss: 0.03034927, Gradient norm: 0.33219317
INFO:root:[   23] Training loss: 0.01429482, Validation loss: 0.03429587, Gradient norm: 0.33075610
INFO:root:[   24] Training loss: 0.01486712, Validation loss: 0.02921957, Gradient norm: 0.38969071
INFO:root:[   25] Training loss: 0.01419165, Validation loss: 0.03217646, Gradient norm: 0.35039114
INFO:root:[   26] Training loss: 0.01485293, Validation loss: 0.03042317, Gradient norm: 0.35760963
INFO:root:[   27] Training loss: 0.01453533, Validation loss: 0.02736390, Gradient norm: 0.37646660
INFO:root:[   28] Training loss: 0.01388872, Validation loss: 0.03002343, Gradient norm: 0.33782704
INFO:root:[   29] Training loss: 0.01228819, Validation loss: 0.03429361, Gradient norm: 0.24948321
INFO:root:[   30] Training loss: 0.01401404, Validation loss: 0.03467073, Gradient norm: 0.35970709
INFO:root:[   31] Training loss: 0.01282409, Validation loss: 0.03258786, Gradient norm: 0.31142664
INFO:root:[   32] Training loss: 0.01350616, Validation loss: 0.03558210, Gradient norm: 0.33216529
INFO:root:[   33] Training loss: 0.01381954, Validation loss: 0.03101315, Gradient norm: 0.32330811
INFO:root:[   34] Training loss: 0.01320589, Validation loss: 0.03438630, Gradient norm: 0.32383889
INFO:root:[   35] Training loss: 0.01299871, Validation loss: 0.03369249, Gradient norm: 0.32857292
INFO:root:[   36] Training loss: 0.01246824, Validation loss: 0.03525718, Gradient norm: 0.31486750
INFO:root:[   37] Training loss: 0.01189847, Validation loss: 0.03105500, Gradient norm: 0.28356398
INFO:root:[   38] Training loss: 0.01300186, Validation loss: 0.03603370, Gradient norm: 0.33961495
INFO:root:[   39] Training loss: 0.01178827, Validation loss: 0.03369309, Gradient norm: 0.27725989
INFO:root:[   40] Training loss: 0.01268907, Validation loss: 0.03379909, Gradient norm: 0.34277170
INFO:root:[   41] Training loss: 0.01162434, Validation loss: 0.03588815, Gradient norm: 0.27017383
INFO:root:[   42] Training loss: 0.01163986, Validation loss: 0.03314065, Gradient norm: 0.26477204
INFO:root:[   43] Training loss: 0.01189302, Validation loss: 0.03019447, Gradient norm: 0.33011729
INFO:root:[   44] Training loss: 0.01246921, Validation loss: 0.03516359, Gradient norm: 0.33499743
INFO:root:[   45] Training loss: 0.01235516, Validation loss: 0.03101260, Gradient norm: 0.33537385
INFO:root:[   46] Training loss: 0.01084739, Validation loss: 0.03500862, Gradient norm: 0.21121371
INFO:root:[   47] Training loss: 0.01190535, Validation loss: 0.03516443, Gradient norm: 0.30758540
INFO:root:[   48] Training loss: 0.01160066, Validation loss: 0.03308306, Gradient norm: 0.28812841
INFO:root:[   49] Training loss: 0.01183517, Validation loss: 0.03151795, Gradient norm: 0.31767901
INFO:root:[   50] Training loss: 0.01162710, Validation loss: 0.02971784, Gradient norm: 0.31070898
INFO:root:[   51] Training loss: 0.01151783, Validation loss: 0.03108789, Gradient norm: 0.26918977
INFO:root:[   52] Training loss: 0.01224639, Validation loss: 0.03711728, Gradient norm: 0.35888535
INFO:root:[   53] Training loss: 0.01067823, Validation loss: 0.03495579, Gradient norm: 0.26384776
INFO:root:[   54] Training loss: 0.01127785, Validation loss: 0.03104042, Gradient norm: 0.30779734
INFO:root:[   55] Training loss: 0.01055151, Validation loss: 0.03078963, Gradient norm: 0.22158063
INFO:root:[   56] Training loss: 0.01180277, Validation loss: 0.03172213, Gradient norm: 0.33552441
INFO:root:[   57] Training loss: 0.01127966, Validation loss: 0.03193667, Gradient norm: 0.30610193
INFO:root:[   58] Training loss: 0.01068915, Validation loss: 0.03440793, Gradient norm: 0.23383283
INFO:root:[   59] Training loss: 0.01061958, Validation loss: 0.03036251, Gradient norm: 0.23212099
INFO:root:[   60] Training loss: 0.01084819, Validation loss: 0.03180271, Gradient norm: 0.26171426
INFO:root:[   61] Training loss: 0.01066678, Validation loss: 0.03157497, Gradient norm: 0.27823287
INFO:root:[   62] Training loss: 0.01155883, Validation loss: 0.03212654, Gradient norm: 0.32032430
INFO:root:[   63] Training loss: 0.01088339, Validation loss: 0.03876630, Gradient norm: 0.28500411
INFO:root:[   64] Training loss: 0.11201381, Validation loss: 0.11171443, Gradient norm: 1.93301308
INFO:root:[   65] Training loss: 0.11369270, Validation loss: 0.11185626, Gradient norm: 0.04828175
INFO:root:[   66] Training loss: 0.11363214, Validation loss: 0.11331430, Gradient norm: 0.04967561
INFO:root:[   67] Training loss: 0.11376332, Validation loss: 0.11339703, Gradient norm: 0.05035791
INFO:root:[   68] Training loss: 0.11377403, Validation loss: 0.11235131, Gradient norm: 0.05583237
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 3898.496s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02165
INFO:root:EnergyScoreTrain: 0.01585
INFO:root:CoverageTrain: 0.97018
INFO:root:IntervalWidthTrain: 0.09867
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03349
INFO:root:EnergyScoreValidation: 0.02455
INFO:root:CoverageValidation: 0.82285
INFO:root:IntervalWidthValidation: 0.09076
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03371
INFO:root:EnergyScoreTest: 0.02477
INFO:root:CoverageTest: 0.81993
INFO:root:IntervalWidthTest: 0.09078
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 236978176
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05323615, Validation loss: 0.04020356, Gradient norm: 0.66185089
INFO:root:[    2] Training loss: 0.02923513, Validation loss: 0.03099819, Gradient norm: 0.51505154
INFO:root:[    3] Training loss: 0.02741813, Validation loss: 0.02915870, Gradient norm: 0.51588119
INFO:root:[    4] Training loss: 0.02418055, Validation loss: 0.02617919, Gradient norm: 0.47300728
INFO:root:[    5] Training loss: 0.02216422, Validation loss: 0.02962566, Gradient norm: 0.45455073
INFO:root:[    6] Training loss: 0.02115129, Validation loss: 0.02494865, Gradient norm: 0.40001613
INFO:root:[    7] Training loss: 0.01970437, Validation loss: 0.02388622, Gradient norm: 0.36017824
INFO:root:[    8] Training loss: 0.01832118, Validation loss: 0.02457770, Gradient norm: 0.33589035
INFO:root:[    9] Training loss: 0.01875481, Validation loss: 0.02816573, Gradient norm: 0.40301036
INFO:root:[   10] Training loss: 0.01713315, Validation loss: 0.02442642, Gradient norm: 0.31014862
INFO:root:[   11] Training loss: 0.01872320, Validation loss: 0.03202751, Gradient norm: 0.41369145
INFO:root:[   12] Training loss: 0.01741713, Validation loss: 0.02588529, Gradient norm: 0.35228320
INFO:root:[   13] Training loss: 0.01603771, Validation loss: 0.02310189, Gradient norm: 0.29035309
INFO:root:[   14] Training loss: 0.01795109, Validation loss: 0.02511191, Gradient norm: 0.37833845
INFO:root:[   15] Training loss: 0.01533806, Validation loss: 0.03069642, Gradient norm: 0.30521425
INFO:root:[   16] Training loss: 0.01679015, Validation loss: 0.02666990, Gradient norm: 0.39598290
INFO:root:[   17] Training loss: 0.01636357, Validation loss: 0.02926515, Gradient norm: 0.37107795
INFO:root:[   18] Training loss: 0.01513666, Validation loss: 0.02535134, Gradient norm: 0.32107278
INFO:root:[   19] Training loss: 0.01523350, Validation loss: 0.03265023, Gradient norm: 0.33395937
INFO:root:[   20] Training loss: 0.01499501, Validation loss: 0.02508419, Gradient norm: 0.35035381
INFO:root:[   21] Training loss: 0.01415747, Validation loss: 0.02813696, Gradient norm: 0.30465335
INFO:root:[   22] Training loss: 0.01486783, Validation loss: 0.03231728, Gradient norm: 0.34002131
INFO:root:[   23] Training loss: 0.01415821, Validation loss: 0.02978199, Gradient norm: 0.33465886
INFO:root:[   24] Training loss: 0.01381139, Validation loss: 0.02684050, Gradient norm: 0.31034261
INFO:root:[   25] Training loss: 0.01442126, Validation loss: 0.02505630, Gradient norm: 0.34204610
INFO:root:[   26] Training loss: 0.01405107, Validation loss: 0.02802846, Gradient norm: 0.33490578
INFO:root:[   27] Training loss: 0.01310127, Validation loss: 0.02632129, Gradient norm: 0.31891207
INFO:root:[   28] Training loss: 0.01302123, Validation loss: 0.02745339, Gradient norm: 0.31855032
INFO:root:[   29] Training loss: 0.01294441, Validation loss: 0.02791389, Gradient norm: 0.27706944
INFO:root:[   30] Training loss: 0.01351585, Validation loss: 0.03019579, Gradient norm: 0.32433418
INFO:root:[   31] Training loss: 0.01356275, Validation loss: 0.02913563, Gradient norm: 0.33755834
INFO:root:[   32] Training loss: 0.01291580, Validation loss: 0.02709495, Gradient norm: 0.31163885
INFO:root:[   33] Training loss: 0.01386445, Validation loss: 0.02943040, Gradient norm: 0.33225654
INFO:root:[   34] Training loss: 0.01342190, Validation loss: 0.03014180, Gradient norm: 0.35698839
INFO:root:[   35] Training loss: 0.01280669, Validation loss: 0.02857875, Gradient norm: 0.32816278
INFO:root:[   36] Training loss: 0.01229832, Validation loss: 0.03125168, Gradient norm: 0.25934108
INFO:root:[   37] Training loss: 0.01219352, Validation loss: 0.02712120, Gradient norm: 0.30296706
INFO:root:[   38] Training loss: 0.01281821, Validation loss: 0.02939999, Gradient norm: 0.31952067
INFO:root:[   39] Training loss: 0.01211671, Validation loss: 0.02932626, Gradient norm: 0.30030212
INFO:root:[   40] Training loss: 0.01244431, Validation loss: 0.02902476, Gradient norm: 0.30369769
INFO:root:[   41] Training loss: 0.01180047, Validation loss: 0.03135741, Gradient norm: 0.26224173
INFO:root:[   42] Training loss: 0.01203799, Validation loss: 0.02784586, Gradient norm: 0.26683639
INFO:root:[   43] Training loss: 0.01128863, Validation loss: 0.02773385, Gradient norm: 0.25244058
INFO:root:[   44] Training loss: 0.01216055, Validation loss: 0.02606884, Gradient norm: 0.28700237
INFO:root:[   45] Training loss: 0.01195796, Validation loss: 0.03017841, Gradient norm: 0.32123205
INFO:root:[   46] Training loss: 0.01201462, Validation loss: 0.03147237, Gradient norm: 0.25821065
INFO:root:[   47] Training loss: 0.01182859, Validation loss: 0.03854854, Gradient norm: 0.27024444
INFO:root:[   48] Training loss: 0.01255216, Validation loss: 0.02946767, Gradient norm: 0.33102629
INFO:root:[   49] Training loss: 0.01214489, Validation loss: 0.03314635, Gradient norm: 0.28452264
INFO:root:[   50] Training loss: 0.01160809, Validation loss: 0.03050958, Gradient norm: 0.25859957
INFO:root:[   51] Training loss: 0.01160277, Validation loss: 0.03160075, Gradient norm: 0.27420983
INFO:root:[   52] Training loss: 0.01136000, Validation loss: 0.02794211, Gradient norm: 0.28507073
INFO:root:[   53] Training loss: 0.01134054, Validation loss: 0.02981078, Gradient norm: 0.27597223
INFO:root:[   54] Training loss: 0.01179188, Validation loss: 0.03075124, Gradient norm: 0.30428567
INFO:root:[   55] Training loss: 0.01138798, Validation loss: 0.02760424, Gradient norm: 0.26679020
INFO:root:[   56] Training loss: 0.01133627, Validation loss: 0.03019160, Gradient norm: 0.26571077
INFO:root:[   57] Training loss: 0.01148376, Validation loss: 0.03040992, Gradient norm: 0.26577290
INFO:root:[   58] Training loss: 0.01192856, Validation loss: 0.03149538, Gradient norm: 0.29583747
INFO:root:[   59] Training loss: 0.01163396, Validation loss: 0.02931082, Gradient norm: 0.28402196
INFO:root:[   60] Training loss: 0.01132327, Validation loss: 0.02955846, Gradient norm: 0.22767025
INFO:root:[   61] Training loss: 0.01115049, Validation loss: 0.02828846, Gradient norm: 0.28616539
INFO:root:[   62] Training loss: 0.01153271, Validation loss: 0.03036562, Gradient norm: 0.32520926
INFO:root:[   63] Training loss: 0.01160242, Validation loss: 0.03093765, Gradient norm: 0.29286588
INFO:root:[   64] Training loss: 0.01136640, Validation loss: 0.02966877, Gradient norm: 0.26943755
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 3668.121s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0202
INFO:root:EnergyScoreTrain: 0.01467
INFO:root:CoverageTrain: 0.97936
INFO:root:IntervalWidthTrain: 0.10011
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03233
INFO:root:EnergyScoreValidation: 0.02342
INFO:root:CoverageValidation: 0.80829
INFO:root:IntervalWidthValidation: 0.09189
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03255
INFO:root:EnergyScoreTest: 0.02362
INFO:root:CoverageTest: 0.80783
INFO:root:IntervalWidthTest: 0.092
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 134217728
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04961992, Validation loss: 0.03551142, Gradient norm: 0.63676202
INFO:root:[    2] Training loss: 0.02951964, Validation loss: 0.03234305, Gradient norm: 0.45468133
INFO:root:[    3] Training loss: 0.02610205, Validation loss: 0.03508079, Gradient norm: 0.44572324
INFO:root:[    4] Training loss: 0.02258522, Validation loss: 0.02305678, Gradient norm: 0.34967899
INFO:root:[    5] Training loss: 0.02224133, Validation loss: 0.02331315, Gradient norm: 0.39945578
INFO:root:[    6] Training loss: 0.02110604, Validation loss: 0.02593396, Gradient norm: 0.38351216
INFO:root:[    7] Training loss: 0.02157812, Validation loss: 0.02285107, Gradient norm: 0.41487059
INFO:root:[    8] Training loss: 0.01819394, Validation loss: 0.03254170, Gradient norm: 0.25961210
INFO:root:[    9] Training loss: 0.01883970, Validation loss: 0.02266670, Gradient norm: 0.32385291
INFO:root:[   10] Training loss: 0.01873996, Validation loss: 0.02657121, Gradient norm: 0.32434010
INFO:root:[   11] Training loss: 0.01903367, Validation loss: 0.02683291, Gradient norm: 0.37474382
INFO:root:[   12] Training loss: 0.01744831, Validation loss: 0.03328180, Gradient norm: 0.34000895
INFO:root:[   13] Training loss: 0.01713217, Validation loss: 0.02379161, Gradient norm: 0.29830503
INFO:root:[   14] Training loss: 0.01677552, Validation loss: 0.02277796, Gradient norm: 0.32293395
INFO:root:[   15] Training loss: 0.01604158, Validation loss: 0.02867415, Gradient norm: 0.31054125
INFO:root:[   16] Training loss: 0.01619322, Validation loss: 0.02474128, Gradient norm: 0.30595731
INFO:root:[   17] Training loss: 0.01497195, Validation loss: 0.02265605, Gradient norm: 0.25772940
INFO:root:[   18] Training loss: 0.01601286, Validation loss: 0.02310478, Gradient norm: 0.30443787
INFO:root:[   19] Training loss: 0.01722088, Validation loss: 0.02358935, Gradient norm: 0.37324566
INFO:root:[   20] Training loss: 0.01559518, Validation loss: 0.02471974, Gradient norm: 0.33064786
INFO:root:[   21] Training loss: 0.01444179, Validation loss: 0.02400861, Gradient norm: 0.26780021
INFO:root:[   22] Training loss: 0.01470632, Validation loss: 0.02442101, Gradient norm: 0.27530879
INFO:root:[   23] Training loss: 0.01527398, Validation loss: 0.02392449, Gradient norm: 0.32465603
INFO:root:[   24] Training loss: 0.01508475, Validation loss: 0.02346265, Gradient norm: 0.31712469
INFO:root:[   25] Training loss: 0.01434441, Validation loss: 0.02380630, Gradient norm: 0.30008544
INFO:root:[   26] Training loss: 0.01405995, Validation loss: 0.02951332, Gradient norm: 0.28138934
INFO:root:[   27] Training loss: 0.01448890, Validation loss: 0.02613106, Gradient norm: 0.30950781
INFO:root:[   28] Training loss: 0.01429601, Validation loss: 0.03396325, Gradient norm: 0.31841770
INFO:root:[   29] Training loss: 0.01449551, Validation loss: 0.03487243, Gradient norm: 0.31229692
INFO:root:[   30] Training loss: 0.01386330, Validation loss: 0.02735084, Gradient norm: 0.27905143
INFO:root:[   31] Training loss: 0.01334457, Validation loss: 0.03016156, Gradient norm: 0.26568102
INFO:root:[   32] Training loss: 0.01378877, Validation loss: 0.02703716, Gradient norm: 0.27614982
INFO:root:[   33] Training loss: 0.01295153, Validation loss: 0.02772991, Gradient norm: 0.25835814
INFO:root:[   34] Training loss: 0.01355056, Validation loss: 0.03043965, Gradient norm: 0.28296578
INFO:root:[   35] Training loss: 0.01246716, Validation loss: 0.02999795, Gradient norm: 0.24956143
INFO:root:[   36] Training loss: 0.01325926, Validation loss: 0.02996111, Gradient norm: 0.27148956
INFO:root:[   37] Training loss: 0.01327304, Validation loss: 0.03139632, Gradient norm: 0.28433534
INFO:root:[   38] Training loss: 0.01335673, Validation loss: 0.03075024, Gradient norm: 0.29767602
INFO:root:[   39] Training loss: 0.01294041, Validation loss: 0.02594916, Gradient norm: 0.28875539
INFO:root:[   40] Training loss: 0.01305219, Validation loss: 0.03007022, Gradient norm: 0.29567080
INFO:root:[   41] Training loss: 0.01232033, Validation loss: 0.03056366, Gradient norm: 0.24775766
INFO:root:[   42] Training loss: 0.01200455, Validation loss: 0.02649682, Gradient norm: 0.22737358
INFO:root:[   43] Training loss: 0.01303633, Validation loss: 0.02651178, Gradient norm: 0.29792618
INFO:root:[   44] Training loss: 0.01181258, Validation loss: 0.02873556, Gradient norm: 0.22475987
INFO:root:[   45] Training loss: 0.01184784, Validation loss: 0.02851366, Gradient norm: 0.23689782
INFO:root:[   46] Training loss: 0.01234059, Validation loss: 0.02589597, Gradient norm: 0.25601049
INFO:root:[   47] Training loss: 0.01189335, Validation loss: 0.02694913, Gradient norm: 0.24094952
INFO:root:[   48] Training loss: 0.01228119, Validation loss: 0.02457500, Gradient norm: 0.26200523
INFO:root:[   49] Training loss: 0.08895683, Validation loss: 0.06526835, Gradient norm: 2.72780981
INFO:root:[   50] Training loss: 0.04615052, Validation loss: 0.05091895, Gradient norm: 2.88836204
INFO:root:[   51] Training loss: 0.03796940, Validation loss: 0.03865475, Gradient norm: 2.35275148
INFO:root:[   52] Training loss: 0.03583958, Validation loss: 0.03797629, Gradient norm: 1.75346052
INFO:root:[   53] Training loss: 0.02970317, Validation loss: 0.02964011, Gradient norm: 1.48447342
INFO:root:[   54] Training loss: 0.02362495, Validation loss: 0.02808404, Gradient norm: 1.12101100
INFO:root:[   55] Training loss: 0.02121761, Validation loss: 0.03679827, Gradient norm: 0.95498186
INFO:root:[   56] Training loss: 0.01990681, Validation loss: 0.03241122, Gradient norm: 0.83852827
INFO:root:[   57] Training loss: 0.02106954, Validation loss: 0.02560396, Gradient norm: 1.08875495
INFO:root:[   58] Training loss: 0.01685003, Validation loss: 0.02858317, Gradient norm: 0.69208436
INFO:root:[   59] Training loss: 0.01771498, Validation loss: 0.02940695, Gradient norm: 0.84470208
INFO:root:[   60] Training loss: 0.01544623, Validation loss: 0.02984739, Gradient norm: 0.59579846
INFO:root:[   61] Training loss: 0.01661357, Validation loss: 0.03231314, Gradient norm: 0.74126449
INFO:root:[   62] Training loss: 0.01409574, Validation loss: 0.03044520, Gradient norm: 0.41083022
INFO:root:[   63] Training loss: 0.01511406, Validation loss: 0.02692464, Gradient norm: 0.57804258
INFO:root:[   64] Training loss: 0.01453472, Validation loss: 0.02783016, Gradient norm: 0.46584624
INFO:root:[   65] Training loss: 0.01354155, Validation loss: 0.03576401, Gradient norm: 0.41086218
INFO:root:[   66] Training loss: 0.01399875, Validation loss: 0.02930589, Gradient norm: 0.46626968
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 3786.356s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02057
INFO:root:EnergyScoreTrain: 0.01498
INFO:root:CoverageTrain: 0.97641
INFO:root:IntervalWidthTrain: 0.09655
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0314
INFO:root:EnergyScoreValidation: 0.02285
INFO:root:CoverageValidation: 0.79732
INFO:root:IntervalWidthValidation: 0.08732
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03189
INFO:root:EnergyScoreTest: 0.02329
INFO:root:CoverageTest: 0.79405
INFO:root:IntervalWidthTest: 0.08721
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04495650, Validation loss: 0.03212905, Gradient norm: 0.61861151
INFO:root:[    2] Training loss: 0.02966228, Validation loss: 0.03873562, Gradient norm: 0.48801979
INFO:root:[    3] Training loss: 0.02707862, Validation loss: 0.02929830, Gradient norm: 0.43062328
INFO:root:[    4] Training loss: 0.02430312, Validation loss: 0.02542979, Gradient norm: 0.41618882
INFO:root:[    5] Training loss: 0.02212594, Validation loss: 0.02486753, Gradient norm: 0.32618704
INFO:root:[    6] Training loss: 0.02180620, Validation loss: 0.02400454, Gradient norm: 0.35696092
INFO:root:[    7] Training loss: 0.02050434, Validation loss: 0.02181799, Gradient norm: 0.32028313
INFO:root:[    8] Training loss: 0.02178635, Validation loss: 0.02466907, Gradient norm: 0.38386718
INFO:root:[    9] Training loss: 0.01977092, Validation loss: 0.02508127, Gradient norm: 0.35243031
INFO:root:[   10] Training loss: 0.01892925, Validation loss: 0.02978095, Gradient norm: 0.33527353
INFO:root:[   11] Training loss: 0.01771407, Validation loss: 0.02323570, Gradient norm: 0.30740144
INFO:root:[   12] Training loss: 0.01753232, Validation loss: 0.02407732, Gradient norm: 0.31806434
INFO:root:[   13] Training loss: 0.01684929, Validation loss: 0.02531791, Gradient norm: 0.27415428
INFO:root:[   14] Training loss: 0.01786781, Validation loss: 0.02422410, Gradient norm: 0.34588741
INFO:root:[   15] Training loss: 0.01668448, Validation loss: 0.03014726, Gradient norm: 0.33059606
INFO:root:[   16] Training loss: 0.01638833, Validation loss: 0.02293430, Gradient norm: 0.31422182
INFO:root:[   17] Training loss: 0.01553805, Validation loss: 0.02403856, Gradient norm: 0.28257778
INFO:root:[   18] Training loss: 0.01560154, Validation loss: 0.02388258, Gradient norm: 0.32842804
INFO:root:[   19] Training loss: 0.01465014, Validation loss: 0.02328644, Gradient norm: 0.27336572
INFO:root:[   20] Training loss: 0.01532022, Validation loss: 0.02510256, Gradient norm: 0.30022808
INFO:root:[   21] Training loss: 0.01413696, Validation loss: 0.02574081, Gradient norm: 0.25300278
INFO:root:[   22] Training loss: 0.01574799, Validation loss: 0.02385634, Gradient norm: 0.34846987
INFO:root:[   23] Training loss: 0.01559490, Validation loss: 0.02543011, Gradient norm: 0.37081266
INFO:root:[   24] Training loss: 0.01326014, Validation loss: 0.03030344, Gradient norm: 0.21109805
INFO:root:[   25] Training loss: 0.01374042, Validation loss: 0.02471094, Gradient norm: 0.26322983
INFO:root:[   26] Training loss: 0.01357470, Validation loss: 0.02172537, Gradient norm: 0.25497452
INFO:root:[   27] Training loss: 0.01365716, Validation loss: 0.02434781, Gradient norm: 0.27055977
INFO:root:[   28] Training loss: 0.01357742, Validation loss: 0.02586631, Gradient norm: 0.25866624
INFO:root:[   29] Training loss: 0.01467105, Validation loss: 0.02647782, Gradient norm: 0.30046432
INFO:root:[   30] Training loss: 0.01290624, Validation loss: 0.03407009, Gradient norm: 0.24685254
INFO:root:[   31] Training loss: 0.01413855, Validation loss: 0.02356198, Gradient norm: 0.30270424
INFO:root:[   32] Training loss: 0.01465170, Validation loss: 0.02613624, Gradient norm: 0.30536595
INFO:root:[   33] Training loss: 0.01301839, Validation loss: 0.02663659, Gradient norm: 0.23934770
INFO:root:[   34] Training loss: 0.01365631, Validation loss: 0.02826733, Gradient norm: 0.27416788
INFO:root:[   35] Training loss: 0.01268613, Validation loss: 0.02832683, Gradient norm: 0.20210780
INFO:root:[   36] Training loss: 0.01367948, Validation loss: 0.02612008, Gradient norm: 0.30681794
INFO:root:[   37] Training loss: 0.01318691, Validation loss: 0.02416708, Gradient norm: 0.25196255
INFO:root:[   38] Training loss: 0.01331662, Validation loss: 0.02977346, Gradient norm: 0.29629252
INFO:root:[   39] Training loss: 0.01248053, Validation loss: 0.02367291, Gradient norm: 0.23465332
INFO:root:[   40] Training loss: 0.01264517, Validation loss: 0.02600507, Gradient norm: 0.24035137
INFO:root:[   41] Training loss: 0.01307970, Validation loss: 0.02365117, Gradient norm: 0.25082594
INFO:root:[   42] Training loss: 0.01232006, Validation loss: 0.02854922, Gradient norm: 0.23927791
INFO:root:[   43] Training loss: 0.01244419, Validation loss: 0.02277749, Gradient norm: 0.24260498
INFO:root:[   44] Training loss: 0.01249211, Validation loss: 0.03498759, Gradient norm: 0.25677733
INFO:root:[   45] Training loss: 0.01313643, Validation loss: 0.02697202, Gradient norm: 0.25764795
INFO:root:[   46] Training loss: 0.01321588, Validation loss: 0.02567361, Gradient norm: 0.28688204
INFO:root:[   47] Training loss: 0.01251704, Validation loss: 0.02694775, Gradient norm: 0.24014700
INFO:root:[   48] Training loss: 0.01253866, Validation loss: 0.03007534, Gradient norm: 0.24675105
INFO:root:[   49] Training loss: 0.01233171, Validation loss: 0.02386513, Gradient norm: 0.25422894
INFO:root:[   50] Training loss: 0.01182305, Validation loss: 0.02680685, Gradient norm: 0.19567867
INFO:root:[   51] Training loss: 0.01260836, Validation loss: 0.02682004, Gradient norm: 0.26087340
INFO:root:[   52] Training loss: 0.01229603, Validation loss: 0.02286001, Gradient norm: 0.22238552
INFO:root:[   53] Training loss: 0.01219820, Validation loss: 0.02528481, Gradient norm: 0.24374991
INFO:root:[   54] Training loss: 0.01184284, Validation loss: 0.02402460, Gradient norm: 0.22324563
INFO:root:[   55] Training loss: 0.01219359, Validation loss: 0.02513090, Gradient norm: 0.23496574
INFO:root:[   56] Training loss: 0.01272515, Validation loss: 0.03068179, Gradient norm: 0.27065492
INFO:root:[   57] Training loss: 0.01242942, Validation loss: 0.02699545, Gradient norm: 0.24615106
INFO:root:[   58] Training loss: 0.01241283, Validation loss: 0.02823005, Gradient norm: 0.26261979
INFO:root:[   59] Training loss: 0.01299530, Validation loss: 0.02591479, Gradient norm: 0.29578540
INFO:root:[   60] Training loss: 0.01227215, Validation loss: 0.02779360, Gradient norm: 0.25393605
INFO:root:[   61] Training loss: 0.01170659, Validation loss: 0.02895807, Gradient norm: 0.20527513
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 108766.925s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02277
INFO:root:EnergyScoreTrain: 0.01698
INFO:root:CoverageTrain: 0.99015
INFO:root:IntervalWidthTrain: 0.10833
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03025
INFO:root:EnergyScoreValidation: 0.02172
INFO:root:CoverageValidation: 0.82058
INFO:root:IntervalWidthValidation: 0.09751
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03063
INFO:root:EnergyScoreTest: 0.02201
INFO:root:CoverageTest: 0.81751
INFO:root:IntervalWidthTest: 0.09732
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05988331, Validation loss: 0.04953222, Gradient norm: 0.78754358
INFO:root:[    2] Training loss: 0.03317634, Validation loss: 0.03184387, Gradient norm: 0.41697260
INFO:root:[    3] Training loss: 0.03058247, Validation loss: 0.02964830, Gradient norm: 0.43406082
INFO:root:[    4] Training loss: 0.02744224, Validation loss: 0.02754832, Gradient norm: 0.43869722
INFO:root:[    5] Training loss: 0.02610182, Validation loss: 0.02644764, Gradient norm: 0.39308594
INFO:root:[    6] Training loss: 0.02353030, Validation loss: 0.02525601, Gradient norm: 0.35194790
INFO:root:[    7] Training loss: 0.02365519, Validation loss: 0.02429490, Gradient norm: 0.41932219
INFO:root:[    8] Training loss: 0.02226860, Validation loss: 0.03180569, Gradient norm: 0.36136337
INFO:root:[    9] Training loss: 0.02029388, Validation loss: 0.02168947, Gradient norm: 0.31014834
INFO:root:[   10] Training loss: 0.01960283, Validation loss: 0.02289051, Gradient norm: 0.30690143
INFO:root:[   11] Training loss: 0.02057553, Validation loss: 0.02440020, Gradient norm: 0.37481511
INFO:root:[   12] Training loss: 0.01991890, Validation loss: 0.02907120, Gradient norm: 0.36309511
INFO:root:[   13] Training loss: 0.01812822, Validation loss: 0.02881679, Gradient norm: 0.27344421
INFO:root:[   14] Training loss: 0.01886155, Validation loss: 0.02567494, Gradient norm: 0.36706796
INFO:root:[   15] Training loss: 0.01711018, Validation loss: 0.02616965, Gradient norm: 0.26132415
INFO:root:[   16] Training loss: 0.01793059, Validation loss: 0.02452900, Gradient norm: 0.29732288
INFO:root:[   17] Training loss: 0.01719393, Validation loss: 0.02324845, Gradient norm: 0.29631343
INFO:root:[   18] Training loss: 0.01661987, Validation loss: 0.02289238, Gradient norm: 0.26895046
INFO:root:[   19] Training loss: 0.01684974, Validation loss: 0.02122110, Gradient norm: 0.29709677
INFO:root:[   20] Training loss: 0.01484910, Validation loss: 0.02179520, Gradient norm: 0.17653464
INFO:root:[   21] Training loss: 0.01715321, Validation loss: 0.02940140, Gradient norm: 0.31048327
INFO:root:[   22] Training loss: 0.01729431, Validation loss: 0.02931530, Gradient norm: 0.31859990
INFO:root:[   23] Training loss: 0.01738941, Validation loss: 0.02367779, Gradient norm: 0.34875630
INFO:root:[   24] Training loss: 0.01527525, Validation loss: 0.02228954, Gradient norm: 0.23861058
INFO:root:[   25] Training loss: 0.01601060, Validation loss: 0.02730308, Gradient norm: 0.28710447
INFO:root:[   26] Training loss: 0.01482115, Validation loss: 0.02393891, Gradient norm: 0.21894380
INFO:root:[   27] Training loss: 0.01563407, Validation loss: 0.02218251, Gradient norm: 0.29211344
INFO:root:[   28] Training loss: 0.01463710, Validation loss: 0.02689705, Gradient norm: 0.20892916
INFO:root:[   29] Training loss: 0.01502260, Validation loss: 0.02622077, Gradient norm: 0.23816542
INFO:root:[   30] Training loss: 0.01554723, Validation loss: 0.02357466, Gradient norm: 0.28606977
INFO:root:[   31] Training loss: 0.01556641, Validation loss: 0.03328999, Gradient norm: 0.27404827
INFO:root:[   32] Training loss: 0.01506363, Validation loss: 0.02441942, Gradient norm: 0.26897608
INFO:root:[   33] Training loss: 0.01465325, Validation loss: 0.02834508, Gradient norm: 0.22648220
INFO:root:[   34] Training loss: 0.01418774, Validation loss: 0.02692840, Gradient norm: 0.23161447
INFO:root:[   35] Training loss: 0.01401481, Validation loss: 0.02759586, Gradient norm: 0.21988476
INFO:root:[   36] Training loss: 0.01416931, Validation loss: 0.02479775, Gradient norm: 0.23265512
INFO:root:[   37] Training loss: 0.01491496, Validation loss: 0.03126836, Gradient norm: 0.27583382
INFO:root:[   38] Training loss: 0.01438953, Validation loss: 0.02484945, Gradient norm: 0.24800324
INFO:root:[   39] Training loss: 0.01508017, Validation loss: 0.03145385, Gradient norm: 0.28517585
INFO:root:[   40] Training loss: 0.01391712, Validation loss: 0.02920472, Gradient norm: 0.23418304
INFO:root:[   41] Training loss: 0.01407193, Validation loss: 0.02676837, Gradient norm: 0.24375086
INFO:root:[   42] Training loss: 0.01369661, Validation loss: 0.03519857, Gradient norm: 0.22815051
INFO:root:[   43] Training loss: 0.01453558, Validation loss: 0.03102807, Gradient norm: 0.28013819
INFO:root:[   44] Training loss: 0.01359748, Validation loss: 0.02947411, Gradient norm: 0.22762329
INFO:root:[   45] Training loss: 0.01430888, Validation loss: 0.02860578, Gradient norm: 0.26605654
INFO:root:[   46] Training loss: 0.01394051, Validation loss: 0.02689123, Gradient norm: 0.24550860
INFO:root:[   47] Training loss: 0.01370090, Validation loss: 0.02744392, Gradient norm: 0.21702837
INFO:root:[   48] Training loss: 0.01352905, Validation loss: 0.03418831, Gradient norm: 0.24508662
INFO:root:[   49] Training loss: 0.01390501, Validation loss: 0.02837957, Gradient norm: 0.25013265
INFO:root:[   50] Training loss: 0.01403897, Validation loss: 0.02266256, Gradient norm: 0.25413096
INFO:root:[   51] Training loss: 0.01322732, Validation loss: 0.02328867, Gradient norm: 0.21576471
INFO:root:[   52] Training loss: 0.01376050, Validation loss: 0.02595933, Gradient norm: 0.22486879
INFO:root:[   53] Training loss: 0.01329509, Validation loss: 0.02838690, Gradient norm: 0.21093937
INFO:root:[   54] Training loss: 0.01284429, Validation loss: 0.02422265, Gradient norm: 0.20472076
INFO:root:[   55] Training loss: 0.01300940, Validation loss: 0.02487253, Gradient norm: 0.21381540
INFO:root:[   56] Training loss: 0.01420929, Validation loss: 0.03485624, Gradient norm: 0.27799719
INFO:root:[   57] Training loss: 0.01333834, Validation loss: 0.02701888, Gradient norm: 0.23583570
INFO:root:[   58] Training loss: 0.01295805, Validation loss: 0.03030449, Gradient norm: 0.22529441
INFO:root:[   59] Training loss: 0.01309415, Validation loss: 0.02522462, Gradient norm: 0.22198047
INFO:root:[   60] Training loss: 0.01317256, Validation loss: 0.02340861, Gradient norm: 0.23918723
INFO:root:[   61] Training loss: 0.01282462, Validation loss: 0.02571628, Gradient norm: 0.21119864
INFO:root:[   62] Training loss: 0.01288295, Validation loss: 0.02223862, Gradient norm: 0.21760836
INFO:root:[   63] Training loss: 0.01325066, Validation loss: 0.02211704, Gradient norm: 0.22319390
INFO:root:[   64] Training loss: 0.01312451, Validation loss: 0.03094459, Gradient norm: 0.23340862
INFO:root:[   65] Training loss: 0.01319783, Validation loss: 0.02398133, Gradient norm: 0.24780189
INFO:root:[   66] Training loss: 0.01288844, Validation loss: 0.02346306, Gradient norm: 0.21861647
INFO:root:[   67] Training loss: 0.01282304, Validation loss: 0.02524492, Gradient norm: 0.22659158
INFO:root:[   68] Training loss: 0.01285721, Validation loss: 0.03085764, Gradient norm: 0.21855076
INFO:root:[   69] Training loss: 0.01298014, Validation loss: 0.02672116, Gradient norm: 0.22197708
INFO:root:[   70] Training loss: 0.01299139, Validation loss: 0.02529499, Gradient norm: 0.23920469
INFO:root:[   71] Training loss: 0.01284167, Validation loss: 0.03053655, Gradient norm: 0.22769480
INFO:root:[   72] Training loss: 0.01258309, Validation loss: 0.02295108, Gradient norm: 0.21137359
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 4133.372s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02277
INFO:root:EnergyScoreTrain: 0.01727
INFO:root:CoverageTrain: 0.9936
INFO:root:IntervalWidthTrain: 0.12997
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02966
INFO:root:EnergyScoreValidation: 0.02134
INFO:root:CoverageValidation: 0.92217
INFO:root:IntervalWidthValidation: 0.11446
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03008
INFO:root:EnergyScoreTest: 0.02168
INFO:root:CoverageTest: 0.91902
INFO:root:IntervalWidthTest: 0.11435
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 268435456
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06782803, Validation loss: 0.03773170, Gradient norm: 1.04224274
INFO:root:[    2] Training loss: 0.03398887, Validation loss: 0.02836620, Gradient norm: 0.68483761
INFO:root:[    3] Training loss: 0.02954775, Validation loss: 0.02948834, Gradient norm: 0.61119133
INFO:root:[    4] Training loss: 0.02538486, Validation loss: 0.02949834, Gradient norm: 0.49964306
INFO:root:[    5] Training loss: 0.02461177, Validation loss: 0.02885690, Gradient norm: 0.52216640
INFO:root:[    6] Training loss: 0.02114681, Validation loss: 0.03207173, Gradient norm: 0.36482614
INFO:root:[    7] Training loss: 0.02305888, Validation loss: 0.02538842, Gradient norm: 0.54701619
INFO:root:[    8] Training loss: 0.01998727, Validation loss: 0.02554455, Gradient norm: 0.36827156
INFO:root:[    9] Training loss: 0.02170954, Validation loss: 0.02834203, Gradient norm: 0.47971343
INFO:root:[   10] Training loss: 0.02083633, Validation loss: 0.02302892, Gradient norm: 0.46389999
INFO:root:[   11] Training loss: 0.01974809, Validation loss: 0.02708606, Gradient norm: 0.44922987
INFO:root:[   12] Training loss: 0.01744986, Validation loss: 0.02516517, Gradient norm: 0.36095480
INFO:root:[   13] Training loss: 0.01730209, Validation loss: 0.02704659, Gradient norm: 0.36756951
INFO:root:[   14] Training loss: 0.01841264, Validation loss: 0.03203200, Gradient norm: 0.37684319
INFO:root:[   15] Training loss: 0.01722964, Validation loss: 0.02561839, Gradient norm: 0.38133637
INFO:root:[   16] Training loss: 0.01575432, Validation loss: 0.02785161, Gradient norm: 0.24959502
INFO:root:[   17] Training loss: 0.01782616, Validation loss: 0.02568866, Gradient norm: 0.42403253
INFO:root:[   18] Training loss: 0.01687652, Validation loss: 0.02855591, Gradient norm: 0.32830495
INFO:root:[   19] Training loss: 0.01602863, Validation loss: 0.02533440, Gradient norm: 0.35865313
INFO:root:[   20] Training loss: 0.01659804, Validation loss: 0.02976335, Gradient norm: 0.36044168
INFO:root:[   21] Training loss: 0.01610409, Validation loss: 0.02549270, Gradient norm: 0.34502617
INFO:root:[   22] Training loss: 0.01531762, Validation loss: 0.03041844, Gradient norm: 0.35845147
INFO:root:[   23] Training loss: 0.01533285, Validation loss: 0.03671970, Gradient norm: 0.32824178
INFO:root:[   24] Training loss: 0.01453248, Validation loss: 0.02964286, Gradient norm: 0.28094350
INFO:root:[   25] Training loss: 0.01610597, Validation loss: 0.02980880, Gradient norm: 0.38377498
INFO:root:[   26] Training loss: 0.01470215, Validation loss: 0.02786893, Gradient norm: 0.33550134
INFO:root:[   27] Training loss: 0.01516554, Validation loss: 0.03209318, Gradient norm: 0.36981384
INFO:root:[   28] Training loss: 0.01518589, Validation loss: 0.02771656, Gradient norm: 0.35614639
INFO:root:[   29] Training loss: 0.01380858, Validation loss: 0.02613691, Gradient norm: 0.30869549
INFO:root:[   30] Training loss: 0.01492804, Validation loss: 0.03117371, Gradient norm: 0.36623354
INFO:root:[   31] Training loss: 0.01416182, Validation loss: 0.02790285, Gradient norm: 0.33936789
INFO:root:[   32] Training loss: 0.01372666, Validation loss: 0.03134324, Gradient norm: 0.32250834
INFO:root:[   33] Training loss: 0.01424962, Validation loss: 0.03083767, Gradient norm: 0.33841243
INFO:root:[   34] Training loss: 0.01357075, Validation loss: 0.02728256, Gradient norm: 0.31421817
INFO:root:[   35] Training loss: 0.01372364, Validation loss: 0.02850942, Gradient norm: 0.33726976
INFO:root:[   36] Training loss: 0.01343593, Validation loss: 0.02885069, Gradient norm: 0.33165552
INFO:root:[   37] Training loss: 0.01364481, Validation loss: 0.03296699, Gradient norm: 0.34016424
INFO:root:[   38] Training loss: 0.01363370, Validation loss: 0.03000841, Gradient norm: 0.31645584
INFO:root:[   39] Training loss: 0.01254390, Validation loss: 0.03042332, Gradient norm: 0.25213819
INFO:root:[   40] Training loss: 0.01341920, Validation loss: 0.03351774, Gradient norm: 0.35715582
INFO:root:[   41] Training loss: 0.01297828, Validation loss: 0.03114621, Gradient norm: 0.29969557
INFO:root:[   42] Training loss: 0.01268079, Validation loss: 0.03051042, Gradient norm: 0.31476699
INFO:root:[   43] Training loss: 0.01281151, Validation loss: 0.03660650, Gradient norm: 0.32068866
INFO:root:[   44] Training loss: 0.01226570, Validation loss: 0.03232873, Gradient norm: 0.24272279
INFO:root:[   45] Training loss: 0.01225676, Validation loss: 0.02997953, Gradient norm: 0.25931073
INFO:root:[   46] Training loss: 0.01322249, Validation loss: 0.03391482, Gradient norm: 0.33917725
INFO:root:[   47] Training loss: 0.01273312, Validation loss: 0.03509151, Gradient norm: 0.29115080
INFO:root:[   48] Training loss: 0.01242042, Validation loss: 0.03029571, Gradient norm: 0.28716521
INFO:root:[   49] Training loss: 0.01264904, Validation loss: 0.03042787, Gradient norm: 0.33111005
INFO:root:[   50] Training loss: 0.01253486, Validation loss: 0.02841581, Gradient norm: 0.31375159
INFO:root:[   51] Training loss: 0.01181357, Validation loss: 0.03104571, Gradient norm: 0.26815845
INFO:root:[   52] Training loss: 0.01194827, Validation loss: 0.02827790, Gradient norm: 0.29351475
INFO:root:[   53] Training loss: 0.01204483, Validation loss: 0.02925903, Gradient norm: 0.26761290
INFO:root:[   54] Training loss: 0.01208837, Validation loss: 0.02756411, Gradient norm: 0.31114079
INFO:root:[   55] Training loss: 0.11501009, Validation loss: 0.06124103, Gradient norm: 4.71863645
INFO:root:[   56] Training loss: 0.05214835, Validation loss: 0.05308783, Gradient norm: 3.76636485
INFO:root:[   57] Training loss: 0.11510557, Validation loss: 0.11226447, Gradient norm: 1.59384640
INFO:root:[   58] Training loss: 0.11365256, Validation loss: 0.11172725, Gradient norm: 0.05394572
INFO:root:[   59] Training loss: 0.11363112, Validation loss: 0.11315548, Gradient norm: 0.05232891
INFO:root:[   60] Training loss: 0.11377557, Validation loss: 0.11210128, Gradient norm: 0.05017540
INFO:root:[   61] Training loss: 0.11363602, Validation loss: 0.11384457, Gradient norm: 0.05129020
INFO:root:[   62] Training loss: 0.11366685, Validation loss: 0.11354283, Gradient norm: 0.06127603
INFO:root:[   63] Training loss: 0.11359949, Validation loss: 0.11319781, Gradient norm: 0.05281287
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 1860.145s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02071
INFO:root:EnergyScoreTrain: 0.01522
INFO:root:CoverageTrain: 0.82486
INFO:root:IntervalWidthTrain: 0.07466
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03187
INFO:root:EnergyScoreValidation: 0.02321
INFO:root:CoverageValidation: 0.64916
INFO:root:IntervalWidthValidation: 0.07495
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03213
INFO:root:EnergyScoreTest: 0.0234
INFO:root:CoverageTest: 0.6461
INFO:root:IntervalWidthTest: 0.07493
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06658463, Validation loss: 0.03779583, Gradient norm: 0.92334868
INFO:root:[    2] Training loss: 0.03767561, Validation loss: 0.04018064, Gradient norm: 0.62116463
INFO:root:[    3] Training loss: 0.03305969, Validation loss: 0.02647844, Gradient norm: 0.53088154
INFO:root:[    4] Training loss: 0.02879804, Validation loss: 0.02482748, Gradient norm: 0.46571760
INFO:root:[    5] Training loss: 0.02643880, Validation loss: 0.02878347, Gradient norm: 0.41261206
INFO:root:[    6] Training loss: 0.02511468, Validation loss: 0.02612864, Gradient norm: 0.39170500
INFO:root:[    7] Training loss: 0.02494559, Validation loss: 0.02549348, Gradient norm: 0.42141543
INFO:root:[    8] Training loss: 0.02317963, Validation loss: 0.02270273, Gradient norm: 0.42887811
INFO:root:[    9] Training loss: 0.02177610, Validation loss: 0.02608959, Gradient norm: 0.39506731
INFO:root:[   10] Training loss: 0.02127030, Validation loss: 0.02459808, Gradient norm: 0.36492402
INFO:root:[   11] Training loss: 0.02077129, Validation loss: 0.02431176, Gradient norm: 0.36528280
INFO:root:[   12] Training loss: 0.02041671, Validation loss: 0.02425341, Gradient norm: 0.37705054
INFO:root:[   13] Training loss: 0.01989121, Validation loss: 0.02733374, Gradient norm: 0.35400723
INFO:root:[   14] Training loss: 0.01961334, Validation loss: 0.02576397, Gradient norm: 0.35482942
INFO:root:[   15] Training loss: 0.01975170, Validation loss: 0.02427979, Gradient norm: 0.38448642
INFO:root:[   16] Training loss: 0.01951190, Validation loss: 0.02706242, Gradient norm: 0.39523495
INFO:root:[   17] Training loss: 0.01872379, Validation loss: 0.03004745, Gradient norm: 0.37369349
INFO:root:[   18] Training loss: 0.01787112, Validation loss: 0.02839775, Gradient norm: 0.33214345
INFO:root:[   19] Training loss: 0.01756055, Validation loss: 0.02596533, Gradient norm: 0.32277173
INFO:root:[   20] Training loss: 0.01872838, Validation loss: 0.02685107, Gradient norm: 0.37403905
INFO:root:[   21] Training loss: 0.01694518, Validation loss: 0.02919824, Gradient norm: 0.32253833
INFO:root:[   22] Training loss: 0.01687968, Validation loss: 0.02864359, Gradient norm: 0.30880514
INFO:root:[   23] Training loss: 0.01702354, Validation loss: 0.02645479, Gradient norm: 0.32557332
INFO:root:[   24] Training loss: 0.01739725, Validation loss: 0.03137695, Gradient norm: 0.35764608
INFO:root:[   25] Training loss: 0.01672134, Validation loss: 0.03220288, Gradient norm: 0.32985219
INFO:root:[   26] Training loss: 0.01602310, Validation loss: 0.02804981, Gradient norm: 0.28153087
INFO:root:[   27] Training loss: 0.01571974, Validation loss: 0.02996201, Gradient norm: 0.29422000
INFO:root:[   28] Training loss: 0.01542918, Validation loss: 0.02891593, Gradient norm: 0.28738230
INFO:root:[   29] Training loss: 0.01669161, Validation loss: 0.03199895, Gradient norm: 0.33689634
INFO:root:[   30] Training loss: 0.01544276, Validation loss: 0.02759036, Gradient norm: 0.29641910
INFO:root:[   31] Training loss: 0.01583569, Validation loss: 0.03322607, Gradient norm: 0.31079010
INFO:root:[   32] Training loss: 0.01524311, Validation loss: 0.03025856, Gradient norm: 0.28908332
INFO:root:[   33] Training loss: 0.01481103, Validation loss: 0.02984202, Gradient norm: 0.25893709
INFO:root:[   34] Training loss: 0.01549822, Validation loss: 0.02788108, Gradient norm: 0.30973440
INFO:root:[   35] Training loss: 0.01488001, Validation loss: 0.02956216, Gradient norm: 0.28338126
INFO:root:[   36] Training loss: 0.01602350, Validation loss: 0.02939696, Gradient norm: 0.34221017
INFO:root:[   37] Training loss: 0.01551413, Validation loss: 0.02871378, Gradient norm: 0.29139644
INFO:root:[   38] Training loss: 0.01492497, Validation loss: 0.03570676, Gradient norm: 0.29426966
INFO:root:[   39] Training loss: 0.01539808, Validation loss: 0.03059219, Gradient norm: 0.31236431
INFO:root:[   40] Training loss: 0.01465545, Validation loss: 0.02932947, Gradient norm: 0.29229449
INFO:root:[   41] Training loss: 0.01376128, Validation loss: 0.03008658, Gradient norm: 0.24830704
INFO:root:[   42] Training loss: 0.01487934, Validation loss: 0.02702046, Gradient norm: 0.29874575
INFO:root:[   43] Training loss: 0.01402531, Validation loss: 0.03303077, Gradient norm: 0.26294669
INFO:root:[   44] Training loss: 0.01403733, Validation loss: 0.03155361, Gradient norm: 0.26192304
INFO:root:[   45] Training loss: 0.01524496, Validation loss: 0.02655200, Gradient norm: 0.32051786
INFO:root:[   46] Training loss: 0.01433146, Validation loss: 0.02880155, Gradient norm: 0.27307107
INFO:root:[   47] Training loss: 0.01427551, Validation loss: 0.03187779, Gradient norm: 0.28611760
INFO:root:[   48] Training loss: 0.01400120, Validation loss: 0.02740738, Gradient norm: 0.28659713
INFO:root:[   49] Training loss: 0.01454714, Validation loss: 0.03333703, Gradient norm: 0.30800720
INFO:root:[   50] Training loss: 0.01418083, Validation loss: 0.02982667, Gradient norm: 0.29315124
INFO:root:[   51] Training loss: 0.01349580, Validation loss: 0.02755296, Gradient norm: 0.23306366
INFO:root:[   52] Training loss: 0.01364604, Validation loss: 0.02943929, Gradient norm: 0.27504008
INFO:root:[   53] Training loss: 0.01405188, Validation loss: 0.02950895, Gradient norm: 0.28580848
INFO:root:[   54] Training loss: 0.01415528, Validation loss: 0.02866225, Gradient norm: 0.29933903
INFO:root:[   55] Training loss: 0.01363374, Validation loss: 0.02666871, Gradient norm: 0.26940160
INFO:root:[   56] Training loss: 0.01361032, Validation loss: 0.02601335, Gradient norm: 0.26451612
INFO:root:[   57] Training loss: 0.01346725, Validation loss: 0.02795642, Gradient norm: 0.26432594
INFO:root:[   58] Training loss: 0.01343764, Validation loss: 0.03080699, Gradient norm: 0.27132316
INFO:root:[   59] Training loss: 0.01444288, Validation loss: 0.03227987, Gradient norm: 0.30740618
INFO:root:[   60] Training loss: 0.01323202, Validation loss: 0.03039579, Gradient norm: 0.22609552
INFO:root:[   61] Training loss: 0.01376365, Validation loss: 0.03047588, Gradient norm: 0.28812167
INFO:root:[   62] Training loss: 0.01339837, Validation loss: 0.03277050, Gradient norm: 0.25219398
INFO:root:[   63] Training loss: 0.01396772, Validation loss: 0.02817244, Gradient norm: 0.23686438
INFO:root:[   64] Training loss: 0.01315233, Validation loss: 0.03043373, Gradient norm: 0.26298304
INFO:root:[   65] Training loss: 0.01379889, Validation loss: 0.02969505, Gradient norm: 0.30715040
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1909.508s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02452
INFO:root:EnergyScoreTrain: 0.01793
INFO:root:CoverageTrain: 0.81361
INFO:root:IntervalWidthTrain: 0.08254
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03125
INFO:root:EnergyScoreValidation: 0.0226
INFO:root:CoverageValidation: 0.69374
INFO:root:IntervalWidthValidation: 0.08447
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03165
INFO:root:EnergyScoreTest: 0.02289
INFO:root:CoverageTest: 0.68745
INFO:root:IntervalWidthTest: 0.08452
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 234881024
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05437205, Validation loss: 0.02959687, Gradient norm: 0.60349121
INFO:root:[    2] Training loss: 0.03718819, Validation loss: 0.03700183, Gradient norm: 0.53350309
INFO:root:[    3] Training loss: 0.03065576, Validation loss: 0.02867865, Gradient norm: 0.45086511
INFO:root:[    4] Training loss: 0.02747448, Validation loss: 0.03011421, Gradient norm: 0.41123423
INFO:root:[    5] Training loss: 0.02481113, Validation loss: 0.02982718, Gradient norm: 0.28405259
INFO:root:[    6] Training loss: 0.02580183, Validation loss: 0.03315699, Gradient norm: 0.40509848
INFO:root:[    7] Training loss: 0.02459438, Validation loss: 0.02971704, Gradient norm: 0.39052372
INFO:root:[    8] Training loss: 0.02489817, Validation loss: 0.02785966, Gradient norm: 0.41908212
INFO:root:[    9] Training loss: 0.02293421, Validation loss: 0.02822719, Gradient norm: 0.35130340
INFO:root:[   10] Training loss: 0.02220803, Validation loss: 0.02696159, Gradient norm: 0.35479554
INFO:root:[   11] Training loss: 0.02123415, Validation loss: 0.02614129, Gradient norm: 0.32336214
INFO:root:[   12] Training loss: 0.02167821, Validation loss: 0.02870034, Gradient norm: 0.34759347
INFO:root:[   13] Training loss: 0.02187578, Validation loss: 0.02489976, Gradient norm: 0.36483030
INFO:root:[   14] Training loss: 0.01940259, Validation loss: 0.02207044, Gradient norm: 0.24532839
INFO:root:[   15] Training loss: 0.01993772, Validation loss: 0.02390133, Gradient norm: 0.26775823
INFO:root:[   16] Training loss: 0.02059681, Validation loss: 0.02639455, Gradient norm: 0.32291472
INFO:root:[   17] Training loss: 0.02014894, Validation loss: 0.02579785, Gradient norm: 0.32341163
INFO:root:[   18] Training loss: 0.01895594, Validation loss: 0.02373868, Gradient norm: 0.27143196
INFO:root:[   19] Training loss: 0.01979149, Validation loss: 0.02464172, Gradient norm: 0.32201494
INFO:root:[   20] Training loss: 0.01904451, Validation loss: 0.03303796, Gradient norm: 0.31449210
INFO:root:[   21] Training loss: 0.01912662, Validation loss: 0.02650803, Gradient norm: 0.27395487
INFO:root:[   22] Training loss: 0.01807973, Validation loss: 0.02515546, Gradient norm: 0.27912769
INFO:root:[   23] Training loss: 0.01742534, Validation loss: 0.02767170, Gradient norm: 0.23422642
INFO:root:[   24] Training loss: 0.01827742, Validation loss: 0.02876324, Gradient norm: 0.30935666
INFO:root:[   25] Training loss: 0.01870607, Validation loss: 0.02490686, Gradient norm: 0.31903364
INFO:root:[   26] Training loss: 0.01746864, Validation loss: 0.02684162, Gradient norm: 0.26039775
INFO:root:[   27] Training loss: 0.01690193, Validation loss: 0.02637474, Gradient norm: 0.24179814
INFO:root:[   28] Training loss: 0.01756444, Validation loss: 0.02607496, Gradient norm: 0.26256708
INFO:root:[   29] Training loss: 0.01717567, Validation loss: 0.02654184, Gradient norm: 0.27535534
INFO:root:[   30] Training loss: 0.01677570, Validation loss: 0.03048407, Gradient norm: 0.25228020
INFO:root:[   31] Training loss: 0.01769147, Validation loss: 0.02544245, Gradient norm: 0.30808976
INFO:root:[   32] Training loss: 0.01628529, Validation loss: 0.02945052, Gradient norm: 0.23110269
INFO:root:[   33] Training loss: 0.01654032, Validation loss: 0.02573614, Gradient norm: 0.25765138
INFO:root:[   34] Training loss: 0.01706976, Validation loss: 0.02770128, Gradient norm: 0.27955069
INFO:root:[   35] Training loss: 0.01637330, Validation loss: 0.02501427, Gradient norm: 0.25203510
INFO:root:[   36] Training loss: 0.01652927, Validation loss: 0.02954837, Gradient norm: 0.25128785
INFO:root:[   37] Training loss: 0.01666285, Validation loss: 0.02568842, Gradient norm: 0.28048517
INFO:root:[   38] Training loss: 0.01669860, Validation loss: 0.02368981, Gradient norm: 0.28364378
INFO:root:[   39] Training loss: 0.01576214, Validation loss: 0.02610299, Gradient norm: 0.23958677
INFO:root:[   40] Training loss: 0.01648306, Validation loss: 0.02591034, Gradient norm: 0.27110892
INFO:root:[   41] Training loss: 0.01611311, Validation loss: 0.02491711, Gradient norm: 0.27158796
INFO:root:[   42] Training loss: 0.01576835, Validation loss: 0.03101541, Gradient norm: 0.23933010
INFO:root:[   43] Training loss: 0.01605773, Validation loss: 0.02828241, Gradient norm: 0.26836928
INFO:root:[   44] Training loss: 0.01571688, Validation loss: 0.02770057, Gradient norm: 0.25004734
INFO:root:[   45] Training loss: 0.01627796, Validation loss: 0.02608793, Gradient norm: 0.27891764
INFO:root:[   46] Training loss: 0.01559030, Validation loss: 0.02729872, Gradient norm: 0.26533208
INFO:root:[   47] Training loss: 0.01551559, Validation loss: 0.02501387, Gradient norm: 0.23934591
INFO:root:[   48] Training loss: 0.01546331, Validation loss: 0.02979357, Gradient norm: 0.23771510
INFO:root:[   49] Training loss: 0.01576564, Validation loss: 0.03267970, Gradient norm: 0.25833056
INFO:root:[   50] Training loss: 0.01557563, Validation loss: 0.03042068, Gradient norm: 0.23851351
INFO:root:[   51] Training loss: 0.01587055, Validation loss: 0.02981198, Gradient norm: 0.25201276
