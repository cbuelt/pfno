INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.01, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 25165824
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04600807, Validation loss: 1.71818325, Gradient norm: 0.79160881
INFO:root:[    2] Training loss: 0.02826042, Validation loss: 1.40329141, Gradient norm: 0.96604708
INFO:root:[    3] Training loss: 0.02491909, Validation loss: 1.37639044, Gradient norm: 0.81232028
INFO:root:[    4] Training loss: 0.02353424, Validation loss: 1.29983874, Gradient norm: 0.80373904
INFO:root:[    5] Training loss: 0.02312172, Validation loss: 1.28288504, Gradient norm: 0.87541416
INFO:root:[    6] Training loss: 0.02245374, Validation loss: 1.20890298, Gradient norm: 0.83226110
INFO:root:[    7] Training loss: 0.02141518, Validation loss: 1.18900800, Gradient norm: 0.74624154
INFO:root:[    8] Training loss: 0.02108130, Validation loss: 1.16346670, Gradient norm: 0.76524770
INFO:root:[    9] Training loss: 0.02048920, Validation loss: 1.12627027, Gradient norm: 0.74470247
INFO:root:[   10] Training loss: 0.01983788, Validation loss: 1.12267769, Gradient norm: 0.71998740
INFO:root:[   11] Training loss: 0.01977346, Validation loss: 1.17330104, Gradient norm: 0.75936701
INFO:root:[   12] Training loss: 0.01922209, Validation loss: 1.06427728, Gradient norm: 0.75119775
INFO:root:[   13] Training loss: 0.01917922, Validation loss: 1.06194989, Gradient norm: 0.76748966
INFO:root:[   14] Training loss: 0.01841888, Validation loss: 1.01147821, Gradient norm: 0.64098670
INFO:root:[   15] Training loss: 0.01848347, Validation loss: 1.11466223, Gradient norm: 0.67515558
INFO:root:[   16] Training loss: 0.01793929, Validation loss: 1.00188049, Gradient norm: 0.62338415
INFO:root:[   17] Training loss: 0.01776506, Validation loss: 1.05860515, Gradient norm: 0.69831031
INFO:root:[   18] Training loss: 0.01769770, Validation loss: 1.09827528, Gradient norm: 0.69780235
INFO:root:[   19] Training loss: 0.01695426, Validation loss: 1.13571615, Gradient norm: 0.54925194
INFO:root:[   20] Training loss: 0.01722659, Validation loss: 0.95523505, Gradient norm: 0.65673355
INFO:root:[   21] Training loss: 0.01681035, Validation loss: 0.97867467, Gradient norm: 0.57536337
INFO:root:[   22] Training loss: 0.01679653, Validation loss: 0.97439830, Gradient norm: 0.61579041
INFO:root:[   23] Training loss: 0.01635976, Validation loss: 0.92566319, Gradient norm: 0.57153977
INFO:root:[   24] Training loss: 0.01645850, Validation loss: 0.90803017, Gradient norm: 0.63746638
INFO:root:[   25] Training loss: 0.01605401, Validation loss: 0.90555006, Gradient norm: 0.56348014
INFO:root:[   26] Training loss: 0.01599464, Validation loss: 0.88864299, Gradient norm: 0.59409638
INFO:root:[   27] Training loss: 0.01583297, Validation loss: 0.86981727, Gradient norm: 0.63913169
INFO:root:[   28] Training loss: 0.01530304, Validation loss: 0.95419482, Gradient norm: 0.53236654
INFO:root:[   29] Training loss: 0.01565854, Validation loss: 0.85674686, Gradient norm: 0.66930122
INFO:root:[   30] Training loss: 0.01539158, Validation loss: 0.85475432, Gradient norm: 0.59123140
INFO:root:[   31] Training loss: 0.01513665, Validation loss: 0.92199233, Gradient norm: 0.57518771
INFO:root:[   32] Training loss: 0.01495749, Validation loss: 0.84788014, Gradient norm: 0.61419397
INFO:root:[   33] Training loss: 0.01456854, Validation loss: 0.82220575, Gradient norm: 0.50843348
INFO:root:[   34] Training loss: 0.01455301, Validation loss: 0.83427229, Gradient norm: 0.58534345
INFO:root:[   35] Training loss: 0.01434286, Validation loss: 0.85896558, Gradient norm: 0.57169744
INFO:root:[   36] Training loss: 0.01420074, Validation loss: 0.78573061, Gradient norm: 0.53319975
INFO:root:[   37] Training loss: 0.01405759, Validation loss: 0.81467441, Gradient norm: 0.58460553
INFO:root:[   38] Training loss: 0.01387034, Validation loss: 0.81377896, Gradient norm: 0.53624274
INFO:root:[   39] Training loss: 0.01393285, Validation loss: 0.77376534, Gradient norm: 0.52878766
INFO:root:[   40] Training loss: 0.01387055, Validation loss: 0.88840299, Gradient norm: 0.54052878
INFO:root:[   41] Training loss: 0.01379348, Validation loss: 0.76079068, Gradient norm: 0.55130248
INFO:root:[   42] Training loss: 0.01329623, Validation loss: 0.75598269, Gradient norm: 0.43462443
INFO:root:[   43] Training loss: 0.01319161, Validation loss: 0.73819170, Gradient norm: 0.51482095
INFO:root:[   44] Training loss: 0.01336338, Validation loss: 0.77031328, Gradient norm: 0.52385577
INFO:root:[   45] Training loss: 0.01272879, Validation loss: 0.85383065, Gradient norm: 0.45354145
INFO:root:[   46] Training loss: 0.01324682, Validation loss: 0.72574319, Gradient norm: 0.57047397
INFO:root:[   47] Training loss: 0.01294903, Validation loss: 0.75337633, Gradient norm: 0.48290618
INFO:root:[   48] Training loss: 0.01257069, Validation loss: 0.68145176, Gradient norm: 0.43184842
INFO:root:[   49] Training loss: 0.01255627, Validation loss: 0.85970100, Gradient norm: 0.44772861
INFO:root:[   50] Training loss: 0.01300496, Validation loss: 0.78557161, Gradient norm: 0.57418667
INFO:root:[   51] Training loss: 0.01244620, Validation loss: 0.74812124, Gradient norm: 0.49221197
INFO:root:[   52] Training loss: 0.01221266, Validation loss: 0.71345484, Gradient norm: 0.44764989
INFO:root:[   53] Training loss: 0.01246401, Validation loss: 0.72979742, Gradient norm: 0.52553605
INFO:root:[   54] Training loss: 0.01207894, Validation loss: 0.68238248, Gradient norm: 0.43546316
INFO:root:[   55] Training loss: 0.01240682, Validation loss: 0.77726210, Gradient norm: 0.51782917
INFO:root:[   56] Training loss: 0.01195959, Validation loss: 0.66033404, Gradient norm: 0.43818810
INFO:root:[   57] Training loss: 0.01172824, Validation loss: 0.69750081, Gradient norm: 0.38353533
INFO:root:[   58] Training loss: 0.01201942, Validation loss: 0.69173620, Gradient norm: 0.46585932
INFO:root:[   59] Training loss: 0.01204892, Validation loss: 0.69614609, Gradient norm: 0.47683730
INFO:root:[   60] Training loss: 0.01153110, Validation loss: 0.67315650, Gradient norm: 0.41068834
INFO:root:[   61] Training loss: 0.01178181, Validation loss: 0.64670048, Gradient norm: 0.47102318
INFO:root:[   62] Training loss: 0.01150240, Validation loss: 0.64556774, Gradient norm: 0.40293182
INFO:root:[   63] Training loss: 0.01142065, Validation loss: 0.63654055, Gradient norm: 0.43205594
INFO:root:[   64] Training loss: 0.01161602, Validation loss: 0.69257493, Gradient norm: 0.47883794
INFO:root:[   65] Training loss: 0.01122748, Validation loss: 0.68153084, Gradient norm: 0.41152221
INFO:root:[   66] Training loss: 0.01172021, Validation loss: 0.63494644, Gradient norm: 0.49219521
INFO:root:[   67] Training loss: 0.01130362, Validation loss: 0.64268057, Gradient norm: 0.45730930
INFO:root:[   68] Training loss: 0.01145248, Validation loss: 0.64467991, Gradient norm: 0.49253714
INFO:root:[   69] Training loss: 0.01116632, Validation loss: 0.67286469, Gradient norm: 0.40539027
INFO:root:[   70] Training loss: 0.01114984, Validation loss: 0.71491064, Gradient norm: 0.46051778
INFO:root:[   71] Training loss: 0.01097865, Validation loss: 0.63470989, Gradient norm: 0.42183475
INFO:root:[   72] Training loss: 0.01089011, Validation loss: 0.68768181, Gradient norm: 0.39451108
INFO:root:[   73] Training loss: 0.01112378, Validation loss: 0.62935255, Gradient norm: 0.47482268
INFO:root:[   74] Training loss: 0.01078126, Validation loss: 0.70576556, Gradient norm: 0.40112321
INFO:root:[   75] Training loss: 0.01065727, Validation loss: 0.59320635, Gradient norm: 0.40831878
INFO:root:[   76] Training loss: 0.01084795, Validation loss: 0.61837461, Gradient norm: 0.42900845
INFO:root:[   77] Training loss: 0.01114719, Validation loss: 0.61940038, Gradient norm: 0.51919994
INFO:root:[   78] Training loss: 0.01088589, Validation loss: 0.66221182, Gradient norm: 0.46963399
INFO:root:[   79] Training loss: 0.01071928, Validation loss: 0.67906781, Gradient norm: 0.42302827
INFO:root:[   80] Training loss: 0.01084576, Validation loss: 0.63125233, Gradient norm: 0.46716771
INFO:root:[   81] Training loss: 0.01046540, Validation loss: 0.69619108, Gradient norm: 0.41795500
INFO:root:[   82] Training loss: 0.01053133, Validation loss: 0.62419020, Gradient norm: 0.39766260
INFO:root:[   83] Training loss: 0.01044718, Validation loss: 0.59328887, Gradient norm: 0.41422638
INFO:root:[   84] Training loss: 0.01053197, Validation loss: 0.57378922, Gradient norm: 0.43662881
INFO:root:[   85] Training loss: 0.01063280, Validation loss: 0.62874021, Gradient norm: 0.43393404
INFO:root:[   86] Training loss: 0.01046398, Validation loss: 0.60161408, Gradient norm: 0.42536030
INFO:root:[   87] Training loss: 0.01041473, Validation loss: 0.56203870, Gradient norm: 0.43139854
INFO:root:[   88] Training loss: 0.01013461, Validation loss: 0.57945233, Gradient norm: 0.36846042
INFO:root:[   89] Training loss: 0.01019936, Validation loss: 0.61124990, Gradient norm: 0.35723108
INFO:root:[   90] Training loss: 0.01059053, Validation loss: 0.57162216, Gradient norm: 0.50155032
INFO:root:[   91] Training loss: 0.01015871, Validation loss: 0.57142595, Gradient norm: 0.41123735
INFO:root:[   92] Training loss: 0.01008262, Validation loss: 0.57991122, Gradient norm: 0.38990351
INFO:root:[   93] Training loss: 0.01003800, Validation loss: 0.56009117, Gradient norm: 0.40456609
INFO:root:[   94] Training loss: 0.01012023, Validation loss: 0.59475343, Gradient norm: 0.43127744
INFO:root:[   95] Training loss: 0.01005785, Validation loss: 0.60615692, Gradient norm: 0.39732679
INFO:root:[   96] Training loss: 0.01001272, Validation loss: 0.56245659, Gradient norm: 0.40232674
INFO:root:[   97] Training loss: 0.01011366, Validation loss: 0.55504963, Gradient norm: 0.41980711
INFO:root:[   98] Training loss: 0.01000174, Validation loss: 0.59356384, Gradient norm: 0.41625584
INFO:root:[   99] Training loss: 0.00970841, Validation loss: 0.57211542, Gradient norm: 0.34861314
INFO:root:[  100] Training loss: 0.00980776, Validation loss: 0.58432506, Gradient norm: 0.37480319
INFO:root:[  101] Training loss: 0.01009653, Validation loss: 0.55382556, Gradient norm: 0.45031274
INFO:root:[  102] Training loss: 0.01003679, Validation loss: 0.59228654, Gradient norm: 0.43231773
INFO:root:[  103] Training loss: 0.00966997, Validation loss: 0.56389743, Gradient norm: 0.37745860
INFO:root:[  104] Training loss: 0.00991639, Validation loss: 0.56646580, Gradient norm: 0.43373073
INFO:root:[  105] Training loss: 0.00987810, Validation loss: 0.56230941, Gradient norm: 0.41809906
INFO:root:[  106] Training loss: 0.00974732, Validation loss: 0.56205736, Gradient norm: 0.39268284
INFO:root:[  107] Training loss: 0.00966669, Validation loss: 0.55118526, Gradient norm: 0.39792860
INFO:root:[  108] Training loss: 0.00986137, Validation loss: 0.54725411, Gradient norm: 0.39185034
INFO:root:[  109] Training loss: 0.00966024, Validation loss: 0.57210796, Gradient norm: 0.39218016
INFO:root:[  110] Training loss: 0.00963065, Validation loss: 0.60259723, Gradient norm: 0.37594886
INFO:root:[  111] Training loss: 0.00950226, Validation loss: 0.57193224, Gradient norm: 0.35266945
INFO:root:[  112] Training loss: 0.00953637, Validation loss: 0.54455575, Gradient norm: 0.35929605
INFO:root:[  113] Training loss: 0.00972487, Validation loss: 0.54952805, Gradient norm: 0.43855173
INFO:root:[  114] Training loss: 0.00985258, Validation loss: 0.54677384, Gradient norm: 0.43569386
INFO:root:[  115] Training loss: 0.00955023, Validation loss: 0.57000818, Gradient norm: 0.41219111
INFO:root:[  116] Training loss: 0.00976226, Validation loss: 0.56614044, Gradient norm: 0.44033666
INFO:root:[  117] Training loss: 0.00962119, Validation loss: 0.60828789, Gradient norm: 0.40954396
INFO:root:[  118] Training loss: 0.00949566, Validation loss: 0.57291985, Gradient norm: 0.40701373
INFO:root:[  119] Training loss: 0.00955093, Validation loss: 0.56734591, Gradient norm: 0.40736560
INFO:root:[  120] Training loss: 0.00950432, Validation loss: 0.63190399, Gradient norm: 0.44486607
INFO:root:[  121] Training loss: 0.00953535, Validation loss: 0.53712720, Gradient norm: 0.39413054
INFO:root:[  122] Training loss: 0.00927845, Validation loss: 0.53324607, Gradient norm: 0.35873773
INFO:root:[  123] Training loss: 0.00939869, Validation loss: 0.52879211, Gradient norm: 0.38960139
INFO:root:[  124] Training loss: 0.00929636, Validation loss: 0.51427842, Gradient norm: 0.34938718
INFO:root:[  125] Training loss: 0.00932981, Validation loss: 0.54553208, Gradient norm: 0.40494375
INFO:root:[  126] Training loss: 0.00922482, Validation loss: 0.54782064, Gradient norm: 0.36095486
INFO:root:[  127] Training loss: 0.00944130, Validation loss: 0.50720625, Gradient norm: 0.44617109
INFO:root:[  128] Training loss: 0.00942732, Validation loss: 0.53238615, Gradient norm: 0.39535105
INFO:root:[  129] Training loss: 0.00923303, Validation loss: 0.55810908, Gradient norm: 0.36316103
INFO:root:[  130] Training loss: 0.00913474, Validation loss: 0.52002984, Gradient norm: 0.34846185
INFO:root:[  131] Training loss: 0.00922892, Validation loss: 0.61524880, Gradient norm: 0.42867381
INFO:root:[  132] Training loss: 0.00949011, Validation loss: 0.52581572, Gradient norm: 0.42808662
INFO:root:[  133] Training loss: 0.00883458, Validation loss: 0.54637086, Gradient norm: 0.30365258
INFO:root:[  134] Training loss: 0.00917068, Validation loss: 0.53202727, Gradient norm: 0.40245180
INFO:root:[  135] Training loss: 0.00921180, Validation loss: 0.53256719, Gradient norm: 0.36610925
INFO:root:[  136] Training loss: 0.00920421, Validation loss: 0.56115876, Gradient norm: 0.38822393
INFO:root:EP 136: Early stopping
INFO:root:Training the model took 3544.493s.
INFO:root:Emptying the cuda cache took 0.011s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08354
INFO:root:EnergyScoretrain: 0.06158
INFO:root:Coveragetrain: 6.88016
INFO:root:IntervalWidthtrain: 205.6302
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02358
INFO:root:EnergyScorevalidation: 0.01734
INFO:root:Coveragevalidation: 1.83342
INFO:root:IntervalWidthvalidation: 54.27354
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04866
INFO:root:EnergyScoretest: 0.04077
INFO:root:Coveragetest: 0.5147
INFO:root:IntervalWidthtest: 53.22786
INFO:root:###2 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.05, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 188743680
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05254292, Validation loss: 1.96677396, Gradient norm: 0.64639720
INFO:root:[    2] Training loss: 0.03036039, Validation loss: 1.55226891, Gradient norm: 0.57732477
INFO:root:[    3] Training loss: 0.02660548, Validation loss: 1.52204239, Gradient norm: 0.52926541
INFO:root:[    4] Training loss: 0.02496284, Validation loss: 1.39178289, Gradient norm: 0.48010597
INFO:root:[    5] Training loss: 0.02455589, Validation loss: 1.35263043, Gradient norm: 0.61078889
INFO:root:[    6] Training loss: 0.02314353, Validation loss: 1.27594926, Gradient norm: 0.53854595
INFO:root:[    7] Training loss: 0.02232801, Validation loss: 1.26292708, Gradient norm: 0.44174551
INFO:root:[    8] Training loss: 0.02197598, Validation loss: 1.25675030, Gradient norm: 0.51621935
INFO:root:[    9] Training loss: 0.02129104, Validation loss: 1.27531940, Gradient norm: 0.49917186
INFO:root:[   10] Training loss: 0.02142144, Validation loss: 1.10775790, Gradient norm: 0.57106297
INFO:root:[   11] Training loss: 0.02026466, Validation loss: 1.14309748, Gradient norm: 0.46925152
INFO:root:[   12] Training loss: 0.02002923, Validation loss: 1.16616253, Gradient norm: 0.51395115
INFO:root:[   13] Training loss: 0.01953113, Validation loss: 1.09712630, Gradient norm: 0.47983592
INFO:root:[   14] Training loss: 0.01907422, Validation loss: 1.06855249, Gradient norm: 0.50701667
INFO:root:[   15] Training loss: 0.01874573, Validation loss: 1.11325076, Gradient norm: 0.49463209
INFO:root:[   16] Training loss: 0.01874218, Validation loss: 1.01370032, Gradient norm: 0.48174435
INFO:root:[   17] Training loss: 0.01832306, Validation loss: 1.15155664, Gradient norm: 0.51037061
INFO:root:[   18] Training loss: 0.01790662, Validation loss: 0.95366549, Gradient norm: 0.52607381
INFO:root:[   19] Training loss: 0.01739973, Validation loss: 1.00153813, Gradient norm: 0.48570692
INFO:root:[   20] Training loss: 0.01702993, Validation loss: 0.93533012, Gradient norm: 0.42753377
INFO:root:[   21] Training loss: 0.01703973, Validation loss: 0.91103135, Gradient norm: 0.49778161
INFO:root:[   22] Training loss: 0.01647200, Validation loss: 0.90477117, Gradient norm: 0.46285225
INFO:root:[   23] Training loss: 0.01626774, Validation loss: 0.89489965, Gradient norm: 0.44862338
INFO:root:[   24] Training loss: 0.01607752, Validation loss: 1.03363621, Gradient norm: 0.43139752
INFO:root:[   25] Training loss: 0.01599941, Validation loss: 0.89928944, Gradient norm: 0.49455341
INFO:root:[   26] Training loss: 0.01560223, Validation loss: 0.86853319, Gradient norm: 0.47404330
INFO:root:[   27] Training loss: 0.01520594, Validation loss: 0.85810149, Gradient norm: 0.43080855
INFO:root:[   28] Training loss: 0.01522027, Validation loss: 0.83018370, Gradient norm: 0.48876621
INFO:root:[   29] Training loss: 0.01472568, Validation loss: 0.87045794, Gradient norm: 0.45479140
INFO:root:[   30] Training loss: 0.01482700, Validation loss: 0.82616434, Gradient norm: 0.48795984
INFO:root:[   31] Training loss: 0.01426800, Validation loss: 0.81891395, Gradient norm: 0.43553721
INFO:root:[   32] Training loss: 0.01417219, Validation loss: 0.80393479, Gradient norm: 0.43318729
INFO:root:[   33] Training loss: 0.01452244, Validation loss: 0.92358660, Gradient norm: 0.52552522
INFO:root:[   34] Training loss: 0.01428114, Validation loss: 0.77146889, Gradient norm: 0.48480027
INFO:root:[   35] Training loss: 0.01361739, Validation loss: 0.86690471, Gradient norm: 0.40019870
INFO:root:[   36] Training loss: 0.01384847, Validation loss: 0.86681527, Gradient norm: 0.44810612
INFO:root:[   37] Training loss: 0.01351393, Validation loss: 0.75234248, Gradient norm: 0.44243512
INFO:root:[   38] Training loss: 0.01344288, Validation loss: 0.77416749, Gradient norm: 0.44541294
INFO:root:[   39] Training loss: 0.01367953, Validation loss: 0.73979536, Gradient norm: 0.47882789
INFO:root:[   40] Training loss: 0.01315956, Validation loss: 0.80001905, Gradient norm: 0.42642103
INFO:root:[   41] Training loss: 0.01347537, Validation loss: 0.91963248, Gradient norm: 0.50068209
INFO:root:[   42] Training loss: 0.01313737, Validation loss: 0.75572696, Gradient norm: 0.44739918
INFO:root:[   43] Training loss: 0.01316743, Validation loss: 0.75859489, Gradient norm: 0.45308904
INFO:root:[   44] Training loss: 0.01270818, Validation loss: 0.78310625, Gradient norm: 0.36075910
INFO:root:[   45] Training loss: 0.01290727, Validation loss: 0.70847154, Gradient norm: 0.41943729
INFO:root:[   46] Training loss: 0.01300733, Validation loss: 0.72317609, Gradient norm: 0.46894007
INFO:root:[   47] Training loss: 0.01255292, Validation loss: 0.68919385, Gradient norm: 0.41449917
INFO:root:[   48] Training loss: 0.01247231, Validation loss: 0.75971607, Gradient norm: 0.39272102
INFO:root:[   49] Training loss: 0.01277118, Validation loss: 0.74194442, Gradient norm: 0.48486665
INFO:root:[   50] Training loss: 0.01235954, Validation loss: 0.73574233, Gradient norm: 0.40603084
INFO:root:[   51] Training loss: 0.01228388, Validation loss: 0.72833988, Gradient norm: 0.40570461
INFO:root:[   52] Training loss: 0.01215427, Validation loss: 0.71653560, Gradient norm: 0.44779523
INFO:root:[   53] Training loss: 0.01210488, Validation loss: 0.66727913, Gradient norm: 0.44034716
INFO:root:[   54] Training loss: 0.01211045, Validation loss: 0.67397506, Gradient norm: 0.41876981
INFO:root:[   55] Training loss: 0.01215837, Validation loss: 0.70166909, Gradient norm: 0.45267044
INFO:root:[   56] Training loss: 0.01215973, Validation loss: 0.76237034, Gradient norm: 0.45258104
INFO:root:[   57] Training loss: 0.01177214, Validation loss: 0.68471272, Gradient norm: 0.40190215
INFO:root:[   58] Training loss: 0.01162649, Validation loss: 0.70702364, Gradient norm: 0.42719308
INFO:root:[   59] Training loss: 0.01161591, Validation loss: 0.70623468, Gradient norm: 0.39744536
INFO:root:[   60] Training loss: 0.01167245, Validation loss: 0.68049727, Gradient norm: 0.45043114
INFO:root:[   61] Training loss: 0.01157714, Validation loss: 0.69972674, Gradient norm: 0.40287973
INFO:root:[   62] Training loss: 0.01166843, Validation loss: 0.69080668, Gradient norm: 0.41968881
INFO:root:[   63] Training loss: 0.01180520, Validation loss: 0.66419191, Gradient norm: 0.49050330
INFO:root:[   64] Training loss: 0.01166657, Validation loss: 0.71170349, Gradient norm: 0.44179706
INFO:root:[   65] Training loss: 0.01130028, Validation loss: 0.62009519, Gradient norm: 0.40531353
INFO:root:[   66] Training loss: 0.01127482, Validation loss: 0.64017157, Gradient norm: 0.42473121
INFO:root:[   67] Training loss: 0.01102161, Validation loss: 0.62707728, Gradient norm: 0.35893839
INFO:root:[   68] Training loss: 0.01133424, Validation loss: 0.65289361, Gradient norm: 0.42817062
INFO:root:[   69] Training loss: 0.01135014, Validation loss: 0.66058886, Gradient norm: 0.43006465
INFO:root:[   70] Training loss: 0.01116079, Validation loss: 0.63638874, Gradient norm: 0.41650359
INFO:root:[   71] Training loss: 0.01121798, Validation loss: 0.62453226, Gradient norm: 0.41323322
INFO:root:[   72] Training loss: 0.01069456, Validation loss: 0.65001411, Gradient norm: 0.35121491
INFO:root:[   73] Training loss: 0.01090822, Validation loss: 0.61624798, Gradient norm: 0.40244587
INFO:root:[   74] Training loss: 0.01083570, Validation loss: 0.63510066, Gradient norm: 0.39767855
INFO:root:[   75] Training loss: 0.01095512, Validation loss: 0.60306486, Gradient norm: 0.42231467
INFO:root:[   76] Training loss: 0.01084635, Validation loss: 0.61686506, Gradient norm: 0.42347152
INFO:root:[   77] Training loss: 0.01092266, Validation loss: 0.67677076, Gradient norm: 0.41421318
INFO:root:[   78] Training loss: 0.01102796, Validation loss: 0.58922514, Gradient norm: 0.46507675
INFO:root:[   79] Training loss: 0.01067176, Validation loss: 0.60633511, Gradient norm: 0.38043331
INFO:root:[   80] Training loss: 0.01099339, Validation loss: 0.60655953, Gradient norm: 0.46860971
INFO:root:[   81] Training loss: 0.01072382, Validation loss: 0.63791284, Gradient norm: 0.39400917
INFO:root:[   82] Training loss: 0.01061200, Validation loss: 0.70189833, Gradient norm: 0.41801065
INFO:root:[   83] Training loss: 0.01057162, Validation loss: 0.62075062, Gradient norm: 0.41846269
INFO:root:[   84] Training loss: 0.01066389, Validation loss: 0.74920275, Gradient norm: 0.41374642
INFO:root:[   85] Training loss: 0.01042458, Validation loss: 0.63904776, Gradient norm: 0.38514676
INFO:root:[   86] Training loss: 0.01053556, Validation loss: 0.58366120, Gradient norm: 0.42284119
INFO:root:[   87] Training loss: 0.01037427, Validation loss: 0.60148058, Gradient norm: 0.38784035
INFO:root:[   88] Training loss: 0.01034513, Validation loss: 0.58161350, Gradient norm: 0.37048412
INFO:root:[   89] Training loss: 0.01035935, Validation loss: 0.64708495, Gradient norm: 0.38458702
INFO:root:[   90] Training loss: 0.01063061, Validation loss: 0.61083545, Gradient norm: 0.43400442
INFO:root:[   91] Training loss: 0.01021435, Validation loss: 0.56767374, Gradient norm: 0.36247548
INFO:root:[   92] Training loss: 0.01024307, Validation loss: 0.59364701, Gradient norm: 0.36304765
INFO:root:[   93] Training loss: 0.01039566, Validation loss: 0.63791656, Gradient norm: 0.41413857
INFO:root:[   94] Training loss: 0.01060646, Validation loss: 0.59207327, Gradient norm: 0.46255727
INFO:root:[   95] Training loss: 0.01019008, Validation loss: 0.59269948, Gradient norm: 0.37613969
INFO:root:[   96] Training loss: 0.01038471, Validation loss: 0.60546684, Gradient norm: 0.43291491
INFO:root:[   97] Training loss: 0.01000563, Validation loss: 0.60252379, Gradient norm: 0.36944048
INFO:root:[   98] Training loss: 0.01001396, Validation loss: 0.61624778, Gradient norm: 0.35814398
INFO:root:[   99] Training loss: 0.01039566, Validation loss: 0.57103478, Gradient norm: 0.46278931
INFO:root:[  100] Training loss: 0.01015955, Validation loss: 0.56796067, Gradient norm: 0.37137790
INFO:root:EP 100: Early stopping
INFO:root:Training the model took 2583.997s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.09192
INFO:root:EnergyScoretrain: 0.06816
INFO:root:Coveragetrain: 6.92387
INFO:root:IntervalWidthtrain: 231.51689
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02523
INFO:root:EnergyScorevalidation: 0.01852
INFO:root:Coveragevalidation: 1.84091
INFO:root:IntervalWidthvalidation: 59.73273
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.0414
INFO:root:EnergyScoretest: 0.03225
INFO:root:Coveragetest: 0.73224
INFO:root:IntervalWidthtest: 62.30045
INFO:root:###3 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.1, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05589361, Validation loss: 2.05246629, Gradient norm: 0.50196583
INFO:root:[    2] Training loss: 0.03116454, Validation loss: 1.63295256, Gradient norm: 0.48699196
INFO:root:[    3] Training loss: 0.02699965, Validation loss: 1.40247366, Gradient norm: 0.50172678
INFO:root:[    4] Training loss: 0.02483489, Validation loss: 1.35085540, Gradient norm: 0.47836101
INFO:root:[    5] Training loss: 0.02356258, Validation loss: 1.27600919, Gradient norm: 0.47624546
INFO:root:[    6] Training loss: 0.02232814, Validation loss: 1.23624652, Gradient norm: 0.41493336
INFO:root:[    7] Training loss: 0.02182698, Validation loss: 1.39263641, Gradient norm: 0.46557822
INFO:root:[    8] Training loss: 0.02139797, Validation loss: 1.31174118, Gradient norm: 0.52597999
INFO:root:[    9] Training loss: 0.02048657, Validation loss: 1.15966462, Gradient norm: 0.43107158
INFO:root:[   10] Training loss: 0.01997211, Validation loss: 1.16318541, Gradient norm: 0.47739019
INFO:root:[   11] Training loss: 0.01991679, Validation loss: 1.10884939, Gradient norm: 0.43981749
INFO:root:[   12] Training loss: 0.01946044, Validation loss: 1.08081156, Gradient norm: 0.44236868
INFO:root:[   13] Training loss: 0.01880474, Validation loss: 1.08072453, Gradient norm: 0.41775373
INFO:root:[   14] Training loss: 0.01846027, Validation loss: 1.03451969, Gradient norm: 0.38930039
INFO:root:[   15] Training loss: 0.01834198, Validation loss: 1.08844285, Gradient norm: 0.39327872
INFO:root:[   16] Training loss: 0.01823295, Validation loss: 1.03373278, Gradient norm: 0.47127146
INFO:root:[   17] Training loss: 0.01748502, Validation loss: 1.03340662, Gradient norm: 0.38325328
INFO:root:[   18] Training loss: 0.01767341, Validation loss: 1.02052685, Gradient norm: 0.40348638
INFO:root:[   19] Training loss: 0.01722892, Validation loss: 0.97063742, Gradient norm: 0.41081802
INFO:root:[   20] Training loss: 0.01708496, Validation loss: 0.97347874, Gradient norm: 0.40220434
INFO:root:[   21] Training loss: 0.01687288, Validation loss: 0.94999662, Gradient norm: 0.40547717
INFO:root:[   22] Training loss: 0.01673748, Validation loss: 0.93505956, Gradient norm: 0.38899102
INFO:root:[   23] Training loss: 0.01642211, Validation loss: 0.94493704, Gradient norm: 0.39058769
INFO:root:[   24] Training loss: 0.01629084, Validation loss: 0.94276623, Gradient norm: 0.38903707
INFO:root:[   25] Training loss: 0.01629368, Validation loss: 1.02054671, Gradient norm: 0.40404325
INFO:root:[   26] Training loss: 0.01629283, Validation loss: 0.91559744, Gradient norm: 0.40985217
INFO:root:[   27] Training loss: 0.01587757, Validation loss: 0.90836870, Gradient norm: 0.39175075
INFO:root:[   28] Training loss: 0.01584393, Validation loss: 0.90449037, Gradient norm: 0.41121546
INFO:root:[   29] Training loss: 0.01541696, Validation loss: 0.90266259, Gradient norm: 0.36135097
INFO:root:[   30] Training loss: 0.01547663, Validation loss: 0.89815935, Gradient norm: 0.36643994
INFO:root:[   31] Training loss: 0.01528376, Validation loss: 0.94080446, Gradient norm: 0.33899305
INFO:root:[   32] Training loss: 0.01531558, Validation loss: 0.88415870, Gradient norm: 0.37806138
INFO:root:[   33] Training loss: 0.01501667, Validation loss: 0.88608296, Gradient norm: 0.35525321
INFO:root:[   34] Training loss: 0.01493049, Validation loss: 0.83767032, Gradient norm: 0.37489974
INFO:root:[   35] Training loss: 0.01478220, Validation loss: 0.83788116, Gradient norm: 0.31697089
INFO:root:[   36] Training loss: 0.01469068, Validation loss: 0.83817077, Gradient norm: 0.35039295
INFO:root:[   37] Training loss: 0.01464304, Validation loss: 0.85351045, Gradient norm: 0.39197755
INFO:root:[   38] Training loss: 0.01438167, Validation loss: 0.82066316, Gradient norm: 0.32148294
INFO:root:[   39] Training loss: 0.01425456, Validation loss: 0.86944960, Gradient norm: 0.32287770
INFO:root:[   40] Training loss: 0.01429931, Validation loss: 0.80633793, Gradient norm: 0.33586791
INFO:root:[   41] Training loss: 0.01429062, Validation loss: 0.83944238, Gradient norm: 0.37319491
INFO:root:[   42] Training loss: 0.01388540, Validation loss: 0.81062802, Gradient norm: 0.30894685
INFO:root:[   43] Training loss: 0.01412966, Validation loss: 0.86083295, Gradient norm: 0.34345914
INFO:root:[   44] Training loss: 0.01377619, Validation loss: 0.82365206, Gradient norm: 0.34467228
INFO:root:[   45] Training loss: 0.01366655, Validation loss: 0.84663618, Gradient norm: 0.32284596
INFO:root:[   46] Training loss: 0.01402254, Validation loss: 0.79887069, Gradient norm: 0.40382384
INFO:root:[   47] Training loss: 0.01357354, Validation loss: 0.77232530, Gradient norm: 0.32110299
INFO:root:[   48] Training loss: 0.01333444, Validation loss: 0.81475930, Gradient norm: 0.30587624
INFO:root:[   49] Training loss: 0.01338819, Validation loss: 0.74416341, Gradient norm: 0.38071043
INFO:root:[   50] Training loss: 0.01337777, Validation loss: 0.78465547, Gradient norm: 0.37044173
INFO:root:[   51] Training loss: 0.01284778, Validation loss: 0.74693345, Gradient norm: 0.27931843
INFO:root:[   52] Training loss: 0.01319209, Validation loss: 0.75020429, Gradient norm: 0.35387640
INFO:root:[   53] Training loss: 0.01296587, Validation loss: 0.72871780, Gradient norm: 0.34102026
INFO:root:[   54] Training loss: 0.01324988, Validation loss: 0.78297834, Gradient norm: 0.42376512
INFO:root:[   55] Training loss: 0.01284070, Validation loss: 0.73595218, Gradient norm: 0.31449218
INFO:root:[   56] Training loss: 0.01265976, Validation loss: 0.71544902, Gradient norm: 0.31791479
INFO:root:[   57] Training loss: 0.01288839, Validation loss: 0.71356687, Gradient norm: 0.36401117
INFO:root:[   58] Training loss: 0.01253631, Validation loss: 0.74959709, Gradient norm: 0.34554047
INFO:root:[   59] Training loss: 0.01267748, Validation loss: 0.80687827, Gradient norm: 0.38400348
INFO:root:[   60] Training loss: 0.01237729, Validation loss: 0.69105771, Gradient norm: 0.31491760
INFO:root:[   61] Training loss: 0.01254769, Validation loss: 0.79383793, Gradient norm: 0.36358523
INFO:root:[   62] Training loss: 0.01209144, Validation loss: 0.69352215, Gradient norm: 0.31086119
INFO:root:[   63] Training loss: 0.01231104, Validation loss: 0.72743127, Gradient norm: 0.32324710
INFO:root:[   64] Training loss: 0.01235111, Validation loss: 0.68606373, Gradient norm: 0.35532696
INFO:root:[   65] Training loss: 0.01205136, Validation loss: 0.68237831, Gradient norm: 0.33100226
INFO:root:[   66] Training loss: 0.01187256, Validation loss: 0.70648277, Gradient norm: 0.35592565
INFO:root:[   67] Training loss: 0.01185601, Validation loss: 0.66786459, Gradient norm: 0.30415487
INFO:root:[   68] Training loss: 0.01217333, Validation loss: 0.69354108, Gradient norm: 0.42299541
INFO:root:[   69] Training loss: 0.01184985, Validation loss: 0.68788555, Gradient norm: 0.31229161
INFO:root:[   70] Training loss: 0.01198333, Validation loss: 0.68432446, Gradient norm: 0.38509238
INFO:root:[   71] Training loss: 0.01170755, Validation loss: 0.65869874, Gradient norm: 0.31524158
INFO:root:[   72] Training loss: 0.01172246, Validation loss: 0.65647019, Gradient norm: 0.30486498
INFO:root:[   73] Training loss: 0.01176542, Validation loss: 0.65658939, Gradient norm: 0.39094859
INFO:root:[   74] Training loss: 0.01146338, Validation loss: 0.69945928, Gradient norm: 0.33698781
INFO:root:[   75] Training loss: 0.01143284, Validation loss: 0.66903632, Gradient norm: 0.33251078
INFO:root:[   76] Training loss: 0.01159967, Validation loss: 0.63585972, Gradient norm: 0.37794914
INFO:root:[   77] Training loss: 0.01129035, Validation loss: 0.64330561, Gradient norm: 0.31507287
INFO:root:[   78] Training loss: 0.01134863, Validation loss: 0.61709861, Gradient norm: 0.34200981
INFO:root:[   79] Training loss: 0.01120212, Validation loss: 0.63349926, Gradient norm: 0.29728359
INFO:root:[   80] Training loss: 0.01128844, Validation loss: 0.64152907, Gradient norm: 0.33699555
INFO:root:[   81] Training loss: 0.01115603, Validation loss: 0.65469367, Gradient norm: 0.30149248
INFO:root:[   82] Training loss: 0.01135585, Validation loss: 0.69200469, Gradient norm: 0.38175068
INFO:root:[   83] Training loss: 0.01119407, Validation loss: 0.65253075, Gradient norm: 0.32221582
INFO:root:[   84] Training loss: 0.01082139, Validation loss: 0.68802496, Gradient norm: 0.30982241
INFO:root:[   85] Training loss: 0.01087466, Validation loss: 0.62844869, Gradient norm: 0.36118772
INFO:root:[   86] Training loss: 0.01098190, Validation loss: 0.64613779, Gradient norm: 0.35915637
INFO:root:[   87] Training loss: 0.01069487, Validation loss: 0.60290853, Gradient norm: 0.31780644
INFO:root:[   88] Training loss: 0.01079939, Validation loss: 0.69217247, Gradient norm: 0.31485445
INFO:root:[   89] Training loss: 0.01100383, Validation loss: 0.63534461, Gradient norm: 0.36065157
INFO:root:[   90] Training loss: 0.01073058, Validation loss: 0.65344668, Gradient norm: 0.33149368
INFO:root:[   91] Training loss: 0.01070960, Validation loss: 0.60902473, Gradient norm: 0.34316783
INFO:root:[   92] Training loss: 0.01068062, Validation loss: 0.61733414, Gradient norm: 0.33598418
INFO:root:[   93] Training loss: 0.01057611, Validation loss: 0.64098323, Gradient norm: 0.30878554
INFO:root:[   94] Training loss: 0.01047642, Validation loss: 0.59550627, Gradient norm: 0.32651156
INFO:root:[   95] Training loss: 0.01041912, Validation loss: 0.59264573, Gradient norm: 0.30529317
INFO:root:[   96] Training loss: 0.01051715, Validation loss: 0.60177426, Gradient norm: 0.33643404
INFO:root:[   97] Training loss: 0.01033165, Validation loss: 0.57500374, Gradient norm: 0.32996488
INFO:root:[   98] Training loss: 0.01031824, Validation loss: 0.60480356, Gradient norm: 0.32878713
INFO:root:[   99] Training loss: 0.01021807, Validation loss: 0.58570692, Gradient norm: 0.31728937
INFO:root:[  100] Training loss: 0.01034209, Validation loss: 0.59618436, Gradient norm: 0.32852871
INFO:root:[  101] Training loss: 0.01020934, Validation loss: 0.56784235, Gradient norm: 0.29479424
INFO:root:[  102] Training loss: 0.01030531, Validation loss: 0.57977075, Gradient norm: 0.31650879
INFO:root:[  103] Training loss: 0.01037900, Validation loss: 0.61130846, Gradient norm: 0.36532070
INFO:root:[  104] Training loss: 0.01017274, Validation loss: 0.55706720, Gradient norm: 0.32262859
INFO:root:[  105] Training loss: 0.01019697, Validation loss: 0.60100882, Gradient norm: 0.34802852
INFO:root:[  106] Training loss: 0.00992599, Validation loss: 0.56963521, Gradient norm: 0.32393541
INFO:root:[  107] Training loss: 0.01007353, Validation loss: 0.57371410, Gradient norm: 0.31292496
INFO:root:[  108] Training loss: 0.01007530, Validation loss: 0.57599550, Gradient norm: 0.33028042
INFO:root:[  109] Training loss: 0.00993312, Validation loss: 0.59896355, Gradient norm: 0.31212694
INFO:root:[  110] Training loss: 0.00988815, Validation loss: 0.64163212, Gradient norm: 0.32073786
INFO:root:[  111] Training loss: 0.00981672, Validation loss: 0.56635644, Gradient norm: 0.31575149
INFO:root:[  112] Training loss: 0.01007182, Validation loss: 0.56968998, Gradient norm: 0.34317209
INFO:root:[  113] Training loss: 0.00980618, Validation loss: 0.55149152, Gradient norm: 0.30909975
INFO:root:[  114] Training loss: 0.00977228, Validation loss: 0.61772625, Gradient norm: 0.32634849
INFO:root:[  115] Training loss: 0.00987332, Validation loss: 0.58742563, Gradient norm: 0.33945702
INFO:root:[  116] Training loss: 0.00972758, Validation loss: 0.53935762, Gradient norm: 0.32874035
INFO:root:[  117] Training loss: 0.00975561, Validation loss: 0.55365406, Gradient norm: 0.33707991
INFO:root:[  118] Training loss: 0.00991338, Validation loss: 0.60923118, Gradient norm: 0.38109641
INFO:root:[  119] Training loss: 0.00966079, Validation loss: 0.54527607, Gradient norm: 0.32391854
INFO:root:[  120] Training loss: 0.00969645, Validation loss: 0.56962454, Gradient norm: 0.32946140
INFO:root:[  121] Training loss: 0.00966604, Validation loss: 0.53449800, Gradient norm: 0.31481102
INFO:root:[  122] Training loss: 0.00963347, Validation loss: 0.53210602, Gradient norm: 0.31377448
INFO:root:[  123] Training loss: 0.00969777, Validation loss: 0.56057027, Gradient norm: 0.36135736
INFO:root:[  124] Training loss: 0.00956456, Validation loss: 0.56907344, Gradient norm: 0.31351127
INFO:root:[  125] Training loss: 0.00958747, Validation loss: 0.52861689, Gradient norm: 0.33057157
INFO:root:[  126] Training loss: 0.00934165, Validation loss: 0.53387778, Gradient norm: 0.29080629
INFO:root:[  127] Training loss: 0.00956915, Validation loss: 0.64253043, Gradient norm: 0.31637522
INFO:root:[  128] Training loss: 0.00963986, Validation loss: 0.58317042, Gradient norm: 0.34817480
INFO:root:[  129] Training loss: 0.00925759, Validation loss: 0.56011031, Gradient norm: 0.29526337
INFO:root:[  130] Training loss: 0.00949063, Validation loss: 0.52826588, Gradient norm: 0.33550453
INFO:root:[  131] Training loss: 0.00927316, Validation loss: 0.51425894, Gradient norm: 0.29414571
INFO:root:[  132] Training loss: 0.00943922, Validation loss: 0.57966969, Gradient norm: 0.33063911
INFO:root:[  133] Training loss: 0.00937315, Validation loss: 0.54002774, Gradient norm: 0.29815979
INFO:root:[  134] Training loss: 0.00920085, Validation loss: 0.52684657, Gradient norm: 0.26997790
INFO:root:[  135] Training loss: 0.00934940, Validation loss: 0.53049367, Gradient norm: 0.32011110
INFO:root:[  136] Training loss: 0.00942145, Validation loss: 0.53740755, Gradient norm: 0.30608509
INFO:root:[  137] Training loss: 0.00962235, Validation loss: 0.53709478, Gradient norm: 0.35833300
INFO:root:[  138] Training loss: 0.00946352, Validation loss: 0.54626345, Gradient norm: 0.37111264
INFO:root:[  139] Training loss: 0.00918296, Validation loss: 0.52026735, Gradient norm: 0.28858424
INFO:root:[  140] Training loss: 0.00911574, Validation loss: 0.52889638, Gradient norm: 0.30172791
INFO:root:EP 140: Early stopping
INFO:root:Training the model took 3628.946s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08429
INFO:root:EnergyScoretrain: 0.06309
INFO:root:Coveragetrain: 6.94924
INFO:root:IntervalWidthtrain: 217.24746
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02267
INFO:root:EnergyScorevalidation: 0.01699
INFO:root:Coveragevalidation: 1.85174
INFO:root:IntervalWidthvalidation: 55.83168
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04552
INFO:root:EnergyScoretest: 0.03677
INFO:root:Coveragetest: 0.66235
INFO:root:IntervalWidthtest: 54.12172
INFO:root:###4 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.2, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04760551, Validation loss: 1.98315608, Gradient norm: 0.38785439
INFO:root:[    2] Training loss: 0.03192954, Validation loss: 1.72220902, Gradient norm: 0.30029625
INFO:root:[    3] Training loss: 0.02802663, Validation loss: 1.56649765, Gradient norm: 0.29452817
INFO:root:[    4] Training loss: 0.02642759, Validation loss: 1.45777806, Gradient norm: 0.27279523
INFO:root:[    5] Training loss: 0.02485483, Validation loss: 1.42809960, Gradient norm: 0.25477720
INFO:root:[    6] Training loss: 0.02390732, Validation loss: 1.31919647, Gradient norm: 0.30079692
INFO:root:[    7] Training loss: 0.02286101, Validation loss: 1.28277755, Gradient norm: 0.25834902
INFO:root:[    8] Training loss: 0.02248879, Validation loss: 1.24352656, Gradient norm: 0.28946828
INFO:root:[    9] Training loss: 0.02166964, Validation loss: 1.18510434, Gradient norm: 0.31057768
INFO:root:[   10] Training loss: 0.02092946, Validation loss: 1.23219006, Gradient norm: 0.26885722
INFO:root:[   11] Training loss: 0.02049444, Validation loss: 1.17025838, Gradient norm: 0.29262639
INFO:root:[   12] Training loss: 0.01987778, Validation loss: 1.16932016, Gradient norm: 0.26201841
INFO:root:[   13] Training loss: 0.01967502, Validation loss: 1.14264946, Gradient norm: 0.27635161
INFO:root:[   14] Training loss: 0.01911182, Validation loss: 1.06579443, Gradient norm: 0.24834157
INFO:root:[   15] Training loss: 0.01894079, Validation loss: 1.06819230, Gradient norm: 0.28586548
INFO:root:[   16] Training loss: 0.01881110, Validation loss: 1.05777802, Gradient norm: 0.28672092
INFO:root:[   17] Training loss: 0.01834653, Validation loss: 1.05436418, Gradient norm: 0.26693384
INFO:root:[   18] Training loss: 0.01798140, Validation loss: 1.00815995, Gradient norm: 0.25707324
INFO:root:[   19] Training loss: 0.01799597, Validation loss: 1.02377466, Gradient norm: 0.27606364
INFO:root:[   20] Training loss: 0.01762240, Validation loss: 1.03453135, Gradient norm: 0.26978795
INFO:root:[   21] Training loss: 0.01746152, Validation loss: 0.97828242, Gradient norm: 0.25385535
INFO:root:[   22] Training loss: 0.01721986, Validation loss: 1.01426721, Gradient norm: 0.25875863
INFO:root:[   23] Training loss: 0.01715195, Validation loss: 0.96270504, Gradient norm: 0.26896829
INFO:root:[   24] Training loss: 0.01705823, Validation loss: 0.98972414, Gradient norm: 0.26284090
INFO:root:[   25] Training loss: 0.01697924, Validation loss: 0.97101187, Gradient norm: 0.26928410
INFO:root:[   26] Training loss: 0.01693787, Validation loss: 0.93752330, Gradient norm: 0.30123369
INFO:root:[   27] Training loss: 0.01650286, Validation loss: 0.93831732, Gradient norm: 0.26516482
INFO:root:[   28] Training loss: 0.01633750, Validation loss: 0.96382174, Gradient norm: 0.25985979
INFO:root:[   29] Training loss: 0.01622573, Validation loss: 0.92774446, Gradient norm: 0.24904599
INFO:root:[   30] Training loss: 0.01615021, Validation loss: 0.94692372, Gradient norm: 0.26930391
INFO:root:[   31] Training loss: 0.01629905, Validation loss: 0.92470272, Gradient norm: 0.28238034
INFO:root:[   32] Training loss: 0.01581170, Validation loss: 0.90332082, Gradient norm: 0.26394338
INFO:root:[   33] Training loss: 0.01578794, Validation loss: 0.99355665, Gradient norm: 0.25719791
INFO:root:[   34] Training loss: 0.01567673, Validation loss: 0.88737950, Gradient norm: 0.24119400
INFO:root:[   35] Training loss: 0.01551278, Validation loss: 0.91427374, Gradient norm: 0.27624026
INFO:root:[   36] Training loss: 0.01542935, Validation loss: 0.87163040, Gradient norm: 0.25787146
INFO:root:[   37] Training loss: 0.01534859, Validation loss: 0.92625039, Gradient norm: 0.26930167
INFO:root:[   38] Training loss: 0.01509596, Validation loss: 0.88603672, Gradient norm: 0.25611537
INFO:root:[   39] Training loss: 0.01530590, Validation loss: 0.95224087, Gradient norm: 0.28831091
INFO:root:[   40] Training loss: 0.01490897, Validation loss: 0.90069230, Gradient norm: 0.26172961
INFO:root:[   41] Training loss: 0.01488084, Validation loss: 0.87856819, Gradient norm: 0.24792457
INFO:root:[   42] Training loss: 0.01491116, Validation loss: 0.87620142, Gradient norm: 0.25229397
INFO:root:[   43] Training loss: 0.01476015, Validation loss: 0.85709443, Gradient norm: 0.25307588
INFO:root:[   44] Training loss: 0.01471298, Validation loss: 0.85790115, Gradient norm: 0.27419596
INFO:root:[   45] Training loss: 0.01455040, Validation loss: 0.87625409, Gradient norm: 0.24609325
INFO:root:[   46] Training loss: 0.01439474, Validation loss: 0.85044951, Gradient norm: 0.26308631
INFO:root:[   47] Training loss: 0.01439250, Validation loss: 0.83094006, Gradient norm: 0.27799343
INFO:root:[   48] Training loss: 0.01467899, Validation loss: 0.84399915, Gradient norm: 0.33508888
INFO:root:[   49] Training loss: 0.01424445, Validation loss: 0.84055698, Gradient norm: 0.26675626
INFO:root:[   50] Training loss: 0.01414298, Validation loss: 0.81016458, Gradient norm: 0.25587009
INFO:root:[   51] Training loss: 0.01426182, Validation loss: 0.79248595, Gradient norm: 0.28562378
INFO:root:[   52] Training loss: 0.01404797, Validation loss: 0.78948291, Gradient norm: 0.26640735
INFO:root:[   53] Training loss: 0.01401028, Validation loss: 0.80115272, Gradient norm: 0.27392548
INFO:root:[   54] Training loss: 0.01399753, Validation loss: 0.82331045, Gradient norm: 0.28295608
INFO:root:[   55] Training loss: 0.01361681, Validation loss: 0.79173589, Gradient norm: 0.23359122
INFO:root:[   56] Training loss: 0.01361966, Validation loss: 0.81330810, Gradient norm: 0.24913998
INFO:root:[   57] Training loss: 0.01371150, Validation loss: 0.79798984, Gradient norm: 0.28717877
INFO:root:[   58] Training loss: 0.01336712, Validation loss: 0.75837837, Gradient norm: 0.27943321
INFO:root:[   59] Training loss: 0.01329668, Validation loss: 0.76085536, Gradient norm: 0.26553298
INFO:root:[   60] Training loss: 0.01334904, Validation loss: 0.78436837, Gradient norm: 0.27363717
INFO:root:[   61] Training loss: 0.01309375, Validation loss: 0.76227493, Gradient norm: 0.23312493
INFO:root:[   62] Training loss: 0.01317601, Validation loss: 0.75569981, Gradient norm: 0.28509122
INFO:root:[   63] Training loss: 0.01302501, Validation loss: 0.77275275, Gradient norm: 0.26525584
INFO:root:[   64] Training loss: 0.01286875, Validation loss: 0.74491705, Gradient norm: 0.25150748
INFO:root:[   65] Training loss: 0.01287235, Validation loss: 0.74525869, Gradient norm: 0.26593119
INFO:root:[   66] Training loss: 0.01277628, Validation loss: 0.79439577, Gradient norm: 0.28356530
INFO:root:[   67] Training loss: 0.01257545, Validation loss: 0.69422989, Gradient norm: 0.27447141
INFO:root:[   68] Training loss: 0.01262005, Validation loss: 0.78240723, Gradient norm: 0.27668444
INFO:root:[   69] Training loss: 0.01255325, Validation loss: 0.71908222, Gradient norm: 0.27970205
INFO:root:[   70] Training loss: 0.01230892, Validation loss: 0.71615683, Gradient norm: 0.25210773
INFO:root:[   71] Training loss: 0.01253015, Validation loss: 0.75099286, Gradient norm: 0.28984946
INFO:root:[   72] Training loss: 0.01222446, Validation loss: 0.69417879, Gradient norm: 0.26874100
INFO:root:[   73] Training loss: 0.01214535, Validation loss: 0.71687494, Gradient norm: 0.26294593
INFO:root:[   74] Training loss: 0.01212408, Validation loss: 0.69859416, Gradient norm: 0.25484053
INFO:root:[   75] Training loss: 0.01222954, Validation loss: 0.75904804, Gradient norm: 0.29828397
INFO:root:[   76] Training loss: 0.01181557, Validation loss: 0.66776881, Gradient norm: 0.25451471
INFO:root:[   77] Training loss: 0.01185320, Validation loss: 0.67409157, Gradient norm: 0.28132449
INFO:root:[   78] Training loss: 0.01188683, Validation loss: 0.68940097, Gradient norm: 0.29078087
INFO:root:[   79] Training loss: 0.01177308, Validation loss: 0.72086683, Gradient norm: 0.29128794
INFO:root:[   80] Training loss: 0.01153461, Validation loss: 0.68150569, Gradient norm: 0.25032408
INFO:root:[   81] Training loss: 0.01155558, Validation loss: 0.68678675, Gradient norm: 0.27269578
INFO:root:[   82] Training loss: 0.01152437, Validation loss: 0.65260588, Gradient norm: 0.26091403
INFO:root:[   83] Training loss: 0.01138644, Validation loss: 0.65892843, Gradient norm: 0.26352656
INFO:root:[   84] Training loss: 0.01159364, Validation loss: 0.64904909, Gradient norm: 0.25365348
INFO:root:[   85] Training loss: 0.01141739, Validation loss: 0.62522744, Gradient norm: 0.28935595
INFO:root:[   86] Training loss: 0.01127032, Validation loss: 0.66550924, Gradient norm: 0.27397211
INFO:root:[   87] Training loss: 0.01130408, Validation loss: 0.65877813, Gradient norm: 0.27047967
INFO:root:[   88] Training loss: 0.01128772, Validation loss: 0.66700789, Gradient norm: 0.27061353
INFO:root:[   89] Training loss: 0.01097360, Validation loss: 0.64550030, Gradient norm: 0.28476309
INFO:root:[   90] Training loss: 0.01111930, Validation loss: 0.65104766, Gradient norm: 0.25041035
INFO:root:[   91] Training loss: 0.01091476, Validation loss: 0.61918429, Gradient norm: 0.25166445
INFO:root:[   92] Training loss: 0.01084210, Validation loss: 0.63576363, Gradient norm: 0.26806073
INFO:root:[   93] Training loss: 0.01100837, Validation loss: 0.64649283, Gradient norm: 0.27780295
INFO:root:[   94] Training loss: 0.01107583, Validation loss: 0.62093378, Gradient norm: 0.28423436
INFO:root:[   95] Training loss: 0.01086302, Validation loss: 0.61392553, Gradient norm: 0.27523161
INFO:root:[   96] Training loss: 0.01071680, Validation loss: 0.59698590, Gradient norm: 0.26552668
INFO:root:[   97] Training loss: 0.01076942, Validation loss: 0.63371192, Gradient norm: 0.27584578
INFO:root:[   98] Training loss: 0.01067080, Validation loss: 0.81072472, Gradient norm: 0.25760147
INFO:root:[   99] Training loss: 0.01083207, Validation loss: 0.60693192, Gradient norm: 0.31608554
INFO:root:[  100] Training loss: 0.01080562, Validation loss: 0.59331921, Gradient norm: 0.31070681
INFO:root:[  101] Training loss: 0.01049592, Validation loss: 0.59616219, Gradient norm: 0.23848417
INFO:root:[  102] Training loss: 0.01041029, Validation loss: 0.62005422, Gradient norm: 0.25932728
INFO:root:[  103] Training loss: 0.01082604, Validation loss: 0.62603036, Gradient norm: 0.29916589
INFO:root:[  104] Training loss: 0.01040697, Validation loss: 0.61058329, Gradient norm: 0.27423947
INFO:root:[  105] Training loss: 0.01036612, Validation loss: 0.59246716, Gradient norm: 0.26515421
INFO:root:[  106] Training loss: 0.01047776, Validation loss: 0.59229861, Gradient norm: 0.28637107
INFO:root:[  107] Training loss: 0.01022180, Validation loss: 0.63831497, Gradient norm: 0.26530255
INFO:root:[  108] Training loss: 0.01030630, Validation loss: 0.57874544, Gradient norm: 0.26567902
INFO:root:[  109] Training loss: 0.01033393, Validation loss: 0.57677348, Gradient norm: 0.29003652
INFO:root:[  110] Training loss: 0.01025250, Validation loss: 0.58738804, Gradient norm: 0.25070597
INFO:root:[  111] Training loss: 0.01026248, Validation loss: 0.62122621, Gradient norm: 0.27883151
INFO:root:[  112] Training loss: 0.01013819, Validation loss: 0.62902110, Gradient norm: 0.25370188
INFO:root:[  113] Training loss: 0.01018227, Validation loss: 0.57487607, Gradient norm: 0.29354204
INFO:root:[  114] Training loss: 0.00999426, Validation loss: 0.57225659, Gradient norm: 0.28691247
INFO:root:[  115] Training loss: 0.01029584, Validation loss: 0.58325262, Gradient norm: 0.30938868
INFO:root:[  116] Training loss: 0.01004463, Validation loss: 0.59021161, Gradient norm: 0.26666611
INFO:root:[  117] Training loss: 0.01009217, Validation loss: 0.64181004, Gradient norm: 0.29164510
INFO:root:[  118] Training loss: 0.00990227, Validation loss: 0.59066768, Gradient norm: 0.25106971
INFO:root:[  119] Training loss: 0.00994774, Validation loss: 0.59609368, Gradient norm: 0.25147961
INFO:root:[  120] Training loss: 0.01009195, Validation loss: 0.59634121, Gradient norm: 0.28692991
INFO:root:[  121] Training loss: 0.00980342, Validation loss: 0.58841262, Gradient norm: 0.27679355
INFO:root:[  122] Training loss: 0.00996670, Validation loss: 0.55929549, Gradient norm: 0.26713962
INFO:root:[  123] Training loss: 0.00995148, Validation loss: 0.56249814, Gradient norm: 0.28295280
INFO:root:[  124] Training loss: 0.00990179, Validation loss: 0.56102910, Gradient norm: 0.27119306
INFO:root:[  125] Training loss: 0.00979585, Validation loss: 0.54530257, Gradient norm: 0.26899785
INFO:root:[  126] Training loss: 0.00959293, Validation loss: 0.56657837, Gradient norm: 0.24374774
INFO:root:[  127] Training loss: 0.01002866, Validation loss: 0.54597905, Gradient norm: 0.31552881
INFO:root:[  128] Training loss: 0.00950894, Validation loss: 0.55200036, Gradient norm: 0.24499118
INFO:root:[  129] Training loss: 0.00964600, Validation loss: 0.61512791, Gradient norm: 0.25856445
INFO:root:[  130] Training loss: 0.00974206, Validation loss: 0.52954641, Gradient norm: 0.27270487
INFO:root:[  131] Training loss: 0.00971798, Validation loss: 0.61167049, Gradient norm: 0.27364861
INFO:root:[  132] Training loss: 0.00975936, Validation loss: 0.55627394, Gradient norm: 0.30978334
INFO:root:[  133] Training loss: 0.00952320, Validation loss: 0.54269137, Gradient norm: 0.26586159
INFO:root:[  134] Training loss: 0.00960359, Validation loss: 0.56511294, Gradient norm: 0.26766516
INFO:root:[  135] Training loss: 0.00953111, Validation loss: 0.67513099, Gradient norm: 0.25834916
INFO:root:[  136] Training loss: 0.00953116, Validation loss: 0.57577359, Gradient norm: 0.29114405
INFO:root:[  137] Training loss: 0.00948800, Validation loss: 0.58934065, Gradient norm: 0.27957067
INFO:root:[  138] Training loss: 0.00954702, Validation loss: 0.54290877, Gradient norm: 0.24820405
INFO:root:[  139] Training loss: 0.00967668, Validation loss: 0.56591836, Gradient norm: 0.28705485
INFO:root:[  140] Training loss: 0.00927293, Validation loss: 0.52905174, Gradient norm: 0.23893551
INFO:root:[  141] Training loss: 0.00940571, Validation loss: 0.54495848, Gradient norm: 0.26796699
INFO:root:[  142] Training loss: 0.00947545, Validation loss: 0.57066055, Gradient norm: 0.26431886
INFO:root:[  143] Training loss: 0.00967079, Validation loss: 0.53892576, Gradient norm: 0.30648468
INFO:root:[  144] Training loss: 0.00915232, Validation loss: 0.56390000, Gradient norm: 0.23791785
INFO:root:[  145] Training loss: 0.00917829, Validation loss: 0.52389935, Gradient norm: 0.26511999
INFO:root:[  146] Training loss: 0.00951535, Validation loss: 0.53390399, Gradient norm: 0.30012413
INFO:root:[  147] Training loss: 0.00954804, Validation loss: 0.52634999, Gradient norm: 0.31064543
INFO:root:[  148] Training loss: 0.00918368, Validation loss: 0.54997894, Gradient norm: 0.23986661
INFO:root:[  149] Training loss: 0.00917533, Validation loss: 0.55074001, Gradient norm: 0.26724682
INFO:root:[  150] Training loss: 0.00912347, Validation loss: 0.52988423, Gradient norm: 0.25391517
INFO:root:[  151] Training loss: 0.00930978, Validation loss: 0.52944728, Gradient norm: 0.28109400
INFO:root:[  152] Training loss: 0.00911758, Validation loss: 0.53029732, Gradient norm: 0.24874590
INFO:root:[  153] Training loss: 0.00913709, Validation loss: 0.53264895, Gradient norm: 0.29189228
INFO:root:[  154] Training loss: 0.00915542, Validation loss: 0.50342534, Gradient norm: 0.25086751
INFO:root:[  155] Training loss: 0.00916715, Validation loss: 0.51574134, Gradient norm: 0.26803142
INFO:root:[  156] Training loss: 0.00917466, Validation loss: 0.59213174, Gradient norm: 0.27473800
INFO:root:[  157] Training loss: 0.00903698, Validation loss: 0.49989298, Gradient norm: 0.28275216
INFO:root:[  158] Training loss: 0.00906399, Validation loss: 0.54820113, Gradient norm: 0.26104703
INFO:root:[  159] Training loss: 0.00920426, Validation loss: 0.54540957, Gradient norm: 0.26817177
INFO:root:[  160] Training loss: 0.00937420, Validation loss: 0.52817928, Gradient norm: 0.27247881
INFO:root:[  161] Training loss: 0.00922027, Validation loss: 0.53039185, Gradient norm: 0.26626404
INFO:root:[  162] Training loss: 0.00913297, Validation loss: 0.52833285, Gradient norm: 0.28686344
INFO:root:[  163] Training loss: 0.00913942, Validation loss: 0.54450360, Gradient norm: 0.29503840
INFO:root:[  164] Training loss: 0.00899858, Validation loss: 0.50632611, Gradient norm: 0.28160890
INFO:root:[  165] Training loss: 0.00889772, Validation loss: 0.50976064, Gradient norm: 0.24549976
INFO:root:[  166] Training loss: 0.00881707, Validation loss: 0.51142619, Gradient norm: 0.25716918
INFO:root:EP 166: Early stopping
INFO:root:Training the model took 4270.157s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08195
INFO:root:EnergyScoretrain: 0.06155
INFO:root:Coveragetrain: 6.96
INFO:root:IntervalWidthtrain: 223.6073
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02354
INFO:root:EnergyScorevalidation: 0.01736
INFO:root:Coveragevalidation: 1.85483
INFO:root:IntervalWidthvalidation: 58.69326
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04077
INFO:root:EnergyScoretest: 0.03254
INFO:root:Coveragetest: 0.69617
INFO:root:IntervalWidthtest: 55.09309
INFO:root:###5 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.01, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04422090, Validation loss: 1.74484535, Gradient norm: 0.63573961
INFO:root:[    2] Training loss: 0.02871732, Validation loss: 1.50354492, Gradient norm: 0.60878448
INFO:root:[    3] Training loss: 0.02526147, Validation loss: 1.41707100, Gradient norm: 0.55152848
INFO:root:[    4] Training loss: 0.02407969, Validation loss: 1.45662635, Gradient norm: 0.65177819
INFO:root:[    5] Training loss: 0.02296197, Validation loss: 1.61542402, Gradient norm: 0.63891399
INFO:root:[    6] Training loss: 0.02216140, Validation loss: 1.20197102, Gradient norm: 0.58477970
INFO:root:[    7] Training loss: 0.02101909, Validation loss: 1.19675422, Gradient norm: 0.44365453
INFO:root:[    8] Training loss: 0.02056129, Validation loss: 1.11621500, Gradient norm: 0.46269945
INFO:root:[    9] Training loss: 0.02041168, Validation loss: 1.10988514, Gradient norm: 0.50846064
INFO:root:[   10] Training loss: 0.01991770, Validation loss: 1.15453748, Gradient norm: 0.47407760
INFO:root:[   11] Training loss: 0.01952021, Validation loss: 1.23070875, Gradient norm: 0.48794282
INFO:root:[   12] Training loss: 0.01921261, Validation loss: 1.06831365, Gradient norm: 0.46483327
INFO:root:[   13] Training loss: 0.01885961, Validation loss: 1.04007661, Gradient norm: 0.42595838
INFO:root:[   14] Training loss: 0.01863421, Validation loss: 1.08784143, Gradient norm: 0.43921084
INFO:root:[   15] Training loss: 0.01826695, Validation loss: 1.16094127, Gradient norm: 0.42367357
INFO:root:[   16] Training loss: 0.01806133, Validation loss: 1.04015697, Gradient norm: 0.44175295
INFO:root:[   17] Training loss: 0.01772008, Validation loss: 1.01935675, Gradient norm: 0.38435139
INFO:root:[   18] Training loss: 0.01778645, Validation loss: 1.00503931, Gradient norm: 0.44019237
INFO:root:[   19] Training loss: 0.01763030, Validation loss: 0.97188378, Gradient norm: 0.45776189
INFO:root:[   20] Training loss: 0.01751118, Validation loss: 0.98499059, Gradient norm: 0.45317018
INFO:root:[   21] Training loss: 0.01715005, Validation loss: 0.96703229, Gradient norm: 0.41922981
INFO:root:[   22] Training loss: 0.01702358, Validation loss: 0.95540568, Gradient norm: 0.42231564
INFO:root:[   23] Training loss: 0.01670985, Validation loss: 0.93743947, Gradient norm: 0.37726481
INFO:root:[   24] Training loss: 0.01672556, Validation loss: 1.25558829, Gradient norm: 0.40407559
INFO:root:[   25] Training loss: 0.01689718, Validation loss: 0.92011553, Gradient norm: 0.47851082
INFO:root:[   26] Training loss: 0.01621985, Validation loss: 0.92322849, Gradient norm: 0.37626776
INFO:root:[   27] Training loss: 0.01622160, Validation loss: 0.90746225, Gradient norm: 0.37986899
INFO:root:[   28] Training loss: 0.01595629, Validation loss: 0.91571866, Gradient norm: 0.38287947
INFO:root:[   29] Training loss: 0.01603094, Validation loss: 0.88998688, Gradient norm: 0.39972839
INFO:root:[   30] Training loss: 0.01590193, Validation loss: 0.87487219, Gradient norm: 0.36921649
INFO:root:[   31] Training loss: 0.01567284, Validation loss: 0.91806366, Gradient norm: 0.38945243
INFO:root:[   32] Training loss: 0.01562112, Validation loss: 0.93866019, Gradient norm: 0.36982324
INFO:root:[   33] Training loss: 0.01542209, Validation loss: 0.97386099, Gradient norm: 0.38427814
INFO:root:[   34] Training loss: 0.01551201, Validation loss: 0.87230430, Gradient norm: 0.44575318
INFO:root:[   35] Training loss: 0.01513733, Validation loss: 0.84613679, Gradient norm: 0.38472297
INFO:root:[   36] Training loss: 0.01490770, Validation loss: 0.87812544, Gradient norm: 0.37000684
INFO:root:[   37] Training loss: 0.01499721, Validation loss: 0.84104695, Gradient norm: 0.39180546
INFO:root:[   38] Training loss: 0.01474604, Validation loss: 0.84886829, Gradient norm: 0.36959948
INFO:root:[   39] Training loss: 0.01453287, Validation loss: 0.82930911, Gradient norm: 0.33901299
INFO:root:[   40] Training loss: 0.01441783, Validation loss: 0.81688199, Gradient norm: 0.35115759
INFO:root:[   41] Training loss: 0.01495959, Validation loss: 0.82459438, Gradient norm: 0.46765309
INFO:root:[   42] Training loss: 0.01440675, Validation loss: 0.83833954, Gradient norm: 0.37743770
INFO:root:[   43] Training loss: 0.01429556, Validation loss: 0.82745968, Gradient norm: 0.35710832
INFO:root:[   44] Training loss: 0.01410898, Validation loss: 0.79370171, Gradient norm: 0.35851443
INFO:root:[   45] Training loss: 0.01392076, Validation loss: 0.78952191, Gradient norm: 0.37422542
INFO:root:[   46] Training loss: 0.01385519, Validation loss: 0.77354822, Gradient norm: 0.35636061
INFO:root:[   47] Training loss: 0.01379944, Validation loss: 0.77370533, Gradient norm: 0.35335541
INFO:root:[   48] Training loss: 0.01380693, Validation loss: 0.81324979, Gradient norm: 0.35165502
INFO:root:[   49] Training loss: 0.01377385, Validation loss: 0.78428015, Gradient norm: 0.38047579
INFO:root:[   50] Training loss: 0.01349494, Validation loss: 0.77222624, Gradient norm: 0.32999426
INFO:root:[   51] Training loss: 0.01340587, Validation loss: 0.76710500, Gradient norm: 0.32465188
INFO:root:[   52] Training loss: 0.01323852, Validation loss: 0.79380627, Gradient norm: 0.32499133
INFO:root:[   53] Training loss: 0.01333168, Validation loss: 0.80103956, Gradient norm: 0.33978126
INFO:root:[   54] Training loss: 0.01316241, Validation loss: 0.81240939, Gradient norm: 0.36859008
INFO:root:[   55] Training loss: 0.01324060, Validation loss: 0.79867553, Gradient norm: 0.37914104
INFO:root:[   56] Training loss: 0.01280171, Validation loss: 0.74781465, Gradient norm: 0.31688681
INFO:root:[   57] Training loss: 0.01303920, Validation loss: 0.76240669, Gradient norm: 0.38127336
INFO:root:[   58] Training loss: 0.01272965, Validation loss: 0.74599353, Gradient norm: 0.32736980
INFO:root:[   59] Training loss: 0.01282942, Validation loss: 0.72803001, Gradient norm: 0.35380195
INFO:root:[   60] Training loss: 0.01274719, Validation loss: 0.75724015, Gradient norm: 0.36118046
INFO:root:[   61] Training loss: 0.01280388, Validation loss: 0.71892187, Gradient norm: 0.38951474
INFO:root:[   62] Training loss: 0.01269444, Validation loss: 0.72389756, Gradient norm: 0.35457505
INFO:root:[   63] Training loss: 0.01235720, Validation loss: 0.70605021, Gradient norm: 0.28151077
INFO:root:[   64] Training loss: 0.01266819, Validation loss: 0.70765489, Gradient norm: 0.37896551
INFO:root:[   65] Training loss: 0.01241947, Validation loss: 0.72089667, Gradient norm: 0.35008208
INFO:root:[   66] Training loss: 0.01252048, Validation loss: 0.71086043, Gradient norm: 0.38824191
INFO:root:[   67] Training loss: 0.01232621, Validation loss: 0.72747705, Gradient norm: 0.37064990
INFO:root:[   68] Training loss: 0.01216755, Validation loss: 0.69721561, Gradient norm: 0.31710890
INFO:root:[   69] Training loss: 0.01232847, Validation loss: 0.76186803, Gradient norm: 0.37071991
INFO:root:[   70] Training loss: 0.01217167, Validation loss: 0.68575517, Gradient norm: 0.36800245
INFO:root:[   71] Training loss: 0.01205504, Validation loss: 0.67245398, Gradient norm: 0.34980326
INFO:root:[   72] Training loss: 0.01187169, Validation loss: 0.71293764, Gradient norm: 0.32079668
INFO:root:[   73] Training loss: 0.01200567, Validation loss: 0.70403712, Gradient norm: 0.37701630
INFO:root:[   74] Training loss: 0.01177525, Validation loss: 0.66265022, Gradient norm: 0.32114163
INFO:root:[   75] Training loss: 0.01173754, Validation loss: 0.68339444, Gradient norm: 0.31375762
INFO:root:[   76] Training loss: 0.01170630, Validation loss: 0.65503692, Gradient norm: 0.33445771
INFO:root:[   77] Training loss: 0.01164171, Validation loss: 0.68632854, Gradient norm: 0.33008104
INFO:root:[   78] Training loss: 0.01163072, Validation loss: 0.71770336, Gradient norm: 0.32436548
INFO:root:[   79] Training loss: 0.01138117, Validation loss: 0.63472943, Gradient norm: 0.32258111
INFO:root:[   80] Training loss: 0.01179987, Validation loss: 0.71179201, Gradient norm: 0.43630840
INFO:root:[   81] Training loss: 0.01126945, Validation loss: 0.66620689, Gradient norm: 0.33242925
INFO:root:[   82] Training loss: 0.01131035, Validation loss: 0.67169724, Gradient norm: 0.37108748
INFO:root:[   83] Training loss: 0.01110047, Validation loss: 0.70646260, Gradient norm: 0.30852702
INFO:root:[   84] Training loss: 0.01121976, Validation loss: 0.65149858, Gradient norm: 0.34948448
INFO:root:[   85] Training loss: 0.01110383, Validation loss: 0.65674548, Gradient norm: 0.34453835
INFO:root:[   86] Training loss: 0.01140514, Validation loss: 0.62890114, Gradient norm: 0.40110084
INFO:root:[   87] Training loss: 0.01120822, Validation loss: 0.69489000, Gradient norm: 0.36483091
INFO:root:[   88] Training loss: 0.01098685, Validation loss: 0.62570748, Gradient norm: 0.35601120
INFO:root:[   89] Training loss: 0.01091763, Validation loss: 0.65816682, Gradient norm: 0.30740060
INFO:root:[   90] Training loss: 0.01076200, Validation loss: 0.62856618, Gradient norm: 0.32590542
INFO:root:[   91] Training loss: 0.01078445, Validation loss: 0.62123168, Gradient norm: 0.35949692
INFO:root:[   92] Training loss: 0.01093277, Validation loss: 0.62403625, Gradient norm: 0.37243782
INFO:root:[   93] Training loss: 0.01083672, Validation loss: 0.60563372, Gradient norm: 0.38907234
INFO:root:[   94] Training loss: 0.01064158, Validation loss: 0.67067291, Gradient norm: 0.33502353
INFO:root:[   95] Training loss: 0.01079865, Validation loss: 0.61897316, Gradient norm: 0.36221323
INFO:root:[   96] Training loss: 0.01055534, Validation loss: 0.63175977, Gradient norm: 0.34030770
INFO:root:[   97] Training loss: 0.01049023, Validation loss: 0.59341708, Gradient norm: 0.31999986
INFO:root:[   98] Training loss: 0.01029206, Validation loss: 0.62218198, Gradient norm: 0.28520358
INFO:root:[   99] Training loss: 0.01051461, Validation loss: 0.60238502, Gradient norm: 0.36012265
INFO:root:[  100] Training loss: 0.01042557, Validation loss: 0.60328790, Gradient norm: 0.31938504
INFO:root:[  101] Training loss: 0.01017641, Validation loss: 0.57507908, Gradient norm: 0.31714735
INFO:root:[  102] Training loss: 0.01039600, Validation loss: 0.58807054, Gradient norm: 0.35312769
INFO:root:[  103] Training loss: 0.01050954, Validation loss: 0.60528496, Gradient norm: 0.35841297
INFO:root:[  104] Training loss: 0.01038771, Validation loss: 0.57517852, Gradient norm: 0.38986374
INFO:root:[  105] Training loss: 0.01039242, Validation loss: 0.64445776, Gradient norm: 0.38015715
INFO:root:[  106] Training loss: 0.01016012, Validation loss: 0.60328091, Gradient norm: 0.36118138
INFO:root:[  107] Training loss: 0.01012696, Validation loss: 0.58748685, Gradient norm: 0.31511937
INFO:root:[  108] Training loss: 0.00998729, Validation loss: 0.59096917, Gradient norm: 0.29967408
INFO:root:[  109] Training loss: 0.01028320, Validation loss: 0.60675161, Gradient norm: 0.39387708
INFO:root:[  110] Training loss: 0.01020901, Validation loss: 0.56208974, Gradient norm: 0.39734575
INFO:root:[  111] Training loss: 0.00981498, Validation loss: 0.57714653, Gradient norm: 0.29759411
INFO:root:[  112] Training loss: 0.00998359, Validation loss: 0.58544610, Gradient norm: 0.35932413
INFO:root:[  113] Training loss: 0.00985375, Validation loss: 0.56463867, Gradient norm: 0.31301095
INFO:root:[  114] Training loss: 0.00997326, Validation loss: 0.61666023, Gradient norm: 0.32445599
INFO:root:[  115] Training loss: 0.00998206, Validation loss: 0.58355336, Gradient norm: 0.35503098
INFO:root:[  116] Training loss: 0.00982539, Validation loss: 0.55637658, Gradient norm: 0.32123556
INFO:root:[  117] Training loss: 0.00986546, Validation loss: 0.58724912, Gradient norm: 0.33734634
INFO:root:[  118] Training loss: 0.00984696, Validation loss: 0.59578002, Gradient norm: 0.30486490
INFO:root:[  119] Training loss: 0.00996487, Validation loss: 0.54322899, Gradient norm: 0.34304497
INFO:root:[  120] Training loss: 0.00968995, Validation loss: 0.55299732, Gradient norm: 0.32121115
INFO:root:[  121] Training loss: 0.00968952, Validation loss: 0.55869131, Gradient norm: 0.32973044
INFO:root:[  122] Training loss: 0.00972252, Validation loss: 0.55283471, Gradient norm: 0.32678292
INFO:root:[  123] Training loss: 0.00982129, Validation loss: 0.55626693, Gradient norm: 0.37847097
INFO:root:[  124] Training loss: 0.00961236, Validation loss: 0.54296709, Gradient norm: 0.32400732
INFO:root:[  125] Training loss: 0.00961390, Validation loss: 0.53301036, Gradient norm: 0.34961197
INFO:root:[  126] Training loss: 0.00953126, Validation loss: 0.54580693, Gradient norm: 0.34178864
INFO:root:[  127] Training loss: 0.00954843, Validation loss: 0.55476571, Gradient norm: 0.33120372
INFO:root:[  128] Training loss: 0.00974952, Validation loss: 0.53509594, Gradient norm: 0.37107627
INFO:root:[  129] Training loss: 0.00941855, Validation loss: 0.57082350, Gradient norm: 0.30889029
INFO:root:[  130] Training loss: 0.00957717, Validation loss: 0.55607659, Gradient norm: 0.34569435
INFO:root:[  131] Training loss: 0.00961599, Validation loss: 0.53655822, Gradient norm: 0.34333272
INFO:root:[  132] Training loss: 0.00961303, Validation loss: 0.54111823, Gradient norm: 0.34676648
INFO:root:[  133] Training loss: 0.00943107, Validation loss: 0.55557791, Gradient norm: 0.33519044
INFO:root:[  134] Training loss: 0.00935564, Validation loss: 0.56002865, Gradient norm: 0.31973654
INFO:root:[  135] Training loss: 0.00955043, Validation loss: 0.52398214, Gradient norm: 0.34943559
INFO:root:[  136] Training loss: 0.00922557, Validation loss: 0.53425080, Gradient norm: 0.32161493
INFO:root:[  137] Training loss: 0.00947044, Validation loss: 0.54600403, Gradient norm: 0.32065519
INFO:root:[  138] Training loss: 0.00948932, Validation loss: 0.53165072, Gradient norm: 0.34282752
INFO:root:[  139] Training loss: 0.00946776, Validation loss: 0.54448344, Gradient norm: 0.34686231
INFO:root:[  140] Training loss: 0.00932502, Validation loss: 0.55571780, Gradient norm: 0.29615851
INFO:root:[  141] Training loss: 0.00949418, Validation loss: 0.53375253, Gradient norm: 0.36891786
INFO:root:[  142] Training loss: 0.00924117, Validation loss: 0.51962973, Gradient norm: 0.33634818
INFO:root:[  143] Training loss: 0.00929340, Validation loss: 0.58320454, Gradient norm: 0.30353167
INFO:root:[  144] Training loss: 0.00933146, Validation loss: 0.66377159, Gradient norm: 0.33269861
INFO:root:[  145] Training loss: 0.00920689, Validation loss: 0.51982481, Gradient norm: 0.30865978
INFO:root:[  146] Training loss: 0.00923095, Validation loss: 0.53630140, Gradient norm: 0.35289346
INFO:root:[  147] Training loss: 0.00923607, Validation loss: 0.60447344, Gradient norm: 0.32033694
INFO:root:[  148] Training loss: 0.00924818, Validation loss: 0.52978969, Gradient norm: 0.33881736
INFO:root:[  149] Training loss: 0.00925507, Validation loss: 0.55129767, Gradient norm: 0.36313813
INFO:root:[  150] Training loss: 0.00912025, Validation loss: 0.52725345, Gradient norm: 0.32113948
INFO:root:[  151] Training loss: 0.00890208, Validation loss: 0.54350251, Gradient norm: 0.28267409
INFO:root:EP 151: Early stopping
INFO:root:Training the model took 3917.349s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08228
INFO:root:EnergyScoretrain: 0.0622
INFO:root:Coveragetrain: 6.87221
INFO:root:IntervalWidthtrain: 186.62508
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02331
INFO:root:EnergyScorevalidation: 0.01753
INFO:root:Coveragevalidation: 1.82999
INFO:root:IntervalWidthvalidation: 49.11344
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04522
INFO:root:EnergyScoretest: 0.03674
INFO:root:Coveragetest: 0.58949
INFO:root:IntervalWidthtest: 47.61651
INFO:root:###6 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.05, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04471447, Validation loss: 1.87649285, Gradient norm: 0.62129690
INFO:root:[    2] Training loss: 0.03055449, Validation loss: 1.70322013, Gradient norm: 0.49284730
INFO:root:[    3] Training loss: 0.02886262, Validation loss: 1.60108881, Gradient norm: 0.54672877
INFO:root:[    4] Training loss: 0.02645713, Validation loss: 1.53597172, Gradient norm: 0.40943675
INFO:root:[    5] Training loss: 0.02589080, Validation loss: 1.41860508, Gradient norm: 0.45852055
INFO:root:[    6] Training loss: 0.02409043, Validation loss: 1.35316567, Gradient norm: 0.36685989
INFO:root:[    7] Training loss: 0.02380339, Validation loss: 1.39384633, Gradient norm: 0.45236457
INFO:root:[    8] Training loss: 0.02256897, Validation loss: 1.31163526, Gradient norm: 0.39989052
INFO:root:[    9] Training loss: 0.02184354, Validation loss: 1.20073704, Gradient norm: 0.42402021
INFO:root:[   10] Training loss: 0.02110436, Validation loss: 1.15405475, Gradient norm: 0.43112623
INFO:root:[   11] Training loss: 0.02035895, Validation loss: 1.18605674, Gradient norm: 0.36645868
INFO:root:[   12] Training loss: 0.01985644, Validation loss: 1.16222779, Gradient norm: 0.39455036
INFO:root:[   13] Training loss: 0.01996043, Validation loss: 1.15651303, Gradient norm: 0.43249601
INFO:root:[   14] Training loss: 0.01885483, Validation loss: 1.04481410, Gradient norm: 0.36763628
INFO:root:[   15] Training loss: 0.01867609, Validation loss: 1.04098300, Gradient norm: 0.37954705
INFO:root:[   16] Training loss: 0.01850953, Validation loss: 1.03546810, Gradient norm: 0.39366381
INFO:root:[   17] Training loss: 0.01825691, Validation loss: 1.06253749, Gradient norm: 0.43787632
INFO:root:[   18] Training loss: 0.01792849, Validation loss: 1.01486301, Gradient norm: 0.41985874
INFO:root:[   19] Training loss: 0.01722668, Validation loss: 1.07985966, Gradient norm: 0.36142866
INFO:root:[   20] Training loss: 0.01719072, Validation loss: 1.00180032, Gradient norm: 0.38198069
INFO:root:[   21] Training loss: 0.01716798, Validation loss: 0.96810238, Gradient norm: 0.40755142
INFO:root:[   22] Training loss: 0.01661453, Validation loss: 0.94915572, Gradient norm: 0.36373075
INFO:root:[   23] Training loss: 0.01660712, Validation loss: 0.94307983, Gradient norm: 0.39482292
INFO:root:[   24] Training loss: 0.01618585, Validation loss: 0.91764789, Gradient norm: 0.35610124
INFO:root:[   25] Training loss: 0.01611698, Validation loss: 0.96016851, Gradient norm: 0.38308069
INFO:root:[   26] Training loss: 0.01568168, Validation loss: 0.89093371, Gradient norm: 0.35443273
INFO:root:[   27] Training loss: 0.01597392, Validation loss: 0.87709195, Gradient norm: 0.41125488
INFO:root:[   28] Training loss: 0.01538145, Validation loss: 0.87433538, Gradient norm: 0.36457674
INFO:root:[   29] Training loss: 0.01539075, Validation loss: 0.88535045, Gradient norm: 0.38898213
INFO:root:[   30] Training loss: 0.01509379, Validation loss: 0.88124247, Gradient norm: 0.35654213
INFO:root:[   31] Training loss: 0.01492814, Validation loss: 0.82133988, Gradient norm: 0.36810240
INFO:root:[   32] Training loss: 0.01460603, Validation loss: 0.84678006, Gradient norm: 0.33880036
INFO:root:[   33] Training loss: 0.01465851, Validation loss: 0.80212349, Gradient norm: 0.37704320
INFO:root:[   34] Training loss: 0.01455693, Validation loss: 0.79395082, Gradient norm: 0.34027639
INFO:root:[   35] Training loss: 0.01447555, Validation loss: 0.80931379, Gradient norm: 0.39316010
INFO:root:[   36] Training loss: 0.01417026, Validation loss: 0.79776850, Gradient norm: 0.36728343
INFO:root:[   37] Training loss: 0.01394015, Validation loss: 0.79180554, Gradient norm: 0.34983671
INFO:root:[   38] Training loss: 0.01397951, Validation loss: 0.78788158, Gradient norm: 0.34865722
INFO:root:[   39] Training loss: 0.01382066, Validation loss: 0.82831767, Gradient norm: 0.40559764
INFO:root:[   40] Training loss: 0.01353661, Validation loss: 0.78070412, Gradient norm: 0.32631203
INFO:root:[   41] Training loss: 0.01349520, Validation loss: 0.76737471, Gradient norm: 0.33221679
INFO:root:[   42] Training loss: 0.01339299, Validation loss: 0.77866265, Gradient norm: 0.36566273
INFO:root:[   43] Training loss: 0.01348176, Validation loss: 0.86357901, Gradient norm: 0.36849759
INFO:root:[   44] Training loss: 0.01323064, Validation loss: 0.75098656, Gradient norm: 0.36116943
INFO:root:[   45] Training loss: 0.01296781, Validation loss: 0.73733264, Gradient norm: 0.30329038
INFO:root:[   46] Training loss: 0.01313890, Validation loss: 0.80352408, Gradient norm: 0.37143477
INFO:root:[   47] Training loss: 0.01298506, Validation loss: 0.74350658, Gradient norm: 0.38041259
INFO:root:[   48] Training loss: 0.01287765, Validation loss: 0.71374370, Gradient norm: 0.33488993
INFO:root:[   49] Training loss: 0.01268313, Validation loss: 0.73535184, Gradient norm: 0.31898247
INFO:root:[   50] Training loss: 0.01273060, Validation loss: 0.71478429, Gradient norm: 0.34382051
INFO:root:[   51] Training loss: 0.01264854, Validation loss: 0.72442496, Gradient norm: 0.35990640
INFO:root:[   52] Training loss: 0.01272992, Validation loss: 0.69165753, Gradient norm: 0.36311625
INFO:root:[   53] Training loss: 0.01240808, Validation loss: 0.77994349, Gradient norm: 0.35870515
INFO:root:[   54] Training loss: 0.01242139, Validation loss: 0.77384412, Gradient norm: 0.36276382
INFO:root:[   55] Training loss: 0.01207468, Validation loss: 0.69201985, Gradient norm: 0.33599637
INFO:root:[   56] Training loss: 0.01216254, Validation loss: 0.67665933, Gradient norm: 0.34004029
INFO:root:[   57] Training loss: 0.01184238, Validation loss: 0.66115202, Gradient norm: 0.30235485
INFO:root:[   58] Training loss: 0.01166877, Validation loss: 0.66855042, Gradient norm: 0.30651148
INFO:root:[   59] Training loss: 0.01172225, Validation loss: 0.68496580, Gradient norm: 0.30730483
INFO:root:[   60] Training loss: 0.01206310, Validation loss: 0.68142562, Gradient norm: 0.38980410
INFO:root:[   61] Training loss: 0.01195328, Validation loss: 0.67033729, Gradient norm: 0.35875119
INFO:root:[   62] Training loss: 0.01161017, Validation loss: 0.66196511, Gradient norm: 0.33003149
INFO:root:[   63] Training loss: 0.01137823, Validation loss: 0.62454961, Gradient norm: 0.30471701
INFO:root:[   64] Training loss: 0.01143058, Validation loss: 0.64606947, Gradient norm: 0.33072155
INFO:root:[   65] Training loss: 0.01143538, Validation loss: 0.63913729, Gradient norm: 0.35579507
INFO:root:[   66] Training loss: 0.01126948, Validation loss: 0.62906187, Gradient norm: 0.32785890
INFO:root:[   67] Training loss: 0.01130670, Validation loss: 0.64118301, Gradient norm: 0.36155220
INFO:root:[   68] Training loss: 0.01102921, Validation loss: 0.67842186, Gradient norm: 0.30136837
INFO:root:[   69] Training loss: 0.01106731, Validation loss: 0.62666849, Gradient norm: 0.35793129
INFO:root:[   70] Training loss: 0.01132501, Validation loss: 0.61900417, Gradient norm: 0.37880838
INFO:root:[   71] Training loss: 0.01110532, Validation loss: 0.61148816, Gradient norm: 0.37166535
INFO:root:[   72] Training loss: 0.01103049, Validation loss: 0.63617489, Gradient norm: 0.34907733
INFO:root:[   73] Training loss: 0.01088790, Validation loss: 0.65466970, Gradient norm: 0.33946428
INFO:root:[   74] Training loss: 0.01068938, Validation loss: 0.69512039, Gradient norm: 0.29738441
INFO:root:[   75] Training loss: 0.01090979, Validation loss: 0.63133285, Gradient norm: 0.34215090
INFO:root:[   76] Training loss: 0.01075149, Validation loss: 0.60204765, Gradient norm: 0.30565277
INFO:root:[   77] Training loss: 0.01077122, Validation loss: 0.62273742, Gradient norm: 0.35772037
INFO:root:[   78] Training loss: 0.01075046, Validation loss: 0.65068126, Gradient norm: 0.33135401
INFO:root:[   79] Training loss: 0.01080263, Validation loss: 0.59500688, Gradient norm: 0.37367598
INFO:root:[   80] Training loss: 0.01079371, Validation loss: 0.62545311, Gradient norm: 0.37154940
INFO:root:[   81] Training loss: 0.01043522, Validation loss: 0.59789689, Gradient norm: 0.30935295
INFO:root:[   82] Training loss: 0.01055195, Validation loss: 0.63043087, Gradient norm: 0.32367576
INFO:root:[   83] Training loss: 0.01070195, Validation loss: 0.61928864, Gradient norm: 0.36497227
INFO:root:[   84] Training loss: 0.01050969, Validation loss: 0.70662444, Gradient norm: 0.35806201
INFO:root:[   85] Training loss: 0.01040182, Validation loss: 0.64380955, Gradient norm: 0.31790143
INFO:root:[   86] Training loss: 0.01043720, Validation loss: 0.56753147, Gradient norm: 0.34576026
INFO:root:[   87] Training loss: 0.01014735, Validation loss: 0.57056211, Gradient norm: 0.30093076
INFO:root:[   88] Training loss: 0.01016389, Validation loss: 0.58637111, Gradient norm: 0.32232702
INFO:root:[   89] Training loss: 0.01023100, Validation loss: 0.60574204, Gradient norm: 0.35003413
INFO:root:[   90] Training loss: 0.01011276, Validation loss: 0.58492121, Gradient norm: 0.33877586
INFO:root:[   91] Training loss: 0.01019727, Validation loss: 0.58156858, Gradient norm: 0.31665957
INFO:root:[   92] Training loss: 0.00998725, Validation loss: 0.59019813, Gradient norm: 0.30224717
INFO:root:[   93] Training loss: 0.01008840, Validation loss: 0.57149829, Gradient norm: 0.34777040
INFO:root:[   94] Training loss: 0.01033952, Validation loss: 0.56171380, Gradient norm: 0.35941461
INFO:root:[   95] Training loss: 0.00989410, Validation loss: 0.55972931, Gradient norm: 0.32012690
INFO:root:[   96] Training loss: 0.00988527, Validation loss: 0.54620186, Gradient norm: 0.29443593
INFO:root:[   97] Training loss: 0.01025700, Validation loss: 0.68613510, Gradient norm: 0.37984979
INFO:root:[   98] Training loss: 0.01005075, Validation loss: 0.55658796, Gradient norm: 0.35372407
INFO:root:[   99] Training loss: 0.00985913, Validation loss: 0.60055726, Gradient norm: 0.32776441
INFO:root:[  100] Training loss: 0.01000832, Validation loss: 0.62119580, Gradient norm: 0.33004428
INFO:root:[  101] Training loss: 0.00992242, Validation loss: 0.56562847, Gradient norm: 0.28983921
INFO:root:[  102] Training loss: 0.00981882, Validation loss: 0.54781153, Gradient norm: 0.31490868
INFO:root:[  103] Training loss: 0.00975915, Validation loss: 0.55707300, Gradient norm: 0.30981426
INFO:root:[  104] Training loss: 0.00989911, Validation loss: 0.56863037, Gradient norm: 0.35365537
INFO:root:[  105] Training loss: 0.00965832, Validation loss: 0.60936906, Gradient norm: 0.32006693
INFO:root:EP 105: Early stopping
INFO:root:Training the model took 2736.674s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08805
INFO:root:EnergyScoretrain: 0.06676
INFO:root:Coveragetrain: 6.94885
INFO:root:IntervalWidthtrain: 233.79455
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02441
INFO:root:EnergyScorevalidation: 0.01834
INFO:root:Coveragevalidation: 1.85218
INFO:root:IntervalWidthvalidation: 61.3167
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04554
INFO:root:EnergyScoretest: 0.03597
INFO:root:Coveragetest: 0.64286
INFO:root:IntervalWidthtest: 60.29689
INFO:root:###7 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.1, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04971032, Validation loss: 2.01899111, Gradient norm: 0.49118860
INFO:root:[    2] Training loss: 0.03244150, Validation loss: 1.78155023, Gradient norm: 0.38602170
INFO:root:[    3] Training loss: 0.02908876, Validation loss: 1.58728296, Gradient norm: 0.36907178
INFO:root:[    4] Training loss: 0.02684766, Validation loss: 1.44864313, Gradient norm: 0.37485722
INFO:root:[    5] Training loss: 0.02525741, Validation loss: 1.34452839, Gradient norm: 0.32724965
INFO:root:[    6] Training loss: 0.02408588, Validation loss: 1.30784542, Gradient norm: 0.35170414
INFO:root:[    7] Training loss: 0.02297419, Validation loss: 1.24949029, Gradient norm: 0.30712091
INFO:root:[    8] Training loss: 0.02215215, Validation loss: 1.24319309, Gradient norm: 0.34765749
INFO:root:[    9] Training loss: 0.02125394, Validation loss: 1.17865922, Gradient norm: 0.31634108
INFO:root:[   10] Training loss: 0.02077212, Validation loss: 1.17148557, Gradient norm: 0.31168569
INFO:root:[   11] Training loss: 0.02001248, Validation loss: 1.21266174, Gradient norm: 0.28688609
INFO:root:[   12] Training loss: 0.02024561, Validation loss: 1.14229066, Gradient norm: 0.35526420
INFO:root:[   13] Training loss: 0.01911248, Validation loss: 1.10188262, Gradient norm: 0.29288385
INFO:root:[   14] Training loss: 0.01892878, Validation loss: 1.08452788, Gradient norm: 0.29016525
INFO:root:[   15] Training loss: 0.01860824, Validation loss: 1.06781426, Gradient norm: 0.29097375
INFO:root:[   16] Training loss: 0.01830008, Validation loss: 1.10621225, Gradient norm: 0.28299078
INFO:root:[   17] Training loss: 0.01794354, Validation loss: 1.07335401, Gradient norm: 0.29604808
INFO:root:[   18] Training loss: 0.01774881, Validation loss: 1.00035311, Gradient norm: 0.29137126
INFO:root:[   19] Training loss: 0.01751330, Validation loss: 1.18810133, Gradient norm: 0.29044563
INFO:root:[   20] Training loss: 0.01726594, Validation loss: 0.98401755, Gradient norm: 0.28114325
INFO:root:[   21] Training loss: 0.01698136, Validation loss: 0.98872836, Gradient norm: 0.28439326
INFO:root:[   22] Training loss: 0.01669392, Validation loss: 0.95016921, Gradient norm: 0.24732592
INFO:root:[   23] Training loss: 0.01687184, Validation loss: 1.00890331, Gradient norm: 0.31874273
INFO:root:[   24] Training loss: 0.01643215, Validation loss: 1.03016728, Gradient norm: 0.27981938
INFO:root:[   25] Training loss: 0.01650501, Validation loss: 0.92389588, Gradient norm: 0.28386479
INFO:root:[   26] Training loss: 0.01622995, Validation loss: 0.93563775, Gradient norm: 0.28953987
INFO:root:[   27] Training loss: 0.01607855, Validation loss: 0.94344473, Gradient norm: 0.27557550
INFO:root:[   28] Training loss: 0.01586722, Validation loss: 0.95168024, Gradient norm: 0.24285339
INFO:root:[   29] Training loss: 0.01582639, Validation loss: 0.91051550, Gradient norm: 0.26878760
INFO:root:[   30] Training loss: 0.01547887, Validation loss: 0.90854987, Gradient norm: 0.28031719
INFO:root:[   31] Training loss: 0.01540584, Validation loss: 0.91269636, Gradient norm: 0.27787879
INFO:root:[   32] Training loss: 0.01532259, Validation loss: 0.99548505, Gradient norm: 0.26999246
INFO:root:[   33] Training loss: 0.01541088, Validation loss: 0.90120667, Gradient norm: 0.28407627
INFO:root:[   34] Training loss: 0.01529636, Validation loss: 0.86445464, Gradient norm: 0.29738407
INFO:root:[   35] Training loss: 0.01479590, Validation loss: 0.84976371, Gradient norm: 0.23980253
INFO:root:[   36] Training loss: 0.01487052, Validation loss: 0.83632794, Gradient norm: 0.25855643
INFO:root:[   37] Training loss: 0.01478223, Validation loss: 0.84914514, Gradient norm: 0.28441503
INFO:root:[   38] Training loss: 0.01466061, Validation loss: 0.83478501, Gradient norm: 0.26983098
INFO:root:[   39] Training loss: 0.01451818, Validation loss: 0.82710389, Gradient norm: 0.27183327
INFO:root:[   40] Training loss: 0.01407265, Validation loss: 0.80665299, Gradient norm: 0.24618634
INFO:root:[   41] Training loss: 0.01434275, Validation loss: 0.84204157, Gradient norm: 0.26581981
INFO:root:[   42] Training loss: 0.01403675, Validation loss: 0.82172995, Gradient norm: 0.26460102
INFO:root:[   43] Training loss: 0.01408450, Validation loss: 0.79163216, Gradient norm: 0.27036024
INFO:root:[   44] Training loss: 0.01385738, Validation loss: 0.79448783, Gradient norm: 0.26604693
INFO:root:[   45] Training loss: 0.01383372, Validation loss: 0.77595468, Gradient norm: 0.25889627
INFO:root:[   46] Training loss: 0.01358425, Validation loss: 0.75189601, Gradient norm: 0.26233470
INFO:root:[   47] Training loss: 0.01337752, Validation loss: 0.80202798, Gradient norm: 0.25680180
INFO:root:[   48] Training loss: 0.01330637, Validation loss: 0.75926925, Gradient norm: 0.25635111
INFO:root:[   49] Training loss: 0.01320885, Validation loss: 0.76931059, Gradient norm: 0.25270233
INFO:root:[   50] Training loss: 0.01308885, Validation loss: 0.74555180, Gradient norm: 0.26378967
INFO:root:[   51] Training loss: 0.01323749, Validation loss: 0.77154673, Gradient norm: 0.28368444
INFO:root:[   52] Training loss: 0.01316471, Validation loss: 0.73970673, Gradient norm: 0.27333233
INFO:root:[   53] Training loss: 0.01295199, Validation loss: 0.79268477, Gradient norm: 0.28804067
INFO:root:[   54] Training loss: 0.01278105, Validation loss: 0.74679664, Gradient norm: 0.27324886
INFO:root:[   55] Training loss: 0.01274180, Validation loss: 0.71495471, Gradient norm: 0.27551379
INFO:root:[   56] Training loss: 0.01291031, Validation loss: 0.77024001, Gradient norm: 0.31743571
INFO:root:[   57] Training loss: 0.01238164, Validation loss: 0.77225088, Gradient norm: 0.26907024
INFO:root:[   58] Training loss: 0.01233130, Validation loss: 0.70353773, Gradient norm: 0.25741721
INFO:root:[   59] Training loss: 0.01237011, Validation loss: 0.72914058, Gradient norm: 0.27100623
INFO:root:[   60] Training loss: 0.01235929, Validation loss: 0.69576134, Gradient norm: 0.26991845
INFO:root:[   61] Training loss: 0.01204687, Validation loss: 0.78858363, Gradient norm: 0.25552845
INFO:root:[   62] Training loss: 0.01194693, Validation loss: 0.80219841, Gradient norm: 0.23250324
INFO:root:[   63] Training loss: 0.01210125, Validation loss: 0.69551519, Gradient norm: 0.23312740
INFO:root:[   64] Training loss: 0.01190358, Validation loss: 0.68813427, Gradient norm: 0.25130355
INFO:root:[   65] Training loss: 0.01219309, Validation loss: 0.74455510, Gradient norm: 0.30157751
INFO:root:[   66] Training loss: 0.01176886, Validation loss: 0.68499022, Gradient norm: 0.25350062
INFO:root:[   67] Training loss: 0.01185379, Validation loss: 0.68853115, Gradient norm: 0.27255847
INFO:root:[   68] Training loss: 0.01192868, Validation loss: 0.73677755, Gradient norm: 0.29306766
INFO:root:[   69] Training loss: 0.01152624, Validation loss: 0.69055978, Gradient norm: 0.23845864
INFO:root:[   70] Training loss: 0.01163255, Validation loss: 0.66014134, Gradient norm: 0.26229354
INFO:root:[   71] Training loss: 0.01164101, Validation loss: 0.65420506, Gradient norm: 0.27868964
INFO:root:[   72] Training loss: 0.01154559, Validation loss: 0.71065095, Gradient norm: 0.25268224
INFO:root:[   73] Training loss: 0.01140136, Validation loss: 0.65714314, Gradient norm: 0.25581233
INFO:root:[   74] Training loss: 0.01139940, Validation loss: 0.70586097, Gradient norm: 0.28823646
INFO:root:[   75] Training loss: 0.01124115, Validation loss: 0.64882437, Gradient norm: 0.23671688
INFO:root:[   76] Training loss: 0.01141683, Validation loss: 0.66273480, Gradient norm: 0.27400711
INFO:root:[   77] Training loss: 0.01127544, Validation loss: 0.66157841, Gradient norm: 0.27267906
INFO:root:[   78] Training loss: 0.01109105, Validation loss: 0.66139001, Gradient norm: 0.25436750
INFO:root:[   79] Training loss: 0.01125753, Validation loss: 0.63538770, Gradient norm: 0.28862080
INFO:root:[   80] Training loss: 0.01115372, Validation loss: 0.69001423, Gradient norm: 0.26823890
INFO:root:[   81] Training loss: 0.01107439, Validation loss: 0.61354714, Gradient norm: 0.25119497
INFO:root:[   82] Training loss: 0.01089245, Validation loss: 0.65626871, Gradient norm: 0.23549258
INFO:root:[   83] Training loss: 0.01105799, Validation loss: 0.62411745, Gradient norm: 0.26850842
INFO:root:[   84] Training loss: 0.01090981, Validation loss: 0.61758666, Gradient norm: 0.25747221
INFO:root:[   85] Training loss: 0.01102273, Validation loss: 0.70144514, Gradient norm: 0.27751395
INFO:root:[   86] Training loss: 0.01080573, Validation loss: 0.62998663, Gradient norm: 0.25125374
INFO:root:[   87] Training loss: 0.01079945, Validation loss: 0.66778150, Gradient norm: 0.27575300
INFO:root:[   88] Training loss: 0.01080150, Validation loss: 0.67909379, Gradient norm: 0.26264269
INFO:root:[   89] Training loss: 0.01065847, Validation loss: 0.64429225, Gradient norm: 0.24610024
INFO:root:[   90] Training loss: 0.01060891, Validation loss: 0.61719942, Gradient norm: 0.24594809
INFO:root:[   91] Training loss: 0.01058374, Validation loss: 0.59233388, Gradient norm: 0.25231063
INFO:root:[   92] Training loss: 0.01061470, Validation loss: 0.61361378, Gradient norm: 0.26616397
INFO:root:[   93] Training loss: 0.01056860, Validation loss: 0.63312413, Gradient norm: 0.24388884
INFO:root:[   94] Training loss: 0.01055430, Validation loss: 0.60408356, Gradient norm: 0.26090682
INFO:root:[   95] Training loss: 0.01053044, Validation loss: 0.64248856, Gradient norm: 0.27068533
INFO:root:[   96] Training loss: 0.01035788, Validation loss: 0.59392128, Gradient norm: 0.25919810
INFO:root:[   97] Training loss: 0.01040454, Validation loss: 0.60803339, Gradient norm: 0.25146172
INFO:root:[   98] Training loss: 0.01074951, Validation loss: 0.62139827, Gradient norm: 0.30407895
INFO:root:[   99] Training loss: 0.01031110, Validation loss: 0.57863679, Gradient norm: 0.25994690
INFO:root:[  100] Training loss: 0.01038175, Validation loss: 0.61705648, Gradient norm: 0.27187701
INFO:root:[  101] Training loss: 0.01022572, Validation loss: 0.61871867, Gradient norm: 0.25699572
INFO:root:[  102] Training loss: 0.01033217, Validation loss: 0.59389317, Gradient norm: 0.26128283
INFO:root:[  103] Training loss: 0.01040389, Validation loss: 0.56407580, Gradient norm: 0.28075799
INFO:root:[  104] Training loss: 0.01011824, Validation loss: 0.58839522, Gradient norm: 0.24706738
INFO:root:[  105] Training loss: 0.01017827, Validation loss: 0.59594900, Gradient norm: 0.25131896
INFO:root:[  106] Training loss: 0.01006574, Validation loss: 0.63518594, Gradient norm: 0.25221984
INFO:root:[  107] Training loss: 0.01025212, Validation loss: 0.60424786, Gradient norm: 0.28351572
INFO:root:[  108] Training loss: 0.00999102, Validation loss: 0.59724492, Gradient norm: 0.25092620
INFO:root:[  109] Training loss: 0.01004176, Validation loss: 0.60528971, Gradient norm: 0.26404289
INFO:root:[  110] Training loss: 0.01008535, Validation loss: 0.58366263, Gradient norm: 0.26161708
INFO:root:[  111] Training loss: 0.01026997, Validation loss: 0.60101125, Gradient norm: 0.28905073
INFO:root:[  112] Training loss: 0.01002074, Validation loss: 0.61149146, Gradient norm: 0.26766003
INFO:root:EP 112: Early stopping
INFO:root:Training the model took 2907.328s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.09013
INFO:root:EnergyScoretrain: 0.06939
INFO:root:Coveragetrain: 6.96741
INFO:root:IntervalWidthtrain: 247.93371
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02412
INFO:root:EnergyScorevalidation: 0.01847
INFO:root:Coveragevalidation: 1.85723
INFO:root:IntervalWidthvalidation: 64.00265
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04787
INFO:root:EnergyScoretest: 0.03744
INFO:root:Coveragetest: 0.63554
INFO:root:IntervalWidthtest: 65.67901
INFO:root:###8 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.2, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05449712, Validation loss: 2.42729888, Gradient norm: 0.30733949
INFO:root:[    2] Training loss: 0.03342316, Validation loss: 1.74261585, Gradient norm: 0.27825493
INFO:root:[    3] Training loss: 0.02963604, Validation loss: 1.63210766, Gradient norm: 0.27661561
INFO:root:[    4] Training loss: 0.02750475, Validation loss: 1.55089026, Gradient norm: 0.27074498
INFO:root:[    5] Training loss: 0.02575136, Validation loss: 1.46366490, Gradient norm: 0.24254792
INFO:root:[    6] Training loss: 0.02454591, Validation loss: 1.36693047, Gradient norm: 0.25320964
INFO:root:[    7] Training loss: 0.02360628, Validation loss: 1.30476255, Gradient norm: 0.24902502
INFO:root:[    8] Training loss: 0.02240408, Validation loss: 1.25044865, Gradient norm: 0.26634423
INFO:root:[    9] Training loss: 0.02194762, Validation loss: 1.23815575, Gradient norm: 0.29401721
INFO:root:[   10] Training loss: 0.02091095, Validation loss: 1.18446192, Gradient norm: 0.25656601
INFO:root:[   11] Training loss: 0.02051536, Validation loss: 1.16910181, Gradient norm: 0.28425147
INFO:root:[   12] Training loss: 0.02002225, Validation loss: 1.15480181, Gradient norm: 0.27439025
INFO:root:[   13] Training loss: 0.01958271, Validation loss: 1.09174584, Gradient norm: 0.26119719
INFO:root:[   14] Training loss: 0.01914624, Validation loss: 1.09767189, Gradient norm: 0.26644577
INFO:root:[   15] Training loss: 0.01918824, Validation loss: 1.07375809, Gradient norm: 0.28492959
INFO:root:[   16] Training loss: 0.01854231, Validation loss: 1.07085534, Gradient norm: 0.27769071
INFO:root:[   17] Training loss: 0.01860789, Validation loss: 1.05990930, Gradient norm: 0.31515313
INFO:root:[   18] Training loss: 0.01815521, Validation loss: 1.06156224, Gradient norm: 0.28313837
INFO:root:[   19] Training loss: 0.01819551, Validation loss: 1.06070756, Gradient norm: 0.33143904
INFO:root:[   20] Training loss: 0.01762752, Validation loss: 1.00406957, Gradient norm: 0.28540697
INFO:root:[   21] Training loss: 0.01745010, Validation loss: 1.00148779, Gradient norm: 0.26875765
INFO:root:[   22] Training loss: 0.01742387, Validation loss: 0.99130829, Gradient norm: 0.29461624
INFO:root:[   23] Training loss: 0.01694840, Validation loss: 0.95323398, Gradient norm: 0.28069022
INFO:root:[   24] Training loss: 0.01702708, Validation loss: 0.94293463, Gradient norm: 0.31075305
INFO:root:[   25] Training loss: 0.01672924, Validation loss: 0.97343190, Gradient norm: 0.27850336
INFO:root:[   26] Training loss: 0.01639155, Validation loss: 0.94111662, Gradient norm: 0.27411368
INFO:root:[   27] Training loss: 0.01633821, Validation loss: 0.93444258, Gradient norm: 0.30651472
INFO:root:[   28] Training loss: 0.01607345, Validation loss: 0.94269525, Gradient norm: 0.28927948
INFO:root:[   29] Training loss: 0.01610620, Validation loss: 0.92879433, Gradient norm: 0.28906078
INFO:root:[   30] Training loss: 0.01605088, Validation loss: 0.90315742, Gradient norm: 0.30318371
INFO:root:[   31] Training loss: 0.01573798, Validation loss: 0.90732535, Gradient norm: 0.28579057
INFO:root:[   32] Training loss: 0.01541689, Validation loss: 0.89015484, Gradient norm: 0.26580140
INFO:root:[   33] Training loss: 0.01543678, Validation loss: 0.87978114, Gradient norm: 0.28557239
INFO:root:[   34] Training loss: 0.01529092, Validation loss: 0.87087726, Gradient norm: 0.25197580
INFO:root:[   35] Training loss: 0.01519000, Validation loss: 0.89177989, Gradient norm: 0.29129632
INFO:root:[   36] Training loss: 0.01508975, Validation loss: 0.90371161, Gradient norm: 0.27119935
INFO:root:[   37] Training loss: 0.01487716, Validation loss: 0.84266707, Gradient norm: 0.28735429
INFO:root:[   38] Training loss: 0.01460602, Validation loss: 0.87512524, Gradient norm: 0.25010058
INFO:root:[   39] Training loss: 0.01445370, Validation loss: 0.83422658, Gradient norm: 0.25495011
INFO:root:[   40] Training loss: 0.01463276, Validation loss: 0.89593828, Gradient norm: 0.28436906
INFO:root:[   41] Training loss: 0.01444996, Validation loss: 0.82014743, Gradient norm: 0.27775294
INFO:root:[   42] Training loss: 0.01405613, Validation loss: 0.81946187, Gradient norm: 0.26720438
INFO:root:[   43] Training loss: 0.01404309, Validation loss: 0.80381085, Gradient norm: 0.26290628
INFO:root:[   44] Training loss: 0.01405558, Validation loss: 0.91581057, Gradient norm: 0.28158422
INFO:root:[   45] Training loss: 0.01375280, Validation loss: 0.81912903, Gradient norm: 0.24952846
INFO:root:[   46] Training loss: 0.01376573, Validation loss: 0.81164847, Gradient norm: 0.26932291
INFO:root:[   47] Training loss: 0.01354734, Validation loss: 0.77384400, Gradient norm: 0.25954300
INFO:root:[   48] Training loss: 0.01349854, Validation loss: 0.76575269, Gradient norm: 0.27880013
INFO:root:[   49] Training loss: 0.01318598, Validation loss: 0.79155665, Gradient norm: 0.24732016
INFO:root:[   50] Training loss: 0.01334980, Validation loss: 0.74717007, Gradient norm: 0.29268457
INFO:root:[   51] Training loss: 0.01311915, Validation loss: 0.75137649, Gradient norm: 0.28627489
INFO:root:[   52] Training loss: 0.01320094, Validation loss: 0.77875967, Gradient norm: 0.27349371
INFO:root:[   53] Training loss: 0.01281318, Validation loss: 0.72378547, Gradient norm: 0.25681649
INFO:root:[   54] Training loss: 0.01299328, Validation loss: 0.72524758, Gradient norm: 0.30390434
INFO:root:[   55] Training loss: 0.01273048, Validation loss: 0.72187853, Gradient norm: 0.26294771
INFO:root:[   56] Training loss: 0.01263031, Validation loss: 0.75995837, Gradient norm: 0.28035078
INFO:root:[   57] Training loss: 0.01248739, Validation loss: 0.76919677, Gradient norm: 0.24194415
INFO:root:[   58] Training loss: 0.01264652, Validation loss: 0.73951983, Gradient norm: 0.32126422
INFO:root:[   59] Training loss: 0.01235769, Validation loss: 0.73658007, Gradient norm: 0.28815693
INFO:root:[   60] Training loss: 0.01224108, Validation loss: 0.68763114, Gradient norm: 0.23985130
INFO:root:[   61] Training loss: 0.01219452, Validation loss: 0.71171697, Gradient norm: 0.28184399
INFO:root:[   62] Training loss: 0.01206935, Validation loss: 0.69511445, Gradient norm: 0.26804675
INFO:root:[   63] Training loss: 0.01207254, Validation loss: 0.71474873, Gradient norm: 0.24951926
INFO:root:[   64] Training loss: 0.01194632, Validation loss: 0.69543489, Gradient norm: 0.24516981
INFO:root:[   65] Training loss: 0.01203402, Validation loss: 0.68779880, Gradient norm: 0.25482154
INFO:root:[   66] Training loss: 0.01186059, Validation loss: 0.70821149, Gradient norm: 0.26956137
INFO:root:[   67] Training loss: 0.01185733, Validation loss: 0.71421040, Gradient norm: 0.26650597
INFO:root:[   68] Training loss: 0.01183400, Validation loss: 0.67557112, Gradient norm: 0.26774939
INFO:root:[   69] Training loss: 0.01184878, Validation loss: 0.70652522, Gradient norm: 0.27220440
INFO:root:[   70] Training loss: 0.01193509, Validation loss: 0.77049266, Gradient norm: 0.30184326
INFO:root:[   71] Training loss: 0.01171450, Validation loss: 0.73271288, Gradient norm: 0.28875670
INFO:root:[   72] Training loss: 0.01138346, Validation loss: 0.72556723, Gradient norm: 0.23702855
INFO:root:[   73] Training loss: 0.01151189, Validation loss: 0.66675580, Gradient norm: 0.25116018
INFO:root:[   74] Training loss: 0.01142969, Validation loss: 0.70387389, Gradient norm: 0.25521072
INFO:root:[   75] Training loss: 0.01151390, Validation loss: 0.71391278, Gradient norm: 0.25006698
INFO:root:[   76] Training loss: 0.01133319, Validation loss: 0.67939473, Gradient norm: 0.27415143
INFO:root:[   77] Training loss: 0.01152877, Validation loss: 0.62840349, Gradient norm: 0.27757222
INFO:root:[   78] Training loss: 0.01131342, Validation loss: 0.68711791, Gradient norm: 0.26407004
INFO:root:[   79] Training loss: 0.01137888, Validation loss: 0.68641697, Gradient norm: 0.26150985
INFO:root:[   80] Training loss: 0.01144089, Validation loss: 0.64076696, Gradient norm: 0.26289157
INFO:root:[   81] Training loss: 0.01112907, Validation loss: 0.64783317, Gradient norm: 0.24673754
INFO:root:[   82] Training loss: 0.01131271, Validation loss: 0.66660858, Gradient norm: 0.26268660
INFO:root:[   83] Training loss: 0.01135120, Validation loss: 0.64804291, Gradient norm: 0.28496898
INFO:root:[   84] Training loss: 0.01115452, Validation loss: 0.67299187, Gradient norm: 0.26159671
INFO:root:[   85] Training loss: 0.01116381, Validation loss: 0.66368299, Gradient norm: 0.27091816
INFO:root:[   86] Training loss: 0.01108475, Validation loss: 0.65361263, Gradient norm: 0.26147299
INFO:root:EP 86: Early stopping
INFO:root:Training the model took 2230.74s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.10226
INFO:root:EnergyScoretrain: 0.07779
INFO:root:Coveragetrain: 6.97668
INFO:root:IntervalWidthtrain: 292.2096
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02876
INFO:root:EnergyScorevalidation: 0.02159
INFO:root:Coveragevalidation: 1.85912
INFO:root:IntervalWidthvalidation: 76.15458
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.03522
INFO:root:EnergyScoretest: 0.02652
INFO:root:Coveragetest: 0.8531
INFO:root:IntervalWidthtest: 69.18251
INFO:root:###9 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.01, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04686508, Validation loss: 1.76516496, Gradient norm: 0.49694312
INFO:root:[    2] Training loss: 0.02961847, Validation loss: 1.62842654, Gradient norm: 0.62691310
INFO:root:[    3] Training loss: 0.02614230, Validation loss: 1.36916377, Gradient norm: 0.53878962
INFO:root:[    4] Training loss: 0.02382621, Validation loss: 1.40440338, Gradient norm: 0.46500362
INFO:root:[    5] Training loss: 0.02250720, Validation loss: 1.23023790, Gradient norm: 0.46440142
INFO:root:[    6] Training loss: 0.02183019, Validation loss: 1.37253097, Gradient norm: 0.51720114
INFO:root:[    7] Training loss: 0.02159627, Validation loss: 1.22162412, Gradient norm: 0.54063083
INFO:root:[    8] Training loss: 0.02022713, Validation loss: 1.14482677, Gradient norm: 0.38395706
INFO:root:[    9] Training loss: 0.01986213, Validation loss: 1.09450394, Gradient norm: 0.35356913
INFO:root:[   10] Training loss: 0.01955406, Validation loss: 1.10735863, Gradient norm: 0.34819088
INFO:root:[   11] Training loss: 0.01938056, Validation loss: 1.10194129, Gradient norm: 0.38839411
INFO:root:[   12] Training loss: 0.01898987, Validation loss: 1.14071673, Gradient norm: 0.36942193
INFO:root:[   13] Training loss: 0.01870655, Validation loss: 1.07059250, Gradient norm: 0.37956148
INFO:root:[   14] Training loss: 0.01861835, Validation loss: 1.03581265, Gradient norm: 0.39663308
INFO:root:[   15] Training loss: 0.01804538, Validation loss: 1.11696750, Gradient norm: 0.34931315
INFO:root:[   16] Training loss: 0.01803412, Validation loss: 1.01309572, Gradient norm: 0.37301330
INFO:root:[   17] Training loss: 0.01774867, Validation loss: 1.01323186, Gradient norm: 0.37290509
INFO:root:[   18] Training loss: 0.01712636, Validation loss: 0.99198630, Gradient norm: 0.27788587
INFO:root:[   19] Training loss: 0.01710278, Validation loss: 1.00972327, Gradient norm: 0.32469922
INFO:root:[   20] Training loss: 0.01695121, Validation loss: 1.01987774, Gradient norm: 0.34799762
INFO:root:[   21] Training loss: 0.01703982, Validation loss: 0.96054463, Gradient norm: 0.38827986
INFO:root:[   22] Training loss: 0.01630439, Validation loss: 0.97017017, Gradient norm: 0.27797448
INFO:root:[   23] Training loss: 0.01626050, Validation loss: 0.95671248, Gradient norm: 0.31253803
INFO:root:[   24] Training loss: 0.01623224, Validation loss: 0.95477505, Gradient norm: 0.31939088
INFO:root:[   25] Training loss: 0.01595235, Validation loss: 0.92828552, Gradient norm: 0.30742132
INFO:root:[   26] Training loss: 0.01565855, Validation loss: 0.91562354, Gradient norm: 0.28752503
INFO:root:[   27] Training loss: 0.01558110, Validation loss: 0.91541252, Gradient norm: 0.28315809
INFO:root:[   28] Training loss: 0.01535624, Validation loss: 0.86746748, Gradient norm: 0.27517058
INFO:root:[   29] Training loss: 0.01535574, Validation loss: 0.90686809, Gradient norm: 0.30374879
INFO:root:[   30] Training loss: 0.01522468, Validation loss: 0.88725018, Gradient norm: 0.30501444
INFO:root:[   31] Training loss: 0.01500189, Validation loss: 0.86580336, Gradient norm: 0.30814091
INFO:root:[   32] Training loss: 0.01482436, Validation loss: 0.84659139, Gradient norm: 0.28967804
INFO:root:[   33] Training loss: 0.01473931, Validation loss: 0.88915054, Gradient norm: 0.28819835
INFO:root:[   34] Training loss: 0.01497801, Validation loss: 0.88720553, Gradient norm: 0.34308445
INFO:root:[   35] Training loss: 0.01438659, Validation loss: 0.88053018, Gradient norm: 0.27828276
INFO:root:[   36] Training loss: 0.01471274, Validation loss: 0.82768148, Gradient norm: 0.33486456
INFO:root:[   37] Training loss: 0.01415689, Validation loss: 0.82084955, Gradient norm: 0.26284593
INFO:root:[   38] Training loss: 0.01392717, Validation loss: 0.82681529, Gradient norm: 0.25381380
INFO:root:[   39] Training loss: 0.01393580, Validation loss: 0.80757875, Gradient norm: 0.26951768
INFO:root:[   40] Training loss: 0.01398960, Validation loss: 0.81356085, Gradient norm: 0.28989503
INFO:root:[   41] Training loss: 0.01377298, Validation loss: 0.79670186, Gradient norm: 0.27059606
INFO:root:[   42] Training loss: 0.01382011, Validation loss: 0.80724852, Gradient norm: 0.29534551
INFO:root:[   43] Training loss: 0.01360954, Validation loss: 0.78286264, Gradient norm: 0.29951585
INFO:root:[   44] Training loss: 0.01343069, Validation loss: 0.77133273, Gradient norm: 0.26974255
INFO:root:[   45] Training loss: 0.01361892, Validation loss: 0.77268079, Gradient norm: 0.31498891
INFO:root:[   46] Training loss: 0.01330884, Validation loss: 0.76111042, Gradient norm: 0.29841984
INFO:root:[   47] Training loss: 0.01308531, Validation loss: 0.75627190, Gradient norm: 0.27956658
INFO:root:[   48] Training loss: 0.01340639, Validation loss: 0.90625100, Gradient norm: 0.33295415
INFO:root:[   49] Training loss: 0.01306404, Validation loss: 0.73159727, Gradient norm: 0.29189794
INFO:root:[   50] Training loss: 0.01290173, Validation loss: 0.82938120, Gradient norm: 0.27795283
INFO:root:[   51] Training loss: 0.01312289, Validation loss: 0.74337820, Gradient norm: 0.31223093
INFO:root:[   52] Training loss: 0.01260897, Validation loss: 0.72733922, Gradient norm: 0.24068488
INFO:root:[   53] Training loss: 0.01272790, Validation loss: 0.74596235, Gradient norm: 0.28579918
INFO:root:[   54] Training loss: 0.01268745, Validation loss: 0.72826953, Gradient norm: 0.31711041
INFO:root:[   55] Training loss: 0.01262949, Validation loss: 0.71697179, Gradient norm: 0.29166787
INFO:root:[   56] Training loss: 0.01221485, Validation loss: 0.75131290, Gradient norm: 0.25031790
INFO:root:[   57] Training loss: 0.01236854, Validation loss: 0.69911438, Gradient norm: 0.29279610
INFO:root:[   58] Training loss: 0.01219204, Validation loss: 0.72688687, Gradient norm: 0.27047094
INFO:root:[   59] Training loss: 0.01225160, Validation loss: 0.71112186, Gradient norm: 0.30103967
INFO:root:[   60] Training loss: 0.01200047, Validation loss: 0.71049450, Gradient norm: 0.27378054
INFO:root:[   61] Training loss: 0.01196922, Validation loss: 0.68298224, Gradient norm: 0.29569382
INFO:root:[   62] Training loss: 0.01187243, Validation loss: 0.68542820, Gradient norm: 0.27260259
INFO:root:[   63] Training loss: 0.01189139, Validation loss: 0.74635790, Gradient norm: 0.31328096
INFO:root:[   64] Training loss: 0.01183094, Validation loss: 0.65670881, Gradient norm: 0.31773509
INFO:root:[   65] Training loss: 0.01143136, Validation loss: 0.68390399, Gradient norm: 0.23451362
INFO:root:[   66] Training loss: 0.01147381, Validation loss: 0.66849508, Gradient norm: 0.28274969
INFO:root:[   67] Training loss: 0.01158971, Validation loss: 0.64747436, Gradient norm: 0.32776310
INFO:root:[   68] Training loss: 0.01145070, Validation loss: 0.70768183, Gradient norm: 0.29948708
INFO:root:[   69] Training loss: 0.01142039, Validation loss: 0.64870456, Gradient norm: 0.33657623
INFO:root:[   70] Training loss: 0.01125743, Validation loss: 0.75007062, Gradient norm: 0.30117395
INFO:root:[   71] Training loss: 0.01112358, Validation loss: 0.62417676, Gradient norm: 0.27870149
INFO:root:[   72] Training loss: 0.01109543, Validation loss: 0.63043715, Gradient norm: 0.29272333
INFO:root:[   73] Training loss: 0.01082982, Validation loss: 0.60962360, Gradient norm: 0.23588373
INFO:root:[   74] Training loss: 0.01101606, Validation loss: 0.66618594, Gradient norm: 0.30283331
INFO:root:[   75] Training loss: 0.01085964, Validation loss: 0.61919131, Gradient norm: 0.27748780
INFO:root:[   76] Training loss: 0.01081678, Validation loss: 0.65938129, Gradient norm: 0.29429464
INFO:root:[   77] Training loss: 0.01083987, Validation loss: 0.60986337, Gradient norm: 0.30169554
INFO:root:[   78] Training loss: 0.01074794, Validation loss: 0.64896887, Gradient norm: 0.27014993
INFO:root:[   79] Training loss: 0.01094568, Validation loss: 0.60816051, Gradient norm: 0.31953307
INFO:root:[   80] Training loss: 0.01083196, Validation loss: 0.61653674, Gradient norm: 0.31112644
INFO:root:[   81] Training loss: 0.01063459, Validation loss: 0.61310825, Gradient norm: 0.26236437
INFO:root:[   82] Training loss: 0.01066486, Validation loss: 0.63564986, Gradient norm: 0.26923573
INFO:root:[   83] Training loss: 0.01056600, Validation loss: 0.60523486, Gradient norm: 0.25631579
INFO:root:[   84] Training loss: 0.01055155, Validation loss: 0.61285009, Gradient norm: 0.29271008
INFO:root:[   85] Training loss: 0.01061865, Validation loss: 0.60446747, Gradient norm: 0.29895128
INFO:root:[   86] Training loss: 0.01050009, Validation loss: 0.59383531, Gradient norm: 0.26964287
INFO:root:[   87] Training loss: 0.01034510, Validation loss: 0.67726626, Gradient norm: 0.26774141
INFO:root:[   88] Training loss: 0.01044982, Validation loss: 0.60872881, Gradient norm: 0.26900669
INFO:root:[   89] Training loss: 0.01047063, Validation loss: 0.60072868, Gradient norm: 0.29663698
INFO:root:[   90] Training loss: 0.01039967, Validation loss: 0.58729289, Gradient norm: 0.27941014
INFO:root:[   91] Training loss: 0.01060213, Validation loss: 0.61449475, Gradient norm: 0.33414007
INFO:root:[   92] Training loss: 0.01041652, Validation loss: 0.59525432, Gradient norm: 0.27779368
INFO:root:[   93] Training loss: 0.01039653, Validation loss: 0.60958626, Gradient norm: 0.28746929
INFO:root:[   94] Training loss: 0.01024217, Validation loss: 0.61622977, Gradient norm: 0.27726181
INFO:root:[   95] Training loss: 0.01021008, Validation loss: 0.59659338, Gradient norm: 0.27115880
INFO:root:[   96] Training loss: 0.01021677, Validation loss: 0.61997235, Gradient norm: 0.28188429
INFO:root:[   97] Training loss: 0.01030478, Validation loss: 0.57786653, Gradient norm: 0.28725699
INFO:root:[   98] Training loss: 0.01021065, Validation loss: 0.60951721, Gradient norm: 0.28375083
INFO:root:[   99] Training loss: 0.01007962, Validation loss: 0.61153779, Gradient norm: 0.26060713
INFO:root:[  100] Training loss: 0.01014214, Validation loss: 0.58194883, Gradient norm: 0.26462554
INFO:root:[  101] Training loss: 0.01013534, Validation loss: 0.59810795, Gradient norm: 0.26649199
INFO:root:[  102] Training loss: 0.00998857, Validation loss: 0.57623925, Gradient norm: 0.24993640
INFO:root:[  103] Training loss: 0.01007187, Validation loss: 0.57637376, Gradient norm: 0.27076666
INFO:root:[  104] Training loss: 0.01004325, Validation loss: 0.57343522, Gradient norm: 0.27454104
INFO:root:[  105] Training loss: 0.01014509, Validation loss: 0.59385816, Gradient norm: 0.29237275
INFO:root:[  106] Training loss: 0.01003043, Validation loss: 0.58477332, Gradient norm: 0.25737341
INFO:root:[  107] Training loss: 0.01011927, Validation loss: 0.58181491, Gradient norm: 0.29443298
INFO:root:[  108] Training loss: 0.01012618, Validation loss: 0.60543846, Gradient norm: 0.26475628
INFO:root:[  109] Training loss: 0.00997846, Validation loss: 0.57861600, Gradient norm: 0.26264022
INFO:root:[  110] Training loss: 0.01005986, Validation loss: 0.58188861, Gradient norm: 0.26678923
INFO:root:[  111] Training loss: 0.00985283, Validation loss: 0.57342463, Gradient norm: 0.25310643
INFO:root:[  112] Training loss: 0.00982356, Validation loss: 0.63424157, Gradient norm: 0.27202734
INFO:root:[  113] Training loss: 0.01000846, Validation loss: 0.55030236, Gradient norm: 0.28512117
INFO:root:[  114] Training loss: 0.00982345, Validation loss: 0.59959876, Gradient norm: 0.27167918
INFO:root:[  115] Training loss: 0.00992208, Validation loss: 0.62699247, Gradient norm: 0.26061084
INFO:root:[  116] Training loss: 0.00994547, Validation loss: 0.56954981, Gradient norm: 0.27831024
INFO:root:[  117] Training loss: 0.01011351, Validation loss: 0.55219005, Gradient norm: 0.30632694
INFO:root:[  118] Training loss: 0.00992740, Validation loss: 0.54837163, Gradient norm: 0.29954639
INFO:root:[  119] Training loss: 0.00979623, Validation loss: 0.56246859, Gradient norm: 0.25336436
INFO:root:[  120] Training loss: 0.00976098, Validation loss: 0.56529376, Gradient norm: 0.24584957
INFO:root:[  121] Training loss: 0.00980828, Validation loss: 0.55845693, Gradient norm: 0.25669005
INFO:root:[  122] Training loss: 0.00996912, Validation loss: 0.56599354, Gradient norm: 0.30151088
INFO:root:[  123] Training loss: 0.00973928, Validation loss: 0.57600570, Gradient norm: 0.26617835
INFO:root:[  124] Training loss: 0.00958163, Validation loss: 0.54757423, Gradient norm: 0.25139348
INFO:root:[  125] Training loss: 0.00981198, Validation loss: 0.59375196, Gradient norm: 0.28635590
INFO:root:[  126] Training loss: 0.00971526, Validation loss: 0.55553566, Gradient norm: 0.27761094
INFO:root:[  127] Training loss: 0.00973856, Validation loss: 0.56579254, Gradient norm: 0.27077822
INFO:root:[  128] Training loss: 0.00959119, Validation loss: 0.57525951, Gradient norm: 0.24053233
INFO:root:[  129] Training loss: 0.00962193, Validation loss: 0.54653534, Gradient norm: 0.25139683
INFO:root:[  130] Training loss: 0.00966065, Validation loss: 0.56543367, Gradient norm: 0.26143589
INFO:root:[  131] Training loss: 0.00982282, Validation loss: 0.57338783, Gradient norm: 0.28064651
INFO:root:[  132] Training loss: 0.00965563, Validation loss: 0.57768292, Gradient norm: 0.26361929
INFO:root:[  133] Training loss: 0.00962844, Validation loss: 0.57239872, Gradient norm: 0.25159490
INFO:root:[  134] Training loss: 0.00970142, Validation loss: 0.57651972, Gradient norm: 0.25437988
INFO:root:[  135] Training loss: 0.00966459, Validation loss: 0.56708304, Gradient norm: 0.25583298
INFO:root:[  136] Training loss: 0.00959989, Validation loss: 0.54572009, Gradient norm: 0.27206174
INFO:root:[  137] Training loss: 0.00956379, Validation loss: 0.55745903, Gradient norm: 0.26693631
INFO:root:[  138] Training loss: 0.00964834, Validation loss: 0.54305174, Gradient norm: 0.26706640
INFO:root:[  139] Training loss: 0.00953496, Validation loss: 0.67427315, Gradient norm: 0.26212033
INFO:root:[  140] Training loss: 0.00946758, Validation loss: 0.54849045, Gradient norm: 0.25051646
INFO:root:[  141] Training loss: 0.00959971, Validation loss: 0.55332660, Gradient norm: 0.27145275
INFO:root:[  142] Training loss: 0.00947341, Validation loss: 0.54107542, Gradient norm: 0.25397405
INFO:root:[  143] Training loss: 0.00946936, Validation loss: 0.55335720, Gradient norm: 0.24205248
INFO:root:[  144] Training loss: 0.00958577, Validation loss: 0.53579722, Gradient norm: 0.26552116
INFO:root:[  145] Training loss: 0.00942739, Validation loss: 0.53943907, Gradient norm: 0.22046772
INFO:root:[  146] Training loss: 0.00945854, Validation loss: 0.55337493, Gradient norm: 0.26530792
INFO:root:[  147] Training loss: 0.00948090, Validation loss: 0.58562259, Gradient norm: 0.27761722
INFO:root:[  148] Training loss: 0.00943960, Validation loss: 0.53748350, Gradient norm: 0.25959347
INFO:root:[  149] Training loss: 0.00946487, Validation loss: 0.53247032, Gradient norm: 0.26896837
INFO:root:[  150] Training loss: 0.00941042, Validation loss: 0.57617319, Gradient norm: 0.25288633
INFO:root:[  151] Training loss: 0.00940444, Validation loss: 0.52990583, Gradient norm: 0.25211401
INFO:root:[  152] Training loss: 0.00948535, Validation loss: 0.54616231, Gradient norm: 0.25325419
INFO:root:[  153] Training loss: 0.00931310, Validation loss: 0.56706566, Gradient norm: 0.24377182
INFO:root:[  154] Training loss: 0.00942099, Validation loss: 0.56513107, Gradient norm: 0.24834803
INFO:root:[  155] Training loss: 0.00953950, Validation loss: 0.54124026, Gradient norm: 0.27170854
INFO:root:[  156] Training loss: 0.00932305, Validation loss: 0.56906865, Gradient norm: 0.26660680
INFO:root:[  157] Training loss: 0.00942624, Validation loss: 0.56547858, Gradient norm: 0.27726689
INFO:root:[  158] Training loss: 0.00939302, Validation loss: 0.54804543, Gradient norm: 0.26911497
INFO:root:[  159] Training loss: 0.00930551, Validation loss: 0.52186846, Gradient norm: 0.24917369
INFO:root:[  160] Training loss: 0.00931914, Validation loss: 0.53563240, Gradient norm: 0.24741898
INFO:root:[  161] Training loss: 0.00942124, Validation loss: 0.53213372, Gradient norm: 0.26134961
INFO:root:[  162] Training loss: 0.00919662, Validation loss: 0.53127592, Gradient norm: 0.23646641
INFO:root:[  163] Training loss: 0.00936742, Validation loss: 0.55034421, Gradient norm: 0.26386951
INFO:root:[  164] Training loss: 0.00934789, Validation loss: 0.58284001, Gradient norm: 0.23693495
INFO:root:[  165] Training loss: 0.00933863, Validation loss: 0.54724659, Gradient norm: 0.26892309
INFO:root:[  166] Training loss: 0.00925470, Validation loss: 0.57955204, Gradient norm: 0.24772994
INFO:root:[  167] Training loss: 0.00941277, Validation loss: 0.56306625, Gradient norm: 0.28468055
INFO:root:[  168] Training loss: 0.00931875, Validation loss: 0.53856124, Gradient norm: 0.27649922
INFO:root:EP 168: Early stopping
INFO:root:Training the model took 4329.478s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08059
INFO:root:EnergyScoretrain: 0.06269
INFO:root:Coveragetrain: 6.89836
INFO:root:IntervalWidthtrain: 201.57988
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02193
INFO:root:EnergyScorevalidation: 0.01701
INFO:root:Coveragevalidation: 1.8358
INFO:root:IntervalWidthvalidation: 52.3759
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04805
INFO:root:EnergyScoretest: 0.03918
INFO:root:Coveragetest: 0.55052
INFO:root:IntervalWidthtest: 50.02936
INFO:root:###10 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.05, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04530438, Validation loss: 1.89352896, Gradient norm: 0.65086539
INFO:root:[    2] Training loss: 0.03057052, Validation loss: 1.57812831, Gradient norm: 0.47538350
INFO:root:[    3] Training loss: 0.02675540, Validation loss: 1.87264416, Gradient norm: 0.40735258
INFO:root:[    4] Training loss: 0.02518585, Validation loss: 1.39861445, Gradient norm: 0.42319415
INFO:root:[    5] Training loss: 0.02367155, Validation loss: 1.27780291, Gradient norm: 0.33335059
INFO:root:[    6] Training loss: 0.02289027, Validation loss: 1.36370823, Gradient norm: 0.34680101
INFO:root:[    7] Training loss: 0.02211569, Validation loss: 1.18679297, Gradient norm: 0.36622926
INFO:root:[    8] Training loss: 0.02150844, Validation loss: 1.24405918, Gradient norm: 0.37327100
INFO:root:[    9] Training loss: 0.02049877, Validation loss: 1.15191242, Gradient norm: 0.32663412
INFO:root:[   10] Training loss: 0.01984691, Validation loss: 1.10723329, Gradient norm: 0.31004987
INFO:root:[   11] Training loss: 0.01965613, Validation loss: 1.13866533, Gradient norm: 0.33852120
INFO:root:[   12] Training loss: 0.01933560, Validation loss: 1.07324510, Gradient norm: 0.35303264
INFO:root:[   13] Training loss: 0.01885031, Validation loss: 1.04534374, Gradient norm: 0.30047907
INFO:root:[   14] Training loss: 0.01849231, Validation loss: 1.27063650, Gradient norm: 0.32678523
INFO:root:[   15] Training loss: 0.01827361, Validation loss: 1.01555856, Gradient norm: 0.30113741
INFO:root:[   16] Training loss: 0.01810860, Validation loss: 1.00290917, Gradient norm: 0.31247362
INFO:root:[   17] Training loss: 0.01773923, Validation loss: 1.03082320, Gradient norm: 0.28739298
INFO:root:[   18] Training loss: 0.01747548, Validation loss: 1.00929014, Gradient norm: 0.28939809
INFO:root:[   19] Training loss: 0.01771692, Validation loss: 0.99809262, Gradient norm: 0.31871895
INFO:root:[   20] Training loss: 0.01736395, Validation loss: 0.97961575, Gradient norm: 0.32648166
INFO:root:[   21] Training loss: 0.01681414, Validation loss: 0.94794808, Gradient norm: 0.27800187
INFO:root:[   22] Training loss: 0.01675098, Validation loss: 1.03488765, Gradient norm: 0.28438814
INFO:root:[   23] Training loss: 0.01681659, Validation loss: 1.03291735, Gradient norm: 0.28588148
INFO:root:[   24] Training loss: 0.01622214, Validation loss: 0.96967389, Gradient norm: 0.26005300
INFO:root:[   25] Training loss: 0.01637656, Validation loss: 0.93616918, Gradient norm: 0.27727387
INFO:root:[   26] Training loss: 0.01624253, Validation loss: 0.96733461, Gradient norm: 0.29957964
INFO:root:[   27] Training loss: 0.01605788, Validation loss: 0.93596578, Gradient norm: 0.26762056
INFO:root:[   28] Training loss: 0.01578850, Validation loss: 0.90606118, Gradient norm: 0.27861847
INFO:root:[   29] Training loss: 0.01551869, Validation loss: 0.90235486, Gradient norm: 0.25823786
INFO:root:[   30] Training loss: 0.01571936, Validation loss: 0.93780767, Gradient norm: 0.31681380
INFO:root:[   31] Training loss: 0.01535071, Validation loss: 0.89279114, Gradient norm: 0.27112418
INFO:root:[   32] Training loss: 0.01535958, Validation loss: 0.88011685, Gradient norm: 0.30051264
INFO:root:[   33] Training loss: 0.01522467, Validation loss: 0.93660385, Gradient norm: 0.28691937
INFO:root:[   34] Training loss: 0.01502633, Validation loss: 0.89392959, Gradient norm: 0.29127088
INFO:root:[   35] Training loss: 0.01503330, Validation loss: 0.94399923, Gradient norm: 0.29476470
INFO:root:[   36] Training loss: 0.01459842, Validation loss: 0.90811284, Gradient norm: 0.26230893
INFO:root:[   37] Training loss: 0.01443403, Validation loss: 0.84341294, Gradient norm: 0.29289314
INFO:root:[   38] Training loss: 0.01439861, Validation loss: 0.87942999, Gradient norm: 0.27770731
INFO:root:[   39] Training loss: 0.01436268, Validation loss: 0.92244682, Gradient norm: 0.29132862
INFO:root:[   40] Training loss: 0.01425433, Validation loss: 0.78149781, Gradient norm: 0.31064882
INFO:root:[   41] Training loss: 0.01434749, Validation loss: 0.82331228, Gradient norm: 0.32368776
INFO:root:[   42] Training loss: 0.01390081, Validation loss: 0.79226243, Gradient norm: 0.27953345
INFO:root:[   43] Training loss: 0.01386479, Validation loss: 0.81289445, Gradient norm: 0.30054592
INFO:root:[   44] Training loss: 0.01344382, Validation loss: 0.78789411, Gradient norm: 0.24537327
INFO:root:[   45] Training loss: 0.01368117, Validation loss: 0.79796468, Gradient norm: 0.29107854
INFO:root:[   46] Training loss: 0.01331894, Validation loss: 0.75820260, Gradient norm: 0.26423244
INFO:root:[   47] Training loss: 0.01327569, Validation loss: 0.74688376, Gradient norm: 0.29801527
INFO:root:[   48] Training loss: 0.01346406, Validation loss: 0.84337977, Gradient norm: 0.30909666
INFO:root:[   49] Training loss: 0.01340069, Validation loss: 0.78064230, Gradient norm: 0.30427264
INFO:root:[   50] Training loss: 0.01290795, Validation loss: 0.73650361, Gradient norm: 0.24700592
INFO:root:[   51] Training loss: 0.01306707, Validation loss: 0.73246190, Gradient norm: 0.29989341
INFO:root:[   52] Training loss: 0.01329631, Validation loss: 0.74630244, Gradient norm: 0.33265110
INFO:root:[   53] Training loss: 0.01299346, Validation loss: 0.81752658, Gradient norm: 0.30380769
INFO:root:[   54] Training loss: 0.01276995, Validation loss: 0.74226811, Gradient norm: 0.26378745
INFO:root:[   55] Training loss: 0.01273234, Validation loss: 0.72753334, Gradient norm: 0.28970927
INFO:root:[   56] Training loss: 0.01246806, Validation loss: 0.74378100, Gradient norm: 0.25114819
INFO:root:[   57] Training loss: 0.01256289, Validation loss: 0.69777364, Gradient norm: 0.29617274
INFO:root:[   58] Training loss: 0.01231416, Validation loss: 0.71650482, Gradient norm: 0.25319412
INFO:root:[   59] Training loss: 0.01249597, Validation loss: 0.72424603, Gradient norm: 0.30070972
INFO:root:[   60] Training loss: 0.01240301, Validation loss: 0.72370264, Gradient norm: 0.29778613
INFO:root:[   61] Training loss: 0.01233106, Validation loss: 0.69862180, Gradient norm: 0.28409624
INFO:root:[   62] Training loss: 0.01240984, Validation loss: 0.69057820, Gradient norm: 0.29180326
INFO:root:[   63] Training loss: 0.01236046, Validation loss: 0.72490992, Gradient norm: 0.29501384
INFO:root:[   64] Training loss: 0.01197151, Validation loss: 0.68149513, Gradient norm: 0.26533411
INFO:root:[   65] Training loss: 0.01236174, Validation loss: 0.70747609, Gradient norm: 0.31480088
INFO:root:[   66] Training loss: 0.01182844, Validation loss: 0.68106249, Gradient norm: 0.24393225
INFO:root:[   67] Training loss: 0.01200653, Validation loss: 0.69788165, Gradient norm: 0.27506551
INFO:root:[   68] Training loss: 0.01227890, Validation loss: 0.67714794, Gradient norm: 0.33243444
INFO:root:[   69] Training loss: 0.01161984, Validation loss: 0.69977213, Gradient norm: 0.23170004
INFO:root:[   70] Training loss: 0.01175923, Validation loss: 0.69107037, Gradient norm: 0.26115734
INFO:root:[   71] Training loss: 0.01177353, Validation loss: 0.67978510, Gradient norm: 0.28014124
INFO:root:[   72] Training loss: 0.01204309, Validation loss: 0.69662104, Gradient norm: 0.33022916
INFO:root:[   73] Training loss: 0.01167062, Validation loss: 0.66110684, Gradient norm: 0.28361703
INFO:root:[   74] Training loss: 0.01153728, Validation loss: 0.67058559, Gradient norm: 0.24835764
INFO:root:[   75] Training loss: 0.01167175, Validation loss: 0.66258820, Gradient norm: 0.27221290
INFO:root:[   76] Training loss: 0.01151606, Validation loss: 0.69667593, Gradient norm: 0.28134921
INFO:root:[   77] Training loss: 0.01167979, Validation loss: 0.66968819, Gradient norm: 0.33245407
INFO:root:[   78] Training loss: 0.01141206, Validation loss: 0.71634063, Gradient norm: 0.27839712
INFO:root:[   79] Training loss: 0.01151160, Validation loss: 0.66555295, Gradient norm: 0.26416940
INFO:root:[   80] Training loss: 0.01152459, Validation loss: 0.65115349, Gradient norm: 0.29833833
INFO:root:[   81] Training loss: 0.01141720, Validation loss: 0.69096222, Gradient norm: 0.29708854
INFO:root:[   82] Training loss: 0.01131933, Validation loss: 0.65597754, Gradient norm: 0.30100089
INFO:root:[   83] Training loss: 0.01142509, Validation loss: 0.67506359, Gradient norm: 0.28912065
INFO:root:[   84] Training loss: 0.01130406, Validation loss: 0.71020140, Gradient norm: 0.29801482
INFO:root:[   85] Training loss: 0.01111338, Validation loss: 0.67095069, Gradient norm: 0.24736208
INFO:root:[   86] Training loss: 0.01117697, Validation loss: 0.81719261, Gradient norm: 0.28373944
INFO:root:[   87] Training loss: 0.01133435, Validation loss: 0.66970235, Gradient norm: 0.30520182
INFO:root:[   88] Training loss: 0.01120025, Validation loss: 0.64699881, Gradient norm: 0.30538698
INFO:root:[   89] Training loss: 0.01104937, Validation loss: 0.68140333, Gradient norm: 0.27647375
INFO:root:[   90] Training loss: 0.01126459, Validation loss: 0.64304099, Gradient norm: 0.30464343
INFO:root:[   91] Training loss: 0.01102969, Validation loss: 0.62964446, Gradient norm: 0.26437128
INFO:root:[   92] Training loss: 0.01118639, Validation loss: 0.71294730, Gradient norm: 0.30614415
INFO:root:[   93] Training loss: 0.01104983, Validation loss: 0.62500153, Gradient norm: 0.29014800
INFO:root:[   94] Training loss: 0.01087427, Validation loss: 0.65895850, Gradient norm: 0.27640078
INFO:root:[   95] Training loss: 0.01104407, Validation loss: 0.64142365, Gradient norm: 0.30408858
INFO:root:[   96] Training loss: 0.01089251, Validation loss: 0.63126979, Gradient norm: 0.27007186
INFO:root:[   97] Training loss: 0.01094942, Validation loss: 0.62016757, Gradient norm: 0.29503799
INFO:root:[   98] Training loss: 0.01081974, Validation loss: 0.64582577, Gradient norm: 0.28017886
INFO:root:[   99] Training loss: 0.01066241, Validation loss: 0.63872298, Gradient norm: 0.25385494
INFO:root:[  100] Training loss: 0.01072753, Validation loss: 0.63352403, Gradient norm: 0.25810552
INFO:root:[  101] Training loss: 0.01082461, Validation loss: 0.62478146, Gradient norm: 0.28818416
INFO:root:[  102] Training loss: 0.01066853, Validation loss: 0.63066555, Gradient norm: 0.28388540
INFO:root:[  103] Training loss: 0.01067043, Validation loss: 0.58946464, Gradient norm: 0.29331805
INFO:root:[  104] Training loss: 0.01061384, Validation loss: 0.63855252, Gradient norm: 0.25473301
INFO:root:[  105] Training loss: 0.01066408, Validation loss: 0.62496562, Gradient norm: 0.27789499
INFO:root:[  106] Training loss: 0.01071034, Validation loss: 0.62950797, Gradient norm: 0.30158741
INFO:root:[  107] Training loss: 0.01049371, Validation loss: 0.60341845, Gradient norm: 0.26052936
INFO:root:[  108] Training loss: 0.01054267, Validation loss: 0.62670040, Gradient norm: 0.28574256
INFO:root:[  109] Training loss: 0.01052177, Validation loss: 0.60038457, Gradient norm: 0.25294442
INFO:root:[  110] Training loss: 0.01054935, Validation loss: 0.60815867, Gradient norm: 0.29149644
INFO:root:[  111] Training loss: 0.01047767, Validation loss: 0.59711116, Gradient norm: 0.28435647
INFO:root:[  112] Training loss: 0.01041496, Validation loss: 0.61305844, Gradient norm: 0.27184802
INFO:root:EP 112: Early stopping
INFO:root:Training the model took 2909.434s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.09134
INFO:root:EnergyScoretrain: 0.07111
INFO:root:Coveragetrain: 6.95652
INFO:root:IntervalWidthtrain: 250.02728
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02491
INFO:root:EnergyScorevalidation: 0.01926
INFO:root:Coveragevalidation: 1.85169
INFO:root:IntervalWidthvalidation: 64.65622
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.03979
INFO:root:EnergyScoretest: 0.03065
INFO:root:Coveragetest: 0.73019
INFO:root:IntervalWidthtest: 63.65735
INFO:root:###11 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.1, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04596744, Validation loss: 1.86048494, Gradient norm: 0.39013691
INFO:root:[    2] Training loss: 0.03163339, Validation loss: 1.70378524, Gradient norm: 0.36293701
INFO:root:[    3] Training loss: 0.02877772, Validation loss: 1.71435566, Gradient norm: 0.28559488
INFO:root:[    4] Training loss: 0.02677499, Validation loss: 1.47766396, Gradient norm: 0.25553504
INFO:root:[    5] Training loss: 0.02553882, Validation loss: 1.42257813, Gradient norm: 0.29979318
INFO:root:[    6] Training loss: 0.02457173, Validation loss: 1.34673336, Gradient norm: 0.30906572
INFO:root:[    7] Training loss: 0.02338600, Validation loss: 1.33588397, Gradient norm: 0.29179358
INFO:root:[    8] Training loss: 0.02231728, Validation loss: 1.26853990, Gradient norm: 0.27512951
INFO:root:[    9] Training loss: 0.02133263, Validation loss: 1.16895442, Gradient norm: 0.28482132
INFO:root:[   10] Training loss: 0.02087161, Validation loss: 1.22753315, Gradient norm: 0.29738817
INFO:root:[   11] Training loss: 0.02037461, Validation loss: 1.15491196, Gradient norm: 0.31429894
INFO:root:[   12] Training loss: 0.01998534, Validation loss: 1.09964840, Gradient norm: 0.30726071
INFO:root:[   13] Training loss: 0.01933406, Validation loss: 1.15086601, Gradient norm: 0.26824735
INFO:root:[   14] Training loss: 0.01905318, Validation loss: 1.10676857, Gradient norm: 0.29961971
INFO:root:[   15] Training loss: 0.01857997, Validation loss: 1.19651902, Gradient norm: 0.26795512
INFO:root:[   16] Training loss: 0.01841069, Validation loss: 1.10851696, Gradient norm: 0.28357640
INFO:root:[   17] Training loss: 0.01792946, Validation loss: 1.06036796, Gradient norm: 0.27015543
INFO:root:[   18] Training loss: 0.01811901, Validation loss: 1.06179018, Gradient norm: 0.29756183
INFO:root:[   19] Training loss: 0.01770281, Validation loss: 1.00056519, Gradient norm: 0.28687773
INFO:root:[   20] Training loss: 0.01738026, Validation loss: 1.04203205, Gradient norm: 0.25159835
INFO:root:[   21] Training loss: 0.01727123, Validation loss: 1.04979119, Gradient norm: 0.27890555
INFO:root:[   22] Training loss: 0.01680069, Validation loss: 1.03355950, Gradient norm: 0.25532714
INFO:root:[   23] Training loss: 0.01683298, Validation loss: 0.96705548, Gradient norm: 0.28081449
INFO:root:[   24] Training loss: 0.01652552, Validation loss: 1.06828990, Gradient norm: 0.25735871
INFO:root:[   25] Training loss: 0.01638297, Validation loss: 1.00634730, Gradient norm: 0.27087095
INFO:root:[   26] Training loss: 0.01622802, Validation loss: 0.94316469, Gradient norm: 0.25580071
INFO:root:[   27] Training loss: 0.01593590, Validation loss: 0.91026759, Gradient norm: 0.22715588
INFO:root:[   28] Training loss: 0.01579685, Validation loss: 0.96635730, Gradient norm: 0.26682812
INFO:root:[   29] Training loss: 0.01597166, Validation loss: 0.94063415, Gradient norm: 0.29541302
INFO:root:[   30] Training loss: 0.01549261, Validation loss: 0.91501591, Gradient norm: 0.25519498
INFO:root:[   31] Training loss: 0.01559690, Validation loss: 0.88726729, Gradient norm: 0.26006467
INFO:root:[   32] Training loss: 0.01512504, Validation loss: 0.88127960, Gradient norm: 0.25642123
INFO:root:[   33] Training loss: 0.01505068, Validation loss: 0.86941355, Gradient norm: 0.23364186
INFO:root:[   34] Training loss: 0.01486811, Validation loss: 0.86174809, Gradient norm: 0.26052073
INFO:root:[   35] Training loss: 0.01481043, Validation loss: 0.87744081, Gradient norm: 0.25493796
INFO:root:[   36] Training loss: 0.01479644, Validation loss: 0.85183617, Gradient norm: 0.28434950
INFO:root:[   37] Training loss: 0.01451883, Validation loss: 0.84284320, Gradient norm: 0.24971553
INFO:root:[   38] Training loss: 0.01448194, Validation loss: 0.82502164, Gradient norm: 0.26118361
INFO:root:[   39] Training loss: 0.01431887, Validation loss: 0.83279937, Gradient norm: 0.24716661
INFO:root:[   40] Training loss: 0.01427657, Validation loss: 0.89148004, Gradient norm: 0.27070170
INFO:root:[   41] Training loss: 0.01397716, Validation loss: 0.79884591, Gradient norm: 0.24108902
INFO:root:[   42] Training loss: 0.01416259, Validation loss: 0.79290528, Gradient norm: 0.28542826
INFO:root:[   43] Training loss: 0.01392590, Validation loss: 0.81432754, Gradient norm: 0.23453941
INFO:root:[   44] Training loss: 0.01368663, Validation loss: 0.79630070, Gradient norm: 0.22564012
INFO:root:[   45] Training loss: 0.01376621, Validation loss: 0.78395505, Gradient norm: 0.25431906
INFO:root:[   46] Training loss: 0.01368566, Validation loss: 0.80185162, Gradient norm: 0.28327585
INFO:root:[   47] Training loss: 0.01329497, Validation loss: 0.79983746, Gradient norm: 0.22300792
INFO:root:[   48] Training loss: 0.01356679, Validation loss: 0.75685803, Gradient norm: 0.28214145
INFO:root:[   49] Training loss: 0.01342522, Validation loss: 0.75512225, Gradient norm: 0.27640123
INFO:root:[   50] Training loss: 0.01304995, Validation loss: 0.78625444, Gradient norm: 0.22495425
INFO:root:[   51] Training loss: 0.01335981, Validation loss: 0.74829552, Gradient norm: 0.29215858
INFO:root:[   52] Training loss: 0.01294928, Validation loss: 0.73089516, Gradient norm: 0.23618299
INFO:root:[   53] Training loss: 0.01303469, Validation loss: 0.78214586, Gradient norm: 0.28097893
INFO:root:[   54] Training loss: 0.01289852, Validation loss: 0.72293463, Gradient norm: 0.26465224
INFO:root:[   55] Training loss: 0.01298654, Validation loss: 0.73158319, Gradient norm: 0.29184925
INFO:root:[   56] Training loss: 0.01279110, Validation loss: 0.73540800, Gradient norm: 0.25253463
INFO:root:[   57] Training loss: 0.01279262, Validation loss: 0.78287743, Gradient norm: 0.27781046
INFO:root:[   58] Training loss: 0.01253620, Validation loss: 0.71776299, Gradient norm: 0.26667349
INFO:root:[   59] Training loss: 0.01240388, Validation loss: 0.73558887, Gradient norm: 0.24421717
INFO:root:[   60] Training loss: 0.01245570, Validation loss: 0.79421130, Gradient norm: 0.28684957
INFO:root:[   61] Training loss: 0.01239890, Validation loss: 0.70128305, Gradient norm: 0.26848453
INFO:root:[   62] Training loss: 0.01220107, Validation loss: 0.70267339, Gradient norm: 0.25405965
INFO:root:[   63] Training loss: 0.01218119, Validation loss: 0.70693752, Gradient norm: 0.24317829
INFO:root:[   64] Training loss: 0.01217118, Validation loss: 0.69845701, Gradient norm: 0.26449033
INFO:root:[   65] Training loss: 0.01224138, Validation loss: 0.71551928, Gradient norm: 0.27026245
INFO:root:[   66] Training loss: 0.01193296, Validation loss: 0.72509353, Gradient norm: 0.25997258
INFO:root:[   67] Training loss: 0.01205700, Validation loss: 0.77597972, Gradient norm: 0.25633303
INFO:root:[   68] Training loss: 0.01182882, Validation loss: 0.68226585, Gradient norm: 0.24526763
INFO:root:[   69] Training loss: 0.01198976, Validation loss: 0.67131621, Gradient norm: 0.27946323
INFO:root:[   70] Training loss: 0.01207425, Validation loss: 0.71457274, Gradient norm: 0.29444733
INFO:root:[   71] Training loss: 0.01176307, Validation loss: 0.67394098, Gradient norm: 0.26613811
INFO:root:[   72] Training loss: 0.01168193, Validation loss: 0.69333825, Gradient norm: 0.25498744
INFO:root:[   73] Training loss: 0.01179671, Validation loss: 0.69903695, Gradient norm: 0.25559810
INFO:root:[   74] Training loss: 0.01170218, Validation loss: 0.65397991, Gradient norm: 0.27296338
INFO:root:[   75] Training loss: 0.01143900, Validation loss: 0.69264194, Gradient norm: 0.22949345
INFO:root:[   76] Training loss: 0.01167542, Validation loss: 0.68242220, Gradient norm: 0.28519472
INFO:root:[   77] Training loss: 0.01158796, Validation loss: 0.68662504, Gradient norm: 0.28267451
INFO:root:[   78] Training loss: 0.01157845, Validation loss: 0.67705715, Gradient norm: 0.28052597
INFO:root:[   79] Training loss: 0.01137487, Validation loss: 0.65607330, Gradient norm: 0.23741867
INFO:root:[   80] Training loss: 0.01138017, Validation loss: 0.65415567, Gradient norm: 0.25164635
INFO:root:[   81] Training loss: 0.01134242, Validation loss: 0.67458317, Gradient norm: 0.25800401
INFO:root:[   82] Training loss: 0.01139333, Validation loss: 0.65816847, Gradient norm: 0.26484978
INFO:root:[   83] Training loss: 0.01118525, Validation loss: 0.67804233, Gradient norm: 0.25630999
INFO:root:[   84] Training loss: 0.01132140, Validation loss: 0.63705456, Gradient norm: 0.26335552
INFO:root:[   85] Training loss: 0.01134835, Validation loss: 0.62949462, Gradient norm: 0.27161647
INFO:root:[   86] Training loss: 0.01107981, Validation loss: 0.66771462, Gradient norm: 0.24297688
INFO:root:[   87] Training loss: 0.01115137, Validation loss: 0.63724524, Gradient norm: 0.26525452
INFO:root:[   88] Training loss: 0.01109480, Validation loss: 0.64065844, Gradient norm: 0.26488002
INFO:root:[   89] Training loss: 0.01112217, Validation loss: 0.71882549, Gradient norm: 0.24807445
INFO:root:[   90] Training loss: 0.01125080, Validation loss: 0.64696534, Gradient norm: 0.29700095
INFO:root:[   91] Training loss: 0.01094963, Validation loss: 0.63593305, Gradient norm: 0.23581643
INFO:root:[   92] Training loss: 0.01098162, Validation loss: 0.65894416, Gradient norm: 0.25662870
INFO:root:[   93] Training loss: 0.01104949, Validation loss: 0.70232368, Gradient norm: 0.29303261
INFO:root:[   94] Training loss: 0.01095151, Validation loss: 0.65164632, Gradient norm: 0.24409175
INFO:root:EP 94: Early stopping
INFO:root:Training the model took 2435.155s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.09645
INFO:root:EnergyScoretrain: 0.07566
INFO:root:Coveragetrain: 6.96872
INFO:root:IntervalWidthtrain: 269.87267
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02652
INFO:root:EnergyScorevalidation: 0.02056
INFO:root:Coveragevalidation: 1.85637
INFO:root:IntervalWidthvalidation: 69.80684
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04254
INFO:root:EnergyScoretest: 0.03305
INFO:root:Coveragetest: 0.77366
INFO:root:IntervalWidthtest: 67.24555
INFO:root:###12 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': 0.2, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05147874, Validation loss: 2.07926008, Gradient norm: 0.41979949
INFO:root:[    2] Training loss: 0.03308842, Validation loss: 1.77511757, Gradient norm: 0.32352083
INFO:root:[    3] Training loss: 0.03016058, Validation loss: 1.71737617, Gradient norm: 0.33396424
INFO:root:[    4] Training loss: 0.02820930, Validation loss: 1.59651259, Gradient norm: 0.33956391
INFO:root:[    5] Training loss: 0.02618951, Validation loss: 1.54273900, Gradient norm: 0.28754317
INFO:root:[    6] Training loss: 0.02537793, Validation loss: 1.40870782, Gradient norm: 0.32166191
INFO:root:[    7] Training loss: 0.02420519, Validation loss: 1.35646744, Gradient norm: 0.33044364
INFO:root:[    8] Training loss: 0.02349108, Validation loss: 1.28987110, Gradient norm: 0.39755314
INFO:root:[    9] Training loss: 0.02291949, Validation loss: 1.26764626, Gradient norm: 0.38835354
INFO:root:[   10] Training loss: 0.02199658, Validation loss: 1.25986739, Gradient norm: 0.28842941
INFO:root:[   11] Training loss: 0.02150380, Validation loss: 1.19360220, Gradient norm: 0.32009591
INFO:root:[   12] Training loss: 0.02123586, Validation loss: 1.21801974, Gradient norm: 0.33657632
INFO:root:[   13] Training loss: 0.02133346, Validation loss: 1.18733410, Gradient norm: 0.35094952
INFO:root:[   14] Training loss: 0.02065640, Validation loss: 1.22005045, Gradient norm: 0.33186291
INFO:root:[   15] Training loss: 0.02020914, Validation loss: 1.16953600, Gradient norm: 0.32769657
INFO:root:[   16] Training loss: 0.02014383, Validation loss: 1.13031356, Gradient norm: 0.33287973
INFO:root:[   17] Training loss: 0.01976385, Validation loss: 1.15297825, Gradient norm: 0.32087993
INFO:root:[   18] Training loss: 0.01924808, Validation loss: 1.08322049, Gradient norm: 0.30318830
INFO:root:[   19] Training loss: 0.01909659, Validation loss: 1.09555126, Gradient norm: 0.29276658
INFO:root:[   20] Training loss: 0.01904191, Validation loss: 1.06515364, Gradient norm: 0.32935840
INFO:root:[   21] Training loss: 0.01854955, Validation loss: 1.07583841, Gradient norm: 0.29271494
INFO:root:[   22] Training loss: 0.01859412, Validation loss: 1.07118772, Gradient norm: 0.34157320
INFO:root:[   23] Training loss: 0.01831542, Validation loss: 1.08325607, Gradient norm: 0.29772461
INFO:root:[   24] Training loss: 0.01800723, Validation loss: 1.02323628, Gradient norm: 0.28921086
INFO:root:[   25] Training loss: 0.01785209, Validation loss: 1.06402154, Gradient norm: 0.31360817
INFO:root:[   26] Training loss: 0.01772203, Validation loss: 1.00621127, Gradient norm: 0.32381585
INFO:root:[   27] Training loss: 0.01741202, Validation loss: 1.01109513, Gradient norm: 0.30610695
INFO:root:[   28] Training loss: 0.01706505, Validation loss: 0.98180926, Gradient norm: 0.27089947
INFO:root:[   29] Training loss: 0.01719726, Validation loss: 1.06546324, Gradient norm: 0.29428786
INFO:root:[   30] Training loss: 0.01693015, Validation loss: 1.08027167, Gradient norm: 0.31254017
INFO:root:[   31] Training loss: 0.01692893, Validation loss: 0.93978477, Gradient norm: 0.29910412
INFO:root:[   32] Training loss: 0.01667593, Validation loss: 0.97422506, Gradient norm: 0.27342613
INFO:root:[   33] Training loss: 0.01663021, Validation loss: 0.93949983, Gradient norm: 0.30588274
INFO:root:[   34] Training loss: 0.01644141, Validation loss: 0.93896583, Gradient norm: 0.31856153
INFO:root:[   35] Training loss: 0.01617382, Validation loss: 0.90278627, Gradient norm: 0.28588356
INFO:root:[   36] Training loss: 0.01632809, Validation loss: 0.93493099, Gradient norm: 0.32685193
INFO:root:[   37] Training loss: 0.01575762, Validation loss: 0.91331363, Gradient norm: 0.27471752
INFO:root:[   38] Training loss: 0.01582371, Validation loss: 0.94882341, Gradient norm: 0.30294007
INFO:root:[   39] Training loss: 0.01578987, Validation loss: 0.92927313, Gradient norm: 0.28041521
INFO:root:[   40] Training loss: 0.01549080, Validation loss: 0.88118378, Gradient norm: 0.28808950
INFO:root:[   41] Training loss: 0.01528869, Validation loss: 0.87017766, Gradient norm: 0.28216243
INFO:root:[   42] Training loss: 0.01507929, Validation loss: 0.85747626, Gradient norm: 0.26878406
INFO:root:[   43] Training loss: 0.01493931, Validation loss: 0.86616311, Gradient norm: 0.26124971
INFO:root:[   44] Training loss: 0.01483261, Validation loss: 1.00941410, Gradient norm: 0.26891672
INFO:root:[   45] Training loss: 0.01476084, Validation loss: 0.82199871, Gradient norm: 0.26391583
INFO:root:[   46] Training loss: 0.01463160, Validation loss: 0.85768832, Gradient norm: 0.29153416
INFO:root:[   47] Training loss: 0.01453413, Validation loss: 0.83341679, Gradient norm: 0.28489376
INFO:root:[   48] Training loss: 0.01451232, Validation loss: 0.81445733, Gradient norm: 0.30793065
INFO:root:[   49] Training loss: 0.01431874, Validation loss: 0.83461823, Gradient norm: 0.30207236
INFO:root:[   50] Training loss: 0.01409871, Validation loss: 0.81518336, Gradient norm: 0.25745658
INFO:root:[   51] Training loss: 0.01400558, Validation loss: 0.80024509, Gradient norm: 0.25819415
INFO:root:[   52] Training loss: 0.01402816, Validation loss: 0.79239988, Gradient norm: 0.29329051
INFO:root:[   53] Training loss: 0.01396796, Validation loss: 0.77292692, Gradient norm: 0.28764226
INFO:root:[   54] Training loss: 0.01351090, Validation loss: 0.85294724, Gradient norm: 0.25359390
INFO:root:[   55] Training loss: 0.01390961, Validation loss: 0.80917095, Gradient norm: 0.29306983
INFO:root:[   56] Training loss: 0.01375698, Validation loss: 0.78909134, Gradient norm: 0.32907818
INFO:root:[   57] Training loss: 0.01354464, Validation loss: 0.78376746, Gradient norm: 0.28227155
INFO:root:[   58] Training loss: 0.01376110, Validation loss: 0.75850285, Gradient norm: 0.30223926
INFO:root:[   59] Training loss: 0.01348448, Validation loss: 0.77791400, Gradient norm: 0.30487458
INFO:root:[   60] Training loss: 0.01324226, Validation loss: 0.76772177, Gradient norm: 0.30535689
INFO:root:[   61] Training loss: 0.01324554, Validation loss: 0.77802422, Gradient norm: 0.29442370
INFO:root:[   62] Training loss: 0.01305794, Validation loss: 0.77510694, Gradient norm: 0.25628999
INFO:root:[   63] Training loss: 0.01314728, Validation loss: 0.77728429, Gradient norm: 0.28258872
INFO:root:[   64] Training loss: 0.01288466, Validation loss: 0.73318935, Gradient norm: 0.25059886
INFO:root:[   65] Training loss: 0.01290233, Validation loss: 0.72571587, Gradient norm: 0.27476357
INFO:root:[   66] Training loss: 0.01268340, Validation loss: 0.82166014, Gradient norm: 0.28407561
INFO:root:[   67] Training loss: 0.01292312, Validation loss: 0.72275315, Gradient norm: 0.30956730
INFO:root:[   68] Training loss: 0.01264582, Validation loss: 0.70469118, Gradient norm: 0.24978574
INFO:root:[   69] Training loss: 0.01258420, Validation loss: 0.73720338, Gradient norm: 0.26510428
INFO:root:[   70] Training loss: 0.01234765, Validation loss: 0.72361298, Gradient norm: 0.24723457
INFO:root:[   71] Training loss: 0.01234278, Validation loss: 0.73065500, Gradient norm: 0.26428872
INFO:root:[   72] Training loss: 0.01251006, Validation loss: 0.72874141, Gradient norm: 0.30695612
INFO:root:[   73] Training loss: 0.01231180, Validation loss: 0.77627127, Gradient norm: 0.26902473
INFO:root:[   74] Training loss: 0.01228282, Validation loss: 0.69159989, Gradient norm: 0.29050322
INFO:root:[   75] Training loss: 0.01210816, Validation loss: 0.70819792, Gradient norm: 0.23888615
INFO:root:[   76] Training loss: 0.01224646, Validation loss: 0.71091935, Gradient norm: 0.31626559
INFO:root:[   77] Training loss: 0.01195270, Validation loss: 0.68852573, Gradient norm: 0.24195176
INFO:root:[   78] Training loss: 0.01191311, Validation loss: 0.70629987, Gradient norm: 0.25075910
INFO:root:[   79] Training loss: 0.01212337, Validation loss: 0.70502789, Gradient norm: 0.29874737
INFO:root:[   80] Training loss: 0.01194336, Validation loss: 0.68442345, Gradient norm: 0.28822702
INFO:root:[   81] Training loss: 0.01174185, Validation loss: 0.69533439, Gradient norm: 0.24710084
INFO:root:[   82] Training loss: 0.01178978, Validation loss: 0.68468545, Gradient norm: 0.24819099
INFO:root:[   83] Training loss: 0.01166108, Validation loss: 0.65130484, Gradient norm: 0.25274059
INFO:root:[   84] Training loss: 0.01190133, Validation loss: 0.68853704, Gradient norm: 0.27637937
INFO:root:[   85] Training loss: 0.01159800, Validation loss: 0.65762633, Gradient norm: 0.26676521
INFO:root:[   86] Training loss: 0.01167639, Validation loss: 0.68334877, Gradient norm: 0.27132998
INFO:root:[   87] Training loss: 0.01174879, Validation loss: 0.69158908, Gradient norm: 0.29523270
INFO:root:[   88] Training loss: 0.01154355, Validation loss: 0.68773501, Gradient norm: 0.28247876
INFO:root:[   89] Training loss: 0.01153318, Validation loss: 0.66161388, Gradient norm: 0.27700215
INFO:root:[   90] Training loss: 0.01176803, Validation loss: 0.63935262, Gradient norm: 0.31660300
INFO:root:[   91] Training loss: 0.01178250, Validation loss: 0.64393221, Gradient norm: 0.31504306
INFO:root:[   92] Training loss: 0.01138870, Validation loss: 0.66603177, Gradient norm: 0.26620181
INFO:root:[   93] Training loss: 0.01122999, Validation loss: 0.66231936, Gradient norm: 0.23592135
INFO:root:[   94] Training loss: 0.01132223, Validation loss: 0.65366385, Gradient norm: 0.27876603
INFO:root:[   95] Training loss: 0.01129623, Validation loss: 0.68594391, Gradient norm: 0.24365013
INFO:root:[   96] Training loss: 0.01128279, Validation loss: 0.66624446, Gradient norm: 0.27693041
INFO:root:[   97] Training loss: 0.01131557, Validation loss: 0.64599812, Gradient norm: 0.27294651
INFO:root:[   98] Training loss: 0.01132346, Validation loss: 0.68732986, Gradient norm: 0.28776943
INFO:root:[   99] Training loss: 0.01113130, Validation loss: 0.63788885, Gradient norm: 0.24362344
INFO:root:[  100] Training loss: 0.01145818, Validation loss: 0.67344814, Gradient norm: 0.28790527
INFO:root:[  101] Training loss: 0.01115959, Validation loss: 0.63951236, Gradient norm: 0.27857159
INFO:root:[  102] Training loss: 0.01118472, Validation loss: 0.62905915, Gradient norm: 0.30587160
INFO:root:[  103] Training loss: 0.01114531, Validation loss: 0.63509821, Gradient norm: 0.25502600
INFO:root:[  104] Training loss: 0.01100032, Validation loss: 0.62936121, Gradient norm: 0.26063851
INFO:root:[  105] Training loss: 0.01110841, Validation loss: 0.63935799, Gradient norm: 0.28799034
INFO:root:[  106] Training loss: 0.01092237, Validation loss: 0.61547682, Gradient norm: 0.24564132
INFO:root:[  107] Training loss: 0.01096351, Validation loss: 0.64452310, Gradient norm: 0.27000451
INFO:root:[  108] Training loss: 0.01077476, Validation loss: 0.64104936, Gradient norm: 0.23103451
INFO:root:[  109] Training loss: 0.01097564, Validation loss: 0.64537239, Gradient norm: 0.27538595
INFO:root:[  110] Training loss: 0.01097053, Validation loss: 0.67827292, Gradient norm: 0.29496165
INFO:root:[  111] Training loss: 0.01088688, Validation loss: 0.62163834, Gradient norm: 0.27986763
INFO:root:[  112] Training loss: 0.01086224, Validation loss: 0.63273342, Gradient norm: 0.25765670
INFO:root:[  113] Training loss: 0.01090230, Validation loss: 0.65845525, Gradient norm: 0.25709573
INFO:root:[  114] Training loss: 0.01079509, Validation loss: 0.61854776, Gradient norm: 0.29476240
INFO:root:[  115] Training loss: 0.01077031, Validation loss: 0.60755217, Gradient norm: 0.26471030
INFO:root:[  116] Training loss: 0.01074496, Validation loss: 0.60944790, Gradient norm: 0.27191506
INFO:root:[  117] Training loss: 0.01073448, Validation loss: 0.62850379, Gradient norm: 0.26788534
INFO:root:[  118] Training loss: 0.01073303, Validation loss: 0.60768081, Gradient norm: 0.25404677
INFO:root:[  119] Training loss: 0.01048867, Validation loss: 0.60695471, Gradient norm: 0.25400891
INFO:root:[  120] Training loss: 0.01068528, Validation loss: 0.60840721, Gradient norm: 0.27405628
INFO:root:[  121] Training loss: 0.01079571, Validation loss: 0.60972415, Gradient norm: 0.27876225
INFO:root:[  122] Training loss: 0.01078570, Validation loss: 0.61041915, Gradient norm: 0.29564501
INFO:root:[  123] Training loss: 0.01050208, Validation loss: 0.60351385, Gradient norm: 0.25358099
INFO:root:[  124] Training loss: 0.01042158, Validation loss: 0.61396240, Gradient norm: 0.25452647
INFO:root:[  125] Training loss: 0.01054289, Validation loss: 0.62000329, Gradient norm: 0.25728201
INFO:root:[  126] Training loss: 0.01045632, Validation loss: 0.60020666, Gradient norm: 0.25458209
INFO:root:[  127] Training loss: 0.01043799, Validation loss: 0.58298477, Gradient norm: 0.25898339
INFO:root:[  128] Training loss: 0.01053017, Validation loss: 0.59671653, Gradient norm: 0.28685239
INFO:root:[  129] Training loss: 0.01051536, Validation loss: 0.58509020, Gradient norm: 0.25132313
INFO:root:[  130] Training loss: 0.01032232, Validation loss: 0.58745748, Gradient norm: 0.24768618
INFO:root:[  131] Training loss: 0.01038374, Validation loss: 0.59852407, Gradient norm: 0.26664198
INFO:root:[  132] Training loss: 0.01047770, Validation loss: 0.58218028, Gradient norm: 0.25745903
INFO:root:[  133] Training loss: 0.01049645, Validation loss: 0.58375329, Gradient norm: 0.25649172
INFO:root:[  134] Training loss: 0.01018408, Validation loss: 0.58894512, Gradient norm: 0.24532791
INFO:root:[  135] Training loss: 0.01037826, Validation loss: 0.58953185, Gradient norm: 0.27851841
INFO:root:[  136] Training loss: 0.01029641, Validation loss: 0.60871796, Gradient norm: 0.26241096
INFO:root:[  137] Training loss: 0.01031701, Validation loss: 0.57565786, Gradient norm: 0.26767134
INFO:root:[  138] Training loss: 0.01024439, Validation loss: 0.59256150, Gradient norm: 0.26930961
INFO:root:[  139] Training loss: 0.01037476, Validation loss: 0.59146542, Gradient norm: 0.25601884
INFO:root:[  140] Training loss: 0.01027692, Validation loss: 0.60161546, Gradient norm: 0.26081828
INFO:root:[  141] Training loss: 0.01040654, Validation loss: 0.61302705, Gradient norm: 0.29284495
INFO:root:[  142] Training loss: 0.01026524, Validation loss: 0.60110942, Gradient norm: 0.27750726
INFO:root:[  143] Training loss: 0.01031756, Validation loss: 0.57307458, Gradient norm: 0.27407920
INFO:root:[  144] Training loss: 0.01029436, Validation loss: 0.57206908, Gradient norm: 0.26959451
INFO:root:[  145] Training loss: 0.01016127, Validation loss: 0.65423542, Gradient norm: 0.23422957
INFO:root:[  146] Training loss: 0.01018501, Validation loss: 0.61330534, Gradient norm: 0.25801124
INFO:root:[  147] Training loss: 0.01000309, Validation loss: 0.56533530, Gradient norm: 0.24137684
INFO:root:[  148] Training loss: 0.01015187, Validation loss: 0.59978789, Gradient norm: 0.24882700
INFO:root:[  149] Training loss: 0.01025519, Validation loss: 0.59587692, Gradient norm: 0.26749660
INFO:root:[  150] Training loss: 0.01008052, Validation loss: 0.60042648, Gradient norm: 0.26039211
INFO:root:[  151] Training loss: 0.01027428, Validation loss: 0.58886393, Gradient norm: 0.26980868
INFO:root:[  152] Training loss: 0.01002236, Validation loss: 0.58267144, Gradient norm: 0.21931262
INFO:root:[  153] Training loss: 0.01005754, Validation loss: 0.58756082, Gradient norm: 0.26070200
INFO:root:[  154] Training loss: 0.01008673, Validation loss: 0.58137007, Gradient norm: 0.27950915
INFO:root:[  155] Training loss: 0.00986371, Validation loss: 0.56501921, Gradient norm: 0.24138067
INFO:root:[  156] Training loss: 0.01006464, Validation loss: 0.57886229, Gradient norm: 0.25497202
INFO:root:[  157] Training loss: 0.01014525, Validation loss: 0.56545589, Gradient norm: 0.25976369
INFO:root:[  158] Training loss: 0.01007104, Validation loss: 0.55734002, Gradient norm: 0.26256507
INFO:root:[  159] Training loss: 0.00986315, Validation loss: 0.56339124, Gradient norm: 0.26009527
INFO:root:[  160] Training loss: 0.00997084, Validation loss: 0.60421489, Gradient norm: 0.24832960
INFO:root:[  161] Training loss: 0.00994574, Validation loss: 0.57546564, Gradient norm: 0.24278972
INFO:root:[  162] Training loss: 0.01002845, Validation loss: 0.56892635, Gradient norm: 0.24747184
INFO:root:[  163] Training loss: 0.01004505, Validation loss: 0.59204465, Gradient norm: 0.27985262
INFO:root:[  164] Training loss: 0.00982853, Validation loss: 0.57287667, Gradient norm: 0.23531784
INFO:root:[  165] Training loss: 0.00978129, Validation loss: 0.57756488, Gradient norm: 0.26659393
INFO:root:[  166] Training loss: 0.00986960, Validation loss: 0.55446101, Gradient norm: 0.25363167
INFO:root:[  167] Training loss: 0.00993528, Validation loss: 0.56977520, Gradient norm: 0.25427521
INFO:root:[  168] Training loss: 0.00989334, Validation loss: 0.57193231, Gradient norm: 0.27029695
INFO:root:[  169] Training loss: 0.00988696, Validation loss: 0.55638375, Gradient norm: 0.26626007
INFO:root:[  170] Training loss: 0.00971990, Validation loss: 0.56737752, Gradient norm: 0.24457794
INFO:root:[  171] Training loss: 0.00967284, Validation loss: 0.56471118, Gradient norm: 0.23111140
INFO:root:[  172] Training loss: 0.00971236, Validation loss: 0.55855917, Gradient norm: 0.22992831
INFO:root:[  173] Training loss: 0.00971360, Validation loss: 0.56568376, Gradient norm: 0.24335192
INFO:root:[  174] Training loss: 0.00997196, Validation loss: 0.57027881, Gradient norm: 0.26498845
INFO:root:[  175] Training loss: 0.00980391, Validation loss: 0.57240947, Gradient norm: 0.25805540
INFO:root:[  176] Training loss: 0.00968800, Validation loss: 0.55178240, Gradient norm: 0.23548942
INFO:root:[  177] Training loss: 0.00978403, Validation loss: 0.54051596, Gradient norm: 0.26144277
INFO:root:[  178] Training loss: 0.00976762, Validation loss: 0.58479979, Gradient norm: 0.27106502
INFO:root:[  179] Training loss: 0.00975488, Validation loss: 0.56012613, Gradient norm: 0.25206736
INFO:root:[  180] Training loss: 0.00969976, Validation loss: 0.57451589, Gradient norm: 0.23616955
INFO:root:[  181] Training loss: 0.00971018, Validation loss: 0.53723826, Gradient norm: 0.24591544
INFO:root:[  182] Training loss: 0.00968541, Validation loss: 0.59009315, Gradient norm: 0.23142765
INFO:root:[  183] Training loss: 0.00977692, Validation loss: 0.55721503, Gradient norm: 0.26735484
INFO:root:[  184] Training loss: 0.00958423, Validation loss: 0.54195976, Gradient norm: 0.24014251
INFO:root:[  185] Training loss: 0.00980409, Validation loss: 0.63626180, Gradient norm: 0.25299599
INFO:root:[  186] Training loss: 0.00966678, Validation loss: 0.53650949, Gradient norm: 0.22217501
INFO:root:[  187] Training loss: 0.00963746, Validation loss: 0.54056546, Gradient norm: 0.27312088
INFO:root:[  188] Training loss: 0.00963163, Validation loss: 0.54915844, Gradient norm: 0.23859036
INFO:root:[  189] Training loss: 0.00962681, Validation loss: 0.54795423, Gradient norm: 0.25109189
INFO:root:[  190] Training loss: 0.00977328, Validation loss: 0.56740213, Gradient norm: 0.25551372
INFO:root:[  191] Training loss: 0.00950536, Validation loss: 0.57208401, Gradient norm: 0.22088030
INFO:root:[  192] Training loss: 0.00949012, Validation loss: 0.58649208, Gradient norm: 0.24621639
INFO:root:[  193] Training loss: 0.00970946, Validation loss: 0.55673086, Gradient norm: 0.25475579
INFO:root:[  194] Training loss: 0.00963847, Validation loss: 0.53770273, Gradient norm: 0.25713273
INFO:root:[  195] Training loss: 0.00941573, Validation loss: 0.55537365, Gradient norm: 0.24347039
INFO:root:EP 195: Early stopping
INFO:root:Training the model took 5019.594s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08297
INFO:root:EnergyScoretrain: 0.06588
INFO:root:Coveragetrain: 6.97123
INFO:root:IntervalWidthtrain: 244.54257
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02287
INFO:root:EnergyScorevalidation: 0.01796
INFO:root:Coveragevalidation: 1.85904
INFO:root:IntervalWidthvalidation: 63.78227
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.03874
INFO:root:EnergyScoretest: 0.03017
INFO:root:Coveragetest: 0.81974
INFO:root:IntervalWidthtest: 62.43661
INFO:root:###13 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.01, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04677728, Validation loss: 1.80623468, Gradient norm: 0.54632448
INFO:root:[    2] Training loss: 0.02944263, Validation loss: 1.52801547, Gradient norm: 0.49523701
INFO:root:[    3] Training loss: 0.02672871, Validation loss: 1.48042727, Gradient norm: 0.51752110
INFO:root:[    4] Training loss: 0.02496625, Validation loss: 1.53455088, Gradient norm: 0.46544653
INFO:root:[    5] Training loss: 0.02407211, Validation loss: 1.31836927, Gradient norm: 0.45667985
INFO:root:[    6] Training loss: 0.02335010, Validation loss: 1.27015067, Gradient norm: 0.44172768
INFO:root:[    7] Training loss: 0.02245614, Validation loss: 1.27117653, Gradient norm: 0.39209786
INFO:root:[    8] Training loss: 0.02230210, Validation loss: 1.29728228, Gradient norm: 0.49789438
INFO:root:[    9] Training loss: 0.02156373, Validation loss: 1.23926282, Gradient norm: 0.43852093
INFO:root:[   10] Training loss: 0.02127512, Validation loss: 1.21536848, Gradient norm: 0.46490796
INFO:root:[   11] Training loss: 0.02028410, Validation loss: 1.14293828, Gradient norm: 0.35543888
INFO:root:[   12] Training loss: 0.01981405, Validation loss: 1.35500225, Gradient norm: 0.36530039
INFO:root:[   13] Training loss: 0.01961882, Validation loss: 1.20504356, Gradient norm: 0.35387050
INFO:root:[   14] Training loss: 0.01907097, Validation loss: 1.08336879, Gradient norm: 0.35449878
INFO:root:[   15] Training loss: 0.01874601, Validation loss: 1.08393737, Gradient norm: 0.37916754
INFO:root:[   16] Training loss: 0.01834139, Validation loss: 1.05985675, Gradient norm: 0.35586494
INFO:root:[   17] Training loss: 0.01811294, Validation loss: 1.00659104, Gradient norm: 0.38656592
INFO:root:[   18] Training loss: 0.01805502, Validation loss: 1.09284116, Gradient norm: 0.42048346
INFO:root:[   19] Training loss: 0.01733839, Validation loss: 0.99227500, Gradient norm: 0.31293035
INFO:root:[   20] Training loss: 0.01751327, Validation loss: 1.03548176, Gradient norm: 0.39380296
INFO:root:[   21] Training loss: 0.01688155, Validation loss: 0.95982330, Gradient norm: 0.33028732
INFO:root:[   22] Training loss: 0.01663152, Validation loss: 0.95254554, Gradient norm: 0.34272799
INFO:root:[   23] Training loss: 0.01675575, Validation loss: 1.00413817, Gradient norm: 0.41838813
INFO:root:[   24] Training loss: 0.01623955, Validation loss: 0.92344619, Gradient norm: 0.30826490
INFO:root:[   25] Training loss: 0.01585328, Validation loss: 0.90314902, Gradient norm: 0.30085437
INFO:root:[   26] Training loss: 0.01612182, Validation loss: 0.92489498, Gradient norm: 0.37271479
INFO:root:[   27] Training loss: 0.01576229, Validation loss: 0.99074979, Gradient norm: 0.32179283
INFO:root:[   28] Training loss: 0.01576041, Validation loss: 0.91235757, Gradient norm: 0.38267210
INFO:root:[   29] Training loss: 0.01539323, Validation loss: 0.89175868, Gradient norm: 0.34020930
INFO:root:[   30] Training loss: 0.01544149, Validation loss: 0.89820669, Gradient norm: 0.36284247
INFO:root:[   31] Training loss: 0.01509494, Validation loss: 0.88284373, Gradient norm: 0.30721630
INFO:root:[   32] Training loss: 0.01501395, Validation loss: 0.93761342, Gradient norm: 0.32852221
INFO:root:[   33] Training loss: 0.01484458, Validation loss: 0.87568866, Gradient norm: 0.34158936
INFO:root:[   34] Training loss: 0.01471327, Validation loss: 0.86753042, Gradient norm: 0.32358175
INFO:root:[   35] Training loss: 0.01460536, Validation loss: 0.85607167, Gradient norm: 0.34089085
INFO:root:[   36] Training loss: 0.01451121, Validation loss: 0.81481078, Gradient norm: 0.35382787
INFO:root:[   37] Training loss: 0.01424337, Validation loss: 0.81518887, Gradient norm: 0.29891454
INFO:root:[   38] Training loss: 0.01422482, Validation loss: 0.83555181, Gradient norm: 0.35203001
INFO:root:[   39] Training loss: 0.01402805, Validation loss: 0.79364030, Gradient norm: 0.32946118
INFO:root:[   40] Training loss: 0.01401572, Validation loss: 0.79445068, Gradient norm: 0.33867897
INFO:root:[   41] Training loss: 0.01391027, Validation loss: 0.80601998, Gradient norm: 0.36463187
INFO:root:[   42] Training loss: 0.01361539, Validation loss: 0.81065883, Gradient norm: 0.29819216
INFO:root:[   43] Training loss: 0.01370356, Validation loss: 0.79354619, Gradient norm: 0.31751624
INFO:root:[   44] Training loss: 0.01357565, Validation loss: 0.75841226, Gradient norm: 0.35934943
INFO:root:[   45] Training loss: 0.01337842, Validation loss: 0.84224249, Gradient norm: 0.28837446
INFO:root:[   46] Training loss: 0.01316984, Validation loss: 0.75663358, Gradient norm: 0.28532435
INFO:root:[   47] Training loss: 0.01324977, Validation loss: 0.74980027, Gradient norm: 0.34719671
INFO:root:[   48] Training loss: 0.01326758, Validation loss: 0.77785114, Gradient norm: 0.35115384
INFO:root:[   49] Training loss: 0.01335817, Validation loss: 0.76175255, Gradient norm: 0.37729835
INFO:root:[   50] Training loss: 0.01300817, Validation loss: 0.73195102, Gradient norm: 0.31170525
INFO:root:[   51] Training loss: 0.01304896, Validation loss: 0.74235159, Gradient norm: 0.36470542
INFO:root:[   52] Training loss: 0.01284158, Validation loss: 0.74985202, Gradient norm: 0.31321100
INFO:root:[   53] Training loss: 0.01263491, Validation loss: 0.72630153, Gradient norm: 0.30612531
INFO:root:[   54] Training loss: 0.01256955, Validation loss: 0.77241234, Gradient norm: 0.30252264
INFO:root:[   55] Training loss: 0.01279575, Validation loss: 0.77230200, Gradient norm: 0.36270757
INFO:root:[   56] Training loss: 0.01273913, Validation loss: 0.75561931, Gradient norm: 0.34518200
INFO:root:[   57] Training loss: 0.01231540, Validation loss: 0.70872874, Gradient norm: 0.27987211
INFO:root:[   58] Training loss: 0.01231106, Validation loss: 0.71179896, Gradient norm: 0.29012632
INFO:root:[   59] Training loss: 0.01237903, Validation loss: 0.72472777, Gradient norm: 0.29015658
INFO:root:[   60] Training loss: 0.01247137, Validation loss: 0.71400313, Gradient norm: 0.33496173
INFO:root:[   61] Training loss: 0.01228231, Validation loss: 0.81299363, Gradient norm: 0.30895017
INFO:root:[   62] Training loss: 0.01211396, Validation loss: 0.68875302, Gradient norm: 0.28755148
INFO:root:[   63] Training loss: 0.01201759, Validation loss: 0.75526795, Gradient norm: 0.28831655
INFO:root:[   64] Training loss: 0.01204016, Validation loss: 0.71010227, Gradient norm: 0.30442765
INFO:root:[   65] Training loss: 0.01225100, Validation loss: 0.71884897, Gradient norm: 0.34116666
INFO:root:[   66] Training loss: 0.01201922, Validation loss: 0.69115542, Gradient norm: 0.30615818
INFO:root:[   67] Training loss: 0.01200094, Validation loss: 0.69808831, Gradient norm: 0.30320831
INFO:root:[   68] Training loss: 0.01173353, Validation loss: 0.72961282, Gradient norm: 0.28122490
INFO:root:[   69] Training loss: 0.01175106, Validation loss: 0.66992246, Gradient norm: 0.28079652
INFO:root:[   70] Training loss: 0.01179152, Validation loss: 0.66235511, Gradient norm: 0.30851001
INFO:root:[   71] Training loss: 0.01174913, Validation loss: 0.73606414, Gradient norm: 0.30312174
INFO:root:[   72] Training loss: 0.01148385, Validation loss: 0.68209219, Gradient norm: 0.24397603
INFO:root:[   73] Training loss: 0.01180018, Validation loss: 0.65337635, Gradient norm: 0.32766827
INFO:root:[   74] Training loss: 0.01161515, Validation loss: 0.67262584, Gradient norm: 0.29470006
INFO:root:[   75] Training loss: 0.01163604, Validation loss: 0.66980109, Gradient norm: 0.32811322
INFO:root:[   76] Training loss: 0.01158353, Validation loss: 0.66795610, Gradient norm: 0.30213554
INFO:root:[   77] Training loss: 0.01158055, Validation loss: 0.67071274, Gradient norm: 0.29375362
INFO:root:[   78] Training loss: 0.01133201, Validation loss: 0.66489314, Gradient norm: 0.26701106
INFO:root:[   79] Training loss: 0.01131811, Validation loss: 0.69328035, Gradient norm: 0.28335214
INFO:root:[   80] Training loss: 0.01124379, Validation loss: 0.66390058, Gradient norm: 0.27244756
INFO:root:[   81] Training loss: 0.01131984, Validation loss: 0.63739190, Gradient norm: 0.28660180
INFO:root:[   82] Training loss: 0.01110057, Validation loss: 0.63556781, Gradient norm: 0.26136209
INFO:root:[   83] Training loss: 0.01147321, Validation loss: 0.65919957, Gradient norm: 0.31546074
INFO:root:[   84] Training loss: 0.01113137, Validation loss: 0.68515788, Gradient norm: 0.26668300
INFO:root:[   85] Training loss: 0.01110945, Validation loss: 0.64395604, Gradient norm: 0.29132915
INFO:root:[   86] Training loss: 0.01108541, Validation loss: 0.63583175, Gradient norm: 0.26563459
INFO:root:[   87] Training loss: 0.01103779, Validation loss: 0.64369607, Gradient norm: 0.27840754
INFO:root:[   88] Training loss: 0.01104090, Validation loss: 0.62876883, Gradient norm: 0.29347401
INFO:root:[   89] Training loss: 0.01097328, Validation loss: 0.68486352, Gradient norm: 0.27682551
INFO:root:[   90] Training loss: 0.01098338, Validation loss: 0.64872687, Gradient norm: 0.27097347
INFO:root:[   91] Training loss: 0.01104159, Validation loss: 0.62134865, Gradient norm: 0.29273594
INFO:root:[   92] Training loss: 0.01081798, Validation loss: 0.72767246, Gradient norm: 0.26243077
INFO:root:[   93] Training loss: 0.01085380, Validation loss: 0.64841832, Gradient norm: 0.27330708
INFO:root:[   94] Training loss: 0.01082756, Validation loss: 0.62080430, Gradient norm: 0.27655321
INFO:root:[   95] Training loss: 0.01089664, Validation loss: 0.65222918, Gradient norm: 0.27409315
INFO:root:[   96] Training loss: 0.01094370, Validation loss: 0.62053483, Gradient norm: 0.29398130
INFO:root:[   97] Training loss: 0.01077565, Validation loss: 0.66113983, Gradient norm: 0.26784130
INFO:root:[   98] Training loss: 0.01080423, Validation loss: 0.62869186, Gradient norm: 0.29009552
INFO:root:[   99] Training loss: 0.01087804, Validation loss: 0.62418883, Gradient norm: 0.28757336
INFO:root:[  100] Training loss: 0.01066829, Validation loss: 0.67006095, Gradient norm: 0.27553046
INFO:root:[  101] Training loss: 0.01067937, Validation loss: 0.61192107, Gradient norm: 0.28936666
INFO:root:[  102] Training loss: 0.01061991, Validation loss: 0.62256461, Gradient norm: 0.26701861
INFO:root:[  103] Training loss: 0.01068257, Validation loss: 0.63555676, Gradient norm: 0.28494648
INFO:root:[  104] Training loss: 0.01049525, Validation loss: 0.61846530, Gradient norm: 0.24206596
INFO:root:[  105] Training loss: 0.01063160, Validation loss: 0.63660357, Gradient norm: 0.26804507
INFO:root:[  106] Training loss: 0.01063631, Validation loss: 0.60508209, Gradient norm: 0.27693888
INFO:root:[  107] Training loss: 0.01050217, Validation loss: 0.59722235, Gradient norm: 0.25818753
INFO:root:[  108] Training loss: 0.01057169, Validation loss: 0.60377753, Gradient norm: 0.27651952
INFO:root:[  109] Training loss: 0.01046776, Validation loss: 0.63155214, Gradient norm: 0.25231535
INFO:root:[  110] Training loss: 0.01059798, Validation loss: 0.62521441, Gradient norm: 0.28538398
INFO:root:[  111] Training loss: 0.01050267, Validation loss: 0.62013740, Gradient norm: 0.28756773
INFO:root:[  112] Training loss: 0.01041458, Validation loss: 0.60158587, Gradient norm: 0.26163868
INFO:root:[  113] Training loss: 0.01028499, Validation loss: 0.58346347, Gradient norm: 0.23079588
INFO:root:[  114] Training loss: 0.01029441, Validation loss: 0.59700351, Gradient norm: 0.23211932
INFO:root:[  115] Training loss: 0.01021919, Validation loss: 0.62381662, Gradient norm: 0.23420946
INFO:root:[  116] Training loss: 0.01054386, Validation loss: 0.62264011, Gradient norm: 0.29028961
INFO:root:[  117] Training loss: 0.01036376, Validation loss: 0.62212294, Gradient norm: 0.27753656
INFO:root:[  118] Training loss: 0.01052064, Validation loss: 0.58851014, Gradient norm: 0.28626169
INFO:root:[  119] Training loss: 0.01040681, Validation loss: 0.59703042, Gradient norm: 0.29089728
INFO:root:[  120] Training loss: 0.01029182, Validation loss: 0.58198664, Gradient norm: 0.25413520
INFO:root:[  121] Training loss: 0.01022295, Validation loss: 0.59091475, Gradient norm: 0.24658360
INFO:root:[  122] Training loss: 0.01026615, Validation loss: 0.60042008, Gradient norm: 0.25688024
INFO:root:[  123] Training loss: 0.01040821, Validation loss: 0.59568895, Gradient norm: 0.29395598
INFO:root:[  124] Training loss: 0.01020526, Validation loss: 0.59257500, Gradient norm: 0.24217044
INFO:root:[  125] Training loss: 0.01016045, Validation loss: 0.59535133, Gradient norm: 0.25487809
INFO:root:[  126] Training loss: 0.01014988, Validation loss: 0.57696493, Gradient norm: 0.23641225
INFO:root:[  127] Training loss: 0.01009061, Validation loss: 0.58440139, Gradient norm: 0.23347813
INFO:root:[  128] Training loss: 0.01022456, Validation loss: 0.58240902, Gradient norm: 0.25297475
INFO:root:[  129] Training loss: 0.01018202, Validation loss: 0.65330250, Gradient norm: 0.24588046
INFO:root:[  130] Training loss: 0.01020033, Validation loss: 0.57852127, Gradient norm: 0.26912920
INFO:root:[  131] Training loss: 0.01017115, Validation loss: 0.57714322, Gradient norm: 0.25067973
INFO:root:[  132] Training loss: 0.01036085, Validation loss: 0.59721419, Gradient norm: 0.28525234
INFO:root:[  133] Training loss: 0.01013399, Validation loss: 0.58848102, Gradient norm: 0.26459102
INFO:root:[  134] Training loss: 0.01010032, Validation loss: 0.58602744, Gradient norm: 0.24193699
INFO:root:[  135] Training loss: 0.01006967, Validation loss: 0.57441193, Gradient norm: 0.24852022
INFO:root:[  136] Training loss: 0.01015621, Validation loss: 0.57835788, Gradient norm: 0.26068125
INFO:root:[  137] Training loss: 0.01014366, Validation loss: 0.58541853, Gradient norm: 0.25480968
INFO:root:[  138] Training loss: 0.01006052, Validation loss: 0.58101862, Gradient norm: 0.23098470
INFO:root:[  139] Training loss: 0.01009406, Validation loss: 0.61083272, Gradient norm: 0.26468654
INFO:root:[  140] Training loss: 0.00996263, Validation loss: 0.58186175, Gradient norm: 0.23670962
INFO:root:[  141] Training loss: 0.01006329, Validation loss: 0.57619597, Gradient norm: 0.25618789
INFO:root:[  142] Training loss: 0.01009870, Validation loss: 0.59434671, Gradient norm: 0.27119372
INFO:root:[  143] Training loss: 0.01006251, Validation loss: 0.58921957, Gradient norm: 0.26074863
INFO:root:[  144] Training loss: 0.00991180, Validation loss: 0.58088110, Gradient norm: 0.22701964
INFO:root:EP 144: Early stopping
INFO:root:Training the model took 3718.317s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08441
INFO:root:EnergyScoretrain: 0.06826
INFO:root:Coveragetrain: 6.95607
INFO:root:IntervalWidthtrain: 241.6727
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02413
INFO:root:EnergyScorevalidation: 0.01923
INFO:root:Coveragevalidation: 1.85293
INFO:root:IntervalWidthvalidation: 63.5558
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04414
INFO:root:EnergyScoretest: 0.03474
INFO:root:Coveragetest: 0.62478
INFO:root:IntervalWidthtest: 59.86929
INFO:root:###14 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.05, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04316450, Validation loss: 1.72147892, Gradient norm: 0.50543654
INFO:root:[    2] Training loss: 0.02889820, Validation loss: 1.64431683, Gradient norm: 0.36113075
INFO:root:[    3] Training loss: 0.02635320, Validation loss: 1.67018825, Gradient norm: 0.35243373
INFO:root:[    4] Training loss: 0.02481265, Validation loss: 1.37609414, Gradient norm: 0.32687853
INFO:root:[    5] Training loss: 0.02383569, Validation loss: 1.28696911, Gradient norm: 0.31595792
INFO:root:[    6] Training loss: 0.02310103, Validation loss: 1.31170373, Gradient norm: 0.35146534
INFO:root:[    7] Training loss: 0.02230504, Validation loss: 1.21364078, Gradient norm: 0.31567789
INFO:root:[    8] Training loss: 0.02158201, Validation loss: 1.22490773, Gradient norm: 0.28135772
INFO:root:[    9] Training loss: 0.02100766, Validation loss: 1.20249171, Gradient norm: 0.28596337
INFO:root:[   10] Training loss: 0.02071704, Validation loss: 1.16844684, Gradient norm: 0.28493530
INFO:root:[   11] Training loss: 0.02029289, Validation loss: 1.14116802, Gradient norm: 0.29537729
INFO:root:[   12] Training loss: 0.01967044, Validation loss: 1.15017375, Gradient norm: 0.28762599
INFO:root:[   13] Training loss: 0.01936648, Validation loss: 1.13331837, Gradient norm: 0.26406268
INFO:root:[   14] Training loss: 0.01932956, Validation loss: 1.08975955, Gradient norm: 0.30942764
INFO:root:[   15] Training loss: 0.01873279, Validation loss: 1.05025054, Gradient norm: 0.25624422
INFO:root:[   16] Training loss: 0.01855544, Validation loss: 1.09655780, Gradient norm: 0.28458819
INFO:root:[   17] Training loss: 0.01806516, Validation loss: 1.03519243, Gradient norm: 0.24059762
INFO:root:[   18] Training loss: 0.01773258, Validation loss: 0.98974499, Gradient norm: 0.23628209
INFO:root:[   19] Training loss: 0.01759113, Validation loss: 0.97456356, Gradient norm: 0.26576988
INFO:root:[   20] Training loss: 0.01716410, Validation loss: 1.01989129, Gradient norm: 0.23292120
INFO:root:[   21] Training loss: 0.01711549, Validation loss: 1.06126881, Gradient norm: 0.27707946
INFO:root:[   22] Training loss: 0.01694270, Validation loss: 0.95231699, Gradient norm: 0.25603814
INFO:root:[   23] Training loss: 0.01648650, Validation loss: 0.96922064, Gradient norm: 0.22788013
INFO:root:[   24] Training loss: 0.01656020, Validation loss: 0.94609609, Gradient norm: 0.26434066
INFO:root:[   25] Training loss: 0.01648571, Validation loss: 0.91917604, Gradient norm: 0.28250493
INFO:root:[   26] Training loss: 0.01593456, Validation loss: 0.92252486, Gradient norm: 0.24760919
INFO:root:[   27] Training loss: 0.01575355, Validation loss: 0.93156260, Gradient norm: 0.23841090
INFO:root:[   28] Training loss: 0.01585745, Validation loss: 0.89853178, Gradient norm: 0.26676164
INFO:root:[   29] Training loss: 0.01551584, Validation loss: 0.96596742, Gradient norm: 0.25938547
INFO:root:[   30] Training loss: 0.01522229, Validation loss: 0.87308309, Gradient norm: 0.24571492
INFO:root:[   31] Training loss: 0.01497066, Validation loss: 0.89842288, Gradient norm: 0.23760272
INFO:root:[   32] Training loss: 0.01512573, Validation loss: 0.86685343, Gradient norm: 0.27409105
INFO:root:[   33] Training loss: 0.01465118, Validation loss: 0.82994629, Gradient norm: 0.24628345
INFO:root:[   34] Training loss: 0.01456784, Validation loss: 0.88066230, Gradient norm: 0.24443431
INFO:root:[   35] Training loss: 0.01439720, Validation loss: 0.84690517, Gradient norm: 0.24229082
INFO:root:[   36] Training loss: 0.01422349, Validation loss: 0.82008949, Gradient norm: 0.26402750
INFO:root:[   37] Training loss: 0.01415060, Validation loss: 0.78320295, Gradient norm: 0.25196346
INFO:root:[   38] Training loss: 0.01401225, Validation loss: 0.93229751, Gradient norm: 0.27030904
INFO:root:[   39] Training loss: 0.01370616, Validation loss: 0.89113517, Gradient norm: 0.25577791
INFO:root:[   40] Training loss: 0.01365078, Validation loss: 0.78708613, Gradient norm: 0.23739709
INFO:root:[   41] Training loss: 0.01351805, Validation loss: 0.77053968, Gradient norm: 0.25476665
INFO:root:[   42] Training loss: 0.01345599, Validation loss: 0.79317069, Gradient norm: 0.25866354
INFO:root:[   43] Training loss: 0.01316000, Validation loss: 0.75794405, Gradient norm: 0.24242180
INFO:root:[   44] Training loss: 0.01316116, Validation loss: 0.75600857, Gradient norm: 0.22483688
INFO:root:[   45] Training loss: 0.01308718, Validation loss: 0.75157076, Gradient norm: 0.25552610
INFO:root:[   46] Training loss: 0.01303395, Validation loss: 0.73430727, Gradient norm: 0.25131210
INFO:root:[   47] Training loss: 0.01284419, Validation loss: 0.78706656, Gradient norm: 0.22392678
INFO:root:[   48] Training loss: 0.01290220, Validation loss: 0.74624751, Gradient norm: 0.24512927
INFO:root:[   49] Training loss: 0.01272023, Validation loss: 0.74428734, Gradient norm: 0.24019743
INFO:root:[   50] Training loss: 0.01258096, Validation loss: 0.71561685, Gradient norm: 0.24168804
INFO:root:[   51] Training loss: 0.01272085, Validation loss: 0.75545475, Gradient norm: 0.25802767
INFO:root:[   52] Training loss: 0.01243773, Validation loss: 0.72208889, Gradient norm: 0.23903477
INFO:root:[   53] Training loss: 0.01235730, Validation loss: 0.70046501, Gradient norm: 0.24290768
INFO:root:[   54] Training loss: 0.01227806, Validation loss: 0.69520145, Gradient norm: 0.23488050
INFO:root:[   55] Training loss: 0.01226458, Validation loss: 0.71568593, Gradient norm: 0.23198164
INFO:root:[   56] Training loss: 0.01217973, Validation loss: 0.67871532, Gradient norm: 0.24766127
INFO:root:[   57] Training loss: 0.01229390, Validation loss: 0.68169449, Gradient norm: 0.24482453
INFO:root:[   58] Training loss: 0.01221751, Validation loss: 0.69676697, Gradient norm: 0.26869210
INFO:root:[   59] Training loss: 0.01217596, Validation loss: 0.73742325, Gradient norm: 0.24335134
INFO:root:[   60] Training loss: 0.01194117, Validation loss: 0.67475222, Gradient norm: 0.22169328
INFO:root:[   61] Training loss: 0.01195556, Validation loss: 0.68265260, Gradient norm: 0.23683957
INFO:root:[   62] Training loss: 0.01193904, Validation loss: 0.66217048, Gradient norm: 0.23495424
INFO:root:[   63] Training loss: 0.01188635, Validation loss: 0.67168369, Gradient norm: 0.26091416
INFO:root:[   64] Training loss: 0.01181024, Validation loss: 0.67166162, Gradient norm: 0.24101981
INFO:root:[   65] Training loss: 0.01176244, Validation loss: 0.67842443, Gradient norm: 0.24371351
INFO:root:[   66] Training loss: 0.01165008, Validation loss: 0.66469028, Gradient norm: 0.25012340
INFO:root:[   67] Training loss: 0.01165367, Validation loss: 0.68298041, Gradient norm: 0.24072961
INFO:root:[   68] Training loss: 0.01155841, Validation loss: 0.67460742, Gradient norm: 0.25992724
INFO:root:[   69] Training loss: 0.01151234, Validation loss: 0.66888032, Gradient norm: 0.22165685
INFO:root:[   70] Training loss: 0.01150873, Validation loss: 0.65896650, Gradient norm: 0.23877614
INFO:root:[   71] Training loss: 0.01135399, Validation loss: 0.68472519, Gradient norm: 0.21661782
INFO:root:[   72] Training loss: 0.01142484, Validation loss: 0.64155575, Gradient norm: 0.22948093
INFO:root:[   73] Training loss: 0.01150679, Validation loss: 0.65390082, Gradient norm: 0.26021747
INFO:root:[   74] Training loss: 0.01140441, Validation loss: 0.70158909, Gradient norm: 0.24334064
INFO:root:[   75] Training loss: 0.01128179, Validation loss: 0.66747455, Gradient norm: 0.22645750
INFO:root:[   76] Training loss: 0.01130416, Validation loss: 0.63571708, Gradient norm: 0.22903407
INFO:root:[   77] Training loss: 0.01146637, Validation loss: 0.65248440, Gradient norm: 0.23700222
INFO:root:[   78] Training loss: 0.01120330, Validation loss: 0.67092648, Gradient norm: 0.22891207
INFO:root:[   79] Training loss: 0.01136535, Validation loss: 0.63064076, Gradient norm: 0.25569074
INFO:root:[   80] Training loss: 0.01123450, Validation loss: 0.63331095, Gradient norm: 0.25241449
INFO:root:[   81] Training loss: 0.01118550, Validation loss: 0.64341135, Gradient norm: 0.21021048
INFO:root:[   82] Training loss: 0.01110014, Validation loss: 0.63029183, Gradient norm: 0.24110304
INFO:root:[   83] Training loss: 0.01102036, Validation loss: 0.63699890, Gradient norm: 0.22874845
INFO:root:[   84] Training loss: 0.01119414, Validation loss: 0.65397224, Gradient norm: 0.25644698
INFO:root:[   85] Training loss: 0.01098615, Validation loss: 0.64380529, Gradient norm: 0.24481562
INFO:root:[   86] Training loss: 0.01093105, Validation loss: 0.63583188, Gradient norm: 0.20688087
INFO:root:[   87] Training loss: 0.01083018, Validation loss: 0.64228071, Gradient norm: 0.20234553
INFO:root:[   88] Training loss: 0.01099301, Validation loss: 0.66573518, Gradient norm: 0.24532294
INFO:root:[   89] Training loss: 0.01083798, Validation loss: 0.64683570, Gradient norm: 0.22646170
INFO:root:[   90] Training loss: 0.01099264, Validation loss: 0.62252237, Gradient norm: 0.25445263
INFO:root:[   91] Training loss: 0.01094753, Validation loss: 0.59855530, Gradient norm: 0.24422092
INFO:root:[   92] Training loss: 0.01083186, Validation loss: 0.62458343, Gradient norm: 0.22343097
INFO:root:[   93] Training loss: 0.01075308, Validation loss: 0.64857093, Gradient norm: 0.21798224
INFO:root:[   94] Training loss: 0.01091974, Validation loss: 0.63005999, Gradient norm: 0.24569056
INFO:root:[   95] Training loss: 0.01075757, Validation loss: 0.62768246, Gradient norm: 0.20984712
INFO:root:[   96] Training loss: 0.01062071, Validation loss: 0.66289391, Gradient norm: 0.21341068
INFO:root:[   97] Training loss: 0.01075050, Validation loss: 0.61802270, Gradient norm: 0.23488807
INFO:root:[   98] Training loss: 0.01066348, Validation loss: 0.62105547, Gradient norm: 0.21678143
INFO:root:[   99] Training loss: 0.01069875, Validation loss: 0.60781528, Gradient norm: 0.22604659
INFO:root:[  100] Training loss: 0.01057138, Validation loss: 0.62164397, Gradient norm: 0.21010954
INFO:root:EP 100: Early stopping
INFO:root:Training the model took 2598.369s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.09087
INFO:root:EnergyScoretrain: 0.07386
INFO:root:Coveragetrain: 6.97395
INFO:root:IntervalWidthtrain: 275.05767
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02464
INFO:root:EnergyScorevalidation: 0.01988
INFO:root:Coveragevalidation: 1.85774
INFO:root:IntervalWidthvalidation: 71.18414
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04633
INFO:root:EnergyScoretest: 0.03601
INFO:root:Coveragetest: 0.68785
INFO:root:IntervalWidthtest: 67.89107
INFO:root:###15 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.1, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04775148, Validation loss: 1.98864761, Gradient norm: 0.49499486
INFO:root:[    2] Training loss: 0.03284697, Validation loss: 1.72958172, Gradient norm: 0.50633372
INFO:root:[    3] Training loss: 0.02908516, Validation loss: 1.63067297, Gradient norm: 0.46234778
INFO:root:[    4] Training loss: 0.02710846, Validation loss: 1.45813797, Gradient norm: 0.43271517
INFO:root:[    5] Training loss: 0.02579244, Validation loss: 1.41464271, Gradient norm: 0.45586652
INFO:root:[    6] Training loss: 0.02449406, Validation loss: 1.37692337, Gradient norm: 0.39075806
INFO:root:[    7] Training loss: 0.02426781, Validation loss: 1.42479966, Gradient norm: 0.45993118
INFO:root:[    8] Training loss: 0.02319656, Validation loss: 1.33539027, Gradient norm: 0.45437770
INFO:root:[    9] Training loss: 0.02233690, Validation loss: 1.27654618, Gradient norm: 0.40177701
INFO:root:[   10] Training loss: 0.02223381, Validation loss: 1.24149112, Gradient norm: 0.47576067
INFO:root:[   11] Training loss: 0.02135861, Validation loss: 1.18458818, Gradient norm: 0.37690247
INFO:root:[   12] Training loss: 0.02090040, Validation loss: 1.19972782, Gradient norm: 0.39519055
INFO:root:[   13] Training loss: 0.02078857, Validation loss: 1.23705592, Gradient norm: 0.45116687
INFO:root:[   14] Training loss: 0.02017475, Validation loss: 1.14983689, Gradient norm: 0.38563486
INFO:root:[   15] Training loss: 0.02006095, Validation loss: 1.10549001, Gradient norm: 0.40295425
INFO:root:[   16] Training loss: 0.01967465, Validation loss: 1.09709685, Gradient norm: 0.40364168
INFO:root:[   17] Training loss: 0.01952061, Validation loss: 1.08452456, Gradient norm: 0.42560529
INFO:root:[   18] Training loss: 0.01907705, Validation loss: 1.10673684, Gradient norm: 0.36270122
INFO:root:[   19] Training loss: 0.01882407, Validation loss: 1.09876500, Gradient norm: 0.36719818
INFO:root:[   20] Training loss: 0.01854923, Validation loss: 1.06423363, Gradient norm: 0.34746531
INFO:root:[   21] Training loss: 0.01874739, Validation loss: 1.05758792, Gradient norm: 0.39741682
INFO:root:[   22] Training loss: 0.01809509, Validation loss: 1.04503444, Gradient norm: 0.30135664
INFO:root:[   23] Training loss: 0.01807549, Validation loss: 1.02431497, Gradient norm: 0.32513992
INFO:root:[   24] Training loss: 0.01800486, Validation loss: 1.07161246, Gradient norm: 0.33673788
INFO:root:[   25] Training loss: 0.01762497, Validation loss: 0.98906781, Gradient norm: 0.32993135
INFO:root:[   26] Training loss: 0.01771717, Validation loss: 1.02590040, Gradient norm: 0.34138167
INFO:root:[   27] Training loss: 0.01783347, Validation loss: 1.01562863, Gradient norm: 0.36621543
INFO:root:[   28] Training loss: 0.01710728, Validation loss: 0.99936995, Gradient norm: 0.28886361
INFO:root:[   29] Training loss: 0.01704507, Validation loss: 0.96755152, Gradient norm: 0.33819077
INFO:root:[   30] Training loss: 0.01695028, Validation loss: 1.00460946, Gradient norm: 0.29288635
INFO:root:[   31] Training loss: 0.01683284, Validation loss: 0.97786303, Gradient norm: 0.32699466
INFO:root:[   32] Training loss: 0.01672238, Validation loss: 0.93524693, Gradient norm: 0.31401183
INFO:root:[   33] Training loss: 0.01674999, Validation loss: 0.98133285, Gradient norm: 0.35521265
INFO:root:[   34] Training loss: 0.01646089, Validation loss: 0.91736813, Gradient norm: 0.30807944
INFO:root:[   35] Training loss: 0.01626385, Validation loss: 0.92973183, Gradient norm: 0.30672404
INFO:root:[   36] Training loss: 0.01599033, Validation loss: 0.90916093, Gradient norm: 0.27498780
INFO:root:[   37] Training loss: 0.01603731, Validation loss: 1.06998000, Gradient norm: 0.33340448
INFO:root:[   38] Training loss: 0.01584460, Validation loss: 0.91688389, Gradient norm: 0.32121565
INFO:root:[   39] Training loss: 0.01575610, Validation loss: 0.94545240, Gradient norm: 0.34217268
INFO:root:[   40] Training loss: 0.01531976, Validation loss: 0.90186011, Gradient norm: 0.27033747
INFO:root:[   41] Training loss: 0.01547745, Validation loss: 0.88964148, Gradient norm: 0.31525000
INFO:root:[   42] Training loss: 0.01523884, Validation loss: 0.88049858, Gradient norm: 0.30749316
INFO:root:[   43] Training loss: 0.01512379, Validation loss: 0.87975290, Gradient norm: 0.30537216
INFO:root:[   44] Training loss: 0.01508220, Validation loss: 0.96469656, Gradient norm: 0.30344993
INFO:root:[   45] Training loss: 0.01482715, Validation loss: 0.85841905, Gradient norm: 0.28906257
INFO:root:[   46] Training loss: 0.01473329, Validation loss: 0.86550411, Gradient norm: 0.32280171
INFO:root:[   47] Training loss: 0.01466362, Validation loss: 0.87953382, Gradient norm: 0.27956444
INFO:root:[   48] Training loss: 0.01446575, Validation loss: 0.87072407, Gradient norm: 0.29579305
INFO:root:[   49] Training loss: 0.01449058, Validation loss: 0.85350485, Gradient norm: 0.30246302
INFO:root:[   50] Training loss: 0.01463026, Validation loss: 0.80776663, Gradient norm: 0.35082481
INFO:root:[   51] Training loss: 0.01425229, Validation loss: 0.83459995, Gradient norm: 0.29988199
INFO:root:[   52] Training loss: 0.01403721, Validation loss: 0.79156160, Gradient norm: 0.25760545
INFO:root:[   53] Training loss: 0.01397575, Validation loss: 0.85142682, Gradient norm: 0.27845888
INFO:root:[   54] Training loss: 0.01396153, Validation loss: 0.83541835, Gradient norm: 0.28795362
INFO:root:[   55] Training loss: 0.01403950, Validation loss: 0.80782884, Gradient norm: 0.29192058
INFO:root:[   56] Training loss: 0.01385427, Validation loss: 0.80368464, Gradient norm: 0.29526216
INFO:root:[   57] Training loss: 0.01377042, Validation loss: 0.82303204, Gradient norm: 0.29508067
INFO:root:[   58] Training loss: 0.01348044, Validation loss: 0.81627993, Gradient norm: 0.27995953
INFO:root:[   59] Training loss: 0.01360455, Validation loss: 0.76857082, Gradient norm: 0.30499342
INFO:root:[   60] Training loss: 0.01322592, Validation loss: 0.79567222, Gradient norm: 0.26093499
INFO:root:[   61] Training loss: 0.01340074, Validation loss: 0.78133673, Gradient norm: 0.27541493
INFO:root:[   62] Training loss: 0.01319717, Validation loss: 0.76320922, Gradient norm: 0.28858697
INFO:root:[   63] Training loss: 0.01323075, Validation loss: 0.73817418, Gradient norm: 0.28784817
INFO:root:[   64] Training loss: 0.01311210, Validation loss: 0.75649742, Gradient norm: 0.28365500
INFO:root:[   65] Training loss: 0.01316846, Validation loss: 0.75478827, Gradient norm: 0.29623545
INFO:root:[   66] Training loss: 0.01298765, Validation loss: 0.74434336, Gradient norm: 0.28519220
INFO:root:[   67] Training loss: 0.01281933, Validation loss: 0.73129790, Gradient norm: 0.28431095
INFO:root:[   68] Training loss: 0.01292476, Validation loss: 0.72751459, Gradient norm: 0.29601863
INFO:root:[   69] Training loss: 0.01259693, Validation loss: 0.71349723, Gradient norm: 0.27089453
INFO:root:[   70] Training loss: 0.01251568, Validation loss: 0.73538037, Gradient norm: 0.25036751
INFO:root:[   71] Training loss: 0.01257773, Validation loss: 0.73279796, Gradient norm: 0.26734462
INFO:root:[   72] Training loss: 0.01240866, Validation loss: 0.71802338, Gradient norm: 0.26222584
INFO:root:[   73] Training loss: 0.01236994, Validation loss: 0.71349701, Gradient norm: 0.26511759
INFO:root:[   74] Training loss: 0.01250236, Validation loss: 0.73428780, Gradient norm: 0.28433479
INFO:root:[   75] Training loss: 0.01223335, Validation loss: 0.71606397, Gradient norm: 0.26460487
INFO:root:[   76] Training loss: 0.01232536, Validation loss: 0.70007875, Gradient norm: 0.27671021
INFO:root:[   77] Training loss: 0.01219491, Validation loss: 0.71120493, Gradient norm: 0.26288263
INFO:root:[   78] Training loss: 0.01215427, Validation loss: 0.68783871, Gradient norm: 0.24553023
INFO:root:[   79] Training loss: 0.01205140, Validation loss: 0.69529191, Gradient norm: 0.25528789
INFO:root:[   80] Training loss: 0.01221080, Validation loss: 0.70423293, Gradient norm: 0.29790671
INFO:root:[   81] Training loss: 0.01207698, Validation loss: 0.69531508, Gradient norm: 0.28372344
INFO:root:[   82] Training loss: 0.01217888, Validation loss: 0.68637505, Gradient norm: 0.28676621
INFO:root:[   83] Training loss: 0.01193759, Validation loss: 0.70118173, Gradient norm: 0.25997925
INFO:root:[   84] Training loss: 0.01182380, Validation loss: 0.69061309, Gradient norm: 0.25499319
INFO:root:[   85] Training loss: 0.01192166, Validation loss: 0.67819090, Gradient norm: 0.28723036
INFO:root:[   86] Training loss: 0.01200451, Validation loss: 0.69222122, Gradient norm: 0.29888913
INFO:root:[   87] Training loss: 0.01174214, Validation loss: 0.66656634, Gradient norm: 0.27253001
INFO:root:[   88] Training loss: 0.01180706, Validation loss: 0.68442590, Gradient norm: 0.27749022
INFO:root:[   89] Training loss: 0.01154467, Validation loss: 0.66323972, Gradient norm: 0.22601457
INFO:root:[   90] Training loss: 0.01163161, Validation loss: 0.66297961, Gradient norm: 0.24396444
INFO:root:[   91] Training loss: 0.01162414, Validation loss: 0.74059657, Gradient norm: 0.28337545
INFO:root:[   92] Training loss: 0.01164560, Validation loss: 0.63978529, Gradient norm: 0.26614952
INFO:root:[   93] Training loss: 0.01166592, Validation loss: 0.66489311, Gradient norm: 0.27070544
INFO:root:[   94] Training loss: 0.01170550, Validation loss: 0.65526024, Gradient norm: 0.29187215
INFO:root:[   95] Training loss: 0.01149146, Validation loss: 0.68580145, Gradient norm: 0.26062025
INFO:root:[   96] Training loss: 0.01148004, Validation loss: 0.65722923, Gradient norm: 0.25012713
INFO:root:[   97] Training loss: 0.01158572, Validation loss: 0.66620306, Gradient norm: 0.27004321
INFO:root:[   98] Training loss: 0.01149610, Validation loss: 0.65363254, Gradient norm: 0.26190339
INFO:root:[   99] Training loss: 0.01143078, Validation loss: 0.68155875, Gradient norm: 0.26671384
INFO:root:[  100] Training loss: 0.01142123, Validation loss: 0.63758791, Gradient norm: 0.28067948
INFO:root:[  101] Training loss: 0.01135580, Validation loss: 0.67881554, Gradient norm: 0.26267575
INFO:root:[  102] Training loss: 0.01137434, Validation loss: 0.62971263, Gradient norm: 0.26543870
INFO:root:[  103] Training loss: 0.01129347, Validation loss: 0.65751591, Gradient norm: 0.25681587
INFO:root:[  104] Training loss: 0.01123071, Validation loss: 0.70907574, Gradient norm: 0.24468004
INFO:root:[  105] Training loss: 0.01126699, Validation loss: 0.68160070, Gradient norm: 0.25846527
INFO:root:[  106] Training loss: 0.01131161, Validation loss: 0.66009744, Gradient norm: 0.25191139
INFO:root:[  107] Training loss: 0.01119865, Validation loss: 0.66342949, Gradient norm: 0.24222887
INFO:root:[  108] Training loss: 0.01119985, Validation loss: 0.67392319, Gradient norm: 0.26545159
INFO:root:[  109] Training loss: 0.01115301, Validation loss: 0.64128446, Gradient norm: 0.25935771
INFO:root:[  110] Training loss: 0.01112235, Validation loss: 0.63834969, Gradient norm: 0.25868240
INFO:root:[  111] Training loss: 0.01109955, Validation loss: 0.62720756, Gradient norm: 0.25944990
INFO:root:[  112] Training loss: 0.01119599, Validation loss: 0.62425168, Gradient norm: 0.29298546
INFO:root:[  113] Training loss: 0.01094140, Validation loss: 0.63251245, Gradient norm: 0.23401416
INFO:root:[  114] Training loss: 0.01107082, Validation loss: 0.65406232, Gradient norm: 0.23032487
INFO:root:[  115] Training loss: 0.01105790, Validation loss: 0.63006836, Gradient norm: 0.26011598
INFO:root:[  116] Training loss: 0.01119436, Validation loss: 0.68639590, Gradient norm: 0.29770371
INFO:root:[  117] Training loss: 0.01104054, Validation loss: 0.63782530, Gradient norm: 0.26404351
INFO:root:[  118] Training loss: 0.01085333, Validation loss: 0.62604505, Gradient norm: 0.20406781
INFO:root:[  119] Training loss: 0.01103754, Validation loss: 0.62086881, Gradient norm: 0.26935049
INFO:root:[  120] Training loss: 0.01091483, Validation loss: 0.62336158, Gradient norm: 0.23423662
INFO:root:[  121] Training loss: 0.01077079, Validation loss: 0.64110291, Gradient norm: 0.21703708
INFO:root:[  122] Training loss: 0.01090093, Validation loss: 0.62820810, Gradient norm: 0.25458219
INFO:root:[  123] Training loss: 0.01087044, Validation loss: 0.61665032, Gradient norm: 0.24841067
INFO:root:[  124] Training loss: 0.01083380, Validation loss: 0.61322890, Gradient norm: 0.23364940
INFO:root:[  125] Training loss: 0.01090859, Validation loss: 0.63187465, Gradient norm: 0.25632070
INFO:root:[  126] Training loss: 0.01087512, Validation loss: 0.63519895, Gradient norm: 0.25915287
INFO:root:[  127] Training loss: 0.01079347, Validation loss: 0.67095683, Gradient norm: 0.24936091
INFO:root:[  128] Training loss: 0.01067440, Validation loss: 0.62051849, Gradient norm: 0.23663024
INFO:root:[  129] Training loss: 0.01084853, Validation loss: 0.64315623, Gradient norm: 0.27649527
INFO:root:[  130] Training loss: 0.01084588, Validation loss: 0.65012547, Gradient norm: 0.26463024
INFO:root:[  131] Training loss: 0.01078608, Validation loss: 0.64746648, Gradient norm: 0.24686399
INFO:root:[  132] Training loss: 0.01079845, Validation loss: 0.62240466, Gradient norm: 0.26239473
INFO:root:[  133] Training loss: 0.01076557, Validation loss: 0.62817513, Gradient norm: 0.24209616
INFO:root:[  134] Training loss: 0.01077275, Validation loss: 0.60747652, Gradient norm: 0.24830066
INFO:root:[  135] Training loss: 0.01059628, Validation loss: 0.61057455, Gradient norm: 0.24851684
INFO:root:[  136] Training loss: 0.01068145, Validation loss: 0.63712869, Gradient norm: 0.24112066
INFO:root:[  137] Training loss: 0.01065673, Validation loss: 0.61523102, Gradient norm: 0.25242291
INFO:root:[  138] Training loss: 0.01049959, Validation loss: 0.62563351, Gradient norm: 0.23433122
INFO:root:[  139] Training loss: 0.01066647, Validation loss: 0.61060583, Gradient norm: 0.26238619
INFO:root:[  140] Training loss: 0.01042316, Validation loss: 0.63024297, Gradient norm: 0.22674388
INFO:root:[  141] Training loss: 0.01066428, Validation loss: 0.63289130, Gradient norm: 0.27109174
INFO:root:[  142] Training loss: 0.01060086, Validation loss: 0.61001686, Gradient norm: 0.24035796
INFO:root:[  143] Training loss: 0.01050034, Validation loss: 0.59748639, Gradient norm: 0.25009193
INFO:root:[  144] Training loss: 0.01045133, Validation loss: 0.59999217, Gradient norm: 0.23862873
INFO:root:[  145] Training loss: 0.01047930, Validation loss: 0.64396276, Gradient norm: 0.23458855
INFO:root:[  146] Training loss: 0.01052172, Validation loss: 0.62661414, Gradient norm: 0.24491075
INFO:root:[  147] Training loss: 0.01048285, Validation loss: 0.61273632, Gradient norm: 0.23805050
INFO:root:[  148] Training loss: 0.01047390, Validation loss: 0.62700919, Gradient norm: 0.24567470
INFO:root:[  149] Training loss: 0.01044091, Validation loss: 0.60845522, Gradient norm: 0.24393918
INFO:root:[  150] Training loss: 0.01041937, Validation loss: 0.61853661, Gradient norm: 0.23379350
INFO:root:[  151] Training loss: 0.01040695, Validation loss: 0.60473418, Gradient norm: 0.22628526
INFO:root:[  152] Training loss: 0.01053201, Validation loss: 0.60603243, Gradient norm: 0.25435348
INFO:root:[  153] Training loss: 0.01044194, Validation loss: 0.59688678, Gradient norm: 0.23837895
INFO:root:[  154] Training loss: 0.01050005, Validation loss: 0.60748645, Gradient norm: 0.23188188
INFO:root:[  155] Training loss: 0.01041596, Validation loss: 0.62810371, Gradient norm: 0.24280899
INFO:root:[  156] Training loss: 0.01043993, Validation loss: 0.60330898, Gradient norm: 0.23062017
INFO:root:[  157] Training loss: 0.01026801, Validation loss: 0.60151151, Gradient norm: 0.21832178
INFO:root:[  158] Training loss: 0.01036663, Validation loss: 0.60113840, Gradient norm: 0.21770448
INFO:root:[  159] Training loss: 0.01033340, Validation loss: 0.59916382, Gradient norm: 0.24601939
INFO:root:[  160] Training loss: 0.01038751, Validation loss: 0.59594611, Gradient norm: 0.25053707
INFO:root:[  161] Training loss: 0.01022281, Validation loss: 0.59400704, Gradient norm: 0.22107485
INFO:root:[  162] Training loss: 0.01033845, Validation loss: 0.60451166, Gradient norm: 0.25759153
INFO:root:[  163] Training loss: 0.01039866, Validation loss: 0.58781584, Gradient norm: 0.23977388
INFO:root:[  164] Training loss: 0.01028457, Validation loss: 0.59408964, Gradient norm: 0.22269919
INFO:root:[  165] Training loss: 0.01014651, Validation loss: 0.59449696, Gradient norm: 0.20958490
INFO:root:[  166] Training loss: 0.01014816, Validation loss: 0.60884623, Gradient norm: 0.20496894
INFO:root:[  167] Training loss: 0.01033147, Validation loss: 0.61154740, Gradient norm: 0.22863718
INFO:root:[  168] Training loss: 0.01035630, Validation loss: 0.58218604, Gradient norm: 0.24891391
INFO:root:[  169] Training loss: 0.01023877, Validation loss: 0.58017362, Gradient norm: 0.23593016
INFO:root:[  170] Training loss: 0.01021061, Validation loss: 0.60128552, Gradient norm: 0.25843095
INFO:root:[  171] Training loss: 0.01008808, Validation loss: 0.60295416, Gradient norm: 0.23487776
INFO:root:[  172] Training loss: 0.01017180, Validation loss: 0.59254910, Gradient norm: 0.22764889
INFO:root:[  173] Training loss: 0.01025588, Validation loss: 0.59060660, Gradient norm: 0.26012880
INFO:root:[  174] Training loss: 0.01015194, Validation loss: 0.59779606, Gradient norm: 0.21713897
INFO:root:[  175] Training loss: 0.01014354, Validation loss: 0.60183917, Gradient norm: 0.24368163
INFO:root:[  176] Training loss: 0.01015994, Validation loss: 0.59669022, Gradient norm: 0.23884970
INFO:root:[  177] Training loss: 0.01021924, Validation loss: 0.57816592, Gradient norm: 0.25186293
INFO:root:[  178] Training loss: 0.01002215, Validation loss: 0.57391737, Gradient norm: 0.22539314
INFO:root:[  179] Training loss: 0.01019136, Validation loss: 0.58541442, Gradient norm: 0.25685151
INFO:root:[  180] Training loss: 0.01011235, Validation loss: 0.62603534, Gradient norm: 0.23140346
INFO:root:[  181] Training loss: 0.01018046, Validation loss: 0.57982182, Gradient norm: 0.25626884
INFO:root:[  182] Training loss: 0.01015680, Validation loss: 0.59120129, Gradient norm: 0.23769509
INFO:root:[  183] Training loss: 0.01011608, Validation loss: 0.61802259, Gradient norm: 0.22375905
INFO:root:[  184] Training loss: 0.01010674, Validation loss: 0.57937978, Gradient norm: 0.24611851
INFO:root:[  185] Training loss: 0.01005711, Validation loss: 0.58782583, Gradient norm: 0.23721899
INFO:root:[  186] Training loss: 0.01012589, Validation loss: 0.59424049, Gradient norm: 0.24086134
INFO:root:[  187] Training loss: 0.00993080, Validation loss: 0.58058031, Gradient norm: 0.20400073
INFO:root:EP 187: Early stopping
INFO:root:Training the model took 4836.264s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08444
INFO:root:EnergyScoretrain: 0.0697
INFO:root:Coveragetrain: 6.99167
INFO:root:IntervalWidthtrain: 270.90692
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02338
INFO:root:EnergyScorevalidation: 0.01913
INFO:root:Coveragevalidation: 1.86417
INFO:root:IntervalWidthvalidation: 71.12828
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04513
INFO:root:EnergyScoretest: 0.03558
INFO:root:Coveragetest: 0.70299
INFO:root:IntervalWidthtest: 62.96048
INFO:root:###16 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': 0.2, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04937532, Validation loss: 2.09661993, Gradient norm: 0.30757223
INFO:root:[    2] Training loss: 0.03455966, Validation loss: 1.84762118, Gradient norm: 0.28005480
INFO:root:[    3] Training loss: 0.03102976, Validation loss: 1.69651957, Gradient norm: 0.28230317
INFO:root:[    4] Training loss: 0.02848644, Validation loss: 1.59668021, Gradient norm: 0.24990225
INFO:root:[    5] Training loss: 0.02660247, Validation loss: 1.50644294, Gradient norm: 0.22970136
INFO:root:[    6] Training loss: 0.02566105, Validation loss: 1.47652540, Gradient norm: 0.26897725
INFO:root:[    7] Training loss: 0.02443086, Validation loss: 1.37769925, Gradient norm: 0.23873648
INFO:root:[    8] Training loss: 0.02397534, Validation loss: 1.46484228, Gradient norm: 0.29530989
INFO:root:[    9] Training loss: 0.02294829, Validation loss: 1.30907303, Gradient norm: 0.26655830
INFO:root:[   10] Training loss: 0.02243473, Validation loss: 1.26080981, Gradient norm: 0.25923778
INFO:root:[   11] Training loss: 0.02234501, Validation loss: 1.28556820, Gradient norm: 0.29251550
INFO:root:[   12] Training loss: 0.02141222, Validation loss: 1.22158476, Gradient norm: 0.22102980
INFO:root:[   13] Training loss: 0.02120064, Validation loss: 1.20811303, Gradient norm: 0.23903816
INFO:root:[   14] Training loss: 0.02088059, Validation loss: 1.17791621, Gradient norm: 0.24342382
INFO:root:[   15] Training loss: 0.02051475, Validation loss: 1.16056803, Gradient norm: 0.24093059
INFO:root:[   16] Training loss: 0.02016582, Validation loss: 1.12950350, Gradient norm: 0.24196134
INFO:root:[   17] Training loss: 0.01998091, Validation loss: 1.13278728, Gradient norm: 0.27763187
INFO:root:[   18] Training loss: 0.01943879, Validation loss: 1.10413752, Gradient norm: 0.25102562
INFO:root:[   19] Training loss: 0.01938164, Validation loss: 1.10889288, Gradient norm: 0.26986812
INFO:root:[   20] Training loss: 0.01882633, Validation loss: 1.07864570, Gradient norm: 0.21720444
INFO:root:[   21] Training loss: 0.01875666, Validation loss: 1.06973449, Gradient norm: 0.24501984
INFO:root:[   22] Training loss: 0.01874424, Validation loss: 1.22204939, Gradient norm: 0.26911044
INFO:root:[   23] Training loss: 0.01844682, Validation loss: 1.05523205, Gradient norm: 0.24521514
INFO:root:[   24] Training loss: 0.01817150, Validation loss: 1.05293072, Gradient norm: 0.24186650
INFO:root:[   25] Training loss: 0.01791363, Validation loss: 1.01642131, Gradient norm: 0.25192789
INFO:root:[   26] Training loss: 0.01794893, Validation loss: 1.03828467, Gradient norm: 0.26137621
INFO:root:[   27] Training loss: 0.01753541, Validation loss: 0.98963382, Gradient norm: 0.24792160
INFO:root:[   28] Training loss: 0.01775836, Validation loss: 1.02288280, Gradient norm: 0.26716034
INFO:root:[   29] Training loss: 0.01717537, Validation loss: 1.00601611, Gradient norm: 0.22527804
INFO:root:[   30] Training loss: 0.01706035, Validation loss: 1.07919353, Gradient norm: 0.22522025
INFO:root:[   31] Training loss: 0.01723155, Validation loss: 0.94739002, Gradient norm: 0.25424528
INFO:root:[   32] Training loss: 0.01692432, Validation loss: 0.98138949, Gradient norm: 0.27704489
INFO:root:[   33] Training loss: 0.01657981, Validation loss: 0.97177416, Gradient norm: 0.22479104
INFO:root:[   34] Training loss: 0.01664749, Validation loss: 0.97414957, Gradient norm: 0.23360784
INFO:root:[   35] Training loss: 0.01637751, Validation loss: 0.97356430, Gradient norm: 0.21569832
INFO:root:[   36] Training loss: 0.01635851, Validation loss: 0.95126799, Gradient norm: 0.26216850
INFO:root:[   37] Training loss: 0.01624034, Validation loss: 0.92060275, Gradient norm: 0.23896901
INFO:root:[   38] Training loss: 0.01639428, Validation loss: 0.95454078, Gradient norm: 0.27746996
INFO:root:[   39] Training loss: 0.01601091, Validation loss: 0.92740701, Gradient norm: 0.23454855
INFO:root:[   40] Training loss: 0.01594731, Validation loss: 0.93825659, Gradient norm: 0.24176750
INFO:root:[   41] Training loss: 0.01596258, Validation loss: 0.90192581, Gradient norm: 0.27934894
INFO:root:[   42] Training loss: 0.01585624, Validation loss: 0.91357304, Gradient norm: 0.23138476
INFO:root:[   43] Training loss: 0.01572860, Validation loss: 0.91751798, Gradient norm: 0.24169933
INFO:root:[   44] Training loss: 0.01552498, Validation loss: 0.92507025, Gradient norm: 0.22561216
INFO:root:[   45] Training loss: 0.01550136, Validation loss: 0.89198579, Gradient norm: 0.21882266
INFO:root:[   46] Training loss: 0.01549435, Validation loss: 0.88987016, Gradient norm: 0.24710506
INFO:root:[   47] Training loss: 0.01534900, Validation loss: 0.85991032, Gradient norm: 0.26622805
INFO:root:[   48] Training loss: 0.01516417, Validation loss: 0.89365001, Gradient norm: 0.23753710
INFO:root:[   49] Training loss: 0.01517953, Validation loss: 0.86606882, Gradient norm: 0.23531594
INFO:root:[   50] Training loss: 0.01516111, Validation loss: 0.92652730, Gradient norm: 0.22302019
INFO:root:[   51] Training loss: 0.01495577, Validation loss: 0.85850224, Gradient norm: 0.23168339
INFO:root:[   52] Training loss: 0.01501328, Validation loss: 0.86323841, Gradient norm: 0.22856642
INFO:root:[   53] Training loss: 0.01504727, Validation loss: 0.86323635, Gradient norm: 0.24618540
INFO:root:[   54] Training loss: 0.01460570, Validation loss: 0.90576395, Gradient norm: 0.21706538
INFO:root:[   55] Training loss: 0.01475416, Validation loss: 0.86281424, Gradient norm: 0.22137451
INFO:root:[   56] Training loss: 0.01473612, Validation loss: 0.86759848, Gradient norm: 0.24345351
INFO:root:[   57] Training loss: 0.01458955, Validation loss: 0.83948069, Gradient norm: 0.23827177
INFO:root:[   58] Training loss: 0.01438458, Validation loss: 0.85845632, Gradient norm: 0.22561033
INFO:root:[   59] Training loss: 0.01449633, Validation loss: 0.83450822, Gradient norm: 0.21984777
INFO:root:[   60] Training loss: 0.01448876, Validation loss: 0.86883674, Gradient norm: 0.23663327
INFO:root:[   61] Training loss: 0.01450863, Validation loss: 0.84802343, Gradient norm: 0.23674968
INFO:root:[   62] Training loss: 0.01428455, Validation loss: 0.82843621, Gradient norm: 0.22828901
INFO:root:[   63] Training loss: 0.01413386, Validation loss: 0.86851054, Gradient norm: 0.20745195
INFO:root:[   64] Training loss: 0.01442075, Validation loss: 0.82128890, Gradient norm: 0.27304931
INFO:root:[   65] Training loss: 0.01424604, Validation loss: 0.83791948, Gradient norm: 0.24179675
INFO:root:[   66] Training loss: 0.01392590, Validation loss: 0.80055281, Gradient norm: 0.21695450
INFO:root:[   67] Training loss: 0.01402772, Validation loss: 0.82570657, Gradient norm: 0.22087180
INFO:root:[   68] Training loss: 0.01390094, Validation loss: 0.79205762, Gradient norm: 0.24412686
INFO:root:[   69] Training loss: 0.01386402, Validation loss: 0.80704388, Gradient norm: 0.22757287
INFO:root:[   70] Training loss: 0.01383382, Validation loss: 0.79686265, Gradient norm: 0.23906239
INFO:root:[   71] Training loss: 0.01382093, Validation loss: 0.79574180, Gradient norm: 0.22733184
INFO:root:[   72] Training loss: 0.01361443, Validation loss: 0.79154029, Gradient norm: 0.23057033
INFO:root:[   73] Training loss: 0.01364223, Validation loss: 0.80687462, Gradient norm: 0.22844348
INFO:root:[   74] Training loss: 0.01360299, Validation loss: 0.77699127, Gradient norm: 0.22356097
INFO:root:[   75] Training loss: 0.01359606, Validation loss: 0.79777476, Gradient norm: 0.24515726
INFO:root:[   76] Training loss: 0.01327588, Validation loss: 0.82524800, Gradient norm: 0.19890403
INFO:root:[   77] Training loss: 0.01344149, Validation loss: 0.79109117, Gradient norm: 0.21661159
INFO:root:[   78] Training loss: 0.01340617, Validation loss: 0.79831027, Gradient norm: 0.22898752
INFO:root:[   79] Training loss: 0.01369994, Validation loss: 0.81332840, Gradient norm: 0.29545858
INFO:root:[   80] Training loss: 0.01307360, Validation loss: 0.74355351, Gradient norm: 0.21877627
INFO:root:[   81] Training loss: 0.01317724, Validation loss: 0.74465208, Gradient norm: 0.21754722
INFO:root:[   82] Training loss: 0.01310673, Validation loss: 0.77641526, Gradient norm: 0.21790702
INFO:root:[   83] Training loss: 0.01303924, Validation loss: 0.74519265, Gradient norm: 0.23108959
INFO:root:[   84] Training loss: 0.01299160, Validation loss: 0.75889303, Gradient norm: 0.21739846
INFO:root:[   85] Training loss: 0.01280847, Validation loss: 0.74261491, Gradient norm: 0.21069081
INFO:root:[   86] Training loss: 0.01310238, Validation loss: 0.79817756, Gradient norm: 0.24363501
INFO:root:[   87] Training loss: 0.01282577, Validation loss: 0.72071087, Gradient norm: 0.21162294
INFO:root:[   88] Training loss: 0.01275984, Validation loss: 0.76891755, Gradient norm: 0.21922425
INFO:root:[   89] Training loss: 0.01281061, Validation loss: 0.73808020, Gradient norm: 0.23943633
INFO:root:[   90] Training loss: 0.01265659, Validation loss: 0.75334322, Gradient norm: 0.23579061
INFO:root:[   91] Training loss: 0.01255748, Validation loss: 0.71376326, Gradient norm: 0.22643520
INFO:root:[   92] Training loss: 0.01259034, Validation loss: 0.75395824, Gradient norm: 0.21574303
INFO:root:[   93] Training loss: 0.01258657, Validation loss: 0.75949085, Gradient norm: 0.22421150
INFO:root:[   94] Training loss: 0.01245028, Validation loss: 0.72754843, Gradient norm: 0.22027787
INFO:root:[   95] Training loss: 0.01243744, Validation loss: 0.73148724, Gradient norm: 0.23920708
INFO:root:[   96] Training loss: 0.01221259, Validation loss: 0.72906487, Gradient norm: 0.21786694
INFO:root:[   97] Training loss: 0.01235205, Validation loss: 0.70809846, Gradient norm: 0.23515172
INFO:root:[   98] Training loss: 0.01225345, Validation loss: 0.70083531, Gradient norm: 0.22767459
INFO:root:[   99] Training loss: 0.01211064, Validation loss: 0.69391080, Gradient norm: 0.20970876
INFO:root:[  100] Training loss: 0.01217093, Validation loss: 0.77395286, Gradient norm: 0.23124705
INFO:root:[  101] Training loss: 0.01215701, Validation loss: 0.76503469, Gradient norm: 0.23124782
INFO:root:[  102] Training loss: 0.01195152, Validation loss: 0.70215307, Gradient norm: 0.22388944
INFO:root:[  103] Training loss: 0.01182284, Validation loss: 0.71321206, Gradient norm: 0.20100496
INFO:root:[  104] Training loss: 0.01187090, Validation loss: 0.66552533, Gradient norm: 0.21792480
INFO:root:[  105] Training loss: 0.01193160, Validation loss: 0.66145079, Gradient norm: 0.23865752
INFO:root:[  106] Training loss: 0.01181784, Validation loss: 0.73640847, Gradient norm: 0.21450632
INFO:root:[  107] Training loss: 0.01181944, Validation loss: 0.72652221, Gradient norm: 0.22733932
INFO:root:[  108] Training loss: 0.01184569, Validation loss: 0.67647977, Gradient norm: 0.24224762
INFO:root:[  109] Training loss: 0.01166880, Validation loss: 0.67414385, Gradient norm: 0.21861128
INFO:root:[  110] Training loss: 0.01185516, Validation loss: 0.67446392, Gradient norm: 0.25170233
INFO:root:[  111] Training loss: 0.01157016, Validation loss: 0.67319567, Gradient norm: 0.22496806
INFO:root:[  112] Training loss: 0.01157533, Validation loss: 0.67052888, Gradient norm: 0.21503812
INFO:root:[  113] Training loss: 0.01152426, Validation loss: 0.70033139, Gradient norm: 0.20657081
INFO:root:[  114] Training loss: 0.01158346, Validation loss: 0.66787952, Gradient norm: 0.23237940
INFO:root:[  115] Training loss: 0.01128429, Validation loss: 0.64989725, Gradient norm: 0.19964376
INFO:root:[  116] Training loss: 0.01144011, Validation loss: 0.64227380, Gradient norm: 0.22643748
INFO:root:[  117] Training loss: 0.01160864, Validation loss: 0.66980504, Gradient norm: 0.24664945
INFO:root:[  118] Training loss: 0.01138351, Validation loss: 0.70062742, Gradient norm: 0.22995017
INFO:root:[  119] Training loss: 0.01142427, Validation loss: 0.65423385, Gradient norm: 0.22434559
INFO:root:[  120] Training loss: 0.01137325, Validation loss: 0.76205543, Gradient norm: 0.24853715
INFO:root:[  121] Training loss: 0.01140048, Validation loss: 0.68866459, Gradient norm: 0.23528834
INFO:root:[  122] Training loss: 0.01137332, Validation loss: 0.64910707, Gradient norm: 0.24206222
INFO:root:[  123] Training loss: 0.01116517, Validation loss: 0.64872767, Gradient norm: 0.20120226
INFO:root:[  124] Training loss: 0.01127349, Validation loss: 0.64236113, Gradient norm: 0.21293781
INFO:root:[  125] Training loss: 0.01132547, Validation loss: 0.65951531, Gradient norm: 0.23741538
INFO:root:[  126] Training loss: 0.01109367, Validation loss: 0.63191835, Gradient norm: 0.20756405
INFO:root:[  127] Training loss: 0.01123133, Validation loss: 0.69359816, Gradient norm: 0.22007036
INFO:root:[  128] Training loss: 0.01105536, Validation loss: 0.62860146, Gradient norm: 0.21940871
INFO:root:[  129] Training loss: 0.01118890, Validation loss: 0.63115014, Gradient norm: 0.22711305
INFO:root:[  130] Training loss: 0.01128610, Validation loss: 0.62603660, Gradient norm: 0.23992625
INFO:root:[  131] Training loss: 0.01111117, Validation loss: 0.64074842, Gradient norm: 0.23240382
INFO:root:[  132] Training loss: 0.01116229, Validation loss: 0.65049300, Gradient norm: 0.23092487
INFO:root:[  133] Training loss: 0.01100472, Validation loss: 0.64190062, Gradient norm: 0.20909302
INFO:root:[  134] Training loss: 0.01089660, Validation loss: 0.62742762, Gradient norm: 0.20325278
INFO:root:[  135] Training loss: 0.01111475, Validation loss: 0.63549269, Gradient norm: 0.24368881
INFO:root:[  136] Training loss: 0.01083007, Validation loss: 0.64042890, Gradient norm: 0.22097429
INFO:root:[  137] Training loss: 0.01092269, Validation loss: 0.62711385, Gradient norm: 0.21183089
INFO:root:[  138] Training loss: 0.01086255, Validation loss: 0.63887716, Gradient norm: 0.23056567
INFO:root:[  139] Training loss: 0.01107952, Validation loss: 0.66412622, Gradient norm: 0.23442367
INFO:root:EP 139: Early stopping
INFO:root:Training the model took 3648.163s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.0917
INFO:root:EnergyScoretrain: 0.07621
INFO:root:Coveragetrain: 6.98258
INFO:root:IntervalWidthtrain: 280.14179
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02537
INFO:root:EnergyScorevalidation: 0.02093
INFO:root:Coveragevalidation: 1.8603
INFO:root:IntervalWidthvalidation: 73.13453
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04661
INFO:root:EnergyScoretest: 0.03627
INFO:root:Coveragetest: 0.78575
INFO:root:IntervalWidthtest: 70.16016
INFO:root:###17 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.01, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06374102, Validation loss: 2.63211755, Gradient norm: 0.66429402
INFO:root:[    2] Training loss: 0.04414275, Validation loss: 2.02224056, Gradient norm: 0.64484934
INFO:root:[    3] Training loss: 0.04006026, Validation loss: 1.91439061, Gradient norm: 0.65608036
INFO:root:[    4] Training loss: 0.03712591, Validation loss: 1.74870299, Gradient norm: 0.61499255
INFO:root:[    5] Training loss: 0.03524166, Validation loss: 1.73185239, Gradient norm: 0.53688681
INFO:root:[    6] Training loss: 0.03374516, Validation loss: 1.59980116, Gradient norm: 0.51864356
INFO:root:[    7] Training loss: 0.03293453, Validation loss: 1.61612752, Gradient norm: 0.45841878
INFO:root:[    8] Training loss: 0.03224968, Validation loss: 1.56047723, Gradient norm: 0.52359075
INFO:root:[    9] Training loss: 0.03117142, Validation loss: 1.55588936, Gradient norm: 0.44716954
INFO:root:[   10] Training loss: 0.03060646, Validation loss: 1.52605398, Gradient norm: 0.46205760
INFO:root:[   11] Training loss: 0.03008099, Validation loss: 1.61684422, Gradient norm: 0.46152356
INFO:root:[   12] Training loss: 0.02955069, Validation loss: 1.46672288, Gradient norm: 0.41411032
INFO:root:[   13] Training loss: 0.02875168, Validation loss: 1.41094345, Gradient norm: 0.37977168
INFO:root:[   14] Training loss: 0.02855006, Validation loss: 1.46073334, Gradient norm: 0.46807309
INFO:root:[   15] Training loss: 0.02840659, Validation loss: 1.42993603, Gradient norm: 0.44893142
INFO:root:[   16] Training loss: 0.02739715, Validation loss: 1.41262399, Gradient norm: 0.38569971
INFO:root:[   17] Training loss: 0.02692176, Validation loss: 1.35879167, Gradient norm: 0.37460828
INFO:root:[   18] Training loss: 0.02680844, Validation loss: 1.29770527, Gradient norm: 0.39889724
INFO:root:[   19] Training loss: 0.02675740, Validation loss: 1.33518147, Gradient norm: 0.45765258
INFO:root:[   20] Training loss: 0.02614227, Validation loss: 1.35487147, Gradient norm: 0.38052709
INFO:root:[   21] Training loss: 0.02609450, Validation loss: 1.32151662, Gradient norm: 0.41123889
INFO:root:[   22] Training loss: 0.02559237, Validation loss: 1.26493825, Gradient norm: 0.38013748
INFO:root:[   23] Training loss: 0.02525909, Validation loss: 1.28207050, Gradient norm: 0.39995366
INFO:root:[   24] Training loss: 0.02525780, Validation loss: 1.23478553, Gradient norm: 0.44631690
INFO:root:[   25] Training loss: 0.02469692, Validation loss: 1.27518028, Gradient norm: 0.38633284
INFO:root:[   26] Training loss: 0.02447822, Validation loss: 1.31108590, Gradient norm: 0.35261062
INFO:root:[   27] Training loss: 0.02443649, Validation loss: 1.19601986, Gradient norm: 0.40518498
INFO:root:[   28] Training loss: 0.02410083, Validation loss: 1.19935261, Gradient norm: 0.40914822
INFO:root:[   29] Training loss: 0.02391561, Validation loss: 1.21065753, Gradient norm: 0.41126690
INFO:root:[   30] Training loss: 0.02371200, Validation loss: 1.23775306, Gradient norm: 0.41284594
INFO:root:[   31] Training loss: 0.02373334, Validation loss: 1.16914441, Gradient norm: 0.44638615
INFO:root:[   32] Training loss: 0.02318080, Validation loss: 1.15770650, Gradient norm: 0.36070822
INFO:root:[   33] Training loss: 0.02296821, Validation loss: 1.17780015, Gradient norm: 0.38416363
INFO:root:[   34] Training loss: 0.02285180, Validation loss: 1.20538632, Gradient norm: 0.37849782
INFO:root:[   35] Training loss: 0.02266706, Validation loss: 1.20240057, Gradient norm: 0.39850802
INFO:root:[   36] Training loss: 0.02227643, Validation loss: 1.18949743, Gradient norm: 0.34536374
INFO:root:[   37] Training loss: 0.02245081, Validation loss: 1.21599795, Gradient norm: 0.38985585
INFO:root:[   38] Training loss: 0.02192999, Validation loss: 1.27450878, Gradient norm: 0.34858836
INFO:root:[   39] Training loss: 0.02214468, Validation loss: 1.10733357, Gradient norm: 0.43103078
INFO:root:[   40] Training loss: 0.02169227, Validation loss: 1.13760914, Gradient norm: 0.36543261
INFO:root:[   41] Training loss: 0.02171805, Validation loss: 1.09812823, Gradient norm: 0.38832353
INFO:root:[   42] Training loss: 0.02157122, Validation loss: 1.09502579, Gradient norm: 0.39731317
INFO:root:[   43] Training loss: 0.02124289, Validation loss: 1.11694260, Gradient norm: 0.35136486
INFO:root:[   44] Training loss: 0.02131759, Validation loss: 1.08259200, Gradient norm: 0.39084882
INFO:root:[   45] Training loss: 0.02111126, Validation loss: 1.06133041, Gradient norm: 0.40443503
INFO:root:[   46] Training loss: 0.02094522, Validation loss: 1.04591874, Gradient norm: 0.37222047
INFO:root:[   47] Training loss: 0.02076267, Validation loss: 1.07605292, Gradient norm: 0.38089715
INFO:root:[   48] Training loss: 0.02052663, Validation loss: 1.00880679, Gradient norm: 0.39300813
INFO:root:[   49] Training loss: 0.02031853, Validation loss: 1.02126449, Gradient norm: 0.37142004
INFO:root:[   50] Training loss: 0.02017709, Validation loss: 1.01741574, Gradient norm: 0.39413962
INFO:root:[   51] Training loss: 0.01994905, Validation loss: 0.97670725, Gradient norm: 0.36521700
INFO:root:[   52] Training loss: 0.01978788, Validation loss: 1.10781039, Gradient norm: 0.35832360
INFO:root:[   53] Training loss: 0.01974566, Validation loss: 1.00379568, Gradient norm: 0.41016471
INFO:root:[   54] Training loss: 0.01946741, Validation loss: 0.95904290, Gradient norm: 0.36535751
INFO:root:[   55] Training loss: 0.01939309, Validation loss: 0.98063237, Gradient norm: 0.37295964
INFO:root:[   56] Training loss: 0.01933103, Validation loss: 1.02401751, Gradient norm: 0.38868967
INFO:root:[   57] Training loss: 0.01905181, Validation loss: 0.96282695, Gradient norm: 0.39595852
INFO:root:[   58] Training loss: 0.01884666, Validation loss: 0.90896222, Gradient norm: 0.37978738
INFO:root:[   59] Training loss: 0.01879435, Validation loss: 0.93519816, Gradient norm: 0.41579530
INFO:root:[   60] Training loss: 0.01853345, Validation loss: 0.96532426, Gradient norm: 0.35631106
INFO:root:[   61] Training loss: 0.01836983, Validation loss: 0.91417028, Gradient norm: 0.38457977
INFO:root:[   62] Training loss: 0.01841308, Validation loss: 0.92735475, Gradient norm: 0.41607332
INFO:root:[   63] Training loss: 0.01783572, Validation loss: 0.93552394, Gradient norm: 0.35908212
INFO:root:[   64] Training loss: 0.01800225, Validation loss: 0.92794926, Gradient norm: 0.41028056
INFO:root:[   65] Training loss: 0.01780520, Validation loss: 0.89153612, Gradient norm: 0.39357906
INFO:root:[   66] Training loss: 0.01762079, Validation loss: 1.01551813, Gradient norm: 0.39633638
INFO:root:[   67] Training loss: 0.01762947, Validation loss: 0.87988550, Gradient norm: 0.37754280
INFO:root:[   68] Training loss: 0.01752807, Validation loss: 0.84860242, Gradient norm: 0.38750838
INFO:root:[   69] Training loss: 0.01722533, Validation loss: 0.87752735, Gradient norm: 0.36866468
INFO:root:[   70] Training loss: 0.01698348, Validation loss: 0.86308528, Gradient norm: 0.36124656
INFO:root:[   71] Training loss: 0.01720795, Validation loss: 0.88593263, Gradient norm: 0.42403123
INFO:root:[   72] Training loss: 0.01732632, Validation loss: 0.84869775, Gradient norm: 0.44660261
INFO:root:[   73] Training loss: 0.01675282, Validation loss: 0.81691413, Gradient norm: 0.34963738
INFO:root:[   74] Training loss: 0.01668423, Validation loss: 0.85922864, Gradient norm: 0.38787958
INFO:root:[   75] Training loss: 0.01684444, Validation loss: 0.83260182, Gradient norm: 0.41128363
INFO:root:[   76] Training loss: 0.01657953, Validation loss: 0.89697287, Gradient norm: 0.39172236
INFO:root:[   77] Training loss: 0.01656572, Validation loss: 0.83529676, Gradient norm: 0.37674947
INFO:root:[   78] Training loss: 0.01636694, Validation loss: 0.79235065, Gradient norm: 0.35483991
INFO:root:[   79] Training loss: 0.01619701, Validation loss: 0.83013265, Gradient norm: 0.35818431
INFO:root:[   80] Training loss: 0.01657162, Validation loss: 0.80275126, Gradient norm: 0.43397870
INFO:root:[   81] Training loss: 0.01616086, Validation loss: 0.84539669, Gradient norm: 0.37523122
INFO:root:[   82] Training loss: 0.01649182, Validation loss: 0.87014089, Gradient norm: 0.42166558
INFO:root:[   83] Training loss: 0.01623201, Validation loss: 0.77966319, Gradient norm: 0.41938402
INFO:root:[   84] Training loss: 0.01599367, Validation loss: 0.78091334, Gradient norm: 0.37003125
INFO:root:[   85] Training loss: 0.01592201, Validation loss: 0.79697508, Gradient norm: 0.36586434
INFO:root:[   86] Training loss: 0.01602630, Validation loss: 0.85060114, Gradient norm: 0.39354544
INFO:root:[   87] Training loss: 0.01606451, Validation loss: 0.79090368, Gradient norm: 0.44829441
INFO:root:[   88] Training loss: 0.01573478, Validation loss: 0.79251071, Gradient norm: 0.37417976
INFO:root:[   89] Training loss: 0.01589710, Validation loss: 0.77938601, Gradient norm: 0.40690254
INFO:root:[   90] Training loss: 0.01580961, Validation loss: 0.85434450, Gradient norm: 0.39782655
INFO:root:[   91] Training loss: 0.01574726, Validation loss: 0.82414817, Gradient norm: 0.41277848
INFO:root:[   92] Training loss: 0.01580574, Validation loss: 0.77564120, Gradient norm: 0.44880911
INFO:root:[   93] Training loss: 0.01556488, Validation loss: 0.76727890, Gradient norm: 0.38202397
INFO:root:[   94] Training loss: 0.01547107, Validation loss: 0.74700028, Gradient norm: 0.39191594
INFO:root:[   95] Training loss: 0.01568558, Validation loss: 0.75137868, Gradient norm: 0.40492004
INFO:root:[   96] Training loss: 0.01530316, Validation loss: 0.83149548, Gradient norm: 0.36243528
INFO:root:[   97] Training loss: 0.01524910, Validation loss: 0.76974976, Gradient norm: 0.35896236
INFO:root:[   98] Training loss: 0.01547889, Validation loss: 0.84797130, Gradient norm: 0.39335145
INFO:root:[   99] Training loss: 0.01532416, Validation loss: 0.82922215, Gradient norm: 0.39120106
INFO:root:[  100] Training loss: 0.01538462, Validation loss: 0.76812338, Gradient norm: 0.43818583
INFO:root:[  101] Training loss: 0.01514024, Validation loss: 0.77723580, Gradient norm: 0.38520067
INFO:root:[  102] Training loss: 0.01534386, Validation loss: 0.96135294, Gradient norm: 0.41767680
INFO:root:[  103] Training loss: 0.01531581, Validation loss: 0.74552375, Gradient norm: 0.45427370
INFO:root:[  104] Training loss: 0.01515375, Validation loss: 0.77476201, Gradient norm: 0.42303548
INFO:root:[  105] Training loss: 0.01510990, Validation loss: 0.74279844, Gradient norm: 0.38817997
INFO:root:[  106] Training loss: 0.01499165, Validation loss: 0.73623387, Gradient norm: 0.39289102
INFO:root:[  107] Training loss: 0.01493794, Validation loss: 0.80992484, Gradient norm: 0.36020339
INFO:root:[  108] Training loss: 0.01511252, Validation loss: 0.76536011, Gradient norm: 0.40444466
INFO:root:[  109] Training loss: 0.01523821, Validation loss: 0.75176960, Gradient norm: 0.46108900
INFO:root:[  110] Training loss: 0.01491434, Validation loss: 0.73981052, Gradient norm: 0.38868169
INFO:root:[  111] Training loss: 0.01478517, Validation loss: 0.77472924, Gradient norm: 0.37379842
INFO:root:[  112] Training loss: 0.01494326, Validation loss: 0.76648269, Gradient norm: 0.40715517
INFO:root:[  113] Training loss: 0.01472315, Validation loss: 0.73135908, Gradient norm: 0.37478144
INFO:root:[  114] Training loss: 0.01484056, Validation loss: 0.78497962, Gradient norm: 0.42087421
INFO:root:[  115] Training loss: 0.01460576, Validation loss: 0.70788828, Gradient norm: 0.37624392
INFO:root:[  116] Training loss: 0.01460942, Validation loss: 0.75465041, Gradient norm: 0.37825496
INFO:root:[  117] Training loss: 0.01479909, Validation loss: 0.77218899, Gradient norm: 0.42995518
INFO:root:[  118] Training loss: 0.01466813, Validation loss: 0.72677645, Gradient norm: 0.38797748
INFO:root:[  119] Training loss: 0.01467190, Validation loss: 0.73325491, Gradient norm: 0.40969507
INFO:root:[  120] Training loss: 0.01449659, Validation loss: 0.70562918, Gradient norm: 0.36739350
INFO:root:[  121] Training loss: 0.01464480, Validation loss: 0.73068445, Gradient norm: 0.40827245
INFO:root:[  122] Training loss: 0.01473604, Validation loss: 0.80891562, Gradient norm: 0.42622068
INFO:root:[  123] Training loss: 0.01442861, Validation loss: 0.71305924, Gradient norm: 0.37008203
INFO:root:[  124] Training loss: 0.01464985, Validation loss: 0.74986196, Gradient norm: 0.43598816
INFO:root:[  125] Training loss: 0.01436915, Validation loss: 0.79128361, Gradient norm: 0.38344174
INFO:root:[  126] Training loss: 0.01442136, Validation loss: 0.71679818, Gradient norm: 0.39480484
INFO:root:[  127] Training loss: 0.01440491, Validation loss: 0.76295305, Gradient norm: 0.40207654
INFO:root:[  128] Training loss: 0.01443900, Validation loss: 0.69832042, Gradient norm: 0.38152281
INFO:root:[  129] Training loss: 0.01438486, Validation loss: 0.71515003, Gradient norm: 0.39538581
INFO:root:[  130] Training loss: 0.01440229, Validation loss: 0.74249479, Gradient norm: 0.39536352
INFO:root:[  131] Training loss: 0.01421600, Validation loss: 0.76777052, Gradient norm: 0.40105972
INFO:root:[  132] Training loss: 0.01431969, Validation loss: 0.75104647, Gradient norm: 0.40277780
INFO:root:[  133] Training loss: 0.01431530, Validation loss: 0.80760889, Gradient norm: 0.43405126
INFO:root:[  134] Training loss: 0.01429955, Validation loss: 0.90325711, Gradient norm: 0.40771803
INFO:root:[  135] Training loss: 0.01419289, Validation loss: 0.82373128, Gradient norm: 0.39187587
INFO:root:[  136] Training loss: 0.01411784, Validation loss: 0.73588265, Gradient norm: 0.37952026
INFO:root:[  137] Training loss: 0.01404714, Validation loss: 0.70116031, Gradient norm: 0.34582574
INFO:root:EP 137: Early stopping
INFO:root:Training the model took 1340.746s.
INFO:root:Emptying the cuda cache took 0.005s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08423
INFO:root:EnergyScoretrain: 0.07196
INFO:root:Coveragetrain: 1.74597
INFO:root:IntervalWidthtrain: 16.02647
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02375
INFO:root:EnergyScorevalidation: 0.02048
INFO:root:Coveragevalidation: 0.43771
INFO:root:IntervalWidthvalidation: 4.0736
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.0465
INFO:root:EnergyScoretest: 0.04428
INFO:root:Coveragetest: 0.0674
INFO:root:IntervalWidthtest: 4.50897
INFO:root:###18 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.05, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07116235, Validation loss: 2.60164296, Gradient norm: 0.48408926
INFO:root:[    2] Training loss: 0.05074133, Validation loss: 2.34134608, Gradient norm: 0.44200160
INFO:root:[    3] Training loss: 0.04662032, Validation loss: 2.19863293, Gradient norm: 0.45321916
INFO:root:[    4] Training loss: 0.04397953, Validation loss: 2.04953838, Gradient norm: 0.43518012
INFO:root:[    5] Training loss: 0.04083716, Validation loss: 1.93915423, Gradient norm: 0.36575217
INFO:root:[    6] Training loss: 0.03874783, Validation loss: 1.90420020, Gradient norm: 0.46991617
INFO:root:[    7] Training loss: 0.03699985, Validation loss: 1.72738052, Gradient norm: 0.44465824
INFO:root:[    8] Training loss: 0.03576235, Validation loss: 1.62445057, Gradient norm: 0.47280496
INFO:root:[    9] Training loss: 0.03458147, Validation loss: 1.58823937, Gradient norm: 0.48522231
INFO:root:[   10] Training loss: 0.03339190, Validation loss: 1.53407493, Gradient norm: 0.39986908
INFO:root:[   11] Training loss: 0.03302512, Validation loss: 1.57291821, Gradient norm: 0.48377389
INFO:root:[   12] Training loss: 0.03184497, Validation loss: 1.52370709, Gradient norm: 0.43928026
INFO:root:[   13] Training loss: 0.03184249, Validation loss: 1.58146599, Gradient norm: 0.50218083
INFO:root:[   14] Training loss: 0.03100611, Validation loss: 1.41760669, Gradient norm: 0.49868200
INFO:root:[   15] Training loss: 0.03015432, Validation loss: 1.46528979, Gradient norm: 0.42624937
INFO:root:[   16] Training loss: 0.02971897, Validation loss: 1.43153561, Gradient norm: 0.45232491
INFO:root:[   17] Training loss: 0.02908646, Validation loss: 1.51638085, Gradient norm: 0.41217339
INFO:root:[   18] Training loss: 0.02909242, Validation loss: 1.37287473, Gradient norm: 0.47242085
INFO:root:[   19] Training loss: 0.02841778, Validation loss: 1.32553397, Gradient norm: 0.41298232
INFO:root:[   20] Training loss: 0.02772244, Validation loss: 1.31710017, Gradient norm: 0.43790913
INFO:root:[   21] Training loss: 0.02728031, Validation loss: 1.29514092, Gradient norm: 0.40717409
INFO:root:[   22] Training loss: 0.02687681, Validation loss: 1.28941486, Gradient norm: 0.37186949
INFO:root:[   23] Training loss: 0.02665330, Validation loss: 1.25443767, Gradient norm: 0.38642340
INFO:root:[   24] Training loss: 0.02658687, Validation loss: 1.31363647, Gradient norm: 0.42867450
INFO:root:[   25] Training loss: 0.02617917, Validation loss: 1.23179428, Gradient norm: 0.39373601
INFO:root:[   26] Training loss: 0.02580243, Validation loss: 1.28319155, Gradient norm: 0.37694259
INFO:root:[   27] Training loss: 0.02524532, Validation loss: 1.22378128, Gradient norm: 0.35489081
INFO:root:[   28] Training loss: 0.02544099, Validation loss: 1.28904065, Gradient norm: 0.41546853
INFO:root:[   29] Training loss: 0.02487463, Validation loss: 1.21219450, Gradient norm: 0.37975982
INFO:root:[   30] Training loss: 0.02451323, Validation loss: 1.18186794, Gradient norm: 0.35302199
INFO:root:[   31] Training loss: 0.02431663, Validation loss: 1.20414123, Gradient norm: 0.39269425
INFO:root:[   32] Training loss: 0.02392498, Validation loss: 1.20852117, Gradient norm: 0.33918305
INFO:root:[   33] Training loss: 0.02384845, Validation loss: 1.19113434, Gradient norm: 0.35689073
INFO:root:[   34] Training loss: 0.02359692, Validation loss: 1.14285449, Gradient norm: 0.40566693
INFO:root:[   35] Training loss: 0.02348806, Validation loss: 1.18353161, Gradient norm: 0.39356973
INFO:root:[   36] Training loss: 0.02319094, Validation loss: 1.13831510, Gradient norm: 0.35441274
INFO:root:[   37] Training loss: 0.02287554, Validation loss: 1.13345977, Gradient norm: 0.35897326
INFO:root:[   38] Training loss: 0.02274456, Validation loss: 1.08453754, Gradient norm: 0.38432478
INFO:root:[   39] Training loss: 0.02253513, Validation loss: 1.10408518, Gradient norm: 0.35422624
INFO:root:[   40] Training loss: 0.02224737, Validation loss: 1.09355499, Gradient norm: 0.37128298
INFO:root:[   41] Training loss: 0.02221585, Validation loss: 1.14031301, Gradient norm: 0.38654072
INFO:root:[   42] Training loss: 0.02213890, Validation loss: 1.13331676, Gradient norm: 0.43445646
INFO:root:[   43] Training loss: 0.02206564, Validation loss: 1.05409917, Gradient norm: 0.40842453
INFO:root:[   44] Training loss: 0.02141073, Validation loss: 1.03364321, Gradient norm: 0.34994550
INFO:root:[   45] Training loss: 0.02156227, Validation loss: 1.11313524, Gradient norm: 0.38152995
INFO:root:[   46] Training loss: 0.02134605, Validation loss: 1.06999968, Gradient norm: 0.39536011
INFO:root:[   47] Training loss: 0.02119750, Validation loss: 1.04137189, Gradient norm: 0.40305941
INFO:root:[   48] Training loss: 0.02093422, Validation loss: 0.99183200, Gradient norm: 0.38112118
INFO:root:[   49] Training loss: 0.02073781, Validation loss: 1.03707243, Gradient norm: 0.34796348
INFO:root:[   50] Training loss: 0.02059167, Validation loss: 1.11776850, Gradient norm: 0.38420552
INFO:root:[   51] Training loss: 0.02054416, Validation loss: 0.97442174, Gradient norm: 0.38398704
INFO:root:[   52] Training loss: 0.02032666, Validation loss: 0.99088807, Gradient norm: 0.38431482
INFO:root:[   53] Training loss: 0.02040561, Validation loss: 0.98875276, Gradient norm: 0.41389585
INFO:root:[   54] Training loss: 0.01994882, Validation loss: 1.02443810, Gradient norm: 0.36295420
INFO:root:[   55] Training loss: 0.01988848, Validation loss: 0.98117272, Gradient norm: 0.39850280
INFO:root:[   56] Training loss: 0.01965688, Validation loss: 1.03998179, Gradient norm: 0.38291135
INFO:root:[   57] Training loss: 0.01970362, Validation loss: 1.08767921, Gradient norm: 0.43684123
INFO:root:[   58] Training loss: 0.01961293, Validation loss: 0.97763756, Gradient norm: 0.46303337
INFO:root:[   59] Training loss: 0.01926994, Validation loss: 0.97810951, Gradient norm: 0.37578831
INFO:root:[   60] Training loss: 0.01911397, Validation loss: 1.03338200, Gradient norm: 0.39403838
INFO:root:[   61] Training loss: 0.01921069, Validation loss: 0.96383165, Gradient norm: 0.39240116
INFO:root:[   62] Training loss: 0.01902532, Validation loss: 0.88673836, Gradient norm: 0.40612854
INFO:root:[   63] Training loss: 0.01902026, Validation loss: 1.00779498, Gradient norm: 0.45361401
INFO:root:[   64] Training loss: 0.01882679, Validation loss: 0.90350453, Gradient norm: 0.43257396
INFO:root:[   65] Training loss: 0.01841614, Validation loss: 0.94290026, Gradient norm: 0.40014205
INFO:root:[   66] Training loss: 0.01843895, Validation loss: 0.87305528, Gradient norm: 0.40767294
INFO:root:[   67] Training loss: 0.01824417, Validation loss: 0.92215087, Gradient norm: 0.39160364
INFO:root:[   68] Training loss: 0.01812946, Validation loss: 0.90866382, Gradient norm: 0.40235078
INFO:root:[   69] Training loss: 0.01795036, Validation loss: 0.96700940, Gradient norm: 0.36433626
INFO:root:[   70] Training loss: 0.01797663, Validation loss: 0.83907141, Gradient norm: 0.38603290
INFO:root:[   71] Training loss: 0.01788155, Validation loss: 0.93759828, Gradient norm: 0.41551651
INFO:root:[   72] Training loss: 0.01795398, Validation loss: 0.89263960, Gradient norm: 0.40463983
INFO:root:[   73] Training loss: 0.01736279, Validation loss: 0.90153334, Gradient norm: 0.34501211
INFO:root:[   74] Training loss: 0.01772683, Validation loss: 0.82846405, Gradient norm: 0.42887626
INFO:root:[   75] Training loss: 0.01742241, Validation loss: 0.80051250, Gradient norm: 0.38485459
INFO:root:[   76] Training loss: 0.01746254, Validation loss: 0.82842218, Gradient norm: 0.43227322
INFO:root:[   77] Training loss: 0.01729914, Validation loss: 0.81125752, Gradient norm: 0.43724767
INFO:root:[   78] Training loss: 0.01722242, Validation loss: 0.83363105, Gradient norm: 0.41121079
INFO:root:[   79] Training loss: 0.01713568, Validation loss: 0.80703895, Gradient norm: 0.40043924
INFO:root:[   80] Training loss: 0.01683349, Validation loss: 0.82841120, Gradient norm: 0.37691317
INFO:root:[   81] Training loss: 0.01738739, Validation loss: 0.78611387, Gradient norm: 0.44129397
INFO:root:[   82] Training loss: 0.01692494, Validation loss: 0.88732548, Gradient norm: 0.39408154
INFO:root:[   83] Training loss: 0.01683481, Validation loss: 0.84412332, Gradient norm: 0.40141805
INFO:root:[   84] Training loss: 0.01691814, Validation loss: 0.81256602, Gradient norm: 0.42814339
INFO:root:[   85] Training loss: 0.01685480, Validation loss: 0.81375151, Gradient norm: 0.40792555
INFO:root:[   86] Training loss: 0.01672679, Validation loss: 0.83168632, Gradient norm: 0.42245487
INFO:root:[   87] Training loss: 0.01666196, Validation loss: 0.77196756, Gradient norm: 0.42487143
INFO:root:[   88] Training loss: 0.01646402, Validation loss: 0.85377275, Gradient norm: 0.38615473
INFO:root:[   89] Training loss: 0.01646028, Validation loss: 0.80922997, Gradient norm: 0.40151635
INFO:root:[   90] Training loss: 0.01639516, Validation loss: 0.75401717, Gradient norm: 0.36109185
INFO:root:[   91] Training loss: 0.01655377, Validation loss: 0.81493969, Gradient norm: 0.41632017
INFO:root:[   92] Training loss: 0.01647297, Validation loss: 0.77296425, Gradient norm: 0.42506209
INFO:root:[   93] Training loss: 0.01617410, Validation loss: 0.80486836, Gradient norm: 0.39583017
INFO:root:[   94] Training loss: 0.01628308, Validation loss: 0.79591844, Gradient norm: 0.39999547
INFO:root:[   95] Training loss: 0.01585073, Validation loss: 0.78366466, Gradient norm: 0.35622530
INFO:root:[   96] Training loss: 0.01636047, Validation loss: 0.81843522, Gradient norm: 0.43103829
INFO:root:[   97] Training loss: 0.01605061, Validation loss: 0.78764946, Gradient norm: 0.40316193
INFO:root:[   98] Training loss: 0.01600901, Validation loss: 0.77277966, Gradient norm: 0.42237352
INFO:root:[   99] Training loss: 0.01625694, Validation loss: 0.75258907, Gradient norm: 0.45386327
INFO:root:[  100] Training loss: 0.01594365, Validation loss: 0.76199170, Gradient norm: 0.41570988
INFO:root:[  101] Training loss: 0.01576543, Validation loss: 0.83924402, Gradient norm: 0.38391745
INFO:root:[  102] Training loss: 0.01588960, Validation loss: 0.80057156, Gradient norm: 0.41896021
INFO:root:[  103] Training loss: 0.01592354, Validation loss: 0.74182694, Gradient norm: 0.44531062
INFO:root:[  104] Training loss: 0.01582545, Validation loss: 0.80580744, Gradient norm: 0.40727374
INFO:root:[  105] Training loss: 0.01602058, Validation loss: 0.78624687, Gradient norm: 0.45748243
INFO:root:[  106] Training loss: 0.01560858, Validation loss: 0.76969572, Gradient norm: 0.39117077
INFO:root:[  107] Training loss: 0.01579564, Validation loss: 0.77511300, Gradient norm: 0.44016579
INFO:root:[  108] Training loss: 0.01556693, Validation loss: 0.81269136, Gradient norm: 0.41850804
INFO:root:[  109] Training loss: 0.01564827, Validation loss: 0.76699055, Gradient norm: 0.40041591
INFO:root:[  110] Training loss: 0.01581184, Validation loss: 0.76308647, Gradient norm: 0.44170848
INFO:root:[  111] Training loss: 0.01572960, Validation loss: 0.73465463, Gradient norm: 0.44451236
INFO:root:[  112] Training loss: 0.01571314, Validation loss: 0.73398099, Gradient norm: 0.43448582
INFO:root:[  113] Training loss: 0.01537603, Validation loss: 0.75778378, Gradient norm: 0.38018304
INFO:root:[  114] Training loss: 0.01539355, Validation loss: 0.74661629, Gradient norm: 0.41194386
INFO:root:[  115] Training loss: 0.01533652, Validation loss: 0.75561197, Gradient norm: 0.38235834
INFO:root:[  116] Training loss: 0.01532726, Validation loss: 0.72828233, Gradient norm: 0.41170077
INFO:root:[  117] Training loss: 0.01544690, Validation loss: 0.74740562, Gradient norm: 0.42814301
INFO:root:[  118] Training loss: 0.01533116, Validation loss: 0.70826249, Gradient norm: 0.39030930
INFO:root:[  119] Training loss: 0.01534369, Validation loss: 0.78632156, Gradient norm: 0.43908118
INFO:root:[  120] Training loss: 0.01541535, Validation loss: 0.73249675, Gradient norm: 0.44987423
INFO:root:[  121] Training loss: 0.01517795, Validation loss: 0.75227038, Gradient norm: 0.40036801
INFO:root:[  122] Training loss: 0.01515698, Validation loss: 0.70246222, Gradient norm: 0.39094210
INFO:root:[  123] Training loss: 0.01515738, Validation loss: 0.74743488, Gradient norm: 0.39373615
INFO:root:[  124] Training loss: 0.01518593, Validation loss: 0.74353297, Gradient norm: 0.40778814
INFO:root:[  125] Training loss: 0.01506316, Validation loss: 0.76189125, Gradient norm: 0.42641931
INFO:root:[  126] Training loss: 0.01511540, Validation loss: 0.76140091, Gradient norm: 0.42630959
INFO:root:[  127] Training loss: 0.01514731, Validation loss: 0.71684387, Gradient norm: 0.42831464
INFO:root:[  128] Training loss: 0.01503918, Validation loss: 0.69989289, Gradient norm: 0.41306559
INFO:root:[  129] Training loss: 0.01493047, Validation loss: 0.74372626, Gradient norm: 0.40445108
INFO:root:[  130] Training loss: 0.01517960, Validation loss: 0.77375850, Gradient norm: 0.42875266
INFO:root:[  131] Training loss: 0.01499509, Validation loss: 0.73499057, Gradient norm: 0.41810535
INFO:root:[  132] Training loss: 0.01501069, Validation loss: 0.75324940, Gradient norm: 0.42128436
INFO:root:[  133] Training loss: 0.01477440, Validation loss: 0.69692427, Gradient norm: 0.37197076
INFO:root:[  134] Training loss: 0.01506610, Validation loss: 0.75896697, Gradient norm: 0.43920343
INFO:root:[  135] Training loss: 0.01482713, Validation loss: 0.73903871, Gradient norm: 0.39556854
INFO:root:[  136] Training loss: 0.01500205, Validation loss: 0.68137277, Gradient norm: 0.44032194
INFO:root:[  137] Training loss: 0.01482980, Validation loss: 0.70785055, Gradient norm: 0.41143834
INFO:root:[  138] Training loss: 0.01466860, Validation loss: 0.71401140, Gradient norm: 0.39199234
INFO:root:[  139] Training loss: 0.01470227, Validation loss: 0.68672867, Gradient norm: 0.38824224
INFO:root:[  140] Training loss: 0.01470679, Validation loss: 0.70302735, Gradient norm: 0.40073381
INFO:root:[  141] Training loss: 0.01463513, Validation loss: 0.70582401, Gradient norm: 0.40498032
INFO:root:[  142] Training loss: 0.01489024, Validation loss: 0.69537522, Gradient norm: 0.41402779
INFO:root:[  143] Training loss: 0.01454952, Validation loss: 0.67709596, Gradient norm: 0.41943653
INFO:root:[  144] Training loss: 0.01451593, Validation loss: 0.74374196, Gradient norm: 0.39132921
INFO:root:[  145] Training loss: 0.01484862, Validation loss: 0.70434847, Gradient norm: 0.46285132
INFO:root:[  146] Training loss: 0.01474535, Validation loss: 0.70291390, Gradient norm: 0.43306583
INFO:root:[  147] Training loss: 0.01465508, Validation loss: 0.77002031, Gradient norm: 0.40674861
INFO:root:[  148] Training loss: 0.01456070, Validation loss: 0.69688405, Gradient norm: 0.39729993
INFO:root:[  149] Training loss: 0.01449482, Validation loss: 0.69064662, Gradient norm: 0.36485680
INFO:root:[  150] Training loss: 0.01461508, Validation loss: 0.74396017, Gradient norm: 0.42950557
INFO:root:[  151] Training loss: 0.01452956, Validation loss: 0.67545368, Gradient norm: 0.45523333
INFO:root:[  152] Training loss: 0.01459296, Validation loss: 0.68882607, Gradient norm: 0.46118834
INFO:root:[  153] Training loss: 0.01420285, Validation loss: 0.69266873, Gradient norm: 0.39778238
INFO:root:[  154] Training loss: 0.01461500, Validation loss: 0.69230580, Gradient norm: 0.43492692
INFO:root:[  155] Training loss: 0.01442170, Validation loss: 0.68567314, Gradient norm: 0.41043262
INFO:root:[  156] Training loss: 0.01436031, Validation loss: 0.67575060, Gradient norm: 0.41889277
INFO:root:[  157] Training loss: 0.01435736, Validation loss: 0.67813303, Gradient norm: 0.40462185
INFO:root:[  158] Training loss: 0.01448012, Validation loss: 0.66925183, Gradient norm: 0.43146339
INFO:root:[  159] Training loss: 0.01423642, Validation loss: 0.77582576, Gradient norm: 0.37448622
INFO:root:[  160] Training loss: 0.01460302, Validation loss: 0.68197866, Gradient norm: 0.46599660
INFO:root:[  161] Training loss: 0.01423070, Validation loss: 0.68138253, Gradient norm: 0.38647782
INFO:root:[  162] Training loss: 0.01418195, Validation loss: 0.72228702, Gradient norm: 0.37861176
INFO:root:[  163] Training loss: 0.01432186, Validation loss: 0.78792856, Gradient norm: 0.42819397
INFO:root:[  164] Training loss: 0.01420126, Validation loss: 0.65445732, Gradient norm: 0.40432106
INFO:root:[  165] Training loss: 0.01424035, Validation loss: 0.71634105, Gradient norm: 0.41600921
INFO:root:[  166] Training loss: 0.01444218, Validation loss: 0.66265696, Gradient norm: 0.45222492
INFO:root:[  167] Training loss: 0.01423465, Validation loss: 0.69115061, Gradient norm: 0.41191384
INFO:root:[  168] Training loss: 0.01425850, Validation loss: 0.71133273, Gradient norm: 0.42107241
INFO:root:[  169] Training loss: 0.01414716, Validation loss: 0.69556517, Gradient norm: 0.37726050
INFO:root:[  170] Training loss: 0.01413387, Validation loss: 0.66613599, Gradient norm: 0.40130824
INFO:root:[  171] Training loss: 0.01410160, Validation loss: 0.73232056, Gradient norm: 0.39937253
INFO:root:[  172] Training loss: 0.01411920, Validation loss: 0.65409303, Gradient norm: 0.42337461
INFO:root:[  173] Training loss: 0.01391100, Validation loss: 0.69745802, Gradient norm: 0.40490996
INFO:root:[  174] Training loss: 0.01403299, Validation loss: 0.67054095, Gradient norm: 0.39971806
INFO:root:[  175] Training loss: 0.01405858, Validation loss: 0.65485634, Gradient norm: 0.39766788
INFO:root:[  176] Training loss: 0.01404638, Validation loss: 0.67550138, Gradient norm: 0.38412229
INFO:root:[  177] Training loss: 0.01415418, Validation loss: 0.66905761, Gradient norm: 0.44232411
INFO:root:[  178] Training loss: 0.01377272, Validation loss: 0.69085061, Gradient norm: 0.37224136
INFO:root:[  179] Training loss: 0.01385091, Validation loss: 0.71300484, Gradient norm: 0.36294255
INFO:root:[  180] Training loss: 0.01394937, Validation loss: 0.65885186, Gradient norm: 0.38900354
INFO:root:[  181] Training loss: 0.01390783, Validation loss: 0.69406440, Gradient norm: 0.37382245
INFO:root:EP 181: Early stopping
INFO:root:Training the model took 1768.75s.
INFO:root:Emptying the cuda cache took 0.004s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.07898
INFO:root:EnergyScoretrain: 0.06916
INFO:root:Coveragetrain: 1.63536
INFO:root:IntervalWidthtrain: 12.31466
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02145
INFO:root:EnergyScorevalidation: 0.01887
INFO:root:Coveragevalidation: 0.42799
INFO:root:IntervalWidthvalidation: 3.16327
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04332
INFO:root:EnergyScoretest: 0.04168
INFO:root:Coveragetest: 0.05757
INFO:root:IntervalWidthtest: 3.28484
INFO:root:###19 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.1, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06991699, Validation loss: 2.52559758, Gradient norm: 0.36685537
INFO:root:[    2] Training loss: 0.04828837, Validation loss: 2.23560542, Gradient norm: 0.35495386
INFO:root:[    3] Training loss: 0.04405618, Validation loss: 2.07007008, Gradient norm: 0.40591730
INFO:root:[    4] Training loss: 0.04111529, Validation loss: 1.88226960, Gradient norm: 0.44757535
INFO:root:[    5] Training loss: 0.03971844, Validation loss: 1.88060267, Gradient norm: 0.48817019
INFO:root:[    6] Training loss: 0.03813189, Validation loss: 1.94544138, Gradient norm: 0.43685854
INFO:root:[    7] Training loss: 0.03684533, Validation loss: 1.70795849, Gradient norm: 0.41213490
INFO:root:[    8] Training loss: 0.03550015, Validation loss: 1.66592777, Gradient norm: 0.38215306
INFO:root:[    9] Training loss: 0.03455768, Validation loss: 1.68499083, Gradient norm: 0.40452218
INFO:root:[   10] Training loss: 0.03388764, Validation loss: 1.54503003, Gradient norm: 0.41597106
INFO:root:[   11] Training loss: 0.03277916, Validation loss: 1.54353958, Gradient norm: 0.37589618
INFO:root:[   12] Training loss: 0.03218960, Validation loss: 1.47451444, Gradient norm: 0.38030344
INFO:root:[   13] Training loss: 0.03137018, Validation loss: 1.46855982, Gradient norm: 0.40532943
INFO:root:[   14] Training loss: 0.03095289, Validation loss: 1.57160833, Gradient norm: 0.44571056
INFO:root:[   15] Training loss: 0.03023248, Validation loss: 1.48915150, Gradient norm: 0.36965066
INFO:root:[   16] Training loss: 0.02973958, Validation loss: 1.48064765, Gradient norm: 0.38067086
INFO:root:[   17] Training loss: 0.02946227, Validation loss: 1.39256670, Gradient norm: 0.40426124
INFO:root:[   18] Training loss: 0.02881561, Validation loss: 1.41722597, Gradient norm: 0.36635230
INFO:root:[   19] Training loss: 0.02854846, Validation loss: 1.43653693, Gradient norm: 0.44419332
INFO:root:[   20] Training loss: 0.02795147, Validation loss: 1.32782114, Gradient norm: 0.37413448
INFO:root:[   21] Training loss: 0.02789099, Validation loss: 1.31985407, Gradient norm: 0.40317942
INFO:root:[   22] Training loss: 0.02743597, Validation loss: 1.32933630, Gradient norm: 0.38705844
INFO:root:[   23] Training loss: 0.02712198, Validation loss: 1.33625803, Gradient norm: 0.42253149
INFO:root:[   24] Training loss: 0.02658643, Validation loss: 1.32121312, Gradient norm: 0.33513337
INFO:root:[   25] Training loss: 0.02659885, Validation loss: 1.32828004, Gradient norm: 0.43489664
INFO:root:[   26] Training loss: 0.02598874, Validation loss: 1.31066763, Gradient norm: 0.38684673
INFO:root:[   27] Training loss: 0.02589327, Validation loss: 1.22936308, Gradient norm: 0.38628884
INFO:root:[   28] Training loss: 0.02555822, Validation loss: 1.28842591, Gradient norm: 0.37964094
INFO:root:[   29] Training loss: 0.02542425, Validation loss: 1.21478725, Gradient norm: 0.37219685
INFO:root:[   30] Training loss: 0.02511094, Validation loss: 1.19305358, Gradient norm: 0.37208461
INFO:root:[   31] Training loss: 0.02490785, Validation loss: 1.30276016, Gradient norm: 0.37722724
INFO:root:[   32] Training loss: 0.02471330, Validation loss: 1.31012711, Gradient norm: 0.39005168
INFO:root:[   33] Training loss: 0.02428710, Validation loss: 1.22192722, Gradient norm: 0.41023703
INFO:root:[   34] Training loss: 0.02423650, Validation loss: 1.17804150, Gradient norm: 0.40254784
INFO:root:[   35] Training loss: 0.02403648, Validation loss: 1.16582685, Gradient norm: 0.37467106
INFO:root:[   36] Training loss: 0.02369102, Validation loss: 1.20619163, Gradient norm: 0.38160846
INFO:root:[   37] Training loss: 0.02350003, Validation loss: 1.14306703, Gradient norm: 0.37304748
INFO:root:[   38] Training loss: 0.02336372, Validation loss: 1.20079928, Gradient norm: 0.38636316
INFO:root:[   39] Training loss: 0.02317626, Validation loss: 1.18838196, Gradient norm: 0.38356417
INFO:root:[   40] Training loss: 0.02299391, Validation loss: 1.13933711, Gradient norm: 0.37869065
INFO:root:[   41] Training loss: 0.02305501, Validation loss: 1.17605175, Gradient norm: 0.43215377
INFO:root:[   42] Training loss: 0.02261997, Validation loss: 1.09503848, Gradient norm: 0.38310225
INFO:root:[   43] Training loss: 0.02249768, Validation loss: 1.10771922, Gradient norm: 0.40505203
INFO:root:[   44] Training loss: 0.02227138, Validation loss: 1.09135621, Gradient norm: 0.37446084
INFO:root:[   45] Training loss: 0.02212412, Validation loss: 1.10014782, Gradient norm: 0.35144639
INFO:root:[   46] Training loss: 0.02195460, Validation loss: 1.10188885, Gradient norm: 0.39699954
INFO:root:[   47] Training loss: 0.02170116, Validation loss: 1.05479073, Gradient norm: 0.33740743
INFO:root:[   48] Training loss: 0.02157468, Validation loss: 1.06903992, Gradient norm: 0.32970343
INFO:root:[   49] Training loss: 0.02182534, Validation loss: 1.11498534, Gradient norm: 0.43089164
INFO:root:[   50] Training loss: 0.02158798, Validation loss: 1.08303641, Gradient norm: 0.42850178
INFO:root:[   51] Training loss: 0.02132702, Validation loss: 1.02903945, Gradient norm: 0.38473475
INFO:root:[   52] Training loss: 0.02097323, Validation loss: 1.14001882, Gradient norm: 0.34734086
INFO:root:[   53] Training loss: 0.02096166, Validation loss: 1.04049658, Gradient norm: 0.37533008
INFO:root:[   54] Training loss: 0.02098381, Validation loss: 1.06182866, Gradient norm: 0.39763926
INFO:root:[   55] Training loss: 0.02059833, Validation loss: 1.03759621, Gradient norm: 0.34860886
INFO:root:[   56] Training loss: 0.02048136, Validation loss: 1.04618054, Gradient norm: 0.36874815
INFO:root:[   57] Training loss: 0.02049898, Validation loss: 1.05242705, Gradient norm: 0.36922040
INFO:root:[   58] Training loss: 0.02042410, Validation loss: 0.99196742, Gradient norm: 0.38241502
INFO:root:[   59] Training loss: 0.02033233, Validation loss: 1.01972662, Gradient norm: 0.40630563
INFO:root:[   60] Training loss: 0.02013982, Validation loss: 0.98701207, Gradient norm: 0.39256441
INFO:root:[   61] Training loss: 0.02018852, Validation loss: 0.99154147, Gradient norm: 0.40373346
INFO:root:[   62] Training loss: 0.02003932, Validation loss: 1.03541385, Gradient norm: 0.38203417
INFO:root:[   63] Training loss: 0.01980507, Validation loss: 0.94862392, Gradient norm: 0.37241714
INFO:root:[   64] Training loss: 0.01954650, Validation loss: 1.01365836, Gradient norm: 0.32476734
INFO:root:[   65] Training loss: 0.01967030, Validation loss: 1.11666949, Gradient norm: 0.38848687
INFO:root:[   66] Training loss: 0.01939633, Validation loss: 1.13143193, Gradient norm: 0.39943788
INFO:root:[   67] Training loss: 0.01948981, Validation loss: 0.94859801, Gradient norm: 0.41357674
INFO:root:[   68] Training loss: 0.01923594, Validation loss: 0.95792732, Gradient norm: 0.35966422
INFO:root:[   69] Training loss: 0.01921817, Validation loss: 0.94070301, Gradient norm: 0.39727369
INFO:root:[   70] Training loss: 0.01901621, Validation loss: 0.96675588, Gradient norm: 0.36705014
INFO:root:[   71] Training loss: 0.01898014, Validation loss: 0.90955429, Gradient norm: 0.39124246
INFO:root:[   72] Training loss: 0.01894778, Validation loss: 0.95188354, Gradient norm: 0.41110919
INFO:root:[   73] Training loss: 0.01873100, Validation loss: 0.92528534, Gradient norm: 0.38634712
INFO:root:[   74] Training loss: 0.01869541, Validation loss: 0.89406755, Gradient norm: 0.39365338
INFO:root:[   75] Training loss: 0.01864394, Validation loss: 0.95177319, Gradient norm: 0.40417288
INFO:root:[   76] Training loss: 0.01850678, Validation loss: 0.87922234, Gradient norm: 0.39208519
INFO:root:[   77] Training loss: 0.01826143, Validation loss: 0.88612180, Gradient norm: 0.38418682
INFO:root:[   78] Training loss: 0.01811640, Validation loss: 0.87933936, Gradient norm: 0.39544909
INFO:root:[   79] Training loss: 0.01798309, Validation loss: 0.89282378, Gradient norm: 0.37742085
INFO:root:[   80] Training loss: 0.01805643, Validation loss: 0.88884675, Gradient norm: 0.39176281
INFO:root:[   81] Training loss: 0.01796121, Validation loss: 0.95230951, Gradient norm: 0.39162968
INFO:root:[   82] Training loss: 0.01811908, Validation loss: 0.95049091, Gradient norm: 0.42461208
INFO:root:[   83] Training loss: 0.01776939, Validation loss: 0.88058482, Gradient norm: 0.39559184
INFO:root:[   84] Training loss: 0.01784150, Validation loss: 0.87982111, Gradient norm: 0.38024538
INFO:root:[   85] Training loss: 0.01761921, Validation loss: 0.85127080, Gradient norm: 0.38970780
INFO:root:[   86] Training loss: 0.01755137, Validation loss: 0.94027400, Gradient norm: 0.36975360
INFO:root:[   87] Training loss: 0.01783946, Validation loss: 0.93872228, Gradient norm: 0.46481659
INFO:root:[   88] Training loss: 0.01752020, Validation loss: 0.82531245, Gradient norm: 0.40839799
INFO:root:[   89] Training loss: 0.01745939, Validation loss: 0.83240177, Gradient norm: 0.42790927
INFO:root:[   90] Training loss: 0.01730881, Validation loss: 0.83582808, Gradient norm: 0.36697283
INFO:root:[   91] Training loss: 0.01726442, Validation loss: 0.82887568, Gradient norm: 0.40876523
INFO:root:[   92] Training loss: 0.01733535, Validation loss: 0.82342806, Gradient norm: 0.43058648
INFO:root:[   93] Training loss: 0.01710413, Validation loss: 0.86146868, Gradient norm: 0.42136737
INFO:root:[   94] Training loss: 0.01714076, Validation loss: 0.81339392, Gradient norm: 0.38022295
INFO:root:[   95] Training loss: 0.01698935, Validation loss: 0.83566597, Gradient norm: 0.41134250
INFO:root:[   96] Training loss: 0.01716255, Validation loss: 0.84584899, Gradient norm: 0.45263823
INFO:root:[   97] Training loss: 0.01689035, Validation loss: 0.80176119, Gradient norm: 0.41056134
INFO:root:[   98] Training loss: 0.01702802, Validation loss: 0.91706635, Gradient norm: 0.40830353
INFO:root:[   99] Training loss: 0.01686872, Validation loss: 0.78877927, Gradient norm: 0.40500765
INFO:root:[  100] Training loss: 0.01687779, Validation loss: 0.79036666, Gradient norm: 0.41565274
INFO:root:[  101] Training loss: 0.01655188, Validation loss: 0.83226966, Gradient norm: 0.36557661
INFO:root:[  102] Training loss: 0.01661047, Validation loss: 0.85067336, Gradient norm: 0.39422021
INFO:root:[  103] Training loss: 0.01674683, Validation loss: 0.78481742, Gradient norm: 0.45298423
INFO:root:[  104] Training loss: 0.01665655, Validation loss: 0.80123117, Gradient norm: 0.43660476
INFO:root:[  105] Training loss: 0.01642946, Validation loss: 0.78791667, Gradient norm: 0.36220642
INFO:root:[  106] Training loss: 0.01649784, Validation loss: 0.78062400, Gradient norm: 0.40353808
INFO:root:[  107] Training loss: 0.01655659, Validation loss: 0.80845139, Gradient norm: 0.42502452
INFO:root:[  108] Training loss: 0.01642321, Validation loss: 0.77988796, Gradient norm: 0.38473687
INFO:root:[  109] Training loss: 0.01649630, Validation loss: 0.78390155, Gradient norm: 0.42479608
INFO:root:[  110] Training loss: 0.01621810, Validation loss: 0.78271886, Gradient norm: 0.39684464
INFO:root:[  111] Training loss: 0.01635659, Validation loss: 0.77708849, Gradient norm: 0.42944643
INFO:root:[  112] Training loss: 0.01614858, Validation loss: 0.78771232, Gradient norm: 0.40758113
INFO:root:[  113] Training loss: 0.01616230, Validation loss: 0.78231237, Gradient norm: 0.38090073
INFO:root:[  114] Training loss: 0.01616467, Validation loss: 0.76258596, Gradient norm: 0.41689217
INFO:root:[  115] Training loss: 0.01635910, Validation loss: 0.77171114, Gradient norm: 0.41806015
INFO:root:[  116] Training loss: 0.01600502, Validation loss: 0.85692073, Gradient norm: 0.37492795
INFO:root:[  117] Training loss: 0.01623074, Validation loss: 0.79027797, Gradient norm: 0.42879055
INFO:root:[  118] Training loss: 0.01596051, Validation loss: 0.78160913, Gradient norm: 0.39232075
INFO:root:[  119] Training loss: 0.01605920, Validation loss: 0.81600666, Gradient norm: 0.41018397
INFO:root:[  120] Training loss: 0.01596766, Validation loss: 0.77384887, Gradient norm: 0.37976821
INFO:root:[  121] Training loss: 0.01601137, Validation loss: 0.75481637, Gradient norm: 0.40741923
INFO:root:[  122] Training loss: 0.01581972, Validation loss: 0.81321472, Gradient norm: 0.41657344
INFO:root:[  123] Training loss: 0.01568914, Validation loss: 0.79953371, Gradient norm: 0.38099205
INFO:root:[  124] Training loss: 0.01577780, Validation loss: 0.76444846, Gradient norm: 0.39866949
INFO:root:[  125] Training loss: 0.01598118, Validation loss: 0.76431165, Gradient norm: 0.43606445
INFO:root:[  126] Training loss: 0.01562525, Validation loss: 0.76223106, Gradient norm: 0.40197991
INFO:root:[  127] Training loss: 0.01570520, Validation loss: 0.79148742, Gradient norm: 0.42274317
INFO:root:[  128] Training loss: 0.01555236, Validation loss: 0.72429345, Gradient norm: 0.39588263
INFO:root:[  129] Training loss: 0.01563298, Validation loss: 0.78197270, Gradient norm: 0.40582609
INFO:root:[  130] Training loss: 0.01560988, Validation loss: 0.75092566, Gradient norm: 0.41610773
INFO:root:[  131] Training loss: 0.01541822, Validation loss: 0.78824790, Gradient norm: 0.34546985
INFO:root:[  132] Training loss: 0.01566244, Validation loss: 0.76856283, Gradient norm: 0.43960993
INFO:root:[  133] Training loss: 0.01552890, Validation loss: 0.76504960, Gradient norm: 0.41636992
INFO:root:[  134] Training loss: 0.01556781, Validation loss: 0.73876736, Gradient norm: 0.41477821
INFO:root:[  135] Training loss: 0.01541623, Validation loss: 0.74766950, Gradient norm: 0.45976836
INFO:root:[  136] Training loss: 0.01526732, Validation loss: 0.76077639, Gradient norm: 0.37647720
INFO:root:[  137] Training loss: 0.01547845, Validation loss: 0.73186962, Gradient norm: 0.45776385
INFO:root:EP 137: Early stopping
INFO:root:Training the model took 1340.764s.
INFO:root:Emptying the cuda cache took 0.005s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.08685
INFO:root:EnergyScoretrain: 0.07699
INFO:root:Coveragetrain: 1.51252
INFO:root:IntervalWidthtrain: 12.15764
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.02416
INFO:root:EnergyScorevalidation: 0.02158
INFO:root:Coveragevalidation: 0.38953
INFO:root:IntervalWidthvalidation: 3.04777
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04176
INFO:root:EnergyScoretest: 0.03995
INFO:root:Coveragetest: 0.0853
INFO:root:IntervalWidthtest: 3.62873
INFO:root:###20 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': 0.2, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08446379, Validation loss: 2.77352138, Gradient norm: 0.41700918
INFO:root:[    2] Training loss: 0.05674724, Validation loss: 2.39765580, Gradient norm: 0.41553394
INFO:root:[    3] Training loss: 0.05176186, Validation loss: 2.29452843, Gradient norm: 0.37485512
INFO:root:[    4] Training loss: 0.04843030, Validation loss: 2.38456514, Gradient norm: 0.32071435
INFO:root:[    5] Training loss: 0.04617915, Validation loss: 2.11767337, Gradient norm: 0.36196941
INFO:root:[    6] Training loss: 0.04466640, Validation loss: 2.13568640, Gradient norm: 0.44204239
INFO:root:[    7] Training loss: 0.04289371, Validation loss: 2.02179514, Gradient norm: 0.43538224
INFO:root:[    8] Training loss: 0.04140144, Validation loss: 1.96749479, Gradient norm: 0.43753034
INFO:root:[    9] Training loss: 0.04010676, Validation loss: 1.84815364, Gradient norm: 0.43108554
INFO:root:[   10] Training loss: 0.03860856, Validation loss: 1.73814898, Gradient norm: 0.41722547
INFO:root:[   11] Training loss: 0.03767667, Validation loss: 1.74454423, Gradient norm: 0.44454471
INFO:root:[   12] Training loss: 0.03667950, Validation loss: 1.65132032, Gradient norm: 0.42812451
INFO:root:[   13] Training loss: 0.03594381, Validation loss: 1.59620823, Gradient norm: 0.48821600
INFO:root:[   14] Training loss: 0.03497022, Validation loss: 1.67295863, Gradient norm: 0.45407453
INFO:root:[   15] Training loss: 0.03415990, Validation loss: 1.60422934, Gradient norm: 0.48686319
INFO:root:[   16] Training loss: 0.03347856, Validation loss: 1.67062791, Gradient norm: 0.46141608
INFO:root:[   17] Training loss: 0.03281150, Validation loss: 1.47757503, Gradient norm: 0.47067916
INFO:root:[   18] Training loss: 0.03225319, Validation loss: 1.48648069, Gradient norm: 0.45445215
INFO:root:[   19] Training loss: 0.03139344, Validation loss: 1.44868841, Gradient norm: 0.40766036
INFO:root:[   20] Training loss: 0.03115559, Validation loss: 1.50248962, Gradient norm: 0.43942393
INFO:root:[   21] Training loss: 0.03060036, Validation loss: 1.40191908, Gradient norm: 0.41862063
INFO:root:[   22] Training loss: 0.03026122, Validation loss: 1.43251152, Gradient norm: 0.47505745
INFO:root:[   23] Training loss: 0.02981140, Validation loss: 1.62397392, Gradient norm: 0.43808173
INFO:root:[   24] Training loss: 0.02954039, Validation loss: 1.38892973, Gradient norm: 0.47692299
INFO:root:[   25] Training loss: 0.02921256, Validation loss: 1.34771871, Gradient norm: 0.46560902
INFO:root:[   26] Training loss: 0.02882287, Validation loss: 1.33502568, Gradient norm: 0.41997222
INFO:root:[   27] Training loss: 0.02856860, Validation loss: 1.32136343, Gradient norm: 0.46754122
INFO:root:[   28] Training loss: 0.02831254, Validation loss: 1.39093124, Gradient norm: 0.49821369
INFO:root:[   29] Training loss: 0.02800312, Validation loss: 1.28643297, Gradient norm: 0.44813698
INFO:root:[   30] Training loss: 0.02785566, Validation loss: 1.27194130, Gradient norm: 0.47575783
INFO:root:[   31] Training loss: 0.02741861, Validation loss: 1.29744585, Gradient norm: 0.43396212
INFO:root:[   32] Training loss: 0.02725033, Validation loss: 1.29088249, Gradient norm: 0.47169701
INFO:root:[   33] Training loss: 0.02706648, Validation loss: 1.26624154, Gradient norm: 0.43757471
INFO:root:[   34] Training loss: 0.02666663, Validation loss: 1.33044874, Gradient norm: 0.42566281
INFO:root:[   35] Training loss: 0.02663756, Validation loss: 1.29860745, Gradient norm: 0.46503617
INFO:root:[   36] Training loss: 0.02622696, Validation loss: 1.23988916, Gradient norm: 0.45807483
INFO:root:[   37] Training loss: 0.02622936, Validation loss: 1.20238614, Gradient norm: 0.46248599
INFO:root:[   38] Training loss: 0.02591184, Validation loss: 1.22106347, Gradient norm: 0.44782734
INFO:root:[   39] Training loss: 0.02561072, Validation loss: 1.20601873, Gradient norm: 0.44993323
INFO:root:[   40] Training loss: 0.02552374, Validation loss: 1.33085702, Gradient norm: 0.42053413
INFO:root:[   41] Training loss: 0.02540703, Validation loss: 1.18559149, Gradient norm: 0.43492082
INFO:root:[   42] Training loss: 0.02509791, Validation loss: 1.20399403, Gradient norm: 0.42445409
INFO:root:[   43] Training loss: 0.02521994, Validation loss: 1.16586534, Gradient norm: 0.49900770
INFO:root:[   44] Training loss: 0.02493597, Validation loss: 1.16171886, Gradient norm: 0.45456017
INFO:root:[   45] Training loss: 0.02482250, Validation loss: 1.17229500, Gradient norm: 0.44544198
INFO:root:[   46] Training loss: 0.02473404, Validation loss: 1.14586975, Gradient norm: 0.46934516
INFO:root:[   47] Training loss: 0.02439291, Validation loss: 1.20251169, Gradient norm: 0.41563573
INFO:root:[   48] Training loss: 0.02450844, Validation loss: 1.14566438, Gradient norm: 0.49731042
INFO:root:[   49] Training loss: 0.02433936, Validation loss: 1.12903703, Gradient norm: 0.46932094
INFO:root:[   50] Training loss: 0.02417628, Validation loss: 1.18134870, Gradient norm: 0.47495432
INFO:root:[   51] Training loss: 0.02379669, Validation loss: 1.14248465, Gradient norm: 0.42536796
INFO:root:[   52] Training loss: 0.02398323, Validation loss: 1.13414168, Gradient norm: 0.47065727
INFO:root:[   53] Training loss: 0.02381600, Validation loss: 1.20929960, Gradient norm: 0.46089842
INFO:root:[   54] Training loss: 0.02361928, Validation loss: 1.10844255, Gradient norm: 0.44960203
INFO:root:[   55] Training loss: 0.02348784, Validation loss: 1.17175986, Gradient norm: 0.43849921
INFO:root:[   56] Training loss: 0.02323914, Validation loss: 1.11539109, Gradient norm: 0.44005057
INFO:root:[   57] Training loss: 0.02317260, Validation loss: 1.14468602, Gradient norm: 0.43975824
INFO:root:[   58] Training loss: 0.02314016, Validation loss: 1.09665836, Gradient norm: 0.45274295
INFO:root:[   59] Training loss: 0.02301202, Validation loss: 1.12093001, Gradient norm: 0.43754439
INFO:root:[   60] Training loss: 0.02293249, Validation loss: 1.07178672, Gradient norm: 0.44226762
INFO:root:[   61] Training loss: 0.02279536, Validation loss: 1.19492936, Gradient norm: 0.46680724
INFO:root:[   62] Training loss: 0.02250962, Validation loss: 1.08136825, Gradient norm: 0.41626437
INFO:root:[   63] Training loss: 0.02238598, Validation loss: 1.13301126, Gradient norm: 0.42160028
INFO:root:[   64] Training loss: 0.02232221, Validation loss: 1.05823859, Gradient norm: 0.40108131
INFO:root:[   65] Training loss: 0.02247906, Validation loss: 1.07885196, Gradient norm: 0.47105856
INFO:root:[   66] Training loss: 0.02229623, Validation loss: 1.04703696, Gradient norm: 0.47748488
INFO:root:[   67] Training loss: 0.02206151, Validation loss: 1.08685631, Gradient norm: 0.41141369
INFO:root:[   68] Training loss: 0.02199261, Validation loss: 1.05848508, Gradient norm: 0.41818054
INFO:root:[   69] Training loss: 0.02215457, Validation loss: 1.09200144, Gradient norm: 0.45677851
INFO:root:[   70] Training loss: 0.02171594, Validation loss: 1.06573042, Gradient norm: 0.41462956
INFO:root:[   71] Training loss: 0.02171348, Validation loss: 1.02453470, Gradient norm: 0.41389652
INFO:root:[   72] Training loss: 0.02170275, Validation loss: 1.04854346, Gradient norm: 0.40494331
INFO:root:[   73] Training loss: 0.02147185, Validation loss: 1.03058535, Gradient norm: 0.39314102
INFO:root:[   74] Training loss: 0.02161896, Validation loss: 1.07981311, Gradient norm: 0.44277958
INFO:root:[   75] Training loss: 0.02130725, Validation loss: 1.09229632, Gradient norm: 0.42925509
INFO:root:[   76] Training loss: 0.02111746, Validation loss: 1.03993534, Gradient norm: 0.41756248
INFO:root:[   77] Training loss: 0.02121596, Validation loss: 1.04909014, Gradient norm: 0.42849122
INFO:root:[   78] Training loss: 0.02084956, Validation loss: 1.03823871, Gradient norm: 0.36919087
INFO:root:[   79] Training loss: 0.02083693, Validation loss: 1.07416442, Gradient norm: 0.41748598
INFO:root:[   80] Training loss: 0.02094123, Validation loss: 1.03526472, Gradient norm: 0.44643190
INFO:root:EP 80: Early stopping
INFO:root:Training the model took 792.339s.
INFO:root:Emptying the cuda cache took 0.004s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.12271
INFO:root:EnergyScoretrain: 0.10717
INFO:root:Coveragetrain: 1.55121
INFO:root:IntervalWidthtrain: 18.90625
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.03322
INFO:root:EnergyScorevalidation: 0.0289
INFO:root:Coveragevalidation: 0.444
INFO:root:IntervalWidthvalidation: 5.24119
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04706
INFO:root:EnergyScoretest: 0.0445
INFO:root:Coveragetest: 0.08145
INFO:root:IntervalWidthtest: 5.38435
INFO:root:###21 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.01, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07491501, Validation loss: 2.51631385, Gradient norm: 0.58866954
INFO:root:[    2] Training loss: 0.04990549, Validation loss: 2.31091387, Gradient norm: 0.58987451
INFO:root:[    3] Training loss: 0.04463819, Validation loss: 1.93760930, Gradient norm: 0.50071784
INFO:root:[    4] Training loss: 0.04150838, Validation loss: 1.87824047, Gradient norm: 0.52296653
INFO:root:[    5] Training loss: 0.03931548, Validation loss: 2.07696326, Gradient norm: 0.54659774
INFO:root:[    6] Training loss: 0.03748904, Validation loss: 1.69949105, Gradient norm: 0.43303507
INFO:root:[    7] Training loss: 0.03632680, Validation loss: 1.63429096, Gradient norm: 0.47619016
INFO:root:[    8] Training loss: 0.03468130, Validation loss: 1.61273172, Gradient norm: 0.36872357
INFO:root:[    9] Training loss: 0.03428324, Validation loss: 1.52787232, Gradient norm: 0.48621273
INFO:root:[   10] Training loss: 0.03334419, Validation loss: 1.71369146, Gradient norm: 0.46577398
INFO:root:[   11] Training loss: 0.03289045, Validation loss: 1.52988989, Gradient norm: 0.46309229
INFO:root:[   12] Training loss: 0.03193735, Validation loss: 1.46813458, Gradient norm: 0.42998644
INFO:root:[   13] Training loss: 0.03117887, Validation loss: 1.44428034, Gradient norm: 0.41600851
INFO:root:[   14] Training loss: 0.03082825, Validation loss: 1.41056912, Gradient norm: 0.42677903
INFO:root:[   15] Training loss: 0.03046185, Validation loss: 1.56712154, Gradient norm: 0.44362854
INFO:root:[   16] Training loss: 0.02998533, Validation loss: 1.43410932, Gradient norm: 0.39849983
INFO:root:[   17] Training loss: 0.02971970, Validation loss: 1.59561134, Gradient norm: 0.47605899
INFO:root:[   18] Training loss: 0.02943584, Validation loss: 1.47838603, Gradient norm: 0.48610850
INFO:root:[   19] Training loss: 0.02874785, Validation loss: 1.35305818, Gradient norm: 0.42255790
INFO:root:[   20] Training loss: 0.02824691, Validation loss: 1.45104751, Gradient norm: 0.41991925
INFO:root:[   21] Training loss: 0.02807447, Validation loss: 1.41680023, Gradient norm: 0.42342396
INFO:root:[   22] Training loss: 0.02771445, Validation loss: 1.32592591, Gradient norm: 0.44500696
INFO:root:[   23] Training loss: 0.02721088, Validation loss: 1.33284588, Gradient norm: 0.37698691
INFO:root:[   24] Training loss: 0.02712183, Validation loss: 1.63893878, Gradient norm: 0.40774733
INFO:root:[   25] Training loss: 0.02696402, Validation loss: 1.55096050, Gradient norm: 0.46658715
INFO:root:[   26] Training loss: 0.02638970, Validation loss: 1.28133805, Gradient norm: 0.39150480
INFO:root:[   27] Training loss: 0.02620210, Validation loss: 1.29665269, Gradient norm: 0.39216293
INFO:root:[   28] Training loss: 0.02603080, Validation loss: 1.21430936, Gradient norm: 0.42763631
INFO:root:[   29] Training loss: 0.02558469, Validation loss: 1.26270547, Gradient norm: 0.41601882
INFO:root:[   30] Training loss: 0.02554042, Validation loss: 1.17650774, Gradient norm: 0.43503979
INFO:root:[   31] Training loss: 0.02514029, Validation loss: 1.40423626, Gradient norm: 0.38449574
INFO:root:[   32] Training loss: 0.02500198, Validation loss: 1.19794506, Gradient norm: 0.42641983
INFO:root:[   33] Training loss: 0.02439055, Validation loss: 1.24754771, Gradient norm: 0.35332478
INFO:root:[   34] Training loss: 0.02443679, Validation loss: 1.29372401, Gradient norm: 0.41732470
INFO:root:[   35] Training loss: 0.02397525, Validation loss: 1.15343383, Gradient norm: 0.33861066
INFO:root:[   36] Training loss: 0.02391707, Validation loss: 1.12338115, Gradient norm: 0.38255980
INFO:root:[   37] Training loss: 0.02384655, Validation loss: 1.33525703, Gradient norm: 0.43084665
INFO:root:[   38] Training loss: 0.02347568, Validation loss: 1.17937845, Gradient norm: 0.38572985
INFO:root:[   39] Training loss: 0.02337091, Validation loss: 1.16667895, Gradient norm: 0.39741322
INFO:root:[   40] Training loss: 0.02285746, Validation loss: 1.30059412, Gradient norm: 0.35044696
INFO:root:[   41] Training loss: 0.02300245, Validation loss: 1.15458015, Gradient norm: 0.41652441
INFO:root:[   42] Training loss: 0.02280025, Validation loss: 1.18943544, Gradient norm: 0.41290265
INFO:root:[   43] Training loss: 0.02231707, Validation loss: 1.14661930, Gradient norm: 0.34056117
INFO:root:[   44] Training loss: 0.02219884, Validation loss: 1.10778388, Gradient norm: 0.37144209
INFO:root:[   45] Training loss: 0.02219969, Validation loss: 1.26260254, Gradient norm: 0.37026554
INFO:root:[   46] Training loss: 0.02202157, Validation loss: 1.20863856, Gradient norm: 0.37506642
INFO:root:[   47] Training loss: 0.02208405, Validation loss: 1.08279824, Gradient norm: 0.38635749
INFO:root:[   48] Training loss: 0.02177329, Validation loss: 1.18973598, Gradient norm: 0.38976294
INFO:root:[   49] Training loss: 0.02145898, Validation loss: 1.08297446, Gradient norm: 0.35301870
INFO:root:[   50] Training loss: 0.02135297, Validation loss: 1.15430625, Gradient norm: 0.36728709
INFO:root:[   51] Training loss: 0.02129800, Validation loss: 1.06602067, Gradient norm: 0.39614817
INFO:root:[   52] Training loss: 0.02113208, Validation loss: 1.05055757, Gradient norm: 0.38966216
INFO:root:[   53] Training loss: 0.02095344, Validation loss: 0.98861935, Gradient norm: 0.35875644
INFO:root:[   54] Training loss: 0.02089436, Validation loss: 1.12830535, Gradient norm: 0.38606948
INFO:root:[   55] Training loss: 0.02067585, Validation loss: 1.05140607, Gradient norm: 0.39265450
INFO:root:[   56] Training loss: 0.02055707, Validation loss: 1.16183322, Gradient norm: 0.36308161
INFO:root:[   57] Training loss: 0.02062351, Validation loss: 1.09914129, Gradient norm: 0.39398937
INFO:root:[   58] Training loss: 0.02044466, Validation loss: 0.98447155, Gradient norm: 0.38380812
INFO:root:[   59] Training loss: 0.02043271, Validation loss: 1.06800509, Gradient norm: 0.42995160
INFO:root:[   60] Training loss: 0.02028416, Validation loss: 1.09566450, Gradient norm: 0.37364793
INFO:root:[   61] Training loss: 0.02008590, Validation loss: 0.98526951, Gradient norm: 0.34589276
INFO:root:[   62] Training loss: 0.01984622, Validation loss: 0.95837886, Gradient norm: 0.33591456
INFO:root:[   63] Training loss: 0.01985177, Validation loss: 1.08263353, Gradient norm: 0.36659953
INFO:root:[   64] Training loss: 0.01989952, Validation loss: 0.92627672, Gradient norm: 0.37359131
INFO:root:[   65] Training loss: 0.01953882, Validation loss: 1.00598892, Gradient norm: 0.31466343
INFO:root:[   66] Training loss: 0.01967086, Validation loss: 0.98715223, Gradient norm: 0.37434852
INFO:root:[   67] Training loss: 0.01959599, Validation loss: 0.96181110, Gradient norm: 0.37618204
INFO:root:[   68] Training loss: 0.01930674, Validation loss: 0.90954003, Gradient norm: 0.34793978
INFO:root:[   69] Training loss: 0.01933113, Validation loss: 0.88668517, Gradient norm: 0.38269448
INFO:root:[   70] Training loss: 0.01907365, Validation loss: 0.88328865, Gradient norm: 0.32593024
INFO:root:[   71] Training loss: 0.01918406, Validation loss: 0.97465251, Gradient norm: 0.38052334
INFO:root:[   72] Training loss: 0.01908211, Validation loss: 0.93817842, Gradient norm: 0.37815565
INFO:root:[   73] Training loss: 0.01902144, Validation loss: 1.04737019, Gradient norm: 0.38121594
INFO:root:[   74] Training loss: 0.01883293, Validation loss: 0.88971027, Gradient norm: 0.36360935
INFO:root:[   75] Training loss: 0.01847897, Validation loss: 0.94163553, Gradient norm: 0.33130520
INFO:root:[   76] Training loss: 0.01876826, Validation loss: 0.93791499, Gradient norm: 0.39798743
INFO:root:[   77] Training loss: 0.01855854, Validation loss: 0.99529416, Gradient norm: 0.37484927
INFO:root:[   78] Training loss: 0.01834554, Validation loss: 1.07232036, Gradient norm: 0.32012440
INFO:root:[   79] Training loss: 0.01822669, Validation loss: 0.95610450, Gradient norm: 0.33015554
INFO:root:EP 79: Early stopping
INFO:root:Training the model took 790.671s.
INFO:root:Emptying the cuda cache took 0.004s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.10627
INFO:root:EnergyScoretrain: 0.09114
INFO:root:Coveragetrain: 1.11355
INFO:root:IntervalWidthtrain: 22.31202
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.0296
INFO:root:EnergyScorevalidation: 0.02525
INFO:root:Coveragevalidation: 0.29329
INFO:root:IntervalWidthvalidation: 6.07106
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.04501
INFO:root:EnergyScoretest: 0.04255
INFO:root:Coveragetest: 0.05427
INFO:root:IntervalWidthtest: 6.19251
INFO:root:###22 out of 48 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': 0.05, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07249314, Validation loss: 2.51705280, Gradient norm: 0.50888373
INFO:root:[    2] Training loss: 0.05080955, Validation loss: 2.27867503, Gradient norm: 0.48200021
INFO:root:[    3] Training loss: 0.04671366, Validation loss: 2.19825557, Gradient norm: 0.44425427
INFO:root:[    4] Training loss: 0.04362232, Validation loss: 1.93870713, Gradient norm: 0.38481394
INFO:root:[    5] Training loss: 0.04151690, Validation loss: 1.94060250, Gradient norm: 0.43443611
INFO:root:[    6] Training loss: 0.04006394, Validation loss: 1.80986104, Gradient norm: 0.53153905
INFO:root:[    7] Training loss: 0.03817477, Validation loss: 1.73119453, Gradient norm: 0.40131247
INFO:root:[    8] Training loss: 0.03695777, Validation loss: 1.83520278, Gradient norm: 0.43026209
INFO:root:[    9] Training loss: 0.03617227, Validation loss: 1.72240899, Gradient norm: 0.40556488
INFO:root:[   10] Training loss: 0.03501527, Validation loss: 1.75512421, Gradient norm: 0.38923733
INFO:root:[   11] Training loss: 0.03416243, Validation loss: 1.68633243, Gradient norm: 0.38498470
INFO:root:[   12] Training loss: 0.03367188, Validation loss: 1.63733676, Gradient norm: 0.38340608
INFO:root:[   13] Training loss: 0.03286501, Validation loss: 1.61843158, Gradient norm: 0.38863764
INFO:root:[   14] Training loss: 0.03217075, Validation loss: 1.61630131, Gradient norm: 0.37420455
INFO:root:[   15] Training loss: 0.03180968, Validation loss: 1.51278075, Gradient norm: 0.43264859
INFO:root:[   16] Training loss: 0.03118095, Validation loss: 1.52628274, Gradient norm: 0.40958280
INFO:root:[   17] Training loss: 0.03095861, Validation loss: 1.47462352, Gradient norm: 0.39977818
INFO:root:[   18] Training loss: 0.03068150, Validation loss: 1.39484912, Gradient norm: 0.45109137
INFO:root:[   19] Training loss: 0.02978754, Validation loss: 1.49094717, Gradient norm: 0.37795863
INFO:root:[   20] Training loss: 0.02948016, Validation loss: 1.45016309, Gradient norm: 0.36777176
INFO:root:[   21] Training loss: 0.02916354, Validation loss: 1.37189276, Gradient norm: 0.39186689
INFO:root:[   22] Training loss: 0.02879817, Validation loss: 1.36830376, Gradient norm: 0.41957903
INFO:root:[   23] Training loss: 0.02835626, Validation loss: 1.37045686, Gradient norm: 0.35922884
INFO:root:[   24] Training loss: 0.02814637, Validation loss: 1.44266393, Gradient norm: 0.41777639
INFO:root:[   25] Training loss: 0.02788102, Validation loss: 1.34359676, Gradient norm: 0.40167821
INFO:root:[   26] Training loss: 0.02746118, Validation loss: 1.34854825, Gradient norm: 0.37393220
INFO:root:[   27] Training loss: 0.02740979, Validation loss: 1.33170844, Gradient norm: 0.39824698
INFO:root:[   28] Training loss: 0.02709094, Validation loss: 1.26914653, Gradient norm: 0.39644625
INFO:root:[   29] Training loss: 0.02663669, Validation loss: 1.24387065, Gradient norm: 0.41158245
INFO:root:[   30] Training loss: 0.02657991, Validation loss: 1.27631876, Gradient norm: 0.45010975
INFO:root:[   31] Training loss: 0.02592973, Validation loss: 1.24694367, Gradient norm: 0.35746635
INFO:root:[   32] Training loss: 0.02576869, Validation loss: 1.25113246, Gradient norm: 0.38372654
INFO:root:[   33] Training loss: 0.02576536, Validation loss: 1.20626243, Gradient norm: 0.41302717
INFO:root:[   34] Training loss: 0.02547431, Validation loss: 1.22237844, Gradient norm: 0.39774856
INFO:root:[   35] Training loss: 0.02545200, Validation loss: 1.26731292, Gradient norm: 0.43702305
INFO:root:[   36] Training loss: 0.02509749, Validation loss: 1.19704743, Gradient norm: 0.39093130
INFO:root:[   37] Training loss: 0.02472436, Validation loss: 1.22852012, Gradient norm: 0.37617989
INFO:root:[   38] Training loss: 0.02473981, Validation loss: 1.27489258, Gradient norm: 0.42165064
INFO:root:[   39] Training loss: 0.02451682, Validation loss: 1.16091938, Gradient norm: 0.44852801
INFO:root:[   40] Training loss: 0.02431191, Validation loss: 1.15826408, Gradient norm: 0.39804754
INFO:root:[   41] Training loss: 0.02406072, Validation loss: 1.13614350, Gradient norm: 0.40352878
INFO:root:[   42] Training loss: 0.02393153, Validation loss: 1.24155909, Gradient norm: 0.38120840
INFO:root:[   43] Training loss: 0.02370459, Validation loss: 1.09824320, Gradient norm: 0.42410975
INFO:root:[   44] Training loss: 0.02345953, Validation loss: 1.16112832, Gradient norm: 0.38117255
INFO:root:[   45] Training loss: 0.02340150, Validation loss: 1.12143753, Gradient norm: 0.39701304
INFO:root:[   46] Training loss: 0.02292615, Validation loss: 1.14756506, Gradient norm: 0.35860653
INFO:root:[   47] Training loss: 0.02286635, Validation loss: 1.10123960, Gradient norm: 0.38140222
INFO:root:[   48] Training loss: 0.02266872, Validation loss: 1.10739894, Gradient norm: 0.39381379
INFO:root:[   49] Training loss: 0.02248554, Validation loss: 1.04968351, Gradient norm: 0.38598788
INFO:root:[   50] Training loss: 0.02222178, Validation loss: 1.07421857, Gradient norm: 0.37292916
