INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.32758425, Validation loss: 0.24735555, Gradient norm: 1.85227971
INFO:root:[    2] Training loss: 0.15712904, Validation loss: 0.18798675, Gradient norm: 1.63514266
INFO:root:[    3] Training loss: 0.13729223, Validation loss: 0.12150785, Gradient norm: 1.70065822
INFO:root:[    4] Training loss: 0.11713801, Validation loss: 0.11768087, Gradient norm: 1.40782852
INFO:root:[    5] Training loss: 0.10602219, Validation loss: 0.09752993, Gradient norm: 1.22289151
INFO:root:[    6] Training loss: 0.10769088, Validation loss: 0.12816706, Gradient norm: 1.33440335
INFO:root:[    7] Training loss: 0.09965073, Validation loss: 0.10096218, Gradient norm: 1.20845739
INFO:root:[    8] Training loss: 0.09680584, Validation loss: 0.11392433, Gradient norm: 1.15906074
INFO:root:[    9] Training loss: 0.08978824, Validation loss: 0.10092464, Gradient norm: 1.09199171
INFO:root:[   10] Training loss: 0.09071731, Validation loss: 0.09432693, Gradient norm: 1.07414791
INFO:root:[   11] Training loss: 0.08725766, Validation loss: 0.09873650, Gradient norm: 1.03197747
INFO:root:[   12] Training loss: 0.08494348, Validation loss: 0.09308024, Gradient norm: 1.02632988
INFO:root:[   13] Training loss: 0.07717586, Validation loss: 0.08504179, Gradient norm: 0.91945626
INFO:root:[   14] Training loss: 0.07639076, Validation loss: 0.09870328, Gradient norm: 0.92016389
INFO:root:[   15] Training loss: 0.07942660, Validation loss: 0.07899194, Gradient norm: 0.98276589
INFO:root:[   16] Training loss: 0.06993019, Validation loss: 0.08587239, Gradient norm: 0.86885314
INFO:root:[   17] Training loss: 0.06629245, Validation loss: 0.08816485, Gradient norm: 0.76894906
INFO:root:[   18] Training loss: 0.07062806, Validation loss: 0.08521532, Gradient norm: 0.89360186
INFO:root:[   19] Training loss: 0.06561470, Validation loss: 0.07759194, Gradient norm: 0.85993730
INFO:root:[   20] Training loss: 0.07075153, Validation loss: 0.07522330, Gradient norm: 0.92801771
INFO:root:[   21] Training loss: 0.06044881, Validation loss: 0.07636888, Gradient norm: 0.78007022
INFO:root:[   22] Training loss: 0.06348514, Validation loss: 0.09240045, Gradient norm: 0.79855161
INFO:root:[   23] Training loss: 0.05613659, Validation loss: 0.07573735, Gradient norm: 0.74468842
INFO:root:[   24] Training loss: 0.05904860, Validation loss: 0.09195254, Gradient norm: 0.91230761
INFO:root:[   25] Training loss: 0.05576013, Validation loss: 0.07364162, Gradient norm: 0.81715502
INFO:root:[   26] Training loss: 0.05750734, Validation loss: 0.07751771, Gradient norm: 0.84811527
INFO:root:[   27] Training loss: 0.05526217, Validation loss: 0.08339385, Gradient norm: 0.76419653
INFO:root:[   28] Training loss: 0.05299233, Validation loss: 0.07919948, Gradient norm: 0.74537391
INFO:root:[   29] Training loss: 0.05359542, Validation loss: 0.07754804, Gradient norm: 0.81763313
INFO:root:[   30] Training loss: 0.05062844, Validation loss: 0.07539485, Gradient norm: 0.83051662
INFO:root:[   31] Training loss: 0.04966358, Validation loss: 0.07501563, Gradient norm: 0.80095755
INFO:root:[   32] Training loss: 0.05371665, Validation loss: 0.07597142, Gradient norm: 0.84290278
INFO:root:[   33] Training loss: 0.04509373, Validation loss: 0.07350892, Gradient norm: 0.74890039
INFO:root:[   34] Training loss: 0.04729143, Validation loss: 0.08142825, Gradient norm: 0.92128853
INFO:root:[   35] Training loss: 0.04685580, Validation loss: 0.08004753, Gradient norm: 0.86977768
INFO:root:[   36] Training loss: 0.04493651, Validation loss: 0.07340742, Gradient norm: 0.76668131
INFO:root:[   37] Training loss: 0.04334150, Validation loss: 0.08720406, Gradient norm: 0.69622803
INFO:root:[   38] Training loss: 0.04602143, Validation loss: 0.07417634, Gradient norm: 0.82823266
INFO:root:[   39] Training loss: 0.04320178, Validation loss: 0.07898853, Gradient norm: 0.80992775
INFO:root:[   40] Training loss: 0.03900425, Validation loss: 0.07091503, Gradient norm: 0.71275503
INFO:root:[   41] Training loss: 0.04221151, Validation loss: 0.06890693, Gradient norm: 0.80603136
INFO:root:[   42] Training loss: 0.04332457, Validation loss: 0.07953416, Gradient norm: 0.83544289
INFO:root:[   43] Training loss: 0.03859753, Validation loss: 0.07488164, Gradient norm: 0.68731669
INFO:root:[   44] Training loss: 0.04007684, Validation loss: 0.07512183, Gradient norm: 0.83809246
INFO:root:[   45] Training loss: 0.03809536, Validation loss: 0.07432734, Gradient norm: 0.81368267
INFO:root:[   46] Training loss: 0.03770599, Validation loss: 0.07229655, Gradient norm: 0.73155330
INFO:root:[   47] Training loss: 0.03898257, Validation loss: 0.07890349, Gradient norm: 0.77427197
INFO:root:[   48] Training loss: 0.03873910, Validation loss: 0.07563192, Gradient norm: 0.86755765
INFO:root:[   49] Training loss: 0.03854025, Validation loss: 0.07594670, Gradient norm: 0.81866873
INFO:root:[   50] Training loss: 0.03667355, Validation loss: 0.06734050, Gradient norm: 0.81701053
INFO:root:[   51] Training loss: 0.03539541, Validation loss: 0.07382328, Gradient norm: 0.74413622
INFO:root:[   52] Training loss: 0.03509263, Validation loss: 0.07221267, Gradient norm: 0.79251836
INFO:root:[   53] Training loss: 0.03468733, Validation loss: 0.06704018, Gradient norm: 0.74493754
INFO:root:[   54] Training loss: 0.03498328, Validation loss: 0.06836712, Gradient norm: 0.77248682
INFO:root:[   55] Training loss: 0.03692290, Validation loss: 0.07734166, Gradient norm: 0.74584422
INFO:root:[   56] Training loss: 0.03188101, Validation loss: 0.07765386, Gradient norm: 0.72573080
INFO:root:[   57] Training loss: 0.03709228, Validation loss: 0.07403626, Gradient norm: 0.85899622
INFO:root:[   58] Training loss: 0.03346885, Validation loss: 0.06830508, Gradient norm: 0.79358400
INFO:root:[   59] Training loss: 0.03606735, Validation loss: 0.06751714, Gradient norm: 0.87125988
INFO:root:[   60] Training loss: 0.03245819, Validation loss: 0.06754334, Gradient norm: 0.77674687
INFO:root:[   61] Training loss: 0.03313362, Validation loss: 0.07489566, Gradient norm: 0.77780812
INFO:root:[   62] Training loss: 0.03180206, Validation loss: 0.06933052, Gradient norm: 0.76368814
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 3152.81s.
INFO:root:Emptying the cuda cache took 0.014s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04569
INFO:root:EnergyScoreTrain: 0.03062
INFO:root:CRPSTrain: 0.02691
INFO:root:Gaussian NLLTrain: 1.12446
INFO:root:CoverageTrain: 0.57136
INFO:root:IntervalWidthTrain: 0.05728
INFO:root:Evaluating the model on Validation data.
