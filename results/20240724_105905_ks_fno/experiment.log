INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 2097152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 127.30115374, Validation loss: 123.56973346, Gradient norm: 354.20956367
INFO:root:[    2] Training loss: 122.24473275, Validation loss: 121.96978207, Gradient norm: 352.56908633
INFO:root:[    3] Training loss: 121.82311148, Validation loss: 121.66176368, Gradient norm: 300.81428430
INFO:root:[    4] Training loss: 121.65760432, Validation loss: 121.28311920, Gradient norm: 353.65485755
INFO:root:[    5] Training loss: 121.56784078, Validation loss: 121.23546127, Gradient norm: 364.01866566
INFO:root:[    6] Training loss: 121.42655749, Validation loss: 121.20740062, Gradient norm: 258.49316205
INFO:root:[    7] Training loss: 121.39306789, Validation loss: 121.18168061, Gradient norm: 311.36793974
INFO:root:[    8] Training loss: 121.28766186, Validation loss: 121.06333450, Gradient norm: 333.11313845
INFO:root:[    9] Training loss: 121.19071508, Validation loss: 120.85660027, Gradient norm: 293.42309788
INFO:root:[   10] Training loss: 120.88952603, Validation loss: 120.92804771, Gradient norm: 262.21619030
INFO:root:[   11] Training loss: 120.83392948, Validation loss: 120.59077901, Gradient norm: 271.94457883
INFO:root:[   12] Training loss: 120.66979926, Validation loss: 120.46509736, Gradient norm: 252.64762535
INFO:root:[   13] Training loss: 120.56178122, Validation loss: 120.48619579, Gradient norm: 241.19498572
INFO:root:[   14] Training loss: 120.47163634, Validation loss: 120.56513977, Gradient norm: 213.96931494
INFO:root:[   15] Training loss: 120.43210865, Validation loss: 120.53071778, Gradient norm: 184.33462286
INFO:root:[   16] Training loss: 120.47794599, Validation loss: 120.24051456, Gradient norm: 255.56316167
INFO:root:[   17] Training loss: 120.40695670, Validation loss: 120.21929063, Gradient norm: 214.22781763
INFO:root:[   18] Training loss: 120.35734153, Validation loss: 120.27142676, Gradient norm: 199.55031626
INFO:root:[   19] Training loss: 120.30036906, Validation loss: 120.36923954, Gradient norm: 178.86294569
INFO:root:[   20] Training loss: 120.22184888, Validation loss: 120.15669645, Gradient norm: 189.17747579
INFO:root:[   21] Training loss: 120.18754456, Validation loss: 120.12667610, Gradient norm: 200.09672144
INFO:root:[   22] Training loss: 120.00993219, Validation loss: 119.75204099, Gradient norm: 227.15353823
INFO:root:[   23] Training loss: 119.28944032, Validation loss: 118.79791628, Gradient norm: 215.99977334
INFO:root:[   24] Training loss: 118.04264015, Validation loss: 117.49849333, Gradient norm: 174.71473665
INFO:root:[   25] Training loss: 116.70427022, Validation loss: 116.17584123, Gradient norm: 164.96926274
INFO:root:[   26] Training loss: 115.57302708, Validation loss: 115.10561476, Gradient norm: 134.16974708
INFO:root:[   27] Training loss: 114.78706319, Validation loss: 114.62056890, Gradient norm: 174.13305258
INFO:root:[   28] Training loss: 114.05971568, Validation loss: 113.77781756, Gradient norm: 167.44862547
INFO:root:[   29] Training loss: 113.47068537, Validation loss: 113.17770096, Gradient norm: 158.37631952
INFO:root:[   30] Training loss: 112.90247122, Validation loss: 112.84227595, Gradient norm: 128.61953615
INFO:root:[   31] Training loss: 112.48592627, Validation loss: 112.44218997, Gradient norm: 121.58719500
INFO:root:[   32] Training loss: 112.10450029, Validation loss: 112.24958880, Gradient norm: 140.24115877
INFO:root:[   33] Training loss: 111.70092058, Validation loss: 111.61715698, Gradient norm: 99.55739094
INFO:root:[   34] Training loss: 111.45160790, Validation loss: 111.40183074, Gradient norm: 122.22513071
INFO:root:[   35] Training loss: 111.14908411, Validation loss: 111.19688258, Gradient norm: 135.74607577
INFO:root:[   36] Training loss: 110.90576077, Validation loss: 110.92807375, Gradient norm: 145.63356205
INFO:root:[   37] Training loss: 110.65033911, Validation loss: 110.79161361, Gradient norm: 119.77803732
INFO:root:[   38] Training loss: 110.41279298, Validation loss: 110.53592208, Gradient norm: 98.85662313
INFO:root:[   39] Training loss: 110.28212076, Validation loss: 110.53301423, Gradient norm: 147.39629584
INFO:root:[   40] Training loss: 110.04729745, Validation loss: 110.59060143, Gradient norm: 106.49055198
INFO:root:[   41] Training loss: 109.88059167, Validation loss: 110.35627878, Gradient norm: 110.27125310
INFO:root:[   42] Training loss: 109.70761108, Validation loss: 109.88211244, Gradient norm: 124.68787231
INFO:root:[   43] Training loss: 109.54739495, Validation loss: 109.89261996, Gradient norm: 131.89665749
INFO:root:[   44] Training loss: 109.35467009, Validation loss: 109.69971387, Gradient norm: 119.92070612
INFO:root:[   45] Training loss: 109.19273073, Validation loss: 109.54671057, Gradient norm: 88.64513439
INFO:root:[   46] Training loss: 109.01903149, Validation loss: 109.49359420, Gradient norm: 99.68019317
INFO:root:[   47] Training loss: 108.85883534, Validation loss: 109.26231253, Gradient norm: 103.08835962
INFO:root:[   48] Training loss: 108.72256375, Validation loss: 109.02672182, Gradient norm: 112.12322828
INFO:root:[   49] Training loss: 108.60138918, Validation loss: 108.93625325, Gradient norm: 97.78510898
INFO:root:[   50] Training loss: 108.44606720, Validation loss: 108.90538130, Gradient norm: 126.08652539
INFO:root:[   51] Training loss: 108.31154754, Validation loss: 108.73259183, Gradient norm: 110.66160462
INFO:root:[   52] Training loss: 108.15886972, Validation loss: 108.60609410, Gradient norm: 96.81341890
INFO:root:[   53] Training loss: 108.00974193, Validation loss: 108.36448459, Gradient norm: 75.49493607
INFO:root:[   54] Training loss: 107.92289795, Validation loss: 108.29196614, Gradient norm: 105.12547028
INFO:root:[   55] Training loss: 107.79449625, Validation loss: 108.35560450, Gradient norm: 91.59299942
INFO:root:[   56] Training loss: 107.63446396, Validation loss: 108.15746071, Gradient norm: 102.64807369
INFO:root:[   57] Training loss: 107.55533458, Validation loss: 108.20162806, Gradient norm: 115.19023271
INFO:root:[   58] Training loss: 107.41777120, Validation loss: 107.87381139, Gradient norm: 106.16026385
INFO:root:[   59] Training loss: 107.32002380, Validation loss: 107.95870498, Gradient norm: 93.80793663
INFO:root:[   60] Training loss: 107.16276125, Validation loss: 107.87830274, Gradient norm: 94.50428456
INFO:root:[   61] Training loss: 107.12248264, Validation loss: 107.69577737, Gradient norm: 99.76151415
INFO:root:[   62] Training loss: 106.97205110, Validation loss: 107.66479098, Gradient norm: 104.17692702
INFO:root:[   63] Training loss: 106.92156051, Validation loss: 107.40497615, Gradient norm: 96.64107632
INFO:root:[   64] Training loss: 106.81228469, Validation loss: 107.28464377, Gradient norm: 92.98961731
INFO:root:[   65] Training loss: 106.67534982, Validation loss: 107.47226215, Gradient norm: 86.27464378
INFO:root:[   66] Training loss: 106.62557025, Validation loss: 107.25229513, Gradient norm: 113.89354911
INFO:root:[   67] Training loss: 106.49996388, Validation loss: 107.09406123, Gradient norm: 101.13065578
INFO:root:[   68] Training loss: 106.38722965, Validation loss: 107.19835189, Gradient norm: 76.70688653
INFO:root:[   69] Training loss: 106.33855843, Validation loss: 107.28794729, Gradient norm: 98.22993011
INFO:root:[   70] Training loss: 106.20964422, Validation loss: 107.22356099, Gradient norm: 81.94695107
INFO:root:[   71] Training loss: 106.13559723, Validation loss: 106.82441106, Gradient norm: 89.77142477
INFO:root:[   72] Training loss: 106.10527288, Validation loss: 107.27271639, Gradient norm: 97.24211380
INFO:root:[   73] Training loss: 105.98102833, Validation loss: 106.85442194, Gradient norm: 97.80059349
INFO:root:[   74] Training loss: 105.85921418, Validation loss: 106.69697676, Gradient norm: 81.20340195
INFO:root:[   75] Training loss: 105.77839134, Validation loss: 106.70022294, Gradient norm: 103.90244964
INFO:root:[   76] Training loss: 105.71145927, Validation loss: 106.73405693, Gradient norm: 84.76634638
INFO:root:[   77] Training loss: 105.66104781, Validation loss: 106.56827966, Gradient norm: 77.92225950
INFO:root:[   78] Training loss: 105.57704298, Validation loss: 106.32262421, Gradient norm: 101.67851655
INFO:root:[   79] Training loss: 105.47678125, Validation loss: 106.39206748, Gradient norm: 86.21690212
INFO:root:[   80] Training loss: 105.41222989, Validation loss: 106.14076338, Gradient norm: 81.76183870
INFO:root:[   81] Training loss: 105.36871270, Validation loss: 106.24794217, Gradient norm: 93.29096799
INFO:root:[   82] Training loss: 105.23085265, Validation loss: 106.07978847, Gradient norm: 77.75413018
INFO:root:[   83] Training loss: 105.15038212, Validation loss: 106.14313165, Gradient norm: 86.15155932
INFO:root:[   84] Training loss: 105.07118023, Validation loss: 106.20057599, Gradient norm: 87.21756326
INFO:root:[   85] Training loss: 105.08506086, Validation loss: 106.02123287, Gradient norm: 69.15107058
INFO:root:[   86] Training loss: 104.95747646, Validation loss: 106.01930395, Gradient norm: 80.01122535
INFO:root:[   87] Training loss: 104.93097930, Validation loss: 106.03609072, Gradient norm: 76.87567866
INFO:root:[   88] Training loss: 104.85612967, Validation loss: 106.17904084, Gradient norm: 79.39463862
INFO:root:[   89] Training loss: 104.80806820, Validation loss: 105.89093254, Gradient norm: 78.68901627
INFO:root:[   90] Training loss: 104.70341269, Validation loss: 105.89783951, Gradient norm: 97.55904398
INFO:root:[   91] Training loss: 104.65243929, Validation loss: 105.78196295, Gradient norm: 84.18058790
INFO:root:[   92] Training loss: 104.58180291, Validation loss: 105.84608828, Gradient norm: 83.54293217
INFO:root:[   93] Training loss: 104.59692666, Validation loss: 105.60554320, Gradient norm: 84.72872416
INFO:root:[   94] Training loss: 104.43571506, Validation loss: 105.57478938, Gradient norm: 82.95865505
INFO:root:[   95] Training loss: 104.40305558, Validation loss: 105.62516601, Gradient norm: 78.00261262
INFO:root:[   96] Training loss: 104.37780620, Validation loss: 105.33516825, Gradient norm: 80.18567683
INFO:root:[   97] Training loss: 104.27173115, Validation loss: 105.55685898, Gradient norm: 86.56002749
INFO:root:[   98] Training loss: 104.19128121, Validation loss: 105.34590754, Gradient norm: 77.13224220
INFO:root:[   99] Training loss: 104.18205984, Validation loss: 105.43218494, Gradient norm: 75.59073929
INFO:root:[  100] Training loss: 104.04185283, Validation loss: 105.20014296, Gradient norm: 72.42638835
INFO:root:[  101] Training loss: 104.01616770, Validation loss: 105.37890967, Gradient norm: 83.02062859
INFO:root:[  102] Training loss: 104.01541266, Validation loss: 105.51582442, Gradient norm: 77.64853236
INFO:root:[  103] Training loss: 103.98920589, Validation loss: 105.35057068, Gradient norm: 94.86233632
INFO:root:[  104] Training loss: 103.85880475, Validation loss: 105.18772415, Gradient norm: 84.44140186
INFO:root:[  105] Training loss: 103.79756435, Validation loss: 104.94505494, Gradient norm: 62.76782032
INFO:root:[  106] Training loss: 103.78273841, Validation loss: 105.12306740, Gradient norm: 89.03725077
INFO:root:[  107] Training loss: 103.78700229, Validation loss: 104.99442291, Gradient norm: 81.10949894
INFO:root:[  108] Training loss: 103.63670957, Validation loss: 105.04194799, Gradient norm: 71.81297494
INFO:root:[  109] Training loss: 103.62880045, Validation loss: 105.04509314, Gradient norm: 79.27473438
INFO:root:[  110] Training loss: 103.56816459, Validation loss: 105.02336357, Gradient norm: 70.71280808
INFO:root:[  111] Training loss: 103.52620515, Validation loss: 105.07257264, Gradient norm: 73.32260779
INFO:root:[  112] Training loss: 103.49302295, Validation loss: 104.92908846, Gradient norm: 77.15016230
INFO:root:[  113] Training loss: 103.42032846, Validation loss: 104.96494898, Gradient norm: 75.91972588
INFO:root:[  114] Training loss: 103.42726277, Validation loss: 104.75089395, Gradient norm: 81.48044640
INFO:root:[  115] Training loss: 103.28428326, Validation loss: 104.98417611, Gradient norm: 72.12466858
INFO:root:[  116] Training loss: 103.33108419, Validation loss: 104.88246970, Gradient norm: 69.39173484
INFO:root:[  117] Training loss: 103.19980655, Validation loss: 104.67432298, Gradient norm: 72.96666259
INFO:root:[  118] Training loss: 103.20790262, Validation loss: 104.64700212, Gradient norm: 78.69914584
INFO:root:[  119] Training loss: 103.10317088, Validation loss: 104.51033757, Gradient norm: 81.35576474
INFO:root:[  120] Training loss: 103.06398044, Validation loss: 104.65176628, Gradient norm: 64.51327547
INFO:root:[  121] Training loss: 103.09180579, Validation loss: 104.96078570, Gradient norm: 72.55290070
INFO:root:[  122] Training loss: 103.01802353, Validation loss: 104.75871961, Gradient norm: 80.46275134
INFO:root:[  123] Training loss: 102.97330920, Validation loss: 104.49886717, Gradient norm: 75.74485501
INFO:root:[  124] Training loss: 102.93475065, Validation loss: 104.43621774, Gradient norm: 73.09148356
INFO:root:[  125] Training loss: 102.85860538, Validation loss: 104.48073210, Gradient norm: 67.94344703
INFO:root:[  126] Training loss: 102.86506376, Validation loss: 104.45520756, Gradient norm: 62.78726886
INFO:root:[  127] Training loss: 102.80585392, Validation loss: 104.53417548, Gradient norm: 74.55282660
INFO:root:[  128] Training loss: 102.72270466, Validation loss: 104.53330757, Gradient norm: 77.24491680
INFO:root:[  129] Training loss: 102.71922424, Validation loss: 104.36720460, Gradient norm: 70.64894102
INFO:root:[  130] Training loss: 102.62388651, Validation loss: 104.55134977, Gradient norm: 83.00366225
INFO:root:[  131] Training loss: 102.59625467, Validation loss: 104.31689821, Gradient norm: 69.58933973
INFO:root:[  132] Training loss: 102.56748929, Validation loss: 104.25322460, Gradient norm: 66.99835715
INFO:root:[  133] Training loss: 102.58323865, Validation loss: 104.33452291, Gradient norm: 75.87703042
INFO:root:[  134] Training loss: 102.51903102, Validation loss: 104.26127730, Gradient norm: 72.15864254
INFO:root:[  135] Training loss: 102.45597360, Validation loss: 104.32595904, Gradient norm: 72.49269810
INFO:root:[  136] Training loss: 102.44011621, Validation loss: 104.39142872, Gradient norm: 70.92969071
INFO:root:[  137] Training loss: 102.33160589, Validation loss: 104.06660172, Gradient norm: 77.48188410
INFO:root:[  138] Training loss: 102.35072657, Validation loss: 104.26819690, Gradient norm: 65.49725929
INFO:root:[  139] Training loss: 102.33337828, Validation loss: 104.18176296, Gradient norm: 82.92621353
INFO:root:[  140] Training loss: 102.24268868, Validation loss: 104.22928462, Gradient norm: 71.40340625
INFO:root:[  141] Training loss: 102.23251471, Validation loss: 104.25595724, Gradient norm: 70.12460058
INFO:root:[  142] Training loss: 102.18757130, Validation loss: 104.21502422, Gradient norm: 65.91020071
INFO:root:[  143] Training loss: 102.13585224, Validation loss: 104.25486861, Gradient norm: 63.86791674
INFO:root:[  144] Training loss: 102.13671058, Validation loss: 104.50114415, Gradient norm: 67.66099683
INFO:root:[  145] Training loss: 102.07642810, Validation loss: 103.96396111, Gradient norm: 65.13290739
INFO:root:[  146] Training loss: 101.96994714, Validation loss: 103.94060806, Gradient norm: 66.55828345
INFO:root:[  147] Training loss: 101.98346602, Validation loss: 104.06457520, Gradient norm: 65.47620455
INFO:root:[  148] Training loss: 101.96584941, Validation loss: 103.86963732, Gradient norm: 66.30271863
INFO:root:[  149] Training loss: 101.91377468, Validation loss: 103.98849066, Gradient norm: 71.27076432
INFO:root:[  150] Training loss: 101.85939910, Validation loss: 103.99019018, Gradient norm: 71.61958347
INFO:root:[  151] Training loss: 101.79154624, Validation loss: 104.11300922, Gradient norm: 75.58022223
INFO:root:[  152] Training loss: 101.80125225, Validation loss: 103.94193373, Gradient norm: 64.38940164
INFO:root:[  153] Training loss: 101.73219448, Validation loss: 103.99098206, Gradient norm: 67.00172507
INFO:root:[  154] Training loss: 101.81090222, Validation loss: 104.13417527, Gradient norm: 71.08644217
INFO:root:[  155] Training loss: 101.65797391, Validation loss: 104.08837391, Gradient norm: 55.39774475
INFO:root:[  156] Training loss: 101.61424512, Validation loss: 104.08283786, Gradient norm: 69.91537287
INFO:root:[  157] Training loss: 101.56150284, Validation loss: 103.78547774, Gradient norm: 69.23349657
INFO:root:[  158] Training loss: 101.58711749, Validation loss: 103.86150702, Gradient norm: 59.51497972
INFO:root:[  159] Training loss: 101.58228153, Validation loss: 103.89436104, Gradient norm: 73.90142266
INFO:root:[  160] Training loss: 101.53062432, Validation loss: 103.87242205, Gradient norm: 64.45240501
INFO:root:[  161] Training loss: 101.52087510, Validation loss: 103.80565670, Gradient norm: 65.09174495
INFO:root:[  162] Training loss: 101.44577168, Validation loss: 103.75419406, Gradient norm: 76.36749309
INFO:root:[  163] Training loss: 101.36996264, Validation loss: 103.98117171, Gradient norm: 65.19890662
INFO:root:[  164] Training loss: 101.35257215, Validation loss: 103.83255899, Gradient norm: 62.80939411
INFO:root:[  165] Training loss: 101.34575491, Validation loss: 103.76497940, Gradient norm: 63.82357378
INFO:root:[  166] Training loss: 101.28197148, Validation loss: 103.86316155, Gradient norm: 69.68187313
INFO:root:[  167] Training loss: 101.28384082, Validation loss: 103.72062736, Gradient norm: 63.48268180
INFO:root:[  168] Training loss: 101.30399734, Validation loss: 103.74602456, Gradient norm: 73.24169270
INFO:root:[  169] Training loss: 101.22492232, Validation loss: 103.67183501, Gradient norm: 65.33436638
INFO:root:[  170] Training loss: 101.20835424, Validation loss: 103.82387858, Gradient norm: 73.08790959
INFO:root:[  171] Training loss: 101.13974148, Validation loss: 103.82214987, Gradient norm: 70.39483636
INFO:root:[  172] Training loss: 101.10145900, Validation loss: 104.06269600, Gradient norm: 65.24699602
INFO:root:[  173] Training loss: 101.04093177, Validation loss: 103.96514419, Gradient norm: 66.17353465
INFO:root:[  174] Training loss: 101.02876383, Validation loss: 103.78228707, Gradient norm: 65.53366781
INFO:root:[  175] Training loss: 101.06039847, Validation loss: 103.73029512, Gradient norm: 68.86174936
INFO:root:[  176] Training loss: 100.96808280, Validation loss: 103.73794503, Gradient norm: 66.27984676
INFO:root:[  177] Training loss: 100.94011871, Validation loss: 103.63342364, Gradient norm: 64.49326619
INFO:root:[  178] Training loss: 100.91119081, Validation loss: 103.83301860, Gradient norm: 54.66959510
INFO:root:[  179] Training loss: 100.82761883, Validation loss: 103.71405029, Gradient norm: 62.94489226
INFO:root:[  180] Training loss: 100.81857185, Validation loss: 103.82465705, Gradient norm: 60.95753029
INFO:root:[  181] Training loss: 100.77354249, Validation loss: 103.68939867, Gradient norm: 68.92340357
INFO:root:[  182] Training loss: 100.77903153, Validation loss: 103.59055434, Gradient norm: 70.18044766
INFO:root:[  183] Training loss: 100.78459654, Validation loss: 103.43746212, Gradient norm: 60.14933388
INFO:root:[  184] Training loss: 100.74569479, Validation loss: 103.43316282, Gradient norm: 67.52532484
INFO:root:[  185] Training loss: 100.74110770, Validation loss: 103.69436698, Gradient norm: 67.71787169
INFO:root:[  186] Training loss: 100.55914786, Validation loss: 103.89056923, Gradient norm: 55.78234960
INFO:root:[  187] Training loss: 100.65384890, Validation loss: 103.58243587, Gradient norm: 71.50410767
INFO:root:[  188] Training loss: 100.54854590, Validation loss: 103.49451762, Gradient norm: 62.51171305
INFO:root:[  189] Training loss: 100.48030455, Validation loss: 103.76490573, Gradient norm: 63.47098632
INFO:root:[  190] Training loss: 100.45611235, Validation loss: 103.63748011, Gradient norm: 65.66235831
INFO:root:[  191] Training loss: 100.55032970, Validation loss: 103.57711371, Gradient norm: 65.12283472
INFO:root:[  192] Training loss: 100.42899066, Validation loss: 103.73789583, Gradient norm: 59.25242074
INFO:root:[  193] Training loss: 100.41739823, Validation loss: 103.76181241, Gradient norm: 66.54705781
INFO:root:EP 193: Early stopping
INFO:root:Training the model took 2368.197s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 9095.9812
INFO:root:EnergyScoreTrain: 6404.61559
INFO:root:CoverageTrain: 0.8124
INFO:root:IntervalWidthTrain: 8.26915
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 9134.08752
INFO:root:EnergyScoreValidation: 6434.008
INFO:root:CoverageValidation: 0.80408
INFO:root:IntervalWidthValidation: 8.2539
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9205.18926
INFO:root:EnergyScoreTest: 6484.40106
INFO:root:CoverageTest: 0.80401
INFO:root:IntervalWidthTest: 8.25249
INFO:root:###2 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 125.34770209, Validation loss: 123.49148428, Gradient norm: 236.51413360
INFO:root:[    2] Training loss: 122.54443791, Validation loss: 122.06803657, Gradient norm: 172.39094330
INFO:root:[    3] Training loss: 121.99015909, Validation loss: 121.92133226, Gradient norm: 160.23066635
INFO:root:[    4] Training loss: 121.80048168, Validation loss: 121.60143359, Gradient norm: 141.27043503
INFO:root:[    5] Training loss: 121.60419248, Validation loss: 121.87338388, Gradient norm: 115.28022389
INFO:root:[    6] Training loss: 121.57716032, Validation loss: 121.69560663, Gradient norm: 105.87825243
INFO:root:[    7] Training loss: 121.67816176, Validation loss: 121.75774541, Gradient norm: 138.61350291
INFO:root:[    8] Training loss: 121.45829759, Validation loss: 121.40987870, Gradient norm: 122.21989213
INFO:root:[    9] Training loss: 121.37927455, Validation loss: 121.15374598, Gradient norm: 136.19677204
INFO:root:[   10] Training loss: 121.34223283, Validation loss: 121.10334909, Gradient norm: 122.83916094
INFO:root:[   11] Training loss: 121.26311520, Validation loss: 121.12295374, Gradient norm: 120.45280999
INFO:root:[   12] Training loss: 121.15546080, Validation loss: 120.99811370, Gradient norm: 114.07755515
INFO:root:[   13] Training loss: 121.04314396, Validation loss: 120.91377074, Gradient norm: 111.25312612
INFO:root:[   14] Training loss: 120.92599676, Validation loss: 121.25478994, Gradient norm: 116.97508241
INFO:root:[   15] Training loss: 120.90718997, Validation loss: 120.99594195, Gradient norm: 151.91272866
INFO:root:[   16] Training loss: 120.84085205, Validation loss: 121.08173186, Gradient norm: 96.27746353
INFO:root:[   17] Training loss: 120.77573624, Validation loss: 120.76062143, Gradient norm: 111.07861603
INFO:root:[   18] Training loss: 120.73219995, Validation loss: 120.67377104, Gradient norm: 127.96894864
INFO:root:[   19] Training loss: 120.69706274, Validation loss: 120.62296479, Gradient norm: 121.54622771
INFO:root:[   20] Training loss: 120.63607221, Validation loss: 120.50322750, Gradient norm: 88.96097864
INFO:root:[   21] Training loss: 120.60648359, Validation loss: 120.38591609, Gradient norm: 101.11105606
INFO:root:[   22] Training loss: 120.52601029, Validation loss: 120.57584328, Gradient norm: 99.44356327
INFO:root:[   23] Training loss: 120.46638921, Validation loss: 120.37215108, Gradient norm: 94.07720760
INFO:root:[   24] Training loss: 120.46856379, Validation loss: 120.38823384, Gradient norm: 96.37336896
INFO:root:[   25] Training loss: 120.44209452, Validation loss: 120.57276232, Gradient norm: 107.13819587
INFO:root:[   26] Training loss: 120.42714192, Validation loss: 120.32117883, Gradient norm: 80.50966288
INFO:root:[   27] Training loss: 120.38184343, Validation loss: 120.26405282, Gradient norm: 94.15892128
INFO:root:[   28] Training loss: 120.36544773, Validation loss: 120.25107890, Gradient norm: 93.10958480
INFO:root:[   29] Training loss: 120.31861938, Validation loss: 120.26127598, Gradient norm: 75.79362634
INFO:root:[   30] Training loss: 120.33014382, Validation loss: 120.20185036, Gradient norm: 93.64151033
INFO:root:[   31] Training loss: 120.32080220, Validation loss: 120.25625058, Gradient norm: 105.67560332
INFO:root:[   32] Training loss: 120.27516525, Validation loss: 120.33026281, Gradient norm: 96.51117929
INFO:root:[   33] Training loss: 120.25324418, Validation loss: 120.19706568, Gradient norm: 100.98005923
INFO:root:[   34] Training loss: 120.19037311, Validation loss: 120.08630555, Gradient norm: 76.38066305
INFO:root:[   35] Training loss: 120.15183373, Validation loss: 120.17725188, Gradient norm: 80.69375774
INFO:root:[   36] Training loss: 120.08647709, Validation loss: 120.09295049, Gradient norm: 78.52839785
INFO:root:[   37] Training loss: 120.04584780, Validation loss: 120.03812251, Gradient norm: 79.92083575
INFO:root:[   38] Training loss: 119.86675553, Validation loss: 119.73153081, Gradient norm: 80.17605275
INFO:root:[   39] Training loss: 119.38073751, Validation loss: 119.22365123, Gradient norm: 72.34808822
INFO:root:[   40] Training loss: 118.70892759, Validation loss: 118.32846964, Gradient norm: 89.31227922
INFO:root:[   41] Training loss: 117.99468839, Validation loss: 117.84084452, Gradient norm: 85.99528330
INFO:root:[   42] Training loss: 117.39538750, Validation loss: 117.12508419, Gradient norm: 84.40288756
INFO:root:[   43] Training loss: 116.84681202, Validation loss: 116.66043512, Gradient norm: 71.81110642
INFO:root:[   44] Training loss: 116.38584069, Validation loss: 116.34908584, Gradient norm: 82.16613652
INFO:root:[   45] Training loss: 115.88575042, Validation loss: 115.98249659, Gradient norm: 57.56756434
INFO:root:[   46] Training loss: 115.44772400, Validation loss: 115.36935977, Gradient norm: 72.62410313
INFO:root:[   47] Training loss: 115.01560353, Validation loss: 114.99382914, Gradient norm: 57.05244656
INFO:root:[   48] Training loss: 114.64632152, Validation loss: 114.65696374, Gradient norm: 66.86850124
INFO:root:[   49] Training loss: 114.14558505, Validation loss: 114.08238510, Gradient norm: 54.79219334
INFO:root:[   50] Training loss: 113.83934399, Validation loss: 114.01675810, Gradient norm: 63.89136604
INFO:root:[   51] Training loss: 113.47573778, Validation loss: 113.47442022, Gradient norm: 65.87388779
INFO:root:[   52] Training loss: 113.10438625, Validation loss: 113.20481925, Gradient norm: 58.88390507
INFO:root:[   53] Training loss: 112.83269021, Validation loss: 112.88852823, Gradient norm: 62.78392995
INFO:root:[   54] Training loss: 112.48538606, Validation loss: 112.52790043, Gradient norm: 49.76042862
INFO:root:[   55] Training loss: 112.21822033, Validation loss: 112.26549346, Gradient norm: 53.56967119
INFO:root:[   56] Training loss: 111.90843295, Validation loss: 112.09112628, Gradient norm: 68.17839995
INFO:root:[   57] Training loss: 111.64657080, Validation loss: 111.65867536, Gradient norm: 55.81875504
INFO:root:[   58] Training loss: 111.39713490, Validation loss: 111.42749155, Gradient norm: 45.44946828
INFO:root:[   59] Training loss: 111.20824635, Validation loss: 111.36603099, Gradient norm: 47.28807712
INFO:root:[   60] Training loss: 110.97211477, Validation loss: 111.00565207, Gradient norm: 51.91964602
INFO:root:[   61] Training loss: 110.69814935, Validation loss: 110.74111281, Gradient norm: 48.39614119
INFO:root:[   62] Training loss: 110.55700501, Validation loss: 110.80034822, Gradient norm: 47.07211489
INFO:root:[   63] Training loss: 110.32564936, Validation loss: 110.48519161, Gradient norm: 52.15819344
INFO:root:[   64] Training loss: 110.15806019, Validation loss: 110.49115885, Gradient norm: 51.15838487
INFO:root:[   65] Training loss: 110.00746168, Validation loss: 110.40471781, Gradient norm: 42.89563603
INFO:root:[   66] Training loss: 109.83429266, Validation loss: 110.15535736, Gradient norm: 45.59783093
INFO:root:[   67] Training loss: 109.66435005, Validation loss: 109.98660278, Gradient norm: 50.60095860
INFO:root:[   68] Training loss: 109.49519362, Validation loss: 109.77390316, Gradient norm: 45.66070922
INFO:root:[   69] Training loss: 109.30761901, Validation loss: 109.65727023, Gradient norm: 44.99476479
INFO:root:[   70] Training loss: 109.23309299, Validation loss: 109.65269260, Gradient norm: 44.91857824
INFO:root:[   71] Training loss: 109.14173450, Validation loss: 109.29597605, Gradient norm: 39.51098427
INFO:root:[   72] Training loss: 108.93911203, Validation loss: 109.15610425, Gradient norm: 46.67665971
INFO:root:[   73] Training loss: 108.76107005, Validation loss: 109.19540747, Gradient norm: 45.78196067
INFO:root:[   74] Training loss: 108.67010910, Validation loss: 109.09172953, Gradient norm: 40.95912849
INFO:root:[   75] Training loss: 108.51572452, Validation loss: 108.90173550, Gradient norm: 45.30908710
INFO:root:[   76] Training loss: 108.41692102, Validation loss: 108.78799702, Gradient norm: 45.28209330
INFO:root:[   77] Training loss: 108.28435753, Validation loss: 108.55084307, Gradient norm: 46.40952107
INFO:root:[   78] Training loss: 108.14616725, Validation loss: 108.56969189, Gradient norm: 41.68693947
INFO:root:[   79] Training loss: 107.98951687, Validation loss: 108.49960906, Gradient norm: 38.92915981
INFO:root:[   80] Training loss: 107.93193493, Validation loss: 108.29150180, Gradient norm: 45.21899599
INFO:root:[   81] Training loss: 107.78942290, Validation loss: 108.27769970, Gradient norm: 41.68362958
INFO:root:[   82] Training loss: 107.69856289, Validation loss: 108.12199218, Gradient norm: 39.87185831
INFO:root:[   83] Training loss: 107.57784906, Validation loss: 107.87062836, Gradient norm: 42.11178384
INFO:root:[   84] Training loss: 107.47278075, Validation loss: 107.85380528, Gradient norm: 39.17587460
INFO:root:[   85] Training loss: 107.35708530, Validation loss: 107.77201001, Gradient norm: 36.78141970
INFO:root:[   86] Training loss: 107.17923683, Validation loss: 107.64750724, Gradient norm: 39.56585110
INFO:root:[   87] Training loss: 107.09903312, Validation loss: 107.58261135, Gradient norm: 34.31888775
INFO:root:[   88] Training loss: 107.05830228, Validation loss: 107.50640501, Gradient norm: 44.39831116
INFO:root:[   89] Training loss: 106.97105874, Validation loss: 107.36786546, Gradient norm: 40.49438344
INFO:root:[   90] Training loss: 106.82913606, Validation loss: 107.30295063, Gradient norm: 38.02199787
INFO:root:[   91] Training loss: 106.72362397, Validation loss: 107.15213302, Gradient norm: 37.82569950
INFO:root:[   92] Training loss: 106.68725302, Validation loss: 107.11036630, Gradient norm: 40.21335185
INFO:root:[   93] Training loss: 106.55506715, Validation loss: 107.06934251, Gradient norm: 45.11118715
INFO:root:[   94] Training loss: 106.46898928, Validation loss: 107.00007708, Gradient norm: 40.05922486
INFO:root:[   95] Training loss: 106.31916802, Validation loss: 106.94779416, Gradient norm: 34.00383371
INFO:root:[   96] Training loss: 106.26666618, Validation loss: 107.07792637, Gradient norm: 36.44076208
INFO:root:[   97] Training loss: 106.20775827, Validation loss: 106.99763042, Gradient norm: 33.85755109
INFO:root:[   98] Training loss: 106.07907921, Validation loss: 106.69464901, Gradient norm: 33.81153745
INFO:root:[   99] Training loss: 106.01902643, Validation loss: 106.76290736, Gradient norm: 36.98119158
INFO:root:[  100] Training loss: 105.85963730, Validation loss: 106.49487226, Gradient norm: 36.79749036
INFO:root:[  101] Training loss: 105.75410704, Validation loss: 106.41035093, Gradient norm: 33.41779449
INFO:root:[  102] Training loss: 105.74866803, Validation loss: 106.39295802, Gradient norm: 38.03595337
INFO:root:[  103] Training loss: 105.64436880, Validation loss: 106.32621002, Gradient norm: 35.11635048
INFO:root:[  104] Training loss: 105.55107880, Validation loss: 106.18439852, Gradient norm: 35.64961130
INFO:root:[  105] Training loss: 105.46380149, Validation loss: 106.08430113, Gradient norm: 38.26914091
INFO:root:[  106] Training loss: 105.33189345, Validation loss: 106.07033933, Gradient norm: 36.79693194
INFO:root:[  107] Training loss: 105.26003745, Validation loss: 106.08563706, Gradient norm: 34.44911879
INFO:root:[  108] Training loss: 105.18479312, Validation loss: 106.18356165, Gradient norm: 36.68441702
INFO:root:[  109] Training loss: 105.13924550, Validation loss: 105.70325023, Gradient norm: 39.60063065
INFO:root:[  110] Training loss: 104.98216450, Validation loss: 105.85488129, Gradient norm: 38.88098301
INFO:root:[  111] Training loss: 104.86238010, Validation loss: 105.82516506, Gradient norm: 35.97484458
INFO:root:[  112] Training loss: 104.83254573, Validation loss: 105.63725807, Gradient norm: 33.95310489
INFO:root:[  113] Training loss: 104.77240780, Validation loss: 105.62736774, Gradient norm: 37.44906578
INFO:root:[  114] Training loss: 104.67210449, Validation loss: 105.47578088, Gradient norm: 33.90175331
INFO:root:[  115] Training loss: 104.59960235, Validation loss: 105.38400347, Gradient norm: 35.98212746
INFO:root:[  116] Training loss: 104.46499472, Validation loss: 105.47300431, Gradient norm: 36.24128914
INFO:root:[  117] Training loss: 104.44212091, Validation loss: 105.05549753, Gradient norm: 33.70760567
INFO:root:[  118] Training loss: 104.35049182, Validation loss: 105.26133149, Gradient norm: 34.11238622
INFO:root:[  119] Training loss: 104.25734265, Validation loss: 105.14797553, Gradient norm: 33.79964200
INFO:root:[  120] Training loss: 104.22269406, Validation loss: 105.01086952, Gradient norm: 38.72633270
INFO:root:[  121] Training loss: 104.13578310, Validation loss: 105.14054476, Gradient norm: 34.50013084
INFO:root:[  122] Training loss: 104.03007291, Validation loss: 104.93112735, Gradient norm: 36.89998535
INFO:root:[  123] Training loss: 103.98617365, Validation loss: 104.79330760, Gradient norm: 36.39954202
INFO:root:[  124] Training loss: 103.91647893, Validation loss: 104.66308383, Gradient norm: 38.06299790
INFO:root:[  125] Training loss: 103.91808596, Validation loss: 104.59324462, Gradient norm: 34.22337240
INFO:root:[  126] Training loss: 103.72295373, Validation loss: 104.58908712, Gradient norm: 33.59862387
INFO:root:[  127] Training loss: 103.69603506, Validation loss: 104.76074745, Gradient norm: 33.87809647
INFO:root:[  128] Training loss: 103.65396219, Validation loss: 104.46217793, Gradient norm: 36.18180516
INFO:root:[  129] Training loss: 103.53014772, Validation loss: 104.22768744, Gradient norm: 36.84537089
INFO:root:[  130] Training loss: 103.51604023, Validation loss: 104.28762923, Gradient norm: 40.18078830
INFO:root:[  131] Training loss: 103.51556964, Validation loss: 104.31336528, Gradient norm: 37.42572343
INFO:root:[  132] Training loss: 103.42579050, Validation loss: 104.21348545, Gradient norm: 38.31987280
INFO:root:[  133] Training loss: 103.29692982, Validation loss: 104.08692301, Gradient norm: 36.76281731
INFO:root:[  134] Training loss: 103.23905749, Validation loss: 103.98965112, Gradient norm: 39.09682259
INFO:root:[  135] Training loss: 103.18302614, Validation loss: 103.94440697, Gradient norm: 35.64563923
INFO:root:[  136] Training loss: 103.10201054, Validation loss: 103.78986227, Gradient norm: 38.37502869
INFO:root:[  137] Training loss: 103.04001206, Validation loss: 104.04352912, Gradient norm: 39.99212010
INFO:root:[  138] Training loss: 102.95855051, Validation loss: 104.21020429, Gradient norm: 41.32912726
INFO:root:[  139] Training loss: 102.96719435, Validation loss: 103.80866583, Gradient norm: 40.77504966
INFO:root:[  140] Training loss: 102.85495049, Validation loss: 103.84772018, Gradient norm: 40.08562760
INFO:root:[  141] Training loss: 102.76915653, Validation loss: 103.56554018, Gradient norm: 36.22321240
INFO:root:[  142] Training loss: 102.78874571, Validation loss: 103.64225059, Gradient norm: 35.98954727
INFO:root:[  143] Training loss: 102.69983484, Validation loss: 103.54709994, Gradient norm: 42.15382567
INFO:root:[  144] Training loss: 102.68042026, Validation loss: 103.32398303, Gradient norm: 44.34573495
INFO:root:[  145] Training loss: 102.60254365, Validation loss: 103.52175061, Gradient norm: 42.53641558
INFO:root:[  146] Training loss: 102.58937566, Validation loss: 103.34950546, Gradient norm: 39.69690634
INFO:root:[  147] Training loss: 102.45122643, Validation loss: 103.22040610, Gradient norm: 38.77974853
INFO:root:[  148] Training loss: 102.47654427, Validation loss: 103.25426720, Gradient norm: 45.37577074
INFO:root:[  149] Training loss: 102.36439123, Validation loss: 103.13014405, Gradient norm: 48.45694339
INFO:root:[  150] Training loss: 102.30595135, Validation loss: 103.02259458, Gradient norm: 38.36725518
INFO:root:[  151] Training loss: 102.30314393, Validation loss: 103.05484509, Gradient norm: 44.12956621
INFO:root:[  152] Training loss: 102.11715138, Validation loss: 103.04536043, Gradient norm: 38.01845758
INFO:root:[  153] Training loss: 102.11579024, Validation loss: 102.94932661, Gradient norm: 44.72833514
INFO:root:[  154] Training loss: 102.16274424, Validation loss: 103.12955712, Gradient norm: 44.48120288
INFO:root:[  155] Training loss: 102.05418781, Validation loss: 102.81771482, Gradient norm: 40.79888389
INFO:root:[  156] Training loss: 102.02203126, Validation loss: 102.81098438, Gradient norm: 49.08962631
INFO:root:[  157] Training loss: 101.92683573, Validation loss: 102.84372606, Gradient norm: 42.74715411
INFO:root:[  158] Training loss: 101.88090252, Validation loss: 102.56214642, Gradient norm: 42.56647726
INFO:root:[  159] Training loss: 101.89907141, Validation loss: 102.74432715, Gradient norm: 44.55622053
INFO:root:[  160] Training loss: 101.80060078, Validation loss: 102.54282458, Gradient norm: 41.71906005
INFO:root:[  161] Training loss: 101.84659084, Validation loss: 102.64588455, Gradient norm: 46.29491935
INFO:root:[  162] Training loss: 101.74388534, Validation loss: 102.64386670, Gradient norm: 43.51289249
INFO:root:[  163] Training loss: 101.67062452, Validation loss: 102.70040394, Gradient norm: 37.44346691
INFO:root:[  164] Training loss: 101.68494246, Validation loss: 102.53681788, Gradient norm: 43.27526013
INFO:root:[  165] Training loss: 101.60832836, Validation loss: 102.53546274, Gradient norm: 49.49480875
INFO:root:[  166] Training loss: 101.51163078, Validation loss: 102.62165990, Gradient norm: 43.77293004
INFO:root:[  167] Training loss: 101.48514584, Validation loss: 102.18208418, Gradient norm: 45.27184707
INFO:root:[  168] Training loss: 101.47851502, Validation loss: 102.27549823, Gradient norm: 52.48080540
INFO:root:[  169] Training loss: 101.41273397, Validation loss: 102.26653527, Gradient norm: 45.21277547
INFO:root:[  170] Training loss: 101.38361352, Validation loss: 102.39581483, Gradient norm: 47.66185734
INFO:root:[  171] Training loss: 101.25965814, Validation loss: 102.36325284, Gradient norm: 45.30629017
INFO:root:[  172] Training loss: 101.28026952, Validation loss: 102.11578948, Gradient norm: 47.48202339
INFO:root:[  173] Training loss: 101.27028015, Validation loss: 102.05531364, Gradient norm: 48.09562229
INFO:root:[  174] Training loss: 101.16830465, Validation loss: 102.15114173, Gradient norm: 43.52522602
INFO:root:[  175] Training loss: 101.10166634, Validation loss: 102.14085046, Gradient norm: 49.16519150
INFO:root:[  176] Training loss: 101.07651540, Validation loss: 102.17875803, Gradient norm: 45.40520243
INFO:root:[  177] Training loss: 101.00862628, Validation loss: 102.18925792, Gradient norm: 42.45291434
INFO:root:[  178] Training loss: 101.03815352, Validation loss: 101.95220526, Gradient norm: 46.43473258
INFO:root:[  179] Training loss: 100.98109652, Validation loss: 101.99302936, Gradient norm: 44.60001962
INFO:root:[  180] Training loss: 100.96555997, Validation loss: 101.82233376, Gradient norm: 46.10386934
INFO:root:[  181] Training loss: 100.90426555, Validation loss: 101.99631632, Gradient norm: 49.85272222
INFO:root:[  182] Training loss: 100.87592390, Validation loss: 101.94387449, Gradient norm: 48.48708660
INFO:root:[  183] Training loss: 100.86068908, Validation loss: 101.63936405, Gradient norm: 53.07077540
INFO:root:[  184] Training loss: 100.77951988, Validation loss: 101.90528501, Gradient norm: 44.76710622
INFO:root:[  185] Training loss: 100.82846198, Validation loss: 101.75571231, Gradient norm: 55.85829690
INFO:root:[  186] Training loss: 100.69728723, Validation loss: 101.76500334, Gradient norm: 52.44271548
INFO:root:[  187] Training loss: 100.65163631, Validation loss: 101.61925638, Gradient norm: 44.75743844
INFO:root:[  188] Training loss: 100.66156451, Validation loss: 101.90716079, Gradient norm: 57.15205394
INFO:root:[  189] Training loss: 100.57084068, Validation loss: 101.52841108, Gradient norm: 44.58647972
INFO:root:[  190] Training loss: 100.52610887, Validation loss: 101.80132662, Gradient norm: 53.59361290
INFO:root:[  191] Training loss: 100.58583751, Validation loss: 101.54401898, Gradient norm: 53.27017591
INFO:root:[  192] Training loss: 100.43422652, Validation loss: 101.55900442, Gradient norm: 47.00237497
INFO:root:[  193] Training loss: 100.48918868, Validation loss: 101.52979094, Gradient norm: 51.63085404
INFO:root:[  194] Training loss: 100.42228753, Validation loss: 101.56577959, Gradient norm: 57.04846730
INFO:root:[  195] Training loss: 100.35363756, Validation loss: 101.66966379, Gradient norm: 42.76421062
INFO:root:[  196] Training loss: 100.30008279, Validation loss: 101.51260192, Gradient norm: 53.89169339
INFO:root:[  197] Training loss: 100.34723805, Validation loss: 101.31864166, Gradient norm: 51.80811490
INFO:root:[  198] Training loss: 100.19050180, Validation loss: 101.40441026, Gradient norm: 51.61881680
INFO:root:[  199] Training loss: 100.27037636, Validation loss: 101.61223655, Gradient norm: 50.53851653
INFO:root:[  200] Training loss: 100.19461741, Validation loss: 101.18102948, Gradient norm: 49.72845902
INFO:root:[  201] Training loss: 100.08792026, Validation loss: 101.49548708, Gradient norm: 51.68889248
INFO:root:[  202] Training loss: 100.05060321, Validation loss: 101.34271898, Gradient norm: 48.51603890
INFO:root:[  203] Training loss: 100.06388106, Validation loss: 101.07180155, Gradient norm: 53.19073883
INFO:root:[  204] Training loss: 100.15873495, Validation loss: 101.43180716, Gradient norm: 57.18594468
INFO:root:[  205] Training loss: 100.00326396, Validation loss: 101.11159489, Gradient norm: 52.66454942
INFO:root:[  206] Training loss: 99.95633083, Validation loss: 101.01669575, Gradient norm: 47.74399958
INFO:root:[  207] Training loss: 99.91325129, Validation loss: 101.73968900, Gradient norm: 49.50816236
INFO:root:[  208] Training loss: 99.92797075, Validation loss: 100.98564332, Gradient norm: 49.49031437
INFO:root:[  209] Training loss: 99.80937127, Validation loss: 101.11193111, Gradient norm: 63.35188524
INFO:root:[  210] Training loss: 99.91475468, Validation loss: 101.04255387, Gradient norm: 50.17029257
INFO:root:[  211] Training loss: 99.77217656, Validation loss: 101.10159118, Gradient norm: 50.88571135
INFO:root:[  212] Training loss: 99.83898149, Validation loss: 100.83781302, Gradient norm: 51.72919630
INFO:root:[  213] Training loss: 99.76185594, Validation loss: 100.87945662, Gradient norm: 55.78024080
INFO:root:[  214] Training loss: 99.68255521, Validation loss: 100.91232563, Gradient norm: 54.29288424
INFO:root:[  215] Training loss: 99.67193597, Validation loss: 101.08274605, Gradient norm: 54.53213391
INFO:root:[  216] Training loss: 99.60017949, Validation loss: 100.86945764, Gradient norm: 49.00631745
INFO:root:[  217] Training loss: 99.61633632, Validation loss: 101.10724272, Gradient norm: 57.49621827
INFO:root:[  218] Training loss: 99.57779275, Validation loss: 100.87351990, Gradient norm: 50.45303283
INFO:root:[  219] Training loss: 99.51577057, Validation loss: 100.68747527, Gradient norm: 55.31902502
INFO:root:[  220] Training loss: 99.57320438, Validation loss: 100.88512315, Gradient norm: 56.39691459
INFO:root:[  221] Training loss: 99.52100528, Validation loss: 100.69149938, Gradient norm: 55.44890732
INFO:root:[  222] Training loss: 99.45625785, Validation loss: 100.71041502, Gradient norm: 54.46247282
INFO:root:[  223] Training loss: 99.49272750, Validation loss: 100.62071675, Gradient norm: 50.59116127
INFO:root:[  224] Training loss: 99.39247793, Validation loss: 100.83221883, Gradient norm: 57.87707434
INFO:root:[  225] Training loss: 99.39012652, Validation loss: 100.72760825, Gradient norm: 53.74662840
INFO:root:[  226] Training loss: 99.43866547, Validation loss: 100.61510152, Gradient norm: 51.68919958
INFO:root:[  227] Training loss: 99.27041322, Validation loss: 100.42081530, Gradient norm: 49.29273018
INFO:root:[  228] Training loss: 99.24751640, Validation loss: 100.81394827, Gradient norm: 56.83012706
INFO:root:[  229] Training loss: 99.34340560, Validation loss: 100.70780077, Gradient norm: 62.37594721
INFO:root:[  230] Training loss: 99.19858443, Validation loss: 100.55329106, Gradient norm: 50.43203828
INFO:root:[  231] Training loss: 99.24127312, Validation loss: 100.73121248, Gradient norm: 56.76469508
INFO:root:[  232] Training loss: 99.23763991, Validation loss: 100.34960911, Gradient norm: 67.31438733
INFO:root:[  233] Training loss: 99.20259121, Validation loss: 100.48487828, Gradient norm: 58.18925517
INFO:root:[  234] Training loss: 99.12544980, Validation loss: 100.41640604, Gradient norm: 45.46089759
INFO:root:[  235] Training loss: 99.11905245, Validation loss: 100.45790758, Gradient norm: 50.32829248
INFO:root:[  236] Training loss: 99.02825354, Validation loss: 100.27504178, Gradient norm: 51.73042366
INFO:root:[  237] Training loss: 99.08917263, Validation loss: 100.51653684, Gradient norm: 55.18815939
INFO:root:[  238] Training loss: 99.04418365, Validation loss: 100.57375415, Gradient norm: 50.05500944
INFO:root:[  239] Training loss: 99.02502738, Validation loss: 100.43665603, Gradient norm: 58.42105000
INFO:root:[  240] Training loss: 98.89953931, Validation loss: 100.51633664, Gradient norm: 57.40524218
INFO:root:[  241] Training loss: 98.87941985, Validation loss: 100.53923640, Gradient norm: 53.44946848
INFO:root:[  242] Training loss: 98.90697716, Validation loss: 100.59422513, Gradient norm: 51.45465419
INFO:root:[  243] Training loss: 98.96838392, Validation loss: 100.20804885, Gradient norm: 59.08638979
INFO:root:[  244] Training loss: 98.80695897, Validation loss: 100.40186178, Gradient norm: 50.51614782
INFO:root:[  245] Training loss: 98.84023521, Validation loss: 100.41869407, Gradient norm: 59.88108299
INFO:root:[  246] Training loss: 98.78144634, Validation loss: 100.34026863, Gradient norm: 56.56230200
INFO:root:[  247] Training loss: 98.75768982, Validation loss: 100.51645634, Gradient norm: 51.56770027
INFO:root:[  248] Training loss: 98.76878944, Validation loss: 100.37148180, Gradient norm: 56.94922387
INFO:root:[  249] Training loss: 98.65660595, Validation loss: 100.10232386, Gradient norm: 56.38055838
INFO:root:[  250] Training loss: 98.64142642, Validation loss: 100.36178747, Gradient norm: 59.88598118
INFO:root:[  251] Training loss: 98.70680392, Validation loss: 100.27875387, Gradient norm: 60.27237849
INFO:root:[  252] Training loss: 98.55984727, Validation loss: 100.45018637, Gradient norm: 53.08291236
INFO:root:[  253] Training loss: 98.63291729, Validation loss: 100.07135746, Gradient norm: 60.18241432
INFO:root:[  254] Training loss: 98.54734357, Validation loss: 100.26849655, Gradient norm: 56.38189183
INFO:root:[  255] Training loss: 98.55712411, Validation loss: 100.21498029, Gradient norm: 50.12947479
INFO:root:[  256] Training loss: 98.59856260, Validation loss: 100.31214984, Gradient norm: 57.33361027
INFO:root:[  257] Training loss: 98.57241382, Validation loss: 100.11784363, Gradient norm: 58.99369881
INFO:root:[  258] Training loss: 98.43585914, Validation loss: 99.87634383, Gradient norm: 49.53534915
INFO:root:[  259] Training loss: 98.34887736, Validation loss: 99.74176815, Gradient norm: 54.23565942
INFO:root:[  260] Training loss: 98.43823546, Validation loss: 100.25676990, Gradient norm: 54.99411333
INFO:root:[  261] Training loss: 98.38321240, Validation loss: 99.74771855, Gradient norm: 55.34729009
INFO:root:[  262] Training loss: 98.39851791, Validation loss: 100.03707228, Gradient norm: 60.60466741
INFO:root:[  263] Training loss: 98.35264284, Validation loss: 99.84147013, Gradient norm: 50.52104694
INFO:root:[  264] Training loss: 98.30373315, Validation loss: 99.71845456, Gradient norm: 52.91819121
INFO:root:[  265] Training loss: 98.29886587, Validation loss: 99.81119143, Gradient norm: 57.09843173
INFO:root:[  266] Training loss: 98.23063660, Validation loss: 99.87366065, Gradient norm: 58.31341659
INFO:root:[  267] Training loss: 98.17595241, Validation loss: 100.02597230, Gradient norm: 66.20616298
INFO:root:[  268] Training loss: 98.18448241, Validation loss: 99.81806262, Gradient norm: 57.10200412
INFO:root:[  269] Training loss: 98.16459480, Validation loss: 100.14868795, Gradient norm: 60.26953204
INFO:root:[  270] Training loss: 98.16275308, Validation loss: 99.96209690, Gradient norm: 59.31512832
INFO:root:[  271] Training loss: 98.16048654, Validation loss: 99.88725781, Gradient norm: 60.35825119
INFO:root:[  272] Training loss: 98.09500271, Validation loss: 99.89082336, Gradient norm: 60.84762857
INFO:root:[  273] Training loss: 97.99091116, Validation loss: 100.05114430, Gradient norm: 56.80952754
INFO:root:EP 273: Early stopping
INFO:root:Training the model took 3342.899s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 8882.27604
INFO:root:EnergyScoreTrain: 6257.84693
INFO:root:CoverageTrain: 0.7835
INFO:root:IntervalWidthTrain: 8.03373
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 8809.3745
INFO:root:EnergyScoreValidation: 6205.5871
INFO:root:CoverageValidation: 0.77819
INFO:root:IntervalWidthValidation: 8.02376
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 8902.22457
INFO:root:EnergyScoreTest: 6270.70501
INFO:root:CoverageTest: 0.77773
INFO:root:IntervalWidthTest: 8.03192
INFO:root:###3 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.33902045, Validation loss: 121.64566566, Gradient norm: 118.39334567
INFO:root:[    2] Training loss: 121.81701336, Validation loss: 121.65994526, Gradient norm: 130.72394473
INFO:root:[    3] Training loss: 121.52838067, Validation loss: 121.12257227, Gradient norm: 118.95087436
INFO:root:[    4] Training loss: 121.43610605, Validation loss: 121.31356917, Gradient norm: 104.86339702
INFO:root:[    5] Training loss: 121.39371450, Validation loss: 121.30918463, Gradient norm: 104.77048296
INFO:root:[    6] Training loss: 121.34551171, Validation loss: 121.24934598, Gradient norm: 103.77944287
INFO:root:[    7] Training loss: 121.34941196, Validation loss: 121.11094560, Gradient norm: 99.66799705
INFO:root:[    8] Training loss: 121.29004041, Validation loss: 121.00672623, Gradient norm: 107.85623520
INFO:root:[    9] Training loss: 121.05470971, Validation loss: 120.94696229, Gradient norm: 84.07807646
INFO:root:[   10] Training loss: 120.86054621, Validation loss: 120.82689772, Gradient norm: 71.38141470
INFO:root:[   11] Training loss: 120.76460955, Validation loss: 120.65241452, Gradient norm: 77.32647206
INFO:root:[   12] Training loss: 120.69744752, Validation loss: 120.64398299, Gradient norm: 69.16909918
INFO:root:[   13] Training loss: 120.61808419, Validation loss: 120.57222932, Gradient norm: 64.75142734
INFO:root:[   14] Training loss: 120.55888508, Validation loss: 120.55483141, Gradient norm: 60.80081482
INFO:root:[   15] Training loss: 120.51769884, Validation loss: 120.44499680, Gradient norm: 49.47911101
INFO:root:[   16] Training loss: 120.49556104, Validation loss: 120.32354473, Gradient norm: 59.73282073
INFO:root:[   17] Training loss: 120.42381044, Validation loss: 120.43850129, Gradient norm: 56.67704347
INFO:root:[   18] Training loss: 120.35791812, Validation loss: 120.10853340, Gradient norm: 52.05061352
INFO:root:[   19] Training loss: 120.22744224, Validation loss: 120.33415196, Gradient norm: 60.94023662
INFO:root:[   20] Training loss: 120.02044239, Validation loss: 119.82551469, Gradient norm: 55.18643420
INFO:root:[   21] Training loss: 119.56667996, Validation loss: 119.17357741, Gradient norm: 57.67326575
INFO:root:[   22] Training loss: 118.69387243, Validation loss: 118.32692613, Gradient norm: 48.15435032
INFO:root:[   23] Training loss: 117.92283320, Validation loss: 117.66765831, Gradient norm: 51.59110287
INFO:root:[   24] Training loss: 117.15771619, Validation loss: 116.94083405, Gradient norm: 41.55500781
INFO:root:[   25] Training loss: 116.39110646, Validation loss: 116.17225121, Gradient norm: 44.90133982
INFO:root:[   26] Training loss: 115.63001163, Validation loss: 115.64907363, Gradient norm: 44.99613767
INFO:root:[   27] Training loss: 115.07488008, Validation loss: 114.78417811, Gradient norm: 43.98842693
INFO:root:[   28] Training loss: 114.48680783, Validation loss: 114.34879145, Gradient norm: 36.91268150
INFO:root:[   29] Training loss: 113.96902203, Validation loss: 113.82055743, Gradient norm: 33.10149347
INFO:root:[   30] Training loss: 113.50894995, Validation loss: 113.36861683, Gradient norm: 33.78792150
INFO:root:[   31] Training loss: 113.08269737, Validation loss: 112.98073973, Gradient norm: 35.09829146
INFO:root:[   32] Training loss: 112.69707698, Validation loss: 112.69551376, Gradient norm: 40.20279704
INFO:root:[   33] Training loss: 112.32220729, Validation loss: 112.25180369, Gradient norm: 26.70197981
INFO:root:[   34] Training loss: 111.95957400, Validation loss: 111.97122666, Gradient norm: 31.44567305
INFO:root:[   35] Training loss: 111.68867196, Validation loss: 111.75900558, Gradient norm: 30.34031912
INFO:root:[   36] Training loss: 111.42800059, Validation loss: 111.56792897, Gradient norm: 28.87987867
INFO:root:[   37] Training loss: 111.24182730, Validation loss: 111.28492053, Gradient norm: 26.10262295
INFO:root:[   38] Training loss: 110.92821692, Validation loss: 110.95834298, Gradient norm: 29.01666030
INFO:root:[   39] Training loss: 110.70100410, Validation loss: 110.82820077, Gradient norm: 30.61705902
INFO:root:[   40] Training loss: 110.55185558, Validation loss: 110.61035077, Gradient norm: 27.04198547
INFO:root:[   41] Training loss: 110.29029522, Validation loss: 110.43699725, Gradient norm: 24.43370932
INFO:root:[   42] Training loss: 110.12965218, Validation loss: 110.37291981, Gradient norm: 29.91878219
INFO:root:[   43] Training loss: 109.97015381, Validation loss: 110.30246287, Gradient norm: 29.39781355
INFO:root:[   44] Training loss: 109.81077663, Validation loss: 109.75018810, Gradient norm: 29.23675176
INFO:root:[   45] Training loss: 109.59456675, Validation loss: 109.98741229, Gradient norm: 27.41890577
INFO:root:[   46] Training loss: 109.44184713, Validation loss: 109.92317515, Gradient norm: 27.21612375
INFO:root:[   47] Training loss: 109.37470792, Validation loss: 109.35306260, Gradient norm: 25.58343573
INFO:root:[   48] Training loss: 109.21205659, Validation loss: 109.47906415, Gradient norm: 28.78734101
INFO:root:[   49] Training loss: 109.03836694, Validation loss: 109.22996837, Gradient norm: 25.25804825
INFO:root:[   50] Training loss: 108.93181549, Validation loss: 109.16348056, Gradient norm: 27.20725940
INFO:root:[   51] Training loss: 108.82239566, Validation loss: 109.28513284, Gradient norm: 25.29923413
INFO:root:[   52] Training loss: 108.71381993, Validation loss: 108.88235895, Gradient norm: 25.81511981
INFO:root:[   53] Training loss: 108.65025195, Validation loss: 108.73713816, Gradient norm: 29.31063404
INFO:root:[   54] Training loss: 108.46926785, Validation loss: 108.80064655, Gradient norm: 25.44746578
INFO:root:[   55] Training loss: 108.33954168, Validation loss: 108.69111133, Gradient norm: 24.64293713
INFO:root:[   56] Training loss: 108.25705192, Validation loss: 108.60969754, Gradient norm: 24.61107984
INFO:root:[   57] Training loss: 108.19782028, Validation loss: 108.44318311, Gradient norm: 26.57485532
INFO:root:[   58] Training loss: 108.06571035, Validation loss: 108.42034570, Gradient norm: 25.65003612
INFO:root:[   59] Training loss: 107.93460954, Validation loss: 108.21063548, Gradient norm: 25.28939795
INFO:root:[   60] Training loss: 107.91426660, Validation loss: 108.00973932, Gradient norm: 24.96986663
INFO:root:[   61] Training loss: 107.77005207, Validation loss: 108.06934462, Gradient norm: 26.20426551
INFO:root:[   62] Training loss: 107.63487831, Validation loss: 108.05005225, Gradient norm: 23.25250090
INFO:root:[   63] Training loss: 107.59327428, Validation loss: 108.06439630, Gradient norm: 25.39875066
INFO:root:[   64] Training loss: 107.47325755, Validation loss: 107.83877590, Gradient norm: 27.00111959
INFO:root:[   65] Training loss: 107.36236424, Validation loss: 107.80249050, Gradient norm: 23.84798410
INFO:root:[   66] Training loss: 107.25391192, Validation loss: 107.87022742, Gradient norm: 25.76815207
INFO:root:[   67] Training loss: 107.17762270, Validation loss: 107.79294665, Gradient norm: 23.64177658
INFO:root:[   68] Training loss: 107.05579869, Validation loss: 107.61447880, Gradient norm: 22.91942237
INFO:root:[   69] Training loss: 106.94206677, Validation loss: 107.46918093, Gradient norm: 24.99784125
INFO:root:[   70] Training loss: 106.85341037, Validation loss: 107.29871684, Gradient norm: 22.46491980
INFO:root:[   71] Training loss: 106.84484870, Validation loss: 107.49475913, Gradient norm: 25.29820837
INFO:root:[   72] Training loss: 106.67915223, Validation loss: 107.11196873, Gradient norm: 24.54401411
INFO:root:[   73] Training loss: 106.66023254, Validation loss: 107.13375644, Gradient norm: 22.87250958
INFO:root:[   74] Training loss: 106.51637484, Validation loss: 107.10111447, Gradient norm: 24.15561808
INFO:root:[   75] Training loss: 106.47201754, Validation loss: 106.91597011, Gradient norm: 24.19617102
INFO:root:[   76] Training loss: 106.37970713, Validation loss: 106.93156986, Gradient norm: 23.76362560
INFO:root:[   77] Training loss: 106.24385631, Validation loss: 106.84499780, Gradient norm: 23.75079849
INFO:root:[   78] Training loss: 106.23092240, Validation loss: 106.90323455, Gradient norm: 24.23945420
INFO:root:[   79] Training loss: 106.15542002, Validation loss: 106.78735088, Gradient norm: 23.77230149
INFO:root:[   80] Training loss: 106.09474243, Validation loss: 106.76351350, Gradient norm: 26.37813297
INFO:root:[   81] Training loss: 106.04147163, Validation loss: 106.76715535, Gradient norm: 25.24582415
INFO:root:[   82] Training loss: 105.96916104, Validation loss: 106.43867466, Gradient norm: 23.81644312
INFO:root:[   83] Training loss: 105.82340639, Validation loss: 106.66483886, Gradient norm: 25.01790706
INFO:root:[   84] Training loss: 105.79952010, Validation loss: 106.45949686, Gradient norm: 25.44290198
INFO:root:[   85] Training loss: 105.66535207, Validation loss: 106.32981083, Gradient norm: 24.48448664
INFO:root:[   86] Training loss: 105.63599740, Validation loss: 106.38409871, Gradient norm: 25.75073252
INFO:root:[   87] Training loss: 105.54770093, Validation loss: 106.34015498, Gradient norm: 25.77897933
INFO:root:[   88] Training loss: 105.48729199, Validation loss: 106.36360589, Gradient norm: 24.62382011
INFO:root:[   89] Training loss: 105.46868815, Validation loss: 106.07659675, Gradient norm: 27.14193668
INFO:root:[   90] Training loss: 105.33415418, Validation loss: 106.13117665, Gradient norm: 25.45707984
INFO:root:[   91] Training loss: 105.32398669, Validation loss: 106.01548636, Gradient norm: 24.82698214
INFO:root:[   92] Training loss: 105.18530652, Validation loss: 105.87033107, Gradient norm: 25.68783225
INFO:root:[   93] Training loss: 105.17666248, Validation loss: 106.10610830, Gradient norm: 26.29624820
INFO:root:[   94] Training loss: 105.13535457, Validation loss: 105.93182899, Gradient norm: 26.25737894
INFO:root:[   95] Training loss: 105.07070518, Validation loss: 105.90376939, Gradient norm: 28.78922046
INFO:root:[   96] Training loss: 104.96580235, Validation loss: 105.72002332, Gradient norm: 28.26721737
INFO:root:[   97] Training loss: 104.86821173, Validation loss: 105.51280949, Gradient norm: 27.98543554
INFO:root:[   98] Training loss: 104.88064454, Validation loss: 105.69821377, Gradient norm: 26.69313697
INFO:root:[   99] Training loss: 104.80815010, Validation loss: 105.56889264, Gradient norm: 25.13176733
INFO:root:[  100] Training loss: 104.74309715, Validation loss: 105.70403027, Gradient norm: 26.93812557
INFO:root:[  101] Training loss: 104.78396931, Validation loss: 105.41236272, Gradient norm: 29.45511643
INFO:root:[  102] Training loss: 104.56451018, Validation loss: 105.51048831, Gradient norm: 27.29161044
INFO:root:[  103] Training loss: 104.54494841, Validation loss: 105.43891302, Gradient norm: 26.46041013
INFO:root:[  104] Training loss: 104.51337332, Validation loss: 105.43186319, Gradient norm: 29.71239733
INFO:root:[  105] Training loss: 104.43206483, Validation loss: 105.28518545, Gradient norm: 24.34524372
INFO:root:[  106] Training loss: 104.44165188, Validation loss: 105.32018780, Gradient norm: 27.23246956
INFO:root:[  107] Training loss: 104.36233257, Validation loss: 105.05245261, Gradient norm: 28.73063187
INFO:root:[  108] Training loss: 104.30296123, Validation loss: 105.24534817, Gradient norm: 30.90104017
INFO:root:[  109] Training loss: 104.27050484, Validation loss: 105.09774622, Gradient norm: 30.16694325
INFO:root:[  110] Training loss: 104.21276450, Validation loss: 104.97110354, Gradient norm: 25.18584479
INFO:root:[  111] Training loss: 104.17633847, Validation loss: 104.91308699, Gradient norm: 33.90477357
INFO:root:[  112] Training loss: 104.12128874, Validation loss: 104.96582584, Gradient norm: 28.15346860
INFO:root:[  113] Training loss: 104.04759669, Validation loss: 105.03175880, Gradient norm: 26.42244043
INFO:root:[  114] Training loss: 103.95008418, Validation loss: 105.17939548, Gradient norm: 29.85360322
INFO:root:[  115] Training loss: 103.91248686, Validation loss: 104.98982370, Gradient norm: 30.31708356
INFO:root:[  116] Training loss: 103.90703657, Validation loss: 104.95535673, Gradient norm: 32.65168403
INFO:root:[  117] Training loss: 103.80714862, Validation loss: 104.97815731, Gradient norm: 28.98461616
INFO:root:[  118] Training loss: 103.81288917, Validation loss: 104.85776967, Gradient norm: 28.91515736
INFO:root:[  119] Training loss: 103.76432226, Validation loss: 104.85390920, Gradient norm: 31.08604861
INFO:root:[  120] Training loss: 103.73987390, Validation loss: 104.92378393, Gradient norm: 29.49066295
INFO:root:[  121] Training loss: 103.67556412, Validation loss: 104.85552610, Gradient norm: 28.46922741
INFO:root:[  122] Training loss: 103.59124850, Validation loss: 104.56925701, Gradient norm: 29.81110697
INFO:root:[  123] Training loss: 103.62579744, Validation loss: 104.92742946, Gradient norm: 33.27376707
INFO:root:[  124] Training loss: 103.55472423, Validation loss: 104.63620732, Gradient norm: 34.56855796
INFO:root:[  125] Training loss: 103.49641668, Validation loss: 104.60413019, Gradient norm: 32.63294097
INFO:root:[  126] Training loss: 103.43164481, Validation loss: 104.48935699, Gradient norm: 30.50480125
INFO:root:[  127] Training loss: 103.45747929, Validation loss: 104.51692778, Gradient norm: 34.03747322
INFO:root:[  128] Training loss: 103.34308503, Validation loss: 104.69819746, Gradient norm: 33.40713327
INFO:root:[  129] Training loss: 103.26820130, Validation loss: 104.50593541, Gradient norm: 30.69618985
INFO:root:[  130] Training loss: 103.26228596, Validation loss: 104.27016607, Gradient norm: 31.84079147
INFO:root:[  131] Training loss: 103.27093013, Validation loss: 104.46138790, Gradient norm: 34.42846576
INFO:root:[  132] Training loss: 103.23246374, Validation loss: 104.37815383, Gradient norm: 33.57265759
INFO:root:[  133] Training loss: 103.13636213, Validation loss: 104.15964245, Gradient norm: 32.54094236
INFO:root:[  134] Training loss: 103.10908832, Validation loss: 104.24526399, Gradient norm: 32.36577611
INFO:root:[  135] Training loss: 103.07517202, Validation loss: 104.50186447, Gradient norm: 31.70071717
INFO:root:[  136] Training loss: 103.02325237, Validation loss: 104.02129022, Gradient norm: 34.08734952
INFO:root:[  137] Training loss: 103.00174227, Validation loss: 104.16420351, Gradient norm: 33.12742270
INFO:root:[  138] Training loss: 102.98556275, Validation loss: 104.19715171, Gradient norm: 32.56135241
INFO:root:[  139] Training loss: 102.93552480, Validation loss: 104.13498293, Gradient norm: 39.11333371
INFO:root:[  140] Training loss: 102.89121651, Validation loss: 104.33520429, Gradient norm: 35.65783545
INFO:root:[  141] Training loss: 102.85674124, Validation loss: 104.24457997, Gradient norm: 35.03448034
INFO:root:[  142] Training loss: 102.81873159, Validation loss: 104.25609904, Gradient norm: 36.71704916
INFO:root:[  143] Training loss: 102.82142268, Validation loss: 104.11177826, Gradient norm: 32.78629913
INFO:root:[  144] Training loss: 102.76461198, Validation loss: 104.01920661, Gradient norm: 40.10649159
INFO:root:[  145] Training loss: 102.65405132, Validation loss: 103.83149482, Gradient norm: 35.91871295
INFO:root:[  146] Training loss: 102.61242210, Validation loss: 103.96445991, Gradient norm: 34.22860286
INFO:root:[  147] Training loss: 102.62593977, Validation loss: 103.80728334, Gradient norm: 37.73671566
INFO:root:[  148] Training loss: 102.64140306, Validation loss: 104.02563082, Gradient norm: 35.66696767
INFO:root:[  149] Training loss: 102.48118450, Validation loss: 104.09444980, Gradient norm: 34.39393589
INFO:root:[  150] Training loss: 102.56420898, Validation loss: 103.81597085, Gradient norm: 42.70208995
INFO:root:[  151] Training loss: 102.57969537, Validation loss: 103.85712275, Gradient norm: 35.43091340
INFO:root:[  152] Training loss: 102.47148382, Validation loss: 103.81806893, Gradient norm: 32.68894523
INFO:root:[  153] Training loss: 102.41573854, Validation loss: 103.92951781, Gradient norm: 40.38391759
INFO:root:[  154] Training loss: 102.32577143, Validation loss: 103.78908249, Gradient norm: 37.98342254
INFO:root:[  155] Training loss: 102.36729181, Validation loss: 103.79523547, Gradient norm: 39.31715008
INFO:root:[  156] Training loss: 102.29975938, Validation loss: 103.90078235, Gradient norm: 33.81218362
INFO:root:[  157] Training loss: 102.33287960, Validation loss: 103.73755356, Gradient norm: 48.42817236
INFO:root:[  158] Training loss: 102.24595851, Validation loss: 103.70064755, Gradient norm: 35.52650706
INFO:root:[  159] Training loss: 102.21073880, Validation loss: 103.67707904, Gradient norm: 34.87777830
INFO:root:[  160] Training loss: 102.17804367, Validation loss: 103.70227577, Gradient norm: 38.12377189
INFO:root:[  161] Training loss: 102.18309784, Validation loss: 103.60815430, Gradient norm: 39.54497139
INFO:root:[  162] Training loss: 102.10475820, Validation loss: 103.51231647, Gradient norm: 35.52026760
INFO:root:[  163] Training loss: 102.05750059, Validation loss: 103.58666256, Gradient norm: 35.70409768
INFO:root:[  164] Training loss: 102.12868837, Validation loss: 103.42736001, Gradient norm: 43.02968401
INFO:root:[  165] Training loss: 102.04327555, Validation loss: 103.64003938, Gradient norm: 37.64202669
INFO:root:[  166] Training loss: 101.98675787, Validation loss: 103.47028193, Gradient norm: 36.08049171
INFO:root:[  167] Training loss: 101.99065689, Validation loss: 103.41138458, Gradient norm: 35.24394480
INFO:root:[  168] Training loss: 102.00828984, Validation loss: 103.31003386, Gradient norm: 37.27227220
INFO:root:[  169] Training loss: 101.95707824, Validation loss: 103.87304872, Gradient norm: 43.50279832
INFO:root:[  170] Training loss: 101.83454902, Validation loss: 103.48913522, Gradient norm: 37.29307966
INFO:root:[  171] Training loss: 101.91434296, Validation loss: 103.67039385, Gradient norm: 42.36205927
INFO:root:[  172] Training loss: 101.90277687, Validation loss: 103.42323724, Gradient norm: 35.52753246
INFO:root:[  173] Training loss: 101.83394495, Validation loss: 103.55267781, Gradient norm: 37.85458805
INFO:root:[  174] Training loss: 101.77559837, Validation loss: 103.36262302, Gradient norm: 40.60453657
INFO:root:[  175] Training loss: 101.76223505, Validation loss: 103.37979600, Gradient norm: 43.92510574
INFO:root:[  176] Training loss: 101.72201545, Validation loss: 103.28209029, Gradient norm: 32.97475531
INFO:root:[  177] Training loss: 101.72676957, Validation loss: 103.31045085, Gradient norm: 36.76133334
INFO:root:[  178] Training loss: 101.70740111, Validation loss: 103.27966256, Gradient norm: 36.97953506
INFO:root:[  179] Training loss: 101.65865954, Validation loss: 103.30722099, Gradient norm: 37.70625245
INFO:root:[  180] Training loss: 101.59254921, Validation loss: 103.32754148, Gradient norm: 33.39316303
INFO:root:[  181] Training loss: 101.55742969, Validation loss: 103.21366672, Gradient norm: 38.38761535
INFO:root:[  182] Training loss: 101.57709348, Validation loss: 103.19124551, Gradient norm: 38.97179765
INFO:root:[  183] Training loss: 101.48737153, Validation loss: 103.46003092, Gradient norm: 35.92053262
INFO:root:[  184] Training loss: 101.51750858, Validation loss: 103.56700345, Gradient norm: 45.98056011
INFO:root:[  185] Training loss: 101.46499188, Validation loss: 103.15589484, Gradient norm: 41.72566456
INFO:root:[  186] Training loss: 101.44849484, Validation loss: 103.52783335, Gradient norm: 37.11181280
INFO:root:[  187] Training loss: 101.46150808, Validation loss: 103.38714889, Gradient norm: 42.62471231
INFO:root:[  188] Training loss: 101.37945327, Validation loss: 103.19745610, Gradient norm: 39.12064381
INFO:root:[  189] Training loss: 101.35521138, Validation loss: 103.17257927, Gradient norm: 39.59748396
INFO:root:[  190] Training loss: 101.34989909, Validation loss: 103.21539307, Gradient norm: 45.40928588
INFO:root:[  191] Training loss: 101.32835388, Validation loss: 103.21585083, Gradient norm: 38.59578805
INFO:root:[  192] Training loss: 101.26449632, Validation loss: 103.16295150, Gradient norm: 38.38036269
INFO:root:[  193] Training loss: 101.28464454, Validation loss: 102.98735441, Gradient norm: 40.61392445
INFO:root:[  194] Training loss: 101.23104669, Validation loss: 102.95673002, Gradient norm: 38.41368568
INFO:root:[  195] Training loss: 101.22077773, Validation loss: 103.09098947, Gradient norm: 43.14565418
INFO:root:[  196] Training loss: 101.24671004, Validation loss: 103.20855450, Gradient norm: 38.86871495
INFO:root:[  197] Training loss: 101.18340302, Validation loss: 102.97837119, Gradient norm: 40.57867015
INFO:root:[  198] Training loss: 101.05323467, Validation loss: 103.14726231, Gradient norm: 39.00379887
INFO:root:[  199] Training loss: 101.10029379, Validation loss: 103.18043071, Gradient norm: 39.73102657
INFO:root:[  200] Training loss: 101.09781053, Validation loss: 103.23376596, Gradient norm: 42.96110618
INFO:root:[  201] Training loss: 101.07583976, Validation loss: 102.94288135, Gradient norm: 46.66883404
INFO:root:[  202] Training loss: 101.00180466, Validation loss: 103.08789220, Gradient norm: 38.06106669
INFO:root:[  203] Training loss: 101.03816561, Validation loss: 102.99913393, Gradient norm: 44.45717547
INFO:root:[  204] Training loss: 101.00705996, Validation loss: 103.11347356, Gradient norm: 41.60870479
INFO:root:[  205] Training loss: 100.97924663, Validation loss: 102.80774715, Gradient norm: 41.72783695
INFO:root:[  206] Training loss: 100.96158843, Validation loss: 103.07744967, Gradient norm: 45.56802878
INFO:root:[  207] Training loss: 100.83860779, Validation loss: 102.79421813, Gradient norm: 40.25946682
INFO:root:[  208] Training loss: 100.80906306, Validation loss: 103.04666322, Gradient norm: 37.89492969
INFO:root:[  209] Training loss: 100.76490838, Validation loss: 102.94114396, Gradient norm: 37.54972404
INFO:root:[  210] Training loss: 100.84762141, Validation loss: 103.08973983, Gradient norm: 37.82390134
INFO:root:[  211] Training loss: 100.83833353, Validation loss: 103.15777509, Gradient norm: 40.29845208
INFO:root:[  212] Training loss: 100.90091618, Validation loss: 102.92921685, Gradient norm: 55.05352387
INFO:root:[  213] Training loss: 100.79651730, Validation loss: 103.20527833, Gradient norm: 37.77427468
INFO:root:[  214] Training loss: 100.73561549, Validation loss: 103.44057886, Gradient norm: 48.43682215
INFO:root:[  215] Training loss: 100.68237392, Validation loss: 103.10843869, Gradient norm: 41.00863744
INFO:root:[  216] Training loss: 100.66621055, Validation loss: 102.90976478, Gradient norm: 44.78014676
INFO:root:[  217] Training loss: 100.63073076, Validation loss: 102.77789649, Gradient norm: 38.77060377
INFO:root:[  218] Training loss: 100.61033853, Validation loss: 102.79494792, Gradient norm: 42.11623337
INFO:root:[  219] Training loss: 100.61188791, Validation loss: 102.93792777, Gradient norm: 48.95459055
INFO:root:[  220] Training loss: 100.58224271, Validation loss: 102.91348346, Gradient norm: 41.05478717
INFO:root:[  221] Training loss: 100.58576108, Validation loss: 102.77710855, Gradient norm: 43.83659136
INFO:root:[  222] Training loss: 100.52016334, Validation loss: 102.64156841, Gradient norm: 41.13289224
INFO:root:[  223] Training loss: 100.56432545, Validation loss: 102.82923021, Gradient norm: 49.17539826
INFO:root:[  224] Training loss: 100.45378464, Validation loss: 102.85059857, Gradient norm: 40.23275981
INFO:root:[  225] Training loss: 100.56390867, Validation loss: 102.85840580, Gradient norm: 46.21937309
INFO:root:[  226] Training loss: 100.45936976, Validation loss: 102.80309375, Gradient norm: 41.64203138
INFO:root:[  227] Training loss: 100.43909049, Validation loss: 102.64588981, Gradient norm: 43.89583372
INFO:root:[  228] Training loss: 100.33989736, Validation loss: 102.82495117, Gradient norm: 41.14090861
INFO:root:[  229] Training loss: 100.38923436, Validation loss: 102.86522938, Gradient norm: 44.24849332
INFO:root:[  230] Training loss: 100.36183356, Validation loss: 102.73810630, Gradient norm: 39.17627966
INFO:root:[  231] Training loss: 100.30087611, Validation loss: 102.76302522, Gradient norm: 42.04237488
INFO:root:EP 231: Early stopping
INFO:root:Training the model took 2832.552s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 9080.09769
INFO:root:EnergyScoreTrain: 6394.83896
INFO:root:CoverageTrain: 0.78718
INFO:root:IntervalWidthTrain: 8.20294
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 9057.76203
INFO:root:EnergyScoreValidation: 6377.88707
INFO:root:CoverageValidation: 0.7801
INFO:root:IntervalWidthValidation: 8.18349
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9139.41747
INFO:root:EnergyScoreTest: 6435.33494
INFO:root:CoverageTest: 0.77952
INFO:root:IntervalWidthTest: 8.17966
INFO:root:###4 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.42888365, Validation loss: 121.32490276, Gradient norm: 23.22091013
INFO:root:[    2] Training loss: 121.43943996, Validation loss: 121.32764382, Gradient norm: 8.22932943
INFO:root:[    3] Training loss: 121.38525465, Validation loss: 121.27178508, Gradient norm: 8.99659335
INFO:root:[    4] Training loss: 121.34282144, Validation loss: 121.33154902, Gradient norm: 7.43045312
INFO:root:[    5] Training loss: 121.34819605, Validation loss: 121.24278496, Gradient norm: 7.79724943
INFO:root:[    6] Training loss: 121.31503208, Validation loss: 121.30017406, Gradient norm: 5.46652999
INFO:root:[    7] Training loss: 121.31882180, Validation loss: 121.26299865, Gradient norm: 5.79285469
INFO:root:[    8] Training loss: 121.30601110, Validation loss: 121.34798431, Gradient norm: 6.62777422
INFO:root:[    9] Training loss: 121.27235143, Validation loss: 121.07492118, Gradient norm: 5.64351721
INFO:root:[   10] Training loss: 120.98993717, Validation loss: 120.81226217, Gradient norm: 7.12177169
INFO:root:[   11] Training loss: 120.56023353, Validation loss: 120.03460457, Gradient norm: 6.71570843
INFO:root:[   12] Training loss: 119.60756771, Validation loss: 118.43627719, Gradient norm: 8.42952496
INFO:root:[   13] Training loss: 117.87880423, Validation loss: 116.30447309, Gradient norm: 10.51108232
INFO:root:[   14] Training loss: 116.21349544, Validation loss: 114.69922296, Gradient norm: 11.39742111
INFO:root:[   15] Training loss: 114.95604199, Validation loss: 113.45416891, Gradient norm: 11.86639220
INFO:root:[   16] Training loss: 114.08426572, Validation loss: 112.55420632, Gradient norm: 11.90101954
INFO:root:[   17] Training loss: 113.36670631, Validation loss: 111.87878918, Gradient norm: 13.20062284
INFO:root:[   18] Training loss: 112.77540210, Validation loss: 111.37391768, Gradient norm: 12.50323548
INFO:root:[   19] Training loss: 112.26336407, Validation loss: 110.80881869, Gradient norm: 12.64980072
INFO:root:[   20] Training loss: 111.79262549, Validation loss: 110.22736017, Gradient norm: 14.58643748
INFO:root:[   21] Training loss: 111.39389355, Validation loss: 109.85996930, Gradient norm: 13.51176983
INFO:root:[   22] Training loss: 110.96816287, Validation loss: 109.62479453, Gradient norm: 13.48302289
INFO:root:[   23] Training loss: 110.69967348, Validation loss: 109.15907051, Gradient norm: 13.05159775
INFO:root:[   24] Training loss: 110.40416373, Validation loss: 108.88608446, Gradient norm: 14.95937767
INFO:root:[   25] Training loss: 110.07053537, Validation loss: 108.75482809, Gradient norm: 15.20834665
INFO:root:[   26] Training loss: 109.84505530, Validation loss: 108.41231537, Gradient norm: 15.59653874
INFO:root:[   27] Training loss: 109.59743959, Validation loss: 108.03786232, Gradient norm: 14.86724441
INFO:root:[   28] Training loss: 109.37233235, Validation loss: 107.92399623, Gradient norm: 13.69690336
INFO:root:[   29] Training loss: 109.18104222, Validation loss: 107.56265154, Gradient norm: 16.37516945
INFO:root:[   30] Training loss: 108.93932235, Validation loss: 107.55171414, Gradient norm: 14.72705899
INFO:root:[   31] Training loss: 108.74938816, Validation loss: 107.15319009, Gradient norm: 15.33941848
INFO:root:[   32] Training loss: 108.52507755, Validation loss: 107.12177619, Gradient norm: 16.00584503
INFO:root:[   33] Training loss: 108.34874887, Validation loss: 106.91266422, Gradient norm: 15.95755018
INFO:root:[   34] Training loss: 108.18644316, Validation loss: 106.64381435, Gradient norm: 15.85047128
INFO:root:[   35] Training loss: 108.02885714, Validation loss: 106.67443585, Gradient norm: 15.81996457
INFO:root:[   36] Training loss: 107.87979038, Validation loss: 106.30893839, Gradient norm: 15.91703309
INFO:root:[   37] Training loss: 107.70570839, Validation loss: 106.16179920, Gradient norm: 16.56545716
INFO:root:[   38] Training loss: 107.50493095, Validation loss: 106.01320780, Gradient norm: 15.61166099
INFO:root:[   39] Training loss: 107.40616966, Validation loss: 105.93453927, Gradient norm: 16.21706072
INFO:root:[   40] Training loss: 107.29019651, Validation loss: 105.76447717, Gradient norm: 18.18054790
INFO:root:[   41] Training loss: 107.13492483, Validation loss: 105.61892016, Gradient norm: 17.83017341
INFO:root:[   42] Training loss: 106.96941592, Validation loss: 105.65858644, Gradient norm: 18.00352187
INFO:root:[   43] Training loss: 106.90159782, Validation loss: 105.45025845, Gradient norm: 18.66897310
INFO:root:[   44] Training loss: 106.76376356, Validation loss: 105.39863902, Gradient norm: 16.94880961
INFO:root:[   45] Training loss: 106.63590693, Validation loss: 105.24891426, Gradient norm: 17.52164001
INFO:root:[   46] Training loss: 106.48862491, Validation loss: 105.06385751, Gradient norm: 18.02361589
INFO:root:[   47] Training loss: 106.44862204, Validation loss: 104.95079698, Gradient norm: 20.22239243
INFO:root:[   48] Training loss: 106.31694206, Validation loss: 105.12520415, Gradient norm: 19.50784286
INFO:root:[   49] Training loss: 106.22358454, Validation loss: 104.79199903, Gradient norm: 21.54499396
INFO:root:[   50] Training loss: 106.11917074, Validation loss: 104.86282086, Gradient norm: 20.25485046
INFO:root:[   51] Training loss: 106.00925614, Validation loss: 104.75366079, Gradient norm: 21.76907977
INFO:root:[   52] Training loss: 105.90777628, Validation loss: 104.54127345, Gradient norm: 21.62976943
INFO:root:[   53] Training loss: 105.87671243, Validation loss: 104.61189244, Gradient norm: 22.86077229
INFO:root:[   54] Training loss: 105.76252240, Validation loss: 104.27046388, Gradient norm: 22.00361673
INFO:root:[   55] Training loss: 105.66636367, Validation loss: 104.19220392, Gradient norm: 21.77396009
INFO:root:[   56] Training loss: 105.60118555, Validation loss: 104.27892277, Gradient norm: 21.40064356
INFO:root:[   57] Training loss: 105.47550127, Validation loss: 104.15333084, Gradient norm: 19.66499812
INFO:root:[   58] Training loss: 105.44747992, Validation loss: 104.21382930, Gradient norm: 25.21178795
INFO:root:[   59] Training loss: 105.36264693, Validation loss: 104.05443152, Gradient norm: 22.72699665
INFO:root:[   60] Training loss: 105.29117483, Validation loss: 103.97696423, Gradient norm: 27.63716389
INFO:root:[   61] Training loss: 105.22160090, Validation loss: 103.90391935, Gradient norm: 23.66563967
INFO:root:[   62] Training loss: 105.17291658, Validation loss: 103.92464368, Gradient norm: 21.75301297
INFO:root:[   63] Training loss: 105.08966179, Validation loss: 103.82219959, Gradient norm: 24.50677433
INFO:root:[   64] Training loss: 104.97904833, Validation loss: 103.72028114, Gradient norm: 22.85077376
INFO:root:[   65] Training loss: 104.96279671, Validation loss: 103.67046356, Gradient norm: 28.86797717
INFO:root:[   66] Training loss: 104.87609721, Validation loss: 103.60243778, Gradient norm: 29.37172333
INFO:root:[   67] Training loss: 104.80797293, Validation loss: 103.49762515, Gradient norm: 24.95765998
INFO:root:[   68] Training loss: 104.77367178, Validation loss: 103.45486292, Gradient norm: 26.48260325
INFO:root:[   69] Training loss: 104.68769073, Validation loss: 103.43261745, Gradient norm: 26.38763485
INFO:root:[   70] Training loss: 104.62878978, Validation loss: 103.41272499, Gradient norm: 26.67020353
INFO:root:[   71] Training loss: 104.58975226, Validation loss: 103.43681178, Gradient norm: 29.73592352
INFO:root:[   72] Training loss: 104.54687703, Validation loss: 103.28921956, Gradient norm: 32.12663764
INFO:root:[   73] Training loss: 104.45165388, Validation loss: 103.33454185, Gradient norm: 25.05422990
INFO:root:[   74] Training loss: 104.39686963, Validation loss: 103.22302851, Gradient norm: 28.56595828
INFO:root:[   75] Training loss: 104.36546130, Validation loss: 103.32902869, Gradient norm: 27.02607075
INFO:root:[   76] Training loss: 104.28740841, Validation loss: 103.23298066, Gradient norm: 31.99853624
INFO:root:[   77] Training loss: 104.26480332, Validation loss: 103.23808710, Gradient norm: 31.12656571
INFO:root:[   78] Training loss: 104.19004329, Validation loss: 103.19740138, Gradient norm: 25.11497452
INFO:root:[   79] Training loss: 104.16096517, Validation loss: 103.06874821, Gradient norm: 32.30048662
INFO:root:[   80] Training loss: 104.12730300, Validation loss: 102.87444305, Gradient norm: 36.20093783
INFO:root:[   81] Training loss: 104.05440089, Validation loss: 103.01475525, Gradient norm: 32.05777796
INFO:root:[   82] Training loss: 103.96306644, Validation loss: 103.05996836, Gradient norm: 27.68361622
INFO:root:[   83] Training loss: 103.95659415, Validation loss: 102.96217425, Gradient norm: 36.65854416
INFO:root:[   84] Training loss: 103.92000796, Validation loss: 102.83005050, Gradient norm: 28.31507573
INFO:root:[   85] Training loss: 103.87020584, Validation loss: 102.75713769, Gradient norm: 30.83539055
INFO:root:[   86] Training loss: 103.79341821, Validation loss: 102.75547370, Gradient norm: 30.07254889
INFO:root:[   87] Training loss: 103.80106563, Validation loss: 102.56690163, Gradient norm: 31.21422584
INFO:root:[   88] Training loss: 103.69865458, Validation loss: 102.73982055, Gradient norm: 30.60575321
INFO:root:[   89] Training loss: 103.69657277, Validation loss: 102.63038267, Gradient norm: 33.16858018
INFO:root:[   90] Training loss: 103.67654952, Validation loss: 102.90047507, Gradient norm: 35.87993181
INFO:root:[   91] Training loss: 103.59778737, Validation loss: 102.44863444, Gradient norm: 34.36500527
INFO:root:[   92] Training loss: 103.58266962, Validation loss: 102.49753544, Gradient norm: 33.24761255
INFO:root:[   93] Training loss: 103.53659544, Validation loss: 102.51044832, Gradient norm: 30.88409695
INFO:root:[   94] Training loss: 103.47023179, Validation loss: 102.60133414, Gradient norm: 35.26967164
INFO:root:[   95] Training loss: 103.47903071, Validation loss: 102.54552512, Gradient norm: 40.65158183
INFO:root:[   96] Training loss: 103.40818672, Validation loss: 102.43551688, Gradient norm: 37.06588017
INFO:root:[   97] Training loss: 103.35408142, Validation loss: 102.39123956, Gradient norm: 31.87166822
INFO:root:[   98] Training loss: 103.29580594, Validation loss: 102.44634378, Gradient norm: 30.60741898
INFO:root:[   99] Training loss: 103.29257148, Validation loss: 102.37023426, Gradient norm: 34.99074054
INFO:root:[  100] Training loss: 103.28420183, Validation loss: 102.26835238, Gradient norm: 41.05393201
INFO:root:[  101] Training loss: 103.21042403, Validation loss: 102.32203832, Gradient norm: 35.13920876
INFO:root:[  102] Training loss: 103.15643061, Validation loss: 102.21491847, Gradient norm: 33.06907481
INFO:root:[  103] Training loss: 103.12137651, Validation loss: 102.20408920, Gradient norm: 38.11824443
INFO:root:[  104] Training loss: 103.15708734, Validation loss: 102.27205895, Gradient norm: 38.07730435
INFO:root:[  105] Training loss: 103.09255016, Validation loss: 102.18070747, Gradient norm: 35.10483386
INFO:root:[  106] Training loss: 103.03816730, Validation loss: 102.06821336, Gradient norm: 31.97652393
INFO:root:[  107] Training loss: 103.02197293, Validation loss: 102.10067775, Gradient norm: 38.63155915
INFO:root:[  108] Training loss: 102.95890781, Validation loss: 102.00050512, Gradient norm: 32.33844409
INFO:root:[  109] Training loss: 102.90889733, Validation loss: 102.05633834, Gradient norm: 31.50037900
INFO:root:[  110] Training loss: 102.88256289, Validation loss: 101.97942300, Gradient norm: 40.95889769
INFO:root:[  111] Training loss: 102.83285516, Validation loss: 102.04099247, Gradient norm: 38.37886670
INFO:root:[  112] Training loss: 102.84437142, Validation loss: 102.22923358, Gradient norm: 40.49537678
INFO:root:[  113] Training loss: 102.84563824, Validation loss: 101.97268782, Gradient norm: 41.88515951
INFO:root:[  114] Training loss: 102.76572310, Validation loss: 101.93972068, Gradient norm: 33.56134308
INFO:root:[  115] Training loss: 102.75754513, Validation loss: 101.83911185, Gradient norm: 43.94811118
INFO:root:[  116] Training loss: 102.68591484, Validation loss: 101.94987961, Gradient norm: 34.89849081
INFO:root:[  117] Training loss: 102.66603926, Validation loss: 102.07065319, Gradient norm: 33.14694484
INFO:root:[  118] Training loss: 102.63850315, Validation loss: 101.89947878, Gradient norm: 36.91663990
INFO:root:[  119] Training loss: 102.59035776, Validation loss: 102.03667239, Gradient norm: 35.13426838
INFO:root:[  120] Training loss: 102.59232762, Validation loss: 101.94160777, Gradient norm: 35.55807118
INFO:root:[  121] Training loss: 102.55528347, Validation loss: 102.00048170, Gradient norm: 38.79111063
INFO:root:[  122] Training loss: 102.50770879, Validation loss: 101.73438736, Gradient norm: 35.16820849
INFO:root:[  123] Training loss: 102.50359864, Validation loss: 101.77137651, Gradient norm: 35.28859321
INFO:root:[  124] Training loss: 102.46291426, Validation loss: 101.66381047, Gradient norm: 42.96471385
INFO:root:[  125] Training loss: 102.50317639, Validation loss: 101.68016631, Gradient norm: 39.97198403
INFO:root:[  126] Training loss: 102.42212758, Validation loss: 101.96562116, Gradient norm: 43.82082648
INFO:root:[  127] Training loss: 102.39862337, Validation loss: 101.70951791, Gradient norm: 35.31761679
INFO:root:[  128] Training loss: 102.32948965, Validation loss: 101.64301879, Gradient norm: 31.74164452
INFO:root:[  129] Training loss: 102.35344703, Validation loss: 101.69154753, Gradient norm: 38.73191326
INFO:root:[  130] Training loss: 102.30783689, Validation loss: 101.76234883, Gradient norm: 38.76229902
INFO:root:[  131] Training loss: 102.33169191, Validation loss: 101.59801615, Gradient norm: 44.06833476
INFO:root:[  132] Training loss: 102.26545195, Validation loss: 101.76393627, Gradient norm: 40.51515558
INFO:root:[  133] Training loss: 102.19829269, Validation loss: 101.52589548, Gradient norm: 34.41048084
INFO:root:[  134] Training loss: 102.19354194, Validation loss: 101.63577139, Gradient norm: 38.72605813
INFO:root:[  135] Training loss: 102.20291489, Validation loss: 101.49379546, Gradient norm: 35.47951101
INFO:root:[  136] Training loss: 102.18193027, Validation loss: 101.71602394, Gradient norm: 40.62128431
INFO:root:[  137] Training loss: 102.11486405, Validation loss: 101.62854320, Gradient norm: 37.07075556
INFO:root:[  138] Training loss: 102.09855949, Validation loss: 101.55796788, Gradient norm: 40.77221177
INFO:root:[  139] Training loss: 102.06658125, Validation loss: 101.63531494, Gradient norm: 34.59961222
INFO:root:[  140] Training loss: 102.05679335, Validation loss: 101.66437688, Gradient norm: 36.95018834
INFO:root:[  141] Training loss: 102.03723273, Validation loss: 101.71376748, Gradient norm: 39.06215516
INFO:root:[  142] Training loss: 101.98009734, Validation loss: 101.52495075, Gradient norm: 39.74920162
INFO:root:[  143] Training loss: 101.97981701, Validation loss: 101.46912252, Gradient norm: 35.28464539
INFO:root:[  144] Training loss: 101.96273621, Validation loss: 101.46459777, Gradient norm: 44.79766742
INFO:root:[  145] Training loss: 101.92234181, Validation loss: 101.36908064, Gradient norm: 37.53323170
INFO:root:[  146] Training loss: 101.89003024, Validation loss: 101.33295677, Gradient norm: 38.00506251
INFO:root:[  147] Training loss: 101.91466671, Validation loss: 101.44857709, Gradient norm: 38.93398697
INFO:root:[  148] Training loss: 101.90124586, Validation loss: 101.35766733, Gradient norm: 42.66482425
INFO:root:[  149] Training loss: 101.85277773, Validation loss: 101.50815372, Gradient norm: 41.98337329
INFO:root:[  150] Training loss: 101.78421763, Validation loss: 101.57719369, Gradient norm: 37.13717957
INFO:root:[  151] Training loss: 101.80101965, Validation loss: 101.48790031, Gradient norm: 39.57995445
INFO:root:[  152] Training loss: 101.83493332, Validation loss: 101.56338159, Gradient norm: 51.55163527
INFO:root:[  153] Training loss: 101.80279825, Validation loss: 101.35727665, Gradient norm: 38.80504956
INFO:root:[  154] Training loss: 101.74273682, Validation loss: 101.32170052, Gradient norm: 39.15610010
INFO:root:[  155] Training loss: 101.69860381, Validation loss: 101.49346056, Gradient norm: 40.25696066
INFO:root:[  156] Training loss: 101.68647483, Validation loss: 101.35869914, Gradient norm: 37.89782376
INFO:root:[  157] Training loss: 101.68172036, Validation loss: 101.47420423, Gradient norm: 38.01171148
INFO:root:[  158] Training loss: 101.63370386, Validation loss: 101.23677484, Gradient norm: 36.81717811
INFO:root:[  159] Training loss: 101.58031538, Validation loss: 101.36423966, Gradient norm: 42.18362448
INFO:root:[  160] Training loss: 101.60833220, Validation loss: 101.47669746, Gradient norm: 41.19726798
INFO:root:[  161] Training loss: 101.57063172, Validation loss: 101.33627740, Gradient norm: 32.73333693
INFO:root:[  162] Training loss: 101.55458622, Validation loss: 101.36585867, Gradient norm: 39.21468139
INFO:root:[  163] Training loss: 101.53646884, Validation loss: 101.38046291, Gradient norm: 39.65730412
INFO:root:[  164] Training loss: 101.54661385, Validation loss: 101.34508804, Gradient norm: 40.59443098
INFO:root:[  165] Training loss: 101.44791932, Validation loss: 101.45234128, Gradient norm: 38.13305778
INFO:root:[  166] Training loss: 101.47141874, Validation loss: 101.39652752, Gradient norm: 40.67318235
INFO:root:[  167] Training loss: 101.42346414, Validation loss: 101.32542525, Gradient norm: 34.61439903
INFO:root:EP 167: Early stopping
INFO:root:Training the model took 1100.27s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 8931.36779
INFO:root:EnergyScoreTrain: 6287.76714
INFO:root:CoverageTrain: 0.93798
INFO:root:IntervalWidthTrain: 8.59108
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 8936.8214
INFO:root:EnergyScoreValidation: 6290.33662
INFO:root:CoverageValidation: 0.93095
INFO:root:IntervalWidthValidation: 8.60615
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9018.48341
INFO:root:EnergyScoreTest: 6347.78716
INFO:root:CoverageTest: 0.93025
INFO:root:IntervalWidthTest: 8.60755
INFO:root:###5 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.23116059, Validation loss: 121.33279735, Gradient norm: 31.46878980
INFO:root:[    2] Training loss: 121.47642288, Validation loss: 121.33673280, Gradient norm: 8.47564248
INFO:root:[    3] Training loss: 121.37507245, Validation loss: 121.31454520, Gradient norm: 8.48200965
INFO:root:[    4] Training loss: 121.36213995, Validation loss: 121.27865627, Gradient norm: 9.10283513
INFO:root:[    5] Training loss: 121.36867240, Validation loss: 121.33606720, Gradient norm: 9.12917859
INFO:root:[    6] Training loss: 121.33127553, Validation loss: 121.29078280, Gradient norm: 6.93112242
INFO:root:[    7] Training loss: 121.33053386, Validation loss: 121.23939067, Gradient norm: 5.99447032
INFO:root:[    8] Training loss: 121.29355081, Validation loss: 121.35131389, Gradient norm: 5.99143396
INFO:root:[    9] Training loss: 121.31842919, Validation loss: 121.39474724, Gradient norm: 7.15126332
INFO:root:[   10] Training loss: 121.31544252, Validation loss: 121.34771229, Gradient norm: 5.79783875
INFO:root:[   11] Training loss: 121.28644366, Validation loss: 121.24289888, Gradient norm: 4.89861798
INFO:root:[   12] Training loss: 121.29956865, Validation loss: 121.25178160, Gradient norm: 4.99576422
INFO:root:[   13] Training loss: 121.30120951, Validation loss: 121.20240705, Gradient norm: 4.23231994
INFO:root:[   14] Training loss: 121.29845314, Validation loss: 121.25930576, Gradient norm: 5.28493071
INFO:root:[   15] Training loss: 121.30125549, Validation loss: 121.25076925, Gradient norm: 5.71127894
INFO:root:[   16] Training loss: 121.30340333, Validation loss: 121.33033595, Gradient norm: 4.58928988
INFO:root:[   17] Training loss: 121.29726747, Validation loss: 121.17270739, Gradient norm: 3.85938739
INFO:root:[   18] Training loss: 121.31695867, Validation loss: 121.21734961, Gradient norm: 3.94245884
INFO:root:[   19] Training loss: 121.26815587, Validation loss: 121.16112124, Gradient norm: 4.07462555
INFO:root:[   20] Training loss: 121.29768264, Validation loss: 121.28850976, Gradient norm: 4.23875245
INFO:root:[   21] Training loss: 121.29679655, Validation loss: 121.21841904, Gradient norm: 3.91428677
INFO:root:[   22] Training loss: 121.28641618, Validation loss: 121.17328328, Gradient norm: 4.82461304
INFO:root:[   23] Training loss: 121.29393080, Validation loss: 121.20312079, Gradient norm: 4.04627269
INFO:root:[   24] Training loss: 121.26310622, Validation loss: 121.29622782, Gradient norm: 3.28985931
INFO:root:[   25] Training loss: 121.28211462, Validation loss: 121.20062572, Gradient norm: 2.84189856
INFO:root:[   26] Training loss: 121.28585147, Validation loss: 121.26003792, Gradient norm: 3.34707495
INFO:root:[   27] Training loss: 121.18711070, Validation loss: 120.84770097, Gradient norm: 3.19729083
INFO:root:[   28] Training loss: 120.75983632, Validation loss: 120.45122896, Gradient norm: 5.81828856
INFO:root:[   29] Training loss: 120.33707934, Validation loss: 119.64867375, Gradient norm: 4.53604018
INFO:root:[   30] Training loss: 119.30384077, Validation loss: 117.89626812, Gradient norm: 6.08610854
INFO:root:[   31] Training loss: 117.45113386, Validation loss: 115.67540925, Gradient norm: 7.21269375
INFO:root:[   32] Training loss: 115.99997961, Validation loss: 114.31958113, Gradient norm: 8.01044656
INFO:root:[   33] Training loss: 115.11623902, Validation loss: 113.35760103, Gradient norm: 8.11959401
INFO:root:[   34] Training loss: 114.44499510, Validation loss: 112.91620294, Gradient norm: 9.10599576
INFO:root:[   35] Training loss: 113.83402637, Validation loss: 112.25362817, Gradient norm: 9.42561277
INFO:root:[   36] Training loss: 113.34666011, Validation loss: 111.55424342, Gradient norm: 9.83697935
INFO:root:[   37] Training loss: 112.94935230, Validation loss: 111.21098275, Gradient norm: 9.59554770
INFO:root:[   38] Training loss: 112.49015484, Validation loss: 110.81175758, Gradient norm: 9.75225268
INFO:root:[   39] Training loss: 112.14946436, Validation loss: 110.50720136, Gradient norm: 9.72212190
INFO:root:[   40] Training loss: 111.83291923, Validation loss: 110.06695320, Gradient norm: 9.70866668
INFO:root:[   41] Training loss: 111.56000458, Validation loss: 109.89563830, Gradient norm: 10.64987415
INFO:root:[   42] Training loss: 111.25847565, Validation loss: 109.61506863, Gradient norm: 10.98249796
INFO:root:[   43] Training loss: 110.97260244, Validation loss: 109.29331234, Gradient norm: 11.52151048
INFO:root:[   44] Training loss: 110.74300844, Validation loss: 109.02778836, Gradient norm: 12.30871420
INFO:root:[   45] Training loss: 110.56429595, Validation loss: 108.75797377, Gradient norm: 11.30976707
INFO:root:[   46] Training loss: 110.32988881, Validation loss: 108.63956346, Gradient norm: 11.76687922
INFO:root:[   47] Training loss: 110.18200366, Validation loss: 108.37886863, Gradient norm: 12.60135878
INFO:root:[   48] Training loss: 109.98606542, Validation loss: 108.18223177, Gradient norm: 13.39595669
INFO:root:[   49] Training loss: 109.81223135, Validation loss: 108.07594457, Gradient norm: 13.37158190
INFO:root:[   50] Training loss: 109.64799304, Validation loss: 107.83108231, Gradient norm: 12.06777335
INFO:root:[   51] Training loss: 109.43669905, Validation loss: 107.78855554, Gradient norm: 12.10948292
INFO:root:[   52] Training loss: 109.31247806, Validation loss: 107.46054893, Gradient norm: 13.12463056
INFO:root:[   53] Training loss: 109.10606324, Validation loss: 107.37505314, Gradient norm: 12.19151376
INFO:root:[   54] Training loss: 109.05982924, Validation loss: 107.10098635, Gradient norm: 12.93093547
INFO:root:[   55] Training loss: 108.89999133, Validation loss: 106.94572396, Gradient norm: 13.62167411
INFO:root:[   56] Training loss: 108.75216202, Validation loss: 106.96572166, Gradient norm: 14.44133658
INFO:root:[   57] Training loss: 108.66635490, Validation loss: 106.91088183, Gradient norm: 14.49839273
INFO:root:[   58] Training loss: 108.54067750, Validation loss: 106.86360721, Gradient norm: 13.71651411
INFO:root:[   59] Training loss: 108.41824186, Validation loss: 106.63930459, Gradient norm: 14.62443676
INFO:root:[   60] Training loss: 108.26826052, Validation loss: 106.56446733, Gradient norm: 13.71378994
INFO:root:[   61] Training loss: 108.18236751, Validation loss: 106.37510365, Gradient norm: 14.30957052
INFO:root:[   62] Training loss: 108.10246803, Validation loss: 106.43841027, Gradient norm: 14.26652722
INFO:root:[   63] Training loss: 108.01494774, Validation loss: 106.24010810, Gradient norm: 15.88228594
INFO:root:[   64] Training loss: 107.89727601, Validation loss: 106.07415719, Gradient norm: 15.68962165
INFO:root:[   65] Training loss: 107.87550550, Validation loss: 105.99513744, Gradient norm: 15.66914978
INFO:root:[   66] Training loss: 107.72820856, Validation loss: 105.77771075, Gradient norm: 16.16084935
INFO:root:[   67] Training loss: 107.64723003, Validation loss: 105.71842062, Gradient norm: 15.21820639
INFO:root:[   68] Training loss: 107.58040119, Validation loss: 105.77631326, Gradient norm: 15.56489288
INFO:root:[   69] Training loss: 107.47069718, Validation loss: 105.65796293, Gradient norm: 15.89254663
INFO:root:[   70] Training loss: 107.43244434, Validation loss: 105.80628625, Gradient norm: 15.73245403
INFO:root:[   71] Training loss: 107.36876888, Validation loss: 105.53997382, Gradient norm: 17.02383890
INFO:root:[   72] Training loss: 107.31042568, Validation loss: 105.59512908, Gradient norm: 17.23205724
INFO:root:[   73] Training loss: 107.20887777, Validation loss: 105.34541715, Gradient norm: 16.85879336
INFO:root:[   74] Training loss: 107.16493158, Validation loss: 105.26871675, Gradient norm: 16.55428366
INFO:root:[   75] Training loss: 107.10770011, Validation loss: 105.21197773, Gradient norm: 16.75917462
INFO:root:[   76] Training loss: 107.02448752, Validation loss: 105.15054163, Gradient norm: 18.60157568
INFO:root:[   77] Training loss: 106.91614296, Validation loss: 105.24296780, Gradient norm: 15.77184297
INFO:root:[   78] Training loss: 106.91541203, Validation loss: 105.09517933, Gradient norm: 17.79761509
INFO:root:[   79] Training loss: 106.78933371, Validation loss: 104.97589585, Gradient norm: 17.49681615
INFO:root:[   80] Training loss: 106.78537271, Validation loss: 104.98382305, Gradient norm: 19.98635998
INFO:root:[   81] Training loss: 106.71134233, Validation loss: 104.85717773, Gradient norm: 17.94577257
INFO:root:[   82] Training loss: 106.70378626, Validation loss: 104.73808867, Gradient norm: 19.37244671
INFO:root:[   83] Training loss: 106.67303494, Validation loss: 104.62213503, Gradient norm: 20.78486773
INFO:root:[   84] Training loss: 106.54028185, Validation loss: 104.71579821, Gradient norm: 20.18398818
INFO:root:[   85] Training loss: 106.50859657, Validation loss: 104.66031857, Gradient norm: 20.46041475
INFO:root:[   86] Training loss: 106.44548885, Validation loss: 104.62790575, Gradient norm: 20.03674097
INFO:root:[   87] Training loss: 106.40078067, Validation loss: 104.42247062, Gradient norm: 20.10272546
INFO:root:[   88] Training loss: 106.35307980, Validation loss: 104.38871449, Gradient norm: 20.16161649
INFO:root:[   89] Training loss: 106.33543207, Validation loss: 104.39783162, Gradient norm: 21.50654149
INFO:root:[   90] Training loss: 106.26770398, Validation loss: 104.44606439, Gradient norm: 22.23398966
INFO:root:[   91] Training loss: 106.19360014, Validation loss: 104.33860779, Gradient norm: 20.44937527
INFO:root:[   92] Training loss: 106.15654363, Validation loss: 104.46853743, Gradient norm: 25.08012259
INFO:root:[   93] Training loss: 106.14379444, Validation loss: 104.20406210, Gradient norm: 23.43870203
INFO:root:[   94] Training loss: 106.11372173, Validation loss: 104.28124290, Gradient norm: 26.40313488
INFO:root:[   95] Training loss: 106.04914896, Validation loss: 104.31825783, Gradient norm: 20.15851690
INFO:root:[   96] Training loss: 106.02925954, Validation loss: 104.25329695, Gradient norm: 23.11255753
INFO:root:[   97] Training loss: 105.99753078, Validation loss: 104.38715415, Gradient norm: 23.44202378
INFO:root:[   98] Training loss: 105.90742175, Validation loss: 104.19391921, Gradient norm: 25.25994850
INFO:root:[   99] Training loss: 105.85088558, Validation loss: 104.19547509, Gradient norm: 24.12260212
INFO:root:[  100] Training loss: 105.84231372, Validation loss: 103.99195415, Gradient norm: 21.38671261
INFO:root:[  101] Training loss: 105.80067943, Validation loss: 104.05324949, Gradient norm: 24.76282596
INFO:root:[  102] Training loss: 105.80096388, Validation loss: 103.82822103, Gradient norm: 28.07320022
INFO:root:[  103] Training loss: 105.75235634, Validation loss: 104.43606883, Gradient norm: 27.09860170
INFO:root:[  104] Training loss: 105.70653966, Validation loss: 104.01087347, Gradient norm: 26.11420277
INFO:root:[  105] Training loss: 105.65608735, Validation loss: 103.89771087, Gradient norm: 22.48481116
INFO:root:[  106] Training loss: 105.66955121, Validation loss: 103.97423133, Gradient norm: 27.16498742
INFO:root:[  107] Training loss: 105.60244596, Validation loss: 104.00160743, Gradient norm: 28.07849174
INFO:root:[  108] Training loss: 105.58186685, Validation loss: 103.92535848, Gradient norm: 25.90844167
INFO:root:[  109] Training loss: 105.51675422, Validation loss: 103.69717960, Gradient norm: 26.37401827
INFO:root:[  110] Training loss: 105.54219683, Validation loss: 103.81137716, Gradient norm: 27.58218222
INFO:root:[  111] Training loss: 105.48932087, Validation loss: 103.53026186, Gradient norm: 31.58994057
INFO:root:[  112] Training loss: 105.45331270, Validation loss: 103.76869859, Gradient norm: 26.25211674
INFO:root:[  113] Training loss: 105.43825963, Validation loss: 103.73391145, Gradient norm: 24.87569999
INFO:root:[  114] Training loss: 105.38502475, Validation loss: 103.51031257, Gradient norm: 31.02412065
INFO:root:[  115] Training loss: 105.37920900, Validation loss: 103.40074000, Gradient norm: 29.78698467
INFO:root:[  116] Training loss: 105.32380372, Validation loss: 103.63899020, Gradient norm: 29.31110211
INFO:root:[  117] Training loss: 105.27743875, Validation loss: 103.54735407, Gradient norm: 24.92273112
INFO:root:[  118] Training loss: 105.25138484, Validation loss: 103.36109766, Gradient norm: 25.91337584
INFO:root:[  119] Training loss: 105.23750602, Validation loss: 103.31288963, Gradient norm: 28.83398729
INFO:root:[  120] Training loss: 105.20124081, Validation loss: 103.42754417, Gradient norm: 28.53798348
INFO:root:[  121] Training loss: 105.17619033, Validation loss: 103.32497353, Gradient norm: 27.86013971
INFO:root:[  122] Training loss: 105.15262705, Validation loss: 103.69007847, Gradient norm: 26.99203017
INFO:root:[  123] Training loss: 105.13846001, Validation loss: 103.55119113, Gradient norm: 28.84753032
INFO:root:[  124] Training loss: 105.07157263, Validation loss: 103.36317707, Gradient norm: 25.17203898
INFO:root:[  125] Training loss: 105.06962842, Validation loss: 103.36102321, Gradient norm: 30.67923606
INFO:root:[  126] Training loss: 105.04121230, Validation loss: 103.79040817, Gradient norm: 30.59326382
INFO:root:[  127] Training loss: 105.00598455, Validation loss: 103.33883772, Gradient norm: 29.74016665
INFO:root:[  128] Training loss: 105.00563623, Validation loss: 103.18721245, Gradient norm: 29.83271169
INFO:root:[  129] Training loss: 104.95479840, Validation loss: 103.24991660, Gradient norm: 29.05248896
INFO:root:[  130] Training loss: 104.93969078, Validation loss: 103.20730565, Gradient norm: 30.07240455
INFO:root:[  131] Training loss: 104.92647688, Validation loss: 103.04443044, Gradient norm: 32.20689079
INFO:root:[  132] Training loss: 104.87076987, Validation loss: 103.36623014, Gradient norm: 29.50519459
INFO:root:[  133] Training loss: 104.84862424, Validation loss: 103.31249763, Gradient norm: 28.72574101
INFO:root:[  134] Training loss: 104.84368863, Validation loss: 103.08611666, Gradient norm: 32.34845710
INFO:root:[  135] Training loss: 104.80205698, Validation loss: 103.09705248, Gradient norm: 29.80585602
INFO:root:[  136] Training loss: 104.74891663, Validation loss: 103.29307688, Gradient norm: 30.81086041
INFO:root:[  137] Training loss: 104.75524301, Validation loss: 102.89963374, Gradient norm: 27.90703682
INFO:root:[  138] Training loss: 104.73634662, Validation loss: 102.86451774, Gradient norm: 31.90330794
INFO:root:[  139] Training loss: 104.74157154, Validation loss: 103.09230331, Gradient norm: 32.59155611
INFO:root:[  140] Training loss: 104.69828601, Validation loss: 102.85000979, Gradient norm: 31.43896581
INFO:root:[  141] Training loss: 104.68816754, Validation loss: 102.95007245, Gradient norm: 33.57403951
INFO:root:[  142] Training loss: 104.64630870, Validation loss: 102.98376754, Gradient norm: 27.66563870
INFO:root:[  143] Training loss: 104.62513848, Validation loss: 102.93504649, Gradient norm: 29.39837343
INFO:root:[  144] Training loss: 104.60272224, Validation loss: 102.78701177, Gradient norm: 32.02458008
INFO:root:[  145] Training loss: 104.57614244, Validation loss: 102.79356937, Gradient norm: 32.87065295
INFO:root:[  146] Training loss: 104.51378051, Validation loss: 103.04473088, Gradient norm: 27.11001519
INFO:root:[  147] Training loss: 104.55253432, Validation loss: 102.90494695, Gradient norm: 33.67659168
INFO:root:[  148] Training loss: 104.54984074, Validation loss: 102.73808683, Gradient norm: 34.83834263
INFO:root:[  149] Training loss: 104.54077142, Validation loss: 102.90713238, Gradient norm: 33.95636664
INFO:root:[  150] Training loss: 104.48795582, Validation loss: 102.75419196, Gradient norm: 31.44590312
INFO:root:[  151] Training loss: 104.40164704, Validation loss: 102.80372909, Gradient norm: 27.98821061
INFO:root:[  152] Training loss: 104.44667080, Validation loss: 102.65640127, Gradient norm: 33.56530032
INFO:root:[  153] Training loss: 104.38635321, Validation loss: 102.86197689, Gradient norm: 30.37983028
INFO:root:[  154] Training loss: 104.41496236, Validation loss: 102.64305588, Gradient norm: 38.14304921
INFO:root:[  155] Training loss: 104.30233758, Validation loss: 102.56305300, Gradient norm: 32.37652806
INFO:root:[  156] Training loss: 104.32879834, Validation loss: 102.76000635, Gradient norm: 25.45225002
INFO:root:[  157] Training loss: 104.33685843, Validation loss: 102.63541281, Gradient norm: 33.15168134
INFO:root:[  158] Training loss: 104.29986903, Validation loss: 102.60877149, Gradient norm: 28.30663445
INFO:root:[  159] Training loss: 104.28095097, Validation loss: 102.66486622, Gradient norm: 29.45647317
INFO:root:[  160] Training loss: 104.26135956, Validation loss: 102.74642471, Gradient norm: 35.65873858
INFO:root:[  161] Training loss: 104.22061171, Validation loss: 102.72005857, Gradient norm: 32.94516683
INFO:root:[  162] Training loss: 104.27526187, Validation loss: 102.76657262, Gradient norm: 34.14744471
INFO:root:[  163] Training loss: 104.24237803, Validation loss: 102.56865718, Gradient norm: 36.33524419
INFO:root:[  164] Training loss: 104.21917164, Validation loss: 102.53505312, Gradient norm: 35.39471767
INFO:root:[  165] Training loss: 104.14508806, Validation loss: 102.84115653, Gradient norm: 28.05133827
INFO:root:[  166] Training loss: 104.20464419, Validation loss: 102.66960828, Gradient norm: 32.00337475
INFO:root:[  167] Training loss: 104.17772391, Validation loss: 102.73215616, Gradient norm: 32.94624776
INFO:root:[  168] Training loss: 104.10918116, Validation loss: 102.40531237, Gradient norm: 31.01772329
INFO:root:[  169] Training loss: 104.11310348, Validation loss: 102.48693111, Gradient norm: 33.64412632
INFO:root:[  170] Training loss: 104.05768437, Validation loss: 102.40503324, Gradient norm: 31.59736736
INFO:root:[  171] Training loss: 104.02630068, Validation loss: 102.41443134, Gradient norm: 33.48090103
INFO:root:[  172] Training loss: 104.01278113, Validation loss: 102.53549984, Gradient norm: 35.94733947
INFO:root:[  173] Training loss: 104.05179299, Validation loss: 102.54293481, Gradient norm: 38.64816794
INFO:root:[  174] Training loss: 104.00715300, Validation loss: 102.45779524, Gradient norm: 35.99664515
INFO:root:[  175] Training loss: 103.96796849, Validation loss: 102.48048848, Gradient norm: 32.28537857
INFO:root:[  176] Training loss: 103.98481143, Validation loss: 102.48781428, Gradient norm: 31.07207093
INFO:root:[  177] Training loss: 103.95642367, Validation loss: 102.37163991, Gradient norm: 41.21718568
INFO:root:[  178] Training loss: 103.90461893, Validation loss: 102.61732614, Gradient norm: 35.29847919
INFO:root:[  179] Training loss: 103.92536622, Validation loss: 102.31561542, Gradient norm: 31.08305806
INFO:root:[  180] Training loss: 103.88429976, Validation loss: 102.22236344, Gradient norm: 28.26712419
INFO:root:[  181] Training loss: 103.86320928, Validation loss: 102.21495477, Gradient norm: 28.73811221
INFO:root:[  182] Training loss: 103.83202497, Validation loss: 102.47019932, Gradient norm: 35.97811720
INFO:root:[  183] Training loss: 103.81750873, Validation loss: 102.43774335, Gradient norm: 32.34120005
INFO:root:[  184] Training loss: 103.83732355, Validation loss: 102.45007061, Gradient norm: 34.82004346
INFO:root:[  185] Training loss: 103.82573241, Validation loss: 102.42736001, Gradient norm: 34.38952030
INFO:root:[  186] Training loss: 103.77092655, Validation loss: 102.26799116, Gradient norm: 33.19950632
INFO:root:[  187] Training loss: 103.79641359, Validation loss: 102.25692617, Gradient norm: 33.73169190
INFO:root:[  188] Training loss: 103.76791996, Validation loss: 102.20702494, Gradient norm: 35.85006677
INFO:root:[  189] Training loss: 103.76316942, Validation loss: 102.29111323, Gradient norm: 37.18642518
INFO:root:[  190] Training loss: 103.69928215, Validation loss: 102.20676869, Gradient norm: 37.11790131
INFO:root:[  191] Training loss: 103.73657976, Validation loss: 102.30086886, Gradient norm: 38.01628489
INFO:root:[  192] Training loss: 103.69550080, Validation loss: 102.39710025, Gradient norm: 34.00986682
INFO:root:[  193] Training loss: 103.69249921, Validation loss: 102.26271531, Gradient norm: 38.61846429
INFO:root:[  194] Training loss: 103.63911681, Validation loss: 102.09703274, Gradient norm: 31.94778274
INFO:root:[  195] Training loss: 103.63627111, Validation loss: 102.05017511, Gradient norm: 39.19445557
INFO:root:[  196] Training loss: 103.61437252, Validation loss: 102.25023520, Gradient norm: 31.14239152
INFO:root:[  197] Training loss: 103.62138745, Validation loss: 102.16715924, Gradient norm: 30.33663762
INFO:root:[  198] Training loss: 103.59400859, Validation loss: 102.25795851, Gradient norm: 32.39256115
INFO:root:[  199] Training loss: 103.63925954, Validation loss: 102.14046215, Gradient norm: 36.36357137
INFO:root:[  200] Training loss: 103.55286239, Validation loss: 101.91108362, Gradient norm: 37.11008628
INFO:root:[  201] Training loss: 103.51597987, Validation loss: 102.07094495, Gradient norm: 32.97960261
INFO:root:[  202] Training loss: 103.51318501, Validation loss: 102.26804983, Gradient norm: 35.37132017
INFO:root:[  203] Training loss: 103.51127949, Validation loss: 102.17384707, Gradient norm: 28.47588546
INFO:root:[  204] Training loss: 103.47969048, Validation loss: 102.00667256, Gradient norm: 36.39131466
INFO:root:[  205] Training loss: 103.49633688, Validation loss: 102.05744355, Gradient norm: 40.75944059
INFO:root:[  206] Training loss: 103.44494447, Validation loss: 102.11211921, Gradient norm: 34.25311714
INFO:root:[  207] Training loss: 103.46851686, Validation loss: 101.93395049, Gradient norm: 37.84447610
INFO:root:[  208] Training loss: 103.43803689, Validation loss: 102.05967370, Gradient norm: 33.31812429
INFO:root:[  209] Training loss: 103.48346454, Validation loss: 101.93226439, Gradient norm: 43.76833565
INFO:root:EP 209: Early stopping
INFO:root:Training the model took 1375.208s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 9061.98495
INFO:root:EnergyScoreTrain: 6386.85525
INFO:root:CoverageTrain: 0.96331
INFO:root:IntervalWidthTrain: 9.05104
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 8982.47496
INFO:root:EnergyScoreValidation: 6327.74182
INFO:root:CoverageValidation: 0.95912
INFO:root:IntervalWidthValidation: 9.0642
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9058.51983
INFO:root:EnergyScoreTest: 6380.76706
INFO:root:CoverageTest: 0.95872
INFO:root:IntervalWidthTest: 9.06355
INFO:root:###6 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.88515945, Validation loss: 123.26440035, Gradient norm: 29.16380765
INFO:root:[    2] Training loss: 121.45926923, Validation loss: 122.39601925, Gradient norm: 8.79554130
INFO:root:[    3] Training loss: 121.39593216, Validation loss: 122.67923658, Gradient norm: 8.37370947
INFO:root:[    4] Training loss: 121.34866583, Validation loss: 122.77576920, Gradient norm: 6.34814573
INFO:root:[    5] Training loss: 121.30950732, Validation loss: 122.82543130, Gradient norm: 4.94454643
INFO:root:[    6] Training loss: 121.29678466, Validation loss: 122.26563552, Gradient norm: 4.57865725
INFO:root:[    7] Training loss: 121.28705880, Validation loss: 122.05952217, Gradient norm: 5.05655580
INFO:root:[    8] Training loss: 121.30578154, Validation loss: 121.97709840, Gradient norm: 5.36052895
INFO:root:[    9] Training loss: 121.30295151, Validation loss: 122.14339710, Gradient norm: 4.53904470
INFO:root:[   10] Training loss: 121.29445959, Validation loss: 121.69205001, Gradient norm: 3.62994190
INFO:root:[   11] Training loss: 121.30111174, Validation loss: 121.50291627, Gradient norm: 3.35924558
INFO:root:[   12] Training loss: 121.30285192, Validation loss: 121.39706316, Gradient norm: 3.19777203
INFO:root:[   13] Training loss: 121.26839636, Validation loss: 121.36655294, Gradient norm: 2.69078140
INFO:root:[   14] Training loss: 121.28915182, Validation loss: 121.28041313, Gradient norm: 2.67332839
INFO:root:[   15] Training loss: 121.27467684, Validation loss: 121.22512844, Gradient norm: 2.36451715
INFO:root:[   16] Training loss: 121.26955637, Validation loss: 121.21644934, Gradient norm: 2.30098982
INFO:root:[   17] Training loss: 121.27633687, Validation loss: 121.27256749, Gradient norm: 1.99609145
INFO:root:[   18] Training loss: 121.28051967, Validation loss: 121.32312512, Gradient norm: 1.74809957
INFO:root:[   19] Training loss: 121.29435440, Validation loss: 121.23570699, Gradient norm: 1.50085471
INFO:root:[   20] Training loss: 121.27661011, Validation loss: 121.34916266, Gradient norm: 1.52858474
INFO:root:[   21] Training loss: 121.27225373, Validation loss: 121.33029254, Gradient norm: 1.45223110
INFO:root:[   22] Training loss: 121.27305758, Validation loss: 121.23876374, Gradient norm: 1.28397909
INFO:root:[   23] Training loss: 121.27734530, Validation loss: 121.42580282, Gradient norm: 1.22011796
INFO:root:[   24] Training loss: 121.29969193, Validation loss: 121.39694845, Gradient norm: 1.25135249
INFO:root:[   25] Training loss: 121.28335943, Validation loss: 121.23801422, Gradient norm: 1.10186302
INFO:root:[   26] Training loss: 121.29667819, Validation loss: 121.41667254, Gradient norm: 0.84825071
INFO:root:[   27] Training loss: 121.30022660, Validation loss: 121.31381989, Gradient norm: 1.05505372
INFO:root:[   28] Training loss: 121.30903815, Validation loss: 121.25566390, Gradient norm: 1.09240374
INFO:root:[   29] Training loss: 121.28312156, Validation loss: 121.50656470, Gradient norm: 0.86977075
INFO:root:[   30] Training loss: 121.28740625, Validation loss: 121.41446975, Gradient norm: 1.11504355
INFO:root:[   31] Training loss: 121.13868018, Validation loss: 120.95563823, Gradient norm: 1.70832827
INFO:root:[   32] Training loss: 120.75540249, Validation loss: 120.56836648, Gradient norm: 3.59579637
INFO:root:[   33] Training loss: 120.35457949, Validation loss: 119.79775791, Gradient norm: 3.08948906
INFO:root:[   34] Training loss: 119.42157853, Validation loss: 118.45616755, Gradient norm: 4.82600063
INFO:root:[   35] Training loss: 118.40449200, Validation loss: 117.10015974, Gradient norm: 6.12290131
INFO:root:[   36] Training loss: 117.40441874, Validation loss: 115.92834183, Gradient norm: 6.82765677
INFO:root:[   37] Training loss: 116.49705438, Validation loss: 114.89530287, Gradient norm: 7.14147521
INFO:root:[   38] Training loss: 115.81351836, Validation loss: 114.20136761, Gradient norm: 7.37168451
INFO:root:[   39] Training loss: 115.29076169, Validation loss: 113.75190893, Gradient norm: 7.43787411
INFO:root:[   40] Training loss: 114.81771445, Validation loss: 113.07288545, Gradient norm: 7.35955404
INFO:root:[   41] Training loss: 114.41288751, Validation loss: 112.87074806, Gradient norm: 8.18474818
INFO:root:[   42] Training loss: 114.01487968, Validation loss: 112.19121262, Gradient norm: 8.96013913
INFO:root:[   43] Training loss: 113.72041935, Validation loss: 112.10714722, Gradient norm: 9.22672911
INFO:root:[   44] Training loss: 113.41877423, Validation loss: 111.89995812, Gradient norm: 9.06303967
INFO:root:[   45] Training loss: 113.16902444, Validation loss: 111.45610573, Gradient norm: 9.26201681
INFO:root:[   46] Training loss: 112.94641539, Validation loss: 111.12353410, Gradient norm: 9.82814263
INFO:root:[   47] Training loss: 112.70726803, Validation loss: 111.05204826, Gradient norm: 10.08645718
INFO:root:[   48] Training loss: 112.54339465, Validation loss: 110.90424400, Gradient norm: 11.21940100
INFO:root:[   49] Training loss: 112.31477984, Validation loss: 110.61367824, Gradient norm: 10.21867706
INFO:root:[   50] Training loss: 112.17750218, Validation loss: 110.62439859, Gradient norm: 10.98925698
INFO:root:[   51] Training loss: 112.00821503, Validation loss: 110.35882726, Gradient norm: 11.72156686
INFO:root:[   52] Training loss: 111.78711174, Validation loss: 110.17714954, Gradient norm: 11.03929589
INFO:root:[   53] Training loss: 111.70186959, Validation loss: 109.85356982, Gradient norm: 12.04250894
INFO:root:[   54] Training loss: 111.55095693, Validation loss: 109.84840393, Gradient norm: 12.06332076
INFO:root:[   55] Training loss: 111.39904583, Validation loss: 109.68931080, Gradient norm: 11.22192886
INFO:root:[   56] Training loss: 111.25876374, Validation loss: 109.58327958, Gradient norm: 12.89095502
INFO:root:[   57] Training loss: 111.15996734, Validation loss: 109.34327645, Gradient norm: 13.58610470
INFO:root:[   58] Training loss: 111.02028440, Validation loss: 109.23128036, Gradient norm: 11.38631933
INFO:root:[   59] Training loss: 110.95138968, Validation loss: 109.32384307, Gradient norm: 11.96166277
INFO:root:[   60] Training loss: 110.82721791, Validation loss: 109.20814330, Gradient norm: 12.08339075
INFO:root:[   61] Training loss: 110.74701805, Validation loss: 109.16778985, Gradient norm: 12.79220498
INFO:root:[   62] Training loss: 110.63497074, Validation loss: 108.76371476, Gradient norm: 13.06456477
INFO:root:[   63] Training loss: 110.51062140, Validation loss: 108.84972645, Gradient norm: 13.05813189
INFO:root:[   64] Training loss: 110.42869075, Validation loss: 108.90141296, Gradient norm: 14.56078791
INFO:root:[   65] Training loss: 110.34181342, Validation loss: 108.71877210, Gradient norm: 14.67520347
INFO:root:[   66] Training loss: 110.22859867, Validation loss: 108.67363897, Gradient norm: 13.77262437
INFO:root:[   67] Training loss: 110.11662637, Validation loss: 108.51480629, Gradient norm: 14.68739399
INFO:root:[   68] Training loss: 110.05756702, Validation loss: 108.33835023, Gradient norm: 15.16872215
INFO:root:[   69] Training loss: 109.95805008, Validation loss: 108.10639322, Gradient norm: 14.44775544
INFO:root:[   70] Training loss: 109.89394872, Validation loss: 108.61627013, Gradient norm: 14.63268042
INFO:root:[   71] Training loss: 109.81570225, Validation loss: 108.36549930, Gradient norm: 14.79556459
INFO:root:[   72] Training loss: 109.69742193, Validation loss: 107.90003546, Gradient norm: 15.61948794
INFO:root:[   73] Training loss: 109.64712997, Validation loss: 108.46569745, Gradient norm: 15.36377728
INFO:root:[   74] Training loss: 109.60709577, Validation loss: 108.32935307, Gradient norm: 15.81426174
INFO:root:[   75] Training loss: 109.51968762, Validation loss: 108.12939637, Gradient norm: 14.60309971
INFO:root:[   76] Training loss: 109.43677494, Validation loss: 107.97345129, Gradient norm: 15.95505189
INFO:root:[   77] Training loss: 109.36656898, Validation loss: 108.10498810, Gradient norm: 15.61174682
INFO:root:[   78] Training loss: 109.30111897, Validation loss: 108.06911337, Gradient norm: 17.80110058
INFO:root:[   79] Training loss: 109.27690948, Validation loss: 108.50321487, Gradient norm: 17.15369553
INFO:root:[   80] Training loss: 109.20222527, Validation loss: 107.54982547, Gradient norm: 16.86483303
INFO:root:[   81] Training loss: 109.14349696, Validation loss: 107.76135280, Gradient norm: 18.24890056
INFO:root:[   82] Training loss: 109.09851473, Validation loss: 107.52404838, Gradient norm: 16.66770197
INFO:root:[   83] Training loss: 109.02699516, Validation loss: 107.55471407, Gradient norm: 17.64108689
INFO:root:[   84] Training loss: 109.00346266, Validation loss: 107.75604564, Gradient norm: 20.67342534
INFO:root:[   85] Training loss: 108.92141251, Validation loss: 107.35350221, Gradient norm: 20.60542802
INFO:root:[   86] Training loss: 108.88963136, Validation loss: 107.61378058, Gradient norm: 15.98473690
INFO:root:[   87] Training loss: 108.82362109, Validation loss: 107.62757400, Gradient norm: 18.65532341
INFO:root:[   88] Training loss: 108.78739180, Validation loss: 107.53908460, Gradient norm: 19.39544723
INFO:root:[   89] Training loss: 108.74364309, Validation loss: 107.56351497, Gradient norm: 19.66021476
INFO:root:[   90] Training loss: 108.66114395, Validation loss: 107.63219505, Gradient norm: 21.33031346
INFO:root:[   91] Training loss: 108.63492699, Validation loss: 107.58798823, Gradient norm: 19.23011030
INFO:root:[   92] Training loss: 108.57716464, Validation loss: 107.25615771, Gradient norm: 20.06049471
INFO:root:[   93] Training loss: 108.48737403, Validation loss: 107.47490324, Gradient norm: 19.03554774
INFO:root:[   94] Training loss: 108.49736246, Validation loss: 107.48715631, Gradient norm: 22.23055261
INFO:root:[   95] Training loss: 108.45456662, Validation loss: 107.60279083, Gradient norm: 21.98463356
INFO:root:[   96] Training loss: 108.40957493, Validation loss: 107.26996928, Gradient norm: 21.58041967
INFO:root:[   97] Training loss: 108.36807150, Validation loss: 107.15758304, Gradient norm: 21.53294392
INFO:root:[   98] Training loss: 108.30944777, Validation loss: 107.65211829, Gradient norm: 23.35329475
INFO:root:[   99] Training loss: 108.29738502, Validation loss: 107.11089772, Gradient norm: 24.26958309
INFO:root:[  100] Training loss: 108.24986470, Validation loss: 107.17022363, Gradient norm: 19.99971326
INFO:root:[  101] Training loss: 108.20357061, Validation loss: 107.67203232, Gradient norm: 23.09833196
INFO:root:[  102] Training loss: 108.13710643, Validation loss: 107.35310311, Gradient norm: 22.36900018
INFO:root:[  103] Training loss: 108.11373847, Validation loss: 106.91954330, Gradient norm: 23.36936049
INFO:root:[  104] Training loss: 108.10765177, Validation loss: 107.38251758, Gradient norm: 24.67529545
INFO:root:[  105] Training loss: 108.06554730, Validation loss: 107.23852302, Gradient norm: 23.85294922
INFO:root:[  106] Training loss: 108.03498462, Validation loss: 107.28059440, Gradient norm: 27.67459836
INFO:root:[  107] Training loss: 108.00387317, Validation loss: 106.99605297, Gradient norm: 24.17524162
INFO:root:[  108] Training loss: 107.98854882, Validation loss: 107.24163950, Gradient norm: 23.59431122
INFO:root:[  109] Training loss: 107.90862517, Validation loss: 107.34895903, Gradient norm: 23.26141170
INFO:root:[  110] Training loss: 107.87380745, Validation loss: 107.36294424, Gradient norm: 24.04948996
INFO:root:[  111] Training loss: 107.86844250, Validation loss: 107.33555971, Gradient norm: 26.77923781
INFO:root:[  112] Training loss: 107.81522221, Validation loss: 107.15074026, Gradient norm: 24.78524880
INFO:root:EP 112: Early stopping
INFO:root:Training the model took 740.3s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 9604.52108
INFO:root:EnergyScoreTrain: 6791.21943
INFO:root:CoverageTrain: 0.97746
INFO:root:IntervalWidthTrain: 10.31131
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 9392.84745
INFO:root:EnergyScoreValidation: 6639.99585
INFO:root:CoverageValidation: 0.97664
INFO:root:IntervalWidthValidation: 10.31372
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9476.28542
INFO:root:EnergyScoreTest: 6698.16768
INFO:root:CoverageTest: 0.9766
INFO:root:IntervalWidthTest: 10.31773
INFO:root:###7 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.51002867, Validation loss: 171.17869410, Gradient norm: 2.15486570
INFO:root:[    2] Training loss: 170.85780618, Validation loss: 170.18685334, Gradient norm: 6.21795461
INFO:root:[    3] Training loss: 169.87306551, Validation loss: 168.83643420, Gradient norm: 6.28078704
INFO:root:[    4] Training loss: 167.56826904, Validation loss: 165.43386630, Gradient norm: 12.22214949
INFO:root:[    5] Training loss: 165.18913755, Validation loss: 163.17859992, Gradient norm: 12.63809731
INFO:root:[    6] Training loss: 163.24596115, Validation loss: 161.47028745, Gradient norm: 13.24744892
INFO:root:[    7] Training loss: 161.76419499, Validation loss: 159.90129247, Gradient norm: 12.60217559
INFO:root:[    8] Training loss: 160.60777215, Validation loss: 158.67502883, Gradient norm: 14.53034327
INFO:root:[    9] Training loss: 159.66188535, Validation loss: 157.87015455, Gradient norm: 14.09049692
INFO:root:[   10] Training loss: 158.83156079, Validation loss: 156.75131804, Gradient norm: 14.78919317
INFO:root:[   11] Training loss: 158.15417251, Validation loss: 156.37383560, Gradient norm: 15.36338725
INFO:root:[   12] Training loss: 157.56207748, Validation loss: 155.60398970, Gradient norm: 15.42553590
INFO:root:[   13] Training loss: 157.02753941, Validation loss: 155.06925070, Gradient norm: 15.68909968
INFO:root:[   14] Training loss: 156.57332847, Validation loss: 154.60685467, Gradient norm: 19.69028704
INFO:root:[   15] Training loss: 156.12048988, Validation loss: 154.09688963, Gradient norm: 17.82581688
INFO:root:[   16] Training loss: 155.66004201, Validation loss: 153.74553338, Gradient norm: 15.06230946
INFO:root:[   17] Training loss: 155.32487704, Validation loss: 153.27327597, Gradient norm: 15.97993143
INFO:root:[   18] Training loss: 155.00679165, Validation loss: 153.15845253, Gradient norm: 16.19987489
INFO:root:[   19] Training loss: 154.68747920, Validation loss: 152.88031269, Gradient norm: 17.34424730
INFO:root:[   20] Training loss: 154.35378677, Validation loss: 152.25635713, Gradient norm: 17.65151284
INFO:root:[   21] Training loss: 154.10359286, Validation loss: 152.20827563, Gradient norm: 18.01246423
INFO:root:[   22] Training loss: 153.82533548, Validation loss: 151.94003612, Gradient norm: 17.34282003
INFO:root:[   23] Training loss: 153.57851592, Validation loss: 151.51520512, Gradient norm: 19.34452781
INFO:root:[   24] Training loss: 153.30317891, Validation loss: 151.34109865, Gradient norm: 19.64210517
INFO:root:[   25] Training loss: 153.00567573, Validation loss: 150.98660804, Gradient norm: 16.57882669
INFO:root:[   26] Training loss: 152.80700170, Validation loss: 150.87968392, Gradient norm: 21.28760246
INFO:root:[   27] Training loss: 152.64393535, Validation loss: 150.68806247, Gradient norm: 17.45075851
INFO:root:[   28] Training loss: 152.39278999, Validation loss: 150.43883593, Gradient norm: 18.06489802
INFO:root:[   29] Training loss: 152.18034349, Validation loss: 150.29245732, Gradient norm: 20.46200150
INFO:root:[   30] Training loss: 152.00373152, Validation loss: 150.02821087, Gradient norm: 21.17576108
INFO:root:[   31] Training loss: 151.80775074, Validation loss: 149.90212276, Gradient norm: 20.58493311
INFO:root:[   32] Training loss: 151.60406373, Validation loss: 149.61288663, Gradient norm: 21.10477142
INFO:root:[   33] Training loss: 151.38537854, Validation loss: 149.24470573, Gradient norm: 20.50969220
INFO:root:[   34] Training loss: 151.28446353, Validation loss: 149.47891446, Gradient norm: 20.16309137
INFO:root:[   35] Training loss: 151.09946124, Validation loss: 149.22203169, Gradient norm: 20.38437893
INFO:root:[   36] Training loss: 150.94223117, Validation loss: 149.03614491, Gradient norm: 21.32452811
INFO:root:[   37] Training loss: 150.79301183, Validation loss: 148.80616918, Gradient norm: 24.33170563
INFO:root:[   38] Training loss: 150.66390816, Validation loss: 148.78285743, Gradient norm: 22.49874706
INFO:root:[   39] Training loss: 150.45714792, Validation loss: 148.73946091, Gradient norm: 23.16924492
INFO:root:[   40] Training loss: 150.31456277, Validation loss: 148.54132133, Gradient norm: 22.38135850
INFO:root:[   41] Training loss: 150.23299914, Validation loss: 148.23253184, Gradient norm: 26.24896628
INFO:root:[   42] Training loss: 150.08531067, Validation loss: 148.19841424, Gradient norm: 23.72468263
INFO:root:[   43] Training loss: 149.99006855, Validation loss: 147.86001219, Gradient norm: 26.21590300
INFO:root:[   44] Training loss: 149.80231483, Validation loss: 148.09312913, Gradient norm: 24.37283045
INFO:root:[   45] Training loss: 149.71002832, Validation loss: 147.84474919, Gradient norm: 23.21113361
INFO:root:[   46] Training loss: 149.61480173, Validation loss: 147.87009614, Gradient norm: 28.58090845
INFO:root:[   47] Training loss: 149.46785702, Validation loss: 147.51020708, Gradient norm: 27.53609501
INFO:root:[   48] Training loss: 149.35915733, Validation loss: 147.59123651, Gradient norm: 26.23292483
INFO:root:[   49] Training loss: 149.24942597, Validation loss: 147.65587169, Gradient norm: 29.18636597
INFO:root:[   50] Training loss: 149.18376349, Validation loss: 147.28358354, Gradient norm: 28.04926961
INFO:root:[   51] Training loss: 149.06293899, Validation loss: 147.33760439, Gradient norm: 27.29128976
INFO:root:[   52] Training loss: 148.98981982, Validation loss: 147.12048498, Gradient norm: 29.25189025
INFO:root:[   53] Training loss: 148.85241942, Validation loss: 146.95674975, Gradient norm: 33.01515311
INFO:root:[   54] Training loss: 148.75522958, Validation loss: 146.88843247, Gradient norm: 28.34679190
INFO:root:[   55] Training loss: 148.64617393, Validation loss: 147.00341218, Gradient norm: 35.37939980
INFO:root:[   56] Training loss: 148.54935935, Validation loss: 146.74840046, Gradient norm: 29.57315541
INFO:root:[   57] Training loss: 148.43739211, Validation loss: 146.70487187, Gradient norm: 32.19443178
INFO:root:[   58] Training loss: 148.40153179, Validation loss: 146.59958412, Gradient norm: 30.88414934
INFO:root:[   59] Training loss: 148.30358266, Validation loss: 146.79302084, Gradient norm: 39.76462898
INFO:root:[   60] Training loss: 148.26526176, Validation loss: 146.64046505, Gradient norm: 34.96537614
INFO:root:[   61] Training loss: 148.13185687, Validation loss: 146.65092626, Gradient norm: 33.46830893
INFO:root:[   62] Training loss: 148.04731615, Validation loss: 146.38479404, Gradient norm: 38.18170459
INFO:root:[   63] Training loss: 147.96943057, Validation loss: 146.21146209, Gradient norm: 35.71634559
INFO:root:[   64] Training loss: 147.88713141, Validation loss: 146.30146421, Gradient norm: 38.72519001
INFO:root:[   65] Training loss: 147.84008735, Validation loss: 146.20412787, Gradient norm: 39.35286612
INFO:root:[   66] Training loss: 147.76638929, Validation loss: 146.35669840, Gradient norm: 42.14924001
INFO:root:[   67] Training loss: 147.69681029, Validation loss: 146.16315118, Gradient norm: 38.58837449
INFO:root:[   68] Training loss: 147.60056055, Validation loss: 145.94091323, Gradient norm: 35.31177655
INFO:root:[   69] Training loss: 147.58029445, Validation loss: 145.95097193, Gradient norm: 41.90952332
INFO:root:[   70] Training loss: 147.43146157, Validation loss: 145.89879687, Gradient norm: 37.03222527
INFO:root:[   71] Training loss: 147.43987808, Validation loss: 145.85008029, Gradient norm: 39.50679520
INFO:root:[   72] Training loss: 147.34468511, Validation loss: 146.03424072, Gradient norm: 40.12791629
INFO:root:[   73] Training loss: 147.22659153, Validation loss: 145.98872428, Gradient norm: 34.53928922
INFO:root:[   74] Training loss: 147.19269852, Validation loss: 145.61734535, Gradient norm: 35.28326713
INFO:root:[   75] Training loss: 147.10713763, Validation loss: 145.82584197, Gradient norm: 42.22960158
INFO:root:[   76] Training loss: 147.07737300, Validation loss: 145.69029972, Gradient norm: 38.03122519
INFO:root:[   77] Training loss: 146.99916914, Validation loss: 145.60709250, Gradient norm: 39.60914778
INFO:root:[   78] Training loss: 146.94880987, Validation loss: 145.50273553, Gradient norm: 43.78891398
INFO:root:[   79] Training loss: 146.86325708, Validation loss: 145.64293750, Gradient norm: 42.11268356
INFO:root:[   80] Training loss: 146.81231419, Validation loss: 145.63654880, Gradient norm: 36.47501279
INFO:root:[   81] Training loss: 146.74617828, Validation loss: 145.69931346, Gradient norm: 41.17754605
INFO:root:[   82] Training loss: 146.72130524, Validation loss: 145.28417390, Gradient norm: 43.51624912
INFO:root:[   83] Training loss: 146.72538285, Validation loss: 145.29206059, Gradient norm: 52.41319850
INFO:root:[   84] Training loss: 146.55264282, Validation loss: 145.28722303, Gradient norm: 40.02225132
INFO:root:[   85] Training loss: 146.53065356, Validation loss: 145.23764775, Gradient norm: 38.59057631
INFO:root:[   86] Training loss: 146.43988456, Validation loss: 145.26640162, Gradient norm: 41.28825969
INFO:root:[   87] Training loss: 146.37808727, Validation loss: 145.00873802, Gradient norm: 40.48023316
INFO:root:[   88] Training loss: 146.32110866, Validation loss: 145.19875415, Gradient norm: 40.68018236
INFO:root:[   89] Training loss: 146.32740014, Validation loss: 145.25895323, Gradient norm: 42.09400813
INFO:root:[   90] Training loss: 146.26312593, Validation loss: 144.97267203, Gradient norm: 47.14240237
INFO:root:[   91] Training loss: 146.23393101, Validation loss: 145.05470539, Gradient norm: 43.69527882
INFO:root:[   92] Training loss: 146.11778124, Validation loss: 145.03220025, Gradient norm: 41.42750671
INFO:root:[   93] Training loss: 146.09678326, Validation loss: 144.98040929, Gradient norm: 50.63940392
INFO:root:[   94] Training loss: 146.11796084, Validation loss: 144.89936618, Gradient norm: 46.57495798
INFO:root:[   95] Training loss: 146.00315020, Validation loss: 144.78374928, Gradient norm: 45.16681638
INFO:root:[   96] Training loss: 145.95719775, Validation loss: 144.76223807, Gradient norm: 47.33458010
INFO:root:[   97] Training loss: 145.88297643, Validation loss: 144.70137287, Gradient norm: 46.99865038
INFO:root:[   98] Training loss: 145.85737799, Validation loss: 145.26850260, Gradient norm: 40.90488339
INFO:root:[   99] Training loss: 145.84540308, Validation loss: 144.88635570, Gradient norm: 50.23751477
INFO:root:[  100] Training loss: 145.78209572, Validation loss: 144.77935107, Gradient norm: 49.52103797
INFO:root:[  101] Training loss: 145.72714112, Validation loss: 144.70520861, Gradient norm: 41.84178568
INFO:root:[  102] Training loss: 145.62028125, Validation loss: 144.88456410, Gradient norm: 46.33609208
INFO:root:[  103] Training loss: 145.59442166, Validation loss: 144.75984192, Gradient norm: 45.90159154
INFO:root:[  104] Training loss: 145.62713083, Validation loss: 144.66593039, Gradient norm: 50.44725414
INFO:root:[  105] Training loss: 145.51971665, Validation loss: 144.41727211, Gradient norm: 36.55605508
INFO:root:[  106] Training loss: 145.46918791, Validation loss: 144.64374516, Gradient norm: 49.50246054
INFO:root:[  107] Training loss: 145.54609194, Validation loss: 144.48431133, Gradient norm: 47.07593537
INFO:root:[  108] Training loss: 145.36852771, Validation loss: 144.48358312, Gradient norm: 43.88641470
INFO:root:[  109] Training loss: 145.37728720, Validation loss: 144.61926901, Gradient norm: 44.76529868
INFO:root:[  110] Training loss: 145.25564062, Validation loss: 144.42074532, Gradient norm: 44.10594932
INFO:root:[  111] Training loss: 145.26800375, Validation loss: 144.49810212, Gradient norm: 52.26110847
INFO:root:[  112] Training loss: 145.21375227, Validation loss: 144.33494726, Gradient norm: 45.14630203
INFO:root:[  113] Training loss: 145.12462029, Validation loss: 144.56818416, Gradient norm: 44.21902526
INFO:root:[  114] Training loss: 145.16360717, Validation loss: 144.46463591, Gradient norm: 49.99787104
INFO:root:[  115] Training loss: 145.11996230, Validation loss: 144.50242983, Gradient norm: 49.69367323
INFO:root:[  116] Training loss: 145.08712120, Validation loss: 144.23228349, Gradient norm: 52.04015909
INFO:root:[  117] Training loss: 145.00690886, Validation loss: 144.39786924, Gradient norm: 43.89475048
INFO:root:[  118] Training loss: 145.01347837, Validation loss: 144.40497510, Gradient norm: 49.61578803
INFO:root:[  119] Training loss: 144.93191218, Validation loss: 144.33490569, Gradient norm: 52.29713340
INFO:root:[  120] Training loss: 144.93195566, Validation loss: 144.20268513, Gradient norm: 53.61987559
INFO:root:[  121] Training loss: 144.91098914, Validation loss: 144.20732117, Gradient norm: 50.07552477
INFO:root:[  122] Training loss: 144.82207820, Validation loss: 144.35270901, Gradient norm: 48.78669024
INFO:root:[  123] Training loss: 144.73570359, Validation loss: 144.32474018, Gradient norm: 44.07746216
INFO:root:[  124] Training loss: 144.78351856, Validation loss: 144.05788237, Gradient norm: 53.71849329
INFO:root:[  125] Training loss: 144.68182643, Validation loss: 144.20904278, Gradient norm: 45.37818211
INFO:root:[  126] Training loss: 144.65766556, Validation loss: 144.28779813, Gradient norm: 54.38836237
INFO:root:[  127] Training loss: 144.62800571, Validation loss: 144.23453706, Gradient norm: 46.82164763
INFO:root:[  128] Training loss: 144.56553015, Validation loss: 144.21019771, Gradient norm: 41.34593303
INFO:root:[  129] Training loss: 144.55616099, Validation loss: 144.15579750, Gradient norm: 49.50471751
INFO:root:[  130] Training loss: 144.51061458, Validation loss: 144.07273917, Gradient norm: 50.21950083
INFO:root:[  131] Training loss: 144.43022385, Validation loss: 144.08151193, Gradient norm: 40.82342473
INFO:root:[  132] Training loss: 144.39146477, Validation loss: 143.94789176, Gradient norm: 42.59900850
INFO:root:[  133] Training loss: 144.40140810, Validation loss: 143.91273183, Gradient norm: 54.35148666
INFO:root:[  134] Training loss: 144.30789738, Validation loss: 144.12696049, Gradient norm: 47.68954879
INFO:root:[  135] Training loss: 144.30909094, Validation loss: 144.07736680, Gradient norm: 45.53734798
INFO:root:[  136] Training loss: 144.24548151, Validation loss: 144.02828348, Gradient norm: 48.81755425
INFO:root:[  137] Training loss: 144.21972157, Validation loss: 144.23398669, Gradient norm: 47.80039665
INFO:root:[  138] Training loss: 144.23667395, Validation loss: 144.00444189, Gradient norm: 55.16577573
INFO:root:[  139] Training loss: 144.16510671, Validation loss: 143.89834121, Gradient norm: 51.01320637
INFO:root:[  140] Training loss: 144.16254270, Validation loss: 143.93165062, Gradient norm: 50.04640807
INFO:root:[  141] Training loss: 144.08416910, Validation loss: 143.88326027, Gradient norm: 50.94216832
INFO:root:[  142] Training loss: 144.06313763, Validation loss: 144.00687856, Gradient norm: 50.74966928
INFO:root:[  143] Training loss: 143.99822606, Validation loss: 143.90298094, Gradient norm: 48.85490113
INFO:root:[  144] Training loss: 143.97196636, Validation loss: 144.16041881, Gradient norm: 52.15464574
INFO:root:[  145] Training loss: 143.97547143, Validation loss: 143.92088423, Gradient norm: 51.80895473
INFO:root:[  146] Training loss: 143.89061244, Validation loss: 143.69447064, Gradient norm: 47.73515088
INFO:root:[  147] Training loss: 143.89764607, Validation loss: 143.96560774, Gradient norm: 43.24510527
INFO:root:[  148] Training loss: 143.85206415, Validation loss: 143.96764400, Gradient norm: 53.40854922
INFO:root:[  149] Training loss: 143.78462044, Validation loss: 143.88830724, Gradient norm: 43.02041098
INFO:root:[  150] Training loss: 143.78196419, Validation loss: 144.04357963, Gradient norm: 52.55759940
INFO:root:[  151] Training loss: 143.74099367, Validation loss: 143.88019167, Gradient norm: 44.83649005
INFO:root:[  152] Training loss: 143.70357277, Validation loss: 143.85963650, Gradient norm: 49.23871545
INFO:root:[  153] Training loss: 143.71306347, Validation loss: 144.04501711, Gradient norm: 49.17706732
INFO:root:[  154] Training loss: 143.64900937, Validation loss: 143.92724452, Gradient norm: 50.40527947
INFO:root:[  155] Training loss: 143.64098527, Validation loss: 143.70202900, Gradient norm: 50.07120290
INFO:root:EP 155: Early stopping
INFO:root:Training the model took 776.29s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 8881.93282
INFO:root:EnergyScoreTrain: 8869.22579
INFO:root:CoverageTrain: 0.00241
INFO:root:IntervalWidthTrain: 0.00592
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 8930.93675
INFO:root:EnergyScoreValidation: 8918.52186
INFO:root:CoverageValidation: 0.00233
INFO:root:IntervalWidthValidation: 0.00588
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9005.0176
INFO:root:EnergyScoreTest: 8992.41705
INFO:root:CoverageTest: 0.00232
INFO:root:IntervalWidthTest: 0.00595
INFO:root:###8 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.56294912, Validation loss: 171.42514827, Gradient norm: 2.64950850
INFO:root:[    2] Training loss: 171.48965103, Validation loss: 171.46475799, Gradient norm: 0.63687157
INFO:root:[    3] Training loss: 171.21454951, Validation loss: 170.53263329, Gradient norm: 5.15522581
INFO:root:[    4] Training loss: 170.35915206, Validation loss: 169.58035015, Gradient norm: 6.79174455
INFO:root:[    5] Training loss: 168.67832812, Validation loss: 166.41487227, Gradient norm: 8.67658921
INFO:root:[    6] Training loss: 165.68171908, Validation loss: 163.01956177, Gradient norm: 12.58815659
INFO:root:[    7] Training loss: 163.42914048, Validation loss: 161.01083269, Gradient norm: 13.43786411
INFO:root:[    8] Training loss: 162.00591258, Validation loss: 159.65548496, Gradient norm: 14.98992532
INFO:root:[    9] Training loss: 161.02140444, Validation loss: 158.52583787, Gradient norm: 14.95097693
INFO:root:[   10] Training loss: 160.21641811, Validation loss: 157.79256992, Gradient norm: 17.87902073
INFO:root:[   11] Training loss: 159.56263166, Validation loss: 157.04454725, Gradient norm: 17.89465706
INFO:root:[   12] Training loss: 158.99959233, Validation loss: 156.36191322, Gradient norm: 15.76672303
INFO:root:[   13] Training loss: 158.50477681, Validation loss: 155.77964625, Gradient norm: 16.94322124
INFO:root:[   14] Training loss: 158.03105204, Validation loss: 155.52251829, Gradient norm: 18.05641843
INFO:root:[   15] Training loss: 157.67039814, Validation loss: 155.02413519, Gradient norm: 18.07974274
INFO:root:[   16] Training loss: 157.26981320, Validation loss: 154.71871843, Gradient norm: 20.07241904
INFO:root:[   17] Training loss: 156.97610177, Validation loss: 154.16352423, Gradient norm: 19.28135972
INFO:root:[   18] Training loss: 156.63128811, Validation loss: 153.85582970, Gradient norm: 17.45159144
INFO:root:[   19] Training loss: 156.28738768, Validation loss: 153.56074840, Gradient norm: 18.16592595
INFO:root:[   20] Training loss: 156.03079791, Validation loss: 153.23535893, Gradient norm: 19.17441437
INFO:root:[   21] Training loss: 155.74259787, Validation loss: 152.85239068, Gradient norm: 19.80045356
INFO:root:[   22] Training loss: 155.46681051, Validation loss: 152.64332528, Gradient norm: 20.48090700
INFO:root:[   23] Training loss: 155.23426346, Validation loss: 152.36245149, Gradient norm: 19.73573338
INFO:root:[   24] Training loss: 154.98890483, Validation loss: 152.11135285, Gradient norm: 19.66870180
INFO:root:[   25] Training loss: 154.76642372, Validation loss: 151.78505733, Gradient norm: 18.68105616
INFO:root:[   26] Training loss: 154.56148387, Validation loss: 151.73686323, Gradient norm: 22.13737597
INFO:root:[   27] Training loss: 154.36189500, Validation loss: 151.39788134, Gradient norm: 21.50989533
INFO:root:[   28] Training loss: 154.14963025, Validation loss: 151.41926575, Gradient norm: 20.15342605
INFO:root:[   29] Training loss: 153.92693592, Validation loss: 151.22192909, Gradient norm: 20.90257109
INFO:root:[   30] Training loss: 153.73039489, Validation loss: 150.88117823, Gradient norm: 21.47460942
INFO:root:[   31] Training loss: 153.57407548, Validation loss: 150.64864639, Gradient norm: 21.97849689
INFO:root:[   32] Training loss: 153.39419569, Validation loss: 150.55611078, Gradient norm: 20.73288609
INFO:root:[   33] Training loss: 153.23677022, Validation loss: 150.46460014, Gradient norm: 23.77732900
INFO:root:[   34] Training loss: 153.10616404, Validation loss: 150.29494608, Gradient norm: 23.10331255
INFO:root:[   35] Training loss: 152.90750176, Validation loss: 150.17917133, Gradient norm: 21.42923001
INFO:root:[   36] Training loss: 152.75564022, Validation loss: 150.01871885, Gradient norm: 22.61298002
INFO:root:[   37] Training loss: 152.65600843, Validation loss: 149.94069593, Gradient norm: 25.08292002
INFO:root:[   38] Training loss: 152.49340969, Validation loss: 149.61294082, Gradient norm: 22.92621195
INFO:root:[   39] Training loss: 152.35058810, Validation loss: 149.42980904, Gradient norm: 25.47995035
INFO:root:[   40] Training loss: 152.20354549, Validation loss: 149.32054822, Gradient norm: 22.26489276
INFO:root:[   41] Training loss: 152.09587975, Validation loss: 149.19916087, Gradient norm: 25.50169901
INFO:root:[   42] Training loss: 151.95143681, Validation loss: 148.91571676, Gradient norm: 23.60215973
INFO:root:[   43] Training loss: 151.82302235, Validation loss: 149.02751738, Gradient norm: 24.98275525
INFO:root:[   44] Training loss: 151.74158052, Validation loss: 149.10933606, Gradient norm: 24.74104256
INFO:root:[   45] Training loss: 151.60503772, Validation loss: 148.60108579, Gradient norm: 25.67688945
INFO:root:[   46] Training loss: 151.52264890, Validation loss: 148.61482554, Gradient norm: 25.86225064
INFO:root:[   47] Training loss: 151.36292814, Validation loss: 148.59306335, Gradient norm: 27.03974881
INFO:root:[   48] Training loss: 151.29275202, Validation loss: 148.44135574, Gradient norm: 30.57006146
INFO:root:[   49] Training loss: 151.23378153, Validation loss: 148.66023728, Gradient norm: 31.13448817
INFO:root:[   50] Training loss: 151.09877892, Validation loss: 148.14653489, Gradient norm: 26.74438223
INFO:root:[   51] Training loss: 151.00089622, Validation loss: 148.07503957, Gradient norm: 28.18142792
INFO:root:[   52] Training loss: 150.88503367, Validation loss: 147.88087095, Gradient norm: 31.23074226
INFO:root:[   53] Training loss: 150.80248726, Validation loss: 147.95219579, Gradient norm: 30.37740546
INFO:root:[   54] Training loss: 150.73373062, Validation loss: 147.87288692, Gradient norm: 38.24090483
INFO:root:[   55] Training loss: 150.60040189, Validation loss: 147.68065775, Gradient norm: 31.69828001
INFO:root:[   56] Training loss: 150.54586697, Validation loss: 147.60182295, Gradient norm: 31.56567544
INFO:root:[   57] Training loss: 150.42869014, Validation loss: 147.59843497, Gradient norm: 31.99143375
INFO:root:[   58] Training loss: 150.34988646, Validation loss: 147.74056902, Gradient norm: 32.16387624
INFO:root:[   59] Training loss: 150.29152754, Validation loss: 147.32389621, Gradient norm: 32.65642234
INFO:root:[   60] Training loss: 150.20287897, Validation loss: 147.19085904, Gradient norm: 35.90794787
INFO:root:[   61] Training loss: 150.11278676, Validation loss: 147.08328878, Gradient norm: 37.24250466
INFO:root:[   62] Training loss: 150.04112284, Validation loss: 147.15578119, Gradient norm: 35.10240288
INFO:root:[   63] Training loss: 149.96151544, Validation loss: 147.22124718, Gradient norm: 35.74127549
INFO:root:[   64] Training loss: 149.87300569, Validation loss: 147.03061018, Gradient norm: 38.36369211
INFO:root:[   65] Training loss: 149.74331030, Validation loss: 146.88384221, Gradient norm: 32.08706792
INFO:root:[   66] Training loss: 149.70814744, Validation loss: 146.74995212, Gradient norm: 41.22887868
INFO:root:[   67] Training loss: 149.62225814, Validation loss: 146.89115169, Gradient norm: 33.37969109
INFO:root:[   68] Training loss: 149.58697483, Validation loss: 146.76302364, Gradient norm: 43.87222218
INFO:root:[   69] Training loss: 149.52693176, Validation loss: 146.79722700, Gradient norm: 42.83327554
INFO:root:[   70] Training loss: 149.42626129, Validation loss: 146.49006653, Gradient norm: 35.38462860
INFO:root:[   71] Training loss: 149.40308549, Validation loss: 146.40498036, Gradient norm: 40.30411713
INFO:root:[   72] Training loss: 149.36259096, Validation loss: 146.74576227, Gradient norm: 41.85113875
INFO:root:[   73] Training loss: 149.20667719, Validation loss: 146.29347913, Gradient norm: 41.60782690
INFO:root:[   74] Training loss: 149.16047993, Validation loss: 146.15561492, Gradient norm: 39.31744094
INFO:root:[   75] Training loss: 149.11423540, Validation loss: 146.62878997, Gradient norm: 41.59464748
INFO:root:[   76] Training loss: 149.10804911, Validation loss: 146.16909369, Gradient norm: 48.34045935
INFO:root:[   77] Training loss: 148.99014984, Validation loss: 146.26666681, Gradient norm: 43.68742392
INFO:root:[   78] Training loss: 148.96770606, Validation loss: 146.09522537, Gradient norm: 48.41730455
INFO:root:[   79] Training loss: 148.90867912, Validation loss: 146.16893426, Gradient norm: 45.60927222
INFO:root:[   80] Training loss: 148.84353152, Validation loss: 145.87198665, Gradient norm: 43.22678170
INFO:root:[   81] Training loss: 148.80312030, Validation loss: 145.70873866, Gradient norm: 41.28611883
INFO:root:[   82] Training loss: 148.71049621, Validation loss: 145.81290883, Gradient norm: 41.61084151
INFO:root:[   83] Training loss: 148.69771974, Validation loss: 145.88846562, Gradient norm: 43.59943726
INFO:root:[   84] Training loss: 148.64467722, Validation loss: 145.69878729, Gradient norm: 41.93154941
INFO:root:[   85] Training loss: 148.56867724, Validation loss: 145.71801389, Gradient norm: 49.47447247
INFO:root:[   86] Training loss: 148.49815247, Validation loss: 145.65158976, Gradient norm: 46.54570302
INFO:root:[   87] Training loss: 148.47471659, Validation loss: 145.57949198, Gradient norm: 45.37898913
INFO:root:[   88] Training loss: 148.42592655, Validation loss: 145.69655109, Gradient norm: 44.88110209
INFO:root:[   89] Training loss: 148.40070174, Validation loss: 145.52349380, Gradient norm: 42.31847737
INFO:root:[   90] Training loss: 148.38171414, Validation loss: 145.59489599, Gradient norm: 54.73224780
INFO:root:[   91] Training loss: 148.33681292, Validation loss: 145.24434004, Gradient norm: 38.92938473
INFO:root:[   92] Training loss: 148.25337233, Validation loss: 145.23604136, Gradient norm: 51.30646676
INFO:root:[   93] Training loss: 148.21335973, Validation loss: 145.59688121, Gradient norm: 45.03115861
INFO:root:[   94] Training loss: 148.19940496, Validation loss: 145.14585192, Gradient norm: 49.08844087
INFO:root:[   95] Training loss: 148.11014847, Validation loss: 145.49531134, Gradient norm: 40.71558905
INFO:root:[   96] Training loss: 148.07844503, Validation loss: 145.28648166, Gradient norm: 53.65272432
INFO:root:[   97] Training loss: 148.08711202, Validation loss: 145.43436353, Gradient norm: 48.02776798
INFO:root:[   98] Training loss: 148.00098277, Validation loss: 144.95098403, Gradient norm: 42.37348891
INFO:root:[   99] Training loss: 147.90010543, Validation loss: 145.02894382, Gradient norm: 49.22800676
INFO:root:[  100] Training loss: 147.85772638, Validation loss: 145.04767530, Gradient norm: 42.31674678
INFO:root:[  101] Training loss: 147.88798969, Validation loss: 145.01832581, Gradient norm: 46.32831440
INFO:root:[  102] Training loss: 147.83104361, Validation loss: 145.48221509, Gradient norm: 51.21531736
INFO:root:[  103] Training loss: 147.72667714, Validation loss: 145.17439375, Gradient norm: 49.03616786
INFO:root:[  104] Training loss: 147.78837059, Validation loss: 145.03089800, Gradient norm: 45.87081128
INFO:root:[  105] Training loss: 147.69172547, Validation loss: 144.85459689, Gradient norm: 54.23216463
INFO:root:[  106] Training loss: 147.67233317, Validation loss: 144.83946596, Gradient norm: 53.05897763
INFO:root:[  107] Training loss: 147.60666542, Validation loss: 144.74199282, Gradient norm: 43.80474389
INFO:root:[  108] Training loss: 147.55364882, Validation loss: 145.11174116, Gradient norm: 41.58473289
INFO:root:[  109] Training loss: 147.52722262, Validation loss: 144.78850792, Gradient norm: 47.94020182
INFO:root:[  110] Training loss: 147.48961295, Validation loss: 144.81245633, Gradient norm: 49.89794707
INFO:root:[  111] Training loss: 147.41501975, Validation loss: 144.86988936, Gradient norm: 48.07364840
INFO:root:[  112] Training loss: 147.45650786, Validation loss: 144.56690979, Gradient norm: 47.72998959
INFO:root:[  113] Training loss: 147.31531734, Validation loss: 144.39331265, Gradient norm: 44.49778390
INFO:root:[  114] Training loss: 147.33861832, Validation loss: 144.44626591, Gradient norm: 52.11542250
INFO:root:[  115] Training loss: 147.38986152, Validation loss: 144.85910771, Gradient norm: 51.00442006
INFO:root:[  116] Training loss: 147.27421502, Validation loss: 144.29452672, Gradient norm: 47.91041483
INFO:root:[  117] Training loss: 147.19482260, Validation loss: 144.43128178, Gradient norm: 51.45925028
INFO:root:[  118] Training loss: 147.20223148, Validation loss: 144.40696348, Gradient norm: 44.15181472
INFO:root:[  119] Training loss: 147.15842404, Validation loss: 144.53775551, Gradient norm: 58.46591725
INFO:root:[  120] Training loss: 147.10999507, Validation loss: 144.30404032, Gradient norm: 46.17728843
INFO:root:[  121] Training loss: 147.10532332, Validation loss: 144.61659030, Gradient norm: 49.12259586
INFO:root:[  122] Training loss: 147.07334765, Validation loss: 144.46497161, Gradient norm: 52.40949671
INFO:root:[  123] Training loss: 147.01067805, Validation loss: 144.21176358, Gradient norm: 44.82376781
INFO:root:[  124] Training loss: 147.03214149, Validation loss: 144.27668289, Gradient norm: 48.07079990
INFO:root:[  125] Training loss: 146.92965982, Validation loss: 144.14324688, Gradient norm: 42.79339731
INFO:root:[  126] Training loss: 146.94812012, Validation loss: 144.00721793, Gradient norm: 59.04176380
INFO:root:[  127] Training loss: 146.90081463, Validation loss: 144.02554163, Gradient norm: 44.90981394
INFO:root:[  128] Training loss: 146.88602306, Validation loss: 144.42919974, Gradient norm: 48.78323661
INFO:root:[  129] Training loss: 146.86106062, Validation loss: 144.12533990, Gradient norm: 54.50412853
INFO:root:[  130] Training loss: 146.85175114, Validation loss: 144.18237147, Gradient norm: 52.11932776
INFO:root:[  131] Training loss: 146.80213604, Validation loss: 143.99686142, Gradient norm: 53.98821034
INFO:root:[  132] Training loss: 146.78924250, Validation loss: 144.31163130, Gradient norm: 51.95553818
INFO:root:[  133] Training loss: 146.69366874, Validation loss: 144.20467193, Gradient norm: 46.69348152
INFO:root:[  134] Training loss: 146.70387903, Validation loss: 144.28289690, Gradient norm: 50.07053930
INFO:root:[  135] Training loss: 146.62472764, Validation loss: 144.05540519, Gradient norm: 43.31406756
INFO:root:[  136] Training loss: 146.62997977, Validation loss: 143.89157262, Gradient norm: 48.75851067
INFO:root:[  137] Training loss: 146.63119223, Validation loss: 143.99406486, Gradient norm: 51.90673182
INFO:root:[  138] Training loss: 146.59169533, Validation loss: 144.18978514, Gradient norm: 54.80422974
INFO:root:[  139] Training loss: 146.51361503, Validation loss: 143.75703746, Gradient norm: 51.14287474
INFO:root:[  140] Training loss: 146.54638442, Validation loss: 143.90855671, Gradient norm: 51.19139976
INFO:root:[  141] Training loss: 146.50682567, Validation loss: 144.15596377, Gradient norm: 49.39167917
INFO:root:[  142] Training loss: 146.44284854, Validation loss: 143.65195386, Gradient norm: 52.28104287
INFO:root:[  143] Training loss: 146.46089794, Validation loss: 143.74042958, Gradient norm: 52.37850336
INFO:root:[  144] Training loss: 146.35648069, Validation loss: 144.20756899, Gradient norm: 44.85377498
INFO:root:[  145] Training loss: 146.35379109, Validation loss: 143.88465408, Gradient norm: 45.50927886
INFO:root:[  146] Training loss: 146.36816352, Validation loss: 143.70208372, Gradient norm: 55.38178358
INFO:root:[  147] Training loss: 146.36975152, Validation loss: 143.92544766, Gradient norm: 61.42106426
INFO:root:[  148] Training loss: 146.31084111, Validation loss: 143.92128201, Gradient norm: 45.22045291
INFO:root:[  149] Training loss: 146.23968263, Validation loss: 143.86581474, Gradient norm: 49.54063301
INFO:root:[  150] Training loss: 146.25226829, Validation loss: 143.63485560, Gradient norm: 57.64609930
INFO:root:[  151] Training loss: 146.26354778, Validation loss: 143.75755100, Gradient norm: 50.43218351
INFO:root:[  152] Training loss: 146.19739823, Validation loss: 143.76188291, Gradient norm: 54.15948930
INFO:root:[  153] Training loss: 146.13290338, Validation loss: 143.71854059, Gradient norm: 48.90809802
INFO:root:[  154] Training loss: 146.10136049, Validation loss: 143.78255857, Gradient norm: 51.51397751
INFO:root:[  155] Training loss: 146.14333904, Validation loss: 144.04360173, Gradient norm: 53.53570697
INFO:root:[  156] Training loss: 146.05997285, Validation loss: 143.54534965, Gradient norm: 50.85960838
INFO:root:[  157] Training loss: 146.11985158, Validation loss: 143.52552322, Gradient norm: 48.17030972
INFO:root:[  158] Training loss: 146.04187930, Validation loss: 143.59473709, Gradient norm: 50.49852919
INFO:root:[  159] Training loss: 146.02122038, Validation loss: 143.27709277, Gradient norm: 52.17041865
INFO:root:[  160] Training loss: 146.06838746, Validation loss: 143.84259349, Gradient norm: 55.85363405
INFO:root:[  161] Training loss: 146.00249582, Validation loss: 143.84673651, Gradient norm: 49.80790423
INFO:root:[  162] Training loss: 146.02271588, Validation loss: 143.56284043, Gradient norm: 61.46140608
INFO:root:[  163] Training loss: 145.91820168, Validation loss: 143.65043851, Gradient norm: 46.96077370
INFO:root:[  164] Training loss: 145.85819994, Validation loss: 143.80658853, Gradient norm: 54.39078577
INFO:root:[  165] Training loss: 145.83306966, Validation loss: 143.33976219, Gradient norm: 49.51272758
INFO:root:[  166] Training loss: 145.81941277, Validation loss: 143.39917361, Gradient norm: 56.58676066
INFO:root:[  167] Training loss: 145.84977020, Validation loss: 143.75824290, Gradient norm: 52.80610380
INFO:root:[  168] Training loss: 145.84256589, Validation loss: 143.94111844, Gradient norm: 56.52487566
INFO:root:EP 168: Early stopping
INFO:root:Training the model took 843.019s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 8972.00993
INFO:root:EnergyScoreTrain: 8959.0022
INFO:root:CoverageTrain: 0.0024
INFO:root:IntervalWidthTrain: 0.00612
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 8896.64888
INFO:root:EnergyScoreValidation: 8883.58675
INFO:root:CoverageValidation: 0.00239
INFO:root:IntervalWidthValidation: 0.00637
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 8970.17637
INFO:root:EnergyScoreTest: 8957.27103
INFO:root:CoverageTest: 0.00235
INFO:root:IntervalWidthTest: 0.0061
INFO:root:###9 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.92922325, Validation loss: 171.39759511, Gradient norm: 11.22271626
INFO:root:[    2] Training loss: 171.49025652, Validation loss: 171.41309593, Gradient norm: 0.76031711
INFO:root:[    3] Training loss: 171.48053478, Validation loss: 171.38882972, Gradient norm: 0.64736576
INFO:root:[    4] Training loss: 171.42354604, Validation loss: 170.98879637, Gradient norm: 2.69036863
INFO:root:[    5] Training loss: 170.93707046, Validation loss: 170.45622569, Gradient norm: 7.40696717
INFO:root:[    6] Training loss: 170.50129173, Validation loss: 169.78095535, Gradient norm: 4.71750859
INFO:root:[    7] Training loss: 169.61720505, Validation loss: 168.09331381, Gradient norm: 5.87124653
INFO:root:[    8] Training loss: 167.83496715, Validation loss: 165.59164902, Gradient norm: 8.60865519
INFO:root:[    9] Training loss: 165.85856817, Validation loss: 163.07173525, Gradient norm: 9.28122755
INFO:root:[   10] Training loss: 164.38377380, Validation loss: 161.55695580, Gradient norm: 10.52217828
INFO:root:[   11] Training loss: 163.34301380, Validation loss: 160.32115805, Gradient norm: 11.79128555
INFO:root:[   12] Training loss: 162.49197833, Validation loss: 159.60658264, Gradient norm: 11.26097743
INFO:root:[   13] Training loss: 161.91808623, Validation loss: 158.90709292, Gradient norm: 12.59459706
INFO:root:[   14] Training loss: 161.34570772, Validation loss: 158.33782854, Gradient norm: 13.09857021
INFO:root:[   15] Training loss: 160.88308824, Validation loss: 157.68873123, Gradient norm: 13.50456201
INFO:root:[   16] Training loss: 160.43696041, Validation loss: 157.23851697, Gradient norm: 14.49360152
INFO:root:[   17] Training loss: 159.99314273, Validation loss: 156.89280017, Gradient norm: 12.89816237
INFO:root:[   18] Training loss: 159.66501570, Validation loss: 156.51510462, Gradient norm: 14.71391142
INFO:root:[   19] Training loss: 159.33241961, Validation loss: 156.12819593, Gradient norm: 14.34152974
INFO:root:[   20] Training loss: 159.03431607, Validation loss: 155.82451288, Gradient norm: 15.40935592
INFO:root:[   21] Training loss: 158.73761000, Validation loss: 155.64269862, Gradient norm: 16.87255816
INFO:root:[   22] Training loss: 158.41548400, Validation loss: 155.23271705, Gradient norm: 15.04607051
INFO:root:[   23] Training loss: 158.17460133, Validation loss: 154.97674140, Gradient norm: 14.64748853
INFO:root:[   24] Training loss: 157.93544479, Validation loss: 154.77876282, Gradient norm: 15.28048730
INFO:root:[   25] Training loss: 157.70450477, Validation loss: 154.41659967, Gradient norm: 16.50856121
INFO:root:[   26] Training loss: 157.45836363, Validation loss: 154.17726661, Gradient norm: 17.51187813
INFO:root:[   27] Training loss: 157.24048810, Validation loss: 154.51883092, Gradient norm: 17.32128324
INFO:root:[   28] Training loss: 157.06469875, Validation loss: 154.17648842, Gradient norm: 17.17042621
INFO:root:[   29] Training loss: 156.81656235, Validation loss: 153.94221812, Gradient norm: 16.25165263
INFO:root:[   30] Training loss: 156.64696793, Validation loss: 153.41587040, Gradient norm: 16.53448545
INFO:root:[   31] Training loss: 156.48377788, Validation loss: 153.49968167, Gradient norm: 18.28870010
INFO:root:[   32] Training loss: 156.29614919, Validation loss: 153.23401563, Gradient norm: 16.55236852
INFO:root:[   33] Training loss: 156.13697464, Validation loss: 153.19089955, Gradient norm: 18.32903860
INFO:root:[   34] Training loss: 155.96189462, Validation loss: 152.85107054, Gradient norm: 17.85248432
INFO:root:[   35] Training loss: 155.79392033, Validation loss: 152.83681041, Gradient norm: 18.31451155
INFO:root:[   36] Training loss: 155.65944975, Validation loss: 152.43417043, Gradient norm: 20.72834579
INFO:root:[   37] Training loss: 155.48732913, Validation loss: 152.40998156, Gradient norm: 20.71740409
INFO:root:[   38] Training loss: 155.32153874, Validation loss: 152.05990022, Gradient norm: 18.91748700
INFO:root:[   39] Training loss: 155.24871759, Validation loss: 151.95598102, Gradient norm: 20.59310261
INFO:root:[   40] Training loss: 155.03687604, Validation loss: 151.91176421, Gradient norm: 20.86331279
INFO:root:[   41] Training loss: 154.93808686, Validation loss: 151.74834889, Gradient norm: 20.61662817
INFO:root:[   42] Training loss: 154.80606120, Validation loss: 151.59054250, Gradient norm: 21.26852330
INFO:root:[   43] Training loss: 154.69216986, Validation loss: 151.40356340, Gradient norm: 20.08471625
INFO:root:[   44] Training loss: 154.55894241, Validation loss: 151.44466111, Gradient norm: 22.43262887
INFO:root:[   45] Training loss: 154.43607310, Validation loss: 151.33349136, Gradient norm: 22.68995766
INFO:root:[   46] Training loss: 154.31604477, Validation loss: 151.28871997, Gradient norm: 20.87772916
INFO:root:[   47] Training loss: 154.22025900, Validation loss: 151.31931331, Gradient norm: 24.45721378
INFO:root:[   48] Training loss: 154.12935740, Validation loss: 151.15800160, Gradient norm: 24.47243371
INFO:root:[   49] Training loss: 154.00899296, Validation loss: 150.64597926, Gradient norm: 25.23708838
INFO:root:[   50] Training loss: 153.91846959, Validation loss: 150.51041176, Gradient norm: 24.63633201
INFO:root:[   51] Training loss: 153.82647273, Validation loss: 150.53849687, Gradient norm: 23.98583058
INFO:root:[   52] Training loss: 153.70428210, Validation loss: 150.66440346, Gradient norm: 24.14611115
INFO:root:[   53] Training loss: 153.60890481, Validation loss: 150.57901580, Gradient norm: 25.31457994
INFO:root:[   54] Training loss: 153.55344749, Validation loss: 150.34755523, Gradient norm: 24.96930300
INFO:root:[   55] Training loss: 153.43856285, Validation loss: 150.10508991, Gradient norm: 24.48121209
INFO:root:[   56] Training loss: 153.32537194, Validation loss: 150.16681329, Gradient norm: 27.84468639
INFO:root:[   57] Training loss: 153.30926446, Validation loss: 150.02110554, Gradient norm: 29.24186657
INFO:root:[   58] Training loss: 153.20535481, Validation loss: 149.99761910, Gradient norm: 25.80473437
INFO:root:[   59] Training loss: 153.18402208, Validation loss: 149.85058278, Gradient norm: 30.21769979
INFO:root:[   60] Training loss: 153.11094922, Validation loss: 150.00457606, Gradient norm: 29.07049282
INFO:root:[   61] Training loss: 153.00924588, Validation loss: 149.85620749, Gradient norm: 26.90639659
INFO:root:[   62] Training loss: 152.92491393, Validation loss: 149.88161232, Gradient norm: 23.80908477
INFO:root:[   63] Training loss: 152.87833141, Validation loss: 149.83204756, Gradient norm: 31.42815476
INFO:root:[   64] Training loss: 152.78770190, Validation loss: 149.65318667, Gradient norm: 29.14525224
INFO:root:[   65] Training loss: 152.70965023, Validation loss: 149.53503208, Gradient norm: 31.51236321
INFO:root:[   66] Training loss: 152.69824597, Validation loss: 149.55582665, Gradient norm: 31.36648310
INFO:root:[   67] Training loss: 152.64340304, Validation loss: 149.07656387, Gradient norm: 31.04938317
INFO:root:[   68] Training loss: 152.59631982, Validation loss: 149.49939965, Gradient norm: 33.31097316
INFO:root:[   69] Training loss: 152.52330274, Validation loss: 149.30100066, Gradient norm: 28.61620448
INFO:root:[   70] Training loss: 152.47284165, Validation loss: 149.58891244, Gradient norm: 38.39721133
INFO:root:[   71] Training loss: 152.38131025, Validation loss: 149.22329817, Gradient norm: 30.43208747
INFO:root:[   72] Training loss: 152.33521912, Validation loss: 149.17786802, Gradient norm: 31.43964115
INFO:root:[   73] Training loss: 152.27941179, Validation loss: 149.02353484, Gradient norm: 35.08203896
INFO:root:[   74] Training loss: 152.22376893, Validation loss: 149.21282696, Gradient norm: 32.96028317
INFO:root:[   75] Training loss: 152.17012726, Validation loss: 149.03187351, Gradient norm: 31.33615230
INFO:root:[   76] Training loss: 152.15391595, Validation loss: 148.69662949, Gradient norm: 35.84710634
INFO:root:[   77] Training loss: 152.08711148, Validation loss: 148.96826593, Gradient norm: 36.42327732
INFO:root:[   78] Training loss: 152.01166204, Validation loss: 149.16853911, Gradient norm: 39.24281484
INFO:root:[   79] Training loss: 152.00214879, Validation loss: 149.10174245, Gradient norm: 40.27028940
INFO:root:[   80] Training loss: 151.95660995, Validation loss: 148.88051263, Gradient norm: 35.70521982
INFO:root:[   81] Training loss: 151.88988785, Validation loss: 148.60472212, Gradient norm: 32.71991327
INFO:root:[   82] Training loss: 151.88675858, Validation loss: 148.85363980, Gradient norm: 43.49774868
INFO:root:[   83] Training loss: 151.79513698, Validation loss: 148.73595350, Gradient norm: 31.44932673
INFO:root:[   84] Training loss: 151.73706851, Validation loss: 149.32603455, Gradient norm: 38.24648816
INFO:root:[   85] Training loss: 151.72942575, Validation loss: 148.54933956, Gradient norm: 36.79658655
INFO:root:[   86] Training loss: 151.61734968, Validation loss: 148.26816059, Gradient norm: 31.73917661
INFO:root:[   87] Training loss: 151.66583076, Validation loss: 148.27914744, Gradient norm: 42.81664363
INFO:root:[   88] Training loss: 151.59110685, Validation loss: 148.73461598, Gradient norm: 38.37367216
INFO:root:[   89] Training loss: 151.53849522, Validation loss: 148.69651900, Gradient norm: 36.06571308
INFO:root:[   90] Training loss: 151.49920627, Validation loss: 148.37685263, Gradient norm: 40.94745150
INFO:root:[   91] Training loss: 151.47361607, Validation loss: 148.54669979, Gradient norm: 37.49231292
INFO:root:[   92] Training loss: 151.49468143, Validation loss: 148.38927170, Gradient norm: 47.70938776
INFO:root:[   93] Training loss: 151.44463247, Validation loss: 148.36427097, Gradient norm: 50.21505228
INFO:root:[   94] Training loss: 151.33294597, Validation loss: 148.93367373, Gradient norm: 37.89864299
INFO:root:[   95] Training loss: 151.31332857, Validation loss: 148.80018037, Gradient norm: 41.59054033
INFO:root:[   96] Training loss: 151.31712274, Validation loss: 148.24389122, Gradient norm: 38.70214972
INFO:root:[   97] Training loss: 151.30670031, Validation loss: 148.35273637, Gradient norm: 42.43692335
INFO:root:[   98] Training loss: 151.23908673, Validation loss: 148.15733180, Gradient norm: 38.44026930
INFO:root:[   99] Training loss: 151.22201268, Validation loss: 148.57066030, Gradient norm: 44.40025215
INFO:root:[  100] Training loss: 151.12605920, Validation loss: 148.21720360, Gradient norm: 40.19134008
INFO:root:[  101] Training loss: 151.11972937, Validation loss: 148.13052000, Gradient norm: 38.34893638
INFO:root:[  102] Training loss: 151.08550350, Validation loss: 148.18730690, Gradient norm: 46.95491568
INFO:root:[  103] Training loss: 151.01938082, Validation loss: 148.14015671, Gradient norm: 42.87205133
INFO:root:[  104] Training loss: 151.05196482, Validation loss: 148.31597848, Gradient norm: 44.24582067
INFO:root:[  105] Training loss: 150.98195425, Validation loss: 148.30068075, Gradient norm: 42.13999702
INFO:root:[  106] Training loss: 150.91533749, Validation loss: 147.96802652, Gradient norm: 39.07161042
INFO:root:[  107] Training loss: 150.92210294, Validation loss: 148.66944149, Gradient norm: 46.08948580
INFO:root:[  108] Training loss: 150.90559198, Validation loss: 147.94130049, Gradient norm: 53.63166637
INFO:root:[  109] Training loss: 150.81897337, Validation loss: 147.96974182, Gradient norm: 39.32898645
INFO:root:[  110] Training loss: 150.81257886, Validation loss: 148.25803033, Gradient norm: 40.84809304
INFO:root:[  111] Training loss: 150.85424643, Validation loss: 148.14500638, Gradient norm: 50.17314052
INFO:root:[  112] Training loss: 150.73344772, Validation loss: 148.68329436, Gradient norm: 50.93825184
INFO:root:[  113] Training loss: 150.72707981, Validation loss: 148.11437936, Gradient norm: 46.12015000
INFO:root:[  114] Training loss: 150.70912184, Validation loss: 147.54112033, Gradient norm: 43.25957811
INFO:root:[  115] Training loss: 150.72124042, Validation loss: 147.89313639, Gradient norm: 50.96808944
INFO:root:[  116] Training loss: 150.66634970, Validation loss: 147.98786663, Gradient norm: 45.45686010
INFO:root:[  117] Training loss: 150.59443394, Validation loss: 148.10217390, Gradient norm: 42.73371833
INFO:root:[  118] Training loss: 150.57952233, Validation loss: 148.05884368, Gradient norm: 43.75663195
INFO:root:[  119] Training loss: 150.59848981, Validation loss: 148.00025361, Gradient norm: 43.51321948
INFO:root:[  120] Training loss: 150.48894751, Validation loss: 148.18055409, Gradient norm: 51.21412688
INFO:root:[  121] Training loss: 150.47549519, Validation loss: 148.31880925, Gradient norm: 48.85026503
INFO:root:[  122] Training loss: 150.47435538, Validation loss: 147.77364323, Gradient norm: 50.22352491
INFO:root:[  123] Training loss: 150.40682470, Validation loss: 148.07995132, Gradient norm: 41.67717677
INFO:root:EP 123: Early stopping
INFO:root:Training the model took 618.731s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 9354.42406
INFO:root:EnergyScoreTrain: 9287.48028
INFO:root:CoverageTrain: 0.0099
INFO:root:IntervalWidthTrain: 0.02977
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 9161.22027
INFO:root:EnergyScoreValidation: 9096.20395
INFO:root:CoverageValidation: 0.00981
INFO:root:IntervalWidthValidation: 0.03044
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9233.92427
INFO:root:EnergyScoreTest: 9164.64132
INFO:root:CoverageTest: 0.01049
INFO:root:IntervalWidthTest: 0.03307
INFO:root:###10 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.56071594, Validation loss: 171.35649951, Gradient norm: 3.70594304
INFO:root:[    2] Training loss: 171.47823800, Validation loss: 171.35734137, Gradient norm: 0.73423360
INFO:root:[    3] Training loss: 171.04781065, Validation loss: 170.45799887, Gradient norm: 5.82663369
INFO:root:[    4] Training loss: 170.15697500, Validation loss: 169.36315129, Gradient norm: 6.59431830
INFO:root:[    5] Training loss: 168.17876090, Validation loss: 167.20830036, Gradient norm: 13.73822772
INFO:root:[    6] Training loss: 165.96042329, Validation loss: 165.08679199, Gradient norm: 14.16707905
INFO:root:[    7] Training loss: 163.95128085, Validation loss: 163.07155951, Gradient norm: 14.62296998
INFO:root:[    8] Training loss: 162.14226215, Validation loss: 161.54644881, Gradient norm: 14.49633840
INFO:root:[    9] Training loss: 160.88929087, Validation loss: 160.53425756, Gradient norm: 17.15147406
INFO:root:[   10] Training loss: 159.89151852, Validation loss: 159.43842921, Gradient norm: 17.59889934
INFO:root:[   11] Training loss: 159.08862507, Validation loss: 158.82844280, Gradient norm: 19.07831454
INFO:root:[   12] Training loss: 158.36829829, Validation loss: 158.21087278, Gradient norm: 18.91082807
INFO:root:[   13] Training loss: 157.78191896, Validation loss: 157.65777904, Gradient norm: 20.94113718
INFO:root:[   14] Training loss: 157.17856902, Validation loss: 157.29950846, Gradient norm: 19.95425726
INFO:root:[   15] Training loss: 156.69322704, Validation loss: 156.70146337, Gradient norm: 19.31963795
INFO:root:[   16] Training loss: 156.22159529, Validation loss: 156.55872687, Gradient norm: 18.39563622
INFO:root:[   17] Training loss: 155.84946313, Validation loss: 155.98321481, Gradient norm: 18.36491564
INFO:root:[   18] Training loss: 155.47304663, Validation loss: 155.74344293, Gradient norm: 20.97728988
INFO:root:[   19] Training loss: 155.11169758, Validation loss: 155.40114094, Gradient norm: 19.94568061
INFO:root:[   20] Training loss: 154.78755728, Validation loss: 154.98593929, Gradient norm: 24.46340683
INFO:root:[   21] Training loss: 154.46696607, Validation loss: 154.83972378, Gradient norm: 22.66071246
INFO:root:[   22] Training loss: 154.19195624, Validation loss: 154.52949997, Gradient norm: 23.03865528
INFO:root:[   23] Training loss: 153.92604119, Validation loss: 154.26290104, Gradient norm: 21.83943953
INFO:root:[   24] Training loss: 153.67104171, Validation loss: 154.08906660, Gradient norm: 23.74736067
INFO:root:[   25] Training loss: 153.42668543, Validation loss: 154.01320254, Gradient norm: 24.93130837
INFO:root:[   26] Training loss: 153.16535518, Validation loss: 153.56136769, Gradient norm: 21.57599610
INFO:root:[   27] Training loss: 152.95811408, Validation loss: 153.46498950, Gradient norm: 22.23639626
INFO:root:[   28] Training loss: 152.72039133, Validation loss: 153.36169486, Gradient norm: 21.02717087
INFO:root:[   29] Training loss: 152.54131337, Validation loss: 153.20028476, Gradient norm: 22.60700652
INFO:root:[   30] Training loss: 152.35956297, Validation loss: 152.91953462, Gradient norm: 24.89382507
INFO:root:[   31] Training loss: 152.11157456, Validation loss: 152.57469125, Gradient norm: 22.24524082
INFO:root:[   32] Training loss: 151.92221137, Validation loss: 152.80905625, Gradient norm: 21.32126815
INFO:root:[   33] Training loss: 151.79862990, Validation loss: 152.57767513, Gradient norm: 25.46939747
INFO:root:[   34] Training loss: 151.63691455, Validation loss: 152.18405941, Gradient norm: 22.15037510
INFO:root:[   35] Training loss: 151.42625063, Validation loss: 152.23981976, Gradient norm: 25.59641998
INFO:root:[   36] Training loss: 151.28019026, Validation loss: 152.05524366, Gradient norm: 25.91702238
INFO:root:[   37] Training loss: 151.14224540, Validation loss: 151.95854766, Gradient norm: 25.02627361
INFO:root:[   38] Training loss: 150.98528270, Validation loss: 151.82960300, Gradient norm: 25.04205765
INFO:root:[   39] Training loss: 150.83002803, Validation loss: 151.57219249, Gradient norm: 25.41702979
INFO:root:[   40] Training loss: 150.67798851, Validation loss: 151.57446079, Gradient norm: 25.06797655
INFO:root:[   41] Training loss: 150.57364054, Validation loss: 151.29947478, Gradient norm: 27.17487321
INFO:root:[   42] Training loss: 150.45585686, Validation loss: 151.56566804, Gradient norm: 23.93074103
INFO:root:[   43] Training loss: 150.28905926, Validation loss: 151.10185452, Gradient norm: 27.43217718
INFO:root:[   44] Training loss: 150.17166718, Validation loss: 151.13402373, Gradient norm: 25.18746492
INFO:root:[   45] Training loss: 150.00977386, Validation loss: 150.86332071, Gradient norm: 27.36285620
INFO:root:[   46] Training loss: 149.93817611, Validation loss: 150.81904129, Gradient norm: 27.02718924
INFO:root:[   47] Training loss: 149.78076888, Validation loss: 150.67185448, Gradient norm: 28.36201814
INFO:root:[   48] Training loss: 149.67325518, Validation loss: 150.74951435, Gradient norm: 27.89238247
INFO:root:[   49] Training loss: 149.59158204, Validation loss: 150.46746774, Gradient norm: 29.86398947
INFO:root:[   50] Training loss: 149.44376704, Validation loss: 150.51027495, Gradient norm: 31.02633390
INFO:root:[   51] Training loss: 149.39080351, Validation loss: 150.52033470, Gradient norm: 29.00272447
INFO:root:[   52] Training loss: 149.27290385, Validation loss: 150.40813157, Gradient norm: 31.09216883
INFO:root:[   53] Training loss: 149.13590855, Validation loss: 150.24466363, Gradient norm: 28.89113193
INFO:root:[   54] Training loss: 149.03534489, Validation loss: 150.28230233, Gradient norm: 31.59189748
INFO:root:[   55] Training loss: 148.96113546, Validation loss: 150.26877462, Gradient norm: 31.59217615
INFO:root:[   56] Training loss: 148.91409801, Validation loss: 150.17628900, Gradient norm: 30.47925603
INFO:root:[   57] Training loss: 148.77265296, Validation loss: 150.04351123, Gradient norm: 30.63813069
INFO:root:[   58] Training loss: 148.69741281, Validation loss: 149.88271779, Gradient norm: 30.97318627
INFO:root:[   59] Training loss: 148.61498470, Validation loss: 149.79742221, Gradient norm: 32.39741865
INFO:root:[   60] Training loss: 148.50239144, Validation loss: 149.95136971, Gradient norm: 31.50811835
INFO:root:[   61] Training loss: 148.44743793, Validation loss: 149.79508446, Gradient norm: 33.67148154
INFO:root:[   62] Training loss: 148.31230407, Validation loss: 149.87845901, Gradient norm: 30.25136766
INFO:root:[   63] Training loss: 148.25690440, Validation loss: 149.64169469, Gradient norm: 34.32401072
INFO:root:[   64] Training loss: 148.09850251, Validation loss: 149.40026645, Gradient norm: 32.80110290
INFO:root:[   65] Training loss: 148.04880179, Validation loss: 149.47872662, Gradient norm: 32.71761467
INFO:root:[   66] Training loss: 147.93817193, Validation loss: 149.44056439, Gradient norm: 31.82071682
INFO:root:[   67] Training loss: 148.02906637, Validation loss: 149.72557962, Gradient norm: 42.11275369
INFO:root:[   68] Training loss: 147.86894321, Validation loss: 149.54863397, Gradient norm: 34.87904693
INFO:root:[   69] Training loss: 147.77112208, Validation loss: 149.34010420, Gradient norm: 34.15243900
INFO:root:[   70] Training loss: 147.65376903, Validation loss: 148.99202965, Gradient norm: 37.26416093
INFO:root:[   71] Training loss: 147.62977127, Validation loss: 149.16939518, Gradient norm: 37.06670644
INFO:root:[   72] Training loss: 147.51301953, Validation loss: 149.10847263, Gradient norm: 33.78260746
INFO:root:[   73] Training loss: 147.47725279, Validation loss: 149.14241712, Gradient norm: 41.30771726
INFO:root:[   74] Training loss: 147.40261044, Validation loss: 148.97939064, Gradient norm: 38.33150989
INFO:root:[   75] Training loss: 147.37837678, Validation loss: 148.99183970, Gradient norm: 40.64065846
INFO:root:[   76] Training loss: 147.27613466, Validation loss: 149.00359581, Gradient norm: 36.68539403
INFO:root:[   77] Training loss: 147.22502366, Validation loss: 149.10542876, Gradient norm: 40.35328270
INFO:root:[   78] Training loss: 147.17988465, Validation loss: 148.66082501, Gradient norm: 48.80844943
INFO:root:[   79] Training loss: 147.04371751, Validation loss: 148.74996580, Gradient norm: 40.53129549
INFO:root:[   80] Training loss: 146.95329852, Validation loss: 148.63894022, Gradient norm: 39.16042875
INFO:root:[   81] Training loss: 146.93619679, Validation loss: 148.67194340, Gradient norm: 44.47555470
INFO:root:[   82] Training loss: 146.89994002, Validation loss: 148.77313916, Gradient norm: 43.71620636
INFO:root:[   83] Training loss: 146.78086164, Validation loss: 148.71948032, Gradient norm: 40.17636306
INFO:root:[   84] Training loss: 146.75696881, Validation loss: 148.68297235, Gradient norm: 40.28051109
INFO:root:[   85] Training loss: 146.69081453, Validation loss: 148.43621405, Gradient norm: 39.13466556
INFO:root:[   86] Training loss: 146.60361812, Validation loss: 148.56048531, Gradient norm: 42.07324595
INFO:root:[   87] Training loss: 146.50607718, Validation loss: 148.28746611, Gradient norm: 38.07222469
INFO:root:[   88] Training loss: 146.50413797, Validation loss: 148.38603842, Gradient norm: 51.71487728
INFO:root:[   89] Training loss: 146.43902723, Validation loss: 148.55512001, Gradient norm: 43.29860713
INFO:root:[   90] Training loss: 146.42745067, Validation loss: 148.62398292, Gradient norm: 42.30800033
INFO:root:[   91] Training loss: 146.37709099, Validation loss: 148.30555199, Gradient norm: 46.62360073
INFO:root:[   92] Training loss: 146.30736427, Validation loss: 148.29764531, Gradient norm: 42.90878832
INFO:root:[   93] Training loss: 146.27912525, Validation loss: 148.19420545, Gradient norm: 55.52480487
INFO:root:[   94] Training loss: 146.19309282, Validation loss: 148.19773602, Gradient norm: 47.19079042
INFO:root:[   95] Training loss: 146.12973320, Validation loss: 148.42032860, Gradient norm: 48.14882048
INFO:root:[   96] Training loss: 146.07509431, Validation loss: 148.35459847, Gradient norm: 48.13684196
INFO:root:[   97] Training loss: 146.03327226, Validation loss: 148.35529880, Gradient norm: 42.93827643
INFO:root:[   98] Training loss: 145.91206265, Validation loss: 148.20109453, Gradient norm: 40.64788903
INFO:root:[   99] Training loss: 145.95828733, Validation loss: 148.10007714, Gradient norm: 42.28372356
INFO:root:[  100] Training loss: 145.86697293, Validation loss: 147.82702847, Gradient norm: 50.98695534
INFO:root:[  101] Training loss: 145.78858462, Validation loss: 148.06614896, Gradient norm: 45.87378183
INFO:root:[  102] Training loss: 145.75285771, Validation loss: 147.99665675, Gradient norm: 50.29523064
INFO:root:[  103] Training loss: 145.73501830, Validation loss: 148.01817269, Gradient norm: 52.32001056
INFO:root:[  104] Training loss: 145.68340821, Validation loss: 148.21471800, Gradient norm: 51.84339109
INFO:root:[  105] Training loss: 145.72982140, Validation loss: 147.92418539, Gradient norm: 60.22953349
INFO:root:[  106] Training loss: 145.52577426, Validation loss: 147.87527203, Gradient norm: 46.97301917
INFO:root:[  107] Training loss: 145.56667699, Validation loss: 147.96629965, Gradient norm: 52.84898455
INFO:root:[  108] Training loss: 145.48089073, Validation loss: 148.07073185, Gradient norm: 52.89005359
INFO:root:[  109] Training loss: 145.47235958, Validation loss: 147.58176501, Gradient norm: 52.60334534
INFO:root:[  110] Training loss: 145.38084236, Validation loss: 147.95066939, Gradient norm: 49.70718662
INFO:root:[  111] Training loss: 145.41137047, Validation loss: 147.82256712, Gradient norm: 49.86311006
INFO:root:[  112] Training loss: 145.36153797, Validation loss: 147.79784998, Gradient norm: 57.62321489
INFO:root:[  113] Training loss: 145.23724217, Validation loss: 147.85851682, Gradient norm: 47.75879676
INFO:root:[  114] Training loss: 145.24658635, Validation loss: 147.76273635, Gradient norm: 59.33380034
INFO:root:[  115] Training loss: 145.18923221, Validation loss: 147.71818069, Gradient norm: 43.48664711
INFO:root:[  116] Training loss: 145.15349518, Validation loss: 147.76923555, Gradient norm: 46.44419243
INFO:root:[  117] Training loss: 145.08262337, Validation loss: 147.59691067, Gradient norm: 51.60366945
INFO:root:[  118] Training loss: 145.01771059, Validation loss: 147.46348782, Gradient norm: 57.79880315
INFO:root:[  119] Training loss: 145.00024225, Validation loss: 147.50287865, Gradient norm: 55.19028798
INFO:root:[  120] Training loss: 144.94932435, Validation loss: 147.77845080, Gradient norm: 45.00006189
INFO:root:[  121] Training loss: 144.94074378, Validation loss: 147.85688466, Gradient norm: 58.14854573
INFO:root:[  122] Training loss: 144.89027648, Validation loss: 147.90933385, Gradient norm: 49.93266515
INFO:root:[  123] Training loss: 144.86346733, Validation loss: 147.47394956, Gradient norm: 59.27986589
INFO:root:[  124] Training loss: 144.80261906, Validation loss: 147.66935677, Gradient norm: 53.93516529
INFO:root:[  125] Training loss: 144.82337168, Validation loss: 147.52471187, Gradient norm: 56.45208202
INFO:root:[  126] Training loss: 144.76447141, Validation loss: 147.51756287, Gradient norm: 52.65546176
INFO:root:[  127] Training loss: 144.69618023, Validation loss: 147.62132316, Gradient norm: 60.16077930
INFO:root:EP 127: Early stopping
INFO:root:Training the model took 644.344s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 9019.53205
INFO:root:EnergyScoreTrain: 7852.77483
INFO:root:CoverageTrain: 0.36358
INFO:root:IntervalWidthTrain: 1.56609
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 8967.18936
INFO:root:EnergyScoreValidation: 7829.66764
INFO:root:CoverageValidation: 0.35302
INFO:root:IntervalWidthValidation: 1.55968
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9036.9288
INFO:root:EnergyScoreTest: 7891.30151
INFO:root:CoverageTest: 0.35245
INFO:root:IntervalWidthTest: 1.55903
INFO:root:###11 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.55605525, Validation loss: 171.39926674, Gradient norm: 2.91608341
INFO:root:[    2] Training loss: 171.43634843, Validation loss: 171.03887782, Gradient norm: 1.92352686
INFO:root:[    3] Training loss: 170.67343653, Validation loss: 170.30964608, Gradient norm: 7.47851442
INFO:root:[    4] Training loss: 169.78062358, Validation loss: 168.69331676, Gradient norm: 6.12784550
INFO:root:[    5] Training loss: 167.62080329, Validation loss: 166.46008932, Gradient norm: 10.92030748
INFO:root:[    6] Training loss: 165.56826296, Validation loss: 164.67721400, Gradient norm: 11.38794424
INFO:root:[    7] Training loss: 163.86915642, Validation loss: 163.23858222, Gradient norm: 12.71482616
INFO:root:[    8] Training loss: 162.45126370, Validation loss: 161.86282191, Gradient norm: 14.15718886
INFO:root:[    9] Training loss: 161.43771011, Validation loss: 161.00827184, Gradient norm: 14.73170831
INFO:root:[   10] Training loss: 160.55746825, Validation loss: 160.44746136, Gradient norm: 14.21988397
INFO:root:[   11] Training loss: 159.78077725, Validation loss: 159.52741741, Gradient norm: 15.54053310
INFO:root:[   12] Training loss: 159.14642699, Validation loss: 159.16550735, Gradient norm: 14.55917391
INFO:root:[   13] Training loss: 158.59034770, Validation loss: 158.52573947, Gradient norm: 16.43779535
INFO:root:[   14] Training loss: 158.08637163, Validation loss: 157.97301667, Gradient norm: 14.30295072
INFO:root:[   15] Training loss: 157.60759769, Validation loss: 157.50718531, Gradient norm: 18.01833961
INFO:root:[   16] Training loss: 157.19861110, Validation loss: 157.26650948, Gradient norm: 19.38467245
INFO:root:[   17] Training loss: 156.76939149, Validation loss: 156.90809842, Gradient norm: 15.31239619
INFO:root:[   18] Training loss: 156.42622051, Validation loss: 156.47615051, Gradient norm: 17.56861478
INFO:root:[   19] Training loss: 156.10211735, Validation loss: 156.34113075, Gradient norm: 16.87580772
INFO:root:[   20] Training loss: 155.78686334, Validation loss: 155.86995671, Gradient norm: 18.15644105
INFO:root:[   21] Training loss: 155.48431437, Validation loss: 155.73591614, Gradient norm: 18.48185160
INFO:root:[   22] Training loss: 155.22870143, Validation loss: 155.43599096, Gradient norm: 18.45076133
INFO:root:[   23] Training loss: 154.95570495, Validation loss: 155.25358161, Gradient norm: 18.59637810
INFO:root:[   24] Training loss: 154.67751346, Validation loss: 154.87376404, Gradient norm: 18.27653763
INFO:root:[   25] Training loss: 154.46499715, Validation loss: 154.76406124, Gradient norm: 21.23127648
INFO:root:[   26] Training loss: 154.21116773, Validation loss: 154.54515970, Gradient norm: 19.30309172
INFO:root:[   27] Training loss: 153.99551446, Validation loss: 154.15971217, Gradient norm: 19.70375815
INFO:root:[   28] Training loss: 153.83842819, Validation loss: 154.05502846, Gradient norm: 19.37595097
INFO:root:[   29] Training loss: 153.61335349, Validation loss: 154.12333416, Gradient norm: 18.13740422
INFO:root:[   30] Training loss: 153.44266287, Validation loss: 153.74573175, Gradient norm: 20.00024797
INFO:root:[   31] Training loss: 153.20520681, Validation loss: 153.47585533, Gradient norm: 18.90110511
INFO:root:[   32] Training loss: 153.01773233, Validation loss: 153.35476527, Gradient norm: 20.83616609
INFO:root:[   33] Training loss: 152.90581452, Validation loss: 153.18046202, Gradient norm: 21.15259187
INFO:root:[   34] Training loss: 152.71573025, Validation loss: 153.14755670, Gradient norm: 20.36343638
INFO:root:[   35] Training loss: 152.53200929, Validation loss: 153.16416931, Gradient norm: 22.27645693
INFO:root:[   36] Training loss: 152.40288604, Validation loss: 152.78698783, Gradient norm: 20.89473961
INFO:root:[   37] Training loss: 152.23233559, Validation loss: 152.72660670, Gradient norm: 20.44098887
INFO:root:[   38] Training loss: 152.10942469, Validation loss: 152.49354185, Gradient norm: 21.75976503
INFO:root:[   39] Training loss: 151.98812974, Validation loss: 152.40758804, Gradient norm: 23.30397595
INFO:root:[   40] Training loss: 151.82317238, Validation loss: 152.27656029, Gradient norm: 22.05089874
INFO:root:[   41] Training loss: 151.73421823, Validation loss: 152.24002970, Gradient norm: 24.16524190
INFO:root:[   42] Training loss: 151.54482816, Validation loss: 152.16111545, Gradient norm: 22.62321282
INFO:root:[   43] Training loss: 151.43919724, Validation loss: 152.02114026, Gradient norm: 23.47852122
INFO:root:[   44] Training loss: 151.37309238, Validation loss: 151.74946594, Gradient norm: 24.57352741
INFO:root:[   45] Training loss: 151.18672396, Validation loss: 151.64594242, Gradient norm: 22.91465276
INFO:root:[   46] Training loss: 151.08013389, Validation loss: 151.85771968, Gradient norm: 22.34934753
INFO:root:[   47] Training loss: 150.95151256, Validation loss: 151.63779055, Gradient norm: 25.68757448
INFO:root:[   48] Training loss: 150.83168394, Validation loss: 151.45593630, Gradient norm: 21.52770947
INFO:root:[   49] Training loss: 150.81878541, Validation loss: 151.63691290, Gradient norm: 25.79427641
INFO:root:[   50] Training loss: 150.64497430, Validation loss: 151.28029396, Gradient norm: 25.18741344
INFO:root:[   51] Training loss: 150.56871411, Validation loss: 151.17178450, Gradient norm: 27.82584205
INFO:root:[   52] Training loss: 150.50565277, Validation loss: 151.31043375, Gradient norm: 29.50608803
INFO:root:[   53] Training loss: 150.37299840, Validation loss: 151.08578912, Gradient norm: 27.06736282
INFO:root:[   54] Training loss: 150.28547668, Validation loss: 151.32243821, Gradient norm: 27.30747532
INFO:root:[   55] Training loss: 150.17335605, Validation loss: 150.80264914, Gradient norm: 30.47296319
INFO:root:[   56] Training loss: 150.11094774, Validation loss: 150.81904602, Gradient norm: 29.69888180
INFO:root:[   57] Training loss: 149.98430789, Validation loss: 150.82307855, Gradient norm: 27.88707134
INFO:root:[   58] Training loss: 149.91509780, Validation loss: 150.75085870, Gradient norm: 31.51617840
INFO:root:[   59] Training loss: 149.83757127, Validation loss: 150.54700181, Gradient norm: 36.92920680
INFO:root:[   60] Training loss: 149.77046946, Validation loss: 150.57220512, Gradient norm: 28.74851880
INFO:root:[   61] Training loss: 149.72043279, Validation loss: 150.60820270, Gradient norm: 30.39527411
INFO:root:[   62] Training loss: 149.61224271, Validation loss: 150.59713903, Gradient norm: 32.63985372
INFO:root:[   63] Training loss: 149.57905984, Validation loss: 150.84951993, Gradient norm: 36.70748053
INFO:root:[   64] Training loss: 149.50404790, Validation loss: 150.22699448, Gradient norm: 35.81908278
INFO:root:[   65] Training loss: 149.42693889, Validation loss: 150.12994964, Gradient norm: 31.86105291
INFO:root:[   66] Training loss: 149.31558592, Validation loss: 150.09279027, Gradient norm: 31.61520021
INFO:root:[   67] Training loss: 149.18866723, Validation loss: 150.15932964, Gradient norm: 31.05276812
INFO:root:[   68] Training loss: 149.16280844, Validation loss: 150.16494172, Gradient norm: 35.67399153
INFO:root:[   69] Training loss: 149.12148521, Validation loss: 149.89657908, Gradient norm: 33.50525973
INFO:root:[   70] Training loss: 149.10400998, Validation loss: 149.80271649, Gradient norm: 38.72852968
INFO:root:[   71] Training loss: 148.96550933, Validation loss: 149.95441516, Gradient norm: 35.18319486
INFO:root:[   72] Training loss: 148.86660794, Validation loss: 149.82935149, Gradient norm: 36.23356039
INFO:root:[   73] Training loss: 148.82972312, Validation loss: 149.75956200, Gradient norm: 33.65420163
INFO:root:[   74] Training loss: 148.82372723, Validation loss: 149.77334805, Gradient norm: 40.08504936
INFO:root:[   75] Training loss: 148.73614326, Validation loss: 149.74944279, Gradient norm: 43.70316840
INFO:root:[   76] Training loss: 148.65731460, Validation loss: 149.74970061, Gradient norm: 35.92592672
INFO:root:[   77] Training loss: 148.63599686, Validation loss: 149.56154291, Gradient norm: 35.97584916
INFO:root:[   78] Training loss: 148.55401584, Validation loss: 149.74978164, Gradient norm: 38.49342948
INFO:root:[   79] Training loss: 148.51670257, Validation loss: 149.45412524, Gradient norm: 34.77102835
INFO:root:[   80] Training loss: 148.44095065, Validation loss: 149.43025313, Gradient norm: 42.24761551
INFO:root:[   81] Training loss: 148.41436579, Validation loss: 149.43712327, Gradient norm: 35.64930206
INFO:root:[   82] Training loss: 148.26993054, Validation loss: 149.27183796, Gradient norm: 36.23260990
INFO:root:[   83] Training loss: 148.27925501, Validation loss: 149.42367238, Gradient norm: 40.39885355
INFO:root:[   84] Training loss: 148.28812537, Validation loss: 149.41101706, Gradient norm: 46.20351923
INFO:root:[   85] Training loss: 148.18719334, Validation loss: 149.13738330, Gradient norm: 41.64972216
INFO:root:[   86] Training loss: 148.19125515, Validation loss: 149.34344851, Gradient norm: 44.04034445
INFO:root:[   87] Training loss: 148.07630677, Validation loss: 149.30130110, Gradient norm: 39.83626511
INFO:root:[   88] Training loss: 147.99988185, Validation loss: 149.23315324, Gradient norm: 40.82289887
INFO:root:[   89] Training loss: 147.96590984, Validation loss: 149.20879022, Gradient norm: 45.76857573
INFO:root:[   90] Training loss: 147.93569474, Validation loss: 149.01958545, Gradient norm: 43.18704338
INFO:root:[   91] Training loss: 147.86193713, Validation loss: 148.93747737, Gradient norm: 41.80817998
INFO:root:[   92] Training loss: 147.80579477, Validation loss: 149.15658885, Gradient norm: 47.59410829
INFO:root:[   93] Training loss: 147.77899386, Validation loss: 148.86834243, Gradient norm: 42.79738826
INFO:root:[   94] Training loss: 147.77665859, Validation loss: 149.00847547, Gradient norm: 51.06194675
INFO:root:[   95] Training loss: 147.70726553, Validation loss: 148.71980654, Gradient norm: 45.52953154
INFO:root:[   96] Training loss: 147.70851905, Validation loss: 148.70417207, Gradient norm: 40.91892278
INFO:root:[   97] Training loss: 147.66183526, Validation loss: 148.93482919, Gradient norm: 51.20825050
INFO:root:[   98] Training loss: 147.56019120, Validation loss: 148.87514522, Gradient norm: 42.29447453
INFO:root:[   99] Training loss: 147.48826518, Validation loss: 148.66765358, Gradient norm: 46.67535378
INFO:root:[  100] Training loss: 147.45675943, Validation loss: 148.98875164, Gradient norm: 49.34564925
INFO:root:[  101] Training loss: 147.38670039, Validation loss: 148.59341115, Gradient norm: 37.04553743
INFO:root:[  102] Training loss: 147.41516869, Validation loss: 148.84456661, Gradient norm: 48.81446823
INFO:root:[  103] Training loss: 147.35348754, Validation loss: 148.42229962, Gradient norm: 48.72426319
INFO:root:[  104] Training loss: 147.32417365, Validation loss: 148.60001084, Gradient norm: 49.89640203
INFO:root:[  105] Training loss: 147.29598553, Validation loss: 148.82101177, Gradient norm: 48.30765163
INFO:root:[  106] Training loss: 147.22023334, Validation loss: 148.79575322, Gradient norm: 42.68430629
INFO:root:[  107] Training loss: 147.19178178, Validation loss: 148.40625579, Gradient norm: 42.91395652
INFO:root:[  108] Training loss: 147.14197196, Validation loss: 148.35875991, Gradient norm: 40.52322278
INFO:root:[  109] Training loss: 147.14116729, Validation loss: 148.72903653, Gradient norm: 46.35758905
INFO:root:[  110] Training loss: 147.08370107, Validation loss: 148.39991234, Gradient norm: 49.64142964
INFO:root:[  111] Training loss: 147.04318966, Validation loss: 148.50011049, Gradient norm: 51.13910022
INFO:root:[  112] Training loss: 146.98866556, Validation loss: 148.41130434, Gradient norm: 48.26547422
INFO:root:[  113] Training loss: 146.96555065, Validation loss: 148.26443113, Gradient norm: 51.15894317
INFO:root:[  114] Training loss: 146.91140409, Validation loss: 148.42798062, Gradient norm: 50.19542544
INFO:root:[  115] Training loss: 146.85817536, Validation loss: 148.16950989, Gradient norm: 48.60263943
INFO:root:[  116] Training loss: 146.87036632, Validation loss: 148.69909247, Gradient norm: 45.80650318
INFO:root:[  117] Training loss: 146.76162774, Validation loss: 148.14281858, Gradient norm: 42.31353273
INFO:root:[  118] Training loss: 146.78506240, Validation loss: 148.33605957, Gradient norm: 50.52710871
INFO:root:[  119] Training loss: 146.73659536, Validation loss: 148.45416207, Gradient norm: 46.30219957
INFO:root:[  120] Training loss: 146.68846738, Validation loss: 147.99655099, Gradient norm: 46.12618794
INFO:root:[  121] Training loss: 146.73280186, Validation loss: 148.01854101, Gradient norm: 53.09129060
INFO:root:[  122] Training loss: 146.66653199, Validation loss: 147.96170623, Gradient norm: 54.00764562
INFO:root:[  123] Training loss: 146.65614238, Validation loss: 148.29469615, Gradient norm: 55.45961273
INFO:root:[  124] Training loss: 146.61957800, Validation loss: 148.19741085, Gradient norm: 50.77699831
INFO:root:[  125] Training loss: 146.53931582, Validation loss: 147.77788412, Gradient norm: 52.69689495
INFO:root:[  126] Training loss: 146.54273744, Validation loss: 148.18696542, Gradient norm: 55.17043415
INFO:root:[  127] Training loss: 146.43979489, Validation loss: 148.04604892, Gradient norm: 46.45788764
INFO:root:[  128] Training loss: 146.45080215, Validation loss: 147.89826965, Gradient norm: 45.97157558
INFO:root:[  129] Training loss: 146.44049572, Validation loss: 148.03002719, Gradient norm: 54.28586239
INFO:root:[  130] Training loss: 146.40918576, Validation loss: 147.89688847, Gradient norm: 51.95032660
INFO:root:[  131] Training loss: 146.34830279, Validation loss: 147.84274239, Gradient norm: 53.27734528
INFO:root:[  132] Training loss: 146.35351076, Validation loss: 147.90612319, Gradient norm: 54.12810945
INFO:root:[  133] Training loss: 146.25469133, Validation loss: 147.73539787, Gradient norm: 46.30815328
INFO:root:[  134] Training loss: 146.18354892, Validation loss: 147.63045054, Gradient norm: 55.89675808
INFO:root:[  135] Training loss: 146.20221150, Validation loss: 147.72638992, Gradient norm: 52.07612131
INFO:root:[  136] Training loss: 146.19594675, Validation loss: 147.89690478, Gradient norm: 57.00179994
INFO:root:[  137] Training loss: 146.21736793, Validation loss: 147.77122392, Gradient norm: 50.97773317
INFO:root:[  138] Training loss: 146.12622840, Validation loss: 147.68996193, Gradient norm: 52.00019077
INFO:root:[  139] Training loss: 146.09796669, Validation loss: 148.18368057, Gradient norm: 61.49750526
INFO:root:[  140] Training loss: 146.06253092, Validation loss: 147.75431034, Gradient norm: 52.19479813
INFO:root:[  141] Training loss: 146.03397687, Validation loss: 147.64409664, Gradient norm: 52.63620552
INFO:root:[  142] Training loss: 145.97007886, Validation loss: 147.69842582, Gradient norm: 48.92239484
INFO:root:[  143] Training loss: 145.97553300, Validation loss: 147.44777127, Gradient norm: 58.19182077
INFO:root:[  144] Training loss: 145.97982396, Validation loss: 147.55664010, Gradient norm: 63.04249827
INFO:root:[  145] Training loss: 145.96903519, Validation loss: 147.51421277, Gradient norm: 52.06685307
INFO:root:[  146] Training loss: 145.86785619, Validation loss: 147.55050343, Gradient norm: 49.68453522
INFO:root:[  147] Training loss: 145.85770990, Validation loss: 147.95816040, Gradient norm: 58.05382581
INFO:root:[  148] Training loss: 145.80065283, Validation loss: 147.63585479, Gradient norm: 47.20883946
INFO:root:[  149] Training loss: 145.82026443, Validation loss: 147.99004811, Gradient norm: 53.68331425
INFO:root:[  150] Training loss: 145.78303447, Validation loss: 147.59106393, Gradient norm: 55.80181150
INFO:root:[  151] Training loss: 145.71016997, Validation loss: 147.45312658, Gradient norm: 52.20850457
INFO:root:[  152] Training loss: 145.72841367, Validation loss: 147.39596715, Gradient norm: 58.82511692
INFO:root:[  153] Training loss: 145.64264861, Validation loss: 147.27234466, Gradient norm: 52.59800355
INFO:root:[  154] Training loss: 145.66443668, Validation loss: 147.70170541, Gradient norm: 59.86778309
INFO:root:[  155] Training loss: 145.62083219, Validation loss: 147.45797045, Gradient norm: 56.94092983
INFO:root:[  156] Training loss: 145.60109528, Validation loss: 147.49745547, Gradient norm: 57.86236362
INFO:root:[  157] Training loss: 145.59755957, Validation loss: 147.14773402, Gradient norm: 54.85676114
INFO:root:[  158] Training loss: 145.57401566, Validation loss: 147.55388247, Gradient norm: 62.99099112
INFO:root:[  159] Training loss: 145.54057042, Validation loss: 147.28559349, Gradient norm: 53.41369956
INFO:root:[  160] Training loss: 145.44492468, Validation loss: 147.28533304, Gradient norm: 47.60562941
INFO:root:[  161] Training loss: 145.48765753, Validation loss: 147.22487667, Gradient norm: 61.21888065
INFO:root:[  162] Training loss: 145.39223730, Validation loss: 147.24266421, Gradient norm: 49.39383934
INFO:root:[  163] Training loss: 145.43039899, Validation loss: 147.38020588, Gradient norm: 57.79788822
INFO:root:[  164] Training loss: 145.34486160, Validation loss: 147.38514341, Gradient norm: 52.09952315
INFO:root:[  165] Training loss: 145.35957255, Validation loss: 147.27695544, Gradient norm: 53.11747747
INFO:root:[  166] Training loss: 145.33720276, Validation loss: 147.27185953, Gradient norm: 58.87356827
INFO:root:[  167] Training loss: 145.33241853, Validation loss: 147.07337110, Gradient norm: 62.08479556
INFO:root:[  168] Training loss: 145.32239067, Validation loss: 147.14036350, Gradient norm: 50.73317269
INFO:root:[  169] Training loss: 145.27478770, Validation loss: 147.09416830, Gradient norm: 54.97653997
INFO:root:[  170] Training loss: 145.19269042, Validation loss: 147.43906113, Gradient norm: 60.81969883
INFO:root:[  171] Training loss: 145.20121184, Validation loss: 147.21095276, Gradient norm: 50.92097230
INFO:root:[  172] Training loss: 145.21611860, Validation loss: 147.12926457, Gradient norm: 61.04552916
INFO:root:[  173] Training loss: 145.25075956, Validation loss: 147.28830009, Gradient norm: 61.53418132
INFO:root:[  174] Training loss: 145.14272477, Validation loss: 147.16239877, Gradient norm: 55.21918425
INFO:root:[  175] Training loss: 145.09845578, Validation loss: 146.95870656, Gradient norm: 51.44351631
INFO:root:[  176] Training loss: 145.12141621, Validation loss: 146.97768218, Gradient norm: 62.87205985
INFO:root:[  177] Training loss: 145.05602703, Validation loss: 146.98796082, Gradient norm: 60.72950660
INFO:root:[  178] Training loss: 145.07215881, Validation loss: 147.09079927, Gradient norm: 55.82287419
INFO:root:[  179] Training loss: 145.04120508, Validation loss: 147.22504241, Gradient norm: 50.56181589
INFO:root:[  180] Training loss: 145.01399677, Validation loss: 147.18801301, Gradient norm: 49.98925105
INFO:root:[  181] Training loss: 144.96056454, Validation loss: 146.99027647, Gradient norm: 55.87059567
INFO:root:[  182] Training loss: 145.00328077, Validation loss: 147.05359413, Gradient norm: 60.12397011
INFO:root:[  183] Training loss: 144.94781872, Validation loss: 147.13154970, Gradient norm: 57.79428350
INFO:root:[  184] Training loss: 144.86459567, Validation loss: 147.00050828, Gradient norm: 55.39170285
INFO:root:EP 184: Early stopping
INFO:root:Training the model took 931.184s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 8972.74129
INFO:root:EnergyScoreTrain: 7691.27422
INFO:root:CoverageTrain: 0.39398
INFO:root:IntervalWidthTrain: 1.73571
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 8879.34622
INFO:root:EnergyScoreValidation: 7628.81966
INFO:root:CoverageValidation: 0.38597
INFO:root:IntervalWidthValidation: 1.73141
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 8957.68642
INFO:root:EnergyScoreTest: 7697.54477
INFO:root:CoverageTest: 0.38462
INFO:root:IntervalWidthTest: 1.73309
INFO:root:###12 out of 12 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0005, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.61737925, Validation loss: 171.38098145, Gradient norm: 4.12247832
INFO:root:[    2] Training loss: 171.48089222, Validation loss: 171.39140793, Gradient norm: 0.67725360
INFO:root:[    3] Training loss: 171.45642387, Validation loss: 171.48641021, Gradient norm: 2.22129707
INFO:root:[    4] Training loss: 170.94821815, Validation loss: 170.55290801, Gradient norm: 8.29777368
INFO:root:[    5] Training loss: 170.24215698, Validation loss: 169.80721993, Gradient norm: 5.23438135
INFO:root:[    6] Training loss: 169.25326214, Validation loss: 168.51848208, Gradient norm: 6.88523781
INFO:root:[    7] Training loss: 168.00040240, Validation loss: 167.44236440, Gradient norm: 9.96237785
INFO:root:[    8] Training loss: 166.69422562, Validation loss: 166.09475445, Gradient norm: 8.98397692
INFO:root:[    9] Training loss: 165.36172391, Validation loss: 165.00422932, Gradient norm: 9.54349631
INFO:root:[   10] Training loss: 164.41089191, Validation loss: 164.06977002, Gradient norm: 10.07049488
INFO:root:[   11] Training loss: 163.62857393, Validation loss: 163.26158458, Gradient norm: 9.67768408
INFO:root:[   12] Training loss: 162.99303281, Validation loss: 162.71652800, Gradient norm: 10.56533095
INFO:root:[   13] Training loss: 162.44259698, Validation loss: 162.23197148, Gradient norm: 10.15216330
INFO:root:[   14] Training loss: 161.95327489, Validation loss: 161.71318528, Gradient norm: 10.91990563
INFO:root:[   15] Training loss: 161.46126833, Validation loss: 161.36510441, Gradient norm: 11.46683543
INFO:root:[   16] Training loss: 161.05001493, Validation loss: 160.77931582, Gradient norm: 12.21030056
INFO:root:[   17] Training loss: 160.62051378, Validation loss: 160.47804313, Gradient norm: 11.88083017
INFO:root:[   18] Training loss: 160.23447465, Validation loss: 160.16979665, Gradient norm: 12.93233527
INFO:root:[   19] Training loss: 159.88332225, Validation loss: 159.91043933, Gradient norm: 12.37437213
INFO:root:[   20] Training loss: 159.57118252, Validation loss: 159.69939659, Gradient norm: 13.19265426
INFO:root:[   21] Training loss: 159.29255312, Validation loss: 159.07145691, Gradient norm: 14.81336347
INFO:root:[   22] Training loss: 158.98474148, Validation loss: 158.91727790, Gradient norm: 13.38347235
INFO:root:[   23] Training loss: 158.71527572, Validation loss: 158.61832244, Gradient norm: 12.83910331
INFO:root:[   24] Training loss: 158.47043711, Validation loss: 158.39358784, Gradient norm: 13.72341363
INFO:root:[   25] Training loss: 158.23739408, Validation loss: 158.38018588, Gradient norm: 15.66525903
INFO:root:[   26] Training loss: 157.98314295, Validation loss: 158.11524753, Gradient norm: 13.77638402
INFO:root:[   27] Training loss: 157.78574891, Validation loss: 157.84428195, Gradient norm: 15.99279080
INFO:root:[   28] Training loss: 157.60854454, Validation loss: 157.79496292, Gradient norm: 14.88436370
INFO:root:[   29] Training loss: 157.42106439, Validation loss: 157.45644037, Gradient norm: 16.45863931
INFO:root:[   30] Training loss: 157.24988239, Validation loss: 157.26916346, Gradient norm: 15.00838062
INFO:root:[   31] Training loss: 157.07290285, Validation loss: 157.18384210, Gradient norm: 15.53785809
INFO:root:[   32] Training loss: 156.87301271, Validation loss: 156.91811397, Gradient norm: 14.51706796
INFO:root:[   33] Training loss: 156.74416156, Validation loss: 156.82817920, Gradient norm: 16.40940663
INFO:root:[   34] Training loss: 156.59410001, Validation loss: 156.55276963, Gradient norm: 15.83315680
INFO:root:[   35] Training loss: 156.42695253, Validation loss: 156.48552099, Gradient norm: 16.99433398
INFO:root:[   36] Training loss: 156.28954241, Validation loss: 156.43973410, Gradient norm: 15.71692484
INFO:root:[   37] Training loss: 156.16789637, Validation loss: 156.34005474, Gradient norm: 16.18697832
INFO:root:[   38] Training loss: 156.00136667, Validation loss: 156.30585190, Gradient norm: 16.21731185
INFO:root:[   39] Training loss: 155.89270060, Validation loss: 155.97254523, Gradient norm: 18.00748542
INFO:root:[   40] Training loss: 155.74509045, Validation loss: 155.98398301, Gradient norm: 16.28731891
INFO:root:[   41] Training loss: 155.64581461, Validation loss: 155.93383105, Gradient norm: 19.34482729
INFO:root:[   42] Training loss: 155.50376055, Validation loss: 155.66614664, Gradient norm: 17.21021029
INFO:root:[   43] Training loss: 155.43433427, Validation loss: 155.61282559, Gradient norm: 18.10958292
INFO:root:[   44] Training loss: 155.28255550, Validation loss: 155.46090435, Gradient norm: 17.68908003
INFO:root:[   45] Training loss: 155.16487945, Validation loss: 155.45821670, Gradient norm: 18.42405748
INFO:root:[   46] Training loss: 155.07018881, Validation loss: 155.48882688, Gradient norm: 17.85027399
INFO:root:[   47] Training loss: 154.97939402, Validation loss: 155.18697594, Gradient norm: 18.14533970
INFO:root:[   48] Training loss: 154.92757287, Validation loss: 155.25705587, Gradient norm: 18.51772432
INFO:root:[   49] Training loss: 154.79606061, Validation loss: 155.01417699, Gradient norm: 19.29310613
INFO:root:[   50] Training loss: 154.68788309, Validation loss: 154.82276338, Gradient norm: 19.29340200
INFO:root:[   51] Training loss: 154.60340179, Validation loss: 154.86983779, Gradient norm: 18.08276818
INFO:root:[   52] Training loss: 154.54409682, Validation loss: 154.84112023, Gradient norm: 19.16577842
INFO:root:[   53] Training loss: 154.42626399, Validation loss: 154.76321148, Gradient norm: 20.82676407
INFO:root:[   54] Training loss: 154.36142548, Validation loss: 154.82957616, Gradient norm: 18.79279467
INFO:root:[   55] Training loss: 154.26820590, Validation loss: 154.61914378, Gradient norm: 19.73944648
INFO:root:[   56] Training loss: 154.24467320, Validation loss: 154.49028752, Gradient norm: 22.19481442
INFO:root:[   57] Training loss: 154.12118868, Validation loss: 154.55882105, Gradient norm: 21.12489092
INFO:root:[   58] Training loss: 154.06080033, Validation loss: 154.41928995, Gradient norm: 21.77480518
INFO:root:[   59] Training loss: 153.94849862, Validation loss: 154.24749124, Gradient norm: 21.61968299
INFO:root:[   60] Training loss: 153.89188270, Validation loss: 154.29834458, Gradient norm: 21.46605668
INFO:root:[   61] Training loss: 153.81590690, Validation loss: 154.20059362, Gradient norm: 21.36007024
INFO:root:[   62] Training loss: 153.71694015, Validation loss: 154.09798405, Gradient norm: 20.96174794
INFO:root:[   63] Training loss: 153.65917982, Validation loss: 153.95959894, Gradient norm: 22.20271207
INFO:root:[   64] Training loss: 153.60874385, Validation loss: 154.04770003, Gradient norm: 23.42047914
INFO:root:[   65] Training loss: 153.56256765, Validation loss: 153.87649536, Gradient norm: 23.93175736
INFO:root:[   66] Training loss: 153.52774764, Validation loss: 153.82150479, Gradient norm: 24.92128351
INFO:root:[   67] Training loss: 153.40348829, Validation loss: 153.77211262, Gradient norm: 22.98381560
INFO:root:[   68] Training loss: 153.34511330, Validation loss: 153.81464781, Gradient norm: 22.19963738
INFO:root:[   69] Training loss: 153.27703763, Validation loss: 153.79526073, Gradient norm: 21.60201292
INFO:root:[   70] Training loss: 153.23058164, Validation loss: 153.59488757, Gradient norm: 25.17542938
INFO:root:[   71] Training loss: 153.15009099, Validation loss: 153.60965597, Gradient norm: 28.37750211
INFO:root:[   72] Training loss: 153.07577636, Validation loss: 153.56132823, Gradient norm: 22.02444048
INFO:root:[   73] Training loss: 153.04615973, Validation loss: 153.53136523, Gradient norm: 25.57285051
INFO:root:[   74] Training loss: 153.02214955, Validation loss: 153.55634176, Gradient norm: 31.71149045
INFO:root:[   75] Training loss: 152.96589242, Validation loss: 153.21839221, Gradient norm: 25.59428436
INFO:root:[   76] Training loss: 152.91262358, Validation loss: 153.29384639, Gradient norm: 26.15114352
INFO:root:[   77] Training loss: 152.86013416, Validation loss: 153.33607220, Gradient norm: 26.99807469
INFO:root:[   78] Training loss: 152.81129321, Validation loss: 153.27055412, Gradient norm: 24.91665096
INFO:root:[   79] Training loss: 152.68932187, Validation loss: 153.06878715, Gradient norm: 25.37875047
INFO:root:[   80] Training loss: 152.69474495, Validation loss: 153.07288966, Gradient norm: 29.33127068
INFO:root:[   81] Training loss: 152.65509465, Validation loss: 153.02652925, Gradient norm: 30.44324092
INFO:root:[   82] Training loss: 152.61256652, Validation loss: 152.93247249, Gradient norm: 30.76290108
INFO:root:[   83] Training loss: 152.56315194, Validation loss: 152.91562047, Gradient norm: 31.17699762
INFO:root:[   84] Training loss: 152.48074692, Validation loss: 152.96447701, Gradient norm: 31.05101571
INFO:root:[   85] Training loss: 152.39441823, Validation loss: 152.89087598, Gradient norm: 27.58380702
INFO:root:[   86] Training loss: 152.37072119, Validation loss: 152.86702123, Gradient norm: 30.25497151
INFO:root:[   87] Training loss: 152.34162579, Validation loss: 152.88259940, Gradient norm: 30.92485751
INFO:root:[   88] Training loss: 152.32779930, Validation loss: 152.86192059, Gradient norm: 29.78498110
INFO:root:[   89] Training loss: 152.29102994, Validation loss: 152.77822508, Gradient norm: 32.63116288
INFO:root:[   90] Training loss: 152.20553184, Validation loss: 152.76284685, Gradient norm: 32.49405427
INFO:root:[   91] Training loss: 152.18514083, Validation loss: 152.67867674, Gradient norm: 28.90858893
INFO:root:[   92] Training loss: 152.10453054, Validation loss: 152.77050992, Gradient norm: 36.26789803
INFO:root:[   93] Training loss: 152.10905335, Validation loss: 152.57278495, Gradient norm: 33.27531252
INFO:root:[   94] Training loss: 152.03414809, Validation loss: 152.50337167, Gradient norm: 34.35053026
INFO:root:[   95] Training loss: 152.00704267, Validation loss: 152.86680445, Gradient norm: 31.09524402
INFO:root:[   96] Training loss: 151.96040830, Validation loss: 152.39670589, Gradient norm: 33.31217320
INFO:root:[   97] Training loss: 151.93539361, Validation loss: 152.65825101, Gradient norm: 37.41203906
INFO:root:[   98] Training loss: 151.88408587, Validation loss: 152.38113140, Gradient norm: 36.10232452
INFO:root:[   99] Training loss: 151.81372529, Validation loss: 152.27551690, Gradient norm: 33.83089298
INFO:root:[  100] Training loss: 151.78882430, Validation loss: 152.37143155, Gradient norm: 34.84881448
INFO:root:[  101] Training loss: 151.73009741, Validation loss: 152.29039001, Gradient norm: 32.37098832
INFO:root:[  102] Training loss: 151.76419351, Validation loss: 152.28872733, Gradient norm: 37.48545183
INFO:root:[  103] Training loss: 151.67698845, Validation loss: 152.09227516, Gradient norm: 34.24401581
INFO:root:[  104] Training loss: 151.62557443, Validation loss: 152.20488344, Gradient norm: 38.77612679
INFO:root:[  105] Training loss: 151.62151600, Validation loss: 152.25096025, Gradient norm: 32.37702964
INFO:root:[  106] Training loss: 151.58358333, Validation loss: 152.33105206, Gradient norm: 38.94370143
INFO:root:[  107] Training loss: 151.52385408, Validation loss: 152.25606090, Gradient norm: 36.83073786
INFO:root:[  108] Training loss: 151.53841744, Validation loss: 152.19163092, Gradient norm: 41.33656013
INFO:root:[  109] Training loss: 151.48099444, Validation loss: 152.30839486, Gradient norm: 37.71342546
INFO:root:[  110] Training loss: 151.41124854, Validation loss: 151.97717127, Gradient norm: 34.99777245
INFO:root:[  111] Training loss: 151.37969363, Validation loss: 152.05881684, Gradient norm: 37.34709094
INFO:root:[  112] Training loss: 151.40043438, Validation loss: 152.28707096, Gradient norm: 35.52251288
INFO:root:[  113] Training loss: 151.37372312, Validation loss: 151.99494250, Gradient norm: 41.79106775
INFO:root:[  114] Training loss: 151.29521274, Validation loss: 151.98255184, Gradient norm: 37.82634325
INFO:root:[  115] Training loss: 151.26323477, Validation loss: 151.84811401, Gradient norm: 36.58647692
INFO:root:[  116] Training loss: 151.31167170, Validation loss: 151.82301278, Gradient norm: 41.42868765
INFO:root:[  117] Training loss: 151.15871814, Validation loss: 151.98776350, Gradient norm: 37.85633946
INFO:root:[  118] Training loss: 151.19403495, Validation loss: 151.87148732, Gradient norm: 37.40851099
INFO:root:[  119] Training loss: 151.14640430, Validation loss: 151.84670600, Gradient norm: 39.24818372
INFO:root:[  120] Training loss: 151.12286958, Validation loss: 151.73426030, Gradient norm: 41.42092568
INFO:root:[  121] Training loss: 151.08297621, Validation loss: 151.66220040, Gradient norm: 40.65396927
INFO:root:[  122] Training loss: 151.09177311, Validation loss: 151.57885426, Gradient norm: 40.51576066
INFO:root:[  123] Training loss: 151.04800955, Validation loss: 151.44257697, Gradient norm: 37.07511176
INFO:root:[  124] Training loss: 151.02348625, Validation loss: 151.58918815, Gradient norm: 41.48439695
INFO:root:[  125] Training loss: 151.01476727, Validation loss: 151.48182415, Gradient norm: 42.50782275
INFO:root:[  126] Training loss: 150.97804409, Validation loss: 152.03272116, Gradient norm: 50.82901121
INFO:root:[  127] Training loss: 150.92486316, Validation loss: 151.62664321, Gradient norm: 38.94318984
INFO:root:[  128] Training loss: 150.86999269, Validation loss: 151.58573493, Gradient norm: 37.58383745
INFO:root:[  129] Training loss: 150.82296145, Validation loss: 151.55371357, Gradient norm: 36.53313641
INFO:root:[  130] Training loss: 150.89998525, Validation loss: 151.40931228, Gradient norm: 49.92161997
INFO:root:[  131] Training loss: 150.74217143, Validation loss: 151.66510168, Gradient norm: 36.04409236
INFO:root:[  132] Training loss: 150.74843739, Validation loss: 151.43962623, Gradient norm: 44.35970034
INFO:root:[  133] Training loss: 150.71314165, Validation loss: 151.61365114, Gradient norm: 44.26188513
INFO:root:[  134] Training loss: 150.74577669, Validation loss: 151.35424226, Gradient norm: 41.63635383
INFO:root:[  135] Training loss: 150.66347916, Validation loss: 151.38549594, Gradient norm: 42.41343121
INFO:root:[  136] Training loss: 150.66807705, Validation loss: 151.36290294, Gradient norm: 42.41039377
INFO:root:[  137] Training loss: 150.67966306, Validation loss: 151.25128069, Gradient norm: 44.93626078
INFO:root:[  138] Training loss: 150.61910525, Validation loss: 151.34335695, Gradient norm: 39.96821292
INFO:root:[  139] Training loss: 150.58931874, Validation loss: 151.24890610, Gradient norm: 41.47633280
INFO:root:[  140] Training loss: 150.54539827, Validation loss: 151.17266635, Gradient norm: 37.12256544
INFO:root:[  141] Training loss: 150.53736607, Validation loss: 151.27855130, Gradient norm: 43.83034919
INFO:root:[  142] Training loss: 150.46472924, Validation loss: 151.22724915, Gradient norm: 40.75639349
INFO:root:[  143] Training loss: 150.47967367, Validation loss: 151.17323935, Gradient norm: 39.81278473
INFO:root:[  144] Training loss: 150.46173892, Validation loss: 151.18294709, Gradient norm: 43.87190931
INFO:root:[  145] Training loss: 150.46647171, Validation loss: 151.01912663, Gradient norm: 44.19607248
INFO:root:[  146] Training loss: 150.39180290, Validation loss: 151.14590875, Gradient norm: 42.50754774
INFO:root:[  147] Training loss: 150.40573552, Validation loss: 151.23318324, Gradient norm: 47.07121612
INFO:root:[  148] Training loss: 150.36083606, Validation loss: 151.26441114, Gradient norm: 44.38790255
INFO:root:[  149] Training loss: 150.31394067, Validation loss: 150.96909569, Gradient norm: 36.15386398
INFO:root:[  150] Training loss: 150.32454796, Validation loss: 151.29707442, Gradient norm: 43.92596825
INFO:root:[  151] Training loss: 150.32979699, Validation loss: 151.06535602, Gradient norm: 43.44864540
INFO:root:[  152] Training loss: 150.30280810, Validation loss: 150.99123409, Gradient norm: 40.33924255
INFO:root:[  153] Training loss: 150.28267987, Validation loss: 150.92645685, Gradient norm: 48.68116412
INFO:root:[  154] Training loss: 150.20980511, Validation loss: 150.97957532, Gradient norm: 47.36480103
INFO:root:[  155] Training loss: 150.23719099, Validation loss: 150.89520158, Gradient norm: 43.88351924
INFO:root:[  156] Training loss: 150.19118790, Validation loss: 151.36821194, Gradient norm: 46.23775244
INFO:root:[  157] Training loss: 150.14403028, Validation loss: 150.82203569, Gradient norm: 48.00307516
INFO:root:[  158] Training loss: 150.15751945, Validation loss: 150.85994852, Gradient norm: 43.05886924
INFO:root:[  159] Training loss: 150.10172873, Validation loss: 150.80943719, Gradient norm: 42.75468110
INFO:root:[  160] Training loss: 150.08266186, Validation loss: 150.69015661, Gradient norm: 46.39768068
INFO:root:[  161] Training loss: 150.10295078, Validation loss: 150.82027988, Gradient norm: 44.07664196
INFO:root:[  162] Training loss: 150.14237247, Validation loss: 150.72913545, Gradient norm: 52.54589222
INFO:root:[  163] Training loss: 150.01219285, Validation loss: 150.72920858, Gradient norm: 40.25055156
INFO:root:[  164] Training loss: 149.98284061, Validation loss: 150.80602554, Gradient norm: 43.34940661
INFO:root:[  165] Training loss: 150.04631650, Validation loss: 150.87309318, Gradient norm: 46.62759472
INFO:root:[  166] Training loss: 149.99756629, Validation loss: 151.00455659, Gradient norm: 49.43451236
INFO:root:[  167] Training loss: 150.00653319, Validation loss: 150.67734501, Gradient norm: 40.68758792
INFO:root:[  168] Training loss: 149.98944632, Validation loss: 150.81215852, Gradient norm: 43.86087539
INFO:root:[  169] Training loss: 149.85726659, Validation loss: 150.49185865, Gradient norm: 39.20494626
INFO:root:[  170] Training loss: 149.92370930, Validation loss: 150.71637910, Gradient norm: 43.68158169
INFO:root:[  171] Training loss: 149.90007600, Validation loss: 150.70775367, Gradient norm: 42.40296754
INFO:root:[  172] Training loss: 149.88907157, Validation loss: 150.61062254, Gradient norm: 52.69234560
INFO:root:[  173] Training loss: 149.86929389, Validation loss: 150.47330396, Gradient norm: 40.00095232
INFO:root:[  174] Training loss: 149.76948993, Validation loss: 150.53287164, Gradient norm: 45.40415969
INFO:root:[  175] Training loss: 149.79344582, Validation loss: 150.75405252, Gradient norm: 49.79436429
INFO:root:[  176] Training loss: 149.79888376, Validation loss: 150.46997596, Gradient norm: 46.63597981
INFO:root:[  177] Training loss: 149.76783010, Validation loss: 150.39402034, Gradient norm: 44.76557564
INFO:root:[  178] Training loss: 149.75048788, Validation loss: 150.66004470, Gradient norm: 42.66390847
INFO:root:[  179] Training loss: 149.72941265, Validation loss: 150.62027398, Gradient norm: 41.18363717
INFO:root:[  180] Training loss: 149.69968056, Validation loss: 150.39661118, Gradient norm: 43.66090710
INFO:root:[  181] Training loss: 149.72566075, Validation loss: 150.46396190, Gradient norm: 53.76588719
INFO:root:[  182] Training loss: 149.69481463, Validation loss: 150.59116495, Gradient norm: 45.01886038
INFO:root:[  183] Training loss: 149.62407556, Validation loss: 150.36127340, Gradient norm: 43.14924460
INFO:root:[  184] Training loss: 149.64373185, Validation loss: 150.41479650, Gradient norm: 44.07774467
INFO:root:[  185] Training loss: 149.61118485, Validation loss: 150.71933562, Gradient norm: 43.89720463
INFO:root:[  186] Training loss: 149.63037082, Validation loss: 150.28578397, Gradient norm: 47.09015854
INFO:root:[  187] Training loss: 149.59470442, Validation loss: 150.45947003, Gradient norm: 46.19422368
INFO:root:[  188] Training loss: 149.61915831, Validation loss: 150.33846941, Gradient norm: 53.36830681
INFO:root:[  189] Training loss: 149.59499163, Validation loss: 150.45789574, Gradient norm: 46.52260486
INFO:root:[  190] Training loss: 149.56287863, Validation loss: 150.45226209, Gradient norm: 45.19141226
INFO:root:[  191] Training loss: 149.51223498, Validation loss: 150.46931142, Gradient norm: 44.34801629
INFO:root:[  192] Training loss: 149.47998871, Validation loss: 150.32725893, Gradient norm: 39.78610995
INFO:root:[  193] Training loss: 149.55030823, Validation loss: 150.55570931, Gradient norm: 50.34286282
INFO:root:[  194] Training loss: 149.49127751, Validation loss: 150.36895015, Gradient norm: 43.81581943
INFO:root:[  195] Training loss: 149.46760181, Validation loss: 150.35523829, Gradient norm: 42.23533054
INFO:root:[  196] Training loss: 149.44296319, Validation loss: 150.24647522, Gradient norm: 47.60964928
INFO:root:[  197] Training loss: 149.49008854, Validation loss: 150.61356433, Gradient norm: 55.17415708
INFO:root:[  198] Training loss: 149.49585596, Validation loss: 150.23476068, Gradient norm: 44.98749754
INFO:root:[  199] Training loss: 149.39543976, Validation loss: 150.21006144, Gradient norm: 49.66278355
INFO:root:[  200] Training loss: 149.36873823, Validation loss: 150.34551107, Gradient norm: 48.88748215
INFO:root:[  201] Training loss: 149.37240331, Validation loss: 150.37551301, Gradient norm: 48.20370617
INFO:root:[  202] Training loss: 149.33335242, Validation loss: 150.11280297, Gradient norm: 43.05496219
INFO:root:[  203] Training loss: 149.35452257, Validation loss: 150.13516235, Gradient norm: 47.72667562
INFO:root:[  204] Training loss: 149.34570326, Validation loss: 150.17663890, Gradient norm: 42.75192761
INFO:root:[  205] Training loss: 149.30529407, Validation loss: 150.36329283, Gradient norm: 43.64489100
INFO:root:[  206] Training loss: 149.29079593, Validation loss: 150.32077868, Gradient norm: 50.54029113
INFO:root:[  207] Training loss: 149.27162954, Validation loss: 150.34555054, Gradient norm: 45.31556736
INFO:root:[  208] Training loss: 149.29077202, Validation loss: 150.11631670, Gradient norm: 48.08655673
INFO:root:[  209] Training loss: 149.26472230, Validation loss: 150.12189247, Gradient norm: 49.81302022
INFO:root:[  210] Training loss: 149.21247756, Validation loss: 150.22876082, Gradient norm: 46.24050773
INFO:root:[  211] Training loss: 149.25045196, Validation loss: 150.21889996, Gradient norm: 48.72723960
INFO:root:[  212] Training loss: 149.21875189, Validation loss: 150.04661087, Gradient norm: 44.47919985
INFO:root:[  213] Training loss: 149.22044400, Validation loss: 150.04146707, Gradient norm: 45.79024494
INFO:root:[  214] Training loss: 149.15640124, Validation loss: 150.20914854, Gradient norm: 49.37115534
INFO:root:[  215] Training loss: 149.20917626, Validation loss: 150.13298771, Gradient norm: 45.69386345
INFO:root:[  216] Training loss: 149.17657444, Validation loss: 149.98058451, Gradient norm: 46.33553531
INFO:root:[  217] Training loss: 149.16023916, Validation loss: 150.14354785, Gradient norm: 43.89400929
INFO:root:[  218] Training loss: 149.13701758, Validation loss: 150.16003576, Gradient norm: 50.55838338
INFO:root:[  219] Training loss: 149.15514961, Validation loss: 150.09280501, Gradient norm: 43.38132842
INFO:root:[  220] Training loss: 149.11319213, Validation loss: 150.12333679, Gradient norm: 46.08493421
INFO:root:[  221] Training loss: 149.08622890, Validation loss: 150.31614948, Gradient norm: 45.68971862
INFO:root:[  222] Training loss: 149.14046458, Validation loss: 149.97705183, Gradient norm: 47.25854454
INFO:root:[  223] Training loss: 149.01226010, Validation loss: 150.02059410, Gradient norm: 41.71531946
INFO:root:[  224] Training loss: 149.05136135, Validation loss: 150.01857836, Gradient norm: 47.39293267
INFO:root:[  225] Training loss: 149.05078382, Validation loss: 149.82454760, Gradient norm: 47.84445806
INFO:root:[  226] Training loss: 148.98073571, Validation loss: 150.01266690, Gradient norm: 42.43102905
INFO:root:[  227] Training loss: 149.01759770, Validation loss: 149.97452045, Gradient norm: 51.34353005
INFO:root:[  228] Training loss: 149.03954106, Validation loss: 149.94837268, Gradient norm: 52.37681584
INFO:root:[  229] Training loss: 149.01266561, Validation loss: 149.91134012, Gradient norm: 54.84102439
INFO:root:[  230] Training loss: 149.02009042, Validation loss: 150.00510485, Gradient norm: 42.41841584
INFO:root:[  231] Training loss: 148.93898469, Validation loss: 149.99618004, Gradient norm: 45.39544758
INFO:root:[  232] Training loss: 148.96499418, Validation loss: 149.85166563, Gradient norm: 49.63264953
INFO:root:[  233] Training loss: 148.91080914, Validation loss: 149.84726005, Gradient norm: 42.22891677
INFO:root:[  234] Training loss: 148.88462965, Validation loss: 150.09408727, Gradient norm: 45.71472365
INFO:root:EP 234: Early stopping
INFO:root:Training the model took 1184.75s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 9216.18321
INFO:root:EnergyScoreTrain: 7889.57623
INFO:root:CoverageTrain: 0.37493
INFO:root:IntervalWidthTrain: 1.75548
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 9049.58042
INFO:root:EnergyScoreValidation: 7756.91812
INFO:root:CoverageValidation: 0.37013
INFO:root:IntervalWidthValidation: 1.75206
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 9120.91792
INFO:root:EnergyScoreTest: 7818.75025
INFO:root:CoverageTest: 0.36986
INFO:root:IntervalWidthTest: 1.75178
