INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno_srr.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.37591834, Validation loss: 0.17158273, Gradient norm: 7.38096534
INFO:root:[    2] Training loss: 0.17468821, Validation loss: 0.12927811, Gradient norm: 4.38390268
INFO:root:[    3] Training loss: 0.15557425, Validation loss: 0.13368987, Gradient norm: 3.68470279
INFO:root:[    4] Training loss: 0.14414879, Validation loss: 0.12392370, Gradient norm: 3.65819495
INFO:root:[    5] Training loss: 0.13410110, Validation loss: 0.12029944, Gradient norm: 3.42328522
INFO:root:[    6] Training loss: 0.12332948, Validation loss: 0.09834704, Gradient norm: 2.48813352
INFO:root:[    7] Training loss: 0.12188408, Validation loss: 0.12431427, Gradient norm: 3.40649895
INFO:root:[    8] Training loss: 0.11544034, Validation loss: 0.09844297, Gradient norm: 3.10429952
INFO:root:[    9] Training loss: 0.11478169, Validation loss: 0.11677264, Gradient norm: 3.07606146
INFO:root:[   10] Training loss: 0.10527819, Validation loss: 0.10124030, Gradient norm: 2.43825548
INFO:root:[   11] Training loss: 0.10200471, Validation loss: 0.10212946, Gradient norm: 2.66187963
INFO:root:[   12] Training loss: 0.09897965, Validation loss: 0.10456611, Gradient norm: 2.56562486
INFO:root:[   13] Training loss: 0.09803659, Validation loss: 0.10830337, Gradient norm: 2.46498795
INFO:root:[   14] Training loss: 0.09382340, Validation loss: 0.11680968, Gradient norm: 2.34688980
INFO:root:[   15] Training loss: 0.09016051, Validation loss: 0.09652378, Gradient norm: 2.43200652
INFO:root:[   16] Training loss: 0.08899736, Validation loss: 0.12526483, Gradient norm: 2.29948759
INFO:root:[   17] Training loss: 0.08529251, Validation loss: 0.11836638, Gradient norm: 1.86525099
INFO:root:[   18] Training loss: 0.08483583, Validation loss: 0.11108818, Gradient norm: 2.28000371
INFO:root:[   19] Training loss: 0.08803035, Validation loss: 0.12326561, Gradient norm: 2.33276303
INFO:root:[   20] Training loss: 0.08309706, Validation loss: 0.11455642, Gradient norm: 2.11138698
INFO:root:[   21] Training loss: 0.08191725, Validation loss: 0.09901796, Gradient norm: 2.09576654
INFO:root:[   22] Training loss: 0.07963856, Validation loss: 0.15633364, Gradient norm: 1.81494911
INFO:root:[   23] Training loss: 0.08110698, Validation loss: 0.11372316, Gradient norm: 2.16773678
INFO:root:[   24] Training loss: 0.07943822, Validation loss: 0.10315983, Gradient norm: 2.00850505
INFO:root:[   25] Training loss: 0.08017684, Validation loss: 0.11109760, Gradient norm: 2.31320527
INFO:root:[   26] Training loss: 0.07676740, Validation loss: 0.12862824, Gradient norm: 1.89079421
INFO:root:[   27] Training loss: 0.07692663, Validation loss: 0.12086452, Gradient norm: 2.05921343
INFO:root:[   28] Training loss: 0.07663561, Validation loss: 0.14362086, Gradient norm: 1.85948869
INFO:root:[   29] Training loss: 0.07488642, Validation loss: 0.15717392, Gradient norm: 1.88831926
INFO:root:[   30] Training loss: 0.07375914, Validation loss: 0.13021308, Gradient norm: 1.80781582
INFO:root:[   31] Training loss: 0.07354080, Validation loss: 0.14412981, Gradient norm: 1.94811703
INFO:root:[   32] Training loss: 0.07400674, Validation loss: 0.12085775, Gradient norm: 2.01980540
INFO:root:[   33] Training loss: 0.07202722, Validation loss: 0.14992351, Gradient norm: 2.09271672
INFO:root:[   34] Training loss: 0.07210040, Validation loss: 0.14594948, Gradient norm: 1.82791601
INFO:root:[   35] Training loss: 0.07065979, Validation loss: 0.11915354, Gradient norm: 1.85218193
INFO:root:[   36] Training loss: 0.07156649, Validation loss: 0.14292956, Gradient norm: 1.88108000
INFO:root:[   37] Training loss: 0.06964996, Validation loss: 0.13419046, Gradient norm: 1.89069357
INFO:root:[   38] Training loss: 0.07206503, Validation loss: 0.13191376, Gradient norm: 2.14453065
INFO:root:[   39] Training loss: 0.06931734, Validation loss: 0.14618974, Gradient norm: 1.84666990
INFO:root:[   40] Training loss: 0.06833580, Validation loss: 0.13074824, Gradient norm: 1.84765284
INFO:root:[   41] Training loss: 0.06698523, Validation loss: 0.12092998, Gradient norm: 1.71774081
INFO:root:[   42] Training loss: 0.06712633, Validation loss: 0.14067403, Gradient norm: 1.85456738
INFO:root:[   43] Training loss: 0.06914467, Validation loss: 0.14517067, Gradient norm: 2.09676731
INFO:root:[   44] Training loss: 0.06711901, Validation loss: 0.13898056, Gradient norm: 1.80965019
INFO:root:[   45] Training loss: 0.06712232, Validation loss: 0.12585605, Gradient norm: 1.84909263
INFO:root:[   46] Training loss: 0.06629470, Validation loss: 0.13036208, Gradient norm: 1.76669859
INFO:root:[   47] Training loss: 0.06705151, Validation loss: 0.14677348, Gradient norm: 1.87139279
INFO:root:[   48] Training loss: 0.06508371, Validation loss: 0.13022157, Gradient norm: 1.91794511
INFO:root:[   49] Training loss: 0.06475357, Validation loss: 0.13446462, Gradient norm: 1.78530599
INFO:root:[   50] Training loss: 0.06438440, Validation loss: 0.12947993, Gradient norm: 1.73488442
INFO:root:[   51] Training loss: 0.06483671, Validation loss: 0.14084103, Gradient norm: 1.86573621
INFO:root:[   52] Training loss: 0.06515187, Validation loss: 0.14880515, Gradient norm: 1.90778583
INFO:root:[   53] Training loss: 0.06302133, Validation loss: 0.13210639, Gradient norm: 1.65010057
INFO:root:[   54] Training loss: 0.06559390, Validation loss: 0.14622690, Gradient norm: 1.95438945
INFO:root:[   55] Training loss: 0.06414943, Validation loss: 0.13896868, Gradient norm: 1.84421302
INFO:root:[   56] Training loss: 0.06394407, Validation loss: 0.13761610, Gradient norm: 1.65937531
INFO:root:[   57] Training loss: 0.06356792, Validation loss: 0.13970482, Gradient norm: 1.67755069
INFO:root:[   58] Training loss: 0.06306625, Validation loss: 0.14274818, Gradient norm: 1.82695988
INFO:root:[   59] Training loss: 0.06305960, Validation loss: 0.15283744, Gradient norm: 1.96178304
INFO:root:[   60] Training loss: 0.06308008, Validation loss: 0.13131614, Gradient norm: 1.79210800
INFO:root:[   61] Training loss: 0.06272441, Validation loss: 0.12371573, Gradient norm: 1.96092592
INFO:root:[   62] Training loss: 0.06309379, Validation loss: 0.14465315, Gradient norm: 1.89861858
INFO:root:[   63] Training loss: 0.06159669, Validation loss: 0.14167718, Gradient norm: 1.73974049
INFO:root:[   64] Training loss: 0.06139351, Validation loss: 0.15146925, Gradient norm: 1.49467704
INFO:root:[   65] Training loss: 0.06082624, Validation loss: 0.14559898, Gradient norm: 1.72965840
INFO:root:[   66] Training loss: 0.06250196, Validation loss: 0.13655752, Gradient norm: 1.80282662
INFO:root:[   67] Training loss: 0.06075152, Validation loss: 0.12952140, Gradient norm: 1.77322579
INFO:root:[   68] Training loss: 0.06118185, Validation loss: 0.14437660, Gradient norm: 1.61809744
INFO:root:[   69] Training loss: 0.06059827, Validation loss: 0.14779207, Gradient norm: 1.70938437
INFO:root:[   70] Training loss: 0.05992909, Validation loss: 0.12367396, Gradient norm: 1.73743244
INFO:root:[   71] Training loss: 0.06014419, Validation loss: 0.14655277, Gradient norm: 1.71279101
INFO:root:[   72] Training loss: 0.06085881, Validation loss: 0.15051174, Gradient norm: 1.83401995
INFO:root:[   73] Training loss: 0.05973002, Validation loss: 0.15239310, Gradient norm: 1.77652202
INFO:root:[   74] Training loss: 0.05983998, Validation loss: 0.13400728, Gradient norm: 1.67625369
INFO:root:[   75] Training loss: 0.05868246, Validation loss: 0.13125525, Gradient norm: 1.46015412
INFO:root:[   76] Training loss: 0.06127976, Validation loss: 0.13261253, Gradient norm: 1.97501598
INFO:root:[   77] Training loss: 0.05933012, Validation loss: 0.14634202, Gradient norm: 1.73051524
INFO:root:[   78] Training loss: 0.05930245, Validation loss: 0.14482133, Gradient norm: 1.45213189
INFO:root:[   79] Training loss: 0.05823880, Validation loss: 0.15069773, Gradient norm: 1.51610117
INFO:root:EP 79: Early stopping
INFO:root:Training the model took 3842.887s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.13384
INFO:root:EnergyScoreTrain: 0.10052
INFO:root:CoverageTrain: 0.60768
INFO:root:IntervalWidthTrain: 0.05354
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.12911
INFO:root:EnergyScoreValidation: 0.09737
INFO:root:CoverageValidation: 0.59051
INFO:root:IntervalWidthValidation: 0.0496
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.13075
INFO:root:EnergyScoreTest: 0.0988
INFO:root:CoverageTest: 0.5843
INFO:root:IntervalWidthTest: 0.04962
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 257949696
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.33209096, Validation loss: 0.13496537, Gradient norm: 5.96940386
INFO:root:[    2] Training loss: 0.16320187, Validation loss: 0.15678682, Gradient norm: 4.38449847
INFO:root:[    3] Training loss: 0.14384441, Validation loss: 0.12339738, Gradient norm: 4.01176486
INFO:root:[    4] Training loss: 0.13699157, Validation loss: 0.12653190, Gradient norm: 3.52769981
INFO:root:[    5] Training loss: 0.12381676, Validation loss: 0.11500015, Gradient norm: 2.63495683
INFO:root:[    6] Training loss: 0.11937320, Validation loss: 0.13183970, Gradient norm: 2.86700191
INFO:root:[    7] Training loss: 0.11273631, Validation loss: 0.10569028, Gradient norm: 2.85258184
INFO:root:[    8] Training loss: 0.10556917, Validation loss: 0.11762055, Gradient norm: 2.46852875
INFO:root:[    9] Training loss: 0.10457033, Validation loss: 0.10577854, Gradient norm: 2.60972110
INFO:root:[   10] Training loss: 0.09943743, Validation loss: 0.09653402, Gradient norm: 2.38145630
INFO:root:[   11] Training loss: 0.09291304, Validation loss: 0.10712711, Gradient norm: 2.03327356
INFO:root:[   12] Training loss: 0.09223978, Validation loss: 0.11460686, Gradient norm: 2.31050992
INFO:root:[   13] Training loss: 0.09042489, Validation loss: 0.13571889, Gradient norm: 1.99717881
INFO:root:[   14] Training loss: 0.08686890, Validation loss: 0.09971940, Gradient norm: 2.14642680
INFO:root:[   15] Training loss: 0.08587099, Validation loss: 0.11581732, Gradient norm: 2.12072825
INFO:root:[   16] Training loss: 0.08299164, Validation loss: 0.14951060, Gradient norm: 1.88014403
INFO:root:[   17] Training loss: 0.08110823, Validation loss: 0.13194777, Gradient norm: 2.03499040
INFO:root:[   18] Training loss: 0.08224354, Validation loss: 0.11076486, Gradient norm: 1.98041485
INFO:root:[   19] Training loss: 0.07969736, Validation loss: 0.11121308, Gradient norm: 1.95562668
INFO:root:[   20] Training loss: 0.07936992, Validation loss: 0.11538017, Gradient norm: 2.09670251
INFO:root:[   21] Training loss: 0.08012047, Validation loss: 0.15661574, Gradient norm: 2.13212680
INFO:root:[   22] Training loss: 0.07591920, Validation loss: 0.11791772, Gradient norm: 2.00471985
INFO:root:[   23] Training loss: 0.07512208, Validation loss: 0.12769357, Gradient norm: 1.74888844
INFO:root:[   24] Training loss: 0.07668898, Validation loss: 0.11770643, Gradient norm: 1.97222618
INFO:root:[   25] Training loss: 0.07620478, Validation loss: 0.11283310, Gradient norm: 1.93233682
INFO:root:[   26] Training loss: 0.07389537, Validation loss: 0.16268854, Gradient norm: 1.97410859
INFO:root:[   27] Training loss: 0.07398789, Validation loss: 0.12564424, Gradient norm: 1.87505502
INFO:root:[   28] Training loss: 0.07294384, Validation loss: 0.11786594, Gradient norm: 1.93072596
INFO:root:[   29] Training loss: 0.07374080, Validation loss: 0.10490133, Gradient norm: 2.07967262
INFO:root:[   30] Training loss: 0.07196261, Validation loss: 0.14747943, Gradient norm: 1.85993438
INFO:root:[   31] Training loss: 0.07204510, Validation loss: 0.17107870, Gradient norm: 1.78767324
INFO:root:[   32] Training loss: 0.07029734, Validation loss: 0.11340420, Gradient norm: 1.85875714
INFO:root:[   33] Training loss: 0.07107770, Validation loss: 0.15397670, Gradient norm: 1.82924205
INFO:root:[   34] Training loss: 0.06881312, Validation loss: 0.12110820, Gradient norm: 1.86621380
INFO:root:[   35] Training loss: 0.07010342, Validation loss: 0.12062798, Gradient norm: 1.94755573
INFO:root:[   36] Training loss: 0.06973985, Validation loss: 0.13411330, Gradient norm: 1.70661686
INFO:root:[   37] Training loss: 0.06976038, Validation loss: 0.13119031, Gradient norm: 1.91052434
INFO:root:[   38] Training loss: 0.06946986, Validation loss: 0.12414811, Gradient norm: 1.81995172
INFO:root:[   39] Training loss: 0.06803693, Validation loss: 0.16633236, Gradient norm: 1.83767432
INFO:root:[   40] Training loss: 0.06746312, Validation loss: 0.15181076, Gradient norm: 1.79087810
INFO:root:[   41] Training loss: 0.06850611, Validation loss: 0.13260704, Gradient norm: 1.84869054
INFO:root:[   42] Training loss: 0.06720734, Validation loss: 0.15200113, Gradient norm: 1.81575769
INFO:root:[   43] Training loss: 0.06560527, Validation loss: 0.14487970, Gradient norm: 1.80273131
INFO:root:[   44] Training loss: 0.06638945, Validation loss: 0.14479394, Gradient norm: 1.82854882
INFO:root:[   45] Training loss: 0.06622368, Validation loss: 0.14238914, Gradient norm: 1.92830539
INFO:root:[   46] Training loss: 0.06605881, Validation loss: 0.15854466, Gradient norm: 1.84906352
INFO:root:[   47] Training loss: 0.06505704, Validation loss: 0.13774473, Gradient norm: 1.75387737
INFO:root:[   48] Training loss: 0.06637467, Validation loss: 0.14539576, Gradient norm: 1.87154404
INFO:root:[   49] Training loss: 0.06512294, Validation loss: 0.14025790, Gradient norm: 1.78559136
INFO:root:[   50] Training loss: 0.06376989, Validation loss: 0.14591065, Gradient norm: 1.81011272
INFO:root:[   51] Training loss: 0.06523175, Validation loss: 0.14001689, Gradient norm: 1.94460457
INFO:root:[   52] Training loss: 0.06464533, Validation loss: 0.12539974, Gradient norm: 1.86813780
INFO:root:[   53] Training loss: 0.06457505, Validation loss: 0.13321057, Gradient norm: 1.91180995
INFO:root:[   54] Training loss: 0.06430424, Validation loss: 0.15236759, Gradient norm: 1.77756564
INFO:root:[   55] Training loss: 0.06196895, Validation loss: 0.14900744, Gradient norm: 1.58855218
INFO:root:[   56] Training loss: 0.06321839, Validation loss: 0.12188965, Gradient norm: 1.66954966
INFO:root:[   57] Training loss: 0.06444969, Validation loss: 0.14235734, Gradient norm: 1.80844174
INFO:root:[   58] Training loss: 0.06300131, Validation loss: 0.14188897, Gradient norm: 1.84344754
INFO:root:[   59] Training loss: 0.06297268, Validation loss: 0.16234273, Gradient norm: 1.87834828
INFO:root:[   60] Training loss: 0.06321287, Validation loss: 0.12517762, Gradient norm: 1.77326449
INFO:root:[   61] Training loss: 0.06210842, Validation loss: 0.15033992, Gradient norm: 1.81040266
INFO:root:[   62] Training loss: 0.06167799, Validation loss: 0.14206854, Gradient norm: 1.71577468
INFO:root:[   63] Training loss: 0.06092983, Validation loss: 0.14171575, Gradient norm: 1.60261757
INFO:root:[   64] Training loss: 0.06016987, Validation loss: 0.14154234, Gradient norm: 1.71195233
INFO:root:[   65] Training loss: 0.05961042, Validation loss: 0.12008516, Gradient norm: 1.53278804
INFO:root:[   66] Training loss: 0.06181227, Validation loss: 0.12339076, Gradient norm: 1.75741350
INFO:root:[   67] Training loss: 0.05983275, Validation loss: 0.14213269, Gradient norm: 1.44562581
INFO:root:[   68] Training loss: 0.06262569, Validation loss: 0.13332936, Gradient norm: 1.90650004
INFO:root:[   69] Training loss: 0.05995194, Validation loss: 0.13600416, Gradient norm: 1.56884534
INFO:root:[   70] Training loss: 0.06070979, Validation loss: 0.13075032, Gradient norm: 1.74270407
INFO:root:[   71] Training loss: 0.06084154, Validation loss: 0.12779916, Gradient norm: 1.62765061
INFO:root:[   72] Training loss: 0.05898342, Validation loss: 0.14333623, Gradient norm: 1.69937966
INFO:root:[   73] Training loss: 0.05896685, Validation loss: 0.16095565, Gradient norm: 1.68575161
INFO:root:[   74] Training loss: 0.06094525, Validation loss: 0.14094510, Gradient norm: 1.87491194
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 3546.384s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.09913
INFO:root:EnergyScoreTrain: 0.07411
INFO:root:CoverageTrain: 0.86055
INFO:root:IntervalWidthTrain: 0.07011
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.1312
INFO:root:EnergyScoreValidation: 0.09716
INFO:root:CoverageValidation: 0.76431
INFO:root:IntervalWidthValidation: 0.06693
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.13462
INFO:root:EnergyScoreTest: 0.10001
INFO:root:CoverageTest: 0.75282
INFO:root:IntervalWidthTest: 0.06685
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 92274688
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.24367125, Validation loss: 0.15061169, Gradient norm: 4.58771657
INFO:root:[    2] Training loss: 0.15159479, Validation loss: 0.11826932, Gradient norm: 2.99513891
INFO:root:[    3] Training loss: 0.13271701, Validation loss: 0.10601602, Gradient norm: 2.98827691
INFO:root:[    4] Training loss: 0.11977503, Validation loss: 0.10583266, Gradient norm: 2.41736181
INFO:root:[    5] Training loss: 0.11177681, Validation loss: 0.09799467, Gradient norm: 2.69265343
INFO:root:[    6] Training loss: 0.10891532, Validation loss: 0.09410844, Gradient norm: 2.34485399
INFO:root:[    7] Training loss: 0.09835362, Validation loss: 0.09844858, Gradient norm: 2.23562359
INFO:root:[    8] Training loss: 0.10451971, Validation loss: 0.12632547, Gradient norm: 2.79745335
INFO:root:[    9] Training loss: 0.09831365, Validation loss: 0.09547145, Gradient norm: 2.56238533
INFO:root:[   10] Training loss: 0.09534660, Validation loss: 0.09821192, Gradient norm: 2.33594419
INFO:root:[   11] Training loss: 0.09523620, Validation loss: 0.12297986, Gradient norm: 2.52563351
INFO:root:[   12] Training loss: 0.09355327, Validation loss: 0.09506271, Gradient norm: 2.39321927
INFO:root:[   13] Training loss: 0.08967925, Validation loss: 0.11263432, Gradient norm: 2.36220394
INFO:root:[   14] Training loss: 0.08891278, Validation loss: 0.13837817, Gradient norm: 2.19309165
INFO:root:[   15] Training loss: 0.08728661, Validation loss: 0.12156392, Gradient norm: 2.37981437
INFO:root:[   16] Training loss: 0.08615119, Validation loss: 0.10132586, Gradient norm: 2.33720296
INFO:root:[   17] Training loss: 0.08603765, Validation loss: 0.13433635, Gradient norm: 2.23992923
INFO:root:[   18] Training loss: 0.08208356, Validation loss: 0.14412819, Gradient norm: 2.12596767
INFO:root:[   19] Training loss: 0.08229776, Validation loss: 0.11898045, Gradient norm: 2.01102317
INFO:root:[   20] Training loss: 0.08251801, Validation loss: 0.15593406, Gradient norm: 2.26285511
INFO:root:[   21] Training loss: 0.07876044, Validation loss: 0.11949349, Gradient norm: 2.10857772
INFO:root:[   22] Training loss: 0.08049464, Validation loss: 0.11856477, Gradient norm: 2.15642521
INFO:root:[   23] Training loss: 0.07833211, Validation loss: 0.09836556, Gradient norm: 2.14093171
INFO:root:[   24] Training loss: 0.07991329, Validation loss: 0.11369845, Gradient norm: 2.32522325
INFO:root:[   25] Training loss: 0.07473824, Validation loss: 0.12436084, Gradient norm: 2.10575975
INFO:root:[   26] Training loss: 0.07416259, Validation loss: 0.11505241, Gradient norm: 2.03544806
INFO:root:[   27] Training loss: 0.07515423, Validation loss: 0.11115889, Gradient norm: 2.00346294
INFO:root:[   28] Training loss: 0.07275461, Validation loss: 0.11461686, Gradient norm: 2.01254774
INFO:root:[   29] Training loss: 0.07165402, Validation loss: 0.13076003, Gradient norm: 2.23786471
INFO:root:[   30] Training loss: 0.07380552, Validation loss: 0.12745568, Gradient norm: 2.27229182
INFO:root:[   31] Training loss: 0.07244918, Validation loss: 0.12422466, Gradient norm: 2.16855550
INFO:root:[   32] Training loss: 0.07025836, Validation loss: 0.12914684, Gradient norm: 1.92356067
INFO:root:[   33] Training loss: 0.07024141, Validation loss: 0.12075269, Gradient norm: 1.96869463
INFO:root:[   34] Training loss: 0.07169997, Validation loss: 0.12719551, Gradient norm: 2.09381171
INFO:root:[   35] Training loss: 0.06915478, Validation loss: 0.14452804, Gradient norm: 2.00897937
INFO:root:[   36] Training loss: 0.07105248, Validation loss: 0.12604098, Gradient norm: 2.20926254
INFO:root:[   37] Training loss: 0.06860405, Validation loss: 0.12280809, Gradient norm: 2.01311673
INFO:root:[   38] Training loss: 0.06865631, Validation loss: 0.13448959, Gradient norm: 2.13924946
INFO:root:[   39] Training loss: 0.06773186, Validation loss: 0.15673578, Gradient norm: 1.84174976
INFO:root:[   40] Training loss: 0.06775511, Validation loss: 0.14170192, Gradient norm: 1.99106286
INFO:root:[   41] Training loss: 0.06734952, Validation loss: 0.12627804, Gradient norm: 1.89528135
INFO:root:[   42] Training loss: 0.06509406, Validation loss: 0.12013170, Gradient norm: 1.86692972
INFO:root:[   43] Training loss: 0.06626125, Validation loss: 0.15757642, Gradient norm: 1.85416981
INFO:root:[   44] Training loss: 0.06849402, Validation loss: 0.13961721, Gradient norm: 1.97625927
INFO:root:[   45] Training loss: 0.06567690, Validation loss: 0.13430186, Gradient norm: 2.04430890
INFO:root:[   46] Training loss: 0.06497835, Validation loss: 0.12660523, Gradient norm: 1.87725799
INFO:root:[   47] Training loss: 0.06584000, Validation loss: 0.14346055, Gradient norm: 2.22307410
INFO:root:[   48] Training loss: 0.06421096, Validation loss: 0.15283837, Gradient norm: 1.62403507
INFO:root:[   49] Training loss: 0.06505706, Validation loss: 0.13149312, Gradient norm: 1.95689496
INFO:root:[   50] Training loss: 0.06589536, Validation loss: 0.13867496, Gradient norm: 1.84573084
INFO:root:[   51] Training loss: 0.06349619, Validation loss: 0.13286672, Gradient norm: 1.70984621
INFO:root:[   52] Training loss: 0.06300254, Validation loss: 0.16006306, Gradient norm: 1.72031585
INFO:root:[   53] Training loss: 0.06388160, Validation loss: 0.14979723, Gradient norm: 1.90575646
INFO:root:[   54] Training loss: 0.06600250, Validation loss: 0.12926583, Gradient norm: 2.07206086
INFO:root:[   55] Training loss: 0.06373643, Validation loss: 0.12412162, Gradient norm: 1.83114680
INFO:root:[   56] Training loss: 0.06458238, Validation loss: 0.15382160, Gradient norm: 1.89149936
INFO:root:[   57] Training loss: 0.06201762, Validation loss: 0.13833163, Gradient norm: 1.80669567
INFO:root:[   58] Training loss: 0.06265418, Validation loss: 0.15026072, Gradient norm: 1.75278224
INFO:root:[   59] Training loss: 0.06285013, Validation loss: 0.13025785, Gradient norm: 1.89327356
INFO:root:[   60] Training loss: 0.06068194, Validation loss: 0.13790409, Gradient norm: 1.68625168
INFO:root:[   61] Training loss: 0.06322544, Validation loss: 0.15386383, Gradient norm: 1.86243720
INFO:root:[   62] Training loss: 0.06282023, Validation loss: 0.14649997, Gradient norm: 1.80845816
INFO:root:[   63] Training loss: 0.06129447, Validation loss: 0.16386450, Gradient norm: 1.65309608
INFO:root:[   64] Training loss: 0.06264871, Validation loss: 0.14274082, Gradient norm: 1.77216498
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 3078.462s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.10742
INFO:root:EnergyScoreTrain: 0.07942
INFO:root:CoverageTrain: 0.85593
INFO:root:IntervalWidthTrain: 0.07817
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.12917
INFO:root:EnergyScoreValidation: 0.09465
INFO:root:CoverageValidation: 0.79929
INFO:root:IntervalWidthValidation: 0.07682
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.13062
INFO:root:EnergyScoreTest: 0.09587
INFO:root:CoverageTest: 0.79355
INFO:root:IntervalWidthTest: 0.07677
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 92274688
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.30038194, Validation loss: 0.18990960, Gradient norm: 5.22420988
INFO:root:[    2] Training loss: 0.16598663, Validation loss: 0.12543822, Gradient norm: 3.74087083
INFO:root:[    3] Training loss: 0.14462284, Validation loss: 0.10986538, Gradient norm: 3.19960453
INFO:root:[    4] Training loss: 0.12873551, Validation loss: 0.09566695, Gradient norm: 3.09020392
INFO:root:[    5] Training loss: 0.11938164, Validation loss: 0.09917709, Gradient norm: 2.54455982
INFO:root:[    6] Training loss: 0.11301737, Validation loss: 0.12102020, Gradient norm: 2.48952229
INFO:root:[    7] Training loss: 0.10789091, Validation loss: 0.09398862, Gradient norm: 2.37980901
INFO:root:[    8] Training loss: 0.10229437, Validation loss: 0.09808631, Gradient norm: 2.11284375
INFO:root:[    9] Training loss: 0.09874499, Validation loss: 0.11474415, Gradient norm: 1.91347235
INFO:root:[   10] Training loss: 0.09967053, Validation loss: 0.12302527, Gradient norm: 2.31783623
INFO:root:[   11] Training loss: 0.09961301, Validation loss: 0.09378864, Gradient norm: 2.24183762
INFO:root:[   12] Training loss: 0.09353004, Validation loss: 0.10918614, Gradient norm: 1.90006072
INFO:root:[   13] Training loss: 0.09186008, Validation loss: 0.12614001, Gradient norm: 2.06485675
INFO:root:[   14] Training loss: 0.09104262, Validation loss: 0.09974830, Gradient norm: 2.03780288
INFO:root:[   15] Training loss: 0.08976355, Validation loss: 0.12745995, Gradient norm: 2.06139049
INFO:root:[   16] Training loss: 0.08963262, Validation loss: 0.11707291, Gradient norm: 2.22974168
INFO:root:[   17] Training loss: 0.08581522, Validation loss: 0.12675672, Gradient norm: 1.97999625
INFO:root:[   18] Training loss: 0.08282409, Validation loss: 0.12990378, Gradient norm: 1.78192226
INFO:root:[   19] Training loss: 0.08401194, Validation loss: 0.09562277, Gradient norm: 1.83864890
INFO:root:[   20] Training loss: 0.08061761, Validation loss: 0.09244625, Gradient norm: 1.91261461
INFO:root:[   21] Training loss: 0.07763658, Validation loss: 0.11324609, Gradient norm: 1.66334727
INFO:root:[   22] Training loss: 0.07734778, Validation loss: 0.13082594, Gradient norm: 1.76276363
INFO:root:[   23] Training loss: 0.07802291, Validation loss: 0.10048364, Gradient norm: 1.94554977
INFO:root:[   24] Training loss: 0.07762789, Validation loss: 0.10695959, Gradient norm: 1.99323533
INFO:root:[   25] Training loss: 0.07683667, Validation loss: 0.10803734, Gradient norm: 2.03422257
INFO:root:[   26] Training loss: 0.07379604, Validation loss: 0.10378723, Gradient norm: 1.73734249
INFO:root:[   27] Training loss: 0.07373672, Validation loss: 0.11954568, Gradient norm: 1.68622322
INFO:root:[   28] Training loss: 0.07263751, Validation loss: 0.11998994, Gradient norm: 1.79086919
INFO:root:[   29] Training loss: 0.07257902, Validation loss: 0.13550825, Gradient norm: 2.00503034
INFO:root:[   30] Training loss: 0.07068988, Validation loss: 0.10392121, Gradient norm: 1.78724789
INFO:root:[   31] Training loss: 0.07061403, Validation loss: 0.14419741, Gradient norm: 1.68016955
INFO:root:[   32] Training loss: 0.07080093, Validation loss: 0.12541224, Gradient norm: 1.75411517
INFO:root:[   33] Training loss: 0.07168049, Validation loss: 0.12304876, Gradient norm: 1.79990637
INFO:root:[   34] Training loss: 0.07148125, Validation loss: 0.11364727, Gradient norm: 1.85285580
INFO:root:[   35] Training loss: 0.07025096, Validation loss: 0.12018229, Gradient norm: 1.75069631
INFO:root:[   36] Training loss: 0.06751656, Validation loss: 0.13386986, Gradient norm: 1.83576085
INFO:root:[   37] Training loss: 0.06900108, Validation loss: 0.13269181, Gradient norm: 1.79089645
INFO:root:[   38] Training loss: 0.06810732, Validation loss: 0.12763056, Gradient norm: 1.86798265
INFO:root:[   39] Training loss: 0.06764447, Validation loss: 0.11756283, Gradient norm: 2.05935550
INFO:root:[   40] Training loss: 0.06877191, Validation loss: 0.11527307, Gradient norm: 1.86352288
INFO:root:[   41] Training loss: 0.06697431, Validation loss: 0.13220990, Gradient norm: 1.65811053
INFO:root:[   42] Training loss: 0.06698783, Validation loss: 0.11514321, Gradient norm: 1.67070191
INFO:root:[   43] Training loss: 0.06680699, Validation loss: 0.13225390, Gradient norm: 1.80215018
INFO:root:[   44] Training loss: 0.06630772, Validation loss: 0.13715496, Gradient norm: 1.86979891
INFO:root:[   45] Training loss: 0.06736414, Validation loss: 0.11845655, Gradient norm: 1.85517298
INFO:root:[   46] Training loss: 0.06470655, Validation loss: 0.12047791, Gradient norm: 1.65339064
INFO:root:[   47] Training loss: 0.06522041, Validation loss: 0.11396537, Gradient norm: 1.82214249
INFO:root:[   48] Training loss: 0.06554795, Validation loss: 0.13241436, Gradient norm: 1.82475172
INFO:root:[   49] Training loss: 0.06628331, Validation loss: 0.13831121, Gradient norm: 1.80359966
INFO:root:[   50] Training loss: 0.06457441, Validation loss: 0.12389688, Gradient norm: 1.73986630
INFO:root:[   51] Training loss: 0.06247390, Validation loss: 0.11401562, Gradient norm: 1.71833043
INFO:root:[   52] Training loss: 0.06282788, Validation loss: 0.14001538, Gradient norm: 1.58547479
INFO:root:[   53] Training loss: 0.06214130, Validation loss: 0.12070981, Gradient norm: 1.61798869
INFO:root:[   54] Training loss: 0.06344242, Validation loss: 0.12451935, Gradient norm: 1.75934842
INFO:root:[   55] Training loss: 0.06271702, Validation loss: 0.13390568, Gradient norm: 1.50257862
INFO:root:[   56] Training loss: 0.06531381, Validation loss: 0.12654188, Gradient norm: 1.93962540
INFO:root:[   57] Training loss: 0.06162773, Validation loss: 0.11092068, Gradient norm: 1.68649873
INFO:root:[   58] Training loss: 0.06366795, Validation loss: 0.12775059, Gradient norm: 1.75628638
INFO:root:[   59] Training loss: 0.06157447, Validation loss: 0.12483546, Gradient norm: 1.63905463
INFO:root:[   60] Training loss: 0.06157991, Validation loss: 0.14312380, Gradient norm: 1.50619910
INFO:root:[   61] Training loss: 0.06337917, Validation loss: 0.12787611, Gradient norm: 1.54210235
INFO:root:[   62] Training loss: 0.06231212, Validation loss: 0.13628089, Gradient norm: 1.73324401
INFO:root:[   63] Training loss: 0.06512840, Validation loss: 0.13207730, Gradient norm: 1.86385722
INFO:root:[   64] Training loss: 0.05972035, Validation loss: 0.14112300, Gradient norm: 1.41828423
INFO:root:[   65] Training loss: 0.06204175, Validation loss: 0.12589094, Gradient norm: 1.49891539
INFO:root:[   66] Training loss: 0.06123076, Validation loss: 0.12243534, Gradient norm: 1.65727151
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 3150.016s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.11493
INFO:root:EnergyScoreTrain: 0.08359
INFO:root:CoverageTrain: 0.75417
INFO:root:IntervalWidthTrain: 0.06924
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.12708
INFO:root:EnergyScoreValidation: 0.09309
INFO:root:CoverageValidation: 0.72509
INFO:root:IntervalWidthValidation: 0.06728
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.12886
INFO:root:EnergyScoreTest: 0.09454
INFO:root:CoverageTest: 0.71768
INFO:root:IntervalWidthTest: 0.0673
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 92274688
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26578039, Validation loss: 0.12943974, Gradient norm: 4.33126630
INFO:root:[    2] Training loss: 0.14783383, Validation loss: 0.11806073, Gradient norm: 2.94734974
INFO:root:[    3] Training loss: 0.13164819, Validation loss: 0.10659488, Gradient norm: 2.84617564
INFO:root:[    4] Training loss: 0.11404749, Validation loss: 0.13762666, Gradient norm: 2.44748733
INFO:root:[    5] Training loss: 0.11052466, Validation loss: 0.09361537, Gradient norm: 2.52584984
INFO:root:[    6] Training loss: 0.10380353, Validation loss: 0.10873744, Gradient norm: 2.09337082
INFO:root:[    7] Training loss: 0.10164644, Validation loss: 0.09543151, Gradient norm: 2.35287887
INFO:root:[    8] Training loss: 0.09910590, Validation loss: 0.11161538, Gradient norm: 2.46739587
INFO:root:[    9] Training loss: 0.09792520, Validation loss: 0.09270278, Gradient norm: 2.17513135
INFO:root:[   10] Training loss: 0.09350005, Validation loss: 0.10594587, Gradient norm: 2.19947814
INFO:root:[   11] Training loss: 0.09207790, Validation loss: 0.09402527, Gradient norm: 2.15253287
INFO:root:[   12] Training loss: 0.09186543, Validation loss: 0.10267629, Gradient norm: 1.96595062
INFO:root:[   13] Training loss: 0.08887513, Validation loss: 0.10761633, Gradient norm: 2.05052335
INFO:root:[   14] Training loss: 0.08994613, Validation loss: 0.13890623, Gradient norm: 2.27316268
INFO:root:[   15] Training loss: 0.08522055, Validation loss: 0.10688378, Gradient norm: 1.92711349
INFO:root:[   16] Training loss: 0.08569650, Validation loss: 0.10733243, Gradient norm: 2.07018252
INFO:root:[   17] Training loss: 0.08723163, Validation loss: 0.10064816, Gradient norm: 2.21561252
INFO:root:[   18] Training loss: 0.08378446, Validation loss: 0.09808347, Gradient norm: 2.10882317
INFO:root:[   19] Training loss: 0.08417657, Validation loss: 0.10644472, Gradient norm: 1.90457732
INFO:root:[   20] Training loss: 0.08179563, Validation loss: 0.11215505, Gradient norm: 1.93966124
INFO:root:[   21] Training loss: 0.08091296, Validation loss: 0.14568419, Gradient norm: 2.02444780
INFO:root:[   22] Training loss: 0.07957429, Validation loss: 0.12985733, Gradient norm: 1.89416452
INFO:root:[   23] Training loss: 0.07773883, Validation loss: 0.14640281, Gradient norm: 1.93270159
INFO:root:[   24] Training loss: 0.07720790, Validation loss: 0.11417715, Gradient norm: 1.70947791
INFO:root:[   25] Training loss: 0.07813251, Validation loss: 0.13436184, Gradient norm: 2.15988996
INFO:root:[   26] Training loss: 0.07656089, Validation loss: 0.12809856, Gradient norm: 1.93778900
INFO:root:[   27] Training loss: 0.07487544, Validation loss: 0.11582692, Gradient norm: 1.93814745
INFO:root:[   28] Training loss: 0.07319017, Validation loss: 0.12784714, Gradient norm: 1.80833927
INFO:root:[   29] Training loss: 0.07164726, Validation loss: 0.12777244, Gradient norm: 1.85882166
INFO:root:[   30] Training loss: 0.07413664, Validation loss: 0.13256838, Gradient norm: 2.00085730
INFO:root:[   31] Training loss: 0.07339842, Validation loss: 0.11839441, Gradient norm: 1.82248463
INFO:root:[   32] Training loss: 0.07168284, Validation loss: 0.16912057, Gradient norm: 2.05407496
INFO:root:[   33] Training loss: 0.07119160, Validation loss: 0.11919642, Gradient norm: 2.05702134
INFO:root:[   34] Training loss: 0.06883791, Validation loss: 0.12522159, Gradient norm: 1.71072072
INFO:root:[   35] Training loss: 0.06904933, Validation loss: 0.14165194, Gradient norm: 1.86153828
INFO:root:[   36] Training loss: 0.07011986, Validation loss: 0.11981836, Gradient norm: 1.87097689
INFO:root:[   37] Training loss: 0.06964700, Validation loss: 0.14047399, Gradient norm: 2.02663493
INFO:root:[   38] Training loss: 0.07023987, Validation loss: 0.14916530, Gradient norm: 1.85104174
INFO:root:[   39] Training loss: 0.06985341, Validation loss: 0.13786974, Gradient norm: 2.08393919
INFO:root:[   40] Training loss: 0.06787630, Validation loss: 0.13790474, Gradient norm: 1.80602730
INFO:root:[   41] Training loss: 0.06874475, Validation loss: 0.14385321, Gradient norm: 1.78813174
INFO:root:[   42] Training loss: 0.06631337, Validation loss: 0.14406440, Gradient norm: 1.89387800
INFO:root:[   43] Training loss: 0.06703805, Validation loss: 0.13532756, Gradient norm: 1.92099540
INFO:root:[   44] Training loss: 0.06595216, Validation loss: 0.14072522, Gradient norm: 1.75377266
INFO:root:[   45] Training loss: 0.06640439, Validation loss: 0.12839626, Gradient norm: 1.74870105
INFO:root:[   46] Training loss: 0.06732916, Validation loss: 0.12242400, Gradient norm: 1.90856137
INFO:root:[   47] Training loss: 0.06403920, Validation loss: 0.12337784, Gradient norm: 1.61929113
INFO:root:[   48] Training loss: 0.06455932, Validation loss: 0.12597614, Gradient norm: 1.83203298
INFO:root:[   49] Training loss: 0.06440200, Validation loss: 0.12925304, Gradient norm: 1.82027660
INFO:root:[   50] Training loss: 0.06292750, Validation loss: 0.13383899, Gradient norm: 1.63658190
INFO:root:[   51] Training loss: 0.06318909, Validation loss: 0.12894894, Gradient norm: 1.99072657
INFO:root:[   52] Training loss: 0.06327070, Validation loss: 0.13985989, Gradient norm: 1.52395133
INFO:root:[   53] Training loss: 0.06254692, Validation loss: 0.12618870, Gradient norm: 1.41355869
INFO:root:[   54] Training loss: 0.06356895, Validation loss: 0.13513013, Gradient norm: 1.89267990
INFO:root:[   55] Training loss: 0.06253776, Validation loss: 0.12781131, Gradient norm: 2.02054193
INFO:root:[   56] Training loss: 0.06301691, Validation loss: 0.12498695, Gradient norm: 1.77117020
INFO:root:[   57] Training loss: 0.06154295, Validation loss: 0.14413188, Gradient norm: 1.74678945
INFO:root:[   58] Training loss: 0.06189312, Validation loss: 0.13166482, Gradient norm: 1.66505560
INFO:root:[   59] Training loss: 0.06240058, Validation loss: 0.14089737, Gradient norm: 1.84814999
INFO:root:[   60] Training loss: 0.06257076, Validation loss: 0.13887484, Gradient norm: 1.70963053
INFO:root:[   61] Training loss: 0.06188960, Validation loss: 0.13350067, Gradient norm: 1.82544975
INFO:root:[   62] Training loss: 0.06027112, Validation loss: 0.14804760, Gradient norm: 1.74395827
INFO:root:[   63] Training loss: 0.06078113, Validation loss: 0.14904180, Gradient norm: 1.75113066
INFO:root:[   64] Training loss: 0.06137347, Validation loss: 0.12664988, Gradient norm: 1.74908578
INFO:root:[   65] Training loss: 0.06073637, Validation loss: 0.13205243, Gradient norm: 1.63864766
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 3070.414s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.12684
INFO:root:EnergyScoreTrain: 0.09413
INFO:root:CoverageTrain: 0.78281
INFO:root:IntervalWidthTrain: 0.06934
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.12423
INFO:root:EnergyScoreValidation: 0.09298
INFO:root:CoverageValidation: 0.81102
INFO:root:IntervalWidthValidation: 0.06955
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.12496
INFO:root:EnergyScoreTest: 0.0938
INFO:root:CoverageTest: 0.8088
INFO:root:IntervalWidthTest: 0.06973
INFO:root:###6 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 92274688
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.30448867, Validation loss: 0.15209071, Gradient norm: 5.79814082
INFO:root:[    2] Training loss: 0.15484838, Validation loss: 0.15352718, Gradient norm: 3.66033916
INFO:root:[    3] Training loss: 0.13829946, Validation loss: 0.11629505, Gradient norm: 3.05395835
INFO:root:[    4] Training loss: 0.12761236, Validation loss: 0.12368451, Gradient norm: 2.61440953
INFO:root:[    5] Training loss: 0.12090449, Validation loss: 0.13283504, Gradient norm: 2.29899432
INFO:root:[    6] Training loss: 0.11341223, Validation loss: 0.14180399, Gradient norm: 2.53489032
INFO:root:[    7] Training loss: 0.10748997, Validation loss: 0.11029157, Gradient norm: 2.34106423
INFO:root:[    8] Training loss: 0.10191822, Validation loss: 0.10390806, Gradient norm: 2.05387818
INFO:root:[    9] Training loss: 0.10019940, Validation loss: 0.10472525, Gradient norm: 2.21915709
INFO:root:[   10] Training loss: 0.09463439, Validation loss: 0.09516480, Gradient norm: 1.87774761
INFO:root:[   11] Training loss: 0.09481836, Validation loss: 0.10265664, Gradient norm: 2.17595046
INFO:root:[   12] Training loss: 0.09108259, Validation loss: 0.10403447, Gradient norm: 2.08467004
INFO:root:[   13] Training loss: 0.09287958, Validation loss: 0.08943155, Gradient norm: 1.90618242
INFO:root:[   14] Training loss: 0.09019876, Validation loss: 0.09186673, Gradient norm: 2.06951399
INFO:root:[   15] Training loss: 0.09004352, Validation loss: 0.11918352, Gradient norm: 2.32650272
INFO:root:[   16] Training loss: 0.08532403, Validation loss: 0.12729889, Gradient norm: 1.88926998
INFO:root:[   17] Training loss: 0.08448116, Validation loss: 0.10711558, Gradient norm: 1.83166207
INFO:root:[   18] Training loss: 0.08537343, Validation loss: 0.16138590, Gradient norm: 1.98211832
INFO:root:[   19] Training loss: 0.08362111, Validation loss: 0.11696527, Gradient norm: 1.91514373
INFO:root:[   20] Training loss: 0.08327117, Validation loss: 0.11620219, Gradient norm: 1.97801929
INFO:root:[   21] Training loss: 0.08006751, Validation loss: 0.14186508, Gradient norm: 1.76690178
INFO:root:[   22] Training loss: 0.07993441, Validation loss: 0.10002253, Gradient norm: 1.76169640
INFO:root:[   23] Training loss: 0.07876150, Validation loss: 0.11285132, Gradient norm: 1.91232391
INFO:root:[   24] Training loss: 0.07783702, Validation loss: 0.12904215, Gradient norm: 1.79206113
INFO:root:[   25] Training loss: 0.07771704, Validation loss: 0.11256473, Gradient norm: 1.70593568
INFO:root:[   26] Training loss: 0.07873629, Validation loss: 0.15691834, Gradient norm: 2.11228324
INFO:root:[   27] Training loss: 0.07641518, Validation loss: 0.11643146, Gradient norm: 2.06618385
INFO:root:[   28] Training loss: 0.07444313, Validation loss: 0.12684946, Gradient norm: 1.82763948
INFO:root:[   29] Training loss: 0.07383038, Validation loss: 0.13312896, Gradient norm: 1.73919847
INFO:root:[   30] Training loss: 0.07455444, Validation loss: 0.12893636, Gradient norm: 1.92554107
INFO:root:[   31] Training loss: 0.07196690, Validation loss: 0.14439949, Gradient norm: 1.95140633
INFO:root:[   32] Training loss: 0.07185497, Validation loss: 0.16945850, Gradient norm: 1.82519763
INFO:root:[   33] Training loss: 0.07192092, Validation loss: 0.11856952, Gradient norm: 1.99069720
INFO:root:[   34] Training loss: 0.07090931, Validation loss: 0.13293015, Gradient norm: 1.90590035
INFO:root:[   35] Training loss: 0.07113881, Validation loss: 0.13760362, Gradient norm: 1.95210163
INFO:root:[   36] Training loss: 0.06918706, Validation loss: 0.13183587, Gradient norm: 1.88755421
INFO:root:[   37] Training loss: 0.06908002, Validation loss: 0.13195153, Gradient norm: 1.60025900
INFO:root:[   38] Training loss: 0.06894257, Validation loss: 0.12671426, Gradient norm: 2.04088914
INFO:root:[   39] Training loss: 0.06918573, Validation loss: 0.12408730, Gradient norm: 1.82377393
INFO:root:[   40] Training loss: 0.06658456, Validation loss: 0.13719703, Gradient norm: 1.85358230
INFO:root:[   41] Training loss: 0.06728147, Validation loss: 0.13578637, Gradient norm: 1.79342106
INFO:root:[   42] Training loss: 0.06657075, Validation loss: 0.14556768, Gradient norm: 1.71343191
INFO:root:[   43] Training loss: 0.06636278, Validation loss: 0.14842114, Gradient norm: 1.86378832
INFO:root:[   44] Training loss: 0.06625722, Validation loss: 0.13960524, Gradient norm: 1.73954124
INFO:root:[   45] Training loss: 0.06547732, Validation loss: 0.14650821, Gradient norm: 1.87134049
INFO:root:[   46] Training loss: 0.06557859, Validation loss: 0.15210525, Gradient norm: 1.58898900
INFO:root:[   47] Training loss: 0.06434620, Validation loss: 0.15135528, Gradient norm: 1.63181247
INFO:root:[   48] Training loss: 0.06670607, Validation loss: 0.14785108, Gradient norm: 2.18388880
INFO:root:[   49] Training loss: 0.06456686, Validation loss: 0.13685437, Gradient norm: 2.05860289
INFO:root:[   50] Training loss: 0.06416085, Validation loss: 0.13014836, Gradient norm: 1.81006150
INFO:root:[   51] Training loss: 0.06401523, Validation loss: 0.13652497, Gradient norm: 1.82048686
INFO:root:[   52] Training loss: 0.06170098, Validation loss: 0.15439361, Gradient norm: 1.82607272
INFO:root:[   53] Training loss: 0.06404546, Validation loss: 0.12229526, Gradient norm: 1.84977534
INFO:root:[   54] Training loss: 0.06186837, Validation loss: 0.14814733, Gradient norm: 1.74004605
INFO:root:[   55] Training loss: 0.06297935, Validation loss: 0.14145888, Gradient norm: 1.84585576
INFO:root:[   56] Training loss: 0.06139093, Validation loss: 0.12642261, Gradient norm: 1.67799443
INFO:root:[   57] Training loss: 0.06105755, Validation loss: 0.13263419, Gradient norm: 1.66191844
INFO:root:[   58] Training loss: 0.06127318, Validation loss: 0.13418827, Gradient norm: 1.53862653
INFO:root:[   59] Training loss: 0.06248874, Validation loss: 0.14670148, Gradient norm: 1.84005194
INFO:root:[   60] Training loss: 0.06203634, Validation loss: 0.14108394, Gradient norm: 1.89659147
INFO:root:[   61] Training loss: 0.06258228, Validation loss: 0.14171835, Gradient norm: 2.03670430
INFO:root:[   62] Training loss: 0.06261001, Validation loss: 0.13309990, Gradient norm: 2.06961285
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 2927.253s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.12311
INFO:root:EnergyScoreTrain: 0.0898
INFO:root:CoverageTrain: 0.77036
INFO:root:IntervalWidthTrain: 0.07679
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.1237
INFO:root:EnergyScoreValidation: 0.09009
INFO:root:CoverageValidation: 0.74767
INFO:root:IntervalWidthValidation: 0.07008
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.12576
INFO:root:EnergyScoreTest: 0.09175
INFO:root:CoverageTest: 0.7382
INFO:root:IntervalWidthTest: 0.07027
INFO:root:###7 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234567, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 234881024
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28993614, Validation loss: 0.14514570, Gradient norm: 5.44629858
INFO:root:[    2] Training loss: 0.15222460, Validation loss: 0.12916773, Gradient norm: 3.00857780
INFO:root:[    3] Training loss: 0.13948296, Validation loss: 0.11984754, Gradient norm: 2.80923563
INFO:root:[    4] Training loss: 0.12184948, Validation loss: 0.11647782, Gradient norm: 2.44239500
INFO:root:[    5] Training loss: 0.11628719, Validation loss: 0.10158639, Gradient norm: 2.29557480
INFO:root:[    6] Training loss: 0.11399090, Validation loss: 0.12470749, Gradient norm: 2.48009965
INFO:root:[    7] Training loss: 0.10675657, Validation loss: 0.09878657, Gradient norm: 2.26673809
INFO:root:[    8] Training loss: 0.10377491, Validation loss: 0.10700615, Gradient norm: 2.12191752
INFO:root:[    9] Training loss: 0.10155682, Validation loss: 0.10180418, Gradient norm: 1.93303207
INFO:root:[   10] Training loss: 0.09936343, Validation loss: 0.11837794, Gradient norm: 2.22352410
INFO:root:[   11] Training loss: 0.09391066, Validation loss: 0.11364817, Gradient norm: 1.74175487
INFO:root:[   12] Training loss: 0.09426116, Validation loss: 0.13454604, Gradient norm: 2.07407308
INFO:root:[   13] Training loss: 0.09189775, Validation loss: 0.11233460, Gradient norm: 2.12119391
INFO:root:[   14] Training loss: 0.08867721, Validation loss: 0.10353704, Gradient norm: 1.83118253
INFO:root:[   15] Training loss: 0.08623836, Validation loss: 0.10090561, Gradient norm: 1.79491326
INFO:root:[   16] Training loss: 0.08750189, Validation loss: 0.11935214, Gradient norm: 1.79625257
INFO:root:[   17] Training loss: 0.08527141, Validation loss: 0.10574089, Gradient norm: 2.02180290
INFO:root:[   18] Training loss: 0.08376029, Validation loss: 0.12523402, Gradient norm: 1.91275302
INFO:root:[   19] Training loss: 0.08151755, Validation loss: 0.11564500, Gradient norm: 2.03022074
INFO:root:[   20] Training loss: 0.08167771, Validation loss: 0.11845596, Gradient norm: 1.68434318
INFO:root:[   21] Training loss: 0.08163711, Validation loss: 0.14200067, Gradient norm: 2.08496167
INFO:root:[   22] Training loss: 0.07958045, Validation loss: 0.12620146, Gradient norm: 2.01897653
INFO:root:[   23] Training loss: 0.07963085, Validation loss: 0.11586655, Gradient norm: 2.06140528
INFO:root:[   24] Training loss: 0.07691038, Validation loss: 0.15157307, Gradient norm: 1.85196703
INFO:root:[   25] Training loss: 0.07689888, Validation loss: 0.15766320, Gradient norm: 1.85770113
INFO:root:[   26] Training loss: 0.07717863, Validation loss: 0.14788598, Gradient norm: 1.91126556
INFO:root:[   27] Training loss: 0.07644806, Validation loss: 0.12832331, Gradient norm: 1.93887121
INFO:root:[   28] Training loss: 0.07411366, Validation loss: 0.13441368, Gradient norm: 1.90872329
INFO:root:[   29] Training loss: 0.07366565, Validation loss: 0.11288825, Gradient norm: 1.72081525
INFO:root:[   30] Training loss: 0.07351094, Validation loss: 0.14290667, Gradient norm: 1.77968870
INFO:root:[   31] Training loss: 0.07292621, Validation loss: 0.13154227, Gradient norm: 2.02065455
INFO:root:[   32] Training loss: 0.07230172, Validation loss: 0.11968573, Gradient norm: 1.88217283
INFO:root:[   33] Training loss: 0.07084982, Validation loss: 0.15213311, Gradient norm: 1.75773508
INFO:root:[   34] Training loss: 0.07305194, Validation loss: 0.13818679, Gradient norm: 2.09545553
INFO:root:[   35] Training loss: 0.07162546, Validation loss: 0.15357071, Gradient norm: 1.87837335
INFO:root:[   36] Training loss: 0.07210322, Validation loss: 0.12811988, Gradient norm: 1.95378327
INFO:root:[   37] Training loss: 0.07164832, Validation loss: 0.13390557, Gradient norm: 2.10043682
INFO:root:[   38] Training loss: 0.07030596, Validation loss: 0.13692430, Gradient norm: 1.87519654
INFO:root:[   39] Training loss: 0.07140737, Validation loss: 0.14348851, Gradient norm: 2.09771024
INFO:root:[   40] Training loss: 0.06970436, Validation loss: 0.13717718, Gradient norm: 1.85694474
INFO:root:[   41] Training loss: 0.06971887, Validation loss: 0.15456886, Gradient norm: 1.93203183
INFO:root:[   42] Training loss: 0.06859264, Validation loss: 0.12884063, Gradient norm: 1.97511243
INFO:root:[   43] Training loss: 0.06858624, Validation loss: 0.14318710, Gradient norm: 1.55901345
INFO:root:[   44] Training loss: 0.06852453, Validation loss: 0.14280157, Gradient norm: 2.02654172
INFO:root:[   45] Training loss: 0.06801932, Validation loss: 0.13950274, Gradient norm: 1.84530162
INFO:root:[   46] Training loss: 0.06679288, Validation loss: 0.14420170, Gradient norm: 1.76145344
INFO:root:[   47] Training loss: 0.06437547, Validation loss: 0.16225688, Gradient norm: 1.50869376
INFO:root:[   48] Training loss: 0.06782750, Validation loss: 0.13650794, Gradient norm: 1.94948123
INFO:root:[   49] Training loss: 0.06691699, Validation loss: 0.14853338, Gradient norm: 1.68489152
INFO:root:[   50] Training loss: 0.06514220, Validation loss: 0.14162350, Gradient norm: 1.81115154
INFO:root:[   51] Training loss: 0.06550775, Validation loss: 0.14407053, Gradient norm: 1.91947482
INFO:root:[   52] Training loss: 0.06482581, Validation loss: 0.15443624, Gradient norm: 1.86240787
INFO:root:[   53] Training loss: 0.06483724, Validation loss: 0.14088488, Gradient norm: 1.74022833
INFO:root:[   54] Training loss: 0.06327548, Validation loss: 0.15150308, Gradient norm: 1.75384533
INFO:root:[   55] Training loss: 0.06404281, Validation loss: 0.14597477, Gradient norm: 1.71109421
INFO:root:[   56] Training loss: 0.06369062, Validation loss: 0.14957142, Gradient norm: 1.63189179
INFO:root:[   57] Training loss: 0.06513818, Validation loss: 0.14180453, Gradient norm: 1.89255273
INFO:root:[   58] Training loss: 0.06238839, Validation loss: 0.13206306, Gradient norm: 1.61347172
INFO:root:[   59] Training loss: 0.06248612, Validation loss: 0.16758439, Gradient norm: 1.75446319
INFO:root:[   60] Training loss: 0.06286775, Validation loss: 0.14686543, Gradient norm: 1.74202293
INFO:root:[   61] Training loss: 0.06305652, Validation loss: 0.13348672, Gradient norm: 1.79634073
INFO:root:[   62] Training loss: 0.06333014, Validation loss: 0.13972458, Gradient norm: 1.83490214
INFO:root:[   63] Training loss: 0.06105088, Validation loss: 0.13636457, Gradient norm: 1.46051323
INFO:root:[   64] Training loss: 0.06425537, Validation loss: 0.13989787, Gradient norm: 1.87038302
INFO:root:[   65] Training loss: 0.06410139, Validation loss: 0.14964764, Gradient norm: 1.75897905
INFO:root:[   66] Training loss: 0.06263861, Validation loss: 0.12837976, Gradient norm: 1.73159658
INFO:root:[   67] Training loss: 0.06215294, Validation loss: 0.14989373, Gradient norm: 1.74686864
INFO:root:[   68] Training loss: 0.06180801, Validation loss: 0.16014630, Gradient norm: 1.72285065
INFO:root:[   69] Training loss: 0.06178860, Validation loss: 0.15528084, Gradient norm: 1.83549129
INFO:root:[   70] Training loss: 0.06229990, Validation loss: 0.16388225, Gradient norm: 1.63993394
INFO:root:[   71] Training loss: 0.06141771, Validation loss: 0.15710751, Gradient norm: 1.67885351
INFO:root:[   72] Training loss: 0.06170590, Validation loss: 0.15264029, Gradient norm: 1.95476588
INFO:root:[   73] Training loss: 0.06280905, Validation loss: 0.14796369, Gradient norm: 1.71701698
INFO:root:[   74] Training loss: 0.06132116, Validation loss: 0.13341484, Gradient norm: 1.60610506
INFO:root:[   75] Training loss: 0.06109654, Validation loss: 0.15237695, Gradient norm: 1.59701629
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 3545.804s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.11612
INFO:root:EnergyScoreTrain: 0.08523
INFO:root:CoverageTrain: 0.71726
INFO:root:IntervalWidthTrain: 0.05676
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.13467
INFO:root:EnergyScoreValidation: 0.09912
INFO:root:CoverageValidation: 0.66871
INFO:root:IntervalWidthValidation: 0.05697
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.13622
INFO:root:EnergyScoreTest: 0.10055
INFO:root:CoverageTest: 0.66139
INFO:root:IntervalWidthTest: 0.05684
INFO:root:###8 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345678, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 234881024
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.32432691, Validation loss: 0.15916899, Gradient norm: 5.83711184
INFO:root:[    2] Training loss: 0.15657555, Validation loss: 0.12604369, Gradient norm: 3.60017236
INFO:root:[    3] Training loss: 0.14205522, Validation loss: 0.11640418, Gradient norm: 3.91832038
INFO:root:[    4] Training loss: 0.13151611, Validation loss: 0.13349885, Gradient norm: 3.17210518
INFO:root:[    5] Training loss: 0.12323316, Validation loss: 0.11145482, Gradient norm: 2.62290635
INFO:root:[    6] Training loss: 0.11570179, Validation loss: 0.10518780, Gradient norm: 2.93237608
INFO:root:[    7] Training loss: 0.10844646, Validation loss: 0.11360444, Gradient norm: 2.45075615
INFO:root:[    8] Training loss: 0.10819881, Validation loss: 0.09205750, Gradient norm: 2.91225231
INFO:root:[    9] Training loss: 0.10152117, Validation loss: 0.10506333, Gradient norm: 2.43414478
INFO:root:[   10] Training loss: 0.09514368, Validation loss: 0.11501713, Gradient norm: 2.33161995
INFO:root:[   11] Training loss: 0.09456350, Validation loss: 0.11378806, Gradient norm: 2.31080933
INFO:root:[   12] Training loss: 0.09163600, Validation loss: 0.10113009, Gradient norm: 2.24848114
INFO:root:[   13] Training loss: 0.08896853, Validation loss: 0.13864936, Gradient norm: 1.88142226
INFO:root:[   14] Training loss: 0.08825178, Validation loss: 0.13397483, Gradient norm: 2.27132302
INFO:root:[   15] Training loss: 0.08786435, Validation loss: 0.09568992, Gradient norm: 2.21860609
INFO:root:[   16] Training loss: 0.08658986, Validation loss: 0.10636493, Gradient norm: 2.11164504
INFO:root:[   17] Training loss: 0.08281674, Validation loss: 0.11418317, Gradient norm: 1.87572746
INFO:root:[   18] Training loss: 0.08435430, Validation loss: 0.11123916, Gradient norm: 2.15804561
INFO:root:[   19] Training loss: 0.08232561, Validation loss: 0.13832315, Gradient norm: 1.88676605
INFO:root:[   20] Training loss: 0.07939934, Validation loss: 0.12556327, Gradient norm: 1.79672109
INFO:root:[   21] Training loss: 0.07659465, Validation loss: 0.15206660, Gradient norm: 1.85904530
INFO:root:[   22] Training loss: 0.07999551, Validation loss: 0.12074028, Gradient norm: 2.10123620
INFO:root:[   23] Training loss: 0.07882486, Validation loss: 0.12701638, Gradient norm: 2.03221410
INFO:root:[   24] Training loss: 0.07592576, Validation loss: 0.14142725, Gradient norm: 1.77832988
INFO:root:[   25] Training loss: 0.07677086, Validation loss: 0.12656365, Gradient norm: 2.08143720
INFO:root:[   26] Training loss: 0.07562375, Validation loss: 0.16265889, Gradient norm: 1.80300019
INFO:root:[   27] Training loss: 0.07319150, Validation loss: 0.15822401, Gradient norm: 1.91593846
INFO:root:[   28] Training loss: 0.07418640, Validation loss: 0.13069377, Gradient norm: 2.06654392
INFO:root:[   29] Training loss: 0.07403440, Validation loss: 0.13660437, Gradient norm: 2.06418457
INFO:root:[   30] Training loss: 0.07490072, Validation loss: 0.14251367, Gradient norm: 2.01361235
INFO:root:[   31] Training loss: 0.07227713, Validation loss: 0.15121240, Gradient norm: 1.87410046
INFO:root:[   32] Training loss: 0.07253366, Validation loss: 0.14534273, Gradient norm: 1.91310478
INFO:root:[   33] Training loss: 0.07183563, Validation loss: 0.14815384, Gradient norm: 1.95007208
INFO:root:[   34] Training loss: 0.07232999, Validation loss: 0.12573240, Gradient norm: 2.08547355
INFO:root:[   35] Training loss: 0.07141175, Validation loss: 0.13993175, Gradient norm: 1.82877272
INFO:root:[   36] Training loss: 0.07263787, Validation loss: 0.12041689, Gradient norm: 2.17052590
INFO:root:[   37] Training loss: 0.06813645, Validation loss: 0.13610699, Gradient norm: 1.57184605
INFO:root:[   38] Training loss: 0.06766338, Validation loss: 0.13374208, Gradient norm: 1.74093202
INFO:root:[   39] Training loss: 0.06783941, Validation loss: 0.14875193, Gradient norm: 1.80700556
INFO:root:[   40] Training loss: 0.06721290, Validation loss: 0.16044333, Gradient norm: 1.85324923
INFO:root:[   41] Training loss: 0.06702770, Validation loss: 0.13927653, Gradient norm: 1.87648487
INFO:root:[   42] Training loss: 0.06705401, Validation loss: 0.14853441, Gradient norm: 1.94296040
INFO:root:[   43] Training loss: 0.06691824, Validation loss: 0.12404544, Gradient norm: 1.94102372
INFO:root:[   44] Training loss: 0.06693373, Validation loss: 0.14195123, Gradient norm: 1.89821986
INFO:root:[   45] Training loss: 0.06582772, Validation loss: 0.14790492, Gradient norm: 1.76796256
INFO:root:[   46] Training loss: 0.06614284, Validation loss: 0.13583850, Gradient norm: 1.82909505
INFO:root:[   47] Training loss: 0.06537900, Validation loss: 0.15637698, Gradient norm: 1.67218795
INFO:root:[   48] Training loss: 0.06604996, Validation loss: 0.13859754, Gradient norm: 1.92024993
INFO:root:[   49] Training loss: 0.06584578, Validation loss: 0.13022153, Gradient norm: 1.68786460
INFO:root:[   50] Training loss: 0.06513968, Validation loss: 0.16011598, Gradient norm: 1.70034658
INFO:root:[   51] Training loss: 0.06591015, Validation loss: 0.15024226, Gradient norm: 1.91999839
INFO:root:[   52] Training loss: 0.06416711, Validation loss: 0.14404438, Gradient norm: 1.62245074
INFO:root:[   53] Training loss: 0.06290791, Validation loss: 0.14960972, Gradient norm: 1.76509705
INFO:root:[   54] Training loss: 0.06435941, Validation loss: 0.15242525, Gradient norm: 1.97193814
INFO:root:[   55] Training loss: 0.06269984, Validation loss: 0.13592562, Gradient norm: 1.75382174
INFO:root:[   56] Training loss: 0.06287156, Validation loss: 0.14839604, Gradient norm: 1.58225236
INFO:root:[   57] Training loss: 0.06279357, Validation loss: 0.12852769, Gradient norm: 1.77145305
INFO:root:[   58] Training loss: 0.06379670, Validation loss: 0.13323888, Gradient norm: 1.79977019
INFO:root:[   59] Training loss: 0.06217168, Validation loss: 0.14209780, Gradient norm: 1.82618497
INFO:root:[   60] Training loss: 0.06199896, Validation loss: 0.13491307, Gradient norm: 1.75010336
INFO:root:[   61] Training loss: 0.06343783, Validation loss: 0.14856424, Gradient norm: 1.75657205
INFO:root:[   62] Training loss: 0.06272426, Validation loss: 0.12758521, Gradient norm: 1.80771476
INFO:root:[   63] Training loss: 0.06134304, Validation loss: 0.13383456, Gradient norm: 1.66991322
INFO:root:[   64] Training loss: 0.06207132, Validation loss: 0.13228587, Gradient norm: 1.71506365
INFO:root:[   65] Training loss: 0.06124095, Validation loss: 0.13110293, Gradient norm: 1.77095224
INFO:root:[   66] Training loss: 0.06162364, Validation loss: 0.13303098, Gradient norm: 1.88619481
INFO:root:[   67] Training loss: 0.05977727, Validation loss: 0.14391335, Gradient norm: 1.68377526
INFO:root:[   68] Training loss: 0.06119525, Validation loss: 0.15188643, Gradient norm: 1.85509943
INFO:root:[   69] Training loss: 0.18676262, Validation loss: 1.11171226, Gradient norm: 4.64879678
INFO:root:[   70] Training loss: 0.62850014, Validation loss: 0.59438884, Gradient norm: 4.51865763
INFO:root:[   71] Training loss: 0.59601107, Validation loss: 0.59385569, Gradient norm: 0.46657920
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 3400.424s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.1141
INFO:root:EnergyScoreTrain: 0.08305
INFO:root:CoverageTrain: 0.76107
INFO:root:IntervalWidthTrain: 0.06053
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.12597
INFO:root:EnergyScoreValidation: 0.09268
INFO:root:CoverageValidation: 0.70918
INFO:root:IntervalWidthValidation: 0.05769
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.12719
INFO:root:EnergyScoreTest: 0.09385
INFO:root:CoverageTest: 0.70138
INFO:root:IntervalWidthTest: 0.05762
INFO:root:###9 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456789, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 92274688
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28346190, Validation loss: 0.13510431, Gradient norm: 4.75791312
INFO:root:[    2] Training loss: 0.15826192, Validation loss: 0.12879047, Gradient norm: 3.75722218
INFO:root:[    3] Training loss: 0.13308657, Validation loss: 0.10661633, Gradient norm: 2.98173282
INFO:root:[    4] Training loss: 0.12377186, Validation loss: 0.14304982, Gradient norm: 2.72561973
INFO:root:[    5] Training loss: 0.11547787, Validation loss: 0.10317763, Gradient norm: 2.59279510
INFO:root:[    6] Training loss: 0.10834027, Validation loss: 0.10821325, Gradient norm: 2.10344679
INFO:root:[    7] Training loss: 0.10751710, Validation loss: 0.10884778, Gradient norm: 2.12491815
INFO:root:[    8] Training loss: 0.10051021, Validation loss: 0.10289052, Gradient norm: 2.03720240
INFO:root:[    9] Training loss: 0.10160657, Validation loss: 0.09562032, Gradient norm: 2.44003385
INFO:root:[   10] Training loss: 0.09712878, Validation loss: 0.10918234, Gradient norm: 2.43933567
INFO:root:[   11] Training loss: 0.09180106, Validation loss: 0.10254171, Gradient norm: 1.88827885
INFO:root:[   12] Training loss: 0.09231422, Validation loss: 0.11755360, Gradient norm: 2.22591259
INFO:root:[   13] Training loss: 0.08789813, Validation loss: 0.11529590, Gradient norm: 1.82476828
INFO:root:[   14] Training loss: 0.08758830, Validation loss: 0.10911542, Gradient norm: 1.87077598
INFO:root:[   15] Training loss: 0.08459906, Validation loss: 0.09406541, Gradient norm: 1.92707845
INFO:root:[   16] Training loss: 0.08691432, Validation loss: 0.10306374, Gradient norm: 2.16397594
INFO:root:[   17] Training loss: 0.08246081, Validation loss: 0.11937874, Gradient norm: 1.88016722
INFO:root:[   18] Training loss: 0.08258796, Validation loss: 0.12346536, Gradient norm: 2.00833618
INFO:root:[   19] Training loss: 0.08259041, Validation loss: 0.10003762, Gradient norm: 1.95475183
INFO:root:[   20] Training loss: 0.08118902, Validation loss: 0.10686393, Gradient norm: 2.04286089
INFO:root:[   21] Training loss: 0.08127977, Validation loss: 0.12589316, Gradient norm: 2.13108434
INFO:root:[   22] Training loss: 0.07816626, Validation loss: 0.11814236, Gradient norm: 1.98430500
INFO:root:[   23] Training loss: 0.08050517, Validation loss: 0.11550887, Gradient norm: 2.08703483
INFO:root:[   24] Training loss: 0.07604181, Validation loss: 0.13165441, Gradient norm: 1.92200687
INFO:root:[   25] Training loss: 0.07444843, Validation loss: 0.12998371, Gradient norm: 1.85291111
INFO:root:[   26] Training loss: 0.07553002, Validation loss: 0.15013434, Gradient norm: 1.88920111
INFO:root:[   27] Training loss: 0.07460083, Validation loss: 0.13188011, Gradient norm: 2.01657526
INFO:root:[   28] Training loss: 0.07466654, Validation loss: 0.16112687, Gradient norm: 2.03268022
INFO:root:[   29] Training loss: 0.07508305, Validation loss: 0.14460662, Gradient norm: 2.24359161
INFO:root:[   30] Training loss: 0.07197189, Validation loss: 0.14182108, Gradient norm: 1.72586671
INFO:root:[   31] Training loss: 0.07210918, Validation loss: 0.14854187, Gradient norm: 1.97974489
INFO:root:[   32] Training loss: 0.07102928, Validation loss: 0.14690297, Gradient norm: 1.97985486
INFO:root:[   33] Training loss: 0.07002297, Validation loss: 0.13761255, Gradient norm: 1.93892762
INFO:root:[   34] Training loss: 0.07125806, Validation loss: 0.14538132, Gradient norm: 2.12828884
INFO:root:[   35] Training loss: 0.07018567, Validation loss: 0.14612237, Gradient norm: 1.96572379
INFO:root:[   36] Training loss: 0.06861152, Validation loss: 0.13864455, Gradient norm: 1.64274673
INFO:root:[   37] Training loss: 0.06818552, Validation loss: 0.14936116, Gradient norm: 1.91157533
INFO:root:[   38] Training loss: 0.06702672, Validation loss: 0.13762951, Gradient norm: 1.80035777
INFO:root:[   39] Training loss: 0.06804142, Validation loss: 0.14225904, Gradient norm: 1.73614690
INFO:root:[   40] Training loss: 0.06942519, Validation loss: 0.12605238, Gradient norm: 1.95522218
INFO:root:[   41] Training loss: 0.06826759, Validation loss: 0.13470177, Gradient norm: 1.89299902
INFO:root:[   42] Training loss: 0.06703381, Validation loss: 0.12672758, Gradient norm: 1.67241100
INFO:root:[   43] Training loss: 0.06800390, Validation loss: 0.15478730, Gradient norm: 1.80520717
INFO:root:[   44] Training loss: 0.06798010, Validation loss: 0.15146993, Gradient norm: 2.08966669
INFO:root:[   45] Training loss: 0.06552157, Validation loss: 0.13952102, Gradient norm: 1.75835807
INFO:root:[   46] Training loss: 0.06695477, Validation loss: 0.14674753, Gradient norm: 1.84639488
INFO:root:[   47] Training loss: 0.06555342, Validation loss: 0.16322708, Gradient norm: 1.79122027
INFO:root:[   48] Training loss: 0.06511359, Validation loss: 0.12861625, Gradient norm: 1.76072666
INFO:root:[   49] Training loss: 0.06492630, Validation loss: 0.14206876, Gradient norm: 1.97685602
INFO:root:[   50] Training loss: 0.06492417, Validation loss: 0.13314213, Gradient norm: 1.68264435
INFO:root:[   51] Training loss: 0.06519875, Validation loss: 0.14770639, Gradient norm: 1.82785716
INFO:root:[   52] Training loss: 0.06383947, Validation loss: 0.12842834, Gradient norm: 1.67510125
INFO:root:[   53] Training loss: 0.06265033, Validation loss: 0.13096338, Gradient norm: 1.75190660
INFO:root:[   54] Training loss: 0.06357017, Validation loss: 0.13303319, Gradient norm: 1.83000374
INFO:root:[   55] Training loss: 0.06401267, Validation loss: 0.13061023, Gradient norm: 1.68947834
INFO:root:[   56] Training loss: 0.06233838, Validation loss: 0.12793022, Gradient norm: 1.60797548
INFO:root:[   57] Training loss: 0.06283573, Validation loss: 0.13098320, Gradient norm: 1.62726140
INFO:root:[   58] Training loss: 0.06294861, Validation loss: 0.14472643, Gradient norm: 1.84479018
INFO:root:[   59] Training loss: 0.06299020, Validation loss: 0.14938489, Gradient norm: 1.63892420
INFO:root:[   60] Training loss: 0.06220353, Validation loss: 0.13914770, Gradient norm: 1.77921549
INFO:root:[   61] Training loss: 0.06189502, Validation loss: 0.13294260, Gradient norm: 1.62571577
INFO:root:[   62] Training loss: 0.06225158, Validation loss: 0.13577283, Gradient norm: 1.86738365
INFO:root:[   63] Training loss: 0.06339167, Validation loss: 0.14495634, Gradient norm: 1.82806290
INFO:root:[   64] Training loss: 0.06299140, Validation loss: 0.15344577, Gradient norm: 1.76680365
INFO:root:[   65] Training loss: 0.06195015, Validation loss: 0.15553012, Gradient norm: 1.96503198
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 3070.326s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.11987
INFO:root:EnergyScoreTrain: 0.08671
INFO:root:CoverageTrain: 0.76325
INFO:root:IntervalWidthTrain: 0.0734
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.12878
INFO:root:EnergyScoreValidation: 0.09416
INFO:root:CoverageValidation: 0.7402
INFO:root:IntervalWidthValidation: 0.07192
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.12991
INFO:root:EnergyScoreTest: 0.09517
INFO:root:CoverageTest: 0.73598
INFO:root:IntervalWidthTest: 0.07183
