INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.31040961, Validation loss: 0.25833474, Gradient norm: 2.87249399
INFO:root:[    2] Training loss: 0.19560616, Validation loss: 0.23311474, Gradient norm: 3.78294739
INFO:root:[    3] Training loss: 0.15999011, Validation loss: 0.20990294, Gradient norm: 3.11803294
INFO:root:[    4] Training loss: 0.15221473, Validation loss: 0.23548488, Gradient norm: 3.38885019
INFO:root:[    5] Training loss: 0.14181627, Validation loss: 0.20271828, Gradient norm: 3.19843992
INFO:root:[    6] Training loss: 0.14025901, Validation loss: 0.25343495, Gradient norm: 3.45331651
INFO:root:[    7] Training loss: 0.13655394, Validation loss: 0.16170690, Gradient norm: 3.46290426
INFO:root:[    8] Training loss: 0.12587773, Validation loss: 0.17037609, Gradient norm: 2.91902187
INFO:root:[    9] Training loss: 0.12429395, Validation loss: 0.20162506, Gradient norm: 2.97438471
INFO:root:[   10] Training loss: 0.12009075, Validation loss: 0.15518343, Gradient norm: 2.67898776
INFO:root:[   11] Training loss: 0.11642768, Validation loss: 0.14233690, Gradient norm: 2.57072846
INFO:root:[   12] Training loss: 0.11822489, Validation loss: 0.19364941, Gradient norm: 3.40294943
INFO:root:[   13] Training loss: 0.11120236, Validation loss: 0.16419982, Gradient norm: 2.63790749
INFO:root:[   14] Training loss: 0.10721533, Validation loss: 0.20669059, Gradient norm: 2.28281329
INFO:root:[   15] Training loss: 0.11175569, Validation loss: 0.18020631, Gradient norm: 2.87042812
INFO:root:[   16] Training loss: 0.10616645, Validation loss: 0.17363651, Gradient norm: 2.58419693
INFO:root:[   17] Training loss: 0.11089077, Validation loss: 0.16500456, Gradient norm: 2.81838464
INFO:root:[   18] Training loss: 0.10372173, Validation loss: 0.14759873, Gradient norm: 2.35721974
INFO:root:[   19] Training loss: 0.10313940, Validation loss: 0.18868367, Gradient norm: 2.71550083
INFO:root:[   20] Training loss: 0.09664871, Validation loss: 0.15742841, Gradient norm: 2.35698545
INFO:root:[   21] Training loss: 0.09621235, Validation loss: 0.19834795, Gradient norm: 2.49208492
INFO:root:[   22] Training loss: 0.09578819, Validation loss: 0.14006233, Gradient norm: 2.46521693
INFO:root:[   23] Training loss: 0.09064518, Validation loss: 0.16463304, Gradient norm: 2.26598300
INFO:root:[   24] Training loss: 0.09238564, Validation loss: 0.17871368, Gradient norm: 2.36690969
INFO:root:[   25] Training loss: 0.09281879, Validation loss: 0.24296790, Gradient norm: 2.02642199
INFO:root:[   26] Training loss: 0.09157257, Validation loss: 0.15373472, Gradient norm: 2.17961300
INFO:root:[   27] Training loss: 0.08657445, Validation loss: 0.23692450, Gradient norm: 1.94272014
INFO:root:[   28] Training loss: 0.08570680, Validation loss: 0.17631488, Gradient norm: 1.96552231
INFO:root:[   29] Training loss: 0.08511190, Validation loss: 0.18240161, Gradient norm: 2.07295774
INFO:root:[   30] Training loss: 0.08867695, Validation loss: 0.22963048, Gradient norm: 2.11061822
INFO:root:[   31] Training loss: 0.08636495, Validation loss: 0.16515870, Gradient norm: 2.16514338
INFO:root:[   32] Training loss: 0.08005000, Validation loss: 0.17235374, Gradient norm: 1.83771476
INFO:root:[   33] Training loss: 0.08068156, Validation loss: 0.19705362, Gradient norm: 1.78118503
INFO:root:[   34] Training loss: 0.08443083, Validation loss: 0.18823914, Gradient norm: 2.15424367
INFO:root:[   35] Training loss: 0.08285973, Validation loss: 0.15710531, Gradient norm: 2.03777543
INFO:root:[   36] Training loss: 0.08219302, Validation loss: 0.17844880, Gradient norm: 2.01097150
INFO:root:[   37] Training loss: 0.07674164, Validation loss: 0.17962906, Gradient norm: 1.58589901
INFO:root:[   38] Training loss: 0.08200267, Validation loss: 0.22247139, Gradient norm: 1.85710596
INFO:root:[   39] Training loss: 0.08297068, Validation loss: 0.16045743, Gradient norm: 2.10320578
INFO:root:[   40] Training loss: 0.08182625, Validation loss: 0.15991201, Gradient norm: 1.88634321
INFO:root:[   41] Training loss: 0.07614424, Validation loss: 0.18215893, Gradient norm: 1.78434486
INFO:root:[   42] Training loss: 0.07741739, Validation loss: 0.20349438, Gradient norm: 1.75446045
INFO:root:[   43] Training loss: 0.07301434, Validation loss: 0.15583983, Gradient norm: 1.68518075
INFO:root:[   44] Training loss: 0.07417400, Validation loss: 0.17043152, Gradient norm: 1.61627055
INFO:root:[   45] Training loss: 0.07579005, Validation loss: 0.18590373, Gradient norm: 1.61660779
INFO:root:[   46] Training loss: 0.07720928, Validation loss: 0.17310946, Gradient norm: 1.79195618
INFO:root:[   47] Training loss: 0.07446879, Validation loss: 0.21162777, Gradient norm: 1.72252321
INFO:root:[   48] Training loss: 0.07324479, Validation loss: 0.22849775, Gradient norm: 1.55229221
INFO:root:[   49] Training loss: 0.07433832, Validation loss: 0.19064562, Gradient norm: 1.64160663
INFO:root:[   50] Training loss: 0.07058969, Validation loss: 0.18495270, Gradient norm: 1.54835205
INFO:root:[   51] Training loss: 0.07511266, Validation loss: 0.17346220, Gradient norm: 1.53411003
INFO:root:[   52] Training loss: 0.07139176, Validation loss: 0.18833966, Gradient norm: 1.70303534
INFO:root:[   53] Training loss: 0.07241361, Validation loss: 0.17480854, Gradient norm: 1.50462391
INFO:root:[   54] Training loss: 0.07013217, Validation loss: 0.16988534, Gradient norm: 1.46540778
INFO:root:[   55] Training loss: 0.07272112, Validation loss: 0.20509270, Gradient norm: 1.85054185
INFO:root:[   56] Training loss: 0.06985830, Validation loss: 0.19040134, Gradient norm: 1.50220632
INFO:root:[   57] Training loss: 0.07220997, Validation loss: 0.18156603, Gradient norm: 1.40459152
INFO:root:[   58] Training loss: 0.06865176, Validation loss: 0.19520554, Gradient norm: 1.52829937
INFO:root:[   59] Training loss: 0.07121505, Validation loss: 0.19311428, Gradient norm: 1.54910866
INFO:root:[   60] Training loss: 0.06911757, Validation loss: 0.20178043, Gradient norm: 1.49563593
INFO:root:[   61] Training loss: 0.07041002, Validation loss: 0.18566081, Gradient norm: 1.63419980
INFO:root:[   62] Training loss: 0.06679171, Validation loss: 0.20825793, Gradient norm: 1.53027287
INFO:root:[   63] Training loss: 0.06760144, Validation loss: 0.18022804, Gradient norm: 1.43744089
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 5212.23s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.1894
INFO:root:EnergyScoreTrain: 0.10836
INFO:root:CRPSTrain: 0.08401
INFO:root:Gaussian NLLTrain: -0.69749
INFO:root:CoverageTrain: 0.80785
INFO:root:IntervalWidthTrain: 0.52224
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.21301
INFO:root:EnergyScoreValidation: 0.14163
INFO:root:CRPSValidation: 0.11869
INFO:root:Gaussian NLLValidation: 3.72144
INFO:root:CoverageValidation: 0.46623
INFO:root:IntervalWidthValidation: 0.24638
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21585
INFO:root:EnergyScoreTest: 0.14578
INFO:root:CRPSTest: 0.12239
INFO:root:Gaussian NLLTest: 3.89579
INFO:root:CoverageTest: 0.46506
INFO:root:IntervalWidthTest: 0.24784
INFO:root:###2 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 150994944
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28301709, Validation loss: 0.21288789, Gradient norm: 2.92739536
INFO:root:[    2] Training loss: 0.18983264, Validation loss: 0.18679302, Gradient norm: 3.26481635
INFO:root:[    3] Training loss: 0.16131222, Validation loss: 0.19182134, Gradient norm: 2.75449570
INFO:root:[    4] Training loss: 0.15275242, Validation loss: 0.15490944, Gradient norm: 2.80749631
INFO:root:[    5] Training loss: 0.13784033, Validation loss: 0.18337187, Gradient norm: 2.47056268
INFO:root:[    6] Training loss: 0.13911266, Validation loss: 0.15226758, Gradient norm: 2.76740865
INFO:root:[    7] Training loss: 0.13391080, Validation loss: 0.14967783, Gradient norm: 2.83140129
INFO:root:[    8] Training loss: 0.12174187, Validation loss: 0.19116759, Gradient norm: 2.34695462
INFO:root:[    9] Training loss: 0.12308158, Validation loss: 0.14880446, Gradient norm: 2.39192100
INFO:root:[   10] Training loss: 0.11659152, Validation loss: 0.14299186, Gradient norm: 2.11268633
INFO:root:[   11] Training loss: 0.11429591, Validation loss: 0.14165140, Gradient norm: 2.18755924
INFO:root:[   12] Training loss: 0.11165439, Validation loss: 0.15944545, Gradient norm: 2.14949507
INFO:root:[   13] Training loss: 0.11285334, Validation loss: 0.19162927, Gradient norm: 2.24407109
INFO:root:[   14] Training loss: 0.11455907, Validation loss: 0.19022987, Gradient norm: 2.19921520
INFO:root:[   15] Training loss: 0.10413483, Validation loss: 0.21003274, Gradient norm: 1.95868371
INFO:root:[   16] Training loss: 0.11126136, Validation loss: 0.14272982, Gradient norm: 2.40065088
INFO:root:[   17] Training loss: 0.10580737, Validation loss: 0.18994231, Gradient norm: 2.37597219
INFO:root:[   18] Training loss: 0.10092102, Validation loss: 0.14997108, Gradient norm: 2.04777866
INFO:root:[   19] Training loss: 0.09423043, Validation loss: 0.21355222, Gradient norm: 1.81873016
INFO:root:[   20] Training loss: 0.09373746, Validation loss: 0.14154625, Gradient norm: 1.79302953
INFO:root:[   21] Training loss: 0.09379695, Validation loss: 0.17135324, Gradient norm: 1.90612124
INFO:root:[   22] Training loss: 0.09216953, Validation loss: 0.14061930, Gradient norm: 1.91935436
INFO:root:[   23] Training loss: 0.08717445, Validation loss: 0.19552945, Gradient norm: 1.66391053
INFO:root:[   24] Training loss: 0.08818108, Validation loss: 0.21697797, Gradient norm: 1.78082304
INFO:root:[   25] Training loss: 0.08868535, Validation loss: 0.16947216, Gradient norm: 1.76853999
INFO:root:[   26] Training loss: 0.08899748, Validation loss: 0.16478789, Gradient norm: 1.93270400
INFO:root:[   27] Training loss: 0.08389178, Validation loss: 0.22347713, Gradient norm: 1.57447170
INFO:root:[   28] Training loss: 0.08355908, Validation loss: 0.17684040, Gradient norm: 1.59571940
INFO:root:[   29] Training loss: 0.08442873, Validation loss: 0.17174481, Gradient norm: 1.69374122
INFO:root:[   30] Training loss: 0.08649275, Validation loss: 0.21608428, Gradient norm: 1.77447260
INFO:root:[   31] Training loss: 0.08268766, Validation loss: 0.15297691, Gradient norm: 1.70404497
INFO:root:[   32] Training loss: 0.08161777, Validation loss: 0.19167230, Gradient norm: 1.67847192
INFO:root:[   33] Training loss: 0.08143913, Validation loss: 0.18773671, Gradient norm: 1.66712077
INFO:root:[   34] Training loss: 0.07921178, Validation loss: 0.20289842, Gradient norm: 1.45179865
INFO:root:[   35] Training loss: 0.08496684, Validation loss: 0.16189317, Gradient norm: 1.85069138
INFO:root:[   36] Training loss: 0.07746716, Validation loss: 0.17147441, Gradient norm: 1.49333244
INFO:root:[   37] Training loss: 0.07761714, Validation loss: 0.17548992, Gradient norm: 1.52619602
INFO:root:[   38] Training loss: 0.07815468, Validation loss: 0.22714653, Gradient norm: 1.58902659
INFO:root:[   39] Training loss: 0.08377210, Validation loss: 0.18263852, Gradient norm: 1.72389604
INFO:root:[   40] Training loss: 0.07540411, Validation loss: 0.17343302, Gradient norm: 1.35188759
INFO:root:[   41] Training loss: 0.07351217, Validation loss: 0.18953445, Gradient norm: 1.52256233
INFO:root:[   42] Training loss: 0.07699726, Validation loss: 0.19702470, Gradient norm: 1.64990697
INFO:root:[   43] Training loss: 0.07591977, Validation loss: 0.16437734, Gradient norm: 1.68846543
INFO:root:[   44] Training loss: 0.07399435, Validation loss: 0.20013011, Gradient norm: 1.45618643
INFO:root:[   45] Training loss: 0.07649523, Validation loss: 0.18519444, Gradient norm: 1.58096491
INFO:root:[   46] Training loss: 0.07761918, Validation loss: 0.18322360, Gradient norm: 1.40945697
INFO:root:[   47] Training loss: 0.07358716, Validation loss: 0.21053719, Gradient norm: 1.47777280
INFO:root:[   48] Training loss: 0.07361934, Validation loss: 0.20165863, Gradient norm: 1.54569469
INFO:root:[   49] Training loss: 0.07331955, Validation loss: 0.19549670, Gradient norm: 1.42639642
INFO:root:[   50] Training loss: 0.07170417, Validation loss: 0.18810669, Gradient norm: 1.32838823
INFO:root:[   51] Training loss: 0.07238527, Validation loss: 0.20022817, Gradient norm: 1.13416114
INFO:root:[   52] Training loss: 0.07334820, Validation loss: 0.21771782, Gradient norm: 1.51545969
INFO:root:[   53] Training loss: 0.07554394, Validation loss: 0.18502426, Gradient norm: 1.63042431
INFO:root:[   54] Training loss: 0.07332156, Validation loss: 0.19051943, Gradient norm: 1.53938725
INFO:root:[   55] Training loss: 0.06974319, Validation loss: 0.23575060, Gradient norm: 1.33181160
INFO:root:[   56] Training loss: 0.07360355, Validation loss: 0.19571352, Gradient norm: 1.52623886
INFO:root:[   57] Training loss: 0.06985396, Validation loss: 0.19084847, Gradient norm: 1.18199729
INFO:root:[   58] Training loss: 0.07074586, Validation loss: 0.20774384, Gradient norm: 1.45678383
INFO:root:[   59] Training loss: 0.07402251, Validation loss: 0.22185766, Gradient norm: 1.51076774
INFO:root:[   60] Training loss: 0.06818479, Validation loss: 0.20141367, Gradient norm: 1.27868748
INFO:root:[   61] Training loss: 0.07184676, Validation loss: 0.21703252, Gradient norm: 1.46776976
INFO:root:[   62] Training loss: 0.06883648, Validation loss: 0.22570376, Gradient norm: 1.36735724
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 5045.122s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.16845
INFO:root:EnergyScoreTrain: 0.09997
INFO:root:CRPSTrain: 0.0781
INFO:root:Gaussian NLLTrain: -0.6386
INFO:root:CoverageTrain: 0.81291
INFO:root:IntervalWidthTrain: 0.46097
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.21627
INFO:root:EnergyScoreValidation: 0.142
INFO:root:CRPSValidation: 0.12094
INFO:root:Gaussian NLLValidation: 3.57507
INFO:root:CoverageValidation: 0.4578
INFO:root:IntervalWidthValidation: 0.24509
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21983
INFO:root:EnergyScoreTest: 0.14682
INFO:root:CRPSTest: 0.12536
INFO:root:Gaussian NLLTest: 3.79587
INFO:root:CoverageTest: 0.45356
INFO:root:IntervalWidthTest: 0.2462
INFO:root:###3 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27376895, Validation loss: 0.19914767, Gradient norm: 2.93111217
INFO:root:[    2] Training loss: 0.18739111, Validation loss: 0.17492677, Gradient norm: 2.91981152
INFO:root:[    3] Training loss: 0.15900753, Validation loss: 0.17829441, Gradient norm: 2.38378800
INFO:root:[    4] Training loss: 0.14778672, Validation loss: 0.15626447, Gradient norm: 2.62759163
INFO:root:[    5] Training loss: 0.13682714, Validation loss: 0.19055997, Gradient norm: 2.27020624
INFO:root:[    6] Training loss: 0.13836416, Validation loss: 0.14765890, Gradient norm: 2.51829096
INFO:root:[    7] Training loss: 0.12881158, Validation loss: 0.13905123, Gradient norm: 2.28862857
INFO:root:[    8] Training loss: 0.12191122, Validation loss: 0.17364957, Gradient norm: 2.17208804
INFO:root:[    9] Training loss: 0.11841509, Validation loss: 0.13430272, Gradient norm: 2.05521544
INFO:root:[   10] Training loss: 0.11874882, Validation loss: 0.14700314, Gradient norm: 1.96485911
INFO:root:[   11] Training loss: 0.11603271, Validation loss: 0.13270115, Gradient norm: 2.07510710
INFO:root:[   12] Training loss: 0.11239497, Validation loss: 0.14303610, Gradient norm: 2.02110993
INFO:root:[   13] Training loss: 0.11193193, Validation loss: 0.21154242, Gradient norm: 2.10870909
INFO:root:[   14] Training loss: 0.11056747, Validation loss: 0.15631526, Gradient norm: 2.07802387
INFO:root:[   15] Training loss: 0.10208874, Validation loss: 0.16506597, Gradient norm: 1.84139501
INFO:root:[   16] Training loss: 0.10490818, Validation loss: 0.14617151, Gradient norm: 1.86323063
INFO:root:[   17] Training loss: 0.10515708, Validation loss: 0.21197716, Gradient norm: 2.03111424
INFO:root:[   18] Training loss: 0.10055784, Validation loss: 0.15873152, Gradient norm: 1.83640044
INFO:root:[   19] Training loss: 0.09456150, Validation loss: 0.17109521, Gradient norm: 1.77452005
INFO:root:[   20] Training loss: 0.09487123, Validation loss: 0.15324598, Gradient norm: 1.78561243
INFO:root:[   21] Training loss: 0.08927112, Validation loss: 0.18534411, Gradient norm: 1.59175939
INFO:root:[   22] Training loss: 0.09154263, Validation loss: 0.17209552, Gradient norm: 1.86649389
INFO:root:[   23] Training loss: 0.08644197, Validation loss: 0.17064192, Gradient norm: 1.42868082
INFO:root:[   24] Training loss: 0.09311814, Validation loss: 0.19061992, Gradient norm: 1.99805129
INFO:root:[   25] Training loss: 0.08673886, Validation loss: 0.17566538, Gradient norm: 1.48553575
INFO:root:[   26] Training loss: 0.08700093, Validation loss: 0.18379654, Gradient norm: 1.56016771
INFO:root:[   27] Training loss: 0.08601717, Validation loss: 0.22550502, Gradient norm: 1.77810076
INFO:root:[   28] Training loss: 0.08394471, Validation loss: 0.19937561, Gradient norm: 1.47638998
INFO:root:[   29] Training loss: 0.08320848, Validation loss: 0.18378151, Gradient norm: 1.49882168
INFO:root:[   30] Training loss: 0.08176323, Validation loss: 0.20170809, Gradient norm: 1.55021197
INFO:root:[   31] Training loss: 0.08146293, Validation loss: 0.16763686, Gradient norm: 1.58707853
INFO:root:[   32] Training loss: 0.07788186, Validation loss: 0.17479131, Gradient norm: 1.48354095
INFO:root:[   33] Training loss: 0.08066524, Validation loss: 0.19033042, Gradient norm: 1.62391880
INFO:root:[   34] Training loss: 0.07690724, Validation loss: 0.17627013, Gradient norm: 1.40311010
INFO:root:[   35] Training loss: 0.07901066, Validation loss: 0.17027039, Gradient norm: 1.36419615
INFO:root:[   36] Training loss: 0.07840587, Validation loss: 0.17605735, Gradient norm: 1.55443708
INFO:root:[   37] Training loss: 0.07807478, Validation loss: 0.18148151, Gradient norm: 1.41030206
INFO:root:[   38] Training loss: 0.07896588, Validation loss: 0.23308128, Gradient norm: 1.40446988
INFO:root:[   39] Training loss: 0.08130368, Validation loss: 0.19738315, Gradient norm: 1.65289769
INFO:root:[   40] Training loss: 0.07747734, Validation loss: 0.16994391, Gradient norm: 1.56732418
INFO:root:[   41] Training loss: 0.07334162, Validation loss: 0.20508914, Gradient norm: 1.35868513
INFO:root:[   42] Training loss: 0.07930833, Validation loss: 0.18887637, Gradient norm: 1.69042418
INFO:root:[   43] Training loss: 0.07490296, Validation loss: 0.17251616, Gradient norm: 1.41446572
INFO:root:[   44] Training loss: 0.07361540, Validation loss: 0.19873701, Gradient norm: 1.40272502
INFO:root:[   45] Training loss: 0.07590907, Validation loss: 0.19033404, Gradient norm: 1.50512967
INFO:root:[   46] Training loss: 0.07753148, Validation loss: 0.18470033, Gradient norm: 1.42715775
INFO:root:[   47] Training loss: 0.07300308, Validation loss: 0.19668222, Gradient norm: 1.38523350
INFO:root:[   48] Training loss: 0.07250963, Validation loss: 0.23342026, Gradient norm: 1.41701980
INFO:root:[   49] Training loss: 0.07705907, Validation loss: 0.18758083, Gradient norm: 1.53326379
INFO:root:[   50] Training loss: 0.07296233, Validation loss: 0.18403460, Gradient norm: 1.37693158
INFO:root:[   51] Training loss: 0.07522351, Validation loss: 0.18486175, Gradient norm: 1.27290809
INFO:root:[   52] Training loss: 0.07172316, Validation loss: 0.21534231, Gradient norm: 1.41080182
INFO:root:[   53] Training loss: 0.07233027, Validation loss: 0.20407254, Gradient norm: 1.26656684
INFO:root:[   54] Training loss: 0.07393366, Validation loss: 0.19211258, Gradient norm: 1.44542811
INFO:root:[   55] Training loss: 0.06937391, Validation loss: 0.23271778, Gradient norm: 1.28979608
INFO:root:[   56] Training loss: 0.07196433, Validation loss: 0.19443839, Gradient norm: 1.33029264
INFO:root:[   57] Training loss: 0.07003102, Validation loss: 0.20987412, Gradient norm: 1.21824204
INFO:root:[   58] Training loss: 0.07027269, Validation loss: 0.19606301, Gradient norm: 1.28784004
INFO:root:[   59] Training loss: 0.07135339, Validation loss: 0.21148390, Gradient norm: 1.37561690
INFO:root:[   60] Training loss: 0.06955771, Validation loss: 0.18439779, Gradient norm: 1.22517717
INFO:root:[   61] Training loss: 0.07226144, Validation loss: 0.20916009, Gradient norm: 1.35354529
INFO:root:[   62] Training loss: 0.06976108, Validation loss: 0.20658159, Gradient norm: 1.33843563
INFO:root:[   63] Training loss: 0.06831834, Validation loss: 0.19151839, Gradient norm: 1.30728231
INFO:root:[   64] Training loss: 0.06862018, Validation loss: 0.21025677, Gradient norm: 1.24056643
INFO:root:[   65] Training loss: 0.12625331, Validation loss: 0.19503863, Gradient norm: 2.12897929
INFO:root:[   66] Training loss: 0.08320308, Validation loss: 0.21500904, Gradient norm: 1.48126276
INFO:root:[   67] Training loss: 0.07085838, Validation loss: 0.19090984, Gradient norm: 1.33435562
INFO:root:[   68] Training loss: 0.07039952, Validation loss: 0.23074499, Gradient norm: 1.38820261
INFO:root:[   69] Training loss: 0.06997595, Validation loss: 0.20605185, Gradient norm: 1.32227584
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 5653.544s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.18691
INFO:root:EnergyScoreTrain: 0.10946
INFO:root:CRPSTrain: 0.08871
INFO:root:Gaussian NLLTrain: -0.56122
INFO:root:CoverageTrain: 0.85636
INFO:root:IntervalWidthTrain: 0.55166
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.21422
INFO:root:EnergyScoreValidation: 0.13315
INFO:root:CRPSValidation: 0.11267
INFO:root:Gaussian NLLValidation: 1.20394
INFO:root:CoverageValidation: 0.6117
INFO:root:IntervalWidthValidation: 0.30552
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21558
INFO:root:EnergyScoreTest: 0.13567
INFO:root:CRPSTest: 0.11509
INFO:root:Gaussian NLLTest: 1.24877
INFO:root:CoverageTest: 0.60672
INFO:root:IntervalWidthTest: 0.30736
INFO:root:###4 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27466269, Validation loss: 0.25296022, Gradient norm: 2.77663621
INFO:root:[    2] Training loss: 0.18187765, Validation loss: 0.15968821, Gradient norm: 2.66593304
INFO:root:[    3] Training loss: 0.15686616, Validation loss: 0.21475023, Gradient norm: 2.37579009
INFO:root:[    4] Training loss: 0.14481772, Validation loss: 0.13921889, Gradient norm: 2.38931593
INFO:root:[    5] Training loss: 0.13362113, Validation loss: 0.15329488, Gradient norm: 2.12421518
INFO:root:[    6] Training loss: 0.13659050, Validation loss: 0.17711750, Gradient norm: 2.53264139
INFO:root:[    7] Training loss: 0.13015569, Validation loss: 0.16475395, Gradient norm: 2.18193755
INFO:root:[    8] Training loss: 0.11980636, Validation loss: 0.15155364, Gradient norm: 1.86841763
INFO:root:[    9] Training loss: 0.11761998, Validation loss: 0.19401836, Gradient norm: 1.97747145
INFO:root:[   10] Training loss: 0.12334502, Validation loss: 0.13296841, Gradient norm: 2.38066603
INFO:root:[   11] Training loss: 0.11813550, Validation loss: 0.12652058, Gradient norm: 2.04450218
INFO:root:[   12] Training loss: 0.11301389, Validation loss: 0.13089580, Gradient norm: 1.97942200
INFO:root:[   13] Training loss: 0.10737333, Validation loss: 0.20652846, Gradient norm: 1.68860271
INFO:root:[   14] Training loss: 0.10852402, Validation loss: 0.15446324, Gradient norm: 1.75609426
INFO:root:[   15] Training loss: 0.10185820, Validation loss: 0.15922650, Gradient norm: 1.65916350
INFO:root:[   16] Training loss: 0.10337960, Validation loss: 0.13010199, Gradient norm: 1.75584621
INFO:root:[   17] Training loss: 0.10360714, Validation loss: 0.19340628, Gradient norm: 1.88257463
INFO:root:[   18] Training loss: 0.10226560, Validation loss: 0.14683722, Gradient norm: 1.75133720
INFO:root:[   19] Training loss: 0.09465778, Validation loss: 0.18835584, Gradient norm: 1.61492106
INFO:root:[   20] Training loss: 0.09688159, Validation loss: 0.14396459, Gradient norm: 1.87115771
INFO:root:[   21] Training loss: 0.08989023, Validation loss: 0.16925833, Gradient norm: 1.46364358
INFO:root:[   22] Training loss: 0.09115813, Validation loss: 0.16006652, Gradient norm: 1.66437622
INFO:root:[   23] Training loss: 0.09027261, Validation loss: 0.14969484, Gradient norm: 1.54812324
INFO:root:[   24] Training loss: 0.09407370, Validation loss: 0.17131385, Gradient norm: 1.80049869
INFO:root:[   25] Training loss: 0.08773429, Validation loss: 0.18999485, Gradient norm: 1.36707678
INFO:root:[   26] Training loss: 0.08892766, Validation loss: 0.16739453, Gradient norm: 1.58082904
INFO:root:[   27] Training loss: 0.08697988, Validation loss: 0.21535162, Gradient norm: 1.56446898
INFO:root:[   28] Training loss: 0.08516296, Validation loss: 0.18690670, Gradient norm: 1.41914193
INFO:root:[   29] Training loss: 0.08632190, Validation loss: 0.17728339, Gradient norm: 1.49731688
INFO:root:[   30] Training loss: 0.08543578, Validation loss: 0.19779355, Gradient norm: 1.56715862
INFO:root:[   31] Training loss: 0.08270078, Validation loss: 0.15696773, Gradient norm: 1.38309229
INFO:root:[   32] Training loss: 0.08445296, Validation loss: 0.18085976, Gradient norm: 1.66370326
INFO:root:[   33] Training loss: 0.08027388, Validation loss: 0.17927582, Gradient norm: 1.42299203
INFO:root:[   34] Training loss: 0.07684250, Validation loss: 0.19181565, Gradient norm: 1.14929550
INFO:root:[   35] Training loss: 0.08357817, Validation loss: 0.17617959, Gradient norm: 1.54039553
INFO:root:[   36] Training loss: 0.07863701, Validation loss: 0.16778079, Gradient norm: 1.42598981
INFO:root:[   37] Training loss: 0.07912475, Validation loss: 0.17070070, Gradient norm: 1.29521995
INFO:root:[   38] Training loss: 0.08044108, Validation loss: 0.19274387, Gradient norm: 1.46116720
INFO:root:[   39] Training loss: 0.07977638, Validation loss: 0.18821226, Gradient norm: 1.50129947
INFO:root:[   40] Training loss: 0.07787677, Validation loss: 0.19914635, Gradient norm: 1.27686519
INFO:root:[   41] Training loss: 0.07404244, Validation loss: 0.22027005, Gradient norm: 1.16428806
INFO:root:[   42] Training loss: 0.08228362, Validation loss: 0.19122503, Gradient norm: 1.64947310
INFO:root:[   43] Training loss: 0.07408996, Validation loss: 0.17528870, Gradient norm: 1.23688422
INFO:root:[   44] Training loss: 0.07694289, Validation loss: 0.20168362, Gradient norm: 1.31383968
INFO:root:[   45] Training loss: 0.07506173, Validation loss: 0.18884140, Gradient norm: 1.29508628
INFO:root:[   46] Training loss: 0.07778364, Validation loss: 0.17139109, Gradient norm: 1.33630031
INFO:root:[   47] Training loss: 0.07262967, Validation loss: 0.20527196, Gradient norm: 1.12302855
INFO:root:[   48] Training loss: 0.07392738, Validation loss: 0.19047706, Gradient norm: 1.20526093
INFO:root:[   49] Training loss: 0.07934821, Validation loss: 0.19192947, Gradient norm: 1.55147807
INFO:root:[   50] Training loss: 0.07295452, Validation loss: 0.18684353, Gradient norm: 1.26572693
INFO:root:[   51] Training loss: 0.07468622, Validation loss: 0.17208998, Gradient norm: 1.12117474
INFO:root:[   52] Training loss: 0.06926384, Validation loss: 0.21744018, Gradient norm: 1.10396146
INFO:root:[   53] Training loss: 0.07670113, Validation loss: 0.16976372, Gradient norm: 1.33457233
INFO:root:[   54] Training loss: 0.07259197, Validation loss: 0.18237709, Gradient norm: 1.31155610
INFO:root:[   55] Training loss: 0.07129777, Validation loss: 0.23773155, Gradient norm: 1.32956990
INFO:root:[   56] Training loss: 0.07297311, Validation loss: 0.17923878, Gradient norm: 1.26103272
INFO:root:[   57] Training loss: 0.07027874, Validation loss: 0.17767362, Gradient norm: 1.10405037
INFO:root:[   58] Training loss: 0.07217569, Validation loss: 0.21114637, Gradient norm: 1.32325820
INFO:root:[   59] Training loss: 0.07437565, Validation loss: 0.20121067, Gradient norm: 1.19621941
INFO:root:[   60] Training loss: 0.07122173, Validation loss: 0.17798029, Gradient norm: 1.19508647
INFO:root:[   61] Training loss: 0.07007225, Validation loss: 0.19994128, Gradient norm: 1.15199168
INFO:root:[   62] Training loss: 0.07106622, Validation loss: 0.18730140, Gradient norm: 1.21691149
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 5050.12s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.20328
INFO:root:EnergyScoreTrain: 0.11363
INFO:root:CRPSTrain: 0.09042
INFO:root:Gaussian NLLTrain: -0.56958
INFO:root:CoverageTrain: 0.89679
INFO:root:IntervalWidthTrain: 0.66988
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.21102
INFO:root:EnergyScoreValidation: 0.12715
INFO:root:CRPSValidation: 0.10656
INFO:root:Gaussian NLLValidation: 0.09983
INFO:root:CoverageValidation: 0.7101
INFO:root:IntervalWidthValidation: 0.42902
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21044
INFO:root:EnergyScoreTest: 0.12931
INFO:root:CRPSTest: 0.10821
INFO:root:Gaussian NLLTest: 0.09723
INFO:root:CoverageTest: 0.7114
INFO:root:IntervalWidthTest: 0.43041
INFO:root:###5 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26495257, Validation loss: 0.21736561, Gradient norm: 2.41142239
INFO:root:[    2] Training loss: 0.18494595, Validation loss: 0.15624259, Gradient norm: 2.53984760
INFO:root:[    3] Training loss: 0.15796093, Validation loss: 0.16444432, Gradient norm: 2.15993284
INFO:root:[    4] Training loss: 0.14400366, Validation loss: 0.13603110, Gradient norm: 2.14384341
INFO:root:[    5] Training loss: 0.13195466, Validation loss: 0.19367868, Gradient norm: 1.92896438
INFO:root:[    6] Training loss: 0.13257721, Validation loss: 0.14820842, Gradient norm: 2.22861961
INFO:root:[    7] Training loss: 0.13267568, Validation loss: 0.14405482, Gradient norm: 2.26953653
INFO:root:[    8] Training loss: 0.12649605, Validation loss: 0.13556410, Gradient norm: 2.13178354
INFO:root:[    9] Training loss: 0.11903996, Validation loss: 0.12397768, Gradient norm: 1.76400777
INFO:root:[   10] Training loss: 0.12247836, Validation loss: 0.13426248, Gradient norm: 2.29134268
INFO:root:[   11] Training loss: 0.11272358, Validation loss: 0.12774068, Gradient norm: 1.63338985
INFO:root:[   12] Training loss: 0.11219031, Validation loss: 0.12891748, Gradient norm: 1.87719933
INFO:root:[   13] Training loss: 0.10734099, Validation loss: 0.14259410, Gradient norm: 1.62619870
INFO:root:[   14] Training loss: 0.10910463, Validation loss: 0.14055640, Gradient norm: 1.61400577
INFO:root:[   15] Training loss: 0.10211414, Validation loss: 0.13112248, Gradient norm: 1.40381864
INFO:root:[   16] Training loss: 0.10451662, Validation loss: 0.13220691, Gradient norm: 1.76110045
INFO:root:[   17] Training loss: 0.10299180, Validation loss: 0.18360474, Gradient norm: 1.71172504
INFO:root:[   18] Training loss: 0.10169856, Validation loss: 0.15685996, Gradient norm: 1.61471258
INFO:root:[   19] Training loss: 0.09659570, Validation loss: 0.14576234, Gradient norm: 1.51486108
INFO:root:[   20] Training loss: 0.09671633, Validation loss: 0.13863980, Gradient norm: 1.54041261
INFO:root:[   21] Training loss: 0.09645705, Validation loss: 0.19312805, Gradient norm: 1.54356944
INFO:root:[   22] Training loss: 0.09420905, Validation loss: 0.13331406, Gradient norm: 1.63707230
INFO:root:[   23] Training loss: 0.08986202, Validation loss: 0.14258236, Gradient norm: 1.37848363
INFO:root:[   24] Training loss: 0.09313042, Validation loss: 0.15973300, Gradient norm: 1.47067843
INFO:root:[   25] Training loss: 0.08766384, Validation loss: 0.17486357, Gradient norm: 1.14848981
INFO:root:[   26] Training loss: 0.09023851, Validation loss: 0.14101127, Gradient norm: 1.48560178
INFO:root:[   27] Training loss: 0.08730738, Validation loss: 0.19604131, Gradient norm: 1.41161631
INFO:root:[   28] Training loss: 0.08718468, Validation loss: 0.18350249, Gradient norm: 1.37122795
INFO:root:[   29] Training loss: 0.08697050, Validation loss: 0.16314699, Gradient norm: 1.44078462
INFO:root:[   30] Training loss: 0.08505936, Validation loss: 0.18684133, Gradient norm: 1.39005980
INFO:root:[   31] Training loss: 0.08267235, Validation loss: 0.15545246, Gradient norm: 1.26361786
INFO:root:[   32] Training loss: 0.07923440, Validation loss: 0.14674307, Gradient norm: 1.21153629
INFO:root:[   33] Training loss: 0.08563051, Validation loss: 0.16374019, Gradient norm: 1.40486851
INFO:root:[   34] Training loss: 0.08658310, Validation loss: 0.15089682, Gradient norm: 1.62984350
INFO:root:[   35] Training loss: 0.07920348, Validation loss: 0.15073937, Gradient norm: 1.09645208
INFO:root:[   36] Training loss: 0.08168774, Validation loss: 0.14921537, Gradient norm: 1.36638415
INFO:root:[   37] Training loss: 0.08452219, Validation loss: 0.14599068, Gradient norm: 1.47196020
INFO:root:[   38] Training loss: 0.08145644, Validation loss: 0.21069355, Gradient norm: 1.34149241
INFO:root:[   39] Training loss: 0.08049739, Validation loss: 0.16045011, Gradient norm: 1.23231023
INFO:root:[   40] Training loss: 0.07990230, Validation loss: 0.16946315, Gradient norm: 1.05765106
INFO:root:[   41] Training loss: 0.07592042, Validation loss: 0.15751927, Gradient norm: 1.08080525
INFO:root:[   42] Training loss: 0.07982057, Validation loss: 0.14875821, Gradient norm: 1.30915479
INFO:root:[   43] Training loss: 0.07730279, Validation loss: 0.15716612, Gradient norm: 1.24992715
INFO:root:[   44] Training loss: 0.07651835, Validation loss: 0.17607757, Gradient norm: 1.17083015
INFO:root:[   45] Training loss: 0.07971050, Validation loss: 0.14773157, Gradient norm: 1.36103967
INFO:root:[   46] Training loss: 0.07779633, Validation loss: 0.16673801, Gradient norm: 1.22846501
INFO:root:[   47] Training loss: 0.07679793, Validation loss: 0.17642350, Gradient norm: 1.13192698
INFO:root:[   48] Training loss: 0.07499051, Validation loss: 0.19105084, Gradient norm: 1.15297514
INFO:root:[   49] Training loss: 0.07965461, Validation loss: 0.16232936, Gradient norm: 1.30404785
INFO:root:[   50] Training loss: 0.07527970, Validation loss: 0.14990738, Gradient norm: 1.24436697
INFO:root:[   51] Training loss: 0.07937053, Validation loss: 0.15987202, Gradient norm: 1.21361777
INFO:root:[   52] Training loss: 0.07313196, Validation loss: 0.15388987, Gradient norm: 0.99297133
INFO:root:[   53] Training loss: 0.07600936, Validation loss: 0.14166797, Gradient norm: 1.19034605
INFO:root:[   54] Training loss: 0.07526685, Validation loss: 0.16484252, Gradient norm: 1.17982413
INFO:root:[   55] Training loss: 0.07742592, Validation loss: 0.18684577, Gradient norm: 1.31856263
INFO:root:[   56] Training loss: 0.07332439, Validation loss: 0.15678134, Gradient norm: 1.08333455
INFO:root:[   57] Training loss: 0.07426432, Validation loss: 0.16769713, Gradient norm: 1.19772063
INFO:root:[   58] Training loss: 0.07376239, Validation loss: 0.18668012, Gradient norm: 1.12991100
INFO:root:[   59] Training loss: 0.07486480, Validation loss: 0.17382350, Gradient norm: 1.11249729
INFO:root:[   60] Training loss: 0.07287107, Validation loss: 0.15181943, Gradient norm: 1.02265916
INFO:root:[   61] Training loss: 0.07250283, Validation loss: 0.15809597, Gradient norm: 1.06739218
INFO:root:[   62] Training loss: 0.07379291, Validation loss: 0.15416803, Gradient norm: 1.10941198
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 5016.647s.
INFO:root:Emptying the cuda cache took 0.033s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.17389
INFO:root:EnergyScoreTrain: 0.10662
INFO:root:CRPSTrain: 0.08325
INFO:root:Gaussian NLLTrain: -0.6709
INFO:root:CoverageTrain: 0.95855
INFO:root:IntervalWidthTrain: 0.66611
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.20802
INFO:root:EnergyScoreValidation: 0.12423
INFO:root:CRPSValidation: 0.0996
INFO:root:Gaussian NLLValidation: -0.40787
INFO:root:CoverageValidation: 0.85631
INFO:root:IntervalWidthValidation: 0.5834
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.20939
INFO:root:EnergyScoreTest: 0.12644
INFO:root:CRPSTest: 0.10181
INFO:root:Gaussian NLLTest: -0.38732
INFO:root:CoverageTest: 0.85485
INFO:root:IntervalWidthTest: 0.58705
INFO:root:###6 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26245191, Validation loss: 0.23518291, Gradient norm: 2.46611468
INFO:root:[    2] Training loss: 0.18379815, Validation loss: 0.15883067, Gradient norm: 2.11639495
INFO:root:[    3] Training loss: 0.15820832, Validation loss: 0.13281242, Gradient norm: 1.89558348
INFO:root:[    4] Training loss: 0.14653271, Validation loss: 0.15403029, Gradient norm: 1.89756930
INFO:root:[    5] Training loss: 0.13883539, Validation loss: 0.13934945, Gradient norm: 2.03995342
INFO:root:[    6] Training loss: 0.13130513, Validation loss: 0.15603589, Gradient norm: 1.83330830
INFO:root:[    7] Training loss: 0.13155873, Validation loss: 0.14008987, Gradient norm: 1.94722158
INFO:root:[    8] Training loss: 0.12589104, Validation loss: 0.16028173, Gradient norm: 1.78277305
INFO:root:[    9] Training loss: 0.12101273, Validation loss: 0.12150061, Gradient norm: 1.61924567
INFO:root:[   10] Training loss: 0.12143129, Validation loss: 0.13989823, Gradient norm: 1.94614639
INFO:root:[   11] Training loss: 0.11414319, Validation loss: 0.13784094, Gradient norm: 1.49330555
INFO:root:[   12] Training loss: 0.11179039, Validation loss: 0.13045621, Gradient norm: 1.59275715
INFO:root:[   13] Training loss: 0.10749692, Validation loss: 0.13780979, Gradient norm: 1.34175964
INFO:root:[   14] Training loss: 0.10928647, Validation loss: 0.14137239, Gradient norm: 1.51290736
INFO:root:[   15] Training loss: 0.10450327, Validation loss: 0.14475930, Gradient norm: 1.46553994
INFO:root:[   16] Training loss: 0.10762453, Validation loss: 0.12030283, Gradient norm: 1.60051263
INFO:root:[   17] Training loss: 0.10324582, Validation loss: 0.16103929, Gradient norm: 1.49961794
INFO:root:[   18] Training loss: 0.10391645, Validation loss: 0.13841491, Gradient norm: 1.46127080
INFO:root:[   19] Training loss: 0.09927919, Validation loss: 0.18238388, Gradient norm: 1.39891861
INFO:root:[   20] Training loss: 0.09662890, Validation loss: 0.14638524, Gradient norm: 1.33472603
INFO:root:[   21] Training loss: 0.09880750, Validation loss: 0.19199411, Gradient norm: 1.41906891
INFO:root:[   22] Training loss: 0.09362572, Validation loss: 0.13182832, Gradient norm: 1.31826038
INFO:root:[   23] Training loss: 0.09654174, Validation loss: 0.18391684, Gradient norm: 1.43876323
INFO:root:[   24] Training loss: 0.09450185, Validation loss: 0.17051565, Gradient norm: 1.35624209
INFO:root:[   25] Training loss: 0.09348190, Validation loss: 0.17819031, Gradient norm: 1.29057918
INFO:root:[   26] Training loss: 0.08980424, Validation loss: 0.13458477, Gradient norm: 1.12739022
INFO:root:[   27] Training loss: 0.08896264, Validation loss: 0.17689003, Gradient norm: 1.23046004
INFO:root:[   28] Training loss: 0.08917349, Validation loss: 0.18619907, Gradient norm: 1.12815382
INFO:root:[   29] Training loss: 0.09026489, Validation loss: 0.17922619, Gradient norm: 1.27770550
INFO:root:[   30] Training loss: 0.09136300, Validation loss: 0.16540152, Gradient norm: 1.38104503
INFO:root:[   31] Training loss: 0.09120210, Validation loss: 0.17898734, Gradient norm: 1.36782710
INFO:root:[   32] Training loss: 0.08925389, Validation loss: 0.15208517, Gradient norm: 1.30828431
INFO:root:[   33] Training loss: 0.08634137, Validation loss: 0.15334266, Gradient norm: 1.13138284
INFO:root:[   34] Training loss: 0.08529751, Validation loss: 0.13900823, Gradient norm: 1.14559815
INFO:root:[   35] Training loss: 0.08263844, Validation loss: 0.14719798, Gradient norm: 1.01503134
INFO:root:[   36] Training loss: 0.08694459, Validation loss: 0.18592840, Gradient norm: 1.22354854
INFO:root:[   37] Training loss: 0.08746709, Validation loss: 0.14654108, Gradient norm: 1.32136484
INFO:root:[   38] Training loss: 0.08415624, Validation loss: 0.19867193, Gradient norm: 1.20499971
INFO:root:[   39] Training loss: 0.08545051, Validation loss: 0.16168203, Gradient norm: 1.14869192
INFO:root:[   40] Training loss: 0.08357267, Validation loss: 0.20553442, Gradient norm: 1.14819924
INFO:root:[   41] Training loss: 0.08252156, Validation loss: 0.15244297, Gradient norm: 1.06593322
INFO:root:[   42] Training loss: 0.08272594, Validation loss: 0.17012693, Gradient norm: 1.09939165
INFO:root:[   43] Training loss: 0.08205199, Validation loss: 0.16446236, Gradient norm: 1.06339056
INFO:root:[   44] Training loss: 0.08016613, Validation loss: 0.15594386, Gradient norm: 0.93283758
INFO:root:[   45] Training loss: 0.08300358, Validation loss: 0.17124682, Gradient norm: 1.23660031
INFO:root:[   46] Training loss: 0.07859317, Validation loss: 0.17607255, Gradient norm: 1.02442725
INFO:root:[   47] Training loss: 0.08087092, Validation loss: 0.15121062, Gradient norm: 1.16084348
INFO:root:[   48] Training loss: 0.07909498, Validation loss: 0.18926514, Gradient norm: 1.00132444
INFO:root:[   49] Training loss: 0.07858343, Validation loss: 0.16059479, Gradient norm: 1.04723909
INFO:root:[   50] Training loss: 0.07781122, Validation loss: 0.16622060, Gradient norm: 1.01594725
INFO:root:[   51] Training loss: 0.08195350, Validation loss: 0.15323624, Gradient norm: 1.08354140
INFO:root:[   52] Training loss: 0.07688617, Validation loss: 0.14727324, Gradient norm: 0.97368700
INFO:root:[   53] Training loss: 0.07921839, Validation loss: 0.15264933, Gradient norm: 1.07123025
INFO:root:[   54] Training loss: 0.07574204, Validation loss: 0.15722605, Gradient norm: 0.93881566
INFO:root:[   55] Training loss: 0.07698464, Validation loss: 0.20668951, Gradient norm: 1.07419433
INFO:root:[   56] Training loss: 0.07754999, Validation loss: 0.17230515, Gradient norm: 0.98488234
INFO:root:[   57] Training loss: 0.07770135, Validation loss: 0.15885682, Gradient norm: 1.02020430
INFO:root:[   58] Training loss: 0.07623024, Validation loss: 0.20264077, Gradient norm: 1.02176716
INFO:root:[   59] Training loss: 0.07656039, Validation loss: 0.16887921, Gradient norm: 0.95437116
INFO:root:[   60] Training loss: 0.07512531, Validation loss: 0.16183960, Gradient norm: 0.95630952
INFO:root:[   61] Training loss: 0.07475683, Validation loss: 0.16109121, Gradient norm: 0.92057522
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 4944.559s.
INFO:root:Emptying the cuda cache took 0.034s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.20542
INFO:root:EnergyScoreTrain: 0.1141
INFO:root:CRPSTrain: 0.09168
INFO:root:Gaussian NLLTrain: -0.57011
INFO:root:CoverageTrain: 0.97489
INFO:root:IntervalWidthTrain: 0.70505
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.20712
INFO:root:EnergyScoreValidation: 0.12185
INFO:root:CRPSValidation: 0.09992
INFO:root:Gaussian NLLValidation: -0.38582
INFO:root:CoverageValidation: 0.8715
INFO:root:IntervalWidthValidation: 0.60376
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.20731
INFO:root:EnergyScoreTest: 0.12399
INFO:root:CRPSTest: 0.10179
INFO:root:Gaussian NLLTest: -0.37062
INFO:root:CoverageTest: 0.86738
INFO:root:IntervalWidthTest: 0.60417
INFO:root:###7 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26007373, Validation loss: 0.21444306, Gradient norm: 2.02687054
INFO:root:[    2] Training loss: 0.18303091, Validation loss: 0.16283051, Gradient norm: 1.97384594
INFO:root:[    3] Training loss: 0.16399093, Validation loss: 0.15728976, Gradient norm: 1.99875923
INFO:root:[    4] Training loss: 0.14768648, Validation loss: 0.13976911, Gradient norm: 1.79923766
INFO:root:[    5] Training loss: 0.13597673, Validation loss: 0.12990419, Gradient norm: 1.64850986
INFO:root:[    6] Training loss: 0.13490659, Validation loss: 0.16299978, Gradient norm: 1.71915838
INFO:root:[    7] Training loss: 0.13738715, Validation loss: 0.13826850, Gradient norm: 1.82257977
INFO:root:[    8] Training loss: 0.12801776, Validation loss: 0.15808055, Gradient norm: 1.68827853
INFO:root:[    9] Training loss: 0.12619381, Validation loss: 0.13028772, Gradient norm: 1.52044494
INFO:root:[   10] Training loss: 0.12546496, Validation loss: 0.14304720, Gradient norm: 1.70773115
INFO:root:[   11] Training loss: 0.11929852, Validation loss: 0.13510187, Gradient norm: 1.43602162
INFO:root:[   12] Training loss: 0.11796742, Validation loss: 0.12667647, Gradient norm: 1.46988942
INFO:root:[   13] Training loss: 0.11474962, Validation loss: 0.14611577, Gradient norm: 1.35674557
INFO:root:[   14] Training loss: 0.11438553, Validation loss: 0.14189845, Gradient norm: 1.35438452
INFO:root:[   15] Training loss: 0.10849490, Validation loss: 0.16667801, Gradient norm: 1.24567406
INFO:root:[   16] Training loss: 0.11270600, Validation loss: 0.14394274, Gradient norm: 1.41734499
INFO:root:[   17] Training loss: 0.11147523, Validation loss: 0.17454373, Gradient norm: 1.45471594
INFO:root:[   18] Training loss: 0.10920074, Validation loss: 0.14044894, Gradient norm: 1.37788431
INFO:root:[   19] Training loss: 0.10451664, Validation loss: 0.20455170, Gradient norm: 1.31092715
INFO:root:[   20] Training loss: 0.10338906, Validation loss: 0.14845251, Gradient norm: 1.42848784
INFO:root:[   21] Training loss: 0.10366333, Validation loss: 0.18992517, Gradient norm: 1.34523034
INFO:root:[   22] Training loss: 0.09772930, Validation loss: 0.13286309, Gradient norm: 1.19980892
INFO:root:[   23] Training loss: 0.09765675, Validation loss: 0.13122271, Gradient norm: 1.15996892
INFO:root:[   24] Training loss: 0.10050944, Validation loss: 0.17040507, Gradient norm: 1.39407755
INFO:root:[   25] Training loss: 0.09513917, Validation loss: 0.17885213, Gradient norm: 1.10760980
INFO:root:[   26] Training loss: 0.09365727, Validation loss: 0.14503568, Gradient norm: 1.21626201
INFO:root:[   27] Training loss: 0.09449987, Validation loss: 0.18415504, Gradient norm: 1.22301841
INFO:root:[   28] Training loss: 0.09309845, Validation loss: 0.18920499, Gradient norm: 1.15610680
INFO:root:[   29] Training loss: 0.09223887, Validation loss: 0.14895939, Gradient norm: 1.11217982
INFO:root:[   30] Training loss: 0.09081577, Validation loss: 0.18339446, Gradient norm: 1.20917468
INFO:root:[   31] Training loss: 0.09274964, Validation loss: 0.15783350, Gradient norm: 1.20594066
INFO:root:[   32] Training loss: 0.08758419, Validation loss: 0.14231853, Gradient norm: 1.05201335
INFO:root:[   33] Training loss: 0.09031228, Validation loss: 0.15393208, Gradient norm: 1.24710478
INFO:root:[   34] Training loss: 0.08733027, Validation loss: 0.20175526, Gradient norm: 1.21138728
INFO:root:[   35] Training loss: 0.08922351, Validation loss: 0.13984725, Gradient norm: 1.09176820
INFO:root:[   36] Training loss: 0.08977124, Validation loss: 0.17967940, Gradient norm: 1.23438502
INFO:root:[   37] Training loss: 0.08908814, Validation loss: 0.15535782, Gradient norm: 1.26692549
INFO:root:[   38] Training loss: 0.08791268, Validation loss: 0.20175775, Gradient norm: 1.15502041
INFO:root:[   39] Training loss: 0.08828620, Validation loss: 0.16400267, Gradient norm: 1.15841855
INFO:root:[   40] Training loss: 0.08619759, Validation loss: 0.20072544, Gradient norm: 1.09810993
INFO:root:[   41] Training loss: 0.08334363, Validation loss: 0.19196496, Gradient norm: 0.95329488
INFO:root:[   42] Training loss: 0.08777440, Validation loss: 0.15729852, Gradient norm: 1.20281759
INFO:root:[   43] Training loss: 0.08397588, Validation loss: 0.17694601, Gradient norm: 1.11509379
INFO:root:[   44] Training loss: 0.08341898, Validation loss: 0.17390756, Gradient norm: 1.04006377
INFO:root:[   45] Training loss: 0.08511446, Validation loss: 0.15246165, Gradient norm: 1.03396573
INFO:root:[   46] Training loss: 0.08381010, Validation loss: 0.16385567, Gradient norm: 1.07851988
INFO:root:[   47] Training loss: 0.08365710, Validation loss: 0.16767311, Gradient norm: 1.09960643
INFO:root:[   48] Training loss: 0.08238709, Validation loss: 0.21086483, Gradient norm: 1.03084579
INFO:root:[   49] Training loss: 0.08575690, Validation loss: 0.15941879, Gradient norm: 1.13057756
INFO:root:[   50] Training loss: 0.08134740, Validation loss: 0.15790890, Gradient norm: 1.07318998
INFO:root:[   51] Training loss: 0.08436419, Validation loss: 0.15797761, Gradient norm: 1.03293039
INFO:root:[   52] Training loss: 0.07963344, Validation loss: 0.16870371, Gradient norm: 0.96018383
INFO:root:[   53] Training loss: 0.08356809, Validation loss: 0.14454746, Gradient norm: 1.07184537
INFO:root:[   54] Training loss: 0.08319450, Validation loss: 0.15916900, Gradient norm: 1.13268783
INFO:root:[   55] Training loss: 0.08187235, Validation loss: 0.20578460, Gradient norm: 1.12458068
INFO:root:[   56] Training loss: 0.08088101, Validation loss: 0.15553828, Gradient norm: 0.95761689
INFO:root:[   57] Training loss: 0.08141937, Validation loss: 0.17513003, Gradient norm: 1.02917654
INFO:root:[   58] Training loss: 0.08043625, Validation loss: 0.19642032, Gradient norm: 1.02556011
INFO:root:[   59] Training loss: 0.08003292, Validation loss: 0.18061397, Gradient norm: 0.95153248
INFO:root:[   60] Training loss: 0.08137347, Validation loss: 0.18520161, Gradient norm: 1.06903332
INFO:root:[   61] Training loss: 0.08004471, Validation loss: 0.16215372, Gradient norm: 0.99289980
INFO:root:[   62] Training loss: 0.08147087, Validation loss: 0.17691625, Gradient norm: 0.92162821
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 5001.839s.
INFO:root:Emptying the cuda cache took 0.034s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.17632
INFO:root:EnergyScoreTrain: 0.11105
INFO:root:CRPSTrain: 0.08814
INFO:root:Gaussian NLLTrain: -0.57295
INFO:root:CoverageTrain: 0.98206
INFO:root:IntervalWidthTrain: 0.75978
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.21572
INFO:root:EnergyScoreValidation: 0.12757
INFO:root:CRPSValidation: 0.10408
INFO:root:Gaussian NLLValidation: -0.33367
INFO:root:CoverageValidation: 0.91932
INFO:root:IntervalWidthValidation: 0.68293
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21732
INFO:root:EnergyScoreTest: 0.13013
INFO:root:CRPSTest: 0.10635
INFO:root:Gaussian NLLTest: -0.32245
INFO:root:CoverageTest: 0.91682
INFO:root:IntervalWidthTest: 0.6852
INFO:root:###8 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26285701, Validation loss: 0.21900952, Gradient norm: 2.08229438
INFO:root:[    2] Training loss: 0.18256449, Validation loss: 0.16746832, Gradient norm: 1.70906727
INFO:root:[    3] Training loss: 0.16084751, Validation loss: 0.14405212, Gradient norm: 1.62856651
INFO:root:[    4] Training loss: 0.15214355, Validation loss: 0.14308012, Gradient norm: 1.79820876
INFO:root:[    5] Training loss: 0.13876375, Validation loss: 0.12893960, Gradient norm: 1.51576953
INFO:root:[    6] Training loss: 0.13911015, Validation loss: 0.16652614, Gradient norm: 1.66142539
INFO:root:[    7] Training loss: 0.13537560, Validation loss: 0.13728716, Gradient norm: 1.63233696
INFO:root:[    8] Training loss: 0.13057631, Validation loss: 0.16246286, Gradient norm: 1.51848185
INFO:root:[    9] Training loss: 0.12613531, Validation loss: 0.13609858, Gradient norm: 1.27410186
INFO:root:[   10] Training loss: 0.12987607, Validation loss: 0.14996430, Gradient norm: 1.67330099
INFO:root:[   11] Training loss: 0.11979592, Validation loss: 0.13612572, Gradient norm: 1.19893494
INFO:root:[   12] Training loss: 0.12351758, Validation loss: 0.13053434, Gradient norm: 1.55783298
INFO:root:[   13] Training loss: 0.11686593, Validation loss: 0.15864758, Gradient norm: 1.34216208
INFO:root:[   14] Training loss: 0.11280488, Validation loss: 0.14460859, Gradient norm: 1.13498250
INFO:root:[   15] Training loss: 0.11438282, Validation loss: 0.17698622, Gradient norm: 1.40095242
INFO:root:[   16] Training loss: 0.11509223, Validation loss: 0.12904521, Gradient norm: 1.30948591
INFO:root:[   17] Training loss: 0.11157346, Validation loss: 0.17811481, Gradient norm: 1.24983383
INFO:root:[   18] Training loss: 0.11106303, Validation loss: 0.14335208, Gradient norm: 1.29998176
INFO:root:[   19] Training loss: 0.10717438, Validation loss: 0.20583499, Gradient norm: 1.22541835
INFO:root:[   20] Training loss: 0.10325199, Validation loss: 0.14058091, Gradient norm: 1.18781860
INFO:root:[   21] Training loss: 0.10611625, Validation loss: 0.18494767, Gradient norm: 1.32490182
INFO:root:[   22] Training loss: 0.10114182, Validation loss: 0.13957570, Gradient norm: 1.20164194
INFO:root:[   23] Training loss: 0.10212704, Validation loss: 0.17072135, Gradient norm: 1.24026676
INFO:root:[   24] Training loss: 0.10080844, Validation loss: 0.16431199, Gradient norm: 1.22011119
INFO:root:[   25] Training loss: 0.09785256, Validation loss: 0.14745562, Gradient norm: 1.08140297
INFO:root:[   26] Training loss: 0.09799539, Validation loss: 0.14882382, Gradient norm: 1.19765491
INFO:root:[   27] Training loss: 0.09731895, Validation loss: 0.15126944, Gradient norm: 1.16909994
INFO:root:[   28] Training loss: 0.09713679, Validation loss: 0.19431612, Gradient norm: 1.17382923
INFO:root:[   29] Training loss: 0.09489728, Validation loss: 0.16268837, Gradient norm: 1.12459164
INFO:root:[   30] Training loss: 0.09520218, Validation loss: 0.15843866, Gradient norm: 1.22196912
INFO:root:[   31] Training loss: 0.09715305, Validation loss: 0.16998333, Gradient norm: 1.25825379
INFO:root:[   32] Training loss: 0.09352493, Validation loss: 0.14084106, Gradient norm: 1.18812770
INFO:root:[   33] Training loss: 0.09339869, Validation loss: 0.16287013, Gradient norm: 1.08989136
INFO:root:[   34] Training loss: 0.09023183, Validation loss: 0.13855490, Gradient norm: 1.06725909
INFO:root:[   35] Training loss: 0.08917300, Validation loss: 0.14177364, Gradient norm: 1.00890072
INFO:root:[   36] Training loss: 0.09216264, Validation loss: 0.17735175, Gradient norm: 1.10941174
INFO:root:[   37] Training loss: 0.09394479, Validation loss: 0.15221947, Gradient norm: 1.22516124
INFO:root:[   38] Training loss: 0.09055080, Validation loss: 0.20654027, Gradient norm: 1.09929638
INFO:root:[   39] Training loss: 0.09163370, Validation loss: 0.14209494, Gradient norm: 1.19058774
INFO:root:[   40] Training loss: 0.08912253, Validation loss: 0.18423129, Gradient norm: 1.07028119
INFO:root:[   41] Training loss: 0.09013054, Validation loss: 0.17275434, Gradient norm: 1.10603778
INFO:root:[   42] Training loss: 0.08946850, Validation loss: 0.15166068, Gradient norm: 1.11124590
INFO:root:[   43] Training loss: 0.08709333, Validation loss: 0.16006612, Gradient norm: 1.03659773
INFO:root:[   44] Training loss: 0.08723113, Validation loss: 0.16266986, Gradient norm: 1.02459449
INFO:root:[   45] Training loss: 0.08827310, Validation loss: 0.15881731, Gradient norm: 1.08367574
INFO:root:[   46] Training loss: 0.08693660, Validation loss: 0.17336385, Gradient norm: 1.00595443
INFO:root:[   47] Training loss: 0.08702273, Validation loss: 0.16360805, Gradient norm: 1.03494530
INFO:root:[   48] Training loss: 0.08539222, Validation loss: 0.20162633, Gradient norm: 0.95733292
INFO:root:[   49] Training loss: 0.08812621, Validation loss: 0.15641643, Gradient norm: 1.05253491
INFO:root:[   50] Training loss: 0.08428072, Validation loss: 0.16168430, Gradient norm: 0.94507434
INFO:root:[   51] Training loss: 0.08747534, Validation loss: 0.15612604, Gradient norm: 0.95166467
INFO:root:[   52] Training loss: 0.08465775, Validation loss: 0.13529692, Gradient norm: 1.05938048
INFO:root:[   53] Training loss: 0.08763692, Validation loss: 0.14336249, Gradient norm: 1.07646956
INFO:root:[   54] Training loss: 0.08334656, Validation loss: 0.15434611, Gradient norm: 0.95171809
INFO:root:[   55] Training loss: 0.08391967, Validation loss: 0.19753237, Gradient norm: 0.98495713
INFO:root:[   56] Training loss: 0.08461402, Validation loss: 0.14987159, Gradient norm: 0.93564274
INFO:root:[   57] Training loss: 0.08420508, Validation loss: 0.17988093, Gradient norm: 0.98617876
INFO:root:[   58] Training loss: 0.08346880, Validation loss: 0.20025164, Gradient norm: 0.83260910
INFO:root:[   59] Training loss: 0.08355285, Validation loss: 0.17513728, Gradient norm: 0.93439747
INFO:root:[   60] Training loss: 0.08455977, Validation loss: 0.18714115, Gradient norm: 1.02540986
INFO:root:[   61] Training loss: 0.08405305, Validation loss: 0.16334997, Gradient norm: 0.96561467
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 4998.848s.
INFO:root:Emptying the cuda cache took 0.096s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.18864
INFO:root:EnergyScoreTrain: 0.12303
INFO:root:CRPSTrain: 0.09778
INFO:root:Gaussian NLLTrain: -0.40782
INFO:root:CoverageTrain: 0.98441
INFO:root:IntervalWidthTrain: 0.91169
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.20748
INFO:root:EnergyScoreValidation: 0.12977
INFO:root:CRPSValidation: 0.10412
INFO:root:Gaussian NLLValidation: -0.36069
INFO:root:CoverageValidation: 0.96524
INFO:root:IntervalWidthValidation: 0.85252
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21062
INFO:root:EnergyScoreTest: 0.13305
INFO:root:CRPSTest: 0.10713
INFO:root:Gaussian NLLTest: -0.34318
INFO:root:CoverageTest: 0.96259
INFO:root:IntervalWidthTest: 0.85319
INFO:root:###9 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26090514, Validation loss: 0.22027374, Gradient norm: 1.81970718
INFO:root:[    2] Training loss: 0.18738335, Validation loss: 0.16408808, Gradient norm: 1.58597864
INFO:root:[    3] Training loss: 0.16854792, Validation loss: 0.16779566, Gradient norm: 1.53796596
INFO:root:[    4] Training loss: 0.15807701, Validation loss: 0.17079633, Gradient norm: 1.77476947
INFO:root:[    5] Training loss: 0.15320421, Validation loss: 0.15097108, Gradient norm: 1.74855053
INFO:root:[    6] Training loss: 0.14550131, Validation loss: 0.16541833, Gradient norm: 1.56011221
INFO:root:[    7] Training loss: 0.13939329, Validation loss: 0.14522865, Gradient norm: 1.44965708
INFO:root:[    8] Training loss: 0.13724553, Validation loss: 0.15754362, Gradient norm: 1.50165372
INFO:root:[    9] Training loss: 0.13318418, Validation loss: 0.14245983, Gradient norm: 1.21961479
INFO:root:[   10] Training loss: 0.13715250, Validation loss: 0.14281788, Gradient norm: 1.66941295
INFO:root:[   11] Training loss: 0.12783879, Validation loss: 0.14235733, Gradient norm: 1.18765847
INFO:root:[   12] Training loss: 0.12992910, Validation loss: 0.14584088, Gradient norm: 1.42874009
INFO:root:[   13] Training loss: 0.12962163, Validation loss: 0.16544488, Gradient norm: 1.44302927
INFO:root:[   14] Training loss: 0.12322688, Validation loss: 0.16795377, Gradient norm: 1.10875275
INFO:root:[   15] Training loss: 0.12166245, Validation loss: 0.19372604, Gradient norm: 1.22235108
INFO:root:[   16] Training loss: 0.12397907, Validation loss: 0.14910176, Gradient norm: 1.37360843
INFO:root:[   17] Training loss: 0.11978687, Validation loss: 0.16461144, Gradient norm: 1.25334846
INFO:root:[   18] Training loss: 0.11725184, Validation loss: 0.14240673, Gradient norm: 1.17859815
INFO:root:[   19] Training loss: 0.11419233, Validation loss: 0.22064215, Gradient norm: 1.18936636
INFO:root:[   20] Training loss: 0.11935314, Validation loss: 0.15027409, Gradient norm: 1.43192377
INFO:root:[   21] Training loss: 0.11153743, Validation loss: 0.17954871, Gradient norm: 1.16963445
