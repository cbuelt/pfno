INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/fno_srd.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10, 'ood': True}
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.18376044, Validation loss: 0.03504035, Gradient norm: 1.62726150
INFO:root:[    2] Training loss: 0.02856886, Validation loss: 0.02743104, Gradient norm: 1.26317253
INFO:root:[    3] Training loss: 0.02538686, Validation loss: 0.02267043, Gradient norm: 1.22381968
INFO:root:[    4] Training loss: 0.02307106, Validation loss: 0.02309558, Gradient norm: 1.27650267
INFO:root:[    5] Training loss: 0.02203443, Validation loss: 0.03070242, Gradient norm: 1.21616895
INFO:root:[    6] Training loss: 0.02024573, Validation loss: 0.01917300, Gradient norm: 1.15899167
INFO:root:[    7] Training loss: 0.01906802, Validation loss: 0.01531160, Gradient norm: 1.23358009
INFO:root:[    8] Training loss: 0.01721425, Validation loss: 0.01368530, Gradient norm: 1.12279334
INFO:root:[    9] Training loss: 0.01726919, Validation loss: 0.01772816, Gradient norm: 1.13346730
INFO:root:[   10] Training loss: 0.01563794, Validation loss: 0.01472712, Gradient norm: 0.95920570
INFO:root:[   11] Training loss: 0.01605621, Validation loss: 0.01748286, Gradient norm: 1.05356537
INFO:root:[   12] Training loss: 0.01561395, Validation loss: 0.01260353, Gradient norm: 1.08384260
INFO:root:[   13] Training loss: 0.01478258, Validation loss: 0.01308180, Gradient norm: 1.03873118
INFO:root:[   14] Training loss: 0.01512244, Validation loss: 0.01690864, Gradient norm: 1.03663139
INFO:root:[   15] Training loss: 0.01444210, Validation loss: 0.01155616, Gradient norm: 0.93861386
INFO:root:[   16] Training loss: 0.01444657, Validation loss: 0.01163019, Gradient norm: 1.03352406
INFO:root:[   17] Training loss: 0.01402197, Validation loss: 0.01054941, Gradient norm: 1.02447751
INFO:root:[   18] Training loss: 0.01321459, Validation loss: 0.01156272, Gradient norm: 0.94235746
INFO:root:[   19] Training loss: 0.01311354, Validation loss: 0.01538592, Gradient norm: 0.96250870
INFO:root:[   20] Training loss: 0.01266908, Validation loss: 0.01367894, Gradient norm: 0.92364458
INFO:root:[   21] Training loss: 0.01259485, Validation loss: 0.00964903, Gradient norm: 0.84450534
INFO:root:[   22] Training loss: 0.01270289, Validation loss: 0.01017755, Gradient norm: 0.92548499
INFO:root:[   23] Training loss: 0.01231738, Validation loss: 0.00994711, Gradient norm: 0.86690434
INFO:root:[   24] Training loss: 0.01198249, Validation loss: 0.01421604, Gradient norm: 0.81485723
INFO:root:[   25] Training loss: 0.01217877, Validation loss: 0.01500374, Gradient norm: 0.85919943
INFO:root:[   26] Training loss: 0.01170498, Validation loss: 0.01005309, Gradient norm: 0.80292244
INFO:root:[   27] Training loss: 0.01153137, Validation loss: 0.00904475, Gradient norm: 0.87669368
INFO:root:[   28] Training loss: 0.01174698, Validation loss: 0.01045571, Gradient norm: 0.90369702
INFO:root:[   29] Training loss: 0.01161479, Validation loss: 0.01232133, Gradient norm: 0.85948125
INFO:root:[   30] Training loss: 0.01101970, Validation loss: 0.01116140, Gradient norm: 0.82602268
INFO:root:[   31] Training loss: 0.01069073, Validation loss: 0.00888330, Gradient norm: 0.77903001
INFO:root:[   32] Training loss: 0.01085161, Validation loss: 0.00803175, Gradient norm: 0.82306358
INFO:root:[   33] Training loss: 0.01097406, Validation loss: 0.00927080, Gradient norm: 0.85078416
INFO:root:[   34] Training loss: 0.01121459, Validation loss: 0.01368486, Gradient norm: 0.83811547
INFO:root:[   35] Training loss: 0.01070560, Validation loss: 0.01141394, Gradient norm: 0.82421693
INFO:root:[   36] Training loss: 0.01023604, Validation loss: 0.01055524, Gradient norm: 0.79829685
INFO:root:[   37] Training loss: 0.01004743, Validation loss: 0.01126202, Gradient norm: 0.71571165
INFO:root:[   38] Training loss: 0.01029288, Validation loss: 0.01069716, Gradient norm: 0.78529732
INFO:root:[   39] Training loss: 0.01017108, Validation loss: 0.00716272, Gradient norm: 0.78731690
INFO:root:[   40] Training loss: 0.00934797, Validation loss: 0.00729499, Gradient norm: 0.71233396
INFO:root:[   41] Training loss: 0.00958866, Validation loss: 0.00719701, Gradient norm: 0.75793140
INFO:root:[   42] Training loss: 0.00952684, Validation loss: 0.01115098, Gradient norm: 0.77889487
INFO:root:[   43] Training loss: 0.00939482, Validation loss: 0.01015444, Gradient norm: 0.79026515
INFO:root:[   44] Training loss: 0.00902612, Validation loss: 0.01016364, Gradient norm: 0.74557732
INFO:root:[   45] Training loss: 0.00963003, Validation loss: 0.00718157, Gradient norm: 0.75294768
INFO:root:[   46] Training loss: 0.00964080, Validation loss: 0.01095847, Gradient norm: 0.71297276
INFO:root:[   47] Training loss: 0.00868025, Validation loss: 0.00798159, Gradient norm: 0.55492096
INFO:root:[   48] Training loss: 0.00879892, Validation loss: 0.00982814, Gradient norm: 0.71355095
INFO:root:[   49] Training loss: 0.00857918, Validation loss: 0.00993977, Gradient norm: 0.71759761
INFO:root:[   50] Training loss: 0.00866385, Validation loss: 0.01055539, Gradient norm: 0.74869970
INFO:root:[   51] Training loss: 0.00859137, Validation loss: 0.00715338, Gradient norm: 0.73065631
INFO:root:[   52] Training loss: 0.00862245, Validation loss: 0.00835707, Gradient norm: 0.71303293
INFO:root:[   53] Training loss: 0.00850771, Validation loss: 0.00780087, Gradient norm: 0.63500939
INFO:root:[   54] Training loss: 0.00803453, Validation loss: 0.00739893, Gradient norm: 0.58632601
INFO:root:[   55] Training loss: 0.00840568, Validation loss: 0.00855622, Gradient norm: 0.69606935
INFO:root:[   56] Training loss: 0.00794724, Validation loss: 0.01037035, Gradient norm: 0.67951237
INFO:root:[   57] Training loss: 0.00829703, Validation loss: 0.00670050, Gradient norm: 0.73042034
INFO:root:[   58] Training loss: 0.00821713, Validation loss: 0.00723968, Gradient norm: 0.68130261
INFO:root:[   59] Training loss: 0.00811908, Validation loss: 0.00883562, Gradient norm: 0.63820006
INFO:root:[   60] Training loss: 0.00775645, Validation loss: 0.00834335, Gradient norm: 0.57975338
INFO:root:[   61] Training loss: 0.00759029, Validation loss: 0.00906509, Gradient norm: 0.56490145
INFO:root:[   62] Training loss: 0.00765186, Validation loss: 0.00885292, Gradient norm: 0.56863224
INFO:root:[   63] Training loss: 0.00728144, Validation loss: 0.00739780, Gradient norm: 0.58483792
INFO:root:[   64] Training loss: 0.00818316, Validation loss: 0.00779222, Gradient norm: 0.63440997
INFO:root:[   65] Training loss: 0.00749670, Validation loss: 0.00806767, Gradient norm: 0.58833589
INFO:root:[   66] Training loss: 0.00808172, Validation loss: 0.01090032, Gradient norm: 0.68750068
INFO:root:[   67] Training loss: 0.00852187, Validation loss: 0.00575053, Gradient norm: 0.74740493
INFO:root:[   68] Training loss: 0.00808040, Validation loss: 0.00919111, Gradient norm: 0.67309637
INFO:root:[   69] Training loss: 0.00756711, Validation loss: 0.00648466, Gradient norm: 0.64385885
INFO:root:[   70] Training loss: 0.00745219, Validation loss: 0.00797209, Gradient norm: 0.63775310
INFO:root:[   71] Training loss: 0.00718854, Validation loss: 0.00800817, Gradient norm: 0.57942721
INFO:root:[   72] Training loss: 0.00683015, Validation loss: 0.00671520, Gradient norm: 0.52647787
INFO:root:[   73] Training loss: 0.00709471, Validation loss: 0.00598767, Gradient norm: 0.56701278
INFO:root:[   74] Training loss: 0.00764855, Validation loss: 0.00682513, Gradient norm: 0.60150711
INFO:root:[   75] Training loss: 0.00743436, Validation loss: 0.00842738, Gradient norm: 0.59675569
INFO:root:[   76] Training loss: 0.00750492, Validation loss: 0.00567288, Gradient norm: 0.66302598
INFO:root:[   77] Training loss: 0.00670015, Validation loss: 0.00602437, Gradient norm: 0.54880753
INFO:root:[   78] Training loss: 0.00702687, Validation loss: 0.00811224, Gradient norm: 0.57259589
INFO:root:[   79] Training loss: 0.00697349, Validation loss: 0.00779699, Gradient norm: 0.54485756
INFO:root:[   80] Training loss: 0.00704157, Validation loss: 0.00543795, Gradient norm: 0.57685031
INFO:root:[   81] Training loss: 0.00682688, Validation loss: 0.00578453, Gradient norm: 0.54277887
INFO:root:[   82] Training loss: 0.00664493, Validation loss: 0.00612400, Gradient norm: 0.55203873
INFO:root:[   83] Training loss: 0.00704888, Validation loss: 0.00834005, Gradient norm: 0.62300498
INFO:root:[   84] Training loss: 0.00679772, Validation loss: 0.00663267, Gradient norm: 0.60507067
INFO:root:[   85] Training loss: 0.00677593, Validation loss: 0.00588848, Gradient norm: 0.58178875
INFO:root:[   86] Training loss: 0.00671279, Validation loss: 0.00794122, Gradient norm: 0.56525780
INFO:root:[   87] Training loss: 0.00665361, Validation loss: 0.00696543, Gradient norm: 0.53561321
INFO:root:[   88] Training loss: 0.00625146, Validation loss: 0.00539091, Gradient norm: 0.50439250
INFO:root:[   89] Training loss: 0.00664270, Validation loss: 0.00540158, Gradient norm: 0.57518716
INFO:root:[   90] Training loss: 0.00655904, Validation loss: 0.00631564, Gradient norm: 0.56491954
INFO:root:[   91] Training loss: 0.00673347, Validation loss: 0.00804404, Gradient norm: 0.57818526
INFO:root:[   92] Training loss: 0.00653383, Validation loss: 0.00603619, Gradient norm: 0.53922122
INFO:root:[   93] Training loss: 0.00631420, Validation loss: 0.00526554, Gradient norm: 0.54108348
INFO:root:[   94] Training loss: 0.00644983, Validation loss: 0.00650222, Gradient norm: 0.54224071
INFO:root:[   95] Training loss: 0.00650212, Validation loss: 0.00739253, Gradient norm: 0.55311334
INFO:root:[   96] Training loss: 0.00652772, Validation loss: 0.00651845, Gradient norm: 0.47349119
INFO:root:[   97] Training loss: 0.00615236, Validation loss: 0.00586410, Gradient norm: 0.51451622
INFO:root:[   98] Training loss: 0.00646265, Validation loss: 0.00499571, Gradient norm: 0.55471926
INFO:root:[   99] Training loss: 0.00630038, Validation loss: 0.00528278, Gradient norm: 0.50176254
INFO:root:[  100] Training loss: 0.00585880, Validation loss: 0.00483590, Gradient norm: 0.45894354
INFO:root:[  101] Training loss: 0.00596123, Validation loss: 0.00504091, Gradient norm: 0.51126775
INFO:root:[  102] Training loss: 0.00630167, Validation loss: 0.00503778, Gradient norm: 0.54492284
INFO:root:[  103] Training loss: 0.00609129, Validation loss: 0.00580344, Gradient norm: 0.51599541
INFO:root:[  104] Training loss: 0.00619825, Validation loss: 0.00553102, Gradient norm: 0.50861854
INFO:root:[  105] Training loss: 0.00591109, Validation loss: 0.00512519, Gradient norm: 0.47736758
INFO:root:[  106] Training loss: 0.00619598, Validation loss: 0.00527194, Gradient norm: 0.50866644
INFO:root:[  107] Training loss: 0.00614608, Validation loss: 0.00500775, Gradient norm: 0.52035425
INFO:root:[  108] Training loss: 0.00612897, Validation loss: 0.00593580, Gradient norm: 0.52585070
INFO:root:[  109] Training loss: 0.00613309, Validation loss: 0.00628560, Gradient norm: 0.48109696
INFO:root:EP 109: Early stopping
INFO:root:Training the model took 4312.581s.
INFO:root:Emptying the cuda cache took 0.084s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00461
INFO:root:EnergyScoreTrain: 0.0048
INFO:root:CoverageTrain: 0.99861
INFO:root:IntervalWidthTrain: 0.05275
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00461
INFO:root:EnergyScoreValidation: 0.00478
INFO:root:CoverageValidation: 0.99859
INFO:root:IntervalWidthValidation: 0.05247
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.00632
INFO:root:EnergyScoreTest: 0.00536
INFO:root:CoverageTest: 0.99406
INFO:root:IntervalWidthTest: 0.0518
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 530579456
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.17397764, Validation loss: 0.02591858, Gradient norm: 1.97946424
INFO:root:[    2] Training loss: 0.02992644, Validation loss: 0.02358353, Gradient norm: 1.37880752
INFO:root:[    3] Training loss: 0.02686609, Validation loss: 0.03156923, Gradient norm: 1.49502405
INFO:root:[    4] Training loss: 0.02577915, Validation loss: 0.01902144, Gradient norm: 1.62623025
INFO:root:[    5] Training loss: 0.02226335, Validation loss: 0.01722385, Gradient norm: 1.50632056
INFO:root:[    6] Training loss: 0.01886667, Validation loss: 0.02449276, Gradient norm: 1.16432543
INFO:root:[    7] Training loss: 0.01860222, Validation loss: 0.02288601, Gradient norm: 1.25476677
INFO:root:[    8] Training loss: 0.01743296, Validation loss: 0.02044808, Gradient norm: 1.21525217
INFO:root:[    9] Training loss: 0.01644330, Validation loss: 0.02186961, Gradient norm: 1.14253444
INFO:root:[   10] Training loss: 0.01646609, Validation loss: 0.01754317, Gradient norm: 1.15049214
INFO:root:[   11] Training loss: 0.01592856, Validation loss: 0.01239637, Gradient norm: 1.10669548
INFO:root:[   12] Training loss: 0.01529859, Validation loss: 0.01280047, Gradient norm: 0.95302936
INFO:root:[   13] Training loss: 0.01644018, Validation loss: 0.01163098, Gradient norm: 1.13883642
INFO:root:[   14] Training loss: 0.01468341, Validation loss: 0.01677170, Gradient norm: 1.02738914
INFO:root:[   15] Training loss: 0.01456626, Validation loss: 0.01061751, Gradient norm: 1.01612502
INFO:root:[   16] Training loss: 0.01448681, Validation loss: 0.01154783, Gradient norm: 0.99798945
INFO:root:[   17] Training loss: 0.01428934, Validation loss: 0.01725841, Gradient norm: 1.00231882
INFO:root:[   18] Training loss: 0.01421529, Validation loss: 0.01562330, Gradient norm: 0.98551899
INFO:root:[   19] Training loss: 0.01326530, Validation loss: 0.00975368, Gradient norm: 0.89980389
INFO:root:[   20] Training loss: 0.01348972, Validation loss: 0.01528552, Gradient norm: 0.89216846
INFO:root:[   21] Training loss: 0.01341491, Validation loss: 0.01018636, Gradient norm: 0.91727080
INFO:root:[   22] Training loss: 0.01280132, Validation loss: 0.01438060, Gradient norm: 0.90533904
INFO:root:[   23] Training loss: 0.01388685, Validation loss: 0.00903116, Gradient norm: 1.04132494
INFO:root:[   24] Training loss: 0.01245161, Validation loss: 0.01338702, Gradient norm: 0.81703947
INFO:root:[   25] Training loss: 0.01226742, Validation loss: 0.01251252, Gradient norm: 0.80048834
INFO:root:[   26] Training loss: 0.01208581, Validation loss: 0.00839373, Gradient norm: 0.86853721
INFO:root:[   27] Training loss: 0.01230882, Validation loss: 0.01247206, Gradient norm: 0.87693448
INFO:root:[   28] Training loss: 0.01222783, Validation loss: 0.01336294, Gradient norm: 0.80598636
INFO:root:[   29] Training loss: 0.01209467, Validation loss: 0.01436464, Gradient norm: 0.87780657
INFO:root:[   30] Training loss: 0.01208249, Validation loss: 0.01030046, Gradient norm: 0.93086708
INFO:root:[   31] Training loss: 0.01177580, Validation loss: 0.01062488, Gradient norm: 0.90747248
INFO:root:[   32] Training loss: 0.01145049, Validation loss: 0.01257443, Gradient norm: 0.88462362
INFO:root:[   33] Training loss: 0.01223628, Validation loss: 0.01077990, Gradient norm: 0.79904254
INFO:root:[   34] Training loss: 0.01140186, Validation loss: 0.01096084, Gradient norm: 0.76030281
INFO:root:[   35] Training loss: 0.01102431, Validation loss: 0.00813113, Gradient norm: 0.82202511
INFO:root:[   36] Training loss: 0.01089157, Validation loss: 0.01196928, Gradient norm: 0.80050015
INFO:root:[   37] Training loss: 0.01114829, Validation loss: 0.01029684, Gradient norm: 0.84944466
INFO:root:[   38] Training loss: 0.01068682, Validation loss: 0.00755883, Gradient norm: 0.81015412
INFO:root:[   39] Training loss: 0.01045221, Validation loss: 0.01394695, Gradient norm: 0.78393162
INFO:root:[   40] Training loss: 0.01066527, Validation loss: 0.01140454, Gradient norm: 0.80619058
INFO:root:[   41] Training loss: 0.01036428, Validation loss: 0.00835201, Gradient norm: 0.80000720
INFO:root:[   42] Training loss: 0.01035276, Validation loss: 0.00983357, Gradient norm: 0.74166798
INFO:root:[   43] Training loss: 0.00993394, Validation loss: 0.01113220, Gradient norm: 0.70361024
INFO:root:[   44] Training loss: 0.00949400, Validation loss: 0.01196015, Gradient norm: 0.73310616
INFO:root:[   45] Training loss: 0.00961215, Validation loss: 0.01038987, Gradient norm: 0.77692075
INFO:root:[   46] Training loss: 0.00947723, Validation loss: 0.00739509, Gradient norm: 0.74087285
INFO:root:[   47] Training loss: 0.00954449, Validation loss: 0.00775322, Gradient norm: 0.68682366
INFO:root:[   48] Training loss: 0.00922211, Validation loss: 0.00818400, Gradient norm: 0.69497501
INFO:root:[   49] Training loss: 0.00980733, Validation loss: 0.01224689, Gradient norm: 0.74309080
INFO:root:[   50] Training loss: 0.00948748, Validation loss: 0.00778262, Gradient norm: 0.76517922
INFO:root:[   51] Training loss: 0.00892076, Validation loss: 0.00709170, Gradient norm: 0.62968573
INFO:root:[   52] Training loss: 0.00907425, Validation loss: 0.00978073, Gradient norm: 0.69724028
INFO:root:[   53] Training loss: 0.00887262, Validation loss: 0.01184483, Gradient norm: 0.70785579
INFO:root:[   54] Training loss: 0.00929918, Validation loss: 0.00648830, Gradient norm: 0.77145651
INFO:root:[   55] Training loss: 0.00933072, Validation loss: 0.01228337, Gradient norm: 0.72491316
INFO:root:[   56] Training loss: 0.00908372, Validation loss: 0.00792054, Gradient norm: 0.72128636
INFO:root:[   57] Training loss: 0.00870090, Validation loss: 0.00928533, Gradient norm: 0.70519964
INFO:root:[   58] Training loss: 0.00899316, Validation loss: 0.00973455, Gradient norm: 0.69804055
INFO:root:[   59] Training loss: 0.00876057, Validation loss: 0.00730023, Gradient norm: 0.65678411
INFO:root:[   60] Training loss: 0.00792173, Validation loss: 0.00644717, Gradient norm: 0.61990449
INFO:root:[   61] Training loss: 0.00794629, Validation loss: 0.00782588, Gradient norm: 0.65308372
INFO:root:[   62] Training loss: 0.00806186, Validation loss: 0.00944786, Gradient norm: 0.63974885
INFO:root:[   63] Training loss: 0.00813332, Validation loss: 0.00878928, Gradient norm: 0.57846848
INFO:root:[   64] Training loss: 0.00800850, Validation loss: 0.00761819, Gradient norm: 0.56727355
INFO:root:[   65] Training loss: 0.00727454, Validation loss: 0.00999112, Gradient norm: 0.52734244
INFO:root:[   66] Training loss: 0.00845858, Validation loss: 0.00643188, Gradient norm: 0.71192351
INFO:root:[   67] Training loss: 0.00848454, Validation loss: 0.00892988, Gradient norm: 0.71447907
INFO:root:[   68] Training loss: 0.00824909, Validation loss: 0.00707833, Gradient norm: 0.66978803
INFO:root:[   69] Training loss: 0.00788802, Validation loss: 0.00681176, Gradient norm: 0.63446373
INFO:root:[   70] Training loss: 0.00808995, Validation loss: 0.00853711, Gradient norm: 0.59103512
INFO:root:[   71] Training loss: 0.00764873, Validation loss: 0.00751399, Gradient norm: 0.55463774
INFO:root:[   72] Training loss: 0.00722806, Validation loss: 0.00810582, Gradient norm: 0.54094217
INFO:root:[   73] Training loss: 0.00771482, Validation loss: 0.00664541, Gradient norm: 0.65140269
INFO:root:[   74] Training loss: 0.00791230, Validation loss: 0.01023379, Gradient norm: 0.67801179
INFO:root:[   75] Training loss: 0.00798942, Validation loss: 0.00549876, Gradient norm: 0.69709436
INFO:root:[   76] Training loss: 0.00788601, Validation loss: 0.00906944, Gradient norm: 0.66197190
INFO:root:[   77] Training loss: 0.00782820, Validation loss: 0.00998450, Gradient norm: 0.67626266
INFO:root:[   78] Training loss: 0.00810541, Validation loss: 0.00633023, Gradient norm: 0.64262405
INFO:root:[   79] Training loss: 0.00712333, Validation loss: 0.00806751, Gradient norm: 0.48330193
INFO:root:[   80] Training loss: 0.00712841, Validation loss: 0.00571873, Gradient norm: 0.54872075
INFO:root:[   81] Training loss: 0.00658622, Validation loss: 0.00531288, Gradient norm: 0.48903622
INFO:root:[   82] Training loss: 0.00696382, Validation loss: 0.00841738, Gradient norm: 0.55994455
INFO:root:[   83] Training loss: 0.00701647, Validation loss: 0.00825800, Gradient norm: 0.53775032
INFO:root:[   84] Training loss: 0.00716701, Validation loss: 0.00778258, Gradient norm: 0.55207654
INFO:root:[   85] Training loss: 0.00690531, Validation loss: 0.00637748, Gradient norm: 0.54877662
INFO:root:[   86] Training loss: 0.00666136, Validation loss: 0.00607294, Gradient norm: 0.52470710
INFO:root:[   87] Training loss: 0.00694752, Validation loss: 0.00555846, Gradient norm: 0.56765768
INFO:root:[   88] Training loss: 0.00725016, Validation loss: 0.00654775, Gradient norm: 0.62947314
INFO:root:[   89] Training loss: 0.00776936, Validation loss: 0.00761201, Gradient norm: 0.66097904
INFO:root:[   90] Training loss: 0.00714160, Validation loss: 0.00623042, Gradient norm: 0.42449482
INFO:root:EP 90: Early stopping
INFO:root:Training the model took 3314.39s.
INFO:root:Emptying the cuda cache took 0.087s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00502
INFO:root:EnergyScoreTrain: 0.00532
INFO:root:CoverageTrain: 0.999
INFO:root:IntervalWidthTrain: 0.05916
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00497
INFO:root:EnergyScoreValidation: 0.00529
INFO:root:CoverageValidation: 0.99903
INFO:root:IntervalWidthValidation: 0.05885
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.00789
INFO:root:EnergyScoreTest: 0.00645
INFO:root:CoverageTest: 0.99249
INFO:root:IntervalWidthTest: 0.05812
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 526385152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.20124870, Validation loss: 0.03399453, Gradient norm: 2.09264605
INFO:root:[    2] Training loss: 0.02952847, Validation loss: 0.03887414, Gradient norm: 1.52622920
INFO:root:[    3] Training loss: 0.02575911, Validation loss: 0.03793804, Gradient norm: 1.39563599
INFO:root:[    4] Training loss: 0.02658143, Validation loss: 0.02263361, Gradient norm: 1.67845780
INFO:root:[    5] Training loss: 0.02427091, Validation loss: 0.01997905, Gradient norm: 1.57130627
INFO:root:[    6] Training loss: 0.02075508, Validation loss: 0.02312060, Gradient norm: 1.41058220
INFO:root:[    7] Training loss: 0.01857952, Validation loss: 0.01570623, Gradient norm: 1.22172598
INFO:root:[    8] Training loss: 0.01903204, Validation loss: 0.02295947, Gradient norm: 1.37169645
INFO:root:[    9] Training loss: 0.01662109, Validation loss: 0.01713727, Gradient norm: 1.08573141
INFO:root:[   10] Training loss: 0.01719057, Validation loss: 0.01626454, Gradient norm: 1.23964934
INFO:root:[   11] Training loss: 0.01556308, Validation loss: 0.01626015, Gradient norm: 1.05664288
INFO:root:[   12] Training loss: 0.01657827, Validation loss: 0.01559308, Gradient norm: 1.19761123
INFO:root:[   13] Training loss: 0.01572920, Validation loss: 0.01583638, Gradient norm: 1.03072559
INFO:root:[   14] Training loss: 0.01545630, Validation loss: 0.01977936, Gradient norm: 1.03979766
INFO:root:[   15] Training loss: 0.01613692, Validation loss: 0.01298343, Gradient norm: 1.18074412
INFO:root:[   16] Training loss: 0.01608498, Validation loss: 0.01635863, Gradient norm: 1.10633568
INFO:root:[   17] Training loss: 0.01430893, Validation loss: 0.01566236, Gradient norm: 0.94267518
INFO:root:[   18] Training loss: 0.01513184, Validation loss: 0.01435577, Gradient norm: 1.06559432
INFO:root:[   19] Training loss: 0.01484538, Validation loss: 0.01077550, Gradient norm: 1.05360886
INFO:root:[   20] Training loss: 0.01426575, Validation loss: 0.01002783, Gradient norm: 1.02596508
INFO:root:[   21] Training loss: 0.01548991, Validation loss: 0.01984332, Gradient norm: 1.07396323
INFO:root:[   22] Training loss: 0.01389232, Validation loss: 0.01571736, Gradient norm: 0.95385889
INFO:root:[   23] Training loss: 0.01314163, Validation loss: 0.01052948, Gradient norm: 0.90206900
INFO:root:[   24] Training loss: 0.01299774, Validation loss: 0.01136365, Gradient norm: 0.91950728
INFO:root:[   25] Training loss: 0.01344874, Validation loss: 0.00919062, Gradient norm: 0.96383824
INFO:root:[   26] Training loss: 0.01281655, Validation loss: 0.01280998, Gradient norm: 0.82444047
INFO:root:[   27] Training loss: 0.01286041, Validation loss: 0.01685706, Gradient norm: 0.85847535
INFO:root:[   28] Training loss: 0.01244616, Validation loss: 0.01291092, Gradient norm: 0.93458234
INFO:root:[   29] Training loss: 0.01205100, Validation loss: 0.01601081, Gradient norm: 0.90729298
INFO:root:[   30] Training loss: 0.01285305, Validation loss: 0.00894533, Gradient norm: 0.96567961
INFO:root:[   31] Training loss: 0.01202523, Validation loss: 0.01185731, Gradient norm: 0.83840087
INFO:root:[   32] Training loss: 0.01194760, Validation loss: 0.01020844, Gradient norm: 0.77661437
INFO:root:[   33] Training loss: 0.01188058, Validation loss: 0.01032987, Gradient norm: 0.89086762
INFO:root:[   34] Training loss: 0.01130450, Validation loss: 0.01076437, Gradient norm: 0.85125783
INFO:root:[   35] Training loss: 0.01113545, Validation loss: 0.01406399, Gradient norm: 0.83957750
INFO:root:[   36] Training loss: 0.01122455, Validation loss: 0.01223348, Gradient norm: 0.83725232
INFO:root:[   37] Training loss: 0.01024261, Validation loss: 0.01233770, Gradient norm: 0.74946589
INFO:root:[   38] Training loss: 0.01043306, Validation loss: 0.01233723, Gradient norm: 0.78963660
INFO:root:[   39] Training loss: 0.01052660, Validation loss: 0.01226339, Gradient norm: 0.83773839
INFO:root:[   40] Training loss: 0.01155489, Validation loss: 0.00832282, Gradient norm: 0.79713312
INFO:root:[   41] Training loss: 0.00998894, Validation loss: 0.00944643, Gradient norm: 0.70237712
INFO:root:[   42] Training loss: 0.01028618, Validation loss: 0.00744824, Gradient norm: 0.79497298
INFO:root:[   43] Training loss: 0.01005939, Validation loss: 0.01246510, Gradient norm: 0.78976201
INFO:root:[   44] Training loss: 0.00975265, Validation loss: 0.01147515, Gradient norm: 0.75318257
INFO:root:[   45] Training loss: 0.01004915, Validation loss: 0.01053925, Gradient norm: 0.73257652
INFO:root:[   46] Training loss: 0.01005687, Validation loss: 0.00739273, Gradient norm: 0.74850065
INFO:root:[   47] Training loss: 0.00940907, Validation loss: 0.01108550, Gradient norm: 0.72241240
INFO:root:[   48] Training loss: 0.00919071, Validation loss: 0.01112605, Gradient norm: 0.71044611
INFO:root:[   49] Training loss: 0.00998416, Validation loss: 0.00741855, Gradient norm: 0.78549665
INFO:root:[   50] Training loss: 0.00890424, Validation loss: 0.00787493, Gradient norm: 0.60136699
INFO:root:[   51] Training loss: 0.00861009, Validation loss: 0.00825853, Gradient norm: 0.63895996
INFO:root:[   52] Training loss: 0.00892276, Validation loss: 0.00942757, Gradient norm: 0.68119946
INFO:root:[   53] Training loss: 0.00853798, Validation loss: 0.01176502, Gradient norm: 0.66699439
INFO:root:[   54] Training loss: 0.01006092, Validation loss: 0.01013185, Gradient norm: 0.80275101
INFO:root:[   55] Training loss: 0.00901061, Validation loss: 0.00749495, Gradient norm: 0.73138926
INFO:root:[   56] Training loss: 0.00878922, Validation loss: 0.00907131, Gradient norm: 0.70097112
INFO:root:[   57] Training loss: 0.00847074, Validation loss: 0.00937626, Gradient norm: 0.66626217
INFO:root:[   58] Training loss: 0.00889554, Validation loss: 0.00856069, Gradient norm: 0.66542685
INFO:root:[   59] Training loss: 0.00897838, Validation loss: 0.00676750, Gradient norm: 0.67769695
INFO:root:[   60] Training loss: 0.00851675, Validation loss: 0.00973292, Gradient norm: 0.63097026
INFO:root:[   61] Training loss: 0.00801602, Validation loss: 0.00834720, Gradient norm: 0.60474371
INFO:root:[   62] Training loss: 0.00793276, Validation loss: 0.00661741, Gradient norm: 0.60773565
INFO:root:[   63] Training loss: 0.00738434, Validation loss: 0.00728330, Gradient norm: 0.56891870
INFO:root:[   64] Training loss: 0.00808854, Validation loss: 0.01060635, Gradient norm: 0.63852908
INFO:root:[   65] Training loss: 0.00819802, Validation loss: 0.00663322, Gradient norm: 0.59228720
INFO:root:[   66] Training loss: 0.00784788, Validation loss: 0.00603946, Gradient norm: 0.57712245
INFO:root:[   67] Training loss: 0.00715170, Validation loss: 0.00643678, Gradient norm: 0.52240050
INFO:root:[   68] Training loss: 0.00789866, Validation loss: 0.00628602, Gradient norm: 0.63912563
INFO:root:[   69] Training loss: 0.00799609, Validation loss: 0.00974467, Gradient norm: 0.65833998
INFO:root:[   70] Training loss: 0.00824656, Validation loss: 0.00594085, Gradient norm: 0.68614796
INFO:root:[   71] Training loss: 0.00771904, Validation loss: 0.00909317, Gradient norm: 0.59143337
INFO:root:[   72] Training loss: 0.00781960, Validation loss: 0.00587607, Gradient norm: 0.58104471
INFO:root:[   73] Training loss: 0.00690295, Validation loss: 0.00568552, Gradient norm: 0.50755004
INFO:root:[   74] Training loss: 0.00728898, Validation loss: 0.00691677, Gradient norm: 0.57776016
INFO:root:[   75] Training loss: 0.00752916, Validation loss: 0.00867558, Gradient norm: 0.58905527
INFO:root:[   76] Training loss: 0.00734683, Validation loss: 0.00605055, Gradient norm: 0.59804260
INFO:root:[   77] Training loss: 0.00712726, Validation loss: 0.00641026, Gradient norm: 0.57760383
INFO:root:[   78] Training loss: 0.00731799, Validation loss: 0.00841756, Gradient norm: 0.59132590
INFO:root:[   79] Training loss: 0.00771106, Validation loss: 0.00783828, Gradient norm: 0.46756822
INFO:root:[   80] Training loss: 0.00631286, Validation loss: 0.00657605, Gradient norm: 0.33237456
INFO:root:[   81] Training loss: 0.00649251, Validation loss: 0.00601674, Gradient norm: 0.46128710
INFO:root:[   82] Training loss: 0.00718552, Validation loss: 0.00606095, Gradient norm: 0.58455114
INFO:root:EP 82: Early stopping
INFO:root:Training the model took 3169.754s.
INFO:root:Emptying the cuda cache took 0.088s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00606
INFO:root:EnergyScoreTrain: 0.00573
INFO:root:CoverageTrain: 0.99725
INFO:root:IntervalWidthTrain: 0.05996
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00597
INFO:root:EnergyScoreValidation: 0.00568
INFO:root:CoverageValidation: 0.99731
INFO:root:IntervalWidthValidation: 0.05957
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.00877
INFO:root:EnergyScoreTest: 0.00688
INFO:root:CoverageTest: 0.99182
INFO:root:IntervalWidthTest: 0.05884
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 526385152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.19733042, Validation loss: 0.04998622, Gradient norm: 2.30465074
INFO:root:[    2] Training loss: 0.03690193, Validation loss: 0.03067345, Gradient norm: 2.27732365
INFO:root:[    3] Training loss: 0.02635719, Validation loss: 0.02056446, Gradient norm: 1.60842571
INFO:root:[    4] Training loss: 0.02825461, Validation loss: 0.04025381, Gradient norm: 1.98907583
INFO:root:[    5] Training loss: 0.02634839, Validation loss: 0.02667464, Gradient norm: 2.02533336
INFO:root:[    6] Training loss: 0.02307981, Validation loss: 0.03116123, Gradient norm: 1.78327676
INFO:root:[    7] Training loss: 0.02173067, Validation loss: 0.02562284, Gradient norm: 1.75733493
INFO:root:[    8] Training loss: 0.01936239, Validation loss: 0.02324622, Gradient norm: 1.53603945
INFO:root:[    9] Training loss: 0.02002292, Validation loss: 0.01581351, Gradient norm: 1.55677397
INFO:root:[   10] Training loss: 0.01739888, Validation loss: 0.01202992, Gradient norm: 1.33488274
INFO:root:[   11] Training loss: 0.01722552, Validation loss: 0.01534059, Gradient norm: 1.37203227
INFO:root:[   12] Training loss: 0.01632104, Validation loss: 0.01391294, Gradient norm: 1.32192649
INFO:root:[   13] Training loss: 0.01611735, Validation loss: 0.01108088, Gradient norm: 1.19402539
INFO:root:[   14] Training loss: 0.01556843, Validation loss: 0.01459157, Gradient norm: 1.18171831
INFO:root:[   15] Training loss: 0.01567675, Validation loss: 0.01327954, Gradient norm: 1.25280151
INFO:root:[   16] Training loss: 0.01538126, Validation loss: 0.01285450, Gradient norm: 1.28356995
INFO:root:[   17] Training loss: 0.01474363, Validation loss: 0.01307386, Gradient norm: 1.20423444
INFO:root:[   18] Training loss: 0.01517924, Validation loss: 0.01169965, Gradient norm: 1.25251555
INFO:root:[   19] Training loss: 0.01455099, Validation loss: 0.01168196, Gradient norm: 1.14323550
INFO:root:[   20] Training loss: 0.01428389, Validation loss: 0.01992474, Gradient norm: 1.08773136
INFO:root:[   21] Training loss: 0.01481585, Validation loss: 0.01031266, Gradient norm: 1.15001184
INFO:root:[   22] Training loss: 0.01363862, Validation loss: 0.01033991, Gradient norm: 1.02273552
INFO:root:[   23] Training loss: 0.01363557, Validation loss: 0.00993816, Gradient norm: 1.07701336
INFO:root:[   24] Training loss: 0.01279942, Validation loss: 0.00945324, Gradient norm: 1.01384029
INFO:root:[   25] Training loss: 0.01303517, Validation loss: 0.00991117, Gradient norm: 1.00218737
INFO:root:[   26] Training loss: 0.01275067, Validation loss: 0.01073180, Gradient norm: 1.01866746
INFO:root:[   27] Training loss: 0.01254864, Validation loss: 0.01602085, Gradient norm: 0.99109408
INFO:root:[   28] Training loss: 0.01334503, Validation loss: 0.01384471, Gradient norm: 1.11032782
INFO:root:[   29] Training loss: 0.01263020, Validation loss: 0.00964461, Gradient norm: 1.03590695
INFO:root:[   30] Training loss: 0.01196329, Validation loss: 0.01123499, Gradient norm: 0.92238866
INFO:root:[   31] Training loss: 0.01201821, Validation loss: 0.01244963, Gradient norm: 0.97774268
INFO:root:[   32] Training loss: 0.01187015, Validation loss: 0.01207419, Gradient norm: 0.95332204
INFO:root:[   33] Training loss: 0.01247268, Validation loss: 0.01036522, Gradient norm: 0.99795241
INFO:root:[   34] Training loss: 0.01196763, Validation loss: 0.01314977, Gradient norm: 0.90100099
INFO:root:[   35] Training loss: 0.01273152, Validation loss: 0.00915542, Gradient norm: 0.99420862
INFO:root:[   36] Training loss: 0.01158502, Validation loss: 0.00964958, Gradient norm: 0.90502322
INFO:root:[   37] Training loss: 0.01104231, Validation loss: 0.01494594, Gradient norm: 0.92557658
INFO:root:[   38] Training loss: 0.01145838, Validation loss: 0.01011785, Gradient norm: 0.97821168
INFO:root:[   39] Training loss: 0.01075974, Validation loss: 0.00961253, Gradient norm: 0.90853224
INFO:root:[   40] Training loss: 0.01100046, Validation loss: 0.01029356, Gradient norm: 0.93355395
INFO:root:[   41] Training loss: 0.01141169, Validation loss: 0.00906035, Gradient norm: 0.83906591
INFO:root:[   42] Training loss: 0.01040362, Validation loss: 0.01154113, Gradient norm: 0.74740879
INFO:root:[   43] Training loss: 0.01050135, Validation loss: 0.01381347, Gradient norm: 0.87753163
INFO:root:[   44] Training loss: 0.01042700, Validation loss: 0.01199280, Gradient norm: 0.89856226
INFO:root:[   45] Training loss: 0.00988427, Validation loss: 0.01199216, Gradient norm: 0.85173571
INFO:root:[   46] Training loss: 0.01023218, Validation loss: 0.00882572, Gradient norm: 0.85169094
INFO:root:[   47] Training loss: 0.01033573, Validation loss: 0.00746437, Gradient norm: 0.83462512
INFO:root:[   48] Training loss: 0.01003962, Validation loss: 0.00975177, Gradient norm: 0.81102492
INFO:root:[   49] Training loss: 0.00997953, Validation loss: 0.00778955, Gradient norm: 0.75509792
INFO:root:[   50] Training loss: 0.00981904, Validation loss: 0.01051495, Gradient norm: 0.81586151
INFO:root:[   51] Training loss: 0.01018272, Validation loss: 0.00981844, Gradient norm: 0.83990533
INFO:root:[   52] Training loss: 0.00960126, Validation loss: 0.00669818, Gradient norm: 0.80614637
INFO:root:[   53] Training loss: 0.00930932, Validation loss: 0.01190020, Gradient norm: 0.78225385
INFO:root:[   54] Training loss: 0.00947685, Validation loss: 0.00822981, Gradient norm: 0.80728036
INFO:root:[   55] Training loss: 0.00920037, Validation loss: 0.00842245, Gradient norm: 0.74848593
INFO:root:[   56] Training loss: 0.00914464, Validation loss: 0.00908491, Gradient norm: 0.74581201
INFO:root:[   57] Training loss: 0.00844216, Validation loss: 0.01044569, Gradient norm: 0.59560897
INFO:root:[   58] Training loss: 0.00917754, Validation loss: 0.01019513, Gradient norm: 0.64575682
INFO:root:[   59] Training loss: 0.00882255, Validation loss: 0.01065187, Gradient norm: 0.67400825
INFO:root:[   60] Training loss: 0.00904009, Validation loss: 0.00860433, Gradient norm: 0.74745019
INFO:root:[   61] Training loss: 0.00913508, Validation loss: 0.00777366, Gradient norm: 0.77616921
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 2284.838s.
INFO:root:Emptying the cuda cache took 0.094s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0078
INFO:root:EnergyScoreTrain: 0.00672
INFO:root:CoverageTrain: 0.99613
INFO:root:IntervalWidthTrain: 0.0662
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00771
INFO:root:EnergyScoreValidation: 0.00667
INFO:root:CoverageValidation: 0.99614
INFO:root:IntervalWidthValidation: 0.0658
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01119
INFO:root:EnergyScoreTest: 0.00835
INFO:root:CoverageTest: 0.98856
INFO:root:IntervalWidthTest: 0.06437
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.17981406, Validation loss: 0.03630363, Gradient norm: 1.80728476
INFO:root:[    2] Training loss: 0.03311737, Validation loss: 0.02299827, Gradient norm: 1.61674494
INFO:root:[    3] Training loss: 0.02801568, Validation loss: 0.02115979, Gradient norm: 1.46745930
INFO:root:[    4] Training loss: 0.02485548, Validation loss: 0.02129846, Gradient norm: 1.42679483
INFO:root:[    5] Training loss: 0.02247219, Validation loss: 0.01913830, Gradient norm: 1.44127702
INFO:root:[    6] Training loss: 0.02036125, Validation loss: 0.01646688, Gradient norm: 1.29933737
INFO:root:[    7] Training loss: 0.01872646, Validation loss: 0.01744226, Gradient norm: 1.22795110
INFO:root:[    8] Training loss: 0.01799917, Validation loss: 0.01584514, Gradient norm: 1.20056703
INFO:root:[    9] Training loss: 0.01690599, Validation loss: 0.01518427, Gradient norm: 1.10791315
INFO:root:[   10] Training loss: 0.01705496, Validation loss: 0.01542882, Gradient norm: 1.10260633
INFO:root:[   11] Training loss: 0.01635711, Validation loss: 0.02007858, Gradient norm: 1.07147088
INFO:root:[   12] Training loss: 0.01695708, Validation loss: 0.01823224, Gradient norm: 1.08483605
INFO:root:[   13] Training loss: 0.01618253, Validation loss: 0.01287357, Gradient norm: 1.08391268
INFO:root:[   14] Training loss: 0.01482092, Validation loss: 0.01215080, Gradient norm: 0.98463515
INFO:root:[   15] Training loss: 0.01473387, Validation loss: 0.01619746, Gradient norm: 0.95988924
INFO:root:[   16] Training loss: 0.01472834, Validation loss: 0.01326269, Gradient norm: 0.89052113
INFO:root:[   17] Training loss: 0.01470467, Validation loss: 0.01055330, Gradient norm: 0.95882412
INFO:root:[   18] Training loss: 0.01461142, Validation loss: 0.01203756, Gradient norm: 1.01081951
INFO:root:[   19] Training loss: 0.01496808, Validation loss: 0.01530856, Gradient norm: 1.00617136
INFO:root:[   20] Training loss: 0.01267959, Validation loss: 0.01129067, Gradient norm: 0.83048252
INFO:root:[   21] Training loss: 0.01347775, Validation loss: 0.00963316, Gradient norm: 0.85264845
INFO:root:[   22] Training loss: 0.01344429, Validation loss: 0.01126764, Gradient norm: 0.90205512
INFO:root:[   23] Training loss: 0.01327441, Validation loss: 0.01788801, Gradient norm: 0.86200044
INFO:root:[   24] Training loss: 0.01299288, Validation loss: 0.01210697, Gradient norm: 0.86713538
INFO:root:[   25] Training loss: 0.01265234, Validation loss: 0.00992354, Gradient norm: 0.87051518
INFO:root:[   26] Training loss: 0.01176332, Validation loss: 0.01144286, Gradient norm: 0.76609408
INFO:root:[   27] Training loss: 0.01131435, Validation loss: 0.01146754, Gradient norm: 0.78881214
INFO:root:[   28] Training loss: 0.01145961, Validation loss: 0.01190491, Gradient norm: 0.81514448
INFO:root:[   29] Training loss: 0.01152308, Validation loss: 0.00963253, Gradient norm: 0.85113435
INFO:root:[   30] Training loss: 0.01142174, Validation loss: 0.00853752, Gradient norm: 0.82525199
INFO:root:[   31] Training loss: 0.01176941, Validation loss: 0.01396252, Gradient norm: 0.81162516
INFO:root:[   32] Training loss: 0.01155490, Validation loss: 0.01079676, Gradient norm: 0.78257038
INFO:root:[   33] Training loss: 0.01135501, Validation loss: 0.01267419, Gradient norm: 0.80688504
INFO:root:[   34] Training loss: 0.01124497, Validation loss: 0.00803234, Gradient norm: 0.85047776
INFO:root:[   35] Training loss: 0.01051079, Validation loss: 0.01258530, Gradient norm: 0.76845395
INFO:root:[   36] Training loss: 0.01028391, Validation loss: 0.01065452, Gradient norm: 0.70014528
INFO:root:[   37] Training loss: 0.01073767, Validation loss: 0.00923524, Gradient norm: 0.74919450
INFO:root:[   38] Training loss: 0.00994907, Validation loss: 0.01165427, Gradient norm: 0.71430058
INFO:root:[   39] Training loss: 0.00978186, Validation loss: 0.00948119, Gradient norm: 0.70778264
INFO:root:[   40] Training loss: 0.00953598, Validation loss: 0.00953226, Gradient norm: 0.69135074
INFO:root:[   41] Training loss: 0.00976121, Validation loss: 0.00846299, Gradient norm: 0.74172998
INFO:root:[   42] Training loss: 0.00988096, Validation loss: 0.01056892, Gradient norm: 0.71070218
INFO:root:[   43] Training loss: 0.00956182, Validation loss: 0.00701821, Gradient norm: 0.69015323
INFO:root:[   44] Training loss: 0.00975526, Validation loss: 0.01053172, Gradient norm: 0.71870537
INFO:root:[   45] Training loss: 0.00928870, Validation loss: 0.00746939, Gradient norm: 0.69923712
INFO:root:[   46] Training loss: 0.00919438, Validation loss: 0.01135866, Gradient norm: 0.72020151
INFO:root:[   47] Training loss: 0.00893831, Validation loss: 0.00697784, Gradient norm: 0.66179370
INFO:root:[   48] Training loss: 0.00888710, Validation loss: 0.00993469, Gradient norm: 0.67342995
INFO:root:[   49] Training loss: 0.00891036, Validation loss: 0.00824318, Gradient norm: 0.66928972
INFO:root:[   50] Training loss: 0.00849817, Validation loss: 0.00621754, Gradient norm: 0.65504027
INFO:root:[   51] Training loss: 0.00843059, Validation loss: 0.01005370, Gradient norm: 0.67060446
INFO:root:[   52] Training loss: 0.00882810, Validation loss: 0.00672008, Gradient norm: 0.74010055
INFO:root:[   53] Training loss: 0.00907756, Validation loss: 0.00837711, Gradient norm: 0.70289501
INFO:root:[   54] Training loss: 0.00875171, Validation loss: 0.01019087, Gradient norm: 0.66680357
INFO:root:[   55] Training loss: 0.00827721, Validation loss: 0.00755018, Gradient norm: 0.57843503
INFO:root:[   56] Training loss: 0.00811322, Validation loss: 0.00778746, Gradient norm: 0.53407142
INFO:root:[   57] Training loss: 0.00744604, Validation loss: 0.00617572, Gradient norm: 0.49164074
INFO:root:[   58] Training loss: 0.00777459, Validation loss: 0.00740302, Gradient norm: 0.59381852
INFO:root:[   59] Training loss: 0.00849367, Validation loss: 0.00954283, Gradient norm: 0.68557408
INFO:root:[   60] Training loss: 0.00804865, Validation loss: 0.00600848, Gradient norm: 0.64155915
INFO:root:[   61] Training loss: 0.00763358, Validation loss: 0.00824254, Gradient norm: 0.60881733
INFO:root:[   62] Training loss: 0.00731939, Validation loss: 0.00960027, Gradient norm: 0.57326985
INFO:root:[   63] Training loss: 0.00808749, Validation loss: 0.00700833, Gradient norm: 0.61760776
INFO:root:[   64] Training loss: 0.00801961, Validation loss: 0.00801498, Gradient norm: 0.63018986
INFO:root:[   65] Training loss: 0.00770150, Validation loss: 0.00783802, Gradient norm: 0.57993308
INFO:root:[   66] Training loss: 0.00805728, Validation loss: 0.00915228, Gradient norm: 0.63522849
INFO:root:[   67] Training loss: 0.00779996, Validation loss: 0.00642224, Gradient norm: 0.60414603
INFO:root:[   68] Training loss: 0.00725378, Validation loss: 0.00712560, Gradient norm: 0.56011861
INFO:root:[   69] Training loss: 0.00711247, Validation loss: 0.00871122, Gradient norm: 0.55722156
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2579.163s.
INFO:root:Emptying the cuda cache took 0.088s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00708
INFO:root:EnergyScoreTrain: 0.00604
INFO:root:CoverageTrain: 0.99094
INFO:root:IntervalWidthTrain: 0.05873
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00699
INFO:root:EnergyScoreValidation: 0.00599
INFO:root:CoverageValidation: 0.99103
INFO:root:IntervalWidthValidation: 0.05837
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01
INFO:root:EnergyScoreTest: 0.00745
INFO:root:CoverageTest: 0.98097
INFO:root:IntervalWidthTest: 0.0573
INFO:root:###6 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.22583337, Validation loss: 0.04679098, Gradient norm: 2.21108029
INFO:root:[    2] Training loss: 0.03469283, Validation loss: 0.03261963, Gradient norm: 1.48411758
INFO:root:[    3] Training loss: 0.02774066, Validation loss: 0.03563215, Gradient norm: 1.69505322
INFO:root:[    4] Training loss: 0.03060317, Validation loss: 0.03423960, Gradient norm: 1.97875832
INFO:root:[    5] Training loss: 0.02634255, Validation loss: 0.03816379, Gradient norm: 1.80167563
INFO:root:[    6] Training loss: 0.02321323, Validation loss: 0.01814708, Gradient norm: 1.60923474
INFO:root:[    7] Training loss: 0.02073171, Validation loss: 0.01674499, Gradient norm: 1.50856410
INFO:root:[    8] Training loss: 0.01822593, Validation loss: 0.01569431, Gradient norm: 1.27406500
INFO:root:[    9] Training loss: 0.01759399, Validation loss: 0.01700182, Gradient norm: 1.28552410
INFO:root:[   10] Training loss: 0.01678224, Validation loss: 0.01210933, Gradient norm: 1.17447670
INFO:root:[   11] Training loss: 0.01607984, Validation loss: 0.01323741, Gradient norm: 1.13616972
INFO:root:[   12] Training loss: 0.01628757, Validation loss: 0.01292328, Gradient norm: 1.15406002
INFO:root:[   13] Training loss: 0.01645507, Validation loss: 0.01850731, Gradient norm: 1.15523674
INFO:root:[   14] Training loss: 0.01548952, Validation loss: 0.01087980, Gradient norm: 1.09794469
INFO:root:[   15] Training loss: 0.01421504, Validation loss: 0.01234368, Gradient norm: 0.99052182
INFO:root:[   16] Training loss: 0.01569640, Validation loss: 0.01488172, Gradient norm: 1.08414001
INFO:root:[   17] Training loss: 0.01479988, Validation loss: 0.01068232, Gradient norm: 0.99001878
INFO:root:[   18] Training loss: 0.01446573, Validation loss: 0.01427168, Gradient norm: 0.94761139
INFO:root:[   19] Training loss: 0.01463663, Validation loss: 0.01139200, Gradient norm: 1.02409652
INFO:root:[   20] Training loss: 0.01443287, Validation loss: 0.01382345, Gradient norm: 0.99639236
INFO:root:[   21] Training loss: 0.01448969, Validation loss: 0.01971879, Gradient norm: 0.98088508
INFO:root:[   22] Training loss: 0.01408616, Validation loss: 0.01572278, Gradient norm: 0.95891053
INFO:root:[   23] Training loss: 0.01320477, Validation loss: 0.00971540, Gradient norm: 0.96545489
INFO:root:[   24] Training loss: 0.01314419, Validation loss: 0.01209035, Gradient norm: 0.92440162
INFO:root:[   25] Training loss: 0.01355729, Validation loss: 0.01104583, Gradient norm: 0.95880940
INFO:root:[   26] Training loss: 0.01276150, Validation loss: 0.01778795, Gradient norm: 0.87675229
INFO:root:[   27] Training loss: 0.01414108, Validation loss: 0.01209889, Gradient norm: 1.03200433
INFO:root:[   28] Training loss: 0.01228163, Validation loss: 0.00982940, Gradient norm: 0.85916150
INFO:root:[   29] Training loss: 0.01218208, Validation loss: 0.01411475, Gradient norm: 0.82404063
INFO:root:[   30] Training loss: 0.01207759, Validation loss: 0.01357158, Gradient norm: 0.83434889
INFO:root:[   31] Training loss: 0.01215158, Validation loss: 0.01197523, Gradient norm: 0.87408806
INFO:root:[   32] Training loss: 0.01268106, Validation loss: 0.01344647, Gradient norm: 0.95378916
INFO:root:[   33] Training loss: 0.01203065, Validation loss: 0.00854587, Gradient norm: 0.93806861
INFO:root:[   34] Training loss: 0.01257793, Validation loss: 0.01487197, Gradient norm: 0.84632344
INFO:root:[   35] Training loss: 0.01222127, Validation loss: 0.01499088, Gradient norm: 0.79272724
INFO:root:[   36] Training loss: 0.01099207, Validation loss: 0.01049105, Gradient norm: 0.78352555
INFO:root:[   37] Training loss: 0.01120956, Validation loss: 0.01334669, Gradient norm: 0.88329150
INFO:root:[   38] Training loss: 0.01065259, Validation loss: 0.01286534, Gradient norm: 0.83995218
INFO:root:[   39] Training loss: 0.01065788, Validation loss: 0.01051027, Gradient norm: 0.85032182
INFO:root:[   40] Training loss: 0.01039792, Validation loss: 0.01252110, Gradient norm: 0.67778272
INFO:root:[   41] Training loss: 0.01111157, Validation loss: 0.01080051, Gradient norm: 0.71663289
INFO:root:[   42] Training loss: 0.01064518, Validation loss: 0.00927727, Gradient norm: 0.76407117
INFO:root:[   43] Training loss: 0.01036767, Validation loss: 0.00983331, Gradient norm: 0.83367541
INFO:root:[   44] Training loss: 0.01033005, Validation loss: 0.01395362, Gradient norm: 0.86047304
INFO:root:[   45] Training loss: 0.01084531, Validation loss: 0.00745931, Gradient norm: 0.87520282
INFO:root:[   46] Training loss: 0.00981133, Validation loss: 0.01198587, Gradient norm: 0.79537449
INFO:root:[   47] Training loss: 0.00971372, Validation loss: 0.00993262, Gradient norm: 0.79535208
INFO:root:[   48] Training loss: 0.00947147, Validation loss: 0.01035237, Gradient norm: 0.73041856
INFO:root:[   49] Training loss: 0.00951184, Validation loss: 0.01241890, Gradient norm: 0.69715024
INFO:root:[   50] Training loss: 0.00961228, Validation loss: 0.00970201, Gradient norm: 0.70563209
INFO:root:[   51] Training loss: 0.00960198, Validation loss: 0.01169257, Gradient norm: 0.70179595
INFO:root:[   52] Training loss: 0.00954508, Validation loss: 0.00970959, Gradient norm: 0.74976859
INFO:root:[   53] Training loss: 0.00913586, Validation loss: 0.00747875, Gradient norm: 0.76815950
INFO:root:[   54] Training loss: 0.00872532, Validation loss: 0.00724150, Gradient norm: 0.73681761
INFO:root:[   55] Training loss: 0.00968279, Validation loss: 0.01175489, Gradient norm: 0.79235967
INFO:root:[   56] Training loss: 0.00914196, Validation loss: 0.00932755, Gradient norm: 0.72869563
INFO:root:[   57] Training loss: 0.00837651, Validation loss: 0.01072008, Gradient norm: 0.64664632
INFO:root:[   58] Training loss: 0.00891471, Validation loss: 0.00757362, Gradient norm: 0.66462019
INFO:root:[   59] Training loss: 0.00832268, Validation loss: 0.00933603, Gradient norm: 0.62509035
INFO:root:[   60] Training loss: 0.00880128, Validation loss: 0.00633792, Gradient norm: 0.73232188
INFO:root:[   61] Training loss: 0.00860130, Validation loss: 0.00904794, Gradient norm: 0.74619247
INFO:root:[   62] Training loss: 0.00913250, Validation loss: 0.00686316, Gradient norm: 0.79314212
INFO:root:[   63] Training loss: 0.00907062, Validation loss: 0.00804301, Gradient norm: 0.74845443
INFO:root:[   64] Training loss: 0.00893357, Validation loss: 0.00970380, Gradient norm: 0.72692533
INFO:root:[   65] Training loss: 0.00819038, Validation loss: 0.00723155, Gradient norm: 0.65517809
INFO:root:[   66] Training loss: 0.00788928, Validation loss: 0.00704064, Gradient norm: 0.61094368
INFO:root:[   67] Training loss: 0.00836353, Validation loss: 0.00728619, Gradient norm: 0.60732843
INFO:root:[   68] Training loss: 0.00791394, Validation loss: 0.00858849, Gradient norm: 0.55109435
INFO:root:[   69] Training loss: 0.00789011, Validation loss: 0.01036208, Gradient norm: 0.64066300
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2578.638s.
INFO:root:Emptying the cuda cache took 0.09s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00747
INFO:root:EnergyScoreTrain: 0.00637
INFO:root:CoverageTrain: 0.99437
INFO:root:IntervalWidthTrain: 0.06269
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00741
INFO:root:EnergyScoreValidation: 0.00632
INFO:root:CoverageValidation: 0.99433
INFO:root:IntervalWidthValidation: 0.0622
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01009
INFO:root:EnergyScoreTest: 0.00759
INFO:root:CoverageTest: 0.98533
INFO:root:IntervalWidthTest: 0.06111
INFO:root:###7 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234567, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.21441369, Validation loss: 0.02504726, Gradient norm: 1.39689501
INFO:root:[    2] Training loss: 0.02702183, Validation loss: 0.02139667, Gradient norm: 1.05032508
INFO:root:[    3] Training loss: 0.02362133, Validation loss: 0.02004621, Gradient norm: 1.08890377
INFO:root:[    4] Training loss: 0.02326292, Validation loss: 0.02471219, Gradient norm: 1.09679307
INFO:root:[    5] Training loss: 0.01812307, Validation loss: 0.01514585, Gradient norm: 0.88991008
INFO:root:[    6] Training loss: 0.01905672, Validation loss: 0.01333757, Gradient norm: 1.22484586
INFO:root:[    7] Training loss: 0.01696704, Validation loss: 0.01232998, Gradient norm: 1.02091147
INFO:root:[    8] Training loss: 0.01660404, Validation loss: 0.01394122, Gradient norm: 1.04599427
INFO:root:[    9] Training loss: 0.01517185, Validation loss: 0.01978249, Gradient norm: 0.92575093
INFO:root:[   10] Training loss: 0.01705292, Validation loss: 0.02256508, Gradient norm: 1.08698322
INFO:root:[   11] Training loss: 0.01573827, Validation loss: 0.01307068, Gradient norm: 0.96856023
INFO:root:[   12] Training loss: 0.01525998, Validation loss: 0.01066053, Gradient norm: 1.00882181
INFO:root:[   13] Training loss: 0.01439574, Validation loss: 0.01253135, Gradient norm: 0.91646458
INFO:root:[   14] Training loss: 0.01469168, Validation loss: 0.01641197, Gradient norm: 0.93599811
INFO:root:[   15] Training loss: 0.01506943, Validation loss: 0.01057254, Gradient norm: 0.96620315
INFO:root:[   16] Training loss: 0.01351083, Validation loss: 0.01560306, Gradient norm: 0.80301714
INFO:root:[   17] Training loss: 0.01368939, Validation loss: 0.01277315, Gradient norm: 0.89450949
INFO:root:[   18] Training loss: 0.01308630, Validation loss: 0.01103197, Gradient norm: 0.83353055
INFO:root:[   19] Training loss: 0.01357706, Validation loss: 0.01044197, Gradient norm: 0.87539154
INFO:root:[   20] Training loss: 0.01302859, Validation loss: 0.01112875, Gradient norm: 0.83964151
INFO:root:[   21] Training loss: 0.01311899, Validation loss: 0.01016583, Gradient norm: 0.88514428
INFO:root:[   22] Training loss: 0.01328201, Validation loss: 0.00939202, Gradient norm: 0.85797338
INFO:root:[   23] Training loss: 0.01269831, Validation loss: 0.01361683, Gradient norm: 0.84426296
INFO:root:[   24] Training loss: 0.01257851, Validation loss: 0.01362555, Gradient norm: 0.85832715
INFO:root:[   25] Training loss: 0.01198569, Validation loss: 0.01059472, Gradient norm: 0.82987390
INFO:root:[   26] Training loss: 0.01245069, Validation loss: 0.01145083, Gradient norm: 0.86467735
INFO:root:[   27] Training loss: 0.01172750, Validation loss: 0.01905104, Gradient norm: 0.82482687
INFO:root:[   28] Training loss: 0.01267251, Validation loss: 0.01243550, Gradient norm: 0.90385056
INFO:root:[   29] Training loss: 0.01152489, Validation loss: 0.01266708, Gradient norm: 0.74887080
INFO:root:[   30] Training loss: 0.01100068, Validation loss: 0.01363696, Gradient norm: 0.73127765
INFO:root:[   31] Training loss: 0.01110820, Validation loss: 0.01021415, Gradient norm: 0.77245111
INFO:root:[   32] Training loss: 0.01031632, Validation loss: 0.00833038, Gradient norm: 0.78008669
INFO:root:[   33] Training loss: 0.01080364, Validation loss: 0.01181098, Gradient norm: 0.81579826
INFO:root:[   34] Training loss: 0.01031660, Validation loss: 0.01348170, Gradient norm: 0.73987443
INFO:root:[   35] Training loss: 0.01062770, Validation loss: 0.00856262, Gradient norm: 0.78376670
INFO:root:[   36] Training loss: 0.01024852, Validation loss: 0.01247881, Gradient norm: 0.77390761
INFO:root:[   37] Training loss: 0.01032360, Validation loss: 0.00821335, Gradient norm: 0.77555708
INFO:root:[   38] Training loss: 0.00957847, Validation loss: 0.00976577, Gradient norm: 0.73226739
INFO:root:[   39] Training loss: 0.00946872, Validation loss: 0.01030780, Gradient norm: 0.70484776
INFO:root:[   40] Training loss: 0.00933047, Validation loss: 0.01146578, Gradient norm: 0.68963528
INFO:root:[   41] Training loss: 0.00970360, Validation loss: 0.00737447, Gradient norm: 0.76093437
INFO:root:[   42] Training loss: 0.00943857, Validation loss: 0.01188852, Gradient norm: 0.72351527
INFO:root:[   43] Training loss: 0.00940547, Validation loss: 0.00809377, Gradient norm: 0.75644306
INFO:root:[   44] Training loss: 0.00926085, Validation loss: 0.00738569, Gradient norm: 0.61550218
INFO:root:[   45] Training loss: 0.00874051, Validation loss: 0.00811499, Gradient norm: 0.64921224
INFO:root:[   46] Training loss: 0.00835812, Validation loss: 0.00966601, Gradient norm: 0.67127203
INFO:root:[   47] Training loss: 0.00866296, Validation loss: 0.00873278, Gradient norm: 0.71983160
INFO:root:[   48] Training loss: 0.00888599, Validation loss: 0.00741456, Gradient norm: 0.74798197
INFO:root:[   49] Training loss: 0.00884341, Validation loss: 0.01084929, Gradient norm: 0.70234293
INFO:root:[   50] Training loss: 0.00849493, Validation loss: 0.00643007, Gradient norm: 0.65696700
INFO:root:[   51] Training loss: 0.00802804, Validation loss: 0.00609487, Gradient norm: 0.64926118
INFO:root:[   52] Training loss: 0.00806829, Validation loss: 0.00783135, Gradient norm: 0.66234274
INFO:root:[   53] Training loss: 0.00780983, Validation loss: 0.00843799, Gradient norm: 0.61240539
INFO:root:[   54] Training loss: 0.00782506, Validation loss: 0.00912806, Gradient norm: 0.52634045
INFO:root:[   55] Training loss: 0.00795049, Validation loss: 0.01017358, Gradient norm: 0.60376670
INFO:root:[   56] Training loss: 0.00849711, Validation loss: 0.00986182, Gradient norm: 0.70169414
INFO:root:[   57] Training loss: 0.00832806, Validation loss: 0.00595749, Gradient norm: 0.70380527
INFO:root:[   58] Training loss: 0.00807561, Validation loss: 0.01033960, Gradient norm: 0.67088947
INFO:root:[   59] Training loss: 0.00844560, Validation loss: 0.00732421, Gradient norm: 0.69216768
INFO:root:[   60] Training loss: 0.00785131, Validation loss: 0.00654425, Gradient norm: 0.56123399
INFO:root:[   61] Training loss: 0.00772252, Validation loss: 0.00888458, Gradient norm: 0.63966029
INFO:root:[   62] Training loss: 0.00786953, Validation loss: 0.00898593, Gradient norm: 0.66334895
INFO:root:[   63] Training loss: 0.00776686, Validation loss: 0.00712166, Gradient norm: 0.65467640
INFO:root:[   64] Training loss: 0.00760101, Validation loss: 0.00905800, Gradient norm: 0.62189307
INFO:root:[   65] Training loss: 0.00740141, Validation loss: 0.00734769, Gradient norm: 0.60457471
INFO:root:[   66] Training loss: 0.00787392, Validation loss: 0.01034049, Gradient norm: 0.66512299
INFO:root:[   67] Training loss: 0.00770623, Validation loss: 0.00562064, Gradient norm: 0.63428252
INFO:root:[   68] Training loss: 0.00720772, Validation loss: 0.00798450, Gradient norm: 0.57012170
INFO:root:[   69] Training loss: 0.00695240, Validation loss: 0.00759470, Gradient norm: 0.53860895
INFO:root:[   70] Training loss: 0.00690776, Validation loss: 0.00835473, Gradient norm: 0.51351230
INFO:root:[   71] Training loss: 0.00711493, Validation loss: 0.00832524, Gradient norm: 0.54477659
INFO:root:[   72] Training loss: 0.00743634, Validation loss: 0.00639696, Gradient norm: 0.63231904
INFO:root:[   73] Training loss: 0.00703923, Validation loss: 0.00581762, Gradient norm: 0.59802863
INFO:root:[   74] Training loss: 0.00757507, Validation loss: 0.00861957, Gradient norm: 0.66222242
INFO:root:[   75] Training loss: 0.00704691, Validation loss: 0.00724495, Gradient norm: 0.56109334
INFO:root:[   76] Training loss: 0.00698269, Validation loss: 0.00598177, Gradient norm: 0.58442122
INFO:root:EP 76: Early stopping
INFO:root:Training the model took 2818.722s.
INFO:root:Emptying the cuda cache took 0.09s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00608
INFO:root:EnergyScoreTrain: 0.00564
INFO:root:CoverageTrain: 0.99729
INFO:root:IntervalWidthTrain: 0.05897
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00609
INFO:root:EnergyScoreValidation: 0.00563
INFO:root:CoverageValidation: 0.9971
INFO:root:IntervalWidthValidation: 0.05862
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.00837
INFO:root:EnergyScoreTest: 0.0066
INFO:root:CoverageTest: 0.99161
INFO:root:IntervalWidthTest: 0.05736
INFO:root:###8 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345678, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.16971009, Validation loss: 0.03943497, Gradient norm: 1.83452377
INFO:root:[    2] Training loss: 0.03408067, Validation loss: 0.03094506, Gradient norm: 1.33634178
INFO:root:[    3] Training loss: 0.02754700, Validation loss: 0.02018841, Gradient norm: 1.48375612
INFO:root:[    4] Training loss: 0.02399260, Validation loss: 0.02137470, Gradient norm: 1.43022511
INFO:root:[    5] Training loss: 0.02120170, Validation loss: 0.02042713, Gradient norm: 1.25027721
INFO:root:[    6] Training loss: 0.01895559, Validation loss: 0.02161124, Gradient norm: 1.17617158
INFO:root:[    7] Training loss: 0.01862156, Validation loss: 0.02593686, Gradient norm: 1.14299486
INFO:root:[    8] Training loss: 0.01803577, Validation loss: 0.01790898, Gradient norm: 1.12070016
INFO:root:[    9] Training loss: 0.01682688, Validation loss: 0.01333892, Gradient norm: 1.02882314
INFO:root:[   10] Training loss: 0.01604687, Validation loss: 0.01684923, Gradient norm: 0.97072554
INFO:root:[   11] Training loss: 0.01586675, Validation loss: 0.01771211, Gradient norm: 0.99565828
INFO:root:[   12] Training loss: 0.01522100, Validation loss: 0.01958056, Gradient norm: 0.94702674
INFO:root:[   13] Training loss: 0.01535310, Validation loss: 0.01744719, Gradient norm: 0.96247024
INFO:root:[   14] Training loss: 0.01471285, Validation loss: 0.01502931, Gradient norm: 0.87772723
INFO:root:[   15] Training loss: 0.01420160, Validation loss: 0.01313152, Gradient norm: 0.70532008
INFO:root:[   16] Training loss: 0.01420232, Validation loss: 0.01769028, Gradient norm: 0.88099925
INFO:root:[   17] Training loss: 0.01396006, Validation loss: 0.01780095, Gradient norm: 0.87848158
INFO:root:[   18] Training loss: 0.01392245, Validation loss: 0.01632812, Gradient norm: 0.90561462
INFO:root:[   19] Training loss: 0.01342524, Validation loss: 0.01411157, Gradient norm: 0.86908870
INFO:root:[   20] Training loss: 0.01314191, Validation loss: 0.01170731, Gradient norm: 0.86088910
INFO:root:[   21] Training loss: 0.01300880, Validation loss: 0.01014373, Gradient norm: 0.84925680
INFO:root:[   22] Training loss: 0.01249582, Validation loss: 0.01041073, Gradient norm: 0.79512124
INFO:root:[   23] Training loss: 0.01249516, Validation loss: 0.01397377, Gradient norm: 0.82405759
INFO:root:[   24] Training loss: 0.01197858, Validation loss: 0.01549610, Gradient norm: 0.79236210
INFO:root:[   25] Training loss: 0.01278786, Validation loss: 0.01074834, Gradient norm: 0.82737374
INFO:root:[   26] Training loss: 0.01175144, Validation loss: 0.01279294, Gradient norm: 0.72001206
INFO:root:[   27] Training loss: 0.01146221, Validation loss: 0.00992521, Gradient norm: 0.75275372
INFO:root:[   28] Training loss: 0.01170188, Validation loss: 0.01045685, Gradient norm: 0.81765885
INFO:root:[   29] Training loss: 0.01145518, Validation loss: 0.01181573, Gradient norm: 0.82301155
INFO:root:[   30] Training loss: 0.01126527, Validation loss: 0.01133111, Gradient norm: 0.73823111
INFO:root:[   31] Training loss: 0.01105880, Validation loss: 0.01120934, Gradient norm: 0.62803328
INFO:root:[   32] Training loss: 0.01094776, Validation loss: 0.00975110, Gradient norm: 0.76084808
INFO:root:[   33] Training loss: 0.01061015, Validation loss: 0.00802539, Gradient norm: 0.69724552
INFO:root:[   34] Training loss: 0.01041317, Validation loss: 0.00808420, Gradient norm: 0.73985090
INFO:root:[   35] Training loss: 0.01006782, Validation loss: 0.00844620, Gradient norm: 0.75561512
INFO:root:[   36] Training loss: 0.01039646, Validation loss: 0.01173277, Gradient norm: 0.77600179
INFO:root:[   37] Training loss: 0.01035466, Validation loss: 0.00816749, Gradient norm: 0.79887637
INFO:root:[   38] Training loss: 0.00988488, Validation loss: 0.01044918, Gradient norm: 0.75815777
INFO:root:[   39] Training loss: 0.00946131, Validation loss: 0.00988412, Gradient norm: 0.70358958
INFO:root:[   40] Training loss: 0.01024041, Validation loss: 0.00882470, Gradient norm: 0.73163204
INFO:root:[   41] Training loss: 0.00902183, Validation loss: 0.00833939, Gradient norm: 0.62570988
INFO:root:[   42] Training loss: 0.00916813, Validation loss: 0.01204909, Gradient norm: 0.65737828
INFO:root:[   43] Training loss: 0.00938427, Validation loss: 0.00938896, Gradient norm: 0.60434152
INFO:root:[   44] Training loss: 0.00893042, Validation loss: 0.01067855, Gradient norm: 0.68584134
INFO:root:[   45] Training loss: 0.00904741, Validation loss: 0.01072863, Gradient norm: 0.73102806
INFO:root:[   46] Training loss: 0.00936388, Validation loss: 0.00762913, Gradient norm: 0.79358880
INFO:root:[   47] Training loss: 0.00878342, Validation loss: 0.00978480, Gradient norm: 0.69658071
INFO:root:[   48] Training loss: 0.00842752, Validation loss: 0.01072183, Gradient norm: 0.58545866
INFO:root:[   49] Training loss: 0.00923318, Validation loss: 0.00965369, Gradient norm: 0.68978058
INFO:root:[   50] Training loss: 0.00901833, Validation loss: 0.00683780, Gradient norm: 0.73456160
INFO:root:[   51] Training loss: 0.00836018, Validation loss: 0.00986685, Gradient norm: 0.68825190
INFO:root:[   52] Training loss: 0.00828601, Validation loss: 0.00763557, Gradient norm: 0.70769736
INFO:root:[   53] Training loss: 0.00841034, Validation loss: 0.00899337, Gradient norm: 0.71992973
INFO:root:[   54] Training loss: 0.00840846, Validation loss: 0.00815606, Gradient norm: 0.66382056
INFO:root:[   55] Training loss: 0.00792696, Validation loss: 0.00744017, Gradient norm: 0.61064830
INFO:root:[   56] Training loss: 0.00821949, Validation loss: 0.00910924, Gradient norm: 0.62541103
INFO:root:[   57] Training loss: 0.00822571, Validation loss: 0.00865400, Gradient norm: 0.66062851
INFO:root:[   58] Training loss: 0.00809037, Validation loss: 0.00656754, Gradient norm: 0.67072801
INFO:root:[   59] Training loss: 0.00769260, Validation loss: 0.00739048, Gradient norm: 0.61254475
INFO:root:[   60] Training loss: 0.00775669, Validation loss: 0.00945730, Gradient norm: 0.62037218
INFO:root:[   61] Training loss: 0.00771238, Validation loss: 0.00740570, Gradient norm: 0.62768159
INFO:root:[   62] Training loss: 0.00781954, Validation loss: 0.00867430, Gradient norm: 0.66208292
INFO:root:[   63] Training loss: 0.00780217, Validation loss: 0.00887699, Gradient norm: 0.61264471
INFO:root:[   64] Training loss: 0.00777258, Validation loss: 0.00608521, Gradient norm: 0.63260432
INFO:root:[   65] Training loss: 0.00739483, Validation loss: 0.00656096, Gradient norm: 0.60791334
INFO:root:[   66] Training loss: 0.00759826, Validation loss: 0.00827216, Gradient norm: 0.62627518
INFO:root:[   67] Training loss: 0.00740634, Validation loss: 0.00588396, Gradient norm: 0.60546075
INFO:root:[   68] Training loss: 0.00716808, Validation loss: 0.00613543, Gradient norm: 0.60979995
INFO:root:[   69] Training loss: 0.00748526, Validation loss: 0.00860978, Gradient norm: 0.65934521
INFO:root:[   70] Training loss: 0.00748437, Validation loss: 0.00587462, Gradient norm: 0.67696610
INFO:root:[   71] Training loss: 0.00705665, Validation loss: 0.00916731, Gradient norm: 0.63037127
INFO:root:[   72] Training loss: 0.00757940, Validation loss: 0.00710093, Gradient norm: 0.61899744
INFO:root:[   73] Training loss: 0.00724521, Validation loss: 0.00751008, Gradient norm: 0.54760646
INFO:root:[   74] Training loss: 0.00671973, Validation loss: 0.00800551, Gradient norm: 0.54672150
INFO:root:[   75] Training loss: 0.00709631, Validation loss: 0.00595375, Gradient norm: 0.56462895
INFO:root:[   76] Training loss: 0.00651878, Validation loss: 0.00636317, Gradient norm: 0.50239480
INFO:root:[   77] Training loss: 0.00669102, Validation loss: 0.00530556, Gradient norm: 0.55605821
INFO:root:[   78] Training loss: 0.00701648, Validation loss: 0.00574376, Gradient norm: 0.63641427
INFO:root:[   79] Training loss: 0.00727982, Validation loss: 0.00918324, Gradient norm: 0.65725289
INFO:root:[   80] Training loss: 0.00719046, Validation loss: 0.00690336, Gradient norm: 0.61097541
INFO:root:[   81] Training loss: 0.00663055, Validation loss: 0.00582636, Gradient norm: 0.47521486
INFO:root:[   82] Training loss: 0.00630742, Validation loss: 0.00570866, Gradient norm: 0.47807174
INFO:root:[   83] Training loss: 0.00687511, Validation loss: 0.00576012, Gradient norm: 0.58627645
INFO:root:[   84] Training loss: 0.00685459, Validation loss: 0.00783343, Gradient norm: 0.57417093
INFO:root:[   85] Training loss: 0.00690672, Validation loss: 0.00723791, Gradient norm: 0.58696638
INFO:root:[   86] Training loss: 0.00689840, Validation loss: 0.00534940, Gradient norm: 0.60088255
INFO:root:EP 86: Early stopping
INFO:root:Training the model took 3161.157s.
INFO:root:Emptying the cuda cache took 0.089s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00569
INFO:root:EnergyScoreTrain: 0.00532
INFO:root:CoverageTrain: 0.99916
INFO:root:IntervalWidthTrain: 0.05601
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00566
INFO:root:EnergyScoreValidation: 0.00529
INFO:root:CoverageValidation: 0.99914
INFO:root:IntervalWidthValidation: 0.05572
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.00833
INFO:root:EnergyScoreTest: 0.00648
INFO:root:CoverageTest: 0.99247
INFO:root:IntervalWidthTest: 0.05517
INFO:root:###9 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456789, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.22782702, Validation loss: 0.04267499, Gradient norm: 1.91818358
INFO:root:[    2] Training loss: 0.02995347, Validation loss: 0.03820948, Gradient norm: 1.29575207
INFO:root:[    3] Training loss: 0.02722019, Validation loss: 0.02242966, Gradient norm: 1.30657387
INFO:root:[    4] Training loss: 0.02279811, Validation loss: 0.02661006, Gradient norm: 1.10706274
INFO:root:[    5] Training loss: 0.02072192, Validation loss: 0.02368399, Gradient norm: 1.19966183
INFO:root:[    6] Training loss: 0.02086567, Validation loss: 0.01674680, Gradient norm: 1.27000344
INFO:root:[    7] Training loss: 0.01822083, Validation loss: 0.01617365, Gradient norm: 1.11587795
INFO:root:[    8] Training loss: 0.01729456, Validation loss: 0.01528637, Gradient norm: 1.10421104
INFO:root:[    9] Training loss: 0.01689372, Validation loss: 0.02290357, Gradient norm: 1.03584992
INFO:root:[   10] Training loss: 0.01666634, Validation loss: 0.01322014, Gradient norm: 1.08961889
INFO:root:[   11] Training loss: 0.01635974, Validation loss: 0.01876076, Gradient norm: 0.99027821
INFO:root:[   12] Training loss: 0.01622343, Validation loss: 0.01290925, Gradient norm: 1.01650739
INFO:root:[   13] Training loss: 0.01508675, Validation loss: 0.02252711, Gradient norm: 0.93323181
INFO:root:[   14] Training loss: 0.01622346, Validation loss: 0.01110325, Gradient norm: 1.07660694
INFO:root:[   15] Training loss: 0.01577973, Validation loss: 0.01259981, Gradient norm: 1.01193058
INFO:root:[   16] Training loss: 0.01495677, Validation loss: 0.01490887, Gradient norm: 0.96975045
INFO:root:[   17] Training loss: 0.01525832, Validation loss: 0.01192580, Gradient norm: 0.90004101
INFO:root:[   18] Training loss: 0.01391017, Validation loss: 0.01334152, Gradient norm: 0.80744685
INFO:root:[   19] Training loss: 0.01381941, Validation loss: 0.01406232, Gradient norm: 0.92333635
INFO:root:[   20] Training loss: 0.01320932, Validation loss: 0.01074243, Gradient norm: 0.85683690
INFO:root:[   21] Training loss: 0.01413059, Validation loss: 0.01397408, Gradient norm: 0.93109792
INFO:root:[   22] Training loss: 0.01225607, Validation loss: 0.01462509, Gradient norm: 0.69565386
INFO:root:[   23] Training loss: 0.01263783, Validation loss: 0.00995147, Gradient norm: 0.81841172
INFO:root:[   24] Training loss: 0.01331611, Validation loss: 0.00967330, Gradient norm: 0.90737146
INFO:root:[   25] Training loss: 0.01281701, Validation loss: 0.00940251, Gradient norm: 0.88397093
INFO:root:[   26] Training loss: 0.01212793, Validation loss: 0.00926294, Gradient norm: 0.83492541
INFO:root:[   27] Training loss: 0.01213959, Validation loss: 0.01195856, Gradient norm: 0.79464110
INFO:root:[   28] Training loss: 0.01258044, Validation loss: 0.00968768, Gradient norm: 0.82564244
INFO:root:[   29] Training loss: 0.01236083, Validation loss: 0.00916880, Gradient norm: 0.84419538
INFO:root:[   30] Training loss: 0.01145419, Validation loss: 0.00866735, Gradient norm: 0.76556532
INFO:root:[   31] Training loss: 0.01100789, Validation loss: 0.01233173, Gradient norm: 0.69359363
INFO:root:[   32] Training loss: 0.01141457, Validation loss: 0.01284871, Gradient norm: 0.81170738
INFO:root:[   33] Training loss: 0.01174340, Validation loss: 0.00797050, Gradient norm: 0.89498971
INFO:root:[   34] Training loss: 0.01122041, Validation loss: 0.01353065, Gradient norm: 0.85145394
INFO:root:[   35] Training loss: 0.01121117, Validation loss: 0.00967051, Gradient norm: 0.84522708
INFO:root:[   36] Training loss: 0.01063771, Validation loss: 0.00846701, Gradient norm: 0.77560816
INFO:root:[   37] Training loss: 0.01045090, Validation loss: 0.00993742, Gradient norm: 0.73663954
INFO:root:[   38] Training loss: 0.01055440, Validation loss: 0.01421650, Gradient norm: 0.76951051
INFO:root:[   39] Training loss: 0.01067925, Validation loss: 0.01225752, Gradient norm: 0.84008702
INFO:root:[   40] Training loss: 0.01009114, Validation loss: 0.00963651, Gradient norm: 0.79402107
INFO:root:[   41] Training loss: 0.00984375, Validation loss: 0.00713676, Gradient norm: 0.78611524
INFO:root:[   42] Training loss: 0.00970563, Validation loss: 0.01089639, Gradient norm: 0.75722712
INFO:root:[   43] Training loss: 0.00964381, Validation loss: 0.00875543, Gradient norm: 0.59771449
INFO:root:[   44] Training loss: 0.00921656, Validation loss: 0.00773191, Gradient norm: 0.51844438
INFO:root:[   45] Training loss: 0.00954201, Validation loss: 0.00769239, Gradient norm: 0.72836318
INFO:root:[   46] Training loss: 0.00926235, Validation loss: 0.00895170, Gradient norm: 0.74296068
INFO:root:[   47] Training loss: 0.00949109, Validation loss: 0.00778058, Gradient norm: 0.79995713
INFO:root:[   48] Training loss: 0.00975100, Validation loss: 0.01201415, Gradient norm: 0.80419757
INFO:root:[   49] Training loss: 0.00935166, Validation loss: 0.00992394, Gradient norm: 0.78400077
INFO:root:[   50] Training loss: 0.00872818, Validation loss: 0.01011949, Gradient norm: 0.67865194
INFO:root:[   51] Training loss: 0.00885222, Validation loss: 0.00656102, Gradient norm: 0.69689321
INFO:root:[   52] Training loss: 0.00871217, Validation loss: 0.00877202, Gradient norm: 0.68993104
INFO:root:[   53] Training loss: 0.00873605, Validation loss: 0.00821676, Gradient norm: 0.47496917
INFO:root:[   54] Training loss: 0.00732524, Validation loss: 0.00763181, Gradient norm: 0.39359529
INFO:root:[   55] Training loss: 0.00828665, Validation loss: 0.01049539, Gradient norm: 0.66437128
INFO:root:[   56] Training loss: 0.00894288, Validation loss: 0.00825619, Gradient norm: 0.75211795
INFO:root:[   57] Training loss: 0.00928988, Validation loss: 0.01026372, Gradient norm: 0.76790736
INFO:root:[   58] Training loss: 0.00892509, Validation loss: 0.00694024, Gradient norm: 0.73125932
INFO:root:[   59] Training loss: 0.00835649, Validation loss: 0.01008224, Gradient norm: 0.67684399
INFO:root:[   60] Training loss: 0.00840839, Validation loss: 0.00750084, Gradient norm: 0.71168409
INFO:root:[   61] Training loss: 0.00880428, Validation loss: 0.00819441, Gradient norm: 0.63005724
INFO:root:[   62] Training loss: 0.00743268, Validation loss: 0.00774025, Gradient norm: 0.46415140
INFO:root:[   63] Training loss: 0.00793043, Validation loss: 0.00919158, Gradient norm: 0.59272392
INFO:root:[   64] Training loss: 0.00788023, Validation loss: 0.00897900, Gradient norm: 0.64121890
INFO:root:[   65] Training loss: 0.00793722, Validation loss: 0.00939573, Gradient norm: 0.64783736
INFO:root:[   66] Training loss: 0.00798578, Validation loss: 0.00585872, Gradient norm: 0.66560643
INFO:root:[   67] Training loss: 0.00745186, Validation loss: 0.00643262, Gradient norm: 0.61097371
INFO:root:[   68] Training loss: 0.00832215, Validation loss: 0.00925103, Gradient norm: 0.69145567
INFO:root:[   69] Training loss: 0.00814656, Validation loss: 0.00731549, Gradient norm: 0.66997223
INFO:root:[   70] Training loss: 0.00773954, Validation loss: 0.00729936, Gradient norm: 0.57101090
INFO:root:[   71] Training loss: 0.00701108, Validation loss: 0.00635842, Gradient norm: 0.53172412
INFO:root:[   72] Training loss: 0.00761752, Validation loss: 0.00662052, Gradient norm: 0.62216268
INFO:root:[   73] Training loss: 0.00757044, Validation loss: 0.00847440, Gradient norm: 0.62584509
INFO:root:[   74] Training loss: 0.00741068, Validation loss: 0.00621984, Gradient norm: 0.61630988
INFO:root:[   75] Training loss: 0.00781293, Validation loss: 0.00919711, Gradient norm: 0.65308534
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 2762.747s.
INFO:root:Emptying the cuda cache took 0.088s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00613
INFO:root:EnergyScoreTrain: 0.00587
INFO:root:CoverageTrain: 0.998
INFO:root:IntervalWidthTrain: 0.06238
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.00613
INFO:root:EnergyScoreValidation: 0.00585
INFO:root:CoverageValidation: 0.99794
INFO:root:IntervalWidthValidation: 0.06204
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.00825
INFO:root:EnergyScoreTest: 0.0067
INFO:root:CoverageTest: 0.99247
INFO:root:IntervalWidthTest: 0.06096
