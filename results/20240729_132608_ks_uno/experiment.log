INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 128.18635047, Validation loss: 122.18041162, Gradient norm: 869.25507913
INFO:root:[    2] Training loss: 121.95595154, Validation loss: 121.53054769, Gradient norm: 605.78093633
INFO:root:[    3] Training loss: 120.89456553, Validation loss: 120.05331287, Gradient norm: 562.57334468
INFO:root:[    4] Training loss: 119.67750186, Validation loss: 119.44623204, Gradient norm: 377.32946574
INFO:root:[    5] Training loss: 119.01380714, Validation loss: 118.80792330, Gradient norm: 317.75982832
INFO:root:[    6] Training loss: 118.62175347, Validation loss: 119.01065813, Gradient norm: 214.15028435
INFO:root:[    7] Training loss: 118.38111599, Validation loss: 118.43036304, Gradient norm: 239.67040070
INFO:root:[    8] Training loss: 118.12104272, Validation loss: 118.18641167, Gradient norm: 258.01183949
INFO:root:[    9] Training loss: 117.91499451, Validation loss: 117.82070642, Gradient norm: 280.44898005
INFO:root:[   10] Training loss: 117.68533512, Validation loss: 117.74309593, Gradient norm: 346.05169230
INFO:root:[   11] Training loss: 117.41128872, Validation loss: 117.43429673, Gradient norm: 331.78446396
INFO:root:[   12] Training loss: 117.15937371, Validation loss: 117.03302671, Gradient norm: 321.32457623
INFO:root:[   13] Training loss: 116.99133548, Validation loss: 116.95033612, Gradient norm: 346.32704739
INFO:root:[   14] Training loss: 116.78676497, Validation loss: 116.71788480, Gradient norm: 341.27871497
INFO:root:[   15] Training loss: 116.52108232, Validation loss: 116.36945865, Gradient norm: 354.48008482
INFO:root:[   16] Training loss: 116.19850037, Validation loss: 116.06604151, Gradient norm: 328.27429629
INFO:root:[   17] Training loss: 115.96573279, Validation loss: 115.89110378, Gradient norm: 356.68886640
INFO:root:[   18] Training loss: 115.57677629, Validation loss: 115.45537969, Gradient norm: 315.02599336
INFO:root:[   19] Training loss: 115.26673045, Validation loss: 115.34756845, Gradient norm: 334.00251191
INFO:root:[   20] Training loss: 115.00523102, Validation loss: 115.38175027, Gradient norm: 304.21273576
INFO:root:[   21] Training loss: 114.80383575, Validation loss: 114.85074682, Gradient norm: 323.14926782
INFO:root:[   22] Training loss: 114.61543467, Validation loss: 114.68826160, Gradient norm: 337.96214152
INFO:root:[   23] Training loss: 114.37844255, Validation loss: 114.35815363, Gradient norm: 302.42685508
INFO:root:[   24] Training loss: 114.23288313, Validation loss: 114.16685178, Gradient norm: 308.19561228
INFO:root:[   25] Training loss: 114.12375634, Validation loss: 114.25773862, Gradient norm: 329.24207241
INFO:root:[   26] Training loss: 113.92151761, Validation loss: 114.14290887, Gradient norm: 307.50191027
INFO:root:[   27] Training loss: 113.78468363, Validation loss: 113.94172909, Gradient norm: 303.07683283
INFO:root:[   28] Training loss: 113.66520294, Validation loss: 113.84506319, Gradient norm: 313.02584632
INFO:root:[   29] Training loss: 113.57486949, Validation loss: 113.81246346, Gradient norm: 306.82260601
INFO:root:[   30] Training loss: 113.42756114, Validation loss: 113.48566704, Gradient norm: 295.90201787
INFO:root:[   31] Training loss: 113.37132280, Validation loss: 113.40550861, Gradient norm: 324.10945106
INFO:root:[   32] Training loss: 113.26295881, Validation loss: 113.31205776, Gradient norm: 341.61460530
INFO:root:[   33] Training loss: 113.12754357, Validation loss: 113.47151412, Gradient norm: 292.43297124
INFO:root:[   34] Training loss: 113.09025998, Validation loss: 113.13254052, Gradient norm: 291.74626744
INFO:root:[   35] Training loss: 113.04421292, Validation loss: 112.99246189, Gradient norm: 331.22324558
INFO:root:[   36] Training loss: 112.99428925, Validation loss: 113.03795410, Gradient norm: 337.95291798
INFO:root:[   37] Training loss: 112.87570987, Validation loss: 113.25415026, Gradient norm: 309.55242636
INFO:root:[   38] Training loss: 112.76453491, Validation loss: 112.68691106, Gradient norm: 307.85652885
INFO:root:[   39] Training loss: 112.73931736, Validation loss: 112.86778915, Gradient norm: 326.90909822
INFO:root:[   40] Training loss: 112.62688873, Validation loss: 112.76994444, Gradient norm: 290.67324693
INFO:root:[   41] Training loss: 112.62733819, Validation loss: 112.62870333, Gradient norm: 312.34497411
INFO:root:[   42] Training loss: 112.56336158, Validation loss: 112.67622135, Gradient norm: 336.29964777
INFO:root:[   43] Training loss: 112.49060028, Validation loss: 112.63519568, Gradient norm: 315.55851316
INFO:root:[   44] Training loss: 112.45988929, Validation loss: 112.89554837, Gradient norm: 329.99433772
INFO:root:[   45] Training loss: 112.41209649, Validation loss: 112.64268668, Gradient norm: 325.20901614
INFO:root:[   46] Training loss: 112.34132189, Validation loss: 112.47423888, Gradient norm: 329.00396852
INFO:root:[   47] Training loss: 112.32761373, Validation loss: 112.34265257, Gradient norm: 344.42488722
INFO:root:[   48] Training loss: 112.26968489, Validation loss: 112.52158142, Gradient norm: 349.40694293
INFO:root:[   49] Training loss: 112.18740736, Validation loss: 112.47069442, Gradient norm: 342.91823893
INFO:root:[   50] Training loss: 112.18809377, Validation loss: 112.37124727, Gradient norm: 345.72277471
INFO:root:[   51] Training loss: 112.11205553, Validation loss: 112.32613801, Gradient norm: 336.38644297
INFO:root:[   52] Training loss: 112.10177904, Validation loss: 112.34522823, Gradient norm: 323.48025860
INFO:root:[   53] Training loss: 112.06478143, Validation loss: 112.37344521, Gradient norm: 349.89313710
INFO:root:[   54] Training loss: 112.01866811, Validation loss: 112.30070268, Gradient norm: 351.96167433
INFO:root:[   55] Training loss: 111.97602078, Validation loss: 112.29260655, Gradient norm: 319.79392953
INFO:root:[   56] Training loss: 111.96895647, Validation loss: 112.16969326, Gradient norm: 374.34148975
INFO:root:[   57] Training loss: 111.97168684, Validation loss: 112.34217701, Gradient norm: 345.92824452
INFO:root:[   58] Training loss: 111.87845313, Validation loss: 112.11260652, Gradient norm: 351.40990222
INFO:root:[   59] Training loss: 111.83063792, Validation loss: 112.13576882, Gradient norm: 307.32729370
INFO:root:[   60] Training loss: 111.88364607, Validation loss: 112.61846348, Gradient norm: 378.35507874
INFO:root:[   61] Training loss: 111.81059140, Validation loss: 111.86394782, Gradient norm: 344.04346541
INFO:root:[   62] Training loss: 111.81682875, Validation loss: 112.08638603, Gradient norm: 339.33562022
INFO:root:[   63] Training loss: 111.82824019, Validation loss: 112.01625824, Gradient norm: 354.34583864
INFO:root:[   64] Training loss: 111.77294325, Validation loss: 112.09363342, Gradient norm: 378.38968127
INFO:root:[   65] Training loss: 111.65769223, Validation loss: 112.27901900, Gradient norm: 332.45289237
INFO:root:[   66] Training loss: 111.71466773, Validation loss: 112.07637466, Gradient norm: 347.39997272
INFO:root:[   67] Training loss: 111.66165704, Validation loss: 111.82839082, Gradient norm: 335.78025021
INFO:root:[   68] Training loss: 111.63667792, Validation loss: 112.00720523, Gradient norm: 355.17147058
INFO:root:[   69] Training loss: 111.62166531, Validation loss: 111.94650590, Gradient norm: 329.23720159
INFO:root:[   70] Training loss: 111.66723636, Validation loss: 111.85072541, Gradient norm: 382.20541311
INFO:root:[   71] Training loss: 111.62647315, Validation loss: 111.67349096, Gradient norm: 356.34830093
INFO:root:[   72] Training loss: 111.54291083, Validation loss: 112.17548558, Gradient norm: 358.01639949
INFO:root:[   73] Training loss: 111.56522681, Validation loss: 111.73081033, Gradient norm: 378.92004125
INFO:root:[   74] Training loss: 111.55949480, Validation loss: 111.87121435, Gradient norm: 370.91545150
INFO:root:[   75] Training loss: 111.49941857, Validation loss: 111.65604307, Gradient norm: 392.45645515
INFO:root:[   76] Training loss: 111.45392029, Validation loss: 111.64548948, Gradient norm: 360.59251907
INFO:root:[   77] Training loss: 111.42298058, Validation loss: 111.62802057, Gradient norm: 374.29820474
INFO:root:[   78] Training loss: 111.45671750, Validation loss: 111.98098782, Gradient norm: 357.59978739
INFO:root:[   79] Training loss: 111.40471792, Validation loss: 112.05832003, Gradient norm: 378.62880480
INFO:root:[   80] Training loss: 111.43173364, Validation loss: 111.63241376, Gradient norm: 367.14234126
INFO:root:[   81] Training loss: 111.38905714, Validation loss: 111.65927004, Gradient norm: 387.55340012
INFO:root:[   82] Training loss: 111.37723497, Validation loss: 111.39409678, Gradient norm: 386.26657099
INFO:root:[   83] Training loss: 111.33930684, Validation loss: 112.10930312, Gradient norm: 370.27836613
INFO:root:[   84] Training loss: 111.30846802, Validation loss: 111.66665449, Gradient norm: 372.04388225
INFO:root:[   85] Training loss: 111.35876733, Validation loss: 111.50984486, Gradient norm: 391.88002094
INFO:root:[   86] Training loss: 111.25617703, Validation loss: 111.73635208, Gradient norm: 370.22385734
INFO:root:[   87] Training loss: 111.27398919, Validation loss: 111.36765450, Gradient norm: 425.90864684
INFO:root:[   88] Training loss: 111.31906949, Validation loss: 111.62170624, Gradient norm: 376.08914470
INFO:root:[   89] Training loss: 111.27198357, Validation loss: 111.25515131, Gradient norm: 361.24732511
INFO:root:[   90] Training loss: 111.22622616, Validation loss: 112.14125730, Gradient norm: 397.77305517
INFO:root:[   91] Training loss: 111.21870829, Validation loss: 111.30950673, Gradient norm: 375.11959403
INFO:root:[   92] Training loss: 111.18000668, Validation loss: 111.71495070, Gradient norm: 393.33188600
INFO:root:[   93] Training loss: 111.13756829, Validation loss: 111.49063124, Gradient norm: 387.99570961
INFO:root:[   94] Training loss: 111.18841882, Validation loss: 111.63343891, Gradient norm: 378.96146548
INFO:root:[   95] Training loss: 111.14035597, Validation loss: 111.74018539, Gradient norm: 403.33533891
INFO:root:[   96] Training loss: 111.08967987, Validation loss: 111.33486510, Gradient norm: 383.96710348
INFO:root:[   97] Training loss: 111.11493703, Validation loss: 111.68984744, Gradient norm: 393.57456194
INFO:root:[   98] Training loss: 111.11015723, Validation loss: 111.23510595, Gradient norm: 393.07464045
INFO:root:[   99] Training loss: 111.12649533, Validation loss: 111.45248534, Gradient norm: 400.90239909
INFO:root:[  100] Training loss: 111.06319617, Validation loss: 111.44571512, Gradient norm: 389.95893216
INFO:root:[  101] Training loss: 111.10124681, Validation loss: 111.42090767, Gradient norm: 419.35347172
INFO:root:[  102] Training loss: 111.06161024, Validation loss: 111.23442559, Gradient norm: 393.10886485
INFO:root:[  103] Training loss: 111.04191403, Validation loss: 111.25410395, Gradient norm: 388.01296671
INFO:root:[  104] Training loss: 111.01246626, Validation loss: 111.28294158, Gradient norm: 393.54491638
INFO:root:[  105] Training loss: 110.95969659, Validation loss: 111.28061957, Gradient norm: 381.50624678
INFO:root:[  106] Training loss: 110.95464671, Validation loss: 111.21625612, Gradient norm: 397.09684350
INFO:root:[  107] Training loss: 110.96773465, Validation loss: 111.29195243, Gradient norm: 410.02025129
INFO:root:[  108] Training loss: 111.01468770, Validation loss: 111.29666927, Gradient norm: 418.94694407
INFO:root:[  109] Training loss: 110.93870127, Validation loss: 111.12396321, Gradient norm: 404.66414375
INFO:root:[  110] Training loss: 110.93117560, Validation loss: 111.13719284, Gradient norm: 401.65091156
INFO:root:[  111] Training loss: 110.92040215, Validation loss: 111.05845763, Gradient norm: 407.11752128
INFO:root:[  112] Training loss: 110.93384939, Validation loss: 111.20678002, Gradient norm: 397.75516287
INFO:root:[  113] Training loss: 110.93150733, Validation loss: 111.10283661, Gradient norm: 402.67062042
INFO:root:[  114] Training loss: 110.91516123, Validation loss: 111.24839354, Gradient norm: 420.64100467
INFO:root:[  115] Training loss: 110.90724721, Validation loss: 111.22164087, Gradient norm: 439.15444224
INFO:root:[  116] Training loss: 110.90513648, Validation loss: 111.42709257, Gradient norm: 368.05119864
INFO:root:[  117] Training loss: 110.89876322, Validation loss: 111.07712461, Gradient norm: 435.46867364
INFO:root:[  118] Training loss: 110.81837060, Validation loss: 111.08025962, Gradient norm: 399.82470575
INFO:root:[  119] Training loss: 110.83044057, Validation loss: 111.36620732, Gradient norm: 403.27116379
INFO:root:[  120] Training loss: 110.80775913, Validation loss: 111.11400363, Gradient norm: 431.22016334
INFO:root:EP 120: Early stopping
INFO:root:Training the model took 5357.065s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 157.21915
INFO:root:EnergyScoreTrain: 110.67956
INFO:root:CoverageTrain: 0.77254
INFO:root:IntervalWidthTrain: 8.2436
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 157.85361
INFO:root:EnergyScoreValidation: 111.13476
INFO:root:CoverageValidation: 0.7709
INFO:root:IntervalWidthValidation: 8.23838
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 157.89161
INFO:root:EnergyScoreTest: 111.16072
INFO:root:CoverageTest: 0.77117
INFO:root:IntervalWidthTest: 8.24437
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 310378496
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.39753428, Validation loss: 121.50892037, Gradient norm: 323.64886006
INFO:root:[    2] Training loss: 121.01489763, Validation loss: 120.45375208, Gradient norm: 182.21411207
INFO:root:[    3] Training loss: 119.96882484, Validation loss: 119.84658278, Gradient norm: 125.81778989
INFO:root:[    4] Training loss: 119.43325782, Validation loss: 119.25412054, Gradient norm: 86.41607457
INFO:root:[    5] Training loss: 119.08479492, Validation loss: 118.95127936, Gradient norm: 65.39304775
INFO:root:[    6] Training loss: 118.75071350, Validation loss: 118.64473443, Gradient norm: 78.47228701
INFO:root:[    7] Training loss: 118.41898058, Validation loss: 118.32657168, Gradient norm: 83.44575197
INFO:root:[    8] Training loss: 118.14166246, Validation loss: 118.15043828, Gradient norm: 93.95450022
INFO:root:[    9] Training loss: 117.90896664, Validation loss: 117.87545656, Gradient norm: 118.77376591
INFO:root:[   10] Training loss: 117.72142670, Validation loss: 117.65595004, Gradient norm: 103.49816627
INFO:root:[   11] Training loss: 117.56417636, Validation loss: 117.59875073, Gradient norm: 126.03280731
INFO:root:[   12] Training loss: 117.41851837, Validation loss: 117.45340809, Gradient norm: 122.58570435
INFO:root:[   13] Training loss: 117.26826385, Validation loss: 117.25899318, Gradient norm: 132.74871741
INFO:root:[   14] Training loss: 117.20824229, Validation loss: 117.39066917, Gradient norm: 133.93568458
INFO:root:[   15] Training loss: 117.05205750, Validation loss: 117.05354028, Gradient norm: 143.64797589
INFO:root:[   16] Training loss: 116.99470873, Validation loss: 117.05605008, Gradient norm: 176.69107055
INFO:root:[   17] Training loss: 116.87945621, Validation loss: 116.99624125, Gradient norm: 171.78727329
INFO:root:[   18] Training loss: 116.78933611, Validation loss: 116.79376127, Gradient norm: 181.46140478
INFO:root:[   19] Training loss: 116.70478536, Validation loss: 116.85832629, Gradient norm: 175.14069956
INFO:root:[   20] Training loss: 116.66938833, Validation loss: 116.75680997, Gradient norm: 191.46968122
INFO:root:[   21] Training loss: 116.56744334, Validation loss: 116.72424263, Gradient norm: 221.79919346
INFO:root:[   22] Training loss: 116.39494748, Validation loss: 116.48722170, Gradient norm: 202.87104473
INFO:root:[   23] Training loss: 116.28214298, Validation loss: 116.41704211, Gradient norm: 240.04665041
INFO:root:[   24] Training loss: 116.03073612, Validation loss: 115.96225739, Gradient norm: 219.65878068
INFO:root:[   25] Training loss: 115.74498633, Validation loss: 115.78005513, Gradient norm: 225.89153921
INFO:root:[   26] Training loss: 115.50631114, Validation loss: 115.47916653, Gradient norm: 225.61780242
INFO:root:[   27] Training loss: 115.04626360, Validation loss: 114.90342766, Gradient norm: 195.23258814
INFO:root:[   28] Training loss: 114.71868208, Validation loss: 114.59927877, Gradient norm: 226.13176640
INFO:root:[   29] Training loss: 114.23047523, Validation loss: 114.31189674, Gradient norm: 190.41363197
INFO:root:[   30] Training loss: 113.91713067, Validation loss: 113.81894537, Gradient norm: 194.18777118
INFO:root:[   31] Training loss: 113.60537059, Validation loss: 113.44376695, Gradient norm: 207.77732602
INFO:root:[   32] Training loss: 113.19052263, Validation loss: 113.39949437, Gradient norm: 181.26029910
INFO:root:[   33] Training loss: 113.05083862, Validation loss: 113.09361910, Gradient norm: 203.51061379
INFO:root:[   34] Training loss: 112.80980737, Validation loss: 112.72610233, Gradient norm: 211.91718837
INFO:root:[   35] Training loss: 112.80873440, Validation loss: 112.63848797, Gradient norm: 222.65330625
INFO:root:[   36] Training loss: 112.57982849, Validation loss: 112.73569636, Gradient norm: 217.16622947
INFO:root:[   37] Training loss: 112.37242133, Validation loss: 112.27462407, Gradient norm: 201.82934259
INFO:root:[   38] Training loss: 112.27104292, Validation loss: 112.40607118, Gradient norm: 221.71766671
INFO:root:[   39] Training loss: 112.11880327, Validation loss: 112.54043070, Gradient norm: 248.05266000
INFO:root:[   40] Training loss: 111.93594381, Validation loss: 112.19799537, Gradient norm: 210.24081415
INFO:root:[   41] Training loss: 111.94343001, Validation loss: 111.95820752, Gradient norm: 233.97093843
INFO:root:[   42] Training loss: 111.81176185, Validation loss: 111.81543303, Gradient norm: 225.79106713
INFO:root:[   43] Training loss: 111.65083123, Validation loss: 111.54316203, Gradient norm: 202.24964172
INFO:root:[   44] Training loss: 111.62563538, Validation loss: 111.73120372, Gradient norm: 216.98666955
INFO:root:[   45] Training loss: 111.68721564, Validation loss: 111.77318265, Gradient norm: 230.59024905
INFO:root:[   46] Training loss: 111.50997013, Validation loss: 111.49887326, Gradient norm: 209.77665376
INFO:root:[   47] Training loss: 111.43520142, Validation loss: 111.60149236, Gradient norm: 200.10473047
INFO:root:[   48] Training loss: 111.46249363, Validation loss: 111.46494106, Gradient norm: 203.74831851
INFO:root:[   49] Training loss: 111.33713504, Validation loss: 111.73939059, Gradient norm: 205.39044309
INFO:root:[   50] Training loss: 111.32795932, Validation loss: 111.59225022, Gradient norm: 198.42781042
INFO:root:[   51] Training loss: 111.20825718, Validation loss: 111.44402166, Gradient norm: 181.09716106
INFO:root:[   52] Training loss: 111.16565898, Validation loss: 111.61978243, Gradient norm: 181.71729425
INFO:root:[   53] Training loss: 111.14565504, Validation loss: 111.25594062, Gradient norm: 181.13560872
INFO:root:[   54] Training loss: 111.11688761, Validation loss: 111.22911995, Gradient norm: 173.79662807
INFO:root:[   55] Training loss: 111.08033837, Validation loss: 111.05964085, Gradient norm: 178.27439937
INFO:root:[   56] Training loss: 110.95644504, Validation loss: 111.06612021, Gradient norm: 177.39145775
INFO:root:[   57] Training loss: 110.94132321, Validation loss: 111.10945491, Gradient norm: 187.42796087
INFO:root:[   58] Training loss: 110.92009338, Validation loss: 110.96521705, Gradient norm: 179.70800725
INFO:root:[   59] Training loss: 110.88340261, Validation loss: 111.01429722, Gradient norm: 173.03659429
INFO:root:[   60] Training loss: 110.77307105, Validation loss: 110.94332698, Gradient norm: 183.30910171
INFO:root:[   61] Training loss: 110.75194495, Validation loss: 111.35443062, Gradient norm: 179.69982966
INFO:root:[   62] Training loss: 110.79014516, Validation loss: 110.92143919, Gradient norm: 190.09024366
INFO:root:[   63] Training loss: 110.68238102, Validation loss: 110.67430476, Gradient norm: 187.72555862
INFO:root:[   64] Training loss: 110.64058278, Validation loss: 110.46362198, Gradient norm: 187.22730743
INFO:root:[   65] Training loss: 110.56553860, Validation loss: 110.79032711, Gradient norm: 182.28475189
INFO:root:[   66] Training loss: 110.52355099, Validation loss: 110.74572888, Gradient norm: 192.81708123
INFO:root:[   67] Training loss: 110.46838898, Validation loss: 110.54923195, Gradient norm: 184.85702526
INFO:root:[   68] Training loss: 110.46714498, Validation loss: 110.41677241, Gradient norm: 204.95764120
INFO:root:[   69] Training loss: 110.46641049, Validation loss: 110.59373461, Gradient norm: 211.11285968
INFO:root:[   70] Training loss: 110.31551341, Validation loss: 110.61539740, Gradient norm: 199.88338664
INFO:root:[   71] Training loss: 110.28786638, Validation loss: 110.36683748, Gradient norm: 196.63500353
INFO:root:[   72] Training loss: 110.34246853, Validation loss: 110.46586154, Gradient norm: 200.54225281
INFO:root:[   73] Training loss: 110.24000875, Validation loss: 110.21781627, Gradient norm: 201.44151135
INFO:root:[   74] Training loss: 110.22167155, Validation loss: 110.10219427, Gradient norm: 208.44827861
INFO:root:[   75] Training loss: 110.16490146, Validation loss: 110.14375479, Gradient norm: 201.63159585
INFO:root:[   76] Training loss: 110.13188090, Validation loss: 110.21422015, Gradient norm: 208.74192934
INFO:root:[   77] Training loss: 110.11033539, Validation loss: 110.13336543, Gradient norm: 224.95176311
INFO:root:[   78] Training loss: 110.04668840, Validation loss: 110.18282184, Gradient norm: 210.75487249
INFO:root:[   79] Training loss: 110.05899662, Validation loss: 110.19993631, Gradient norm: 222.84113315
INFO:root:[   80] Training loss: 110.03980225, Validation loss: 110.06532555, Gradient norm: 218.66163457
INFO:root:[   81] Training loss: 110.01946910, Validation loss: 109.97149618, Gradient norm: 229.07506902
INFO:root:[   82] Training loss: 109.98619792, Validation loss: 110.03459930, Gradient norm: 215.06202670
INFO:root:[   83] Training loss: 109.98478068, Validation loss: 110.07140926, Gradient norm: 229.23447998
INFO:root:[   84] Training loss: 109.93301134, Validation loss: 110.11852960, Gradient norm: 227.23764920
INFO:root:[   85] Training loss: 109.89385722, Validation loss: 110.00737829, Gradient norm: 220.54585944
INFO:root:[   86] Training loss: 109.89095283, Validation loss: 109.86440652, Gradient norm: 235.13299140
INFO:root:[   87] Training loss: 109.85497450, Validation loss: 109.74034306, Gradient norm: 239.86854265
INFO:root:[   88] Training loss: 109.78117947, Validation loss: 110.14294849, Gradient norm: 232.38770627
INFO:root:[   89] Training loss: 109.82471235, Validation loss: 110.08001950, Gradient norm: 223.21409560
INFO:root:[   90] Training loss: 109.81500383, Validation loss: 109.87599932, Gradient norm: 242.82809783
INFO:root:[   91] Training loss: 109.86196204, Validation loss: 109.88889621, Gradient norm: 247.28922940
INFO:root:[   92] Training loss: 109.84742933, Validation loss: 110.25750103, Gradient norm: 237.24844034
INFO:root:[   93] Training loss: 109.77336853, Validation loss: 109.96811904, Gradient norm: 232.95414593
INFO:root:[   94] Training loss: 109.73227831, Validation loss: 110.15108985, Gradient norm: 254.89630006
INFO:root:[   95] Training loss: 109.78885474, Validation loss: 109.79895060, Gradient norm: 238.98609992
INFO:root:[   96] Training loss: 109.68213016, Validation loss: 109.87589706, Gradient norm: 241.11735655
INFO:root:EP 96: Early stopping
INFO:root:Training the model took 4168.54s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 155.48129
INFO:root:EnergyScoreTrain: 109.56219
INFO:root:CoverageTrain: 0.52242
INFO:root:IntervalWidthTrain: 6.74615
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 155.77372
INFO:root:EnergyScoreValidation: 109.77429
INFO:root:CoverageValidation: 0.5218
INFO:root:IntervalWidthValidation: 6.74474
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 155.79251
INFO:root:EnergyScoreTest: 109.77966
INFO:root:CoverageTest: 0.52183
INFO:root:IntervalWidthTest: 6.74521
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.49683478, Validation loss: 121.22790286, Gradient norm: 167.85362672
INFO:root:[    2] Training loss: 120.16765601, Validation loss: 119.32568614, Gradient norm: 107.50367125
INFO:root:[    3] Training loss: 118.72921492, Validation loss: 118.26363386, Gradient norm: 84.94733928
INFO:root:[    4] Training loss: 117.98036034, Validation loss: 117.91861524, Gradient norm: 65.86993908
INFO:root:[    5] Training loss: 117.61120884, Validation loss: 117.57766188, Gradient norm: 58.88328518
INFO:root:[    6] Training loss: 117.39848697, Validation loss: 117.35692048, Gradient norm: 68.70909720
INFO:root:[    7] Training loss: 117.18553352, Validation loss: 117.20503824, Gradient norm: 69.08294334
INFO:root:[    8] Training loss: 117.04587050, Validation loss: 117.12808482, Gradient norm: 82.60604859
INFO:root:[    9] Training loss: 116.82726515, Validation loss: 116.92244881, Gradient norm: 75.44925800
INFO:root:[   10] Training loss: 116.65523675, Validation loss: 116.61896635, Gradient norm: 97.55835095
INFO:root:[   11] Training loss: 116.45876370, Validation loss: 116.49960929, Gradient norm: 86.83184101
INFO:root:[   12] Training loss: 116.24292948, Validation loss: 116.17761726, Gradient norm: 101.29498996
INFO:root:[   13] Training loss: 115.95517073, Validation loss: 116.20757990, Gradient norm: 105.53720706
INFO:root:[   14] Training loss: 115.67694926, Validation loss: 115.73817792, Gradient norm: 106.66257083
INFO:root:[   15] Training loss: 115.33421644, Validation loss: 115.33668987, Gradient norm: 119.67486655
INFO:root:[   16] Training loss: 115.02321387, Validation loss: 115.07096796, Gradient norm: 120.23435625
INFO:root:[   17] Training loss: 114.76038544, Validation loss: 114.77611020, Gradient norm: 131.96853637
INFO:root:[   18] Training loss: 114.50717536, Validation loss: 114.49883417, Gradient norm: 131.75292389
INFO:root:[   19] Training loss: 114.29207292, Validation loss: 114.35201103, Gradient norm: 144.57661905
INFO:root:[   20] Training loss: 114.16811130, Validation loss: 114.46478298, Gradient norm: 153.48809502
INFO:root:[   21] Training loss: 114.01281735, Validation loss: 114.03001123, Gradient norm: 172.62648340
INFO:root:[   22] Training loss: 113.85309730, Validation loss: 113.72639037, Gradient norm: 171.25297613
INFO:root:[   23] Training loss: 113.73660112, Validation loss: 113.78021802, Gradient norm: 180.49663877
INFO:root:[   24] Training loss: 113.64198463, Validation loss: 113.99378606, Gradient norm: 190.15565531
INFO:root:[   25] Training loss: 113.55896993, Validation loss: 113.51816358, Gradient norm: 220.50330038
INFO:root:[   26] Training loss: 113.42135847, Validation loss: 113.68357729, Gradient norm: 203.77700299
INFO:root:[   27] Training loss: 113.37104211, Validation loss: 113.42181075, Gradient norm: 229.42795576
INFO:root:[   28] Training loss: 113.28781199, Validation loss: 113.62081722, Gradient norm: 231.06115756
INFO:root:[   29] Training loss: 113.18342397, Validation loss: 113.62065205, Gradient norm: 241.15345294
INFO:root:[   30] Training loss: 113.11995422, Validation loss: 113.29200611, Gradient norm: 264.33815540
INFO:root:[   31] Training loss: 112.96941532, Validation loss: 113.01251529, Gradient norm: 248.61476046
INFO:root:[   32] Training loss: 112.93611983, Validation loss: 113.04259919, Gradient norm: 284.97978468
INFO:root:[   33] Training loss: 112.78118513, Validation loss: 112.93311363, Gradient norm: 241.98448428
INFO:root:[   34] Training loss: 112.69812171, Validation loss: 112.88244495, Gradient norm: 268.14658232
INFO:root:[   35] Training loss: 112.59753784, Validation loss: 112.72160460, Gradient norm: 271.37881701
INFO:root:[   36] Training loss: 112.46125810, Validation loss: 112.44875898, Gradient norm: 267.46171642
INFO:root:[   37] Training loss: 112.38144894, Validation loss: 112.59011961, Gradient norm: 272.96067940
INFO:root:[   38] Training loss: 112.33583106, Validation loss: 112.47683622, Gradient norm: 298.89928259
INFO:root:[   39] Training loss: 112.19994117, Validation loss: 112.39929266, Gradient norm: 288.39260246
INFO:root:[   40] Training loss: 112.10115482, Validation loss: 112.10038731, Gradient norm: 267.97822764
INFO:root:[   41] Training loss: 112.00506307, Validation loss: 111.92463376, Gradient norm: 280.02867863
INFO:root:[   42] Training loss: 111.88883664, Validation loss: 112.06824132, Gradient norm: 279.31982750
INFO:root:[   43] Training loss: 111.81543715, Validation loss: 111.76592803, Gradient norm: 299.23669949
INFO:root:[   44] Training loss: 111.64485270, Validation loss: 112.01052883, Gradient norm: 263.35545059
INFO:root:[   45] Training loss: 111.58579054, Validation loss: 111.61867577, Gradient norm: 290.84861851
INFO:root:[   46] Training loss: 111.46218377, Validation loss: 111.59581516, Gradient norm: 282.90957391
INFO:root:[   47] Training loss: 111.35246412, Validation loss: 111.36854419, Gradient norm: 297.47998504
INFO:root:[   48] Training loss: 111.25171587, Validation loss: 111.26093493, Gradient norm: 272.28350145
INFO:root:[   49] Training loss: 111.15881504, Validation loss: 111.15496251, Gradient norm: 296.56915114
INFO:root:[   50] Training loss: 111.09418169, Validation loss: 111.17643925, Gradient norm: 297.40434821
INFO:root:[   51] Training loss: 111.03252096, Validation loss: 111.04838964, Gradient norm: 294.29073248
INFO:root:[   52] Training loss: 110.87035088, Validation loss: 111.06672173, Gradient norm: 302.49365432
INFO:root:[   53] Training loss: 110.78625119, Validation loss: 111.06894443, Gradient norm: 283.37401683
INFO:root:[   54] Training loss: 110.78746948, Validation loss: 110.97541782, Gradient norm: 292.18038615
INFO:root:[   55] Training loss: 110.76276974, Validation loss: 110.84509237, Gradient norm: 315.68642226
INFO:root:[   56] Training loss: 110.69693600, Validation loss: 110.60447666, Gradient norm: 310.02769620
INFO:root:[   57] Training loss: 110.65303677, Validation loss: 111.00711501, Gradient norm: 304.98728390
INFO:root:[   58] Training loss: 110.61523092, Validation loss: 110.59311596, Gradient norm: 330.65330058
INFO:root:[   59] Training loss: 110.46839216, Validation loss: 110.75846033, Gradient norm: 301.23945952
INFO:root:[   60] Training loss: 110.38629208, Validation loss: 110.58295842, Gradient norm: 302.00755840
INFO:root:[   61] Training loss: 110.44013251, Validation loss: 110.56044395, Gradient norm: 315.22936590
INFO:root:[   62] Training loss: 110.35110881, Validation loss: 110.50416940, Gradient norm: 320.86502917
INFO:root:[   63] Training loss: 110.27985341, Validation loss: 110.43331360, Gradient norm: 320.68302695
INFO:root:[   64] Training loss: 110.25256378, Validation loss: 110.46105087, Gradient norm: 315.17847631
INFO:root:[   65] Training loss: 110.24014008, Validation loss: 110.18626310, Gradient norm: 337.23994221
INFO:root:[   66] Training loss: 110.16716441, Validation loss: 110.35793264, Gradient norm: 317.86575013
INFO:root:[   67] Training loss: 110.18180810, Validation loss: 110.44340796, Gradient norm: 331.98344380
INFO:root:[   68] Training loss: 110.05753130, Validation loss: 110.13296335, Gradient norm: 318.34993676
INFO:root:[   69] Training loss: 110.02785506, Validation loss: 110.06629890, Gradient norm: 325.37631781
INFO:root:[   70] Training loss: 109.95912543, Validation loss: 110.32074082, Gradient norm: 330.23177716
INFO:root:[   71] Training loss: 109.98642595, Validation loss: 110.17637715, Gradient norm: 338.04502897
INFO:root:[   72] Training loss: 109.95453620, Validation loss: 110.00688386, Gradient norm: 314.86092345
INFO:root:[   73] Training loss: 109.96252163, Validation loss: 110.07542861, Gradient norm: 339.64987923
INFO:root:[   74] Training loss: 109.84419776, Validation loss: 110.14121648, Gradient norm: 328.21786591
INFO:root:[   75] Training loss: 109.85157328, Validation loss: 110.26931134, Gradient norm: 337.87858622
INFO:root:[   76] Training loss: 109.78919776, Validation loss: 110.34401341, Gradient norm: 323.65301157
INFO:root:[   77] Training loss: 109.77664018, Validation loss: 110.01800323, Gradient norm: 336.19215635
INFO:root:[   78] Training loss: 109.79316288, Validation loss: 109.97625438, Gradient norm: 323.36647830
INFO:root:[   79] Training loss: 109.74791582, Validation loss: 110.06489657, Gradient norm: 331.20820371
INFO:root:[   80] Training loss: 109.81909658, Validation loss: 110.03484840, Gradient norm: 343.01452601
INFO:root:[   81] Training loss: 109.71374298, Validation loss: 109.97735515, Gradient norm: 338.21767765
INFO:root:[   82] Training loss: 109.70985782, Validation loss: 109.78294011, Gradient norm: 339.83672466
INFO:root:[   83] Training loss: 109.65820489, Validation loss: 109.78431488, Gradient norm: 350.09835571
INFO:root:[   84] Training loss: 109.61881785, Validation loss: 109.70641113, Gradient norm: 349.25593606
INFO:root:[   85] Training loss: 109.58362640, Validation loss: 109.63323921, Gradient norm: 334.86312724
INFO:root:[   86] Training loss: 109.58291694, Validation loss: 110.08438391, Gradient norm: 321.79720769
INFO:root:[   87] Training loss: 109.49077301, Validation loss: 109.99782281, Gradient norm: 314.86279989
INFO:root:[   88] Training loss: 109.61831997, Validation loss: 109.59321942, Gradient norm: 361.01608024
INFO:root:[   89] Training loss: 109.54911699, Validation loss: 109.67704345, Gradient norm: 354.59682203
INFO:root:[   90] Training loss: 109.49347751, Validation loss: 109.64816833, Gradient norm: 350.62176717
INFO:root:[   91] Training loss: 109.48271040, Validation loss: 109.68424907, Gradient norm: 354.57725669
INFO:root:[   92] Training loss: 109.49913157, Validation loss: 109.77662110, Gradient norm: 344.91167944
INFO:root:[   93] Training loss: 109.40070777, Validation loss: 109.62817316, Gradient norm: 331.37662420
INFO:root:[   94] Training loss: 109.45905562, Validation loss: 109.66657217, Gradient norm: 365.35213965
INFO:root:[   95] Training loss: 109.40184262, Validation loss: 109.71534702, Gradient norm: 363.20753244
INFO:root:[   96] Training loss: 109.33445513, Validation loss: 109.70637445, Gradient norm: 362.34623187
INFO:root:[   97] Training loss: 109.32555979, Validation loss: 109.52145694, Gradient norm: 362.40438583
INFO:root:[   98] Training loss: 109.31232205, Validation loss: 109.59405464, Gradient norm: 356.07264253
INFO:root:[   99] Training loss: 109.35777086, Validation loss: 109.84476538, Gradient norm: 368.52069132
INFO:root:[  100] Training loss: 109.30658308, Validation loss: 109.53226913, Gradient norm: 369.94369328
INFO:root:[  101] Training loss: 109.34677012, Validation loss: 109.41175588, Gradient norm: 365.46101197
INFO:root:[  102] Training loss: 109.26335676, Validation loss: 109.83644559, Gradient norm: 383.52447787
INFO:root:[  103] Training loss: 109.29782006, Validation loss: 109.77263588, Gradient norm: 358.53609665
INFO:root:[  104] Training loss: 109.30556624, Validation loss: 109.42679542, Gradient norm: 387.27189298
INFO:root:[  105] Training loss: 109.31606720, Validation loss: 109.26710443, Gradient norm: 387.34902262
INFO:root:[  106] Training loss: 109.19900767, Validation loss: 109.24512013, Gradient norm: 346.97360130
INFO:root:[  107] Training loss: 109.22428155, Validation loss: 109.43134308, Gradient norm: 382.85819057
INFO:root:[  108] Training loss: 109.18662065, Validation loss: 109.50332601, Gradient norm: 378.53621363
INFO:root:[  109] Training loss: 109.20340366, Validation loss: 109.47709763, Gradient norm: 379.87845640
INFO:root:[  110] Training loss: 109.16204688, Validation loss: 109.49142737, Gradient norm: 392.78449576
INFO:root:[  111] Training loss: 109.11750990, Validation loss: 109.44544100, Gradient norm: 362.01603891
INFO:root:[  112] Training loss: 109.16251692, Validation loss: 109.41917192, Gradient norm: 388.33053622
INFO:root:[  113] Training loss: 109.17715217, Validation loss: 109.06563782, Gradient norm: 389.93770137
INFO:root:[  114] Training loss: 109.12503269, Validation loss: 109.27090227, Gradient norm: 376.40298737
INFO:root:[  115] Training loss: 109.10963450, Validation loss: 109.47325456, Gradient norm: 396.24359728
INFO:root:[  116] Training loss: 109.12286699, Validation loss: 109.53542167, Gradient norm: 384.90545089
INFO:root:[  117] Training loss: 109.09735097, Validation loss: 109.15600653, Gradient norm: 390.40644500
INFO:root:[  118] Training loss: 109.09031389, Validation loss: 109.36239196, Gradient norm: 375.72449264
INFO:root:[  119] Training loss: 109.07119602, Validation loss: 109.37910702, Gradient norm: 377.87979068
INFO:root:[  120] Training loss: 109.09963036, Validation loss: 109.29936901, Gradient norm: 382.26670355
INFO:root:[  121] Training loss: 109.04762519, Validation loss: 109.20511922, Gradient norm: 377.02134591
INFO:root:[  122] Training loss: 109.10339213, Validation loss: 109.42288328, Gradient norm: 385.86636804
INFO:root:EP 122: Early stopping
INFO:root:Training the model took 5291.335s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 154.40996
INFO:root:EnergyScoreTrain: 108.8206
INFO:root:CoverageTrain: 0.57525
INFO:root:IntervalWidthTrain: 7.24916
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 154.89315
INFO:root:EnergyScoreValidation: 109.16104
INFO:root:CoverageValidation: 0.57388
INFO:root:IntervalWidthValidation: 7.23769
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 154.81169
INFO:root:EnergyScoreTest: 109.10387
INFO:root:CoverageTest: 0.57489
INFO:root:IntervalWidthTest: 7.24986
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.55227153, Validation loss: 121.44287765, Gradient norm: 150.10537019
INFO:root:[    2] Training loss: 120.61434214, Validation loss: 119.93681349, Gradient norm: 89.05448168
INFO:root:[    3] Training loss: 119.31102743, Validation loss: 118.90748717, Gradient norm: 71.76321908
INFO:root:[    4] Training loss: 118.57970879, Validation loss: 118.46066043, Gradient norm: 69.15281163
INFO:root:[    5] Training loss: 118.18582435, Validation loss: 118.13221286, Gradient norm: 79.32022638
INFO:root:[    6] Training loss: 117.80298214, Validation loss: 117.94391378, Gradient norm: 64.51029417
INFO:root:[    7] Training loss: 117.51393945, Validation loss: 117.51814377, Gradient norm: 65.30867590
INFO:root:[    8] Training loss: 117.26173876, Validation loss: 117.32948223, Gradient norm: 78.76220822
INFO:root:[    9] Training loss: 117.03057756, Validation loss: 117.08509144, Gradient norm: 88.66962303
INFO:root:[   10] Training loss: 116.78812893, Validation loss: 116.85171870, Gradient norm: 94.20836609
INFO:root:[   11] Training loss: 116.52998183, Validation loss: 116.45745930, Gradient norm: 99.54384936
INFO:root:[   12] Training loss: 116.17279107, Validation loss: 115.98077366, Gradient norm: 103.66813893
INFO:root:[   13] Training loss: 115.95867825, Validation loss: 116.02754546, Gradient norm: 108.90463978
INFO:root:[   14] Training loss: 115.77199944, Validation loss: 116.03835256, Gradient norm: 124.62720985
INFO:root:[   15] Training loss: 115.54164815, Validation loss: 115.69122127, Gradient norm: 116.10841691
INFO:root:[   16] Training loss: 115.41455790, Validation loss: 115.54783389, Gradient norm: 142.90752504
INFO:root:[   17] Training loss: 115.26637950, Validation loss: 115.35335956, Gradient norm: 148.72297268
INFO:root:[   18] Training loss: 115.09603373, Validation loss: 115.12445483, Gradient norm: 150.85307080
INFO:root:[   19] Training loss: 114.96522003, Validation loss: 115.14366163, Gradient norm: 161.77634516
INFO:root:[   20] Training loss: 114.84382216, Validation loss: 115.17247277, Gradient norm: 169.34610804
INFO:root:[   21] Training loss: 114.71924154, Validation loss: 114.64683118, Gradient norm: 172.27943816
INFO:root:[   22] Training loss: 114.58681569, Validation loss: 114.54148048, Gradient norm: 182.60140126
INFO:root:[   23] Training loss: 114.49336823, Validation loss: 114.59952799, Gradient norm: 214.40501929
INFO:root:[   24] Training loss: 114.32711683, Validation loss: 114.65695927, Gradient norm: 203.74905386
INFO:root:[   25] Training loss: 114.14442810, Validation loss: 114.12487793, Gradient norm: 205.87847357
INFO:root:[   26] Training loss: 114.04464406, Validation loss: 114.01328063, Gradient norm: 236.77683069
INFO:root:[   27] Training loss: 113.92915243, Validation loss: 113.97674427, Gradient norm: 243.39605518
INFO:root:[   28] Training loss: 113.78946638, Validation loss: 113.69857226, Gradient norm: 258.23653818
INFO:root:[   29] Training loss: 113.55151035, Validation loss: 113.88023872, Gradient norm: 244.20390048
INFO:root:[   30] Training loss: 113.36846676, Validation loss: 113.56454254, Gradient norm: 242.89803966
INFO:root:[   31] Training loss: 113.20078569, Validation loss: 113.47194631, Gradient norm: 244.63543729
INFO:root:[   32] Training loss: 113.07315304, Validation loss: 112.94784707, Gradient norm: 234.78685829
INFO:root:[   33] Training loss: 112.93416755, Validation loss: 112.98358609, Gradient norm: 246.55461577
INFO:root:[   34] Training loss: 112.75823893, Validation loss: 112.86447384, Gradient norm: 224.09686857
INFO:root:[   35] Training loss: 112.81909017, Validation loss: 112.68514881, Gradient norm: 280.01753575
INFO:root:[   36] Training loss: 112.54041236, Validation loss: 112.76793885, Gradient norm: 245.19232564
INFO:root:[   37] Training loss: 112.37506900, Validation loss: 112.56442167, Gradient norm: 237.08603913
INFO:root:[   38] Training loss: 112.25262611, Validation loss: 112.33828079, Gradient norm: 229.23497618
INFO:root:[   39] Training loss: 112.19315904, Validation loss: 112.33580753, Gradient norm: 233.64370409
INFO:root:[   40] Training loss: 112.12560710, Validation loss: 112.12133856, Gradient norm: 258.58919271
INFO:root:[   41] Training loss: 111.99277900, Validation loss: 112.01084365, Gradient norm: 244.70826700
INFO:root:[   42] Training loss: 111.93565033, Validation loss: 111.89895322, Gradient norm: 245.28270799
INFO:root:[   43] Training loss: 111.86214010, Validation loss: 112.04100572, Gradient norm: 271.93895092
INFO:root:[   44] Training loss: 111.76056122, Validation loss: 111.96507210, Gradient norm: 255.31572646
INFO:root:[   45] Training loss: 111.65901533, Validation loss: 111.77430631, Gradient norm: 250.96239203
INFO:root:[   46] Training loss: 111.64158837, Validation loss: 111.77753810, Gradient norm: 265.26249429
INFO:root:[   47] Training loss: 111.51926619, Validation loss: 111.65677134, Gradient norm: 240.53064281
INFO:root:[   48] Training loss: 111.42811832, Validation loss: 111.37043173, Gradient norm: 244.71820872
INFO:root:[   49] Training loss: 111.41222432, Validation loss: 111.54762268, Gradient norm: 251.07918404
INFO:root:[   50] Training loss: 111.39568231, Validation loss: 111.50241838, Gradient norm: 260.16593182
INFO:root:[   51] Training loss: 111.29473992, Validation loss: 111.55913048, Gradient norm: 262.71132741
INFO:root:[   52] Training loss: 111.27941708, Validation loss: 111.36771139, Gradient norm: 263.40012023
INFO:root:[   53] Training loss: 111.18015981, Validation loss: 111.13027860, Gradient norm: 249.63855775
INFO:root:[   54] Training loss: 111.14775981, Validation loss: 111.36441816, Gradient norm: 248.81115393
INFO:root:[   55] Training loss: 111.11722822, Validation loss: 111.24814713, Gradient norm: 260.72134160
INFO:root:[   56] Training loss: 111.15777018, Validation loss: 111.21376359, Gradient norm: 267.70751906
INFO:root:[   57] Training loss: 111.04799564, Validation loss: 111.15480269, Gradient norm: 243.91549733
INFO:root:[   58] Training loss: 111.01152880, Validation loss: 111.00996332, Gradient norm: 251.68862155
INFO:root:[   59] Training loss: 110.99688144, Validation loss: 110.92137320, Gradient norm: 255.54021504
INFO:root:[   60] Training loss: 110.87612233, Validation loss: 111.13772315, Gradient norm: 248.14205115
INFO:root:[   61] Training loss: 110.88208913, Validation loss: 111.09392909, Gradient norm: 247.46002181
INFO:root:[   62] Training loss: 110.78528503, Validation loss: 111.01967072, Gradient norm: 244.94409761
INFO:root:[   63] Training loss: 110.77597293, Validation loss: 111.01624285, Gradient norm: 252.46766153
INFO:root:[   64] Training loss: 110.75874932, Validation loss: 111.15219785, Gradient norm: 255.35852490
INFO:root:[   65] Training loss: 110.67522671, Validation loss: 110.69566854, Gradient norm: 234.86948390
INFO:root:[   66] Training loss: 110.67001753, Validation loss: 110.94909681, Gradient norm: 247.07237415
INFO:root:[   67] Training loss: 110.71697398, Validation loss: 110.76599362, Gradient norm: 249.69168572
INFO:root:[   68] Training loss: 110.62684143, Validation loss: 110.91561809, Gradient norm: 248.60574448
INFO:root:[   69] Training loss: 110.59108456, Validation loss: 111.10735107, Gradient norm: 251.02541563
INFO:root:[   70] Training loss: 110.56355991, Validation loss: 110.57924692, Gradient norm: 249.54323585
INFO:root:[   71] Training loss: 110.55684519, Validation loss: 110.72890834, Gradient norm: 255.25328221
INFO:root:[   72] Training loss: 110.54265021, Validation loss: 110.70202369, Gradient norm: 257.30982540
INFO:root:[   73] Training loss: 110.56401998, Validation loss: 110.85625659, Gradient norm: 251.70757303
INFO:root:[   74] Training loss: 110.49343862, Validation loss: 110.41333677, Gradient norm: 254.47983887
INFO:root:[   75] Training loss: 110.46952701, Validation loss: 110.84539179, Gradient norm: 260.43726392
INFO:root:[   76] Training loss: 110.47255514, Validation loss: 110.67864602, Gradient norm: 261.24514195
INFO:root:[   77] Training loss: 110.34400408, Validation loss: 110.54298361, Gradient norm: 241.66094546
INFO:root:[   78] Training loss: 110.36055718, Validation loss: 110.33722352, Gradient norm: 256.26014734
INFO:root:[   79] Training loss: 110.34355289, Validation loss: 110.82934088, Gradient norm: 251.46231476
INFO:root:[   80] Training loss: 110.31434564, Validation loss: 110.45065428, Gradient norm: 250.96421163
INFO:root:[   81] Training loss: 110.36410421, Validation loss: 110.27325734, Gradient norm: 264.37357944
INFO:root:[   82] Training loss: 110.34640828, Validation loss: 110.50078931, Gradient norm: 269.30931051
INFO:root:[   83] Training loss: 110.26097955, Validation loss: 110.31892756, Gradient norm: 251.16007726
INFO:root:[   84] Training loss: 110.24466437, Validation loss: 110.21361154, Gradient norm: 256.42482585
INFO:root:[   85] Training loss: 110.24618161, Validation loss: 110.27648444, Gradient norm: 257.49344751
INFO:root:[   86] Training loss: 110.23307495, Validation loss: 110.41680266, Gradient norm: 275.29804094
INFO:root:[   87] Training loss: 110.22801303, Validation loss: 110.12103057, Gradient norm: 254.98830909
INFO:root:[   88] Training loss: 110.16102804, Validation loss: 110.10657394, Gradient norm: 267.66286019
INFO:root:[   89] Training loss: 110.10126017, Validation loss: 110.17446899, Gradient norm: 261.82281326
INFO:root:[   90] Training loss: 110.19545837, Validation loss: 110.17729963, Gradient norm: 282.38584507
INFO:root:[   91] Training loss: 110.07419603, Validation loss: 110.29391734, Gradient norm: 266.32426289
INFO:root:[   92] Training loss: 110.08616214, Validation loss: 110.24312391, Gradient norm: 273.03429087
INFO:root:[   93] Training loss: 110.02925198, Validation loss: 110.10975152, Gradient norm: 264.78746390
INFO:root:[   94] Training loss: 109.97481740, Validation loss: 110.29562927, Gradient norm: 257.46528844
INFO:root:[   95] Training loss: 110.04241808, Validation loss: 110.24554952, Gradient norm: 265.16666894
INFO:root:[   96] Training loss: 109.93832943, Validation loss: 110.36140736, Gradient norm: 275.89246218
INFO:root:[   97] Training loss: 110.01961873, Validation loss: 110.06903866, Gradient norm: 278.75220029
INFO:root:[   98] Training loss: 109.93458801, Validation loss: 109.97159737, Gradient norm: 251.71123092
INFO:root:[   99] Training loss: 109.95611491, Validation loss: 110.12288840, Gradient norm: 265.90969825
INFO:root:[  100] Training loss: 109.94180396, Validation loss: 110.44744619, Gradient norm: 266.84009624
INFO:root:[  101] Training loss: 109.92973087, Validation loss: 109.99433778, Gradient norm: 271.77229911
INFO:root:[  102] Training loss: 109.88709917, Validation loss: 110.24617741, Gradient norm: 276.94774827
INFO:root:[  103] Training loss: 109.85161770, Validation loss: 110.08472416, Gradient norm: 269.18448442
INFO:root:[  104] Training loss: 109.84961121, Validation loss: 110.11578824, Gradient norm: 272.15158172
INFO:root:[  105] Training loss: 109.86864644, Validation loss: 110.08951756, Gradient norm: 270.20115217
INFO:root:[  106] Training loss: 109.75008325, Validation loss: 110.08415865, Gradient norm: 260.69476158
INFO:root:[  107] Training loss: 109.84810520, Validation loss: 110.18759811, Gradient norm: 284.60869273
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 4642.551s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 155.78373
INFO:root:EnergyScoreTrain: 109.79326
INFO:root:CoverageTrain: 0.53476
INFO:root:IntervalWidthTrain: 6.90804
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 156.1197
INFO:root:EnergyScoreValidation: 110.03352
INFO:root:CoverageValidation: 0.53364
INFO:root:IntervalWidthValidation: 6.90276
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 156.07751
INFO:root:EnergyScoreTest: 110.00073
INFO:root:CoverageTest: 0.53394
INFO:root:IntervalWidthTest: 6.90049
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.39510891, Validation loss: 121.23779859, Gradient norm: 168.29353180
INFO:root:[    2] Training loss: 120.38284932, Validation loss: 119.95159778, Gradient norm: 74.39299470
INFO:root:[    3] Training loss: 119.68134837, Validation loss: 119.50221400, Gradient norm: 62.32683235
INFO:root:[    4] Training loss: 119.17076138, Validation loss: 118.91490039, Gradient norm: 61.00881016
INFO:root:[    5] Training loss: 118.58791812, Validation loss: 118.64815682, Gradient norm: 60.92817829
INFO:root:[    6] Training loss: 118.19025367, Validation loss: 118.08269086, Gradient norm: 59.43684318
INFO:root:[    7] Training loss: 117.91250353, Validation loss: 117.86806943, Gradient norm: 62.73611781
INFO:root:[    8] Training loss: 117.71015622, Validation loss: 117.90380685, Gradient norm: 63.12224851
INFO:root:[    9] Training loss: 117.59779104, Validation loss: 117.63089391, Gradient norm: 87.02721560
INFO:root:[   10] Training loss: 117.37815779, Validation loss: 117.38955247, Gradient norm: 88.16233947
INFO:root:[   11] Training loss: 117.20414439, Validation loss: 117.29637320, Gradient norm: 85.85823605
INFO:root:[   12] Training loss: 117.05014404, Validation loss: 117.08931398, Gradient norm: 96.34700939
INFO:root:[   13] Training loss: 116.87423692, Validation loss: 117.37367034, Gradient norm: 120.53022685
INFO:root:[   14] Training loss: 116.69102746, Validation loss: 116.83215051, Gradient norm: 129.71168531
INFO:root:[   15] Training loss: 116.55292681, Validation loss: 116.78683780, Gradient norm: 144.30579114
INFO:root:[   16] Training loss: 116.38847619, Validation loss: 116.77823719, Gradient norm: 152.85347528
INFO:root:[   17] Training loss: 116.21261631, Validation loss: 116.41306225, Gradient norm: 179.29501992
INFO:root:[   18] Training loss: 116.00340196, Validation loss: 116.17578821, Gradient norm: 174.72536593
INFO:root:[   19] Training loss: 115.86153965, Validation loss: 115.94239432, Gradient norm: 211.91749375
INFO:root:[   20] Training loss: 115.64231232, Validation loss: 115.83025307, Gradient norm: 208.17518788
INFO:root:[   21] Training loss: 115.42475477, Validation loss: 115.56391063, Gradient norm: 221.02657809
INFO:root:[   22] Training loss: 115.17133670, Validation loss: 115.25242120, Gradient norm: 226.79104642
INFO:root:[   23] Training loss: 114.83740241, Validation loss: 114.80333442, Gradient norm: 204.11580009
INFO:root:[   24] Training loss: 114.60015418, Validation loss: 114.35688206, Gradient norm: 211.17715016
INFO:root:[   25] Training loss: 114.34014859, Validation loss: 113.98427287, Gradient norm: 197.16906515
INFO:root:[   26] Training loss: 114.02024350, Validation loss: 114.05193195, Gradient norm: 172.77076469
INFO:root:[   27] Training loss: 113.88491269, Validation loss: 113.75529922, Gradient norm: 184.66915050
INFO:root:[   28] Training loss: 113.58280965, Validation loss: 113.73814660, Gradient norm: 164.72632267
INFO:root:[   29] Training loss: 113.48643168, Validation loss: 113.26236765, Gradient norm: 177.72533973
INFO:root:[   30] Training loss: 113.25351752, Validation loss: 113.12191197, Gradient norm: 184.27278181
INFO:root:[   31] Training loss: 113.20678335, Validation loss: 113.02963364, Gradient norm: 201.66300199
INFO:root:[   32] Training loss: 112.93034424, Validation loss: 113.06282579, Gradient norm: 189.91977190
INFO:root:[   33] Training loss: 112.88595293, Validation loss: 112.81387838, Gradient norm: 207.83122984
INFO:root:[   34] Training loss: 112.80118818, Validation loss: 112.81104640, Gradient norm: 187.08273390
INFO:root:[   35] Training loss: 112.67940904, Validation loss: 113.09388171, Gradient norm: 206.83065434
INFO:root:[   36] Training loss: 112.49438351, Validation loss: 112.62736980, Gradient norm: 195.84554770
INFO:root:[   37] Training loss: 112.45854153, Validation loss: 112.62060600, Gradient norm: 221.96381377
INFO:root:[   38] Training loss: 112.41101342, Validation loss: 112.65695726, Gradient norm: 215.26917294
INFO:root:[   39] Training loss: 112.30928870, Validation loss: 112.24506445, Gradient norm: 222.42945721
INFO:root:[   40] Training loss: 112.28149923, Validation loss: 112.34925909, Gradient norm: 209.51974513
INFO:root:[   41] Training loss: 112.17434937, Validation loss: 112.11502156, Gradient norm: 221.13545131
INFO:root:[   42] Training loss: 112.03747382, Validation loss: 112.06846391, Gradient norm: 204.89495240
INFO:root:[   43] Training loss: 111.95845371, Validation loss: 112.04891031, Gradient norm: 212.91191678
INFO:root:[   44] Training loss: 111.89957614, Validation loss: 111.88622525, Gradient norm: 205.27841266
INFO:root:[   45] Training loss: 111.94604441, Validation loss: 111.88536607, Gradient norm: 225.62559157
INFO:root:[   46] Training loss: 111.73375292, Validation loss: 112.09812807, Gradient norm: 214.34946129
INFO:root:[   47] Training loss: 111.70655487, Validation loss: 111.81589562, Gradient norm: 223.57006745
INFO:root:[   48] Training loss: 111.68956390, Validation loss: 111.56254872, Gradient norm: 217.29847214
INFO:root:[   49] Training loss: 111.62679270, Validation loss: 111.61908066, Gradient norm: 209.84099820
INFO:root:[   50] Training loss: 111.68079749, Validation loss: 111.79299298, Gradient norm: 213.36744975
INFO:root:[   51] Training loss: 111.51720940, Validation loss: 111.53166118, Gradient norm: 220.15496694
INFO:root:[   52] Training loss: 111.54673319, Validation loss: 111.63472989, Gradient norm: 217.09767978
INFO:root:[   53] Training loss: 111.49063863, Validation loss: 111.54320098, Gradient norm: 220.40737239
INFO:root:[   54] Training loss: 111.42933377, Validation loss: 111.18765888, Gradient norm: 222.45179069
INFO:root:[   55] Training loss: 111.33618544, Validation loss: 111.41474767, Gradient norm: 206.68719666
INFO:root:[   56] Training loss: 111.36641069, Validation loss: 111.60412745, Gradient norm: 217.85273990
INFO:root:[   57] Training loss: 111.32015957, Validation loss: 111.44501991, Gradient norm: 221.42089881
INFO:root:[   58] Training loss: 111.20244724, Validation loss: 111.48050797, Gradient norm: 213.40583439
INFO:root:[   59] Training loss: 111.30767487, Validation loss: 111.30079972, Gradient norm: 234.52054431
INFO:root:[   60] Training loss: 111.26243496, Validation loss: 111.10064978, Gradient norm: 229.36147042
INFO:root:[   61] Training loss: 111.13581763, Validation loss: 111.07991751, Gradient norm: 221.46794574
INFO:root:[   62] Training loss: 111.16226579, Validation loss: 111.23632344, Gradient norm: 215.72002792
INFO:root:[   63] Training loss: 111.19319088, Validation loss: 111.30767099, Gradient norm: 224.89507214
INFO:root:[   64] Training loss: 111.05254744, Validation loss: 111.30707671, Gradient norm: 230.77400098
INFO:root:[   65] Training loss: 111.09768806, Validation loss: 110.85022562, Gradient norm: 231.45665340
INFO:root:[   66] Training loss: 111.03939565, Validation loss: 110.99794314, Gradient norm: 223.34083073
INFO:root:[   67] Training loss: 110.94547021, Validation loss: 111.16459361, Gradient norm: 228.08957811
INFO:root:[   68] Training loss: 111.04094035, Validation loss: 111.21070179, Gradient norm: 235.08076614
INFO:root:[   69] Training loss: 110.92798018, Validation loss: 111.08474986, Gradient norm: 239.33418914
INFO:root:[   70] Training loss: 110.91725742, Validation loss: 111.01091164, Gradient norm: 221.77305907
INFO:root:[   71] Training loss: 110.92881656, Validation loss: 111.22394883, Gradient norm: 234.47880030
INFO:root:[   72] Training loss: 110.85510685, Validation loss: 111.07303900, Gradient norm: 222.77121124
INFO:root:[   73] Training loss: 110.86130578, Validation loss: 111.18648944, Gradient norm: 214.56996558
INFO:root:[   74] Training loss: 110.84546312, Validation loss: 110.77240566, Gradient norm: 237.96451173
INFO:root:[   75] Training loss: 110.76338437, Validation loss: 110.85366527, Gradient norm: 220.98150030
INFO:root:[   76] Training loss: 110.80952199, Validation loss: 110.94834820, Gradient norm: 234.52003356
INFO:root:[   77] Training loss: 110.76382884, Validation loss: 110.91687320, Gradient norm: 238.71405466
INFO:root:[   78] Training loss: 110.75081109, Validation loss: 110.85056037, Gradient norm: 236.40951382
INFO:root:[   79] Training loss: 110.70888953, Validation loss: 110.76875171, Gradient norm: 246.62118751
INFO:root:[   80] Training loss: 110.73348202, Validation loss: 111.16791682, Gradient norm: 244.23078094
INFO:root:[   81] Training loss: 110.64360870, Validation loss: 110.77388723, Gradient norm: 235.18791219
INFO:root:[   82] Training loss: 110.66066359, Validation loss: 110.82308773, Gradient norm: 230.32851642
INFO:root:[   83] Training loss: 110.63905267, Validation loss: 110.84650461, Gradient norm: 245.58293178
INFO:root:[   84] Training loss: 110.65467957, Validation loss: 110.81049922, Gradient norm: 237.93038629
INFO:root:[   85] Training loss: 110.57032488, Validation loss: 110.69481646, Gradient norm: 239.94089416
INFO:root:[   86] Training loss: 110.54302833, Validation loss: 110.82645711, Gradient norm: 243.47427363
INFO:root:[   87] Training loss: 110.55328627, Validation loss: 110.58975421, Gradient norm: 245.12512111
INFO:root:[   88] Training loss: 110.63659576, Validation loss: 110.75084111, Gradient norm: 231.62524677
INFO:root:[   89] Training loss: 110.53507358, Validation loss: 110.58728389, Gradient norm: 233.35021672
INFO:root:[   90] Training loss: 110.49477770, Validation loss: 110.75824055, Gradient norm: 230.71587820
INFO:root:[   91] Training loss: 110.51276801, Validation loss: 110.63635227, Gradient norm: 243.57913698
INFO:root:[   92] Training loss: 110.48223538, Validation loss: 110.64127658, Gradient norm: 238.40315132
INFO:root:[   93] Training loss: 110.48608795, Validation loss: 110.22598869, Gradient norm: 247.22213691
INFO:root:[   94] Training loss: 110.46590240, Validation loss: 110.36197796, Gradient norm: 239.84074450
INFO:root:[   95] Training loss: 110.46238007, Validation loss: 110.58317205, Gradient norm: 241.61989265
INFO:root:[   96] Training loss: 110.42746484, Validation loss: 110.90312770, Gradient norm: 240.66622616
INFO:root:[   97] Training loss: 110.50560377, Validation loss: 110.51178634, Gradient norm: 260.84597338
INFO:root:[   98] Training loss: 110.48088277, Validation loss: 110.41866610, Gradient norm: 248.37299048
INFO:root:[   99] Training loss: 110.38324704, Validation loss: 110.37770710, Gradient norm: 242.10679647
INFO:root:[  100] Training loss: 110.36961443, Validation loss: 110.35623316, Gradient norm: 240.50859470
INFO:root:[  101] Training loss: 110.30120995, Validation loss: 110.35524723, Gradient norm: 235.27451918
INFO:root:[  102] Training loss: 110.37148275, Validation loss: 110.56373422, Gradient norm: 258.50720121
INFO:root:EP 102: Early stopping
INFO:root:Training the model took 4419.823s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 156.25775
INFO:root:EnergyScoreTrain: 110.19824
INFO:root:CoverageTrain: 0.50637
INFO:root:IntervalWidthTrain: 6.72543
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 156.5123
INFO:root:EnergyScoreValidation: 110.37813
INFO:root:CoverageValidation: 0.50539
INFO:root:IntervalWidthValidation: 6.7064
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 156.5292
INFO:root:EnergyScoreTest: 110.38293
INFO:root:CoverageTest: 0.50507
INFO:root:IntervalWidthTest: 6.71202
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 157286400
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.43324639, Validation loss: 121.57770926, Gradient norm: 65.55482159
INFO:root:[    2] Training loss: 121.03885715, Validation loss: 120.33167963, Gradient norm: 34.79575343
INFO:root:[    3] Training loss: 119.97749285, Validation loss: 120.06560784, Gradient norm: 30.44951319
INFO:root:[    4] Training loss: 119.50288015, Validation loss: 119.40136705, Gradient norm: 34.09120880
INFO:root:[    5] Training loss: 119.00821008, Validation loss: 119.00580798, Gradient norm: 35.76670419
INFO:root:[    6] Training loss: 118.69847795, Validation loss: 118.44359562, Gradient norm: 42.93666674
INFO:root:[    7] Training loss: 118.32405853, Validation loss: 118.20325497, Gradient norm: 49.94631633
INFO:root:[    8] Training loss: 117.98916501, Validation loss: 118.08305212, Gradient norm: 55.00327320
INFO:root:[    9] Training loss: 117.76634101, Validation loss: 117.87636794, Gradient norm: 71.82853390
INFO:root:[   10] Training loss: 117.68389564, Validation loss: 117.68080045, Gradient norm: 79.89492816
INFO:root:[   11] Training loss: 117.58417748, Validation loss: 117.54290678, Gradient norm: 105.97857136
INFO:root:[   12] Training loss: 117.43808787, Validation loss: 117.57788139, Gradient norm: 133.00664355
INFO:root:[   13] Training loss: 117.29261831, Validation loss: 117.36254361, Gradient norm: 168.70513688
INFO:root:[   14] Training loss: 117.20692976, Validation loss: 117.33221195, Gradient norm: 221.45624139
INFO:root:[   15] Training loss: 117.08880880, Validation loss: 117.25875854, Gradient norm: 256.63035767
INFO:root:[   16] Training loss: 116.95304392, Validation loss: 116.88705712, Gradient norm: 288.83992799
INFO:root:[   17] Training loss: 116.81565830, Validation loss: 117.01373465, Gradient norm: 257.20639033
INFO:root:[   18] Training loss: 116.56134915, Validation loss: 116.38287086, Gradient norm: 200.28219669
INFO:root:[   19] Training loss: 116.32060082, Validation loss: 116.36797882, Gradient norm: 212.03405768
INFO:root:[   20] Training loss: 116.02503669, Validation loss: 115.81858023, Gradient norm: 220.41845434
INFO:root:[   21] Training loss: 115.88171302, Validation loss: 115.72506165, Gradient norm: 244.24015939
INFO:root:[   22] Training loss: 115.70866225, Validation loss: 115.74540215, Gradient norm: 261.82188381
INFO:root:[   23] Training loss: 115.64669552, Validation loss: 115.52805087, Gradient norm: 281.73921891
INFO:root:[   24] Training loss: 115.45292538, Validation loss: 115.54616212, Gradient norm: 302.86814580
INFO:root:[   25] Training loss: 115.40714444, Validation loss: 115.28607994, Gradient norm: 292.68246950
INFO:root:[   26] Training loss: 115.32122491, Validation loss: 115.23609951, Gradient norm: 307.45403489
INFO:root:[   27] Training loss: 115.27774750, Validation loss: 114.84139707, Gradient norm: 321.81543595
INFO:root:[   28] Training loss: 115.02768022, Validation loss: 115.41460178, Gradient norm: 317.66873360
INFO:root:[   29] Training loss: 115.00921377, Validation loss: 114.99992518, Gradient norm: 336.42067712
INFO:root:[   30] Training loss: 114.90771996, Validation loss: 115.08086288, Gradient norm: 335.94616166
INFO:root:[   31] Training loss: 114.87398878, Validation loss: 114.64226171, Gradient norm: 346.40247041
INFO:root:[   32] Training loss: 114.73057766, Validation loss: 114.58868622, Gradient norm: 343.31544169
INFO:root:[   33] Training loss: 114.61384586, Validation loss: 114.77491988, Gradient norm: 306.35293608
INFO:root:[   34] Training loss: 114.53707113, Validation loss: 114.52568135, Gradient norm: 311.61202293
INFO:root:[   35] Training loss: 114.43488325, Validation loss: 114.24110038, Gradient norm: 296.86904662
INFO:root:[   36] Training loss: 114.32730238, Validation loss: 114.28314209, Gradient norm: 301.31723233
INFO:root:[   37] Training loss: 114.27473904, Validation loss: 114.12101705, Gradient norm: 297.15745255
INFO:root:[   38] Training loss: 114.17474552, Validation loss: 114.45500678, Gradient norm: 269.01409678
INFO:root:[   39] Training loss: 114.09040714, Validation loss: 114.10835802, Gradient norm: 281.68018515
INFO:root:[   40] Training loss: 114.10647993, Validation loss: 114.09872677, Gradient norm: 278.58542678
INFO:root:[   41] Training loss: 113.91095357, Validation loss: 113.91892242, Gradient norm: 243.75671756
INFO:root:[   42] Training loss: 113.91642334, Validation loss: 113.88548413, Gradient norm: 269.96803346
INFO:root:[   43] Training loss: 113.81161648, Validation loss: 113.66100472, Gradient norm: 258.51723523
INFO:root:[   44] Training loss: 113.75259657, Validation loss: 113.61892339, Gradient norm: 241.67243081
INFO:root:[   45] Training loss: 113.68400140, Validation loss: 113.44051254, Gradient norm: 255.84831262
INFO:root:[   46] Training loss: 113.62302199, Validation loss: 113.54695491, Gradient norm: 236.33028944
INFO:root:[   47] Training loss: 113.46178250, Validation loss: 113.41862903, Gradient norm: 224.98033844
INFO:root:[   48] Training loss: 113.55058770, Validation loss: 113.36219814, Gradient norm: 233.73778920
INFO:root:[   49] Training loss: 113.36519826, Validation loss: 113.61451079, Gradient norm: 225.49936454
INFO:root:[   50] Training loss: 113.23840786, Validation loss: 113.09443250, Gradient norm: 213.60542461
INFO:root:[   51] Training loss: 113.18493554, Validation loss: 113.81556180, Gradient norm: 215.31312197
INFO:root:[   52] Training loss: 113.08815057, Validation loss: 113.77848066, Gradient norm: 221.27497201
INFO:root:[   53] Training loss: 113.05377340, Validation loss: 112.90546953, Gradient norm: 214.43267998
INFO:root:[   54] Training loss: 112.93733660, Validation loss: 113.45201392, Gradient norm: 213.26701702
INFO:root:[   55] Training loss: 112.86185516, Validation loss: 112.85614937, Gradient norm: 206.04881935
INFO:root:[   56] Training loss: 112.78000529, Validation loss: 112.82420965, Gradient norm: 209.41922301
INFO:root:[   57] Training loss: 112.77324554, Validation loss: 112.89587617, Gradient norm: 205.16501674
INFO:root:[   58] Training loss: 112.67298842, Validation loss: 112.94145015, Gradient norm: 206.34223561
INFO:root:[   59] Training loss: 112.67324022, Validation loss: 112.51509108, Gradient norm: 215.14338317
INFO:root:[   60] Training loss: 112.50495283, Validation loss: 112.71213946, Gradient norm: 192.13855956
INFO:root:[   61] Training loss: 112.59588908, Validation loss: 112.44538812, Gradient norm: 215.27382515
INFO:root:[   62] Training loss: 112.51554633, Validation loss: 112.47491455, Gradient norm: 203.13721728
INFO:root:[   63] Training loss: 112.46862040, Validation loss: 112.59536382, Gradient norm: 202.66480786
INFO:root:[   64] Training loss: 112.43310093, Validation loss: 112.38890464, Gradient norm: 203.14444670
INFO:root:[   65] Training loss: 112.42677399, Validation loss: 112.67378315, Gradient norm: 203.10038220
INFO:root:[   66] Training loss: 112.32259443, Validation loss: 112.56199954, Gradient norm: 205.70999918
INFO:root:[   67] Training loss: 112.31762614, Validation loss: 112.03703027, Gradient norm: 194.02503255
INFO:root:[   68] Training loss: 112.23181085, Validation loss: 112.52494544, Gradient norm: 205.08534906
INFO:root:[   69] Training loss: 112.22528605, Validation loss: 112.11398235, Gradient norm: 212.67103046
INFO:root:[   70] Training loss: 112.10738692, Validation loss: 112.49464591, Gradient norm: 204.71006843
INFO:root:[   71] Training loss: 112.21593618, Validation loss: 112.45498042, Gradient norm: 210.13397294
INFO:root:[   72] Training loss: 112.14300381, Validation loss: 112.21282705, Gradient norm: 210.47030293
INFO:root:[   73] Training loss: 112.05354892, Validation loss: 112.06558723, Gradient norm: 196.27790487
INFO:root:[   74] Training loss: 112.03834435, Validation loss: 112.73659368, Gradient norm: 212.39288221
INFO:root:[   75] Training loss: 112.01209269, Validation loss: 112.30236321, Gradient norm: 188.45492378
INFO:root:[   76] Training loss: 112.01152781, Validation loss: 112.03797458, Gradient norm: 204.87044071
INFO:root:EP 76: Early stopping
INFO:root:Training the model took 3311.565s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 158.88917
INFO:root:EnergyScoreTrain: 112.03633
INFO:root:CoverageTrain: 0.48526
INFO:root:IntervalWidthTrain: 6.76809
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 159.05313
INFO:root:EnergyScoreValidation: 112.15687
INFO:root:CoverageValidation: 0.48421
INFO:root:IntervalWidthValidation: 6.75361
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 159.02836
INFO:root:EnergyScoreTest: 112.13707
INFO:root:CoverageTest: 0.48484
INFO:root:IntervalWidthTest: 6.76803
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 299892736
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.23736325, Validation loss: 120.44961588, Gradient norm: 74.67846039
INFO:root:[    2] Training loss: 120.07273014, Validation loss: 119.29068957, Gradient norm: 16.62844940
INFO:root:[    3] Training loss: 118.97660387, Validation loss: 118.25462248, Gradient norm: 24.92279447
INFO:root:[    4] Training loss: 118.25158827, Validation loss: 117.82746245, Gradient norm: 25.22882675
INFO:root:[    5] Training loss: 117.86243500, Validation loss: 117.45482033, Gradient norm: 27.29394945
INFO:root:[    6] Training loss: 117.58874390, Validation loss: 117.28077042, Gradient norm: 27.00723161
INFO:root:[    7] Training loss: 117.42027191, Validation loss: 117.11352111, Gradient norm: 32.99898037
INFO:root:[    8] Training loss: 117.27714220, Validation loss: 116.93317775, Gradient norm: 34.70654634
INFO:root:[    9] Training loss: 117.11150835, Validation loss: 116.94367352, Gradient norm: 41.49609279
INFO:root:[   10] Training loss: 116.99074571, Validation loss: 116.93110965, Gradient norm: 50.61547652
INFO:root:[   11] Training loss: 116.97944465, Validation loss: 117.07699732, Gradient norm: 55.92979386
INFO:root:[   12] Training loss: 116.94497097, Validation loss: 116.71882214, Gradient norm: 59.30255580
INFO:root:[   13] Training loss: 116.84277822, Validation loss: 116.69042942, Gradient norm: 55.34458151
INFO:root:[   14] Training loss: 116.81712172, Validation loss: 116.70228818, Gradient norm: 62.62748502
INFO:root:[   15] Training loss: 116.82062629, Validation loss: 116.95288447, Gradient norm: 69.37383963
INFO:root:[   16] Training loss: 116.72543121, Validation loss: 116.64881562, Gradient norm: 68.29484116
INFO:root:[   17] Training loss: 116.67740974, Validation loss: 116.99905195, Gradient norm: 66.20607314
INFO:root:[   18] Training loss: 116.60433306, Validation loss: 116.51603672, Gradient norm: 68.23667350
INFO:root:[   19] Training loss: 116.63548703, Validation loss: 116.43975763, Gradient norm: 69.11121584
INFO:root:[   20] Training loss: 116.58726718, Validation loss: 116.48854988, Gradient norm: 59.67075251
INFO:root:[   21] Training loss: 116.51579854, Validation loss: 116.42493077, Gradient norm: 68.47793891
INFO:root:[   22] Training loss: 116.49201847, Validation loss: 116.58322572, Gradient norm: 62.87532547
INFO:root:[   23] Training loss: 116.48406392, Validation loss: 116.30563033, Gradient norm: 73.43630769
INFO:root:[   24] Training loss: 116.33772590, Validation loss: 116.04320941, Gradient norm: 72.33266568
INFO:root:[   25] Training loss: 116.15964715, Validation loss: 115.84455497, Gradient norm: 77.01067524
INFO:root:[   26] Training loss: 116.00015283, Validation loss: 115.93218459, Gradient norm: 71.74704491
INFO:root:[   27] Training loss: 115.88073368, Validation loss: 115.49715504, Gradient norm: 76.10703383
INFO:root:[   28] Training loss: 115.69041290, Validation loss: 115.31002527, Gradient norm: 71.63347797
INFO:root:[   29] Training loss: 115.55912944, Validation loss: 115.20363015, Gradient norm: 75.07084116
INFO:root:[   30] Training loss: 115.42288367, Validation loss: 115.26772737, Gradient norm: 73.80376389
INFO:root:[   31] Training loss: 115.27599362, Validation loss: 115.00895704, Gradient norm: 79.72369411
INFO:root:[   32] Training loss: 115.14804104, Validation loss: 114.71875214, Gradient norm: 78.36475031
INFO:root:[   33] Training loss: 114.99021396, Validation loss: 114.59855264, Gradient norm: 77.88958879
INFO:root:[   34] Training loss: 114.82471208, Validation loss: 114.40200766, Gradient norm: 82.08272955
INFO:root:[   35] Training loss: 114.70272851, Validation loss: 114.26181378, Gradient norm: 84.92682723
INFO:root:[   36] Training loss: 114.53891663, Validation loss: 114.02536841, Gradient norm: 91.83291519
INFO:root:[   37] Training loss: 114.41033132, Validation loss: 113.96403878, Gradient norm: 98.24880854
INFO:root:[   38] Training loss: 114.29174896, Validation loss: 113.81141649, Gradient norm: 101.26587948
INFO:root:[   39] Training loss: 114.18510488, Validation loss: 113.99185837, Gradient norm: 104.75161919
INFO:root:[   40] Training loss: 114.10865360, Validation loss: 113.67349123, Gradient norm: 112.98565246
INFO:root:[   41] Training loss: 114.02196269, Validation loss: 113.42138003, Gradient norm: 119.07962102
INFO:root:[   42] Training loss: 113.95355103, Validation loss: 113.62771647, Gradient norm: 124.87028256
INFO:root:[   43] Training loss: 113.85672285, Validation loss: 113.33487099, Gradient norm: 120.59995159
INFO:root:[   44] Training loss: 113.81527008, Validation loss: 113.37785861, Gradient norm: 131.37506465
INFO:root:[   45] Training loss: 113.75931827, Validation loss: 113.42793823, Gradient norm: 127.74977426
INFO:root:[   46] Training loss: 113.67277846, Validation loss: 113.09067883, Gradient norm: 133.35030263
INFO:root:[   47] Training loss: 113.63396077, Validation loss: 113.23269519, Gradient norm: 138.15310783
INFO:root:[   48] Training loss: 113.72310781, Validation loss: 113.32378628, Gradient norm: 150.43491172
INFO:root:[   49] Training loss: 113.57523773, Validation loss: 113.38273902, Gradient norm: 147.26259882
INFO:root:[   50] Training loss: 113.65440477, Validation loss: 113.15216894, Gradient norm: 154.14144952
INFO:root:[   51] Training loss: 113.57778385, Validation loss: 113.01032498, Gradient norm: 159.66176402
INFO:root:[   52] Training loss: 113.43132673, Validation loss: 112.97930654, Gradient norm: 153.46283955
INFO:root:[   53] Training loss: 113.38642114, Validation loss: 113.14035596, Gradient norm: 159.00071971
INFO:root:[   54] Training loss: 113.33892629, Validation loss: 112.95585887, Gradient norm: 163.51137815
INFO:root:[   55] Training loss: 113.26261861, Validation loss: 112.75282689, Gradient norm: 158.32342239
INFO:root:[   56] Training loss: 113.14861264, Validation loss: 112.71333848, Gradient norm: 153.37477285
INFO:root:[   57] Training loss: 113.13752421, Validation loss: 112.75487264, Gradient norm: 169.18327932
INFO:root:[   58] Training loss: 113.13875529, Validation loss: 112.65289735, Gradient norm: 160.99825588
INFO:root:[   59] Training loss: 113.01820940, Validation loss: 112.55386232, Gradient norm: 165.57650988
INFO:root:[   60] Training loss: 112.95820133, Validation loss: 112.29665294, Gradient norm: 164.38332073
INFO:root:[   61] Training loss: 112.92202257, Validation loss: 112.51327140, Gradient norm: 173.43923676
INFO:root:[   62] Training loss: 112.86514330, Validation loss: 112.32621979, Gradient norm: 172.46374964
INFO:root:[   63] Training loss: 112.82431078, Validation loss: 112.40276645, Gradient norm: 172.73056871
INFO:root:[   64] Training loss: 112.80147573, Validation loss: 112.34258578, Gradient norm: 166.72714845
INFO:root:[   65] Training loss: 112.88235009, Validation loss: 113.09616771, Gradient norm: 178.44218929
INFO:root:[   66] Training loss: 112.78453430, Validation loss: 112.32720974, Gradient norm: 161.25527995
INFO:root:[   67] Training loss: 112.73922574, Validation loss: 112.35121128, Gradient norm: 168.95472354
INFO:root:[   68] Training loss: 112.77112779, Validation loss: 112.31729635, Gradient norm: 170.71208539
INFO:root:[   69] Training loss: 112.67221113, Validation loss: 112.36410496, Gradient norm: 178.51139370
INFO:root:[   70] Training loss: 112.60756588, Validation loss: 112.18714637, Gradient norm: 172.29462798
INFO:root:[   71] Training loss: 112.63108415, Validation loss: 112.28139750, Gradient norm: 176.74885882
INFO:root:[   72] Training loss: 112.70577742, Validation loss: 112.42895521, Gradient norm: 180.86705373
INFO:root:[   73] Training loss: 112.58898105, Validation loss: 112.14576119, Gradient norm: 174.90591193
INFO:root:[   74] Training loss: 112.58376550, Validation loss: 112.29035655, Gradient norm: 187.65166167
INFO:root:[   75] Training loss: 112.51723239, Validation loss: 112.20217066, Gradient norm: 180.35666213
INFO:root:[   76] Training loss: 112.46178613, Validation loss: 111.95789083, Gradient norm: 181.37721810
INFO:root:[   77] Training loss: 112.45153663, Validation loss: 111.91233143, Gradient norm: 178.97116740
INFO:root:[   78] Training loss: 112.51621724, Validation loss: 112.10009177, Gradient norm: 188.09921318
INFO:root:[   79] Training loss: 112.51069360, Validation loss: 111.92704505, Gradient norm: 178.40101254
INFO:root:[   80] Training loss: 112.40738719, Validation loss: 111.88857872, Gradient norm: 178.43672369
INFO:root:[   81] Training loss: 112.40570889, Validation loss: 112.02246027, Gradient norm: 181.38163197
INFO:root:[   82] Training loss: 112.42280426, Validation loss: 111.91241924, Gradient norm: 183.09213148
INFO:root:[   83] Training loss: 112.31231998, Validation loss: 111.98568083, Gradient norm: 169.24968561
INFO:root:[   84] Training loss: 112.30688765, Validation loss: 111.91402690, Gradient norm: 180.90022205
INFO:root:[   85] Training loss: 112.28417274, Validation loss: 111.90053331, Gradient norm: 178.05697063
INFO:root:[   86] Training loss: 112.25618856, Validation loss: 111.80663768, Gradient norm: 171.46142576
INFO:root:[   87] Training loss: 112.23291843, Validation loss: 111.92129396, Gradient norm: 181.43015150
INFO:root:[   88] Training loss: 112.20676822, Validation loss: 111.78058758, Gradient norm: 174.63188985
INFO:root:[   89] Training loss: 112.21751611, Validation loss: 111.76795852, Gradient norm: 186.33718617
INFO:root:[   90] Training loss: 112.13638584, Validation loss: 111.86081776, Gradient norm: 179.91850681
INFO:root:[   91] Training loss: 112.14045376, Validation loss: 112.03454068, Gradient norm: 184.62570573
INFO:root:[   92] Training loss: 112.09458625, Validation loss: 111.82906730, Gradient norm: 181.73241278
INFO:root:[   93] Training loss: 112.17159108, Validation loss: 111.71982601, Gradient norm: 190.79059585
INFO:root:[   94] Training loss: 112.09316050, Validation loss: 111.60893932, Gradient norm: 188.30281876
INFO:root:[   95] Training loss: 112.03164914, Validation loss: 111.62382119, Gradient norm: 192.00487960
INFO:root:[   96] Training loss: 112.05968146, Validation loss: 111.96320303, Gradient norm: 201.78344176
INFO:root:[   97] Training loss: 112.04229811, Validation loss: 111.66978348, Gradient norm: 200.99651773
INFO:root:[   98] Training loss: 111.94988481, Validation loss: 111.62053426, Gradient norm: 190.44456358
INFO:root:[   99] Training loss: 111.90213749, Validation loss: 111.55763017, Gradient norm: 183.26174980
INFO:root:[  100] Training loss: 111.92268212, Validation loss: 111.75306849, Gradient norm: 187.46773047
INFO:root:[  101] Training loss: 111.87897522, Validation loss: 111.45132634, Gradient norm: 189.06126026
INFO:root:[  102] Training loss: 111.91468726, Validation loss: 111.61133629, Gradient norm: 193.47522630
INFO:root:[  103] Training loss: 111.87868011, Validation loss: 111.46663679, Gradient norm: 181.76549946
INFO:root:[  104] Training loss: 111.84366377, Validation loss: 111.33863523, Gradient norm: 192.16032097
INFO:root:[  105] Training loss: 111.82892266, Validation loss: 111.47065039, Gradient norm: 194.09674613
INFO:root:[  106] Training loss: 111.76705905, Validation loss: 111.32670513, Gradient norm: 179.62773123
INFO:root:[  107] Training loss: 111.75563721, Validation loss: 111.39485383, Gradient norm: 186.72752152
INFO:root:[  108] Training loss: 111.72060055, Validation loss: 111.27691838, Gradient norm: 177.40150349
INFO:root:[  109] Training loss: 111.75950256, Validation loss: 111.43673050, Gradient norm: 190.12561694
INFO:root:[  110] Training loss: 111.79584429, Validation loss: 111.36387447, Gradient norm: 188.52485180
INFO:root:[  111] Training loss: 111.80216970, Validation loss: 111.29390248, Gradient norm: 197.40699210
INFO:root:[  112] Training loss: 111.70784661, Validation loss: 111.29403713, Gradient norm: 184.20622550
INFO:root:[  113] Training loss: 111.71431396, Validation loss: 111.32191789, Gradient norm: 197.47342741
INFO:root:[  114] Training loss: 111.66358202, Validation loss: 111.29080374, Gradient norm: 185.14757987
INFO:root:[  115] Training loss: 111.66252608, Validation loss: 111.22544714, Gradient norm: 179.62980320
INFO:root:[  116] Training loss: 111.65882952, Validation loss: 111.33687190, Gradient norm: 190.15472438
INFO:root:[  117] Training loss: 111.70426809, Validation loss: 111.16651140, Gradient norm: 188.60339700
INFO:root:[  118] Training loss: 111.65143511, Validation loss: 111.34541856, Gradient norm: 192.92366189
INFO:root:[  119] Training loss: 111.69157532, Validation loss: 111.39218622, Gradient norm: 192.87750416
INFO:root:[  120] Training loss: 111.75397586, Validation loss: 111.39695044, Gradient norm: 201.93731834
INFO:root:[  121] Training loss: 111.84896094, Validation loss: 111.40834500, Gradient norm: 198.85464692
INFO:root:[  122] Training loss: 111.67007877, Validation loss: 111.28492068, Gradient norm: 194.17298043
INFO:root:[  123] Training loss: 111.61997064, Validation loss: 111.26483623, Gradient norm: 188.74110085
INFO:root:[  124] Training loss: 111.59941589, Validation loss: 111.10875836, Gradient norm: 190.33158135
INFO:root:[  125] Training loss: 111.59812842, Validation loss: 111.33675947, Gradient norm: 195.41166027
INFO:root:[  126] Training loss: 111.67919020, Validation loss: 111.37937526, Gradient norm: 191.77878688
INFO:root:[  127] Training loss: 111.70849121, Validation loss: 111.29499094, Gradient norm: 204.72451236
INFO:root:[  128] Training loss: 111.59130595, Validation loss: 111.22272679, Gradient norm: 185.42925941
INFO:root:[  129] Training loss: 111.52996287, Validation loss: 111.50156362, Gradient norm: 181.55817591
INFO:root:[  130] Training loss: 111.61986847, Validation loss: 111.25088943, Gradient norm: 196.80242421
INFO:root:[  131] Training loss: 111.66561086, Validation loss: 111.36305612, Gradient norm: 200.68374205
INFO:root:[  132] Training loss: 111.83386726, Validation loss: 111.28834962, Gradient norm: 223.05173186
INFO:root:[  133] Training loss: 111.72445414, Validation loss: 111.41916590, Gradient norm: 214.81530539
INFO:root:EP 133: Early stopping
INFO:root:Training the model took 14582.37s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 157.43139
INFO:root:EnergyScoreTrain: 110.79855
INFO:root:CoverageTrain: 0.5039
INFO:root:IntervalWidthTrain: 6.63269
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 157.90706
INFO:root:EnergyScoreValidation: 111.13537
INFO:root:CoverageValidation: 0.50491
INFO:root:IntervalWidthValidation: 6.64281
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 157.74738
INFO:root:EnergyScoreTest: 111.02114
INFO:root:CoverageTest: 0.5046
INFO:root:IntervalWidthTest: 6.63724
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.75879656, Validation loss: 120.72497425, Gradient norm: 47.08342649
INFO:root:[    2] Training loss: 120.25742306, Validation loss: 119.54626947, Gradient norm: 33.94317888
INFO:root:[    3] Training loss: 119.31286919, Validation loss: 129.99048387, Gradient norm: 50.86415198
INFO:root:[    4] Training loss: 119.02437083, Validation loss: 124.66761619, Gradient norm: 64.34659267
INFO:root:[    5] Training loss: 118.67157423, Validation loss: 133.82306711, Gradient norm: 65.43139082
INFO:root:[    6] Training loss: 118.53007833, Validation loss: 135.84558105, Gradient norm: 73.85670179
INFO:root:[    7] Training loss: 118.41704061, Validation loss: 123.97324666, Gradient norm: 67.20764175
INFO:root:[    8] Training loss: 118.22334615, Validation loss: 129.06420550, Gradient norm: 71.67315816
INFO:root:[    9] Training loss: 118.20085459, Validation loss: 123.92993499, Gradient norm: 78.27712427
INFO:root:[   10] Training loss: 118.16387885, Validation loss: 119.94579891, Gradient norm: 85.44375249
INFO:root:[   11] Training loss: 118.11331736, Validation loss: 121.52993574, Gradient norm: 95.89018907
INFO:root:[   12] Training loss: 118.19362010, Validation loss: 126.50669165, Gradient norm: 101.04975160
INFO:root:[   13] Training loss: 118.09021895, Validation loss: 118.41333021, Gradient norm: 96.15806631
INFO:root:[   14] Training loss: 118.05337474, Validation loss: 117.98771882, Gradient norm: 108.40601844
INFO:root:[   15] Training loss: 118.01484829, Validation loss: 124.61005415, Gradient norm: 141.12138143
INFO:root:[   16] Training loss: 117.90284078, Validation loss: 117.67214845, Gradient norm: 133.99976518
INFO:root:[   17] Training loss: 117.79951202, Validation loss: 118.86672639, Gradient norm: 157.83436817
INFO:root:[   18] Training loss: 117.77671760, Validation loss: 117.15470431, Gradient norm: 183.68598473
INFO:root:[   19] Training loss: 117.81549269, Validation loss: 117.13242969, Gradient norm: 215.77174146
INFO:root:[   20] Training loss: 117.72079498, Validation loss: 117.34677218, Gradient norm: 233.02715006
INFO:root:[   21] Training loss: 117.52859941, Validation loss: 117.30098135, Gradient norm: 224.36166952
INFO:root:[   22] Training loss: 117.71686812, Validation loss: 116.93681041, Gradient norm: 258.39364015
INFO:root:[   23] Training loss: 117.39606517, Validation loss: 116.66800235, Gradient norm: 278.24621989
INFO:root:[   24] Training loss: 117.14971222, Validation loss: 116.61275643, Gradient norm: 288.74269352
INFO:root:[   25] Training loss: 116.97004605, Validation loss: 116.17949891, Gradient norm: 268.46174965
INFO:root:[   26] Training loss: 116.81333591, Validation loss: 116.34043791, Gradient norm: 260.22193070
INFO:root:[   27] Training loss: 119.11495690, Validation loss: 120.29219430, Gradient norm: 350.97070025
INFO:root:[   28] Training loss: 119.50913405, Validation loss: 169.43078453, Gradient norm: 317.23011416
INFO:root:[   29] Training loss: 119.97864105, Validation loss: 168.77994926, Gradient norm: 184.73037823
INFO:root:[   30] Training loss: 119.57717563, Validation loss: 168.16536780, Gradient norm: 236.14600903
INFO:root:[   31] Training loss: 119.36002180, Validation loss: 168.01046887, Gradient norm: 261.93606835
INFO:root:[   32] Training loss: 119.25248186, Validation loss: 167.89860669, Gradient norm: 275.46080754
INFO:root:[   33] Training loss: 119.25728268, Validation loss: 167.70642920, Gradient norm: 278.45017343
INFO:root:[   34] Training loss: 118.57505880, Validation loss: 165.86569669, Gradient norm: 252.55835002
INFO:root:[   35] Training loss: 117.65272878, Validation loss: 164.81094066, Gradient norm: 242.12753484
INFO:root:[   36] Training loss: 117.46063541, Validation loss: 164.93237091, Gradient norm: 258.32712585
INFO:root:[   37] Training loss: 117.24515347, Validation loss: 164.41083460, Gradient norm: 241.41374118
INFO:root:[   38] Training loss: 117.09083123, Validation loss: 164.51986668, Gradient norm: 238.70544449
INFO:root:[   39] Training loss: 116.90450083, Validation loss: 164.33404755, Gradient norm: 219.13839409
INFO:root:[   40] Training loss: 116.88008199, Validation loss: 163.63338377, Gradient norm: 226.96633742
INFO:root:[   41] Training loss: 116.75090068, Validation loss: 163.18883233, Gradient norm: 228.04426490
INFO:root:[   42] Training loss: 116.85254076, Validation loss: 162.26420995, Gradient norm: 230.79001967
INFO:root:[   43] Training loss: 116.55726735, Validation loss: 163.67548491, Gradient norm: 210.36755941
INFO:root:[   44] Training loss: 116.45060713, Validation loss: 163.50515479, Gradient norm: 210.60157151
INFO:root:[   45] Training loss: 116.29124444, Validation loss: 163.38460768, Gradient norm: 192.44403739
INFO:root:[   46] Training loss: 116.16319570, Validation loss: 163.47965281, Gradient norm: 205.85787023
INFO:root:[   47] Training loss: 116.08939395, Validation loss: 162.86007101, Gradient norm: 186.38489868
INFO:root:[   48] Training loss: 115.92743062, Validation loss: 162.75075999, Gradient norm: 179.89093336
INFO:root:[   49] Training loss: 115.78732483, Validation loss: 162.48784597, Gradient norm: 171.73488575
INFO:root:[   50] Training loss: 116.28296594, Validation loss: 167.69230973, Gradient norm: 224.68211674
INFO:root:[   51] Training loss: 116.57036991, Validation loss: 162.90342498, Gradient norm: 222.40560450
INFO:root:[   52] Training loss: 115.80789412, Validation loss: 162.55571653, Gradient norm: 188.13874037
INFO:root:[   53] Training loss: 115.62621796, Validation loss: 162.09231005, Gradient norm: 174.06901899
INFO:root:[   54] Training loss: 115.45469089, Validation loss: 161.99506686, Gradient norm: 167.38889298
INFO:root:[   55] Training loss: 115.40949382, Validation loss: 161.90131338, Gradient norm: 180.27444902
INFO:root:[   56] Training loss: 115.29571832, Validation loss: 162.17708307, Gradient norm: 176.41277641
INFO:root:[   57] Training loss: 115.24116859, Validation loss: 161.56057605, Gradient norm: 178.60019193
INFO:root:[   58] Training loss: 115.07335059, Validation loss: 161.32864326, Gradient norm: 171.62399369
INFO:root:[   59] Training loss: 115.02485819, Validation loss: 161.11688607, Gradient norm: 169.65225999
INFO:root:[   60] Training loss: 114.96059689, Validation loss: 160.91670066, Gradient norm: 169.16687320
INFO:root:[   61] Training loss: 114.95470256, Validation loss: 160.98024121, Gradient norm: 173.74570788
INFO:root:[   62] Training loss: 114.93418705, Validation loss: 161.27491814, Gradient norm: 185.83058774
INFO:root:[   63] Training loss: 115.03723314, Validation loss: 160.97075318, Gradient norm: 188.91116106
INFO:root:[   64] Training loss: 114.79855899, Validation loss: 160.75523082, Gradient norm: 162.54053373
INFO:root:[   65] Training loss: 114.83636064, Validation loss: 160.91200551, Gradient norm: 173.93225464
INFO:root:[   66] Training loss: 114.82987671, Validation loss: 160.85772544, Gradient norm: 177.74329820
INFO:root:[   67] Training loss: 114.77028164, Validation loss: 160.70247048, Gradient norm: 173.91836714
INFO:root:[   68] Training loss: 114.69301008, Validation loss: 160.54549582, Gradient norm: 172.32440421
INFO:root:[   69] Training loss: 114.78621538, Validation loss: 160.96194860, Gradient norm: 167.91663082
INFO:root:[   70] Training loss: 114.72141239, Validation loss: 160.81609304, Gradient norm: 169.19910384
INFO:root:[   71] Training loss: 114.69876255, Validation loss: 160.91995801, Gradient norm: 177.44854813
INFO:root:[   72] Training loss: 114.65663564, Validation loss: 160.66652184, Gradient norm: 169.80384374
INFO:root:[   73] Training loss: 114.71302626, Validation loss: 160.47840774, Gradient norm: 164.21488365
INFO:root:[   74] Training loss: 114.63057831, Validation loss: 160.52894833, Gradient norm: 164.55848623
INFO:root:[   75] Training loss: 114.63920169, Validation loss: 160.69629522, Gradient norm: 168.83843221
INFO:root:[   76] Training loss: 114.72043427, Validation loss: 160.67020483, Gradient norm: 167.52515355
INFO:root:[   77] Training loss: 114.55862681, Validation loss: 160.28841226, Gradient norm: 157.18679303
INFO:root:[   78] Training loss: 114.54622599, Validation loss: 160.68811544, Gradient norm: 160.60581795
INFO:root:[   79] Training loss: 114.50486379, Validation loss: 160.80810092, Gradient norm: 161.38460040
INFO:root:[   80] Training loss: 114.47611576, Validation loss: 160.50623388, Gradient norm: 161.84267666
INFO:root:[   81] Training loss: 114.49477183, Validation loss: 160.77728352, Gradient norm: 154.10189195
INFO:root:[   82] Training loss: 114.44018565, Validation loss: 160.20146099, Gradient norm: 158.13775536
INFO:root:[   83] Training loss: 114.53576830, Validation loss: 160.19245188, Gradient norm: 159.25249728
INFO:root:[   84] Training loss: 114.37656609, Validation loss: 160.41457594, Gradient norm: 142.77711573
INFO:root:[   85] Training loss: 114.37621399, Validation loss: 160.51292794, Gradient norm: 147.88671382
INFO:root:[   86] Training loss: 114.39417959, Validation loss: 160.45329633, Gradient norm: 146.89465881
INFO:root:[   87] Training loss: 114.36862630, Validation loss: 160.33788768, Gradient norm: 148.59950315
INFO:root:[   88] Training loss: 114.34082455, Validation loss: 160.68483132, Gradient norm: 145.91203614
INFO:root:[   89] Training loss: 114.37950066, Validation loss: 160.53322133, Gradient norm: 146.09225465
INFO:root:[   90] Training loss: 114.28943648, Validation loss: 160.21972255, Gradient norm: 148.22068335
INFO:root:[   91] Training loss: 114.31908681, Validation loss: 160.86556097, Gradient norm: 148.20408584
INFO:root:[   92] Training loss: 114.36563385, Validation loss: 160.42719470, Gradient norm: 143.25826538
INFO:root:EP 92: Early stopping
INFO:root:Training the model took 2410.352s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 164.83137
INFO:root:EnergyScoreTrain: 116.0182
INFO:root:CoverageTrain: 0.58555
INFO:root:IntervalWidthTrain: 7.31151
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 165.0492
INFO:root:EnergyScoreValidation: 116.17341
INFO:root:CoverageValidation: 0.58576
INFO:root:IntervalWidthValidation: 7.31533
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 164.93695
INFO:root:EnergyScoreTest: 116.09127
INFO:root:CoverageTest: 0.5865
INFO:root:IntervalWidthTest: 7.32879
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.79963043, Validation loss: 134.71044172, Gradient norm: 69.60874669
INFO:root:[    2] Training loss: 121.59263502, Validation loss: 124.86598661, Gradient norm: 48.76215015
INFO:root:[    3] Training loss: 121.47388862, Validation loss: 143.74728046, Gradient norm: 39.89712840
INFO:root:[    4] Training loss: 121.42800188, Validation loss: 126.92456202, Gradient norm: 35.77778381
INFO:root:[    5] Training loss: 121.43087497, Validation loss: 123.79314356, Gradient norm: 33.83418862
INFO:root:[    6] Training loss: 121.38329091, Validation loss: 124.88142181, Gradient norm: 27.80717253
INFO:root:[    7] Training loss: 121.36651554, Validation loss: 122.72053073, Gradient norm: 24.69149413
INFO:root:[    8] Training loss: 121.37005975, Validation loss: 123.14469080, Gradient norm: 23.92939486
INFO:root:[    9] Training loss: 121.34411441, Validation loss: 122.60615606, Gradient norm: 20.47211126
INFO:root:[   10] Training loss: 121.33317729, Validation loss: 122.28610912, Gradient norm: 21.10568673
INFO:root:[   11] Training loss: 121.33998077, Validation loss: 121.85185804, Gradient norm: 20.63656047
INFO:root:[   12] Training loss: 121.32858202, Validation loss: 121.36243024, Gradient norm: 17.31118345
INFO:root:[   13] Training loss: 121.32033518, Validation loss: 121.55598490, Gradient norm: 16.40599360
INFO:root:[   14] Training loss: 121.32429097, Validation loss: 121.51689576, Gradient norm: 19.56166172
INFO:root:[   15] Training loss: 121.19065684, Validation loss: 120.86786812, Gradient norm: 19.75865224
INFO:root:[   16] Training loss: 119.95256605, Validation loss: 123.90410386, Gradient norm: 45.52958154
INFO:root:[   17] Training loss: 119.43931983, Validation loss: 132.47229191, Gradient norm: 56.37384580
INFO:root:[   18] Training loss: 119.25207720, Validation loss: 143.13206134, Gradient norm: 56.30142027
INFO:root:[   19] Training loss: 119.10081373, Validation loss: 127.39907516, Gradient norm: 66.20086627
INFO:root:[   20] Training loss: 118.79292247, Validation loss: 127.23630109, Gradient norm: 64.18428887
INFO:root:[   21] Training loss: 118.56962880, Validation loss: 131.46668176, Gradient norm: 74.15142781
INFO:root:[   22] Training loss: 118.38882236, Validation loss: 127.81091710, Gradient norm: 82.35656807
INFO:root:[   23] Training loss: 118.17845022, Validation loss: 143.44320518, Gradient norm: 95.14453215
INFO:root:[   24] Training loss: 118.09229967, Validation loss: 138.50334382, Gradient norm: 98.36429736
INFO:root:[   25] Training loss: 117.95339813, Validation loss: 121.47337141, Gradient norm: 91.99979704
INFO:root:[   26] Training loss: 117.75387695, Validation loss: 118.64286657, Gradient norm: 94.97226222
INFO:root:[   27] Training loss: 117.54701067, Validation loss: 118.19418241, Gradient norm: 104.61162007
INFO:root:[   28] Training loss: 117.38621026, Validation loss: 116.94987635, Gradient norm: 113.29435605
INFO:root:[   29] Training loss: 117.32104964, Validation loss: 116.55703347, Gradient norm: 127.85749127
INFO:root:[   30] Training loss: 117.18264503, Validation loss: 116.39340223, Gradient norm: 124.98619083
INFO:root:[   31] Training loss: 117.09007992, Validation loss: 116.29754853, Gradient norm: 134.33454591
INFO:root:[   32] Training loss: 116.98128645, Validation loss: 116.22344154, Gradient norm: 134.53375399
INFO:root:[   33] Training loss: 116.89119188, Validation loss: 115.93784747, Gradient norm: 126.07282042
INFO:root:[   34] Training loss: 116.82094442, Validation loss: 115.98594465, Gradient norm: 136.18614572
INFO:root:[   35] Training loss: 116.80681068, Validation loss: 115.91774937, Gradient norm: 144.69689124
INFO:root:[   36] Training loss: 116.67110959, Validation loss: 115.94702028, Gradient norm: 142.25179737
INFO:root:[   37] Training loss: 116.60410756, Validation loss: 116.10723783, Gradient norm: 137.77784270
INFO:root:[   38] Training loss: 116.58846364, Validation loss: 115.81953029, Gradient norm: 141.60991796
INFO:root:[   39] Training loss: 116.41939775, Validation loss: 116.73634526, Gradient norm: 151.89343714
INFO:root:[   40] Training loss: 116.34134671, Validation loss: 115.63327415, Gradient norm: 165.22788972
INFO:root:[   41] Training loss: 116.26327888, Validation loss: 116.42210027, Gradient norm: 161.90384615
INFO:root:[   42] Training loss: 116.18127001, Validation loss: 115.26944893, Gradient norm: 167.41219208
INFO:root:[   43] Training loss: 116.07689317, Validation loss: 114.94657697, Gradient norm: 162.02029575
INFO:root:[   44] Training loss: 115.98332272, Validation loss: 114.95592780, Gradient norm: 147.05955660
INFO:root:[   45] Training loss: 115.85421577, Validation loss: 115.12057602, Gradient norm: 160.66217357
INFO:root:[   46] Training loss: 115.83226173, Validation loss: 114.69193402, Gradient norm: 144.68054988
INFO:root:[   47] Training loss: 115.74906420, Validation loss: 114.83306323, Gradient norm: 151.98170562
INFO:root:[   48] Training loss: 115.69090240, Validation loss: 114.86022976, Gradient norm: 156.50629301
INFO:root:[   49] Training loss: 115.62889103, Validation loss: 114.55585466, Gradient norm: 164.51183615
INFO:root:[   50] Training loss: 115.57421254, Validation loss: 114.24941575, Gradient norm: 159.58476267
INFO:root:[   51] Training loss: 115.55112142, Validation loss: 114.38070103, Gradient norm: 157.49754378
INFO:root:[   52] Training loss: 115.53170675, Validation loss: 114.74229458, Gradient norm: 173.27243608
INFO:root:[   53] Training loss: 115.69989617, Validation loss: 114.50394266, Gradient norm: 191.99068313
INFO:root:[   54] Training loss: 115.47737244, Validation loss: 114.18698709, Gradient norm: 172.87405903
INFO:root:[   55] Training loss: 115.48711134, Validation loss: 114.39350249, Gradient norm: 172.18351660
INFO:root:[   56] Training loss: 115.41358334, Validation loss: 114.08226816, Gradient norm: 169.03106577
INFO:root:[   57] Training loss: 115.41858032, Validation loss: 114.28905326, Gradient norm: 174.53405848
INFO:root:[   58] Training loss: 115.33372104, Validation loss: 113.89566027, Gradient norm: 163.68939163
INFO:root:[   59] Training loss: 115.30485463, Validation loss: 114.19413476, Gradient norm: 172.51661259
INFO:root:[   60] Training loss: 115.31551799, Validation loss: 114.00277148, Gradient norm: 164.90424444
INFO:root:[   61] Training loss: 115.28632490, Validation loss: 114.40370031, Gradient norm: 160.89359608
INFO:root:[   62] Training loss: 115.25641551, Validation loss: 114.26731926, Gradient norm: 164.58176643
INFO:root:[   63] Training loss: 115.36875278, Validation loss: 114.14663522, Gradient norm: 177.86544845
INFO:root:[   64] Training loss: 115.27575426, Validation loss: 114.15411136, Gradient norm: 167.61198142
INFO:root:[   65] Training loss: 115.22777456, Validation loss: 114.00600822, Gradient norm: 165.92432655
INFO:root:[   66] Training loss: 115.61406630, Validation loss: 115.73433565, Gradient norm: 210.97978065
INFO:root:[   67] Training loss: 115.52475735, Validation loss: 115.12102964, Gradient norm: 207.05588694
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 1571.861s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 161.41862
INFO:root:EnergyScoreTrain: 113.75613
INFO:root:CoverageTrain: 0.71634
INFO:root:IntervalWidthTrain: 7.73664
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 161.63477
INFO:root:EnergyScoreValidation: 113.91233
INFO:root:CoverageValidation: 0.71744
INFO:root:IntervalWidthValidation: 7.74911
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 161.57988
INFO:root:EnergyScoreTest: 113.87134
INFO:root:CoverageTest: 0.71712
INFO:root:IntervalWidthTest: 7.74495
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.73257480, Validation loss: 121.34260733, Gradient norm: 40.35199584
INFO:root:[    2] Training loss: 121.39121470, Validation loss: 125.27691570, Gradient norm: 28.57285700
INFO:root:[    3] Training loss: 120.85057824, Validation loss: 167.17710073, Gradient norm: 66.06376564
INFO:root:[    4] Training loss: 120.54847846, Validation loss: 168.64054630, Gradient norm: 85.55505160
INFO:root:[    5] Training loss: 120.30463199, Validation loss: 168.82424311, Gradient norm: 117.21476704
INFO:root:[    6] Training loss: 120.19112528, Validation loss: 168.80742819, Gradient norm: 184.83409851
INFO:root:[    7] Training loss: 120.24048594, Validation loss: 168.69331628, Gradient norm: 353.55343934
INFO:root:[    8] Training loss: 119.88814104, Validation loss: 168.32005738, Gradient norm: 413.34060978
INFO:root:[    9] Training loss: 119.80910665, Validation loss: 169.22981423, Gradient norm: 592.45111446
INFO:root:[   10] Training loss: 119.82960344, Validation loss: 168.31394477, Gradient norm: 627.97427086
INFO:root:[   11] Training loss: 119.89207496, Validation loss: 168.17580026, Gradient norm: 595.64643107
INFO:root:[   12] Training loss: 119.71559445, Validation loss: 168.45537420, Gradient norm: 561.38519085
INFO:root:[   13] Training loss: 119.62354163, Validation loss: 167.95950478, Gradient norm: 528.10858689
INFO:root:[   14] Training loss: 119.72231940, Validation loss: 168.38742387, Gradient norm: 472.34514493
INFO:root:[   15] Training loss: 119.57888652, Validation loss: 168.27544577, Gradient norm: 496.32776215
INFO:root:[   16] Training loss: 120.23362054, Validation loss: 169.21813724, Gradient norm: 687.40167150
INFO:root:[   17] Training loss: 120.08836483, Validation loss: 168.58277090, Gradient norm: 548.10255201
INFO:root:[   18] Training loss: 119.74490946, Validation loss: 168.13041660, Gradient norm: 517.46684691
INFO:root:[   19] Training loss: 119.52951874, Validation loss: 168.31820250, Gradient norm: 510.73016197
INFO:root:[   20] Training loss: 119.48842862, Validation loss: 168.27290264, Gradient norm: 464.60639693
INFO:root:[   21] Training loss: 119.81609836, Validation loss: 168.12394714, Gradient norm: 629.58842796
INFO:root:[   22] Training loss: 119.60767273, Validation loss: 167.89855796, Gradient norm: 492.28076311
INFO:root:[   23] Training loss: 119.53641029, Validation loss: 167.94943211, Gradient norm: 561.07780046
INFO:root:[   24] Training loss: 119.82415927, Validation loss: 170.44554272, Gradient norm: 487.79168886
INFO:root:[   25] Training loss: 120.98746046, Validation loss: 170.22372276, Gradient norm: 526.03785826
INFO:root:[   26] Training loss: 120.98819838, Validation loss: 170.20856543, Gradient norm: 491.46881470
INFO:root:[   27] Training loss: 120.94907993, Validation loss: 170.47728234, Gradient norm: 473.76425172
INFO:root:[   28] Training loss: 121.22847422, Validation loss: 170.75368567, Gradient norm: 728.04383637
INFO:root:[   29] Training loss: 121.03236559, Validation loss: 170.30516721, Gradient norm: 495.66206205
INFO:root:[   30] Training loss: 120.89594242, Validation loss: 170.18198997, Gradient norm: 458.33267139
INFO:root:[   31] Training loss: 120.96338328, Validation loss: 170.25946206, Gradient norm: 349.08059661
INFO:root:[   32] Training loss: 121.01415724, Validation loss: 170.48755177, Gradient norm: 490.68236022
INFO:root:[   33] Training loss: 120.73388852, Validation loss: 170.11691739, Gradient norm: 375.64795197
INFO:root:[   34] Training loss: 120.59877279, Validation loss: 169.92165361, Gradient norm: 501.73588715
INFO:root:[   35] Training loss: 123.01107988, Validation loss: 171.46681561, Gradient norm: 1169.83755752
INFO:root:[   36] Training loss: 122.13250631, Validation loss: 171.50279905, Gradient norm: 1849.07624835
INFO:root:[   37] Training loss: 121.80673818, Validation loss: 171.49112072, Gradient norm: 1026.41549070
INFO:root:[   38] Training loss: 121.63531592, Validation loss: 171.45999520, Gradient norm: 748.52369569
INFO:root:[   39] Training loss: 121.65878045, Validation loss: 171.52462099, Gradient norm: 601.90186659
INFO:root:[   40] Training loss: 121.58601783, Validation loss: 171.53823638, Gradient norm: 600.03369117
INFO:root:[   41] Training loss: 121.65735297, Validation loss: 171.45765231, Gradient norm: 487.46025467
INFO:root:[   42] Training loss: 121.60813612, Validation loss: 171.51910159, Gradient norm: 480.00474200
INFO:root:[   43] Training loss: 121.56443549, Validation loss: 171.48266093, Gradient norm: 406.30253172
INFO:root:[   44] Training loss: 121.69534817, Validation loss: 171.48535477, Gradient norm: 462.99120871
INFO:root:[   45] Training loss: 121.66019175, Validation loss: 171.47120827, Gradient norm: 347.95278875
INFO:root:[   46] Training loss: 121.69831611, Validation loss: 171.50482258, Gradient norm: 349.46038942
INFO:root:[   47] Training loss: 121.66607208, Validation loss: 171.44257877, Gradient norm: 386.51965063
INFO:root:[   48] Training loss: 121.57592607, Validation loss: 171.44098837, Gradient norm: 314.83935763
INFO:root:[   49] Training loss: 121.55119405, Validation loss: 171.48385647, Gradient norm: 347.32126826
INFO:root:[   50] Training loss: 121.62945326, Validation loss: 171.49918968, Gradient norm: 337.90196712
INFO:root:[   51] Training loss: 121.56558818, Validation loss: 171.45712173, Gradient norm: 275.64019837
INFO:root:[   52] Training loss: 121.59802921, Validation loss: 171.52056778, Gradient norm: 287.32908532
INFO:root:[   53] Training loss: 121.60387238, Validation loss: 171.50811420, Gradient norm: 220.49311372
INFO:root:[   54] Training loss: 121.48415436, Validation loss: 171.51513806, Gradient norm: 227.40808778
INFO:root:[   55] Training loss: 121.55181769, Validation loss: 171.48795466, Gradient norm: 208.73653668
INFO:root:[   56] Training loss: 121.55622097, Validation loss: 171.53600753, Gradient norm: 201.06915482
INFO:root:[   57] Training loss: 121.62446442, Validation loss: 171.47440940, Gradient norm: 225.16059305
INFO:root:[   58] Training loss: 121.56370938, Validation loss: 171.47529682, Gradient norm: 189.19655216
INFO:root:[   59] Training loss: 121.52427456, Validation loss: 171.48472970, Gradient norm: 179.61951754
INFO:root:[   60] Training loss: 121.49316522, Validation loss: 171.46643977, Gradient norm: 164.35526015
INFO:root:[   61] Training loss: 121.48669529, Validation loss: 171.49931871, Gradient norm: 143.42093744
INFO:root:[   62] Training loss: 121.54226715, Validation loss: 171.49856567, Gradient norm: 133.84141883
INFO:root:[   63] Training loss: 121.55442169, Validation loss: 171.51938723, Gradient norm: 135.05849367
INFO:root:[   64] Training loss: 121.62921956, Validation loss: 171.46672620, Gradient norm: 139.17372152
INFO:root:[   65] Training loss: 121.51438616, Validation loss: 171.49013238, Gradient norm: 143.66899460
INFO:root:[   66] Training loss: 121.83454369, Validation loss: 171.48216274, Gradient norm: 387.24009664
INFO:root:[   67] Training loss: 121.60778307, Validation loss: 171.49402016, Gradient norm: 252.80356829
INFO:root:[   68] Training loss: 121.59653093, Validation loss: 171.51457188, Gradient norm: 208.41272211
INFO:root:[   69] Training loss: 121.56409610, Validation loss: 171.51272637, Gradient norm: 203.18710606
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 1736.511s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 172.23286
INFO:root:EnergyScoreTrain: 121.31641
INFO:root:CoverageTrain: 0.73348
INFO:root:IntervalWidthTrain: 8.2593
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 172.26635
INFO:root:EnergyScoreValidation: 121.34006
INFO:root:CoverageValidation: 0.73336
INFO:root:IntervalWidthValidation: 8.2598
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 172.27895
INFO:root:EnergyScoreTest: 121.34875
INFO:root:CoverageTest: 0.73301
INFO:root:IntervalWidthTest: 8.25996
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 367.20488939, Validation loss: 171.52219779, Gradient norm: 78162.66921987
INFO:root:[    2] Training loss: 171.46160034, Validation loss: 171.49231466, Gradient norm: 0.39982510
INFO:root:[    3] Training loss: 171.45573663, Validation loss: 171.46380000, Gradient norm: 0.11940640
INFO:root:[    4] Training loss: 171.45573629, Validation loss: 171.46340835, Gradient norm: 0.11606606
INFO:root:[    5] Training loss: 247.68180535, Validation loss: 171.48971558, Gradient norm: 2986.30939722
INFO:root:[    6] Training loss: 171.45574592, Validation loss: 171.47679567, Gradient norm: 0.12121075
INFO:root:[    7] Training loss: 171.45573798, Validation loss: 171.42937938, Gradient norm: 0.12234797
INFO:root:[    8] Training loss: 171.45575277, Validation loss: 171.49968037, Gradient norm: 0.10895031
INFO:root:[    9] Training loss: 171.45576463, Validation loss: 171.51145828, Gradient norm: 0.12778338
INFO:root:[   10] Training loss: 171.45575507, Validation loss: 171.48982694, Gradient norm: 0.13196817
INFO:root:[   11] Training loss: 171.45580858, Validation loss: 171.47801128, Gradient norm: 0.14732638
INFO:root:[   12] Training loss: 171.45576762, Validation loss: 171.51832634, Gradient norm: 0.12179151
INFO:root:[   13] Training loss: 171.45576341, Validation loss: 171.49582097, Gradient norm: 0.12342025
INFO:root:[   14] Training loss: 171.45578613, Validation loss: 171.49722504, Gradient norm: 0.13015084
INFO:root:[   15] Training loss: 171.45579129, Validation loss: 171.46230517, Gradient norm: 0.12387530
INFO:root:[   16] Training loss: 171.45580614, Validation loss: 171.49585523, Gradient norm: 0.12673959
INFO:root:[   17] Training loss: 171.45578478, Validation loss: 171.47852312, Gradient norm: 0.11628413
INFO:root:[   18] Training loss: 5955.53507189, Validation loss: 171.46724902, Gradient norm: 6658695.57287565
INFO:root:[   19] Training loss: 86385.95857639, Validation loss: 171.48069736, Gradient norm: 37626043.86072797
INFO:root:[   20] Training loss: 171.45579793, Validation loss: 171.49597837, Gradient norm: 0.12594346
INFO:root:[   21] Training loss: 171.45582072, Validation loss: 171.45229862, Gradient norm: 0.13356001
INFO:root:[   22] Training loss: 171.45582879, Validation loss: 171.46535693, Gradient norm: 0.13590771
INFO:root:[   23] Training loss: 171.45578959, Validation loss: 171.51057782, Gradient norm: 0.11908407
INFO:root:[   24] Training loss: 171.45576911, Validation loss: 171.49886442, Gradient norm: 0.12203302
INFO:root:[   25] Training loss: 171.45578064, Validation loss: 171.46264595, Gradient norm: 0.13262126
INFO:root:[   26] Training loss: 171.45580254, Validation loss: 171.46813456, Gradient norm: 0.13778255
INFO:root:[   27] Training loss: 171.45578837, Validation loss: 171.51145400, Gradient norm: 0.12704865
INFO:root:[   28] Training loss: 171.45577488, Validation loss: 171.49033717, Gradient norm: 0.13160786
INFO:root:[   29] Training loss: 171.45582052, Validation loss: 171.54157458, Gradient norm: 0.13268014
INFO:root:[   30] Training loss: 171.45580926, Validation loss: 171.50146645, Gradient norm: 0.13196764
INFO:root:[   31] Training loss: 171.45583299, Validation loss: 171.48650882, Gradient norm: 0.14000289
INFO:root:[   32] Training loss: 171.45578654, Validation loss: 171.53511288, Gradient norm: 0.11480372
INFO:root:[   33] Training loss: 171.45580254, Validation loss: 171.50424783, Gradient norm: 0.13319110
INFO:root:[   34] Training loss: 171.45583625, Validation loss: 171.50688600, Gradient norm: 0.13258083
INFO:root:[   35] Training loss: 171.45585022, Validation loss: 171.49655392, Gradient norm: 0.13751378
INFO:root:[   36] Training loss: 171.45580634, Validation loss: 171.49057489, Gradient norm: 0.13141603
INFO:root:[   37] Training loss: 171.45579820, Validation loss: 171.52936219, Gradient norm: 0.12241748
INFO:root:[   38] Training loss: 171.45578478, Validation loss: 171.46984355, Gradient norm: 0.11938673
INFO:root:[   39] Training loss: 171.45580092, Validation loss: 171.50486836, Gradient norm: 0.12997704
INFO:root:[   40] Training loss: 171.45579861, Validation loss: 171.50842258, Gradient norm: 0.12386137
INFO:root:[   41] Training loss: 171.45582709, Validation loss: 171.50160646, Gradient norm: 0.13508221
INFO:root:[   42] Training loss: 171.45575433, Validation loss: 171.48095998, Gradient norm: 0.13368629
INFO:root:[   43] Training loss: 171.45582404, Validation loss: 171.52494866, Gradient norm: 0.13196157
INFO:root:[   44] Training loss: 171.45579149, Validation loss: 171.48476530, Gradient norm: 0.12285812
INFO:root:[   45] Training loss: 171.45580512, Validation loss: 171.52195419, Gradient norm: 0.13172318
INFO:root:[   46] Training loss: 171.45579074, Validation loss: 171.51412241, Gradient norm: 0.13324163
INFO:root:[   47] Training loss: 171.45577053, Validation loss: 171.51527726, Gradient norm: 0.11942042
INFO:root:[   48] Training loss: 171.45579522, Validation loss: 171.46925461, Gradient norm: 0.12433204
INFO:root:[   49] Training loss: 171.45580288, Validation loss: 171.47621556, Gradient norm: 0.11969696
INFO:root:[   50] Training loss: 171.45581373, Validation loss: 171.45047372, Gradient norm: 0.13522720
INFO:root:[   51] Training loss: 171.45579569, Validation loss: 171.45561165, Gradient norm: 0.13107577
INFO:root:[   52] Training loss: 171.45581312, Validation loss: 171.50394694, Gradient norm: 0.12256802
INFO:root:[   53] Training loss: 171.45580336, Validation loss: 171.50635194, Gradient norm: 0.12479493
INFO:root:[   54] Training loss: 171.45585449, Validation loss: 171.45661096, Gradient norm: 0.13719953
INFO:root:[   55] Training loss: 736073.71372226, Validation loss: 171.44761042, Gradient norm: 88743397.29222845
INFO:root:[   56] Training loss: 171.45578654, Validation loss: 171.47443430, Gradient norm: 0.12214984
INFO:root:[   57] Training loss: 171.45579997, Validation loss: 171.49117801, Gradient norm: 0.13857767
INFO:root:[   58] Training loss: 171.45580688, Validation loss: 171.42289011, Gradient norm: 0.13717418
INFO:root:[   59] Training loss: 171.45578369, Validation loss: 171.47753157, Gradient norm: 0.11811149
INFO:root:[   60] Training loss: 171.45581509, Validation loss: 171.44437690, Gradient norm: 0.13410889
INFO:root:[   61] Training loss: 3196.87502563, Validation loss: 171.45570507, Gradient norm: 577111.50857348
INFO:root:[   62] Training loss: 171.45582635, Validation loss: 171.43140425, Gradient norm: 0.13274134
INFO:root:[   63] Training loss: 171.45578620, Validation loss: 171.50222216, Gradient norm: 0.12428915
INFO:root:[   64] Training loss: 171.45580234, Validation loss: 171.48065774, Gradient norm: 0.12194340
INFO:root:[   65] Training loss: 171.45585246, Validation loss: 171.50812035, Gradient norm: 0.14029293
INFO:root:[   66] Training loss: 171.45581679, Validation loss: 171.49888290, Gradient norm: 0.11769221
INFO:root:[   67] Training loss: 171.45580587, Validation loss: 171.46811127, Gradient norm: 0.12629020
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 1720.393s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.4558
INFO:root:EnergyScoreTrain: 171.45575
INFO:root:CoverageTrain: 0.0
INFO:root:IntervalWidthTrain: 0.0
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.48713
INFO:root:EnergyScoreValidation: 171.48708
INFO:root:CoverageValidation: 0.0
INFO:root:IntervalWidthValidation: 0.0
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.52222
INFO:root:EnergyScoreTest: 171.52217
INFO:root:CoverageTest: 0.0
INFO:root:IntervalWidthTest: 0.0
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 157286400
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 125.16126099, Validation loss: 170.25401146, Gradient norm: 77.10201993
INFO:root:[    2] Training loss: 121.64897976, Validation loss: 139.76596685, Gradient norm: 46.75192181
INFO:root:[    3] Training loss: 121.54141883, Validation loss: 134.44831286, Gradient norm: 45.07927269
INFO:root:[    4] Training loss: 121.46643704, Validation loss: 152.99219500, Gradient norm: 41.62295515
INFO:root:[    5] Training loss: 121.47796505, Validation loss: 155.00270910, Gradient norm: 48.44114254
INFO:root:[    6] Training loss: 121.43135132, Validation loss: 167.89011932, Gradient norm: 37.37065470
INFO:root:[    7] Training loss: 121.45326233, Validation loss: 156.60506238, Gradient norm: 48.63217568
INFO:root:[    8] Training loss: 121.42026147, Validation loss: 164.25694891, Gradient norm: 37.89674222
INFO:root:[    9] Training loss: 121.35083954, Validation loss: 168.15951056, Gradient norm: 37.27766662
INFO:root:[   10] Training loss: 121.38659502, Validation loss: 171.31976399, Gradient norm: 41.78521292
INFO:root:[   11] Training loss: 121.37919508, Validation loss: 171.39177583, Gradient norm: 46.53721786
INFO:root:[   12] Training loss: 121.38112583, Validation loss: 171.52053779, Gradient norm: 43.36811734
INFO:root:[   13] Training loss: 121.34990967, Validation loss: 171.51573128, Gradient norm: 44.23894247
INFO:root:[   14] Training loss: 121.40654924, Validation loss: 171.46699390, Gradient norm: 52.97596634
INFO:root:[   15] Training loss: 121.41820279, Validation loss: 171.50514007, Gradient norm: 55.13215514
INFO:root:[   16] Training loss: 121.41000288, Validation loss: 171.47454834, Gradient norm: 67.75023128
INFO:root:[   17] Training loss: 121.45107001, Validation loss: 171.48632491, Gradient norm: 54.56019897
INFO:root:[   18] Training loss: 121.38007473, Validation loss: 171.49787019, Gradient norm: 57.31028619
INFO:root:[   19] Training loss: 121.35495819, Validation loss: 171.46005651, Gradient norm: 58.67044666
INFO:root:[   20] Training loss: 121.38750848, Validation loss: 171.52724042, Gradient norm: 67.28698053
INFO:root:[   21] Training loss: 121.35454227, Validation loss: 171.47486208, Gradient norm: 62.10258664
INFO:root:[   22] Training loss: 121.41239217, Validation loss: 171.46904768, Gradient norm: 70.62707513
INFO:root:[   23] Training loss: 121.39696184, Validation loss: 171.47905771, Gradient norm: 69.65851776
INFO:root:[   24] Training loss: 121.47179331, Validation loss: 171.49319538, Gradient norm: 117.73779199
INFO:root:[   25] Training loss: 121.36878448, Validation loss: 171.43015035, Gradient norm: 97.37613667
INFO:root:[   26] Training loss: 121.39600837, Validation loss: 171.49632879, Gradient norm: 114.23392439
INFO:root:[   27] Training loss: 121.40142415, Validation loss: 171.49383117, Gradient norm: 110.79121029
INFO:root:[   28] Training loss: 121.38732771, Validation loss: 171.49495336, Gradient norm: 110.75107042
INFO:root:[   29] Training loss: 121.43576765, Validation loss: 171.50962026, Gradient norm: 96.29790108
INFO:root:[   30] Training loss: 121.32299778, Validation loss: 171.50816238, Gradient norm: 82.21065082
INFO:root:[   31] Training loss: 121.39486742, Validation loss: 171.49476008, Gradient norm: 117.19274814
INFO:root:[   32] Training loss: 121.35501899, Validation loss: 171.49177578, Gradient norm: 105.46850973
INFO:root:[   33] Training loss: 121.61361413, Validation loss: 171.45301150, Gradient norm: 210.58888757
INFO:root:[   34] Training loss: 121.51931617, Validation loss: 171.45281527, Gradient norm: 291.85263016
INFO:root:[   35] Training loss: 121.37094262, Validation loss: 171.47841310, Gradient norm: 190.34567251
INFO:root:[   36] Training loss: 121.57770247, Validation loss: 171.51913800, Gradient norm: 387.48371473
INFO:root:[   37] Training loss: 121.47217723, Validation loss: 171.49619146, Gradient norm: 292.45525287
INFO:root:[   38] Training loss: 121.54863678, Validation loss: 171.45610662, Gradient norm: 392.91181643
INFO:root:[   39] Training loss: 121.61513268, Validation loss: 171.46126543, Gradient norm: 402.14051251
INFO:root:[   40] Training loss: 121.76677138, Validation loss: 171.48863006, Gradient norm: 664.39192436
INFO:root:[   41] Training loss: 121.68578623, Validation loss: 171.45185290, Gradient norm: 707.26698818
INFO:root:[   42] Training loss: 121.67313015, Validation loss: 171.48541795, Gradient norm: 664.84094508
INFO:root:[   43] Training loss: 121.44613858, Validation loss: 171.45845594, Gradient norm: 461.62450928
INFO:root:[   44] Training loss: 121.49978163, Validation loss: 171.49857424, Gradient norm: 515.08502481
INFO:root:[   45] Training loss: 121.40209666, Validation loss: 171.47379049, Gradient norm: 357.68381623
INFO:root:[   46] Training loss: 121.40651279, Validation loss: 171.45266188, Gradient norm: 339.83661954
INFO:root:[   47] Training loss: 121.42784529, Validation loss: 171.46218310, Gradient norm: 321.75403270
INFO:root:[   48] Training loss: 121.37641117, Validation loss: 171.50892987, Gradient norm: 308.75540621
INFO:root:[   49] Training loss: 121.41641127, Validation loss: 171.50195232, Gradient norm: 240.02201294
INFO:root:[   50] Training loss: 121.39802060, Validation loss: 171.48480198, Gradient norm: 261.51184891
INFO:root:[   51] Training loss: 122.16943892, Validation loss: 171.50405964, Gradient norm: 1176.24287286
INFO:root:[   52] Training loss: 122.68646742, Validation loss: 171.50143004, Gradient norm: 1721.42855004
INFO:root:[   53] Training loss: 122.37035078, Validation loss: 171.54144608, Gradient norm: 903.29850933
INFO:root:[   54] Training loss: 122.30863736, Validation loss: 171.52205591, Gradient norm: 1329.61356953
INFO:root:[   55] Training loss: 122.17994320, Validation loss: 171.50278781, Gradient norm: 1989.85745205
INFO:root:[   56] Training loss: 121.90970534, Validation loss: 171.45558167, Gradient norm: 1744.71020584
INFO:root:[   57] Training loss: 121.73680332, Validation loss: 171.48701772, Gradient norm: 1072.58676231
INFO:root:[   58] Training loss: 121.72403904, Validation loss: 171.44873368, Gradient norm: 977.90754492
INFO:root:[   59] Training loss: 121.68269524, Validation loss: 171.49226995, Gradient norm: 929.88879607
INFO:root:[   60] Training loss: 121.59637482, Validation loss: 171.46152590, Gradient norm: 691.41272167
INFO:root:[   61] Training loss: 121.59854926, Validation loss: 171.48671441, Gradient norm: 632.49350454
INFO:root:[   62] Training loss: 122.52024977, Validation loss: 171.48227330, Gradient norm: 1144.77619329
INFO:root:[   63] Training loss: 122.44160899, Validation loss: 171.49159535, Gradient norm: 1947.53578227
INFO:root:[   64] Training loss: 121.81190277, Validation loss: 171.48740240, Gradient norm: 1346.29318046
INFO:root:[   65] Training loss: 121.75542816, Validation loss: 171.50764626, Gradient norm: 1095.06104883
INFO:root:[   66] Training loss: 121.64511617, Validation loss: 171.48613485, Gradient norm: 1060.30558500
INFO:root:[   67] Training loss: 121.73133623, Validation loss: 171.49442117, Gradient norm: 891.81296912
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 1746.513s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.61854
INFO:root:EnergyScoreTrain: 134.26618
INFO:root:CoverageTrain: 0.301
INFO:root:IntervalWidthTrain: 2.79853
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.64659
INFO:root:EnergyScoreValidation: 134.31786
INFO:root:CoverageValidation: 0.30065
INFO:root:IntervalWidthValidation: 2.7959
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.68476
INFO:root:EnergyScoreTest: 134.37136
INFO:root:CoverageTest: 0.30041
INFO:root:IntervalWidthTest: 2.79365
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 175.43168796, Validation loss: 170.25629546, Gradient norm: 112.33212610
INFO:root:[    2] Training loss: 169.73348477, Validation loss: 168.42802349, Gradient norm: 41.70166223
INFO:root:[    3] Training loss: 168.31210110, Validation loss: 167.39860053, Gradient norm: 48.05070990
INFO:root:[    4] Training loss: 167.40985528, Validation loss: 166.86637075, Gradient norm: 38.06362976
INFO:root:[    5] Training loss: 166.92534865, Validation loss: 166.45499755, Gradient norm: 36.47458901
INFO:root:[    6] Training loss: 166.65319804, Validation loss: 166.31134328, Gradient norm: 33.45853957
INFO:root:[    7] Training loss: 166.36645860, Validation loss: 165.91713969, Gradient norm: 28.63038833
INFO:root:[    8] Training loss: 166.14095669, Validation loss: 165.80533051, Gradient norm: 36.39190222
INFO:root:[    9] Training loss: 165.97195794, Validation loss: 165.71830402, Gradient norm: 49.21879289
INFO:root:[   10] Training loss: 165.81375773, Validation loss: 165.42745597, Gradient norm: 71.00932122
INFO:root:[   11] Training loss: 165.64369181, Validation loss: 165.39804505, Gradient norm: 99.64020900
INFO:root:[   12] Training loss: 165.38989604, Validation loss: 164.98987834, Gradient norm: 136.86831608
INFO:root:[   13] Training loss: 165.10969659, Validation loss: 164.90950708, Gradient norm: 182.39618199
INFO:root:[   14] Training loss: 164.91954508, Validation loss: 164.51084927, Gradient norm: 229.61878348
INFO:root:[   15] Training loss: 164.69495131, Validation loss: 164.26323339, Gradient norm: 248.92547397
INFO:root:[   16] Training loss: 164.43054742, Validation loss: 164.43697130, Gradient norm: 272.40316728
INFO:root:[   17] Training loss: 164.19103421, Validation loss: 163.74818608, Gradient norm: 270.86486412
INFO:root:[   18] Training loss: 163.97086100, Validation loss: 163.78510244, Gradient norm: 288.23009159
INFO:root:[   19] Training loss: 163.75855252, Validation loss: 163.50351407, Gradient norm: 283.93446792
INFO:root:[   20] Training loss: 163.58617574, Validation loss: 163.19312728, Gradient norm: 283.48166329
INFO:root:[   21] Training loss: 163.38639181, Validation loss: 163.17088666, Gradient norm: 271.98521582
INFO:root:[   22] Training loss: 163.16807604, Validation loss: 162.65660390, Gradient norm: 279.19886898
INFO:root:[   23] Training loss: 162.93123149, Validation loss: 162.53565283, Gradient norm: 276.08634526
INFO:root:[   24] Training loss: 162.74855055, Validation loss: 162.31826782, Gradient norm: 277.46902582
INFO:root:[   25] Training loss: 162.46844489, Validation loss: 162.08588503, Gradient norm: 273.03104849
INFO:root:[   26] Training loss: 162.28192586, Validation loss: 161.69638543, Gradient norm: 252.46875770
INFO:root:[   27] Training loss: 162.09056769, Validation loss: 161.60285147, Gradient norm: 263.05113405
INFO:root:[   28] Training loss: 161.85093825, Validation loss: 161.48891784, Gradient norm: 239.01125211
INFO:root:[   29] Training loss: 161.70416640, Validation loss: 161.06460893, Gradient norm: 247.43454411
INFO:root:[   30] Training loss: 161.53794556, Validation loss: 161.09070226, Gradient norm: 251.87158748
INFO:root:[   31] Training loss: 161.33479072, Validation loss: 160.75855028, Gradient norm: 237.57909053
INFO:root:[   32] Training loss: 161.23351861, Validation loss: 160.74874476, Gradient norm: 239.96271247
INFO:root:[   33] Training loss: 161.11000576, Validation loss: 160.52691624, Gradient norm: 240.33818726
INFO:root:[   34] Training loss: 160.98072815, Validation loss: 160.34881003, Gradient norm: 221.61066520
INFO:root:[   35] Training loss: 160.86752869, Validation loss: 160.25963124, Gradient norm: 217.80643306
INFO:root:[   36] Training loss: 160.79002916, Validation loss: 160.20458395, Gradient norm: 206.42472492
INFO:root:[   37] Training loss: 160.68872030, Validation loss: 160.30742766, Gradient norm: 214.45941770
INFO:root:[   38] Training loss: 160.60886481, Validation loss: 159.94057692, Gradient norm: 206.12574535
INFO:root:[   39] Training loss: 160.51351705, Validation loss: 160.06079958, Gradient norm: 204.64414742
INFO:root:[   40] Training loss: 160.41859063, Validation loss: 159.72737978, Gradient norm: 177.58247922
INFO:root:[   41] Training loss: 160.34988661, Validation loss: 159.74493970, Gradient norm: 192.66824888
INFO:root:[   42] Training loss: 160.27396661, Validation loss: 159.72023224, Gradient norm: 190.47855400
INFO:root:[   43] Training loss: 160.18309543, Validation loss: 159.49560895, Gradient norm: 187.91818593
INFO:root:[   44] Training loss: 160.08880432, Validation loss: 159.82503924, Gradient norm: 182.38784015
INFO:root:[   45] Training loss: 160.03345493, Validation loss: 159.86672492, Gradient norm: 187.47906835
INFO:root:[   46] Training loss: 159.94140605, Validation loss: 159.35608820, Gradient norm: 178.92317786
INFO:root:[   47] Training loss: 159.89315294, Validation loss: 159.30444041, Gradient norm: 184.93712192
INFO:root:[   48] Training loss: 159.79978468, Validation loss: 159.60430185, Gradient norm: 178.39401257
INFO:root:[   49] Training loss: 159.74472588, Validation loss: 159.28750182, Gradient norm: 183.63638764
INFO:root:[   50] Training loss: 159.68642436, Validation loss: 159.21221710, Gradient norm: 185.37186666
INFO:root:[   51] Training loss: 159.62059394, Validation loss: 159.03566247, Gradient norm: 182.07195833
INFO:root:[   52] Training loss: 159.56644287, Validation loss: 158.96007498, Gradient norm: 182.37318307
INFO:root:[   53] Training loss: 159.51213860, Validation loss: 159.14293282, Gradient norm: 179.08837941
INFO:root:[   54] Training loss: 159.43771227, Validation loss: 158.84156639, Gradient norm: 186.40209969
INFO:root:[   55] Training loss: 159.36288161, Validation loss: 158.96138937, Gradient norm: 174.05526979
INFO:root:[   56] Training loss: 159.33498834, Validation loss: 158.93198194, Gradient norm: 174.13465159
INFO:root:[   57] Training loss: 159.29134325, Validation loss: 158.92624597, Gradient norm: 172.43375961
INFO:root:[   58] Training loss: 159.26917874, Validation loss: 158.72383546, Gradient norm: 179.13719162
INFO:root:[   59] Training loss: 159.21449585, Validation loss: 158.62530223, Gradient norm: 175.79065563
INFO:root:[   60] Training loss: 159.14391880, Validation loss: 158.77705303, Gradient norm: 171.36284640
INFO:root:[   61] Training loss: 159.14137363, Validation loss: 158.73030251, Gradient norm: 171.95214446
INFO:root:[   62] Training loss: 159.11237881, Validation loss: 158.80153375, Gradient norm: 175.71651389
INFO:root:[   63] Training loss: 159.03899611, Validation loss: 158.51009971, Gradient norm: 173.46697827
INFO:root:[   64] Training loss: 158.98579807, Validation loss: 158.50609548, Gradient norm: 167.84770983
INFO:root:[   65] Training loss: 158.97713637, Validation loss: 158.44910980, Gradient norm: 170.90477860
INFO:root:[   66] Training loss: 158.92024577, Validation loss: 158.43969593, Gradient norm: 171.36482708
INFO:root:[   67] Training loss: 158.90696967, Validation loss: 158.47906173, Gradient norm: 172.48666594
INFO:root:[   68] Training loss: 158.87966526, Validation loss: 158.28204372, Gradient norm: 171.60818673
INFO:root:[   69] Training loss: 158.84491543, Validation loss: 158.31687740, Gradient norm: 172.92590990
INFO:root:[   70] Training loss: 158.78458557, Validation loss: 158.19757241, Gradient norm: 169.95224135
INFO:root:[   71] Training loss: 158.78998366, Validation loss: 158.42621251, Gradient norm: 168.01211789
INFO:root:[   72] Training loss: 158.74413886, Validation loss: 158.20169201, Gradient norm: 164.53685538
INFO:root:[   73] Training loss: 158.73378493, Validation loss: 158.21785910, Gradient norm: 166.32322631
INFO:root:[   74] Training loss: 158.64820509, Validation loss: 158.11343116, Gradient norm: 158.54811904
INFO:root:[   75] Training loss: 158.63244819, Validation loss: 157.99795318, Gradient norm: 166.50854621
INFO:root:[   76] Training loss: 158.57185621, Validation loss: 158.04102955, Gradient norm: 167.77699416
INFO:root:[   77] Training loss: 158.53730313, Validation loss: 157.96944923, Gradient norm: 170.66098417
INFO:root:[   78] Training loss: 158.51000448, Validation loss: 157.95209249, Gradient norm: 168.41298018
INFO:root:[   79] Training loss: 158.47904331, Validation loss: 157.83800145, Gradient norm: 167.08999991
INFO:root:[   80] Training loss: 158.45958849, Validation loss: 157.93470577, Gradient norm: 171.78524384
INFO:root:[   81] Training loss: 158.40811578, Validation loss: 158.06362058, Gradient norm: 160.64336353
INFO:root:[   82] Training loss: 158.38892395, Validation loss: 157.78834079, Gradient norm: 172.75905424
INFO:root:[   83] Training loss: 158.36187981, Validation loss: 158.00488281, Gradient norm: 164.94785095
INFO:root:[   84] Training loss: 158.33729404, Validation loss: 157.91471675, Gradient norm: 166.85426096
INFO:root:[   85] Training loss: 158.32179233, Validation loss: 157.89554235, Gradient norm: 169.51076169
INFO:root:[   86] Training loss: 158.29349684, Validation loss: 157.75335774, Gradient norm: 164.24184562
INFO:root:[   87] Training loss: 158.24894436, Validation loss: 157.86020406, Gradient norm: 164.97496094
INFO:root:[   88] Training loss: 158.24616523, Validation loss: 157.61530558, Gradient norm: 168.38640227
INFO:root:[   89] Training loss: 158.21134081, Validation loss: 157.63475251, Gradient norm: 166.93730254
INFO:root:[   90] Training loss: 158.20239380, Validation loss: 157.66971762, Gradient norm: 165.46546830
INFO:root:[   91] Training loss: 158.20472290, Validation loss: 157.81960230, Gradient norm: 169.85604413
INFO:root:[   92] Training loss: 158.14429593, Validation loss: 157.75584545, Gradient norm: 159.08230921
INFO:root:[   93] Training loss: 158.13096490, Validation loss: 157.65601683, Gradient norm: 159.77755101
INFO:root:[   94] Training loss: 158.13447191, Validation loss: 157.77253027, Gradient norm: 172.00763435
INFO:root:[   95] Training loss: 158.11241747, Validation loss: 157.71610728, Gradient norm: 166.69492626
INFO:root:[   96] Training loss: 158.08112278, Validation loss: 157.65021073, Gradient norm: 163.21520671
INFO:root:[   97] Training loss: 158.05750454, Validation loss: 157.70368074, Gradient norm: 158.99459737
INFO:root:[   98] Training loss: 158.04982524, Validation loss: 157.51241262, Gradient norm: 157.67793703
INFO:root:[   99] Training loss: 158.04085999, Validation loss: 157.44190497, Gradient norm: 162.29936636
INFO:root:[  100] Training loss: 157.99178352, Validation loss: 157.47127707, Gradient norm: 160.25526092
INFO:root:[  101] Training loss: 158.00749159, Validation loss: 157.59108132, Gradient norm: 165.56374411
INFO:root:[  102] Training loss: 157.94958774, Validation loss: 157.71001689, Gradient norm: 156.85645817
INFO:root:[  103] Training loss: 157.97103726, Validation loss: 157.54500406, Gradient norm: 165.96651537
INFO:root:[  104] Training loss: 157.94304308, Validation loss: 157.39486507, Gradient norm: 157.49101375
INFO:root:[  105] Training loss: 157.88032050, Validation loss: 157.60292562, Gradient norm: 160.97625365
INFO:root:[  106] Training loss: 157.86023627, Validation loss: 157.58512664, Gradient norm: 158.43916235
INFO:root:[  107] Training loss: 157.82038296, Validation loss: 157.47650682, Gradient norm: 160.72214377
INFO:root:[  108] Training loss: 157.80917006, Validation loss: 157.51403514, Gradient norm: 156.19869902
INFO:root:[  109] Training loss: 157.79006042, Validation loss: 157.65346246, Gradient norm: 158.08884665
INFO:root:[  110] Training loss: 157.78044562, Validation loss: 157.27539277, Gradient norm: 160.07592370
INFO:root:[  111] Training loss: 157.76054979, Validation loss: 157.74285112, Gradient norm: 157.56386277
INFO:root:[  112] Training loss: 157.76848612, Validation loss: 157.31124771, Gradient norm: 161.38811893
INFO:root:[  113] Training loss: 157.73777262, Validation loss: 157.65063102, Gradient norm: 156.33766857
INFO:root:[  114] Training loss: 157.71484178, Validation loss: 157.32888553, Gradient norm: 156.76568411
INFO:root:[  115] Training loss: 157.69620741, Validation loss: 157.45214871, Gradient norm: 160.06569019
INFO:root:[  116] Training loss: 157.70122782, Validation loss: 157.26654347, Gradient norm: 161.88998171
INFO:root:[  117] Training loss: 157.65173910, Validation loss: 157.39054094, Gradient norm: 158.12185381
INFO:root:[  118] Training loss: 157.63669881, Validation loss: 157.15113723, Gradient norm: 156.59337970
INFO:root:[  119] Training loss: 157.62891161, Validation loss: 157.16550164, Gradient norm: 157.52927565
INFO:root:[  120] Training loss: 157.63221537, Validation loss: 157.13599436, Gradient norm: 161.18180106
INFO:root:[  121] Training loss: 157.62068509, Validation loss: 157.20192598, Gradient norm: 163.16537438
INFO:root:[  122] Training loss: 157.56859639, Validation loss: 157.36601016, Gradient norm: 151.74684641
INFO:root:[  123] Training loss: 157.57280830, Validation loss: 157.29539490, Gradient norm: 162.21680174
INFO:root:[  124] Training loss: 157.52593641, Validation loss: 157.26192970, Gradient norm: 153.51835436
INFO:root:[  125] Training loss: 157.53173977, Validation loss: 157.29934826, Gradient norm: 150.14045188
INFO:root:[  126] Training loss: 157.53807115, Validation loss: 157.46115434, Gradient norm: 156.36985435
INFO:root:[  127] Training loss: 157.54563449, Validation loss: 157.45087982, Gradient norm: 160.88004869
INFO:root:[  128] Training loss: 157.49528761, Validation loss: 157.23376866, Gradient norm: 153.87718307
INFO:root:[  129] Training loss: 157.49191467, Validation loss: 157.08011721, Gradient norm: 162.70526317
INFO:root:[  130] Training loss: 157.46052612, Validation loss: 157.26964127, Gradient norm: 157.45314511
INFO:root:[  131] Training loss: 157.45247043, Validation loss: 157.07895219, Gradient norm: 159.85630167
INFO:root:[  132] Training loss: 157.45688721, Validation loss: 157.07905632, Gradient norm: 158.30135029
INFO:root:[  133] Training loss: 157.43138828, Validation loss: 157.04499255, Gradient norm: 158.53735458
INFO:root:[  134] Training loss: 157.45933112, Validation loss: 157.06475402, Gradient norm: 163.79656941
INFO:root:[  135] Training loss: 157.43808295, Validation loss: 157.23247675, Gradient norm: 161.61639739
INFO:root:[  136] Training loss: 157.37958150, Validation loss: 157.12210859, Gradient norm: 155.90484855
INFO:root:[  137] Training loss: 157.39244141, Validation loss: 157.08507150, Gradient norm: 156.73598295
INFO:root:[  138] Training loss: 157.40437981, Validation loss: 156.98949928, Gradient norm: 160.32802819
INFO:root:[  139] Training loss: 157.37345846, Validation loss: 157.01469823, Gradient norm: 159.10462113
INFO:root:[  140] Training loss: 157.33072842, Validation loss: 157.07270706, Gradient norm: 159.31316138
INFO:root:[  141] Training loss: 157.37514262, Validation loss: 156.92748086, Gradient norm: 163.72372809
INFO:root:[  142] Training loss: 157.32430542, Validation loss: 156.90194515, Gradient norm: 153.93218270
INFO:root:[  143] Training loss: 157.31309930, Validation loss: 156.91700530, Gradient norm: 165.18259539
INFO:root:[  144] Training loss: 157.27879652, Validation loss: 156.85643648, Gradient norm: 155.03564588
INFO:root:[  145] Training loss: 157.28404134, Validation loss: 157.31011079, Gradient norm: 157.56974635
INFO:root:[  146] Training loss: 157.27713270, Validation loss: 157.17603476, Gradient norm: 160.87675552
INFO:root:[  147] Training loss: 157.25769260, Validation loss: 157.06131544, Gradient norm: 160.29854112
INFO:root:[  148] Training loss: 157.23579054, Validation loss: 156.92192613, Gradient norm: 155.95344388
INFO:root:[  149] Training loss: 157.26957031, Validation loss: 156.92394083, Gradient norm: 164.67485010
INFO:root:[  150] Training loss: 157.23909064, Validation loss: 156.96604759, Gradient norm: 156.85778327
INFO:root:[  151] Training loss: 157.25707743, Validation loss: 156.77617337, Gradient norm: 159.79995261
INFO:root:[  152] Training loss: 157.24444411, Validation loss: 156.78991271, Gradient norm: 163.70975297
INFO:root:[  153] Training loss: 157.22474304, Validation loss: 157.04979318, Gradient norm: 160.35081564
INFO:root:[  154] Training loss: 157.19827596, Validation loss: 156.93210428, Gradient norm: 161.83014697
INFO:root:[  155] Training loss: 157.19918498, Validation loss: 156.85275295, Gradient norm: 159.75800278
INFO:root:[  156] Training loss: 157.17077779, Validation loss: 156.92892563, Gradient norm: 153.40367369
INFO:root:[  157] Training loss: 157.17237684, Validation loss: 156.99627900, Gradient norm: 162.62225879
INFO:root:[  158] Training loss: 157.19649455, Validation loss: 156.96413328, Gradient norm: 164.47473647
INFO:root:[  159] Training loss: 157.14662123, Validation loss: 156.78609667, Gradient norm: 157.64979280
INFO:root:[  160] Training loss: 157.14051561, Validation loss: 157.01621875, Gradient norm: 158.97402495
INFO:root:EP 160: Early stopping
INFO:root:Training the model took 4069.748s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 155.97179
INFO:root:EnergyScoreTrain: 155.82095
INFO:root:CoverageTrain: 0.00088
INFO:root:IntervalWidthTrain: 0.00385
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 156.74978
INFO:root:EnergyScoreValidation: 156.60036
INFO:root:CoverageValidation: 0.00085
INFO:root:IntervalWidthValidation: 0.00382
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 156.60301
INFO:root:EnergyScoreTest: 156.4546
INFO:root:CoverageTest: 0.00084
INFO:root:IntervalWidthTest: 0.00375
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 175.03976393, Validation loss: 170.05290249, Gradient norm: 60.53171202
INFO:root:[    2] Training loss: 169.63366509, Validation loss: 168.07396658, Gradient norm: 23.44550219
INFO:root:[    3] Training loss: 168.24966315, Validation loss: 167.10119254, Gradient norm: 40.69165403
INFO:root:[    4] Training loss: 167.60218967, Validation loss: 166.58323241, Gradient norm: 46.90765148
INFO:root:[    5] Training loss: 167.18980414, Validation loss: 166.42489785, Gradient norm: 50.89337934
INFO:root:[    6] Training loss: 166.92710626, Validation loss: 166.09319346, Gradient norm: 57.96699206
INFO:root:[    7] Training loss: 166.78563721, Validation loss: 166.17358345, Gradient norm: 121.40024448
INFO:root:[    8] Training loss: 166.64066840, Validation loss: 166.54039028, Gradient norm: 245.73465847
INFO:root:[    9] Training loss: 166.57636509, Validation loss: 165.84700815, Gradient norm: 316.72592924
INFO:root:[   10] Training loss: 166.48160956, Validation loss: 165.80941612, Gradient norm: 346.47372347
INFO:root:[   11] Training loss: 166.34847819, Validation loss: 165.45916962, Gradient norm: 378.28498144
INFO:root:[   12] Training loss: 166.22027310, Validation loss: 165.79767408, Gradient norm: 403.62161621
INFO:root:[   13] Training loss: 166.14344849, Validation loss: 165.28475096, Gradient norm: 419.78908721
INFO:root:[   14] Training loss: 166.02480625, Validation loss: 165.31872907, Gradient norm: 407.11519992
INFO:root:[   15] Training loss: 165.94829258, Validation loss: 165.22323662, Gradient norm: 379.65214634
INFO:root:[   16] Training loss: 165.91301954, Validation loss: 164.98194778, Gradient norm: 360.63327834
INFO:root:[   17] Training loss: 165.80447442, Validation loss: 165.04338877, Gradient norm: 377.04669164
INFO:root:[   18] Training loss: 165.64314012, Validation loss: 164.88941929, Gradient norm: 360.20874572
INFO:root:[   19] Training loss: 165.44056030, Validation loss: 164.56708834, Gradient norm: 313.55449262
INFO:root:[   20] Training loss: 165.03612522, Validation loss: 163.84369432, Gradient norm: 275.19031138
INFO:root:[   21] Training loss: 164.48245829, Validation loss: 163.25680515, Gradient norm: 255.82034508
INFO:root:[   22] Training loss: 164.06589722, Validation loss: 162.87262043, Gradient norm: 267.24413334
INFO:root:[   23] Training loss: 163.78295566, Validation loss: 162.55704217, Gradient norm: 288.51792916
INFO:root:[   24] Training loss: 163.57188287, Validation loss: 162.80121653, Gradient norm: 288.98779759
INFO:root:[   25] Training loss: 163.42837355, Validation loss: 162.83362753, Gradient norm: 311.75049600
INFO:root:[   26] Training loss: 163.28025981, Validation loss: 162.46177914, Gradient norm: 323.92776104
INFO:root:[   27] Training loss: 163.11543972, Validation loss: 162.22009786, Gradient norm: 318.98556955
INFO:root:[   28] Training loss: 162.91598219, Validation loss: 162.03986426, Gradient norm: 335.14693768
INFO:root:[   29] Training loss: 162.82897678, Validation loss: 161.84037674, Gradient norm: 337.93904866
INFO:root:[   30] Training loss: 162.68620870, Validation loss: 161.49876029, Gradient norm: 324.14642594
INFO:root:[   31] Training loss: 162.60439718, Validation loss: 161.09642216, Gradient norm: 325.32267132
INFO:root:[   32] Training loss: 162.47817091, Validation loss: 161.23350230, Gradient norm: 309.40026005
INFO:root:[   33] Training loss: 162.35125387, Validation loss: 161.24080511, Gradient norm: 291.49281927
INFO:root:[   34] Training loss: 162.26743171, Validation loss: 160.97183468, Gradient norm: 297.00678610
INFO:root:[   35] Training loss: 162.12716600, Validation loss: 161.32476673, Gradient norm: 275.22494419
INFO:root:[   36] Training loss: 162.02738173, Validation loss: 160.84152757, Gradient norm: 285.13901427
INFO:root:[   37] Training loss: 161.90892341, Validation loss: 160.56028292, Gradient norm: 261.33670103
INFO:root:[   38] Training loss: 161.77519613, Validation loss: 160.39968524, Gradient norm: 241.87193859
INFO:root:[   39] Training loss: 161.67530240, Validation loss: 160.45544139, Gradient norm: 253.84685195
INFO:root:[   40] Training loss: 161.58365431, Validation loss: 160.19359950, Gradient norm: 239.47330828
INFO:root:[   41] Training loss: 161.46411316, Validation loss: 160.03534150, Gradient norm: 226.22214757
INFO:root:[   42] Training loss: 161.38372382, Validation loss: 160.29616319, Gradient norm: 236.72479153
INFO:root:[   43] Training loss: 161.28311490, Validation loss: 160.05029779, Gradient norm: 227.59005128
INFO:root:[   44] Training loss: 161.17511624, Validation loss: 159.81702624, Gradient norm: 212.26346604
INFO:root:[   45] Training loss: 161.09121630, Validation loss: 159.74563117, Gradient norm: 204.82532111
INFO:root:[   46] Training loss: 161.01567295, Validation loss: 159.58505517, Gradient norm: 206.28095440
INFO:root:[   47] Training loss: 160.96122470, Validation loss: 159.51946085, Gradient norm: 200.54724114
INFO:root:[   48] Training loss: 160.87978638, Validation loss: 159.35644799, Gradient norm: 194.85360797
INFO:root:[   49] Training loss: 160.84789775, Validation loss: 159.60693199, Gradient norm: 194.30595473
INFO:root:[   50] Training loss: 160.79764174, Validation loss: 159.21758578, Gradient norm: 194.48980814
INFO:root:[   51] Training loss: 160.72109714, Validation loss: 159.48837414, Gradient norm: 192.22253639
INFO:root:[   52] Training loss: 160.66867594, Validation loss: 159.18882805, Gradient norm: 188.54430431
INFO:root:[   53] Training loss: 160.64649733, Validation loss: 159.24719934, Gradient norm: 181.90387371
INFO:root:[   54] Training loss: 160.55927450, Validation loss: 159.56354322, Gradient norm: 177.00361654
INFO:root:[   55] Training loss: 160.51415500, Validation loss: 159.11505288, Gradient norm: 189.68169332
INFO:root:[   56] Training loss: 160.43559794, Validation loss: 159.44410036, Gradient norm: 171.63367082
INFO:root:[   57] Training loss: 160.42513801, Validation loss: 159.09316722, Gradient norm: 176.10196035
INFO:root:[   58] Training loss: 160.38858778, Validation loss: 159.22240354, Gradient norm: 175.68752035
INFO:root:[   59] Training loss: 160.34600471, Validation loss: 158.91340691, Gradient norm: 164.64388191
INFO:root:[   60] Training loss: 160.32363274, Validation loss: 158.86485639, Gradient norm: 170.04643658
INFO:root:[   61] Training loss: 160.32356791, Validation loss: 158.89382827, Gradient norm: 174.87871488
INFO:root:[   62] Training loss: 160.29520962, Validation loss: 158.89820648, Gradient norm: 175.46995523
INFO:root:[   63] Training loss: 160.24378852, Validation loss: 158.98893309, Gradient norm: 171.66809425
INFO:root:[   64] Training loss: 160.23836270, Validation loss: 158.85948449, Gradient norm: 171.14414895
INFO:root:[   65] Training loss: 160.20131076, Validation loss: 158.77129351, Gradient norm: 162.29351251
INFO:root:[   66] Training loss: 160.21064446, Validation loss: 158.80733209, Gradient norm: 168.72787788
INFO:root:[   67] Training loss: 160.16694288, Validation loss: 158.98719252, Gradient norm: 163.18033198
INFO:root:[   68] Training loss: 160.14894362, Validation loss: 158.91616125, Gradient norm: 163.99222586
INFO:root:[   69] Training loss: 160.11336480, Validation loss: 158.65228111, Gradient norm: 159.73958273
INFO:root:[   70] Training loss: 160.09765076, Validation loss: 158.80012593, Gradient norm: 158.97190774
INFO:root:[   71] Training loss: 160.05668932, Validation loss: 158.70727030, Gradient norm: 159.94156387
INFO:root:[   72] Training loss: 160.07424628, Validation loss: 158.86963747, Gradient norm: 161.76853357
INFO:root:[   73] Training loss: 160.05632684, Validation loss: 158.69768430, Gradient norm: 159.64906736
INFO:root:[   74] Training loss: 160.03998840, Validation loss: 158.88693692, Gradient norm: 163.30635360
INFO:root:[   75] Training loss: 160.01076084, Validation loss: 158.78332627, Gradient norm: 154.36028538
INFO:root:[   76] Training loss: 159.98012099, Validation loss: 158.62109536, Gradient norm: 148.20169895
INFO:root:[   77] Training loss: 159.98661072, Validation loss: 158.84479750, Gradient norm: 164.99000789
INFO:root:[   78] Training loss: 159.94602186, Validation loss: 158.61938048, Gradient norm: 157.17916418
INFO:root:[   79] Training loss: 159.91343180, Validation loss: 158.66828463, Gradient norm: 158.37787387
INFO:root:[   80] Training loss: 159.88501397, Validation loss: 158.45369520, Gradient norm: 155.14180268
INFO:root:[   81] Training loss: 159.89412760, Validation loss: 158.59289149, Gradient norm: 155.31653543
INFO:root:[   82] Training loss: 159.85740892, Validation loss: 158.60667393, Gradient norm: 152.77524744
INFO:root:[   83] Training loss: 159.87115167, Validation loss: 158.69940400, Gradient norm: 161.27875347
INFO:root:[   84] Training loss: 159.83863905, Validation loss: 158.89503934, Gradient norm: 157.48411839
INFO:root:[   85] Training loss: 159.82258111, Validation loss: 158.67426394, Gradient norm: 150.36745360
INFO:root:[   86] Training loss: 159.79473619, Validation loss: 158.47987473, Gradient norm: 153.39972752
INFO:root:[   87] Training loss: 159.78875149, Validation loss: 158.63902657, Gradient norm: 152.95991264
INFO:root:[   88] Training loss: 159.74240241, Validation loss: 158.53620429, Gradient norm: 150.07776430
INFO:root:[   89] Training loss: 159.72053860, Validation loss: 158.32424498, Gradient norm: 150.31885634
INFO:root:[   90] Training loss: 159.71628791, Validation loss: 158.32672066, Gradient norm: 157.46461654
INFO:root:[   91] Training loss: 159.67073771, Validation loss: 158.56645363, Gradient norm: 152.48546867
INFO:root:[   92] Training loss: 159.67985494, Validation loss: 158.32758880, Gradient norm: 158.98855613
INFO:root:[   93] Training loss: 159.67042514, Validation loss: 158.59941556, Gradient norm: 152.01508677
INFO:root:[   94] Training loss: 159.64433601, Validation loss: 158.62315797, Gradient norm: 162.53374679
INFO:root:[   95] Training loss: 159.60337497, Validation loss: 158.38452416, Gradient norm: 146.59069961
INFO:root:[   96] Training loss: 159.60218126, Validation loss: 158.33200930, Gradient norm: 156.24228204
INFO:root:[   97] Training loss: 159.59381863, Validation loss: 158.17711439, Gradient norm: 154.40609941
INFO:root:[   98] Training loss: 159.58900431, Validation loss: 158.45097538, Gradient norm: 159.33269149
INFO:root:[   99] Training loss: 159.55769674, Validation loss: 158.10290875, Gradient norm: 155.37755177
INFO:root:[  100] Training loss: 159.52018995, Validation loss: 158.23291551, Gradient norm: 152.48982694
INFO:root:[  101] Training loss: 159.53608643, Validation loss: 158.26582979, Gradient norm: 158.27021950
INFO:root:[  102] Training loss: 159.51173977, Validation loss: 158.25889962, Gradient norm: 150.63983654
INFO:root:[  103] Training loss: 159.47604079, Validation loss: 158.23860409, Gradient norm: 156.10266570
INFO:root:[  104] Training loss: 159.50215305, Validation loss: 158.34921827, Gradient norm: 160.92567100
INFO:root:[  105] Training loss: 159.44423455, Validation loss: 158.16720126, Gradient norm: 153.67148662
INFO:root:[  106] Training loss: 159.43199836, Validation loss: 157.98684612, Gradient norm: 157.72258344
INFO:root:[  107] Training loss: 159.43715339, Validation loss: 158.16135099, Gradient norm: 157.35885724
INFO:root:[  108] Training loss: 159.37171319, Validation loss: 158.34083557, Gradient norm: 150.36636050
INFO:root:[  109] Training loss: 159.38420200, Validation loss: 158.01855121, Gradient norm: 155.02748394
INFO:root:[  110] Training loss: 159.39199788, Validation loss: 158.17010471, Gradient norm: 162.89875909
INFO:root:[  111] Training loss: 159.35860243, Validation loss: 158.12915976, Gradient norm: 154.09416543
INFO:root:[  112] Training loss: 159.38576504, Validation loss: 158.19718505, Gradient norm: 167.39933413
INFO:root:[  113] Training loss: 159.35880649, Validation loss: 157.92073059, Gradient norm: 163.90419852
INFO:root:[  114] Training loss: 159.33834412, Validation loss: 157.91442978, Gradient norm: 161.77808774
INFO:root:[  115] Training loss: 159.29610135, Validation loss: 157.97469879, Gradient norm: 159.56965477
INFO:root:[  116] Training loss: 159.30248691, Validation loss: 158.06405131, Gradient norm: 165.24853788
INFO:root:[  117] Training loss: 159.29028083, Validation loss: 157.91860427, Gradient norm: 164.70854640
INFO:root:[  118] Training loss: 159.26115987, Validation loss: 157.86777697, Gradient norm: 160.56948101
INFO:root:[  119] Training loss: 159.23603990, Validation loss: 157.83646433, Gradient norm: 164.67427394
INFO:root:[  120] Training loss: 159.24890069, Validation loss: 157.97040491, Gradient norm: 162.22995123
INFO:root:[  121] Training loss: 159.24297350, Validation loss: 157.81838775, Gradient norm: 167.05890583
INFO:root:[  122] Training loss: 159.21211717, Validation loss: 157.85894695, Gradient norm: 162.01372160
INFO:root:[  123] Training loss: 159.19447428, Validation loss: 157.87425874, Gradient norm: 165.85699717
INFO:root:[  124] Training loss: 159.19237793, Validation loss: 157.71200749, Gradient norm: 167.36718463
INFO:root:[  125] Training loss: 159.16387573, Validation loss: 157.76682723, Gradient norm: 161.67936247
INFO:root:[  126] Training loss: 159.18313524, Validation loss: 158.00672377, Gradient norm: 171.15113136
INFO:root:[  127] Training loss: 159.15730035, Validation loss: 157.89950106, Gradient norm: 159.77152039
INFO:root:[  128] Training loss: 159.16696303, Validation loss: 157.96967731, Gradient norm: 165.82848395
INFO:root:[  129] Training loss: 159.13838013, Validation loss: 157.70664576, Gradient norm: 169.16622210
INFO:root:[  130] Training loss: 159.10024367, Validation loss: 158.15371490, Gradient norm: 172.20371518
INFO:root:[  131] Training loss: 159.11127475, Validation loss: 157.84732029, Gradient norm: 163.48657058
INFO:root:[  132] Training loss: 159.09582703, Validation loss: 157.95954226, Gradient norm: 166.49886383
INFO:root:[  133] Training loss: 159.08584086, Validation loss: 158.06056133, Gradient norm: 168.07400473
INFO:root:[  134] Training loss: 159.06992106, Validation loss: 158.45431679, Gradient norm: 169.16253668
INFO:root:[  135] Training loss: 159.05371657, Validation loss: 157.80019071, Gradient norm: 168.36923247
INFO:root:[  136] Training loss: 159.04091173, Validation loss: 157.83881980, Gradient norm: 164.62832806
INFO:root:[  137] Training loss: 159.02431837, Validation loss: 157.82150991, Gradient norm: 164.54776298
INFO:root:[  138] Training loss: 159.01905484, Validation loss: 157.59383245, Gradient norm: 168.22576443
INFO:root:[  139] Training loss: 159.02331434, Validation loss: 157.64335017, Gradient norm: 170.55768839
INFO:root:[  140] Training loss: 159.00190335, Validation loss: 157.74674533, Gradient norm: 165.52386713
INFO:root:[  141] Training loss: 158.98900967, Validation loss: 157.91072752, Gradient norm: 170.61588872
INFO:root:[  142] Training loss: 158.95747125, Validation loss: 157.56190999, Gradient norm: 163.92677766
INFO:root:[  143] Training loss: 158.95796529, Validation loss: 157.76039766, Gradient norm: 167.87404237
INFO:root:[  144] Training loss: 158.94403395, Validation loss: 157.75806521, Gradient norm: 164.83494365
INFO:root:[  145] Training loss: 158.94588630, Validation loss: 157.91966810, Gradient norm: 172.47060434
INFO:root:[  146] Training loss: 158.93152839, Validation loss: 157.91441988, Gradient norm: 175.78448980
INFO:root:[  147] Training loss: 158.90302911, Validation loss: 157.79875424, Gradient norm: 165.04116561
INFO:root:[  148] Training loss: 158.90865329, Validation loss: 157.62854057, Gradient norm: 174.88932466
INFO:root:[  149] Training loss: 158.88440809, Validation loss: 157.82247309, Gradient norm: 168.31704831
INFO:root:[  150] Training loss: 158.86589973, Validation loss: 157.57811295, Gradient norm: 168.04780495
INFO:root:[  151] Training loss: 158.84297953, Validation loss: 157.42756706, Gradient norm: 166.20945163
INFO:root:[  152] Training loss: 158.83230157, Validation loss: 157.46460416, Gradient norm: 166.61595749
INFO:root:[  153] Training loss: 158.80660631, Validation loss: 157.46046582, Gradient norm: 159.52634940
INFO:root:[  154] Training loss: 158.81482456, Validation loss: 157.74262626, Gradient norm: 166.98186993
INFO:root:[  155] Training loss: 158.83848524, Validation loss: 157.58823649, Gradient norm: 173.32978936
INFO:root:[  156] Training loss: 158.80586812, Validation loss: 157.67497387, Gradient norm: 169.36850621
INFO:root:[  157] Training loss: 158.81042738, Validation loss: 157.58780443, Gradient norm: 174.37332998
INFO:root:[  158] Training loss: 158.77968614, Validation loss: 157.45609913, Gradient norm: 176.03388992
INFO:root:[  159] Training loss: 158.75613539, Validation loss: 157.49239925, Gradient norm: 167.51099022
INFO:root:[  160] Training loss: 158.75575724, Validation loss: 157.47230235, Gradient norm: 175.81474149
INFO:root:EP 160: Early stopping
INFO:root:Training the model took 4034.83s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 156.84949
INFO:root:EnergyScoreTrain: 156.5944
INFO:root:CoverageTrain: 0.0015
INFO:root:IntervalWidthTrain: 0.00682
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 157.41425
INFO:root:EnergyScoreValidation: 157.1091
INFO:root:CoverageValidation: 0.00178
INFO:root:IntervalWidthValidation: 0.00821
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 157.35328
INFO:root:EnergyScoreTest: 157.12002
INFO:root:CoverageTest: 0.00138
INFO:root:IntervalWidthTest: 0.00622
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 173.39502645, Validation loss: 171.35116738, Gradient norm: 31.24641587
INFO:root:[    2] Training loss: 170.51706150, Validation loss: 169.65461276, Gradient norm: 33.80139047
INFO:root:[    3] Training loss: 169.64900906, Validation loss: 168.65578474, Gradient norm: 200.25658421
INFO:root:[    4] Training loss: 169.32768507, Validation loss: 169.81933513, Gradient norm: 554.50018154
INFO:root:[    5] Training loss: 169.45370124, Validation loss: 168.75964409, Gradient norm: 629.38429112
INFO:root:[    6] Training loss: 169.15392815, Validation loss: 168.40158964, Gradient norm: 702.71635303
INFO:root:[    7] Training loss: 168.90227159, Validation loss: 169.86753631, Gradient norm: 912.30257500
INFO:root:[    8] Training loss: 168.85737061, Validation loss: 168.21812252, Gradient norm: 901.69764745
INFO:root:[    9] Training loss: 168.76103360, Validation loss: 167.88608056, Gradient norm: 998.74714075
INFO:root:[   10] Training loss: 169.03703871, Validation loss: 167.84342849, Gradient norm: 859.10434182
INFO:root:[   11] Training loss: 170.16971476, Validation loss: 171.45406837, Gradient norm: 518.62380885
INFO:root:[   12] Training loss: 171.45578715, Validation loss: 171.52267429, Gradient norm: 0.12395908
INFO:root:[   13] Training loss: 171.45580722, Validation loss: 171.56933781, Gradient norm: 0.15896867
INFO:root:[   14] Training loss: 171.45582018, Validation loss: 171.46647002, Gradient norm: 0.11970664
INFO:root:[   15] Training loss: 171.45581733, Validation loss: 171.49991755, Gradient norm: 0.13334665
INFO:root:[   16] Training loss: 171.45581428, Validation loss: 171.54473502, Gradient norm: 0.11469399
INFO:root:[   17] Training loss: 171.45580844, Validation loss: 171.44751164, Gradient norm: 0.12307229
INFO:root:[   18] Training loss: 171.45579420, Validation loss: 171.49145053, Gradient norm: 0.13605087
INFO:root:[   19] Training loss: 171.45582635, Validation loss: 171.52180642, Gradient norm: 0.11759016
INFO:root:[   20] Training loss: 171.45586711, Validation loss: 171.46261115, Gradient norm: 0.12622168
INFO:root:[   21] Training loss: 171.45584181, Validation loss: 171.47931310, Gradient norm: 0.12729354
INFO:root:[   22] Training loss: 171.45588603, Validation loss: 171.48884984, Gradient norm: 0.13842455
INFO:root:[   23] Training loss: 171.45585334, Validation loss: 171.49255666, Gradient norm: 0.12504488
INFO:root:[   24] Training loss: 171.45584534, Validation loss: 171.46763423, Gradient norm: 0.12807256
INFO:root:[   25] Training loss: 171.45585958, Validation loss: 171.47942714, Gradient norm: 0.12099489
INFO:root:[   26] Training loss: 171.45583374, Validation loss: 171.49296945, Gradient norm: 0.12279625
INFO:root:[   27] Training loss: 171.45586975, Validation loss: 171.44482904, Gradient norm: 0.12634576
INFO:root:[   28] Training loss: 171.45581713, Validation loss: 171.48466893, Gradient norm: 0.11558562
INFO:root:[   29] Training loss: 171.45586473, Validation loss: 171.47566223, Gradient norm: 0.13123050
INFO:root:[   30] Training loss: 171.45586249, Validation loss: 171.48498589, Gradient norm: 0.12826926
INFO:root:[   31] Training loss: 171.45583455, Validation loss: 171.49331504, Gradient norm: 0.13462214
INFO:root:[   32] Training loss: 171.45581387, Validation loss: 171.51858895, Gradient norm: 0.12485453
INFO:root:[   33] Training loss: 171.45580458, Validation loss: 171.50698290, Gradient norm: 0.11598481
INFO:root:[   34] Training loss: 171.45584683, Validation loss: 171.52895208, Gradient norm: 0.12822567
INFO:root:[   35] Training loss: 171.45581624, Validation loss: 171.46633911, Gradient norm: 0.11151512
INFO:root:[   36] Training loss: 171.45585015, Validation loss: 171.50825581, Gradient norm: 0.12308974
INFO:root:[   37] Training loss: 171.45584791, Validation loss: 171.48800980, Gradient norm: 0.12275752
INFO:root:[   38] Training loss: 171.45584405, Validation loss: 171.43744592, Gradient norm: 0.11796016
INFO:root:[   39] Training loss: 171.45585809, Validation loss: 171.51605760, Gradient norm: 0.12855926
INFO:root:[   40] Training loss: 171.45586738, Validation loss: 171.49079868, Gradient norm: 0.13959062
INFO:root:[   41] Training loss: 171.45584323, Validation loss: 171.47912303, Gradient norm: 0.12106025
INFO:root:[   42] Training loss: 171.45581733, Validation loss: 171.49955669, Gradient norm: 0.12513137
INFO:root:[   43] Training loss: 171.45581462, Validation loss: 171.44736655, Gradient norm: 0.11091314
INFO:root:[   44] Training loss: 171.45583435, Validation loss: 171.47968493, Gradient norm: 0.12558931
INFO:root:[   45] Training loss: 171.45587518, Validation loss: 171.49830039, Gradient norm: 0.13596551
INFO:root:[   46] Training loss: 171.45588474, Validation loss: 171.46028914, Gradient norm: 0.13480558
INFO:root:[   47] Training loss: 171.45587633, Validation loss: 171.44441946, Gradient norm: 0.13854181
INFO:root:[   48] Training loss: 171.45586392, Validation loss: 171.52078140, Gradient norm: 0.14313917
INFO:root:[   49] Training loss: 171.45584262, Validation loss: 171.48858080, Gradient norm: 0.11483982
INFO:root:[   50] Training loss: 171.45583096, Validation loss: 171.46557483, Gradient norm: 0.13224578
INFO:root:[   51] Training loss: 171.45581801, Validation loss: 171.46843358, Gradient norm: 0.11887297
INFO:root:[   52] Training loss: 171.45586670, Validation loss: 171.44508362, Gradient norm: 0.13513398
INFO:root:[   53] Training loss: 171.45583930, Validation loss: 171.48085691, Gradient norm: 0.12273691
INFO:root:[   54] Training loss: 171.45587179, Validation loss: 171.47027722, Gradient norm: 0.12175667
INFO:root:[   55] Training loss: 171.45584452, Validation loss: 171.44038016, Gradient norm: 0.12022403
INFO:root:[   56] Training loss: 171.45589220, Validation loss: 171.49224880, Gradient norm: 0.13365787
INFO:root:[   57] Training loss: 171.45582960, Validation loss: 171.51426670, Gradient norm: 0.12586964
INFO:root:[   58] Training loss: 171.45583394, Validation loss: 171.47853838, Gradient norm: 0.12332786
INFO:root:[   59] Training loss: 171.45585110, Validation loss: 171.47117186, Gradient norm: 0.12659475
INFO:root:[   60] Training loss: 171.45582255, Validation loss: 171.46952258, Gradient norm: 0.12673582
INFO:root:[   61] Training loss: 171.45587341, Validation loss: 171.47094834, Gradient norm: 0.12429367
INFO:root:[   62] Training loss: 171.45585144, Validation loss: 171.48722117, Gradient norm: 0.12533837
INFO:root:[   63] Training loss: 171.45583937, Validation loss: 171.55865371, Gradient norm: 0.12627239
INFO:root:[   64] Training loss: 171.45582954, Validation loss: 171.45192625, Gradient norm: 0.13144877
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 1523.93s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 167.74761
INFO:root:EnergyScoreTrain: 166.80254
INFO:root:CoverageTrain: 0.00259
INFO:root:IntervalWidthTrain: 0.01514
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 167.83983
INFO:root:EnergyScoreValidation: 166.9582
INFO:root:CoverageValidation: 0.00238
INFO:root:IntervalWidthValidation: 0.01412
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 167.81089
INFO:root:EnergyScoreTest: 166.77553
INFO:root:CoverageTest: 0.00283
INFO:root:IntervalWidthTest: 0.01658
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 173.47924798, Validation loss: 171.53078233, Gradient norm: 49.64586469
INFO:root:[    2] Training loss: 329.20301432, Validation loss: 171.49390130, Gradient norm: 43392.98790112
INFO:root:[    3] Training loss: 171.45578139, Validation loss: 171.45644392, Gradient norm: 0.11832157
INFO:root:[    4] Training loss: 171.45577922, Validation loss: 171.50643573, Gradient norm: 0.12404292
INFO:root:[    5] Training loss: 171.45577657, Validation loss: 171.53273144, Gradient norm: 0.11694285
INFO:root:[    6] Training loss: 171.45580146, Validation loss: 171.50605479, Gradient norm: 0.12917434
INFO:root:[    7] Training loss: 171.45579936, Validation loss: 171.47589272, Gradient norm: 0.12019549
INFO:root:[    8] Training loss: 171.45580919, Validation loss: 171.50613002, Gradient norm: 0.11829445
INFO:root:[    9] Training loss: 171.45587077, Validation loss: 171.47284926, Gradient norm: 0.13428059
INFO:root:[   10] Training loss: 171.45583238, Validation loss: 171.45879913, Gradient norm: 0.13648522
INFO:root:[   11] Training loss: 171.45583449, Validation loss: 171.46754616, Gradient norm: 0.12934444
INFO:root:[   12] Training loss: 171.45585910, Validation loss: 171.46390413, Gradient norm: 0.13336891
INFO:root:[   13] Training loss: 171.45581957, Validation loss: 171.42783155, Gradient norm: 0.12035034
INFO:root:[   14] Training loss: 171.45581190, Validation loss: 171.48716415, Gradient norm: 0.12323872
INFO:root:[   15] Training loss: 171.45583510, Validation loss: 171.52536198, Gradient norm: 0.12066357
INFO:root:[   16] Training loss: 171.45584520, Validation loss: 171.47076095, Gradient norm: 0.12038406
INFO:root:[   17] Training loss: 171.45583388, Validation loss: 171.46871440, Gradient norm: 0.12426665
INFO:root:[   18] Training loss: 171.45582845, Validation loss: 171.53269022, Gradient norm: 0.13538953
INFO:root:[   19] Training loss: 171.45585822, Validation loss: 171.53750369, Gradient norm: 0.12737912
INFO:root:[   20] Training loss: 171.45583123, Validation loss: 171.54120649, Gradient norm: 0.12246942
INFO:root:[   21] Training loss: 171.45586799, Validation loss: 171.47153433, Gradient norm: 0.13074497
INFO:root:[   22] Training loss: 171.45584988, Validation loss: 171.47146018, Gradient norm: 0.12281807
INFO:root:[   23] Training loss: 171.45586222, Validation loss: 171.47774947, Gradient norm: 0.13426882
INFO:root:[   24] Training loss: 171.45585626, Validation loss: 171.42753735, Gradient norm: 0.12506621
INFO:root:[   25] Training loss: 171.45581963, Validation loss: 171.52989384, Gradient norm: 0.12327952
INFO:root:[   26] Training loss: 171.45583544, Validation loss: 171.48565192, Gradient norm: 0.12929800
INFO:root:[   27] Training loss: 171.45582642, Validation loss: 171.44240128, Gradient norm: 0.11647497
INFO:root:[   28] Training loss: 171.45583998, Validation loss: 171.52317007, Gradient norm: 0.12372993
INFO:root:[   29] Training loss: 171.45585863, Validation loss: 171.48039031, Gradient norm: 0.13426909
INFO:root:[   30] Training loss: 171.45584751, Validation loss: 171.48785293, Gradient norm: 0.13730976
INFO:root:[   31] Training loss: 171.45582825, Validation loss: 171.48901822, Gradient norm: 0.10997459
INFO:root:[   32] Training loss: 171.45586304, Validation loss: 171.47788520, Gradient norm: 0.12492242
INFO:root:[   33] Training loss: 171.45583496, Validation loss: 171.49904378, Gradient norm: 0.12314358
INFO:root:[   34] Training loss: 171.45583089, Validation loss: 171.46721743, Gradient norm: 0.12012486
INFO:root:[   35] Training loss: 171.45585592, Validation loss: 171.45840695, Gradient norm: 0.12601448
INFO:root:[   36] Training loss: 171.45584188, Validation loss: 171.47633255, Gradient norm: 0.12462249
INFO:root:[   37] Training loss: 171.45586751, Validation loss: 171.43161814, Gradient norm: 0.12678365
INFO:root:[   38] Training loss: 171.45581746, Validation loss: 171.46872350, Gradient norm: 0.12260158
INFO:root:[   39] Training loss: 171.45585449, Validation loss: 171.46949929, Gradient norm: 0.13264104
INFO:root:[   40] Training loss: 171.45582886, Validation loss: 171.48207467, Gradient norm: 0.11923283
INFO:root:[   41] Training loss: 171.45582642, Validation loss: 171.49163711, Gradient norm: 0.12273098
INFO:root:[   42] Training loss: 171.45588664, Validation loss: 171.53233685, Gradient norm: 0.13908366
INFO:root:[   43] Training loss: 171.45582845, Validation loss: 171.45733134, Gradient norm: 0.12377316
INFO:root:[   44] Training loss: 171.45583428, Validation loss: 171.49607876, Gradient norm: 0.12686066
INFO:root:[   45] Training loss: 171.45588243, Validation loss: 171.52088821, Gradient norm: 0.14290850
INFO:root:[   46] Training loss: 171.45586561, Validation loss: 171.47312526, Gradient norm: 0.13009333
INFO:root:[   47] Training loss: 171.45583388, Validation loss: 171.51050902, Gradient norm: 0.12012084
INFO:root:[   48] Training loss: 171.45583761, Validation loss: 171.47412805, Gradient norm: 0.12577757
INFO:root:[   49] Training loss: 171.45579210, Validation loss: 171.45596019, Gradient norm: 0.11684486
INFO:root:[   50] Training loss: 171.45587619, Validation loss: 171.46826225, Gradient norm: 0.13296975
INFO:root:[   51] Training loss: 171.45584629, Validation loss: 171.51234463, Gradient norm: 0.12583644
INFO:root:[   52] Training loss: 171.45584235, Validation loss: 171.46135645, Gradient norm: 0.13171735
INFO:root:[   53] Training loss: 171.45583367, Validation loss: 171.54581090, Gradient norm: 0.12098138
INFO:root:[   54] Training loss: 171.45584635, Validation loss: 171.52783953, Gradient norm: 0.13018060
INFO:root:[   55] Training loss: 171.45583191, Validation loss: 171.45723176, Gradient norm: 0.12763124
INFO:root:[   56] Training loss: 171.45586107, Validation loss: 171.51471911, Gradient norm: 0.12770094
INFO:root:[   57] Training loss: 171.45586331, Validation loss: 171.46326808, Gradient norm: 0.13444696
INFO:root:[   58] Training loss: 171.45586616, Validation loss: 171.49363976, Gradient norm: 0.13674593
INFO:root:[   59] Training loss: 171.45582160, Validation loss: 171.51103906, Gradient norm: 0.12884861
INFO:root:[   60] Training loss: 171.45583910, Validation loss: 171.48743934, Gradient norm: 0.12270636
INFO:root:[   61] Training loss: 171.45586982, Validation loss: 171.50409953, Gradient norm: 0.13108963
INFO:root:[   62] Training loss: 171.45583964, Validation loss: 171.47680959, Gradient norm: 0.12849207
INFO:root:[   63] Training loss: 171.45582194, Validation loss: 171.49529655, Gradient norm: 0.12172857
INFO:root:[   64] Training loss: 171.45585409, Validation loss: 171.44964707, Gradient norm: 0.13781332
INFO:root:[   65] Training loss: 171.45586595, Validation loss: 171.47436390, Gradient norm: 0.12592110
INFO:root:[   66] Training loss: 171.45586351, Validation loss: 171.47041910, Gradient norm: 0.12745443
INFO:root:[   67] Training loss: 171.45581692, Validation loss: 171.43364863, Gradient norm: 0.13349431
INFO:root:[   68] Training loss: 171.45584242, Validation loss: 171.46651472, Gradient norm: 0.12591243
INFO:root:[   69] Training loss: 171.45582574, Validation loss: 171.49097322, Gradient norm: 0.12936367
INFO:root:[   70] Training loss: 171.45585931, Validation loss: 171.48565085, Gradient norm: 0.14149336
INFO:root:[   71] Training loss: 171.45585347, Validation loss: 171.41297993, Gradient norm: 0.13956091
INFO:root:[   72] Training loss: 171.45581977, Validation loss: 171.44877919, Gradient norm: 0.12665346
INFO:root:[   73] Training loss: 171.45581665, Validation loss: 171.52640948, Gradient norm: 0.12093840
INFO:root:[   74] Training loss: 171.45585110, Validation loss: 171.48046688, Gradient norm: 0.13591561
INFO:root:[   75] Training loss: 171.45583252, Validation loss: 171.48444139, Gradient norm: 0.13237308
INFO:root:[   76] Training loss: 171.45582967, Validation loss: 171.49135978, Gradient norm: 0.12903665
INFO:root:[   77] Training loss: 171.45583659, Validation loss: 171.48369639, Gradient norm: 0.12661686
INFO:root:[   78] Training loss: 171.45585402, Validation loss: 171.45105516, Gradient norm: 0.13148156
INFO:root:[   79] Training loss: 171.45585524, Validation loss: 171.49073738, Gradient norm: 0.13266037
INFO:root:[   80] Training loss: 171.45587124, Validation loss: 171.47009465, Gradient norm: 0.13049103
INFO:root:EP 80: Early stopping
INFO:root:Training the model took 1908.868s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.45579
INFO:root:EnergyScoreTrain: 171.4497
INFO:root:CoverageTrain: 4e-05
INFO:root:IntervalWidthTrain: 0.00027
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.48719
INFO:root:EnergyScoreValidation: 171.48123
INFO:root:CoverageValidation: 4e-05
INFO:root:IntervalWidthValidation: 0.00027
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.52223
INFO:root:EnergyScoreTest: 171.51687
INFO:root:CoverageTest: 3e-05
INFO:root:IntervalWidthTest: 0.00024
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 179.11416619, Validation loss: 171.44928594, Gradient norm: 1057.10287020
INFO:root:[    2] Training loss: 171.45591756, Validation loss: 171.44972550, Gradient norm: 0.13916176
INFO:root:[    3] Training loss: 2016.23818332, Validation loss: 171.48676019, Gradient norm: 758560.15575301
INFO:root:[    4] Training loss: 244.95220039, Validation loss: 171.49594812, Gradient norm: 315621.20998263
INFO:root:[    5] Training loss: 171.45578301, Validation loss: 171.48609550, Gradient norm: 0.12093650
INFO:root:[    6] Training loss: 171.45580329, Validation loss: 171.48872643, Gradient norm: 0.13087170
INFO:root:[    7] Training loss: 171.45579481, Validation loss: 171.52136471, Gradient norm: 0.12399569
INFO:root:[    8] Training loss: 171.45580471, Validation loss: 171.48335480, Gradient norm: 0.12359136
INFO:root:[    9] Training loss: 171.45582628, Validation loss: 171.49514530, Gradient norm: 0.12954055
INFO:root:[   10] Training loss: 171.45581319, Validation loss: 171.48056111, Gradient norm: 0.11991876
INFO:root:[   11] Training loss: 171.45584384, Validation loss: 171.48816293, Gradient norm: 0.13129723
INFO:root:[   12] Training loss: 171.45585110, Validation loss: 171.45407774, Gradient norm: 0.13750874
INFO:root:[   13] Training loss: 171.45580492, Validation loss: 171.50653424, Gradient norm: 0.12261101
INFO:root:[   14] Training loss: 171.45583245, Validation loss: 171.45890808, Gradient norm: 0.12140901
INFO:root:[   15] Training loss: 171.45585931, Validation loss: 171.48142470, Gradient norm: 0.13263050
INFO:root:[   16] Training loss: 171.45586629, Validation loss: 171.48726105, Gradient norm: 0.12084617
INFO:root:[   17] Training loss: 171.45583496, Validation loss: 171.46043958, Gradient norm: 0.11614248
INFO:root:[   18] Training loss: 171.45583049, Validation loss: 171.42641543, Gradient norm: 0.12605955
INFO:root:[   19] Training loss: 171.45582391, Validation loss: 171.49271165, Gradient norm: 0.12003328
INFO:root:[   20] Training loss: 171.45585015, Validation loss: 171.50251850, Gradient norm: 0.12128169
INFO:root:[   21] Training loss: 171.45584466, Validation loss: 171.46575232, Gradient norm: 0.12214703
INFO:root:[   22] Training loss: 171.45584466, Validation loss: 171.45964264, Gradient norm: 0.13416420
INFO:root:[   23] Training loss: 171.45584290, Validation loss: 171.47890620, Gradient norm: 0.12800071
INFO:root:[   24] Training loss: 171.45586595, Validation loss: 171.48467670, Gradient norm: 0.13512984
INFO:root:[   25] Training loss: 171.45583978, Validation loss: 171.49798718, Gradient norm: 0.11687710
INFO:root:[   26] Training loss: 171.45588691, Validation loss: 171.48194591, Gradient norm: 0.13419286
INFO:root:[   27] Training loss: 171.45587341, Validation loss: 171.52921844, Gradient norm: 0.12828244
INFO:root:[   28] Training loss: 171.45584635, Validation loss: 171.47765016, Gradient norm: 0.13256288
INFO:root:[   29] Training loss: 171.45585605, Validation loss: 171.45654618, Gradient norm: 0.12964979
INFO:root:[   30] Training loss: 171.45585280, Validation loss: 171.51295257, Gradient norm: 0.13326236
INFO:root:[   31] Training loss: 171.45582065, Validation loss: 171.48797768, Gradient norm: 0.11201184
INFO:root:[   32] Training loss: 171.45582377, Validation loss: 171.48817979, Gradient norm: 0.12805065
INFO:root:[   33] Training loss: 171.45584927, Validation loss: 171.54488600, Gradient norm: 0.12546372
INFO:root:[   34] Training loss: 171.45584073, Validation loss: 171.47716750, Gradient norm: 0.12278336
INFO:root:[   35] Training loss: 171.45586202, Validation loss: 171.47422202, Gradient norm: 0.13750226
INFO:root:[   36] Training loss: 171.45584771, Validation loss: 171.47627098, Gradient norm: 0.12280016
INFO:root:[   37] Training loss: 171.45584466, Validation loss: 171.41764243, Gradient norm: 0.12905786
INFO:root:[   38] Training loss: 171.45583381, Validation loss: 171.44761042, Gradient norm: 0.13334727
INFO:root:[   39] Training loss: 171.45584676, Validation loss: 171.47581428, Gradient norm: 0.11980184
INFO:root:[   40] Training loss: 171.45581584, Validation loss: 171.45532200, Gradient norm: 0.12278634
INFO:root:[   41] Training loss: 171.45583293, Validation loss: 171.45716965, Gradient norm: 0.12199169
INFO:root:[   42] Training loss: 171.45587084, Validation loss: 171.49671722, Gradient norm: 0.13697189
INFO:root:[   43] Training loss: 171.45586555, Validation loss: 171.51293035, Gradient norm: 0.13386207
INFO:root:[   44] Training loss: 171.45588535, Validation loss: 171.46596594, Gradient norm: 0.13632701
INFO:root:[   45] Training loss: 171.45586168, Validation loss: 171.48538235, Gradient norm: 0.12580948
INFO:root:[   46] Training loss: 171.45586053, Validation loss: 171.49314773, Gradient norm: 0.13157694
INFO:root:[   47] Training loss: 171.45582967, Validation loss: 171.47686768, Gradient norm: 0.11192049
INFO:root:[   48] Training loss: 171.45588786, Validation loss: 171.51038187, Gradient norm: 0.12650796
INFO:root:[   49] Training loss: 171.45585958, Validation loss: 171.49489768, Gradient norm: 0.13657970
INFO:root:[   50] Training loss: 171.45583177, Validation loss: 171.48033490, Gradient norm: 0.12709631
INFO:root:[   51] Training loss: 171.45585965, Validation loss: 171.48401950, Gradient norm: 0.13414705
INFO:root:[   52] Training loss: 171.45586405, Validation loss: 171.48201016, Gradient norm: 0.13681239
INFO:root:[   53] Training loss: 171.45583503, Validation loss: 171.46704128, Gradient norm: 0.11838573
INFO:root:[   54] Training loss: 171.45586697, Validation loss: 171.48461941, Gradient norm: 0.13499766
INFO:root:[   55] Training loss: 171.45581089, Validation loss: 171.46210948, Gradient norm: 0.12863724
INFO:root:[   56] Training loss: 171.45585470, Validation loss: 171.47893323, Gradient norm: 0.12551545
INFO:root:[   57] Training loss: 171.45582723, Validation loss: 171.42846814, Gradient norm: 0.12906823
INFO:root:[   58] Training loss: 171.45584405, Validation loss: 171.48936971, Gradient norm: 0.12291491
INFO:root:[   59] Training loss: 171.45580241, Validation loss: 171.49393343, Gradient norm: 0.10892319
INFO:root:[   60] Training loss: 171.45584059, Validation loss: 171.52499363, Gradient norm: 0.12849341
INFO:root:[   61] Training loss: 171.45584622, Validation loss: 171.54041330, Gradient norm: 0.12415466
INFO:root:[   62] Training loss: 171.45588535, Validation loss: 171.48262666, Gradient norm: 0.14303874
INFO:root:[   63] Training loss: 171.45586480, Validation loss: 171.48045376, Gradient norm: 0.13088044
INFO:root:[   64] Training loss: 171.45583408, Validation loss: 171.49692147, Gradient norm: 0.13036257
INFO:root:[   65] Training loss: 171.45585137, Validation loss: 171.50962000, Gradient norm: 0.12719349
INFO:root:[   66] Training loss: 171.45579034, Validation loss: 171.49039071, Gradient norm: 0.11308185
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 1666.629s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.45583
INFO:root:EnergyScoreTrain: 171.44985
INFO:root:CoverageTrain: 4e-05
INFO:root:IntervalWidthTrain: 0.00027
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.48724
INFO:root:EnergyScoreValidation: 171.48115
INFO:root:CoverageValidation: 4e-05
INFO:root:IntervalWidthValidation: 0.00027
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.52227
INFO:root:EnergyScoreTest: 171.51608
INFO:root:CoverageTest: 4e-05
INFO:root:IntervalWidthTest: 0.00028
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 862.82552144, Validation loss: 171.43373857, Gradient norm: 193389.09613653
INFO:root:[    2] Training loss: 171.45601061, Validation loss: 171.49268997, Gradient norm: 0.15533072
INFO:root:[    3] Training loss: 171.45577867, Validation loss: 171.49956446, Gradient norm: 0.11736414
INFO:root:[    4] Training loss: 171.45577623, Validation loss: 171.45725505, Gradient norm: 0.13110133
INFO:root:[    5] Training loss: 171.45577806, Validation loss: 171.48240207, Gradient norm: 0.10870924
INFO:root:[    6] Training loss: 171.45577861, Validation loss: 171.46250273, Gradient norm: 0.11641184
INFO:root:[    7] Training loss: 171.45578118, Validation loss: 171.49246002, Gradient norm: 0.12015235
INFO:root:[    8] Training loss: 171.45578349, Validation loss: 171.47379423, Gradient norm: 0.12516427
INFO:root:[    9] Training loss: 171.47329393, Validation loss: 171.50616669, Gradient norm: 122.27712882
INFO:root:[   10] Training loss: 171.45579814, Validation loss: 171.51501385, Gradient norm: 0.12520961
INFO:root:[   11] Training loss: 171.45578851, Validation loss: 171.47254837, Gradient norm: 0.12630954
INFO:root:[   12] Training loss: 171.45578627, Validation loss: 171.51915647, Gradient norm: 0.11674419
INFO:root:[   13] Training loss: 171.45579149, Validation loss: 171.51220623, Gradient norm: 0.12288070
INFO:root:[   14] Training loss: 171.45580112, Validation loss: 171.49973712, Gradient norm: 0.12725374
INFO:root:[   15] Training loss: 171.45582194, Validation loss: 171.44710634, Gradient norm: 0.12460026
INFO:root:[   16] Training loss: 171.45582113, Validation loss: 171.46753840, Gradient norm: 0.11519364
INFO:root:[   17] Training loss: 171.45581753, Validation loss: 171.51600754, Gradient norm: 0.12398827
INFO:root:[   18] Training loss: 171.45579461, Validation loss: 171.48101432, Gradient norm: 0.12212115
INFO:root:[   19] Training loss: 171.45584290, Validation loss: 171.46223316, Gradient norm: 0.12788669
INFO:root:[   20] Training loss: 171.45582648, Validation loss: 171.54349264, Gradient norm: 0.12502916
INFO:root:[   21] Training loss: 171.45588542, Validation loss: 171.44236488, Gradient norm: 0.12586892
INFO:root:[   22] Training loss: 171.45584595, Validation loss: 171.47047130, Gradient norm: 0.13420781
INFO:root:[   23] Training loss: 171.45581604, Validation loss: 171.50308549, Gradient norm: 0.12831614
INFO:root:[   24] Training loss: 171.45584981, Validation loss: 171.46107884, Gradient norm: 0.11995950
INFO:root:[   25] Training loss: 171.45588114, Validation loss: 171.51221158, Gradient norm: 0.12270511
INFO:root:[   26] Training loss: 171.45589789, Validation loss: 171.49729357, Gradient norm: 0.15072787
INFO:root:[   27] Training loss: 171.45584066, Validation loss: 171.47003575, Gradient norm: 0.12975275
INFO:root:[   28] Training loss: 171.45585266, Validation loss: 171.48058252, Gradient norm: 0.12766927
INFO:root:[   29] Training loss: 171.45585083, Validation loss: 171.46071344, Gradient norm: 0.12554003
INFO:root:[   30] Training loss: 171.45584086, Validation loss: 171.50831042, Gradient norm: 0.13409540
INFO:root:[   31] Training loss: 171.45587782, Validation loss: 171.51983214, Gradient norm: 0.13419722
INFO:root:[   32] Training loss: 171.45583917, Validation loss: 171.45022181, Gradient norm: 0.13043774
INFO:root:[   33] Training loss: 171.45583421, Validation loss: 171.45284499, Gradient norm: 0.12469972
INFO:root:[   34] Training loss: 171.45584181, Validation loss: 171.48755525, Gradient norm: 0.12543241
INFO:root:[   35] Training loss: 171.45583577, Validation loss: 171.51612560, Gradient norm: 0.11762696
INFO:root:[   36] Training loss: 171.45584242, Validation loss: 171.51985864, Gradient norm: 0.12444583
INFO:root:[   37] Training loss: 171.45588386, Validation loss: 171.51451057, Gradient norm: 0.13277330
INFO:root:[   38] Training loss: 171.45584859, Validation loss: 171.46562382, Gradient norm: 0.12132009
INFO:root:[   39] Training loss: 171.45584730, Validation loss: 171.50712719, Gradient norm: 0.13319965
INFO:root:[   40] Training loss: 171.45584873, Validation loss: 171.46993751, Gradient norm: 0.12478486
INFO:root:[   41] Training loss: 171.45585232, Validation loss: 171.45780490, Gradient norm: 0.13389820
INFO:root:[   42] Training loss: 171.45586704, Validation loss: 171.48723562, Gradient norm: 0.12771911
INFO:root:[   43] Training loss: 171.45586229, Validation loss: 171.48079374, Gradient norm: 0.12812416
INFO:root:[   44] Training loss: 171.45584351, Validation loss: 171.46701826, Gradient norm: 0.12010175
INFO:root:[   45] Training loss: 171.45581692, Validation loss: 171.50061062, Gradient norm: 0.12113535
INFO:root:[   46] Training loss: 171.45584798, Validation loss: 171.49576421, Gradient norm: 0.13325561
INFO:root:[   47] Training loss: 171.45587097, Validation loss: 171.47399929, Gradient norm: 0.12901120
INFO:root:[   48] Training loss: 171.45582160, Validation loss: 171.46639212, Gradient norm: 0.12724057
INFO:root:[   49] Training loss: 171.45585151, Validation loss: 171.54636423, Gradient norm: 0.13315279
INFO:root:[   50] Training loss: 171.45588148, Validation loss: 171.51020411, Gradient norm: 0.12570432
INFO:root:[   51] Training loss: 171.45584106, Validation loss: 171.45024671, Gradient norm: 0.12387090
INFO:root:[   52] Training loss: 171.45582689, Validation loss: 171.45595778, Gradient norm: 0.13870175
INFO:root:[   53] Training loss: 171.45583198, Validation loss: 171.48630617, Gradient norm: 0.11720515
INFO:root:[   54] Training loss: 171.45588325, Validation loss: 171.46360806, Gradient norm: 0.12525691
INFO:root:[   55] Training loss: 171.45584344, Validation loss: 171.48346724, Gradient norm: 0.12753476
INFO:root:[   56] Training loss: 171.45584846, Validation loss: 171.50579834, Gradient norm: 0.13019246
INFO:root:[   57] Training loss: 171.45585124, Validation loss: 171.48755017, Gradient norm: 0.12479393
INFO:root:[   58] Training loss: 171.45582926, Validation loss: 171.47045818, Gradient norm: 0.12118917
INFO:root:[   59] Training loss: 171.45584283, Validation loss: 171.48904446, Gradient norm: 0.11832238
INFO:root:[   60] Training loss: 171.45586392, Validation loss: 171.44707208, Gradient norm: 0.13132244
INFO:root:[   61] Training loss: 171.45585490, Validation loss: 171.42512137, Gradient norm: 0.13100706
INFO:root:[   62] Training loss: 171.45583625, Validation loss: 171.49974060, Gradient norm: 0.12803918
INFO:root:[   63] Training loss: 171.45586270, Validation loss: 171.46948456, Gradient norm: 0.12682657
INFO:root:[   64] Training loss: 171.45583401, Validation loss: 171.50290051, Gradient norm: 0.12061068
INFO:root:[   65] Training loss: 171.45584547, Validation loss: 171.46124830, Gradient norm: 0.13813679
INFO:root:[   66] Training loss: 171.45591960, Validation loss: 171.49713536, Gradient norm: 0.14495027
INFO:root:[   67] Training loss: 171.45583110, Validation loss: 171.48586474, Gradient norm: 0.12530006
INFO:root:[   68] Training loss: 171.45589871, Validation loss: 171.45565207, Gradient norm: 0.13123794
INFO:root:[   69] Training loss: 171.45587267, Validation loss: 171.50855857, Gradient norm: 0.13606240
INFO:root:[   70] Training loss: 171.45583835, Validation loss: 171.48856849, Gradient norm: 0.13097609
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1644.778s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.45582
INFO:root:EnergyScoreTrain: 171.45022
INFO:root:CoverageTrain: 4e-05
INFO:root:IntervalWidthTrain: 0.00025
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.48723
INFO:root:EnergyScoreValidation: 171.48076
INFO:root:CoverageValidation: 4e-05
INFO:root:IntervalWidthValidation: 0.00029
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.52226
INFO:root:EnergyScoreTest: 171.51659
INFO:root:CoverageTest: 4e-05
INFO:root:IntervalWidthTest: 0.00026
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 173.82221164, Validation loss: 169.79343428, Gradient norm: 90.11562079
INFO:root:[    2] Training loss: 168.69571696, Validation loss: 167.97161865, Gradient norm: 14.38816671
INFO:root:[    3] Training loss: 167.32799208, Validation loss: 166.98338586, Gradient norm: 17.65005952
INFO:root:[    4] Training loss: 166.56449409, Validation loss: 166.47625652, Gradient norm: 19.27721156
INFO:root:[    5] Training loss: 166.12669678, Validation loss: 166.03456169, Gradient norm: 24.18748177
INFO:root:[    6] Training loss: 165.79485813, Validation loss: 165.84009485, Gradient norm: 28.68929539
INFO:root:[    7] Training loss: 165.51558980, Validation loss: 165.53410741, Gradient norm: 30.76545306
INFO:root:[    8] Training loss: 165.35246908, Validation loss: 165.48199624, Gradient norm: 42.66317060
INFO:root:[    9] Training loss: 165.24395033, Validation loss: 165.37393403, Gradient norm: 71.11308194
INFO:root:[   10] Training loss: 165.20532342, Validation loss: 165.50110265, Gradient norm: 134.15889065
INFO:root:[   11] Training loss: 165.12309787, Validation loss: 165.16855126, Gradient norm: 176.78273665
INFO:root:[   12] Training loss: 165.07740384, Validation loss: 165.68578539, Gradient norm: 231.14981531
INFO:root:[   13] Training loss: 164.93652289, Validation loss: 165.06048771, Gradient norm: 270.38222242
INFO:root:[   14] Training loss: 164.82937127, Validation loss: 164.87095829, Gradient norm: 297.71132276
INFO:root:[   15] Training loss: 164.62119331, Validation loss: 164.66616554, Gradient norm: 324.71697102
INFO:root:[   16] Training loss: 164.31239129, Validation loss: 164.24928364, Gradient norm: 311.76677157
INFO:root:[   17] Training loss: 163.84073900, Validation loss: 163.82789799, Gradient norm: 340.04191863
INFO:root:[   18] Training loss: 163.27796760, Validation loss: 163.14423732, Gradient norm: 291.21827353
INFO:root:[   19] Training loss: 162.84644891, Validation loss: 163.80712409, Gradient norm: 307.10727605
INFO:root:[   20] Training loss: 162.51148397, Validation loss: 162.37868433, Gradient norm: 307.29237042
INFO:root:[   21] Training loss: 162.26768270, Validation loss: 162.28702318, Gradient norm: 316.12222454
INFO:root:[   22] Training loss: 162.00337097, Validation loss: 162.04410647, Gradient norm: 296.09800632
INFO:root:[   23] Training loss: 161.76141486, Validation loss: 161.73810377, Gradient norm: 311.67500558
INFO:root:[   24] Training loss: 161.54226949, Validation loss: 161.73194162, Gradient norm: 287.59530587
INFO:root:[   25] Training loss: 161.29129578, Validation loss: 161.29357161, Gradient norm: 267.65990036
INFO:root:[   26] Training loss: 161.06067729, Validation loss: 161.03118281, Gradient norm: 266.56306996
INFO:root:[   27] Training loss: 160.85255995, Validation loss: 160.82138543, Gradient norm: 264.93072006
INFO:root:[   28] Training loss: 160.68769823, Validation loss: 161.03710456, Gradient norm: 244.96832282
INFO:root:[   29] Training loss: 160.54460429, Validation loss: 160.63086982, Gradient norm: 246.70259523
INFO:root:[   30] Training loss: 160.40205234, Validation loss: 160.47296464, Gradient norm: 229.42576571
INFO:root:[   31] Training loss: 160.24755025, Validation loss: 160.37185348, Gradient norm: 220.96951993
INFO:root:[   32] Training loss: 160.10887505, Validation loss: 160.27441861, Gradient norm: 233.12374370
INFO:root:[   33] Training loss: 159.96366930, Validation loss: 160.07249986, Gradient norm: 223.66870411
INFO:root:[   34] Training loss: 159.83072876, Validation loss: 159.90872701, Gradient norm: 218.69437972
INFO:root:[   35] Training loss: 159.74307061, Validation loss: 159.83245528, Gradient norm: 226.58851402
INFO:root:[   36] Training loss: 159.61084418, Validation loss: 159.57362500, Gradient norm: 238.21269437
INFO:root:[   37] Training loss: 159.46336216, Validation loss: 159.72692068, Gradient norm: 225.15344721
INFO:root:[   38] Training loss: 159.43236294, Validation loss: 159.51847839, Gradient norm: 226.84454480
INFO:root:[   39] Training loss: 159.33823107, Validation loss: 159.38757672, Gradient norm: 220.04908910
INFO:root:[   40] Training loss: 159.25581441, Validation loss: 159.40527665, Gradient norm: 225.34033360
INFO:root:[   41] Training loss: 159.21874213, Validation loss: 159.60720102, Gradient norm: 222.84583033
INFO:root:[   42] Training loss: 159.14645562, Validation loss: 159.27110746, Gradient norm: 221.97157713
INFO:root:[   43] Training loss: 159.10361484, Validation loss: 159.31742886, Gradient norm: 221.45281637
INFO:root:[   44] Training loss: 159.06584554, Validation loss: 159.34667246, Gradient norm: 235.14406250
INFO:root:[   45] Training loss: 158.96095764, Validation loss: 159.05405065, Gradient norm: 214.58846866
INFO:root:[   46] Training loss: 158.89546149, Validation loss: 159.31841158, Gradient norm: 215.31486941
INFO:root:[   47] Training loss: 158.87451701, Validation loss: 159.19361556, Gradient norm: 215.01790220
INFO:root:[   48] Training loss: 158.82884650, Validation loss: 159.13048112, Gradient norm: 214.71772203
INFO:root:[   49] Training loss: 158.82230123, Validation loss: 159.14036694, Gradient norm: 213.92072533
INFO:root:[   50] Training loss: 158.76730367, Validation loss: 159.27604675, Gradient norm: 218.56399704
INFO:root:[   51] Training loss: 158.72448507, Validation loss: 158.93696889, Gradient norm: 217.79938718
INFO:root:[   52] Training loss: 158.67671197, Validation loss: 159.03928121, Gradient norm: 210.94974141
INFO:root:[   53] Training loss: 158.63853563, Validation loss: 158.96452733, Gradient norm: 207.15325402
INFO:root:[   54] Training loss: 158.60763692, Validation loss: 158.87975459, Gradient norm: 223.72078697
INFO:root:[   55] Training loss: 158.54428819, Validation loss: 159.17974720, Gradient norm: 204.98446458
INFO:root:[   56] Training loss: 158.53242967, Validation loss: 158.76459409, Gradient norm: 218.12657810
INFO:root:[   57] Training loss: 158.46542114, Validation loss: 158.98514999, Gradient norm: 213.49935291
INFO:root:[   58] Training loss: 158.43658773, Validation loss: 158.87640943, Gradient norm: 212.62459633
INFO:root:[   59] Training loss: 158.36676426, Validation loss: 158.65904370, Gradient norm: 211.79609238
INFO:root:[   60] Training loss: 158.38833496, Validation loss: 158.73720617, Gradient norm: 210.09371491
INFO:root:[   61] Training loss: 158.31283691, Validation loss: 158.95976632, Gradient norm: 203.85964377
INFO:root:[   62] Training loss: 158.27088386, Validation loss: 158.59563004, Gradient norm: 209.42913228
INFO:root:[   63] Training loss: 158.23441196, Validation loss: 158.58959533, Gradient norm: 203.80769022
INFO:root:[   64] Training loss: 158.21847555, Validation loss: 158.70011072, Gradient norm: 209.23418513
INFO:root:[   65] Training loss: 158.20571743, Validation loss: 158.55898968, Gradient norm: 206.55647634
INFO:root:[   66] Training loss: 158.14808539, Validation loss: 158.44551970, Gradient norm: 209.65360075
INFO:root:[   67] Training loss: 158.12199999, Validation loss: 158.65506276, Gradient norm: 215.81235823
INFO:root:[   68] Training loss: 158.05643365, Validation loss: 158.51341622, Gradient norm: 206.98593885
INFO:root:[   69] Training loss: 158.00054321, Validation loss: 158.26175087, Gradient norm: 206.97837691
INFO:root:[   70] Training loss: 157.97014459, Validation loss: 158.39785472, Gradient norm: 199.53964281
INFO:root:[   71] Training loss: 157.97977593, Validation loss: 158.38286684, Gradient norm: 221.21679107
INFO:root:[   72] Training loss: 157.91307753, Validation loss: 158.38101410, Gradient norm: 209.81834704
INFO:root:[   73] Training loss: 157.84162055, Validation loss: 158.21054880, Gradient norm: 202.97251557
INFO:root:[   74] Training loss: 157.84223185, Validation loss: 158.16373537, Gradient norm: 209.10835089
INFO:root:[   75] Training loss: 157.80031779, Validation loss: 158.27509723, Gradient norm: 201.15549383
INFO:root:[   76] Training loss: 157.81241177, Validation loss: 158.36799381, Gradient norm: 212.26010580
INFO:root:[   77] Training loss: 157.77062052, Validation loss: 158.25931482, Gradient norm: 204.32121245
INFO:root:[   78] Training loss: 157.76363458, Validation loss: 157.95041001, Gradient norm: 214.59555016
INFO:root:[   79] Training loss: 157.73198120, Validation loss: 158.06600845, Gradient norm: 206.19714948
INFO:root:[   80] Training loss: 157.65432956, Validation loss: 158.17265454, Gradient norm: 196.83184124
INFO:root:[   81] Training loss: 157.67861274, Validation loss: 158.10146800, Gradient norm: 208.18294863
INFO:root:[   82] Training loss: 157.62856554, Validation loss: 158.02692213, Gradient norm: 207.39410494
INFO:root:[   83] Training loss: 157.61942708, Validation loss: 158.07644359, Gradient norm: 202.63531004
INFO:root:[   84] Training loss: 157.58040263, Validation loss: 157.99622546, Gradient norm: 196.72599091
INFO:root:[   85] Training loss: 157.52733236, Validation loss: 157.93500211, Gradient norm: 201.46999745
INFO:root:[   86] Training loss: 157.51367004, Validation loss: 157.94112008, Gradient norm: 208.79165356
INFO:root:[   87] Training loss: 157.53432882, Validation loss: 158.11587417, Gradient norm: 212.65525862
INFO:root:[   88] Training loss: 157.47349514, Validation loss: 157.79955118, Gradient norm: 200.29668912
INFO:root:[   89] Training loss: 157.45860338, Validation loss: 157.82328181, Gradient norm: 197.03352194
INFO:root:[   90] Training loss: 157.42011054, Validation loss: 157.95724005, Gradient norm: 209.54072699
INFO:root:[   91] Training loss: 157.42376078, Validation loss: 157.88224284, Gradient norm: 209.51537405
INFO:root:[   92] Training loss: 157.37001933, Validation loss: 157.84970789, Gradient norm: 205.38315127
INFO:root:[   93] Training loss: 157.36760939, Validation loss: 157.81315532, Gradient norm: 206.77620088
INFO:root:[   94] Training loss: 157.33990533, Validation loss: 157.83388238, Gradient norm: 211.51266624
INFO:root:[   95] Training loss: 157.30653395, Validation loss: 157.85620519, Gradient norm: 206.46942128
INFO:root:[   96] Training loss: 157.25329325, Validation loss: 157.68722802, Gradient norm: 196.57537204
INFO:root:[   97] Training loss: 157.28040310, Validation loss: 157.61558185, Gradient norm: 209.56455957
INFO:root:[   98] Training loss: 157.24654446, Validation loss: 157.64488889, Gradient norm: 204.16001784
INFO:root:[   99] Training loss: 157.23094564, Validation loss: 157.55442783, Gradient norm: 204.39528908
INFO:root:[  100] Training loss: 157.22786906, Validation loss: 157.56495479, Gradient norm: 204.62322439
INFO:root:[  101] Training loss: 157.21998711, Validation loss: 157.63187556, Gradient norm: 206.51944144
INFO:root:[  102] Training loss: 157.16439758, Validation loss: 157.54741843, Gradient norm: 202.74313421
INFO:root:[  103] Training loss: 157.19714796, Validation loss: 157.75073135, Gradient norm: 209.44822519
INFO:root:[  104] Training loss: 157.10807400, Validation loss: 157.82429344, Gradient norm: 198.92286659
INFO:root:[  105] Training loss: 157.11596056, Validation loss: 157.54663595, Gradient norm: 205.85869780
INFO:root:[  106] Training loss: 157.12116408, Validation loss: 157.66822119, Gradient norm: 206.73523698
INFO:root:[  107] Training loss: 157.12381816, Validation loss: 157.70025983, Gradient norm: 220.12432461
INFO:root:[  108] Training loss: 157.07819085, Validation loss: 157.54502440, Gradient norm: 200.14542984
INFO:root:[  109] Training loss: 157.09132304, Validation loss: 157.58765585, Gradient norm: 216.77878680
INFO:root:[  110] Training loss: 157.04260491, Validation loss: 157.54941331, Gradient norm: 203.46908028
INFO:root:[  111] Training loss: 157.03739285, Validation loss: 157.50801140, Gradient norm: 205.11045507
INFO:root:[  112] Training loss: 157.02052802, Validation loss: 157.60101961, Gradient norm: 206.09461692
INFO:root:[  113] Training loss: 157.01324361, Validation loss: 157.61240427, Gradient norm: 206.89765910
INFO:root:[  114] Training loss: 157.01641412, Validation loss: 157.61609984, Gradient norm: 207.44579167
INFO:root:[  115] Training loss: 156.98183302, Validation loss: 157.45595698, Gradient norm: 211.95724185
INFO:root:[  116] Training loss: 156.98613505, Validation loss: 157.80321543, Gradient norm: 206.73898193
INFO:root:[  117] Training loss: 156.97650133, Validation loss: 157.47087285, Gradient norm: 209.54558241
INFO:root:[  118] Training loss: 156.90430583, Validation loss: 157.48036140, Gradient norm: 199.53840555
INFO:root:[  119] Training loss: 156.93357449, Validation loss: 157.32493056, Gradient norm: 203.10029117
INFO:root:[  120] Training loss: 156.89442105, Validation loss: 157.51949645, Gradient norm: 194.65397956
INFO:root:[  121] Training loss: 156.91592889, Validation loss: 157.39930404, Gradient norm: 211.40295493
INFO:root:[  122] Training loss: 156.88682102, Validation loss: 157.43201754, Gradient norm: 200.21537628
INFO:root:[  123] Training loss: 156.87424398, Validation loss: 157.51024989, Gradient norm: 209.22733072
INFO:root:[  124] Training loss: 156.87198900, Validation loss: 157.34137257, Gradient norm: 206.52262419
INFO:root:[  125] Training loss: 156.88169278, Validation loss: 157.26337687, Gradient norm: 210.08709315
INFO:root:[  126] Training loss: 156.84964606, Validation loss: 157.71034669, Gradient norm: 199.37600427
INFO:root:[  127] Training loss: 156.81558234, Validation loss: 157.47638314, Gradient norm: 198.13127575
INFO:root:[  128] Training loss: 156.82771308, Validation loss: 157.34021076, Gradient norm: 202.58786513
INFO:root:[  129] Training loss: 156.82093825, Validation loss: 157.38171815, Gradient norm: 207.96684497
INFO:root:[  130] Training loss: 156.78057814, Validation loss: 157.48276239, Gradient norm: 210.67072776
INFO:root:[  131] Training loss: 156.80005893, Validation loss: 157.37562936, Gradient norm: 206.96481115
INFO:root:[  132] Training loss: 156.76941169, Validation loss: 157.36866760, Gradient norm: 198.67347927
INFO:root:[  133] Training loss: 156.75429735, Validation loss: 157.32895513, Gradient norm: 203.67044890
INFO:root:[  134] Training loss: 156.77017354, Validation loss: 157.27201549, Gradient norm: 208.43479067
INFO:root:EP 134: Early stopping
INFO:root:Training the model took 3566.929s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 155.77519
INFO:root:EnergyScoreTrain: 145.10383
INFO:root:CoverageTrain: 0.13693
INFO:root:IntervalWidthTrain: 0.64823
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 156.47451
INFO:root:EnergyScoreValidation: 145.837
INFO:root:CoverageValidation: 0.1348
INFO:root:IntervalWidthValidation: 0.64573
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 156.34095
INFO:root:EnergyScoreTest: 145.71682
INFO:root:CoverageTest: 0.135
INFO:root:IntervalWidthTest: 0.64512
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.48035882, Validation loss: 169.45604425, Gradient norm: 20.39039348
INFO:root:[    2] Training loss: 168.29497830, Validation loss: 167.70103615, Gradient norm: 21.11998477
INFO:root:[    3] Training loss: 167.19693278, Validation loss: 167.00859766, Gradient norm: 29.35684735
INFO:root:[    4] Training loss: 166.66232564, Validation loss: 166.86607682, Gradient norm: 75.18784139
INFO:root:[    5] Training loss: 166.25947835, Validation loss: 166.17942676, Gradient norm: 183.39988551
INFO:root:[    6] Training loss: 165.77931451, Validation loss: 165.86492438, Gradient norm: 252.32948044
INFO:root:[    7] Training loss: 165.27983744, Validation loss: 165.52282661, Gradient norm: 268.55626899
INFO:root:[    8] Training loss: 164.86095093, Validation loss: 164.73623871, Gradient norm: 294.28442190
INFO:root:[    9] Training loss: 164.54488085, Validation loss: 164.48260632, Gradient norm: 302.30412560
INFO:root:[   10] Training loss: 164.27021267, Validation loss: 164.40188786, Gradient norm: 321.98316769
INFO:root:[   11] Training loss: 163.98420288, Validation loss: 164.13037431, Gradient norm: 294.53071021
INFO:root:[   12] Training loss: 163.80764872, Validation loss: 163.93291808, Gradient norm: 303.98581427
INFO:root:[   13] Training loss: 163.64099053, Validation loss: 163.65491231, Gradient norm: 305.47419881
INFO:root:[   14] Training loss: 163.49798821, Validation loss: 163.63151550, Gradient norm: 302.31921737
INFO:root:[   15] Training loss: 163.35417921, Validation loss: 163.41150652, Gradient norm: 290.11986659
INFO:root:[   16] Training loss: 163.21153761, Validation loss: 163.35194290, Gradient norm: 274.16982181
INFO:root:[   17] Training loss: 163.06111749, Validation loss: 162.99540523, Gradient norm: 266.50279035
INFO:root:[   18] Training loss: 162.93233853, Validation loss: 163.17890234, Gradient norm: 252.42565464
INFO:root:[   19] Training loss: 162.82312649, Validation loss: 162.84064069, Gradient norm: 246.74611496
INFO:root:[   20] Training loss: 162.69919569, Validation loss: 162.88533261, Gradient norm: 236.98529072
INFO:root:[   21] Training loss: 162.60032844, Validation loss: 162.73396596, Gradient norm: 216.57823403
INFO:root:[   22] Training loss: 162.45745165, Validation loss: 162.40639295, Gradient norm: 216.21647256
INFO:root:[   23] Training loss: 162.33009291, Validation loss: 162.60326934, Gradient norm: 220.16439622
INFO:root:[   24] Training loss: 162.18197645, Validation loss: 162.19704317, Gradient norm: 221.40430519
INFO:root:[   25] Training loss: 162.04249912, Validation loss: 162.12942532, Gradient norm: 200.09676371
INFO:root:[   26] Training loss: 161.97099460, Validation loss: 161.97823267, Gradient norm: 206.98915446
INFO:root:[   27] Training loss: 161.87002265, Validation loss: 162.06248340, Gradient norm: 195.55353317
INFO:root:[   28] Training loss: 161.79290148, Validation loss: 162.04524606, Gradient norm: 188.63870895
INFO:root:[   29] Training loss: 161.70909241, Validation loss: 161.86392640, Gradient norm: 190.61448697
INFO:root:[   30] Training loss: 161.62803562, Validation loss: 161.57859294, Gradient norm: 187.00552733
INFO:root:[   31] Training loss: 161.54638014, Validation loss: 161.67855166, Gradient norm: 186.18023877
INFO:root:[   32] Training loss: 161.43761149, Validation loss: 161.66856465, Gradient norm: 183.56807852
INFO:root:[   33] Training loss: 161.35958333, Validation loss: 161.49479033, Gradient norm: 174.13515132
INFO:root:[   34] Training loss: 161.32238254, Validation loss: 161.37859572, Gradient norm: 182.63130107
INFO:root:[   35] Training loss: 161.24104302, Validation loss: 161.48214026, Gradient norm: 185.64767886
INFO:root:[   36] Training loss: 161.15956475, Validation loss: 161.38019548, Gradient norm: 182.15022251
INFO:root:[   37] Training loss: 161.09586928, Validation loss: 161.24857344, Gradient norm: 181.98791617
INFO:root:[   38] Training loss: 161.02055956, Validation loss: 161.28146095, Gradient norm: 180.33783753
INFO:root:[   39] Training loss: 160.94275289, Validation loss: 161.07625513, Gradient norm: 183.69902949
INFO:root:[   40] Training loss: 160.87447903, Validation loss: 160.98490317, Gradient norm: 180.63775204
INFO:root:[   41] Training loss: 160.82008613, Validation loss: 161.07768062, Gradient norm: 191.37556924
INFO:root:[   42] Training loss: 160.74534932, Validation loss: 160.97244370, Gradient norm: 183.79768030
INFO:root:[   43] Training loss: 160.69020094, Validation loss: 160.98447110, Gradient norm: 184.63229362
INFO:root:[   44] Training loss: 160.62311544, Validation loss: 160.76976923, Gradient norm: 184.76027174
INFO:root:[   45] Training loss: 160.57716607, Validation loss: 160.79925136, Gradient norm: 184.25940159
INFO:root:[   46] Training loss: 160.51539734, Validation loss: 160.85418942, Gradient norm: 188.74524576
INFO:root:[   47] Training loss: 160.48676236, Validation loss: 160.63655626, Gradient norm: 187.07431609
INFO:root:[   48] Training loss: 160.45161174, Validation loss: 160.60883934, Gradient norm: 193.55564457
INFO:root:[   49] Training loss: 160.40449042, Validation loss: 160.78475738, Gradient norm: 190.42044391
INFO:root:[   50] Training loss: 160.35325989, Validation loss: 160.55462914, Gradient norm: 193.65213569
INFO:root:[   51] Training loss: 160.33346144, Validation loss: 160.46425937, Gradient norm: 188.94561087
INFO:root:[   52] Training loss: 160.27534465, Validation loss: 160.38954537, Gradient norm: 189.53884006
INFO:root:[   53] Training loss: 160.24059665, Validation loss: 160.51335974, Gradient norm: 187.81238782
INFO:root:[   54] Training loss: 160.24662835, Validation loss: 160.32790656, Gradient norm: 196.75704599
INFO:root:[   55] Training loss: 160.21243612, Validation loss: 160.23493877, Gradient norm: 192.69045721
INFO:root:[   56] Training loss: 160.15684591, Validation loss: 160.23736224, Gradient norm: 194.09578503
INFO:root:[   57] Training loss: 160.14333903, Validation loss: 160.64971576, Gradient norm: 197.06101524
INFO:root:[   58] Training loss: 160.10333266, Validation loss: 160.35243145, Gradient norm: 193.88799023
INFO:root:[   59] Training loss: 160.04334113, Validation loss: 160.29121292, Gradient norm: 196.36182050
INFO:root:[   60] Training loss: 160.03860053, Validation loss: 160.33394957, Gradient norm: 190.88307081
INFO:root:[   61] Training loss: 160.02265510, Validation loss: 160.26019608, Gradient norm: 192.18730685
INFO:root:[   62] Training loss: 159.98911038, Validation loss: 160.37868567, Gradient norm: 202.53272021
INFO:root:[   63] Training loss: 159.96619561, Validation loss: 160.23299073, Gradient norm: 201.57728896
INFO:root:[   64] Training loss: 159.98652384, Validation loss: 160.50114146, Gradient norm: 206.51874217
INFO:root:[   65] Training loss: 159.91583713, Validation loss: 160.21045457, Gradient norm: 181.90113695
INFO:root:[   66] Training loss: 159.90580010, Validation loss: 160.22989668, Gradient norm: 197.63313573
INFO:root:[   67] Training loss: 159.86631551, Validation loss: 160.02562031, Gradient norm: 188.51259378
INFO:root:[   68] Training loss: 159.88814338, Validation loss: 160.17119103, Gradient norm: 203.23426354
INFO:root:[   69] Training loss: 159.81716112, Validation loss: 160.06890682, Gradient norm: 189.21748890
INFO:root:[   70] Training loss: 159.81889628, Validation loss: 160.08692397, Gradient norm: 190.29349921
INFO:root:[   71] Training loss: 159.78161167, Validation loss: 160.03965492, Gradient norm: 197.98302446
INFO:root:[   72] Training loss: 159.77243462, Validation loss: 159.95461768, Gradient norm: 199.28079398
INFO:root:[   73] Training loss: 159.71962721, Validation loss: 160.04501423, Gradient norm: 190.81065186
INFO:root:[   74] Training loss: 159.71189067, Validation loss: 159.89565799, Gradient norm: 185.73653712
INFO:root:[   75] Training loss: 159.69928290, Validation loss: 160.03631003, Gradient norm: 194.81575131
INFO:root:[   76] Training loss: 159.70579230, Validation loss: 159.95242176, Gradient norm: 199.09561165
INFO:root:[   77] Training loss: 159.66671733, Validation loss: 160.05705609, Gradient norm: 195.27206314
INFO:root:[   78] Training loss: 159.67884576, Validation loss: 159.80830142, Gradient norm: 189.00052003
INFO:root:[   79] Training loss: 159.63104824, Validation loss: 159.83990318, Gradient norm: 189.20375639
INFO:root:[   80] Training loss: 159.60311673, Validation loss: 159.89846775, Gradient norm: 187.16800325
INFO:root:[   81] Training loss: 159.59543552, Validation loss: 160.16259096, Gradient norm: 194.21478756
INFO:root:[   82] Training loss: 159.58850165, Validation loss: 159.81748561, Gradient norm: 192.57126326
INFO:root:[   83] Training loss: 159.57154846, Validation loss: 159.94038712, Gradient norm: 196.74503890
INFO:root:[   84] Training loss: 159.55260952, Validation loss: 159.98967890, Gradient norm: 194.40021799
INFO:root:[   85] Training loss: 159.50378594, Validation loss: 159.90553391, Gradient norm: 182.51366072
INFO:root:[   86] Training loss: 159.51745721, Validation loss: 159.72209730, Gradient norm: 188.45582823
INFO:root:[   87] Training loss: 159.49373589, Validation loss: 159.75211187, Gradient norm: 188.75591313
INFO:root:[   88] Training loss: 159.45986315, Validation loss: 159.73058038, Gradient norm: 186.41129190
INFO:root:[   89] Training loss: 159.48635837, Validation loss: 159.74580223, Gradient norm: 188.65951343
INFO:root:[   90] Training loss: 159.46312832, Validation loss: 159.60276607, Gradient norm: 198.94605677
INFO:root:[   91] Training loss: 159.40073853, Validation loss: 159.85689371, Gradient norm: 181.21696874
INFO:root:[   92] Training loss: 159.41669522, Validation loss: 159.71398631, Gradient norm: 190.20864951
INFO:root:[   93] Training loss: 159.41710314, Validation loss: 159.73744576, Gradient norm: 194.63804939
INFO:root:[   94] Training loss: 159.38467767, Validation loss: 159.62560527, Gradient norm: 188.90251395
INFO:root:[   95] Training loss: 159.36170064, Validation loss: 159.62736190, Gradient norm: 189.38078590
INFO:root:[   96] Training loss: 159.39697903, Validation loss: 159.74306234, Gradient norm: 193.63210251
INFO:root:[   97] Training loss: 159.36383470, Validation loss: 159.76743464, Gradient norm: 196.35583284
INFO:root:[   98] Training loss: 159.31702243, Validation loss: 159.60132237, Gradient norm: 192.68403901
INFO:root:[   99] Training loss: 159.34237847, Validation loss: 159.66076660, Gradient norm: 195.50911217
INFO:root:[  100] Training loss: 159.32690450, Validation loss: 159.73164903, Gradient norm: 198.46267789
INFO:root:[  101] Training loss: 159.30570319, Validation loss: 159.59636621, Gradient norm: 191.83859166
INFO:root:[  102] Training loss: 159.28713569, Validation loss: 159.87334188, Gradient norm: 192.25458735
INFO:root:[  103] Training loss: 159.31333103, Validation loss: 159.57915564, Gradient norm: 189.71382389
INFO:root:[  104] Training loss: 159.29169061, Validation loss: 159.69072630, Gradient norm: 200.27562494
INFO:root:[  105] Training loss: 159.27327271, Validation loss: 159.52418250, Gradient norm: 193.30501407
INFO:root:[  106] Training loss: 159.23801439, Validation loss: 159.52662097, Gradient norm: 196.32052643
INFO:root:[  107] Training loss: 159.25314541, Validation loss: 159.62020687, Gradient norm: 209.59184148
INFO:root:[  108] Training loss: 159.22142083, Validation loss: 159.48596620, Gradient norm: 202.33484274
INFO:root:[  109] Training loss: 159.21393182, Validation loss: 159.76352839, Gradient norm: 204.00065012
INFO:root:[  110] Training loss: 159.18124301, Validation loss: 159.40887585, Gradient norm: 199.90636593
INFO:root:[  111] Training loss: 159.17873854, Validation loss: 159.41092615, Gradient norm: 203.10873038
INFO:root:[  112] Training loss: 159.18575419, Validation loss: 159.82615688, Gradient norm: 205.74859531
INFO:root:[  113] Training loss: 159.15077922, Validation loss: 159.68713379, Gradient norm: 210.47776284
INFO:root:[  114] Training loss: 159.11141635, Validation loss: 159.54174724, Gradient norm: 205.26173153
INFO:root:[  115] Training loss: 159.14805332, Validation loss: 159.41204245, Gradient norm: 213.76693113
INFO:root:[  116] Training loss: 159.13083096, Validation loss: 159.58868462, Gradient norm: 213.37772737
INFO:root:[  117] Training loss: 159.12921149, Validation loss: 159.42377753, Gradient norm: 218.33063080
INFO:root:[  118] Training loss: 159.05693448, Validation loss: 159.52456933, Gradient norm: 199.61907801
INFO:root:[  119] Training loss: 159.09394999, Validation loss: 159.32456890, Gradient norm: 214.49015080
INFO:root:[  120] Training loss: 159.07535292, Validation loss: 159.35850230, Gradient norm: 207.99133687
INFO:root:[  121] Training loss: 159.05035787, Validation loss: 159.41856277, Gradient norm: 213.23297421
INFO:root:[  122] Training loss: 159.08383626, Validation loss: 159.45178464, Gradient norm: 219.56468041
INFO:root:[  123] Training loss: 159.02031338, Validation loss: 159.39285519, Gradient norm: 208.99908962
INFO:root:[  124] Training loss: 159.01440389, Validation loss: 159.39025584, Gradient norm: 208.76854311
INFO:root:[  125] Training loss: 159.02342068, Validation loss: 159.51163469, Gradient norm: 215.87166535
INFO:root:[  126] Training loss: 159.00036797, Validation loss: 159.51654160, Gradient norm: 212.88267723
INFO:root:[  127] Training loss: 158.99016140, Validation loss: 159.45566251, Gradient norm: 210.33056633
INFO:root:[  128] Training loss: 159.00560574, Validation loss: 159.33240710, Gradient norm: 211.62788089
INFO:root:[  129] Training loss: 158.96939684, Validation loss: 159.28856913, Gradient norm: 207.23412478
INFO:root:[  130] Training loss: 158.93926629, Validation loss: 159.31639179, Gradient norm: 201.67320661
INFO:root:[  131] Training loss: 158.95418586, Validation loss: 159.40723754, Gradient norm: 213.90009928
INFO:root:[  132] Training loss: 158.93984382, Validation loss: 159.26448032, Gradient norm: 203.71748407
INFO:root:[  133] Training loss: 158.92979187, Validation loss: 159.35342782, Gradient norm: 211.53349915
INFO:root:[  134] Training loss: 158.95482130, Validation loss: 159.26486929, Gradient norm: 208.67096290
INFO:root:[  135] Training loss: 158.90014208, Validation loss: 159.28188498, Gradient norm: 201.87172411
INFO:root:[  136] Training loss: 158.92788418, Validation loss: 159.25846354, Gradient norm: 206.50694435
INFO:root:[  137] Training loss: 158.88301819, Validation loss: 159.37230053, Gradient norm: 205.15452738
INFO:root:[  138] Training loss: 158.90757351, Validation loss: 159.23739731, Gradient norm: 210.89737784
INFO:root:[  139] Training loss: 158.89876587, Validation loss: 159.21289464, Gradient norm: 202.89572443
INFO:root:[  140] Training loss: 158.86255337, Validation loss: 159.07843633, Gradient norm: 202.08661620
INFO:root:[  141] Training loss: 158.83615295, Validation loss: 159.24594759, Gradient norm: 209.95635533
INFO:root:[  142] Training loss: 158.85372158, Validation loss: 159.47964638, Gradient norm: 213.00094780
INFO:root:[  143] Training loss: 158.85867622, Validation loss: 159.23596834, Gradient norm: 202.67425028
INFO:root:[  144] Training loss: 158.83777134, Validation loss: 159.14446165, Gradient norm: 217.42094469
INFO:root:[  145] Training loss: 158.79960558, Validation loss: 159.08569041, Gradient norm: 201.67503703
INFO:root:[  146] Training loss: 158.80599609, Validation loss: 159.15884855, Gradient norm: 205.59544453
INFO:root:[  147] Training loss: 158.81527791, Validation loss: 159.51014656, Gradient norm: 213.29182651
INFO:root:[  148] Training loss: 158.82046909, Validation loss: 159.15616568, Gradient norm: 211.70153783
INFO:root:[  149] Training loss: 158.79988797, Validation loss: 159.12861018, Gradient norm: 206.74359320
INFO:root:EP 149: Early stopping
INFO:root:Training the model took 3959.83s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 156.84428
INFO:root:EnergyScoreTrain: 142.13319
INFO:root:CoverageTrain: 0.18882
INFO:root:IntervalWidthTrain: 1.01423
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 157.37491
INFO:root:EnergyScoreValidation: 142.71589
INFO:root:CoverageValidation: 0.18685
INFO:root:IntervalWidthValidation: 1.01135
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 157.33899
INFO:root:EnergyScoreTest: 142.71083
INFO:root:CoverageTest: 0.18617
INFO:root:IntervalWidthTest: 1.00688
INFO:root:###21 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 385.74905063, Validation loss: 171.51288859, Gradient norm: 38145.30760382
INFO:root:[    2] Training loss: 171.45578071, Validation loss: 171.50644590, Gradient norm: 0.11374919
INFO:root:[    3] Training loss: 171.45577630, Validation loss: 171.50789495, Gradient norm: 0.12267692
INFO:root:[    4] Training loss: 171.45578742, Validation loss: 171.50843088, Gradient norm: 0.12300698
INFO:root:[    5] Training loss: 171.45579251, Validation loss: 171.48495002, Gradient norm: 0.12816171
INFO:root:[    6] Training loss: 171.45578708, Validation loss: 171.45176563, Gradient norm: 0.12278916
INFO:root:[    7] Training loss: 171.45579183, Validation loss: 171.48541019, Gradient norm: 0.11516828
INFO:root:[    8] Training loss: 171.45579407, Validation loss: 171.49735809, Gradient norm: 0.12178348
INFO:root:[    9] Training loss: 171.45581984, Validation loss: 171.48758845, Gradient norm: 0.12153185
INFO:root:[   10] Training loss: 171.45581170, Validation loss: 171.46537513, Gradient norm: 0.12005345
INFO:root:[   11] Training loss: 171.45581184, Validation loss: 171.44294766, Gradient norm: 0.11547687
INFO:root:[   12] Training loss: 171.45581896, Validation loss: 171.45911073, Gradient norm: 0.12685109
INFO:root:[   13] Training loss: 171.45580919, Validation loss: 171.48278969, Gradient norm: 0.12127640
INFO:root:[   14] Training loss: 171.45584229, Validation loss: 171.50751241, Gradient norm: 0.12492891
INFO:root:[   15] Training loss: 171.45581902, Validation loss: 171.46714381, Gradient norm: 0.11847044
INFO:root:[   16] Training loss: 171.45585137, Validation loss: 171.53011576, Gradient norm: 0.13311140
INFO:root:[   17] Training loss: 171.45584100, Validation loss: 171.46986255, Gradient norm: 0.13161610
INFO:root:[   18] Training loss: 171.45584561, Validation loss: 171.51204936, Gradient norm: 0.13155493
INFO:root:[   19] Training loss: 171.45582167, Validation loss: 171.43953638, Gradient norm: 0.11989296
INFO:root:[   20] Training loss: 171.45581997, Validation loss: 171.49338304, Gradient norm: 0.12501530
INFO:root:[   21] Training loss: 171.45583082, Validation loss: 171.50516390, Gradient norm: 0.12417993
INFO:root:[   22] Training loss: 171.45587857, Validation loss: 171.47483156, Gradient norm: 0.14249629
INFO:root:[   23] Training loss: 171.45584229, Validation loss: 171.49068411, Gradient norm: 0.12801039
INFO:root:[   24] Training loss: 171.45585843, Validation loss: 171.50778118, Gradient norm: 0.13181439
INFO:root:[   25] Training loss: 171.45582357, Validation loss: 171.52914937, Gradient norm: 0.13003233
INFO:root:[   26] Training loss: 171.45587504, Validation loss: 171.47644552, Gradient norm: 0.13972626
INFO:root:[   27] Training loss: 171.45583042, Validation loss: 171.49653706, Gradient norm: 0.12055529
INFO:root:[   28] Training loss: 171.45585782, Validation loss: 171.48082131, Gradient norm: 0.12522786
INFO:root:[   29] Training loss: 171.45581007, Validation loss: 171.50605265, Gradient norm: 0.12044593
INFO:root:[   30] Training loss: 171.45584649, Validation loss: 171.48928271, Gradient norm: 0.12606906
INFO:root:[   31] Training loss: 171.45583110, Validation loss: 171.52401198, Gradient norm: 0.12079632
INFO:root:[   32] Training loss: 171.45585754, Validation loss: 171.49230555, Gradient norm: 0.11028517
INFO:root:[   33] Training loss: 171.45589071, Validation loss: 171.48793886, Gradient norm: 0.13709511
INFO:root:[   34] Training loss: 171.45582642, Validation loss: 171.49633548, Gradient norm: 0.13562644
INFO:root:[   35] Training loss: 171.45581713, Validation loss: 171.49977728, Gradient norm: 0.12537729
INFO:root:[   36] Training loss: 171.45580688, Validation loss: 171.48164100, Gradient norm: 0.11771817
INFO:root:[   37] Training loss: 171.45585700, Validation loss: 171.48656932, Gradient norm: 0.13061359
INFO:root:[   38] Training loss: 171.45586195, Validation loss: 171.49679994, Gradient norm: 0.13203159
INFO:root:[   39] Training loss: 171.45583008, Validation loss: 171.46013601, Gradient norm: 0.12022487
INFO:root:[   40] Training loss: 171.45589389, Validation loss: 171.48246765, Gradient norm: 0.13371132
INFO:root:[   41] Training loss: 171.45581828, Validation loss: 171.50702359, Gradient norm: 0.12456152
INFO:root:[   42] Training loss: 171.45583747, Validation loss: 171.49700633, Gradient norm: 0.13222771
INFO:root:[   43] Training loss: 171.45585795, Validation loss: 171.47372115, Gradient norm: 0.13419380
INFO:root:[   44] Training loss: 171.45582560, Validation loss: 171.51835525, Gradient norm: 0.12005698
INFO:root:[   45] Training loss: 171.45582676, Validation loss: 171.48232363, Gradient norm: 0.12581720
INFO:root:[   46] Training loss: 171.45584819, Validation loss: 171.50173736, Gradient norm: 0.11669788
INFO:root:[   47] Training loss: 171.45585368, Validation loss: 171.53755188, Gradient norm: 0.12473735
INFO:root:[   48] Training loss: 171.45586548, Validation loss: 171.47521544, Gradient norm: 0.13353753
INFO:root:[   49] Training loss: 171.45581434, Validation loss: 171.47189278, Gradient norm: 0.11745683
INFO:root:[   50] Training loss: 171.45586589, Validation loss: 171.49083830, Gradient norm: 0.13241777
INFO:root:[   51] Training loss: 171.45583469, Validation loss: 171.46458462, Gradient norm: 0.12632319
INFO:root:[   52] Training loss: 171.45581000, Validation loss: 171.49356186, Gradient norm: 0.12388097
INFO:root:[   53] Training loss: 171.45585239, Validation loss: 171.47178463, Gradient norm: 0.12890472
INFO:root:[   54] Training loss: 171.45585727, Validation loss: 171.47682832, Gradient norm: 0.12589851
INFO:root:[   55] Training loss: 171.45585442, Validation loss: 171.52791047, Gradient norm: 0.13691761
INFO:root:[   56] Training loss: 171.45583767, Validation loss: 171.47343177, Gradient norm: 0.12821582
INFO:root:[   57] Training loss: 171.45584418, Validation loss: 171.49904459, Gradient norm: 0.12648343
INFO:root:[   58] Training loss: 171.45582255, Validation loss: 171.47555033, Gradient norm: 0.12791749
INFO:root:[   59] Training loss: 171.45585653, Validation loss: 171.48671923, Gradient norm: 0.13088359
INFO:root:[   60] Training loss: 171.45585985, Validation loss: 171.53713052, Gradient norm: 0.13206206
INFO:root:[   61] Training loss: 171.45586121, Validation loss: 171.52134919, Gradient norm: 0.12879629
INFO:root:[   62] Training loss: 171.45586012, Validation loss: 171.50913921, Gradient norm: 0.12585276
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1574.683s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.64152
INFO:root:EnergyScoreTrain: 171.45518
INFO:root:CoverageTrain: 0.0
INFO:root:IntervalWidthTrain: 0.0
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.48713
INFO:root:EnergyScoreValidation: 171.48674
INFO:root:CoverageValidation: 0.0
INFO:root:IntervalWidthValidation: 0.0
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.52222
INFO:root:EnergyScoreTest: 171.52184
INFO:root:CoverageTest: 0.0
INFO:root:IntervalWidthTest: 0.0
INFO:root:###22 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 173.69018697, Validation loss: 171.49016825, Gradient norm: 34.83542623
INFO:root:[    2] Training loss: 171.46591058, Validation loss: 171.51805543, Gradient norm: 9.76384308
INFO:root:[    3] Training loss: 171.45861003, Validation loss: 171.51232107, Gradient norm: 9.14775410
INFO:root:[    4] Training loss: 171.45696818, Validation loss: 171.49741190, Gradient norm: 5.78554074
INFO:root:[    5] Training loss: 171.45650811, Validation loss: 171.53588385, Gradient norm: 3.16456530
INFO:root:[    6] Training loss: 171.45634379, Validation loss: 171.50761199, Gradient norm: 1.80454171
INFO:root:[    7] Training loss: 171.45594503, Validation loss: 171.50720215, Gradient norm: 0.74863648
INFO:root:[    8] Training loss: 9884.45050463, Validation loss: 171.44732050, Gradient norm: 2400477.88193699
INFO:root:[    9] Training loss: 171.45580017, Validation loss: 171.47900257, Gradient norm: 0.11622034
INFO:root:[   10] Training loss: 171.45581597, Validation loss: 171.45778589, Gradient norm: 0.13009573
INFO:root:[   11] Training loss: 171.45581746, Validation loss: 171.46758498, Gradient norm: 0.13015516
INFO:root:[   12] Training loss: 171.45580146, Validation loss: 171.49196959, Gradient norm: 0.12397769
INFO:root:[   13] Training loss: 171.45582933, Validation loss: 171.47739584, Gradient norm: 0.11992268
INFO:root:[   14] Training loss: 171.45582852, Validation loss: 171.52227703, Gradient norm: 0.12014412
INFO:root:[   15] Training loss: 171.45581238, Validation loss: 171.46482073, Gradient norm: 0.11737116
INFO:root:[   16] Training loss: 171.45582201, Validation loss: 171.46231882, Gradient norm: 0.11597711
INFO:root:[   17] Training loss: 171.45583462, Validation loss: 171.48812759, Gradient norm: 0.12959499
INFO:root:[   18] Training loss: 171.45585856, Validation loss: 171.47503903, Gradient norm: 0.13705787
INFO:root:[   19] Training loss: 171.45585103, Validation loss: 171.48140195, Gradient norm: 0.12083897
INFO:root:[   20] Training loss: 171.45586419, Validation loss: 171.45643509, Gradient norm: 0.13762515
INFO:root:[   21] Training loss: 171.45586304, Validation loss: 171.46698908, Gradient norm: 0.12315490
INFO:root:[   22] Training loss: 171.45583761, Validation loss: 171.44385114, Gradient norm: 0.13229779
INFO:root:[   23] Training loss: 171.45585388, Validation loss: 171.43830309, Gradient norm: 0.12987936
INFO:root:[   24] Training loss: 171.45583971, Validation loss: 171.48069629, Gradient norm: 0.12369853
INFO:root:[   25] Training loss: 171.45583625, Validation loss: 171.52197694, Gradient norm: 0.11916642
INFO:root:[   26] Training loss: 171.45587294, Validation loss: 171.45209035, Gradient norm: 0.12676623
INFO:root:[   27] Training loss: 171.45581407, Validation loss: 171.48028725, Gradient norm: 0.12624133
INFO:root:[   28] Training loss: 171.45583028, Validation loss: 171.48746236, Gradient norm: 0.12969968
INFO:root:[   29] Training loss: 171.45585171, Validation loss: 171.47197603, Gradient norm: 0.12126758
INFO:root:[   30] Training loss: 171.45585375, Validation loss: 171.47539480, Gradient norm: 0.12797870
INFO:root:[   31] Training loss: 171.45583374, Validation loss: 171.44996027, Gradient norm: 0.11702070
INFO:root:[   32] Training loss: 171.45583530, Validation loss: 171.47014578, Gradient norm: 0.11011357
INFO:root:[   33] Training loss: 171.45586060, Validation loss: 171.49146472, Gradient norm: 0.13537225
INFO:root:[   34] Training loss: 171.45584351, Validation loss: 171.52810187, Gradient norm: 0.12744755
INFO:root:[   35] Training loss: 171.45582689, Validation loss: 171.44808585, Gradient norm: 0.11230109
INFO:root:[   36] Training loss: 171.45587328, Validation loss: 171.53813760, Gradient norm: 0.13616191
INFO:root:[   37] Training loss: 171.45582669, Validation loss: 171.49959685, Gradient norm: 0.12674851
INFO:root:[   38] Training loss: 171.45584907, Validation loss: 171.45238000, Gradient norm: 0.12744671
INFO:root:[   39] Training loss: 171.45582757, Validation loss: 171.42784975, Gradient norm: 0.11690812
INFO:root:[   40] Training loss: 171.45583089, Validation loss: 171.49754494, Gradient norm: 0.12019414
INFO:root:[   41] Training loss: 171.45589600, Validation loss: 171.47573049, Gradient norm: 0.13802235
INFO:root:[   42] Training loss: 171.45586968, Validation loss: 171.45661391, Gradient norm: 0.12798123
INFO:root:[   43] Training loss: 171.45583733, Validation loss: 171.44274019, Gradient norm: 0.11862261
INFO:root:[   44] Training loss: 171.45582994, Validation loss: 171.45711852, Gradient norm: 0.12669398
INFO:root:[   45] Training loss: 171.45589572, Validation loss: 171.52180829, Gradient norm: 0.12997833
INFO:root:[   46] Training loss: 171.45584181, Validation loss: 171.54128761, Gradient norm: 0.12969157
INFO:root:[   47] Training loss: 171.45586690, Validation loss: 171.51618342, Gradient norm: 0.13143917
INFO:root:[   48] Training loss: 171.45586609, Validation loss: 171.44219034, Gradient norm: 0.12758373
INFO:root:[   49] Training loss: 171.45585666, Validation loss: 171.47073257, Gradient norm: 0.13005652
INFO:root:[   50] Training loss: 171.45584791, Validation loss: 171.47347567, Gradient norm: 0.12482317
INFO:root:[   51] Training loss: 171.45586778, Validation loss: 171.46780904, Gradient norm: 0.13421044
INFO:root:[   52] Training loss: 171.45584147, Validation loss: 171.46643843, Gradient norm: 0.12119063
INFO:root:[   53] Training loss: 171.45585931, Validation loss: 171.45817164, Gradient norm: 0.13033178
INFO:root:[   54] Training loss: 171.45584913, Validation loss: 171.49458795, Gradient norm: 0.13308549
INFO:root:[   55] Training loss: 171.45583191, Validation loss: 171.50972574, Gradient norm: 0.11238690
INFO:root:[   56] Training loss: 171.45581767, Validation loss: 171.49982064, Gradient norm: 0.12560018
INFO:root:[   57] Training loss: 171.45582289, Validation loss: 171.50645233, Gradient norm: 0.12418299
INFO:root:[   58] Training loss: 171.45585164, Validation loss: 171.50025967, Gradient norm: 0.12620846
INFO:root:[   59] Training loss: 171.45584988, Validation loss: 171.44762862, Gradient norm: 0.12949863
INFO:root:[   60] Training loss: 171.45587206, Validation loss: 171.49767638, Gradient norm: 0.13587858
INFO:root:[   61] Training loss: 171.45588196, Validation loss: 171.43886472, Gradient norm: 0.13771692
INFO:root:[   62] Training loss: 171.45583211, Validation loss: 171.48597450, Gradient norm: 0.12310087
INFO:root:[   63] Training loss: 171.45581000, Validation loss: 171.49547323, Gradient norm: 0.11240551
INFO:root:[   64] Training loss: 171.45581428, Validation loss: 171.44583264, Gradient norm: 0.12305924
INFO:root:[   65] Training loss: 171.45588474, Validation loss: 171.49393503, Gradient norm: 0.13669859
INFO:root:[   66] Training loss: 171.45585537, Validation loss: 171.47588174, Gradient norm: 0.12820883
INFO:root:[   67] Training loss: 171.45582513, Validation loss: 171.47388365, Gradient norm: 0.12583915
INFO:root:[   68] Training loss: 171.45581950, Validation loss: 171.49976576, Gradient norm: 0.11329887
INFO:root:[   69] Training loss: 171.45584961, Validation loss: 171.45253928, Gradient norm: 0.12615487
INFO:root:[   70] Training loss: 171.45584039, Validation loss: 171.51400275, Gradient norm: 0.12683756
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1789.233s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.45577
INFO:root:EnergyScoreTrain: 171.45554
INFO:root:CoverageTrain: 0.0
INFO:root:IntervalWidthTrain: 0.0
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.48715
INFO:root:EnergyScoreValidation: 171.48692
INFO:root:CoverageValidation: 0.0
INFO:root:IntervalWidthValidation: 0.0
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.5222
INFO:root:EnergyScoreTest: 171.52197
INFO:root:CoverageTest: 0.0
INFO:root:IntervalWidthTest: 0.0
INFO:root:###23 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 174.59790894, Validation loss: 172.06175741, Gradient norm: 117.95325909
INFO:root:[    2] Training loss: 207.81438911, Validation loss: 4498.61138220, Gradient norm: 33088.67043516
INFO:root:[    3] Training loss: 439.21300958, Validation loss: 171.51212485, Gradient norm: 83401.31935290
INFO:root:[    4] Training loss: 171.45577616, Validation loss: 171.49269425, Gradient norm: 0.12472128
INFO:root:[    5] Training loss: 171.45580587, Validation loss: 171.47831084, Gradient norm: 0.12999985
INFO:root:[    6] Training loss: 171.45580302, Validation loss: 171.46570627, Gradient norm: 0.12505502
INFO:root:[    7] Training loss: 171.45579698, Validation loss: 171.44895614, Gradient norm: 0.12924841
INFO:root:[    8] Training loss: 171.45583184, Validation loss: 171.50401815, Gradient norm: 0.13215424
INFO:root:[    9] Training loss: 171.45581828, Validation loss: 171.48938979, Gradient norm: 0.12561907
INFO:root:[   10] Training loss: 171.45582499, Validation loss: 171.45315846, Gradient norm: 0.11863010
INFO:root:[   11] Training loss: 171.45582865, Validation loss: 171.48982854, Gradient norm: 0.12549279
INFO:root:[   12] Training loss: 171.45582079, Validation loss: 171.49433792, Gradient norm: 0.12182483
INFO:root:[   13] Training loss: 171.45583272, Validation loss: 171.45368074, Gradient norm: 0.12449710
INFO:root:[   14] Training loss: 171.45581163, Validation loss: 171.47487011, Gradient norm: 0.12348663
INFO:root:[   15] Training loss: 171.45586365, Validation loss: 171.44975388, Gradient norm: 0.12331317
INFO:root:[   16] Training loss: 171.45582920, Validation loss: 171.52495347, Gradient norm: 0.12678647
INFO:root:[   17] Training loss: 171.45586155, Validation loss: 171.50443710, Gradient norm: 0.12139774
INFO:root:[   18] Training loss: 171.45586582, Validation loss: 171.46087700, Gradient norm: 0.13377937
INFO:root:[   19] Training loss: 171.45584425, Validation loss: 171.49729946, Gradient norm: 0.12050785
INFO:root:[   20] Training loss: 171.45587857, Validation loss: 171.51871664, Gradient norm: 0.13935785
INFO:root:[   21] Training loss: 171.45583055, Validation loss: 171.49352197, Gradient norm: 0.12524900
INFO:root:[   22] Training loss: 171.45583598, Validation loss: 171.49981127, Gradient norm: 0.12169031
INFO:root:[   23] Training loss: 171.45584513, Validation loss: 171.47191767, Gradient norm: 0.13153100
INFO:root:[   24] Training loss: 171.45584351, Validation loss: 171.47374846, Gradient norm: 0.13062856
INFO:root:[   25] Training loss: 171.45584764, Validation loss: 171.48103921, Gradient norm: 0.12672796
INFO:root:[   26] Training loss: 171.45583394, Validation loss: 171.45359963, Gradient norm: 0.12798088
INFO:root:[   27] Training loss: 171.45584635, Validation loss: 171.53456035, Gradient norm: 0.13485119
INFO:root:[   28] Training loss: 171.45585870, Validation loss: 171.49025981, Gradient norm: 0.13746840
INFO:root:[   29] Training loss: 171.45581930, Validation loss: 171.45364808, Gradient norm: 0.11586535
INFO:root:[   30] Training loss: 171.45583394, Validation loss: 171.46301698, Gradient norm: 0.11826238
INFO:root:[   31] Training loss: 171.45585910, Validation loss: 171.49289074, Gradient norm: 0.12908602
INFO:root:[   32] Training loss: 171.45582601, Validation loss: 171.51612104, Gradient norm: 0.12503301
INFO:root:[   33] Training loss: 171.45584493, Validation loss: 171.47594198, Gradient norm: 0.13996642
INFO:root:[   34] Training loss: 171.45586026, Validation loss: 171.46523379, Gradient norm: 0.13187509
INFO:root:[   35] Training loss: 171.45587545, Validation loss: 171.47681440, Gradient norm: 0.13212171
INFO:root:[   36] Training loss: 171.45585232, Validation loss: 171.49208122, Gradient norm: 0.15097805
INFO:root:[   37] Training loss: 171.45584134, Validation loss: 171.47555676, Gradient norm: 0.12467967
INFO:root:[   38] Training loss: 171.45583062, Validation loss: 171.44426125, Gradient norm: 0.13332563
INFO:root:[   39] Training loss: 171.45581862, Validation loss: 171.49394333, Gradient norm: 0.12667869
INFO:root:[   40] Training loss: 171.45588786, Validation loss: 171.50230086, Gradient norm: 0.12967959
INFO:root:[   41] Training loss: 171.45587992, Validation loss: 171.50359117, Gradient norm: 0.13191567
INFO:root:[   42] Training loss: 171.45584798, Validation loss: 171.46271903, Gradient norm: 0.13184473
INFO:root:[   43] Training loss: 171.45587592, Validation loss: 171.43397468, Gradient norm: 0.13339237
INFO:root:[   44] Training loss: 171.45583808, Validation loss: 171.44909936, Gradient norm: 0.11908303
INFO:root:[   45] Training loss: 171.45582079, Validation loss: 171.46048991, Gradient norm: 0.12458789
INFO:root:[   46] Training loss: 171.45583082, Validation loss: 171.49185181, Gradient norm: 0.11949983
INFO:root:[   47] Training loss: 171.45582899, Validation loss: 171.47792214, Gradient norm: 0.12123930
INFO:root:[   48] Training loss: 171.45587260, Validation loss: 171.49960515, Gradient norm: 0.13679578
INFO:root:[   49] Training loss: 171.45583123, Validation loss: 171.48079159, Gradient norm: 0.13079216
INFO:root:[   50] Training loss: 171.45585151, Validation loss: 171.49731927, Gradient norm: 0.12557552
INFO:root:[   51] Training loss: 171.45584235, Validation loss: 171.47824364, Gradient norm: 0.12449797
INFO:root:[   52] Training loss: 171.45582452, Validation loss: 171.49872254, Gradient norm: 0.12137912
INFO:root:[   53] Training loss: 171.45584744, Validation loss: 171.49335600, Gradient norm: 0.12716782
INFO:root:[   54] Training loss: 171.45585259, Validation loss: 171.49697180, Gradient norm: 0.12917859
INFO:root:[   55] Training loss: 171.45586066, Validation loss: 171.47382208, Gradient norm: 0.13219873
INFO:root:[   56] Training loss: 171.45582845, Validation loss: 171.45328508, Gradient norm: 0.12241655
INFO:root:[   57] Training loss: 171.45584418, Validation loss: 171.50331946, Gradient norm: 0.12680255
INFO:root:[   58] Training loss: 171.45584602, Validation loss: 171.52623869, Gradient norm: 0.11970749
INFO:root:[   59] Training loss: 171.45584188, Validation loss: 171.49187295, Gradient norm: 0.12466241
INFO:root:[   60] Training loss: 171.45585490, Validation loss: 171.54512131, Gradient norm: 0.12629883
INFO:root:[   61] Training loss: 171.45584534, Validation loss: 171.46036757, Gradient norm: 0.13558104
INFO:root:[   62] Training loss: 171.45585978, Validation loss: 171.51316111, Gradient norm: 0.13320188
INFO:root:[   63] Training loss: 171.45582431, Validation loss: 171.47391229, Gradient norm: 0.12098497
INFO:root:[   64] Training loss: 171.45585490, Validation loss: 171.48487319, Gradient norm: 0.12957584
INFO:root:[   65] Training loss: 171.45586480, Validation loss: 171.51545555, Gradient norm: 0.13137340
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1593.71s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.45587
INFO:root:EnergyScoreTrain: 171.45535
INFO:root:CoverageTrain: 0.0
INFO:root:IntervalWidthTrain: 0.0
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.48717
INFO:root:EnergyScoreValidation: 171.48664
INFO:root:CoverageValidation: 0.0
INFO:root:IntervalWidthValidation: 0.0
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.52229
INFO:root:EnergyScoreTest: 171.52176
INFO:root:CoverageTest: 0.0
INFO:root:IntervalWidthTest: 0.0
INFO:root:###24 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 281.68434814, Validation loss: 2418.85868728, Gradient norm: 25353.49756921
INFO:root:[    2] Training loss: 171.45578274, Validation loss: 171.49654777, Gradient norm: 0.11757216
INFO:root:[    3] Training loss: 14579.36336060, Validation loss: 171.49377334, Gradient norm: 2844665.52450528
INFO:root:[    4] Training loss: 21409.48016934, Validation loss: 171.50159843, Gradient norm: 7535666.43018169
INFO:root:[    5] Training loss: 171.45578552, Validation loss: 171.48808235, Gradient norm: 0.12778130
INFO:root:[    6] Training loss: 171.45577135, Validation loss: 171.45936156, Gradient norm: 0.12014179
INFO:root:[    7] Training loss: 171.45578423, Validation loss: 171.45734687, Gradient norm: 0.12768389
INFO:root:[    8] Training loss: 171.45579298, Validation loss: 171.52456451, Gradient norm: 0.11788193
INFO:root:[    9] Training loss: 171.45579427, Validation loss: 171.48896227, Gradient norm: 0.12385865
INFO:root:[   10] Training loss: 171.45580187, Validation loss: 171.54628740, Gradient norm: 0.13168371
INFO:root:[   11] Training loss: 171.45578708, Validation loss: 171.49302111, Gradient norm: 0.11099311
INFO:root:[   12] Training loss: 171.45580797, Validation loss: 171.49954224, Gradient norm: 0.13159325
INFO:root:[   13] Training loss: 171.45582357, Validation loss: 171.44186080, Gradient norm: 0.12696949
INFO:root:[   14] Training loss: 171.45581794, Validation loss: 171.48420421, Gradient norm: 0.11799109
INFO:root:[   15] Training loss: 171.45582275, Validation loss: 171.47443725, Gradient norm: 0.12575527
INFO:root:[   16] Training loss: 171.45583116, Validation loss: 171.46516526, Gradient norm: 0.12180470
INFO:root:[   17] Training loss: 171.45583340, Validation loss: 171.43400119, Gradient norm: 0.12961869
INFO:root:[   18] Training loss: 171.45585687, Validation loss: 171.50772255, Gradient norm: 0.12814317
INFO:root:[   19] Training loss: 171.45584608, Validation loss: 171.49521061, Gradient norm: 0.11807376
INFO:root:[   20] Training loss: 171.45582635, Validation loss: 171.48304856, Gradient norm: 0.11816428
INFO:root:[   21] Training loss: 171.45584615, Validation loss: 171.48618491, Gradient norm: 0.13116535
INFO:root:[   22] Training loss: 171.45582031, Validation loss: 171.47545691, Gradient norm: 0.11396883
INFO:root:[   23] Training loss: 171.45583293, Validation loss: 171.53011683, Gradient norm: 0.12577471
INFO:root:[   24] Training loss: 171.45584757, Validation loss: 171.48882066, Gradient norm: 0.12368123
INFO:root:[   25] Training loss: 171.45585144, Validation loss: 171.43394952, Gradient norm: 0.13042669
INFO:root:[   26] Training loss: 171.45585022, Validation loss: 171.46257688, Gradient norm: 0.12022109
INFO:root:[   27] Training loss: 171.45587524, Validation loss: 171.47618960, Gradient norm: 0.13742793
INFO:root:[   28] Training loss: 171.45587857, Validation loss: 171.48761227, Gradient norm: 0.12197040
INFO:root:[   29] Training loss: 171.45583686, Validation loss: 171.50765536, Gradient norm: 0.12421714
INFO:root:[   30] Training loss: 171.45581577, Validation loss: 171.50317356, Gradient norm: 0.12590855
INFO:root:[   31] Training loss: 171.45584975, Validation loss: 171.49490972, Gradient norm: 0.13339767
INFO:root:[   32] Training loss: 171.45585178, Validation loss: 171.46535398, Gradient norm: 0.12964295
INFO:root:[   33] Training loss: 171.45583510, Validation loss: 171.48171274, Gradient norm: 0.13147111
INFO:root:[   34] Training loss: 171.45582628, Validation loss: 171.45648675, Gradient norm: 0.12297743
INFO:root:[   35] Training loss: 171.45583794, Validation loss: 171.51585629, Gradient norm: 0.12941543
INFO:root:[   36] Training loss: 171.45583618, Validation loss: 171.47547538, Gradient norm: 0.12648170
INFO:root:[   37] Training loss: 171.45581706, Validation loss: 171.49829530, Gradient norm: 0.12293884
INFO:root:[   38] Training loss: 171.45584269, Validation loss: 171.49199369, Gradient norm: 0.12621844
INFO:root:[   39] Training loss: 171.45582533, Validation loss: 171.49323206, Gradient norm: 0.11837374
INFO:root:[   40] Training loss: 171.45583130, Validation loss: 171.44375343, Gradient norm: 0.12271943
INFO:root:[   41] Training loss: 171.45588725, Validation loss: 171.51482860, Gradient norm: 0.13882697
INFO:root:[   42] Training loss: 171.45584595, Validation loss: 171.48454740, Gradient norm: 0.11939331
INFO:root:[   43] Training loss: 171.45583679, Validation loss: 171.55779253, Gradient norm: 0.12483992
INFO:root:[   44] Training loss: 171.45584242, Validation loss: 171.48861320, Gradient norm: 0.12794653
INFO:root:[   45] Training loss: 171.45582289, Validation loss: 171.50107775, Gradient norm: 0.12073301
INFO:root:[   46] Training loss: 171.45586297, Validation loss: 171.50187790, Gradient norm: 0.12585381
INFO:root:[   47] Training loss: 171.45584683, Validation loss: 171.48986843, Gradient norm: 0.12300544
INFO:root:[   48] Training loss: 171.45587830, Validation loss: 171.49204508, Gradient norm: 0.14166673
INFO:root:[   49] Training loss: 171.45585225, Validation loss: 171.52466329, Gradient norm: 0.12278311
INFO:root:[   50] Training loss: 171.45584507, Validation loss: 171.51938402, Gradient norm: 0.13130209
INFO:root:[   51] Training loss: 171.45584540, Validation loss: 171.45835930, Gradient norm: 0.11812072
INFO:root:[   52] Training loss: 171.45585259, Validation loss: 171.46159550, Gradient norm: 0.12403864
INFO:root:[   53] Training loss: 171.45583625, Validation loss: 171.48416325, Gradient norm: 0.11523828
INFO:root:[   54] Training loss: 171.45582865, Validation loss: 171.45339350, Gradient norm: 0.12460982
INFO:root:[   55] Training loss: 171.45582818, Validation loss: 171.50931402, Gradient norm: 0.12372855
INFO:root:[   56] Training loss: 171.45582560, Validation loss: 171.47802360, Gradient norm: 0.11925629
INFO:root:[   57] Training loss: 171.45587992, Validation loss: 171.52455380, Gradient norm: 0.12924762
INFO:root:[   58] Training loss: 171.45582364, Validation loss: 171.49018405, Gradient norm: 0.12361522
INFO:root:[   59] Training loss: 171.45583998, Validation loss: 171.52380478, Gradient norm: 0.12640870
INFO:root:[   60] Training loss: 171.45585978, Validation loss: 171.46897272, Gradient norm: 0.13027169
INFO:root:[   61] Training loss: 171.45584086, Validation loss: 171.50323888, Gradient norm: 0.12939999
INFO:root:[   62] Training loss: 171.45580750, Validation loss: 171.48464618, Gradient norm: 0.11709941
INFO:root:[   63] Training loss: 171.45582241, Validation loss: 171.48784223, Gradient norm: 0.12499609
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 1597.015s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 171.4558
INFO:root:EnergyScoreTrain: 171.45579
INFO:root:CoverageTrain: 0.0
INFO:root:IntervalWidthTrain: 0.0
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 171.48713
INFO:root:EnergyScoreValidation: 171.48713
INFO:root:CoverageValidation: 0.0
INFO:root:IntervalWidthValidation: 0.0
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 171.52222
INFO:root:EnergyScoreTest: 171.52221
INFO:root:CoverageTest: 0.0
INFO:root:IntervalWidthTest: 0.0
