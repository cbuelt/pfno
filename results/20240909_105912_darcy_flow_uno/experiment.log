INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.31040899, Validation loss: 0.25832393, Gradient norm: 2.87247000
INFO:root:[    2] Training loss: 0.19723248, Validation loss: 0.17395642, Gradient norm: 3.65283752
INFO:root:[    3] Training loss: 0.16512126, Validation loss: 0.20716627, Gradient norm: 3.41650231
INFO:root:[    4] Training loss: 0.15285054, Validation loss: 0.18887395, Gradient norm: 3.48669925
INFO:root:[    5] Training loss: 0.14003201, Validation loss: 0.18815352, Gradient norm: 3.00295229
INFO:root:[    6] Training loss: 0.14067883, Validation loss: 0.24774912, Gradient norm: 3.57915487
INFO:root:[    7] Training loss: 0.13705190, Validation loss: 0.17509245, Gradient norm: 3.51095927
INFO:root:[    8] Training loss: 0.12910530, Validation loss: 0.15364629, Gradient norm: 2.88354743
INFO:root:[    9] Training loss: 0.12477947, Validation loss: 0.16612226, Gradient norm: 3.13727191
INFO:root:[   10] Training loss: 0.12057966, Validation loss: 0.15444334, Gradient norm: 2.81017897
INFO:root:[   11] Training loss: 0.11639275, Validation loss: 0.14399775, Gradient norm: 2.53398464
INFO:root:[   12] Training loss: 0.11123145, Validation loss: 0.15029958, Gradient norm: 2.54777030
INFO:root:[   13] Training loss: 0.11351987, Validation loss: 0.16376855, Gradient norm: 2.46807048
INFO:root:[   14] Training loss: 0.11264620, Validation loss: 0.17752153, Gradient norm: 2.82205628
INFO:root:[   15] Training loss: 0.11119452, Validation loss: 0.19729351, Gradient norm: 2.76277711
INFO:root:[   16] Training loss: 0.10758982, Validation loss: 0.14264421, Gradient norm: 2.65950038
INFO:root:[   17] Training loss: 0.10996266, Validation loss: 0.16472434, Gradient norm: 2.65000161
INFO:root:[   18] Training loss: 0.10420244, Validation loss: 0.14398349, Gradient norm: 2.49375740
INFO:root:[   19] Training loss: 0.10386203, Validation loss: 0.15982750, Gradient norm: 2.81856394
INFO:root:[   20] Training loss: 0.09654363, Validation loss: 0.15053813, Gradient norm: 2.40763058
INFO:root:[   21] Training loss: 0.09734228, Validation loss: 0.19739994, Gradient norm: 2.51709209
INFO:root:[   22] Training loss: 0.09392253, Validation loss: 0.14612893, Gradient norm: 2.34873420
INFO:root:[   23] Training loss: 0.09054316, Validation loss: 0.15985209, Gradient norm: 2.12925137
INFO:root:[   24] Training loss: 0.09263153, Validation loss: 0.19127215, Gradient norm: 2.35540202
INFO:root:[   25] Training loss: 0.09196460, Validation loss: 0.24321552, Gradient norm: 2.07630127
INFO:root:[   26] Training loss: 0.09037030, Validation loss: 0.15593592, Gradient norm: 2.26737598
INFO:root:[   27] Training loss: 0.08816449, Validation loss: 0.24168205, Gradient norm: 2.20901626
INFO:root:[   28] Training loss: 0.08678541, Validation loss: 0.19135319, Gradient norm: 1.95959233
INFO:root:[   29] Training loss: 0.08614229, Validation loss: 0.18314867, Gradient norm: 2.13482346
INFO:root:[   30] Training loss: 0.08766935, Validation loss: 0.23297603, Gradient norm: 1.98024757
INFO:root:[   31] Training loss: 0.08581657, Validation loss: 0.14423012, Gradient norm: 2.20573568
INFO:root:[   32] Training loss: 0.08309531, Validation loss: 0.17283084, Gradient norm: 1.96860426
INFO:root:[   33] Training loss: 0.08006473, Validation loss: 0.17387810, Gradient norm: 1.72692934
INFO:root:[   34] Training loss: 0.08297723, Validation loss: 0.18942582, Gradient norm: 2.00141228
INFO:root:[   35] Training loss: 0.08514077, Validation loss: 0.16478508, Gradient norm: 2.07882773
INFO:root:[   36] Training loss: 0.08090220, Validation loss: 0.17272539, Gradient norm: 1.98310148
INFO:root:[   37] Training loss: 0.07907753, Validation loss: 0.18153576, Gradient norm: 1.66047526
INFO:root:[   38] Training loss: 0.07972897, Validation loss: 0.23013621, Gradient norm: 1.63962105
INFO:root:[   39] Training loss: 0.07973010, Validation loss: 0.15798206, Gradient norm: 1.78099450
INFO:root:[   40] Training loss: 0.07934284, Validation loss: 0.18614869, Gradient norm: 1.74626099
INFO:root:[   41] Training loss: 0.07641883, Validation loss: 0.19298190, Gradient norm: 1.82569363
INFO:root:[   42] Training loss: 0.07849203, Validation loss: 0.18926424, Gradient norm: 1.82936695
INFO:root:[   43] Training loss: 0.07316181, Validation loss: 0.16176363, Gradient norm: 1.77704368
INFO:root:[   44] Training loss: 0.07515670, Validation loss: 0.16541210, Gradient norm: 1.70242937
INFO:root:[   45] Training loss: 0.07739325, Validation loss: 0.19138193, Gradient norm: 1.65441124
INFO:root:[   46] Training loss: 0.07710043, Validation loss: 0.17766275, Gradient norm: 1.73518613
INFO:root:[   47] Training loss: 0.07468311, Validation loss: 0.20116149, Gradient norm: 1.66946910
INFO:root:[   48] Training loss: 0.07446393, Validation loss: 0.21161507, Gradient norm: 1.71095177
INFO:root:[   49] Training loss: 0.07612321, Validation loss: 0.19083445, Gradient norm: 1.72180523
INFO:root:[   50] Training loss: 0.07224297, Validation loss: 0.18985543, Gradient norm: 1.63632550
INFO:root:[   51] Training loss: 0.07508844, Validation loss: 0.17421959, Gradient norm: 1.56135021
INFO:root:[   52] Training loss: 0.07148297, Validation loss: 0.20979810, Gradient norm: 1.74612826
INFO:root:[   53] Training loss: 0.07778606, Validation loss: 0.16782616, Gradient norm: 1.67764924
INFO:root:[   54] Training loss: 0.07151918, Validation loss: 0.17202267, Gradient norm: 1.59756960
INFO:root:[   55] Training loss: 0.07170711, Validation loss: 0.22187926, Gradient norm: 1.68414652
INFO:root:[   56] Training loss: 0.07299404, Validation loss: 0.18655417, Gradient norm: 1.80043966
INFO:root:[   57] Training loss: 0.07054305, Validation loss: 0.19226669, Gradient norm: 1.37032600
INFO:root:[   58] Training loss: 0.07255551, Validation loss: 0.18611176, Gradient norm: 1.54209672
INFO:root:[   59] Training loss: 0.07484147, Validation loss: 0.20598445, Gradient norm: 1.88185618
INFO:root:[   60] Training loss: 0.06889787, Validation loss: 0.20119173, Gradient norm: 1.44558370
INFO:root:[   61] Training loss: 0.06870368, Validation loss: 0.21240096, Gradient norm: 1.49200397
INFO:root:[   62] Training loss: 0.06825021, Validation loss: 0.22001472, Gradient norm: 1.39265862
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 5137.334s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.27779
INFO:root:EnergyScoreTrain: 0.13636
INFO:root:CRPSTrain: 0.10977
INFO:root:Gaussian NLLTrain: 0.11959
INFO:root:CoverageTrain: 0.68595
INFO:root:IntervalWidthTrain: 0.70103
INFO:root:Evaluating the model on Validation data.
