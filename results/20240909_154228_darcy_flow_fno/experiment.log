INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.32758433, Validation loss: 0.24735789, Gradient norm: 1.85228060
INFO:root:[    2] Training loss: 0.15712937, Validation loss: 0.18804683, Gradient norm: 1.63505338
INFO:root:[    3] Training loss: 0.13728654, Validation loss: 0.12151852, Gradient norm: 1.70059750
INFO:root:[    4] Training loss: 0.11718784, Validation loss: 0.11653195, Gradient norm: 1.40973232
INFO:root:[    5] Training loss: 0.11275906, Validation loss: 0.10131097, Gradient norm: 1.36950625
INFO:root:[    6] Training loss: 0.10441549, Validation loss: 0.10002693, Gradient norm: 1.27501719
INFO:root:[    7] Training loss: 0.09847601, Validation loss: 0.11498737, Gradient norm: 1.16638721
INFO:root:[    8] Training loss: 0.09886017, Validation loss: 0.09157132, Gradient norm: 1.19016700
INFO:root:[    9] Training loss: 0.09573365, Validation loss: 0.10456830, Gradient norm: 1.18268332
INFO:root:[   10] Training loss: 0.09062887, Validation loss: 0.09344481, Gradient norm: 1.03122483
INFO:root:[   11] Training loss: 0.08750955, Validation loss: 0.11399666, Gradient norm: 0.98253307
INFO:root:[   12] Training loss: 0.08654110, Validation loss: 0.08641174, Gradient norm: 1.01814639
INFO:root:[   13] Training loss: 0.07412371, Validation loss: 0.08746279, Gradient norm: 0.81118391
INFO:root:[   14] Training loss: 0.07539566, Validation loss: 0.07788772, Gradient norm: 0.83910231
INFO:root:[   15] Training loss: 0.07807924, Validation loss: 0.07668954, Gradient norm: 0.93890164
INFO:root:[   16] Training loss: 0.07073464, Validation loss: 0.09176811, Gradient norm: 0.85576359
INFO:root:[   17] Training loss: 0.06632831, Validation loss: 0.08649708, Gradient norm: 0.83149958
INFO:root:[   18] Training loss: 0.06684389, Validation loss: 0.08996823, Gradient norm: 0.88101281
INFO:root:[   19] Training loss: 0.06612395, Validation loss: 0.07721629, Gradient norm: 0.83137067
INFO:root:[   20] Training loss: 0.06532709, Validation loss: 0.08029803, Gradient norm: 0.83346005
INFO:root:[   21] Training loss: 0.06115418, Validation loss: 0.07958973, Gradient norm: 0.79558776
INFO:root:[   22] Training loss: 0.06123148, Validation loss: 0.08826836, Gradient norm: 0.77276488
INFO:root:[   23] Training loss: 0.05493471, Validation loss: 0.08448902, Gradient norm: 0.78174365
INFO:root:[   24] Training loss: 0.06145991, Validation loss: 0.08699247, Gradient norm: 0.90498524
INFO:root:[   25] Training loss: 0.05926924, Validation loss: 0.07786231, Gradient norm: 0.88650698
INFO:root:[   26] Training loss: 0.05666059, Validation loss: 0.07098549, Gradient norm: 0.83934976
INFO:root:[   27] Training loss: 0.05544492, Validation loss: 0.08460094, Gradient norm: 0.83062622
INFO:root:[   28] Training loss: 0.05289834, Validation loss: 0.08461060, Gradient norm: 0.77622493
INFO:root:[   29] Training loss: 0.05426158, Validation loss: 0.07540026, Gradient norm: 0.84052932
INFO:root:[   30] Training loss: 0.04971801, Validation loss: 0.07930049, Gradient norm: 0.80922962
INFO:root:[   31] Training loss: 0.04925503, Validation loss: 0.07601785, Gradient norm: 0.78360997
INFO:root:[   32] Training loss: 0.04590357, Validation loss: 0.07271447, Gradient norm: 0.66248946
INFO:root:[   33] Training loss: 0.04949034, Validation loss: 0.08782453, Gradient norm: 0.87938120
INFO:root:[   34] Training loss: 0.05111138, Validation loss: 0.07960534, Gradient norm: 0.92400563
INFO:root:[   35] Training loss: 0.04603482, Validation loss: 0.07250625, Gradient norm: 0.83684754
INFO:root:[   36] Training loss: 0.04151157, Validation loss: 0.06841608, Gradient norm: 0.67477519
INFO:root:[   37] Training loss: 0.04194078, Validation loss: 0.08037098, Gradient norm: 0.69087658
INFO:root:[   38] Training loss: 0.04312952, Validation loss: 0.07835652, Gradient norm: 0.79976161
INFO:root:[   39] Training loss: 0.04263829, Validation loss: 0.07635523, Gradient norm: 0.81260577
INFO:root:[   40] Training loss: 0.03949183, Validation loss: 0.07186351, Gradient norm: 0.72357139
INFO:root:[   41] Training loss: 0.04404454, Validation loss: 0.07418955, Gradient norm: 0.88413827
INFO:root:[   42] Training loss: 0.04463584, Validation loss: 0.06679680, Gradient norm: 0.88164675
INFO:root:[   43] Training loss: 0.03864902, Validation loss: 0.07355662, Gradient norm: 0.71712281
INFO:root:[   44] Training loss: 0.03579236, Validation loss: 0.07722346, Gradient norm: 0.68492944
INFO:root:[   45] Training loss: 0.04140962, Validation loss: 0.07391439, Gradient norm: 0.82339190
INFO:root:[   46] Training loss: 0.03699734, Validation loss: 0.07200939, Gradient norm: 0.73874686
INFO:root:[   47] Training loss: 0.04032907, Validation loss: 0.07641268, Gradient norm: 0.86746396
INFO:root:[   48] Training loss: 0.03762347, Validation loss: 0.07683387, Gradient norm: 0.68028751
INFO:root:[   49] Training loss: 0.03793853, Validation loss: 0.07305907, Gradient norm: 0.77272060
INFO:root:[   50] Training loss: 0.03645190, Validation loss: 0.06718080, Gradient norm: 0.82543670
INFO:root:[   51] Training loss: 0.03736253, Validation loss: 0.06962040, Gradient norm: 0.85439285
INFO:root:[   52] Training loss: 0.03762752, Validation loss: 0.07433528, Gradient norm: 0.75234178
INFO:root:[   53] Training loss: 0.03453116, Validation loss: 0.06927078, Gradient norm: 0.80445399
INFO:root:[   54] Training loss: 0.03733341, Validation loss: 0.07428615, Gradient norm: 0.75884040
INFO:root:[   55] Training loss: 0.03358666, Validation loss: 0.07210353, Gradient norm: 0.72065510
INFO:root:[   56] Training loss: 0.03536642, Validation loss: 0.07463830, Gradient norm: 0.74714927
INFO:root:[   57] Training loss: 0.03590178, Validation loss: 0.07106648, Gradient norm: 0.79366437
INFO:root:[   58] Training loss: 0.03445719, Validation loss: 0.06859161, Gradient norm: 0.80454459
INFO:root:[   59] Training loss: 0.03128849, Validation loss: 0.07431069, Gradient norm: 0.67454344
INFO:root:[   60] Training loss: 0.03357576, Validation loss: 0.06767784, Gradient norm: 0.79206094
INFO:root:[   61] Training loss: 0.03366628, Validation loss: 0.07686972, Gradient norm: 0.91494457
INFO:root:[   62] Training loss: 0.03258984, Validation loss: 0.06431790, Gradient norm: 0.76774094
INFO:root:[   63] Training loss: 0.03164568, Validation loss: 0.06941296, Gradient norm: 0.78380849
INFO:root:[   64] Training loss: 0.03178900, Validation loss: 0.07296418, Gradient norm: 0.76401749
INFO:root:[   65] Training loss: 0.03406723, Validation loss: 0.06955787, Gradient norm: 0.78905700
INFO:root:[   66] Training loss: 0.03100627, Validation loss: 0.07581449, Gradient norm: 0.71256046
INFO:root:[   67] Training loss: 0.03185974, Validation loss: 0.07019522, Gradient norm: 0.72659558
INFO:root:[   68] Training loss: 0.03035674, Validation loss: 0.06727351, Gradient norm: 0.79301943
INFO:root:[   69] Training loss: 0.03358633, Validation loss: 0.07237395, Gradient norm: 0.74954627
INFO:root:[   70] Training loss: 0.03127048, Validation loss: 0.07468591, Gradient norm: 0.82117948
INFO:root:[   71] Training loss: 0.02924306, Validation loss: 0.07055650, Gradient norm: 0.79693637
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 3582.084s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04484
INFO:root:EnergyScoreTrain: 0.02964
INFO:root:CRPSTrain: 0.02529
INFO:root:Gaussian NLLTrain: 0.27186
INFO:root:CoverageTrain: 0.5522
INFO:root:IntervalWidthTrain: 0.05653
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07325
INFO:root:EnergyScoreValidation: 0.0649
INFO:root:CRPSValidation: 0.05526
INFO:root:Gaussian NLLValidation: 8.95471
INFO:root:CoverageValidation: 0.42555
INFO:root:IntervalWidthValidation: 0.05646
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07571
INFO:root:EnergyScoreTest: 0.06748
INFO:root:CRPSTest: 0.05742
INFO:root:Gaussian NLLTest: 9.57021
INFO:root:CoverageTest: 0.41676
INFO:root:IntervalWidthTest: 0.05606
INFO:root:###2 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.31109284, Validation loss: 0.15892554, Gradient norm: 2.01887893
INFO:root:[    2] Training loss: 0.14896772, Validation loss: 0.11288672, Gradient norm: 1.90732683
INFO:root:[    3] Training loss: 0.12846132, Validation loss: 0.11515836, Gradient norm: 1.72414806
INFO:root:[    4] Training loss: 0.10996177, Validation loss: 0.14893554, Gradient norm: 1.42783699
INFO:root:[    5] Training loss: 0.10143923, Validation loss: 0.10273513, Gradient norm: 1.36080669
INFO:root:[    6] Training loss: 0.08627435, Validation loss: 0.08257560, Gradient norm: 1.10762338
INFO:root:[    7] Training loss: 0.08524634, Validation loss: 0.10343780, Gradient norm: 1.09862405
INFO:root:[    8] Training loss: 0.08498875, Validation loss: 0.09814781, Gradient norm: 1.10030301
INFO:root:[    9] Training loss: 0.07972876, Validation loss: 0.09699374, Gradient norm: 0.95873472
INFO:root:[   10] Training loss: 0.07681246, Validation loss: 0.08931143, Gradient norm: 0.88154142
INFO:root:[   11] Training loss: 0.07370916, Validation loss: 0.08624300, Gradient norm: 0.91864407
INFO:root:[   12] Training loss: 0.07300394, Validation loss: 0.08013509, Gradient norm: 0.82997787
INFO:root:[   13] Training loss: 0.06677477, Validation loss: 0.07907494, Gradient norm: 0.74298809
INFO:root:[   14] Training loss: 0.06355850, Validation loss: 0.10899571, Gradient norm: 0.69990576
INFO:root:[   15] Training loss: 0.06370765, Validation loss: 0.07623129, Gradient norm: 0.71214843
INFO:root:[   16] Training loss: 0.06064636, Validation loss: 0.08828637, Gradient norm: 0.69273015
INFO:root:[   17] Training loss: 0.06154562, Validation loss: 0.07927448, Gradient norm: 0.66278497
INFO:root:[   18] Training loss: 0.05969080, Validation loss: 0.06336818, Gradient norm: 0.62569130
INFO:root:[   19] Training loss: 0.05952214, Validation loss: 0.06508367, Gradient norm: 0.64267179
INFO:root:[   20] Training loss: 0.06088698, Validation loss: 0.07107167, Gradient norm: 0.72612979
INFO:root:[   21] Training loss: 0.05835356, Validation loss: 0.07784793, Gradient norm: 0.65665655
INFO:root:[   22] Training loss: 0.05449795, Validation loss: 0.07189300, Gradient norm: 0.60577664
INFO:root:[   23] Training loss: 0.05334377, Validation loss: 0.06948743, Gradient norm: 0.59432200
INFO:root:[   24] Training loss: 0.05240940, Validation loss: 0.06831560, Gradient norm: 0.62981643
INFO:root:[   25] Training loss: 0.05011828, Validation loss: 0.06784380, Gradient norm: 0.55028830
INFO:root:[   26] Training loss: 0.04771416, Validation loss: 0.08293642, Gradient norm: 0.56279311
INFO:root:[   27] Training loss: 0.05000943, Validation loss: 0.07186314, Gradient norm: 0.56111136
INFO:root:[   28] Training loss: 0.04722126, Validation loss: 0.07520129, Gradient norm: 0.51805285
INFO:root:[   29] Training loss: 0.04997105, Validation loss: 0.08038695, Gradient norm: 0.65230129
INFO:root:[   30] Training loss: 0.04647068, Validation loss: 0.07794078, Gradient norm: 0.57169312
INFO:root:[   31] Training loss: 0.04686809, Validation loss: 0.07717408, Gradient norm: 0.58095483
INFO:root:[   32] Training loss: 0.04105435, Validation loss: 0.08032699, Gradient norm: 0.44515001
INFO:root:[   33] Training loss: 0.04656764, Validation loss: 0.08116449, Gradient norm: 0.62289389
INFO:root:[   34] Training loss: 0.04340255, Validation loss: 0.06830965, Gradient norm: 0.55536047
INFO:root:[   35] Training loss: 0.04383363, Validation loss: 0.07215478, Gradient norm: 0.62465973
INFO:root:[   36] Training loss: 0.04419945, Validation loss: 0.06112353, Gradient norm: 0.51265555
INFO:root:[   37] Training loss: 0.04264927, Validation loss: 0.07887502, Gradient norm: 0.55692131
INFO:root:[   38] Training loss: 0.03893099, Validation loss: 0.07064285, Gradient norm: 0.51390811
INFO:root:[   39] Training loss: 0.03846101, Validation loss: 0.07135800, Gradient norm: 0.54012779
INFO:root:[   40] Training loss: 0.03963027, Validation loss: 0.06551401, Gradient norm: 0.53913838
INFO:root:[   41] Training loss: 0.04181754, Validation loss: 0.06987576, Gradient norm: 0.59771419
INFO:root:[   42] Training loss: 0.03976993, Validation loss: 0.07167132, Gradient norm: 0.53529172
INFO:root:[   43] Training loss: 0.03718639, Validation loss: 0.06992333, Gradient norm: 0.53640048
INFO:root:[   44] Training loss: 0.03772632, Validation loss: 0.06430731, Gradient norm: 0.50972594
INFO:root:[   45] Training loss: 0.03711652, Validation loss: 0.06978878, Gradient norm: 0.46271894
INFO:root:[   46] Training loss: 0.03574793, Validation loss: 0.07089739, Gradient norm: 0.52525050
INFO:root:[   47] Training loss: 0.03570707, Validation loss: 0.06985655, Gradient norm: 0.49006957
INFO:root:[   48] Training loss: 0.03505819, Validation loss: 0.06941307, Gradient norm: 0.50699469
INFO:root:[   49] Training loss: 0.03618432, Validation loss: 0.06594730, Gradient norm: 0.56038818
INFO:root:[   50] Training loss: 0.03593711, Validation loss: 0.06973606, Gradient norm: 0.54046889
INFO:root:[   51] Training loss: 0.03477568, Validation loss: 0.06594996, Gradient norm: 0.55367189
INFO:root:[   52] Training loss: 0.03711570, Validation loss: 0.06804850, Gradient norm: 0.58248546
INFO:root:[   53] Training loss: 0.03410418, Validation loss: 0.06446043, Gradient norm: 0.51458406
INFO:root:[   54] Training loss: 0.03317474, Validation loss: 0.06302888, Gradient norm: 0.54219269
INFO:root:[   55] Training loss: 0.03289543, Validation loss: 0.06954112, Gradient norm: 0.50418865
INFO:root:[   56] Training loss: 0.03509909, Validation loss: 0.07072352, Gradient norm: 0.60865735
INFO:root:[   57] Training loss: 0.03091168, Validation loss: 0.07432686, Gradient norm: 0.47876883
INFO:root:[   58] Training loss: 0.03397323, Validation loss: 0.06818568, Gradient norm: 0.59583673
INFO:root:[   59] Training loss: 0.03018104, Validation loss: 0.06362457, Gradient norm: 0.47176304
INFO:root:[   60] Training loss: 0.03251785, Validation loss: 0.07208144, Gradient norm: 0.57577336
INFO:root:[   61] Training loss: 0.03169454, Validation loss: 0.07126231, Gradient norm: 0.51537447
INFO:root:[   62] Training loss: 0.03023292, Validation loss: 0.06240411, Gradient norm: 0.44919969
INFO:root:[   63] Training loss: 0.03022086, Validation loss: 0.07038266, Gradient norm: 0.54672376
INFO:root:[   64] Training loss: 0.03371689, Validation loss: 0.06241566, Gradient norm: 0.57330714
INFO:root:[   65] Training loss: 0.03177816, Validation loss: 0.06455187, Gradient norm: 0.56273313
INFO:root:[   66] Training loss: 0.02879568, Validation loss: 0.05990902, Gradient norm: 0.52153040
INFO:root:[   67] Training loss: 0.03386946, Validation loss: 0.07105842, Gradient norm: 0.58260245
INFO:root:[   68] Training loss: 0.03022051, Validation loss: 0.06786378, Gradient norm: 0.49674783
INFO:root:[   69] Training loss: 0.02857860, Validation loss: 0.06374032, Gradient norm: 0.42426623
INFO:root:[   70] Training loss: 0.02774968, Validation loss: 0.06290866, Gradient norm: 0.48008937
INFO:root:[   71] Training loss: 0.03046304, Validation loss: 0.06158768, Gradient norm: 0.55700450
INFO:root:[   72] Training loss: 0.02974907, Validation loss: 0.06680017, Gradient norm: 0.53549516
INFO:root:[   73] Training loss: 0.02968124, Validation loss: 0.07203039, Gradient norm: 0.55289943
INFO:root:[   74] Training loss: 0.03165279, Validation loss: 0.07249004, Gradient norm: 0.57993903
INFO:root:[   75] Training loss: 0.03022960, Validation loss: 0.06505698, Gradient norm: 0.55586115
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 3695.05s.
INFO:root:Emptying the cuda cache took 0.022s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0457
INFO:root:EnergyScoreTrain: 0.02709
INFO:root:CRPSTrain: 0.02146
INFO:root:Gaussian NLLTrain: -2.01061
INFO:root:CoverageTrain: 0.93306
INFO:root:IntervalWidthTrain: 0.16691
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07461
INFO:root:EnergyScoreValidation: 0.06113
INFO:root:CRPSValidation: 0.0508
INFO:root:Gaussian NLLValidation: -0.48078
INFO:root:CoverageValidation: 0.71899
INFO:root:IntervalWidthValidation: 0.13779
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07541
INFO:root:EnergyScoreTest: 0.06146
INFO:root:CRPSTest: 0.05122
INFO:root:Gaussian NLLTest: -0.40919
INFO:root:CoverageTest: 0.71582
INFO:root:IntervalWidthTest: 0.13845
INFO:root:###3 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.30030483, Validation loss: 0.13965118, Gradient norm: 2.15046514
INFO:root:[    2] Training loss: 0.14190275, Validation loss: 0.12395405, Gradient norm: 2.08217396
INFO:root:[    3] Training loss: 0.11387980, Validation loss: 0.12053434, Gradient norm: 1.73678091
INFO:root:[    4] Training loss: 0.10736931, Validation loss: 0.13090234, Gradient norm: 1.56861100
INFO:root:[    5] Training loss: 0.09446815, Validation loss: 0.12393016, Gradient norm: 1.34431058
INFO:root:[    6] Training loss: 0.09073207, Validation loss: 0.10310744, Gradient norm: 1.29734082
INFO:root:[    7] Training loss: 0.08817203, Validation loss: 0.08277661, Gradient norm: 1.16539607
INFO:root:[    8] Training loss: 0.08557621, Validation loss: 0.10618745, Gradient norm: 1.12997365
INFO:root:[    9] Training loss: 0.07959652, Validation loss: 0.10269537, Gradient norm: 1.01201060
INFO:root:[   10] Training loss: 0.08031062, Validation loss: 0.08973614, Gradient norm: 0.99761724
INFO:root:[   11] Training loss: 0.07454831, Validation loss: 0.08724605, Gradient norm: 0.92172843
INFO:root:[   12] Training loss: 0.06927383, Validation loss: 0.06884050, Gradient norm: 0.77847294
INFO:root:[   13] Training loss: 0.06746900, Validation loss: 0.07483244, Gradient norm: 0.80957124
INFO:root:[   14] Training loss: 0.06574672, Validation loss: 0.09559434, Gradient norm: 0.76322880
INFO:root:[   15] Training loss: 0.06703399, Validation loss: 0.08050900, Gradient norm: 0.78622624
INFO:root:[   16] Training loss: 0.05841800, Validation loss: 0.07001060, Gradient norm: 0.64843329
INFO:root:[   17] Training loss: 0.06199741, Validation loss: 0.07943599, Gradient norm: 0.70319622
INFO:root:[   18] Training loss: 0.05904962, Validation loss: 0.06497647, Gradient norm: 0.61503953
INFO:root:[   19] Training loss: 0.05941394, Validation loss: 0.06726153, Gradient norm: 0.67297838
INFO:root:[   20] Training loss: 0.06272774, Validation loss: 0.06691586, Gradient norm: 0.72750639
INFO:root:[   21] Training loss: 0.05463621, Validation loss: 0.07194523, Gradient norm: 0.64264621
INFO:root:[   22] Training loss: 0.05653129, Validation loss: 0.07017343, Gradient norm: 0.65754196
INFO:root:[   23] Training loss: 0.05193552, Validation loss: 0.08179177, Gradient norm: 0.62241551
INFO:root:[   24] Training loss: 0.05457493, Validation loss: 0.06408087, Gradient norm: 0.68363258
INFO:root:[   25] Training loss: 0.05033558, Validation loss: 0.07042439, Gradient norm: 0.58864335
INFO:root:[   26] Training loss: 0.05351383, Validation loss: 0.06857579, Gradient norm: 0.63683430
INFO:root:[   27] Training loss: 0.04596295, Validation loss: 0.07531578, Gradient norm: 0.51700161
INFO:root:[   28] Training loss: 0.04429418, Validation loss: 0.08385959, Gradient norm: 0.47005569
INFO:root:[   29] Training loss: 0.04918396, Validation loss: 0.07076170, Gradient norm: 0.62998098
INFO:root:[   30] Training loss: 0.04514404, Validation loss: 0.06478976, Gradient norm: 0.58424803
INFO:root:[   31] Training loss: 0.04726971, Validation loss: 0.06645989, Gradient norm: 0.56460988
INFO:root:[   32] Training loss: 0.04251342, Validation loss: 0.07362188, Gradient norm: 0.50596226
INFO:root:[   33] Training loss: 0.04244623, Validation loss: 0.07379999, Gradient norm: 0.49791720
INFO:root:[   34] Training loss: 0.04401813, Validation loss: 0.07319113, Gradient norm: 0.56239170
INFO:root:[   35] Training loss: 0.04675616, Validation loss: 0.07629484, Gradient norm: 0.61552625
INFO:root:[   36] Training loss: 0.04161095, Validation loss: 0.06534904, Gradient norm: 0.56135553
INFO:root:[   37] Training loss: 0.03940905, Validation loss: 0.06518744, Gradient norm: 0.47648746
INFO:root:[   38] Training loss: 0.03941406, Validation loss: 0.07114949, Gradient norm: 0.46480866
INFO:root:[   39] Training loss: 0.04018466, Validation loss: 0.06624564, Gradient norm: 0.51708261
INFO:root:[   40] Training loss: 0.03986441, Validation loss: 0.06562643, Gradient norm: 0.58323706
INFO:root:[   41] Training loss: 0.03973441, Validation loss: 0.06894610, Gradient norm: 0.57659424
INFO:root:[   42] Training loss: 0.04068236, Validation loss: 0.06366237, Gradient norm: 0.56151597
INFO:root:[   43] Training loss: 0.03879170, Validation loss: 0.06318566, Gradient norm: 0.51239810
INFO:root:[   44] Training loss: 0.03723103, Validation loss: 0.06718124, Gradient norm: 0.51791977
INFO:root:[   45] Training loss: 0.03487545, Validation loss: 0.06273616, Gradient norm: 0.39591699
INFO:root:[   46] Training loss: 0.03632953, Validation loss: 0.07984701, Gradient norm: 0.54476293
INFO:root:[   47] Training loss: 0.03508625, Validation loss: 0.06893049, Gradient norm: 0.46976135
INFO:root:[   48] Training loss: 0.03520582, Validation loss: 0.06460620, Gradient norm: 0.49885556
INFO:root:[   49] Training loss: 0.03377429, Validation loss: 0.06498656, Gradient norm: 0.50849917
INFO:root:[   50] Training loss: 0.03494158, Validation loss: 0.06380286, Gradient norm: 0.45534122
INFO:root:[   51] Training loss: 0.03789811, Validation loss: 0.06219340, Gradient norm: 0.52863951
INFO:root:[   52] Training loss: 0.03855297, Validation loss: 0.06438629, Gradient norm: 0.61089302
INFO:root:[   53] Training loss: 0.03462572, Validation loss: 0.06202972, Gradient norm: 0.46449733
INFO:root:[   54] Training loss: 0.03217741, Validation loss: 0.06072139, Gradient norm: 0.41148389
INFO:root:[   55] Training loss: 0.03323965, Validation loss: 0.07301402, Gradient norm: 0.50141319
INFO:root:[   56] Training loss: 0.03573401, Validation loss: 0.07433205, Gradient norm: 0.59837034
INFO:root:[   57] Training loss: 0.03300676, Validation loss: 0.06736560, Gradient norm: 0.48508899
INFO:root:[   58] Training loss: 0.03248652, Validation loss: 0.06845533, Gradient norm: 0.45153897
INFO:root:[   59] Training loss: 0.03272107, Validation loss: 0.06268233, Gradient norm: 0.49097892
INFO:root:[   60] Training loss: 0.03075591, Validation loss: 0.06110287, Gradient norm: 0.47040579
INFO:root:[   61] Training loss: 0.03078397, Validation loss: 0.07002994, Gradient norm: 0.49554398
INFO:root:[   62] Training loss: 0.03038259, Validation loss: 0.06438112, Gradient norm: 0.44204669
INFO:root:[   63] Training loss: 0.03325280, Validation loss: 0.07018845, Gradient norm: 0.50553711
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 3096.788s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0488
INFO:root:EnergyScoreTrain: 0.03069
INFO:root:CRPSTrain: 0.02437
INFO:root:Gaussian NLLTrain: -1.87787
INFO:root:CoverageTrain: 0.94989
INFO:root:IntervalWidthTrain: 0.21355
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07964
INFO:root:EnergyScoreValidation: 0.06158
INFO:root:CRPSValidation: 0.05059
INFO:root:Gaussian NLLValidation: -1.11113
INFO:root:CoverageValidation: 0.81647
INFO:root:IntervalWidthValidation: 0.19361
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08139
INFO:root:EnergyScoreTest: 0.06426
INFO:root:CRPSTest: 0.05266
INFO:root:Gaussian NLLTest: -1.04219
INFO:root:CoverageTest: 0.80943
INFO:root:IntervalWidthTest: 0.19464
INFO:root:###4 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.29014422, Validation loss: 0.15880401, Gradient norm: 2.23589597
INFO:root:[    2] Training loss: 0.13273885, Validation loss: 0.10723041, Gradient norm: 2.05212313
INFO:root:[    3] Training loss: 0.12006253, Validation loss: 0.11389108, Gradient norm: 1.89991086
INFO:root:[    4] Training loss: 0.10057066, Validation loss: 0.14658007, Gradient norm: 1.40966330
INFO:root:[    5] Training loss: 0.09849371, Validation loss: 0.09092525, Gradient norm: 1.34597484
INFO:root:[    6] Training loss: 0.08765647, Validation loss: 0.10762377, Gradient norm: 1.13879856
INFO:root:[    7] Training loss: 0.08508835, Validation loss: 0.10083440, Gradient norm: 1.14501559
INFO:root:[    8] Training loss: 0.08558935, Validation loss: 0.07755018, Gradient norm: 1.11136890
INFO:root:[    9] Training loss: 0.07811078, Validation loss: 0.10790161, Gradient norm: 1.00826908
INFO:root:[   10] Training loss: 0.07945529, Validation loss: 0.07024180, Gradient norm: 0.96829670
INFO:root:[   11] Training loss: 0.06811533, Validation loss: 0.09127515, Gradient norm: 0.75154815
INFO:root:[   12] Training loss: 0.07668654, Validation loss: 0.08240600, Gradient norm: 0.95622235
INFO:root:[   13] Training loss: 0.06464097, Validation loss: 0.08916760, Gradient norm: 0.70853519
INFO:root:[   14] Training loss: 0.06109058, Validation loss: 0.08024527, Gradient norm: 0.63565868
INFO:root:[   15] Training loss: 0.06757928, Validation loss: 0.06730311, Gradient norm: 0.79418146
INFO:root:[   16] Training loss: 0.06356177, Validation loss: 0.07233530, Gradient norm: 0.79349978
INFO:root:[   17] Training loss: 0.05955046, Validation loss: 0.07214226, Gradient norm: 0.62027954
INFO:root:[   18] Training loss: 0.05633295, Validation loss: 0.06224736, Gradient norm: 0.60582886
INFO:root:[   19] Training loss: 0.05694554, Validation loss: 0.07076071, Gradient norm: 0.65852755
INFO:root:[   20] Training loss: 0.06495585, Validation loss: 0.06819587, Gradient norm: 0.73827905
INFO:root:[   21] Training loss: 0.05441766, Validation loss: 0.08204149, Gradient norm: 0.57742011
INFO:root:[   22] Training loss: 0.05620493, Validation loss: 0.06586544, Gradient norm: 0.64336221
INFO:root:[   23] Training loss: 0.05149465, Validation loss: 0.07093880, Gradient norm: 0.55946323
INFO:root:[   24] Training loss: 0.05145738, Validation loss: 0.06183421, Gradient norm: 0.54039861
INFO:root:[   25] Training loss: 0.04775516, Validation loss: 0.06330981, Gradient norm: 0.48964357
INFO:root:[   26] Training loss: 0.04982779, Validation loss: 0.07337512, Gradient norm: 0.57743655
INFO:root:[   27] Training loss: 0.04802346, Validation loss: 0.06369707, Gradient norm: 0.53212629
INFO:root:[   28] Training loss: 0.04702769, Validation loss: 0.06691462, Gradient norm: 0.52671522
INFO:root:[   29] Training loss: 0.04773099, Validation loss: 0.06586860, Gradient norm: 0.57161595
INFO:root:[   30] Training loss: 0.04393003, Validation loss: 0.06730383, Gradient norm: 0.49419846
INFO:root:[   31] Training loss: 0.04444832, Validation loss: 0.06537492, Gradient norm: 0.51521114
INFO:root:[   32] Training loss: 0.04373051, Validation loss: 0.08339389, Gradient norm: 0.50223594
INFO:root:[   33] Training loss: 0.04496273, Validation loss: 0.06403307, Gradient norm: 0.49843082
INFO:root:[   34] Training loss: 0.04748409, Validation loss: 0.06970527, Gradient norm: 0.56098271
INFO:root:[   35] Training loss: 0.04336023, Validation loss: 0.06943986, Gradient norm: 0.51609383
INFO:root:[   36] Training loss: 0.04177824, Validation loss: 0.06147312, Gradient norm: 0.46800896
INFO:root:[   37] Training loss: 0.03950788, Validation loss: 0.06344085, Gradient norm: 0.45745937
INFO:root:[   38] Training loss: 0.03860888, Validation loss: 0.06459156, Gradient norm: 0.41873731
INFO:root:[   39] Training loss: 0.04544444, Validation loss: 0.06020893, Gradient norm: 0.52147079
INFO:root:[   40] Training loss: 0.03870919, Validation loss: 0.07009176, Gradient norm: 0.43324385
INFO:root:[   41] Training loss: 0.04009112, Validation loss: 0.06613521, Gradient norm: 0.51167576
INFO:root:[   42] Training loss: 0.03795514, Validation loss: 0.06321369, Gradient norm: 0.44719656
INFO:root:[   43] Training loss: 0.04089215, Validation loss: 0.06399742, Gradient norm: 0.52242271
INFO:root:[   44] Training loss: 0.03781932, Validation loss: 0.06287476, Gradient norm: 0.48668611
INFO:root:[   45] Training loss: 0.03563695, Validation loss: 0.06176699, Gradient norm: 0.43010434
INFO:root:[   46] Training loss: 0.03768212, Validation loss: 0.07301057, Gradient norm: 0.47475750
INFO:root:[   47] Training loss: 0.03832268, Validation loss: 0.06090976, Gradient norm: 0.53278696
INFO:root:[   48] Training loss: 0.03652921, Validation loss: 0.06497430, Gradient norm: 0.44928066
INFO:root:[   49] Training loss: 0.03723867, Validation loss: 0.06189769, Gradient norm: 0.46441515
INFO:root:[   50] Training loss: 0.03590751, Validation loss: 0.06022159, Gradient norm: 0.45101161
INFO:root:[   51] Training loss: 0.03554715, Validation loss: 0.05978526, Gradient norm: 0.44814111
INFO:root:[   52] Training loss: 0.03803715, Validation loss: 0.06242228, Gradient norm: 0.48861075
INFO:root:[   53] Training loss: 0.03532380, Validation loss: 0.06935972, Gradient norm: 0.49400048
INFO:root:[   54] Training loss: 0.03406922, Validation loss: 0.06116301, Gradient norm: 0.44373969
INFO:root:[   55] Training loss: 0.03428806, Validation loss: 0.06616243, Gradient norm: 0.48069590
INFO:root:[   56] Training loss: 0.03503465, Validation loss: 0.06180842, Gradient norm: 0.47265586
INFO:root:[   57] Training loss: 0.03052346, Validation loss: 0.06235255, Gradient norm: 0.37687787
INFO:root:[   58] Training loss: 0.03326470, Validation loss: 0.06246563, Gradient norm: 0.48376445
INFO:root:[   59] Training loss: 0.03142040, Validation loss: 0.06651084, Gradient norm: 0.36478861
INFO:root:[   60] Training loss: 0.03211544, Validation loss: 0.06075383, Gradient norm: 0.44201832
INFO:root:[   61] Training loss: 0.03332335, Validation loss: 0.06348957, Gradient norm: 0.45125945
INFO:root:[   62] Training loss: 0.03151781, Validation loss: 0.05978448, Gradient norm: 0.39629644
INFO:root:[   63] Training loss: 0.03205826, Validation loss: 0.06305239, Gradient norm: 0.44618087
INFO:root:[   64] Training loss: 0.03125289, Validation loss: 0.06246109, Gradient norm: 0.38974057
INFO:root:[   65] Training loss: 0.03251605, Validation loss: 0.06192005, Gradient norm: 0.43197157
INFO:root:[   66] Training loss: 0.03396219, Validation loss: 0.06378990, Gradient norm: 0.46069843
INFO:root:[   67] Training loss: 0.02936168, Validation loss: 0.06922681, Gradient norm: 0.35735640
INFO:root:[   68] Training loss: 0.03547070, Validation loss: 0.06516922, Gradient norm: 0.58846628
INFO:root:[   69] Training loss: 0.03236475, Validation loss: 0.07243668, Gradient norm: 0.47005918
INFO:root:[   70] Training loss: 0.03114895, Validation loss: 0.05997755, Gradient norm: 0.48998841
INFO:root:[   71] Training loss: 0.03241700, Validation loss: 0.06101392, Gradient norm: 0.47074135
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 3485.918s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05147
INFO:root:EnergyScoreTrain: 0.03061
INFO:root:CRPSTrain: 0.02413
INFO:root:Gaussian NLLTrain: -1.74958
INFO:root:CoverageTrain: 0.96143
INFO:root:IntervalWidthTrain: 0.24436
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08188
INFO:root:EnergyScoreValidation: 0.06027
INFO:root:CRPSValidation: 0.04908
INFO:root:Gaussian NLLValidation: -1.27401
INFO:root:CoverageValidation: 0.85888
INFO:root:IntervalWidthValidation: 0.24233
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08336
INFO:root:EnergyScoreTest: 0.06061
INFO:root:CRPSTest: 0.04934
INFO:root:Gaussian NLLTest: -1.25477
INFO:root:CoverageTest: 0.84875
INFO:root:IntervalWidthTest: 0.24332
INFO:root:###5 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28803888, Validation loss: 0.12345470, Gradient norm: 1.97035221
INFO:root:[    2] Training loss: 0.12811757, Validation loss: 0.10999321, Gradient norm: 1.51403671
INFO:root:[    3] Training loss: 0.10502011, Validation loss: 0.14156644, Gradient norm: 1.30859102
INFO:root:[    4] Training loss: 0.09764567, Validation loss: 0.08761972, Gradient norm: 1.14481436
INFO:root:[    5] Training loss: 0.09560377, Validation loss: 0.11236459, Gradient norm: 1.07748320
INFO:root:[    6] Training loss: 0.08492693, Validation loss: 0.08659760, Gradient norm: 0.86080004
INFO:root:[    7] Training loss: 0.08216669, Validation loss: 0.07732459, Gradient norm: 0.90433710
INFO:root:[    8] Training loss: 0.07679670, Validation loss: 0.09416409, Gradient norm: 0.77970476
INFO:root:[    9] Training loss: 0.07834749, Validation loss: 0.08183949, Gradient norm: 0.83509269
INFO:root:[   10] Training loss: 0.07421783, Validation loss: 0.06567483, Gradient norm: 0.73912064
INFO:root:[   11] Training loss: 0.06943378, Validation loss: 0.07046479, Gradient norm: 0.66615855
INFO:root:[   12] Training loss: 0.07033131, Validation loss: 0.08519270, Gradient norm: 0.70084402
INFO:root:[   13] Training loss: 0.06201675, Validation loss: 0.07542826, Gradient norm: 0.56294023
INFO:root:[   14] Training loss: 0.06130669, Validation loss: 0.08470318, Gradient norm: 0.55044421
INFO:root:[   15] Training loss: 0.06771935, Validation loss: 0.07438076, Gradient norm: 0.68695372
INFO:root:[   16] Training loss: 0.05783504, Validation loss: 0.06395886, Gradient norm: 0.49252862
INFO:root:[   17] Training loss: 0.05934952, Validation loss: 0.07621004, Gradient norm: 0.50686347
INFO:root:[   18] Training loss: 0.06796412, Validation loss: 0.06027025, Gradient norm: 0.73491027
INFO:root:[   19] Training loss: 0.05584281, Validation loss: 0.06116864, Gradient norm: 0.48757998
INFO:root:[   20] Training loss: 0.06010196, Validation loss: 0.06563883, Gradient norm: 0.57137866
INFO:root:[   21] Training loss: 0.05781761, Validation loss: 0.06374861, Gradient norm: 0.55928556
INFO:root:[   22] Training loss: 0.05722144, Validation loss: 0.06837659, Gradient norm: 0.50900687
INFO:root:[   23] Training loss: 0.04967289, Validation loss: 0.07044573, Gradient norm: 0.41667858
INFO:root:[   24] Training loss: 0.05312645, Validation loss: 0.07216599, Gradient norm: 0.52899860
INFO:root:[   25] Training loss: 0.05262457, Validation loss: 0.06623029, Gradient norm: 0.52942781
INFO:root:[   26] Training loss: 0.05289585, Validation loss: 0.07192922, Gradient norm: 0.49687458
INFO:root:[   27] Training loss: 0.04981366, Validation loss: 0.06989692, Gradient norm: 0.45850107
INFO:root:[   28] Training loss: 0.04424679, Validation loss: 0.06790096, Gradient norm: 0.35181612
INFO:root:[   29] Training loss: 0.04897589, Validation loss: 0.06395196, Gradient norm: 0.45296008
INFO:root:[   30] Training loss: 0.04725515, Validation loss: 0.06485609, Gradient norm: 0.51593144
INFO:root:[   31] Training loss: 0.04726450, Validation loss: 0.06488120, Gradient norm: 0.44247676
INFO:root:[   32] Training loss: 0.04596647, Validation loss: 0.06555425, Gradient norm: 0.44418882
INFO:root:[   33] Training loss: 0.04527283, Validation loss: 0.06004850, Gradient norm: 0.44457696
INFO:root:[   34] Training loss: 0.04383009, Validation loss: 0.07987403, Gradient norm: 0.47118924
INFO:root:[   35] Training loss: 0.04909036, Validation loss: 0.06265495, Gradient norm: 0.48736976
INFO:root:[   36] Training loss: 0.04329510, Validation loss: 0.06154777, Gradient norm: 0.39909956
INFO:root:[   37] Training loss: 0.04100055, Validation loss: 0.06327775, Gradient norm: 0.37049297
INFO:root:[   38] Training loss: 0.04170837, Validation loss: 0.06984458, Gradient norm: 0.41162736
INFO:root:[   39] Training loss: 0.04971623, Validation loss: 0.06397543, Gradient norm: 0.60041021
INFO:root:[   40] Training loss: 0.04038176, Validation loss: 0.06053563, Gradient norm: 0.40186893
INFO:root:[   41] Training loss: 0.04317424, Validation loss: 0.06329552, Gradient norm: 0.49413763
INFO:root:[   42] Training loss: 0.04419679, Validation loss: 0.06496314, Gradient norm: 0.50658883
INFO:root:[   43] Training loss: 0.04009321, Validation loss: 0.06537252, Gradient norm: 0.40566576
INFO:root:[   44] Training loss: 0.04115213, Validation loss: 0.06101513, Gradient norm: 0.47663919
INFO:root:[   45] Training loss: 0.03910033, Validation loss: 0.06052133, Gradient norm: 0.41748346
INFO:root:[   46] Training loss: 0.04057633, Validation loss: 0.06725639, Gradient norm: 0.42541851
INFO:root:[   47] Training loss: 0.04159069, Validation loss: 0.06469464, Gradient norm: 0.50365965
INFO:root:[   48] Training loss: 0.04083829, Validation loss: 0.06370668, Gradient norm: 0.50316605
INFO:root:[   49] Training loss: 0.03870605, Validation loss: 0.07761080, Gradient norm: 0.41551979
INFO:root:[   50] Training loss: 0.04044217, Validation loss: 0.06183482, Gradient norm: 0.48936002
INFO:root:[   51] Training loss: 0.03875100, Validation loss: 0.05678655, Gradient norm: 0.41029374
INFO:root:[   52] Training loss: 0.03703718, Validation loss: 0.07270846, Gradient norm: 0.40216704
INFO:root:[   53] Training loss: 0.03843593, Validation loss: 0.05754698, Gradient norm: 0.40666453
INFO:root:[   54] Training loss: 0.03561177, Validation loss: 0.06177602, Gradient norm: 0.39209505
INFO:root:[   55] Training loss: 0.03580302, Validation loss: 0.07474126, Gradient norm: 0.36346405
INFO:root:[   56] Training loss: 0.03776131, Validation loss: 0.06025778, Gradient norm: 0.46648902
INFO:root:[   57] Training loss: 0.03327978, Validation loss: 0.05916345, Gradient norm: 0.30282364
INFO:root:[   58] Training loss: 0.03510500, Validation loss: 0.06536439, Gradient norm: 0.40441582
INFO:root:[   59] Training loss: 0.03296775, Validation loss: 0.06189902, Gradient norm: 0.30204186
INFO:root:[   60] Training loss: 0.03678794, Validation loss: 0.06110126, Gradient norm: 0.46648190
INFO:root:[   61] Training loss: 0.03497771, Validation loss: 0.06653452, Gradient norm: 0.40319544
INFO:root:[   62] Training loss: 0.03797191, Validation loss: 0.05990106, Gradient norm: 0.48891081
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 3031.091s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05385
INFO:root:EnergyScoreTrain: 0.03506
INFO:root:CRPSTrain: 0.02769
INFO:root:Gaussian NLLTrain: -1.67474
INFO:root:CoverageTrain: 0.99158
INFO:root:IntervalWidthTrain: 0.2767
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.081
INFO:root:EnergyScoreValidation: 0.05843
INFO:root:CRPSValidation: 0.04746
INFO:root:Gaussian NLLValidation: -1.37852
INFO:root:CoverageValidation: 0.93274
INFO:root:IntervalWidthValidation: 0.27465
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08294
INFO:root:EnergyScoreTest: 0.0592
INFO:root:CRPSTest: 0.048
INFO:root:Gaussian NLLTest: -1.36031
INFO:root:CoverageTest: 0.92808
INFO:root:IntervalWidthTest: 0.27526
INFO:root:###6 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27734237, Validation loss: 0.13673857, Gradient norm: 1.56918584
INFO:root:[    2] Training loss: 0.13235014, Validation loss: 0.11837232, Gradient norm: 1.30400986
INFO:root:[    3] Training loss: 0.10618485, Validation loss: 0.08752454, Gradient norm: 1.04852945
INFO:root:[    4] Training loss: 0.09998832, Validation loss: 0.12831442, Gradient norm: 0.97307643
INFO:root:[    5] Training loss: 0.09963152, Validation loss: 0.08073943, Gradient norm: 0.97923366
INFO:root:[    6] Training loss: 0.08093576, Validation loss: 0.07996160, Gradient norm: 0.69010020
INFO:root:[    7] Training loss: 0.07599239, Validation loss: 0.07173102, Gradient norm: 0.60450493
INFO:root:[    8] Training loss: 0.07943521, Validation loss: 0.07400224, Gradient norm: 0.71174841
INFO:root:[    9] Training loss: 0.07221652, Validation loss: 0.08843308, Gradient norm: 0.59399245
INFO:root:[   10] Training loss: 0.07316962, Validation loss: 0.06965781, Gradient norm: 0.58898913
INFO:root:[   11] Training loss: 0.07210715, Validation loss: 0.07910034, Gradient norm: 0.59329271
INFO:root:[   12] Training loss: 0.07038961, Validation loss: 0.06816649, Gradient norm: 0.66185335
INFO:root:[   13] Training loss: 0.06048442, Validation loss: 0.07308333, Gradient norm: 0.41162127
INFO:root:[   14] Training loss: 0.06222894, Validation loss: 0.06760617, Gradient norm: 0.44998087
INFO:root:[   15] Training loss: 0.06938430, Validation loss: 0.07205341, Gradient norm: 0.62580271
INFO:root:[   16] Training loss: 0.06259009, Validation loss: 0.06440009, Gradient norm: 0.54297894
INFO:root:[   17] Training loss: 0.05849569, Validation loss: 0.08090492, Gradient norm: 0.42727139
INFO:root:[   18] Training loss: 0.06503618, Validation loss: 0.06043134, Gradient norm: 0.57042093
INFO:root:[   19] Training loss: 0.06011734, Validation loss: 0.06826635, Gradient norm: 0.51400487
INFO:root:[   20] Training loss: 0.06179796, Validation loss: 0.06197952, Gradient norm: 0.55295626
INFO:root:[   21] Training loss: 0.05837758, Validation loss: 0.07272559, Gradient norm: 0.47895385
INFO:root:[   22] Training loss: 0.05391306, Validation loss: 0.06373758, Gradient norm: 0.42651112
INFO:root:[   23] Training loss: 0.05095032, Validation loss: 0.06575026, Gradient norm: 0.33977132
INFO:root:[   24] Training loss: 0.05370790, Validation loss: 0.07183543, Gradient norm: 0.49698722
INFO:root:[   25] Training loss: 0.05602107, Validation loss: 0.06633291, Gradient norm: 0.47241147
INFO:root:[   26] Training loss: 0.06162045, Validation loss: 0.06754921, Gradient norm: 0.59683479
INFO:root:[   27] Training loss: 0.05089957, Validation loss: 0.06877410, Gradient norm: 0.37319326
INFO:root:[   28] Training loss: 0.05118981, Validation loss: 0.06399678, Gradient norm: 0.47912300
INFO:root:[   29] Training loss: 0.05063661, Validation loss: 0.06448403, Gradient norm: 0.44476070
INFO:root:[   30] Training loss: 0.05292779, Validation loss: 0.06547160, Gradient norm: 0.50326639
INFO:root:[   31] Training loss: 0.04812077, Validation loss: 0.06424732, Gradient norm: 0.36033889
INFO:root:[   32] Training loss: 0.04590681, Validation loss: 0.06262557, Gradient norm: 0.31146314
INFO:root:[   33] Training loss: 0.04957424, Validation loss: 0.06525933, Gradient norm: 0.45299816
INFO:root:[   34] Training loss: 0.05331701, Validation loss: 0.06257395, Gradient norm: 0.55647621
INFO:root:[   35] Training loss: 0.04761992, Validation loss: 0.06449551, Gradient norm: 0.41143038
INFO:root:[   36] Training loss: 0.04973063, Validation loss: 0.06090229, Gradient norm: 0.49904356
INFO:root:[   37] Training loss: 0.04462496, Validation loss: 0.06995967, Gradient norm: 0.38199503
INFO:root:[   38] Training loss: 0.04654970, Validation loss: 0.06199027, Gradient norm: 0.42721271
INFO:root:[   39] Training loss: 0.04752861, Validation loss: 0.06209093, Gradient norm: 0.45451397
INFO:root:[   40] Training loss: 0.04721348, Validation loss: 0.07163249, Gradient norm: 0.47875876
INFO:root:[   41] Training loss: 0.04611464, Validation loss: 0.06563456, Gradient norm: 0.45115843
INFO:root:[   42] Training loss: 0.04461335, Validation loss: 0.05789561, Gradient norm: 0.40856409
INFO:root:[   43] Training loss: 0.04199229, Validation loss: 0.06785907, Gradient norm: 0.35840652
INFO:root:[   44] Training loss: 0.04266641, Validation loss: 0.05887438, Gradient norm: 0.40810732
INFO:root:[   45] Training loss: 0.04844746, Validation loss: 0.06025498, Gradient norm: 0.49592507
INFO:root:[   46] Training loss: 0.04476323, Validation loss: 0.06306257, Gradient norm: 0.42185187
INFO:root:[   47] Training loss: 0.04305389, Validation loss: 0.06480994, Gradient norm: 0.48372932
INFO:root:[   48] Training loss: 0.04071188, Validation loss: 0.06189736, Gradient norm: 0.38500673
INFO:root:[   49] Training loss: 0.04233444, Validation loss: 0.06762681, Gradient norm: 0.43211729
INFO:root:[   50] Training loss: 0.04244643, Validation loss: 0.06026312, Gradient norm: 0.43946559
INFO:root:[   51] Training loss: 0.04091411, Validation loss: 0.05764543, Gradient norm: 0.38367618
INFO:root:[   52] Training loss: 0.04440472, Validation loss: 0.05839451, Gradient norm: 0.51873636
INFO:root:[   53] Training loss: 0.04073700, Validation loss: 0.06227451, Gradient norm: 0.39431100
INFO:root:[   54] Training loss: 0.04274860, Validation loss: 0.07004912, Gradient norm: 0.47453692
INFO:root:[   55] Training loss: 0.03997587, Validation loss: 0.06088216, Gradient norm: 0.39216399
INFO:root:[   56] Training loss: 0.04051814, Validation loss: 0.06377459, Gradient norm: 0.41512213
INFO:root:[   57] Training loss: 0.03851001, Validation loss: 0.06257016, Gradient norm: 0.38977748
INFO:root:[   58] Training loss: 0.03830109, Validation loss: 0.06444575, Gradient norm: 0.38510785
INFO:root:[   59] Training loss: 0.03695622, Validation loss: 0.06140050, Gradient norm: 0.29238146
INFO:root:[   60] Training loss: 0.03875681, Validation loss: 0.06666510, Gradient norm: 0.36706032
INFO:root:[   61] Training loss: 0.04080399, Validation loss: 0.06479537, Gradient norm: 0.47557464
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 2988.486s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06171
INFO:root:EnergyScoreTrain: 0.03906
INFO:root:CRPSTrain: 0.03135
INFO:root:Gaussian NLLTrain: -1.53029
INFO:root:CoverageTrain: 0.99372
INFO:root:IntervalWidthTrain: 0.30867
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08652
INFO:root:EnergyScoreValidation: 0.05873
INFO:root:CRPSValidation: 0.04785
INFO:root:Gaussian NLLValidation: -1.32806
INFO:root:CoverageValidation: 0.95271
INFO:root:IntervalWidthValidation: 0.30689
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08933
INFO:root:EnergyScoreTest: 0.06127
INFO:root:CRPSTest: 0.05001
INFO:root:Gaussian NLLTest: -1.30023
INFO:root:CoverageTest: 0.947
INFO:root:IntervalWidthTest: 0.30814
INFO:root:###7 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27174385, Validation loss: 0.15985709, Gradient norm: 1.33409974
INFO:root:[    2] Training loss: 0.13360508, Validation loss: 0.10405861, Gradient norm: 1.18092349
INFO:root:[    3] Training loss: 0.10928695, Validation loss: 0.13260395, Gradient norm: 0.95442989
INFO:root:[    4] Training loss: 0.09748830, Validation loss: 0.12079618, Gradient norm: 0.80982616
INFO:root:[    5] Training loss: 0.09647577, Validation loss: 0.08992450, Gradient norm: 0.83856506
INFO:root:[    6] Training loss: 0.08673670, Validation loss: 0.10992641, Gradient norm: 0.74542252
INFO:root:[    7] Training loss: 0.07964435, Validation loss: 0.07714655, Gradient norm: 0.61103076
INFO:root:[    8] Training loss: 0.07971860, Validation loss: 0.07361273, Gradient norm: 0.66778509
INFO:root:[    9] Training loss: 0.07147971, Validation loss: 0.08461276, Gradient norm: 0.47409915
INFO:root:[   10] Training loss: 0.07671386, Validation loss: 0.07735173, Gradient norm: 0.57204673
INFO:root:[   11] Training loss: 0.07067276, Validation loss: 0.07637071, Gradient norm: 0.52237487
INFO:root:[   12] Training loss: 0.07087271, Validation loss: 0.06672646, Gradient norm: 0.61441877
INFO:root:[   13] Training loss: 0.06459852, Validation loss: 0.06507513, Gradient norm: 0.43875826
INFO:root:[   14] Training loss: 0.06289141, Validation loss: 0.06707445, Gradient norm: 0.38186707
INFO:root:[   15] Training loss: 0.07129465, Validation loss: 0.08069645, Gradient norm: 0.60501912
INFO:root:[   16] Training loss: 0.06235180, Validation loss: 0.06565546, Gradient norm: 0.45966706
INFO:root:[   17] Training loss: 0.05919472, Validation loss: 0.07238039, Gradient norm: 0.41264632
INFO:root:[   18] Training loss: 0.06350636, Validation loss: 0.07083781, Gradient norm: 0.49979574
INFO:root:[   19] Training loss: 0.06161676, Validation loss: 0.06466994, Gradient norm: 0.43897393
INFO:root:[   20] Training loss: 0.06346315, Validation loss: 0.07640349, Gradient norm: 0.56238713
INFO:root:[   21] Training loss: 0.05970790, Validation loss: 0.06283092, Gradient norm: 0.47894357
INFO:root:[   22] Training loss: 0.05998524, Validation loss: 0.06341579, Gradient norm: 0.46828656
INFO:root:[   23] Training loss: 0.05381472, Validation loss: 0.06358235, Gradient norm: 0.37310791
INFO:root:[   24] Training loss: 0.05687489, Validation loss: 0.06598503, Gradient norm: 0.45075434
INFO:root:[   25] Training loss: 0.06066245, Validation loss: 0.06680794, Gradient norm: 0.56817509
INFO:root:[   26] Training loss: 0.05787338, Validation loss: 0.07640206, Gradient norm: 0.50757360
INFO:root:[   27] Training loss: 0.05622936, Validation loss: 0.07479283, Gradient norm: 0.43760388
INFO:root:[   28] Training loss: 0.05486944, Validation loss: 0.06705345, Gradient norm: 0.43038011
INFO:root:[   29] Training loss: 0.05527682, Validation loss: 0.06857295, Gradient norm: 0.48922136
INFO:root:[   30] Training loss: 0.05367777, Validation loss: 0.06269692, Gradient norm: 0.45477277
INFO:root:[   31] Training loss: 0.05148270, Validation loss: 0.07228909, Gradient norm: 0.38284511
INFO:root:[   32] Training loss: 0.05025085, Validation loss: 0.07275212, Gradient norm: 0.40398411
INFO:root:[   33] Training loss: 0.05263102, Validation loss: 0.06346022, Gradient norm: 0.49814683
INFO:root:[   34] Training loss: 0.05300119, Validation loss: 0.07124251, Gradient norm: 0.46985906
INFO:root:[   35] Training loss: 0.05375815, Validation loss: 0.06625211, Gradient norm: 0.50098249
INFO:root:[   36] Training loss: 0.05433430, Validation loss: 0.07096037, Gradient norm: 0.51759708
INFO:root:[   37] Training loss: 0.04746952, Validation loss: 0.06670689, Gradient norm: 0.37139222
INFO:root:[   38] Training loss: 0.04861155, Validation loss: 0.06322209, Gradient norm: 0.41414099
INFO:root:[   39] Training loss: 0.04524469, Validation loss: 0.07268701, Gradient norm: 0.31054313
INFO:root:[   40] Training loss: 0.04895036, Validation loss: 0.06631553, Gradient norm: 0.39661823
INFO:root:[   41] Training loss: 0.04825334, Validation loss: 0.06423281, Gradient norm: 0.47661685
INFO:root:[   42] Training loss: 0.04833292, Validation loss: 0.06227515, Gradient norm: 0.44788047
INFO:root:[   43] Training loss: 0.04481141, Validation loss: 0.06361773, Gradient norm: 0.35821673
INFO:root:[   44] Training loss: 0.05017488, Validation loss: 0.06271265, Gradient norm: 0.50153645
INFO:root:[   45] Training loss: 0.04839664, Validation loss: 0.06131376, Gradient norm: 0.44422061
INFO:root:[   46] Training loss: 0.04850007, Validation loss: 0.06424065, Gradient norm: 0.44260061
INFO:root:[   47] Training loss: 0.04328615, Validation loss: 0.06404363, Gradient norm: 0.38590344
INFO:root:[   48] Training loss: 0.04446409, Validation loss: 0.06267643, Gradient norm: 0.41993296
INFO:root:[   49] Training loss: 0.04192965, Validation loss: 0.06935776, Gradient norm: 0.33203792
INFO:root:[   50] Training loss: 0.04722472, Validation loss: 0.05878187, Gradient norm: 0.50449871
INFO:root:[   51] Training loss: 0.04734868, Validation loss: 0.06971311, Gradient norm: 0.49049546
INFO:root:[   52] Training loss: 0.04510573, Validation loss: 0.07177341, Gradient norm: 0.45173902
INFO:root:[   53] Training loss: 0.04567564, Validation loss: 0.06323446, Gradient norm: 0.45489900
INFO:root:[   54] Training loss: 0.04314500, Validation loss: 0.06914363, Gradient norm: 0.40951241
INFO:root:[   55] Training loss: 0.04322082, Validation loss: 0.07037291, Gradient norm: 0.37393003
INFO:root:[   56] Training loss: 0.04302527, Validation loss: 0.06256893, Gradient norm: 0.41517842
INFO:root:[   57] Training loss: 0.04268089, Validation loss: 0.06321418, Gradient norm: 0.43320000
INFO:root:[   58] Training loss: 0.04149872, Validation loss: 0.06483593, Gradient norm: 0.38208552
INFO:root:[   59] Training loss: 0.04505006, Validation loss: 0.06341858, Gradient norm: 0.47241895
INFO:root:[   60] Training loss: 0.04169731, Validation loss: 0.06645109, Gradient norm: 0.40573858
INFO:root:[   61] Training loss: 0.04091405, Validation loss: 0.06027608, Gradient norm: 0.39920844
INFO:root:[   62] Training loss: 0.04216160, Validation loss: 0.06770080, Gradient norm: 0.40030127
INFO:root:[   63] Training loss: 0.04045400, Validation loss: 0.06160365, Gradient norm: 0.32142728
INFO:root:[   64] Training loss: 0.04219250, Validation loss: 0.06158306, Gradient norm: 0.40817574
INFO:root:[   65] Training loss: 0.04070917, Validation loss: 0.06284282, Gradient norm: 0.38286581
INFO:root:[   66] Training loss: 0.04040041, Validation loss: 0.06241952, Gradient norm: 0.40090259
INFO:root:[   67] Training loss: 0.04201345, Validation loss: 0.05661609, Gradient norm: 0.38407873
INFO:root:[   68] Training loss: 0.04077399, Validation loss: 0.06371522, Gradient norm: 0.43337155
INFO:root:[   69] Training loss: 0.03821423, Validation loss: 0.06956473, Gradient norm: 0.36790775
INFO:root:[   70] Training loss: 0.04114837, Validation loss: 0.06345026, Gradient norm: 0.46744878
INFO:root:[   71] Training loss: 0.04016096, Validation loss: 0.06270783, Gradient norm: 0.41694455
INFO:root:[   72] Training loss: 0.03975570, Validation loss: 0.06823740, Gradient norm: 0.42687152
INFO:root:[   73] Training loss: 0.04081748, Validation loss: 0.06358760, Gradient norm: 0.44962411
INFO:root:[   74] Training loss: 0.03983052, Validation loss: 0.06000781, Gradient norm: 0.43630419
INFO:root:[   75] Training loss: 0.03961769, Validation loss: 0.05811739, Gradient norm: 0.38618927
INFO:root:[   76] Training loss: 0.03872153, Validation loss: 0.05649502, Gradient norm: 0.37423925
INFO:root:[   77] Training loss: 0.04039644, Validation loss: 0.06702766, Gradient norm: 0.40857018
INFO:root:[   78] Training loss: 0.03870175, Validation loss: 0.06166522, Gradient norm: 0.38425406
INFO:root:[   79] Training loss: 0.03536963, Validation loss: 0.06489485, Gradient norm: 0.26686640
INFO:root:[   80] Training loss: 0.03888083, Validation loss: 0.06717505, Gradient norm: 0.44124521
INFO:root:[   81] Training loss: 0.03879615, Validation loss: 0.06180919, Gradient norm: 0.39771354
INFO:root:[   82] Training loss: 0.04023049, Validation loss: 0.05977981, Gradient norm: 0.45525931
INFO:root:[   83] Training loss: 0.03868974, Validation loss: 0.06331989, Gradient norm: 0.40526737
INFO:root:[   84] Training loss: 0.03829477, Validation loss: 0.06769469, Gradient norm: 0.42705798
INFO:root:[   85] Training loss: 0.03768193, Validation loss: 0.06218574, Gradient norm: 0.40031559
INFO:root:EP 85: Early stopping
INFO:root:Training the model took 4161.366s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05012
INFO:root:EnergyScoreTrain: 0.03618
INFO:root:CRPSTrain: 0.02862
INFO:root:Gaussian NLLTrain: -1.603
INFO:root:CoverageTrain: 0.99758
INFO:root:IntervalWidthTrain: 0.32074
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07711
INFO:root:EnergyScoreValidation: 0.05756
INFO:root:CRPSValidation: 0.04652
INFO:root:Gaussian NLLValidation: -1.42316
INFO:root:CoverageValidation: 0.96889
INFO:root:IntervalWidthValidation: 0.31757
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07724
INFO:root:EnergyScoreTest: 0.05621
INFO:root:CRPSTest: 0.0454
INFO:root:Gaussian NLLTest: -1.4284
INFO:root:CoverageTest: 0.97082
INFO:root:IntervalWidthTest: 0.31823
INFO:root:###8 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27813733, Validation loss: 0.15370193, Gradient norm: 1.29331683
INFO:root:[    2] Training loss: 0.13331738, Validation loss: 0.11942545, Gradient norm: 0.97741845
INFO:root:[    3] Training loss: 0.10677158, Validation loss: 0.11067705, Gradient norm: 0.80232035
INFO:root:[    4] Training loss: 0.10242537, Validation loss: 0.08677647, Gradient norm: 0.82787199
INFO:root:[    5] Training loss: 0.09728729, Validation loss: 0.08364991, Gradient norm: 0.80605255
INFO:root:[    6] Training loss: 0.08677669, Validation loss: 0.09512444, Gradient norm: 0.65795316
INFO:root:[    7] Training loss: 0.07884749, Validation loss: 0.07115562, Gradient norm: 0.49587529
INFO:root:[    8] Training loss: 0.08412941, Validation loss: 0.07629469, Gradient norm: 0.66611962
INFO:root:[    9] Training loss: 0.07505836, Validation loss: 0.09795862, Gradient norm: 0.49972489
INFO:root:[   10] Training loss: 0.07770303, Validation loss: 0.07647061, Gradient norm: 0.59641306
INFO:root:[   11] Training loss: 0.07155289, Validation loss: 0.07742405, Gradient norm: 0.49205835
INFO:root:[   12] Training loss: 0.07802024, Validation loss: 0.07770031, Gradient norm: 0.62505477
INFO:root:[   13] Training loss: 0.06563422, Validation loss: 0.07390644, Gradient norm: 0.40869814
INFO:root:[   14] Training loss: 0.06864617, Validation loss: 0.06866846, Gradient norm: 0.51946982
INFO:root:[   15] Training loss: 0.07348432, Validation loss: 0.06974503, Gradient norm: 0.59319967
INFO:root:[   16] Training loss: 0.06465072, Validation loss: 0.09504934, Gradient norm: 0.44003005
INFO:root:[   17] Training loss: 0.06188528, Validation loss: 0.06491201, Gradient norm: 0.38525301
INFO:root:[   18] Training loss: 0.06364542, Validation loss: 0.06131581, Gradient norm: 0.44437737
INFO:root:[   19] Training loss: 0.06877539, Validation loss: 0.07035310, Gradient norm: 0.61306074
INFO:root:[   20] Training loss: 0.06506498, Validation loss: 0.07230309, Gradient norm: 0.44118423
INFO:root:[   21] Training loss: 0.06214328, Validation loss: 0.06840666, Gradient norm: 0.43953527
INFO:root:[   22] Training loss: 0.06361569, Validation loss: 0.06820912, Gradient norm: 0.53765296
INFO:root:[   23] Training loss: 0.05523425, Validation loss: 0.06252982, Gradient norm: 0.30674869
INFO:root:[   24] Training loss: 0.06047418, Validation loss: 0.07464063, Gradient norm: 0.51039836
INFO:root:[   25] Training loss: 0.06352815, Validation loss: 0.06183503, Gradient norm: 0.50367963
INFO:root:[   26] Training loss: 0.06351886, Validation loss: 0.06959590, Gradient norm: 0.53819430
INFO:root:[   27] Training loss: 0.05765703, Validation loss: 0.07968492, Gradient norm: 0.39899818
INFO:root:[   28] Training loss: 0.05894789, Validation loss: 0.06950940, Gradient norm: 0.49306882
INFO:root:[   29] Training loss: 0.05805059, Validation loss: 0.07081112, Gradient norm: 0.46563200
INFO:root:[   30] Training loss: 0.05554293, Validation loss: 0.06275146, Gradient norm: 0.41896649
INFO:root:[   31] Training loss: 0.05428380, Validation loss: 0.06612582, Gradient norm: 0.41605455
INFO:root:[   32] Training loss: 0.05369853, Validation loss: 0.07685672, Gradient norm: 0.40674579
INFO:root:[   33] Training loss: 0.05520553, Validation loss: 0.06204144, Gradient norm: 0.46410677
INFO:root:[   34] Training loss: 0.05719686, Validation loss: 0.07566805, Gradient norm: 0.49755831
INFO:root:[   35] Training loss: 0.05469342, Validation loss: 0.06266383, Gradient norm: 0.44221686
INFO:root:[   36] Training loss: 0.05343737, Validation loss: 0.06167652, Gradient norm: 0.43125497
INFO:root:[   37] Training loss: 0.05242600, Validation loss: 0.06367719, Gradient norm: 0.42508309
INFO:root:[   38] Training loss: 0.05324520, Validation loss: 0.06353684, Gradient norm: 0.39528175
INFO:root:[   39] Training loss: 0.04889419, Validation loss: 0.06102135, Gradient norm: 0.36857905
INFO:root:[   40] Training loss: 0.05018920, Validation loss: 0.06708831, Gradient norm: 0.38378456
INFO:root:[   41] Training loss: 0.04780523, Validation loss: 0.06286330, Gradient norm: 0.36083820
INFO:root:[   42] Training loss: 0.05063015, Validation loss: 0.06077919, Gradient norm: 0.45190148
INFO:root:[   43] Training loss: 0.04939574, Validation loss: 0.06284990, Gradient norm: 0.43182536
INFO:root:[   44] Training loss: 0.04930479, Validation loss: 0.06254450, Gradient norm: 0.38906044
INFO:root:[   45] Training loss: 0.05362201, Validation loss: 0.06229205, Gradient norm: 0.52426296
INFO:root:[   46] Training loss: 0.05097695, Validation loss: 0.06681228, Gradient norm: 0.45496497
INFO:root:[   47] Training loss: 0.04689383, Validation loss: 0.06700631, Gradient norm: 0.36131365
INFO:root:[   48] Training loss: 0.04824391, Validation loss: 0.06402277, Gradient norm: 0.40528917
INFO:root:[   49] Training loss: 0.04597142, Validation loss: 0.07105495, Gradient norm: 0.36178644
INFO:root:[   50] Training loss: 0.04833203, Validation loss: 0.06190167, Gradient norm: 0.49867155
INFO:root:[   51] Training loss: 0.04965659, Validation loss: 0.07474897, Gradient norm: 0.47552507
INFO:root:[   52] Training loss: 0.04580486, Validation loss: 0.06825536, Gradient norm: 0.40076643
INFO:root:[   53] Training loss: 0.04516268, Validation loss: 0.05794989, Gradient norm: 0.37696791
INFO:root:[   54] Training loss: 0.04425502, Validation loss: 0.06951814, Gradient norm: 0.34867073
INFO:root:[   55] Training loss: 0.04601338, Validation loss: 0.07200209, Gradient norm: 0.40030296
INFO:root:[   56] Training loss: 0.04640369, Validation loss: 0.06397917, Gradient norm: 0.41690542
INFO:root:[   57] Training loss: 0.04694714, Validation loss: 0.06060499, Gradient norm: 0.44496235
INFO:root:[   58] Training loss: 0.04454213, Validation loss: 0.07136865, Gradient norm: 0.38389518
INFO:root:[   59] Training loss: 0.04646459, Validation loss: 0.06416653, Gradient norm: 0.45198622
INFO:root:[   60] Training loss: 0.04305307, Validation loss: 0.06246022, Gradient norm: 0.36738972
INFO:root:[   61] Training loss: 0.04238706, Validation loss: 0.05971579, Gradient norm: 0.35193786
INFO:root:[   62] Training loss: 0.04387061, Validation loss: 0.06686366, Gradient norm: 0.37363851
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 3030.09s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06094
INFO:root:EnergyScoreTrain: 0.04245
INFO:root:CRPSTrain: 0.03347
INFO:root:Gaussian NLLTrain: -1.44652
INFO:root:CoverageTrain: 0.99778
INFO:root:IntervalWidthTrain: 0.37399
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07802
INFO:root:EnergyScoreValidation: 0.05689
INFO:root:CRPSValidation: 0.04573
INFO:root:Gaussian NLLValidation: -1.35516
INFO:root:CoverageValidation: 0.98392
INFO:root:IntervalWidthValidation: 0.37085
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07807
INFO:root:EnergyScoreTest: 0.05621
INFO:root:CRPSTest: 0.04512
INFO:root:Gaussian NLLTest: -1.35423
INFO:root:CoverageTest: 0.98485
INFO:root:IntervalWidthTest: 0.37264
INFO:root:###9 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28034605, Validation loss: 0.15254824, Gradient norm: 1.13925584
INFO:root:[    2] Training loss: 0.13877785, Validation loss: 0.15010201, Gradient norm: 0.87500159
INFO:root:[    3] Training loss: 0.11780386, Validation loss: 0.10428774, Gradient norm: 0.80956152
INFO:root:[    4] Training loss: 0.10929596, Validation loss: 0.16280127, Gradient norm: 0.75250134
INFO:root:[    5] Training loss: 0.10335796, Validation loss: 0.08730695, Gradient norm: 0.74009464
INFO:root:[    6] Training loss: 0.09268163, Validation loss: 0.12048002, Gradient norm: 0.62154149
INFO:root:[    7] Training loss: 0.08778735, Validation loss: 0.08295419, Gradient norm: 0.54727093
INFO:root:[    8] Training loss: 0.08767543, Validation loss: 0.08812560, Gradient norm: 0.60840769
INFO:root:[    9] Training loss: 0.08056601, Validation loss: 0.08757572, Gradient norm: 0.48394095
INFO:root:[   10] Training loss: 0.08389743, Validation loss: 0.07392753, Gradient norm: 0.58751197
INFO:root:[   11] Training loss: 0.07407342, Validation loss: 0.07684790, Gradient norm: 0.46958916
INFO:root:[   12] Training loss: 0.08182259, Validation loss: 0.07143181, Gradient norm: 0.62249186
INFO:root:[   13] Training loss: 0.06920779, Validation loss: 0.07480598, Gradient norm: 0.39815126
INFO:root:[   14] Training loss: 0.07499653, Validation loss: 0.07736444, Gradient norm: 0.53638763
INFO:root:[   15] Training loss: 0.07726483, Validation loss: 0.06941116, Gradient norm: 0.58307248
INFO:root:[   16] Training loss: 0.07157286, Validation loss: 0.07113719, Gradient norm: 0.50469459
INFO:root:[   17] Training loss: 0.06763142, Validation loss: 0.07093545, Gradient norm: 0.40035139
INFO:root:[   18] Training loss: 0.06747596, Validation loss: 0.07107274, Gradient norm: 0.48256046
INFO:root:[   19] Training loss: 0.06580410, Validation loss: 0.06575895, Gradient norm: 0.43095198
INFO:root:[   20] Training loss: 0.07560387, Validation loss: 0.07291115, Gradient norm: 0.57140901
INFO:root:[   21] Training loss: 0.06810785, Validation loss: 0.07817693, Gradient norm: 0.50676503
INFO:root:[   22] Training loss: 0.06745229, Validation loss: 0.06534300, Gradient norm: 0.53161867
INFO:root:[   23] Training loss: 0.06137253, Validation loss: 0.06354544, Gradient norm: 0.38967033
INFO:root:[   24] Training loss: 0.06426224, Validation loss: 0.08072151, Gradient norm: 0.44897667
INFO:root:[   25] Training loss: 0.06630732, Validation loss: 0.06727325, Gradient norm: 0.50060866
INFO:root:[   26] Training loss: 0.07096876, Validation loss: 0.08014924, Gradient norm: 0.63039551
INFO:root:[   27] Training loss: 0.06175870, Validation loss: 0.08049810, Gradient norm: 0.38840756
INFO:root:[   28] Training loss: 0.06209638, Validation loss: 0.07857323, Gradient norm: 0.47890662
INFO:root:[   29] Training loss: 0.06522151, Validation loss: 0.06702557, Gradient norm: 0.50461592
INFO:root:[   30] Training loss: 0.05995408, Validation loss: 0.06615115, Gradient norm: 0.41227672
INFO:root:[   31] Training loss: 0.05781299, Validation loss: 0.07919210, Gradient norm: 0.36566966
INFO:root:[   32] Training loss: 0.06026491, Validation loss: 0.07598642, Gradient norm: 0.45550855
INFO:root:[   33] Training loss: 0.05981865, Validation loss: 0.07570657, Gradient norm: 0.46613382
INFO:root:[   34] Training loss: 0.06119066, Validation loss: 0.07053792, Gradient norm: 0.49566851
INFO:root:[   35] Training loss: 0.05800577, Validation loss: 0.06898563, Gradient norm: 0.42569708
INFO:root:[   36] Training loss: 0.06218987, Validation loss: 0.06663019, Gradient norm: 0.48435580
INFO:root:[   37] Training loss: 0.05508179, Validation loss: 0.06448834, Gradient norm: 0.37027738
INFO:root:[   38] Training loss: 0.06032373, Validation loss: 0.06741887, Gradient norm: 0.51176333
INFO:root:[   39] Training loss: 0.05444854, Validation loss: 0.06874300, Gradient norm: 0.35209141
INFO:root:[   40] Training loss: 0.05727572, Validation loss: 0.07133013, Gradient norm: 0.46596158
INFO:root:[   41] Training loss: 0.05370467, Validation loss: 0.06496354, Gradient norm: 0.38500035
INFO:root:[   42] Training loss: 0.05396451, Validation loss: 0.06248842, Gradient norm: 0.39219946
INFO:root:[   43] Training loss: 0.05543120, Validation loss: 0.06521271, Gradient norm: 0.45150995
INFO:root:[   44] Training loss: 0.05160965, Validation loss: 0.06487453, Gradient norm: 0.32165704
INFO:root:[   45] Training loss: 0.05324355, Validation loss: 0.06051246, Gradient norm: 0.36866577
INFO:root:[   46] Training loss: 0.05464247, Validation loss: 0.06120191, Gradient norm: 0.47205808
INFO:root:[   47] Training loss: 0.05475024, Validation loss: 0.06008767, Gradient norm: 0.42017697
INFO:root:[   48] Training loss: 0.05166335, Validation loss: 0.08421641, Gradient norm: 0.36097344
INFO:root:[   49] Training loss: 0.05237167, Validation loss: 0.06331347, Gradient norm: 0.36959614
INFO:root:[   50] Training loss: 0.05478705, Validation loss: 0.07064705, Gradient norm: 0.46594533
INFO:root:[   51] Training loss: 0.05216369, Validation loss: 0.07519881, Gradient norm: 0.42527290
INFO:root:[   52] Training loss: 0.05242507, Validation loss: 0.07257199, Gradient norm: 0.45573099
INFO:root:[   53] Training loss: 0.05234903, Validation loss: 0.06646728, Gradient norm: 0.44022315
INFO:root:[   54] Training loss: 0.05049031, Validation loss: 0.06468541, Gradient norm: 0.36496548
INFO:root:[   55] Training loss: 0.04992910, Validation loss: 0.06748719, Gradient norm: 0.33768017
INFO:root:[   56] Training loss: 0.04908431, Validation loss: 0.06592672, Gradient norm: 0.37789463
INFO:root:[   57] Training loss: 0.05407035, Validation loss: 0.06638578, Gradient norm: 0.49995658
INFO:root:[   58] Training loss: 0.05142430, Validation loss: 0.06031561, Gradient norm: 0.44354398
INFO:root:[   59] Training loss: 0.04821403, Validation loss: 0.06119884, Gradient norm: 0.38564153
INFO:root:[   60] Training loss: 0.04773961, Validation loss: 0.06542647, Gradient norm: 0.34531043
INFO:root:[   61] Training loss: 0.04820913, Validation loss: 0.06605488, Gradient norm: 0.40093028
INFO:root:[   62] Training loss: 0.04772201, Validation loss: 0.07477086, Gradient norm: 0.37041184
INFO:root:[   63] Training loss: 0.04796718, Validation loss: 0.06714569, Gradient norm: 0.38295374
INFO:root:[   64] Training loss: 0.04756605, Validation loss: 0.06307245, Gradient norm: 0.38427814
INFO:root:[   65] Training loss: 0.04574914, Validation loss: 0.06910377, Gradient norm: 0.33104678
INFO:root:[   66] Training loss: 0.04613400, Validation loss: 0.07559442, Gradient norm: 0.37105795
INFO:root:[   67] Training loss: 0.04712136, Validation loss: 0.07046383, Gradient norm: 0.37441623
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 3281.762s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05523
INFO:root:EnergyScoreTrain: 0.04635
INFO:root:CRPSTrain: 0.03648
INFO:root:Gaussian NLLTrain: -1.33964
INFO:root:CoverageTrain: 0.99828
INFO:root:IntervalWidthTrain: 0.44227
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08137
INFO:root:EnergyScoreValidation: 0.05933
INFO:root:CRPSValidation: 0.04757
INFO:root:Gaussian NLLValidation: -1.24519
INFO:root:CoverageValidation: 0.99079
INFO:root:IntervalWidthValidation: 0.43936
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08213
INFO:root:EnergyScoreTest: 0.05913
INFO:root:CRPSTest: 0.04748
INFO:root:Gaussian NLLTest: -1.24096
INFO:root:CoverageTest: 0.99184
INFO:root:IntervalWidthTest: 0.43962
INFO:root:###10 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.24495109, Validation loss: 0.18111213, Gradient norm: 1.37390125
INFO:root:[    2] Training loss: 0.12309339, Validation loss: 0.11985941, Gradient norm: 1.23788822
INFO:root:[    3] Training loss: 0.10210796, Validation loss: 0.08764127, Gradient norm: 1.07284285
INFO:root:[    4] Training loss: 0.10003566, Validation loss: 0.08612709, Gradient norm: 1.07238520
INFO:root:[    5] Training loss: 0.08818012, Validation loss: 0.07910658, Gradient norm: 0.87946725
INFO:root:[    6] Training loss: 0.08336241, Validation loss: 0.07370801, Gradient norm: 0.82746957
INFO:root:[    7] Training loss: 0.07394479, Validation loss: 0.08260813, Gradient norm: 0.74877116
INFO:root:[    8] Training loss: 0.07622774, Validation loss: 0.07551789, Gradient norm: 0.71225113
INFO:root:[    9] Training loss: 0.07212090, Validation loss: 0.08426420, Gradient norm: 0.74349836
INFO:root:[   10] Training loss: 0.06864827, Validation loss: 0.06555330, Gradient norm: 0.69541261
INFO:root:[   11] Training loss: 0.06550145, Validation loss: 0.07177353, Gradient norm: 0.64988340
INFO:root:[   12] Training loss: 0.06158522, Validation loss: 0.06851675, Gradient norm: 0.65817715
INFO:root:[   13] Training loss: 0.05961074, Validation loss: 0.07320226, Gradient norm: 0.59083964
INFO:root:[   14] Training loss: 0.06375639, Validation loss: 0.06742112, Gradient norm: 0.65169258
INFO:root:[   15] Training loss: 0.05957911, Validation loss: 0.07108363, Gradient norm: 0.59600353
INFO:root:[   16] Training loss: 0.05399178, Validation loss: 0.06456049, Gradient norm: 0.51814279
INFO:root:[   17] Training loss: 0.05674489, Validation loss: 0.07019610, Gradient norm: 0.65167632
INFO:root:[   18] Training loss: 0.05391318, Validation loss: 0.06773499, Gradient norm: 0.61883126
INFO:root:[   19] Training loss: 0.05126481, Validation loss: 0.06336958, Gradient norm: 0.47749923
INFO:root:[   20] Training loss: 0.05265538, Validation loss: 0.06967806, Gradient norm: 0.61585986
INFO:root:[   21] Training loss: 0.04953694, Validation loss: 0.06624653, Gradient norm: 0.59969219
INFO:root:[   22] Training loss: 0.04924296, Validation loss: 0.06203908, Gradient norm: 0.61054883
INFO:root:[   23] Training loss: 0.05130384, Validation loss: 0.07336765, Gradient norm: 0.60771745
INFO:root:[   24] Training loss: 0.04486917, Validation loss: 0.06235514, Gradient norm: 0.54931898
INFO:root:[   25] Training loss: 0.04460278, Validation loss: 0.06694867, Gradient norm: 0.58399620
INFO:root:[   26] Training loss: 0.04445627, Validation loss: 0.05908846, Gradient norm: 0.57778724
INFO:root:[   27] Training loss: 0.04634462, Validation loss: 0.07770879, Gradient norm: 0.57802169
INFO:root:[   28] Training loss: 0.04183969, Validation loss: 0.05972971, Gradient norm: 0.45865441
INFO:root:[   29] Training loss: 0.04292840, Validation loss: 0.06292060, Gradient norm: 0.54281465
INFO:root:[   30] Training loss: 0.03999184, Validation loss: 0.06560505, Gradient norm: 0.48006852
INFO:root:[   31] Training loss: 0.04349615, Validation loss: 0.06878004, Gradient norm: 0.58928972
INFO:root:[   32] Training loss: 0.04226682, Validation loss: 0.09104510, Gradient norm: 0.55648101
INFO:root:[   33] Training loss: 0.04247690, Validation loss: 0.06402340, Gradient norm: 0.59742470
INFO:root:[   34] Training loss: 0.03777166, Validation loss: 0.07293096, Gradient norm: 0.52582147
INFO:root:[   35] Training loss: 0.04078800, Validation loss: 0.06111317, Gradient norm: 0.61841741
INFO:root:[   36] Training loss: 0.03776107, Validation loss: 0.06622598, Gradient norm: 0.55827955
INFO:root:[   37] Training loss: 0.03713903, Validation loss: 0.06660096, Gradient norm: 0.50338045
INFO:root:[   38] Training loss: 0.04009093, Validation loss: 0.06318740, Gradient norm: 0.54807855
INFO:root:[   39] Training loss: 0.03614334, Validation loss: 0.06358974, Gradient norm: 0.49576412
INFO:root:[   40] Training loss: 0.03852663, Validation loss: 0.06390157, Gradient norm: 0.63470511
INFO:root:[   41] Training loss: 0.03754861, Validation loss: 0.06858071, Gradient norm: 0.60633322
INFO:root:[   42] Training loss: 0.04007148, Validation loss: 0.05930081, Gradient norm: 0.67089392
INFO:root:[   43] Training loss: 0.03771850, Validation loss: 0.07187054, Gradient norm: 0.59819662
INFO:root:[   44] Training loss: 0.03737990, Validation loss: 0.06662461, Gradient norm: 0.60288000
INFO:root:[   45] Training loss: 0.03622419, Validation loss: 0.06985180, Gradient norm: 0.52327705
INFO:root:[   46] Training loss: 0.03391419, Validation loss: 0.06924718, Gradient norm: 0.58343454
INFO:root:[   47] Training loss: 0.03651192, Validation loss: 0.06316116, Gradient norm: 0.59158525
INFO:root:[   48] Training loss: 0.03325157, Validation loss: 0.07008192, Gradient norm: 0.58245849
INFO:root:[   49] Training loss: 0.03498645, Validation loss: 0.06804128, Gradient norm: 0.56940883
INFO:root:[   50] Training loss: 0.03648621, Validation loss: 0.06153602, Gradient norm: 0.63454056
INFO:root:[   51] Training loss: 0.03232364, Validation loss: 0.07177814, Gradient norm: 0.54840051
INFO:root:[   52] Training loss: 0.03395469, Validation loss: 0.06639469, Gradient norm: 0.60839548
INFO:root:[   53] Training loss: 0.03559786, Validation loss: 0.06347106, Gradient norm: 0.63983083
INFO:root:[   54] Training loss: 0.03498718, Validation loss: 0.06423746, Gradient norm: 0.61143621
INFO:root:[   55] Training loss: 0.03221294, Validation loss: 0.05965306, Gradient norm: 0.56567457
INFO:root:[   56] Training loss: 0.02919953, Validation loss: 0.06021155, Gradient norm: 0.48096511
INFO:root:[   57] Training loss: 0.03515163, Validation loss: 0.06596134, Gradient norm: 0.63726726
INFO:root:[   58] Training loss: 0.03234513, Validation loss: 0.06530216, Gradient norm: 0.56066311
INFO:root:[   59] Training loss: 0.03200817, Validation loss: 0.06917240, Gradient norm: 0.52520425
INFO:root:[   60] Training loss: 0.03172472, Validation loss: 0.05916371, Gradient norm: 0.56071904
INFO:root:[   61] Training loss: 0.03051717, Validation loss: 0.06871192, Gradient norm: 0.54390266
INFO:root:[   62] Training loss: 0.03291753, Validation loss: 0.06457454, Gradient norm: 0.64252088
INFO:root:[   63] Training loss: 0.03061346, Validation loss: 0.06528194, Gradient norm: 0.61155312
INFO:root:[   64] Training loss: 0.03220943, Validation loss: 0.06938197, Gradient norm: 0.59166714
INFO:root:[   65] Training loss: 0.03184334, Validation loss: 0.06591943, Gradient norm: 0.66407731
INFO:root:[   66] Training loss: 0.03375211, Validation loss: 0.05994678, Gradient norm: 0.61261179
INFO:root:[   67] Training loss: 0.03134385, Validation loss: 0.06521111, Gradient norm: 0.58102306
INFO:root:[   68] Training loss: 0.03092246, Validation loss: 0.05991143, Gradient norm: 0.55694893
INFO:root:[   69] Training loss: 0.02949990, Validation loss: 0.06562312, Gradient norm: 0.57722507
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2573.932s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05999
INFO:root:EnergyScoreTrain: 0.03852
INFO:root:CRPSTrain: 0.0347
INFO:root:Gaussian NLLTrain: 570.78337
INFO:root:CoverageTrain: 0.70392
INFO:root:IntervalWidthTrain: 0.1577
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08266
INFO:root:EnergyScoreValidation: 0.05987
INFO:root:CRPSValidation: 0.05499
INFO:root:Gaussian NLLValidation: 607.76153
INFO:root:CoverageValidation: 0.62839
INFO:root:IntervalWidthValidation: 0.15861
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08335
INFO:root:EnergyScoreTest: 0.06122
INFO:root:CRPSTest: 0.05591
INFO:root:Gaussian NLLTest: 582.94645
INFO:root:CoverageTest: 0.62496
INFO:root:IntervalWidthTest: 0.15824
INFO:root:###11 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.24854245, Validation loss: 0.17387368, Gradient norm: 1.35638136
INFO:root:[    2] Training loss: 0.12857795, Validation loss: 0.12259948, Gradient norm: 1.16691300
INFO:root:[    3] Training loss: 0.10335103, Validation loss: 0.09887217, Gradient norm: 1.01820408
INFO:root:[    4] Training loss: 0.09949003, Validation loss: 0.08759457, Gradient norm: 0.97511592
INFO:root:[    5] Training loss: 0.09068065, Validation loss: 0.08201329, Gradient norm: 0.84530585
INFO:root:[    6] Training loss: 0.07624565, Validation loss: 0.08518472, Gradient norm: 0.61948767
INFO:root:[    7] Training loss: 0.08190884, Validation loss: 0.08200522, Gradient norm: 0.80163547
INFO:root:[    8] Training loss: 0.07921488, Validation loss: 0.07472070, Gradient norm: 0.69357557
INFO:root:[    9] Training loss: 0.07086852, Validation loss: 0.07257684, Gradient norm: 0.67966261
INFO:root:[   10] Training loss: 0.07211643, Validation loss: 0.06829265, Gradient norm: 0.61429335
INFO:root:[   11] Training loss: 0.06754349, Validation loss: 0.07485468, Gradient norm: 0.61072125
INFO:root:[   12] Training loss: 0.06361749, Validation loss: 0.07082270, Gradient norm: 0.65666196
INFO:root:[   13] Training loss: 0.06368474, Validation loss: 0.06623498, Gradient norm: 0.60261623
INFO:root:[   14] Training loss: 0.06271658, Validation loss: 0.07712707, Gradient norm: 0.52660589
INFO:root:[   15] Training loss: 0.06192333, Validation loss: 0.06709967, Gradient norm: 0.63078829
INFO:root:[   16] Training loss: 0.05633035, Validation loss: 0.06774930, Gradient norm: 0.55166871
INFO:root:[   17] Training loss: 0.05777627, Validation loss: 0.06052023, Gradient norm: 0.53449797
INFO:root:[   18] Training loss: 0.05787624, Validation loss: 0.06407831, Gradient norm: 0.59155157
INFO:root:[   19] Training loss: 0.05540950, Validation loss: 0.06924693, Gradient norm: 0.55893666
INFO:root:[   20] Training loss: 0.05409115, Validation loss: 0.07028198, Gradient norm: 0.57163235
INFO:root:[   21] Training loss: 0.05149027, Validation loss: 0.06579282, Gradient norm: 0.50616120
INFO:root:[   22] Training loss: 0.04842580, Validation loss: 0.06234044, Gradient norm: 0.52646558
INFO:root:[   23] Training loss: 0.05186875, Validation loss: 0.06562169, Gradient norm: 0.57735980
INFO:root:[   24] Training loss: 0.04995802, Validation loss: 0.07504499, Gradient norm: 0.51933007
INFO:root:[   25] Training loss: 0.04658832, Validation loss: 0.06409607, Gradient norm: 0.51924699
INFO:root:[   26] Training loss: 0.04887359, Validation loss: 0.06789195, Gradient norm: 0.49101775
INFO:root:[   27] Training loss: 0.04964520, Validation loss: 0.07347249, Gradient norm: 0.57787121
INFO:root:[   28] Training loss: 0.04697874, Validation loss: 0.06520822, Gradient norm: 0.54184248
INFO:root:[   29] Training loss: 0.04375892, Validation loss: 0.05832215, Gradient norm: 0.47743087
INFO:root:[   30] Training loss: 0.04336239, Validation loss: 0.07027886, Gradient norm: 0.51343710
INFO:root:[   31] Training loss: 0.04889890, Validation loss: 0.06760862, Gradient norm: 0.64047905
INFO:root:[   32] Training loss: 0.04559043, Validation loss: 0.08468294, Gradient norm: 0.58052996
INFO:root:[   33] Training loss: 0.04436041, Validation loss: 0.06521023, Gradient norm: 0.56075569
INFO:root:[   34] Training loss: 0.04065099, Validation loss: 0.06690997, Gradient norm: 0.47550539
INFO:root:[   35] Training loss: 0.04361185, Validation loss: 0.06457110, Gradient norm: 0.59491485
INFO:root:[   36] Training loss: 0.03967166, Validation loss: 0.06258298, Gradient norm: 0.45434083
INFO:root:[   37] Training loss: 0.04301354, Validation loss: 0.06651421, Gradient norm: 0.58886052
INFO:root:[   38] Training loss: 0.04227718, Validation loss: 0.06241096, Gradient norm: 0.54159272
INFO:root:[   39] Training loss: 0.04024970, Validation loss: 0.06298442, Gradient norm: 0.53240509
INFO:root:[   40] Training loss: 0.04331855, Validation loss: 0.06389115, Gradient norm: 0.55554897
INFO:root:[   41] Training loss: 0.03927302, Validation loss: 0.07126077, Gradient norm: 0.57333739
INFO:root:[   42] Training loss: 0.03940074, Validation loss: 0.06260562, Gradient norm: 0.55509339
INFO:root:[   43] Training loss: 0.04313166, Validation loss: 0.06362984, Gradient norm: 0.60870488
INFO:root:[   44] Training loss: 0.03984150, Validation loss: 0.06602924, Gradient norm: 0.53966072
INFO:root:[   45] Training loss: 0.03988926, Validation loss: 0.06855659, Gradient norm: 0.50923489
INFO:root:[   46] Training loss: 0.03786238, Validation loss: 0.06265449, Gradient norm: 0.49585629
INFO:root:[   47] Training loss: 0.03972226, Validation loss: 0.06589265, Gradient norm: 0.51325989
INFO:root:[   48] Training loss: 0.03890715, Validation loss: 0.06111386, Gradient norm: 0.57294562
INFO:root:[   49] Training loss: 0.03826482, Validation loss: 0.06554487, Gradient norm: 0.55639366
INFO:root:[   50] Training loss: 0.03801170, Validation loss: 0.06156234, Gradient norm: 0.51098756
INFO:root:[   51] Training loss: 0.03510576, Validation loss: 0.06903863, Gradient norm: 0.49907275
INFO:root:[   52] Training loss: 0.03789959, Validation loss: 0.06496918, Gradient norm: 0.58300687
INFO:root:[   53] Training loss: 0.03772241, Validation loss: 0.06385086, Gradient norm: 0.54923481
INFO:root:[   54] Training loss: 0.03648406, Validation loss: 0.06635766, Gradient norm: 0.50363345
INFO:root:[   55] Training loss: 0.03798108, Validation loss: 0.06145888, Gradient norm: 0.53954055
INFO:root:[   56] Training loss: 0.03585721, Validation loss: 0.06595488, Gradient norm: 0.54017374
INFO:root:[   57] Training loss: 0.03799271, Validation loss: 0.06136355, Gradient norm: 0.55465724
INFO:root:[   58] Training loss: 0.03423485, Validation loss: 0.05854488, Gradient norm: 0.48267063
INFO:root:[   59] Training loss: 0.03938777, Validation loss: 0.06900370, Gradient norm: 0.65255095
INFO:root:[   60] Training loss: 0.03636175, Validation loss: 0.06076375, Gradient norm: 0.55136623
INFO:root:[   61] Training loss: 0.03544836, Validation loss: 0.06584515, Gradient norm: 0.52532921
INFO:root:[   62] Training loss: 0.03720598, Validation loss: 0.06857004, Gradient norm: 0.58671903
INFO:root:[   63] Training loss: 0.03302937, Validation loss: 0.05946309, Gradient norm: 0.40204655
INFO:root:[   64] Training loss: 0.03287049, Validation loss: 0.06131346, Gradient norm: 0.52968478
INFO:root:[   65] Training loss: 0.03799248, Validation loss: 0.06256341, Gradient norm: 0.65277984
INFO:root:[   66] Training loss: 0.03370033, Validation loss: 0.06177774, Gradient norm: 0.51752078
INFO:root:[   67] Training loss: 0.03319178, Validation loss: 0.06544422, Gradient norm: 0.50502413
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 2495.521s.
INFO:root:Emptying the cuda cache took 0.01s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05281
INFO:root:EnergyScoreTrain: 0.0345
INFO:root:CRPSTrain: 0.03045
INFO:root:Gaussian NLLTrain: 312.25342
INFO:root:CoverageTrain: 0.81245
INFO:root:IntervalWidthTrain: 0.16034
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07676
INFO:root:EnergyScoreValidation: 0.05859
INFO:root:CRPSValidation: 0.05253
INFO:root:Gaussian NLLValidation: 457.01159
INFO:root:CoverageValidation: 0.73439
INFO:root:IntervalWidthValidation: 0.16148
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07566
INFO:root:EnergyScoreTest: 0.05694
INFO:root:CRPSTest: 0.05083
INFO:root:Gaussian NLLTest: 582.71034
INFO:root:CoverageTest: 0.73507
INFO:root:IntervalWidthTest: 0.16108
INFO:root:###12 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.24535924, Validation loss: 0.13874191, Gradient norm: 1.25562466
INFO:root:[    2] Training loss: 0.12705249, Validation loss: 0.10577466, Gradient norm: 1.20807133
INFO:root:[    3] Training loss: 0.11182060, Validation loss: 0.10596336, Gradient norm: 1.15271863
INFO:root:[    4] Training loss: 0.09511927, Validation loss: 0.12937261, Gradient norm: 0.85705692
INFO:root:[    5] Training loss: 0.09311038, Validation loss: 0.07819812, Gradient norm: 0.83162249
INFO:root:[    6] Training loss: 0.08744487, Validation loss: 0.08096654, Gradient norm: 0.72989077
INFO:root:[    7] Training loss: 0.08036657, Validation loss: 0.06815969, Gradient norm: 0.70678069
INFO:root:[    8] Training loss: 0.08149189, Validation loss: 0.08409779, Gradient norm: 0.76231571
INFO:root:[    9] Training loss: 0.07568172, Validation loss: 0.06694597, Gradient norm: 0.64558688
INFO:root:[   10] Training loss: 0.06937097, Validation loss: 0.06866199, Gradient norm: 0.50741956
INFO:root:[   11] Training loss: 0.07185022, Validation loss: 0.06989632, Gradient norm: 0.66862493
INFO:root:[   12] Training loss: 0.06425580, Validation loss: 0.07076677, Gradient norm: 0.49970900
INFO:root:[   13] Training loss: 0.06593570, Validation loss: 0.06728950, Gradient norm: 0.62750036
INFO:root:[   14] Training loss: 0.06702195, Validation loss: 0.06619209, Gradient norm: 0.59231502
INFO:root:[   15] Training loss: 0.06464473, Validation loss: 0.06740639, Gradient norm: 0.59221700
INFO:root:[   16] Training loss: 0.05783163, Validation loss: 0.07047614, Gradient norm: 0.51179484
INFO:root:[   17] Training loss: 0.06305072, Validation loss: 0.06496032, Gradient norm: 0.58904354
INFO:root:[   18] Training loss: 0.06104941, Validation loss: 0.06752821, Gradient norm: 0.58725199
INFO:root:[   19] Training loss: 0.05654561, Validation loss: 0.06707211, Gradient norm: 0.53599200
INFO:root:[   20] Training loss: 0.05877663, Validation loss: 0.07330374, Gradient norm: 0.57867706
INFO:root:[   21] Training loss: 0.05469580, Validation loss: 0.06238982, Gradient norm: 0.48811436
INFO:root:[   22] Training loss: 0.05564032, Validation loss: 0.06170928, Gradient norm: 0.57041488
INFO:root:[   23] Training loss: 0.05324904, Validation loss: 0.06738271, Gradient norm: 0.53422306
INFO:root:[   24] Training loss: 0.05167850, Validation loss: 0.07064789, Gradient norm: 0.46327529
INFO:root:[   25] Training loss: 0.05000889, Validation loss: 0.06376247, Gradient norm: 0.52041385
INFO:root:[   26] Training loss: 0.05205877, Validation loss: 0.06836735, Gradient norm: 0.54580063
INFO:root:[   27] Training loss: 0.05241253, Validation loss: 0.07254120, Gradient norm: 0.59428405
INFO:root:[   28] Training loss: 0.04949751, Validation loss: 0.07105874, Gradient norm: 0.50693499
INFO:root:[   29] Training loss: 0.04757960, Validation loss: 0.05951809, Gradient norm: 0.51201661
INFO:root:[   30] Training loss: 0.04541888, Validation loss: 0.06633930, Gradient norm: 0.44786937
INFO:root:[   31] Training loss: 0.04775265, Validation loss: 0.06822186, Gradient norm: 0.52929555
INFO:root:[   32] Training loss: 0.04686764, Validation loss: 0.06439313, Gradient norm: 0.50423727
INFO:root:[   33] Training loss: 0.04749215, Validation loss: 0.06363683, Gradient norm: 0.47686704
INFO:root:[   34] Training loss: 0.04550724, Validation loss: 0.06896695, Gradient norm: 0.49409436
INFO:root:[   35] Training loss: 0.04799883, Validation loss: 0.06646235, Gradient norm: 0.61966217
INFO:root:[   36] Training loss: 0.04694066, Validation loss: 0.06678525, Gradient norm: 0.56769484
INFO:root:[   37] Training loss: 0.04389068, Validation loss: 0.06357260, Gradient norm: 0.52174598
INFO:root:[   38] Training loss: 0.04466905, Validation loss: 0.06688613, Gradient norm: 0.58274805
INFO:root:[   39] Training loss: 0.04527392, Validation loss: 0.06488503, Gradient norm: 0.54302672
INFO:root:[   40] Training loss: 0.04518486, Validation loss: 0.06704075, Gradient norm: 0.52055658
INFO:root:[   41] Training loss: 0.04310666, Validation loss: 0.06843609, Gradient norm: 0.57841743
INFO:root:[   42] Training loss: 0.04287971, Validation loss: 0.06428098, Gradient norm: 0.54530480
INFO:root:[   43] Training loss: 0.04692791, Validation loss: 0.07530835, Gradient norm: 0.64302378
INFO:root:[   44] Training loss: 0.04494058, Validation loss: 0.06496026, Gradient norm: 0.56407722
INFO:root:[   45] Training loss: 0.04063419, Validation loss: 0.06121108, Gradient norm: 0.44736954
INFO:root:[   46] Training loss: 0.03822850, Validation loss: 0.06548379, Gradient norm: 0.48469117
INFO:root:[   47] Training loss: 0.04428018, Validation loss: 0.06325203, Gradient norm: 0.62263223
INFO:root:[   48] Training loss: 0.04224492, Validation loss: 0.06459423, Gradient norm: 0.53770072
INFO:root:[   49] Training loss: 0.03830566, Validation loss: 0.06872427, Gradient norm: 0.40075776
INFO:root:[   50] Training loss: 0.04289524, Validation loss: 0.06692789, Gradient norm: 0.62944302
INFO:root:[   51] Training loss: 0.04053704, Validation loss: 0.06386710, Gradient norm: 0.50650991
INFO:root:[   52] Training loss: 0.04092513, Validation loss: 0.06080789, Gradient norm: 0.54167694
INFO:root:[   53] Training loss: 0.04049429, Validation loss: 0.06910978, Gradient norm: 0.52838699
INFO:root:[   54] Training loss: 0.04022699, Validation loss: 0.06167494, Gradient norm: 0.53639717
INFO:root:[   55] Training loss: 0.03926053, Validation loss: 0.05937113, Gradient norm: 0.52390038
INFO:root:[   56] Training loss: 0.04212445, Validation loss: 0.06258950, Gradient norm: 0.59732767
INFO:root:[   57] Training loss: 0.04093124, Validation loss: 0.05927213, Gradient norm: 0.52705996
INFO:root:[   58] Training loss: 0.04042307, Validation loss: 0.06208137, Gradient norm: 0.60100211
INFO:root:[   59] Training loss: 0.04152838, Validation loss: 0.06769547, Gradient norm: 0.58477570
INFO:root:[   60] Training loss: 0.03829843, Validation loss: 0.06057946, Gradient norm: 0.52064790
INFO:root:[   61] Training loss: 0.03588133, Validation loss: 0.06050332, Gradient norm: 0.47532557
INFO:root:[   62] Training loss: 0.04027538, Validation loss: 0.06337928, Gradient norm: 0.61565769
INFO:root:[   63] Training loss: 0.04075527, Validation loss: 0.06114349, Gradient norm: 0.57886506
INFO:root:[   64] Training loss: 0.03761948, Validation loss: 0.06378772, Gradient norm: 0.52407996
INFO:root:[   65] Training loss: 0.03832623, Validation loss: 0.06619063, Gradient norm: 0.50263596
INFO:root:[   66] Training loss: 0.03731914, Validation loss: 0.06011282, Gradient norm: 0.51297321
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 2466.64s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04801
INFO:root:EnergyScoreTrain: 0.03043
INFO:root:CRPSTrain: 0.02625
INFO:root:Gaussian NLLTrain: 1.78246
INFO:root:CoverageTrain: 0.86032
INFO:root:IntervalWidthTrain: 0.15784
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08061
INFO:root:EnergyScoreValidation: 0.06007
INFO:root:CRPSValidation: 0.05283
INFO:root:Gaussian NLLValidation: 16.91234
INFO:root:CoverageValidation: 0.72537
INFO:root:IntervalWidthValidation: 0.1584
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08267
INFO:root:EnergyScoreTest: 0.06175
INFO:root:CRPSTest: 0.054
INFO:root:Gaussian NLLTest: 14.56299
INFO:root:CoverageTest: 0.71767
INFO:root:IntervalWidthTest: 0.15813
INFO:root:###13 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.24466484, Validation loss: 0.13692900, Gradient norm: 1.18354083
INFO:root:[    2] Training loss: 0.12937309, Validation loss: 0.09786725, Gradient norm: 1.20505813
INFO:root:[    3] Training loss: 0.11299466, Validation loss: 0.09275079, Gradient norm: 0.99918479
INFO:root:[    4] Training loss: 0.09934739, Validation loss: 0.10937758, Gradient norm: 0.85030897
INFO:root:[    5] Training loss: 0.09673772, Validation loss: 0.07217028, Gradient norm: 0.82581068
INFO:root:[    6] Training loss: 0.09096606, Validation loss: 0.08117242, Gradient norm: 0.74366517
INFO:root:[    7] Training loss: 0.07983426, Validation loss: 0.07274292, Gradient norm: 0.65520000
INFO:root:[    8] Training loss: 0.08384952, Validation loss: 0.07659723, Gradient norm: 0.71278767
INFO:root:[    9] Training loss: 0.08048261, Validation loss: 0.07845827, Gradient norm: 0.69163496
INFO:root:[   10] Training loss: 0.07440137, Validation loss: 0.07157856, Gradient norm: 0.61730790
INFO:root:[   11] Training loss: 0.07012782, Validation loss: 0.06897331, Gradient norm: 0.56086409
INFO:root:[   12] Training loss: 0.06775552, Validation loss: 0.07357228, Gradient norm: 0.56328144
INFO:root:[   13] Training loss: 0.07039094, Validation loss: 0.06711801, Gradient norm: 0.58811055
INFO:root:[   14] Training loss: 0.06362869, Validation loss: 0.07989064, Gradient norm: 0.46598336
INFO:root:[   15] Training loss: 0.06366651, Validation loss: 0.06506672, Gradient norm: 0.48466766
INFO:root:[   16] Training loss: 0.06211424, Validation loss: 0.07928153, Gradient norm: 0.55443035
INFO:root:[   17] Training loss: 0.06442827, Validation loss: 0.09434995, Gradient norm: 0.55744029
INFO:root:[   18] Training loss: 0.06457255, Validation loss: 0.06986562, Gradient norm: 0.58975960
INFO:root:[   19] Training loss: 0.06167978, Validation loss: 0.07040952, Gradient norm: 0.57560825
INFO:root:[   20] Training loss: 0.05778028, Validation loss: 0.07615050, Gradient norm: 0.48674283
INFO:root:[   21] Training loss: 0.05609153, Validation loss: 0.06342676, Gradient norm: 0.44944385
INFO:root:[   22] Training loss: 0.05989161, Validation loss: 0.06145515, Gradient norm: 0.59476749
INFO:root:[   23] Training loss: 0.05705977, Validation loss: 0.08587209, Gradient norm: 0.49077626
INFO:root:[   24] Training loss: 0.05635971, Validation loss: 0.06697236, Gradient norm: 0.51685560
INFO:root:[   25] Training loss: 0.05359689, Validation loss: 0.07010682, Gradient norm: 0.51129328
INFO:root:[   26] Training loss: 0.05917155, Validation loss: 0.06696469, Gradient norm: 0.61717558
INFO:root:[   27] Training loss: 0.05304160, Validation loss: 0.06552910, Gradient norm: 0.52455294
INFO:root:[   28] Training loss: 0.05303407, Validation loss: 0.06522935, Gradient norm: 0.48217291
INFO:root:[   29] Training loss: 0.05172737, Validation loss: 0.06482483, Gradient norm: 0.54371800
INFO:root:[   30] Training loss: 0.05221044, Validation loss: 0.06310712, Gradient norm: 0.56209366
INFO:root:[   31] Training loss: 0.05260216, Validation loss: 0.06912090, Gradient norm: 0.52937832
INFO:root:[   32] Training loss: 0.05002116, Validation loss: 0.07739754, Gradient norm: 0.47852839
INFO:root:[   33] Training loss: 0.05153338, Validation loss: 0.05902452, Gradient norm: 0.52534397
INFO:root:[   34] Training loss: 0.05029153, Validation loss: 0.06908680, Gradient norm: 0.56753415
INFO:root:[   35] Training loss: 0.04996129, Validation loss: 0.06526120, Gradient norm: 0.53021328
INFO:root:[   36] Training loss: 0.04888518, Validation loss: 0.06301604, Gradient norm: 0.57514015
INFO:root:[   37] Training loss: 0.04690664, Validation loss: 0.06640513, Gradient norm: 0.48537992
INFO:root:[   38] Training loss: 0.04765038, Validation loss: 0.07263517, Gradient norm: 0.55018876
INFO:root:[   39] Training loss: 0.04882809, Validation loss: 0.06705811, Gradient norm: 0.57850361
INFO:root:[   40] Training loss: 0.04997227, Validation loss: 0.07316306, Gradient norm: 0.58402904
INFO:root:[   41] Training loss: 0.04414760, Validation loss: 0.06946208, Gradient norm: 0.44448131
INFO:root:[   42] Training loss: 0.04840212, Validation loss: 0.06530926, Gradient norm: 0.62776285
INFO:root:[   43] Training loss: 0.04950581, Validation loss: 0.07537474, Gradient norm: 0.62268916
INFO:root:[   44] Training loss: 0.04881778, Validation loss: 0.06450646, Gradient norm: 0.59624704
INFO:root:[   45] Training loss: 0.04492463, Validation loss: 0.06316455, Gradient norm: 0.46502363
INFO:root:[   46] Training loss: 0.04252921, Validation loss: 0.06499478, Gradient norm: 0.45296907
INFO:root:[   47] Training loss: 0.04733450, Validation loss: 0.06467966, Gradient norm: 0.58127268
INFO:root:[   48] Training loss: 0.04365742, Validation loss: 0.05991568, Gradient norm: 0.53433685
INFO:root:[   49] Training loss: 0.04568069, Validation loss: 0.06749783, Gradient norm: 0.55703377
INFO:root:[   50] Training loss: 0.04413153, Validation loss: 0.07306874, Gradient norm: 0.55195010
INFO:root:[   51] Training loss: 0.04433186, Validation loss: 0.06930877, Gradient norm: 0.52306512
INFO:root:[   52] Training loss: 0.04323164, Validation loss: 0.06968821, Gradient norm: 0.48748542
INFO:root:[   53] Training loss: 0.04372948, Validation loss: 0.06083242, Gradient norm: 0.50053212
INFO:root:[   54] Training loss: 0.04422838, Validation loss: 0.07664748, Gradient norm: 0.58759771
INFO:root:[   55] Training loss: 0.04470634, Validation loss: 0.06099433, Gradient norm: 0.52255214
INFO:root:[   56] Training loss: 0.04160478, Validation loss: 0.06929770, Gradient norm: 0.49837103
INFO:root:[   57] Training loss: 0.04641053, Validation loss: 0.05794618, Gradient norm: 0.61854853
INFO:root:[   58] Training loss: 0.04014674, Validation loss: 0.07159660, Gradient norm: 0.46526981
INFO:root:[   59] Training loss: 0.04361721, Validation loss: 0.06186482, Gradient norm: 0.59391550
INFO:root:[   60] Training loss: 0.04085767, Validation loss: 0.05966754, Gradient norm: 0.49679670
INFO:root:[   61] Training loss: 0.03932308, Validation loss: 0.06578757, Gradient norm: 0.45090351
INFO:root:[   62] Training loss: 0.04350153, Validation loss: 0.07738333, Gradient norm: 0.58393089
INFO:root:[   63] Training loss: 0.04597710, Validation loss: 0.06464727, Gradient norm: 0.63622510
INFO:root:[   64] Training loss: 0.04125350, Validation loss: 0.06478977, Gradient norm: 0.52343316
INFO:root:[   65] Training loss: 0.04055231, Validation loss: 0.06071480, Gradient norm: 0.51094893
INFO:root:[   66] Training loss: 0.04050096, Validation loss: 0.06565728, Gradient norm: 0.51835296
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 2469.198s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04983
INFO:root:EnergyScoreTrain: 0.03056
INFO:root:CRPSTrain: 0.02623
INFO:root:Gaussian NLLTrain: 0.10053
INFO:root:CoverageTrain: 0.81698
INFO:root:IntervalWidthTrain: 0.15015
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07787
INFO:root:EnergyScoreValidation: 0.05865
INFO:root:CRPSValidation: 0.05086
INFO:root:Gaussian NLLValidation: 7.85231
INFO:root:CoverageValidation: 0.69997
INFO:root:IntervalWidthValidation: 0.15065
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07882
INFO:root:EnergyScoreTest: 0.05955
INFO:root:CRPSTest: 0.05164
INFO:root:Gaussian NLLTest: 6.93883
INFO:root:CoverageTest: 0.69317
INFO:root:IntervalWidthTest: 0.15063
INFO:root:###14 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.25377679, Validation loss: 0.13287375, Gradient norm: 1.11754054
INFO:root:[    2] Training loss: 0.13797017, Validation loss: 0.13585585, Gradient norm: 1.09696026
INFO:root:[    3] Training loss: 0.11996974, Validation loss: 0.10939205, Gradient norm: 0.95993401
INFO:root:[    4] Training loss: 0.10593918, Validation loss: 0.11876031, Gradient norm: 0.79877222
INFO:root:[    5] Training loss: 0.10577927, Validation loss: 0.07748830, Gradient norm: 0.79694110
INFO:root:[    6] Training loss: 0.09480937, Validation loss: 0.07758035, Gradient norm: 0.65187963
INFO:root:[    7] Training loss: 0.08441915, Validation loss: 0.07958953, Gradient norm: 0.54224920
INFO:root:[    8] Training loss: 0.08396993, Validation loss: 0.09245047, Gradient norm: 0.56232517
INFO:root:[    9] Training loss: 0.08405024, Validation loss: 0.08631948, Gradient norm: 0.63881428
INFO:root:[   10] Training loss: 0.08389676, Validation loss: 0.08011098, Gradient norm: 0.63354161
INFO:root:[   11] Training loss: 0.07719052, Validation loss: 0.06783164, Gradient norm: 0.58863731
INFO:root:[   12] Training loss: 0.07407130, Validation loss: 0.06700725, Gradient norm: 0.56070540
INFO:root:[   13] Training loss: 0.07627002, Validation loss: 0.06631899, Gradient norm: 0.57379489
INFO:root:[   14] Training loss: 0.07490444, Validation loss: 0.06340238, Gradient norm: 0.58771093
INFO:root:[   15] Training loss: 0.07669975, Validation loss: 0.06851690, Gradient norm: 0.61712428
INFO:root:[   16] Training loss: 0.06539804, Validation loss: 0.06509646, Gradient norm: 0.38173505
INFO:root:[   17] Training loss: 0.06620408, Validation loss: 0.06984358, Gradient norm: 0.41361043
INFO:root:[   18] Training loss: 0.07179929, Validation loss: 0.07101778, Gradient norm: 0.62190602
INFO:root:[   19] Training loss: 0.06830323, Validation loss: 0.06486796, Gradient norm: 0.56844342
INFO:root:[   20] Training loss: 0.06927629, Validation loss: 0.09704605, Gradient norm: 0.60035701
INFO:root:[   21] Training loss: 0.06974469, Validation loss: 0.08618154, Gradient norm: 0.61619702
INFO:root:[   22] Training loss: 0.06535927, Validation loss: 0.06447022, Gradient norm: 0.57836700
INFO:root:[   23] Training loss: 0.06210808, Validation loss: 0.07287636, Gradient norm: 0.47100255
INFO:root:[   24] Training loss: 0.06113622, Validation loss: 0.06761541, Gradient norm: 0.42903529
INFO:root:[   25] Training loss: 0.06135584, Validation loss: 0.07947673, Gradient norm: 0.51125171
INFO:root:[   26] Training loss: 0.06317042, Validation loss: 0.06494835, Gradient norm: 0.51397374
INFO:root:[   27] Training loss: 0.06163561, Validation loss: 0.07025017, Gradient norm: 0.57834114
INFO:root:[   28] Training loss: 0.06109157, Validation loss: 0.06164631, Gradient norm: 0.55325621
INFO:root:[   29] Training loss: 0.06065758, Validation loss: 0.06384669, Gradient norm: 0.54724986
INFO:root:[   30] Training loss: 0.05990034, Validation loss: 0.06233507, Gradient norm: 0.53835044
INFO:root:[   31] Training loss: 0.06097239, Validation loss: 0.06827843, Gradient norm: 0.59498078
INFO:root:[   32] Training loss: 0.05716631, Validation loss: 0.11026789, Gradient norm: 0.49969855
INFO:root:[   33] Training loss: 0.06012768, Validation loss: 0.07329246, Gradient norm: 0.56903024
INFO:root:[   34] Training loss: 0.05899025, Validation loss: 0.07599982, Gradient norm: 0.59424466
INFO:root:[   35] Training loss: 0.05507283, Validation loss: 0.06701573, Gradient norm: 0.46809133
INFO:root:[   36] Training loss: 0.05204498, Validation loss: 0.07859373, Gradient norm: 0.39836722
INFO:root:[   37] Training loss: 0.05327123, Validation loss: 0.06214434, Gradient norm: 0.44342065
INFO:root:[   38] Training loss: 0.05327702, Validation loss: 0.07450440, Gradient norm: 0.50098959
INFO:root:[   39] Training loss: 0.05857439, Validation loss: 0.07166816, Gradient norm: 0.63098992
INFO:root:[   40] Training loss: 0.05660597, Validation loss: 0.06463743, Gradient norm: 0.57216598
INFO:root:[   41] Training loss: 0.05496904, Validation loss: 0.07073588, Gradient norm: 0.55221804
INFO:root:[   42] Training loss: 0.05282777, Validation loss: 0.06063979, Gradient norm: 0.54103463
INFO:root:[   43] Training loss: 0.05292451, Validation loss: 0.07814215, Gradient norm: 0.54274711
INFO:root:[   44] Training loss: 0.05346139, Validation loss: 0.06011259, Gradient norm: 0.53005339
INFO:root:[   45] Training loss: 0.05404563, Validation loss: 0.06517402, Gradient norm: 0.59602833
INFO:root:[   46] Training loss: 0.05172059, Validation loss: 0.08549157, Gradient norm: 0.49396146
INFO:root:[   47] Training loss: 0.05521239, Validation loss: 0.06715409, Gradient norm: 0.61820953
INFO:root:[   48] Training loss: 0.05308230, Validation loss: 0.06334595, Gradient norm: 0.51806396
INFO:root:[   49] Training loss: 0.05143724, Validation loss: 0.06564925, Gradient norm: 0.54750025
INFO:root:[   50] Training loss: 0.05058245, Validation loss: 0.06367612, Gradient norm: 0.54698328
INFO:root:[   51] Training loss: 0.05061046, Validation loss: 0.07185366, Gradient norm: 0.53264642
INFO:root:[   52] Training loss: 0.05070107, Validation loss: 0.07178892, Gradient norm: 0.54064777
INFO:root:[   53] Training loss: 0.04961155, Validation loss: 0.08374435, Gradient norm: 0.52140847
INFO:root:[   54] Training loss: 0.04955880, Validation loss: 0.06939259, Gradient norm: 0.54025216
INFO:root:[   55] Training loss: 0.04694373, Validation loss: 0.06369534, Gradient norm: 0.46295726
INFO:root:[   56] Training loss: 0.04862308, Validation loss: 0.07573977, Gradient norm: 0.51906251
INFO:root:[   57] Training loss: 0.05219632, Validation loss: 0.07247808, Gradient norm: 0.52985528
INFO:root:[   58] Training loss: 0.04594657, Validation loss: 0.06998714, Gradient norm: 0.47356794
INFO:root:[   59] Training loss: 0.05231706, Validation loss: 0.07702445, Gradient norm: 0.68044851
INFO:root:[   60] Training loss: 0.04930820, Validation loss: 0.05989303, Gradient norm: 0.56433542
INFO:root:[   61] Training loss: 0.04888810, Validation loss: 0.08695785, Gradient norm: 0.55184515
INFO:root:[   62] Training loss: 0.04710621, Validation loss: 0.07147456, Gradient norm: 0.48649544
INFO:root:[   63] Training loss: 0.04483681, Validation loss: 0.07614106, Gradient norm: 0.43688415
INFO:root:[   64] Training loss: 0.04653707, Validation loss: 0.06563639, Gradient norm: 0.47157051
INFO:root:[   65] Training loss: 0.04682550, Validation loss: 0.06280818, Gradient norm: 0.49714700
INFO:root:[   66] Training loss: 0.04685768, Validation loss: 0.06873891, Gradient norm: 0.53931294
INFO:root:[   67] Training loss: 0.05013155, Validation loss: 0.06489171, Gradient norm: 0.59995343
INFO:root:[   68] Training loss: 0.04809199, Validation loss: 0.06185992, Gradient norm: 0.58246309
INFO:root:[   69] Training loss: 0.04905191, Validation loss: 0.07320180, Gradient norm: 0.62656671
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2609.728s.
INFO:root:Emptying the cuda cache took 0.01s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05922
INFO:root:EnergyScoreTrain: 0.03551
INFO:root:CRPSTrain: 0.03074
INFO:root:Gaussian NLLTrain: 1.2127
INFO:root:CoverageTrain: 0.78084
INFO:root:IntervalWidthTrain: 0.17428
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08085
INFO:root:EnergyScoreValidation: 0.05973
INFO:root:CRPSValidation: 0.05156
INFO:root:Gaussian NLLValidation: 5.77537
INFO:root:CoverageValidation: 0.72448
INFO:root:IntervalWidthValidation: 0.17456
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0833
INFO:root:EnergyScoreTest: 0.06149
INFO:root:CRPSTest: 0.05296
INFO:root:Gaussian NLLTest: 5.34757
INFO:root:CoverageTest: 0.71342
INFO:root:IntervalWidthTest: 0.17495
INFO:root:###15 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26756070, Validation loss: 0.18572693, Gradient norm: 1.09589734
INFO:root:[    2] Training loss: 0.15219541, Validation loss: 0.11436311, Gradient norm: 0.95899527
INFO:root:[    3] Training loss: 0.12686886, Validation loss: 0.10772816, Gradient norm: 0.80385152
INFO:root:[    4] Training loss: 0.11486853, Validation loss: 0.11985339, Gradient norm: 0.75266406
INFO:root:[    5] Training loss: 0.11427390, Validation loss: 0.08159071, Gradient norm: 0.79150132
INFO:root:[    6] Training loss: 0.10691964, Validation loss: 0.08209439, Gradient norm: 0.68164524
INFO:root:[    7] Training loss: 0.09805160, Validation loss: 0.07792563, Gradient norm: 0.62584709
INFO:root:[    8] Training loss: 0.09471982, Validation loss: 0.07571464, Gradient norm: 0.55021151
INFO:root:[    9] Training loss: 0.09227088, Validation loss: 0.07891262, Gradient norm: 0.62944921
INFO:root:[   10] Training loss: 0.08922294, Validation loss: 0.09138582, Gradient norm: 0.55427800
INFO:root:[   11] Training loss: 0.08781961, Validation loss: 0.07127324, Gradient norm: 0.56059031
INFO:root:[   12] Training loss: 0.08111144, Validation loss: 0.06630787, Gradient norm: 0.49708238
INFO:root:[   13] Training loss: 0.08219670, Validation loss: 0.08138758, Gradient norm: 0.54658952
INFO:root:[   14] Training loss: 0.08101549, Validation loss: 0.09059866, Gradient norm: 0.55008272
INFO:root:[   15] Training loss: 0.08539856, Validation loss: 0.06791332, Gradient norm: 0.64350253
INFO:root:[   16] Training loss: 0.07510994, Validation loss: 0.07285320, Gradient norm: 0.41960400
INFO:root:[   17] Training loss: 0.07716717, Validation loss: 0.12508565, Gradient norm: 0.52304083
INFO:root:[   18] Training loss: 0.08203881, Validation loss: 0.08745724, Gradient norm: 0.68759758
INFO:root:[   19] Training loss: 0.07549328, Validation loss: 0.07866217, Gradient norm: 0.53555446
INFO:root:[   20] Training loss: 0.07712048, Validation loss: 0.07820191, Gradient norm: 0.50586157
INFO:root:[   21] Training loss: 0.07529329, Validation loss: 0.07825385, Gradient norm: 0.61168997
INFO:root:[   22] Training loss: 0.07408817, Validation loss: 0.06494008, Gradient norm: 0.52442888
INFO:root:[   23] Training loss: 0.07073405, Validation loss: 0.06534430, Gradient norm: 0.48146559
INFO:root:[   24] Training loss: 0.06880194, Validation loss: 0.06881831, Gradient norm: 0.46265377
INFO:root:[   25] Training loss: 0.06854369, Validation loss: 0.06053523, Gradient norm: 0.49411569
INFO:root:[   26] Training loss: 0.07187948, Validation loss: 0.07899405, Gradient norm: 0.56223298
INFO:root:[   27] Training loss: 0.06611317, Validation loss: 0.07007374, Gradient norm: 0.41430265
INFO:root:[   28] Training loss: 0.06683876, Validation loss: 0.08573043, Gradient norm: 0.51163021
INFO:root:[   29] Training loss: 0.07086146, Validation loss: 0.06326169, Gradient norm: 0.56852408
INFO:root:[   30] Training loss: 0.06975515, Validation loss: 0.07426989, Gradient norm: 0.61464902
INFO:root:[   31] Training loss: 0.06572371, Validation loss: 0.07516539, Gradient norm: 0.48343091
INFO:root:[   32] Training loss: 0.06991038, Validation loss: 0.09243800, Gradient norm: 0.63363084
INFO:root:[   33] Training loss: 0.06354236, Validation loss: 0.06797499, Gradient norm: 0.41554763
INFO:root:[   34] Training loss: 0.06689781, Validation loss: 0.08172524, Gradient norm: 0.56960456
INFO:root:[   35] Training loss: 0.06302037, Validation loss: 0.07796728, Gradient norm: 0.47368802
INFO:root:[   36] Training loss: 0.05945482, Validation loss: 0.07443252, Gradient norm: 0.37329250
INFO:root:[   37] Training loss: 0.05859875, Validation loss: 0.08726429, Gradient norm: 0.37083852
INFO:root:[   38] Training loss: 0.06199495, Validation loss: 0.06854769, Gradient norm: 0.49700787
INFO:root:[   39] Training loss: 0.06077102, Validation loss: 0.07850059, Gradient norm: 0.49334610
INFO:root:[   40] Training loss: 0.06472725, Validation loss: 0.07888458, Gradient norm: 0.60350839
INFO:root:[   41] Training loss: 0.06313431, Validation loss: 0.08916915, Gradient norm: 0.54721623
INFO:root:[   42] Training loss: 0.06143771, Validation loss: 0.06409562, Gradient norm: 0.56133605
INFO:root:[   43] Training loss: 0.06163942, Validation loss: 0.07628350, Gradient norm: 0.57508545
INFO:root:[   44] Training loss: 0.06043663, Validation loss: 0.06208740, Gradient norm: 0.51621381
INFO:root:[   45] Training loss: 0.05912532, Validation loss: 0.06933304, Gradient norm: 0.47490548
INFO:root:[   46] Training loss: 0.05448549, Validation loss: 0.07049861, Gradient norm: 0.39716922
INFO:root:[   47] Training loss: 0.05485099, Validation loss: 0.06853750, Gradient norm: 0.41477674
INFO:root:[   48] Training loss: 0.05722939, Validation loss: 0.07255524, Gradient norm: 0.50527407
INFO:root:[   49] Training loss: 0.05851326, Validation loss: 0.07116119, Gradient norm: 0.53196052
INFO:root:[   50] Training loss: 0.05693406, Validation loss: 0.07485740, Gradient norm: 0.48458429
INFO:root:[   51] Training loss: 0.05517922, Validation loss: 0.07663784, Gradient norm: 0.46839678
INFO:root:[   52] Training loss: 0.06022081, Validation loss: 0.08054152, Gradient norm: 0.62371870
INFO:root:[   53] Training loss: 0.05919155, Validation loss: 0.07633879, Gradient norm: 0.60114518
INFO:root:[   54] Training loss: 0.05322832, Validation loss: 0.06932473, Gradient norm: 0.46162905
INFO:root:[   55] Training loss: 0.06027268, Validation loss: 0.07872350, Gradient norm: 0.62594330
INFO:root:[   56] Training loss: 0.05661972, Validation loss: 0.09050531, Gradient norm: 0.55679290
INFO:root:[   57] Training loss: 0.05512589, Validation loss: 0.08413608, Gradient norm: 0.45587435
INFO:root:[   58] Training loss: 0.05409889, Validation loss: 0.07628966, Gradient norm: 0.50820981
INFO:root:[   59] Training loss: 0.05192821, Validation loss: 0.08838970, Gradient norm: 0.52310055
INFO:root:[   60] Training loss: 0.05359399, Validation loss: 0.07456286, Gradient norm: 0.50876760
INFO:root:[   61] Training loss: 0.05448212, Validation loss: 0.07528725, Gradient norm: 0.57222981
INFO:root:[   62] Training loss: 0.05029356, Validation loss: 0.08034262, Gradient norm: 0.45679041
INFO:root:[   63] Training loss: 0.04854983, Validation loss: 0.08008714, Gradient norm: 0.39525573
INFO:root:[   64] Training loss: 0.05268624, Validation loss: 0.06601509, Gradient norm: 0.53615262
INFO:root:[   65] Training loss: 0.05397950, Validation loss: 0.06563496, Gradient norm: 0.56464854
INFO:root:[   66] Training loss: 0.05242815, Validation loss: 0.06810595, Gradient norm: 0.49234062
INFO:root:[   67] Training loss: 0.05309565, Validation loss: 0.07975270, Gradient norm: 0.59625504
INFO:root:[   68] Training loss: 0.04945485, Validation loss: 0.06529501, Gradient norm: 0.47624314
INFO:root:[   69] Training loss: 0.05476182, Validation loss: 0.08715485, Gradient norm: 0.60174932
INFO:root:[   70] Training loss: 0.04972883, Validation loss: 0.08725380, Gradient norm: 0.49915065
INFO:root:[   71] Training loss: 0.04883954, Validation loss: 0.07702929, Gradient norm: 0.47640001
INFO:root:[   72] Training loss: 0.04627493, Validation loss: 0.06078052, Gradient norm: 0.38393331
INFO:root:[   73] Training loss: 0.04865964, Validation loss: 0.06484246, Gradient norm: 0.48694859
INFO:root:[   74] Training loss: 0.05214233, Validation loss: 0.07748648, Gradient norm: 0.57983867
INFO:root:[   75] Training loss: 0.04966538, Validation loss: 0.07275204, Gradient norm: 0.51800030
INFO:root:[   76] Training loss: 0.04803408, Validation loss: 0.07546561, Gradient norm: 0.50152274
INFO:root:[   77] Training loss: 0.04859131, Validation loss: 0.07645272, Gradient norm: 0.51524760
INFO:root:[   78] Training loss: 0.04824614, Validation loss: 0.06549492, Gradient norm: 0.46685880
INFO:root:[   79] Training loss: 0.04744739, Validation loss: 0.07977439, Gradient norm: 0.51151778
INFO:root:[   80] Training loss: 0.04711872, Validation loss: 0.07104670, Gradient norm: 0.46441590
INFO:root:[   81] Training loss: 0.05062142, Validation loss: 0.06379072, Gradient norm: 0.60440166
INFO:root:EP 81: Early stopping
INFO:root:Training the model took 3113.598s.
INFO:root:Emptying the cuda cache took 0.028s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.07471
INFO:root:EnergyScoreTrain: 0.04872
INFO:root:CRPSTrain: 0.04128
INFO:root:Gaussian NLLTrain: -0.21162
INFO:root:CoverageTrain: 0.86444
INFO:root:IntervalWidthTrain: 0.24813
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.09143
INFO:root:EnergyScoreValidation: 0.06115
INFO:root:CRPSValidation: 0.05238
INFO:root:Gaussian NLLValidation: 0.6448
INFO:root:CoverageValidation: 0.81722
INFO:root:IntervalWidthValidation: 0.24903
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.09238
INFO:root:EnergyScoreTest: 0.06198
INFO:root:CRPSTest: 0.05311
INFO:root:Gaussian NLLTest: 0.99962
INFO:root:CoverageTest: 0.81106
INFO:root:IntervalWidthTest: 0.24799
INFO:root:###16 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27541829, Validation loss: 0.19343612, Gradient norm: 0.98242781
INFO:root:[    2] Training loss: 0.16645640, Validation loss: 0.14841715, Gradient norm: 1.04865682
INFO:root:[    3] Training loss: 0.13568716, Validation loss: 0.12351475, Gradient norm: 0.80703178
INFO:root:[    4] Training loss: 0.12684068, Validation loss: 0.10940228, Gradient norm: 0.75906155
INFO:root:[    5] Training loss: 0.11663810, Validation loss: 0.09167421, Gradient norm: 0.69451213
INFO:root:[    6] Training loss: 0.10902979, Validation loss: 0.07854297, Gradient norm: 0.58699977
INFO:root:[    7] Training loss: 0.10079594, Validation loss: 0.10826536, Gradient norm: 0.54840917
INFO:root:[    8] Training loss: 0.10592010, Validation loss: 0.08800020, Gradient norm: 0.70162493
INFO:root:[    9] Training loss: 0.09923826, Validation loss: 0.08432888, Gradient norm: 0.62634152
INFO:root:[   10] Training loss: 0.09885209, Validation loss: 0.09091408, Gradient norm: 0.62809113
INFO:root:[   11] Training loss: 0.09098325, Validation loss: 0.07988901, Gradient norm: 0.48491230
INFO:root:[   12] Training loss: 0.08776995, Validation loss: 0.07084865, Gradient norm: 0.49011084
