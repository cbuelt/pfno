INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.83964642, Validation loss: 0.74205471, Gradient norm: 4.89907025
INFO:root:[    2] Training loss: 0.73618455, Validation loss: 0.72502482, Gradient norm: 2.73403591
INFO:root:[    3] Training loss: 0.71856102, Validation loss: 0.71464202, Gradient norm: 3.07947782
INFO:root:[    4] Training loss: 0.71486948, Validation loss: 0.71833439, Gradient norm: 3.99848624
INFO:root:[    5] Training loss: 0.71034742, Validation loss: 0.71297376, Gradient norm: 3.93684914
INFO:root:[    6] Training loss: 0.70762964, Validation loss: 0.70340605, Gradient norm: 4.03725988
INFO:root:[    7] Training loss: 0.70143268, Validation loss: 0.70326314, Gradient norm: 2.84511427
INFO:root:[    8] Training loss: 0.70076882, Validation loss: 0.69927178, Gradient norm: 2.12787137
INFO:root:[    9] Training loss: 0.70052793, Validation loss: 0.70243332, Gradient norm: 2.25582648
INFO:root:[   10] Training loss: 0.69956530, Validation loss: 0.70302682, Gradient norm: 2.22615849
INFO:root:[   11] Training loss: 0.69942528, Validation loss: 0.69713728, Gradient norm: 2.06948093
INFO:root:[   12] Training loss: 0.69898671, Validation loss: 0.70160008, Gradient norm: 1.72441087
INFO:root:[   13] Training loss: 0.69763268, Validation loss: 0.69729183, Gradient norm: 1.73108707
INFO:root:[   14] Training loss: 0.69673592, Validation loss: 0.69994238, Gradient norm: 1.43920473
INFO:root:[   15] Training loss: 0.69520597, Validation loss: 0.69718517, Gradient norm: 1.38895087
INFO:root:[   16] Training loss: 0.69384842, Validation loss: 0.69503725, Gradient norm: 1.15503411
INFO:root:[   17] Training loss: 0.69456528, Validation loss: 0.69721869, Gradient norm: 1.35298871
INFO:root:[   18] Training loss: 0.69450275, Validation loss: 0.70113172, Gradient norm: 1.33848693
INFO:root:[   19] Training loss: 0.69336645, Validation loss: 0.69171817, Gradient norm: 1.19218904
INFO:root:[   20] Training loss: 0.69259274, Validation loss: 0.69636260, Gradient norm: 1.00737823
INFO:root:[   21] Training loss: 0.69234568, Validation loss: 0.69469263, Gradient norm: 1.24669492
INFO:root:[   22] Training loss: 0.69226696, Validation loss: 0.69328498, Gradient norm: 1.13411659
INFO:root:[   23] Training loss: 0.69086782, Validation loss: 0.68871279, Gradient norm: 1.06924902
INFO:root:[   24] Training loss: 0.69085985, Validation loss: 0.69443943, Gradient norm: 1.08277460
INFO:root:[   25] Training loss: 0.69025464, Validation loss: 0.68916434, Gradient norm: 1.00900156
INFO:root:[   26] Training loss: 0.68914177, Validation loss: 0.68759789, Gradient norm: 1.02554339
INFO:root:[   27] Training loss: 0.68874040, Validation loss: 0.68827007, Gradient norm: 0.89281233
INFO:root:[   28] Training loss: 0.68824097, Validation loss: 0.68720275, Gradient norm: 0.94191882
INFO:root:[   29] Training loss: 0.68746181, Validation loss: 0.69076898, Gradient norm: 0.94426387
INFO:root:[   30] Training loss: 0.68626528, Validation loss: 0.69087702, Gradient norm: 0.85945634
INFO:root:[   31] Training loss: 0.68636745, Validation loss: 0.68622779, Gradient norm: 0.80535055
INFO:root:[   32] Training loss: 0.68505150, Validation loss: 0.68860375, Gradient norm: 0.80940713
INFO:root:[   33] Training loss: 0.68500915, Validation loss: 0.68598414, Gradient norm: 0.83037010
INFO:root:[   34] Training loss: 0.68398119, Validation loss: 0.68431453, Gradient norm: 0.79607109
INFO:root:[   35] Training loss: 0.68222271, Validation loss: 0.68221898, Gradient norm: 0.70542922
INFO:root:[   36] Training loss: 0.68176609, Validation loss: 0.68210258, Gradient norm: 0.63789595
INFO:root:[   37] Training loss: 0.68047429, Validation loss: 0.68423552, Gradient norm: 0.67198807
INFO:root:[   38] Training loss: 0.68019066, Validation loss: 0.68037739, Gradient norm: 0.61894858
INFO:root:[   39] Training loss: 0.67886922, Validation loss: 0.68011329, Gradient norm: 0.55638771
INFO:root:[   40] Training loss: 0.67918869, Validation loss: 0.67792702, Gradient norm: 0.65326162
INFO:root:[   41] Training loss: 0.67850913, Validation loss: 0.67813995, Gradient norm: 0.69540455
INFO:root:[   42] Training loss: 0.67749114, Validation loss: 0.68249326, Gradient norm: 0.58707160
INFO:root:[   43] Training loss: 0.67841450, Validation loss: 0.67859907, Gradient norm: 0.70062718
INFO:root:[   44] Training loss: 0.67787947, Validation loss: 0.67969306, Gradient norm: 0.59890444
INFO:root:[   45] Training loss: 0.67708831, Validation loss: 0.67834610, Gradient norm: 0.64362620
INFO:root:[   46] Training loss: 0.67628530, Validation loss: 0.67705243, Gradient norm: 0.54172662
INFO:root:[   47] Training loss: 0.67643731, Validation loss: 0.67699736, Gradient norm: 0.65017088
INFO:root:[   48] Training loss: 0.67545195, Validation loss: 0.67575865, Gradient norm: 0.59877256
INFO:root:[   49] Training loss: 0.67557496, Validation loss: 0.67754188, Gradient norm: 0.64327849
INFO:root:[   50] Training loss: 0.67501058, Validation loss: 0.67744800, Gradient norm: 0.56049235
INFO:root:[   51] Training loss: 0.67508591, Validation loss: 0.67593074, Gradient norm: 0.65524387
INFO:root:[   52] Training loss: 0.67409777, Validation loss: 0.67498469, Gradient norm: 0.56043267
INFO:root:[   53] Training loss: 0.67411373, Validation loss: 0.67189550, Gradient norm: 0.55458344
INFO:root:[   54] Training loss: 0.67434966, Validation loss: 0.67473945, Gradient norm: 0.65359562
INFO:root:[   55] Training loss: 0.67296590, Validation loss: 0.67257969, Gradient norm: 0.57190386
INFO:root:[   56] Training loss: 0.67456690, Validation loss: 0.67420361, Gradient norm: 0.68453729
INFO:root:[   57] Training loss: 0.67272862, Validation loss: 0.67756711, Gradient norm: 0.54576084
INFO:root:[   58] Training loss: 0.67323460, Validation loss: 0.67403503, Gradient norm: 0.58824271
INFO:root:[   59] Training loss: 0.67278528, Validation loss: 0.67536336, Gradient norm: 0.65883899
INFO:root:[   60] Training loss: 0.67251710, Validation loss: 0.67304924, Gradient norm: 0.53285287
INFO:root:[   61] Training loss: 0.67209930, Validation loss: 0.67531059, Gradient norm: 0.57888741
INFO:root:[   62] Training loss: 0.67266917, Validation loss: 0.67494557, Gradient norm: 0.61897209
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 4008.262s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.92997
INFO:root:EnergyScoreTrain: 0.67134
INFO:root:CRPSTrain: 0.5547
INFO:root:Gaussian NLLTrain: 1.42186
INFO:root:CoverageTrain: 0.92542
INFO:root:IntervalWidthTrain: 3.82911
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.93179
INFO:root:EnergyScoreValidation: 0.67281
INFO:root:CRPSValidation: 0.556
INFO:root:Gaussian NLLValidation: 1.42394
INFO:root:CoverageValidation: 0.92565
INFO:root:IntervalWidthValidation: 3.83654
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.93141
INFO:root:EnergyScoreTest: 0.67266
INFO:root:CRPSTest: 0.55594
INFO:root:Gaussian NLLTest: 1.42331
INFO:root:CoverageTest: 0.92527
INFO:root:IntervalWidthTest: 3.82858
INFO:root:###2 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 310378496
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.76250964, Validation loss: 0.72605328, Gradient norm: 8.10827247
INFO:root:[    2] Training loss: 0.71772512, Validation loss: 0.72247765, Gradient norm: 7.16094402
INFO:root:[    3] Training loss: 0.71034160, Validation loss: 0.70579039, Gradient norm: 6.07865038
INFO:root:[    4] Training loss: 0.70703042, Validation loss: 0.70330099, Gradient norm: 5.65502166
INFO:root:[    5] Training loss: 0.70534105, Validation loss: 0.70450686, Gradient norm: 5.43652771
INFO:root:[    6] Training loss: 0.70341214, Validation loss: 0.70374057, Gradient norm: 5.10620349
INFO:root:[    7] Training loss: 0.70222022, Validation loss: 0.70433625, Gradient norm: 5.24019935
INFO:root:[    8] Training loss: 0.70141703, Validation loss: 0.69931391, Gradient norm: 5.36639995
INFO:root:[    9] Training loss: 0.70003399, Validation loss: 0.69986446, Gradient norm: 4.60675067
INFO:root:[   10] Training loss: 0.69826456, Validation loss: 0.69787002, Gradient norm: 4.37562053
INFO:root:[   11] Training loss: 0.69818964, Validation loss: 0.69810743, Gradient norm: 4.85960012
INFO:root:[   12] Training loss: 0.69722734, Validation loss: 0.69659189, Gradient norm: 4.45239332
INFO:root:[   13] Training loss: 0.69702211, Validation loss: 0.69870147, Gradient norm: 4.78613050
INFO:root:[   14] Training loss: 0.69541769, Validation loss: 0.69515217, Gradient norm: 3.56617806
INFO:root:[   15] Training loss: 0.69360628, Validation loss: 0.69565461, Gradient norm: 3.66468869
INFO:root:[   16] Training loss: 0.69231836, Validation loss: 0.69136532, Gradient norm: 3.78769519
INFO:root:[   17] Training loss: 0.69164411, Validation loss: 0.69168998, Gradient norm: 3.81203570
INFO:root:[   18] Training loss: 0.69062164, Validation loss: 0.69118109, Gradient norm: 3.75901154
INFO:root:[   19] Training loss: 0.68985797, Validation loss: 0.69214684, Gradient norm: 3.45024501
INFO:root:[   20] Training loss: 0.68939244, Validation loss: 0.69015390, Gradient norm: 3.30177345
INFO:root:[   21] Training loss: 0.68888090, Validation loss: 0.69105339, Gradient norm: 3.36945757
INFO:root:[   22] Training loss: 0.68820128, Validation loss: 0.68885398, Gradient norm: 3.26536288
INFO:root:[   23] Training loss: 0.68730909, Validation loss: 0.68968259, Gradient norm: 3.26605095
INFO:root:[   24] Training loss: 0.68697568, Validation loss: 0.68804131, Gradient norm: 3.23528658
INFO:root:[   25] Training loss: 0.68640483, Validation loss: 0.68507601, Gradient norm: 3.26539026
INFO:root:[   26] Training loss: 0.68580126, Validation loss: 0.68664183, Gradient norm: 3.11007780
INFO:root:[   27] Training loss: 0.68547292, Validation loss: 0.68462668, Gradient norm: 3.01672597
INFO:root:[   28] Training loss: 0.68453503, Validation loss: 0.68524518, Gradient norm: 3.00582860
INFO:root:[   29] Training loss: 0.68402137, Validation loss: 0.68365983, Gradient norm: 3.09980379
INFO:root:[   30] Training loss: 0.68331582, Validation loss: 0.68502431, Gradient norm: 3.14557099
INFO:root:[   31] Training loss: 0.68322659, Validation loss: 0.68348025, Gradient norm: 3.22080395
INFO:root:[   32] Training loss: 0.68266910, Validation loss: 0.68289156, Gradient norm: 2.91890787
INFO:root:[   33] Training loss: 0.68179669, Validation loss: 0.68200124, Gradient norm: 2.88969507
INFO:root:[   34] Training loss: 0.68106723, Validation loss: 0.68217275, Gradient norm: 3.09440245
INFO:root:[   35] Training loss: 0.68018275, Validation loss: 0.68301868, Gradient norm: 3.13833918
INFO:root:[   36] Training loss: 0.67948288, Validation loss: 0.67949422, Gradient norm: 2.99571103
INFO:root:[   37] Training loss: 0.67869946, Validation loss: 0.67864336, Gradient norm: 2.99898649
INFO:root:[   38] Training loss: 0.67879284, Validation loss: 0.68265719, Gradient norm: 3.07225926
INFO:root:[   39] Training loss: 0.67718799, Validation loss: 0.67769908, Gradient norm: 2.95684248
INFO:root:[   40] Training loss: 0.67671970, Validation loss: 0.67729193, Gradient norm: 3.14856663
INFO:root:[   41] Training loss: 0.67620323, Validation loss: 0.67605586, Gradient norm: 2.91011634
INFO:root:[   42] Training loss: 0.67565466, Validation loss: 0.67796854, Gradient norm: 2.91808237
INFO:root:[   43] Training loss: 0.67525411, Validation loss: 0.67460892, Gradient norm: 2.89902849
INFO:root:[   44] Training loss: 0.67497184, Validation loss: 0.67775083, Gradient norm: 2.86267797
INFO:root:[   45] Training loss: 0.67466602, Validation loss: 0.67655876, Gradient norm: 2.78572952
INFO:root:[   46] Training loss: 0.67442901, Validation loss: 0.67531417, Gradient norm: 2.96753425
INFO:root:[   47] Training loss: 0.67404637, Validation loss: 0.67598213, Gradient norm: 2.82090556
INFO:root:[   48] Training loss: 0.67467473, Validation loss: 0.67426406, Gradient norm: 2.82853907
INFO:root:[   49] Training loss: 0.67353150, Validation loss: 0.67483051, Gradient norm: 2.72363216
INFO:root:[   50] Training loss: 0.67360228, Validation loss: 0.67500663, Gradient norm: 2.74213559
INFO:root:[   51] Training loss: 0.67283007, Validation loss: 0.67540703, Gradient norm: 2.80876180
INFO:root:[   52] Training loss: 0.67292742, Validation loss: 0.67395209, Gradient norm: 2.72065804
INFO:root:[   53] Training loss: 0.67310853, Validation loss: 0.67244991, Gradient norm: 2.54383901
INFO:root:[   54] Training loss: 0.67218649, Validation loss: 0.67375604, Gradient norm: 2.78980877
INFO:root:[   55] Training loss: 0.67217264, Validation loss: 0.67395528, Gradient norm: 2.59683550
INFO:root:[   56] Training loss: 0.67182631, Validation loss: 0.67225558, Gradient norm: 2.55002301
INFO:root:[   57] Training loss: 0.67143043, Validation loss: 0.67250104, Gradient norm: 2.68039724
INFO:root:[   58] Training loss: 0.67123173, Validation loss: 0.67259312, Gradient norm: 2.62031271
INFO:root:[   59] Training loss: 0.67101697, Validation loss: 0.67350290, Gradient norm: 2.67760176
INFO:root:[   60] Training loss: 0.67093181, Validation loss: 0.67141569, Gradient norm: 2.57444066
INFO:root:[   61] Training loss: 0.67078591, Validation loss: 0.67041604, Gradient norm: 2.51478835
INFO:root:[   62] Training loss: 0.67047857, Validation loss: 0.67354449, Gradient norm: 2.54723597
INFO:root:[   63] Training loss: 0.67055838, Validation loss: 0.67202926, Gradient norm: 2.38201717
INFO:root:[   64] Training loss: 0.66949885, Validation loss: 0.66970667, Gradient norm: 2.38985530
INFO:root:[   65] Training loss: 0.66927462, Validation loss: 0.67108413, Gradient norm: 2.48404048
INFO:root:[   66] Training loss: 0.66934821, Validation loss: 0.67191220, Gradient norm: 2.30090274
INFO:root:[   67] Training loss: 0.66860097, Validation loss: 0.66994126, Gradient norm: 2.40384634
INFO:root:[   68] Training loss: 0.66854775, Validation loss: 0.66976769, Gradient norm: 2.40809496
INFO:root:[   69] Training loss: 0.66843750, Validation loss: 0.66797575, Gradient norm: 2.47759068
INFO:root:[   70] Training loss: 0.66814662, Validation loss: 0.67078434, Gradient norm: 2.33915516
INFO:root:[   71] Training loss: 0.66791924, Validation loss: 0.66915801, Gradient norm: 2.50749663
INFO:root:[   72] Training loss: 0.66774744, Validation loss: 0.67009754, Gradient norm: 2.47302325
INFO:root:[   73] Training loss: 0.66776415, Validation loss: 0.67089526, Gradient norm: 2.27105177
INFO:root:[   74] Training loss: 0.66757079, Validation loss: 0.66995785, Gradient norm: 2.38792450
INFO:root:[   75] Training loss: 0.66732218, Validation loss: 0.66808026, Gradient norm: 2.29785204
INFO:root:[   76] Training loss: 0.66712000, Validation loss: 0.67001049, Gradient norm: 2.37877384
INFO:root:[   77] Training loss: 0.66662566, Validation loss: 0.66961269, Gradient norm: 2.45858300
INFO:root:[   78] Training loss: 0.66662294, Validation loss: 0.66773017, Gradient norm: 2.34231902
INFO:root:[   79] Training loss: 0.66644918, Validation loss: 0.66998396, Gradient norm: 2.23741642
INFO:root:[   80] Training loss: 0.66593955, Validation loss: 0.66776537, Gradient norm: 2.31139856
INFO:root:[   81] Training loss: 0.66604571, Validation loss: 0.66570364, Gradient norm: 2.26340150
INFO:root:[   82] Training loss: 0.66598040, Validation loss: 0.66612378, Gradient norm: 2.37244083
INFO:root:[   83] Training loss: 0.66555460, Validation loss: 0.66704978, Gradient norm: 2.17606939
INFO:root:[   84] Training loss: 0.66560415, Validation loss: 0.66530660, Gradient norm: 2.29841869
INFO:root:[   85] Training loss: 0.66557881, Validation loss: 0.66718838, Gradient norm: 2.23578567
INFO:root:[   86] Training loss: 0.66516562, Validation loss: 0.66575509, Gradient norm: 2.16254819
INFO:root:[   87] Training loss: 0.66505952, Validation loss: 0.66474725, Gradient norm: 2.40064122
INFO:root:[   88] Training loss: 0.66446322, Validation loss: 0.66514526, Gradient norm: 2.20202460
INFO:root:[   89] Training loss: 0.66467520, Validation loss: 0.66448566, Gradient norm: 2.14639645
INFO:root:[   90] Training loss: 0.66476955, Validation loss: 0.66537022, Gradient norm: 2.17225670
INFO:root:[   91] Training loss: 0.66467545, Validation loss: 0.66428058, Gradient norm: 2.24535311
INFO:root:[   92] Training loss: 0.66451065, Validation loss: 0.66577807, Gradient norm: 2.23500556
INFO:root:[   93] Training loss: 0.66445198, Validation loss: 0.66515493, Gradient norm: 2.21956595
INFO:root:[   94] Training loss: 0.66433063, Validation loss: 0.66522569, Gradient norm: 2.26188357
INFO:root:[   95] Training loss: 0.66423454, Validation loss: 0.66728283, Gradient norm: 2.23165282
INFO:root:[   96] Training loss: 0.66432590, Validation loss: 0.66799855, Gradient norm: 2.10801194
INFO:root:[   97] Training loss: 0.66405618, Validation loss: 0.66656093, Gradient norm: 2.04551699
INFO:root:[   98] Training loss: 0.66362077, Validation loss: 0.66387820, Gradient norm: 1.89201345
INFO:root:[   99] Training loss: 0.66365358, Validation loss: 0.66411589, Gradient norm: 1.96550543
INFO:root:[  100] Training loss: 0.66371531, Validation loss: 0.66552861, Gradient norm: 1.99612843
INFO:root:[  101] Training loss: 0.66400515, Validation loss: 0.66494270, Gradient norm: 2.19466859
INFO:root:[  102] Training loss: 0.66333687, Validation loss: 0.66588480, Gradient norm: 2.06420037
INFO:root:[  103] Training loss: 0.66359824, Validation loss: 0.66591770, Gradient norm: 1.97045159
INFO:root:[  104] Training loss: 0.66354162, Validation loss: 0.66490845, Gradient norm: 2.11023969
INFO:root:[  105] Training loss: 0.66325463, Validation loss: 0.66454565, Gradient norm: 1.88716341
INFO:root:[  106] Training loss: 0.66313425, Validation loss: 0.66455912, Gradient norm: 1.94800774
INFO:root:[  107] Training loss: 0.66323382, Validation loss: 0.66684199, Gradient norm: 1.98551165
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 6713.695s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91638
INFO:root:EnergyScoreTrain: 0.66167
INFO:root:CRPSTrain: 0.60007
INFO:root:Gaussian NLLTrain: 2.031
INFO:root:CoverageTrain: 0.67848
INFO:root:IntervalWidthTrain: 2.20417
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91925
INFO:root:EnergyScoreValidation: 0.66391
INFO:root:CRPSValidation: 0.60248
INFO:root:Gaussian NLLValidation: 2.038
INFO:root:CoverageValidation: 0.67684
INFO:root:IntervalWidthValidation: 2.20354
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91839
INFO:root:EnergyScoreTest: 0.66342
INFO:root:CRPSTest: 0.60196
INFO:root:Gaussian NLLTest: 2.03482
INFO:root:CoverageTest: 0.67759
INFO:root:IntervalWidthTest: 2.20552
INFO:root:###3 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.75418192, Validation loss: 0.72547712, Gradient norm: 6.07634321
INFO:root:[    2] Training loss: 0.72147482, Validation loss: 0.71587961, Gradient norm: 4.57673764
INFO:root:[    3] Training loss: 0.71602340, Validation loss: 0.71618440, Gradient norm: 3.91362971
INFO:root:[    4] Training loss: 0.71182481, Validation loss: 0.70989574, Gradient norm: 4.26257327
INFO:root:[    5] Training loss: 0.70488553, Validation loss: 0.71307104, Gradient norm: 3.43201850
INFO:root:[    6] Training loss: 0.70295296, Validation loss: 0.70114748, Gradient norm: 3.91062051
INFO:root:[    7] Training loss: 0.70181302, Validation loss: 0.69964320, Gradient norm: 3.64393389
INFO:root:[    8] Training loss: 0.70005931, Validation loss: 0.70016203, Gradient norm: 3.13675375
INFO:root:[    9] Training loss: 0.69920120, Validation loss: 0.69752493, Gradient norm: 3.35479556
INFO:root:[   10] Training loss: 0.69786899, Validation loss: 0.69870719, Gradient norm: 3.02482102
INFO:root:[   11] Training loss: 0.69727644, Validation loss: 0.69634672, Gradient norm: 2.93604435
INFO:root:[   12] Training loss: 0.69709074, Validation loss: 0.69783063, Gradient norm: 3.02208533
INFO:root:[   13] Training loss: 0.69568782, Validation loss: 0.69572809, Gradient norm: 2.91191987
INFO:root:[   14] Training loss: 0.69418202, Validation loss: 0.69510683, Gradient norm: 2.49604600
INFO:root:[   15] Training loss: 0.69312299, Validation loss: 0.69434441, Gradient norm: 2.61028175
INFO:root:[   16] Training loss: 0.69220266, Validation loss: 0.69275744, Gradient norm: 2.48810790
INFO:root:[   17] Training loss: 0.69100882, Validation loss: 0.69081492, Gradient norm: 2.61766076
INFO:root:[   18] Training loss: 0.68986116, Validation loss: 0.68986927, Gradient norm: 2.36395197
INFO:root:[   19] Training loss: 0.68864076, Validation loss: 0.69044076, Gradient norm: 2.35679791
INFO:root:[   20] Training loss: 0.68766866, Validation loss: 0.68832820, Gradient norm: 2.09456974
INFO:root:[   21] Training loss: 0.68643826, Validation loss: 0.68685180, Gradient norm: 2.04305295
INFO:root:[   22] Training loss: 0.68591053, Validation loss: 0.68841815, Gradient norm: 1.99671766
INFO:root:[   23] Training loss: 0.68500297, Validation loss: 0.68681976, Gradient norm: 2.20507384
INFO:root:[   24] Training loss: 0.68433818, Validation loss: 0.68549345, Gradient norm: 1.91364084
INFO:root:[   25] Training loss: 0.68334262, Validation loss: 0.68402316, Gradient norm: 2.01270961
INFO:root:[   26] Training loss: 0.68268720, Validation loss: 0.68412543, Gradient norm: 2.00179022
INFO:root:[   27] Training loss: 0.68177636, Validation loss: 0.68196464, Gradient norm: 2.01055724
INFO:root:[   28] Training loss: 0.68130377, Validation loss: 0.68203456, Gradient norm: 1.86455731
INFO:root:[   29] Training loss: 0.68040655, Validation loss: 0.68156321, Gradient norm: 1.86994929
INFO:root:[   30] Training loss: 0.67960454, Validation loss: 0.67936057, Gradient norm: 1.90358645
INFO:root:[   31] Training loss: 0.67902441, Validation loss: 0.67865655, Gradient norm: 1.93932336
INFO:root:[   32] Training loss: 0.67881203, Validation loss: 0.67970451, Gradient norm: 1.80241258
INFO:root:[   33] Training loss: 0.67793176, Validation loss: 0.67992020, Gradient norm: 1.81527973
INFO:root:[   34] Training loss: 0.67717333, Validation loss: 0.67865256, Gradient norm: 1.83690784
INFO:root:[   35] Training loss: 0.67691200, Validation loss: 0.67869200, Gradient norm: 1.82956080
INFO:root:[   36] Training loss: 0.67622722, Validation loss: 0.67797114, Gradient norm: 1.81124705
INFO:root:[   37] Training loss: 0.67566850, Validation loss: 0.67461228, Gradient norm: 1.80711133
INFO:root:[   38] Training loss: 0.67500807, Validation loss: 0.67825634, Gradient norm: 1.81454695
INFO:root:[   39] Training loss: 0.67435897, Validation loss: 0.67550050, Gradient norm: 1.67801211
INFO:root:[   40] Training loss: 0.67390521, Validation loss: 0.67347606, Gradient norm: 1.83897394
INFO:root:[   41] Training loss: 0.67284429, Validation loss: 0.67465946, Gradient norm: 1.78525802
INFO:root:[   42] Training loss: 0.67278778, Validation loss: 0.67517083, Gradient norm: 1.78571498
INFO:root:[   43] Training loss: 0.67215053, Validation loss: 0.67281534, Gradient norm: 1.85714960
INFO:root:[   44] Training loss: 0.67184166, Validation loss: 0.67220253, Gradient norm: 1.68743557
INFO:root:[   45] Training loss: 0.67144246, Validation loss: 0.67308394, Gradient norm: 1.79100100
INFO:root:[   46] Training loss: 0.67151640, Validation loss: 0.67230228, Gradient norm: 1.82031933
INFO:root:[   47] Training loss: 0.67101755, Validation loss: 0.67396825, Gradient norm: 1.76176737
INFO:root:[   48] Training loss: 0.67099755, Validation loss: 0.67097037, Gradient norm: 1.78460518
INFO:root:[   49] Training loss: 0.67029689, Validation loss: 0.67067589, Gradient norm: 1.65441898
INFO:root:[   50] Training loss: 0.66960914, Validation loss: 0.67210565, Gradient norm: 1.81804877
INFO:root:[   51] Training loss: 0.66939381, Validation loss: 0.67158209, Gradient norm: 1.85036818
INFO:root:[   52] Training loss: 0.66926169, Validation loss: 0.66879220, Gradient norm: 1.76196179
INFO:root:[   53] Training loss: 0.66917381, Validation loss: 0.67012632, Gradient norm: 1.76563370
INFO:root:[   54] Training loss: 0.66864832, Validation loss: 0.66866545, Gradient norm: 1.67928296
INFO:root:[   55] Training loss: 0.66873904, Validation loss: 0.66980671, Gradient norm: 1.75782291
INFO:root:[   56] Training loss: 0.66816387, Validation loss: 0.66966083, Gradient norm: 1.67025740
INFO:root:[   57] Training loss: 0.66785583, Validation loss: 0.66919111, Gradient norm: 1.89051260
INFO:root:[   58] Training loss: 0.66790652, Validation loss: 0.67147944, Gradient norm: 1.67358294
INFO:root:[   59] Training loss: 0.66756943, Validation loss: 0.66843873, Gradient norm: 1.60590651
INFO:root:[   60] Training loss: 0.66713319, Validation loss: 0.66852541, Gradient norm: 1.70215558
INFO:root:[   61] Training loss: 0.66703512, Validation loss: 0.66940251, Gradient norm: 1.65897682
INFO:root:[   62] Training loss: 0.66690399, Validation loss: 0.66934350, Gradient norm: 1.74603201
INFO:root:[   63] Training loss: 0.66688029, Validation loss: 0.66703835, Gradient norm: 1.67382653
INFO:root:[   64] Training loss: 0.66603430, Validation loss: 0.66794218, Gradient norm: 1.66433037
INFO:root:[   65] Training loss: 0.66646674, Validation loss: 0.66833093, Gradient norm: 1.51267097
INFO:root:[   66] Training loss: 0.66599970, Validation loss: 0.66671277, Gradient norm: 1.70761726
INFO:root:[   67] Training loss: 0.66568549, Validation loss: 0.66772040, Gradient norm: 1.71696966
INFO:root:[   68] Training loss: 0.66567781, Validation loss: 0.66660026, Gradient norm: 1.71824346
INFO:root:[   69] Training loss: 0.66581381, Validation loss: 0.66539917, Gradient norm: 1.68923612
INFO:root:[   70] Training loss: 0.66510990, Validation loss: 0.66792151, Gradient norm: 1.68503060
INFO:root:[   71] Training loss: 0.66481865, Validation loss: 0.66665960, Gradient norm: 1.68428004
INFO:root:[   72] Training loss: 0.66470664, Validation loss: 0.66603003, Gradient norm: 1.69740767
INFO:root:[   73] Training loss: 0.66483086, Validation loss: 0.66531282, Gradient norm: 1.58992517
INFO:root:[   74] Training loss: 0.66409174, Validation loss: 0.66672483, Gradient norm: 1.62492283
INFO:root:[   75] Training loss: 0.66452087, Validation loss: 0.66550308, Gradient norm: 1.62836511
INFO:root:[   76] Training loss: 0.66414412, Validation loss: 0.66555663, Gradient norm: 1.65353527
INFO:root:[   77] Training loss: 0.66398576, Validation loss: 0.66440963, Gradient norm: 1.68358780
INFO:root:[   78] Training loss: 0.66336663, Validation loss: 0.66303842, Gradient norm: 1.61918023
INFO:root:[   79] Training loss: 0.66386378, Validation loss: 0.66486266, Gradient norm: 1.57304537
INFO:root:[   80] Training loss: 0.66338536, Validation loss: 0.66649203, Gradient norm: 1.63278534
INFO:root:[   81] Training loss: 0.66344388, Validation loss: 0.66441573, Gradient norm: 1.62231298
INFO:root:[   82] Training loss: 0.66372715, Validation loss: 0.66685582, Gradient norm: 1.55332497
INFO:root:[   83] Training loss: 0.66350171, Validation loss: 0.66734063, Gradient norm: 1.70023642
INFO:root:[   84] Training loss: 0.66327673, Validation loss: 0.66634450, Gradient norm: 1.52549336
INFO:root:[   85] Training loss: 0.66337788, Validation loss: 0.66585515, Gradient norm: 1.61901814
INFO:root:[   86] Training loss: 0.66280589, Validation loss: 0.66320678, Gradient norm: 1.60739541
INFO:root:[   87] Training loss: 0.66299714, Validation loss: 0.66406048, Gradient norm: 1.57717003
INFO:root:EP 87: Early stopping
INFO:root:Training the model took 5470.417s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91539
INFO:root:EnergyScoreTrain: 0.66095
INFO:root:CRPSTrain: 0.64232
INFO:root:Gaussian NLLTrain: 3.21055
INFO:root:CoverageTrain: 0.53897
INFO:root:IntervalWidthTrain: 1.77775
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91832
INFO:root:EnergyScoreValidation: 0.66323
INFO:root:CRPSValidation: 0.64506
INFO:root:Gaussian NLLValidation: 3.23243
INFO:root:CoverageValidation: 0.53672
INFO:root:IntervalWidthValidation: 1.77571
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91758
INFO:root:EnergyScoreTest: 0.66281
INFO:root:CRPSTest: 0.64457
INFO:root:Gaussian NLLTest: 3.22077
INFO:root:CoverageTest: 0.53741
INFO:root:IntervalWidthTest: 1.77678
INFO:root:###4 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.74615537, Validation loss: 0.72821265, Gradient norm: 3.34895731
INFO:root:[    2] Training loss: 0.71689014, Validation loss: 0.71046709, Gradient norm: 3.02299075
INFO:root:[    3] Training loss: 0.70738682, Validation loss: 0.70635013, Gradient norm: 2.23339313
INFO:root:[    4] Training loss: 0.70494240, Validation loss: 0.70233448, Gradient norm: 2.67544215
INFO:root:[    5] Training loss: 0.70200339, Validation loss: 0.70317131, Gradient norm: 2.56818550
INFO:root:[    6] Training loss: 0.69921438, Validation loss: 0.69711812, Gradient norm: 2.22478516
INFO:root:[    7] Training loss: 0.69623428, Validation loss: 0.69357982, Gradient norm: 2.28759872
INFO:root:[    8] Training loss: 0.69403424, Validation loss: 0.69585538, Gradient norm: 2.24112183
INFO:root:[    9] Training loss: 0.69153969, Validation loss: 0.69398671, Gradient norm: 2.12545122
INFO:root:[   10] Training loss: 0.69018961, Validation loss: 0.69099184, Gradient norm: 1.98835999
INFO:root:[   11] Training loss: 0.68920987, Validation loss: 0.68842464, Gradient norm: 2.01023113
INFO:root:[   12] Training loss: 0.68818671, Validation loss: 0.68954174, Gradient norm: 2.09369615
INFO:root:[   13] Training loss: 0.68739577, Validation loss: 0.68846862, Gradient norm: 1.93853709
INFO:root:[   14] Training loss: 0.68664719, Validation loss: 0.68781389, Gradient norm: 1.77748403
INFO:root:[   15] Training loss: 0.68533454, Validation loss: 0.68470476, Gradient norm: 1.94977253
INFO:root:[   16] Training loss: 0.68447368, Validation loss: 0.68462834, Gradient norm: 1.89118691
INFO:root:[   17] Training loss: 0.68349027, Validation loss: 0.68388277, Gradient norm: 1.90103642
INFO:root:[   18] Training loss: 0.68304028, Validation loss: 0.68216208, Gradient norm: 1.96622839
INFO:root:[   19] Training loss: 0.68211236, Validation loss: 0.68229827, Gradient norm: 1.83201417
INFO:root:[   20] Training loss: 0.68153813, Validation loss: 0.68199247, Gradient norm: 1.91658611
INFO:root:[   21] Training loss: 0.68088940, Validation loss: 0.68241079, Gradient norm: 1.90988698
INFO:root:[   22] Training loss: 0.68028091, Validation loss: 0.68142853, Gradient norm: 1.96651719
INFO:root:[   23] Training loss: 0.67970920, Validation loss: 0.67805686, Gradient norm: 1.94948751
INFO:root:[   24] Training loss: 0.67919014, Validation loss: 0.68227848, Gradient norm: 1.83400991
INFO:root:[   25] Training loss: 0.67837030, Validation loss: 0.67729017, Gradient norm: 1.92160092
INFO:root:[   26] Training loss: 0.67815303, Validation loss: 0.67968047, Gradient norm: 1.84082407
INFO:root:[   27] Training loss: 0.67801442, Validation loss: 0.67819771, Gradient norm: 1.86936388
INFO:root:[   28] Training loss: 0.67718616, Validation loss: 0.67859621, Gradient norm: 1.88146122
INFO:root:[   29] Training loss: 0.67676598, Validation loss: 0.67673346, Gradient norm: 1.75005594
INFO:root:[   30] Training loss: 0.67642053, Validation loss: 0.67825418, Gradient norm: 1.88966287
INFO:root:[   31] Training loss: 0.67625385, Validation loss: 0.67503390, Gradient norm: 1.82437722
INFO:root:[   32] Training loss: 0.67597412, Validation loss: 0.67891458, Gradient norm: 1.76833600
INFO:root:[   33] Training loss: 0.67528693, Validation loss: 0.67680972, Gradient norm: 1.81414032
INFO:root:[   34] Training loss: 0.67518055, Validation loss: 0.67693892, Gradient norm: 1.80861876
INFO:root:[   35] Training loss: 0.67477274, Validation loss: 0.67655554, Gradient norm: 1.86950356
INFO:root:[   36] Training loss: 0.67428490, Validation loss: 0.67667707, Gradient norm: 1.78348698
INFO:root:[   37] Training loss: 0.67403452, Validation loss: 0.67437098, Gradient norm: 1.74067542
INFO:root:[   38] Training loss: 0.67387596, Validation loss: 0.67390164, Gradient norm: 1.79994426
INFO:root:[   39] Training loss: 0.67377086, Validation loss: 0.67577228, Gradient norm: 1.83496985
INFO:root:[   40] Training loss: 0.67327971, Validation loss: 0.67300476, Gradient norm: 1.76557726
INFO:root:[   41] Training loss: 0.67273982, Validation loss: 0.67607004, Gradient norm: 1.79389525
INFO:root:[   42] Training loss: 0.67288537, Validation loss: 0.67458048, Gradient norm: 1.69702390
INFO:root:[   43] Training loss: 0.67258578, Validation loss: 0.67421807, Gradient norm: 1.76467247
INFO:root:[   44] Training loss: 0.67204736, Validation loss: 0.67368903, Gradient norm: 1.75424391
INFO:root:[   45] Training loss: 0.67236482, Validation loss: 0.67458639, Gradient norm: 1.82796729
INFO:root:[   46] Training loss: 0.67168518, Validation loss: 0.67161722, Gradient norm: 1.73355928
INFO:root:[   47] Training loss: 0.67157836, Validation loss: 0.67322625, Gradient norm: 1.80699753
INFO:root:[   48] Training loss: 0.67171562, Validation loss: 0.67065889, Gradient norm: 1.78249046
INFO:root:[   49] Training loss: 0.67101927, Validation loss: 0.67132548, Gradient norm: 1.78123190
INFO:root:[   50] Training loss: 0.67183625, Validation loss: 0.67375615, Gradient norm: 1.74065436
INFO:root:[   51] Training loss: 0.67076117, Validation loss: 0.67082863, Gradient norm: 1.77936732
INFO:root:[   52] Training loss: 0.67019234, Validation loss: 0.66964544, Gradient norm: 1.68261158
INFO:root:[   53] Training loss: 0.66955258, Validation loss: 0.66973306, Gradient norm: 1.65512578
INFO:root:[   54] Training loss: 0.66933573, Validation loss: 0.67041255, Gradient norm: 1.69167740
INFO:root:[   55] Training loss: 0.66879036, Validation loss: 0.66914541, Gradient norm: 1.65776614
INFO:root:[   56] Training loss: 0.66774354, Validation loss: 0.67084704, Gradient norm: 1.55514669
INFO:root:[   57] Training loss: 0.66706763, Validation loss: 0.66682148, Gradient norm: 1.48485974
INFO:root:[   58] Training loss: 0.66682164, Validation loss: 0.66694989, Gradient norm: 1.56016003
INFO:root:[   59] Training loss: 0.66668799, Validation loss: 0.66927643, Gradient norm: 1.46717221
INFO:root:[   60] Training loss: 0.66597485, Validation loss: 0.66725472, Gradient norm: 1.46598186
INFO:root:[   61] Training loss: 0.66571396, Validation loss: 0.66444762, Gradient norm: 1.49262494
INFO:root:[   62] Training loss: 0.66489468, Validation loss: 0.66408239, Gradient norm: 1.23981841
INFO:root:[   63] Training loss: 0.66489892, Validation loss: 0.66336758, Gradient norm: 1.27795733
INFO:root:[   64] Training loss: 0.66420638, Validation loss: 0.66706722, Gradient norm: 1.28297907
INFO:root:[   65] Training loss: 0.66461695, Validation loss: 0.66380813, Gradient norm: 1.23874226
INFO:root:[   66] Training loss: 0.66389156, Validation loss: 0.66437815, Gradient norm: 1.16262954
INFO:root:[   67] Training loss: 0.66353107, Validation loss: 0.66633240, Gradient norm: 1.20726521
INFO:root:[   68] Training loss: 0.66322866, Validation loss: 0.66498165, Gradient norm: 1.14593739
INFO:root:[   69] Training loss: 0.66326035, Validation loss: 0.66592622, Gradient norm: 1.13666494
INFO:root:[   70] Training loss: 0.66305188, Validation loss: 0.66575789, Gradient norm: 1.12799286
INFO:root:[   71] Training loss: 0.66252206, Validation loss: 0.66577415, Gradient norm: 1.06338131
INFO:root:[   72] Training loss: 0.66234981, Validation loss: 0.66646598, Gradient norm: 1.08032759
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 4529.33s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91651
INFO:root:EnergyScoreTrain: 0.66179
INFO:root:CRPSTrain: 0.57948
INFO:root:Gaussian NLLTrain: 1.58089
INFO:root:CoverageTrain: 0.85665
INFO:root:IntervalWidthTrain: 3.64665
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91874
INFO:root:EnergyScoreValidation: 0.66355
INFO:root:CRPSValidation: 0.5812
INFO:root:Gaussian NLLValidation: 1.58462
INFO:root:CoverageValidation: 0.85602
INFO:root:IntervalWidthValidation: 3.64671
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91824
INFO:root:EnergyScoreTest: 0.6633
INFO:root:CRPSTest: 0.58084
INFO:root:Gaussian NLLTest: 1.58304
INFO:root:CoverageTest: 0.85658
INFO:root:IntervalWidthTest: 3.64801
INFO:root:###5 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.73464677, Validation loss: 0.71460202, Gradient norm: 2.03328550
INFO:root:[    2] Training loss: 0.71052467, Validation loss: 0.70516219, Gradient norm: 1.80262749
INFO:root:[    3] Training loss: 0.70181987, Validation loss: 0.69952153, Gradient norm: 1.44901001
INFO:root:[    4] Training loss: 0.69718877, Validation loss: 0.69833746, Gradient norm: 1.37265308
INFO:root:[    5] Training loss: 0.69356323, Validation loss: 0.69321998, Gradient norm: 1.38073060
INFO:root:[    6] Training loss: 0.69168960, Validation loss: 0.69123981, Gradient norm: 1.26763444
INFO:root:[    7] Training loss: 0.69042501, Validation loss: 0.69306234, Gradient norm: 1.31900766
INFO:root:[    8] Training loss: 0.68946077, Validation loss: 0.69025355, Gradient norm: 1.21525711
INFO:root:[    9] Training loss: 0.68790685, Validation loss: 0.69146644, Gradient norm: 1.08185661
INFO:root:[   10] Training loss: 0.68738617, Validation loss: 0.68628982, Gradient norm: 1.13560417
INFO:root:[   11] Training loss: 0.68627076, Validation loss: 0.68554844, Gradient norm: 1.03566821
INFO:root:[   12] Training loss: 0.68518243, Validation loss: 0.68593751, Gradient norm: 1.13363165
INFO:root:[   13] Training loss: 0.68412074, Validation loss: 0.68470517, Gradient norm: 1.02623693
INFO:root:[   14] Training loss: 0.68309418, Validation loss: 0.68367870, Gradient norm: 1.02911858
INFO:root:[   15] Training loss: 0.68188831, Validation loss: 0.68320993, Gradient norm: 1.14750538
INFO:root:[   16] Training loss: 0.68079715, Validation loss: 0.68637275, Gradient norm: 1.10241959
INFO:root:[   17] Training loss: 0.68026280, Validation loss: 0.68199969, Gradient norm: 1.08826448
INFO:root:[   18] Training loss: 0.67975709, Validation loss: 0.68042674, Gradient norm: 1.07907025
INFO:root:[   19] Training loss: 0.67907679, Validation loss: 0.68002402, Gradient norm: 1.10211431
INFO:root:[   20] Training loss: 0.67824894, Validation loss: 0.67937185, Gradient norm: 1.02728828
INFO:root:[   21] Training loss: 0.67786936, Validation loss: 0.67861405, Gradient norm: 1.10103147
INFO:root:[   22] Training loss: 0.67691673, Validation loss: 0.67925909, Gradient norm: 1.00738693
INFO:root:[   23] Training loss: 0.67629128, Validation loss: 0.67709259, Gradient norm: 1.06133042
INFO:root:[   24] Training loss: 0.67587614, Validation loss: 0.67707267, Gradient norm: 1.07419775
INFO:root:[   25] Training loss: 0.67515814, Validation loss: 0.67823042, Gradient norm: 1.08877246
INFO:root:[   26] Training loss: 0.67479272, Validation loss: 0.67637826, Gradient norm: 1.08519638
INFO:root:[   27] Training loss: 0.67404809, Validation loss: 0.67501041, Gradient norm: 1.00892109
INFO:root:[   28] Training loss: 0.67370963, Validation loss: 0.67297712, Gradient norm: 1.09276722
INFO:root:[   29] Training loss: 0.67333740, Validation loss: 0.67421289, Gradient norm: 1.01138665
INFO:root:[   30] Training loss: 0.67273300, Validation loss: 0.67328568, Gradient norm: 0.99836911
INFO:root:[   31] Training loss: 0.67283689, Validation loss: 0.67255849, Gradient norm: 1.05789401
INFO:root:[   32] Training loss: 0.67208337, Validation loss: 0.67346226, Gradient norm: 0.99594068
INFO:root:[   33] Training loss: 0.67155594, Validation loss: 0.67527909, Gradient norm: 1.00292611
INFO:root:[   34] Training loss: 0.67162951, Validation loss: 0.67143507, Gradient norm: 1.07111603
INFO:root:[   35] Training loss: 0.67127397, Validation loss: 0.67330383, Gradient norm: 0.99732763
INFO:root:[   36] Training loss: 0.67106197, Validation loss: 0.67132527, Gradient norm: 0.98716271
INFO:root:[   37] Training loss: 0.67077041, Validation loss: 0.67110908, Gradient norm: 1.02616747
INFO:root:[   38] Training loss: 0.67068971, Validation loss: 0.67284849, Gradient norm: 0.98596420
INFO:root:[   39] Training loss: 0.67043811, Validation loss: 0.67386276, Gradient norm: 0.99620873
INFO:root:[   40] Training loss: 0.66969836, Validation loss: 0.66944737, Gradient norm: 1.02177059
INFO:root:[   41] Training loss: 0.66954910, Validation loss: 0.67019537, Gradient norm: 0.91344556
INFO:root:[   42] Training loss: 0.66983608, Validation loss: 0.66991299, Gradient norm: 1.02229534
INFO:root:[   43] Training loss: 0.66947691, Validation loss: 0.67029684, Gradient norm: 0.97495775
INFO:root:[   44] Training loss: 0.66912637, Validation loss: 0.66972070, Gradient norm: 0.99893300
INFO:root:[   45] Training loss: 0.66890731, Validation loss: 0.67148336, Gradient norm: 0.97937761
INFO:root:[   46] Training loss: 0.66864760, Validation loss: 0.66933915, Gradient norm: 0.97756553
INFO:root:[   47] Training loss: 0.66835913, Validation loss: 0.66802201, Gradient norm: 1.02888381
INFO:root:[   48] Training loss: 0.66819482, Validation loss: 0.66998714, Gradient norm: 0.98966485
INFO:root:[   49] Training loss: 0.66819850, Validation loss: 0.66904346, Gradient norm: 0.95543427
INFO:root:[   50] Training loss: 0.66798734, Validation loss: 0.66926717, Gradient norm: 0.96446019
INFO:root:[   51] Training loss: 0.66744949, Validation loss: 0.66842088, Gradient norm: 0.91498376
INFO:root:[   52] Training loss: 0.66753026, Validation loss: 0.67070847, Gradient norm: 0.93414523
INFO:root:[   53] Training loss: 0.66717366, Validation loss: 0.66816055, Gradient norm: 0.97385356
INFO:root:[   54] Training loss: 0.66748865, Validation loss: 0.67001865, Gradient norm: 1.01170036
INFO:root:[   55] Training loss: 0.66719394, Validation loss: 0.66798001, Gradient norm: 0.92459605
INFO:root:[   56] Training loss: 0.66730431, Validation loss: 0.66883046, Gradient norm: 0.99182034
INFO:root:[   57] Training loss: 0.66670987, Validation loss: 0.66708650, Gradient norm: 0.95218891
INFO:root:[   58] Training loss: 0.66644384, Validation loss: 0.66834875, Gradient norm: 0.93255813
INFO:root:[   59] Training loss: 0.66637685, Validation loss: 0.66738110, Gradient norm: 0.85757555
INFO:root:[   60] Training loss: 0.66610471, Validation loss: 0.66876642, Gradient norm: 0.91896547
INFO:root:[   61] Training loss: 0.66632205, Validation loss: 0.66824763, Gradient norm: 0.93596736
INFO:root:[   62] Training loss: 0.66613361, Validation loss: 0.66934519, Gradient norm: 0.93147648
INFO:root:[   63] Training loss: 0.66602036, Validation loss: 0.66761658, Gradient norm: 0.96365446
INFO:root:[   64] Training loss: 0.66553877, Validation loss: 0.66658397, Gradient norm: 0.89157173
INFO:root:[   65] Training loss: 0.66609143, Validation loss: 0.66736177, Gradient norm: 0.93531089
INFO:root:[   66] Training loss: 0.66596653, Validation loss: 0.66823849, Gradient norm: 0.92739733
INFO:root:[   67] Training loss: 0.66552796, Validation loss: 0.66691020, Gradient norm: 0.96524869
INFO:root:[   68] Training loss: 0.66598031, Validation loss: 0.66531967, Gradient norm: 0.92438719
INFO:root:[   69] Training loss: 0.66543740, Validation loss: 0.66588227, Gradient norm: 0.89186501
INFO:root:[   70] Training loss: 0.66527175, Validation loss: 0.66560410, Gradient norm: 0.89619321
INFO:root:[   71] Training loss: 0.66540742, Validation loss: 0.66801422, Gradient norm: 0.95384622
INFO:root:[   72] Training loss: 0.66539336, Validation loss: 0.66568212, Gradient norm: 0.92483154
INFO:root:[   73] Training loss: 0.66496750, Validation loss: 0.66605708, Gradient norm: 0.85290401
INFO:root:[   74] Training loss: 0.66519958, Validation loss: 0.66676440, Gradient norm: 0.86169455
INFO:root:[   75] Training loss: 0.66491578, Validation loss: 0.66499643, Gradient norm: 0.87758192
INFO:root:[   76] Training loss: 0.66478425, Validation loss: 0.66781759, Gradient norm: 0.92539022
INFO:root:[   77] Training loss: 0.66507211, Validation loss: 0.66565754, Gradient norm: 0.89818451
INFO:root:[   78] Training loss: 0.66491672, Validation loss: 0.66532054, Gradient norm: 0.88921577
INFO:root:[   79] Training loss: 0.66435594, Validation loss: 0.66542369, Gradient norm: 0.84022487
INFO:root:[   80] Training loss: 0.66449752, Validation loss: 0.66839628, Gradient norm: 0.87149696
INFO:root:[   81] Training loss: 0.66458300, Validation loss: 0.66824719, Gradient norm: 0.85846216
INFO:root:[   82] Training loss: 0.66432937, Validation loss: 0.66427312, Gradient norm: 0.86938636
INFO:root:[   83] Training loss: 0.66393337, Validation loss: 0.66770484, Gradient norm: 0.88470430
INFO:root:[   84] Training loss: 0.66416746, Validation loss: 0.66537575, Gradient norm: 0.83361366
INFO:root:[   85] Training loss: 0.66425493, Validation loss: 0.66585827, Gradient norm: 0.89867570
INFO:root:[   86] Training loss: 0.66396742, Validation loss: 0.66588919, Gradient norm: 0.84583593
INFO:root:[   87] Training loss: 0.66405316, Validation loss: 0.66572855, Gradient norm: 0.81893269
INFO:root:[   88] Training loss: 0.66387986, Validation loss: 0.66476805, Gradient norm: 0.82173627
INFO:root:[   89] Training loss: 0.66413964, Validation loss: 0.66578246, Gradient norm: 0.89255718
INFO:root:[   90] Training loss: 0.66358603, Validation loss: 0.66468826, Gradient norm: 0.87619870
INFO:root:[   91] Training loss: 0.66370888, Validation loss: 0.66592881, Gradient norm: 0.85082340
INFO:root:EP 91: Early stopping
INFO:root:Training the model took 5741.163s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91692
INFO:root:EnergyScoreTrain: 0.66227
INFO:root:CRPSTrain: 0.5928
INFO:root:Gaussian NLLTrain: 1.59927
INFO:root:CoverageTrain: 0.86496
INFO:root:IntervalWidthTrain: 3.9545
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91978
INFO:root:EnergyScoreValidation: 0.66453
INFO:root:CRPSValidation: 0.59523
INFO:root:Gaussian NLLValidation: 1.60755
INFO:root:CoverageValidation: 0.86357
INFO:root:IntervalWidthValidation: 3.94996
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91916
INFO:root:EnergyScoreTest: 0.66416
INFO:root:CRPSTest: 0.59478
INFO:root:Gaussian NLLTest: 1.6064
INFO:root:CoverageTest: 0.8636
INFO:root:IntervalWidthTest: 3.95153
INFO:root:###6 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.73474900, Validation loss: 0.71612805, Gradient norm: 1.49389840
INFO:root:[    2] Training loss: 0.70932223, Validation loss: 0.70175933, Gradient norm: 0.95808348
INFO:root:[    3] Training loss: 0.69904204, Validation loss: 0.69678995, Gradient norm: 0.90556755
INFO:root:[    4] Training loss: 0.69562862, Validation loss: 0.69659842, Gradient norm: 0.87386786
INFO:root:[    5] Training loss: 0.69308605, Validation loss: 0.69278072, Gradient norm: 0.81801023
INFO:root:[    6] Training loss: 0.69106293, Validation loss: 0.69145408, Gradient norm: 0.77895214
INFO:root:[    7] Training loss: 0.68967542, Validation loss: 0.69044440, Gradient norm: 0.77320686
INFO:root:[    8] Training loss: 0.68897505, Validation loss: 0.69072951, Gradient norm: 0.76948906
INFO:root:[    9] Training loss: 0.68768969, Validation loss: 0.68876730, Gradient norm: 0.73623751
INFO:root:[   10] Training loss: 0.68682516, Validation loss: 0.68604256, Gradient norm: 0.77344467
INFO:root:[   11] Training loss: 0.68606203, Validation loss: 0.68626901, Gradient norm: 0.74687585
INFO:root:[   12] Training loss: 0.68485827, Validation loss: 0.68675533, Gradient norm: 0.69095279
INFO:root:[   13] Training loss: 0.68403347, Validation loss: 0.68435464, Gradient norm: 0.71914693
INFO:root:[   14] Training loss: 0.68320951, Validation loss: 0.68551768, Gradient norm: 0.77168091
INFO:root:[   15] Training loss: 0.68224401, Validation loss: 0.68361167, Gradient norm: 0.76874321
INFO:root:[   16] Training loss: 0.68112371, Validation loss: 0.68154600, Gradient norm: 0.75763409
INFO:root:[   17] Training loss: 0.68059599, Validation loss: 0.68101921, Gradient norm: 0.77660154
INFO:root:[   18] Training loss: 0.67992958, Validation loss: 0.68301008, Gradient norm: 0.80446254
INFO:root:[   19] Training loss: 0.67937802, Validation loss: 0.67883536, Gradient norm: 0.79013599
INFO:root:[   20] Training loss: 0.67865435, Validation loss: 0.67826472, Gradient norm: 0.69486814
INFO:root:[   21] Training loss: 0.67799305, Validation loss: 0.67973337, Gradient norm: 0.72119816
INFO:root:[   22] Training loss: 0.67762893, Validation loss: 0.67924850, Gradient norm: 0.77734064
INFO:root:[   23] Training loss: 0.67725534, Validation loss: 0.67850536, Gradient norm: 0.81126033
INFO:root:[   24] Training loss: 0.67644296, Validation loss: 0.67811666, Gradient norm: 0.77968666
INFO:root:[   25] Training loss: 0.67612736, Validation loss: 0.67671171, Gradient norm: 0.79097155
INFO:root:[   26] Training loss: 0.67548107, Validation loss: 0.67583412, Gradient norm: 0.77878704
INFO:root:[   27] Training loss: 0.67493187, Validation loss: 0.67693279, Gradient norm: 0.80353649
INFO:root:[   28] Training loss: 0.67483221, Validation loss: 0.67447150, Gradient norm: 0.76627018
INFO:root:[   29] Training loss: 0.67420642, Validation loss: 0.67613683, Gradient norm: 0.79079542
INFO:root:[   30] Training loss: 0.67369284, Validation loss: 0.67347610, Gradient norm: 0.80172624
INFO:root:[   31] Training loss: 0.67348457, Validation loss: 0.67397359, Gradient norm: 0.75775142
INFO:root:[   32] Training loss: 0.67254515, Validation loss: 0.67363254, Gradient norm: 0.76928501
INFO:root:[   33] Training loss: 0.67217398, Validation loss: 0.67225991, Gradient norm: 0.76140531
INFO:root:[   34] Training loss: 0.67176979, Validation loss: 0.67258280, Gradient norm: 0.69680438
INFO:root:[   35] Training loss: 0.67139710, Validation loss: 0.67146544, Gradient norm: 0.78217307
INFO:root:[   36] Training loss: 0.67070820, Validation loss: 0.67235657, Gradient norm: 0.70130148
INFO:root:[   37] Training loss: 0.67024925, Validation loss: 0.67282041, Gradient norm: 0.75146945
INFO:root:[   38] Training loss: 0.67042634, Validation loss: 0.67093796, Gradient norm: 0.79116350
INFO:root:[   39] Training loss: 0.66980656, Validation loss: 0.67107352, Gradient norm: 0.72862279
INFO:root:[   40] Training loss: 0.66943002, Validation loss: 0.66988937, Gradient norm: 0.70834682
INFO:root:[   41] Training loss: 0.66920036, Validation loss: 0.67060386, Gradient norm: 0.75981265
INFO:root:[   42] Training loss: 0.66899112, Validation loss: 0.66974341, Gradient norm: 0.68236382
INFO:root:[   43] Training loss: 0.66915042, Validation loss: 0.67042979, Gradient norm: 0.73636349
INFO:root:[   44] Training loss: 0.66861055, Validation loss: 0.66870727, Gradient norm: 0.70816013
INFO:root:[   45] Training loss: 0.66859224, Validation loss: 0.66802825, Gradient norm: 0.75876287
INFO:root:[   46] Training loss: 0.66836272, Validation loss: 0.67146932, Gradient norm: 0.71734169
INFO:root:[   47] Training loss: 0.66803306, Validation loss: 0.67007008, Gradient norm: 0.78172698
INFO:root:[   48] Training loss: 0.66774385, Validation loss: 0.66865342, Gradient norm: 0.70922516
INFO:root:[   49] Training loss: 0.66707477, Validation loss: 0.66753148, Gradient norm: 0.67885768
INFO:root:[   50] Training loss: 0.66692876, Validation loss: 0.66746291, Gradient norm: 0.63759158
INFO:root:[   51] Training loss: 0.66696310, Validation loss: 0.66794957, Gradient norm: 0.70189385
INFO:root:[   52] Training loss: 0.66637631, Validation loss: 0.66944356, Gradient norm: 0.58325126
INFO:root:[   53] Training loss: 0.66680385, Validation loss: 0.66737791, Gradient norm: 0.77218239
INFO:root:[   54] Training loss: 0.66603412, Validation loss: 0.66649781, Gradient norm: 0.63972111
INFO:root:[   55] Training loss: 0.66629079, Validation loss: 0.66667391, Gradient norm: 0.72613834
INFO:root:[   56] Training loss: 0.66583221, Validation loss: 0.66783105, Gradient norm: 0.67455462
INFO:root:[   57] Training loss: 0.66555358, Validation loss: 0.66693158, Gradient norm: 0.69719307
INFO:root:[   58] Training loss: 0.66557570, Validation loss: 0.66603520, Gradient norm: 0.66055236
INFO:root:[   59] Training loss: 0.66598040, Validation loss: 0.66783403, Gradient norm: 0.72178418
INFO:root:[   60] Training loss: 0.66513343, Validation loss: 0.66990411, Gradient norm: 0.62739218
INFO:root:[   61] Training loss: 0.66536678, Validation loss: 0.66828183, Gradient norm: 0.71866315
INFO:root:[   62] Training loss: 0.66471131, Validation loss: 0.66692706, Gradient norm: 0.60367727
INFO:root:[   63] Training loss: 0.66544383, Validation loss: 0.66894954, Gradient norm: 0.74631381
INFO:root:[   64] Training loss: 0.66489804, Validation loss: 0.66595044, Gradient norm: 0.69163847
INFO:root:[   65] Training loss: 0.66427596, Validation loss: 0.66617394, Gradient norm: 0.49426565
INFO:root:[   66] Training loss: 0.66458999, Validation loss: 0.66983477, Gradient norm: 0.70993067
INFO:root:[   67] Training loss: 0.66484995, Validation loss: 0.66667229, Gradient norm: 0.72877883
INFO:root:[   68] Training loss: 0.66470919, Validation loss: 0.66807116, Gradient norm: 0.61403290
INFO:root:[   69] Training loss: 0.66434267, Validation loss: 0.66708878, Gradient norm: 0.66666266
INFO:root:[   70] Training loss: 0.66438916, Validation loss: 0.67004444, Gradient norm: 0.71721425
INFO:root:[   71] Training loss: 0.66396185, Validation loss: 0.66709372, Gradient norm: 0.57068588
INFO:root:[   72] Training loss: 0.66436893, Validation loss: 0.66496125, Gradient norm: 0.67306366
INFO:root:[   73] Training loss: 0.66417627, Validation loss: 0.66683204, Gradient norm: 0.69671671
INFO:root:[   74] Training loss: 0.66425945, Validation loss: 0.66540493, Gradient norm: 0.68210524
INFO:root:[   75] Training loss: 0.66394496, Validation loss: 0.66683917, Gradient norm: 0.64754187
INFO:root:[   76] Training loss: 0.66376909, Validation loss: 0.66497391, Gradient norm: 0.61088827
INFO:root:[   77] Training loss: 0.66395475, Validation loss: 0.66422046, Gradient norm: 0.68850838
INFO:root:[   78] Training loss: 0.66361537, Validation loss: 0.66659953, Gradient norm: 0.64778190
INFO:root:[   79] Training loss: 0.66370483, Validation loss: 0.66508701, Gradient norm: 0.67591414
INFO:root:[   80] Training loss: 0.66348353, Validation loss: 0.66464394, Gradient norm: 0.64515053
INFO:root:[   81] Training loss: 0.66381505, Validation loss: 0.66525307, Gradient norm: 0.69644728
INFO:root:[   82] Training loss: 0.66322929, Validation loss: 0.66461973, Gradient norm: 0.60579719
INFO:root:[   83] Training loss: 0.66365840, Validation loss: 0.66482847, Gradient norm: 0.74228843
INFO:root:[   84] Training loss: 0.66295521, Validation loss: 0.66390637, Gradient norm: 0.45807271
INFO:root:[   85] Training loss: 0.66351570, Validation loss: 0.66518373, Gradient norm: 0.67136887
INFO:root:[   86] Training loss: 0.66352254, Validation loss: 0.66412772, Gradient norm: 0.66365415
INFO:root:[   87] Training loss: 0.66283630, Validation loss: 0.66365265, Gradient norm: 0.55288084
INFO:root:[   88] Training loss: 0.66308562, Validation loss: 0.66491215, Gradient norm: 0.63264486
INFO:root:[   89] Training loss: 0.66341985, Validation loss: 0.66297368, Gradient norm: 0.68456557
INFO:root:[   90] Training loss: 0.66302814, Validation loss: 0.66523597, Gradient norm: 0.66287186
INFO:root:[   91] Training loss: 0.66308951, Validation loss: 0.66597483, Gradient norm: 0.68152342
INFO:root:[   92] Training loss: 0.66282105, Validation loss: 0.66534707, Gradient norm: 0.65177112
INFO:root:[   93] Training loss: 0.66266635, Validation loss: 0.66375056, Gradient norm: 0.64097084
INFO:root:[   94] Training loss: 0.66305465, Validation loss: 0.66462581, Gradient norm: 0.65699162
INFO:root:[   95] Training loss: 0.66225294, Validation loss: 0.66403045, Gradient norm: 0.53066374
INFO:root:[   96] Training loss: 0.66258344, Validation loss: 0.66787556, Gradient norm: 0.59990405
INFO:root:[   97] Training loss: 0.66272995, Validation loss: 0.66393559, Gradient norm: 0.65226169
INFO:root:[   98] Training loss: 0.66223126, Validation loss: 0.66558785, Gradient norm: 0.47870357
INFO:root:EP 98: Early stopping
INFO:root:Training the model took 6256.904s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91446
INFO:root:EnergyScoreTrain: 0.66083
INFO:root:CRPSTrain: 0.55796
INFO:root:Gaussian NLLTrain: 1.56981
INFO:root:CoverageTrain: 0.85753
INFO:root:IntervalWidthTrain: 3.50818
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.9177
INFO:root:EnergyScoreValidation: 0.66336
INFO:root:CRPSValidation: 0.56024
INFO:root:Gaussian NLLValidation: 1.57537
INFO:root:CoverageValidation: 0.85625
INFO:root:IntervalWidthValidation: 3.50329
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91698
INFO:root:EnergyScoreTest: 0.66289
INFO:root:CRPSTest: 0.55984
INFO:root:Gaussian NLLTest: 1.57414
INFO:root:CoverageTest: 0.85672
INFO:root:IntervalWidthTest: 3.50537
INFO:root:###7 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.73266270, Validation loss: 0.71291611, Gradient norm: 0.92420588
INFO:root:[    2] Training loss: 0.70590871, Validation loss: 0.70535989, Gradient norm: 0.63560264
INFO:root:[    3] Training loss: 0.69794002, Validation loss: 0.69557426, Gradient norm: 0.54487374
INFO:root:[    4] Training loss: 0.69436322, Validation loss: 0.69697801, Gradient norm: 0.43940323
INFO:root:[    5] Training loss: 0.69223545, Validation loss: 0.69523588, Gradient norm: 0.50931587
INFO:root:[    6] Training loss: 0.69037131, Validation loss: 0.69107994, Gradient norm: 0.40360529
INFO:root:[    7] Training loss: 0.68957447, Validation loss: 0.68861249, Gradient norm: 0.45538118
INFO:root:[    8] Training loss: 0.68825137, Validation loss: 0.68897515, Gradient norm: 0.33063518
INFO:root:[    9] Training loss: 0.68829450, Validation loss: 0.68929232, Gradient norm: 0.52226009
INFO:root:[   10] Training loss: 0.68654073, Validation loss: 0.68674501, Gradient norm: 0.36264585
INFO:root:[   11] Training loss: 0.68638827, Validation loss: 0.68792888, Gradient norm: 0.48794803
INFO:root:[   12] Training loss: 0.68465242, Validation loss: 0.68573626, Gradient norm: 0.43008363
INFO:root:[   13] Training loss: 0.68268499, Validation loss: 0.68193779, Gradient norm: 0.36879215
INFO:root:[   14] Training loss: 0.68144953, Validation loss: 0.68178909, Gradient norm: 0.37819512
INFO:root:[   15] Training loss: 0.67933497, Validation loss: 0.67966320, Gradient norm: 0.38121851
INFO:root:[   16] Training loss: 0.67770536, Validation loss: 0.67910336, Gradient norm: 0.27690370
INFO:root:[   17] Training loss: 0.67744532, Validation loss: 0.67848329, Gradient norm: 0.41939170
INFO:root:[   18] Training loss: 0.67598358, Validation loss: 0.67487651, Gradient norm: 0.32892194
INFO:root:[   19] Training loss: 0.67474982, Validation loss: 0.67469603, Gradient norm: 0.32395766
INFO:root:[   20] Training loss: 0.67429504, Validation loss: 0.67367975, Gradient norm: 0.33702548
INFO:root:[   21] Training loss: 0.67386670, Validation loss: 0.67309680, Gradient norm: 0.32301707
INFO:root:[   22] Training loss: 0.67299130, Validation loss: 0.67226917, Gradient norm: 0.30850323
INFO:root:[   23] Training loss: 0.67229613, Validation loss: 0.67268409, Gradient norm: 0.37211852
INFO:root:[   24] Training loss: 0.67145173, Validation loss: 0.67362066, Gradient norm: 0.29529224
INFO:root:[   25] Training loss: 0.67138845, Validation loss: 0.67624132, Gradient norm: 0.34236295
INFO:root:[   26] Training loss: 0.67069313, Validation loss: 0.67138203, Gradient norm: 0.32375487
INFO:root:[   27] Training loss: 0.67056453, Validation loss: 0.66964270, Gradient norm: 0.39494195
INFO:root:[   28] Training loss: 0.66983128, Validation loss: 0.67032958, Gradient norm: 0.34690820
INFO:root:[   29] Training loss: 0.66970199, Validation loss: 0.66840033, Gradient norm: 0.33654586
INFO:root:[   30] Training loss: 0.66963724, Validation loss: 0.66749440, Gradient norm: 0.34141503
INFO:root:[   31] Training loss: 0.66869793, Validation loss: 0.66882465, Gradient norm: 0.36182883
INFO:root:[   32] Training loss: 0.66873583, Validation loss: 0.66881916, Gradient norm: 0.40769050
INFO:root:[   33] Training loss: 0.66818469, Validation loss: 0.66997462, Gradient norm: 0.36007237
INFO:root:[   34] Training loss: 0.66743489, Validation loss: 0.66814467, Gradient norm: 0.31510929
INFO:root:[   35] Training loss: 0.66748784, Validation loss: 0.66650089, Gradient norm: 0.34392025
INFO:root:[   36] Training loss: 0.66804081, Validation loss: 0.66828705, Gradient norm: 0.40219893
INFO:root:[   37] Training loss: 0.66704081, Validation loss: 0.66850290, Gradient norm: 0.36980653
INFO:root:[   38] Training loss: 0.66706163, Validation loss: 0.66586681, Gradient norm: 0.33958897
INFO:root:[   39] Training loss: 0.66718023, Validation loss: 0.66873763, Gradient norm: 0.41544914
INFO:root:[   40] Training loss: 0.66621689, Validation loss: 0.66775044, Gradient norm: 0.37429228
INFO:root:[   41] Training loss: 0.66726599, Validation loss: 0.66751026, Gradient norm: 0.44585358
INFO:root:[   42] Training loss: 0.66566078, Validation loss: 0.66814573, Gradient norm: 0.31420470
INFO:root:[   43] Training loss: 0.66658337, Validation loss: 0.67142870, Gradient norm: 0.42400241
INFO:root:[   44] Training loss: 0.66610115, Validation loss: 0.66690997, Gradient norm: 0.41599486
INFO:root:[   45] Training loss: 0.66560481, Validation loss: 0.66737893, Gradient norm: 0.34806227
INFO:root:[   46] Training loss: 0.66607716, Validation loss: 0.66679900, Gradient norm: 0.44946706
INFO:root:[   47] Training loss: 0.66576926, Validation loss: 0.66516498, Gradient norm: 0.45312137
INFO:root:[   48] Training loss: 0.66581693, Validation loss: 0.66703609, Gradient norm: 0.44235355
INFO:root:[   49] Training loss: 0.66547487, Validation loss: 0.66637391, Gradient norm: 0.39651714
INFO:root:[   50] Training loss: 0.66597013, Validation loss: 0.66574001, Gradient norm: 0.44819705
INFO:root:[   51] Training loss: 0.66461867, Validation loss: 0.66552464, Gradient norm: 0.38512185
INFO:root:[   52] Training loss: 0.66495722, Validation loss: 0.66536058, Gradient norm: 0.43526557
INFO:root:[   53] Training loss: 0.66499062, Validation loss: 0.66469331, Gradient norm: 0.44514386
INFO:root:[   54] Training loss: 0.66473083, Validation loss: 0.66379025, Gradient norm: 0.43299283
INFO:root:[   55] Training loss: 0.66413008, Validation loss: 0.66429747, Gradient norm: 0.33705334
INFO:root:[   56] Training loss: 0.66497561, Validation loss: 0.66395191, Gradient norm: 0.49268492
INFO:root:[   57] Training loss: 0.66416279, Validation loss: 0.66819609, Gradient norm: 0.42812153
INFO:root:[   58] Training loss: 0.66456560, Validation loss: 0.66351841, Gradient norm: 0.45735273
INFO:root:[   59] Training loss: 0.66403212, Validation loss: 0.66366166, Gradient norm: 0.42055935
INFO:root:[   60] Training loss: 0.66405080, Validation loss: 0.66486307, Gradient norm: 0.49268782
INFO:root:[   61] Training loss: 0.66379067, Validation loss: 0.66407073, Gradient norm: 0.38137414
INFO:root:[   62] Training loss: 0.66385538, Validation loss: 0.66412220, Gradient norm: 0.43795817
INFO:root:[   63] Training loss: 0.66387786, Validation loss: 0.66446573, Gradient norm: 0.48794210
INFO:root:[   64] Training loss: 0.66379039, Validation loss: 0.66452132, Gradient norm: 0.48912372
INFO:root:[   65] Training loss: 0.66345711, Validation loss: 0.66249718, Gradient norm: 0.45920617
INFO:root:[   66] Training loss: 0.66257891, Validation loss: 0.66281406, Gradient norm: 0.40466974
INFO:root:[   67] Training loss: 0.66287502, Validation loss: 0.66508598, Gradient norm: 0.45534601
INFO:root:[   68] Training loss: 0.66275198, Validation loss: 0.66510804, Gradient norm: 0.44859296
INFO:root:[   69] Training loss: 0.66313541, Validation loss: 0.66740388, Gradient norm: 0.50531172
INFO:root:[   70] Training loss: 0.66279454, Validation loss: 0.66174000, Gradient norm: 0.47781386
INFO:root:[   71] Training loss: 0.66209729, Validation loss: 0.66154821, Gradient norm: 0.44628253
INFO:root:[   72] Training loss: 0.66234001, Validation loss: 0.66303917, Gradient norm: 0.47405234
INFO:root:[   73] Training loss: 0.66254547, Validation loss: 0.66256316, Gradient norm: 0.57384086
INFO:root:[   74] Training loss: 0.66192394, Validation loss: 0.66708353, Gradient norm: 0.54174677
INFO:root:[   75] Training loss: 0.66176512, Validation loss: 0.66227225, Gradient norm: 0.45035971
INFO:root:[   76] Training loss: 0.66113626, Validation loss: 0.66259755, Gradient norm: 0.39935688
INFO:root:[   77] Training loss: 0.66079040, Validation loss: 0.66128050, Gradient norm: 0.44834706
INFO:root:[   78] Training loss: 0.66226765, Validation loss: 0.66242966, Gradient norm: 0.57192624
INFO:root:[   79] Training loss: 0.66143158, Validation loss: 0.66217812, Gradient norm: 0.52164567
INFO:root:[   80] Training loss: 0.66134673, Validation loss: 0.66192710, Gradient norm: 0.54544315
INFO:root:[   81] Training loss: 0.66119899, Validation loss: 0.66116549, Gradient norm: 0.49200603
INFO:root:[   82] Training loss: 0.66113375, Validation loss: 0.66685936, Gradient norm: 0.50368427
INFO:root:[   83] Training loss: 0.66101099, Validation loss: 0.66348956, Gradient norm: 0.48156705
INFO:root:[   84] Training loss: 0.66110795, Validation loss: 0.66434338, Gradient norm: 0.54014019
INFO:root:[   85] Training loss: 0.66050770, Validation loss: 0.66230606, Gradient norm: 0.46940466
INFO:root:[   86] Training loss: 0.66096580, Validation loss: 0.66128061, Gradient norm: 0.50489600
INFO:root:[   87] Training loss: 0.66036320, Validation loss: 0.66126706, Gradient norm: 0.50600284
INFO:root:[   88] Training loss: 0.66078713, Validation loss: 0.66494501, Gradient norm: 0.46735166
INFO:root:[   89] Training loss: 0.66086255, Validation loss: 0.66661743, Gradient norm: 0.57282772
INFO:root:[   90] Training loss: 0.66016681, Validation loss: 0.66115741, Gradient norm: 0.51429825
INFO:root:[   91] Training loss: 0.66046657, Validation loss: 0.66441692, Gradient norm: 0.53099384
INFO:root:[   92] Training loss: 0.66015419, Validation loss: 0.66213353, Gradient norm: 0.48681310
INFO:root:[   93] Training loss: 0.66025150, Validation loss: 0.66070926, Gradient norm: 0.52622186
INFO:root:[   94] Training loss: 0.65973741, Validation loss: 0.65964429, Gradient norm: 0.48361984
INFO:root:[   95] Training loss: 0.66047593, Validation loss: 0.66173708, Gradient norm: 0.57060753
INFO:root:[   96] Training loss: 0.65971265, Validation loss: 0.66021551, Gradient norm: 0.47144091
INFO:root:[   97] Training loss: 0.66004067, Validation loss: 0.66028163, Gradient norm: 0.50335946
INFO:root:[   98] Training loss: 0.66024017, Validation loss: 0.66003431, Gradient norm: 0.58242884
INFO:root:[   99] Training loss: 0.65951340, Validation loss: 0.66295454, Gradient norm: 0.53661137
INFO:root:[  100] Training loss: 0.65987730, Validation loss: 0.65946789, Gradient norm: 0.48722225
INFO:root:[  101] Training loss: 0.65969581, Validation loss: 0.65998311, Gradient norm: 0.50196898
INFO:root:[  102] Training loss: 0.65989556, Validation loss: 0.66222699, Gradient norm: 0.57619786
INFO:root:[  103] Training loss: 0.65918671, Validation loss: 0.66156737, Gradient norm: 0.50494804
INFO:root:[  104] Training loss: 0.65866599, Validation loss: 0.66018175, Gradient norm: 0.43602890
INFO:root:[  105] Training loss: 0.65919210, Validation loss: 0.65937169, Gradient norm: 0.53586515
INFO:root:[  106] Training loss: 0.65998301, Validation loss: 0.66148089, Gradient norm: 0.61587017
INFO:root:[  107] Training loss: 0.65880891, Validation loss: 0.66180912, Gradient norm: 0.47150822
INFO:root:[  108] Training loss: 0.65804249, Validation loss: 0.65978759, Gradient norm: 0.45677395
INFO:root:[  109] Training loss: 0.65862282, Validation loss: 0.65911677, Gradient norm: 0.57470965
INFO:root:[  110] Training loss: 0.65866259, Validation loss: 0.65939950, Gradient norm: 0.47237477
INFO:root:[  111] Training loss: 0.65888607, Validation loss: 0.65872321, Gradient norm: 0.56975168
INFO:root:[  112] Training loss: 0.65849300, Validation loss: 0.65922838, Gradient norm: 0.51888797
INFO:root:[  113] Training loss: 0.65854741, Validation loss: 0.65950855, Gradient norm: 0.52720269
INFO:root:[  114] Training loss: 0.65817134, Validation loss: 0.65803672, Gradient norm: 0.44604789
INFO:root:[  115] Training loss: 0.65796023, Validation loss: 0.65934913, Gradient norm: 0.46684978
INFO:root:[  116] Training loss: 0.65812942, Validation loss: 0.65946373, Gradient norm: 0.53347720
INFO:root:[  117] Training loss: 0.65794029, Validation loss: 0.65928639, Gradient norm: 0.41397113
INFO:root:[  118] Training loss: 0.65842944, Validation loss: 0.65876558, Gradient norm: 0.60875126
INFO:root:[  119] Training loss: 0.65761737, Validation loss: 0.65822251, Gradient norm: 0.49769744
INFO:root:[  120] Training loss: 0.65766017, Validation loss: 0.66243562, Gradient norm: 0.51597540
INFO:root:[  121] Training loss: 0.65795272, Validation loss: 0.65825672, Gradient norm: 0.51400686
INFO:root:[  122] Training loss: 0.65795721, Validation loss: 0.65779638, Gradient norm: 0.54897300
INFO:root:[  123] Training loss: 0.65730000, Validation loss: 0.65841927, Gradient norm: 0.45845874
INFO:root:[  124] Training loss: 0.65717358, Validation loss: 0.65733607, Gradient norm: 0.47125422
INFO:root:[  125] Training loss: 0.65752543, Validation loss: 0.66052440, Gradient norm: 0.47789095
INFO:root:[  126] Training loss: 0.65808760, Validation loss: 0.65946756, Gradient norm: 0.53398021
INFO:root:[  127] Training loss: 0.65723934, Validation loss: 0.65787669, Gradient norm: 0.53703956
INFO:root:[  128] Training loss: 0.65741953, Validation loss: 0.65762112, Gradient norm: 0.47127803
INFO:root:[  129] Training loss: 0.65731961, Validation loss: 0.65807044, Gradient norm: 0.49789069
INFO:root:[  130] Training loss: 0.65767117, Validation loss: 0.65976017, Gradient norm: 0.57828536
INFO:root:[  131] Training loss: 0.65697693, Validation loss: 0.65906609, Gradient norm: 0.49397239
INFO:root:[  132] Training loss: 0.65726533, Validation loss: 0.65898756, Gradient norm: 0.50569499
INFO:root:[  133] Training loss: 0.65663773, Validation loss: 0.66011021, Gradient norm: 0.43663024
INFO:root:EP 133: Early stopping
INFO:root:Training the model took 8350.558s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
