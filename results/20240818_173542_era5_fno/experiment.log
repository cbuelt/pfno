INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26371849, Validation loss: 0.21067172, Gradient norm: 3.08260918
INFO:root:[    2] Training loss: 0.20815665, Validation loss: 0.20368348, Gradient norm: 1.85341512
INFO:root:[    3] Training loss: 0.20080185, Validation loss: 0.19575151, Gradient norm: 1.74784008
INFO:root:[    4] Training loss: 0.19403283, Validation loss: 0.18979164, Gradient norm: 1.44992051
INFO:root:[    5] Training loss: 0.18761873, Validation loss: 0.19193632, Gradient norm: 1.27355670
INFO:root:[    6] Training loss: 0.18269191, Validation loss: 0.18116056, Gradient norm: 1.26310052
INFO:root:[    7] Training loss: 0.17544965, Validation loss: 0.17760326, Gradient norm: 1.14651086
INFO:root:[    8] Training loss: 0.16964765, Validation loss: 0.17298291, Gradient norm: 1.10836344
INFO:root:[    9] Training loss: 0.16476343, Validation loss: 0.16815178, Gradient norm: 1.14335210
INFO:root:[   10] Training loss: 0.15965946, Validation loss: 0.16941814, Gradient norm: 1.06507092
INFO:root:[   11] Training loss: 0.15637628, Validation loss: 0.16742281, Gradient norm: 1.11852553
INFO:root:[   12] Training loss: 0.15277893, Validation loss: 0.16132492, Gradient norm: 1.04038519
INFO:root:[   13] Training loss: 0.14992616, Validation loss: 0.16032214, Gradient norm: 1.04023711
INFO:root:[   14] Training loss: 0.14753559, Validation loss: 0.16660831, Gradient norm: 0.97307932
INFO:root:[   15] Training loss: 0.14500343, Validation loss: 0.15995577, Gradient norm: 0.99199463
INFO:root:[   16] Training loss: 0.14360272, Validation loss: 0.15392235, Gradient norm: 0.98260522
INFO:root:[   17] Training loss: 0.14213713, Validation loss: 0.15197181, Gradient norm: 1.04437941
INFO:root:[   18] Training loss: 0.14007733, Validation loss: 0.15183945, Gradient norm: 0.98412297
INFO:root:[   19] Training loss: 0.13879456, Validation loss: 0.15112036, Gradient norm: 0.98113157
INFO:root:[   20] Training loss: 0.13779048, Validation loss: 0.14915793, Gradient norm: 0.98997399
INFO:root:[   21] Training loss: 0.13625099, Validation loss: 0.14999739, Gradient norm: 0.97459965
INFO:root:[   22] Training loss: 0.13508604, Validation loss: 0.15310490, Gradient norm: 0.96213079
INFO:root:[   23] Training loss: 0.13430388, Validation loss: 0.14705851, Gradient norm: 1.00906157
INFO:root:[   24] Training loss: 0.13325335, Validation loss: 0.14582333, Gradient norm: 0.97904875
INFO:root:[   25] Training loss: 0.13217609, Validation loss: 0.14618969, Gradient norm: 0.94974642
INFO:root:[   26] Training loss: 0.13163963, Validation loss: 0.14312096, Gradient norm: 0.96517791
INFO:root:[   27] Training loss: 0.13029594, Validation loss: 0.14213594, Gradient norm: 0.96086912
INFO:root:[   28] Training loss: 0.13016269, Validation loss: 0.14334255, Gradient norm: 0.94724959
INFO:root:[   29] Training loss: 0.12951052, Validation loss: 0.14522077, Gradient norm: 0.97423605
INFO:root:[   30] Training loss: 0.12835480, Validation loss: 0.14188354, Gradient norm: 0.88101572
INFO:root:[   31] Training loss: 0.12781284, Validation loss: 0.14294814, Gradient norm: 0.92429930
INFO:root:[   32] Training loss: 0.12734642, Validation loss: 0.14260828, Gradient norm: 0.91051007
INFO:root:[   33] Training loss: 0.12675039, Validation loss: 0.14412364, Gradient norm: 0.90209766
INFO:root:[   34] Training loss: 0.12602593, Validation loss: 0.13813998, Gradient norm: 0.88854263
INFO:root:[   35] Training loss: 0.12584561, Validation loss: 0.13880881, Gradient norm: 0.98201776
INFO:root:[   36] Training loss: 0.12520594, Validation loss: 0.13793478, Gradient norm: 0.89969220
INFO:root:[   37] Training loss: 0.12491712, Validation loss: 0.13878400, Gradient norm: 0.92108057
INFO:root:[   38] Training loss: 0.12429551, Validation loss: 0.13774893, Gradient norm: 0.90673894
INFO:root:[   39] Training loss: 0.12383055, Validation loss: 0.13627317, Gradient norm: 0.89935637
INFO:root:[   40] Training loss: 0.12391103, Validation loss: 0.13793320, Gradient norm: 0.94150156
INFO:root:[   41] Training loss: 0.12335664, Validation loss: 0.13494636, Gradient norm: 0.90099161
INFO:root:[   42] Training loss: 0.12279154, Validation loss: 0.13715737, Gradient norm: 0.88369224
INFO:root:[   43] Training loss: 0.12251971, Validation loss: 0.13489863, Gradient norm: 0.90908815
INFO:root:[   44] Training loss: 0.12240272, Validation loss: 0.13401589, Gradient norm: 0.89381740
INFO:root:[   45] Training loss: 0.12160689, Validation loss: 0.13646287, Gradient norm: 0.86968750
INFO:root:[   46] Training loss: 0.12133960, Validation loss: 0.13615102, Gradient norm: 0.87469089
INFO:root:[   47] Training loss: 0.12098214, Validation loss: 0.13394977, Gradient norm: 0.89597957
INFO:root:[   48] Training loss: 0.12079666, Validation loss: 0.13725201, Gradient norm: 0.87795471
INFO:root:[   49] Training loss: 0.12054626, Validation loss: 0.13451933, Gradient norm: 0.84949498
INFO:root:[   50] Training loss: 0.12011745, Validation loss: 0.13509494, Gradient norm: 0.87318652
INFO:root:[   51] Training loss: 0.11966839, Validation loss: 0.13831070, Gradient norm: 0.86216807
INFO:root:[   52] Training loss: 0.11989918, Validation loss: 0.13716124, Gradient norm: 0.89079232
INFO:root:[   53] Training loss: 0.11928438, Validation loss: 0.13631009, Gradient norm: 0.86191114
INFO:root:[   54] Training loss: 0.11902756, Validation loss: 0.13460976, Gradient norm: 0.84485149
INFO:root:[   55] Training loss: 0.11888887, Validation loss: 0.13269337, Gradient norm: 0.88435822
INFO:root:[   56] Training loss: 0.11854819, Validation loss: 0.13154915, Gradient norm: 0.85848178
INFO:root:[   57] Training loss: 0.11884144, Validation loss: 0.13228347, Gradient norm: 0.92571799
INFO:root:[   58] Training loss: 0.11789052, Validation loss: 0.13178304, Gradient norm: 0.80250919
INFO:root:[   59] Training loss: 0.11790161, Validation loss: 0.13224677, Gradient norm: 0.87494739
INFO:root:[   60] Training loss: 0.11797511, Validation loss: 0.13157370, Gradient norm: 0.89535070
INFO:root:[   61] Training loss: 0.11763349, Validation loss: 0.13340106, Gradient norm: 0.86646098
INFO:root:[   62] Training loss: 0.11731159, Validation loss: 0.13173955, Gradient norm: 0.82279989
INFO:root:[   63] Training loss: 0.11710997, Validation loss: 0.13072003, Gradient norm: 0.87006972
INFO:root:[   64] Training loss: 0.11686552, Validation loss: 0.13216620, Gradient norm: 0.85242310
INFO:root:[   65] Training loss: 0.11683961, Validation loss: 0.13230484, Gradient norm: 0.86053885
INFO:root:[   66] Training loss: 0.11650315, Validation loss: 0.12910315, Gradient norm: 0.82983647
INFO:root:[   67] Training loss: 0.11649060, Validation loss: 0.13399378, Gradient norm: 0.85573610
INFO:root:[   68] Training loss: 0.11642561, Validation loss: 0.12916703, Gradient norm: 0.86096643
INFO:root:[   69] Training loss: 0.11609977, Validation loss: 0.12967645, Gradient norm: 0.85143633
INFO:root:[   70] Training loss: 0.11565853, Validation loss: 0.13053835, Gradient norm: 0.81617838
INFO:root:[   71] Training loss: 0.11571488, Validation loss: 0.12949331, Gradient norm: 0.83404721
INFO:root:[   72] Training loss: 0.11547075, Validation loss: 0.13109866, Gradient norm: 0.85109624
INFO:root:[   73] Training loss: 0.11513521, Validation loss: 0.13008349, Gradient norm: 0.80180895
INFO:root:[   74] Training loss: 0.11518184, Validation loss: 0.12962923, Gradient norm: 0.84715761
INFO:root:[   75] Training loss: 0.11531818, Validation loss: 0.13279031, Gradient norm: 0.87327672
INFO:root:[   76] Training loss: 0.11474668, Validation loss: 0.12866702, Gradient norm: 0.81556993
INFO:root:[   77] Training loss: 0.11484940, Validation loss: 0.13005158, Gradient norm: 0.82182872
INFO:root:[   78] Training loss: 0.11464679, Validation loss: 0.12996859, Gradient norm: 0.87155910
INFO:root:[   79] Training loss: 0.11419029, Validation loss: 0.12851462, Gradient norm: 0.78828809
INFO:root:[   80] Training loss: 0.11461437, Validation loss: 0.12895484, Gradient norm: 0.85632050
INFO:root:[   81] Training loss: 0.11445270, Validation loss: 0.12807961, Gradient norm: 0.87610791
INFO:root:[   82] Training loss: 0.11441946, Validation loss: 0.12697923, Gradient norm: 0.88434897
INFO:root:[   83] Training loss: 0.11397940, Validation loss: 0.13184897, Gradient norm: 0.83085008
INFO:root:[   84] Training loss: 0.11349463, Validation loss: 0.12841940, Gradient norm: 0.78820904
INFO:root:[   85] Training loss: 0.11380722, Validation loss: 0.12714890, Gradient norm: 0.84535390
INFO:root:[   86] Training loss: 0.11331837, Validation loss: 0.12669895, Gradient norm: 0.81408783
INFO:root:[   87] Training loss: 0.11313624, Validation loss: 0.12680568, Gradient norm: 0.80871243
INFO:root:[   88] Training loss: 0.11346137, Validation loss: 0.12774584, Gradient norm: 0.85286233
INFO:root:[   89] Training loss: 0.11283894, Validation loss: 0.12622507, Gradient norm: 0.81869626
INFO:root:[   90] Training loss: 0.11304148, Validation loss: 0.12623241, Gradient norm: 0.84886201
INFO:root:[   91] Training loss: 0.11286168, Validation loss: 0.12982419, Gradient norm: 0.82428615
INFO:root:[   92] Training loss: 0.11271089, Validation loss: 0.13086049, Gradient norm: 0.82353769
INFO:root:[   93] Training loss: 0.11244623, Validation loss: 0.13111597, Gradient norm: 0.82375505
INFO:root:[   94] Training loss: 0.11272644, Validation loss: 0.12828920, Gradient norm: 0.83183394
INFO:root:[   95] Training loss: 0.11248673, Validation loss: 0.12845286, Gradient norm: 0.85813355
INFO:root:[   96] Training loss: 0.11222902, Validation loss: 0.12512793, Gradient norm: 0.83102708
INFO:root:[   97] Training loss: 0.11229283, Validation loss: 0.12744474, Gradient norm: 0.81983060
INFO:root:[   98] Training loss: 0.11204735, Validation loss: 0.12891545, Gradient norm: 0.82748665
INFO:root:[   99] Training loss: 0.11209830, Validation loss: 0.12801138, Gradient norm: 0.87029853
INFO:root:[  100] Training loss: 0.11162944, Validation loss: 0.12494485, Gradient norm: 0.81184308
INFO:root:[  101] Training loss: 0.11150787, Validation loss: 0.12798633, Gradient norm: 0.80252022
INFO:root:[  102] Training loss: 0.11182765, Validation loss: 0.12592512, Gradient norm: 0.83311993
INFO:root:[  103] Training loss: 0.11164184, Validation loss: 0.12813552, Gradient norm: 0.85273875
INFO:root:[  104] Training loss: 0.11162622, Validation loss: 0.12487506, Gradient norm: 0.82325007
INFO:root:[  105] Training loss: 0.11116194, Validation loss: 0.12428973, Gradient norm: 0.80085204
INFO:root:[  106] Training loss: 0.11107853, Validation loss: 0.12521399, Gradient norm: 0.80109698
INFO:root:[  107] Training loss: 0.11126593, Validation loss: 0.12300745, Gradient norm: 0.85065877
INFO:root:[  108] Training loss: 0.11115390, Validation loss: 0.12504962, Gradient norm: 0.82759538
INFO:root:[  109] Training loss: 0.11091664, Validation loss: 0.12454830, Gradient norm: 0.84223498
INFO:root:[  110] Training loss: 0.11129941, Validation loss: 0.12746717, Gradient norm: 0.89440972
INFO:root:[  111] Training loss: 0.11067114, Validation loss: 0.12330398, Gradient norm: 0.78454840
INFO:root:[  112] Training loss: 0.11087951, Validation loss: 0.12296003, Gradient norm: 0.85644393
INFO:root:[  113] Training loss: 0.11039735, Validation loss: 0.12370345, Gradient norm: 0.79194645
INFO:root:[  114] Training loss: 0.11048933, Validation loss: 0.12404805, Gradient norm: 0.82332850
INFO:root:[  115] Training loss: 0.11050992, Validation loss: 0.12510944, Gradient norm: 0.86200128
INFO:root:[  116] Training loss: 0.11030120, Validation loss: 0.13097972, Gradient norm: 0.80740128
INFO:root:[  117] Training loss: 0.11022439, Validation loss: 0.12649310, Gradient norm: 0.80826635
INFO:root:[  118] Training loss: 0.11004288, Validation loss: 0.12376697, Gradient norm: 0.81075158
INFO:root:[  119] Training loss: 0.11033600, Validation loss: 0.12268312, Gradient norm: 0.83918200
INFO:root:[  120] Training loss: 0.10992361, Validation loss: 0.12360782, Gradient norm: 0.81167851
INFO:root:[  121] Training loss: 0.10983907, Validation loss: 0.12520911, Gradient norm: 0.80438967
INFO:root:[  122] Training loss: 0.11000644, Validation loss: 0.12338768, Gradient norm: 0.86291748
INFO:root:[  123] Training loss: 0.10963462, Validation loss: 0.12207441, Gradient norm: 0.82032640
INFO:root:[  124] Training loss: 0.10990658, Validation loss: 0.12222327, Gradient norm: 0.84236031
INFO:root:[  125] Training loss: 0.10965138, Validation loss: 0.12776275, Gradient norm: 0.83155037
INFO:root:[  126] Training loss: 0.10951995, Validation loss: 0.12184872, Gradient norm: 0.83500507
INFO:root:[  127] Training loss: 0.10917643, Validation loss: 0.12146779, Gradient norm: 0.78141454
INFO:root:[  128] Training loss: 0.10906334, Validation loss: 0.12306414, Gradient norm: 0.78214298
INFO:root:[  129] Training loss: 0.10949578, Validation loss: 0.12185233, Gradient norm: 0.84724866
INFO:root:[  130] Training loss: 0.10880619, Validation loss: 0.12195093, Gradient norm: 0.78227310
INFO:root:[  131] Training loss: 0.10920027, Validation loss: 0.12140406, Gradient norm: 0.83745360
INFO:root:[  132] Training loss: 0.10920136, Validation loss: 0.12225851, Gradient norm: 0.79497152
INFO:root:[  133] Training loss: 0.10884696, Validation loss: 0.12080082, Gradient norm: 0.81333627
INFO:root:[  134] Training loss: 0.10906701, Validation loss: 0.12172584, Gradient norm: 0.84886483
INFO:root:[  135] Training loss: 0.10921208, Validation loss: 0.12493439, Gradient norm: 0.84429243
INFO:root:[  136] Training loss: 0.10874795, Validation loss: 0.12303274, Gradient norm: 0.81580820
INFO:root:[  137] Training loss: 0.10890275, Validation loss: 0.12638833, Gradient norm: 0.85500123
INFO:root:[  138] Training loss: 0.10852266, Validation loss: 0.12382221, Gradient norm: 0.76928833
INFO:root:[  139] Training loss: 0.10874732, Validation loss: 0.12081517, Gradient norm: 0.83744411
INFO:root:[  140] Training loss: 0.10810456, Validation loss: 0.12060320, Gradient norm: 0.73366214
INFO:root:[  141] Training loss: 0.10872174, Validation loss: 0.12135680, Gradient norm: 0.84559118
INFO:root:[  142] Training loss: 0.10798057, Validation loss: 0.12020632, Gradient norm: 0.74865480
INFO:root:[  143] Training loss: 0.10834530, Validation loss: 0.12213817, Gradient norm: 0.82228133
INFO:root:[  144] Training loss: 0.10840877, Validation loss: 0.12223449, Gradient norm: 0.81306209
INFO:root:[  145] Training loss: 0.10799199, Validation loss: 0.12348596, Gradient norm: 0.80849672
INFO:root:[  146] Training loss: 0.10789064, Validation loss: 0.12219440, Gradient norm: 0.78542231
INFO:root:[  147] Training loss: 0.10830851, Validation loss: 0.12122139, Gradient norm: 0.82980668
INFO:root:[  148] Training loss: 0.10792584, Validation loss: 0.12175842, Gradient norm: 0.81883967
INFO:root:[  149] Training loss: 0.10788572, Validation loss: 0.12191313, Gradient norm: 0.82826729
INFO:root:[  150] Training loss: 0.10797449, Validation loss: 0.12059682, Gradient norm: 0.80934320
INFO:root:[  151] Training loss: 0.10767857, Validation loss: 0.12126343, Gradient norm: 0.80665405
INFO:root:[  152] Training loss: 0.10773828, Validation loss: 0.12006796, Gradient norm: 0.79927757
INFO:root:[  153] Training loss: 0.10768499, Validation loss: 0.12006290, Gradient norm: 0.81578685
INFO:root:[  154] Training loss: 0.10733612, Validation loss: 0.12156091, Gradient norm: 0.77986498
INFO:root:[  155] Training loss: 0.10740909, Validation loss: 0.12296432, Gradient norm: 0.78839428
INFO:root:[  156] Training loss: 0.10741075, Validation loss: 0.11950988, Gradient norm: 0.79533597
INFO:root:[  157] Training loss: 0.10756426, Validation loss: 0.12369483, Gradient norm: 0.83406451
INFO:root:[  158] Training loss: 0.10727348, Validation loss: 0.12271264, Gradient norm: 0.79875664
INFO:root:[  159] Training loss: 0.10719846, Validation loss: 0.11981231, Gradient norm: 0.75579568
INFO:root:[  160] Training loss: 0.10718713, Validation loss: 0.12118815, Gradient norm: 0.78535047
INFO:root:[  161] Training loss: 0.10748244, Validation loss: 0.12068313, Gradient norm: 0.84924262
INFO:root:[  162] Training loss: 0.10709637, Validation loss: 0.12079806, Gradient norm: 0.79465824
INFO:root:[  163] Training loss: 0.10702903, Validation loss: 0.11928094, Gradient norm: 0.78513518
INFO:root:[  164] Training loss: 0.10692432, Validation loss: 0.12051604, Gradient norm: 0.80121146
INFO:root:[  165] Training loss: 0.10681266, Validation loss: 0.11911986, Gradient norm: 0.78918983
INFO:root:[  166] Training loss: 0.10686550, Validation loss: 0.12450356, Gradient norm: 0.81238360
INFO:root:[  167] Training loss: 0.10710763, Validation loss: 0.12590710, Gradient norm: 0.86746579
INFO:root:[  168] Training loss: 0.10673492, Validation loss: 0.11862727, Gradient norm: 0.78474653
INFO:root:[  169] Training loss: 0.10678559, Validation loss: 0.11903012, Gradient norm: 0.80580565
INFO:root:[  170] Training loss: 0.10693270, Validation loss: 0.12153736, Gradient norm: 0.81475165
INFO:root:[  171] Training loss: 0.10639442, Validation loss: 0.11844294, Gradient norm: 0.75938368
INFO:root:[  172] Training loss: 0.10657367, Validation loss: 0.12283235, Gradient norm: 0.82642775
INFO:root:[  173] Training loss: 0.10669661, Validation loss: 0.12171875, Gradient norm: 0.84427150
INFO:root:[  174] Training loss: 0.10642098, Validation loss: 0.11900368, Gradient norm: 0.79535213
INFO:root:[  175] Training loss: 0.10648787, Validation loss: 0.11896024, Gradient norm: 0.83104891
INFO:root:[  176] Training loss: 0.10625355, Validation loss: 0.11971172, Gradient norm: 0.75644906
INFO:root:[  177] Training loss: 0.10627144, Validation loss: 0.12019916, Gradient norm: 0.79771388
INFO:root:[  178] Training loss: 0.10646519, Validation loss: 0.12343982, Gradient norm: 0.82727446
INFO:root:[  179] Training loss: 0.10595949, Validation loss: 0.11832927, Gradient norm: 0.77467572
INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:Training the model took 20.966s.
INFO:root:Emptying the cuda cache took 0.0s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Validation data.
INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:Training the model took 1.816s.
INFO:root:Emptying the cuda cache took 0.0s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Validation data.
INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:Training the model took 1.626s.
INFO:root:Emptying the cuda cache took 0.0s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Validation data.
INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 6, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:Training the model took 1.563s.
INFO:root:Emptying the cuda cache took 0.0s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.16771
INFO:root:EnergyScoreValidation: 0.11818
INFO:root:CoverageValidation: 0.94268
INFO:root:IntervalWidthValidation: 0.08336
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.32913
INFO:root:EnergyScoreTest: 0.24274
INFO:root:CoverageTest: 0.78651
INFO:root:IntervalWidthTest: 0.07148
