INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06461407, Validation loss: 0.03311342, Gradient norm: 0.50058815
INFO:root:[    2] Training loss: 0.02646606, Validation loss: 0.02371500, Gradient norm: 0.60017216
INFO:root:[    3] Training loss: 0.01985005, Validation loss: 0.01746377, Gradient norm: 0.64951536
INFO:root:[    4] Training loss: 0.01726132, Validation loss: 0.01669890, Gradient norm: 0.52877445
INFO:root:[    5] Training loss: 0.01593087, Validation loss: 0.01533012, Gradient norm: 0.49295959
INFO:root:[    6] Training loss: 0.01564004, Validation loss: 0.01548718, Gradient norm: 0.64431361
INFO:root:[    7] Training loss: 0.01377053, Validation loss: 0.01337293, Gradient norm: 0.42095874
INFO:root:[    8] Training loss: 0.01309075, Validation loss: 0.01247987, Gradient norm: 0.41089204
INFO:root:[    9] Training loss: 0.01264385, Validation loss: 0.01278340, Gradient norm: 0.44365474
INFO:root:[   10] Training loss: 0.01241105, Validation loss: 0.01268950, Gradient norm: 0.52861400
INFO:root:[   11] Training loss: 0.01191544, Validation loss: 0.01299627, Gradient norm: 0.47661444
INFO:root:[   12] Training loss: 0.01162378, Validation loss: 0.01057784, Gradient norm: 0.50379787
INFO:root:[   13] Training loss: 0.01086338, Validation loss: 0.01245860, Gradient norm: 0.43020679
INFO:root:[   14] Training loss: 0.01036177, Validation loss: 0.01010730, Gradient norm: 0.34484048
INFO:root:[   15] Training loss: 0.01023841, Validation loss: 0.01010952, Gradient norm: 0.38033524
INFO:root:[   16] Training loss: 0.01042067, Validation loss: 0.00979765, Gradient norm: 0.46138214
INFO:root:[   17] Training loss: 0.00981056, Validation loss: 0.01214664, Gradient norm: 0.38372423
INFO:root:[   18] Training loss: 0.01031593, Validation loss: 0.01161963, Gradient norm: 0.48820401
INFO:root:[   19] Training loss: 0.00936204, Validation loss: 0.01137234, Gradient norm: 0.31639000
INFO:root:[   20] Training loss: 0.00924528, Validation loss: 0.01094208, Gradient norm: 0.35054322
INFO:root:[   21] Training loss: 0.00975615, Validation loss: 0.01185308, Gradient norm: 0.48800877
INFO:root:[   22] Training loss: 0.00947245, Validation loss: 0.00946810, Gradient norm: 0.44329914
INFO:root:[   23] Training loss: 0.00926980, Validation loss: 0.01285041, Gradient norm: 0.41167445
INFO:root:[   24] Training loss: 0.00891696, Validation loss: 0.00966394, Gradient norm: 0.35526912
INFO:root:[   25] Training loss: 0.00896280, Validation loss: 0.00910499, Gradient norm: 0.37492479
INFO:root:[   26] Training loss: 0.00858247, Validation loss: 0.00926514, Gradient norm: 0.33918977
INFO:root:[   27] Training loss: 0.00932216, Validation loss: 0.00861967, Gradient norm: 0.46725511
INFO:root:[   28] Training loss: 0.00873937, Validation loss: 0.00898487, Gradient norm: 0.40486395
INFO:root:[   29] Training loss: 0.00830633, Validation loss: 0.00853809, Gradient norm: 0.32594982
INFO:root:[   30] Training loss: 0.00878758, Validation loss: 0.00906412, Gradient norm: 0.42877511
INFO:root:[   31] Training loss: 0.00864311, Validation loss: 0.00894631, Gradient norm: 0.37733718
INFO:root:[   32] Training loss: 0.00842518, Validation loss: 0.00856088, Gradient norm: 0.39074440
INFO:root:[   33] Training loss: 0.00842314, Validation loss: 0.00848890, Gradient norm: 0.39313756
INFO:root:[   34] Training loss: 0.00820667, Validation loss: 0.00831740, Gradient norm: 0.37066922
INFO:root:[   35] Training loss: 0.00825449, Validation loss: 0.00826521, Gradient norm: 0.37366783
INFO:root:[   36] Training loss: 0.00821279, Validation loss: 0.00847175, Gradient norm: 0.38188114
INFO:root:[   37] Training loss: 0.00777235, Validation loss: 0.00840037, Gradient norm: 0.31051780
INFO:root:[   38] Training loss: 0.00780656, Validation loss: 0.00828875, Gradient norm: 0.30729560
INFO:root:[   39] Training loss: 0.00841517, Validation loss: 0.00848164, Gradient norm: 0.43714295
INFO:root:[   40] Training loss: 0.00774851, Validation loss: 0.00884663, Gradient norm: 0.31496996
INFO:root:[   41] Training loss: 0.00770702, Validation loss: 0.00861352, Gradient norm: 0.33614361
INFO:root:[   42] Training loss: 0.00774277, Validation loss: 0.00901080, Gradient norm: 0.36975454
INFO:root:[   43] Training loss: 0.00771518, Validation loss: 0.00821874, Gradient norm: 0.34844363
INFO:root:[   44] Training loss: 0.00760861, Validation loss: 0.00908092, Gradient norm: 0.38508639
INFO:root:[   45] Training loss: 0.00753359, Validation loss: 0.00884337, Gradient norm: 0.36997711
INFO:root:[   46] Training loss: 0.00772261, Validation loss: 0.00849348, Gradient norm: 0.38900120
INFO:root:[   47] Training loss: 0.00731091, Validation loss: 0.00809606, Gradient norm: 0.31568274
INFO:root:[   48] Training loss: 0.00764743, Validation loss: 0.00786660, Gradient norm: 0.36180855
INFO:root:[   49] Training loss: 0.00730946, Validation loss: 0.00832859, Gradient norm: 0.32941336
INFO:root:[   50] Training loss: 0.00741838, Validation loss: 0.00815720, Gradient norm: 0.36999488
INFO:root:[   51] Training loss: 0.00733775, Validation loss: 0.00921858, Gradient norm: 0.34872220
INFO:root:[   52] Training loss: 0.00728612, Validation loss: 0.00814935, Gradient norm: 0.34702791
INFO:root:[   53] Training loss: 0.00718886, Validation loss: 0.00791827, Gradient norm: 0.32057810
INFO:root:[   54] Training loss: 0.00695365, Validation loss: 0.00813664, Gradient norm: 0.27557777
INFO:root:[   55] Training loss: 0.00699950, Validation loss: 0.00850602, Gradient norm: 0.31139226
INFO:root:[   56] Training loss: 0.00725812, Validation loss: 0.00818578, Gradient norm: 0.33674115
INFO:root:[   57] Training loss: 0.00700659, Validation loss: 0.00800448, Gradient norm: 0.33374247
INFO:root:[   58] Training loss: 0.00696592, Validation loss: 0.00890992, Gradient norm: 0.34852635
INFO:root:[   59] Training loss: 0.00698837, Validation loss: 0.00860209, Gradient norm: 0.34239575
INFO:root:[   60] Training loss: 0.00678168, Validation loss: 0.00800007, Gradient norm: 0.30514065
INFO:root:[   61] Training loss: 0.00706097, Validation loss: 0.00800043, Gradient norm: 0.36094551
INFO:root:[   62] Training loss: 0.00667519, Validation loss: 0.00837537, Gradient norm: 0.31040951
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 841.738s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.28255
INFO:root:EnergyScoretrain: 0.21104
INFO:root:Coveragetrain: 31.01218
INFO:root:IntervalWidthtrain: 1.33821
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.33462
INFO:root:EnergyScorevalidation: 0.25154
INFO:root:Coveragevalidation: 30.01992
INFO:root:IntervalWidthvalidation: 1.33115
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 1.20193
INFO:root:EnergyScoretest: 0.99963
INFO:root:Coveragetest: 13.09463
INFO:root:IntervalWidthtest: 1.42621
INFO:root:###2 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05261130, Validation loss: 0.02952272, Gradient norm: 0.41619176
INFO:root:[    2] Training loss: 0.02414883, Validation loss: 0.02110947, Gradient norm: 0.36326997
INFO:root:[    3] Training loss: 0.01877945, Validation loss: 0.01802095, Gradient norm: 0.29313507
INFO:root:[    4] Training loss: 0.01700454, Validation loss: 0.01627583, Gradient norm: 0.35864838
INFO:root:[    5] Training loss: 0.01512280, Validation loss: 0.01464205, Gradient norm: 0.24550674
INFO:root:[    6] Training loss: 0.01447148, Validation loss: 0.01425572, Gradient norm: 0.34092397
INFO:root:[    7] Training loss: 0.01314060, Validation loss: 0.01523089, Gradient norm: 0.24391697
INFO:root:[    8] Training loss: 0.01300577, Validation loss: 0.01232476, Gradient norm: 0.32800710
INFO:root:[    9] Training loss: 0.01236237, Validation loss: 0.01248488, Gradient norm: 0.29156091
INFO:root:[   10] Training loss: 0.01194006, Validation loss: 0.01143987, Gradient norm: 0.30590848
INFO:root:[   11] Training loss: 0.01135873, Validation loss: 0.01197214, Gradient norm: 0.25754087
INFO:root:[   12] Training loss: 0.01108789, Validation loss: 0.01130604, Gradient norm: 0.25933399
INFO:root:[   13] Training loss: 0.01108249, Validation loss: 0.01066615, Gradient norm: 0.29998540
INFO:root:[   14] Training loss: 0.01043489, Validation loss: 0.01042521, Gradient norm: 0.26219725
INFO:root:[   15] Training loss: 0.01092153, Validation loss: 0.01013869, Gradient norm: 0.36507814
INFO:root:[   16] Training loss: 0.01011761, Validation loss: 0.01105000, Gradient norm: 0.26467450
INFO:root:[   17] Training loss: 0.01013260, Validation loss: 0.01030326, Gradient norm: 0.27765196
INFO:root:[   18] Training loss: 0.00987426, Validation loss: 0.00993432, Gradient norm: 0.27257620
INFO:root:[   19] Training loss: 0.00965016, Validation loss: 0.01019540, Gradient norm: 0.25165072
INFO:root:[   20] Training loss: 0.00967378, Validation loss: 0.01097235, Gradient norm: 0.29754832
INFO:root:[   21] Training loss: 0.00943581, Validation loss: 0.00939637, Gradient norm: 0.26437000
INFO:root:[   22] Training loss: 0.00945027, Validation loss: 0.00977822, Gradient norm: 0.28954422
INFO:root:[   23] Training loss: 0.00922044, Validation loss: 0.00921260, Gradient norm: 0.25686201
INFO:root:[   24] Training loss: 0.00911100, Validation loss: 0.00996402, Gradient norm: 0.25458541
INFO:root:[   25] Training loss: 0.00900498, Validation loss: 0.00907776, Gradient norm: 0.27854239
INFO:root:[   26] Training loss: 0.00914936, Validation loss: 0.00941056, Gradient norm: 0.31323854
INFO:root:[   27] Training loss: 0.00860884, Validation loss: 0.00910727, Gradient norm: 0.22627816
INFO:root:[   28] Training loss: 0.00880892, Validation loss: 0.01010451, Gradient norm: 0.28176466
INFO:root:[   29] Training loss: 0.00867194, Validation loss: 0.00908267, Gradient norm: 0.26262862
INFO:root:[   30] Training loss: 0.00865372, Validation loss: 0.00868424, Gradient norm: 0.29103147
INFO:root:[   31] Training loss: 0.00852633, Validation loss: 0.00858208, Gradient norm: 0.24949729
INFO:root:[   32] Training loss: 0.00841391, Validation loss: 0.00840019, Gradient norm: 0.26345339
INFO:root:[   33] Training loss: 0.00855606, Validation loss: 0.00854483, Gradient norm: 0.28132172
INFO:root:[   34] Training loss: 0.00849764, Validation loss: 0.00916249, Gradient norm: 0.28257702
INFO:root:[   35] Training loss: 0.00807175, Validation loss: 0.00860504, Gradient norm: 0.23072535
INFO:root:[   36] Training loss: 0.00802642, Validation loss: 0.00861405, Gradient norm: 0.25242628
INFO:root:[   37] Training loss: 0.00807766, Validation loss: 0.00825455, Gradient norm: 0.24146180
INFO:root:[   38] Training loss: 0.00822152, Validation loss: 0.00879743, Gradient norm: 0.29365452
INFO:root:[   39] Training loss: 0.00774609, Validation loss: 0.00854482, Gradient norm: 0.20189265
INFO:root:[   40] Training loss: 0.00795029, Validation loss: 0.00872533, Gradient norm: 0.23598733
INFO:root:[   41] Training loss: 0.00792694, Validation loss: 0.00843643, Gradient norm: 0.26800492
INFO:root:[   42] Training loss: 0.00775844, Validation loss: 0.00878763, Gradient norm: 0.23872385
INFO:root:[   43] Training loss: 0.00753781, Validation loss: 0.00808946, Gradient norm: 0.21658927
INFO:root:[   44] Training loss: 0.00777081, Validation loss: 0.00842221, Gradient norm: 0.26073365
INFO:root:[   45] Training loss: 0.00759402, Validation loss: 0.00835430, Gradient norm: 0.23789071
INFO:root:[   46] Training loss: 0.00755866, Validation loss: 0.00792119, Gradient norm: 0.21524929
INFO:root:[   47] Training loss: 0.00787305, Validation loss: 0.00998558, Gradient norm: 0.29482390
INFO:root:[   48] Training loss: 0.00724788, Validation loss: 0.00881502, Gradient norm: 0.19611953
INFO:root:[   49] Training loss: 0.00762362, Validation loss: 0.00789053, Gradient norm: 0.31730311
INFO:root:[   50] Training loss: 0.00748004, Validation loss: 0.00789170, Gradient norm: 0.25162844
INFO:root:[   51] Training loss: 0.00733129, Validation loss: 0.00839817, Gradient norm: 0.22822417
INFO:root:[   52] Training loss: 0.00738690, Validation loss: 0.00871132, Gradient norm: 0.24478531
INFO:root:[   53] Training loss: 0.00708452, Validation loss: 0.00807352, Gradient norm: 0.23197815
INFO:root:[   54] Training loss: 0.00733300, Validation loss: 0.00787595, Gradient norm: 0.28911836
INFO:root:[   55] Training loss: 0.00717426, Validation loss: 0.00877618, Gradient norm: 0.23858515
INFO:root:[   56] Training loss: 0.00698946, Validation loss: 0.00824453, Gradient norm: 0.21396425
INFO:root:[   57] Training loss: 0.00705159, Validation loss: 0.00843133, Gradient norm: 0.21480986
INFO:root:[   58] Training loss: 0.00687907, Validation loss: 0.00871068, Gradient norm: 0.22787358
INFO:root:[   59] Training loss: 0.00694097, Validation loss: 0.00841314, Gradient norm: 0.21913249
INFO:root:[   60] Training loss: 0.00695491, Validation loss: 0.00773951, Gradient norm: 0.23213444
INFO:root:[   61] Training loss: 0.00691811, Validation loss: 0.00830825, Gradient norm: 0.22975142
INFO:root:[   62] Training loss: 0.00670671, Validation loss: 0.00876998, Gradient norm: 0.21673951
INFO:root:[   63] Training loss: 0.00695226, Validation loss: 0.00792647, Gradient norm: 0.24860135
INFO:root:[   64] Training loss: 0.00681982, Validation loss: 0.00814058, Gradient norm: 0.24093004
INFO:root:[   65] Training loss: 0.00668574, Validation loss: 0.00770656, Gradient norm: 0.22080699
INFO:root:[   66] Training loss: 0.00690230, Validation loss: 0.00792420, Gradient norm: 0.26559929
INFO:root:[   67] Training loss: 0.00688089, Validation loss: 0.00770377, Gradient norm: 0.28063500
INFO:root:[   68] Training loss: 0.00665472, Validation loss: 0.00848996, Gradient norm: 0.22384480
INFO:root:[   69] Training loss: 0.00646540, Validation loss: 0.00795193, Gradient norm: 0.19948222
INFO:root:[   70] Training loss: 0.00670565, Validation loss: 0.00827781, Gradient norm: 0.25744534
INFO:root:[   71] Training loss: 0.00644733, Validation loss: 0.00773774, Gradient norm: 0.19781069
INFO:root:[   72] Training loss: 0.00648691, Validation loss: 0.00769563, Gradient norm: 0.21640485
INFO:root:[   73] Training loss: 0.00652453, Validation loss: 0.00782475, Gradient norm: 0.23313425
INFO:root:[   74] Training loss: 0.00647533, Validation loss: 0.00820065, Gradient norm: 0.22959350
INFO:root:[   75] Training loss: 0.00637578, Validation loss: 0.00778941, Gradient norm: 0.23597979
INFO:root:[   76] Training loss: 0.00643161, Validation loss: 0.00758690, Gradient norm: 0.21962787
INFO:root:[   77] Training loss: 0.00629517, Validation loss: 0.00770046, Gradient norm: 0.24165867
INFO:root:[   78] Training loss: 0.00627250, Validation loss: 0.00950682, Gradient norm: 0.21824139
INFO:root:[   79] Training loss: 0.00643845, Validation loss: 0.00757015, Gradient norm: 0.25143004
INFO:root:[   80] Training loss: 0.00623114, Validation loss: 0.00810080, Gradient norm: 0.23654550
INFO:root:[   81] Training loss: 0.00600423, Validation loss: 0.00776827, Gradient norm: 0.19452569
INFO:root:[   82] Training loss: 0.00628645, Validation loss: 0.00776165, Gradient norm: 0.25286803
INFO:root:[   83] Training loss: 0.00605990, Validation loss: 0.00784201, Gradient norm: 0.21476674
INFO:root:[   84] Training loss: 0.00631029, Validation loss: 0.00781491, Gradient norm: 0.24796190
INFO:root:[   85] Training loss: 0.00616326, Validation loss: 0.00762042, Gradient norm: 0.22935098
INFO:root:[   86] Training loss: 0.00619446, Validation loss: 0.00777790, Gradient norm: 0.23541695
INFO:root:[   87] Training loss: 0.00612998, Validation loss: 0.00775280, Gradient norm: 0.23427972
INFO:root:[   88] Training loss: 0.00592493, Validation loss: 0.00789551, Gradient norm: 0.20897005
INFO:root:EP 88: Early stopping
INFO:root:Training the model took 1165.736s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.25007
INFO:root:EnergyScoretrain: 0.19545
INFO:root:Coveragetrain: 31.75184
INFO:root:IntervalWidthtrain: 1.54416
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.3237
INFO:root:EnergyScorevalidation: 0.2421
INFO:root:Coveragevalidation: 30.88833
INFO:root:IntervalWidthvalidation: 1.54083
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.69655
INFO:root:EnergyScoretest: 0.51728
INFO:root:Coveragetest: 21.11411
INFO:root:IntervalWidthtest: 1.54871
INFO:root:###3 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05226589, Validation loss: 0.02742088, Gradient norm: 0.30667882
INFO:root:[    2] Training loss: 0.02372313, Validation loss: 0.02110225, Gradient norm: 0.26361801
INFO:root:[    3] Training loss: 0.01957999, Validation loss: 0.01840288, Gradient norm: 0.27496155
INFO:root:[    4] Training loss: 0.01782078, Validation loss: 0.01681849, Gradient norm: 0.27365257
INFO:root:[    5] Training loss: 0.01638543, Validation loss: 0.01705220, Gradient norm: 0.28411715
INFO:root:[    6] Training loss: 0.01506434, Validation loss: 0.01497248, Gradient norm: 0.22091693
INFO:root:[    7] Training loss: 0.01406100, Validation loss: 0.01422775, Gradient norm: 0.20601505
INFO:root:[    8] Training loss: 0.01372788, Validation loss: 0.01375872, Gradient norm: 0.29014270
INFO:root:[    9] Training loss: 0.01292721, Validation loss: 0.01281956, Gradient norm: 0.24639810
INFO:root:[   10] Training loss: 0.01229955, Validation loss: 0.01248931, Gradient norm: 0.20269077
INFO:root:[   11] Training loss: 0.01200728, Validation loss: 0.01182568, Gradient norm: 0.24609854
INFO:root:[   12] Training loss: 0.01173979, Validation loss: 0.01143053, Gradient norm: 0.24512003
INFO:root:[   13] Training loss: 0.01118385, Validation loss: 0.01099657, Gradient norm: 0.19459935
INFO:root:[   14] Training loss: 0.01104246, Validation loss: 0.01077596, Gradient norm: 0.23433251
INFO:root:[   15] Training loss: 0.01085291, Validation loss: 0.01084919, Gradient norm: 0.24168196
INFO:root:[   16] Training loss: 0.01055361, Validation loss: 0.01050943, Gradient norm: 0.23223458
INFO:root:[   17] Training loss: 0.01047959, Validation loss: 0.01108829, Gradient norm: 0.25519274
INFO:root:[   18] Training loss: 0.01011271, Validation loss: 0.01086259, Gradient norm: 0.22950746
INFO:root:[   19] Training loss: 0.01024782, Validation loss: 0.00975262, Gradient norm: 0.28094587
INFO:root:[   20] Training loss: 0.01010933, Validation loss: 0.01037805, Gradient norm: 0.23585941
INFO:root:[   21] Training loss: 0.00976905, Validation loss: 0.01028136, Gradient norm: 0.23397240
INFO:root:[   22] Training loss: 0.00967405, Validation loss: 0.00958717, Gradient norm: 0.24243447
INFO:root:[   23] Training loss: 0.00953471, Validation loss: 0.00936931, Gradient norm: 0.24972297
INFO:root:[   24] Training loss: 0.00935482, Validation loss: 0.00957038, Gradient norm: 0.22403862
INFO:root:[   25] Training loss: 0.00937260, Validation loss: 0.00966645, Gradient norm: 0.23753729
INFO:root:[   26] Training loss: 0.00918738, Validation loss: 0.00924600, Gradient norm: 0.22793657
INFO:root:[   27] Training loss: 0.00896229, Validation loss: 0.00923799, Gradient norm: 0.19168300
INFO:root:[   28] Training loss: 0.00891453, Validation loss: 0.00918552, Gradient norm: 0.21774109
INFO:root:[   29] Training loss: 0.00884255, Validation loss: 0.00974423, Gradient norm: 0.21181843
INFO:root:[   30] Training loss: 0.00880112, Validation loss: 0.00900679, Gradient norm: 0.21429077
INFO:root:[   31] Training loss: 0.00885992, Validation loss: 0.00970377, Gradient norm: 0.26447411
INFO:root:[   32] Training loss: 0.00862535, Validation loss: 0.00870798, Gradient norm: 0.22052714
INFO:root:[   33] Training loss: 0.00872822, Validation loss: 0.00918978, Gradient norm: 0.24866022
INFO:root:[   34] Training loss: 0.00867195, Validation loss: 0.00889633, Gradient norm: 0.23934008
INFO:root:[   35] Training loss: 0.00849140, Validation loss: 0.00845544, Gradient norm: 0.23464069
INFO:root:[   36] Training loss: 0.00832839, Validation loss: 0.00874060, Gradient norm: 0.20562821
INFO:root:[   37] Training loss: 0.00811857, Validation loss: 0.00885354, Gradient norm: 0.19590203
INFO:root:[   38] Training loss: 0.00825603, Validation loss: 0.00924812, Gradient norm: 0.21418793
INFO:root:[   39] Training loss: 0.00813411, Validation loss: 0.00856333, Gradient norm: 0.19127396
INFO:root:[   40] Training loss: 0.00819874, Validation loss: 0.01015848, Gradient norm: 0.21418595
INFO:root:[   41] Training loss: 0.00808188, Validation loss: 0.00908187, Gradient norm: 0.23291326
INFO:root:[   42] Training loss: 0.00793711, Validation loss: 0.00860951, Gradient norm: 0.22471741
INFO:root:[   43] Training loss: 0.00810835, Validation loss: 0.00837622, Gradient norm: 0.24213455
INFO:root:[   44] Training loss: 0.00787531, Validation loss: 0.00864485, Gradient norm: 0.22547688
INFO:root:[   45] Training loss: 0.00775633, Validation loss: 0.00843749, Gradient norm: 0.20296664
INFO:root:[   46] Training loss: 0.00807768, Validation loss: 0.00852688, Gradient norm: 0.25797852
INFO:root:[   47] Training loss: 0.00783025, Validation loss: 0.00842795, Gradient norm: 0.22268676
INFO:root:[   48] Training loss: 0.00759906, Validation loss: 0.00829619, Gradient norm: 0.18063267
INFO:root:[   49] Training loss: 0.00742842, Validation loss: 0.00929889, Gradient norm: 0.18546402
INFO:root:[   50] Training loss: 0.00760622, Validation loss: 0.00824106, Gradient norm: 0.21285836
INFO:root:[   51] Training loss: 0.00762016, Validation loss: 0.00817221, Gradient norm: 0.22021516
INFO:root:[   52] Training loss: 0.00763042, Validation loss: 0.00871520, Gradient norm: 0.23933837
INFO:root:[   53] Training loss: 0.00753213, Validation loss: 0.00807370, Gradient norm: 0.22236853
INFO:root:[   54] Training loss: 0.00745150, Validation loss: 0.00813870, Gradient norm: 0.20399513
INFO:root:[   55] Training loss: 0.00750534, Validation loss: 0.00775304, Gradient norm: 0.24197993
INFO:root:[   56] Training loss: 0.00738961, Validation loss: 0.00814800, Gradient norm: 0.20640107
INFO:root:[   57] Training loss: 0.00734985, Validation loss: 0.00837956, Gradient norm: 0.22254823
INFO:root:[   58] Training loss: 0.00725200, Validation loss: 0.00872469, Gradient norm: 0.22054995
INFO:root:[   59] Training loss: 0.00728754, Validation loss: 0.00785852, Gradient norm: 0.23501831
INFO:root:[   60] Training loss: 0.00736539, Validation loss: 0.00819396, Gradient norm: 0.23819570
INFO:root:[   61] Training loss: 0.00732295, Validation loss: 0.00803075, Gradient norm: 0.24297484
INFO:root:[   62] Training loss: 0.00713534, Validation loss: 0.00797377, Gradient norm: 0.21131605
INFO:root:[   63] Training loss: 0.00692557, Validation loss: 0.00788471, Gradient norm: 0.18858140
INFO:root:[   64] Training loss: 0.00748208, Validation loss: 0.00791492, Gradient norm: 0.27812572
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 850.979s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.2823
INFO:root:EnergyScoretrain: 0.22307
INFO:root:Coveragetrain: 31.78243
INFO:root:IntervalWidthtrain: 1.82905
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.33099
INFO:root:EnergyScorevalidation: 0.25348
INFO:root:Coveragevalidation: 31.15726
INFO:root:IntervalWidthvalidation: 1.82568
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.66472
INFO:root:EnergyScoretest: 0.48454
INFO:root:Coveragetest: 24.08607
INFO:root:IntervalWidthtest: 1.86441
INFO:root:###4 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04927762, Validation loss: 0.02843156, Gradient norm: 0.33362822
INFO:root:[    2] Training loss: 0.02597672, Validation loss: 0.02463935, Gradient norm: 0.28481349
INFO:root:[    3] Training loss: 0.02141118, Validation loss: 0.01931265, Gradient norm: 0.26881944
INFO:root:[    4] Training loss: 0.01859865, Validation loss: 0.01825365, Gradient norm: 0.25506230
INFO:root:[    5] Training loss: 0.01677815, Validation loss: 0.01704576, Gradient norm: 0.21737556
INFO:root:[    6] Training loss: 0.01567423, Validation loss: 0.01913543, Gradient norm: 0.24582059
INFO:root:[    7] Training loss: 0.01517838, Validation loss: 0.01512148, Gradient norm: 0.27423731
INFO:root:[    8] Training loss: 0.01400050, Validation loss: 0.01392144, Gradient norm: 0.21348785
INFO:root:[    9] Training loss: 0.01351164, Validation loss: 0.01402354, Gradient norm: 0.22866655
INFO:root:[   10] Training loss: 0.01321171, Validation loss: 0.01299920, Gradient norm: 0.25685593
INFO:root:[   11] Training loss: 0.01254924, Validation loss: 0.01257882, Gradient norm: 0.19138216
INFO:root:[   12] Training loss: 0.01252181, Validation loss: 0.01234614, Gradient norm: 0.24266294
INFO:root:[   13] Training loss: 0.01190047, Validation loss: 0.01176990, Gradient norm: 0.22536512
INFO:root:[   14] Training loss: 0.01163010, Validation loss: 0.01202325, Gradient norm: 0.22353095
INFO:root:[   15] Training loss: 0.01153666, Validation loss: 0.01199617, Gradient norm: 0.22541303
INFO:root:[   16] Training loss: 0.01141467, Validation loss: 0.01137904, Gradient norm: 0.26380548
INFO:root:[   17] Training loss: 0.01123672, Validation loss: 0.01111401, Gradient norm: 0.25291326
INFO:root:[   18] Training loss: 0.01083759, Validation loss: 0.01081344, Gradient norm: 0.21406415
INFO:root:[   19] Training loss: 0.01067142, Validation loss: 0.01086558, Gradient norm: 0.23909989
INFO:root:[   20] Training loss: 0.01050046, Validation loss: 0.01110980, Gradient norm: 0.21307216
INFO:root:[   21] Training loss: 0.01049138, Validation loss: 0.01041491, Gradient norm: 0.24331422
INFO:root:[   22] Training loss: 0.01055858, Validation loss: 0.01134104, Gradient norm: 0.29267437
INFO:root:[   23] Training loss: 0.01003367, Validation loss: 0.01001412, Gradient norm: 0.19303689
INFO:root:[   24] Training loss: 0.01034137, Validation loss: 0.01006028, Gradient norm: 0.27411151
INFO:root:[   25] Training loss: 0.00988520, Validation loss: 0.01148302, Gradient norm: 0.22512361
INFO:root:[   26] Training loss: 0.01013060, Validation loss: 0.01017964, Gradient norm: 0.27322603
INFO:root:[   27] Training loss: 0.00965306, Validation loss: 0.00994898, Gradient norm: 0.19595244
INFO:root:[   28] Training loss: 0.00943921, Validation loss: 0.00997684, Gradient norm: 0.19018823
INFO:root:[   29] Training loss: 0.00979119, Validation loss: 0.01101084, Gradient norm: 0.27018950
INFO:root:[   30] Training loss: 0.00973687, Validation loss: 0.01064164, Gradient norm: 0.29528714
INFO:root:[   31] Training loss: 0.00931151, Validation loss: 0.00959475, Gradient norm: 0.22051975
INFO:root:[   32] Training loss: 0.00921251, Validation loss: 0.00983545, Gradient norm: 0.21961696
INFO:root:[   33] Training loss: 0.00910588, Validation loss: 0.00988748, Gradient norm: 0.22327297
INFO:root:[   34] Training loss: 0.00931978, Validation loss: 0.00919337, Gradient norm: 0.25838076
INFO:root:[   35] Training loss: 0.00911932, Validation loss: 0.00986384, Gradient norm: 0.23025404
INFO:root:[   36] Training loss: 0.00922541, Validation loss: 0.00990982, Gradient norm: 0.24796603
INFO:root:[   37] Training loss: 0.00908495, Validation loss: 0.00963216, Gradient norm: 0.25866259
INFO:root:[   38] Training loss: 0.00894204, Validation loss: 0.01018728, Gradient norm: 0.24547827
INFO:root:[   39] Training loss: 0.00883683, Validation loss: 0.00893890, Gradient norm: 0.21255843
INFO:root:[   40] Training loss: 0.00891950, Validation loss: 0.00913825, Gradient norm: 0.25921258
INFO:root:[   41] Training loss: 0.00872344, Validation loss: 0.00887424, Gradient norm: 0.23400792
INFO:root:[   42] Training loss: 0.00877814, Validation loss: 0.01015718, Gradient norm: 0.26064178
INFO:root:[   43] Training loss: 0.00864143, Validation loss: 0.00950163, Gradient norm: 0.22490797
INFO:root:[   44] Training loss: 0.00859203, Validation loss: 0.00862845, Gradient norm: 0.24150117
INFO:root:[   45] Training loss: 0.00845518, Validation loss: 0.00901085, Gradient norm: 0.22647126
INFO:root:[   46] Training loss: 0.00848725, Validation loss: 0.00981836, Gradient norm: 0.22879160
INFO:root:[   47] Training loss: 0.00863077, Validation loss: 0.00905042, Gradient norm: 0.25186790
INFO:root:[   48] Training loss: 0.00859063, Validation loss: 0.00877120, Gradient norm: 0.27888486
INFO:root:[   49] Training loss: 0.00813726, Validation loss: 0.00884135, Gradient norm: 0.21076572
INFO:root:[   50] Training loss: 0.00851112, Validation loss: 0.00879232, Gradient norm: 0.26957931
INFO:root:[   51] Training loss: 0.00849395, Validation loss: 0.00836345, Gradient norm: 0.26627349
INFO:root:[   52] Training loss: 0.00809943, Validation loss: 0.00899275, Gradient norm: 0.25478506
INFO:root:[   53] Training loss: 0.00798487, Validation loss: 0.00879313, Gradient norm: 0.20871015
INFO:root:[   54] Training loss: 0.00806426, Validation loss: 0.00885686, Gradient norm: 0.24792140
INFO:root:[   55] Training loss: 0.00799322, Validation loss: 0.00841055, Gradient norm: 0.23801975
INFO:root:[   56] Training loss: 0.00803174, Validation loss: 0.00856103, Gradient norm: 0.24669448
INFO:root:[   57] Training loss: 0.00812080, Validation loss: 0.00867849, Gradient norm: 0.26914714
INFO:root:[   58] Training loss: 0.00803369, Validation loss: 0.00886348, Gradient norm: 0.26016618
INFO:root:[   59] Training loss: 0.00786097, Validation loss: 0.00849624, Gradient norm: 0.24007525
INFO:root:[   60] Training loss: 0.00784432, Validation loss: 0.00859621, Gradient norm: 0.22169590
INFO:root:[   61] Training loss: 0.00795816, Validation loss: 0.00838185, Gradient norm: 0.26259834
INFO:root:[   62] Training loss: 0.00817722, Validation loss: 0.00827686, Gradient norm: 0.27985295
INFO:root:[   63] Training loss: 0.00777675, Validation loss: 0.00913467, Gradient norm: 0.24413880
INFO:root:[   64] Training loss: 0.00777548, Validation loss: 0.01101024, Gradient norm: 0.25031101
INFO:root:[   65] Training loss: 0.00770761, Validation loss: 0.00819475, Gradient norm: 0.23037509
INFO:root:[   66] Training loss: 0.00770288, Validation loss: 0.00917178, Gradient norm: 0.26103112
INFO:root:[   67] Training loss: 0.00758900, Validation loss: 0.00895074, Gradient norm: 0.24904683
INFO:root:[   68] Training loss: 0.00761461, Validation loss: 0.00905936, Gradient norm: 0.25790495
INFO:root:[   69] Training loss: 0.00790596, Validation loss: 0.00849830, Gradient norm: 0.31627684
INFO:root:[   70] Training loss: 0.00757061, Validation loss: 0.00962697, Gradient norm: 0.24346041
INFO:root:[   71] Training loss: 0.00746518, Validation loss: 0.00889600, Gradient norm: 0.23263677
INFO:root:[   72] Training loss: 0.00748907, Validation loss: 0.00816375, Gradient norm: 0.24125039
INFO:root:[   73] Training loss: 0.00740817, Validation loss: 0.00956448, Gradient norm: 0.23275164
INFO:root:[   74] Training loss: 0.00747332, Validation loss: 0.00810335, Gradient norm: 0.24907377
INFO:root:[   75] Training loss: 0.00740403, Validation loss: 0.00825740, Gradient norm: 0.25713684
INFO:root:[   76] Training loss: 0.00737903, Validation loss: 0.00820036, Gradient norm: 0.26439083
INFO:root:[   77] Training loss: 0.00736188, Validation loss: 0.00863667, Gradient norm: 0.26680124
INFO:root:[   78] Training loss: 0.00729124, Validation loss: 0.00817090, Gradient norm: 0.23702737
INFO:root:[   79] Training loss: 0.00741027, Validation loss: 0.00874592, Gradient norm: 0.28365211
INFO:root:[   80] Training loss: 0.00727801, Validation loss: 0.00815565, Gradient norm: 0.24061693
INFO:root:[   81] Training loss: 0.00712879, Validation loss: 0.00886831, Gradient norm: 0.23145524
INFO:root:[   82] Training loss: 0.00720755, Validation loss: 0.00819357, Gradient norm: 0.24534476
INFO:root:[   83] Training loss: 0.00729452, Validation loss: 0.00817193, Gradient norm: 0.25791519
INFO:root:EP 83: Early stopping
INFO:root:Training the model took 1102.325s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.28449
INFO:root:EnergyScoretrain: 0.2285
INFO:root:Coveragetrain: 31.82808
INFO:root:IntervalWidthtrain: 1.9501
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.33468
INFO:root:EnergyScorevalidation: 0.25836
INFO:root:Coveragevalidation: 31.22251
INFO:root:IntervalWidthvalidation: 1.94021
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.50108
INFO:root:EnergyScoretest: 0.35488
INFO:root:Coveragetest: 29.32396
INFO:root:IntervalWidthtest: 2.04255
INFO:root:###5 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05303459, Validation loss: 0.02924914, Gradient norm: 0.30303951
INFO:root:[    2] Training loss: 0.02650508, Validation loss: 0.02403162, Gradient norm: 0.20037428
INFO:root:[    3] Training loss: 0.02215317, Validation loss: 0.02149315, Gradient norm: 0.22825428
INFO:root:[    4] Training loss: 0.01930867, Validation loss: 0.01866473, Gradient norm: 0.18942117
INFO:root:[    5] Training loss: 0.01787348, Validation loss: 0.01691538, Gradient norm: 0.24037343
INFO:root:[    6] Training loss: 0.01663552, Validation loss: 0.01607649, Gradient norm: 0.20250915
INFO:root:[    7] Training loss: 0.01574284, Validation loss: 0.01921076, Gradient norm: 0.23311045
INFO:root:[    8] Training loss: 0.01497085, Validation loss: 0.01450705, Gradient norm: 0.20341939
INFO:root:[    9] Training loss: 0.01432626, Validation loss: 0.01464990, Gradient norm: 0.21869334
INFO:root:[   10] Training loss: 0.01366351, Validation loss: 0.01382109, Gradient norm: 0.19363215
INFO:root:[   11] Training loss: 0.01339523, Validation loss: 0.01354642, Gradient norm: 0.22883367
INFO:root:[   12] Training loss: 0.01290894, Validation loss: 0.01282443, Gradient norm: 0.21512993
INFO:root:[   13] Training loss: 0.01261802, Validation loss: 0.01258350, Gradient norm: 0.22449230
INFO:root:[   14] Training loss: 0.01233295, Validation loss: 0.01173738, Gradient norm: 0.22761295
INFO:root:[   15] Training loss: 0.01173297, Validation loss: 0.01212099, Gradient norm: 0.17312420
INFO:root:[   16] Training loss: 0.01170278, Validation loss: 0.01163236, Gradient norm: 0.21068219
INFO:root:[   17] Training loss: 0.01142964, Validation loss: 0.01214261, Gradient norm: 0.21355948
INFO:root:[   18] Training loss: 0.01129366, Validation loss: 0.01186072, Gradient norm: 0.20583504
INFO:root:[   19] Training loss: 0.01128611, Validation loss: 0.01121460, Gradient norm: 0.25223712
INFO:root:[   20] Training loss: 0.01106636, Validation loss: 0.01096430, Gradient norm: 0.25653233
INFO:root:[   21] Training loss: 0.01070104, Validation loss: 0.01194237, Gradient norm: 0.21577926
INFO:root:[   22] Training loss: 0.01084865, Validation loss: 0.01091859, Gradient norm: 0.23747646
INFO:root:[   23] Training loss: 0.01056260, Validation loss: 0.01074420, Gradient norm: 0.22039952
INFO:root:[   24] Training loss: 0.01029338, Validation loss: 0.01116822, Gradient norm: 0.20626671
INFO:root:[   25] Training loss: 0.01031902, Validation loss: 0.01246677, Gradient norm: 0.24344798
INFO:root:[   26] Training loss: 0.01017287, Validation loss: 0.01032057, Gradient norm: 0.23201198
INFO:root:[   27] Training loss: 0.01006013, Validation loss: 0.01053769, Gradient norm: 0.23244327
INFO:root:[   28] Training loss: 0.01005042, Validation loss: 0.01119399, Gradient norm: 0.22900749
INFO:root:[   29] Training loss: 0.00969719, Validation loss: 0.00978707, Gradient norm: 0.21433756
INFO:root:[   30] Training loss: 0.00984035, Validation loss: 0.00987235, Gradient norm: 0.25033386
INFO:root:[   31] Training loss: 0.00959724, Validation loss: 0.01103907, Gradient norm: 0.23920640
INFO:root:[   32] Training loss: 0.00964376, Validation loss: 0.01041711, Gradient norm: 0.22866437
INFO:root:[   33] Training loss: 0.00953640, Validation loss: 0.00981488, Gradient norm: 0.24158626
INFO:root:[   34] Training loss: 0.00947887, Validation loss: 0.01004445, Gradient norm: 0.25390665
INFO:root:[   35] Training loss: 0.00923486, Validation loss: 0.00973590, Gradient norm: 0.20745661
INFO:root:[   36] Training loss: 0.00951756, Validation loss: 0.01050748, Gradient norm: 0.27841860
INFO:root:[   37] Training loss: 0.00933340, Validation loss: 0.00933151, Gradient norm: 0.24085210
INFO:root:[   38] Training loss: 0.00927072, Validation loss: 0.00931191, Gradient norm: 0.26216998
INFO:root:[   39] Training loss: 0.00939011, Validation loss: 0.00930231, Gradient norm: 0.26831831
INFO:root:[   40] Training loss: 0.00893561, Validation loss: 0.00969728, Gradient norm: 0.22367759
INFO:root:[   41] Training loss: 0.00922472, Validation loss: 0.00930432, Gradient norm: 0.28661698
INFO:root:[   42] Training loss: 0.00884945, Validation loss: 0.00912537, Gradient norm: 0.24019394
INFO:root:[   43] Training loss: 0.00878948, Validation loss: 0.00885244, Gradient norm: 0.22677648
INFO:root:[   44] Training loss: 0.00887289, Validation loss: 0.00903909, Gradient norm: 0.23504674
INFO:root:[   45] Training loss: 0.00892346, Validation loss: 0.00917998, Gradient norm: 0.26312862
INFO:root:[   46] Training loss: 0.00873862, Validation loss: 0.00902666, Gradient norm: 0.23071763
INFO:root:[   47] Training loss: 0.00860348, Validation loss: 0.01111004, Gradient norm: 0.24401169
INFO:root:[   48] Training loss: 0.00890450, Validation loss: 0.01001724, Gradient norm: 0.28364742
INFO:root:[   49] Training loss: 0.00862634, Validation loss: 0.00879125, Gradient norm: 0.24728579
INFO:root:[   50] Training loss: 0.00835652, Validation loss: 0.00895292, Gradient norm: 0.21405011
INFO:root:[   51] Training loss: 0.00862214, Validation loss: 0.00894911, Gradient norm: 0.25875615
INFO:root:[   52] Training loss: 0.00832543, Validation loss: 0.00943350, Gradient norm: 0.22539857
INFO:root:[   53] Training loss: 0.00829937, Validation loss: 0.00915671, Gradient norm: 0.24423409
INFO:root:[   54] Training loss: 0.00849937, Validation loss: 0.00899266, Gradient norm: 0.27619272
INFO:root:[   55] Training loss: 0.00834061, Validation loss: 0.00883729, Gradient norm: 0.25979496
INFO:root:[   56] Training loss: 0.00820348, Validation loss: 0.01126332, Gradient norm: 0.25616624
INFO:root:[   57] Training loss: 0.00831125, Validation loss: 0.00873342, Gradient norm: 0.25428110
INFO:root:[   58] Training loss: 0.00825389, Validation loss: 0.00856275, Gradient norm: 0.25802451
INFO:root:[   59] Training loss: 0.00816451, Validation loss: 0.01006958, Gradient norm: 0.27229246
INFO:root:[   60] Training loss: 0.00801665, Validation loss: 0.00909544, Gradient norm: 0.24953293
INFO:root:[   61] Training loss: 0.00798156, Validation loss: 0.00982229, Gradient norm: 0.23784626
INFO:root:[   62] Training loss: 0.00805342, Validation loss: 0.00856596, Gradient norm: 0.25224949
INFO:root:[   63] Training loss: 0.00828964, Validation loss: 0.00884038, Gradient norm: 0.28842582
INFO:root:[   64] Training loss: 0.00804845, Validation loss: 0.00867533, Gradient norm: 0.26210192
INFO:root:[   65] Training loss: 0.00824523, Validation loss: 0.00878832, Gradient norm: 0.29411925
INFO:root:[   66] Training loss: 0.00793985, Validation loss: 0.00847729, Gradient norm: 0.26195609
INFO:root:[   67] Training loss: 0.00785951, Validation loss: 0.00849536, Gradient norm: 0.23708066
INFO:root:[   68] Training loss: 0.00776988, Validation loss: 0.01119645, Gradient norm: 0.22610849
INFO:root:[   69] Training loss: 0.00780470, Validation loss: 0.01121455, Gradient norm: 0.25937660
INFO:root:[   70] Training loss: 0.00786315, Validation loss: 0.00908855, Gradient norm: 0.25571779
INFO:root:[   71] Training loss: 0.00767733, Validation loss: 0.00894314, Gradient norm: 0.24640178
INFO:root:[   72] Training loss: 0.00785438, Validation loss: 0.00843831, Gradient norm: 0.26269195
INFO:root:[   73] Training loss: 0.00775332, Validation loss: 0.00822065, Gradient norm: 0.28182903
INFO:root:[   74] Training loss: 0.00760583, Validation loss: 0.00853320, Gradient norm: 0.24167746
INFO:root:[   75] Training loss: 0.00761759, Validation loss: 0.00824496, Gradient norm: 0.24590567
INFO:root:[   76] Training loss: 0.00777777, Validation loss: 0.00867114, Gradient norm: 0.25989364
INFO:root:[   77] Training loss: 0.00760209, Validation loss: 0.00865068, Gradient norm: 0.25079754
INFO:root:[   78] Training loss: 0.00753588, Validation loss: 0.00887982, Gradient norm: 0.24110257
INFO:root:[   79] Training loss: 0.00737212, Validation loss: 0.00821694, Gradient norm: 0.22623519
INFO:root:[   80] Training loss: 0.00760649, Validation loss: 0.00925190, Gradient norm: 0.30099308
INFO:root:[   81] Training loss: 0.00735839, Validation loss: 0.00837740, Gradient norm: 0.22953669
INFO:root:[   82] Training loss: 0.00748555, Validation loss: 0.00806712, Gradient norm: 0.27248557
INFO:root:[   83] Training loss: 0.00759957, Validation loss: 0.00916657, Gradient norm: 0.27886950
INFO:root:[   84] Training loss: 0.00753586, Validation loss: 0.00854662, Gradient norm: 0.27603870
INFO:root:[   85] Training loss: 0.00716057, Validation loss: 0.00853117, Gradient norm: 0.20716413
INFO:root:[   86] Training loss: 0.00732571, Validation loss: 0.00893353, Gradient norm: 0.28607351
INFO:root:[   87] Training loss: 0.00740507, Validation loss: 0.00857624, Gradient norm: 0.27126962
INFO:root:[   88] Training loss: 0.00706698, Validation loss: 0.00795769, Gradient norm: 0.19986630
INFO:root:[   89] Training loss: 0.00741470, Validation loss: 0.00839912, Gradient norm: 0.27733253
INFO:root:[   90] Training loss: 0.00708932, Validation loss: 0.00897903, Gradient norm: 0.21396519
INFO:root:[   91] Training loss: 0.00743742, Validation loss: 0.00818978, Gradient norm: 0.28607416
INFO:root:[   92] Training loss: 0.00723687, Validation loss: 0.00860241, Gradient norm: 0.25180109
INFO:root:[   93] Training loss: 0.00714373, Validation loss: 0.00814925, Gradient norm: 0.23748838
INFO:root:[   94] Training loss: 0.00701285, Validation loss: 0.00839260, Gradient norm: 0.22984566
INFO:root:[   95] Training loss: 0.00731747, Validation loss: 0.00820628, Gradient norm: 0.28227010
INFO:root:[   96] Training loss: 0.00705276, Validation loss: 0.00846730, Gradient norm: 0.25115135
INFO:root:[   97] Training loss: 0.00705905, Validation loss: 0.00821272, Gradient norm: 0.25763458
INFO:root:EP 97: Early stopping
INFO:root:Training the model took 1299.564s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on train data.
INFO:root:MSEtrain: 0.26097
INFO:root:EnergyScoretrain: 0.2193
INFO:root:Coveragetrain: 31.87539
INFO:root:IntervalWidthtrain: 1.97194
INFO:root:Evaluating the model on validation data.
INFO:root:MSEvalidation: 0.32574
INFO:root:EnergyScorevalidation: 0.2572
INFO:root:Coveragevalidation: 31.24687
INFO:root:IntervalWidthvalidation: 1.9629
INFO:root:Evaluating the model on test data.
INFO:root:MSEtest: 0.62707
INFO:root:EnergyScoretest: 0.45087
INFO:root:Coveragetest: 28.6277
INFO:root:IntervalWidthtest: 2.09146
INFO:root:###6 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05804555, Validation loss: 0.03050475, Gradient norm: 0.35588922
INFO:root:[    2] Training loss: 0.02824035, Validation loss: 0.02621776, Gradient norm: 0.20455170
INFO:root:[    3] Training loss: 0.02436311, Validation loss: 0.02257392, Gradient norm: 0.19880394
INFO:root:[    4] Training loss: 0.02155859, Validation loss: 0.02077968, Gradient norm: 0.22192146
INFO:root:[    5] Training loss: 0.01945590, Validation loss: 0.01898681, Gradient norm: 0.20768981
INFO:root:[    6] Training loss: 0.01817512, Validation loss: 0.02045447, Gradient norm: 0.20419717
INFO:root:[    7] Training loss: 0.01732840, Validation loss: 0.01661213, Gradient norm: 0.20533939
INFO:root:[    8] Training loss: 0.01632088, Validation loss: 0.01601086, Gradient norm: 0.17699361
INFO:root:[    9] Training loss: 0.01555525, Validation loss: 0.01548369, Gradient norm: 0.17708381
INFO:root:[   10] Training loss: 0.01515286, Validation loss: 0.01477449, Gradient norm: 0.20579139
INFO:root:[   11] Training loss: 0.01465990, Validation loss: 0.01450267, Gradient norm: 0.20132936
INFO:root:[   12] Training loss: 0.01390487, Validation loss: 0.01490391, Gradient norm: 0.18288680
INFO:root:[   13] Training loss: 0.01369448, Validation loss: 0.01337873, Gradient norm: 0.20871893
INFO:root:[   14] Training loss: 0.01356430, Validation loss: 0.01296667, Gradient norm: 0.24181154
INFO:root:[   15] Training loss: 0.01302539, Validation loss: 0.01325497, Gradient norm: 0.20633275
INFO:root:[   16] Training loss: 0.01299584, Validation loss: 0.01264066, Gradient norm: 0.24536532
INFO:root:[   17] Training loss: 0.01248273, Validation loss: 0.01281302, Gradient norm: 0.21190469
INFO:root:[   18] Training loss: 0.01243095, Validation loss: 0.01303440, Gradient norm: 0.24647323
INFO:root:[   19] Training loss: 0.01222395, Validation loss: 0.01333494, Gradient norm: 0.23747207
INFO:root:[   20] Training loss: 0.01186129, Validation loss: 0.01151530, Gradient norm: 0.21958207
INFO:root:[   21] Training loss: 0.01190149, Validation loss: 0.01234063, Gradient norm: 0.24586803
INFO:root:[   22] Training loss: 0.01143330, Validation loss: 0.01145277, Gradient norm: 0.19183703
INFO:root:[   23] Training loss: 0.01145426, Validation loss: 0.01141799, Gradient norm: 0.21110404
INFO:root:[   24] Training loss: 0.01133508, Validation loss: 0.01169606, Gradient norm: 0.23497313
INFO:root:[   25] Training loss: 0.01115797, Validation loss: 0.01117930, Gradient norm: 0.23345743
INFO:root:[   26] Training loss: 0.01125099, Validation loss: 0.01094788, Gradient norm: 0.26110086
INFO:root:[   27] Training loss: 0.01103213, Validation loss: 0.01079563, Gradient norm: 0.23964466
INFO:root:[   28] Training loss: 0.01102125, Validation loss: 0.01090148, Gradient norm: 0.27383151
INFO:root:[   29] Training loss: 0.01063027, Validation loss: 0.01103348, Gradient norm: 0.22148443
INFO:root:[   30] Training loss: 0.01048160, Validation loss: 0.01078615, Gradient norm: 0.20291962
INFO:root:[   31] Training loss: 0.01062770, Validation loss: 0.01044157, Gradient norm: 0.27051053
INFO:root:[   32] Training loss: 0.01047516, Validation loss: 0.01219703, Gradient norm: 0.26228685
INFO:root:[   33] Training loss: 0.01035877, Validation loss: 0.01055105, Gradient norm: 0.23580424
INFO:root:[   34] Training loss: 0.01039035, Validation loss: 0.01019245, Gradient norm: 0.29680402
INFO:root:[   35] Training loss: 0.01025658, Validation loss: 0.01038646, Gradient norm: 0.25039358
INFO:root:[   36] Training loss: 0.01010711, Validation loss: 0.01013756, Gradient norm: 0.24019498
INFO:root:[   37] Training loss: 0.00986853, Validation loss: 0.01094940, Gradient norm: 0.20698000
INFO:root:[   38] Training loss: 0.01017124, Validation loss: 0.01063491, Gradient norm: 0.28537301
INFO:root:[   39] Training loss: 0.00973538, Validation loss: 0.01128378, Gradient norm: 0.22774070
INFO:root:[   40] Training loss: 0.00980832, Validation loss: 0.01015481, Gradient norm: 0.28392168
INFO:root:[   41] Training loss: 0.00965279, Validation loss: 0.00954282, Gradient norm: 0.23024918
INFO:root:[   42] Training loss: 0.00952386, Validation loss: 0.01002483, Gradient norm: 0.22998239
INFO:root:[   43] Training loss: 0.00992016, Validation loss: 0.01033313, Gradient norm: 0.31008452
INFO:root:[   44] Training loss: 0.00949296, Validation loss: 0.00967318, Gradient norm: 0.23532719
INFO:root:[   45] Training loss: 0.00938864, Validation loss: 0.00975898, Gradient norm: 0.25837310
INFO:root:[   46] Training loss: 0.00957855, Validation loss: 0.00987652, Gradient norm: 0.30833661
INFO:root:[   47] Training loss: 0.00925126, Validation loss: 0.01085681, Gradient norm: 0.25070698
INFO:root:[   48] Training loss: 0.00933350, Validation loss: 0.00989765, Gradient norm: 0.25854406
INFO:root:[   49] Training loss: 0.00935554, Validation loss: 0.01164626, Gradient norm: 0.26643803
INFO:root:[   50] Training loss: 0.00929730, Validation loss: 0.00956925, Gradient norm: 0.27365436
INFO:root:[   51] Training loss: 0.00933999, Validation loss: 0.00931446, Gradient norm: 0.28650623
INFO:root:[   52] Training loss: 0.00912861, Validation loss: 0.00999984, Gradient norm: 0.27835671
INFO:root:[   53] Training loss: 0.00929608, Validation loss: 0.00929978, Gradient norm: 0.28790980
INFO:root:[   54] Training loss: 0.00907650, Validation loss: 0.00990561, Gradient norm: 0.27876111
INFO:root:[   55] Training loss: 0.00895069, Validation loss: 0.00973634, Gradient norm: 0.26456755
INFO:root:[   56] Training loss: 0.00902379, Validation loss: 0.00975487, Gradient norm: 0.29733100
INFO:root:[   57] Training loss: 0.00894139, Validation loss: 0.00920926, Gradient norm: 0.28874602
INFO:root:[   58] Training loss: 0.00896829, Validation loss: 0.00898307, Gradient norm: 0.27780255
INFO:root:[   59] Training loss: 0.00882156, Validation loss: 0.00903024, Gradient norm: 0.27393204
INFO:root:[   60] Training loss: 0.00879215, Validation loss: 0.00912308, Gradient norm: 0.27952855
INFO:root:[   61] Training loss: 0.00892599, Validation loss: 0.01025926, Gradient norm: 0.28354913
