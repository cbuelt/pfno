INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05091358, Validation loss: 0.02538679, Gradient norm: 0.63226096
INFO:root:[    2] Training loss: 0.02535810, Validation loss: 0.01646847, Gradient norm: 0.66538246
INFO:root:[    3] Training loss: 0.01988878, Validation loss: 0.01598889, Gradient norm: 0.58995111
INFO:root:[    4] Training loss: 0.01784887, Validation loss: 0.01727170, Gradient norm: 0.57377523
INFO:root:[    5] Training loss: 0.01586198, Validation loss: 0.01925322, Gradient norm: 0.53744293
INFO:root:[    6] Training loss: 0.01481028, Validation loss: 0.01232467, Gradient norm: 0.53034115
INFO:root:[    7] Training loss: 0.01437714, Validation loss: 0.01551332, Gradient norm: 0.50341516
INFO:root:[    8] Training loss: 0.01315644, Validation loss: 0.01251203, Gradient norm: 0.45706063
INFO:root:[    9] Training loss: 0.01334890, Validation loss: 0.01622292, Gradient norm: 0.50306471
INFO:root:[   10] Training loss: 0.01224161, Validation loss: 0.01712520, Gradient norm: 0.45937349
INFO:root:[   11] Training loss: 0.01076916, Validation loss: 0.01305893, Gradient norm: 0.37137126
INFO:root:[   12] Training loss: 0.01116791, Validation loss: 0.01301055, Gradient norm: 0.43619074
INFO:root:[   13] Training loss: 0.00999652, Validation loss: 0.01021232, Gradient norm: 0.34570329
INFO:root:[   14] Training loss: 0.01001236, Validation loss: 0.01000836, Gradient norm: 0.37828906
INFO:root:[   15] Training loss: 0.01082898, Validation loss: 0.01202187, Gradient norm: 0.42327568
INFO:root:[   16] Training loss: 0.00919545, Validation loss: 0.00969141, Gradient norm: 0.28844525
INFO:root:[   17] Training loss: 0.01055141, Validation loss: 0.01042139, Gradient norm: 0.42267848
INFO:root:[   18] Training loss: 0.00974075, Validation loss: 0.01114514, Gradient norm: 0.38462238
INFO:root:[   19] Training loss: 0.00897536, Validation loss: 0.00974655, Gradient norm: 0.35779001
INFO:root:[   20] Training loss: 0.00857463, Validation loss: 0.01273856, Gradient norm: 0.32101041
INFO:root:[   21] Training loss: 0.00919751, Validation loss: 0.01015385, Gradient norm: 0.39126079
INFO:root:[   22] Training loss: 0.00822055, Validation loss: 0.01069025, Gradient norm: 0.34524000
INFO:root:[   23] Training loss: 0.00854198, Validation loss: 0.01010651, Gradient norm: 0.31970061
INFO:root:[   24] Training loss: 0.00861278, Validation loss: 0.01022182, Gradient norm: 0.34540110
INFO:root:[   25] Training loss: 0.00802054, Validation loss: 0.01094738, Gradient norm: 0.33557172
INFO:root:[   26] Training loss: 0.00787764, Validation loss: 0.00956329, Gradient norm: 0.31449566
INFO:root:[   27] Training loss: 0.00803957, Validation loss: 0.00900832, Gradient norm: 0.35158186
INFO:root:[   28] Training loss: 0.00675436, Validation loss: 0.00975199, Gradient norm: 0.22370752
INFO:root:[   29] Training loss: 0.00801209, Validation loss: 0.01166236, Gradient norm: 0.38507752
INFO:root:[   30] Training loss: 0.00686767, Validation loss: 0.00937745, Gradient norm: 0.27113042
INFO:root:[   31] Training loss: 0.00654487, Validation loss: 0.01014295, Gradient norm: 0.25053614
INFO:root:[   32] Training loss: 0.00733425, Validation loss: 0.00974286, Gradient norm: 0.27713761
INFO:root:[   33] Training loss: 0.00672565, Validation loss: 0.00996385, Gradient norm: 0.27679925
INFO:root:[   34] Training loss: 0.00712308, Validation loss: 0.00971396, Gradient norm: 0.33985578
INFO:root:[   35] Training loss: 0.00646669, Validation loss: 0.00963347, Gradient norm: 0.29759226
INFO:root:[   36] Training loss: 0.00672784, Validation loss: 0.00979421, Gradient norm: 0.27778659
INFO:root:[   37] Training loss: 0.00621984, Validation loss: 0.01011124, Gradient norm: 0.27954999
INFO:root:[   38] Training loss: 0.00654113, Validation loss: 0.01032409, Gradient norm: 0.31017372
INFO:root:[   39] Training loss: 0.00628717, Validation loss: 0.01100355, Gradient norm: 0.29610843
INFO:root:[   40] Training loss: 0.00630418, Validation loss: 0.00967960, Gradient norm: 0.26697863
INFO:root:[   41] Training loss: 0.00595652, Validation loss: 0.01023841, Gradient norm: 0.25730478
INFO:root:[   42] Training loss: 0.00577461, Validation loss: 0.00909221, Gradient norm: 0.25185951
INFO:root:[   43] Training loss: 0.00601344, Validation loss: 0.00883824, Gradient norm: 0.28302100
INFO:root:[   44] Training loss: 0.00600407, Validation loss: 0.00926370, Gradient norm: 0.29654894
INFO:root:[   45] Training loss: 0.00627453, Validation loss: 0.01066381, Gradient norm: 0.34066592
INFO:root:[   46] Training loss: 0.00592950, Validation loss: 0.01007994, Gradient norm: 0.24542322
INFO:root:[   47] Training loss: 0.00506626, Validation loss: 0.01028402, Gradient norm: 0.20603801
INFO:root:[   48] Training loss: 0.00572362, Validation loss: 0.00920185, Gradient norm: 0.31191716
INFO:root:[   49] Training loss: 0.00536369, Validation loss: 0.00960744, Gradient norm: 0.25586375
INFO:root:[   50] Training loss: 0.00546748, Validation loss: 0.00950087, Gradient norm: 0.26429360
INFO:root:[   51] Training loss: 0.00546537, Validation loss: 0.00988578, Gradient norm: 0.29575849
INFO:root:[   52] Training loss: 0.00558677, Validation loss: 0.00917413, Gradient norm: 0.25656553
INFO:root:[   53] Training loss: 0.00520346, Validation loss: 0.00875054, Gradient norm: 0.26182260
INFO:root:[   54] Training loss: 0.00540717, Validation loss: 0.01057005, Gradient norm: 0.29088700
INFO:root:[   55] Training loss: 0.00523508, Validation loss: 0.00941158, Gradient norm: 0.22218304
INFO:root:[   56] Training loss: 0.00491493, Validation loss: 0.00999586, Gradient norm: 0.25216863
INFO:root:[   57] Training loss: 0.00567538, Validation loss: 0.00911811, Gradient norm: 0.28452848
INFO:root:[   58] Training loss: 0.00525804, Validation loss: 0.00991347, Gradient norm: 0.23162156
INFO:root:[   59] Training loss: 0.00518204, Validation loss: 0.01005226, Gradient norm: 0.27950027
INFO:root:[   60] Training loss: 0.00532770, Validation loss: 0.00980787, Gradient norm: 0.31916162
INFO:root:[   61] Training loss: 0.00491336, Validation loss: 0.01001426, Gradient norm: 0.23024636
INFO:root:[   62] Training loss: 0.00508655, Validation loss: 0.01103733, Gradient norm: 0.27387883
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 2147.641s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00702
INFO:root:EnergyScoreTrain: 0.00496
INFO:root:CoverageTrain: 0.95871
INFO:root:IntervalWidthTrain: 0.03598
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0119
INFO:root:EnergyScoreValidation: 0.009
INFO:root:CoverageValidation: 0.86509
INFO:root:IntervalWidthValidation: 0.03559
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01196
INFO:root:EnergyScoreTest: 0.00904
INFO:root:CoverageTest: 0.86194
INFO:root:IntervalWidthTest: 0.03581
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05203607, Validation loss: 0.02197479, Gradient norm: 0.70454912
INFO:root:[    2] Training loss: 0.02340704, Validation loss: 0.01689711, Gradient norm: 0.61669657
INFO:root:[    3] Training loss: 0.01825492, Validation loss: 0.02209168, Gradient norm: 0.52182461
INFO:root:[    4] Training loss: 0.01687488, Validation loss: 0.02542531, Gradient norm: 0.50427994
INFO:root:[    5] Training loss: 0.01410850, Validation loss: 0.01130455, Gradient norm: 0.41034571
INFO:root:[    6] Training loss: 0.01534279, Validation loss: 0.01169998, Gradient norm: 0.51163307
INFO:root:[    7] Training loss: 0.01307770, Validation loss: 0.01170873, Gradient norm: 0.40784059
INFO:root:[    8] Training loss: 0.01319208, Validation loss: 0.01501346, Gradient norm: 0.44122943
INFO:root:[    9] Training loss: 0.01200376, Validation loss: 0.01870465, Gradient norm: 0.37465541
INFO:root:[   10] Training loss: 0.01087603, Validation loss: 0.01038587, Gradient norm: 0.23738527
INFO:root:[   11] Training loss: 0.01292950, Validation loss: 0.01419609, Gradient norm: 0.46910800
INFO:root:[   12] Training loss: 0.01179979, Validation loss: 0.01327812, Gradient norm: 0.36454782
INFO:root:[   13] Training loss: 0.01191891, Validation loss: 0.01059980, Gradient norm: 0.41947377
INFO:root:[   14] Training loss: 0.01042447, Validation loss: 0.01112135, Gradient norm: 0.31964730
INFO:root:[   15] Training loss: 0.01012096, Validation loss: 0.00998252, Gradient norm: 0.28294674
INFO:root:[   16] Training loss: 0.01012001, Validation loss: 0.01011845, Gradient norm: 0.32522074
INFO:root:[   17] Training loss: 0.00965119, Validation loss: 0.01282701, Gradient norm: 0.31032906
INFO:root:[   18] Training loss: 0.01005663, Validation loss: 0.00987763, Gradient norm: 0.32217559
INFO:root:[   19] Training loss: 0.00906151, Validation loss: 0.00902894, Gradient norm: 0.27575243
INFO:root:[   20] Training loss: 0.00930297, Validation loss: 0.01003333, Gradient norm: 0.29359747
INFO:root:[   21] Training loss: 0.00918274, Validation loss: 0.00862567, Gradient norm: 0.28832851
INFO:root:[   22] Training loss: 0.00897633, Validation loss: 0.00951576, Gradient norm: 0.29801368
INFO:root:[   23] Training loss: 0.00978708, Validation loss: 0.01022490, Gradient norm: 0.34959410
INFO:root:[   24] Training loss: 0.00854565, Validation loss: 0.01028929, Gradient norm: 0.27679236
INFO:root:[   25] Training loss: 0.00843976, Validation loss: 0.00951092, Gradient norm: 0.23483521
INFO:root:[   26] Training loss: 0.00826755, Validation loss: 0.00970203, Gradient norm: 0.27383377
INFO:root:[   27] Training loss: 0.00879562, Validation loss: 0.00963709, Gradient norm: 0.30695300
INFO:root:[   28] Training loss: 0.00833694, Validation loss: 0.00974854, Gradient norm: 0.27860499
INFO:root:[   29] Training loss: 0.00769318, Validation loss: 0.01042572, Gradient norm: 0.23696147
INFO:root:[   30] Training loss: 0.00838530, Validation loss: 0.00986917, Gradient norm: 0.30974878
INFO:root:[   31] Training loss: 0.00791284, Validation loss: 0.01039810, Gradient norm: 0.26543280
INFO:root:[   32] Training loss: 0.00761370, Validation loss: 0.00881215, Gradient norm: 0.24984720
INFO:root:[   33] Training loss: 0.00839504, Validation loss: 0.00936390, Gradient norm: 0.27792594
INFO:root:[   34] Training loss: 0.00716019, Validation loss: 0.00990890, Gradient norm: 0.23782549
INFO:root:[   35] Training loss: 0.00766610, Validation loss: 0.00987068, Gradient norm: 0.27941195
INFO:root:[   36] Training loss: 0.00750069, Validation loss: 0.00866212, Gradient norm: 0.25745143
INFO:root:[   37] Training loss: 0.00785203, Validation loss: 0.00959016, Gradient norm: 0.30496952
INFO:root:[   38] Training loss: 0.00646414, Validation loss: 0.00977406, Gradient norm: 0.18610061
INFO:root:[   39] Training loss: 0.00787593, Validation loss: 0.00891101, Gradient norm: 0.26754822
INFO:root:[   40] Training loss: 0.00722628, Validation loss: 0.01015041, Gradient norm: 0.29287150
INFO:root:[   41] Training loss: 0.00781486, Validation loss: 0.00888204, Gradient norm: 0.32535458
INFO:root:[   42] Training loss: 0.00700013, Validation loss: 0.00966956, Gradient norm: 0.22147034
INFO:root:[   43] Training loss: 0.00659918, Validation loss: 0.00828803, Gradient norm: 0.23585206
INFO:root:[   44] Training loss: 0.00676748, Validation loss: 0.00983924, Gradient norm: 0.25906590
INFO:root:[   45] Training loss: 0.00656533, Validation loss: 0.00917747, Gradient norm: 0.22759102
INFO:root:[   46] Training loss: 0.00686737, Validation loss: 0.01035292, Gradient norm: 0.25667052
INFO:root:[   47] Training loss: 0.00668876, Validation loss: 0.00946838, Gradient norm: 0.23351808
INFO:root:[   48] Training loss: 0.00682192, Validation loss: 0.00972018, Gradient norm: 0.29767754
INFO:root:[   49] Training loss: 0.00617969, Validation loss: 0.00831326, Gradient norm: 0.21293639
INFO:root:[   50] Training loss: 0.00645105, Validation loss: 0.00988418, Gradient norm: 0.22773311
INFO:root:[   51] Training loss: 0.00684214, Validation loss: 0.01119820, Gradient norm: 0.27784088
INFO:root:[   52] Training loss: 0.00612659, Validation loss: 0.00958334, Gradient norm: 0.18357783
INFO:root:[   53] Training loss: 0.00629718, Validation loss: 0.00975578, Gradient norm: 0.23363459
INFO:root:[   54] Training loss: 0.00684890, Validation loss: 0.00965122, Gradient norm: 0.26621171
INFO:root:[   55] Training loss: 0.00661318, Validation loss: 0.01000556, Gradient norm: 0.28326776
INFO:root:[   56] Training loss: 0.00653505, Validation loss: 0.00888784, Gradient norm: 0.25371598
INFO:root:[   57] Training loss: 0.00629404, Validation loss: 0.00884042, Gradient norm: 0.23476911
INFO:root:[   58] Training loss: 0.00604909, Validation loss: 0.01066384, Gradient norm: 0.23618235
INFO:root:[   59] Training loss: 0.00599544, Validation loss: 0.00824729, Gradient norm: 0.21231081
INFO:root:[   60] Training loss: 0.00624681, Validation loss: 0.01114057, Gradient norm: 0.27083122
INFO:root:[   61] Training loss: 0.00589373, Validation loss: 0.00885195, Gradient norm: 0.21711233
INFO:root:[   62] Training loss: 0.00575088, Validation loss: 0.01145285, Gradient norm: 0.20107261
INFO:root:[   63] Training loss: 0.00659417, Validation loss: 0.00834767, Gradient norm: 0.24173674
INFO:root:[   64] Training loss: 0.00615930, Validation loss: 0.00856022, Gradient norm: 0.25050229
INFO:root:[   65] Training loss: 0.00592689, Validation loss: 0.01048133, Gradient norm: 0.22146706
INFO:root:[   66] Training loss: 0.00577355, Validation loss: 0.00868190, Gradient norm: 0.24286129
INFO:root:[   67] Training loss: 0.00586561, Validation loss: 0.00995335, Gradient norm: 0.24441431
INFO:root:[   68] Training loss: 0.00615091, Validation loss: 0.00870260, Gradient norm: 0.26789838
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 2262.105s.
INFO:root:Emptying the cuda cache took 0.022s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0063
INFO:root:EnergyScoreTrain: 0.00505
INFO:root:CoverageTrain: 0.99599
INFO:root:IntervalWidthTrain: 0.04507
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01117
INFO:root:EnergyScoreValidation: 0.00826
INFO:root:CoverageValidation: 0.95765
INFO:root:IntervalWidthValidation: 0.04478
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01124
INFO:root:EnergyScoreTest: 0.00828
INFO:root:CoverageTest: 0.9541
INFO:root:IntervalWidthTest: 0.04488
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04559376, Validation loss: 0.02507100, Gradient norm: 0.65093285
INFO:root:[    2] Training loss: 0.02102530, Validation loss: 0.02468818, Gradient norm: 0.43083610
INFO:root:[    3] Training loss: 0.01919855, Validation loss: 0.01675736, Gradient norm: 0.43678844
INFO:root:[    4] Training loss: 0.01826558, Validation loss: 0.01555597, Gradient norm: 0.48009622
INFO:root:[    5] Training loss: 0.01588311, Validation loss: 0.01330020, Gradient norm: 0.42159643
INFO:root:[    6] Training loss: 0.01347626, Validation loss: 0.01629123, Gradient norm: 0.33944482
INFO:root:[    7] Training loss: 0.01378237, Validation loss: 0.01362646, Gradient norm: 0.37925444
INFO:root:[    8] Training loss: 0.01271777, Validation loss: 0.01287287, Gradient norm: 0.32061499
INFO:root:[    9] Training loss: 0.01328014, Validation loss: 0.01107240, Gradient norm: 0.37257313
INFO:root:[   10] Training loss: 0.01131711, Validation loss: 0.01372349, Gradient norm: 0.27618000
INFO:root:[   11] Training loss: 0.01150411, Validation loss: 0.01164432, Gradient norm: 0.29653452
INFO:root:[   12] Training loss: 0.01226250, Validation loss: 0.01187255, Gradient norm: 0.37363412
INFO:root:[   13] Training loss: 0.01085810, Validation loss: 0.01027073, Gradient norm: 0.24697137
INFO:root:[   14] Training loss: 0.01115872, Validation loss: 0.01032146, Gradient norm: 0.27586700
INFO:root:[   15] Training loss: 0.01013857, Validation loss: 0.01053655, Gradient norm: 0.25079569
INFO:root:[   16] Training loss: 0.01072196, Validation loss: 0.00996388, Gradient norm: 0.29103314
INFO:root:[   17] Training loss: 0.00992468, Validation loss: 0.01014925, Gradient norm: 0.27281822
INFO:root:[   18] Training loss: 0.01045051, Validation loss: 0.01352342, Gradient norm: 0.28972282
INFO:root:[   19] Training loss: 0.00988547, Validation loss: 0.01004605, Gradient norm: 0.29660911
INFO:root:[   20] Training loss: 0.01009532, Validation loss: 0.01235782, Gradient norm: 0.30429535
INFO:root:[   21] Training loss: 0.00959039, Validation loss: 0.01008031, Gradient norm: 0.26297920
INFO:root:[   22] Training loss: 0.00999002, Validation loss: 0.01006164, Gradient norm: 0.23411974
INFO:root:[   23] Training loss: 0.00937412, Validation loss: 0.00939438, Gradient norm: 0.27349363
INFO:root:[   24] Training loss: 0.00853008, Validation loss: 0.00952804, Gradient norm: 0.19983723
INFO:root:[   25] Training loss: 0.00908053, Validation loss: 0.00896959, Gradient norm: 0.25542453
INFO:root:[   26] Training loss: 0.00886510, Validation loss: 0.00943167, Gradient norm: 0.26695356
INFO:root:[   27] Training loss: 0.00843368, Validation loss: 0.00945847, Gradient norm: 0.20845986
INFO:root:[   28] Training loss: 0.00788440, Validation loss: 0.00887422, Gradient norm: 0.21640543
INFO:root:[   29] Training loss: 0.00896336, Validation loss: 0.00941752, Gradient norm: 0.29896914
INFO:root:[   30] Training loss: 0.00727158, Validation loss: 0.01055308, Gradient norm: 0.14260553
INFO:root:[   31] Training loss: 0.00841175, Validation loss: 0.01114174, Gradient norm: 0.25853544
INFO:root:[   32] Training loss: 0.00895646, Validation loss: 0.00926943, Gradient norm: 0.28543735
INFO:root:[   33] Training loss: 0.00724126, Validation loss: 0.00884896, Gradient norm: 0.17653577
INFO:root:[   34] Training loss: 0.00809441, Validation loss: 0.01287870, Gradient norm: 0.26204592
INFO:root:[   35] Training loss: 0.00886306, Validation loss: 0.01068209, Gradient norm: 0.30112933
INFO:root:[   36] Training loss: 0.00848032, Validation loss: 0.00912703, Gradient norm: 0.26690245
INFO:root:[   37] Training loss: 0.00710837, Validation loss: 0.00907434, Gradient norm: 0.18641474
INFO:root:[   38] Training loss: 0.00759794, Validation loss: 0.00980196, Gradient norm: 0.22142653
INFO:root:[   39] Training loss: 0.00703782, Validation loss: 0.00865653, Gradient norm: 0.20361046
INFO:root:[   40] Training loss: 0.00678037, Validation loss: 0.01067798, Gradient norm: 0.19266411
INFO:root:[   41] Training loss: 0.00812470, Validation loss: 0.01165007, Gradient norm: 0.29031592
INFO:root:[   42] Training loss: 0.00770698, Validation loss: 0.00940809, Gradient norm: 0.23832784
INFO:root:[   43] Training loss: 0.00682005, Validation loss: 0.00932215, Gradient norm: 0.18859012
INFO:root:[   44] Training loss: 0.00668180, Validation loss: 0.01040022, Gradient norm: 0.20109118
INFO:root:[   45] Training loss: 0.00775992, Validation loss: 0.00966958, Gradient norm: 0.26358248
INFO:root:[   46] Training loss: 0.00732849, Validation loss: 0.00991360, Gradient norm: 0.26594499
INFO:root:[   47] Training loss: 0.00699783, Validation loss: 0.01084143, Gradient norm: 0.24582663
INFO:root:[   48] Training loss: 0.00693341, Validation loss: 0.00966805, Gradient norm: 0.26015193
INFO:root:[   49] Training loss: 0.00714383, Validation loss: 0.00992529, Gradient norm: 0.24582481
INFO:root:[   50] Training loss: 0.00676725, Validation loss: 0.00964652, Gradient norm: 0.23060784
INFO:root:[   51] Training loss: 0.00708150, Validation loss: 0.00961798, Gradient norm: 0.23924994
INFO:root:[   52] Training loss: 0.00698471, Validation loss: 0.00954006, Gradient norm: 0.25561988
INFO:root:[   53] Training loss: 0.00688822, Validation loss: 0.00944219, Gradient norm: 0.23884149
INFO:root:[   54] Training loss: 0.00672069, Validation loss: 0.00907172, Gradient norm: 0.19258530
INFO:root:[   55] Training loss: 0.00651784, Validation loss: 0.00862553, Gradient norm: 0.19682544
INFO:root:[   56] Training loss: 0.00642463, Validation loss: 0.00993837, Gradient norm: 0.22584574
INFO:root:[   57] Training loss: 0.00657837, Validation loss: 0.01011112, Gradient norm: 0.25691512
INFO:root:[   58] Training loss: 0.00632425, Validation loss: 0.00976889, Gradient norm: 0.23481154
INFO:root:[   59] Training loss: 0.00681326, Validation loss: 0.00895905, Gradient norm: 0.26568319
INFO:root:[   60] Training loss: 0.00685517, Validation loss: 0.00968250, Gradient norm: 0.28003166
INFO:root:[   61] Training loss: 0.00624417, Validation loss: 0.01061727, Gradient norm: 0.18890389
INFO:root:[   62] Training loss: 0.00646782, Validation loss: 0.01061761, Gradient norm: 0.24284170
INFO:root:[   63] Training loss: 0.00696983, Validation loss: 0.00943440, Gradient norm: 0.25567841
INFO:root:[   64] Training loss: 0.00644093, Validation loss: 0.00938302, Gradient norm: 0.23079462
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 2109.077s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0067
INFO:root:EnergyScoreTrain: 0.00557
INFO:root:CoverageTrain: 0.99678
INFO:root:IntervalWidthTrain: 0.0506
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01155
INFO:root:EnergyScoreValidation: 0.00856
INFO:root:CoverageValidation: 0.97169
INFO:root:IntervalWidthValidation: 0.05016
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01142
INFO:root:EnergyScoreTest: 0.00845
INFO:root:CoverageTest: 0.9701
INFO:root:IntervalWidthTest: 0.05035
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04524421, Validation loss: 0.02359016, Gradient norm: 0.47602750
INFO:root:[    2] Training loss: 0.02256222, Validation loss: 0.01926892, Gradient norm: 0.37827514
INFO:root:[    3] Training loss: 0.01964903, Validation loss: 0.02168890, Gradient norm: 0.38359806
INFO:root:[    4] Training loss: 0.01675359, Validation loss: 0.01513888, Gradient norm: 0.32334599
INFO:root:[    5] Training loss: 0.01626166, Validation loss: 0.01608825, Gradient norm: 0.34893396
INFO:root:[    6] Training loss: 0.01427027, Validation loss: 0.01487405, Gradient norm: 0.29342678
INFO:root:[    7] Training loss: 0.01381130, Validation loss: 0.01682799, Gradient norm: 0.31863778
INFO:root:[    8] Training loss: 0.01503664, Validation loss: 0.01187548, Gradient norm: 0.36089579
INFO:root:[    9] Training loss: 0.01281754, Validation loss: 0.01395511, Gradient norm: 0.27610231
INFO:root:[   10] Training loss: 0.01171702, Validation loss: 0.01445552, Gradient norm: 0.25106538
INFO:root:[   11] Training loss: 0.01271065, Validation loss: 0.01114242, Gradient norm: 0.32049026
INFO:root:[   12] Training loss: 0.01175640, Validation loss: 0.01420796, Gradient norm: 0.27591113
INFO:root:[   13] Training loss: 0.01273918, Validation loss: 0.01073358, Gradient norm: 0.33236065
INFO:root:[   14] Training loss: 0.01169548, Validation loss: 0.01146538, Gradient norm: 0.30418448
INFO:root:[   15] Training loss: 0.01085738, Validation loss: 0.01202102, Gradient norm: 0.25071429
INFO:root:[   16] Training loss: 0.01112624, Validation loss: 0.01162754, Gradient norm: 0.29795520
INFO:root:[   17] Training loss: 0.01151101, Validation loss: 0.01162393, Gradient norm: 0.31590954
INFO:root:[   18] Training loss: 0.01053067, Validation loss: 0.01183901, Gradient norm: 0.23664080
INFO:root:[   19] Training loss: 0.01012554, Validation loss: 0.01029103, Gradient norm: 0.25590243
INFO:root:[   20] Training loss: 0.01012424, Validation loss: 0.01185706, Gradient norm: 0.25792934
INFO:root:[   21] Training loss: 0.01026674, Validation loss: 0.01274693, Gradient norm: 0.26662340
INFO:root:[   22] Training loss: 0.01070582, Validation loss: 0.01028043, Gradient norm: 0.33021505
INFO:root:[   23] Training loss: 0.00965039, Validation loss: 0.01040271, Gradient norm: 0.23097658
INFO:root:[   24] Training loss: 0.01010010, Validation loss: 0.00995252, Gradient norm: 0.28254047
INFO:root:[   25] Training loss: 0.00987121, Validation loss: 0.01056546, Gradient norm: 0.27018340
INFO:root:[   26] Training loss: 0.00968542, Validation loss: 0.01158302, Gradient norm: 0.27438674
INFO:root:[   27] Training loss: 0.00965603, Validation loss: 0.01004470, Gradient norm: 0.24926581
INFO:root:[   28] Training loss: 0.00938615, Validation loss: 0.01044466, Gradient norm: 0.23492500
INFO:root:[   29] Training loss: 0.00905682, Validation loss: 0.01097726, Gradient norm: 0.24301111
INFO:root:[   30] Training loss: 0.00851660, Validation loss: 0.01037153, Gradient norm: 0.22062905
INFO:root:[   31] Training loss: 0.00871680, Validation loss: 0.00927086, Gradient norm: 0.19438053
INFO:root:[   32] Training loss: 0.00815861, Validation loss: 0.00989014, Gradient norm: 0.23030716
INFO:root:[   33] Training loss: 0.00865870, Validation loss: 0.01025694, Gradient norm: 0.24264739
INFO:root:[   34] Training loss: 0.00844847, Validation loss: 0.00916408, Gradient norm: 0.24164757
INFO:root:[   35] Training loss: 0.00821472, Validation loss: 0.00958748, Gradient norm: 0.24819864
INFO:root:[   36] Training loss: 0.00867232, Validation loss: 0.00955341, Gradient norm: 0.26535667
INFO:root:[   37] Training loss: 0.00866544, Validation loss: 0.00870352, Gradient norm: 0.23104596
INFO:root:[   38] Training loss: 0.00797504, Validation loss: 0.00990631, Gradient norm: 0.17245119
INFO:root:[   39] Training loss: 0.00802007, Validation loss: 0.00924562, Gradient norm: 0.21129409
INFO:root:[   40] Training loss: 0.00851161, Validation loss: 0.00877632, Gradient norm: 0.30612767
INFO:root:[   41] Training loss: 0.00778314, Validation loss: 0.01054055, Gradient norm: 0.24807146
INFO:root:[   42] Training loss: 0.00719686, Validation loss: 0.00921551, Gradient norm: 0.20405063
INFO:root:[   43] Training loss: 0.00802040, Validation loss: 0.00867553, Gradient norm: 0.26219776
INFO:root:[   44] Training loss: 0.00793137, Validation loss: 0.00934585, Gradient norm: 0.26190606
INFO:root:[   45] Training loss: 0.00779196, Validation loss: 0.00888777, Gradient norm: 0.20893552
INFO:root:[   46] Training loss: 0.00756535, Validation loss: 0.00951065, Gradient norm: 0.24342615
INFO:root:[   47] Training loss: 0.00838008, Validation loss: 0.00883399, Gradient norm: 0.28215645
INFO:root:[   48] Training loss: 0.00719352, Validation loss: 0.00957095, Gradient norm: 0.21454003
INFO:root:[   49] Training loss: 0.00732184, Validation loss: 0.00896630, Gradient norm: 0.21858982
INFO:root:[   50] Training loss: 0.00784653, Validation loss: 0.00906267, Gradient norm: 0.23704334
INFO:root:[   51] Training loss: 0.00716548, Validation loss: 0.01118747, Gradient norm: 0.22085612
INFO:root:[   52] Training loss: 0.00765371, Validation loss: 0.00953572, Gradient norm: 0.28480743
INFO:root:[   53] Training loss: 0.00687522, Validation loss: 0.01175486, Gradient norm: 0.21902912
INFO:root:[   54] Training loss: 0.00764253, Validation loss: 0.00926995, Gradient norm: 0.24894550
INFO:root:[   55] Training loss: 0.00727852, Validation loss: 0.00943853, Gradient norm: 0.25282746
INFO:root:[   56] Training loss: 0.00709652, Validation loss: 0.01087366, Gradient norm: 0.26650451
INFO:root:[   57] Training loss: 0.00710763, Validation loss: 0.01059019, Gradient norm: 0.22842430
INFO:root:[   58] Training loss: 0.00707366, Validation loss: 0.00971981, Gradient norm: 0.21719542
INFO:root:[   59] Training loss: 0.00741159, Validation loss: 0.01106964, Gradient norm: 0.29794360
INFO:root:[   60] Training loss: 0.00691591, Validation loss: 0.00896066, Gradient norm: 0.19527124
INFO:root:[   61] Training loss: 0.00735233, Validation loss: 0.01052843, Gradient norm: 0.29889430
INFO:root:[   62] Training loss: 0.00696355, Validation loss: 0.00910579, Gradient norm: 0.24068311
INFO:root:[   63] Training loss: 0.00657795, Validation loss: 0.00901086, Gradient norm: 0.16862817
INFO:root:[   64] Training loss: 0.00694072, Validation loss: 0.00902915, Gradient norm: 0.26617938
INFO:root:[   65] Training loss: 0.00645913, Validation loss: 0.01053037, Gradient norm: 0.18848881
INFO:root:[   66] Training loss: 0.00652716, Validation loss: 0.00978817, Gradient norm: 0.23050609
INFO:root:[   67] Training loss: 0.00681898, Validation loss: 0.00961337, Gradient norm: 0.23310540
INFO:root:[   68] Training loss: 0.00712368, Validation loss: 0.00999684, Gradient norm: 0.25457468
INFO:root:[   69] Training loss: 0.00656624, Validation loss: 0.01002218, Gradient norm: 0.20247592
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2276.735s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00848
INFO:root:EnergyScoreTrain: 0.00669
INFO:root:CoverageTrain: 0.99648
INFO:root:IntervalWidthTrain: 0.05696
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01189
INFO:root:EnergyScoreValidation: 0.0088
INFO:root:CoverageValidation: 0.9793
INFO:root:IntervalWidthValidation: 0.05655
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01202
INFO:root:EnergyScoreTest: 0.00889
INFO:root:CoverageTest: 0.97741
INFO:root:IntervalWidthTest: 0.05668
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04603316, Validation loss: 0.02451562, Gradient norm: 0.49025527
INFO:root:[    2] Training loss: 0.02275103, Validation loss: 0.02779755, Gradient norm: 0.39055598
INFO:root:[    3] Training loss: 0.01991053, Validation loss: 0.01852984, Gradient norm: 0.39482933
INFO:root:[    4] Training loss: 0.01731588, Validation loss: 0.01759683, Gradient norm: 0.32954303
INFO:root:[    5] Training loss: 0.01625162, Validation loss: 0.01425099, Gradient norm: 0.34399512
INFO:root:[    6] Training loss: 0.01498831, Validation loss: 0.01583536, Gradient norm: 0.32322132
INFO:root:[    7] Training loss: 0.01436648, Validation loss: 0.01999722, Gradient norm: 0.32402821
INFO:root:[    8] Training loss: 0.01383477, Validation loss: 0.01467482, Gradient norm: 0.26761779
INFO:root:[    9] Training loss: 0.01293989, Validation loss: 0.01221986, Gradient norm: 0.28132968
INFO:root:[   10] Training loss: 0.01279412, Validation loss: 0.01500310, Gradient norm: 0.28509360
INFO:root:[   11] Training loss: 0.01273941, Validation loss: 0.01097503, Gradient norm: 0.25903704
INFO:root:[   12] Training loss: 0.01054658, Validation loss: 0.01082380, Gradient norm: 0.18332438
INFO:root:[   13] Training loss: 0.01247976, Validation loss: 0.01145238, Gradient norm: 0.29951280
INFO:root:[   14] Training loss: 0.01063912, Validation loss: 0.01167586, Gradient norm: 0.20664314
INFO:root:[   15] Training loss: 0.01067001, Validation loss: 0.01056123, Gradient norm: 0.23398962
INFO:root:[   16] Training loss: 0.01198632, Validation loss: 0.01191817, Gradient norm: 0.30801494
INFO:root:[   17] Training loss: 0.01026894, Validation loss: 0.01278303, Gradient norm: 0.22563793
INFO:root:[   18] Training loss: 0.01054111, Validation loss: 0.01264018, Gradient norm: 0.22223073
INFO:root:[   19] Training loss: 0.01021495, Validation loss: 0.01060557, Gradient norm: 0.20566352
INFO:root:[   20] Training loss: 0.01016602, Validation loss: 0.01247837, Gradient norm: 0.23430667
INFO:root:[   21] Training loss: 0.01199615, Validation loss: 0.00985383, Gradient norm: 0.34654497
INFO:root:[   22] Training loss: 0.00953986, Validation loss: 0.00979574, Gradient norm: 0.20019824
INFO:root:[   23] Training loss: 0.00984533, Validation loss: 0.01047272, Gradient norm: 0.22551979
INFO:root:[   24] Training loss: 0.00910369, Validation loss: 0.00976933, Gradient norm: 0.17915591
INFO:root:[   25] Training loss: 0.00985074, Validation loss: 0.01044443, Gradient norm: 0.25740148
INFO:root:[   26] Training loss: 0.00960463, Validation loss: 0.01010421, Gradient norm: 0.24223885
INFO:root:[   27] Training loss: 0.00975419, Validation loss: 0.01064437, Gradient norm: 0.21852427
INFO:root:[   28] Training loss: 0.00879783, Validation loss: 0.00959053, Gradient norm: 0.20463680
INFO:root:[   29] Training loss: 0.00935602, Validation loss: 0.00926891, Gradient norm: 0.23278224
INFO:root:[   30] Training loss: 0.00839109, Validation loss: 0.01074378, Gradient norm: 0.16029247
INFO:root:[   31] Training loss: 0.00908718, Validation loss: 0.01060800, Gradient norm: 0.25535703
INFO:root:[   32] Training loss: 0.00922095, Validation loss: 0.00983720, Gradient norm: 0.23214255
INFO:root:[   33] Training loss: 0.00893355, Validation loss: 0.01402678, Gradient norm: 0.23722773
INFO:root:[   34] Training loss: 0.00891558, Validation loss: 0.01042300, Gradient norm: 0.20650145
INFO:root:[   35] Training loss: 0.00915952, Validation loss: 0.00920555, Gradient norm: 0.25675945
INFO:root:[   36] Training loss: 0.00815027, Validation loss: 0.00893784, Gradient norm: 0.19310166
INFO:root:[   37] Training loss: 0.00915237, Validation loss: 0.01006395, Gradient norm: 0.25663457
INFO:root:[   38] Training loss: 0.00786039, Validation loss: 0.01030192, Gradient norm: 0.17286462
INFO:root:[   39] Training loss: 0.00879899, Validation loss: 0.00908587, Gradient norm: 0.21442818
INFO:root:[   40] Training loss: 0.00779769, Validation loss: 0.00948399, Gradient norm: 0.19343898
INFO:root:[   41] Training loss: 0.00794339, Validation loss: 0.00987407, Gradient norm: 0.20723711
INFO:root:[   42] Training loss: 0.00797259, Validation loss: 0.00929070, Gradient norm: 0.23746947
INFO:root:[   43] Training loss: 0.00800592, Validation loss: 0.01008299, Gradient norm: 0.21259488
INFO:root:[   44] Training loss: 0.00811859, Validation loss: 0.00917390, Gradient norm: 0.20764812
INFO:root:[   45] Training loss: 0.00788288, Validation loss: 0.00904596, Gradient norm: 0.20433613
INFO:root:[   46] Training loss: 0.00787051, Validation loss: 0.00975534, Gradient norm: 0.22050030
INFO:root:[   47] Training loss: 0.00746285, Validation loss: 0.00922227, Gradient norm: 0.21072756
INFO:root:[   48] Training loss: 0.00769056, Validation loss: 0.00973387, Gradient norm: 0.22759306
INFO:root:[   49] Training loss: 0.00802323, Validation loss: 0.00962071, Gradient norm: 0.25524169
INFO:root:[   50] Training loss: 0.00750895, Validation loss: 0.01198885, Gradient norm: 0.22869882
INFO:root:[   51] Training loss: 0.00747189, Validation loss: 0.00919995, Gradient norm: 0.21458182
INFO:root:[   52] Training loss: 0.00752820, Validation loss: 0.00950150, Gradient norm: 0.21583520
INFO:root:[   53] Training loss: 0.00744457, Validation loss: 0.00971815, Gradient norm: 0.21466282
INFO:root:[   54] Training loss: 0.00776401, Validation loss: 0.00920489, Gradient norm: 0.25639428
INFO:root:[   55] Training loss: 0.00785363, Validation loss: 0.00989979, Gradient norm: 0.23446856
INFO:root:[   56] Training loss: 0.00735055, Validation loss: 0.01170660, Gradient norm: 0.22800940
INFO:root:[   57] Training loss: 0.00755692, Validation loss: 0.00885439, Gradient norm: 0.23351382
INFO:root:[   58] Training loss: 0.00690101, Validation loss: 0.00987186, Gradient norm: 0.19911153
INFO:root:[   59] Training loss: 0.00732731, Validation loss: 0.00864145, Gradient norm: 0.22957933
INFO:root:[   60] Training loss: 0.00742570, Validation loss: 0.00975343, Gradient norm: 0.26828420
INFO:root:[   61] Training loss: 0.00730524, Validation loss: 0.00951662, Gradient norm: 0.21050781
INFO:root:[   62] Training loss: 0.00690192, Validation loss: 0.00855572, Gradient norm: 0.25774704
INFO:root:[   63] Training loss: 0.00690122, Validation loss: 0.00889944, Gradient norm: 0.19461140
INFO:root:[   64] Training loss: 0.00748263, Validation loss: 0.00976264, Gradient norm: 0.26739865
INFO:root:[   65] Training loss: 0.00684920, Validation loss: 0.00926227, Gradient norm: 0.19820005
INFO:root:[   66] Training loss: 0.00702427, Validation loss: 0.01195193, Gradient norm: 0.23114479
INFO:root:[   67] Training loss: 0.00738670, Validation loss: 0.00995475, Gradient norm: 0.22254654
INFO:root:[   68] Training loss: 0.00654768, Validation loss: 0.00859659, Gradient norm: 0.17697003
INFO:root:[   69] Training loss: 0.00702850, Validation loss: 0.00893681, Gradient norm: 0.22519434
INFO:root:[   70] Training loss: 0.00661891, Validation loss: 0.01001171, Gradient norm: 0.19877071
INFO:root:[   71] Training loss: 0.00682603, Validation loss: 0.00954725, Gradient norm: 0.24918698
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 2359.307s.
INFO:root:Emptying the cuda cache took 0.014s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00718
INFO:root:EnergyScoreTrain: 0.00606
INFO:root:CoverageTrain: 0.99787
INFO:root:IntervalWidthTrain: 0.0563
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01131
INFO:root:EnergyScoreValidation: 0.00849
INFO:root:CoverageValidation: 0.98276
INFO:root:IntervalWidthValidation: 0.05566
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01132
INFO:root:EnergyScoreTest: 0.00846
INFO:root:CoverageTest: 0.98267
INFO:root:IntervalWidthTest: 0.0561
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04330845, Validation loss: 0.03747767, Gradient norm: 0.48233053
INFO:root:[    2] Training loss: 0.02487114, Validation loss: 0.03242515, Gradient norm: 0.37973209
INFO:root:[    3] Training loss: 0.02090992, Validation loss: 0.01632216, Gradient norm: 0.32663632
INFO:root:[    4] Training loss: 0.01845474, Validation loss: 0.01486809, Gradient norm: 0.32809751
INFO:root:[    5] Training loss: 0.01668702, Validation loss: 0.01450026, Gradient norm: 0.29919721
INFO:root:[    6] Training loss: 0.01552685, Validation loss: 0.01458917, Gradient norm: 0.29189711
INFO:root:[    7] Training loss: 0.01450149, Validation loss: 0.01869202, Gradient norm: 0.28791681
INFO:root:[    8] Training loss: 0.01451925, Validation loss: 0.01262184, Gradient norm: 0.26578899
INFO:root:[    9] Training loss: 0.01379130, Validation loss: 0.01543236, Gradient norm: 0.25781050
INFO:root:[   10] Training loss: 0.01310282, Validation loss: 0.01317050, Gradient norm: 0.29079131
INFO:root:[   11] Training loss: 0.01325812, Validation loss: 0.01226147, Gradient norm: 0.28799050
INFO:root:[   12] Training loss: 0.01280453, Validation loss: 0.01222340, Gradient norm: 0.28162817
INFO:root:[   13] Training loss: 0.01220564, Validation loss: 0.01330151, Gradient norm: 0.26286045
INFO:root:[   14] Training loss: 0.01199584, Validation loss: 0.01187536, Gradient norm: 0.25803845
INFO:root:[   15] Training loss: 0.01214143, Validation loss: 0.01099153, Gradient norm: 0.24181139
INFO:root:[   16] Training loss: 0.01078349, Validation loss: 0.01064492, Gradient norm: 0.21012627
INFO:root:[   17] Training loss: 0.01062937, Validation loss: 0.01699984, Gradient norm: 0.19206469
INFO:root:[   18] Training loss: 0.01248708, Validation loss: 0.01126596, Gradient norm: 0.29798447
INFO:root:[   19] Training loss: 0.01084105, Validation loss: 0.01050314, Gradient norm: 0.24435342
INFO:root:[   20] Training loss: 0.01043983, Validation loss: 0.01090107, Gradient norm: 0.22078812
INFO:root:[   21] Training loss: 0.01175077, Validation loss: 0.01143950, Gradient norm: 0.30962146
INFO:root:[   22] Training loss: 0.01022676, Validation loss: 0.01079256, Gradient norm: 0.21916956
INFO:root:[   23] Training loss: 0.01016317, Validation loss: 0.01125147, Gradient norm: 0.20452972
INFO:root:[   24] Training loss: 0.01037330, Validation loss: 0.01126843, Gradient norm: 0.22564887
INFO:root:[   25] Training loss: 0.00988166, Validation loss: 0.01065329, Gradient norm: 0.19010334
INFO:root:[   26] Training loss: 0.01009606, Validation loss: 0.01096117, Gradient norm: 0.19240582
INFO:root:[   27] Training loss: 0.01021575, Validation loss: 0.01016902, Gradient norm: 0.24176543
INFO:root:[   28] Training loss: 0.01014017, Validation loss: 0.01182618, Gradient norm: 0.24855778
INFO:root:[   29] Training loss: 0.00977339, Validation loss: 0.01009635, Gradient norm: 0.21884132
INFO:root:[   30] Training loss: 0.00885197, Validation loss: 0.00946516, Gradient norm: 0.18070004
INFO:root:[   31] Training loss: 0.00967263, Validation loss: 0.01101805, Gradient norm: 0.26608644
INFO:root:[   32] Training loss: 0.00926264, Validation loss: 0.00954457, Gradient norm: 0.20872785
INFO:root:[   33] Training loss: 0.01040778, Validation loss: 0.01028858, Gradient norm: 0.30042895
INFO:root:[   34] Training loss: 0.01000348, Validation loss: 0.00934493, Gradient norm: 0.23700467
INFO:root:[   35] Training loss: 0.00867303, Validation loss: 0.00922033, Gradient norm: 0.16215874
INFO:root:[   36] Training loss: 0.00899659, Validation loss: 0.00984743, Gradient norm: 0.19753992
INFO:root:[   37] Training loss: 0.00890966, Validation loss: 0.00866512, Gradient norm: 0.19600365
INFO:root:[   38] Training loss: 0.00835789, Validation loss: 0.00918968, Gradient norm: 0.17543646
INFO:root:[   39] Training loss: 0.00930447, Validation loss: 0.01059573, Gradient norm: 0.26851838
INFO:root:[   40] Training loss: 0.00939839, Validation loss: 0.01005160, Gradient norm: 0.26517064
INFO:root:[   41] Training loss: 0.00889292, Validation loss: 0.01021364, Gradient norm: 0.24663815
INFO:root:[   42] Training loss: 0.00863060, Validation loss: 0.01139820, Gradient norm: 0.19988485
INFO:root:[   43] Training loss: 0.00922796, Validation loss: 0.00900926, Gradient norm: 0.21431173
INFO:root:[   44] Training loss: 0.00802771, Validation loss: 0.00876860, Gradient norm: 0.16607001
INFO:root:[   45] Training loss: 0.00905502, Validation loss: 0.00883463, Gradient norm: 0.23918270
INFO:root:[   46] Training loss: 0.00867924, Validation loss: 0.00930811, Gradient norm: 0.19281635
INFO:root:[   47] Training loss: 0.00862819, Validation loss: 0.00919130, Gradient norm: 0.19858517
INFO:root:[   48] Training loss: 0.00836611, Validation loss: 0.01073660, Gradient norm: 0.21713295
INFO:root:[   49] Training loss: 0.00805969, Validation loss: 0.00930088, Gradient norm: 0.19020928
INFO:root:[   50] Training loss: 0.00857160, Validation loss: 0.00864495, Gradient norm: 0.25097809
INFO:root:[   51] Training loss: 0.00835469, Validation loss: 0.00894812, Gradient norm: 0.21550503
INFO:root:[   52] Training loss: 0.00796020, Validation loss: 0.00940858, Gradient norm: 0.22248663
INFO:root:[   53] Training loss: 0.00815708, Validation loss: 0.01517391, Gradient norm: 0.23458233
INFO:root:[   54] Training loss: 0.00944404, Validation loss: 0.01039499, Gradient norm: 0.27547172
INFO:root:[   55] Training loss: 0.00838948, Validation loss: 0.00939549, Gradient norm: 0.18802768
INFO:root:[   56] Training loss: 0.00788967, Validation loss: 0.00864161, Gradient norm: 0.20390613
INFO:root:[   57] Training loss: 0.00841889, Validation loss: 0.00910263, Gradient norm: 0.22683551
INFO:root:[   58] Training loss: 0.00787678, Validation loss: 0.01142907, Gradient norm: 0.23447232
INFO:root:[   59] Training loss: 0.00812967, Validation loss: 0.01209559, Gradient norm: 0.23275531
INFO:root:[   60] Training loss: 0.00852562, Validation loss: 0.00879874, Gradient norm: 0.26393359
INFO:root:[   61] Training loss: 0.00803146, Validation loss: 0.01007430, Gradient norm: 0.22115530
INFO:root:[   62] Training loss: 0.00801936, Validation loss: 0.00881288, Gradient norm: 0.22988637
INFO:root:[   63] Training loss: 0.00805649, Validation loss: 0.00909793, Gradient norm: 0.20289596
INFO:root:[   64] Training loss: 0.00771315, Validation loss: 0.00913338, Gradient norm: 0.23314875
INFO:root:[   65] Training loss: 0.00818489, Validation loss: 0.00933125, Gradient norm: 0.23144871
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 2145.326s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00857
INFO:root:EnergyScoreTrain: 0.00711
INFO:root:CoverageTrain: 0.99766
INFO:root:IntervalWidthTrain: 0.06419
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01111
INFO:root:EnergyScoreValidation: 0.00859
INFO:root:CoverageValidation: 0.98957
INFO:root:IntervalWidthValidation: 0.0638
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01126
INFO:root:EnergyScoreTest: 0.00868
INFO:root:CoverageTest: 0.98983
INFO:root:IntervalWidthTest: 0.06394
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05560257, Validation loss: 0.03153185, Gradient norm: 0.56670089
INFO:root:[    2] Training loss: 0.02520075, Validation loss: 0.02250395, Gradient norm: 0.54499061
INFO:root:[    3] Training loss: 0.01882115, Validation loss: 0.01603008, Gradient norm: 0.47697521
INFO:root:[    4] Training loss: 0.01681301, Validation loss: 0.01278001, Gradient norm: 0.46008960
INFO:root:[    5] Training loss: 0.01607397, Validation loss: 0.01691244, Gradient norm: 0.48782339
INFO:root:[    6] Training loss: 0.01369328, Validation loss: 0.01319804, Gradient norm: 0.36215171
INFO:root:[    7] Training loss: 0.01419782, Validation loss: 0.01377015, Gradient norm: 0.41505994
INFO:root:[    8] Training loss: 0.01317571, Validation loss: 0.01386311, Gradient norm: 0.38045147
INFO:root:[    9] Training loss: 0.01364648, Validation loss: 0.01067134, Gradient norm: 0.42577577
INFO:root:[   10] Training loss: 0.01187751, Validation loss: 0.01446572, Gradient norm: 0.37585150
INFO:root:[   11] Training loss: 0.01174156, Validation loss: 0.01123758, Gradient norm: 0.38556319
INFO:root:[   12] Training loss: 0.01133341, Validation loss: 0.01039561, Gradient norm: 0.34984508
INFO:root:[   13] Training loss: 0.01041514, Validation loss: 0.01010130, Gradient norm: 0.30691375
INFO:root:[   14] Training loss: 0.01104229, Validation loss: 0.01153781, Gradient norm: 0.39169385
INFO:root:[   15] Training loss: 0.01056503, Validation loss: 0.01242700, Gradient norm: 0.34903018
INFO:root:[   16] Training loss: 0.00968579, Validation loss: 0.01214329, Gradient norm: 0.28359162
INFO:root:[   17] Training loss: 0.01009330, Validation loss: 0.00997415, Gradient norm: 0.34326918
INFO:root:[   18] Training loss: 0.01002249, Validation loss: 0.01229637, Gradient norm: 0.30456100
INFO:root:[   19] Training loss: 0.00966936, Validation loss: 0.00997351, Gradient norm: 0.30523245
INFO:root:[   20] Training loss: 0.00937937, Validation loss: 0.01551909, Gradient norm: 0.32577410
INFO:root:[   21] Training loss: 0.01066428, Validation loss: 0.01123775, Gradient norm: 0.38981637
INFO:root:[   22] Training loss: 0.00918496, Validation loss: 0.00887762, Gradient norm: 0.30654015
INFO:root:[   23] Training loss: 0.00922021, Validation loss: 0.00972462, Gradient norm: 0.34212262
INFO:root:[   24] Training loss: 0.00969319, Validation loss: 0.00936415, Gradient norm: 0.35536973
INFO:root:[   25] Training loss: 0.00889961, Validation loss: 0.00957855, Gradient norm: 0.30264667
INFO:root:[   26] Training loss: 0.00939317, Validation loss: 0.00965130, Gradient norm: 0.31808700
INFO:root:[   27] Training loss: 0.00830056, Validation loss: 0.00912319, Gradient norm: 0.25602168
INFO:root:[   28] Training loss: 0.00877659, Validation loss: 0.01084602, Gradient norm: 0.30982123
INFO:root:[   29] Training loss: 0.00891541, Validation loss: 0.00906835, Gradient norm: 0.33493549
INFO:root:[   30] Training loss: 0.00802696, Validation loss: 0.00997222, Gradient norm: 0.24566194
INFO:root:[   31] Training loss: 0.00825717, Validation loss: 0.00980430, Gradient norm: 0.30840195
INFO:root:[   32] Training loss: 0.00841573, Validation loss: 0.01020970, Gradient norm: 0.31850802
INFO:root:[   33] Training loss: 0.00796990, Validation loss: 0.00849627, Gradient norm: 0.28201126
INFO:root:[   34] Training loss: 0.00799232, Validation loss: 0.00955878, Gradient norm: 0.30173441
INFO:root:[   35] Training loss: 0.00837528, Validation loss: 0.01232676, Gradient norm: 0.28968765
INFO:root:[   36] Training loss: 0.00830217, Validation loss: 0.01032725, Gradient norm: 0.33196725
INFO:root:[   37] Training loss: 0.00774587, Validation loss: 0.01424493, Gradient norm: 0.30412175
INFO:root:[   38] Training loss: 0.00865715, Validation loss: 0.01270159, Gradient norm: 0.35644182
INFO:root:[   39] Training loss: 0.00849476, Validation loss: 0.00924305, Gradient norm: 0.31618311
INFO:root:[   40] Training loss: 0.00723118, Validation loss: 0.00869101, Gradient norm: 0.27402225
INFO:root:[   41] Training loss: 0.00749208, Validation loss: 0.00877407, Gradient norm: 0.30257007
INFO:root:[   42] Training loss: 0.00765580, Validation loss: 0.00795644, Gradient norm: 0.32257518
INFO:root:[   43] Training loss: 0.00778262, Validation loss: 0.00942250, Gradient norm: 0.30436823
INFO:root:[   44] Training loss: 0.00707154, Validation loss: 0.01157267, Gradient norm: 0.25651908
INFO:root:[   45] Training loss: 0.00695852, Validation loss: 0.00856916, Gradient norm: 0.27489211
INFO:root:[   46] Training loss: 0.00729612, Validation loss: 0.00977494, Gradient norm: 0.28100984
INFO:root:[   47] Training loss: 0.00756893, Validation loss: 0.00867697, Gradient norm: 0.30837036
INFO:root:[   48] Training loss: 0.00739542, Validation loss: 0.01125076, Gradient norm: 0.31180168
INFO:root:[   49] Training loss: 0.00735598, Validation loss: 0.01046511, Gradient norm: 0.30370435
INFO:root:[   50] Training loss: 0.00688228, Validation loss: 0.01063472, Gradient norm: 0.25866928
INFO:root:[   51] Training loss: 0.00770663, Validation loss: 0.01006199, Gradient norm: 0.32887603
INFO:root:[   52] Training loss: 0.00707640, Validation loss: 0.00976452, Gradient norm: 0.27896067
INFO:root:[   53] Training loss: 0.00695965, Validation loss: 0.00924712, Gradient norm: 0.33093417
INFO:root:[   54] Training loss: 0.00678531, Validation loss: 0.00911956, Gradient norm: 0.28463095
INFO:root:[   55] Training loss: 0.00669354, Validation loss: 0.00951919, Gradient norm: 0.27973562
INFO:root:[   56] Training loss: 0.00712751, Validation loss: 0.00945526, Gradient norm: 0.33494051
INFO:root:[   57] Training loss: 0.00662771, Validation loss: 0.00868404, Gradient norm: 0.24933634
INFO:root:[   58] Training loss: 0.00701801, Validation loss: 0.01036335, Gradient norm: 0.29430485
INFO:root:[   59] Training loss: 0.00708982, Validation loss: 0.00947163, Gradient norm: 0.31842995
INFO:root:[   60] Training loss: 0.00637299, Validation loss: 0.01031546, Gradient norm: 0.29763643
INFO:root:[   61] Training loss: 0.00714600, Validation loss: 0.01068896, Gradient norm: 0.32693625
INFO:root:[   62] Training loss: 0.00632753, Validation loss: 0.00826619, Gradient norm: 0.26853939
INFO:root:[   63] Training loss: 0.00653985, Validation loss: 0.01066640, Gradient norm: 0.28138337
INFO:root:[   64] Training loss: 0.00634730, Validation loss: 0.00909682, Gradient norm: 0.26457095
INFO:root:[   65] Training loss: 0.00668225, Validation loss: 0.00891799, Gradient norm: 0.31164701
INFO:root:[   66] Training loss: 0.00654637, Validation loss: 0.00912580, Gradient norm: 0.26814623
INFO:root:[   67] Training loss: 0.00669171, Validation loss: 0.00924669, Gradient norm: 0.30022041
INFO:root:[   68] Training loss: 0.00649915, Validation loss: 0.00902613, Gradient norm: 0.32780279
INFO:root:[   69] Training loss: 0.00584810, Validation loss: 0.01065664, Gradient norm: 0.25906455
INFO:root:[   70] Training loss: 0.00640168, Validation loss: 0.01006543, Gradient norm: 0.26004569
INFO:root:[   71] Training loss: 0.00683179, Validation loss: 0.00854303, Gradient norm: 0.28545655
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 1524.215s.
INFO:root:Emptying the cuda cache took 0.01s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00668
INFO:root:EnergyScoreTrain: 0.00501
INFO:root:CoverageTrain: 0.87386
INFO:root:IntervalWidthTrain: 0.02744
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01073
INFO:root:EnergyScoreValidation: 0.00803
INFO:root:CoverageValidation: 0.79033
INFO:root:IntervalWidthValidation: 0.02758
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01086
INFO:root:EnergyScoreTest: 0.00815
INFO:root:CoverageTest: 0.78437
INFO:root:IntervalWidthTest: 0.02746
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05383724, Validation loss: 0.02269900, Gradient norm: 0.67400991
INFO:root:[    2] Training loss: 0.02588006, Validation loss: 0.01721465, Gradient norm: 0.50994849
INFO:root:[    3] Training loss: 0.02258134, Validation loss: 0.01616290, Gradient norm: 0.48782740
INFO:root:[    4] Training loss: 0.01808809, Validation loss: 0.01235270, Gradient norm: 0.37774365
INFO:root:[    5] Training loss: 0.01771491, Validation loss: 0.01762562, Gradient norm: 0.40501857
INFO:root:[    6] Training loss: 0.01670540, Validation loss: 0.01215259, Gradient norm: 0.40304422
INFO:root:[    7] Training loss: 0.01553259, Validation loss: 0.01229191, Gradient norm: 0.38528632
INFO:root:[    8] Training loss: 0.01494073, Validation loss: 0.01172997, Gradient norm: 0.34627468
INFO:root:[    9] Training loss: 0.01503481, Validation loss: 0.01645332, Gradient norm: 0.34878329
INFO:root:[   10] Training loss: 0.01336670, Validation loss: 0.01041228, Gradient norm: 0.31374596
INFO:root:[   11] Training loss: 0.01277669, Validation loss: 0.01110355, Gradient norm: 0.27191066
INFO:root:[   12] Training loss: 0.01340115, Validation loss: 0.01135548, Gradient norm: 0.31864189
INFO:root:[   13] Training loss: 0.01268674, Validation loss: 0.01085873, Gradient norm: 0.31060722
INFO:root:[   14] Training loss: 0.01300633, Validation loss: 0.01344707, Gradient norm: 0.30822815
INFO:root:[   15] Training loss: 0.01269142, Validation loss: 0.01582992, Gradient norm: 0.34428081
INFO:root:[   16] Training loss: 0.01240599, Validation loss: 0.01217063, Gradient norm: 0.32550336
INFO:root:[   17] Training loss: 0.01174627, Validation loss: 0.01231638, Gradient norm: 0.28955102
INFO:root:[   18] Training loss: 0.01161760, Validation loss: 0.00974893, Gradient norm: 0.31532757
INFO:root:[   19] Training loss: 0.01168304, Validation loss: 0.01042650, Gradient norm: 0.27653908
INFO:root:[   20] Training loss: 0.01211162, Validation loss: 0.01341419, Gradient norm: 0.36157450
INFO:root:[   21] Training loss: 0.01124810, Validation loss: 0.00914186, Gradient norm: 0.28691525
INFO:root:[   22] Training loss: 0.01096695, Validation loss: 0.01041050, Gradient norm: 0.26010713
INFO:root:[   23] Training loss: 0.01103350, Validation loss: 0.00931794, Gradient norm: 0.28257723
INFO:root:[   24] Training loss: 0.01094489, Validation loss: 0.00869963, Gradient norm: 0.31020814
INFO:root:[   25] Training loss: 0.01042559, Validation loss: 0.00978052, Gradient norm: 0.27483091
INFO:root:[   26] Training loss: 0.01055265, Validation loss: 0.01219416, Gradient norm: 0.26644616
INFO:root:[   27] Training loss: 0.01010735, Validation loss: 0.01357373, Gradient norm: 0.27710132
INFO:root:[   28] Training loss: 0.01069172, Validation loss: 0.00904157, Gradient norm: 0.30537072
INFO:root:[   29] Training loss: 0.01063200, Validation loss: 0.01302637, Gradient norm: 0.28707855
INFO:root:[   30] Training loss: 0.01025342, Validation loss: 0.01162002, Gradient norm: 0.29192552
INFO:root:[   31] Training loss: 0.01001351, Validation loss: 0.01097892, Gradient norm: 0.28113270
INFO:root:[   32] Training loss: 0.01045285, Validation loss: 0.01155344, Gradient norm: 0.31036717
INFO:root:[   33] Training loss: 0.00961212, Validation loss: 0.00935634, Gradient norm: 0.28262132
INFO:root:[   34] Training loss: 0.01008241, Validation loss: 0.00959374, Gradient norm: 0.27690958
INFO:root:[   35] Training loss: 0.00985696, Validation loss: 0.01061823, Gradient norm: 0.31957018
INFO:root:[   36] Training loss: 0.00972227, Validation loss: 0.00975251, Gradient norm: 0.28572101
INFO:root:[   37] Training loss: 0.00905617, Validation loss: 0.00848681, Gradient norm: 0.25186424
INFO:root:[   38] Training loss: 0.00998049, Validation loss: 0.00952561, Gradient norm: 0.29005570
INFO:root:[   39] Training loss: 0.00957631, Validation loss: 0.01016768, Gradient norm: 0.31441005
INFO:root:[   40] Training loss: 0.00933923, Validation loss: 0.01109229, Gradient norm: 0.31613449
INFO:root:[   41] Training loss: 0.00917348, Validation loss: 0.01068940, Gradient norm: 0.28523304
INFO:root:[   42] Training loss: 0.00952457, Validation loss: 0.01011694, Gradient norm: 0.28124389
INFO:root:[   43] Training loss: 0.00926165, Validation loss: 0.01285902, Gradient norm: 0.26826813
INFO:root:[   44] Training loss: 0.00872120, Validation loss: 0.01267999, Gradient norm: 0.29570001
INFO:root:[   45] Training loss: 0.00866142, Validation loss: 0.01049556, Gradient norm: 0.26365454
INFO:root:[   46] Training loss: 0.00928928, Validation loss: 0.01186520, Gradient norm: 0.30682609
INFO:root:[   47] Training loss: 0.00924676, Validation loss: 0.00970308, Gradient norm: 0.33477975
INFO:root:[   48] Training loss: 0.00865457, Validation loss: 0.01258467, Gradient norm: 0.26286850
INFO:root:[   49] Training loss: 0.00870372, Validation loss: 0.00838808, Gradient norm: 0.27577536
INFO:root:[   50] Training loss: 0.00851462, Validation loss: 0.01107753, Gradient norm: 0.29207374
INFO:root:[   51] Training loss: 0.00809082, Validation loss: 0.00931995, Gradient norm: 0.27616783
INFO:root:[   52] Training loss: 0.00829785, Validation loss: 0.01046576, Gradient norm: 0.25192299
INFO:root:[   53] Training loss: 0.00894591, Validation loss: 0.00971531, Gradient norm: 0.29232227
INFO:root:[   54] Training loss: 0.00844517, Validation loss: 0.01020677, Gradient norm: 0.26175442
INFO:root:[   55] Training loss: 0.00901228, Validation loss: 0.01013813, Gradient norm: 0.33550102
INFO:root:[   56] Training loss: 0.00796386, Validation loss: 0.01075313, Gradient norm: 0.26872806
INFO:root:[   57] Training loss: 0.00859562, Validation loss: 0.01050371, Gradient norm: 0.29423474
INFO:root:[   58] Training loss: 0.00832295, Validation loss: 0.01410543, Gradient norm: 0.31828195
INFO:root:[   59] Training loss: 0.00900087, Validation loss: 0.00996514, Gradient norm: 0.28651724
INFO:root:[   60] Training loss: 0.00787890, Validation loss: 0.01078521, Gradient norm: 0.25193394
INFO:root:[   61] Training loss: 0.00763981, Validation loss: 0.00830682, Gradient norm: 0.27009202
INFO:root:[   62] Training loss: 0.00761437, Validation loss: 0.01122409, Gradient norm: 0.25584506
INFO:root:[   63] Training loss: 0.00823137, Validation loss: 0.01081301, Gradient norm: 0.31476963
INFO:root:[   64] Training loss: 0.00792412, Validation loss: 0.00954096, Gradient norm: 0.29902389
INFO:root:[   65] Training loss: 0.00805448, Validation loss: 0.01035832, Gradient norm: 0.28090059
INFO:root:[   66] Training loss: 0.00754072, Validation loss: 0.00898703, Gradient norm: 0.27142349
INFO:root:[   67] Training loss: 0.00834449, Validation loss: 0.01163227, Gradient norm: 0.30540092
INFO:root:[   68] Training loss: 0.00780339, Validation loss: 0.01065841, Gradient norm: 0.26181699
INFO:root:[   69] Training loss: 0.00788537, Validation loss: 0.00956482, Gradient norm: 0.29571633
INFO:root:[   70] Training loss: 0.00791145, Validation loss: 0.01013952, Gradient norm: 0.29350526
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1498.722s.
INFO:root:Emptying the cuda cache took 0.01s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0061
INFO:root:EnergyScoreTrain: 0.00456
INFO:root:CoverageTrain: 0.91653
INFO:root:IntervalWidthTrain: 0.0259
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01089
INFO:root:EnergyScoreValidation: 0.00839
INFO:root:CoverageValidation: 0.80732
INFO:root:IntervalWidthValidation: 0.0259
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01107
INFO:root:EnergyScoreTest: 0.00856
INFO:root:CoverageTest: 0.80322
INFO:root:IntervalWidthTest: 0.02597
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05838983, Validation loss: 0.02932016, Gradient norm: 0.54937774
INFO:root:[    2] Training loss: 0.02869804, Validation loss: 0.01880500, Gradient norm: 0.45642725
INFO:root:[    3] Training loss: 0.02428631, Validation loss: 0.02452300, Gradient norm: 0.43419209
INFO:root:[    4] Training loss: 0.02114553, Validation loss: 0.01315172, Gradient norm: 0.39656851
INFO:root:[    5] Training loss: 0.01776570, Validation loss: 0.01316507, Gradient norm: 0.32100387
INFO:root:[    6] Training loss: 0.01755606, Validation loss: 0.01280629, Gradient norm: 0.32814467
INFO:root:[    7] Training loss: 0.01745059, Validation loss: 0.01166039, Gradient norm: 0.37798149
INFO:root:[    8] Training loss: 0.01643435, Validation loss: 0.01286649, Gradient norm: 0.33243307
INFO:root:[    9] Training loss: 0.01478112, Validation loss: 0.01462990, Gradient norm: 0.30244571
INFO:root:[   10] Training loss: 0.01692617, Validation loss: 0.01130340, Gradient norm: 0.36446525
INFO:root:[   11] Training loss: 0.01538092, Validation loss: 0.01689660, Gradient norm: 0.33586354
INFO:root:[   12] Training loss: 0.01400429, Validation loss: 0.01003818, Gradient norm: 0.26926043
INFO:root:[   13] Training loss: 0.01317967, Validation loss: 0.01616081, Gradient norm: 0.27209826
INFO:root:[   14] Training loss: 0.01432031, Validation loss: 0.01337088, Gradient norm: 0.30674680
INFO:root:[   15] Training loss: 0.01348567, Validation loss: 0.00948768, Gradient norm: 0.30412194
INFO:root:[   16] Training loss: 0.01275039, Validation loss: 0.00998727, Gradient norm: 0.24926165
INFO:root:[   17] Training loss: 0.01253484, Validation loss: 0.01217270, Gradient norm: 0.26120818
INFO:root:[   18] Training loss: 0.01288407, Validation loss: 0.01003168, Gradient norm: 0.32642288
INFO:root:[   19] Training loss: 0.01292082, Validation loss: 0.01343687, Gradient norm: 0.28420422
INFO:root:[   20] Training loss: 0.01232413, Validation loss: 0.01089492, Gradient norm: 0.30352941
INFO:root:[   21] Training loss: 0.01255658, Validation loss: 0.00954426, Gradient norm: 0.28987006
INFO:root:[   22] Training loss: 0.01211301, Validation loss: 0.01263767, Gradient norm: 0.23626890
INFO:root:[   23] Training loss: 0.01130734, Validation loss: 0.00940494, Gradient norm: 0.27404684
INFO:root:[   24] Training loss: 0.01182256, Validation loss: 0.01103417, Gradient norm: 0.28148816
INFO:root:[   25] Training loss: 0.01163635, Validation loss: 0.01132531, Gradient norm: 0.27967434
INFO:root:[   26] Training loss: 0.01183637, Validation loss: 0.01454492, Gradient norm: 0.28816290
INFO:root:[   27] Training loss: 0.01232711, Validation loss: 0.01769372, Gradient norm: 0.30239377
INFO:root:[   28] Training loss: 0.01098622, Validation loss: 0.01102645, Gradient norm: 0.22551142
INFO:root:[   29] Training loss: 0.01065827, Validation loss: 0.01023252, Gradient norm: 0.26079081
INFO:root:[   30] Training loss: 0.01176036, Validation loss: 0.01436883, Gradient norm: 0.29399355
INFO:root:[   31] Training loss: 0.01140872, Validation loss: 0.00875114, Gradient norm: 0.28736609
INFO:root:[   32] Training loss: 0.01069278, Validation loss: 0.01088152, Gradient norm: 0.25837992
INFO:root:[   33] Training loss: 0.01162085, Validation loss: 0.01038230, Gradient norm: 0.27956707
INFO:root:[   34] Training loss: 0.01047741, Validation loss: 0.00938842, Gradient norm: 0.26582566
INFO:root:[   35] Training loss: 0.01048208, Validation loss: 0.00989549, Gradient norm: 0.27060541
INFO:root:[   36] Training loss: 0.01064321, Validation loss: 0.01179208, Gradient norm: 0.28880040
INFO:root:[   37] Training loss: 0.01054157, Validation loss: 0.01074811, Gradient norm: 0.27137399
INFO:root:[   38] Training loss: 0.01063781, Validation loss: 0.01237974, Gradient norm: 0.27656533
INFO:root:[   39] Training loss: 0.01070273, Validation loss: 0.01046224, Gradient norm: 0.26655321
INFO:root:[   40] Training loss: 0.00985002, Validation loss: 0.00944876, Gradient norm: 0.24079584
INFO:root:[   41] Training loss: 0.00972042, Validation loss: 0.01219486, Gradient norm: 0.26685619
INFO:root:[   42] Training loss: 0.00997988, Validation loss: 0.01395863, Gradient norm: 0.25896314
INFO:root:[   43] Training loss: 0.00985472, Validation loss: 0.01033893, Gradient norm: 0.27654259
INFO:root:[   44] Training loss: 0.00985981, Validation loss: 0.00969419, Gradient norm: 0.24921131
INFO:root:[   45] Training loss: 0.00964342, Validation loss: 0.01339084, Gradient norm: 0.27284671
INFO:root:[   46] Training loss: 0.00961772, Validation loss: 0.01774061, Gradient norm: 0.25511943
INFO:root:[   47] Training loss: 0.00986986, Validation loss: 0.01118790, Gradient norm: 0.24548515
INFO:root:[   48] Training loss: 0.00948268, Validation loss: 0.00927747, Gradient norm: 0.27145382
INFO:root:[   49] Training loss: 0.00989958, Validation loss: 0.01060401, Gradient norm: 0.26433865
INFO:root:[   50] Training loss: 0.00976814, Validation loss: 0.01087583, Gradient norm: 0.25678659
INFO:root:[   51] Training loss: 0.00974165, Validation loss: 0.00955639, Gradient norm: 0.29001786
INFO:root:[   52] Training loss: 0.00917836, Validation loss: 0.00864147, Gradient norm: 0.24323719
INFO:root:[   53] Training loss: 0.00885746, Validation loss: 0.00845833, Gradient norm: 0.26664532
INFO:root:[   54] Training loss: 0.00936776, Validation loss: 0.00907885, Gradient norm: 0.27907398
INFO:root:[   55] Training loss: 0.00928584, Validation loss: 0.01134441, Gradient norm: 0.27633769
INFO:root:[   56] Training loss: 0.00876567, Validation loss: 0.00894791, Gradient norm: 0.26067680
INFO:root:[   57] Training loss: 0.00940811, Validation loss: 0.00994187, Gradient norm: 0.28395491
INFO:root:[   58] Training loss: 0.00897298, Validation loss: 0.01062475, Gradient norm: 0.26522061
INFO:root:[   59] Training loss: 0.00928534, Validation loss: 0.01145782, Gradient norm: 0.24758572
INFO:root:[   60] Training loss: 0.00907508, Validation loss: 0.01523713, Gradient norm: 0.30232223
INFO:root:[   61] Training loss: 0.00856200, Validation loss: 0.00898636, Gradient norm: 0.26775122
INFO:root:[   62] Training loss: 0.00842852, Validation loss: 0.01048107, Gradient norm: 0.24917752
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1329.539s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00782
INFO:root:EnergyScoreTrain: 0.00575
INFO:root:CoverageTrain: 0.9223
INFO:root:IntervalWidthTrain: 0.03242
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01125
INFO:root:EnergyScoreValidation: 0.00834
INFO:root:CoverageValidation: 0.83733
INFO:root:IntervalWidthValidation: 0.0324
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01117
INFO:root:EnergyScoreTest: 0.00828
INFO:root:CoverageTest: 0.83282
INFO:root:IntervalWidthTest: 0.0324
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06179522, Validation loss: 0.02448593, Gradient norm: 0.47012127
INFO:root:[    2] Training loss: 0.03007944, Validation loss: 0.02131641, Gradient norm: 0.41547198
INFO:root:[    3] Training loss: 0.02651250, Validation loss: 0.01627786, Gradient norm: 0.44750099
INFO:root:[    4] Training loss: 0.02300296, Validation loss: 0.01837393, Gradient norm: 0.36631789
INFO:root:[    5] Training loss: 0.02089704, Validation loss: 0.01946039, Gradient norm: 0.38169059
INFO:root:[    6] Training loss: 0.02083814, Validation loss: 0.01526580, Gradient norm: 0.41799871
INFO:root:[    7] Training loss: 0.01933127, Validation loss: 0.01224727, Gradient norm: 0.39230914
INFO:root:[    8] Training loss: 0.01648804, Validation loss: 0.01474274, Gradient norm: 0.29401319
INFO:root:[    9] Training loss: 0.01738019, Validation loss: 0.01507145, Gradient norm: 0.32284362
INFO:root:[   10] Training loss: 0.01699945, Validation loss: 0.01334573, Gradient norm: 0.37062634
INFO:root:[   11] Training loss: 0.01675445, Validation loss: 0.01511576, Gradient norm: 0.35240437
INFO:root:[   12] Training loss: 0.01534898, Validation loss: 0.01336534, Gradient norm: 0.31145469
INFO:root:[   13] Training loss: 0.01541672, Validation loss: 0.01758844, Gradient norm: 0.32935692
INFO:root:[   14] Training loss: 0.01580908, Validation loss: 0.01563904, Gradient norm: 0.34820319
INFO:root:[   15] Training loss: 0.01360385, Validation loss: 0.01055638, Gradient norm: 0.23457983
INFO:root:[   16] Training loss: 0.01462852, Validation loss: 0.01211134, Gradient norm: 0.31848051
INFO:root:[   17] Training loss: 0.01460784, Validation loss: 0.01228465, Gradient norm: 0.25056937
INFO:root:[   18] Training loss: 0.01314527, Validation loss: 0.00983238, Gradient norm: 0.26807831
INFO:root:[   19] Training loss: 0.01429360, Validation loss: 0.01018916, Gradient norm: 0.32509243
INFO:root:[   20] Training loss: 0.01338104, Validation loss: 0.01615769, Gradient norm: 0.26014549
INFO:root:[   21] Training loss: 0.01294909, Validation loss: 0.01099043, Gradient norm: 0.26974680
INFO:root:[   22] Training loss: 0.01252359, Validation loss: 0.01332906, Gradient norm: 0.27450566
INFO:root:[   23] Training loss: 0.01286976, Validation loss: 0.01003329, Gradient norm: 0.26805634
INFO:root:[   24] Training loss: 0.01307911, Validation loss: 0.00920667, Gradient norm: 0.33000262
INFO:root:[   25] Training loss: 0.01291336, Validation loss: 0.01251710, Gradient norm: 0.27388236
INFO:root:[   26] Training loss: 0.01268564, Validation loss: 0.01304283, Gradient norm: 0.27911818
INFO:root:[   27] Training loss: 0.01318313, Validation loss: 0.01166458, Gradient norm: 0.30125726
INFO:root:[   28] Training loss: 0.01262964, Validation loss: 0.00946242, Gradient norm: 0.32081744
INFO:root:[   29] Training loss: 0.01288782, Validation loss: 0.01297884, Gradient norm: 0.32549193
INFO:root:[   30] Training loss: 0.01265560, Validation loss: 0.00934613, Gradient norm: 0.28210705
INFO:root:[   31] Training loss: 0.01155661, Validation loss: 0.00955897, Gradient norm: 0.27293074
INFO:root:[   32] Training loss: 0.01137992, Validation loss: 0.01149965, Gradient norm: 0.24496520
INFO:root:[   33] Training loss: 0.01158552, Validation loss: 0.01079451, Gradient norm: 0.28802916
INFO:root:[   34] Training loss: 0.01102599, Validation loss: 0.01160037, Gradient norm: 0.25848940
INFO:root:[   35] Training loss: 0.01172550, Validation loss: 0.01360172, Gradient norm: 0.29558698
INFO:root:[   36] Training loss: 0.01172670, Validation loss: 0.00952121, Gradient norm: 0.30359803
INFO:root:[   37] Training loss: 0.01096670, Validation loss: 0.01163015, Gradient norm: 0.25250002
INFO:root:[   38] Training loss: 0.01210851, Validation loss: 0.01791478, Gradient norm: 0.29365874
INFO:root:[   39] Training loss: 0.01114664, Validation loss: 0.01184217, Gradient norm: 0.24954407
INFO:root:[   40] Training loss: 0.01063191, Validation loss: 0.01124422, Gradient norm: 0.27612086
INFO:root:[   41] Training loss: 0.01168835, Validation loss: 0.01478013, Gradient norm: 0.31779911
INFO:root:[   42] Training loss: 0.01102530, Validation loss: 0.00949810, Gradient norm: 0.25649749
INFO:root:[   43] Training loss: 0.01130004, Validation loss: 0.00872625, Gradient norm: 0.28933854
INFO:root:[   44] Training loss: 0.01030119, Validation loss: 0.00953605, Gradient norm: 0.24436703
INFO:root:[   45] Training loss: 0.01011317, Validation loss: 0.01077282, Gradient norm: 0.27292016
INFO:root:[   46] Training loss: 0.01135308, Validation loss: 0.01349319, Gradient norm: 0.25627446
INFO:root:[   47] Training loss: 0.01053056, Validation loss: 0.00906758, Gradient norm: 0.29336508
INFO:root:[   48] Training loss: 0.01046652, Validation loss: 0.01055372, Gradient norm: 0.28125991
INFO:root:[   49] Training loss: 0.01003865, Validation loss: 0.01278849, Gradient norm: 0.25654882
INFO:root:[   50] Training loss: 0.01030338, Validation loss: 0.00948202, Gradient norm: 0.26186554
INFO:root:[   51] Training loss: 0.01046237, Validation loss: 0.01199650, Gradient norm: 0.23626054
INFO:root:[   52] Training loss: 0.01034228, Validation loss: 0.01087659, Gradient norm: 0.26399240
INFO:root:[   53] Training loss: 0.01024364, Validation loss: 0.00944156, Gradient norm: 0.24220112
INFO:root:[   54] Training loss: 0.00981523, Validation loss: 0.00995927, Gradient norm: 0.26887606
INFO:root:[   55] Training loss: 0.01070194, Validation loss: 0.00928744, Gradient norm: 0.28688945
INFO:root:[   56] Training loss: 0.00996640, Validation loss: 0.00871170, Gradient norm: 0.25615594
INFO:root:[   57] Training loss: 0.00950490, Validation loss: 0.00955426, Gradient norm: 0.26498007
INFO:root:[   58] Training loss: 0.00970105, Validation loss: 0.01108106, Gradient norm: 0.27340783
INFO:root:[   59] Training loss: 0.00992242, Validation loss: 0.01079459, Gradient norm: 0.22085057
INFO:root:[   60] Training loss: 0.00987747, Validation loss: 0.01114026, Gradient norm: 0.29357901
INFO:root:[   61] Training loss: 0.00923229, Validation loss: 0.01106121, Gradient norm: 0.24908869
INFO:root:[   62] Training loss: 0.00979644, Validation loss: 0.00996256, Gradient norm: 0.25773828
INFO:root:[   63] Training loss: 0.00942079, Validation loss: 0.00956399, Gradient norm: 0.29833019
INFO:root:[   64] Training loss: 0.00906131, Validation loss: 0.00910813, Gradient norm: 0.25472870
INFO:root:[   65] Training loss: 0.00955229, Validation loss: 0.01006602, Gradient norm: 0.22132973
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1396.529s.
INFO:root:Emptying the cuda cache took 0.01s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00827
INFO:root:EnergyScoreTrain: 0.00619
INFO:root:CoverageTrain: 0.9245
INFO:root:IntervalWidthTrain: 0.032
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01149
INFO:root:EnergyScoreValidation: 0.00885
INFO:root:CoverageValidation: 0.8602
INFO:root:IntervalWidthValidation: 0.03198
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01135
INFO:root:EnergyScoreTest: 0.00871
INFO:root:CoverageTest: 0.85652
INFO:root:IntervalWidthTest: 0.03198
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06239304, Validation loss: 0.03129943, Gradient norm: 0.50010894
INFO:root:[    2] Training loss: 0.03130386, Validation loss: 0.02876806, Gradient norm: 0.38578531
INFO:root:[    3] Training loss: 0.02800241, Validation loss: 0.01770025, Gradient norm: 0.45292941
INFO:root:[    4] Training loss: 0.02419919, Validation loss: 0.02021638, Gradient norm: 0.41288247
INFO:root:[    5] Training loss: 0.02128710, Validation loss: 0.01311150, Gradient norm: 0.37511499
INFO:root:[    6] Training loss: 0.02062480, Validation loss: 0.01825450, Gradient norm: 0.37612156
INFO:root:[    7] Training loss: 0.01932816, Validation loss: 0.01586649, Gradient norm: 0.37050308
INFO:root:[    8] Training loss: 0.01733776, Validation loss: 0.01309038, Gradient norm: 0.30493302
INFO:root:[    9] Training loss: 0.01749108, Validation loss: 0.01573174, Gradient norm: 0.32691699
INFO:root:[   10] Training loss: 0.01779412, Validation loss: 0.02125865, Gradient norm: 0.38214640
INFO:root:[   11] Training loss: 0.01594466, Validation loss: 0.01287764, Gradient norm: 0.25279231
INFO:root:[   12] Training loss: 0.01627930, Validation loss: 0.01417342, Gradient norm: 0.35651573
INFO:root:[   13] Training loss: 0.01607368, Validation loss: 0.01580400, Gradient norm: 0.28764191
INFO:root:[   14] Training loss: 0.01581915, Validation loss: 0.01486945, Gradient norm: 0.34146674
INFO:root:[   15] Training loss: 0.01486872, Validation loss: 0.01007289, Gradient norm: 0.26780331
INFO:root:[   16] Training loss: 0.01419008, Validation loss: 0.01017072, Gradient norm: 0.27017641
INFO:root:[   17] Training loss: 0.01444587, Validation loss: 0.01266483, Gradient norm: 0.25620459
INFO:root:[   18] Training loss: 0.01368249, Validation loss: 0.01721291, Gradient norm: 0.27629072
INFO:root:[   19] Training loss: 0.01427042, Validation loss: 0.01352876, Gradient norm: 0.31554686
INFO:root:[   20] Training loss: 0.01387246, Validation loss: 0.01146515, Gradient norm: 0.28752297
INFO:root:[   21] Training loss: 0.01392685, Validation loss: 0.01488350, Gradient norm: 0.30686621
INFO:root:[   22] Training loss: 0.01434073, Validation loss: 0.01282237, Gradient norm: 0.31472408
INFO:root:[   23] Training loss: 0.01269961, Validation loss: 0.01635157, Gradient norm: 0.25463636
INFO:root:[   24] Training loss: 0.01402376, Validation loss: 0.01820087, Gradient norm: 0.27430868
INFO:root:[   25] Training loss: 0.01336239, Validation loss: 0.01120984, Gradient norm: 0.29286690
INFO:root:[   26] Training loss: 0.01273571, Validation loss: 0.00994543, Gradient norm: 0.24448724
INFO:root:[   27] Training loss: 0.01230678, Validation loss: 0.01000230, Gradient norm: 0.28207480
INFO:root:[   28] Training loss: 0.01269662, Validation loss: 0.01954544, Gradient norm: 0.28215962
INFO:root:[   29] Training loss: 0.01336804, Validation loss: 0.01386595, Gradient norm: 0.30637390
INFO:root:[   30] Training loss: 0.01306738, Validation loss: 0.01419170, Gradient norm: 0.28658963
INFO:root:[   31] Training loss: 0.01289182, Validation loss: 0.01159154, Gradient norm: 0.29844805
INFO:root:[   32] Training loss: 0.01261609, Validation loss: 0.01223532, Gradient norm: 0.29733733
INFO:root:[   33] Training loss: 0.01201375, Validation loss: 0.01002695, Gradient norm: 0.25213393
INFO:root:[   34] Training loss: 0.01178573, Validation loss: 0.01805624, Gradient norm: 0.26715673
INFO:root:[   35] Training loss: 0.01195226, Validation loss: 0.01321822, Gradient norm: 0.25663167
INFO:root:[   36] Training loss: 0.01296740, Validation loss: 0.01165917, Gradient norm: 0.27301162
INFO:root:[   37] Training loss: 0.01199273, Validation loss: 0.01239195, Gradient norm: 0.28129999
INFO:root:[   38] Training loss: 0.01188395, Validation loss: 0.01277758, Gradient norm: 0.24824486
INFO:root:[   39] Training loss: 0.01239971, Validation loss: 0.01275999, Gradient norm: 0.30816567
INFO:root:[   40] Training loss: 0.01136634, Validation loss: 0.01254039, Gradient norm: 0.27891356
INFO:root:[   41] Training loss: 0.01131885, Validation loss: 0.01365958, Gradient norm: 0.27164584
INFO:root:[   42] Training loss: 0.01145662, Validation loss: 0.01210747, Gradient norm: 0.24972295
INFO:root:[   43] Training loss: 0.01060332, Validation loss: 0.01130394, Gradient norm: 0.27084643
INFO:root:[   44] Training loss: 0.01266812, Validation loss: 0.01132912, Gradient norm: 0.31797600
INFO:root:[   45] Training loss: 0.01084952, Validation loss: 0.01097333, Gradient norm: 0.26096269
INFO:root:[   46] Training loss: 0.01099515, Validation loss: 0.01385429, Gradient norm: 0.26657052
INFO:root:[   47] Training loss: 0.01074273, Validation loss: 0.01152182, Gradient norm: 0.21014598
INFO:root:[   48] Training loss: 0.01090491, Validation loss: 0.01085865, Gradient norm: 0.23964266
INFO:root:[   49] Training loss: 0.01112952, Validation loss: 0.01144950, Gradient norm: 0.29909994
INFO:root:[   50] Training loss: 0.01164433, Validation loss: 0.01134579, Gradient norm: 0.25353395
INFO:root:[   51] Training loss: 0.01130691, Validation loss: 0.01383118, Gradient norm: 0.22836720
INFO:root:[   52] Training loss: 0.01113560, Validation loss: 0.00977523, Gradient norm: 0.28270078
INFO:root:[   53] Training loss: 0.01036634, Validation loss: 0.01321983, Gradient norm: 0.24093652
INFO:root:[   54] Training loss: 0.01096778, Validation loss: 0.01264395, Gradient norm: 0.28342979
INFO:root:[   55] Training loss: 0.01073312, Validation loss: 0.01505950, Gradient norm: 0.23989678
INFO:root:[   56] Training loss: 0.01085773, Validation loss: 0.01334563, Gradient norm: 0.25070987
INFO:root:[   57] Training loss: 0.01012957, Validation loss: 0.01126207, Gradient norm: 0.26997711
INFO:root:[   58] Training loss: 0.00973121, Validation loss: 0.01303736, Gradient norm: 0.18303124
INFO:root:[   59] Training loss: 0.01032364, Validation loss: 0.01298813, Gradient norm: 0.23674458
INFO:root:[   60] Training loss: 0.01037687, Validation loss: 0.01534197, Gradient norm: 0.27189740
INFO:root:[   61] Training loss: 0.01055637, Validation loss: 0.01096577, Gradient norm: 0.27936899
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1321.048s.
INFO:root:Emptying the cuda cache took 0.01s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01003
INFO:root:EnergyScoreTrain: 0.00742
INFO:root:CoverageTrain: 0.79804
INFO:root:IntervalWidthTrain: 0.02587
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01281
INFO:root:EnergyScoreValidation: 0.00984
INFO:root:CoverageValidation: 0.75005
INFO:root:IntervalWidthValidation: 0.02581
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01278
INFO:root:EnergyScoreTest: 0.00981
INFO:root:CoverageTest: 0.74379
INFO:root:IntervalWidthTest: 0.02583
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06123259, Validation loss: 0.03809134, Gradient norm: 0.39885789
INFO:root:[    2] Training loss: 0.03345621, Validation loss: 0.03459428, Gradient norm: 0.38853652
INFO:root:[    3] Training loss: 0.02845878, Validation loss: 0.02459240, Gradient norm: 0.38195326
INFO:root:[    4] Training loss: 0.02565229, Validation loss: 0.02076809, Gradient norm: 0.39463569
INFO:root:[    5] Training loss: 0.02222329, Validation loss: 0.01538454, Gradient norm: 0.35462563
INFO:root:[    6] Training loss: 0.02071235, Validation loss: 0.01981840, Gradient norm: 0.33040423
INFO:root:[    7] Training loss: 0.02141196, Validation loss: 0.01856185, Gradient norm: 0.36039224
INFO:root:[    8] Training loss: 0.01988252, Validation loss: 0.01966144, Gradient norm: 0.31583895
INFO:root:[    9] Training loss: 0.01840692, Validation loss: 0.01458623, Gradient norm: 0.32342231
INFO:root:[   10] Training loss: 0.01773436, Validation loss: 0.01773873, Gradient norm: 0.30510345
INFO:root:[   11] Training loss: 0.01677606, Validation loss: 0.02004238, Gradient norm: 0.27471641
INFO:root:[   12] Training loss: 0.01733913, Validation loss: 0.01463332, Gradient norm: 0.28372856
INFO:root:[   13] Training loss: 0.01737748, Validation loss: 0.01318957, Gradient norm: 0.33161136
INFO:root:[   14] Training loss: 0.01775975, Validation loss: 0.01523322, Gradient norm: 0.32005547
INFO:root:[   15] Training loss: 0.01560340, Validation loss: 0.01313615, Gradient norm: 0.28237377
INFO:root:[   16] Training loss: 0.01608437, Validation loss: 0.01349501, Gradient norm: 0.25154617
INFO:root:[   17] Training loss: 0.01485862, Validation loss: 0.01602060, Gradient norm: 0.24536493
INFO:root:[   18] Training loss: 0.01517747, Validation loss: 0.01284575, Gradient norm: 0.29735928
INFO:root:[   19] Training loss: 0.01515414, Validation loss: 0.01887420, Gradient norm: 0.29807863
INFO:root:[   20] Training loss: 0.01491917, Validation loss: 0.01151656, Gradient norm: 0.28693945
INFO:root:[   21] Training loss: 0.01432993, Validation loss: 0.01374068, Gradient norm: 0.24886517
INFO:root:[   22] Training loss: 0.01578613, Validation loss: 0.01294230, Gradient norm: 0.32120165
INFO:root:[   23] Training loss: 0.01361015, Validation loss: 0.01215274, Gradient norm: 0.25231379
INFO:root:[   24] Training loss: 0.01481820, Validation loss: 0.01712255, Gradient norm: 0.28117277
INFO:root:[   25] Training loss: 0.01441535, Validation loss: 0.01327340, Gradient norm: 0.27936986
INFO:root:[   26] Training loss: 0.01359833, Validation loss: 0.01235992, Gradient norm: 0.26904551
INFO:root:[   27] Training loss: 0.01460293, Validation loss: 0.01230404, Gradient norm: 0.28892542
INFO:root:[   28] Training loss: 0.01317656, Validation loss: 0.01405283, Gradient norm: 0.27294097
INFO:root:[   29] Training loss: 0.01363093, Validation loss: 0.01430810, Gradient norm: 0.25252454
INFO:root:[   30] Training loss: 0.01450160, Validation loss: 0.01284539, Gradient norm: 0.29350050
INFO:root:[   31] Training loss: 0.01313682, Validation loss: 0.01178000, Gradient norm: 0.26085893
INFO:root:[   32] Training loss: 0.01400200, Validation loss: 0.01349855, Gradient norm: 0.25878584
INFO:root:[   33] Training loss: 0.01337631, Validation loss: 0.01555740, Gradient norm: 0.26442680
INFO:root:[   34] Training loss: 0.01278102, Validation loss: 0.01357418, Gradient norm: 0.21305492
INFO:root:[   35] Training loss: 0.01342352, Validation loss: 0.01571430, Gradient norm: 0.28345949
INFO:root:[   36] Training loss: 0.01246529, Validation loss: 0.01205487, Gradient norm: 0.21236578
INFO:root:[   37] Training loss: 0.01325509, Validation loss: 0.01201111, Gradient norm: 0.28344950
INFO:root:[   38] Training loss: 0.01298618, Validation loss: 0.01162849, Gradient norm: 0.27802636
INFO:root:[   39] Training loss: 0.01259857, Validation loss: 0.01480739, Gradient norm: 0.22711363
INFO:root:[   40] Training loss: 0.01260825, Validation loss: 0.01984801, Gradient norm: 0.26845156
INFO:root:[   41] Training loss: 0.01320660, Validation loss: 0.01350177, Gradient norm: 0.28986316
INFO:root:[   42] Training loss: 0.01296154, Validation loss: 0.01593402, Gradient norm: 0.28186270
INFO:root:[   43] Training loss: 0.01211567, Validation loss: 0.01554485, Gradient norm: 0.22473455
INFO:root:[   44] Training loss: 0.01379190, Validation loss: 0.01738809, Gradient norm: 0.27714345
INFO:root:[   45] Training loss: 0.01246434, Validation loss: 0.01161396, Gradient norm: 0.20598138
INFO:root:[   46] Training loss: 0.01235023, Validation loss: 0.01292455, Gradient norm: 0.25446093
INFO:root:[   47] Training loss: 0.01165879, Validation loss: 0.01502955, Gradient norm: 0.25340280
INFO:root:[   48] Training loss: 0.01189670, Validation loss: 0.01341945, Gradient norm: 0.24628316
INFO:root:[   49] Training loss: 0.01148751, Validation loss: 0.01465998, Gradient norm: 0.25471862
INFO:root:[   50] Training loss: 0.01229199, Validation loss: 0.01210893, Gradient norm: 0.26501512
INFO:root:[   51] Training loss: 0.01134600, Validation loss: 0.01117833, Gradient norm: 0.21407271
INFO:root:[   52] Training loss: 0.01220780, Validation loss: 0.01705341, Gradient norm: 0.25494022
INFO:root:[   53] Training loss: 0.01251082, Validation loss: 0.01865872, Gradient norm: 0.27324372
INFO:root:[   54] Training loss: 0.01222531, Validation loss: 0.01214955, Gradient norm: 0.21708037
INFO:root:[   55] Training loss: 0.01171029, Validation loss: 0.01188477, Gradient norm: 0.28670910
INFO:root:[   56] Training loss: 0.01203629, Validation loss: 0.01175268, Gradient norm: 0.26138227
INFO:root:[   57] Training loss: 0.01174490, Validation loss: 0.01146304, Gradient norm: 0.25379675
INFO:root:[   58] Training loss: 0.01147936, Validation loss: 0.01294384, Gradient norm: 0.23222659
INFO:root:[   59] Training loss: 0.01142354, Validation loss: 0.01922420, Gradient norm: 0.23630604
INFO:root:[   60] Training loss: 0.01184271, Validation loss: 0.01275348, Gradient norm: 0.23692516
INFO:root:[   61] Training loss: 0.01152570, Validation loss: 0.01128605, Gradient norm: 0.22537177
INFO:root:[   62] Training loss: 0.01208035, Validation loss: 0.01782279, Gradient norm: 0.27067852
INFO:root:[   63] Training loss: 0.01181240, Validation loss: 0.01132264, Gradient norm: 0.23351316
INFO:root:[   64] Training loss: 0.01103424, Validation loss: 0.01303163, Gradient norm: 0.22541063
INFO:root:[   65] Training loss: 0.01093309, Validation loss: 0.01385974, Gradient norm: 0.24608361
INFO:root:[   66] Training loss: 0.01150886, Validation loss: 0.01056791, Gradient norm: 0.23606813
INFO:root:[   67] Training loss: 0.01080627, Validation loss: 0.01366806, Gradient norm: 0.22669369
INFO:root:[   68] Training loss: 0.01095214, Validation loss: 0.01253506, Gradient norm: 0.25064221
INFO:root:[   69] Training loss: 0.01179886, Validation loss: 0.01172823, Gradient norm: 0.29438849
INFO:root:[   70] Training loss: 0.01106817, Validation loss: 0.01231654, Gradient norm: 0.25176969
INFO:root:[   71] Training loss: 0.01098210, Validation loss: 0.01265617, Gradient norm: 0.21126629
INFO:root:[   72] Training loss: 0.01042053, Validation loss: 0.01176686, Gradient norm: 0.21175995
INFO:root:[   73] Training loss: 0.01094097, Validation loss: 0.01193857, Gradient norm: 0.22523803
INFO:root:[   74] Training loss: 0.01146028, Validation loss: 0.01374764, Gradient norm: 0.20884061
INFO:root:[   75] Training loss: 0.01099113, Validation loss: 0.01117659, Gradient norm: 0.26035479
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 1623.564s.
INFO:root:Emptying the cuda cache took 0.01s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01103
INFO:root:EnergyScoreTrain: 0.00879
INFO:root:CoverageTrain: 0.74704
INFO:root:IntervalWidthTrain: 0.02085
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01317
INFO:root:EnergyScoreValidation: 0.01069
INFO:root:CoverageValidation: 0.69169
INFO:root:IntervalWidthValidation: 0.02082
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01351
INFO:root:EnergyScoreTest: 0.01104
INFO:root:CoverageTest: 0.6869
INFO:root:IntervalWidthTest: 0.02081
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05705349, Validation loss: 0.02664111, Gradient norm: 0.67032370
INFO:root:[    2] Training loss: 0.02986371, Validation loss: 0.02450558, Gradient norm: 0.63401745
INFO:root:[    3] Training loss: 0.02679935, Validation loss: 0.01871365, Gradient norm: 0.68826294
INFO:root:[    4] Training loss: 0.02286421, Validation loss: 0.01807425, Gradient norm: 0.55588278
INFO:root:[    5] Training loss: 0.02296709, Validation loss: 0.02024015, Gradient norm: 0.65201681
INFO:root:[    6] Training loss: 0.01915534, Validation loss: 0.01684782, Gradient norm: 0.54762964
INFO:root:[    7] Training loss: 0.01763639, Validation loss: 0.02513667, Gradient norm: 0.50372838
INFO:root:[    8] Training loss: 0.01761701, Validation loss: 0.01465754, Gradient norm: 0.54087670
INFO:root:[    9] Training loss: 0.01587017, Validation loss: 0.01424878, Gradient norm: 0.45065821
INFO:root:[   10] Training loss: 0.01710609, Validation loss: 0.01512699, Gradient norm: 0.44767539
INFO:root:[   11] Training loss: 0.01630019, Validation loss: 0.01785766, Gradient norm: 0.50856816
INFO:root:[   12] Training loss: 0.01492840, Validation loss: 0.01362486, Gradient norm: 0.40400351
INFO:root:[   13] Training loss: 0.01643015, Validation loss: 0.01829482, Gradient norm: 0.56281853
INFO:root:[   14] Training loss: 0.01554841, Validation loss: 0.01639730, Gradient norm: 0.43554135
INFO:root:[   15] Training loss: 0.01397029, Validation loss: 0.01480523, Gradient norm: 0.38983766
INFO:root:[   16] Training loss: 0.01306544, Validation loss: 0.01530212, Gradient norm: 0.37036459
INFO:root:[   17] Training loss: 0.01455015, Validation loss: 0.01790968, Gradient norm: 0.47175426
INFO:root:[   18] Training loss: 0.01358493, Validation loss: 0.01286763, Gradient norm: 0.40525761
INFO:root:[   19] Training loss: 0.01349916, Validation loss: 0.01433894, Gradient norm: 0.47058000
INFO:root:[   20] Training loss: 0.01314299, Validation loss: 0.01258408, Gradient norm: 0.37500146
INFO:root:[   21] Training loss: 0.01332678, Validation loss: 0.01359004, Gradient norm: 0.40713635
INFO:root:[   22] Training loss: 0.01167801, Validation loss: 0.01290342, Gradient norm: 0.30967175
INFO:root:[   23] Training loss: 0.01217257, Validation loss: 0.01249607, Gradient norm: 0.39456187
INFO:root:[   24] Training loss: 0.01183391, Validation loss: 0.01412682, Gradient norm: 0.35797028
INFO:root:[   25] Training loss: 0.01239765, Validation loss: 0.01350884, Gradient norm: 0.40160062
INFO:root:[   26] Training loss: 0.01227639, Validation loss: 0.01433912, Gradient norm: 0.43892914
INFO:root:[   27] Training loss: 0.01163187, Validation loss: 0.01532556, Gradient norm: 0.36144545
INFO:root:[   28] Training loss: 0.01156955, Validation loss: 0.01141306, Gradient norm: 0.38861981
INFO:root:[   29] Training loss: 0.01184700, Validation loss: 0.01402971, Gradient norm: 0.43201216
INFO:root:[   30] Training loss: 0.01236806, Validation loss: 0.01592378, Gradient norm: 0.33373166
INFO:root:[   31] Training loss: 0.01159249, Validation loss: 0.01223815, Gradient norm: 0.42647734
INFO:root:[   32] Training loss: 0.01044994, Validation loss: 0.01106839, Gradient norm: 0.34219466
INFO:root:[   33] Training loss: 0.01093023, Validation loss: 0.01252776, Gradient norm: 0.36941614
INFO:root:[   34] Training loss: 0.01249318, Validation loss: 0.01287169, Gradient norm: 0.40516785
INFO:root:[   35] Training loss: 0.01067674, Validation loss: 0.01138837, Gradient norm: 0.39430841
INFO:root:[   36] Training loss: 0.01037751, Validation loss: 0.01295575, Gradient norm: 0.31446616
INFO:root:[   37] Training loss: 0.01048688, Validation loss: 0.01491201, Gradient norm: 0.40334245
INFO:root:[   38] Training loss: 0.00979366, Validation loss: 0.01491778, Gradient norm: 0.33619107
INFO:root:[   39] Training loss: 0.01073026, Validation loss: 0.01405167, Gradient norm: 0.42959736
INFO:root:[   40] Training loss: 0.01000363, Validation loss: 0.01218821, Gradient norm: 0.36298177
INFO:root:[   41] Training loss: 0.00983060, Validation loss: 0.01395065, Gradient norm: 0.32897598
INFO:root:[   42] Training loss: 0.01044428, Validation loss: 0.01355074, Gradient norm: 0.40432021
INFO:root:[   43] Training loss: 0.00961256, Validation loss: 0.01254347, Gradient norm: 0.34014933
INFO:root:[   44] Training loss: 0.01050195, Validation loss: 0.01297322, Gradient norm: 0.43330918
INFO:root:[   45] Training loss: 0.00938818, Validation loss: 0.01287503, Gradient norm: 0.35005811
INFO:root:[   46] Training loss: 0.00979390, Validation loss: 0.01319464, Gradient norm: 0.37895439
INFO:root:[   47] Training loss: 0.00962610, Validation loss: 0.01699215, Gradient norm: 0.39466645
INFO:root:[   48] Training loss: 0.00993803, Validation loss: 0.01164143, Gradient norm: 0.38852187
INFO:root:[   49] Training loss: 0.00946845, Validation loss: 0.01318195, Gradient norm: 0.41164498
INFO:root:[   50] Training loss: 0.01002346, Validation loss: 0.01367367, Gradient norm: 0.43558562
INFO:root:[   51] Training loss: 0.00951269, Validation loss: 0.01353989, Gradient norm: 0.41688262
INFO:root:[   52] Training loss: 0.00946732, Validation loss: 0.01284621, Gradient norm: 0.41926674
INFO:root:[   53] Training loss: 0.00970056, Validation loss: 0.01201471, Gradient norm: 0.34848374
INFO:root:[   54] Training loss: 0.00916814, Validation loss: 0.01256030, Gradient norm: 0.35956372
INFO:root:[   55] Training loss: 0.00911396, Validation loss: 0.01285684, Gradient norm: 0.32991281
INFO:root:[   56] Training loss: 0.00944380, Validation loss: 0.01251849, Gradient norm: 0.42414619
INFO:root:[   57] Training loss: 0.00870697, Validation loss: 0.01326847, Gradient norm: 0.40882117
INFO:root:[   58] Training loss: 0.00925233, Validation loss: 0.01197846, Gradient norm: 0.40786777
INFO:root:[   59] Training loss: 0.00908959, Validation loss: 0.01221565, Gradient norm: 0.35899721
INFO:root:[   60] Training loss: 0.00886836, Validation loss: 0.01415283, Gradient norm: 0.37716167
INFO:root:[   61] Training loss: 0.00871716, Validation loss: 0.01159019, Gradient norm: 0.41005231
INFO:root:[   62] Training loss: 0.00936179, Validation loss: 0.01214173, Gradient norm: 0.40474004
INFO:root:[   63] Training loss: 0.00889730, Validation loss: 0.01288269, Gradient norm: 0.42342429
INFO:root:[   64] Training loss: 0.00888895, Validation loss: 0.01177941, Gradient norm: 0.40939931
INFO:root:[   65] Training loss: 0.00842437, Validation loss: 0.01176519, Gradient norm: 0.41176366
INFO:root:[   66] Training loss: 0.00853579, Validation loss: 0.01710441, Gradient norm: 0.34307017
INFO:root:[   67] Training loss: 0.00879467, Validation loss: 0.01180502, Gradient norm: 0.37783658
INFO:root:[   68] Training loss: 0.00932889, Validation loss: 0.01172631, Gradient norm: 0.41290851
INFO:root:[   69] Training loss: 0.00820516, Validation loss: 0.01442748, Gradient norm: 0.36137054
INFO:root:[   70] Training loss: 0.00943402, Validation loss: 0.01121599, Gradient norm: 0.40856247
INFO:root:[   71] Training loss: 0.00829000, Validation loss: 0.01325013, Gradient norm: 0.37645359
INFO:root:[   72] Training loss: 0.00869116, Validation loss: 0.01226772, Gradient norm: 0.41783938
INFO:root:[   73] Training loss: 0.00850168, Validation loss: 0.01287025, Gradient norm: 0.42184741
INFO:root:[   74] Training loss: 0.00847193, Validation loss: 0.01338197, Gradient norm: 0.42665177
INFO:root:[   75] Training loss: 0.00840397, Validation loss: 0.01282726, Gradient norm: 0.37729093
INFO:root:[   76] Training loss: 0.00834532, Validation loss: 0.01152332, Gradient norm: 0.42917893
INFO:root:[   77] Training loss: 0.00817271, Validation loss: 0.01117237, Gradient norm: 0.38577679
INFO:root:[   78] Training loss: 0.00780144, Validation loss: 0.01305515, Gradient norm: 0.40613910
INFO:root:[   79] Training loss: 0.00903969, Validation loss: 0.01134059, Gradient norm: 0.44517947
INFO:root:[   80] Training loss: 0.00801388, Validation loss: 0.01247203, Gradient norm: 0.40552379
INFO:root:[   81] Training loss: 0.00923984, Validation loss: 0.01199870, Gradient norm: 0.41367412
INFO:root:[   82] Training loss: 0.00854228, Validation loss: 0.01096588, Gradient norm: 0.41239677
INFO:root:[   83] Training loss: 0.00823331, Validation loss: 0.01099076, Gradient norm: 0.36619739
INFO:root:[   84] Training loss: 0.00830943, Validation loss: 0.01045195, Gradient norm: 0.40194543
INFO:root:[   85] Training loss: 0.00799798, Validation loss: 0.01151174, Gradient norm: 0.36928486
INFO:root:[   86] Training loss: 0.00789973, Validation loss: 0.01386666, Gradient norm: 0.41186755
INFO:root:[   87] Training loss: 0.00759859, Validation loss: 0.01150274, Gradient norm: 0.37356017
INFO:root:[   88] Training loss: 0.00794070, Validation loss: 0.01129183, Gradient norm: 0.43649200
INFO:root:[   89] Training loss: 0.00803254, Validation loss: 0.01355160, Gradient norm: 0.38974524
INFO:root:[   90] Training loss: 0.00853648, Validation loss: 0.01311327, Gradient norm: 0.43459279
INFO:root:[   91] Training loss: 0.00838272, Validation loss: 0.01043133, Gradient norm: 0.43959673
INFO:root:[   92] Training loss: 0.00802771, Validation loss: 0.01227217, Gradient norm: 0.42754133
INFO:root:[   93] Training loss: 0.00824109, Validation loss: 0.01206695, Gradient norm: 0.41612050
INFO:root:[   94] Training loss: 0.00841961, Validation loss: 0.01123407, Gradient norm: 0.42464384
INFO:root:[   95] Training loss: 0.00830208, Validation loss: 0.01353042, Gradient norm: 0.42040098
INFO:root:[   96] Training loss: 0.00809086, Validation loss: 0.01198208, Gradient norm: 0.43664543
INFO:root:[   97] Training loss: 0.00747111, Validation loss: 0.01178022, Gradient norm: 0.38215230
INFO:root:[   98] Training loss: 0.00812146, Validation loss: 0.01169887, Gradient norm: 0.43259661
INFO:root:[   99] Training loss: 0.00801007, Validation loss: 0.01183680, Gradient norm: 0.42848742
INFO:root:[  100] Training loss: 0.00829010, Validation loss: 0.01052239, Gradient norm: 0.42100501
INFO:root:EP 100: Early stopping
INFO:root:Training the model took 1956.814s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00903
INFO:root:EnergyScoreTrain: 0.0048
INFO:root:CoverageTrain: 0.65018
INFO:root:IntervalWidthTrain: 0.0196
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0128
INFO:root:EnergyScoreValidation: 0.00795
INFO:root:CoverageValidation: 0.55072
INFO:root:IntervalWidthValidation: 0.0192
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01248
INFO:root:EnergyScoreTest: 0.00758
INFO:root:CoverageTest: 0.58625
INFO:root:IntervalWidthTest: 0.01937
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1149239296
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06218128, Validation loss: 0.03296061, Gradient norm: 0.65807196
INFO:root:[    2] Training loss: 0.03553547, Validation loss: 0.02913029, Gradient norm: 0.60421537
INFO:root:[    3] Training loss: 0.02969306, Validation loss: 0.02792604, Gradient norm: 0.56706732
INFO:root:[    4] Training loss: 0.02646911, Validation loss: 0.02159425, Gradient norm: 0.56058159
INFO:root:[    5] Training loss: 0.02588269, Validation loss: 0.02282902, Gradient norm: 0.56410829
INFO:root:[    6] Training loss: 0.02191457, Validation loss: 0.01769660, Gradient norm: 0.47537083
INFO:root:[    7] Training loss: 0.02044778, Validation loss: 0.01499614, Gradient norm: 0.43997098
INFO:root:[    8] Training loss: 0.02106583, Validation loss: 0.02201810, Gradient norm: 0.50417702
INFO:root:[    9] Training loss: 0.01938456, Validation loss: 0.01575879, Gradient norm: 0.46925212
INFO:root:[   10] Training loss: 0.01944632, Validation loss: 0.01600589, Gradient norm: 0.48662273
INFO:root:[   11] Training loss: 0.01860341, Validation loss: 0.01599851, Gradient norm: 0.44444472
INFO:root:[   12] Training loss: 0.01713349, Validation loss: 0.02012307, Gradient norm: 0.38203611
INFO:root:[   13] Training loss: 0.01903979, Validation loss: 0.01419126, Gradient norm: 0.49338533
INFO:root:[   14] Training loss: 0.01687302, Validation loss: 0.01376987, Gradient norm: 0.42340041
INFO:root:[   15] Training loss: 0.01674599, Validation loss: 0.01652043, Gradient norm: 0.35837667
INFO:root:[   16] Training loss: 0.01578313, Validation loss: 0.01832970, Gradient norm: 0.41301130
INFO:root:[   17] Training loss: 0.01602304, Validation loss: 0.01300681, Gradient norm: 0.42152394
INFO:root:[   18] Training loss: 0.01557453, Validation loss: 0.01626287, Gradient norm: 0.40269600
INFO:root:[   19] Training loss: 0.01653794, Validation loss: 0.01204775, Gradient norm: 0.43069803
INFO:root:[   20] Training loss: 0.01445887, Validation loss: 0.01460387, Gradient norm: 0.33588847
INFO:root:[   21] Training loss: 0.01495429, Validation loss: 0.01506036, Gradient norm: 0.40544384
INFO:root:[   22] Training loss: 0.01528409, Validation loss: 0.01615009, Gradient norm: 0.41315947
INFO:root:[   23] Training loss: 0.01409976, Validation loss: 0.01834818, Gradient norm: 0.37466867
INFO:root:[   24] Training loss: 0.01521959, Validation loss: 0.01260649, Gradient norm: 0.39253895
INFO:root:[   25] Training loss: 0.01387876, Validation loss: 0.01396990, Gradient norm: 0.39517993
INFO:root:[   26] Training loss: 0.01476751, Validation loss: 0.01590527, Gradient norm: 0.42585465
INFO:root:[   27] Training loss: 0.01420941, Validation loss: 0.01216770, Gradient norm: 0.44333811
INFO:root:[   28] Training loss: 0.01368944, Validation loss: 0.01411558, Gradient norm: 0.41284829
INFO:root:[   29] Training loss: 0.01357298, Validation loss: 0.01406729, Gradient norm: 0.40172420
INFO:root:[   30] Training loss: 0.01405610, Validation loss: 0.01316428, Gradient norm: 0.40328012
INFO:root:[   31] Training loss: 0.01377013, Validation loss: 0.01321590, Gradient norm: 0.38013598
INFO:root:[   32] Training loss: 0.01249892, Validation loss: 0.01318223, Gradient norm: 0.34598004
INFO:root:[   33] Training loss: 0.01386803, Validation loss: 0.01260709, Gradient norm: 0.42850957
INFO:root:[   34] Training loss: 0.01257207, Validation loss: 0.01273837, Gradient norm: 0.39901288
INFO:root:[   35] Training loss: 0.01344331, Validation loss: 0.01383217, Gradient norm: 0.45086919
INFO:root:[   36] Training loss: 0.01262603, Validation loss: 0.01218079, Gradient norm: 0.38367669
INFO:root:[   37] Training loss: 0.01290018, Validation loss: 0.01233313, Gradient norm: 0.45386542
INFO:root:[   38] Training loss: 0.01291741, Validation loss: 0.01480158, Gradient norm: 0.39004810
INFO:root:[   39] Training loss: 0.01265448, Validation loss: 0.01437917, Gradient norm: 0.40606710
INFO:root:[   40] Training loss: 0.01202060, Validation loss: 0.01389836, Gradient norm: 0.35505363
INFO:root:[   41] Training loss: 0.01174562, Validation loss: 0.01380847, Gradient norm: 0.36967307
INFO:root:[   42] Training loss: 0.01260780, Validation loss: 0.01321376, Gradient norm: 0.42545401
INFO:root:[   43] Training loss: 0.01190306, Validation loss: 0.01302691, Gradient norm: 0.38055200
INFO:root:[   44] Training loss: 0.01193048, Validation loss: 0.01373906, Gradient norm: 0.44593125
INFO:root:[   45] Training loss: 0.01195292, Validation loss: 0.01381766, Gradient norm: 0.40086532
INFO:root:[   46] Training loss: 0.01153448, Validation loss: 0.01280070, Gradient norm: 0.37685305
INFO:root:[   47] Training loss: 0.01154458, Validation loss: 0.01389751, Gradient norm: 0.37711474
INFO:root:[   48] Training loss: 0.01132580, Validation loss: 0.01327729, Gradient norm: 0.39478018
INFO:root:[   49] Training loss: 0.01049071, Validation loss: 0.01418020, Gradient norm: 0.31521988
INFO:root:[   50] Training loss: 0.01147442, Validation loss: 0.01268965, Gradient norm: 0.40030474
INFO:root:[   51] Training loss: 0.01054556, Validation loss: 0.01198963, Gradient norm: 0.39669517
INFO:root:[   52] Training loss: 0.01080551, Validation loss: 0.01350674, Gradient norm: 0.34866437
INFO:root:[   53] Training loss: 0.01139620, Validation loss: 0.01696982, Gradient norm: 0.39502167
INFO:root:[   54] Training loss: 0.01215124, Validation loss: 0.01555109, Gradient norm: 0.39663568
INFO:root:[   55] Training loss: 0.01088668, Validation loss: 0.01602021, Gradient norm: 0.37018183
INFO:root:[   56] Training loss: 0.01076865, Validation loss: 0.01059835, Gradient norm: 0.39459116
INFO:root:[   57] Training loss: 0.01092577, Validation loss: 0.01307305, Gradient norm: 0.40381325
INFO:root:[   58] Training loss: 0.01050660, Validation loss: 0.01424721, Gradient norm: 0.42933295
INFO:root:[   59] Training loss: 0.01077328, Validation loss: 0.01516217, Gradient norm: 0.38217368
INFO:root:[   60] Training loss: 0.01078127, Validation loss: 0.01266716, Gradient norm: 0.34949375
INFO:root:[   61] Training loss: 0.01010177, Validation loss: 0.01264739, Gradient norm: 0.39796067
INFO:root:[   62] Training loss: 0.01062667, Validation loss: 0.01462322, Gradient norm: 0.40103694
INFO:root:[   63] Training loss: 0.01041600, Validation loss: 0.01169441, Gradient norm: 0.37835867
INFO:root:[   64] Training loss: 0.01101288, Validation loss: 0.01304689, Gradient norm: 0.45051365
INFO:root:[   65] Training loss: 0.01011969, Validation loss: 0.01580742, Gradient norm: 0.40682894
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1271.501s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00968
INFO:root:EnergyScoreTrain: 0.00529
INFO:root:CoverageTrain: 0.6296
INFO:root:IntervalWidthTrain: 0.02059
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01363
INFO:root:EnergyScoreValidation: 0.00835
INFO:root:CoverageValidation: 0.57197
INFO:root:IntervalWidthValidation: 0.02183
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01442
INFO:root:EnergyScoreTest: 0.00913
INFO:root:CoverageTest: 0.5053
INFO:root:IntervalWidthTest: 0.01929
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1147142144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06094721, Validation loss: 0.02942474, Gradient norm: 0.55577037
INFO:root:[    2] Training loss: 0.03484472, Validation loss: 0.02824508, Gradient norm: 0.47619892
INFO:root:[    3] Training loss: 0.03194874, Validation loss: 0.03177018, Gradient norm: 0.51498704
INFO:root:[    4] Training loss: 0.02487534, Validation loss: 0.02380183, Gradient norm: 0.37969633
INFO:root:[    5] Training loss: 0.02423821, Validation loss: 0.02462795, Gradient norm: 0.44774066
INFO:root:[    6] Training loss: 0.02340517, Validation loss: 0.02040264, Gradient norm: 0.43654237
INFO:root:[    7] Training loss: 0.02220889, Validation loss: 0.02160156, Gradient norm: 0.41938485
INFO:root:[    8] Training loss: 0.02080232, Validation loss: 0.01748728, Gradient norm: 0.37576979
INFO:root:[    9] Training loss: 0.01973894, Validation loss: 0.02580511, Gradient norm: 0.38391599
INFO:root:[   10] Training loss: 0.02019673, Validation loss: 0.01440757, Gradient norm: 0.40829053
INFO:root:[   11] Training loss: 0.01896848, Validation loss: 0.01606967, Gradient norm: 0.35727449
INFO:root:[   12] Training loss: 0.01918076, Validation loss: 0.01497879, Gradient norm: 0.43363014
INFO:root:[   13] Training loss: 0.01698769, Validation loss: 0.01407798, Gradient norm: 0.30939063
INFO:root:[   14] Training loss: 0.01825896, Validation loss: 0.01753926, Gradient norm: 0.40598909
INFO:root:[   15] Training loss: 0.01742067, Validation loss: 0.01795583, Gradient norm: 0.38400074
INFO:root:[   16] Training loss: 0.01682648, Validation loss: 0.01312993, Gradient norm: 0.37296686
INFO:root:[   17] Training loss: 0.01729026, Validation loss: 0.01983784, Gradient norm: 0.41758449
INFO:root:[   18] Training loss: 0.01680238, Validation loss: 0.01586422, Gradient norm: 0.39077283
INFO:root:[   19] Training loss: 0.01702402, Validation loss: 0.01431657, Gradient norm: 0.40514207
INFO:root:[   20] Training loss: 0.01692007, Validation loss: 0.02056022, Gradient norm: 0.38204701
INFO:root:[   21] Training loss: 0.01581691, Validation loss: 0.01848383, Gradient norm: 0.36994136
INFO:root:[   22] Training loss: 0.01573683, Validation loss: 0.01343291, Gradient norm: 0.34917423
INFO:root:[   23] Training loss: 0.01508098, Validation loss: 0.01688716, Gradient norm: 0.36052073
INFO:root:[   24] Training loss: 0.01633363, Validation loss: 0.01370184, Gradient norm: 0.44366742
INFO:root:[   25] Training loss: 0.01563264, Validation loss: 0.01762148, Gradient norm: 0.40973218
INFO:root:[   26] Training loss: 0.01590941, Validation loss: 0.01455752, Gradient norm: 0.42713677
INFO:root:[   27] Training loss: 0.01452188, Validation loss: 0.01276527, Gradient norm: 0.32126727
INFO:root:[   28] Training loss: 0.01559964, Validation loss: 0.01571180, Gradient norm: 0.38853028
INFO:root:[   29] Training loss: 0.01459709, Validation loss: 0.01807172, Gradient norm: 0.34628545
INFO:root:[   30] Training loss: 0.01492913, Validation loss: 0.01494551, Gradient norm: 0.43209707
INFO:root:[   31] Training loss: 0.01356727, Validation loss: 0.01281589, Gradient norm: 0.30145964
INFO:root:[   32] Training loss: 0.01443827, Validation loss: 0.01375687, Gradient norm: 0.37877297
INFO:root:[   33] Training loss: 0.01467266, Validation loss: 0.01301825, Gradient norm: 0.40944279
INFO:root:[   34] Training loss: 0.01479393, Validation loss: 0.01778364, Gradient norm: 0.41159175
INFO:root:[   35] Training loss: 0.01478892, Validation loss: 0.01604389, Gradient norm: 0.38148135
INFO:root:[   36] Training loss: 0.01353896, Validation loss: 0.01468882, Gradient norm: 0.36852602
INFO:root:[   37] Training loss: 0.01415160, Validation loss: 0.01176307, Gradient norm: 0.33094653
INFO:root:[   38] Training loss: 0.01358693, Validation loss: 0.01600471, Gradient norm: 0.38088541
INFO:root:[   39] Training loss: 0.01411259, Validation loss: 0.01552633, Gradient norm: 0.37602614
INFO:root:[   40] Training loss: 0.01303772, Validation loss: 0.01265396, Gradient norm: 0.28977770
INFO:root:[   41] Training loss: 0.01399265, Validation loss: 0.01338643, Gradient norm: 0.41910222
INFO:root:[   42] Training loss: 0.01269991, Validation loss: 0.01295448, Gradient norm: 0.37144138
INFO:root:[   43] Training loss: 0.01295133, Validation loss: 0.01409585, Gradient norm: 0.33122209
INFO:root:[   44] Training loss: 0.01341085, Validation loss: 0.01723805, Gradient norm: 0.42482096
INFO:root:[   45] Training loss: 0.01345288, Validation loss: 0.01741006, Gradient norm: 0.33662663
INFO:root:[   46] Training loss: 0.01361556, Validation loss: 0.01285208, Gradient norm: 0.40963387
INFO:root:[   47] Training loss: 0.01199471, Validation loss: 0.01200827, Gradient norm: 0.34520101
INFO:root:[   48] Training loss: 0.01286207, Validation loss: 0.01116047, Gradient norm: 0.38569107
INFO:root:[   49] Training loss: 0.01191153, Validation loss: 0.01395794, Gradient norm: 0.28614543
INFO:root:[   50] Training loss: 0.01238015, Validation loss: 0.01171854, Gradient norm: 0.38763237
INFO:root:[   51] Training loss: 0.01243185, Validation loss: 0.01477899, Gradient norm: 0.40220761
INFO:root:[   52] Training loss: 0.01275317, Validation loss: 0.01255975, Gradient norm: 0.37583606
INFO:root:[   53] Training loss: 0.01249265, Validation loss: 0.01487944, Gradient norm: 0.39444377
INFO:root:[   54] Training loss: 0.01337133, Validation loss: 0.01165776, Gradient norm: 0.34042120
INFO:root:[   55] Training loss: 0.01212418, Validation loss: 0.01197553, Gradient norm: 0.32014427
INFO:root:[   56] Training loss: 0.01200565, Validation loss: 0.01123348, Gradient norm: 0.35050164
INFO:root:[   57] Training loss: 0.01234494, Validation loss: 0.01335339, Gradient norm: 0.36542856
INFO:root:[   58] Training loss: 0.01226475, Validation loss: 0.01093531, Gradient norm: 0.36698558
INFO:root:[   59] Training loss: 0.01186431, Validation loss: 0.01111511, Gradient norm: 0.31340092
INFO:root:[   60] Training loss: 0.01153920, Validation loss: 0.01276717, Gradient norm: 0.34555299
INFO:root:[   61] Training loss: 0.01228681, Validation loss: 0.01562271, Gradient norm: 0.37871221
INFO:root:[   62] Training loss: 0.01207940, Validation loss: 0.01208614, Gradient norm: 0.33989324
INFO:root:[   63] Training loss: 0.01203979, Validation loss: 0.01436347, Gradient norm: 0.37108541
INFO:root:[   64] Training loss: 0.01154949, Validation loss: 0.01315308, Gradient norm: 0.39879105
INFO:root:[   65] Training loss: 0.01087088, Validation loss: 0.01389584, Gradient norm: 0.30692411
INFO:root:[   66] Training loss: 0.01225682, Validation loss: 0.01410447, Gradient norm: 0.37171152
INFO:root:[   67] Training loss: 0.01201792, Validation loss: 0.01206093, Gradient norm: 0.40813271
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 1313.781s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01189
INFO:root:EnergyScoreTrain: 0.00665
INFO:root:CoverageTrain: 0.62052
INFO:root:IntervalWidthTrain: 0.02534
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01393
INFO:root:EnergyScoreValidation: 0.00793
INFO:root:CoverageValidation: 0.62315
INFO:root:IntervalWidthValidation: 0.02786
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01421
INFO:root:EnergyScoreTest: 0.00827
INFO:root:CoverageTest: 0.58281
INFO:root:IntervalWidthTest: 0.02599
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1147142144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06505621, Validation loss: 0.04402343, Gradient norm: 0.52381007
INFO:root:[    2] Training loss: 0.03944038, Validation loss: 0.02485743, Gradient norm: 0.52176803
INFO:root:[    3] Training loss: 0.03471572, Validation loss: 0.02946213, Gradient norm: 0.55854869
INFO:root:[    4] Training loss: 0.02984205, Validation loss: 0.01987881, Gradient norm: 0.47249114
INFO:root:[    5] Training loss: 0.02752782, Validation loss: 0.02448009, Gradient norm: 0.49222245
INFO:root:[    6] Training loss: 0.02675857, Validation loss: 0.02054612, Gradient norm: 0.49701477
INFO:root:[    7] Training loss: 0.02551451, Validation loss: 0.02188828, Gradient norm: 0.49128825
INFO:root:[    8] Training loss: 0.02339999, Validation loss: 0.01813961, Gradient norm: 0.41259067
INFO:root:[    9] Training loss: 0.02353759, Validation loss: 0.01816705, Gradient norm: 0.45642389
INFO:root:[   10] Training loss: 0.02322157, Validation loss: 0.02251986, Gradient norm: 0.48084066
INFO:root:[   11] Training loss: 0.02151115, Validation loss: 0.01571867, Gradient norm: 0.42017799
INFO:root:[   12] Training loss: 0.02190098, Validation loss: 0.01521559, Gradient norm: 0.45003281
INFO:root:[   13] Training loss: 0.01925508, Validation loss: 0.01925876, Gradient norm: 0.35010427
INFO:root:[   14] Training loss: 0.02039269, Validation loss: 0.01668955, Gradient norm: 0.40540953
INFO:root:[   15] Training loss: 0.02012957, Validation loss: 0.01379385, Gradient norm: 0.43164047
INFO:root:[   16] Training loss: 0.01966834, Validation loss: 0.02296416, Gradient norm: 0.39351898
INFO:root:[   17] Training loss: 0.01969971, Validation loss: 0.02226962, Gradient norm: 0.42061253
INFO:root:[   18] Training loss: 0.01809538, Validation loss: 0.01372943, Gradient norm: 0.36272240
INFO:root:[   19] Training loss: 0.01909662, Validation loss: 0.01378505, Gradient norm: 0.43457112
INFO:root:[   20] Training loss: 0.01737777, Validation loss: 0.01666052, Gradient norm: 0.33295302
INFO:root:[   21] Training loss: 0.01820581, Validation loss: 0.01423883, Gradient norm: 0.36636543
INFO:root:[   22] Training loss: 0.01742947, Validation loss: 0.01834793, Gradient norm: 0.34917145
INFO:root:[   23] Training loss: 0.01773221, Validation loss: 0.01368343, Gradient norm: 0.36986479
INFO:root:[   24] Training loss: 0.01764455, Validation loss: 0.01851145, Gradient norm: 0.41167739
INFO:root:[   25] Training loss: 0.01553387, Validation loss: 0.01280513, Gradient norm: 0.29404186
INFO:root:[   26] Training loss: 0.01752720, Validation loss: 0.01474726, Gradient norm: 0.33657274
INFO:root:[   27] Training loss: 0.01666033, Validation loss: 0.01901401, Gradient norm: 0.36042602
INFO:root:[   28] Training loss: 0.01641500, Validation loss: 0.01748295, Gradient norm: 0.33236836
INFO:root:[   29] Training loss: 0.01554871, Validation loss: 0.01396608, Gradient norm: 0.35037691
INFO:root:[   30] Training loss: 0.01607517, Validation loss: 0.01561989, Gradient norm: 0.35070659
INFO:root:[   31] Training loss: 0.01639435, Validation loss: 0.01500234, Gradient norm: 0.38724011
INFO:root:[   32] Training loss: 0.01551031, Validation loss: 0.01422515, Gradient norm: 0.36352349
INFO:root:[   33] Training loss: 0.01491295, Validation loss: 0.01246252, Gradient norm: 0.31464279
INFO:root:[   34] Training loss: 0.01537103, Validation loss: 0.01582446, Gradient norm: 0.30710859
INFO:root:[   35] Training loss: 0.01643671, Validation loss: 0.01466122, Gradient norm: 0.38151856
INFO:root:[   36] Training loss: 0.01498137, Validation loss: 0.01348914, Gradient norm: 0.38050965
INFO:root:[   37] Training loss: 0.01474108, Validation loss: 0.01261254, Gradient norm: 0.32891017
INFO:root:[   38] Training loss: 0.01509106, Validation loss: 0.01856494, Gradient norm: 0.29198672
INFO:root:[   39] Training loss: 0.01525479, Validation loss: 0.02007716, Gradient norm: 0.36496209
INFO:root:[   40] Training loss: 0.01539130, Validation loss: 0.01901159, Gradient norm: 0.41774642
INFO:root:[   41] Training loss: 0.01425674, Validation loss: 0.01232622, Gradient norm: 0.34128284
INFO:root:[   42] Training loss: 0.01506701, Validation loss: 0.01381241, Gradient norm: 0.31528513
INFO:root:[   43] Training loss: 0.01461737, Validation loss: 0.01187780, Gradient norm: 0.35197389
INFO:root:[   44] Training loss: 0.01467767, Validation loss: 0.01215392, Gradient norm: 0.34210953
INFO:root:[   45] Training loss: 0.01462335, Validation loss: 0.01576790, Gradient norm: 0.36426712
INFO:root:[   46] Training loss: 0.01422972, Validation loss: 0.01585264, Gradient norm: 0.33581919
INFO:root:[   47] Training loss: 0.01412513, Validation loss: 0.01715134, Gradient norm: 0.34533185
INFO:root:[   48] Training loss: 0.01414183, Validation loss: 0.01259696, Gradient norm: 0.36018408
INFO:root:[   49] Training loss: 0.01319428, Validation loss: 0.01394780, Gradient norm: 0.30448237
INFO:root:[   50] Training loss: 0.01470813, Validation loss: 0.01665902, Gradient norm: 0.38654455
INFO:root:[   51] Training loss: 0.01387240, Validation loss: 0.01526118, Gradient norm: 0.32700064
INFO:root:[   52] Training loss: 0.01376438, Validation loss: 0.01274929, Gradient norm: 0.36803569
INFO:root:[   53] Training loss: 0.01328578, Validation loss: 0.01593218, Gradient norm: 0.37826906
INFO:root:[   54] Training loss: 0.01408991, Validation loss: 0.01331926, Gradient norm: 0.38066535
INFO:root:[   55] Training loss: 0.01353885, Validation loss: 0.01131664, Gradient norm: 0.33365806
INFO:root:[   56] Training loss: 0.01383147, Validation loss: 0.01828492, Gradient norm: 0.38586637
INFO:root:[   57] Training loss: 0.01311009, Validation loss: 0.01313468, Gradient norm: 0.33352754
INFO:root:[   58] Training loss: 0.01272743, Validation loss: 0.01562374, Gradient norm: 0.34310719
INFO:root:[   59] Training loss: 0.01311777, Validation loss: 0.01676162, Gradient norm: 0.36647828
INFO:root:[   60] Training loss: 0.01303850, Validation loss: 0.01468661, Gradient norm: 0.31693843
INFO:root:[   61] Training loss: 0.01231633, Validation loss: 0.01169667, Gradient norm: 0.28524816
INFO:root:[   62] Training loss: 0.01366534, Validation loss: 0.01384667, Gradient norm: 0.38974311
INFO:root:[   63] Training loss: 0.01294924, Validation loss: 0.01564605, Gradient norm: 0.41032988
INFO:root:[   64] Training loss: 0.01376344, Validation loss: 0.01827943, Gradient norm: 0.34231251
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 1260.785s.
INFO:root:Emptying the cuda cache took 0.004s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01182
INFO:root:EnergyScoreTrain: 0.00658
INFO:root:CoverageTrain: 0.60932
INFO:root:IntervalWidthTrain: 0.02566
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01475
INFO:root:EnergyScoreValidation: 0.00894
INFO:root:CoverageValidation: 0.51154
INFO:root:IntervalWidthValidation: 0.02275
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01381
INFO:root:EnergyScoreTest: 0.00812
INFO:root:CoverageTest: 0.55198
INFO:root:IntervalWidthTest: 0.0247
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1147142144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06697001, Validation loss: 0.03749248, Gradient norm: 0.52385501
INFO:root:[    2] Training loss: 0.04472880, Validation loss: 0.02445358, Gradient norm: 0.52932920
INFO:root:[    3] Training loss: 0.03527772, Validation loss: 0.02411124, Gradient norm: 0.49298772
INFO:root:[    4] Training loss: 0.02986308, Validation loss: 0.02318595, Gradient norm: 0.40814295
INFO:root:[    5] Training loss: 0.02900919, Validation loss: 0.01933430, Gradient norm: 0.43483565
INFO:root:[    6] Training loss: 0.02707360, Validation loss: 0.01760057, Gradient norm: 0.43736789
INFO:root:[    7] Training loss: 0.02568731, Validation loss: 0.01462184, Gradient norm: 0.45576316
INFO:root:[    8] Training loss: 0.02367523, Validation loss: 0.01875336, Gradient norm: 0.36586414
INFO:root:[    9] Training loss: 0.02531620, Validation loss: 0.01889060, Gradient norm: 0.43454012
INFO:root:[   10] Training loss: 0.02175656, Validation loss: 0.01603585, Gradient norm: 0.27339065
INFO:root:[   11] Training loss: 0.02213131, Validation loss: 0.02987450, Gradient norm: 0.42731527
INFO:root:[   12] Training loss: 0.02231302, Validation loss: 0.01626149, Gradient norm: 0.43598918
INFO:root:[   13] Training loss: 0.02021697, Validation loss: 0.01424975, Gradient norm: 0.31814070
INFO:root:[   14] Training loss: 0.02198376, Validation loss: 0.01479606, Gradient norm: 0.43630898
INFO:root:[   15] Training loss: 0.02009566, Validation loss: 0.02255603, Gradient norm: 0.39182855
INFO:root:[   16] Training loss: 0.01909475, Validation loss: 0.02285877, Gradient norm: 0.36365797
INFO:root:[   17] Training loss: 0.01923785, Validation loss: 0.02102040, Gradient norm: 0.37693123
INFO:root:[   18] Training loss: 0.01861677, Validation loss: 0.01855015, Gradient norm: 0.30946597
INFO:root:[   19] Training loss: 0.01864166, Validation loss: 0.01614970, Gradient norm: 0.32987761
INFO:root:[   20] Training loss: 0.01737517, Validation loss: 0.01598909, Gradient norm: 0.27234223
INFO:root:[   21] Training loss: 0.01788743, Validation loss: 0.01572211, Gradient norm: 0.31969805
INFO:root:[   22] Training loss: 0.01988350, Validation loss: 0.02029911, Gradient norm: 0.42674639
INFO:root:[   23] Training loss: 0.01802355, Validation loss: 0.02393298, Gradient norm: 0.34730659
INFO:root:[   24] Training loss: 0.01811503, Validation loss: 0.01693276, Gradient norm: 0.40875510
INFO:root:[   25] Training loss: 0.01663186, Validation loss: 0.01340577, Gradient norm: 0.34657084
INFO:root:[   26] Training loss: 0.01721989, Validation loss: 0.01780378, Gradient norm: 0.31470156
INFO:root:[   27] Training loss: 0.01697380, Validation loss: 0.01455779, Gradient norm: 0.38069801
INFO:root:[   28] Training loss: 0.01732811, Validation loss: 0.01695069, Gradient norm: 0.33350000
INFO:root:[   29] Training loss: 0.01647286, Validation loss: 0.01688957, Gradient norm: 0.36825991
INFO:root:[   30] Training loss: 0.01748237, Validation loss: 0.01848834, Gradient norm: 0.38365392
INFO:root:[   31] Training loss: 0.01741223, Validation loss: 0.01848140, Gradient norm: 0.39568164
INFO:root:[   32] Training loss: 0.01669078, Validation loss: 0.01468448, Gradient norm: 0.37379017
INFO:root:[   33] Training loss: 0.01755714, Validation loss: 0.02131755, Gradient norm: 0.43440351
INFO:root:[   34] Training loss: 0.01637725, Validation loss: 0.01546609, Gradient norm: 0.32200440
INFO:root:[   35] Training loss: 0.01566541, Validation loss: 0.01399436, Gradient norm: 0.33111588
INFO:root:[   36] Training loss: 0.01605743, Validation loss: 0.01445910, Gradient norm: 0.34530461
INFO:root:[   37] Training loss: 0.01607440, Validation loss: 0.01904361, Gradient norm: 0.37365140
INFO:root:[   38] Training loss: 0.01579365, Validation loss: 0.01753649, Gradient norm: 0.33069810
INFO:root:[   39] Training loss: 0.01614538, Validation loss: 0.01563773, Gradient norm: 0.36649594
INFO:root:[   40] Training loss: 0.01536180, Validation loss: 0.01871443, Gradient norm: 0.36182112
INFO:root:[   41] Training loss: 0.01604985, Validation loss: 0.01461127, Gradient norm: 0.38359530
INFO:root:[   42] Training loss: 0.01534487, Validation loss: 0.02055831, Gradient norm: 0.35859232
INFO:root:[   43] Training loss: 0.01499152, Validation loss: 0.01506516, Gradient norm: 0.34904588
INFO:root:[   44] Training loss: 0.01540018, Validation loss: 0.01683441, Gradient norm: 0.32765039
INFO:root:[   45] Training loss: 0.01495722, Validation loss: 0.01442536, Gradient norm: 0.33775566
INFO:root:[   46] Training loss: 0.01470735, Validation loss: 0.01311465, Gradient norm: 0.34524521
