INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 1000, 'downscaling_factor': 2}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 25165824
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09194073, Validation loss: 0.06328279, Gradient norm: 0.77899737
INFO:root:[    2] Training loss: 0.05205347, Validation loss: 0.04643861, Gradient norm: 1.07577981
INFO:root:[    3] Training loss: 0.04622306, Validation loss: 0.04078963, Gradient norm: 1.37291866
INFO:root:[    4] Training loss: 0.03957115, Validation loss: 0.04335407, Gradient norm: 0.57710431
INFO:root:[    5] Training loss: 0.03909369, Validation loss: 0.03767704, Gradient norm: 0.87537431
INFO:root:[    6] Training loss: 0.03854642, Validation loss: 0.04267948, Gradient norm: 1.05490889
INFO:root:[    7] Training loss: 0.03566637, Validation loss: 0.03400251, Gradient norm: 0.68674870
INFO:root:[    8] Training loss: 0.03261516, Validation loss: 0.03138826, Gradient norm: 0.72061538
INFO:root:[    9] Training loss: 0.03231691, Validation loss: 0.03372111, Gradient norm: 0.93934554
INFO:root:[   10] Training loss: 0.03176314, Validation loss: 0.02906638, Gradient norm: 0.96678634
INFO:root:[   11] Training loss: 0.02940448, Validation loss: 0.02819507, Gradient norm: 0.62019385
INFO:root:[   12] Training loss: 0.02969999, Validation loss: 0.02822169, Gradient norm: 0.97181700
INFO:root:[   13] Training loss: 0.02893488, Validation loss: 0.02804056, Gradient norm: 0.73601489
INFO:root:[   14] Training loss: 0.02723208, Validation loss: 0.02611483, Gradient norm: 0.79762858
INFO:root:[   15] Training loss: 0.02683323, Validation loss: 0.02588479, Gradient norm: 0.68446550
INFO:root:[   16] Training loss: 0.02626396, Validation loss: 0.02942827, Gradient norm: 0.76339112
INFO:root:[   17] Training loss: 0.02636382, Validation loss: 0.02946130, Gradient norm: 0.99498555
INFO:root:[   18] Training loss: 0.02647135, Validation loss: 0.02428495, Gradient norm: 1.00583815
INFO:root:[   19] Training loss: 0.02461239, Validation loss: 0.02559793, Gradient norm: 0.60167963
INFO:root:[   20] Training loss: 0.02606028, Validation loss: 0.02385851, Gradient norm: 1.05421024
INFO:root:[   21] Training loss: 0.02546622, Validation loss: 0.02313642, Gradient norm: 0.74944400
INFO:root:[   22] Training loss: 0.02482463, Validation loss: 0.02433295, Gradient norm: 0.76069377
INFO:root:[   23] Training loss: 0.02528019, Validation loss: 0.02507205, Gradient norm: 0.99998378
INFO:root:[   24] Training loss: 0.02398928, Validation loss: 0.02422639, Gradient norm: 0.80121562
INFO:root:[   25] Training loss: 0.02404209, Validation loss: 0.02360791, Gradient norm: 0.65793900
INFO:root:[   26] Training loss: 0.02258407, Validation loss: 0.02408377, Gradient norm: 0.62778960
INFO:root:[   27] Training loss: 0.02283962, Validation loss: 0.02249146, Gradient norm: 0.67459695
INFO:root:[   28] Training loss: 0.02323900, Validation loss: 0.02254382, Gradient norm: 0.84385303
INFO:root:[   29] Training loss: 0.02304621, Validation loss: 0.02211182, Gradient norm: 0.69396316
INFO:root:[   30] Training loss: 0.02251215, Validation loss: 0.02200479, Gradient norm: 0.56494769
INFO:root:[   31] Training loss: 0.02257169, Validation loss: 0.02590603, Gradient norm: 0.79592560
INFO:root:[   32] Training loss: 0.02433519, Validation loss: 0.02512744, Gradient norm: 1.05575470
INFO:root:[   33] Training loss: 0.02229622, Validation loss: 0.02274961, Gradient norm: 0.68602304
INFO:root:[   34] Training loss: 0.02257395, Validation loss: 0.02156685, Gradient norm: 0.54352802
INFO:root:[   35] Training loss: 0.02250304, Validation loss: 0.02111476, Gradient norm: 0.70933247
INFO:root:[   36] Training loss: 0.02118542, Validation loss: 0.02158137, Gradient norm: 0.59423282
INFO:root:[   37] Training loss: 0.02168399, Validation loss: 0.02102399, Gradient norm: 0.56393834
INFO:root:[   38] Training loss: 0.02221629, Validation loss: 0.02127515, Gradient norm: 0.65212826
INFO:root:[   39] Training loss: 0.02273568, Validation loss: 0.02340831, Gradient norm: 0.97390605
INFO:root:[   40] Training loss: 0.02277174, Validation loss: 0.02190491, Gradient norm: 0.89153860
INFO:root:[   41] Training loss: 0.02196160, Validation loss: 0.02375207, Gradient norm: 0.77555319
INFO:root:[   42] Training loss: 0.02234086, Validation loss: 0.02511087, Gradient norm: 0.95403358
INFO:root:[   43] Training loss: 0.02341731, Validation loss: 0.02100495, Gradient norm: 1.10465000
INFO:root:[   44] Training loss: 0.02219356, Validation loss: 0.02348974, Gradient norm: 0.73669075
INFO:root:[   45] Training loss: 0.02183342, Validation loss: 0.02150865, Gradient norm: 0.74927153
INFO:root:[   46] Training loss: 0.02375307, Validation loss: 0.02110033, Gradient norm: 1.02706443
INFO:root:[   47] Training loss: 0.02221462, Validation loss: 0.02134218, Gradient norm: 0.94095244
INFO:root:[   48] Training loss: 0.02013251, Validation loss: 0.02070781, Gradient norm: 0.56308612
INFO:root:[   49] Training loss: 0.02109323, Validation loss: 0.02213079, Gradient norm: 0.74345711
INFO:root:[   50] Training loss: 0.02210514, Validation loss: 0.02404914, Gradient norm: 0.86440738
INFO:root:[   51] Training loss: 0.02159969, Validation loss: 0.02170549, Gradient norm: 0.86546681
INFO:root:[   52] Training loss: 0.02358465, Validation loss: 0.02387637, Gradient norm: 1.06307318
INFO:root:[   53] Training loss: 0.02150332, Validation loss: 0.02366922, Gradient norm: 0.76943700
INFO:root:[   54] Training loss: 0.02199755, Validation loss: 0.02085116, Gradient norm: 0.79924317
INFO:root:[   55] Training loss: 0.02156313, Validation loss: 0.02019996, Gradient norm: 0.74524736
INFO:root:[   56] Training loss: 0.02043094, Validation loss: 0.02058298, Gradient norm: 0.54396468
INFO:root:[   57] Training loss: 0.01992202, Validation loss: 0.02068651, Gradient norm: 0.47411735
INFO:root:[   58] Training loss: 0.02098272, Validation loss: 0.02104515, Gradient norm: 0.56599179
INFO:root:[   59] Training loss: 0.02029613, Validation loss: 0.02028117, Gradient norm: 0.64110966
INFO:root:[   60] Training loss: 0.01999898, Validation loss: 0.02032102, Gradient norm: 0.42880218
INFO:root:[   61] Training loss: 0.02008660, Validation loss: 0.01985736, Gradient norm: 0.61834143
INFO:root:[   62] Training loss: 0.02078797, Validation loss: 0.02074674, Gradient norm: 0.59412254
INFO:root:[   63] Training loss: 0.02009671, Validation loss: 0.02022813, Gradient norm: 0.70445625
INFO:root:[   64] Training loss: 0.01981305, Validation loss: 0.02058134, Gradient norm: 0.58870301
INFO:root:[   65] Training loss: 0.01994024, Validation loss: 0.02017931, Gradient norm: 0.55405065
INFO:root:[   66] Training loss: 0.01978618, Validation loss: 0.02142567, Gradient norm: 0.58421790
INFO:root:[   67] Training loss: 0.02037150, Validation loss: 0.01974570, Gradient norm: 0.67304605
INFO:root:[   68] Training loss: 0.02035326, Validation loss: 0.02125736, Gradient norm: 0.69024015
INFO:root:[   69] Training loss: 0.02226860, Validation loss: 0.02154723, Gradient norm: 0.91518662
INFO:root:[   70] Training loss: 0.02139711, Validation loss: 0.02149075, Gradient norm: 0.77469377
INFO:root:[   71] Training loss: 0.01995348, Validation loss: 0.02058801, Gradient norm: 0.75918920
INFO:root:[   72] Training loss: 0.02105519, Validation loss: 0.01966299, Gradient norm: 0.83355527
INFO:root:[   73] Training loss: 0.02023833, Validation loss: 0.02037027, Gradient norm: 0.70421452
INFO:root:[   74] Training loss: 0.02069573, Validation loss: 0.01925481, Gradient norm: 0.75617129
INFO:root:[   75] Training loss: 0.02056814, Validation loss: 0.01935680, Gradient norm: 0.68585065
INFO:root:[   76] Training loss: 0.02024456, Validation loss: 0.02180686, Gradient norm: 0.61698220
INFO:root:[   77] Training loss: 0.01938946, Validation loss: 0.01955795, Gradient norm: 0.63342886
INFO:root:[   78] Training loss: 0.01950918, Validation loss: 0.01904636, Gradient norm: 0.45215676
INFO:root:[   79] Training loss: 0.01945053, Validation loss: 0.01909894, Gradient norm: 0.55528508
INFO:root:[   80] Training loss: 0.01900822, Validation loss: 0.02119988, Gradient norm: 0.47669308
INFO:root:[   81] Training loss: 0.01913907, Validation loss: 0.01878529, Gradient norm: 0.53217951
INFO:root:[   82] Training loss: 0.01924855, Validation loss: 0.01946412, Gradient norm: 0.61918918
INFO:root:[   83] Training loss: 0.01921099, Validation loss: 0.02564862, Gradient norm: 0.62602311
INFO:root:[   84] Training loss: 0.02091069, Validation loss: 0.02100851, Gradient norm: 0.95190803
INFO:root:[   85] Training loss: 0.01936383, Validation loss: 0.01869077, Gradient norm: 0.69368124
INFO:root:[   86] Training loss: 0.01933594, Validation loss: 0.01891878, Gradient norm: 0.59729343
INFO:root:[   87] Training loss: 0.01935862, Validation loss: 0.01907865, Gradient norm: 0.54477460
INFO:root:[   88] Training loss: 0.01865310, Validation loss: 0.02072991, Gradient norm: 0.53271467
INFO:root:[   89] Training loss: 0.02036128, Validation loss: 0.01866684, Gradient norm: 0.84039714
INFO:root:[   90] Training loss: 0.01976553, Validation loss: 0.01874038, Gradient norm: 0.75567649
INFO:root:[   91] Training loss: 0.02060139, Validation loss: 0.02337575, Gradient norm: 0.89500169
INFO:root:[   92] Training loss: 0.01951908, Validation loss: 0.01831792, Gradient norm: 0.69151760
INFO:root:[   93] Training loss: 0.01891122, Validation loss: 0.01972680, Gradient norm: 0.43190855
INFO:root:[   94] Training loss: 0.01874338, Validation loss: 0.02055971, Gradient norm: 0.57856002
INFO:root:[   95] Training loss: 0.02013537, Validation loss: 0.02150027, Gradient norm: 0.72798283
INFO:root:[   96] Training loss: 0.01871250, Validation loss: 0.01846718, Gradient norm: 0.54254731
INFO:root:[   97] Training loss: 0.01832672, Validation loss: 0.01840138, Gradient norm: 0.52563189
INFO:root:[   98] Training loss: 0.01868690, Validation loss: 0.01829499, Gradient norm: 0.61493625
INFO:root:[   99] Training loss: 0.01838268, Validation loss: 0.01886992, Gradient norm: 0.38400623
INFO:root:[  100] Training loss: 0.01833226, Validation loss: 0.01832369, Gradient norm: 0.55763337
INFO:root:[  101] Training loss: 0.01824329, Validation loss: 0.02101902, Gradient norm: 0.52364886
INFO:root:[  102] Training loss: 0.01895503, Validation loss: 0.02256242, Gradient norm: 0.70992027
INFO:root:[  103] Training loss: 0.02027032, Validation loss: 0.01860898, Gradient norm: 0.81857770
INFO:root:[  104] Training loss: 0.01978797, Validation loss: 0.02091934, Gradient norm: 0.79951000
INFO:root:[  105] Training loss: 0.01831682, Validation loss: 0.01927668, Gradient norm: 0.60883492
INFO:root:[  106] Training loss: 0.01898699, Validation loss: 0.01900274, Gradient norm: 0.62846780
INFO:root:[  107] Training loss: 0.01899891, Validation loss: 0.02126640, Gradient norm: 0.69168806
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 611.887s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.78579
INFO:root:EnergyScoreTrain: 0.56518
INFO:root:CoverageTrain: 0.95185
INFO:root:IntervalWidthTrain: 0.11761
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.80703
INFO:root:EnergyScoreValidation: 0.5851
INFO:root:CoverageValidation: 0.94867
INFO:root:IntervalWidthValidation: 0.11709
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.81138
INFO:root:EnergyScoreTest: 1.38982
INFO:root:CoverageTest: 0.71808
INFO:root:IntervalWidthTest: 0.12565
INFO:root:###2 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08850362, Validation loss: 0.06447108, Gradient norm: 0.77975613
INFO:root:[    2] Training loss: 0.05342454, Validation loss: 0.04688280, Gradient norm: 0.67325200
INFO:root:[    3] Training loss: 0.04669258, Validation loss: 0.04379509, Gradient norm: 0.79671348
INFO:root:[    4] Training loss: 0.04006567, Validation loss: 0.03690538, Gradient norm: 0.40527498
INFO:root:[    5] Training loss: 0.03640576, Validation loss: 0.03501898, Gradient norm: 0.38534998
INFO:root:[    6] Training loss: 0.03482446, Validation loss: 0.03830885, Gradient norm: 0.45005418
INFO:root:[    7] Training loss: 0.03642401, Validation loss: 0.03472901, Gradient norm: 0.61075592
INFO:root:[    8] Training loss: 0.03341701, Validation loss: 0.03319560, Gradient norm: 0.35983110
INFO:root:[    9] Training loss: 0.03138888, Validation loss: 0.03313949, Gradient norm: 0.36730032
INFO:root:[   10] Training loss: 0.03206338, Validation loss: 0.03246091, Gradient norm: 0.41687327
INFO:root:[   11] Training loss: 0.03172436, Validation loss: 0.03079451, Gradient norm: 0.42827474
INFO:root:[   12] Training loss: 0.03227201, Validation loss: 0.03022798, Gradient norm: 0.44215650
INFO:root:[   13] Training loss: 0.03084106, Validation loss: 0.03008977, Gradient norm: 0.38300291
INFO:root:[   14] Training loss: 0.03077800, Validation loss: 0.02977874, Gradient norm: 0.44793355
INFO:root:[   15] Training loss: 0.03076938, Validation loss: 0.03052486, Gradient norm: 0.56332632
INFO:root:[   16] Training loss: 0.03082163, Validation loss: 0.02865392, Gradient norm: 0.42321459
INFO:root:[   17] Training loss: 0.03184868, Validation loss: 0.02851067, Gradient norm: 0.74020080
INFO:root:[   18] Training loss: 0.02846703, Validation loss: 0.02776504, Gradient norm: 0.38320679
INFO:root:[   19] Training loss: 0.02805013, Validation loss: 0.02797049, Gradient norm: 0.33148116
INFO:root:[   20] Training loss: 0.02778395, Validation loss: 0.02710377, Gradient norm: 0.33668311
INFO:root:[   21] Training loss: 0.02777596, Validation loss: 0.02982965, Gradient norm: 0.31283578
INFO:root:[   22] Training loss: 0.02815831, Validation loss: 0.02916240, Gradient norm: 0.44641494
INFO:root:[   23] Training loss: 0.02824908, Validation loss: 0.02678858, Gradient norm: 0.47151104
INFO:root:[   24] Training loss: 0.02749056, Validation loss: 0.02722007, Gradient norm: 0.39962311
INFO:root:[   25] Training loss: 0.02711523, Validation loss: 0.02682752, Gradient norm: 0.34789283
INFO:root:[   26] Training loss: 0.02774696, Validation loss: 0.02541799, Gradient norm: 0.48487031
INFO:root:[   27] Training loss: 0.02601388, Validation loss: 0.02537933, Gradient norm: 0.32466097
INFO:root:[   28] Training loss: 0.02538330, Validation loss: 0.02608614, Gradient norm: 0.29827075
INFO:root:[   29] Training loss: 0.02603460, Validation loss: 0.02520303, Gradient norm: 0.45648108
INFO:root:[   30] Training loss: 0.02492339, Validation loss: 0.02402835, Gradient norm: 0.35726852
INFO:root:[   31] Training loss: 0.02561686, Validation loss: 0.02420525, Gradient norm: 0.39836715
INFO:root:[   32] Training loss: 0.02463432, Validation loss: 0.02683831, Gradient norm: 0.37192563
INFO:root:[   33] Training loss: 0.02377321, Validation loss: 0.02443896, Gradient norm: 0.35529763
INFO:root:[   34] Training loss: 0.02373826, Validation loss: 0.02516266, Gradient norm: 0.32534975
INFO:root:[   35] Training loss: 0.02373008, Validation loss: 0.02510599, Gradient norm: 0.37552394
INFO:root:[   36] Training loss: 0.02398813, Validation loss: 0.02311711, Gradient norm: 0.41611928
INFO:root:[   37] Training loss: 0.02318532, Validation loss: 0.02295162, Gradient norm: 0.32174929
INFO:root:[   38] Training loss: 0.02590940, Validation loss: 0.02425259, Gradient norm: 0.69208454
INFO:root:[   39] Training loss: 0.02327211, Validation loss: 0.02413745, Gradient norm: 0.41219419
INFO:root:[   40] Training loss: 0.02358431, Validation loss: 0.02211908, Gradient norm: 0.34911121
INFO:root:[   41] Training loss: 0.02282749, Validation loss: 0.02211733, Gradient norm: 0.33193514
INFO:root:[   42] Training loss: 0.02288986, Validation loss: 0.02358050, Gradient norm: 0.33255935
INFO:root:[   43] Training loss: 0.02302690, Validation loss: 0.02423969, Gradient norm: 0.39829744
INFO:root:[   44] Training loss: 0.02369632, Validation loss: 0.02250613, Gradient norm: 0.54692229
INFO:root:[   45] Training loss: 0.02249727, Validation loss: 0.02219735, Gradient norm: 0.34929317
INFO:root:[   46] Training loss: 0.02307373, Validation loss: 0.02240783, Gradient norm: 0.46841578
INFO:root:[   47] Training loss: 0.02223399, Validation loss: 0.02165097, Gradient norm: 0.37282965
INFO:root:[   48] Training loss: 0.02175467, Validation loss: 0.02283915, Gradient norm: 0.36364162
INFO:root:[   49] Training loss: 0.02255128, Validation loss: 0.02222112, Gradient norm: 0.43957999
INFO:root:[   50] Training loss: 0.02229812, Validation loss: 0.02115981, Gradient norm: 0.35608150
INFO:root:[   51] Training loss: 0.02161414, Validation loss: 0.02132195, Gradient norm: 0.39502249
INFO:root:[   52] Training loss: 0.02066847, Validation loss: 0.02158938, Gradient norm: 0.33522944
INFO:root:[   53] Training loss: 0.02120501, Validation loss: 0.02123270, Gradient norm: 0.36197556
INFO:root:[   54] Training loss: 0.02123520, Validation loss: 0.02143918, Gradient norm: 0.38025615
INFO:root:[   55] Training loss: 0.02117595, Validation loss: 0.02139930, Gradient norm: 0.38476500
INFO:root:[   56] Training loss: 0.02173842, Validation loss: 0.02344245, Gradient norm: 0.44196206
INFO:root:[   57] Training loss: 0.02135869, Validation loss: 0.02142522, Gradient norm: 0.44190727
INFO:root:[   58] Training loss: 0.02191468, Validation loss: 0.02139985, Gradient norm: 0.52851866
INFO:root:[   59] Training loss: 0.02083058, Validation loss: 0.02111024, Gradient norm: 0.37120831
INFO:root:[   60] Training loss: 0.02060790, Validation loss: 0.02199054, Gradient norm: 0.38639884
INFO:root:[   61] Training loss: 0.02090641, Validation loss: 0.02138452, Gradient norm: 0.33734149
INFO:root:[   62] Training loss: 0.02124273, Validation loss: 0.02403115, Gradient norm: 0.39595208
INFO:root:[   63] Training loss: 0.02152793, Validation loss: 0.02154490, Gradient norm: 0.45527833
INFO:root:[   64] Training loss: 0.02075165, Validation loss: 0.02122535, Gradient norm: 0.37866869
INFO:root:[   65] Training loss: 0.02077247, Validation loss: 0.02090148, Gradient norm: 0.42351914
INFO:root:[   66] Training loss: 0.02028646, Validation loss: 0.02020592, Gradient norm: 0.36751099
INFO:root:[   67] Training loss: 0.02036931, Validation loss: 0.01988248, Gradient norm: 0.35278907
INFO:root:[   68] Training loss: 0.01993789, Validation loss: 0.02001864, Gradient norm: 0.34323320
INFO:root:[   69] Training loss: 0.01983557, Validation loss: 0.02053054, Gradient norm: 0.28891724
INFO:root:[   70] Training loss: 0.02028379, Validation loss: 0.01969504, Gradient norm: 0.30957700
INFO:root:[   71] Training loss: 0.02063658, Validation loss: 0.02102916, Gradient norm: 0.40483990
INFO:root:[   72] Training loss: 0.02126912, Validation loss: 0.02005213, Gradient norm: 0.52259431
INFO:root:[   73] Training loss: 0.02016129, Validation loss: 0.01988626, Gradient norm: 0.30336042
INFO:root:[   74] Training loss: 0.02010135, Validation loss: 0.02007157, Gradient norm: 0.35693617
INFO:root:[   75] Training loss: 0.02001763, Validation loss: 0.01936754, Gradient norm: 0.30352722
INFO:root:[   76] Training loss: 0.02035778, Validation loss: 0.01983015, Gradient norm: 0.33311872
INFO:root:[   77] Training loss: 0.01980335, Validation loss: 0.01997276, Gradient norm: 0.38871385
INFO:root:[   78] Training loss: 0.01990154, Validation loss: 0.01953102, Gradient norm: 0.40485997
INFO:root:[   79] Training loss: 0.02041047, Validation loss: 0.01943518, Gradient norm: 0.46039685
INFO:root:[   80] Training loss: 0.01981366, Validation loss: 0.01908221, Gradient norm: 0.37136495
INFO:root:[   81] Training loss: 0.01906674, Validation loss: 0.02018582, Gradient norm: 0.26380079
INFO:root:[   82] Training loss: 0.02072644, Validation loss: 0.02284452, Gradient norm: 0.50762613
INFO:root:[   83] Training loss: 0.01933939, Validation loss: 0.01912185, Gradient norm: 0.38509882
INFO:root:[   84] Training loss: 0.01874060, Validation loss: 0.01922584, Gradient norm: 0.34152157
INFO:root:[   85] Training loss: 0.01850912, Validation loss: 0.01893543, Gradient norm: 0.23548521
INFO:root:[   86] Training loss: 0.01909111, Validation loss: 0.01893075, Gradient norm: 0.40967268
INFO:root:[   87] Training loss: 0.01924109, Validation loss: 0.02247586, Gradient norm: 0.42442621
INFO:root:[   88] Training loss: 0.02067524, Validation loss: 0.01942663, Gradient norm: 0.61846735
INFO:root:[   89] Training loss: 0.01868608, Validation loss: 0.01887653, Gradient norm: 0.31929604
INFO:root:[   90] Training loss: 0.01998183, Validation loss: 0.01878892, Gradient norm: 0.39968009
INFO:root:[   91] Training loss: 0.01893418, Validation loss: 0.01885001, Gradient norm: 0.37127933
INFO:root:[   92] Training loss: 0.01855997, Validation loss: 0.01884094, Gradient norm: 0.33898484
INFO:root:[   93] Training loss: 0.01871885, Validation loss: 0.01871421, Gradient norm: 0.28002318
INFO:root:[   94] Training loss: 0.01896851, Validation loss: 0.01835522, Gradient norm: 0.34538145
INFO:root:[   95] Training loss: 0.01807106, Validation loss: 0.01828182, Gradient norm: 0.26746243
INFO:root:[   96] Training loss: 0.01829895, Validation loss: 0.01915832, Gradient norm: 0.25603218
INFO:root:[   97] Training loss: 0.01827489, Validation loss: 0.02040987, Gradient norm: 0.29350434
INFO:root:[   98] Training loss: 0.01953194, Validation loss: 0.01970765, Gradient norm: 0.45310366
INFO:root:[   99] Training loss: 0.01868014, Validation loss: 0.01796480, Gradient norm: 0.34434057
INFO:root:[  100] Training loss: 0.01830478, Validation loss: 0.01942094, Gradient norm: 0.29821498
INFO:root:[  101] Training loss: 0.01960621, Validation loss: 0.01907834, Gradient norm: 0.47574684
INFO:root:[  102] Training loss: 0.01842829, Validation loss: 0.01829122, Gradient norm: 0.30518371
INFO:root:[  103] Training loss: 0.01758025, Validation loss: 0.01804746, Gradient norm: 0.27357432
INFO:root:[  104] Training loss: 0.01751264, Validation loss: 0.01870712, Gradient norm: 0.27602154
INFO:root:[  105] Training loss: 0.01883089, Validation loss: 0.01797293, Gradient norm: 0.44870098
INFO:root:[  106] Training loss: 0.01792280, Validation loss: 0.01888819, Gradient norm: 0.24292874
INFO:root:[  107] Training loss: 0.01861039, Validation loss: 0.01813405, Gradient norm: 0.33543931
INFO:root:[  108] Training loss: 0.01846903, Validation loss: 0.01805632, Gradient norm: 0.40540949
INFO:root:EP 108: Early stopping
INFO:root:Training the model took 560.464s.
INFO:root:Emptying the cuda cache took 0.014s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.72243
INFO:root:EnergyScoreTrain: 0.55374
INFO:root:CoverageTrain: 0.98061
INFO:root:IntervalWidthTrain: 0.13654
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.75806
INFO:root:EnergyScoreValidation: 0.57813
INFO:root:CoverageValidation: 0.97835
INFO:root:IntervalWidthValidation: 0.13689
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.1764
INFO:root:EnergyScoreTest: 1.59758
INFO:root:CoverageTest: 0.757
INFO:root:IntervalWidthTest: 0.17179
INFO:root:###3 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08343213, Validation loss: 0.06225344, Gradient norm: 0.81954400
INFO:root:[    2] Training loss: 0.05474172, Validation loss: 0.05222950, Gradient norm: 0.32720549
INFO:root:[    3] Training loss: 0.04841268, Validation loss: 0.04789012, Gradient norm: 0.65993640
INFO:root:[    4] Training loss: 0.04362714, Validation loss: 0.04161233, Gradient norm: 0.43229757
INFO:root:[    5] Training loss: 0.04022400, Validation loss: 0.03933026, Gradient norm: 0.33506594
INFO:root:[    6] Training loss: 0.03933977, Validation loss: 0.03931375, Gradient norm: 0.46362343
INFO:root:[    7] Training loss: 0.03747468, Validation loss: 0.03476124, Gradient norm: 0.52466555
INFO:root:[    8] Training loss: 0.03609676, Validation loss: 0.03428197, Gradient norm: 0.51350699
INFO:root:[    9] Training loss: 0.03457962, Validation loss: 0.03404213, Gradient norm: 0.38484470
INFO:root:[   10] Training loss: 0.03377369, Validation loss: 0.03337199, Gradient norm: 0.39318127
INFO:root:[   11] Training loss: 0.03476607, Validation loss: 0.03253909, Gradient norm: 0.57420207
INFO:root:[   12] Training loss: 0.03384521, Validation loss: 0.03186222, Gradient norm: 0.57342391
INFO:root:[   13] Training loss: 0.03209934, Validation loss: 0.03211515, Gradient norm: 0.35791274
INFO:root:[   14] Training loss: 0.03181998, Validation loss: 0.03190851, Gradient norm: 0.28829938
INFO:root:[   15] Training loss: 0.03164891, Validation loss: 0.03104728, Gradient norm: 0.37214625
INFO:root:[   16] Training loss: 0.03166182, Validation loss: 0.03022896, Gradient norm: 0.37399954
INFO:root:[   17] Training loss: 0.03110679, Validation loss: 0.03038713, Gradient norm: 0.37970593
INFO:root:[   18] Training loss: 0.02980970, Validation loss: 0.03341619, Gradient norm: 0.31772737
INFO:root:[   19] Training loss: 0.03263349, Validation loss: 0.03231228, Gradient norm: 0.58766091
INFO:root:[   20] Training loss: 0.03041874, Validation loss: 0.02957395, Gradient norm: 0.45850291
INFO:root:[   21] Training loss: 0.03013208, Validation loss: 0.03234427, Gradient norm: 0.33513379
INFO:root:[   22] Training loss: 0.02993470, Validation loss: 0.02891106, Gradient norm: 0.33445161
INFO:root:[   23] Training loss: 0.02919169, Validation loss: 0.02875972, Gradient norm: 0.24666463
INFO:root:[   24] Training loss: 0.02860761, Validation loss: 0.02882416, Gradient norm: 0.32877706
INFO:root:[   25] Training loss: 0.02946533, Validation loss: 0.02846899, Gradient norm: 0.46445208
INFO:root:[   26] Training loss: 0.02848876, Validation loss: 0.02956816, Gradient norm: 0.26244168
INFO:root:[   27] Training loss: 0.02770730, Validation loss: 0.02715174, Gradient norm: 0.29831268
INFO:root:[   28] Training loss: 0.02758116, Validation loss: 0.02757674, Gradient norm: 0.35596711
INFO:root:[   29] Training loss: 0.02781126, Validation loss: 0.02775917, Gradient norm: 0.34265582
INFO:root:[   30] Training loss: 0.02803928, Validation loss: 0.02705197, Gradient norm: 0.36884641
INFO:root:[   31] Training loss: 0.02748179, Validation loss: 0.02650141, Gradient norm: 0.45025094
INFO:root:[   32] Training loss: 0.02837255, Validation loss: 0.02775782, Gradient norm: 0.47786907
INFO:root:[   33] Training loss: 0.02677634, Validation loss: 0.02603173, Gradient norm: 0.28458947
INFO:root:[   34] Training loss: 0.02553829, Validation loss: 0.02728760, Gradient norm: 0.27544533
INFO:root:[   35] Training loss: 0.02703645, Validation loss: 0.02623457, Gradient norm: 0.38991502
INFO:root:[   36] Training loss: 0.02707347, Validation loss: 0.02495806, Gradient norm: 0.54599344
INFO:root:[   37] Training loss: 0.02652755, Validation loss: 0.02560402, Gradient norm: 0.41505963
INFO:root:[   38] Training loss: 0.02587966, Validation loss: 0.02495455, Gradient norm: 0.34039223
INFO:root:[   39] Training loss: 0.02513232, Validation loss: 0.02664120, Gradient norm: 0.37933859
INFO:root:[   40] Training loss: 0.02562263, Validation loss: 0.02358401, Gradient norm: 0.33454288
INFO:root:[   41] Training loss: 0.02449196, Validation loss: 0.02609182, Gradient norm: 0.30749319
INFO:root:[   42] Training loss: 0.02498743, Validation loss: 0.02560832, Gradient norm: 0.36752175
INFO:root:[   43] Training loss: 0.02476504, Validation loss: 0.02415789, Gradient norm: 0.36747869
INFO:root:[   44] Training loss: 0.02445186, Validation loss: 0.02422187, Gradient norm: 0.34786433
INFO:root:[   45] Training loss: 0.02456750, Validation loss: 0.02331203, Gradient norm: 0.31899602
INFO:root:[   46] Training loss: 0.02329221, Validation loss: 0.02326569, Gradient norm: 0.22322247
INFO:root:[   47] Training loss: 0.02257041, Validation loss: 0.02321660, Gradient norm: 0.24789068
INFO:root:[   48] Training loss: 0.02326777, Validation loss: 0.02335777, Gradient norm: 0.26315656
INFO:root:[   49] Training loss: 0.02435634, Validation loss: 0.02334861, Gradient norm: 0.41697198
INFO:root:[   50] Training loss: 0.02356323, Validation loss: 0.02493405, Gradient norm: 0.34352494
INFO:root:[   51] Training loss: 0.02450734, Validation loss: 0.02263381, Gradient norm: 0.43074751
INFO:root:[   52] Training loss: 0.02256574, Validation loss: 0.02291959, Gradient norm: 0.28559236
INFO:root:[   53] Training loss: 0.02264261, Validation loss: 0.02230317, Gradient norm: 0.31186112
INFO:root:[   54] Training loss: 0.02208183, Validation loss: 0.02232922, Gradient norm: 0.27431147
INFO:root:[   55] Training loss: 0.02410285, Validation loss: 0.02220751, Gradient norm: 0.48710213
INFO:root:[   56] Training loss: 0.02194451, Validation loss: 0.02204018, Gradient norm: 0.32866490
INFO:root:[   57] Training loss: 0.02255568, Validation loss: 0.02275295, Gradient norm: 0.34383891
INFO:root:[   58] Training loss: 0.02234017, Validation loss: 0.02197939, Gradient norm: 0.35200418
INFO:root:[   59] Training loss: 0.02165575, Validation loss: 0.02178883, Gradient norm: 0.31397765
INFO:root:[   60] Training loss: 0.02179726, Validation loss: 0.02425988, Gradient norm: 0.27958035
INFO:root:[   61] Training loss: 0.02239828, Validation loss: 0.02166145, Gradient norm: 0.36663371
INFO:root:[   62] Training loss: 0.02108201, Validation loss: 0.02170059, Gradient norm: 0.22892999
INFO:root:[   63] Training loss: 0.02239643, Validation loss: 0.02297776, Gradient norm: 0.45036723
INFO:root:[   64] Training loss: 0.02256482, Validation loss: 0.02177504, Gradient norm: 0.42158296
INFO:root:[   65] Training loss: 0.02159340, Validation loss: 0.02154272, Gradient norm: 0.34034296
INFO:root:[   66] Training loss: 0.02213015, Validation loss: 0.02436622, Gradient norm: 0.43952797
INFO:root:[   67] Training loss: 0.02129902, Validation loss: 0.02084184, Gradient norm: 0.28562226
INFO:root:[   68] Training loss: 0.02050615, Validation loss: 0.02058350, Gradient norm: 0.21842824
INFO:root:[   69] Training loss: 0.02123867, Validation loss: 0.02126434, Gradient norm: 0.26292554
INFO:root:[   70] Training loss: 0.02131933, Validation loss: 0.02163929, Gradient norm: 0.35525878
INFO:root:[   71] Training loss: 0.02179422, Validation loss: 0.02108752, Gradient norm: 0.38785583
INFO:root:[   72] Training loss: 0.02078618, Validation loss: 0.02102651, Gradient norm: 0.26243732
INFO:root:[   73] Training loss: 0.02106388, Validation loss: 0.02052495, Gradient norm: 0.30399521
INFO:root:[   74] Training loss: 0.02023628, Validation loss: 0.02088746, Gradient norm: 0.26268318
INFO:root:[   75] Training loss: 0.02176560, Validation loss: 0.02041153, Gradient norm: 0.47821160
INFO:root:[   76] Training loss: 0.02085242, Validation loss: 0.02105734, Gradient norm: 0.33957062
INFO:root:[   77] Training loss: 0.02109092, Validation loss: 0.02024282, Gradient norm: 0.34762316
INFO:root:[   78] Training loss: 0.02076916, Validation loss: 0.02057463, Gradient norm: 0.32203031
INFO:root:[   79] Training loss: 0.02075301, Validation loss: 0.02069147, Gradient norm: 0.26718912
INFO:root:[   80] Training loss: 0.02114179, Validation loss: 0.02087304, Gradient norm: 0.31502967
INFO:root:[   81] Training loss: 0.02042184, Validation loss: 0.01985009, Gradient norm: 0.26868944
INFO:root:[   82] Training loss: 0.02003063, Validation loss: 0.02086231, Gradient norm: 0.31864107
INFO:root:[   83] Training loss: 0.02023734, Validation loss: 0.02006043, Gradient norm: 0.29243527
INFO:root:[   84] Training loss: 0.02006005, Validation loss: 0.02082225, Gradient norm: 0.25797230
INFO:root:[   85] Training loss: 0.02023359, Validation loss: 0.02028919, Gradient norm: 0.39806793
INFO:root:[   86] Training loss: 0.02067735, Validation loss: 0.02207811, Gradient norm: 0.34698848
INFO:root:[   87] Training loss: 0.02002205, Validation loss: 0.02203879, Gradient norm: 0.26699999
INFO:root:[   88] Training loss: 0.02153473, Validation loss: 0.01970677, Gradient norm: 0.37034328
INFO:root:[   89] Training loss: 0.01993435, Validation loss: 0.02026179, Gradient norm: 0.31896993
INFO:root:[   90] Training loss: 0.01959506, Validation loss: 0.02023296, Gradient norm: 0.28408272
INFO:root:[   91] Training loss: 0.02023338, Validation loss: 0.01982100, Gradient norm: 0.35327085
INFO:root:[   92] Training loss: 0.01986974, Validation loss: 0.01991935, Gradient norm: 0.26421330
INFO:root:[   93] Training loss: 0.02021427, Validation loss: 0.02397259, Gradient norm: 0.35425592
INFO:root:[   94] Training loss: 0.02085650, Validation loss: 0.02256334, Gradient norm: 0.47914635
INFO:root:[   95] Training loss: 0.02032088, Validation loss: 0.02048587, Gradient norm: 0.40512644
INFO:root:[   96] Training loss: 0.02011797, Validation loss: 0.01953073, Gradient norm: 0.36743056
INFO:root:[   97] Training loss: 0.02050736, Validation loss: 0.02060035, Gradient norm: 0.40930813
INFO:root:[   98] Training loss: 0.01962031, Validation loss: 0.01996789, Gradient norm: 0.34720620
INFO:root:[   99] Training loss: 0.01928398, Validation loss: 0.02032188, Gradient norm: 0.33495410
INFO:root:[  100] Training loss: 0.01956787, Validation loss: 0.01923439, Gradient norm: 0.31090455
INFO:root:[  101] Training loss: 0.01919263, Validation loss: 0.01910861, Gradient norm: 0.23861593
INFO:root:[  102] Training loss: 0.01910570, Validation loss: 0.01880583, Gradient norm: 0.29617165
INFO:root:[  103] Training loss: 0.01968644, Validation loss: 0.02030035, Gradient norm: 0.42966380
INFO:root:[  104] Training loss: 0.01947164, Validation loss: 0.02130190, Gradient norm: 0.32615020
INFO:root:[  105] Training loss: 0.01991967, Validation loss: 0.01913394, Gradient norm: 0.39850386
INFO:root:[  106] Training loss: 0.01945834, Validation loss: 0.01934630, Gradient norm: 0.31318852
INFO:root:[  107] Training loss: 0.01961759, Validation loss: 0.02186771, Gradient norm: 0.38142894
INFO:root:[  108] Training loss: 0.01919080, Validation loss: 0.01954284, Gradient norm: 0.37524708
INFO:root:[  109] Training loss: 0.01883306, Validation loss: 0.01912299, Gradient norm: 0.28337373
INFO:root:[  110] Training loss: 0.01906442, Validation loss: 0.01895018, Gradient norm: 0.31138865
INFO:root:[  111] Training loss: 0.01944709, Validation loss: 0.02112084, Gradient norm: 0.30004573
INFO:root:EP 111: Early stopping
INFO:root:Training the model took 576.104s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.73257
INFO:root:EnergyScoreTrain: 0.583
INFO:root:CoverageTrain: 0.98546
INFO:root:IntervalWidthTrain: 0.14945
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.76654
INFO:root:EnergyScoreValidation: 0.60252
INFO:root:CoverageValidation: 0.98269
INFO:root:IntervalWidthValidation: 0.14984
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.74016
INFO:root:EnergyScoreTest: 1.25237
INFO:root:CoverageTest: 0.86693
INFO:root:IntervalWidthTest: 0.16939
INFO:root:###4 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07363621, Validation loss: 0.06702582, Gradient norm: 0.36295804
INFO:root:[    2] Training loss: 0.05863837, Validation loss: 0.05312029, Gradient norm: 0.29025234
INFO:root:[    3] Training loss: 0.05059653, Validation loss: 0.04654601, Gradient norm: 0.44519164
INFO:root:[    4] Training loss: 0.04340031, Validation loss: 0.04077027, Gradient norm: 0.48006504
INFO:root:[    5] Training loss: 0.03812844, Validation loss: 0.03655899, Gradient norm: 0.34047764
INFO:root:[    6] Training loss: 0.03728881, Validation loss: 0.03510119, Gradient norm: 0.44457342
INFO:root:[    7] Training loss: 0.03641172, Validation loss: 0.03553998, Gradient norm: 0.39077816
INFO:root:[    8] Training loss: 0.03463087, Validation loss: 0.03312104, Gradient norm: 0.41675905
INFO:root:[    9] Training loss: 0.03424558, Validation loss: 0.03451780, Gradient norm: 0.28235648
INFO:root:[   10] Training loss: 0.03304972, Validation loss: 0.03277528, Gradient norm: 0.45067170
INFO:root:[   11] Training loss: 0.03252334, Validation loss: 0.03149813, Gradient norm: 0.42993980
INFO:root:[   12] Training loss: 0.03194034, Validation loss: 0.03050392, Gradient norm: 0.36237720
INFO:root:[   13] Training loss: 0.03146553, Validation loss: 0.02971057, Gradient norm: 0.44106530
INFO:root:[   14] Training loss: 0.03046523, Validation loss: 0.02994379, Gradient norm: 0.32446730
INFO:root:[   15] Training loss: 0.03062903, Validation loss: 0.02964428, Gradient norm: 0.41046809
INFO:root:[   16] Training loss: 0.02980916, Validation loss: 0.03174052, Gradient norm: 0.47761631
INFO:root:[   17] Training loss: 0.03006667, Validation loss: 0.02981473, Gradient norm: 0.40592971
INFO:root:[   18] Training loss: 0.02969797, Validation loss: 0.03160875, Gradient norm: 0.45504374
INFO:root:[   19] Training loss: 0.02968948, Validation loss: 0.02888821, Gradient norm: 0.48816034
INFO:root:[   20] Training loss: 0.02806729, Validation loss: 0.03075360, Gradient norm: 0.29382475
INFO:root:[   21] Training loss: 0.02908615, Validation loss: 0.02707633, Gradient norm: 0.48085295
INFO:root:[   22] Training loss: 0.02742734, Validation loss: 0.02753640, Gradient norm: 0.28849316
INFO:root:[   23] Training loss: 0.02732579, Validation loss: 0.02731875, Gradient norm: 0.27173242
INFO:root:[   24] Training loss: 0.02761373, Validation loss: 0.02807204, Gradient norm: 0.33101978
INFO:root:[   25] Training loss: 0.02781120, Validation loss: 0.02750290, Gradient norm: 0.32571270
INFO:root:[   26] Training loss: 0.02636598, Validation loss: 0.02680197, Gradient norm: 0.30804987
INFO:root:[   27] Training loss: 0.02621986, Validation loss: 0.02667107, Gradient norm: 0.33793427
INFO:root:[   28] Training loss: 0.02799885, Validation loss: 0.02688158, Gradient norm: 0.53266496
INFO:root:[   29] Training loss: 0.02607194, Validation loss: 0.02565623, Gradient norm: 0.27370441
INFO:root:[   30] Training loss: 0.02614998, Validation loss: 0.02617248, Gradient norm: 0.30205040
INFO:root:[   31] Training loss: 0.02613183, Validation loss: 0.02664815, Gradient norm: 0.34819958
INFO:root:[   32] Training loss: 0.02766962, Validation loss: 0.02554768, Gradient norm: 0.49111964
INFO:root:[   33] Training loss: 0.02595017, Validation loss: 0.02654466, Gradient norm: 0.26606488
INFO:root:[   34] Training loss: 0.02659041, Validation loss: 0.02629315, Gradient norm: 0.39337415
INFO:root:[   35] Training loss: 0.02566887, Validation loss: 0.02641738, Gradient norm: 0.31464115
INFO:root:[   36] Training loss: 0.02560527, Validation loss: 0.02541486, Gradient norm: 0.29761885
INFO:root:[   37] Training loss: 0.02583382, Validation loss: 0.02476702, Gradient norm: 0.32164724
INFO:root:[   38] Training loss: 0.02517970, Validation loss: 0.02511297, Gradient norm: 0.21892083
INFO:root:[   39] Training loss: 0.02523844, Validation loss: 0.02508150, Gradient norm: 0.34027203
INFO:root:[   40] Training loss: 0.02466553, Validation loss: 0.02470414, Gradient norm: 0.28560373
INFO:root:[   41] Training loss: 0.02495360, Validation loss: 0.02465531, Gradient norm: 0.30686183
INFO:root:[   42] Training loss: 0.02486371, Validation loss: 0.02401733, Gradient norm: 0.29092948
INFO:root:[   43] Training loss: 0.02449817, Validation loss: 0.02385561, Gradient norm: 0.33755367
INFO:root:[   44] Training loss: 0.02489931, Validation loss: 0.02542500, Gradient norm: 0.32799013
INFO:root:[   45] Training loss: 0.02495484, Validation loss: 0.02387608, Gradient norm: 0.48814958
INFO:root:[   46] Training loss: 0.02464610, Validation loss: 0.02404176, Gradient norm: 0.35237183
INFO:root:[   47] Training loss: 0.02484328, Validation loss: 0.02421081, Gradient norm: 0.33386186
INFO:root:[   48] Training loss: 0.02393996, Validation loss: 0.02353975, Gradient norm: 0.29014591
INFO:root:[   49] Training loss: 0.02382761, Validation loss: 0.02452300, Gradient norm: 0.37965532
INFO:root:[   50] Training loss: 0.02349755, Validation loss: 0.02411170, Gradient norm: 0.32556813
INFO:root:[   51] Training loss: 0.02333235, Validation loss: 0.02553584, Gradient norm: 0.26376044
INFO:root:[   52] Training loss: 0.02374540, Validation loss: 0.02581677, Gradient norm: 0.36115044
INFO:root:[   53] Training loss: 0.02427064, Validation loss: 0.02259574, Gradient norm: 0.36214682
INFO:root:[   54] Training loss: 0.02410624, Validation loss: 0.02404499, Gradient norm: 0.39602923
INFO:root:[   55] Training loss: 0.02345624, Validation loss: 0.02372001, Gradient norm: 0.27211845
INFO:root:[   56] Training loss: 0.02303307, Validation loss: 0.02250355, Gradient norm: 0.26636146
INFO:root:[   57] Training loss: 0.02298966, Validation loss: 0.02368310, Gradient norm: 0.34999371
INFO:root:[   58] Training loss: 0.02277892, Validation loss: 0.02248711, Gradient norm: 0.29130016
INFO:root:[   59] Training loss: 0.02376547, Validation loss: 0.02270470, Gradient norm: 0.31080377
INFO:root:[   60] Training loss: 0.02329615, Validation loss: 0.02242576, Gradient norm: 0.36224133
INFO:root:[   61] Training loss: 0.02241606, Validation loss: 0.02198490, Gradient norm: 0.33304296
INFO:root:[   62] Training loss: 0.02249895, Validation loss: 0.02225902, Gradient norm: 0.28591399
INFO:root:[   63] Training loss: 0.02258548, Validation loss: 0.02224967, Gradient norm: 0.25116612
INFO:root:[   64] Training loss: 0.02322243, Validation loss: 0.02260848, Gradient norm: 0.37388427
INFO:root:[   65] Training loss: 0.02247438, Validation loss: 0.02294241, Gradient norm: 0.26628279
INFO:root:[   66] Training loss: 0.02304774, Validation loss: 0.02332142, Gradient norm: 0.47854627
INFO:root:[   67] Training loss: 0.02240040, Validation loss: 0.02204212, Gradient norm: 0.31200204
INFO:root:[   68] Training loss: 0.02187145, Validation loss: 0.02171955, Gradient norm: 0.26030535
INFO:root:[   69] Training loss: 0.02196701, Validation loss: 0.02211452, Gradient norm: 0.30376879
INFO:root:[   70] Training loss: 0.02187921, Validation loss: 0.02212827, Gradient norm: 0.26797678
INFO:root:[   71] Training loss: 0.02182134, Validation loss: 0.02184665, Gradient norm: 0.30000635
INFO:root:[   72] Training loss: 0.02172515, Validation loss: 0.02170217, Gradient norm: 0.30072726
INFO:root:[   73] Training loss: 0.02179712, Validation loss: 0.02238190, Gradient norm: 0.34551176
INFO:root:[   74] Training loss: 0.02136386, Validation loss: 0.02172126, Gradient norm: 0.26516311
INFO:root:[   75] Training loss: 0.02095052, Validation loss: 0.02147457, Gradient norm: 0.28847566
INFO:root:[   76] Training loss: 0.02116947, Validation loss: 0.02134027, Gradient norm: 0.28527645
INFO:root:[   77] Training loss: 0.02146502, Validation loss: 0.02257834, Gradient norm: 0.33641692
INFO:root:[   78] Training loss: 0.02100439, Validation loss: 0.02151509, Gradient norm: 0.34060866
INFO:root:[   79] Training loss: 0.02211774, Validation loss: 0.02103656, Gradient norm: 0.30939267
INFO:root:[   80] Training loss: 0.02156937, Validation loss: 0.02239915, Gradient norm: 0.32970129
INFO:root:[   81] Training loss: 0.02244500, Validation loss: 0.02068399, Gradient norm: 0.48255523
INFO:root:[   82] Training loss: 0.02133376, Validation loss: 0.02115893, Gradient norm: 0.26287491
INFO:root:[   83] Training loss: 0.02165863, Validation loss: 0.02069646, Gradient norm: 0.25476998
INFO:root:[   84] Training loss: 0.02076854, Validation loss: 0.02086517, Gradient norm: 0.31621395
INFO:root:[   85] Training loss: 0.02141650, Validation loss: 0.02054034, Gradient norm: 0.33967232
INFO:root:[   86] Training loss: 0.02119533, Validation loss: 0.02073826, Gradient norm: 0.34463240
INFO:root:[   87] Training loss: 0.02144560, Validation loss: 0.02271527, Gradient norm: 0.33454856
INFO:root:[   88] Training loss: 0.02112051, Validation loss: 0.02048834, Gradient norm: 0.23845406
INFO:root:[   89] Training loss: 0.02079883, Validation loss: 0.02028555, Gradient norm: 0.29997877
INFO:root:[   90] Training loss: 0.02119081, Validation loss: 0.02055047, Gradient norm: 0.31571684
INFO:root:[   91] Training loss: 0.02102405, Validation loss: 0.02070532, Gradient norm: 0.35656340
INFO:root:[   92] Training loss: 0.02072459, Validation loss: 0.02045923, Gradient norm: 0.27269421
INFO:root:[   93] Training loss: 0.02102190, Validation loss: 0.02085565, Gradient norm: 0.42312830
INFO:root:[   94] Training loss: 0.02047220, Validation loss: 0.02044187, Gradient norm: 0.28916772
INFO:root:[   95] Training loss: 0.02095279, Validation loss: 0.02089313, Gradient norm: 0.33676345
INFO:root:[   96] Training loss: 0.01981129, Validation loss: 0.01988622, Gradient norm: 0.26424642
INFO:root:[   97] Training loss: 0.01999325, Validation loss: 0.02050158, Gradient norm: 0.25897569
INFO:root:[   98] Training loss: 0.02037874, Validation loss: 0.02011221, Gradient norm: 0.35737625
INFO:root:[   99] Training loss: 0.02005410, Validation loss: 0.02157045, Gradient norm: 0.25403714
INFO:root:[  100] Training loss: 0.02093304, Validation loss: 0.02007144, Gradient norm: 0.39315723
INFO:root:[  101] Training loss: 0.02019218, Validation loss: 0.02060117, Gradient norm: 0.35569930
INFO:root:[  102] Training loss: 0.02002532, Validation loss: 0.01992884, Gradient norm: 0.32484844
INFO:root:[  103] Training loss: 0.01975682, Validation loss: 0.02020804, Gradient norm: 0.27724770
INFO:root:[  104] Training loss: 0.01965100, Validation loss: 0.02012421, Gradient norm: 0.24558787
INFO:root:[  105] Training loss: 0.02032697, Validation loss: 0.02079050, Gradient norm: 0.39019019
INFO:root:EP 105: Early stopping
INFO:root:Training the model took 545.692s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.78266
INFO:root:EnergyScoreTrain: 0.61857
INFO:root:CoverageTrain: 0.98107
INFO:root:IntervalWidthTrain: 0.15687
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.80951
INFO:root:EnergyScoreValidation: 0.63537
INFO:root:CoverageValidation: 0.97926
INFO:root:IntervalWidthValidation: 0.15747
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.52467
INFO:root:EnergyScoreTest: 1.07332
INFO:root:CoverageTest: 0.93818
INFO:root:IntervalWidthTest: 0.18222
INFO:root:###5 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07495720, Validation loss: 0.06675984, Gradient norm: 0.29714771
INFO:root:[    2] Training loss: 0.06269983, Validation loss: 0.06267382, Gradient norm: 0.23438539
INFO:root:[    3] Training loss: 0.05652572, Validation loss: 0.05509241, Gradient norm: 0.24367259
INFO:root:[    4] Training loss: 0.05047634, Validation loss: 0.04863746, Gradient norm: 0.20491783
INFO:root:[    5] Training loss: 0.04809854, Validation loss: 0.04792911, Gradient norm: 0.43422573
INFO:root:[    6] Training loss: 0.04257493, Validation loss: 0.03922845, Gradient norm: 0.35951107
INFO:root:[    7] Training loss: 0.03738005, Validation loss: 0.03570697, Gradient norm: 0.24481613
INFO:root:[    8] Training loss: 0.03702802, Validation loss: 0.03645444, Gradient norm: 0.40416271
INFO:root:[    9] Training loss: 0.03603219, Validation loss: 0.03320476, Gradient norm: 0.39573876
INFO:root:[   10] Training loss: 0.03464509, Validation loss: 0.03738897, Gradient norm: 0.38736933
INFO:root:[   11] Training loss: 0.03411000, Validation loss: 0.03247680, Gradient norm: 0.36255973
INFO:root:[   12] Training loss: 0.03274710, Validation loss: 0.03082353, Gradient norm: 0.37993928
INFO:root:[   13] Training loss: 0.03196825, Validation loss: 0.03383826, Gradient norm: 0.34408402
INFO:root:[   14] Training loss: 0.03323009, Validation loss: 0.03145897, Gradient norm: 0.45689264
INFO:root:[   15] Training loss: 0.03074810, Validation loss: 0.03040490, Gradient norm: 0.26177181
INFO:root:[   16] Training loss: 0.02997577, Validation loss: 0.02957237, Gradient norm: 0.26991159
INFO:root:[   17] Training loss: 0.03001636, Validation loss: 0.02931948, Gradient norm: 0.21431422
INFO:root:[   18] Training loss: 0.02909437, Validation loss: 0.02878520, Gradient norm: 0.32253886
INFO:root:[   19] Training loss: 0.02906316, Validation loss: 0.02879314, Gradient norm: 0.30840800
INFO:root:[   20] Training loss: 0.02817906, Validation loss: 0.02836801, Gradient norm: 0.28971274
INFO:root:[   21] Training loss: 0.02856656, Validation loss: 0.02859741, Gradient norm: 0.28597210
INFO:root:[   22] Training loss: 0.02897269, Validation loss: 0.02998925, Gradient norm: 0.37979611
INFO:root:[   23] Training loss: 0.02841436, Validation loss: 0.02884867, Gradient norm: 0.38779239
INFO:root:[   24] Training loss: 0.02789932, Validation loss: 0.02873251, Gradient norm: 0.25422618
INFO:root:[   25] Training loss: 0.02823222, Validation loss: 0.02881265, Gradient norm: 0.36342947
INFO:root:[   26] Training loss: 0.02894470, Validation loss: 0.02900462, Gradient norm: 0.46784415
INFO:root:[   27] Training loss: 0.02761064, Validation loss: 0.02727072, Gradient norm: 0.32077403
INFO:root:[   28] Training loss: 0.02769512, Validation loss: 0.02659025, Gradient norm: 0.28623150
INFO:root:[   29] Training loss: 0.02650628, Validation loss: 0.02626411, Gradient norm: 0.26605257
INFO:root:[   30] Training loss: 0.02671764, Validation loss: 0.02595112, Gradient norm: 0.37113387
INFO:root:[   31] Training loss: 0.02630079, Validation loss: 0.02592155, Gradient norm: 0.32245824
INFO:root:[   32] Training loss: 0.02563604, Validation loss: 0.02569232, Gradient norm: 0.23718136
INFO:root:[   33] Training loss: 0.02650801, Validation loss: 0.02682894, Gradient norm: 0.29503707
INFO:root:[   34] Training loss: 0.02533191, Validation loss: 0.02579951, Gradient norm: 0.29490442
INFO:root:[   35] Training loss: 0.02514481, Validation loss: 0.02457253, Gradient norm: 0.27754273
INFO:root:[   36] Training loss: 0.02535375, Validation loss: 0.02516510, Gradient norm: 0.24823241
INFO:root:[   37] Training loss: 0.02536236, Validation loss: 0.02575425, Gradient norm: 0.28998633
INFO:root:[   38] Training loss: 0.02515084, Validation loss: 0.02501132, Gradient norm: 0.32537058
INFO:root:[   39] Training loss: 0.02508802, Validation loss: 0.02530235, Gradient norm: 0.27047807
INFO:root:[   40] Training loss: 0.02469464, Validation loss: 0.02559545, Gradient norm: 0.28005944
INFO:root:[   41] Training loss: 0.02504619, Validation loss: 0.02486283, Gradient norm: 0.31884733
INFO:root:[   42] Training loss: 0.02497915, Validation loss: 0.02541661, Gradient norm: 0.36984947
INFO:root:[   43] Training loss: 0.02541864, Validation loss: 0.02378725, Gradient norm: 0.27185935
INFO:root:[   44] Training loss: 0.02489732, Validation loss: 0.02460311, Gradient norm: 0.31856801
INFO:root:[   45] Training loss: 0.02498084, Validation loss: 0.02586012, Gradient norm: 0.31465153
INFO:root:[   46] Training loss: 0.02434871, Validation loss: 0.02425661, Gradient norm: 0.28935939
INFO:root:[   47] Training loss: 0.02411471, Validation loss: 0.02397261, Gradient norm: 0.33881251
INFO:root:[   48] Training loss: 0.02396614, Validation loss: 0.02391430, Gradient norm: 0.28366524
INFO:root:[   49] Training loss: 0.02395969, Validation loss: 0.02535975, Gradient norm: 0.32430511
INFO:root:[   50] Training loss: 0.02381711, Validation loss: 0.02347195, Gradient norm: 0.28387523
INFO:root:[   51] Training loss: 0.02418459, Validation loss: 0.02412008, Gradient norm: 0.33992399
INFO:root:[   52] Training loss: 0.02413387, Validation loss: 0.02383264, Gradient norm: 0.33820928
INFO:root:[   53] Training loss: 0.02392388, Validation loss: 0.02337484, Gradient norm: 0.25918837
INFO:root:[   54] Training loss: 0.02311122, Validation loss: 0.02496261, Gradient norm: 0.23860716
INFO:root:[   55] Training loss: 0.02358313, Validation loss: 0.02421400, Gradient norm: 0.27585089
INFO:root:[   56] Training loss: 0.02355529, Validation loss: 0.02345629, Gradient norm: 0.35923127
INFO:root:[   57] Training loss: 0.02293055, Validation loss: 0.02308345, Gradient norm: 0.26893603
INFO:root:[   58] Training loss: 0.02311265, Validation loss: 0.02369090, Gradient norm: 0.21603704
INFO:root:[   59] Training loss: 0.02328411, Validation loss: 0.02281402, Gradient norm: 0.32042134
INFO:root:[   60] Training loss: 0.02290089, Validation loss: 0.02299174, Gradient norm: 0.32195746
INFO:root:[   61] Training loss: 0.02307368, Validation loss: 0.02264384, Gradient norm: 0.28295932
INFO:root:[   62] Training loss: 0.02262197, Validation loss: 0.02249912, Gradient norm: 0.27238550
INFO:root:[   63] Training loss: 0.02254243, Validation loss: 0.02263378, Gradient norm: 0.31030382
INFO:root:[   64] Training loss: 0.02335900, Validation loss: 0.02261989, Gradient norm: 0.25424700
INFO:root:[   65] Training loss: 0.02199650, Validation loss: 0.02194698, Gradient norm: 0.25032126
INFO:root:[   66] Training loss: 0.02307973, Validation loss: 0.02227026, Gradient norm: 0.28209053
INFO:root:[   67] Training loss: 0.02303963, Validation loss: 0.02257596, Gradient norm: 0.32959233
INFO:root:[   68] Training loss: 0.02205008, Validation loss: 0.02256199, Gradient norm: 0.28322441
INFO:root:[   69] Training loss: 0.02198917, Validation loss: 0.02234519, Gradient norm: 0.24337276
INFO:root:[   70] Training loss: 0.02212123, Validation loss: 0.02197860, Gradient norm: 0.25221056
INFO:root:[   71] Training loss: 0.02240094, Validation loss: 0.02217377, Gradient norm: 0.21932853
INFO:root:[   72] Training loss: 0.02226794, Validation loss: 0.02460948, Gradient norm: 0.29214105
INFO:root:[   73] Training loss: 0.02208234, Validation loss: 0.02348374, Gradient norm: 0.32557905
INFO:root:[   74] Training loss: 0.02267345, Validation loss: 0.02171709, Gradient norm: 0.28759961
INFO:root:[   75] Training loss: 0.02127960, Validation loss: 0.02181441, Gradient norm: 0.28196059
INFO:root:[   76] Training loss: 0.02208463, Validation loss: 0.02184522, Gradient norm: 0.25852004
INFO:root:[   77] Training loss: 0.02238112, Validation loss: 0.02207617, Gradient norm: 0.26619407
INFO:root:[   78] Training loss: 0.02163938, Validation loss: 0.02225497, Gradient norm: 0.32244585
INFO:root:[   79] Training loss: 0.02225690, Validation loss: 0.02240414, Gradient norm: 0.29743983
INFO:root:[   80] Training loss: 0.02198473, Validation loss: 0.02160341, Gradient norm: 0.29097954
INFO:root:[   81] Training loss: 0.02151765, Validation loss: 0.02147653, Gradient norm: 0.32615874
INFO:root:[   82] Training loss: 0.02118143, Validation loss: 0.02149238, Gradient norm: 0.22926683
INFO:root:[   83] Training loss: 0.02176010, Validation loss: 0.02219156, Gradient norm: 0.32492044
INFO:root:[   84] Training loss: 0.02185857, Validation loss: 0.02120994, Gradient norm: 0.31309495
INFO:root:[   85] Training loss: 0.02172295, Validation loss: 0.02144980, Gradient norm: 0.20171041
INFO:root:[   86] Training loss: 0.02083928, Validation loss: 0.02131244, Gradient norm: 0.25691796
INFO:root:[   87] Training loss: 0.02103490, Validation loss: 0.02147385, Gradient norm: 0.24213775
INFO:root:[   88] Training loss: 0.02110295, Validation loss: 0.02228004, Gradient norm: 0.29854593
INFO:root:[   89] Training loss: 0.02171017, Validation loss: 0.02124818, Gradient norm: 0.25744276
INFO:root:[   90] Training loss: 0.02107894, Validation loss: 0.02151379, Gradient norm: 0.30969348
INFO:root:[   91] Training loss: 0.02128272, Validation loss: 0.02097107, Gradient norm: 0.25052988
INFO:root:[   92] Training loss: 0.02101983, Validation loss: 0.02082327, Gradient norm: 0.20100158
INFO:root:[   93] Training loss: 0.02102467, Validation loss: 0.02185617, Gradient norm: 0.22004169
INFO:root:[   94] Training loss: 0.02165749, Validation loss: 0.02266130, Gradient norm: 0.41827389
INFO:root:[   95] Training loss: 0.02131297, Validation loss: 0.02084270, Gradient norm: 0.32146328
INFO:root:[   96] Training loss: 0.02041563, Validation loss: 0.02116682, Gradient norm: 0.27840091
INFO:root:[   97] Training loss: 0.02064173, Validation loss: 0.02272548, Gradient norm: 0.29065135
INFO:root:[   98] Training loss: 0.02137188, Validation loss: 0.02034917, Gradient norm: 0.36691750
INFO:root:[   99] Training loss: 0.02043972, Validation loss: 0.02108484, Gradient norm: 0.22414440
INFO:root:[  100] Training loss: 0.02043302, Validation loss: 0.02098654, Gradient norm: 0.26038037
INFO:root:[  101] Training loss: 0.02020916, Validation loss: 0.02039479, Gradient norm: 0.26105674
INFO:root:[  102] Training loss: 0.02012640, Validation loss: 0.02044740, Gradient norm: 0.22144585
INFO:root:[  103] Training loss: 0.02009016, Validation loss: 0.02046147, Gradient norm: 0.19344687
INFO:root:[  104] Training loss: 0.02046129, Validation loss: 0.02024664, Gradient norm: 0.33964648
INFO:root:[  105] Training loss: 0.02034765, Validation loss: 0.02126821, Gradient norm: 0.24533110
INFO:root:[  106] Training loss: 0.02024191, Validation loss: 0.02060424, Gradient norm: 0.23865052
INFO:root:[  107] Training loss: 0.01954651, Validation loss: 0.02037700, Gradient norm: 0.20072353
INFO:root:[  108] Training loss: 0.02058641, Validation loss: 0.02039215, Gradient norm: 0.31904113
INFO:root:[  109] Training loss: 0.02048273, Validation loss: 0.02060505, Gradient norm: 0.24719542
INFO:root:[  110] Training loss: 0.02010138, Validation loss: 0.02025046, Gradient norm: 0.19876066
INFO:root:[  111] Training loss: 0.02032068, Validation loss: 0.02007468, Gradient norm: 0.25555582
INFO:root:[  112] Training loss: 0.01980875, Validation loss: 0.02073980, Gradient norm: 0.23148968
INFO:root:[  113] Training loss: 0.02019042, Validation loss: 0.01988452, Gradient norm: 0.32343517
INFO:root:[  114] Training loss: 0.02048202, Validation loss: 0.02089383, Gradient norm: 0.29223164
INFO:root:[  115] Training loss: 0.01982861, Validation loss: 0.02207912, Gradient norm: 0.25319703
INFO:root:[  116] Training loss: 0.01942288, Validation loss: 0.02078698, Gradient norm: 0.31573828
INFO:root:[  117] Training loss: 0.02056120, Validation loss: 0.01966733, Gradient norm: 0.39276989
INFO:root:[  118] Training loss: 0.01950862, Validation loss: 0.01996575, Gradient norm: 0.20078991
INFO:root:[  119] Training loss: 0.01927474, Validation loss: 0.01973107, Gradient norm: 0.19417893
INFO:root:[  120] Training loss: 0.01965060, Validation loss: 0.02028915, Gradient norm: 0.26531602
INFO:root:[  121] Training loss: 0.01996745, Validation loss: 0.01938587, Gradient norm: 0.27916424
INFO:root:[  122] Training loss: 0.01937943, Validation loss: 0.01981580, Gradient norm: 0.26324859
INFO:root:[  123] Training loss: 0.01938805, Validation loss: 0.01968820, Gradient norm: 0.18359922
INFO:root:[  124] Training loss: 0.01910937, Validation loss: 0.02014520, Gradient norm: 0.18740239
INFO:root:[  125] Training loss: 0.01955589, Validation loss: 0.02016718, Gradient norm: 0.24842801
INFO:root:[  126] Training loss: 0.01912117, Validation loss: 0.01985113, Gradient norm: 0.21417024
INFO:root:[  127] Training loss: 0.01972329, Validation loss: 0.01952685, Gradient norm: 0.37922803
INFO:root:[  128] Training loss: 0.01949327, Validation loss: 0.01953466, Gradient norm: 0.31244168
INFO:root:[  129] Training loss: 0.01935856, Validation loss: 0.01919633, Gradient norm: 0.21022015
INFO:root:[  130] Training loss: 0.02001741, Validation loss: 0.02037411, Gradient norm: 0.33812067
INFO:root:[  131] Training loss: 0.01993975, Validation loss: 0.01936928, Gradient norm: 0.32261995
INFO:root:[  132] Training loss: 0.02011908, Validation loss: 0.01961588, Gradient norm: 0.42355511
INFO:root:[  133] Training loss: 0.01908803, Validation loss: 0.01925125, Gradient norm: 0.22939396
INFO:root:[  134] Training loss: 0.01867980, Validation loss: 0.01951537, Gradient norm: 0.21725187
INFO:root:[  135] Training loss: 0.01874628, Validation loss: 0.01966484, Gradient norm: 0.22250111
INFO:root:[  136] Training loss: 0.01939620, Validation loss: 0.02124617, Gradient norm: 0.23755617
INFO:root:[  137] Training loss: 0.01946535, Validation loss: 0.02012967, Gradient norm: 0.36036908
INFO:root:[  138] Training loss: 0.01915453, Validation loss: 0.01961971, Gradient norm: 0.30169280
INFO:root:[  139] Training loss: 0.02003856, Validation loss: 0.01903789, Gradient norm: 0.28779875
INFO:root:[  140] Training loss: 0.01927495, Validation loss: 0.01905959, Gradient norm: 0.26339102
INFO:root:[  141] Training loss: 0.01886942, Validation loss: 0.01935598, Gradient norm: 0.19677850
INFO:root:[  142] Training loss: 0.01964190, Validation loss: 0.01927640, Gradient norm: 0.32222081
INFO:root:[  143] Training loss: 0.01902748, Validation loss: 0.01911106, Gradient norm: 0.23295570
INFO:root:[  144] Training loss: 0.01865255, Validation loss: 0.01927725, Gradient norm: 0.21057815
INFO:root:[  145] Training loss: 0.01866671, Validation loss: 0.01965507, Gradient norm: 0.21776478
INFO:root:[  146] Training loss: 0.01864772, Validation loss: 0.01880135, Gradient norm: 0.21814394
INFO:root:[  147] Training loss: 0.01916183, Validation loss: 0.01895816, Gradient norm: 0.39003465
INFO:root:[  148] Training loss: 0.01908240, Validation loss: 0.01925437, Gradient norm: 0.28426641
INFO:root:[  149] Training loss: 0.01847775, Validation loss: 0.01890582, Gradient norm: 0.24257212
INFO:root:[  150] Training loss: 0.01914897, Validation loss: 0.01912948, Gradient norm: 0.32998549
INFO:root:[  151] Training loss: 0.01904688, Validation loss: 0.02004046, Gradient norm: 0.25035585
INFO:root:[  152] Training loss: 0.01905452, Validation loss: 0.01938367, Gradient norm: 0.26945101
INFO:root:[  153] Training loss: 0.01852509, Validation loss: 0.01895885, Gradient norm: 0.22721484
INFO:root:[  154] Training loss: 0.01879503, Validation loss: 0.01923588, Gradient norm: 0.25531532
INFO:root:[  155] Training loss: 0.01941550, Validation loss: 0.01884312, Gradient norm: 0.29813636
INFO:root:EP 155: Early stopping
INFO:root:Training the model took 802.367s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.71606
INFO:root:EnergyScoreTrain: 0.57475
INFO:root:CoverageTrain: 0.98141
INFO:root:IntervalWidthTrain: 0.14476
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.74893
INFO:root:EnergyScoreValidation: 0.59556
INFO:root:CoverageValidation: 0.97854
INFO:root:IntervalWidthValidation: 0.14544
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.23273
INFO:root:EnergyScoreTest: 0.88702
INFO:root:CoverageTest: 0.93453
INFO:root:IntervalWidthTest: 0.15722
INFO:root:###6 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07947642, Validation loss: 0.06862232, Gradient norm: 0.27278400
INFO:root:[    2] Training loss: 0.06246046, Validation loss: 0.05902416, Gradient norm: 0.21063901
INFO:root:[    3] Training loss: 0.05471768, Validation loss: 0.05113734, Gradient norm: 0.24433579
INFO:root:[    4] Training loss: 0.04840672, Validation loss: 0.04500768, Gradient norm: 0.29951459
INFO:root:[    5] Training loss: 0.04279656, Validation loss: 0.04252431, Gradient norm: 0.26097926
INFO:root:[    6] Training loss: 0.04143319, Validation loss: 0.04042370, Gradient norm: 0.19840207
INFO:root:[    7] Training loss: 0.04021604, Validation loss: 0.03954113, Gradient norm: 0.36990860
INFO:root:[    8] Training loss: 0.03882973, Validation loss: 0.03778642, Gradient norm: 0.27830645
INFO:root:[    9] Training loss: 0.03822432, Validation loss: 0.03760247, Gradient norm: 0.23763242
INFO:root:[   10] Training loss: 0.03626737, Validation loss: 0.03680074, Gradient norm: 0.30000195
INFO:root:[   11] Training loss: 0.03627147, Validation loss: 0.03618225, Gradient norm: 0.29316356
INFO:root:[   12] Training loss: 0.03519061, Validation loss: 0.03656186, Gradient norm: 0.27697767
INFO:root:[   13] Training loss: 0.03459360, Validation loss: 0.03420227, Gradient norm: 0.27512079
INFO:root:[   14] Training loss: 0.03369474, Validation loss: 0.03390587, Gradient norm: 0.30243187
INFO:root:[   15] Training loss: 0.03469007, Validation loss: 0.03384237, Gradient norm: 0.27732572
INFO:root:[   16] Training loss: 0.03363954, Validation loss: 0.03273527, Gradient norm: 0.21276700
INFO:root:[   17] Training loss: 0.03261682, Validation loss: 0.03275964, Gradient norm: 0.22486001
INFO:root:[   18] Training loss: 0.03230042, Validation loss: 0.03264792, Gradient norm: 0.21385656
INFO:root:[   19] Training loss: 0.03266043, Validation loss: 0.03364040, Gradient norm: 0.33878190
INFO:root:[   20] Training loss: 0.03216266, Validation loss: 0.03200428, Gradient norm: 0.24732971
INFO:root:[   21] Training loss: 0.03213644, Validation loss: 0.03116539, Gradient norm: 0.29385490
INFO:root:[   22] Training loss: 0.03179972, Validation loss: 0.03179980, Gradient norm: 0.28759413
INFO:root:[   23] Training loss: 0.03195493, Validation loss: 0.03284669, Gradient norm: 0.38253791
INFO:root:[   24] Training loss: 0.03110763, Validation loss: 0.02993242, Gradient norm: 0.24575312
INFO:root:[   25] Training loss: 0.03062001, Validation loss: 0.02979788, Gradient norm: 0.26216308
INFO:root:[   26] Training loss: 0.02939236, Validation loss: 0.02960446, Gradient norm: 0.26973137
INFO:root:[   27] Training loss: 0.03022385, Validation loss: 0.02996353, Gradient norm: 0.23412550
INFO:root:[   28] Training loss: 0.02945001, Validation loss: 0.02945359, Gradient norm: 0.35895808
INFO:root:[   29] Training loss: 0.02934329, Validation loss: 0.02907370, Gradient norm: 0.27573119
INFO:root:[   30] Training loss: 0.02901338, Validation loss: 0.03077404, Gradient norm: 0.24118865
INFO:root:[   31] Training loss: 0.02967845, Validation loss: 0.02907245, Gradient norm: 0.30575764
INFO:root:[   32] Training loss: 0.02975065, Validation loss: 0.03039277, Gradient norm: 0.32030477
INFO:root:[   33] Training loss: 0.02958095, Validation loss: 0.02955105, Gradient norm: 0.30505711
INFO:root:[   34] Training loss: 0.02969139, Validation loss: 0.02880990, Gradient norm: 0.36056762
INFO:root:[   35] Training loss: 0.02883773, Validation loss: 0.02819472, Gradient norm: 0.30338814
INFO:root:[   36] Training loss: 0.02808646, Validation loss: 0.02804543, Gradient norm: 0.29903190
INFO:root:[   37] Training loss: 0.02764722, Validation loss: 0.02727279, Gradient norm: 0.26765485
INFO:root:[   38] Training loss: 0.02763059, Validation loss: 0.02814841, Gradient norm: 0.26732615
INFO:root:[   39] Training loss: 0.02841735, Validation loss: 0.02719113, Gradient norm: 0.31467679
INFO:root:[   40] Training loss: 0.02700908, Validation loss: 0.02699065, Gradient norm: 0.27897517
INFO:root:[   41] Training loss: 0.02671533, Validation loss: 0.02746389, Gradient norm: 0.26637420
INFO:root:[   42] Training loss: 0.02724332, Validation loss: 0.02769904, Gradient norm: 0.33968570
INFO:root:[   43] Training loss: 0.02721170, Validation loss: 0.02751403, Gradient norm: 0.36064350
INFO:root:[   44] Training loss: 0.02648637, Validation loss: 0.02631675, Gradient norm: 0.26360330
INFO:root:[   45] Training loss: 0.02717587, Validation loss: 0.02697434, Gradient norm: 0.34484344
INFO:root:[   46] Training loss: 0.02737756, Validation loss: 0.02623798, Gradient norm: 0.40669797
INFO:root:[   47] Training loss: 0.02597094, Validation loss: 0.02749806, Gradient norm: 0.30153142
INFO:root:[   48] Training loss: 0.02653034, Validation loss: 0.02549678, Gradient norm: 0.31195469
INFO:root:[   49] Training loss: 0.02625695, Validation loss: 0.02528504, Gradient norm: 0.30493761
INFO:root:[   50] Training loss: 0.02577607, Validation loss: 0.02544536, Gradient norm: 0.35719100
INFO:root:[   51] Training loss: 0.02598669, Validation loss: 0.02584501, Gradient norm: 0.28651800
INFO:root:[   52] Training loss: 0.02475343, Validation loss: 0.02623948, Gradient norm: 0.29926659
INFO:root:[   53] Training loss: 0.02505794, Validation loss: 0.02550921, Gradient norm: 0.25117352
INFO:root:[   54] Training loss: 0.02527111, Validation loss: 0.02577992, Gradient norm: 0.36151588
INFO:root:[   55] Training loss: 0.02491097, Validation loss: 0.02522288, Gradient norm: 0.30551556
INFO:root:[   56] Training loss: 0.02419288, Validation loss: 0.02551738, Gradient norm: 0.21839570
INFO:root:[   57] Training loss: 0.02467581, Validation loss: 0.02474345, Gradient norm: 0.28748214
INFO:root:[   58] Training loss: 0.02587282, Validation loss: 0.02552107, Gradient norm: 0.42763245
INFO:root:[   59] Training loss: 0.02428236, Validation loss: 0.02554249, Gradient norm: 0.30023342
INFO:root:[   60] Training loss: 0.02469107, Validation loss: 0.02509666, Gradient norm: 0.28766368
INFO:root:[   61] Training loss: 0.02441942, Validation loss: 0.02479157, Gradient norm: 0.25313320
INFO:root:[   62] Training loss: 0.02462725, Validation loss: 0.02446837, Gradient norm: 0.34678743
INFO:root:[   63] Training loss: 0.02479072, Validation loss: 0.02393378, Gradient norm: 0.27231803
INFO:root:[   64] Training loss: 0.02428218, Validation loss: 0.02446094, Gradient norm: 0.31086582
INFO:root:[   65] Training loss: 0.02443905, Validation loss: 0.02480211, Gradient norm: 0.25566652
INFO:root:[   66] Training loss: 0.02416130, Validation loss: 0.02492344, Gradient norm: 0.31578555
INFO:root:[   67] Training loss: 0.02441415, Validation loss: 0.02444826, Gradient norm: 0.29505619
INFO:root:[   68] Training loss: 0.02470518, Validation loss: 0.02354798, Gradient norm: 0.36833962
INFO:root:[   69] Training loss: 0.02367247, Validation loss: 0.02458163, Gradient norm: 0.28791357
INFO:root:[   70] Training loss: 0.02357968, Validation loss: 0.02441407, Gradient norm: 0.26920895
INFO:root:[   71] Training loss: 0.02402989, Validation loss: 0.02372386, Gradient norm: 0.29777232
INFO:root:[   72] Training loss: 0.02362610, Validation loss: 0.02371765, Gradient norm: 0.28719691
INFO:root:[   73] Training loss: 0.02348964, Validation loss: 0.02368866, Gradient norm: 0.25488112
INFO:root:[   74] Training loss: 0.02379418, Validation loss: 0.02333401, Gradient norm: 0.26164416
INFO:root:[   75] Training loss: 0.02417204, Validation loss: 0.02418837, Gradient norm: 0.36855078
INFO:root:[   76] Training loss: 0.02328704, Validation loss: 0.02319323, Gradient norm: 0.22732801
INFO:root:[   77] Training loss: 0.02293637, Validation loss: 0.02300404, Gradient norm: 0.22024336
INFO:root:[   78] Training loss: 0.02356215, Validation loss: 0.02381071, Gradient norm: 0.24388995
INFO:root:[   79] Training loss: 0.02371009, Validation loss: 0.02439403, Gradient norm: 0.34691226
INFO:root:[   80] Training loss: 0.02322535, Validation loss: 0.02299959, Gradient norm: 0.28982359
INFO:root:[   81] Training loss: 0.02316234, Validation loss: 0.02349138, Gradient norm: 0.30042468
INFO:root:[   82] Training loss: 0.02346174, Validation loss: 0.02275626, Gradient norm: 0.33551217
INFO:root:[   83] Training loss: 0.02349816, Validation loss: 0.02393152, Gradient norm: 0.33205503
INFO:root:[   84] Training loss: 0.02369805, Validation loss: 0.02322585, Gradient norm: 0.36107109
INFO:root:[   85] Training loss: 0.02324845, Validation loss: 0.02447815, Gradient norm: 0.31873558
INFO:root:[   86] Training loss: 0.02310203, Validation loss: 0.02337962, Gradient norm: 0.37496135
INFO:root:[   87] Training loss: 0.02274861, Validation loss: 0.02259497, Gradient norm: 0.22393596
INFO:root:[   88] Training loss: 0.02283737, Validation loss: 0.02295979, Gradient norm: 0.32629339
INFO:root:[   89] Training loss: 0.02262139, Validation loss: 0.02227380, Gradient norm: 0.28095738
INFO:root:[   90] Training loss: 0.02241774, Validation loss: 0.02298897, Gradient norm: 0.23208022
INFO:root:[   91] Training loss: 0.02299865, Validation loss: 0.02254909, Gradient norm: 0.27149181
INFO:root:[   92] Training loss: 0.02279056, Validation loss: 0.02193131, Gradient norm: 0.31564466
INFO:root:[   93] Training loss: 0.02271086, Validation loss: 0.02275228, Gradient norm: 0.30329262
INFO:root:[   94] Training loss: 0.02313296, Validation loss: 0.02229313, Gradient norm: 0.35803393
INFO:root:[   95] Training loss: 0.02165014, Validation loss: 0.02253608, Gradient norm: 0.20441619
INFO:root:[   96] Training loss: 0.02189261, Validation loss: 0.02235927, Gradient norm: 0.27687814
INFO:root:[   97] Training loss: 0.02213614, Validation loss: 0.02213698, Gradient norm: 0.24662197
INFO:root:[   98] Training loss: 0.02211123, Validation loss: 0.02299936, Gradient norm: 0.21373485
INFO:root:[   99] Training loss: 0.02235729, Validation loss: 0.02183874, Gradient norm: 0.29994248
INFO:root:[  100] Training loss: 0.02262411, Validation loss: 0.02216661, Gradient norm: 0.39055096
INFO:root:[  101] Training loss: 0.02175546, Validation loss: 0.02259880, Gradient norm: 0.28373264
INFO:root:[  102] Training loss: 0.02230664, Validation loss: 0.02175653, Gradient norm: 0.24382920
INFO:root:[  103] Training loss: 0.02241829, Validation loss: 0.02249067, Gradient norm: 0.33346317
INFO:root:[  104] Training loss: 0.02219237, Validation loss: 0.02175980, Gradient norm: 0.28534250
INFO:root:[  105] Training loss: 0.02144707, Validation loss: 0.02224264, Gradient norm: 0.27104864
INFO:root:[  106] Training loss: 0.02234639, Validation loss: 0.02204320, Gradient norm: 0.35183492
INFO:root:[  107] Training loss: 0.02146871, Validation loss: 0.02170611, Gradient norm: 0.28043581
INFO:root:[  108] Training loss: 0.02116796, Validation loss: 0.02176659, Gradient norm: 0.20926982
INFO:root:[  109] Training loss: 0.02160081, Validation loss: 0.02143135, Gradient norm: 0.26831087
INFO:root:[  110] Training loss: 0.02196789, Validation loss: 0.02196744, Gradient norm: 0.31258056
INFO:root:[  111] Training loss: 0.02150381, Validation loss: 0.02124479, Gradient norm: 0.33321577
INFO:root:[  112] Training loss: 0.02166336, Validation loss: 0.02228260, Gradient norm: 0.25000328
INFO:root:[  113] Training loss: 0.02181791, Validation loss: 0.02167640, Gradient norm: 0.19688808
INFO:root:[  114] Training loss: 0.02124305, Validation loss: 0.02144396, Gradient norm: 0.26063875
INFO:root:[  115] Training loss: 0.02168941, Validation loss: 0.02137655, Gradient norm: 0.25280676
INFO:root:[  116] Training loss: 0.02151635, Validation loss: 0.02157922, Gradient norm: 0.35995359
INFO:root:[  117] Training loss: 0.02082353, Validation loss: 0.02177217, Gradient norm: 0.27815934
INFO:root:[  118] Training loss: 0.02131374, Validation loss: 0.02107480, Gradient norm: 0.29577223
INFO:root:[  119] Training loss: 0.02097063, Validation loss: 0.02204755, Gradient norm: 0.26236359
INFO:root:[  120] Training loss: 0.02181157, Validation loss: 0.02224965, Gradient norm: 0.42561855
INFO:root:[  121] Training loss: 0.02141537, Validation loss: 0.02131606, Gradient norm: 0.29301516
INFO:root:[  122] Training loss: 0.02244245, Validation loss: 0.02207828, Gradient norm: 0.37833872
INFO:root:[  123] Training loss: 0.02157028, Validation loss: 0.02172909, Gradient norm: 0.33246207
INFO:root:[  124] Training loss: 0.02124672, Validation loss: 0.02183495, Gradient norm: 0.28543384
INFO:root:[  125] Training loss: 0.02135072, Validation loss: 0.02112542, Gradient norm: 0.26150508
INFO:root:[  126] Training loss: 0.02123029, Validation loss: 0.02189602, Gradient norm: 0.32943426
INFO:root:[  127] Training loss: 0.02070506, Validation loss: 0.02111800, Gradient norm: 0.28704848
INFO:root:EP 127: Early stopping
INFO:root:Training the model took 657.976s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82182
INFO:root:EnergyScoreTrain: 0.65427
INFO:root:CoverageTrain: 0.98639
INFO:root:IntervalWidthTrain: 0.16989
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.84413
INFO:root:EnergyScoreValidation: 0.67166
INFO:root:CoverageValidation: 0.98385
INFO:root:IntervalWidthValidation: 0.17131
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.40079
INFO:root:EnergyScoreTest: 0.99742
INFO:root:CoverageTest: 0.96853
INFO:root:IntervalWidthTest: 0.20463
INFO:root:###7 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.16955607, Validation loss: 0.13690592, Gradient norm: 0.57204183
INFO:root:[    2] Training loss: 0.11660569, Validation loss: 0.10357645, Gradient norm: 0.35203379
INFO:root:[    3] Training loss: 0.09508187, Validation loss: 0.08808739, Gradient norm: 0.27791415
INFO:root:[    4] Training loss: 0.08157529, Validation loss: 0.07925634, Gradient norm: 0.22191439
INFO:root:[    5] Training loss: 0.07502591, Validation loss: 0.07304796, Gradient norm: 0.21165159
INFO:root:[    6] Training loss: 0.06964018, Validation loss: 0.06875383, Gradient norm: 0.19620935
INFO:root:[    7] Training loss: 0.06599480, Validation loss: 0.06714812, Gradient norm: 0.19950015
INFO:root:[    8] Training loss: 0.06307653, Validation loss: 0.06261769, Gradient norm: 0.18071769
INFO:root:[    9] Training loss: 0.06148782, Validation loss: 0.06115129, Gradient norm: 0.18947839
INFO:root:[   10] Training loss: 0.06047405, Validation loss: 0.05981642, Gradient norm: 0.22221388
INFO:root:[   11] Training loss: 0.05773980, Validation loss: 0.05777364, Gradient norm: 0.17621799
INFO:root:[   12] Training loss: 0.05588801, Validation loss: 0.05644239, Gradient norm: 0.16206002
INFO:root:[   13] Training loss: 0.05590643, Validation loss: 0.05579118, Gradient norm: 0.19699191
INFO:root:[   14] Training loss: 0.05509662, Validation loss: 0.05481436, Gradient norm: 0.19145746
INFO:root:[   15] Training loss: 0.05338862, Validation loss: 0.05340493, Gradient norm: 0.21411950
INFO:root:[   16] Training loss: 0.05197842, Validation loss: 0.05205459, Gradient norm: 0.18297949
INFO:root:[   17] Training loss: 0.05034079, Validation loss: 0.05078260, Gradient norm: 0.19187154
INFO:root:[   18] Training loss: 0.05054150, Validation loss: 0.04980473, Gradient norm: 0.19247431
INFO:root:[   19] Training loss: 0.04835745, Validation loss: 0.04933600, Gradient norm: 0.21054998
INFO:root:[   20] Training loss: 0.04843924, Validation loss: 0.04790752, Gradient norm: 0.20605879
INFO:root:[   21] Training loss: 0.04729115, Validation loss: 0.04624421, Gradient norm: 0.20347510
INFO:root:[   22] Training loss: 0.04573113, Validation loss: 0.04758939, Gradient norm: 0.19589314
INFO:root:[   23] Training loss: 0.04402369, Validation loss: 0.04484927, Gradient norm: 0.22502348
INFO:root:[   24] Training loss: 0.04436129, Validation loss: 0.04452032, Gradient norm: 0.20592206
INFO:root:[   25] Training loss: 0.04205594, Validation loss: 0.04230389, Gradient norm: 0.25285020
INFO:root:[   26] Training loss: 0.04113277, Validation loss: 0.04199069, Gradient norm: 0.21294099
INFO:root:[   27] Training loss: 0.04108620, Validation loss: 0.04030109, Gradient norm: 0.25705987
INFO:root:[   28] Training loss: 0.03787421, Validation loss: 0.04022276, Gradient norm: 0.22311480
INFO:root:[   29] Training loss: 0.03886488, Validation loss: 0.03807715, Gradient norm: 0.23900142
INFO:root:[   30] Training loss: 0.03761587, Validation loss: 0.03912594, Gradient norm: 0.20166382
INFO:root:[   31] Training loss: 0.03797097, Validation loss: 0.03776087, Gradient norm: 0.22791926
INFO:root:[   32] Training loss: 0.03756201, Validation loss: 0.03651626, Gradient norm: 0.30474693
INFO:root:[   33] Training loss: 0.03731298, Validation loss: 0.03579941, Gradient norm: 0.26867434
INFO:root:[   34] Training loss: 0.03593332, Validation loss: 0.03549642, Gradient norm: 0.22012266
INFO:root:[   35] Training loss: 0.03668206, Validation loss: 0.03490822, Gradient norm: 0.25543852
INFO:root:[   36] Training loss: 0.03582804, Validation loss: 0.03469107, Gradient norm: 0.24258522
INFO:root:[   37] Training loss: 0.03511058, Validation loss: 0.03463793, Gradient norm: 0.32390535
INFO:root:[   38] Training loss: 0.03491377, Validation loss: 0.03463985, Gradient norm: 0.28381134
INFO:root:[   39] Training loss: 0.03426194, Validation loss: 0.03409210, Gradient norm: 0.26766387
INFO:root:[   40] Training loss: 0.03320823, Validation loss: 0.03328214, Gradient norm: 0.22952343
INFO:root:[   41] Training loss: 0.03357512, Validation loss: 0.03336864, Gradient norm: 0.22321599
INFO:root:[   42] Training loss: 0.03261687, Validation loss: 0.03376995, Gradient norm: 0.24822239
INFO:root:[   43] Training loss: 0.03289093, Validation loss: 0.03391492, Gradient norm: 0.23218199
INFO:root:[   44] Training loss: 0.03344984, Validation loss: 0.03315304, Gradient norm: 0.28403191
INFO:root:[   45] Training loss: 0.03272963, Validation loss: 0.03292602, Gradient norm: 0.22228980
INFO:root:[   46] Training loss: 0.03219574, Validation loss: 0.03262940, Gradient norm: 0.24006166
INFO:root:[   47] Training loss: 0.03221383, Validation loss: 0.03174043, Gradient norm: 0.32211035
INFO:root:[   48] Training loss: 0.03354192, Validation loss: 0.03278344, Gradient norm: 0.31472488
INFO:root:[   49] Training loss: 0.03173252, Validation loss: 0.03236331, Gradient norm: 0.33997983
INFO:root:[   50] Training loss: 0.03212782, Validation loss: 0.03239869, Gradient norm: 0.23327430
INFO:root:[   51] Training loss: 0.03198557, Validation loss: 0.03349398, Gradient norm: 0.31984012
INFO:root:[   52] Training loss: 0.03172796, Validation loss: 0.03160604, Gradient norm: 0.24482608
INFO:root:[   53] Training loss: 0.03159825, Validation loss: 0.03113021, Gradient norm: 0.22941646
INFO:root:[   54] Training loss: 0.03042675, Validation loss: 0.03147443, Gradient norm: 0.24574258
INFO:root:[   55] Training loss: 0.03090767, Validation loss: 0.03076933, Gradient norm: 0.27578479
INFO:root:[   56] Training loss: 0.03067089, Validation loss: 0.03083021, Gradient norm: 0.27566763
INFO:root:[   57] Training loss: 0.03067879, Validation loss: 0.03045798, Gradient norm: 0.24936571
INFO:root:[   58] Training loss: 0.03046471, Validation loss: 0.03107377, Gradient norm: 0.24315529
INFO:root:[   59] Training loss: 0.03054559, Validation loss: 0.02999038, Gradient norm: 0.30219248
INFO:root:[   60] Training loss: 0.03052775, Validation loss: 0.02976321, Gradient norm: 0.25752046
INFO:root:[   61] Training loss: 0.03104808, Validation loss: 0.03188159, Gradient norm: 0.32162472
INFO:root:[   62] Training loss: 0.03073429, Validation loss: 0.02978244, Gradient norm: 0.28468402
INFO:root:[   63] Training loss: 0.02971712, Validation loss: 0.03042708, Gradient norm: 0.25139961
INFO:root:[   64] Training loss: 0.03071806, Validation loss: 0.02949260, Gradient norm: 0.35687275
INFO:root:[   65] Training loss: 0.03033395, Validation loss: 0.02961847, Gradient norm: 0.30821111
INFO:root:[   66] Training loss: 0.02921575, Validation loss: 0.02997421, Gradient norm: 0.21407687
INFO:root:[   67] Training loss: 0.02935040, Validation loss: 0.02999640, Gradient norm: 0.28738699
INFO:root:[   68] Training loss: 0.02964600, Validation loss: 0.02914921, Gradient norm: 0.26651401
INFO:root:[   69] Training loss: 0.02950374, Validation loss: 0.02935821, Gradient norm: 0.31309842
INFO:root:[   70] Training loss: 0.02958405, Validation loss: 0.02906485, Gradient norm: 0.24704281
INFO:root:[   71] Training loss: 0.02865138, Validation loss: 0.02910250, Gradient norm: 0.24021769
INFO:root:[   72] Training loss: 0.02958949, Validation loss: 0.02910277, Gradient norm: 0.29348853
INFO:root:[   73] Training loss: 0.03002352, Validation loss: 0.03029457, Gradient norm: 0.32061664
INFO:root:[   74] Training loss: 0.02999544, Validation loss: 0.02950998, Gradient norm: 0.32667521
INFO:root:[   75] Training loss: 0.02879336, Validation loss: 0.02923754, Gradient norm: 0.28833431
INFO:root:[   76] Training loss: 0.02926398, Validation loss: 0.02887365, Gradient norm: 0.28275938
INFO:root:[   77] Training loss: 0.02790654, Validation loss: 0.02850916, Gradient norm: 0.21989225
INFO:root:[   78] Training loss: 0.02828576, Validation loss: 0.02912094, Gradient norm: 0.24899281
INFO:root:[   79] Training loss: 0.02870988, Validation loss: 0.02933595, Gradient norm: 0.32286567
INFO:root:[   80] Training loss: 0.02805036, Validation loss: 0.02821949, Gradient norm: 0.21333909
INFO:root:[   81] Training loss: 0.02881275, Validation loss: 0.02828927, Gradient norm: 0.29858087
INFO:root:[   82] Training loss: 0.02843278, Validation loss: 0.02828383, Gradient norm: 0.24471159
INFO:root:[   83] Training loss: 0.02795873, Validation loss: 0.02848852, Gradient norm: 0.25247004
INFO:root:[   84] Training loss: 0.02755849, Validation loss: 0.02846901, Gradient norm: 0.30056244
INFO:root:[   85] Training loss: 0.02819045, Validation loss: 0.02823425, Gradient norm: 0.35041963
INFO:root:[   86] Training loss: 0.02807910, Validation loss: 0.02762446, Gradient norm: 0.24800225
INFO:root:[   87] Training loss: 0.02848597, Validation loss: 0.02997104, Gradient norm: 0.30636508
INFO:root:[   88] Training loss: 0.02808849, Validation loss: 0.02775552, Gradient norm: 0.33261675
INFO:root:[   89] Training loss: 0.02756093, Validation loss: 0.02820004, Gradient norm: 0.29424420
INFO:root:[   90] Training loss: 0.02823817, Validation loss: 0.02781999, Gradient norm: 0.29113593
INFO:root:[   91] Training loss: 0.02779443, Validation loss: 0.02818555, Gradient norm: 0.32618833
INFO:root:[   92] Training loss: 0.02770132, Validation loss: 0.02745009, Gradient norm: 0.35680642
INFO:root:[   93] Training loss: 0.02764358, Validation loss: 0.02785479, Gradient norm: 0.31398044
INFO:root:[   94] Training loss: 0.02797811, Validation loss: 0.02796025, Gradient norm: 0.26254114
INFO:root:[   95] Training loss: 0.02680768, Validation loss: 0.02790997, Gradient norm: 0.24844287
INFO:root:[   96] Training loss: 0.02747900, Validation loss: 0.02768301, Gradient norm: 0.23936055
INFO:root:[   97] Training loss: 0.02789292, Validation loss: 0.02710199, Gradient norm: 0.22523120
INFO:root:[   98] Training loss: 0.02739992, Validation loss: 0.02739008, Gradient norm: 0.28017155
INFO:root:[   99] Training loss: 0.02725089, Validation loss: 0.02779780, Gradient norm: 0.24844756
INFO:root:[  100] Training loss: 0.02735839, Validation loss: 0.02719244, Gradient norm: 0.26846088
INFO:root:[  101] Training loss: 0.02719709, Validation loss: 0.02728063, Gradient norm: 0.28158526
INFO:root:[  102] Training loss: 0.02727583, Validation loss: 0.02733363, Gradient norm: 0.28920933
INFO:root:[  103] Training loss: 0.02625850, Validation loss: 0.02702268, Gradient norm: 0.25723825
INFO:root:[  104] Training loss: 0.02699583, Validation loss: 0.02740757, Gradient norm: 0.31728796
INFO:root:[  105] Training loss: 0.02764470, Validation loss: 0.02680231, Gradient norm: 0.28043515
INFO:root:[  106] Training loss: 0.02701306, Validation loss: 0.02681757, Gradient norm: 0.26861723
INFO:root:[  107] Training loss: 0.02657973, Validation loss: 0.02773349, Gradient norm: 0.32277872
INFO:root:[  108] Training loss: 0.02724973, Validation loss: 0.02685216, Gradient norm: 0.30386785
INFO:root:[  109] Training loss: 0.02673318, Validation loss: 0.02740534, Gradient norm: 0.35872282
INFO:root:[  110] Training loss: 0.02612908, Validation loss: 0.02636644, Gradient norm: 0.27212984
INFO:root:[  111] Training loss: 0.02649303, Validation loss: 0.02683082, Gradient norm: 0.24463797
INFO:root:[  112] Training loss: 0.02674190, Validation loss: 0.02633460, Gradient norm: 0.29213179
INFO:root:[  113] Training loss: 0.02655132, Validation loss: 0.02653877, Gradient norm: 0.31879354
INFO:root:[  114] Training loss: 0.02606117, Validation loss: 0.02686544, Gradient norm: 0.29895248
INFO:root:[  115] Training loss: 0.02667484, Validation loss: 0.02728211, Gradient norm: 0.30421024
INFO:root:[  116] Training loss: 0.02625786, Validation loss: 0.02628106, Gradient norm: 0.37959406
INFO:root:[  117] Training loss: 0.02625640, Validation loss: 0.02658604, Gradient norm: 0.32510432
INFO:root:[  118] Training loss: 0.02662069, Validation loss: 0.02603921, Gradient norm: 0.30253816
INFO:root:[  119] Training loss: 0.02546971, Validation loss: 0.02594392, Gradient norm: 0.23894104
INFO:root:[  120] Training loss: 0.02589117, Validation loss: 0.02631763, Gradient norm: 0.26825304
INFO:root:[  121] Training loss: 0.02587108, Validation loss: 0.02647061, Gradient norm: 0.24489841
INFO:root:[  122] Training loss: 0.02567712, Validation loss: 0.02567981, Gradient norm: 0.25179721
INFO:root:[  123] Training loss: 0.02593976, Validation loss: 0.02587898, Gradient norm: 0.28083916
INFO:root:[  124] Training loss: 0.02651643, Validation loss: 0.02620441, Gradient norm: 0.27876014
INFO:root:[  125] Training loss: 0.02585439, Validation loss: 0.02666330, Gradient norm: 0.21605481
INFO:root:[  126] Training loss: 0.02570071, Validation loss: 0.02719432, Gradient norm: 0.26311422
INFO:root:[  127] Training loss: 0.02641016, Validation loss: 0.02664218, Gradient norm: 0.39093275
INFO:root:[  128] Training loss: 0.02668424, Validation loss: 0.02635820, Gradient norm: 0.35293390
INFO:root:[  129] Training loss: 0.02551382, Validation loss: 0.02573666, Gradient norm: 0.27102261
INFO:root:[  130] Training loss: 0.02576781, Validation loss: 0.02661280, Gradient norm: 0.26998353
INFO:root:[  131] Training loss: 0.02528317, Validation loss: 0.02563466, Gradient norm: 0.30719525
INFO:root:[  132] Training loss: 0.02609373, Validation loss: 0.02545366, Gradient norm: 0.27224967
INFO:root:[  133] Training loss: 0.02561694, Validation loss: 0.02533211, Gradient norm: 0.23215318
INFO:root:[  134] Training loss: 0.02530220, Validation loss: 0.02643657, Gradient norm: 0.26798546
INFO:root:[  135] Training loss: 0.02632074, Validation loss: 0.02620055, Gradient norm: 0.43193435
INFO:root:[  136] Training loss: 0.02496164, Validation loss: 0.02507436, Gradient norm: 0.22068918
INFO:root:[  137] Training loss: 0.02533591, Validation loss: 0.02672417, Gradient norm: 0.27250346
INFO:root:[  138] Training loss: 0.02550386, Validation loss: 0.02554256, Gradient norm: 0.32588087
INFO:root:[  139] Training loss: 0.02597412, Validation loss: 0.02518271, Gradient norm: 0.24176938
INFO:root:[  140] Training loss: 0.02533351, Validation loss: 0.02579082, Gradient norm: 0.26032091
INFO:root:[  141] Training loss: 0.02569884, Validation loss: 0.02507015, Gradient norm: 0.28712819
INFO:root:[  142] Training loss: 0.02556611, Validation loss: 0.02516589, Gradient norm: 0.31192548
INFO:root:[  143] Training loss: 0.02456385, Validation loss: 0.02460022, Gradient norm: 0.24654644
INFO:root:[  144] Training loss: 0.02554029, Validation loss: 0.02513662, Gradient norm: 0.28212742
INFO:root:[  145] Training loss: 0.02567817, Validation loss: 0.02684056, Gradient norm: 0.28053690
INFO:root:[  146] Training loss: 0.02566492, Validation loss: 0.02481334, Gradient norm: 0.34950014
INFO:root:[  147] Training loss: 0.02455815, Validation loss: 0.02460779, Gradient norm: 0.24847732
INFO:root:[  148] Training loss: 0.02477827, Validation loss: 0.02476773, Gradient norm: 0.33597798
INFO:root:[  149] Training loss: 0.02456160, Validation loss: 0.02473505, Gradient norm: 0.32609034
INFO:root:[  150] Training loss: 0.02453171, Validation loss: 0.02478379, Gradient norm: 0.25957443
INFO:root:[  151] Training loss: 0.02467768, Validation loss: 0.02467807, Gradient norm: 0.31706315
INFO:root:[  152] Training loss: 0.02466384, Validation loss: 0.02505984, Gradient norm: 0.31711578
INFO:root:[  153] Training loss: 0.02442644, Validation loss: 0.02458750, Gradient norm: 0.27913533
INFO:root:[  154] Training loss: 0.02500142, Validation loss: 0.02500313, Gradient norm: 0.33889729
INFO:root:[  155] Training loss: 0.02426610, Validation loss: 0.02489429, Gradient norm: 0.24814665
INFO:root:[  156] Training loss: 0.02443750, Validation loss: 0.02474185, Gradient norm: 0.24998391
INFO:root:[  157] Training loss: 0.02498196, Validation loss: 0.02480170, Gradient norm: 0.28173430
INFO:root:[  158] Training loss: 0.02598671, Validation loss: 0.02450288, Gradient norm: 0.43697575
INFO:root:[  159] Training loss: 0.02513214, Validation loss: 0.02466443, Gradient norm: 0.33783847
INFO:root:[  160] Training loss: 0.02458817, Validation loss: 0.02435121, Gradient norm: 0.31975994
INFO:root:[  161] Training loss: 0.02461151, Validation loss: 0.02449086, Gradient norm: 0.38033350
INFO:root:[  162] Training loss: 0.02397592, Validation loss: 0.02565252, Gradient norm: 0.26010767
INFO:root:[  163] Training loss: 0.02395156, Validation loss: 0.02442043, Gradient norm: 0.22466576
INFO:root:[  164] Training loss: 0.02465883, Validation loss: 0.02505531, Gradient norm: 0.30521148
INFO:root:[  165] Training loss: 0.02434463, Validation loss: 0.02405334, Gradient norm: 0.39714661
INFO:root:[  166] Training loss: 0.02418535, Validation loss: 0.02512555, Gradient norm: 0.32513323
INFO:root:[  167] Training loss: 0.02546139, Validation loss: 0.02498103, Gradient norm: 0.29289170
INFO:root:[  168] Training loss: 0.02386953, Validation loss: 0.02480959, Gradient norm: 0.27223339
INFO:root:[  169] Training loss: 0.02453583, Validation loss: 0.02468746, Gradient norm: 0.26836094
INFO:root:[  170] Training loss: 0.02394297, Validation loss: 0.02586865, Gradient norm: 0.29739559
INFO:root:[  171] Training loss: 0.02416956, Validation loss: 0.02439015, Gradient norm: 0.37125437
INFO:root:[  172] Training loss: 0.02418123, Validation loss: 0.02402178, Gradient norm: 0.25106654
INFO:root:[  173] Training loss: 0.02378895, Validation loss: 0.02465070, Gradient norm: 0.29060824
INFO:root:[  174] Training loss: 0.02338269, Validation loss: 0.02395884, Gradient norm: 0.18404748
INFO:root:[  175] Training loss: 0.02390739, Validation loss: 0.02352979, Gradient norm: 0.27215632
INFO:root:[  176] Training loss: 0.02389847, Validation loss: 0.02403586, Gradient norm: 0.26625818
INFO:root:[  177] Training loss: 0.02347783, Validation loss: 0.02396876, Gradient norm: 0.24987458
INFO:root:[  178] Training loss: 0.02422504, Validation loss: 0.02354243, Gradient norm: 0.25674044
INFO:root:[  179] Training loss: 0.02358421, Validation loss: 0.02427831, Gradient norm: 0.26238861
INFO:root:[  180] Training loss: 0.02394563, Validation loss: 0.02406114, Gradient norm: 0.26684412
INFO:root:[  181] Training loss: 0.02452926, Validation loss: 0.02371670, Gradient norm: 0.42368926
INFO:root:[  182] Training loss: 0.02350376, Validation loss: 0.02386489, Gradient norm: 0.23925186
INFO:root:[  183] Training loss: 0.02387450, Validation loss: 0.02362628, Gradient norm: 0.33935000
INFO:root:[  184] Training loss: 0.02365942, Validation loss: 0.02346292, Gradient norm: 0.25457417
INFO:root:[  185] Training loss: 0.02376976, Validation loss: 0.02402947, Gradient norm: 0.30896113
INFO:root:[  186] Training loss: 0.02385202, Validation loss: 0.02308783, Gradient norm: 0.29540559
INFO:root:[  187] Training loss: 0.02332442, Validation loss: 0.02326681, Gradient norm: 0.29202824
INFO:root:[  188] Training loss: 0.02313012, Validation loss: 0.02361157, Gradient norm: 0.27888279
INFO:root:[  189] Training loss: 0.02309784, Validation loss: 0.02343673, Gradient norm: 0.28491058
INFO:root:[  190] Training loss: 0.02290269, Validation loss: 0.02330138, Gradient norm: 0.31261557
INFO:root:[  191] Training loss: 0.02356740, Validation loss: 0.02361869, Gradient norm: 0.28854107
INFO:root:[  192] Training loss: 0.02365847, Validation loss: 0.02330709, Gradient norm: 0.30309474
INFO:root:[  193] Training loss: 0.02337385, Validation loss: 0.02339737, Gradient norm: 0.29713318
INFO:root:[  194] Training loss: 0.02397851, Validation loss: 0.02340732, Gradient norm: 0.25871415
INFO:root:[  195] Training loss: 0.02332275, Validation loss: 0.02303345, Gradient norm: 0.27454714
INFO:root:[  196] Training loss: 0.02371949, Validation loss: 0.02318373, Gradient norm: 0.29286891
INFO:root:[  197] Training loss: 0.02289578, Validation loss: 0.02308501, Gradient norm: 0.25074407
INFO:root:[  198] Training loss: 0.02292205, Validation loss: 0.02321725, Gradient norm: 0.29425876
INFO:root:[  199] Training loss: 0.02269751, Validation loss: 0.02283015, Gradient norm: 0.23892349
INFO:root:[  200] Training loss: 0.02292439, Validation loss: 0.02425280, Gradient norm: 0.25444519
INFO:root:[  201] Training loss: 0.02289011, Validation loss: 0.02310777, Gradient norm: 0.32188941
INFO:root:[  202] Training loss: 0.02245958, Validation loss: 0.02320795, Gradient norm: 0.25325490
INFO:root:[  203] Training loss: 0.02229655, Validation loss: 0.02292434, Gradient norm: 0.22146708
INFO:root:[  204] Training loss: 0.02247242, Validation loss: 0.02413267, Gradient norm: 0.25273283
INFO:root:[  205] Training loss: 0.02311520, Validation loss: 0.02365935, Gradient norm: 0.33158295
INFO:root:[  206] Training loss: 0.02251625, Validation loss: 0.02314577, Gradient norm: 0.22936630
INFO:root:[  207] Training loss: 0.02344090, Validation loss: 0.02371792, Gradient norm: 0.34798225
INFO:root:[  208] Training loss: 0.02310958, Validation loss: 0.02284238, Gradient norm: 0.33344241
INFO:root:[  209] Training loss: 0.02327326, Validation loss: 0.02262394, Gradient norm: 0.33850195
INFO:root:[  210] Training loss: 0.02260938, Validation loss: 0.02303002, Gradient norm: 0.23350630
INFO:root:[  211] Training loss: 0.02293094, Validation loss: 0.02298087, Gradient norm: 0.29194580
INFO:root:[  212] Training loss: 0.02238536, Validation loss: 0.02302831, Gradient norm: 0.25852013
INFO:root:[  213] Training loss: 0.02278803, Validation loss: 0.02288429, Gradient norm: 0.28523940
INFO:root:[  214] Training loss: 0.02279217, Validation loss: 0.02413129, Gradient norm: 0.26986386
INFO:root:[  215] Training loss: 0.02309410, Validation loss: 0.02275551, Gradient norm: 0.40216281
INFO:root:[  216] Training loss: 0.02279986, Validation loss: 0.02246374, Gradient norm: 0.25560781
INFO:root:[  217] Training loss: 0.02310414, Validation loss: 0.02257790, Gradient norm: 0.29180296
INFO:root:[  218] Training loss: 0.02297373, Validation loss: 0.02235444, Gradient norm: 0.26185555
INFO:root:[  219] Training loss: 0.02267201, Validation loss: 0.02252308, Gradient norm: 0.34466080
INFO:root:[  220] Training loss: 0.02217690, Validation loss: 0.02233470, Gradient norm: 0.26522943
INFO:root:[  221] Training loss: 0.02224228, Validation loss: 0.02273828, Gradient norm: 0.28110855
INFO:root:[  222] Training loss: 0.02212579, Validation loss: 0.02233715, Gradient norm: 0.31673415
INFO:root:[  223] Training loss: 0.02257650, Validation loss: 0.02250625, Gradient norm: 0.28515905
INFO:root:[  224] Training loss: 0.02166237, Validation loss: 0.02213599, Gradient norm: 0.24328229
INFO:root:[  225] Training loss: 0.02250379, Validation loss: 0.02262812, Gradient norm: 0.26275489
INFO:root:[  226] Training loss: 0.02175725, Validation loss: 0.02244165, Gradient norm: 0.23095830
INFO:root:[  227] Training loss: 0.02220700, Validation loss: 0.02228372, Gradient norm: 0.28291684
INFO:root:[  228] Training loss: 0.02172076, Validation loss: 0.02192069, Gradient norm: 0.19869022
INFO:root:[  229] Training loss: 0.02252428, Validation loss: 0.02223593, Gradient norm: 0.21808243
INFO:root:[  230] Training loss: 0.02194451, Validation loss: 0.02227809, Gradient norm: 0.24525560
INFO:root:[  231] Training loss: 0.02257436, Validation loss: 0.02198652, Gradient norm: 0.24906193
INFO:root:[  232] Training loss: 0.02204611, Validation loss: 0.02189729, Gradient norm: 0.26395423
INFO:root:[  233] Training loss: 0.02229334, Validation loss: 0.02285120, Gradient norm: 0.24717472
INFO:root:[  234] Training loss: 0.02185444, Validation loss: 0.02184900, Gradient norm: 0.26937497
INFO:root:[  235] Training loss: 0.02189837, Validation loss: 0.02308366, Gradient norm: 0.23603131
INFO:root:[  236] Training loss: 0.02214255, Validation loss: 0.02180507, Gradient norm: 0.24849785
INFO:root:[  237] Training loss: 0.02164586, Validation loss: 0.02198552, Gradient norm: 0.26507019
INFO:root:[  238] Training loss: 0.02123367, Validation loss: 0.02184139, Gradient norm: 0.23088209
INFO:root:[  239] Training loss: 0.02134993, Validation loss: 0.02186711, Gradient norm: 0.22712761
INFO:root:[  240] Training loss: 0.02182006, Validation loss: 0.02197439, Gradient norm: 0.22450991
INFO:root:[  241] Training loss: 0.02136639, Validation loss: 0.02150775, Gradient norm: 0.22610045
INFO:root:[  242] Training loss: 0.02191990, Validation loss: 0.02169196, Gradient norm: 0.25270867
INFO:root:[  243] Training loss: 0.02177959, Validation loss: 0.02145960, Gradient norm: 0.22913273
INFO:root:[  244] Training loss: 0.02142213, Validation loss: 0.02150975, Gradient norm: 0.23433574
INFO:root:[  245] Training loss: 0.02159042, Validation loss: 0.02167315, Gradient norm: 0.23078264
INFO:root:[  246] Training loss: 0.02165632, Validation loss: 0.02203248, Gradient norm: 0.29350349
INFO:root:[  247] Training loss: 0.02141148, Validation loss: 0.02171212, Gradient norm: 0.23321100
INFO:root:[  248] Training loss: 0.02125821, Validation loss: 0.02154397, Gradient norm: 0.19872033
INFO:root:[  249] Training loss: 0.02118117, Validation loss: 0.02161043, Gradient norm: 0.24676346
INFO:root:[  250] Training loss: 0.02145336, Validation loss: 0.02162173, Gradient norm: 0.31317639
INFO:root:[  251] Training loss: 0.02165159, Validation loss: 0.02232938, Gradient norm: 0.29200126
INFO:root:[  252] Training loss: 0.02161301, Validation loss: 0.02212782, Gradient norm: 0.29683470
INFO:root:EP 252: Early stopping
INFO:root:Training the model took 1301.697s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.81865
INFO:root:EnergyScoreTrain: 0.67132
INFO:root:CoverageTrain: 0.99038
INFO:root:IntervalWidthTrain: 0.17376
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.84342
INFO:root:EnergyScoreValidation: 0.68913
INFO:root:CoverageValidation: 0.98994
INFO:root:IntervalWidthValidation: 0.17536
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.38785
INFO:root:EnergyScoreTest: 1.03285
INFO:root:CoverageTest: 0.94403
INFO:root:IntervalWidthTest: 0.17099
INFO:root:###8 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09919436, Validation loss: 0.08531466, Gradient norm: 0.31023269
INFO:root:[    2] Training loss: 0.07797492, Validation loss: 0.06940224, Gradient norm: 0.57184931
INFO:root:[    3] Training loss: 0.06558271, Validation loss: 0.05763975, Gradient norm: 0.48974848
INFO:root:[    4] Training loss: 0.05723863, Validation loss: 0.04884757, Gradient norm: 0.94646741
INFO:root:[    5] Training loss: 0.05239734, Validation loss: 0.04336454, Gradient norm: 0.81296385
INFO:root:[    6] Training loss: 0.04837070, Validation loss: 0.04334874, Gradient norm: 0.74955468
INFO:root:[    7] Training loss: 0.04758410, Validation loss: 0.04026460, Gradient norm: 0.84167723
INFO:root:[    8] Training loss: 0.04596201, Validation loss: 0.04625857, Gradient norm: 0.71750910
INFO:root:[    9] Training loss: 0.04549652, Validation loss: 0.03820146, Gradient norm: 0.61821057
INFO:root:[   10] Training loss: 0.04374317, Validation loss: 0.04297780, Gradient norm: 0.55230767
INFO:root:[   11] Training loss: 0.04324588, Validation loss: 0.04017871, Gradient norm: 0.64460184
INFO:root:[   12] Training loss: 0.04294586, Validation loss: 0.03824026, Gradient norm: 0.75262411
INFO:root:[   13] Training loss: 0.04268746, Validation loss: 0.03612362, Gradient norm: 0.79821910
INFO:root:[   14] Training loss: 0.04096041, Validation loss: 0.03561327, Gradient norm: 0.77727112
INFO:root:[   15] Training loss: 0.04014243, Validation loss: 0.03564379, Gradient norm: 0.55753802
INFO:root:[   16] Training loss: 0.03939718, Validation loss: 0.03651600, Gradient norm: 0.60847943
INFO:root:[   17] Training loss: 0.03928996, Validation loss: 0.03572211, Gradient norm: 0.55519531
INFO:root:[   18] Training loss: 0.04006945, Validation loss: 0.03536902, Gradient norm: 0.81499587
INFO:root:[   19] Training loss: 0.03821176, Validation loss: 0.03351745, Gradient norm: 0.51601103
INFO:root:[   20] Training loss: 0.03988073, Validation loss: 0.03302779, Gradient norm: 0.99985301
INFO:root:[   21] Training loss: 0.04002749, Validation loss: 0.03596533, Gradient norm: 1.05930639
INFO:root:[   22] Training loss: 0.03687354, Validation loss: 0.03271932, Gradient norm: 0.69163658
INFO:root:[   23] Training loss: 0.03800900, Validation loss: 0.03566668, Gradient norm: 0.92375981
INFO:root:[   24] Training loss: 0.03771105, Validation loss: 0.03215796, Gradient norm: 0.68452305
INFO:root:[   25] Training loss: 0.03753849, Validation loss: 0.03308248, Gradient norm: 0.92398597
INFO:root:[   26] Training loss: 0.03528702, Validation loss: 0.03258185, Gradient norm: 0.60722863
INFO:root:[   27] Training loss: 0.03646685, Validation loss: 0.03064282, Gradient norm: 0.85497426
INFO:root:[   28] Training loss: 0.03599884, Validation loss: 0.03297377, Gradient norm: 0.73099152
INFO:root:[   29] Training loss: 0.03519508, Validation loss: 0.03030765, Gradient norm: 0.59675917
INFO:root:[   30] Training loss: 0.03586794, Validation loss: 0.03075344, Gradient norm: 0.68318864
INFO:root:[   31] Training loss: 0.03451566, Validation loss: 0.03235292, Gradient norm: 0.61188125
INFO:root:[   32] Training loss: 0.03539666, Validation loss: 0.02980531, Gradient norm: 0.77786503
INFO:root:[   33] Training loss: 0.03487384, Validation loss: 0.03117168, Gradient norm: 0.58749709
INFO:root:[   34] Training loss: 0.03548741, Validation loss: 0.03002145, Gradient norm: 0.85528327
INFO:root:[   35] Training loss: 0.03404718, Validation loss: 0.03071933, Gradient norm: 0.67200700
INFO:root:[   36] Training loss: 0.03359213, Validation loss: 0.02996444, Gradient norm: 0.47161156
INFO:root:[   37] Training loss: 0.03441770, Validation loss: 0.03203582, Gradient norm: 0.78852067
INFO:root:[   38] Training loss: 0.03403046, Validation loss: 0.03031382, Gradient norm: 0.73814775
INFO:root:[   39] Training loss: 0.03386548, Validation loss: 0.03011077, Gradient norm: 0.56436260
INFO:root:[   40] Training loss: 0.03368022, Validation loss: 0.02921280, Gradient norm: 0.56902680
INFO:root:[   41] Training loss: 0.03347668, Validation loss: 0.02851969, Gradient norm: 0.57670680
INFO:root:[   42] Training loss: 0.03262875, Validation loss: 0.02944862, Gradient norm: 0.53995418
INFO:root:[   43] Training loss: 0.03297011, Validation loss: 0.02894865, Gradient norm: 0.70936590
INFO:root:[   44] Training loss: 0.03245104, Validation loss: 0.02932094, Gradient norm: 0.49210526
INFO:root:[   45] Training loss: 0.03285662, Validation loss: 0.02892368, Gradient norm: 0.59684343
INFO:root:[   46] Training loss: 0.03252078, Validation loss: 0.02797204, Gradient norm: 0.49875686
INFO:root:[   47] Training loss: 0.03316800, Validation loss: 0.02919658, Gradient norm: 0.68389523
INFO:root:[   48] Training loss: 0.03340193, Validation loss: 0.03046560, Gradient norm: 0.68019133
INFO:root:[   49] Training loss: 0.03245105, Validation loss: 0.02935751, Gradient norm: 0.73917298
INFO:root:[   50] Training loss: 0.03204562, Validation loss: 0.02760796, Gradient norm: 0.79119433
INFO:root:[   51] Training loss: 0.03222782, Validation loss: 0.02870636, Gradient norm: 0.72829452
INFO:root:[   52] Training loss: 0.03203724, Validation loss: 0.03017556, Gradient norm: 0.71489718
INFO:root:[   53] Training loss: 0.03261595, Validation loss: 0.02894698, Gradient norm: 0.66319696
INFO:root:[   54] Training loss: 0.03165412, Validation loss: 0.02923859, Gradient norm: 0.62240307
INFO:root:[   55] Training loss: 0.03100190, Validation loss: 0.02720843, Gradient norm: 0.63338815
INFO:root:[   56] Training loss: 0.03139211, Validation loss: 0.02770132, Gradient norm: 0.57331120
INFO:root:[   57] Training loss: 0.03120915, Validation loss: 0.02712786, Gradient norm: 0.62601977
INFO:root:[   58] Training loss: 0.03151311, Validation loss: 0.02768720, Gradient norm: 0.59456524
INFO:root:[   59] Training loss: 0.03119994, Validation loss: 0.02830477, Gradient norm: 0.72161177
INFO:root:[   60] Training loss: 0.03192898, Validation loss: 0.02828182, Gradient norm: 0.87119257
INFO:root:[   61] Training loss: 0.03060976, Validation loss: 0.02981219, Gradient norm: 0.50813512
INFO:root:[   62] Training loss: 0.03155274, Validation loss: 0.02697560, Gradient norm: 0.68840286
INFO:root:[   63] Training loss: 0.03036762, Validation loss: 0.02726890, Gradient norm: 0.62101537
INFO:root:[   64] Training loss: 0.03093404, Validation loss: 0.02646793, Gradient norm: 0.57337463
INFO:root:[   65] Training loss: 0.03112935, Validation loss: 0.02826325, Gradient norm: 0.50594954
INFO:root:[   66] Training loss: 0.03070046, Validation loss: 0.02617503, Gradient norm: 0.65301786
INFO:root:[   67] Training loss: 0.02995959, Validation loss: 0.02756076, Gradient norm: 0.63889648
INFO:root:[   68] Training loss: 0.03129726, Validation loss: 0.02684728, Gradient norm: 0.76588362
INFO:root:[   69] Training loss: 0.02988257, Validation loss: 0.02801273, Gradient norm: 0.55947783
INFO:root:[   70] Training loss: 0.03092313, Validation loss: 0.02643180, Gradient norm: 0.67076981
INFO:root:[   71] Training loss: 0.02987694, Validation loss: 0.02865173, Gradient norm: 0.57985212
INFO:root:[   72] Training loss: 0.03095011, Validation loss: 0.02751789, Gradient norm: 0.57533296
INFO:root:[   73] Training loss: 0.03056294, Validation loss: 0.02769611, Gradient norm: 0.70756118
INFO:root:[   74] Training loss: 0.03054086, Validation loss: 0.02651570, Gradient norm: 0.87154490
INFO:root:[   75] Training loss: 0.02994121, Validation loss: 0.02623864, Gradient norm: 0.63887591
INFO:root:[   76] Training loss: 0.02952629, Validation loss: 0.02555622, Gradient norm: 0.66653340
INFO:root:[   77] Training loss: 0.02974372, Validation loss: 0.02623490, Gradient norm: 0.73192532
INFO:root:[   78] Training loss: 0.03045374, Validation loss: 0.02866532, Gradient norm: 0.83553437
INFO:root:[   79] Training loss: 0.02944150, Validation loss: 0.02793785, Gradient norm: 0.51841695
INFO:root:[   80] Training loss: 0.02971418, Validation loss: 0.02596426, Gradient norm: 0.61071228
INFO:root:[   81] Training loss: 0.02983416, Validation loss: 0.02567967, Gradient norm: 0.69654886
INFO:root:[   82] Training loss: 0.02901050, Validation loss: 0.02591451, Gradient norm: 0.57936913
INFO:root:[   83] Training loss: 0.02909839, Validation loss: 0.02635769, Gradient norm: 0.46976445
INFO:root:[   84] Training loss: 0.02897458, Validation loss: 0.02562355, Gradient norm: 0.51055971
INFO:root:[   85] Training loss: 0.02991078, Validation loss: 0.02551148, Gradient norm: 0.86782004
INFO:root:[   86] Training loss: 0.02896003, Validation loss: 0.02619027, Gradient norm: 0.50107211
INFO:root:[   87] Training loss: 0.02959456, Validation loss: 0.02846374, Gradient norm: 0.51477456
INFO:root:[   88] Training loss: 0.02930858, Validation loss: 0.02675153, Gradient norm: 0.74964462
INFO:root:[   89] Training loss: 0.02945600, Validation loss: 0.02564496, Gradient norm: 0.75433283
INFO:root:[   90] Training loss: 0.02884182, Validation loss: 0.02550688, Gradient norm: 0.44445789
INFO:root:[   91] Training loss: 0.02856394, Validation loss: 0.02816140, Gradient norm: 0.42654297
INFO:root:[   92] Training loss: 0.02805112, Validation loss: 0.02486074, Gradient norm: 0.59868627
INFO:root:[   93] Training loss: 0.02877590, Validation loss: 0.02692614, Gradient norm: 0.65729564
INFO:root:[   94] Training loss: 0.02890573, Validation loss: 0.02588245, Gradient norm: 0.75887740
INFO:root:[   95] Training loss: 0.02933738, Validation loss: 0.02524451, Gradient norm: 0.73756684
INFO:root:[   96] Training loss: 0.02818718, Validation loss: 0.02586757, Gradient norm: 0.49588640
INFO:root:[   97] Training loss: 0.02808738, Validation loss: 0.02647720, Gradient norm: 0.51734057
INFO:root:[   98] Training loss: 0.02831998, Validation loss: 0.02497192, Gradient norm: 0.39505963
INFO:root:[   99] Training loss: 0.02804021, Validation loss: 0.02532231, Gradient norm: 0.50084056
INFO:root:[  100] Training loss: 0.02846843, Validation loss: 0.02483363, Gradient norm: 0.58882464
INFO:root:[  101] Training loss: 0.02782671, Validation loss: 0.02617269, Gradient norm: 0.63869243
INFO:root:[  102] Training loss: 0.02865884, Validation loss: 0.02458886, Gradient norm: 0.69904852
INFO:root:[  103] Training loss: 0.02843921, Validation loss: 0.02440203, Gradient norm: 0.64650186
INFO:root:[  104] Training loss: 0.02760947, Validation loss: 0.02504677, Gradient norm: 0.59628929
INFO:root:[  105] Training loss: 0.02849967, Validation loss: 0.03167759, Gradient norm: 0.63869031
INFO:root:[  106] Training loss: 0.02919744, Validation loss: 0.02438176, Gradient norm: 0.78564622
INFO:root:[  107] Training loss: 0.02779118, Validation loss: 0.02689785, Gradient norm: 0.54015024
INFO:root:[  108] Training loss: 0.02893267, Validation loss: 0.02610748, Gradient norm: 0.84380963
INFO:root:[  109] Training loss: 0.02830993, Validation loss: 0.02520987, Gradient norm: 0.70833744
INFO:root:[  110] Training loss: 0.02706054, Validation loss: 0.02473662, Gradient norm: 0.43856557
INFO:root:[  111] Training loss: 0.02679275, Validation loss: 0.02500316, Gradient norm: 0.43827358
INFO:root:[  112] Training loss: 0.02739931, Validation loss: 0.02421293, Gradient norm: 0.49105610
INFO:root:[  113] Training loss: 0.02726064, Validation loss: 0.02465732, Gradient norm: 0.48290039
INFO:root:[  114] Training loss: 0.02728910, Validation loss: 0.02460015, Gradient norm: 0.53545424
INFO:root:[  115] Training loss: 0.02709009, Validation loss: 0.02471289, Gradient norm: 0.53266663
INFO:root:[  116] Training loss: 0.02707261, Validation loss: 0.02393221, Gradient norm: 0.48540386
INFO:root:[  117] Training loss: 0.02709081, Validation loss: 0.02448204, Gradient norm: 0.53945158
INFO:root:[  118] Training loss: 0.02793296, Validation loss: 0.02728489, Gradient norm: 0.62058467
INFO:root:[  119] Training loss: 0.02771013, Validation loss: 0.02479934, Gradient norm: 0.67617345
INFO:root:[  120] Training loss: 0.02751469, Validation loss: 0.02419113, Gradient norm: 0.56963081
INFO:root:[  121] Training loss: 0.02735305, Validation loss: 0.02425251, Gradient norm: 0.39533763
INFO:root:[  122] Training loss: 0.02706101, Validation loss: 0.02358613, Gradient norm: 0.61875976
INFO:root:[  123] Training loss: 0.02607073, Validation loss: 0.02395355, Gradient norm: 0.47465375
INFO:root:[  124] Training loss: 0.02688430, Validation loss: 0.02517114, Gradient norm: 0.60115102
INFO:root:[  125] Training loss: 0.02625904, Validation loss: 0.02560275, Gradient norm: 0.48650806
INFO:root:[  126] Training loss: 0.02703924, Validation loss: 0.02618256, Gradient norm: 0.60532525
INFO:root:[  127] Training loss: 0.02691222, Validation loss: 0.02404256, Gradient norm: 0.60817224
INFO:root:[  128] Training loss: 0.02689018, Validation loss: 0.02417625, Gradient norm: 0.60931835
INFO:root:[  129] Training loss: 0.02790048, Validation loss: 0.02388259, Gradient norm: 0.63888167
INFO:root:[  130] Training loss: 0.02738479, Validation loss: 0.02514697, Gradient norm: 0.60519435
INFO:root:[  131] Training loss: 0.02664950, Validation loss: 0.02403066, Gradient norm: 0.57851612
INFO:root:EP 131: Early stopping
INFO:root:Training the model took 253.897s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.72711
INFO:root:EnergyScoreTrain: 0.62478
INFO:root:CoverageTrain: 0.2479
INFO:root:IntervalWidthTrain: 0.0095
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.75923
INFO:root:EnergyScoreValidation: 0.65128
INFO:root:CoverageValidation: 0.24885
INFO:root:IntervalWidthValidation: 0.00987
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.66092
INFO:root:EnergyScoreTest: 1.52838
INFO:root:CoverageTest: 0.05639
INFO:root:IntervalWidthTest: 0.0107
INFO:root:###9 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 211812352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09423843, Validation loss: 0.07567205, Gradient norm: 0.28571895
INFO:root:[    2] Training loss: 0.07760078, Validation loss: 0.06138607, Gradient norm: 0.38625101
INFO:root:[    3] Training loss: 0.06854617, Validation loss: 0.06724998, Gradient norm: 0.55374814
INFO:root:[    4] Training loss: 0.06297043, Validation loss: 0.05129076, Gradient norm: 0.47332243
INFO:root:[    5] Training loss: 0.05919666, Validation loss: 0.04944650, Gradient norm: 0.47676865
INFO:root:[    6] Training loss: 0.05749988, Validation loss: 0.05069051, Gradient norm: 0.39541952
INFO:root:[    7] Training loss: 0.05526927, Validation loss: 0.04587301, Gradient norm: 0.37016909
INFO:root:[    8] Training loss: 0.05462268, Validation loss: 0.04580924, Gradient norm: 0.36705603
INFO:root:[    9] Training loss: 0.05338370, Validation loss: 0.04698154, Gradient norm: 0.37389313
INFO:root:[   10] Training loss: 0.05136801, Validation loss: 0.04358172, Gradient norm: 0.27623946
INFO:root:[   11] Training loss: 0.05226049, Validation loss: 0.04611654, Gradient norm: 0.36637462
INFO:root:[   12] Training loss: 0.05067165, Validation loss: 0.04303108, Gradient norm: 0.26495078
INFO:root:[   13] Training loss: 0.05170152, Validation loss: 0.04301985, Gradient norm: 0.38540263
INFO:root:[   14] Training loss: 0.05023059, Validation loss: 0.04335786, Gradient norm: 0.31005766
INFO:root:[   15] Training loss: 0.04897490, Validation loss: 0.04163405, Gradient norm: 0.35707919
INFO:root:[   16] Training loss: 0.04965663, Validation loss: 0.04275376, Gradient norm: 0.36395243
INFO:root:[   17] Training loss: 0.04741848, Validation loss: 0.03980250, Gradient norm: 0.29493069
INFO:root:[   18] Training loss: 0.04695679, Validation loss: 0.04054684, Gradient norm: 0.25046053
INFO:root:[   19] Training loss: 0.04714131, Validation loss: 0.04063090, Gradient norm: 0.38638564
INFO:root:[   20] Training loss: 0.04719066, Validation loss: 0.03985564, Gradient norm: 0.40208448
INFO:root:[   21] Training loss: 0.04608805, Validation loss: 0.04033875, Gradient norm: 0.33130744
INFO:root:[   22] Training loss: 0.04625290, Validation loss: 0.03778183, Gradient norm: 0.46047005
INFO:root:[   23] Training loss: 0.04514878, Validation loss: 0.03702478, Gradient norm: 0.33651217
INFO:root:[   24] Training loss: 0.04477974, Validation loss: 0.03694305, Gradient norm: 0.27521517
INFO:root:[   25] Training loss: 0.04460597, Validation loss: 0.03659649, Gradient norm: 0.30216476
INFO:root:[   26] Training loss: 0.04391662, Validation loss: 0.03667617, Gradient norm: 0.42291258
INFO:root:[   27] Training loss: 0.04379844, Validation loss: 0.03547790, Gradient norm: 0.34067967
INFO:root:[   28] Training loss: 0.04348132, Validation loss: 0.03538359, Gradient norm: 0.36839654
INFO:root:[   29] Training loss: 0.04275721, Validation loss: 0.03438909, Gradient norm: 0.31656399
INFO:root:[   30] Training loss: 0.04124133, Validation loss: 0.03452457, Gradient norm: 0.28152078
INFO:root:[   31] Training loss: 0.04190158, Validation loss: 0.03498387, Gradient norm: 0.46380225
INFO:root:[   32] Training loss: 0.04097827, Validation loss: 0.03504558, Gradient norm: 0.24813528
INFO:root:[   33] Training loss: 0.04230811, Validation loss: 0.03546948, Gradient norm: 0.42342963
INFO:root:[   34] Training loss: 0.04190365, Validation loss: 0.03388648, Gradient norm: 0.50425960
INFO:root:[   35] Training loss: 0.04069122, Validation loss: 0.03483000, Gradient norm: 0.27401366
INFO:root:[   36] Training loss: 0.04150436, Validation loss: 0.03519372, Gradient norm: 0.38901183
INFO:root:[   37] Training loss: 0.04057798, Validation loss: 0.03378405, Gradient norm: 0.35556012
INFO:root:[   38] Training loss: 0.03988515, Validation loss: 0.03357663, Gradient norm: 0.28322597
INFO:root:[   39] Training loss: 0.04002274, Validation loss: 0.03388307, Gradient norm: 0.30074817
INFO:root:[   40] Training loss: 0.03994235, Validation loss: 0.03485271, Gradient norm: 0.41589414
INFO:root:[   41] Training loss: 0.03982532, Validation loss: 0.03268125, Gradient norm: 0.36835084
INFO:root:[   42] Training loss: 0.03955809, Validation loss: 0.03412607, Gradient norm: 0.41093608
INFO:root:[   43] Training loss: 0.04101566, Validation loss: 0.03359330, Gradient norm: 0.43496125
INFO:root:[   44] Training loss: 0.03972186, Validation loss: 0.03311925, Gradient norm: 0.44351505
INFO:root:[   45] Training loss: 0.03959443, Validation loss: 0.03238449, Gradient norm: 0.38810417
INFO:root:[   46] Training loss: 0.03824227, Validation loss: 0.03363958, Gradient norm: 0.37253459
INFO:root:[   47] Training loss: 0.03916169, Validation loss: 0.03297375, Gradient norm: 0.39040960
INFO:root:[   48] Training loss: 0.03947241, Validation loss: 0.03268050, Gradient norm: 0.37889750
INFO:root:[   49] Training loss: 0.03841626, Validation loss: 0.03622442, Gradient norm: 0.35875993
INFO:root:[   50] Training loss: 0.03911848, Validation loss: 0.03164714, Gradient norm: 0.40905194
INFO:root:[   51] Training loss: 0.03865883, Validation loss: 0.03167186, Gradient norm: 0.41735968
INFO:root:[   52] Training loss: 0.03903024, Validation loss: 0.03160216, Gradient norm: 0.46336627
INFO:root:[   53] Training loss: 0.03762808, Validation loss: 0.03264218, Gradient norm: 0.33414251
INFO:root:[   54] Training loss: 0.03741928, Validation loss: 0.03308654, Gradient norm: 0.36903802
INFO:root:[   55] Training loss: 0.03710981, Validation loss: 0.03214199, Gradient norm: 0.32266805
INFO:root:[   56] Training loss: 0.03745843, Validation loss: 0.03129405, Gradient norm: 0.33141160
INFO:root:[   57] Training loss: 0.03751093, Validation loss: 0.03285947, Gradient norm: 0.36420956
INFO:root:[   58] Training loss: 0.03731020, Validation loss: 0.03187432, Gradient norm: 0.40357685
INFO:root:[   59] Training loss: 0.03698720, Validation loss: 0.03038320, Gradient norm: 0.34619626
INFO:root:[   60] Training loss: 0.03637553, Validation loss: 0.03102672, Gradient norm: 0.30435432
INFO:root:[   61] Training loss: 0.03698798, Validation loss: 0.03071560, Gradient norm: 0.41693777
INFO:root:[   62] Training loss: 0.03638426, Validation loss: 0.03057049, Gradient norm: 0.35964743
INFO:root:[   63] Training loss: 0.03646251, Validation loss: 0.03167853, Gradient norm: 0.33002540
INFO:root:[   64] Training loss: 0.03593345, Validation loss: 0.03098025, Gradient norm: 0.31036293
INFO:root:[   65] Training loss: 0.03633776, Validation loss: 0.03006124, Gradient norm: 0.37186310
INFO:root:[   66] Training loss: 0.03689637, Validation loss: 0.03005495, Gradient norm: 0.48705394
INFO:root:[   67] Training loss: 0.03673224, Validation loss: 0.02956245, Gradient norm: 0.44786088
INFO:root:[   68] Training loss: 0.03546974, Validation loss: 0.03068481, Gradient norm: 0.31889430
INFO:root:[   69] Training loss: 0.03555086, Validation loss: 0.02989600, Gradient norm: 0.30322387
INFO:root:[   70] Training loss: 0.03604421, Validation loss: 0.02979597, Gradient norm: 0.34977616
INFO:root:[   71] Training loss: 0.03477983, Validation loss: 0.02962400, Gradient norm: 0.35738307
INFO:root:[   72] Training loss: 0.03639501, Validation loss: 0.02923632, Gradient norm: 0.39577426
INFO:root:[   73] Training loss: 0.03516636, Validation loss: 0.02965092, Gradient norm: 0.35261786
INFO:root:[   74] Training loss: 0.03512628, Validation loss: 0.02982761, Gradient norm: 0.36094241
INFO:root:[   75] Training loss: 0.03524891, Validation loss: 0.03128546, Gradient norm: 0.34989745
INFO:root:[   76] Training loss: 0.03535088, Validation loss: 0.03078540, Gradient norm: 0.48437073
INFO:root:[   77] Training loss: 0.03593358, Validation loss: 0.03075315, Gradient norm: 0.50904494
INFO:root:[   78] Training loss: 0.03456526, Validation loss: 0.02869823, Gradient norm: 0.26941314
INFO:root:[   79] Training loss: 0.03438740, Validation loss: 0.02865333, Gradient norm: 0.27923307
INFO:root:[   80] Training loss: 0.03440971, Validation loss: 0.02854683, Gradient norm: 0.29857132
INFO:root:[   81] Training loss: 0.03398418, Validation loss: 0.02805369, Gradient norm: 0.28339632
INFO:root:[   82] Training loss: 0.03421505, Validation loss: 0.02831959, Gradient norm: 0.36961129
INFO:root:[   83] Training loss: 0.03395872, Validation loss: 0.02809426, Gradient norm: 0.31146576
INFO:root:[   84] Training loss: 0.03464358, Validation loss: 0.02791036, Gradient norm: 0.46814559
INFO:root:[   85] Training loss: 0.03418538, Validation loss: 0.03212827, Gradient norm: 0.43062083
INFO:root:[   86] Training loss: 0.03461988, Validation loss: 0.02761859, Gradient norm: 0.49550194
INFO:root:[   87] Training loss: 0.03340759, Validation loss: 0.02829939, Gradient norm: 0.30847819
INFO:root:[   88] Training loss: 0.03347604, Validation loss: 0.02795519, Gradient norm: 0.31225902
INFO:root:[   89] Training loss: 0.03333445, Validation loss: 0.02952543, Gradient norm: 0.31539032
INFO:root:[   90] Training loss: 0.03375787, Validation loss: 0.02847226, Gradient norm: 0.47991013
INFO:root:[   91] Training loss: 0.03302744, Validation loss: 0.02840351, Gradient norm: 0.32064677
INFO:root:[   92] Training loss: 0.03338444, Validation loss: 0.02737886, Gradient norm: 0.42516529
INFO:root:[   93] Training loss: 0.03265749, Validation loss: 0.02806015, Gradient norm: 0.30665614
INFO:root:[   94] Training loss: 0.03245038, Validation loss: 0.02727203, Gradient norm: 0.34957291
INFO:root:[   95] Training loss: 0.03376883, Validation loss: 0.02769152, Gradient norm: 0.34718796
INFO:root:[   96] Training loss: 0.03216077, Validation loss: 0.02802858, Gradient norm: 0.30663904
INFO:root:[   97] Training loss: 0.03301845, Validation loss: 0.02886095, Gradient norm: 0.45431909
INFO:root:[   98] Training loss: 0.03306758, Validation loss: 0.02662761, Gradient norm: 0.34735174
INFO:root:[   99] Training loss: 0.03204326, Validation loss: 0.02691651, Gradient norm: 0.28911973
INFO:root:[  100] Training loss: 0.03178755, Validation loss: 0.02762497, Gradient norm: 0.30441921
INFO:root:[  101] Training loss: 0.03267642, Validation loss: 0.02720390, Gradient norm: 0.34434619
INFO:root:[  102] Training loss: 0.03255238, Validation loss: 0.02854286, Gradient norm: 0.42510384
INFO:root:[  103] Training loss: 0.03220725, Validation loss: 0.02757246, Gradient norm: 0.43167962
INFO:root:[  104] Training loss: 0.03207182, Validation loss: 0.02694965, Gradient norm: 0.32064632
INFO:root:[  105] Training loss: 0.03198390, Validation loss: 0.02742165, Gradient norm: 0.35437077
INFO:root:[  106] Training loss: 0.03180972, Validation loss: 0.02698674, Gradient norm: 0.34154531
INFO:root:[  107] Training loss: 0.03199143, Validation loss: 0.02814563, Gradient norm: 0.31557331
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 208.848s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.83521
INFO:root:EnergyScoreTrain: 0.70191
INFO:root:CoverageTrain: 0.28423
INFO:root:IntervalWidthTrain: 0.01275
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.85891
INFO:root:EnergyScoreValidation: 0.7198
INFO:root:CoverageValidation: 0.27693
INFO:root:IntervalWidthValidation: 0.01294
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.64104
INFO:root:EnergyScoreTest: 2.40542
INFO:root:CoverageTest: 0.03674
INFO:root:IntervalWidthTest: 0.01884
INFO:root:###10 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 251658240
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.10533272, Validation loss: 0.08598569, Gradient norm: 0.49885272
INFO:root:[    2] Training loss: 0.08447936, Validation loss: 0.06670729, Gradient norm: 0.32585549
INFO:root:[    3] Training loss: 0.07594713, Validation loss: 0.06579085, Gradient norm: 0.39934799
INFO:root:[    4] Training loss: 0.07101464, Validation loss: 0.05786036, Gradient norm: 0.30529222
INFO:root:[    5] Training loss: 0.06486174, Validation loss: 0.05124731, Gradient norm: 0.35573984
INFO:root:[    6] Training loss: 0.06252974, Validation loss: 0.04689530, Gradient norm: 0.28719878
INFO:root:[    7] Training loss: 0.05940037, Validation loss: 0.04708493, Gradient norm: 0.33496611
INFO:root:[    8] Training loss: 0.05622497, Validation loss: 0.04689724, Gradient norm: 0.31324263
INFO:root:[    9] Training loss: 0.05523003, Validation loss: 0.04302745, Gradient norm: 0.28086059
INFO:root:[   10] Training loss: 0.05377767, Validation loss: 0.04188443, Gradient norm: 0.28657562
INFO:root:[   11] Training loss: 0.05349022, Validation loss: 0.04507628, Gradient norm: 0.41741179
INFO:root:[   12] Training loss: 0.05184397, Validation loss: 0.04176330, Gradient norm: 0.27599954
INFO:root:[   13] Training loss: 0.05210409, Validation loss: 0.04092499, Gradient norm: 0.30636228
INFO:root:[   14] Training loss: 0.05133018, Validation loss: 0.04044963, Gradient norm: 0.27911069
INFO:root:[   15] Training loss: 0.04982636, Validation loss: 0.04097418, Gradient norm: 0.25120169
INFO:root:[   16] Training loss: 0.05007432, Validation loss: 0.04024631, Gradient norm: 0.30705575
INFO:root:[   17] Training loss: 0.04972979, Validation loss: 0.04351661, Gradient norm: 0.29866344
INFO:root:[   18] Training loss: 0.04840184, Validation loss: 0.04105368, Gradient norm: 0.25514830
INFO:root:[   19] Training loss: 0.04871796, Validation loss: 0.03899435, Gradient norm: 0.29898635
INFO:root:[   20] Training loss: 0.04748413, Validation loss: 0.03946744, Gradient norm: 0.22319065
INFO:root:[   21] Training loss: 0.04875292, Validation loss: 0.03992612, Gradient norm: 0.32379291
INFO:root:[   22] Training loss: 0.04688638, Validation loss: 0.04060399, Gradient norm: 0.27745481
INFO:root:[   23] Training loss: 0.04824393, Validation loss: 0.03944319, Gradient norm: 0.30547121
INFO:root:[   24] Training loss: 0.04620479, Validation loss: 0.03901025, Gradient norm: 0.30005788
INFO:root:[   25] Training loss: 0.04603706, Validation loss: 0.04000266, Gradient norm: 0.25703572
INFO:root:[   26] Training loss: 0.04590010, Validation loss: 0.04046790, Gradient norm: 0.28206683
INFO:root:[   27] Training loss: 0.04494596, Validation loss: 0.04204106, Gradient norm: 0.32753533
INFO:root:[   28] Training loss: 0.04621563, Validation loss: 0.04023230, Gradient norm: 0.34293155
INFO:root:[   29] Training loss: 0.04448160, Validation loss: 0.03619812, Gradient norm: 0.32026522
INFO:root:[   30] Training loss: 0.04446790, Validation loss: 0.03835145, Gradient norm: 0.30758212
INFO:root:[   31] Training loss: 0.04318441, Validation loss: 0.03754950, Gradient norm: 0.21009715
INFO:root:[   32] Training loss: 0.04420970, Validation loss: 0.03617152, Gradient norm: 0.31818485
INFO:root:[   33] Training loss: 0.04319021, Validation loss: 0.03570601, Gradient norm: 0.37488383
INFO:root:[   34] Training loss: 0.04342586, Validation loss: 0.03608934, Gradient norm: 0.31109062
INFO:root:[   35] Training loss: 0.04257239, Validation loss: 0.03490533, Gradient norm: 0.26951673
INFO:root:[   36] Training loss: 0.04276650, Validation loss: 0.03645884, Gradient norm: 0.32537397
INFO:root:[   37] Training loss: 0.04192162, Validation loss: 0.03449539, Gradient norm: 0.22218495
INFO:root:[   38] Training loss: 0.04251238, Validation loss: 0.03460648, Gradient norm: 0.33308415
INFO:root:[   39] Training loss: 0.04198430, Validation loss: 0.03477346, Gradient norm: 0.38161498
INFO:root:[   40] Training loss: 0.04217621, Validation loss: 0.03862726, Gradient norm: 0.40430722
INFO:root:[   41] Training loss: 0.04213823, Validation loss: 0.03510669, Gradient norm: 0.41306259
INFO:root:[   42] Training loss: 0.04126847, Validation loss: 0.03624068, Gradient norm: 0.39644400
INFO:root:[   43] Training loss: 0.04119976, Validation loss: 0.03527233, Gradient norm: 0.28893204
INFO:root:[   44] Training loss: 0.04033548, Validation loss: 0.03441185, Gradient norm: 0.32467902
INFO:root:[   45] Training loss: 0.04119564, Validation loss: 0.03250997, Gradient norm: 0.38330458
INFO:root:[   46] Training loss: 0.03928998, Validation loss: 0.03237068, Gradient norm: 0.30088299
INFO:root:[   47] Training loss: 0.03973659, Validation loss: 0.03305698, Gradient norm: 0.28956133
INFO:root:[   48] Training loss: 0.03960120, Validation loss: 0.03385699, Gradient norm: 0.36749135
INFO:root:[   49] Training loss: 0.03946986, Validation loss: 0.03327682, Gradient norm: 0.28277644
INFO:root:[   50] Training loss: 0.04026287, Validation loss: 0.03236858, Gradient norm: 0.35424853
INFO:root:[   51] Training loss: 0.03939490, Validation loss: 0.03313423, Gradient norm: 0.33789561
INFO:root:[   52] Training loss: 0.03883009, Validation loss: 0.03156426, Gradient norm: 0.33212827
INFO:root:[   53] Training loss: 0.03850485, Validation loss: 0.03245413, Gradient norm: 0.29373612
INFO:root:[   54] Training loss: 0.03845874, Validation loss: 0.03137542, Gradient norm: 0.29942356
INFO:root:[   55] Training loss: 0.03946712, Validation loss: 0.03225489, Gradient norm: 0.42992774
INFO:root:[   56] Training loss: 0.03845266, Validation loss: 0.03239356, Gradient norm: 0.28853568
INFO:root:[   57] Training loss: 0.03834882, Validation loss: 0.03240372, Gradient norm: 0.30982110
INFO:root:[   58] Training loss: 0.03874901, Validation loss: 0.03101867, Gradient norm: 0.31409084
INFO:root:[   59] Training loss: 0.03778272, Validation loss: 0.03203062, Gradient norm: 0.33880837
INFO:root:[   60] Training loss: 0.03854188, Validation loss: 0.03124550, Gradient norm: 0.39168255
INFO:root:[   61] Training loss: 0.03817604, Validation loss: 0.03329365, Gradient norm: 0.30425542
INFO:root:[   62] Training loss: 0.03804905, Validation loss: 0.03298822, Gradient norm: 0.28813236
INFO:root:[   63] Training loss: 0.03691786, Validation loss: 0.03167691, Gradient norm: 0.31014364
INFO:root:[   64] Training loss: 0.03795118, Validation loss: 0.03120223, Gradient norm: 0.29938679
INFO:root:[   65] Training loss: 0.03770633, Validation loss: 0.02975771, Gradient norm: 0.37218511
INFO:root:[   66] Training loss: 0.03679245, Validation loss: 0.02997077, Gradient norm: 0.35938998
INFO:root:[   67] Training loss: 0.03723696, Validation loss: 0.03072969, Gradient norm: 0.39620171
INFO:root:[   68] Training loss: 0.03673591, Validation loss: 0.03020718, Gradient norm: 0.29674886
INFO:root:[   69] Training loss: 0.03685399, Validation loss: 0.02989335, Gradient norm: 0.34961200
INFO:root:[   70] Training loss: 0.03703546, Validation loss: 0.02945183, Gradient norm: 0.31928716
INFO:root:[   71] Training loss: 0.03762559, Validation loss: 0.02972553, Gradient norm: 0.35559271
INFO:root:[   72] Training loss: 0.03717785, Validation loss: 0.02982714, Gradient norm: 0.40711121
INFO:root:[   73] Training loss: 0.03620837, Validation loss: 0.02997596, Gradient norm: 0.27856469
INFO:root:[   74] Training loss: 0.03581443, Validation loss: 0.03078447, Gradient norm: 0.31624112
INFO:root:[   75] Training loss: 0.03645488, Validation loss: 0.03018676, Gradient norm: 0.34013567
INFO:root:[   76] Training loss: 0.03549085, Validation loss: 0.03019573, Gradient norm: 0.29480073
INFO:root:[   77] Training loss: 0.03621270, Validation loss: 0.02987144, Gradient norm: 0.27504630
INFO:root:[   78] Training loss: 0.03567963, Validation loss: 0.03317512, Gradient norm: 0.33382295
INFO:root:[   79] Training loss: 0.03530067, Validation loss: 0.03230963, Gradient norm: 0.28321402
INFO:root:[   80] Training loss: 0.03557162, Validation loss: 0.02898386, Gradient norm: 0.35065251
INFO:root:[   81] Training loss: 0.03602524, Validation loss: 0.02848003, Gradient norm: 0.37760703
INFO:root:[   82] Training loss: 0.03500287, Validation loss: 0.02931562, Gradient norm: 0.32891970
INFO:root:[   83] Training loss: 0.03664498, Validation loss: 0.02904229, Gradient norm: 0.32796366
INFO:root:[   84] Training loss: 0.03547499, Validation loss: 0.02845135, Gradient norm: 0.36465531
INFO:root:[   85] Training loss: 0.03529606, Validation loss: 0.03253307, Gradient norm: 0.32017857
INFO:root:[   86] Training loss: 0.03480726, Validation loss: 0.03057623, Gradient norm: 0.33948969
INFO:root:[   87] Training loss: 0.03528472, Validation loss: 0.02950235, Gradient norm: 0.32339780
INFO:root:[   88] Training loss: 0.03522898, Validation loss: 0.03215240, Gradient norm: 0.32924035
INFO:root:[   89] Training loss: 0.03534143, Validation loss: 0.02881470, Gradient norm: 0.37740218
INFO:root:[   90] Training loss: 0.03404608, Validation loss: 0.02897629, Gradient norm: 0.29293543
INFO:root:[   91] Training loss: 0.03423572, Validation loss: 0.02784168, Gradient norm: 0.31364081
INFO:root:[   92] Training loss: 0.03530117, Validation loss: 0.02763973, Gradient norm: 0.35995362
INFO:root:[   93] Training loss: 0.03452825, Validation loss: 0.03045217, Gradient norm: 0.31873482
INFO:root:[   94] Training loss: 0.03414034, Validation loss: 0.02880077, Gradient norm: 0.30008747
INFO:root:[   95] Training loss: 0.03417342, Validation loss: 0.02766030, Gradient norm: 0.31744649
INFO:root:[   96] Training loss: 0.03440550, Validation loss: 0.02802065, Gradient norm: 0.36612845
INFO:root:[   97] Training loss: 0.03349357, Validation loss: 0.02833174, Gradient norm: 0.26758218
INFO:root:[   98] Training loss: 0.03444814, Validation loss: 0.02898161, Gradient norm: 0.36566080
INFO:root:[   99] Training loss: 0.03416335, Validation loss: 0.03034104, Gradient norm: 0.33778217
INFO:root:[  100] Training loss: 0.03484042, Validation loss: 0.02937731, Gradient norm: 0.38272180
INFO:root:[  101] Training loss: 0.03431036, Validation loss: 0.02720533, Gradient norm: 0.36416717
INFO:root:[  102] Training loss: 0.03357451, Validation loss: 0.02780768, Gradient norm: 0.29316970
INFO:root:[  103] Training loss: 0.03330953, Validation loss: 0.02900365, Gradient norm: 0.28941412
INFO:root:[  104] Training loss: 0.03309753, Validation loss: 0.03142619, Gradient norm: 0.30325838
INFO:root:[  105] Training loss: 0.03318793, Validation loss: 0.02885665, Gradient norm: 0.33021655
INFO:root:[  106] Training loss: 0.03275289, Validation loss: 0.02757168, Gradient norm: 0.23627685
INFO:root:[  107] Training loss: 0.03292610, Validation loss: 0.02751214, Gradient norm: 0.27642983
INFO:root:[  108] Training loss: 0.03383194, Validation loss: 0.02768673, Gradient norm: 0.36435538
INFO:root:[  109] Training loss: 0.03350208, Validation loss: 0.02995637, Gradient norm: 0.38279696
INFO:root:[  110] Training loss: 0.03302215, Validation loss: 0.02761563, Gradient norm: 0.38108708
INFO:root:EP 110: Early stopping
INFO:root:Training the model took 214.286s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.89452
INFO:root:EnergyScoreTrain: 0.67501
INFO:root:CoverageTrain: 0.40542
INFO:root:IntervalWidthTrain: 0.0251
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91074
INFO:root:EnergyScoreValidation: 0.69274
INFO:root:CoverageValidation: 0.39025
INFO:root:IntervalWidthValidation: 0.02395
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.18314
INFO:root:EnergyScoreTest: 0.93168
INFO:root:CoverageTest: 0.29655
INFO:root:IntervalWidthTest: 0.02498
INFO:root:###11 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 211812352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.12076053, Validation loss: 0.08940664, Gradient norm: 0.46912529
INFO:root:[    2] Training loss: 0.09570428, Validation loss: 0.07345978, Gradient norm: 0.25630801
INFO:root:[    3] Training loss: 0.08297615, Validation loss: 0.07288269, Gradient norm: 0.22594566
INFO:root:[    4] Training loss: 0.07856792, Validation loss: 0.05845321, Gradient norm: 0.27575937
INFO:root:[    5] Training loss: 0.07102861, Validation loss: 0.05345778, Gradient norm: 0.28075770
INFO:root:[    6] Training loss: 0.06854412, Validation loss: 0.05281695, Gradient norm: 0.29060636
INFO:root:[    7] Training loss: 0.06663392, Validation loss: 0.05916995, Gradient norm: 0.31637935
INFO:root:[    8] Training loss: 0.06307260, Validation loss: 0.05062075, Gradient norm: 0.28414350
INFO:root:[    9] Training loss: 0.06238552, Validation loss: 0.05735868, Gradient norm: 0.32762514
INFO:root:[   10] Training loss: 0.06093460, Validation loss: 0.05041037, Gradient norm: 0.25923453
INFO:root:[   11] Training loss: 0.05940799, Validation loss: 0.04757826, Gradient norm: 0.27998667
INFO:root:[   12] Training loss: 0.05925347, Validation loss: 0.04275754, Gradient norm: 0.38461670
INFO:root:[   13] Training loss: 0.05846509, Validation loss: 0.04762239, Gradient norm: 0.38131762
INFO:root:[   14] Training loss: 0.05645502, Validation loss: 0.04893383, Gradient norm: 0.34014881
INFO:root:[   15] Training loss: 0.05693636, Validation loss: 0.04207218, Gradient norm: 0.39693919
INFO:root:[   16] Training loss: 0.05545475, Validation loss: 0.04462045, Gradient norm: 0.36262061
INFO:root:[   17] Training loss: 0.05478145, Validation loss: 0.04548081, Gradient norm: 0.25274295
INFO:root:[   18] Training loss: 0.05389475, Validation loss: 0.05014752, Gradient norm: 0.27610728
INFO:root:[   19] Training loss: 0.05340534, Validation loss: 0.04515287, Gradient norm: 0.25079707
INFO:root:[   20] Training loss: 0.05299123, Validation loss: 0.05349232, Gradient norm: 0.30751447
INFO:root:[   21] Training loss: 0.05309634, Validation loss: 0.05173246, Gradient norm: 0.46173208
INFO:root:[   22] Training loss: 0.05238036, Validation loss: 0.04784352, Gradient norm: 0.39976131
INFO:root:[   23] Training loss: 0.05083075, Validation loss: 0.04420054, Gradient norm: 0.25476070
INFO:root:[   24] Training loss: 0.05184641, Validation loss: 0.04334516, Gradient norm: 0.37011392
INFO:root:[   25] Training loss: 0.05032491, Validation loss: 0.04332305, Gradient norm: 0.35782553
INFO:root:[   26] Training loss: 0.05032971, Validation loss: 0.04290792, Gradient norm: 0.32283230
INFO:root:[   27] Training loss: 0.05002061, Validation loss: 0.04394036, Gradient norm: 0.33051828
INFO:root:[   28] Training loss: 0.04935034, Validation loss: 0.04071517, Gradient norm: 0.37254196
INFO:root:[   29] Training loss: 0.04909783, Validation loss: 0.04097538, Gradient norm: 0.31806622
INFO:root:[   30] Training loss: 0.04908816, Validation loss: 0.04244571, Gradient norm: 0.34205562
INFO:root:[   31] Training loss: 0.04852838, Validation loss: 0.03930496, Gradient norm: 0.29784177
INFO:root:[   32] Training loss: 0.04834938, Validation loss: 0.04683036, Gradient norm: 0.33541490
INFO:root:[   33] Training loss: 0.04880232, Validation loss: 0.04326469, Gradient norm: 0.38151350
INFO:root:[   34] Training loss: 0.04706922, Validation loss: 0.04414286, Gradient norm: 0.32462134
INFO:root:[   35] Training loss: 0.04703509, Validation loss: 0.04945366, Gradient norm: 0.41403669
INFO:root:[   36] Training loss: 0.04731288, Validation loss: 0.04226174, Gradient norm: 0.37710312
INFO:root:[   37] Training loss: 0.04674524, Validation loss: 0.04295034, Gradient norm: 0.35304073
INFO:root:[   38] Training loss: 0.04610057, Validation loss: 0.04379173, Gradient norm: 0.32154250
INFO:root:[   39] Training loss: 0.04680363, Validation loss: 0.04052728, Gradient norm: 0.33573643
INFO:root:[   40] Training loss: 0.04615213, Validation loss: 0.03940383, Gradient norm: 0.32884391
INFO:root:[   41] Training loss: 0.04609339, Validation loss: 0.03862307, Gradient norm: 0.41904226
INFO:root:[   42] Training loss: 0.04541885, Validation loss: 0.03824834, Gradient norm: 0.27439178
INFO:root:[   43] Training loss: 0.04682906, Validation loss: 0.03971688, Gradient norm: 0.42659458
INFO:root:[   44] Training loss: 0.04482544, Validation loss: 0.03751232, Gradient norm: 0.36372047
INFO:root:[   45] Training loss: 0.04548369, Validation loss: 0.03817937, Gradient norm: 0.38727342
INFO:root:[   46] Training loss: 0.04416954, Validation loss: 0.04304443, Gradient norm: 0.34197592
INFO:root:[   47] Training loss: 0.04433140, Validation loss: 0.03834416, Gradient norm: 0.34547396
INFO:root:[   48] Training loss: 0.04381470, Validation loss: 0.04076551, Gradient norm: 0.34682645
INFO:root:[   49] Training loss: 0.04512086, Validation loss: 0.04560924, Gradient norm: 0.34122378
INFO:root:[   50] Training loss: 0.04399243, Validation loss: 0.03860949, Gradient norm: 0.31797035
INFO:root:[   51] Training loss: 0.04359107, Validation loss: 0.04115865, Gradient norm: 0.30209467
INFO:root:[   52] Training loss: 0.04395738, Validation loss: 0.03876436, Gradient norm: 0.30259713
INFO:root:[   53] Training loss: 0.04343508, Validation loss: 0.03996227, Gradient norm: 0.31114767
INFO:root:[   54] Training loss: 0.04357628, Validation loss: 0.03777696, Gradient norm: 0.30384259
INFO:root:[   55] Training loss: 0.04386983, Validation loss: 0.04189075, Gradient norm: 0.34011580
INFO:root:[   56] Training loss: 0.04278820, Validation loss: 0.04422502, Gradient norm: 0.31788978
INFO:root:[   57] Training loss: 0.04266386, Validation loss: 0.03975333, Gradient norm: 0.34174985
INFO:root:[   58] Training loss: 0.04286169, Validation loss: 0.03882594, Gradient norm: 0.27490148
INFO:root:[   59] Training loss: 0.04271256, Validation loss: 0.04022158, Gradient norm: 0.29858263
INFO:root:[   60] Training loss: 0.04189949, Validation loss: 0.03823857, Gradient norm: 0.27891571
INFO:root:[   61] Training loss: 0.04321777, Validation loss: 0.04162739, Gradient norm: 0.39275238
INFO:root:[   62] Training loss: 0.04275237, Validation loss: 0.03966086, Gradient norm: 0.33375949
INFO:root:[   63] Training loss: 0.04251296, Validation loss: 0.04517286, Gradient norm: 0.34685497
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 123.779s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.22909
INFO:root:EnergyScoreTrain: 0.94178
INFO:root:CoverageTrain: 0.38955
INFO:root:IntervalWidthTrain: 0.03196
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.27155
INFO:root:EnergyScoreValidation: 0.97133
INFO:root:CoverageValidation: 0.37922
INFO:root:IntervalWidthValidation: 0.03259
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.68243
INFO:root:EnergyScoreTest: 1.26806
INFO:root:CoverageTest: 0.31381
INFO:root:IntervalWidthTest: 0.04358
INFO:root:###12 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 251658240
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.13591153, Validation loss: 0.08760479, Gradient norm: 0.44616457
INFO:root:[    2] Training loss: 0.10500865, Validation loss: 0.07114754, Gradient norm: 0.33900242
INFO:root:[    3] Training loss: 0.09434691, Validation loss: 0.06532428, Gradient norm: 0.32214112
INFO:root:[    4] Training loss: 0.08582846, Validation loss: 0.06278597, Gradient norm: 0.30277540
INFO:root:[    5] Training loss: 0.08257818, Validation loss: 0.06186393, Gradient norm: 0.36867052
INFO:root:[    6] Training loss: 0.07659470, Validation loss: 0.05567448, Gradient norm: 0.27502368
INFO:root:[    7] Training loss: 0.07462369, Validation loss: 0.05442648, Gradient norm: 0.46247252
INFO:root:[    8] Training loss: 0.07126703, Validation loss: 0.04844481, Gradient norm: 0.34871011
INFO:root:[    9] Training loss: 0.06968118, Validation loss: 0.05147213, Gradient norm: 0.43208455
INFO:root:[   10] Training loss: 0.06804456, Validation loss: 0.04708593, Gradient norm: 0.33178190
INFO:root:[   11] Training loss: 0.06618886, Validation loss: 0.04791673, Gradient norm: 0.23922209
INFO:root:[   12] Training loss: 0.06538338, Validation loss: 0.04723076, Gradient norm: 0.33257909
INFO:root:[   13] Training loss: 0.06294782, Validation loss: 0.04795064, Gradient norm: 0.29171575
INFO:root:[   14] Training loss: 0.06293894, Validation loss: 0.04705562, Gradient norm: 0.41481082
INFO:root:[   15] Training loss: 0.06208352, Validation loss: 0.04422703, Gradient norm: 0.32469120
INFO:root:[   16] Training loss: 0.06131944, Validation loss: 0.04431378, Gradient norm: 0.29344645
INFO:root:[   17] Training loss: 0.06051381, Validation loss: 0.04692566, Gradient norm: 0.23805150
INFO:root:[   18] Training loss: 0.05962822, Validation loss: 0.04468146, Gradient norm: 0.31992325
INFO:root:[   19] Training loss: 0.05922803, Validation loss: 0.04291632, Gradient norm: 0.34843110
INFO:root:[   20] Training loss: 0.05768701, Validation loss: 0.04469646, Gradient norm: 0.27257147
INFO:root:[   21] Training loss: 0.05807485, Validation loss: 0.04274871, Gradient norm: 0.26069436
INFO:root:[   22] Training loss: 0.05841909, Validation loss: 0.04426945, Gradient norm: 0.26032418
INFO:root:[   23] Training loss: 0.05548064, Validation loss: 0.04261794, Gradient norm: 0.22081524
INFO:root:[   24] Training loss: 0.05563704, Validation loss: 0.04382694, Gradient norm: 0.22825875
INFO:root:[   25] Training loss: 0.05471551, Validation loss: 0.04319652, Gradient norm: 0.26140896
INFO:root:[   26] Training loss: 0.05553197, Validation loss: 0.04686975, Gradient norm: 0.34113656
INFO:root:[   27] Training loss: 0.05583727, Validation loss: 0.04159298, Gradient norm: 0.37317170
INFO:root:[   28] Training loss: 0.05444861, Validation loss: 0.04146947, Gradient norm: 0.35274630
INFO:root:[   29] Training loss: 0.05330005, Validation loss: 0.04219024, Gradient norm: 0.29985244
INFO:root:[   30] Training loss: 0.05327204, Validation loss: 0.04072858, Gradient norm: 0.29015055
INFO:root:[   31] Training loss: 0.05258115, Validation loss: 0.04170638, Gradient norm: 0.27413424
INFO:root:[   32] Training loss: 0.05272166, Validation loss: 0.04278582, Gradient norm: 0.35463329
INFO:root:[   33] Training loss: 0.05257090, Validation loss: 0.04129067, Gradient norm: 0.31878516
INFO:root:[   34] Training loss: 0.05193785, Validation loss: 0.03976517, Gradient norm: 0.34471102
INFO:root:[   35] Training loss: 0.05219264, Validation loss: 0.03988401, Gradient norm: 0.30991351
INFO:root:[   36] Training loss: 0.05130583, Validation loss: 0.04154443, Gradient norm: 0.42319826
INFO:root:[   37] Training loss: 0.05107898, Validation loss: 0.03879878, Gradient norm: 0.30306862
INFO:root:[   38] Training loss: 0.05032071, Validation loss: 0.03925455, Gradient norm: 0.27109992
INFO:root:[   39] Training loss: 0.05045101, Validation loss: 0.04194827, Gradient norm: 0.27644200
INFO:root:[   40] Training loss: 0.05028596, Validation loss: 0.04005931, Gradient norm: 0.37114394
INFO:root:[   41] Training loss: 0.04958648, Validation loss: 0.03823131, Gradient norm: 0.28986695
INFO:root:[   42] Training loss: 0.04996131, Validation loss: 0.03877151, Gradient norm: 0.28923586
INFO:root:[   43] Training loss: 0.04848855, Validation loss: 0.03737280, Gradient norm: 0.25961519
INFO:root:[   44] Training loss: 0.04871177, Validation loss: 0.03784219, Gradient norm: 0.31399988
INFO:root:[   45] Training loss: 0.04942772, Validation loss: 0.03868445, Gradient norm: 0.26640601
INFO:root:[   46] Training loss: 0.04805100, Validation loss: 0.03812064, Gradient norm: 0.29803254
INFO:root:[   47] Training loss: 0.04746796, Validation loss: 0.03930866, Gradient norm: 0.30129817
INFO:root:[   48] Training loss: 0.04704070, Validation loss: 0.03673033, Gradient norm: 0.35988178
INFO:root:[   49] Training loss: 0.04810014, Validation loss: 0.03753526, Gradient norm: 0.41940535
INFO:root:[   50] Training loss: 0.04678411, Validation loss: 0.03702186, Gradient norm: 0.31038508
INFO:root:[   51] Training loss: 0.04630125, Validation loss: 0.03771678, Gradient norm: 0.33197645
INFO:root:[   52] Training loss: 0.04593355, Validation loss: 0.03922603, Gradient norm: 0.29628128
INFO:root:[   53] Training loss: 0.04705814, Validation loss: 0.03763083, Gradient norm: 0.39338845
INFO:root:[   54] Training loss: 0.04616252, Validation loss: 0.03669828, Gradient norm: 0.28430022
INFO:root:[   55] Training loss: 0.04498476, Validation loss: 0.03517306, Gradient norm: 0.32277837
INFO:root:[   56] Training loss: 0.04600260, Validation loss: 0.03751630, Gradient norm: 0.33261493
INFO:root:[   57] Training loss: 0.04532038, Validation loss: 0.03564790, Gradient norm: 0.32709148
INFO:root:[   58] Training loss: 0.04515566, Validation loss: 0.04028995, Gradient norm: 0.39917125
INFO:root:[   59] Training loss: 0.04547246, Validation loss: 0.03483006, Gradient norm: 0.41062775
INFO:root:[   60] Training loss: 0.04431456, Validation loss: 0.03654750, Gradient norm: 0.35257338
INFO:root:[   61] Training loss: 0.04476367, Validation loss: 0.03658956, Gradient norm: 0.24716096
INFO:root:[   62] Training loss: 0.04405471, Validation loss: 0.03690988, Gradient norm: 0.33789947
INFO:root:[   63] Training loss: 0.04383928, Validation loss: 0.03519511, Gradient norm: 0.33714072
INFO:root:[   64] Training loss: 0.04365887, Validation loss: 0.03632088, Gradient norm: 0.29926872
INFO:root:[   65] Training loss: 0.04374236, Validation loss: 0.03437448, Gradient norm: 0.30888820
INFO:root:[   66] Training loss: 0.04379481, Validation loss: 0.03668128, Gradient norm: 0.31971195
INFO:root:[   67] Training loss: 0.04311925, Validation loss: 0.03820285, Gradient norm: 0.25807551
INFO:root:[   68] Training loss: 0.04420597, Validation loss: 0.03389609, Gradient norm: 0.30213717
INFO:root:[   69] Training loss: 0.04307702, Validation loss: 0.03485588, Gradient norm: 0.35794028
INFO:root:[   70] Training loss: 0.04243823, Validation loss: 0.03591597, Gradient norm: 0.31305692
INFO:root:[   71] Training loss: 0.04359004, Validation loss: 0.03630538, Gradient norm: 0.31003800
INFO:root:[   72] Training loss: 0.04340802, Validation loss: 0.03573660, Gradient norm: 0.38678866
INFO:root:[   73] Training loss: 0.04230640, Validation loss: 0.03543191, Gradient norm: 0.27094458
INFO:root:[   74] Training loss: 0.04291386, Validation loss: 0.03555909, Gradient norm: 0.27887902
INFO:root:[   75] Training loss: 0.04264987, Validation loss: 0.03582576, Gradient norm: 0.40363231
INFO:root:[   76] Training loss: 0.04309145, Validation loss: 0.03517276, Gradient norm: 0.34585007
INFO:root:[   77] Training loss: 0.04219859, Validation loss: 0.03435816, Gradient norm: 0.34082308
INFO:root:EP 77: Early stopping
INFO:root:Training the model took 150.779s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.19167
INFO:root:EnergyScoreTrain: 0.82193
INFO:root:CoverageTrain: 0.48978
INFO:root:IntervalWidthTrain: 0.04538
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.20136
INFO:root:EnergyScoreValidation: 0.80651
INFO:root:CoverageValidation: 0.49972
INFO:root:IntervalWidthValidation: 0.04786
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.49069
INFO:root:EnergyScoreTest: 1.04098
INFO:root:CoverageTest: 0.40279
INFO:root:IntervalWidthTest: 0.0508
INFO:root:###13 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 211812352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.18911381, Validation loss: 0.10699792, Gradient norm: 0.65610573
INFO:root:[    2] Training loss: 0.13562055, Validation loss: 0.09773279, Gradient norm: 0.32704413
INFO:root:[    3] Training loss: 0.11756872, Validation loss: 0.08987800, Gradient norm: 0.29188660
INFO:root:[    4] Training loss: 0.10724504, Validation loss: 0.08884346, Gradient norm: 0.26299313
INFO:root:[    5] Training loss: 0.10055596, Validation loss: 0.08434681, Gradient norm: 0.22178023
INFO:root:[    6] Training loss: 0.09430004, Validation loss: 0.08805724, Gradient norm: 0.24790009
INFO:root:[    7] Training loss: 0.09034437, Validation loss: 0.08516644, Gradient norm: 0.22775140
INFO:root:[    8] Training loss: 0.08862408, Validation loss: 0.08449480, Gradient norm: 0.22049522
INFO:root:[    9] Training loss: 0.08465799, Validation loss: 0.07664129, Gradient norm: 0.24286379
INFO:root:[   10] Training loss: 0.08261192, Validation loss: 0.07623883, Gradient norm: 0.22680364
INFO:root:[   11] Training loss: 0.08008778, Validation loss: 0.07960893, Gradient norm: 0.27205906
INFO:root:[   12] Training loss: 0.07851474, Validation loss: 0.07316109, Gradient norm: 0.19825670
INFO:root:[   13] Training loss: 0.07698487, Validation loss: 0.08144376, Gradient norm: 0.23225882
INFO:root:[   14] Training loss: 0.07641605, Validation loss: 0.08606527, Gradient norm: 0.23424547
INFO:root:[   15] Training loss: 0.07477801, Validation loss: 0.08217737, Gradient norm: 0.30183899
INFO:root:[   16] Training loss: 0.07451535, Validation loss: 0.07947985, Gradient norm: 0.28927377
INFO:root:[   17] Training loss: 0.07263974, Validation loss: 0.08396145, Gradient norm: 0.25957070
INFO:root:[   18] Training loss: 0.07132722, Validation loss: 0.08740061, Gradient norm: 0.18532733
INFO:root:[   19] Training loss: 0.06962135, Validation loss: 0.08385698, Gradient norm: 0.23826853
INFO:root:[   20] Training loss: 0.07075985, Validation loss: 0.07875072, Gradient norm: 0.22640976
INFO:root:[   21] Training loss: 0.06831623, Validation loss: 0.07792760, Gradient norm: 0.24921096
INFO:root:[   22] Training loss: 0.06809758, Validation loss: 0.08145657, Gradient norm: 0.30343705
INFO:root:[   23] Training loss: 0.06774953, Validation loss: 0.08374086, Gradient norm: 0.25731636
INFO:root:[   24] Training loss: 0.06614898, Validation loss: 0.08140730, Gradient norm: 0.29047390
INFO:root:[   25] Training loss: 0.06556307, Validation loss: 0.07926577, Gradient norm: 0.26251774
INFO:root:[   26] Training loss: 0.06428331, Validation loss: 0.07903653, Gradient norm: 0.26003253
INFO:root:[   27] Training loss: 0.06319894, Validation loss: 0.07586912, Gradient norm: 0.21002815
INFO:root:[   28] Training loss: 0.06396021, Validation loss: 0.07922159, Gradient norm: 0.30303491
INFO:root:[   29] Training loss: 0.06239537, Validation loss: 0.07760565, Gradient norm: 0.27941477
INFO:root:[   30] Training loss: 0.06188336, Validation loss: 0.07509561, Gradient norm: 0.23022233
INFO:root:[   31] Training loss: 0.06108271, Validation loss: 0.07549171, Gradient norm: 0.28684150
INFO:root:[   32] Training loss: 0.06123042, Validation loss: 0.07666438, Gradient norm: 0.22384211
INFO:root:[   33] Training loss: 0.05983259, Validation loss: 0.07979668, Gradient norm: 0.23907852
INFO:root:[   34] Training loss: 0.06027205, Validation loss: 0.07542225, Gradient norm: 0.37744275
INFO:root:[   35] Training loss: 0.05947306, Validation loss: 0.07253674, Gradient norm: 0.24550933
INFO:root:[   36] Training loss: 0.05952355, Validation loss: 0.06744679, Gradient norm: 0.27862916
INFO:root:[   37] Training loss: 0.05837777, Validation loss: 0.07430957, Gradient norm: 0.31829469
INFO:root:[   38] Training loss: 0.05909148, Validation loss: 0.07389102, Gradient norm: 0.24399598
INFO:root:[   39] Training loss: 0.05859962, Validation loss: 0.07152953, Gradient norm: 0.24232786
INFO:root:[   40] Training loss: 0.05734356, Validation loss: 0.07603696, Gradient norm: 0.25490025
INFO:root:[   41] Training loss: 0.05703801, Validation loss: 0.07057703, Gradient norm: 0.25411387
INFO:root:[   42] Training loss: 0.05688351, Validation loss: 0.07318243, Gradient norm: 0.25073868
INFO:root:[   43] Training loss: 0.05598583, Validation loss: 0.07025150, Gradient norm: 0.21392321
INFO:root:[   44] Training loss: 0.05594173, Validation loss: 0.07131965, Gradient norm: 0.23246826
INFO:root:[   45] Training loss: 0.05458172, Validation loss: 0.06936151, Gradient norm: 0.24571685
INFO:root:[   46] Training loss: 0.05554193, Validation loss: 0.06828308, Gradient norm: 0.25146996
INFO:root:[   47] Training loss: 0.05389178, Validation loss: 0.06828784, Gradient norm: 0.25306175
INFO:root:[   48] Training loss: 0.05450441, Validation loss: 0.07385631, Gradient norm: 0.37534248
INFO:root:[   49] Training loss: 0.05363215, Validation loss: 0.06863698, Gradient norm: 0.28171688
INFO:root:[   50] Training loss: 0.05409071, Validation loss: 0.07179680, Gradient norm: 0.27735242
INFO:root:[   51] Training loss: 0.05240447, Validation loss: 0.07143848, Gradient norm: 0.30204058
INFO:root:[   52] Training loss: 0.05267874, Validation loss: 0.07792013, Gradient norm: 0.32611025
INFO:root:[   53] Training loss: 0.05305050, Validation loss: 0.06924371, Gradient norm: 0.45940275
INFO:root:[   54] Training loss: 0.05250895, Validation loss: 0.06649643, Gradient norm: 0.31165771
INFO:root:[   55] Training loss: 0.05101556, Validation loss: 0.06410378, Gradient norm: 0.27255291
INFO:root:[   56] Training loss: 0.05253246, Validation loss: 0.07072839, Gradient norm: 0.30302283
INFO:root:[   57] Training loss: 0.05101894, Validation loss: 0.06545736, Gradient norm: 0.42394637
INFO:root:[   58] Training loss: 0.05060291, Validation loss: 0.07361632, Gradient norm: 0.33895683
INFO:root:[   59] Training loss: 0.05077595, Validation loss: 0.06773533, Gradient norm: 0.34320930
INFO:root:[   60] Training loss: 0.04976691, Validation loss: 0.06948443, Gradient norm: 0.30400300
INFO:root:[   61] Training loss: 0.05023461, Validation loss: 0.06938833, Gradient norm: 0.32239193
INFO:root:[   62] Training loss: 0.04960220, Validation loss: 0.07335243, Gradient norm: 0.39762263
INFO:root:[   63] Training loss: 0.04975344, Validation loss: 0.07281082, Gradient norm: 0.37626053
INFO:root:[   64] Training loss: 0.04881931, Validation loss: 0.06907141, Gradient norm: 0.38455437
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 124.972s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 2.07136
INFO:root:EnergyScoreTrain: 1.41013
INFO:root:CoverageTrain: 0.32542
INFO:root:IntervalWidthTrain: 0.07709
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 2.11177
INFO:root:EnergyScoreValidation: 1.4638
INFO:root:CoverageValidation: 0.29435
INFO:root:IntervalWidthValidation: 0.07115
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.15275
INFO:root:EnergyScoreTest: 1.47718
INFO:root:CoverageTest: 0.33889
INFO:root:IntervalWidthTest: 0.07531
INFO:root:###14 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 251658240
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.39934752, Validation loss: 0.11747917, Gradient norm: 1.46772895
INFO:root:[    2] Training loss: 0.21453474, Validation loss: 0.11853347, Gradient norm: 0.92002172
INFO:root:[    3] Training loss: 0.14787423, Validation loss: 0.11918708, Gradient norm: 0.64288831
INFO:root:[    4] Training loss: 0.12852537, Validation loss: 0.11976863, Gradient norm: 0.42410040
INFO:root:[    5] Training loss: 0.11992712, Validation loss: 0.11765683, Gradient norm: 0.37951148
INFO:root:[    6] Training loss: 0.11624992, Validation loss: 0.11610780, Gradient norm: 0.38131818
INFO:root:[    7] Training loss: 0.11425953, Validation loss: 0.11660572, Gradient norm: 0.37904405
INFO:root:[    8] Training loss: 0.11493505, Validation loss: 0.11518862, Gradient norm: 0.55251411
INFO:root:[    9] Training loss: 0.11252588, Validation loss: 0.11477313, Gradient norm: 0.45490819
INFO:root:[   10] Training loss: 0.11462649, Validation loss: 0.11558650, Gradient norm: 0.48831639
INFO:root:[   11] Training loss: 0.11238039, Validation loss: 0.11376490, Gradient norm: 0.61575868
INFO:root:[   12] Training loss: 0.11089927, Validation loss: 0.11021543, Gradient norm: 0.52821528
INFO:root:[   13] Training loss: 0.10932553, Validation loss: 0.10735770, Gradient norm: 0.57857900
INFO:root:[   14] Training loss: 0.10959766, Validation loss: 0.10630283, Gradient norm: 0.56699248
INFO:root:[   15] Training loss: 0.10753904, Validation loss: 0.10591544, Gradient norm: 0.60163246
INFO:root:[   16] Training loss: 0.10500750, Validation loss: 0.10310348, Gradient norm: 0.64945818
INFO:root:[   17] Training loss: 0.10443606, Validation loss: 0.10281124, Gradient norm: 0.58904023
INFO:root:[   18] Training loss: 0.10271890, Validation loss: 0.09970968, Gradient norm: 0.54677707
INFO:root:[   19] Training loss: 0.10182386, Validation loss: 0.09940953, Gradient norm: 0.57750380
INFO:root:[   20] Training loss: 0.10044716, Validation loss: 0.09511232, Gradient norm: 0.51937632
INFO:root:[   21] Training loss: 0.09919465, Validation loss: 0.09449108, Gradient norm: 0.52064190
INFO:root:[   22] Training loss: 0.09810769, Validation loss: 0.09135753, Gradient norm: 0.62494882
INFO:root:[   23] Training loss: 0.09697439, Validation loss: 0.08890899, Gradient norm: 0.44009191
INFO:root:[   24] Training loss: 0.09556696, Validation loss: 0.08840587, Gradient norm: 0.49725774
INFO:root:[   25] Training loss: 0.09551058, Validation loss: 0.08890364, Gradient norm: 0.51912792
INFO:root:[   26] Training loss: 0.09406346, Validation loss: 0.08762991, Gradient norm: 0.51402790
INFO:root:[   27] Training loss: 0.09408678, Validation loss: 0.08718353, Gradient norm: 0.48353598
INFO:root:[   28] Training loss: 0.09188017, Validation loss: 0.08454459, Gradient norm: 0.50727747
INFO:root:[   29] Training loss: 0.09182226, Validation loss: 0.08184655, Gradient norm: 0.50743976
INFO:root:[   30] Training loss: 0.08953297, Validation loss: 0.07882105, Gradient norm: 0.50793273
INFO:root:[   31] Training loss: 0.08664740, Validation loss: 0.07269903, Gradient norm: 0.49487758
INFO:root:[   32] Training loss: 0.08658957, Validation loss: 0.07107554, Gradient norm: 0.46375033
INFO:root:[   33] Training loss: 0.08257436, Validation loss: 0.06823784, Gradient norm: 0.49993605
INFO:root:[   34] Training loss: 0.08086492, Validation loss: 0.06973591, Gradient norm: 0.55679285
INFO:root:[   35] Training loss: 0.08040914, Validation loss: 0.06700832, Gradient norm: 0.49097876
INFO:root:[   36] Training loss: 0.07876413, Validation loss: 0.06678273, Gradient norm: 0.61996653
INFO:root:[   37] Training loss: 0.07827141, Validation loss: 0.06376156, Gradient norm: 0.65067559
INFO:root:[   38] Training loss: 0.07711015, Validation loss: 0.06436836, Gradient norm: 0.54693634
INFO:root:[   39] Training loss: 0.07709822, Validation loss: 0.06280828, Gradient norm: 0.63908573
INFO:root:[   40] Training loss: 0.07618978, Validation loss: 0.06259103, Gradient norm: 0.59031813
INFO:root:[   41] Training loss: 0.07588719, Validation loss: 0.06084084, Gradient norm: 0.71236895
INFO:root:[   42] Training loss: 0.07579818, Validation loss: 0.06319163, Gradient norm: 0.80347960
INFO:root:[   43] Training loss: 0.07446042, Validation loss: 0.05898157, Gradient norm: 0.66970092
INFO:root:[   44] Training loss: 0.07442620, Validation loss: 0.05840030, Gradient norm: 0.73716884
INFO:root:[   45] Training loss: 0.07465616, Validation loss: 0.06018533, Gradient norm: 0.86859864
INFO:root:[   46] Training loss: 0.07392546, Validation loss: 0.06035164, Gradient norm: 0.92448407
INFO:root:[   47] Training loss: 0.07343285, Validation loss: 0.05908430, Gradient norm: 0.97814341
INFO:root:[   48] Training loss: 0.07268599, Validation loss: 0.05782720, Gradient norm: 0.85149778
INFO:root:[   49] Training loss: 0.07197576, Validation loss: 0.05849036, Gradient norm: 0.74832094
INFO:root:[   50] Training loss: 0.07218642, Validation loss: 0.05837152, Gradient norm: 1.02412915
INFO:root:[   51] Training loss: 0.07080166, Validation loss: 0.05818570, Gradient norm: 0.72784047
INFO:root:[   52] Training loss: 0.07195803, Validation loss: 0.05981522, Gradient norm: 0.62483732
INFO:root:[   53] Training loss: 0.07044961, Validation loss: 0.05835519, Gradient norm: 1.01819434
INFO:root:[   54] Training loss: 0.07096799, Validation loss: 0.05912565, Gradient norm: 0.85808220
INFO:root:[   55] Training loss: 0.07232332, Validation loss: 0.05952538, Gradient norm: 1.00986341
INFO:root:[   56] Training loss: 0.07027103, Validation loss: 0.05691828, Gradient norm: 0.94825336
INFO:root:[   57] Training loss: 0.06912068, Validation loss: 0.05926640, Gradient norm: 0.99539123
INFO:root:[   58] Training loss: 0.06967572, Validation loss: 0.05385626, Gradient norm: 0.89400575
INFO:root:[   59] Training loss: 0.07005699, Validation loss: 0.05880043, Gradient norm: 1.01246572
INFO:root:[   60] Training loss: 0.06886176, Validation loss: 0.05775115, Gradient norm: 1.08733959
INFO:root:[   61] Training loss: 0.06965633, Validation loss: 0.05549835, Gradient norm: 1.29027652
INFO:root:[   62] Training loss: 0.06822734, Validation loss: 0.05484879, Gradient norm: 1.17598552
INFO:root:[   63] Training loss: 0.06836930, Validation loss: 0.05383102, Gradient norm: 1.17367260
INFO:root:[   64] Training loss: 0.06965134, Validation loss: 0.05698936, Gradient norm: 1.07121423
INFO:root:[   65] Training loss: 0.06865772, Validation loss: 0.05733452, Gradient norm: 1.00212061
INFO:root:[   66] Training loss: 0.06770509, Validation loss: 0.05656142, Gradient norm: 1.38134577
INFO:root:[   67] Training loss: 0.06793038, Validation loss: 0.05478615, Gradient norm: 1.39769795
INFO:root:[   68] Training loss: 0.06781549, Validation loss: 0.05667075, Gradient norm: 0.99731321
INFO:root:[   69] Training loss: 0.06723433, Validation loss: 0.05245984, Gradient norm: 1.12864050
INFO:root:[   70] Training loss: 0.06692253, Validation loss: 0.05734318, Gradient norm: 1.33837708
INFO:root:[   71] Training loss: 0.06658312, Validation loss: 0.05709596, Gradient norm: 1.08364531
INFO:root:[   72] Training loss: 0.06684819, Validation loss: 0.05404154, Gradient norm: 1.36696728
INFO:root:[   73] Training loss: 0.06713736, Validation loss: 0.05923779, Gradient norm: 1.11112349
INFO:root:[   74] Training loss: 0.06694562, Validation loss: 0.05489790, Gradient norm: 1.06806464
INFO:root:[   75] Training loss: 0.06637789, Validation loss: 0.05571126, Gradient norm: 1.07135142
INFO:root:[   76] Training loss: 0.06650067, Validation loss: 0.05595338, Gradient norm: 1.31664995
INFO:root:[   77] Training loss: 0.06611484, Validation loss: 0.05424204, Gradient norm: 1.57003620
INFO:root:[   78] Training loss: 0.06675249, Validation loss: 0.05568480, Gradient norm: 1.33629349
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 153.713s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.85666
INFO:root:EnergyScoreTrain: 1.23534
INFO:root:CoverageTrain: 0.4324
INFO:root:IntervalWidthTrain: 0.0761
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.89403
INFO:root:EnergyScoreValidation: 1.31685
INFO:root:CoverageValidation: 0.37428
INFO:root:IntervalWidthValidation: 0.0666
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.03478
INFO:root:EnergyScoreTest: 1.27274
INFO:root:CoverageTest: 0.46514
INFO:root:IntervalWidthTest: 0.09139
INFO:root:###15 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 211812352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.10875587, Validation loss: 0.09023229, Gradient norm: 1.00147700
INFO:root:[    2] Training loss: 0.07739111, Validation loss: 0.07350483, Gradient norm: 0.68954791
INFO:root:[    3] Training loss: 0.06683247, Validation loss: 0.06502549, Gradient norm: 1.23717681
INFO:root:[    4] Training loss: 0.06553939, Validation loss: 0.06727206, Gradient norm: 1.45588217
INFO:root:[    5] Training loss: 0.06177370, Validation loss: 0.05619065, Gradient norm: 1.31927422
INFO:root:[    6] Training loss: 0.05708716, Validation loss: 0.05329883, Gradient norm: 0.92697956
INFO:root:[    7] Training loss: 0.05213983, Validation loss: 0.05049082, Gradient norm: 0.67174466
INFO:root:[    8] Training loss: 0.05037737, Validation loss: 0.04815842, Gradient norm: 0.81821935
INFO:root:[    9] Training loss: 0.04870286, Validation loss: 0.04791589, Gradient norm: 0.67826538
INFO:root:[   10] Training loss: 0.04951309, Validation loss: 0.04997935, Gradient norm: 0.93312074
INFO:root:[   11] Training loss: 0.04659531, Validation loss: 0.04723817, Gradient norm: 0.78425807
INFO:root:[   12] Training loss: 0.04594560, Validation loss: 0.04601667, Gradient norm: 0.66281100
INFO:root:[   13] Training loss: 0.04554171, Validation loss: 0.04366138, Gradient norm: 0.69694071
INFO:root:[   14] Training loss: 0.04535957, Validation loss: 0.04488726, Gradient norm: 0.77544418
INFO:root:[   15] Training loss: 0.04572531, Validation loss: 0.04273077, Gradient norm: 0.99689311
INFO:root:[   16] Training loss: 0.04391971, Validation loss: 0.04199973, Gradient norm: 0.64178989
INFO:root:[   17] Training loss: 0.04159764, Validation loss: 0.04321632, Gradient norm: 0.55361285
INFO:root:[   18] Training loss: 0.04136710, Validation loss: 0.04043646, Gradient norm: 0.70687561
INFO:root:[   19] Training loss: 0.04218872, Validation loss: 0.05063924, Gradient norm: 0.85513694
INFO:root:[   20] Training loss: 0.04466572, Validation loss: 0.04185841, Gradient norm: 1.22821669
INFO:root:[   21] Training loss: 0.04101647, Validation loss: 0.04221116, Gradient norm: 0.82024549
INFO:root:[   22] Training loss: 0.03960684, Validation loss: 0.03952047, Gradient norm: 0.57776351
INFO:root:[   23] Training loss: 0.03883181, Validation loss: 0.03913346, Gradient norm: 0.70876593
INFO:root:[   24] Training loss: 0.03867620, Validation loss: 0.04047460, Gradient norm: 0.61588343
INFO:root:[   25] Training loss: 0.03848273, Validation loss: 0.04112297, Gradient norm: 0.74585069
INFO:root:[   26] Training loss: 0.03799494, Validation loss: 0.03750251, Gradient norm: 0.72071846
INFO:root:[   27] Training loss: 0.03736687, Validation loss: 0.03701784, Gradient norm: 0.53698646
INFO:root:[   28] Training loss: 0.03788119, Validation loss: 0.03929746, Gradient norm: 0.81538915
INFO:root:[   29] Training loss: 0.03744423, Validation loss: 0.03605187, Gradient norm: 0.66402060
INFO:root:[   30] Training loss: 0.03808702, Validation loss: 0.03994295, Gradient norm: 0.77041904
INFO:root:[   31] Training loss: 0.03679567, Validation loss: 0.03563736, Gradient norm: 0.56575289
INFO:root:[   32] Training loss: 0.03702710, Validation loss: 0.03607111, Gradient norm: 0.54751135
INFO:root:[   33] Training loss: 0.03690331, Validation loss: 0.03879516, Gradient norm: 0.64895942
INFO:root:[   34] Training loss: 0.03633270, Validation loss: 0.03529400, Gradient norm: 0.68192565
INFO:root:[   35] Training loss: 0.03469221, Validation loss: 0.03552980, Gradient norm: 0.62448297
INFO:root:[   36] Training loss: 0.03532768, Validation loss: 0.03605243, Gradient norm: 0.68016998
INFO:root:[   37] Training loss: 0.03693475, Validation loss: 0.03509512, Gradient norm: 0.83438935
INFO:root:[   38] Training loss: 0.03504770, Validation loss: 0.03536832, Gradient norm: 0.72072845
INFO:root:[   39] Training loss: 0.03519366, Validation loss: 0.03401350, Gradient norm: 0.75723772
INFO:root:[   40] Training loss: 0.03435948, Validation loss: 0.03420725, Gradient norm: 0.45358813
INFO:root:[   41] Training loss: 0.03551071, Validation loss: 0.03592744, Gradient norm: 0.65213820
INFO:root:[   42] Training loss: 0.03524151, Validation loss: 0.03509763, Gradient norm: 0.62091888
INFO:root:[   43] Training loss: 0.03403374, Validation loss: 0.03371663, Gradient norm: 0.61888956
INFO:root:[   44] Training loss: 0.03489426, Validation loss: 0.03435645, Gradient norm: 0.72987670
INFO:root:[   45] Training loss: 0.03415718, Validation loss: 0.03410018, Gradient norm: 0.57557180
INFO:root:[   46] Training loss: 0.03348368, Validation loss: 0.03374757, Gradient norm: 0.49318011
INFO:root:[   47] Training loss: 0.03460331, Validation loss: 0.03463042, Gradient norm: 0.76311674
INFO:root:[   48] Training loss: 0.03326769, Validation loss: 0.03266490, Gradient norm: 0.50060967
INFO:root:[   49] Training loss: 0.03288894, Validation loss: 0.03374133, Gradient norm: 0.47423291
INFO:root:[   50] Training loss: 0.03347324, Validation loss: 0.03628871, Gradient norm: 0.58809278
INFO:root:[   51] Training loss: 0.03359223, Validation loss: 0.03366225, Gradient norm: 0.62543591
INFO:root:[   52] Training loss: 0.03390914, Validation loss: 0.03481883, Gradient norm: 0.82668229
INFO:root:[   53] Training loss: 0.03369250, Validation loss: 0.03238069, Gradient norm: 0.76219898
INFO:root:[   54] Training loss: 0.03237777, Validation loss: 0.03241791, Gradient norm: 0.57738198
INFO:root:[   55] Training loss: 0.03269349, Validation loss: 0.03248290, Gradient norm: 0.66318634
INFO:root:[   56] Training loss: 0.03290471, Validation loss: 0.03257415, Gradient norm: 0.51983047
INFO:root:[   57] Training loss: 0.03176526, Validation loss: 0.03256258, Gradient norm: 0.51339755
INFO:root:[   58] Training loss: 0.03156409, Validation loss: 0.03239774, Gradient norm: 0.50432359
INFO:root:[   59] Training loss: 0.03242729, Validation loss: 0.03216234, Gradient norm: 0.51110810
INFO:root:[   60] Training loss: 0.03255337, Validation loss: 0.03375112, Gradient norm: 0.47848667
INFO:root:[   61] Training loss: 0.03321135, Validation loss: 0.03280100, Gradient norm: 0.67073309
INFO:root:[   62] Training loss: 0.03205509, Validation loss: 0.03115753, Gradient norm: 0.54609695
INFO:root:[   63] Training loss: 0.03241008, Validation loss: 0.03261712, Gradient norm: 0.61159578
INFO:root:[   64] Training loss: 0.03205208, Validation loss: 0.03184301, Gradient norm: 0.70857060
INFO:root:[   65] Training loss: 0.03196027, Validation loss: 0.03633405, Gradient norm: 0.59268969
INFO:root:[   66] Training loss: 0.03277289, Validation loss: 0.03259336, Gradient norm: 0.74396342
INFO:root:[   67] Training loss: 0.03113880, Validation loss: 0.03039392, Gradient norm: 0.60192753
INFO:root:[   68] Training loss: 0.03049569, Validation loss: 0.03090148, Gradient norm: 0.34410025
INFO:root:[   69] Training loss: 0.03151997, Validation loss: 0.03180245, Gradient norm: 0.64359616
INFO:root:[   70] Training loss: 0.03177206, Validation loss: 0.03060416, Gradient norm: 0.64907322
INFO:root:[   71] Training loss: 0.03081584, Validation loss: 0.03317164, Gradient norm: 0.50583504
INFO:root:[   72] Training loss: 0.03213541, Validation loss: 0.03149504, Gradient norm: 0.66477088
INFO:root:[   73] Training loss: 0.03124437, Validation loss: 0.03179677, Gradient norm: 0.57459753
INFO:root:[   74] Training loss: 0.03153839, Validation loss: 0.03093426, Gradient norm: 0.62788100
INFO:root:[   75] Training loss: 0.03071528, Validation loss: 0.02995505, Gradient norm: 0.45026957
INFO:root:[   76] Training loss: 0.03047958, Validation loss: 0.03025665, Gradient norm: 0.60764780
INFO:root:[   77] Training loss: 0.03121187, Validation loss: 0.03045915, Gradient norm: 0.57275994
INFO:root:[   78] Training loss: 0.03080451, Validation loss: 0.03011976, Gradient norm: 0.56096766
INFO:root:[   79] Training loss: 0.02960073, Validation loss: 0.03047244, Gradient norm: 0.48811800
INFO:root:[   80] Training loss: 0.03031970, Validation loss: 0.03005258, Gradient norm: 0.67898768
INFO:root:[   81] Training loss: 0.03064703, Validation loss: 0.03032145, Gradient norm: 0.65962371
INFO:root:[   82] Training loss: 0.03145016, Validation loss: 0.03168592, Gradient norm: 0.74101258
INFO:root:[   83] Training loss: 0.03028989, Validation loss: 0.03003204, Gradient norm: 0.65839968
INFO:root:[   84] Training loss: 0.03115176, Validation loss: 0.03160194, Gradient norm: 0.84786452
INFO:root:EP 84: Early stopping
INFO:root:Training the model took 165.386s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.80224
INFO:root:EnergyScoreTrain: 0.62588
INFO:root:CoverageTrain: 0.79041
INFO:root:IntervalWidthTrain: 0.05609
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.82973
INFO:root:EnergyScoreValidation: 0.64767
INFO:root:CoverageValidation: 0.782
INFO:root:IntervalWidthValidation: 0.05648
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.46461
INFO:root:EnergyScoreTest: 1.21254
INFO:root:CoverageTest: 0.49694
INFO:root:IntervalWidthTest: 0.06083
INFO:root:###16 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 207618048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09645397, Validation loss: 0.08771655, Gradient norm: 0.21092855
INFO:root:[    2] Training loss: 0.08022777, Validation loss: 0.07348569, Gradient norm: 0.30977673
INFO:root:[    3] Training loss: 0.06797408, Validation loss: 0.06138200, Gradient norm: 0.57899437
INFO:root:[    4] Training loss: 0.05872212, Validation loss: 0.05642735, Gradient norm: 0.53482674
INFO:root:[    5] Training loss: 0.05456965, Validation loss: 0.05365632, Gradient norm: 0.61016261
INFO:root:[    6] Training loss: 0.05300121, Validation loss: 0.05455601, Gradient norm: 0.58471453
INFO:root:[    7] Training loss: 0.05328823, Validation loss: 0.05129677, Gradient norm: 0.92487854
INFO:root:[    8] Training loss: 0.05001976, Validation loss: 0.04936617, Gradient norm: 0.46249165
INFO:root:[    9] Training loss: 0.04978408, Validation loss: 0.04920240, Gradient norm: 0.63604820
INFO:root:[   10] Training loss: 0.04828808, Validation loss: 0.04800017, Gradient norm: 0.58911792
INFO:root:[   11] Training loss: 0.04899489, Validation loss: 0.04661643, Gradient norm: 0.81366102
INFO:root:[   12] Training loss: 0.04914003, Validation loss: 0.04949439, Gradient norm: 1.00986144
INFO:root:[   13] Training loss: 0.04633658, Validation loss: 0.04569936, Gradient norm: 0.48325172
INFO:root:[   14] Training loss: 0.04590317, Validation loss: 0.04590722, Gradient norm: 0.57851463
INFO:root:[   15] Training loss: 0.04503898, Validation loss: 0.04506403, Gradient norm: 0.61396775
INFO:root:[   16] Training loss: 0.04552388, Validation loss: 0.04499999, Gradient norm: 0.56664289
INFO:root:[   17] Training loss: 0.04481565, Validation loss: 0.04424524, Gradient norm: 0.51228618
INFO:root:[   18] Training loss: 0.04387520, Validation loss: 0.04359948, Gradient norm: 0.52042671
INFO:root:[   19] Training loss: 0.04391347, Validation loss: 0.04252821, Gradient norm: 0.69956175
INFO:root:[   20] Training loss: 0.04311209, Validation loss: 0.04306185, Gradient norm: 0.54828282
INFO:root:[   21] Training loss: 0.04281510, Validation loss: 0.04268086, Gradient norm: 0.59370218
INFO:root:[   22] Training loss: 0.04248032, Validation loss: 0.04166595, Gradient norm: 0.59316386
INFO:root:[   23] Training loss: 0.04227384, Validation loss: 0.04150141, Gradient norm: 0.71092787
INFO:root:[   24] Training loss: 0.04162235, Validation loss: 0.04066109, Gradient norm: 0.46370277
INFO:root:[   25] Training loss: 0.04119581, Validation loss: 0.04063539, Gradient norm: 0.45860099
INFO:root:[   26] Training loss: 0.04127638, Validation loss: 0.04382132, Gradient norm: 0.54788049
INFO:root:[   27] Training loss: 0.04150783, Validation loss: 0.03965464, Gradient norm: 0.68911522
INFO:root:[   28] Training loss: 0.03988266, Validation loss: 0.04034940, Gradient norm: 0.42363618
INFO:root:[   29] Training loss: 0.04036420, Validation loss: 0.04085704, Gradient norm: 0.76213080
INFO:root:[   30] Training loss: 0.04053520, Validation loss: 0.04133611, Gradient norm: 0.87754191
INFO:root:[   31] Training loss: 0.03991236, Validation loss: 0.03859206, Gradient norm: 0.62633694
INFO:root:[   32] Training loss: 0.04035298, Validation loss: 0.03817039, Gradient norm: 0.79503268
INFO:root:[   33] Training loss: 0.03991647, Validation loss: 0.04089191, Gradient norm: 0.80513328
INFO:root:[   34] Training loss: 0.03959425, Validation loss: 0.03958597, Gradient norm: 0.61684484
INFO:root:[   35] Training loss: 0.03971957, Validation loss: 0.03876493, Gradient norm: 0.64885486
INFO:root:[   36] Training loss: 0.03860316, Validation loss: 0.03846934, Gradient norm: 0.63661135
INFO:root:[   37] Training loss: 0.03801620, Validation loss: 0.03910718, Gradient norm: 0.54263423
INFO:root:[   38] Training loss: 0.03965011, Validation loss: 0.03701728, Gradient norm: 0.69646580
INFO:root:[   39] Training loss: 0.03765403, Validation loss: 0.03738088, Gradient norm: 0.52883664
INFO:root:[   40] Training loss: 0.03666276, Validation loss: 0.03702486, Gradient norm: 0.39420193
INFO:root:[   41] Training loss: 0.03775760, Validation loss: 0.03679767, Gradient norm: 0.46626695
INFO:root:[   42] Training loss: 0.03799109, Validation loss: 0.03969516, Gradient norm: 0.63798176
INFO:root:[   43] Training loss: 0.03770632, Validation loss: 0.03697070, Gradient norm: 0.46415880
INFO:root:[   44] Training loss: 0.03710733, Validation loss: 0.03855123, Gradient norm: 0.47231026
INFO:root:[   45] Training loss: 0.03757585, Validation loss: 0.03664653, Gradient norm: 0.65809564
INFO:root:[   46] Training loss: 0.03635258, Validation loss: 0.03702234, Gradient norm: 0.44957648
INFO:root:[   47] Training loss: 0.03751003, Validation loss: 0.03624500, Gradient norm: 0.50548359
INFO:root:[   48] Training loss: 0.03736680, Validation loss: 0.03922578, Gradient norm: 0.65751582
INFO:root:[   49] Training loss: 0.03641886, Validation loss: 0.03624967, Gradient norm: 0.60778923
INFO:root:[   50] Training loss: 0.03603403, Validation loss: 0.03621590, Gradient norm: 0.50541845
INFO:root:[   51] Training loss: 0.03682603, Validation loss: 0.03827426, Gradient norm: 0.75047913
INFO:root:[   52] Training loss: 0.03631322, Validation loss: 0.03779995, Gradient norm: 0.70371651
INFO:root:[   53] Training loss: 0.03598384, Validation loss: 0.03536676, Gradient norm: 0.51627803
INFO:root:[   54] Training loss: 0.03562649, Validation loss: 0.03579954, Gradient norm: 0.63755892
INFO:root:[   55] Training loss: 0.03656518, Validation loss: 0.03645631, Gradient norm: 0.71286085
INFO:root:[   56] Training loss: 0.03609987, Validation loss: 0.03597175, Gradient norm: 0.75140665
INFO:root:[   57] Training loss: 0.03606089, Validation loss: 0.03681822, Gradient norm: 0.53438106
INFO:root:[   58] Training loss: 0.03646998, Validation loss: 0.03534564, Gradient norm: 0.68565649
INFO:root:[   59] Training loss: 0.03513265, Validation loss: 0.03478779, Gradient norm: 0.54530430
INFO:root:[   60] Training loss: 0.03548731, Validation loss: 0.03481963, Gradient norm: 0.54860889
INFO:root:[   61] Training loss: 0.03549319, Validation loss: 0.03543325, Gradient norm: 0.55297698
INFO:root:[   62] Training loss: 0.03507517, Validation loss: 0.03460170, Gradient norm: 0.44075192
INFO:root:[   63] Training loss: 0.03503815, Validation loss: 0.03567846, Gradient norm: 0.54182175
INFO:root:[   64] Training loss: 0.03447924, Validation loss: 0.03548609, Gradient norm: 0.46242482
INFO:root:[   65] Training loss: 0.03454269, Validation loss: 0.03556620, Gradient norm: 0.54089128
INFO:root:[   66] Training loss: 0.03476271, Validation loss: 0.03643367, Gradient norm: 0.53939275
INFO:root:[   67] Training loss: 0.03450188, Validation loss: 0.03409556, Gradient norm: 0.53035839
INFO:root:[   68] Training loss: 0.03406591, Validation loss: 0.03573015, Gradient norm: 0.62604081
