INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 1000, 'downscaling_factor': 2}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 25165824
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09194073, Validation loss: 0.06328279, Gradient norm: 0.77899737
INFO:root:[    2] Training loss: 0.05205347, Validation loss: 0.04643861, Gradient norm: 1.07577981
INFO:root:[    3] Training loss: 0.04622306, Validation loss: 0.04078963, Gradient norm: 1.37291866
INFO:root:[    4] Training loss: 0.03957115, Validation loss: 0.04335407, Gradient norm: 0.57710431
INFO:root:[    5] Training loss: 0.03909369, Validation loss: 0.03767704, Gradient norm: 0.87537431
INFO:root:[    6] Training loss: 0.03854642, Validation loss: 0.04267948, Gradient norm: 1.05490889
INFO:root:[    7] Training loss: 0.03566637, Validation loss: 0.03400251, Gradient norm: 0.68674870
INFO:root:[    8] Training loss: 0.03261516, Validation loss: 0.03138826, Gradient norm: 0.72061538
INFO:root:[    9] Training loss: 0.03231691, Validation loss: 0.03372111, Gradient norm: 0.93934554
INFO:root:[   10] Training loss: 0.03176314, Validation loss: 0.02906638, Gradient norm: 0.96678634
INFO:root:[   11] Training loss: 0.02940448, Validation loss: 0.02819507, Gradient norm: 0.62019385
INFO:root:[   12] Training loss: 0.02969999, Validation loss: 0.02822169, Gradient norm: 0.97181700
INFO:root:[   13] Training loss: 0.02893488, Validation loss: 0.02804056, Gradient norm: 0.73601489
INFO:root:[   14] Training loss: 0.02723208, Validation loss: 0.02611483, Gradient norm: 0.79762858
INFO:root:[   15] Training loss: 0.02683323, Validation loss: 0.02588479, Gradient norm: 0.68446550
INFO:root:[   16] Training loss: 0.02626396, Validation loss: 0.02942827, Gradient norm: 0.76339112
INFO:root:[   17] Training loss: 0.02636382, Validation loss: 0.02946130, Gradient norm: 0.99498555
INFO:root:[   18] Training loss: 0.02647135, Validation loss: 0.02428495, Gradient norm: 1.00583815
INFO:root:[   19] Training loss: 0.02461239, Validation loss: 0.02559793, Gradient norm: 0.60167963
INFO:root:[   20] Training loss: 0.02606028, Validation loss: 0.02385851, Gradient norm: 1.05421024
INFO:root:[   21] Training loss: 0.02546622, Validation loss: 0.02313642, Gradient norm: 0.74944400
INFO:root:[   22] Training loss: 0.02482463, Validation loss: 0.02433295, Gradient norm: 0.76069377
INFO:root:[   23] Training loss: 0.02528019, Validation loss: 0.02507205, Gradient norm: 0.99998378
INFO:root:[   24] Training loss: 0.02398928, Validation loss: 0.02422639, Gradient norm: 0.80121562
INFO:root:[   25] Training loss: 0.02404209, Validation loss: 0.02360791, Gradient norm: 0.65793900
INFO:root:[   26] Training loss: 0.02258407, Validation loss: 0.02408377, Gradient norm: 0.62778960
INFO:root:[   27] Training loss: 0.02283962, Validation loss: 0.02249146, Gradient norm: 0.67459695
INFO:root:[   28] Training loss: 0.02323900, Validation loss: 0.02254382, Gradient norm: 0.84385303
INFO:root:[   29] Training loss: 0.02304621, Validation loss: 0.02211182, Gradient norm: 0.69396316
INFO:root:[   30] Training loss: 0.02251215, Validation loss: 0.02200479, Gradient norm: 0.56494769
INFO:root:[   31] Training loss: 0.02257169, Validation loss: 0.02590603, Gradient norm: 0.79592560
INFO:root:[   32] Training loss: 0.02433519, Validation loss: 0.02512744, Gradient norm: 1.05575470
INFO:root:[   33] Training loss: 0.02229622, Validation loss: 0.02274961, Gradient norm: 0.68602304
INFO:root:[   34] Training loss: 0.02257395, Validation loss: 0.02156685, Gradient norm: 0.54352802
INFO:root:[   35] Training loss: 0.02250304, Validation loss: 0.02111476, Gradient norm: 0.70933247
INFO:root:[   36] Training loss: 0.02118542, Validation loss: 0.02158137, Gradient norm: 0.59423282
INFO:root:[   37] Training loss: 0.02168399, Validation loss: 0.02102399, Gradient norm: 0.56393834
INFO:root:[   38] Training loss: 0.02221629, Validation loss: 0.02127515, Gradient norm: 0.65212826
INFO:root:[   39] Training loss: 0.02273568, Validation loss: 0.02340831, Gradient norm: 0.97390605
INFO:root:[   40] Training loss: 0.02277174, Validation loss: 0.02190491, Gradient norm: 0.89153860
INFO:root:[   41] Training loss: 0.02196160, Validation loss: 0.02375207, Gradient norm: 0.77555319
INFO:root:[   42] Training loss: 0.02234086, Validation loss: 0.02511087, Gradient norm: 0.95403358
INFO:root:[   43] Training loss: 0.02341731, Validation loss: 0.02100495, Gradient norm: 1.10465000
INFO:root:[   44] Training loss: 0.02219356, Validation loss: 0.02348974, Gradient norm: 0.73669075
INFO:root:[   45] Training loss: 0.02183342, Validation loss: 0.02150865, Gradient norm: 0.74927153
INFO:root:[   46] Training loss: 0.02375307, Validation loss: 0.02110033, Gradient norm: 1.02706443
INFO:root:[   47] Training loss: 0.02221462, Validation loss: 0.02134218, Gradient norm: 0.94095244
INFO:root:[   48] Training loss: 0.02013251, Validation loss: 0.02070781, Gradient norm: 0.56308612
INFO:root:[   49] Training loss: 0.02109323, Validation loss: 0.02213079, Gradient norm: 0.74345711
INFO:root:[   50] Training loss: 0.02210514, Validation loss: 0.02404914, Gradient norm: 0.86440738
INFO:root:[   51] Training loss: 0.02159969, Validation loss: 0.02170549, Gradient norm: 0.86546681
INFO:root:[   52] Training loss: 0.02358465, Validation loss: 0.02387637, Gradient norm: 1.06307318
INFO:root:[   53] Training loss: 0.02150332, Validation loss: 0.02366922, Gradient norm: 0.76943700
INFO:root:[   54] Training loss: 0.02199755, Validation loss: 0.02085116, Gradient norm: 0.79924317
INFO:root:[   55] Training loss: 0.02156313, Validation loss: 0.02019996, Gradient norm: 0.74524736
INFO:root:[   56] Training loss: 0.02043094, Validation loss: 0.02058298, Gradient norm: 0.54396468
INFO:root:[   57] Training loss: 0.01992202, Validation loss: 0.02068651, Gradient norm: 0.47411735
INFO:root:[   58] Training loss: 0.02098272, Validation loss: 0.02104515, Gradient norm: 0.56599179
INFO:root:[   59] Training loss: 0.02029613, Validation loss: 0.02028117, Gradient norm: 0.64110966
INFO:root:[   60] Training loss: 0.01999898, Validation loss: 0.02032102, Gradient norm: 0.42880218
INFO:root:[   61] Training loss: 0.02008660, Validation loss: 0.01985736, Gradient norm: 0.61834143
INFO:root:[   62] Training loss: 0.02078797, Validation loss: 0.02074674, Gradient norm: 0.59412254
INFO:root:[   63] Training loss: 0.02009671, Validation loss: 0.02022813, Gradient norm: 0.70445625
INFO:root:[   64] Training loss: 0.01981305, Validation loss: 0.02058134, Gradient norm: 0.58870301
INFO:root:[   65] Training loss: 0.01994024, Validation loss: 0.02017931, Gradient norm: 0.55405065
INFO:root:[   66] Training loss: 0.01978618, Validation loss: 0.02142567, Gradient norm: 0.58421790
INFO:root:[   67] Training loss: 0.02037150, Validation loss: 0.01974570, Gradient norm: 0.67304605
INFO:root:[   68] Training loss: 0.02035326, Validation loss: 0.02125736, Gradient norm: 0.69024015
INFO:root:[   69] Training loss: 0.02226860, Validation loss: 0.02154723, Gradient norm: 0.91518662
INFO:root:[   70] Training loss: 0.02139711, Validation loss: 0.02149075, Gradient norm: 0.77469377
INFO:root:[   71] Training loss: 0.01995348, Validation loss: 0.02058801, Gradient norm: 0.75918920
INFO:root:[   72] Training loss: 0.02105519, Validation loss: 0.01966299, Gradient norm: 0.83355527
INFO:root:[   73] Training loss: 0.02023833, Validation loss: 0.02037027, Gradient norm: 0.70421452
INFO:root:[   74] Training loss: 0.02069573, Validation loss: 0.01925481, Gradient norm: 0.75617129
INFO:root:[   75] Training loss: 0.02056814, Validation loss: 0.01935680, Gradient norm: 0.68585065
INFO:root:[   76] Training loss: 0.02024456, Validation loss: 0.02180686, Gradient norm: 0.61698220
INFO:root:[   77] Training loss: 0.01938946, Validation loss: 0.01955795, Gradient norm: 0.63342886
INFO:root:[   78] Training loss: 0.01950918, Validation loss: 0.01904636, Gradient norm: 0.45215676
INFO:root:[   79] Training loss: 0.01945053, Validation loss: 0.01909894, Gradient norm: 0.55528508
INFO:root:[   80] Training loss: 0.01900822, Validation loss: 0.02119988, Gradient norm: 0.47669308
INFO:root:[   81] Training loss: 0.01913907, Validation loss: 0.01878529, Gradient norm: 0.53217951
INFO:root:[   82] Training loss: 0.01924855, Validation loss: 0.01946412, Gradient norm: 0.61918918
INFO:root:[   83] Training loss: 0.01921099, Validation loss: 0.02564862, Gradient norm: 0.62602311
INFO:root:[   84] Training loss: 0.02091069, Validation loss: 0.02100851, Gradient norm: 0.95190803
INFO:root:[   85] Training loss: 0.01936383, Validation loss: 0.01869077, Gradient norm: 0.69368124
INFO:root:[   86] Training loss: 0.01933594, Validation loss: 0.01891878, Gradient norm: 0.59729343
INFO:root:[   87] Training loss: 0.01935862, Validation loss: 0.01907865, Gradient norm: 0.54477460
INFO:root:[   88] Training loss: 0.01865310, Validation loss: 0.02072991, Gradient norm: 0.53271467
INFO:root:[   89] Training loss: 0.02036128, Validation loss: 0.01866684, Gradient norm: 0.84039714
INFO:root:[   90] Training loss: 0.01976553, Validation loss: 0.01874038, Gradient norm: 0.75567649
INFO:root:[   91] Training loss: 0.02060139, Validation loss: 0.02337575, Gradient norm: 0.89500169
INFO:root:[   92] Training loss: 0.01951908, Validation loss: 0.01831792, Gradient norm: 0.69151760
INFO:root:[   93] Training loss: 0.01891122, Validation loss: 0.01972680, Gradient norm: 0.43190855
INFO:root:[   94] Training loss: 0.01874338, Validation loss: 0.02055971, Gradient norm: 0.57856002
INFO:root:[   95] Training loss: 0.02013537, Validation loss: 0.02150027, Gradient norm: 0.72798283
INFO:root:[   96] Training loss: 0.01871250, Validation loss: 0.01846718, Gradient norm: 0.54254731
INFO:root:[   97] Training loss: 0.01832672, Validation loss: 0.01840138, Gradient norm: 0.52563189
INFO:root:[   98] Training loss: 0.01868690, Validation loss: 0.01829499, Gradient norm: 0.61493625
INFO:root:[   99] Training loss: 0.01838268, Validation loss: 0.01886992, Gradient norm: 0.38400623
INFO:root:[  100] Training loss: 0.01833226, Validation loss: 0.01832369, Gradient norm: 0.55763337
INFO:root:[  101] Training loss: 0.01824329, Validation loss: 0.02101902, Gradient norm: 0.52364886
INFO:root:[  102] Training loss: 0.01895503, Validation loss: 0.02256242, Gradient norm: 0.70992027
INFO:root:[  103] Training loss: 0.02027032, Validation loss: 0.01860898, Gradient norm: 0.81857770
INFO:root:[  104] Training loss: 0.01978797, Validation loss: 0.02091934, Gradient norm: 0.79951000
INFO:root:[  105] Training loss: 0.01831682, Validation loss: 0.01927668, Gradient norm: 0.60883492
INFO:root:[  106] Training loss: 0.01898699, Validation loss: 0.01900274, Gradient norm: 0.62846780
INFO:root:[  107] Training loss: 0.01899891, Validation loss: 0.02126640, Gradient norm: 0.69168806
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 611.887s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.78579
INFO:root:EnergyScoreTrain: 0.56518
INFO:root:CoverageTrain: 0.95185
INFO:root:IntervalWidthTrain: 0.11761
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.80703
INFO:root:EnergyScoreValidation: 0.5851
INFO:root:CoverageValidation: 0.94867
INFO:root:IntervalWidthValidation: 0.11709
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.81138
INFO:root:EnergyScoreTest: 1.38982
INFO:root:CoverageTest: 0.71808
INFO:root:IntervalWidthTest: 0.12565
INFO:root:###2 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08850362, Validation loss: 0.06447108, Gradient norm: 0.77975613
INFO:root:[    2] Training loss: 0.05342454, Validation loss: 0.04688280, Gradient norm: 0.67325200
INFO:root:[    3] Training loss: 0.04669258, Validation loss: 0.04379509, Gradient norm: 0.79671348
INFO:root:[    4] Training loss: 0.04006567, Validation loss: 0.03690538, Gradient norm: 0.40527498
INFO:root:[    5] Training loss: 0.03640576, Validation loss: 0.03501898, Gradient norm: 0.38534998
INFO:root:[    6] Training loss: 0.03482446, Validation loss: 0.03830885, Gradient norm: 0.45005418
INFO:root:[    7] Training loss: 0.03642401, Validation loss: 0.03472901, Gradient norm: 0.61075592
INFO:root:[    8] Training loss: 0.03341701, Validation loss: 0.03319560, Gradient norm: 0.35983110
INFO:root:[    9] Training loss: 0.03138888, Validation loss: 0.03313949, Gradient norm: 0.36730032
INFO:root:[   10] Training loss: 0.03206338, Validation loss: 0.03246091, Gradient norm: 0.41687327
INFO:root:[   11] Training loss: 0.03172436, Validation loss: 0.03079451, Gradient norm: 0.42827474
INFO:root:[   12] Training loss: 0.03227201, Validation loss: 0.03022798, Gradient norm: 0.44215650
INFO:root:[   13] Training loss: 0.03084106, Validation loss: 0.03008977, Gradient norm: 0.38300291
INFO:root:[   14] Training loss: 0.03077800, Validation loss: 0.02977874, Gradient norm: 0.44793355
INFO:root:[   15] Training loss: 0.03076938, Validation loss: 0.03052486, Gradient norm: 0.56332632
INFO:root:[   16] Training loss: 0.03082163, Validation loss: 0.02865392, Gradient norm: 0.42321459
INFO:root:[   17] Training loss: 0.03184868, Validation loss: 0.02851067, Gradient norm: 0.74020080
INFO:root:[   18] Training loss: 0.02846703, Validation loss: 0.02776504, Gradient norm: 0.38320679
INFO:root:[   19] Training loss: 0.02805013, Validation loss: 0.02797049, Gradient norm: 0.33148116
INFO:root:[   20] Training loss: 0.02778395, Validation loss: 0.02710377, Gradient norm: 0.33668311
INFO:root:[   21] Training loss: 0.02777596, Validation loss: 0.02982965, Gradient norm: 0.31283578
INFO:root:[   22] Training loss: 0.02815831, Validation loss: 0.02916240, Gradient norm: 0.44641494
INFO:root:[   23] Training loss: 0.02824908, Validation loss: 0.02678858, Gradient norm: 0.47151104
INFO:root:[   24] Training loss: 0.02749056, Validation loss: 0.02722007, Gradient norm: 0.39962311
INFO:root:[   25] Training loss: 0.02711523, Validation loss: 0.02682752, Gradient norm: 0.34789283
INFO:root:[   26] Training loss: 0.02774696, Validation loss: 0.02541799, Gradient norm: 0.48487031
INFO:root:[   27] Training loss: 0.02601388, Validation loss: 0.02537933, Gradient norm: 0.32466097
INFO:root:[   28] Training loss: 0.02538330, Validation loss: 0.02608614, Gradient norm: 0.29827075
INFO:root:[   29] Training loss: 0.02603460, Validation loss: 0.02520303, Gradient norm: 0.45648108
INFO:root:[   30] Training loss: 0.02492339, Validation loss: 0.02402835, Gradient norm: 0.35726852
INFO:root:[   31] Training loss: 0.02561686, Validation loss: 0.02420525, Gradient norm: 0.39836715
INFO:root:[   32] Training loss: 0.02463432, Validation loss: 0.02683831, Gradient norm: 0.37192563
INFO:root:[   33] Training loss: 0.02377321, Validation loss: 0.02443896, Gradient norm: 0.35529763
INFO:root:[   34] Training loss: 0.02373826, Validation loss: 0.02516266, Gradient norm: 0.32534975
INFO:root:[   35] Training loss: 0.02373008, Validation loss: 0.02510599, Gradient norm: 0.37552394
INFO:root:[   36] Training loss: 0.02398813, Validation loss: 0.02311711, Gradient norm: 0.41611928
INFO:root:[   37] Training loss: 0.02318532, Validation loss: 0.02295162, Gradient norm: 0.32174929
INFO:root:[   38] Training loss: 0.02590940, Validation loss: 0.02425259, Gradient norm: 0.69208454
INFO:root:[   39] Training loss: 0.02327211, Validation loss: 0.02413745, Gradient norm: 0.41219419
INFO:root:[   40] Training loss: 0.02358431, Validation loss: 0.02211908, Gradient norm: 0.34911121
INFO:root:[   41] Training loss: 0.02282749, Validation loss: 0.02211733, Gradient norm: 0.33193514
INFO:root:[   42] Training loss: 0.02288986, Validation loss: 0.02358050, Gradient norm: 0.33255935
INFO:root:[   43] Training loss: 0.02302690, Validation loss: 0.02423969, Gradient norm: 0.39829744
INFO:root:[   44] Training loss: 0.02369632, Validation loss: 0.02250613, Gradient norm: 0.54692229
INFO:root:[   45] Training loss: 0.02249727, Validation loss: 0.02219735, Gradient norm: 0.34929317
INFO:root:[   46] Training loss: 0.02307373, Validation loss: 0.02240783, Gradient norm: 0.46841578
INFO:root:[   47] Training loss: 0.02223399, Validation loss: 0.02165097, Gradient norm: 0.37282965
INFO:root:[   48] Training loss: 0.02175467, Validation loss: 0.02283915, Gradient norm: 0.36364162
INFO:root:[   49] Training loss: 0.02255128, Validation loss: 0.02222112, Gradient norm: 0.43957999
INFO:root:[   50] Training loss: 0.02229812, Validation loss: 0.02115981, Gradient norm: 0.35608150
INFO:root:[   51] Training loss: 0.02161414, Validation loss: 0.02132195, Gradient norm: 0.39502249
INFO:root:[   52] Training loss: 0.02066847, Validation loss: 0.02158938, Gradient norm: 0.33522944
INFO:root:[   53] Training loss: 0.02120501, Validation loss: 0.02123270, Gradient norm: 0.36197556
INFO:root:[   54] Training loss: 0.02123520, Validation loss: 0.02143918, Gradient norm: 0.38025615
INFO:root:[   55] Training loss: 0.02117595, Validation loss: 0.02139930, Gradient norm: 0.38476500
INFO:root:[   56] Training loss: 0.02173842, Validation loss: 0.02344245, Gradient norm: 0.44196206
INFO:root:[   57] Training loss: 0.02135869, Validation loss: 0.02142522, Gradient norm: 0.44190727
INFO:root:[   58] Training loss: 0.02191468, Validation loss: 0.02139985, Gradient norm: 0.52851866
INFO:root:[   59] Training loss: 0.02083058, Validation loss: 0.02111024, Gradient norm: 0.37120831
INFO:root:[   60] Training loss: 0.02060790, Validation loss: 0.02199054, Gradient norm: 0.38639884
INFO:root:[   61] Training loss: 0.02090641, Validation loss: 0.02138452, Gradient norm: 0.33734149
INFO:root:[   62] Training loss: 0.02124273, Validation loss: 0.02403115, Gradient norm: 0.39595208
INFO:root:[   63] Training loss: 0.02152793, Validation loss: 0.02154490, Gradient norm: 0.45527833
INFO:root:[   64] Training loss: 0.02075165, Validation loss: 0.02122535, Gradient norm: 0.37866869
INFO:root:[   65] Training loss: 0.02077247, Validation loss: 0.02090148, Gradient norm: 0.42351914
INFO:root:[   66] Training loss: 0.02028646, Validation loss: 0.02020592, Gradient norm: 0.36751099
INFO:root:[   67] Training loss: 0.02036931, Validation loss: 0.01988248, Gradient norm: 0.35278907
INFO:root:[   68] Training loss: 0.01993789, Validation loss: 0.02001864, Gradient norm: 0.34323320
INFO:root:[   69] Training loss: 0.01983557, Validation loss: 0.02053054, Gradient norm: 0.28891724
INFO:root:[   70] Training loss: 0.02028379, Validation loss: 0.01969504, Gradient norm: 0.30957700
INFO:root:[   71] Training loss: 0.02063658, Validation loss: 0.02102916, Gradient norm: 0.40483990
INFO:root:[   72] Training loss: 0.02126912, Validation loss: 0.02005213, Gradient norm: 0.52259431
INFO:root:[   73] Training loss: 0.02016129, Validation loss: 0.01988626, Gradient norm: 0.30336042
INFO:root:[   74] Training loss: 0.02010135, Validation loss: 0.02007157, Gradient norm: 0.35693617
INFO:root:[   75] Training loss: 0.02001763, Validation loss: 0.01936754, Gradient norm: 0.30352722
INFO:root:[   76] Training loss: 0.02035778, Validation loss: 0.01983015, Gradient norm: 0.33311872
INFO:root:[   77] Training loss: 0.01980335, Validation loss: 0.01997276, Gradient norm: 0.38871385
INFO:root:[   78] Training loss: 0.01990154, Validation loss: 0.01953102, Gradient norm: 0.40485997
INFO:root:[   79] Training loss: 0.02041047, Validation loss: 0.01943518, Gradient norm: 0.46039685
INFO:root:[   80] Training loss: 0.01981366, Validation loss: 0.01908221, Gradient norm: 0.37136495
INFO:root:[   81] Training loss: 0.01906674, Validation loss: 0.02018582, Gradient norm: 0.26380079
INFO:root:[   82] Training loss: 0.02072644, Validation loss: 0.02284452, Gradient norm: 0.50762613
INFO:root:[   83] Training loss: 0.01933939, Validation loss: 0.01912185, Gradient norm: 0.38509882
INFO:root:[   84] Training loss: 0.01874060, Validation loss: 0.01922584, Gradient norm: 0.34152157
INFO:root:[   85] Training loss: 0.01850912, Validation loss: 0.01893543, Gradient norm: 0.23548521
INFO:root:[   86] Training loss: 0.01909111, Validation loss: 0.01893075, Gradient norm: 0.40967268
INFO:root:[   87] Training loss: 0.01924109, Validation loss: 0.02247586, Gradient norm: 0.42442621
INFO:root:[   88] Training loss: 0.02067524, Validation loss: 0.01942663, Gradient norm: 0.61846735
INFO:root:[   89] Training loss: 0.01868608, Validation loss: 0.01887653, Gradient norm: 0.31929604
INFO:root:[   90] Training loss: 0.01998183, Validation loss: 0.01878892, Gradient norm: 0.39968009
INFO:root:[   91] Training loss: 0.01893418, Validation loss: 0.01885001, Gradient norm: 0.37127933
INFO:root:[   92] Training loss: 0.01855997, Validation loss: 0.01884094, Gradient norm: 0.33898484
INFO:root:[   93] Training loss: 0.01871885, Validation loss: 0.01871421, Gradient norm: 0.28002318
INFO:root:[   94] Training loss: 0.01896851, Validation loss: 0.01835522, Gradient norm: 0.34538145
INFO:root:[   95] Training loss: 0.01807106, Validation loss: 0.01828182, Gradient norm: 0.26746243
INFO:root:[   96] Training loss: 0.01829895, Validation loss: 0.01915832, Gradient norm: 0.25603218
INFO:root:[   97] Training loss: 0.01827489, Validation loss: 0.02040987, Gradient norm: 0.29350434
INFO:root:[   98] Training loss: 0.01953194, Validation loss: 0.01970765, Gradient norm: 0.45310366
INFO:root:[   99] Training loss: 0.01868014, Validation loss: 0.01796480, Gradient norm: 0.34434057
INFO:root:[  100] Training loss: 0.01830478, Validation loss: 0.01942094, Gradient norm: 0.29821498
INFO:root:[  101] Training loss: 0.01960621, Validation loss: 0.01907834, Gradient norm: 0.47574684
INFO:root:[  102] Training loss: 0.01842829, Validation loss: 0.01829122, Gradient norm: 0.30518371
INFO:root:[  103] Training loss: 0.01758025, Validation loss: 0.01804746, Gradient norm: 0.27357432
INFO:root:[  104] Training loss: 0.01751264, Validation loss: 0.01870712, Gradient norm: 0.27602154
INFO:root:[  105] Training loss: 0.01883089, Validation loss: 0.01797293, Gradient norm: 0.44870098
INFO:root:[  106] Training loss: 0.01792280, Validation loss: 0.01888819, Gradient norm: 0.24292874
INFO:root:[  107] Training loss: 0.01861039, Validation loss: 0.01813405, Gradient norm: 0.33543931
INFO:root:[  108] Training loss: 0.01846903, Validation loss: 0.01805632, Gradient norm: 0.40540949
INFO:root:EP 108: Early stopping
INFO:root:Training the model took 560.464s.
INFO:root:Emptying the cuda cache took 0.014s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.72243
INFO:root:EnergyScoreTrain: 0.55374
INFO:root:CoverageTrain: 0.98061
INFO:root:IntervalWidthTrain: 0.13654
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.75806
INFO:root:EnergyScoreValidation: 0.57813
INFO:root:CoverageValidation: 0.97835
INFO:root:IntervalWidthValidation: 0.13689
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.1764
INFO:root:EnergyScoreTest: 1.59758
INFO:root:CoverageTest: 0.757
INFO:root:IntervalWidthTest: 0.17179
INFO:root:###3 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08343213, Validation loss: 0.06225344, Gradient norm: 0.81954400
INFO:root:[    2] Training loss: 0.05474172, Validation loss: 0.05222950, Gradient norm: 0.32720549
INFO:root:[    3] Training loss: 0.04841268, Validation loss: 0.04789012, Gradient norm: 0.65993640
INFO:root:[    4] Training loss: 0.04362714, Validation loss: 0.04161233, Gradient norm: 0.43229757
INFO:root:[    5] Training loss: 0.04022400, Validation loss: 0.03933026, Gradient norm: 0.33506594
INFO:root:[    6] Training loss: 0.03933977, Validation loss: 0.03931375, Gradient norm: 0.46362343
INFO:root:[    7] Training loss: 0.03747468, Validation loss: 0.03476124, Gradient norm: 0.52466555
INFO:root:[    8] Training loss: 0.03609676, Validation loss: 0.03428197, Gradient norm: 0.51350699
INFO:root:[    9] Training loss: 0.03457962, Validation loss: 0.03404213, Gradient norm: 0.38484470
INFO:root:[   10] Training loss: 0.03377369, Validation loss: 0.03337199, Gradient norm: 0.39318127
INFO:root:[   11] Training loss: 0.03476607, Validation loss: 0.03253909, Gradient norm: 0.57420207
INFO:root:[   12] Training loss: 0.03384521, Validation loss: 0.03186222, Gradient norm: 0.57342391
INFO:root:[   13] Training loss: 0.03209934, Validation loss: 0.03211515, Gradient norm: 0.35791274
INFO:root:[   14] Training loss: 0.03181998, Validation loss: 0.03190851, Gradient norm: 0.28829938
INFO:root:[   15] Training loss: 0.03164891, Validation loss: 0.03104728, Gradient norm: 0.37214625
INFO:root:[   16] Training loss: 0.03166182, Validation loss: 0.03022896, Gradient norm: 0.37399954
INFO:root:[   17] Training loss: 0.03110679, Validation loss: 0.03038713, Gradient norm: 0.37970593
INFO:root:[   18] Training loss: 0.02980970, Validation loss: 0.03341619, Gradient norm: 0.31772737
INFO:root:[   19] Training loss: 0.03263349, Validation loss: 0.03231228, Gradient norm: 0.58766091
INFO:root:[   20] Training loss: 0.03041874, Validation loss: 0.02957395, Gradient norm: 0.45850291
INFO:root:[   21] Training loss: 0.03013208, Validation loss: 0.03234427, Gradient norm: 0.33513379
INFO:root:[   22] Training loss: 0.02993470, Validation loss: 0.02891106, Gradient norm: 0.33445161
INFO:root:[   23] Training loss: 0.02919169, Validation loss: 0.02875972, Gradient norm: 0.24666463
INFO:root:[   24] Training loss: 0.02860761, Validation loss: 0.02882416, Gradient norm: 0.32877706
INFO:root:[   25] Training loss: 0.02946533, Validation loss: 0.02846899, Gradient norm: 0.46445208
INFO:root:[   26] Training loss: 0.02848876, Validation loss: 0.02956816, Gradient norm: 0.26244168
INFO:root:[   27] Training loss: 0.02770730, Validation loss: 0.02715174, Gradient norm: 0.29831268
INFO:root:[   28] Training loss: 0.02758116, Validation loss: 0.02757674, Gradient norm: 0.35596711
INFO:root:[   29] Training loss: 0.02781126, Validation loss: 0.02775917, Gradient norm: 0.34265582
INFO:root:[   30] Training loss: 0.02803928, Validation loss: 0.02705197, Gradient norm: 0.36884641
INFO:root:[   31] Training loss: 0.02748179, Validation loss: 0.02650141, Gradient norm: 0.45025094
INFO:root:[   32] Training loss: 0.02837255, Validation loss: 0.02775782, Gradient norm: 0.47786907
INFO:root:[   33] Training loss: 0.02677634, Validation loss: 0.02603173, Gradient norm: 0.28458947
INFO:root:[   34] Training loss: 0.02553829, Validation loss: 0.02728760, Gradient norm: 0.27544533
INFO:root:[   35] Training loss: 0.02703645, Validation loss: 0.02623457, Gradient norm: 0.38991502
INFO:root:[   36] Training loss: 0.02707347, Validation loss: 0.02495806, Gradient norm: 0.54599344
INFO:root:[   37] Training loss: 0.02652755, Validation loss: 0.02560402, Gradient norm: 0.41505963
INFO:root:[   38] Training loss: 0.02587966, Validation loss: 0.02495455, Gradient norm: 0.34039223
INFO:root:[   39] Training loss: 0.02513232, Validation loss: 0.02664120, Gradient norm: 0.37933859
INFO:root:[   40] Training loss: 0.02562263, Validation loss: 0.02358401, Gradient norm: 0.33454288
INFO:root:[   41] Training loss: 0.02449196, Validation loss: 0.02609182, Gradient norm: 0.30749319
INFO:root:[   42] Training loss: 0.02498743, Validation loss: 0.02560832, Gradient norm: 0.36752175
INFO:root:[   43] Training loss: 0.02476504, Validation loss: 0.02415789, Gradient norm: 0.36747869
INFO:root:[   44] Training loss: 0.02445186, Validation loss: 0.02422187, Gradient norm: 0.34786433
INFO:root:[   45] Training loss: 0.02456750, Validation loss: 0.02331203, Gradient norm: 0.31899602
INFO:root:[   46] Training loss: 0.02329221, Validation loss: 0.02326569, Gradient norm: 0.22322247
INFO:root:[   47] Training loss: 0.02257041, Validation loss: 0.02321660, Gradient norm: 0.24789068
INFO:root:[   48] Training loss: 0.02326777, Validation loss: 0.02335777, Gradient norm: 0.26315656
INFO:root:[   49] Training loss: 0.02435634, Validation loss: 0.02334861, Gradient norm: 0.41697198
INFO:root:[   50] Training loss: 0.02356323, Validation loss: 0.02493405, Gradient norm: 0.34352494
INFO:root:[   51] Training loss: 0.02450734, Validation loss: 0.02263381, Gradient norm: 0.43074751
INFO:root:[   52] Training loss: 0.02256574, Validation loss: 0.02291959, Gradient norm: 0.28559236
INFO:root:[   53] Training loss: 0.02264261, Validation loss: 0.02230317, Gradient norm: 0.31186112
INFO:root:[   54] Training loss: 0.02208183, Validation loss: 0.02232922, Gradient norm: 0.27431147
INFO:root:[   55] Training loss: 0.02410285, Validation loss: 0.02220751, Gradient norm: 0.48710213
INFO:root:[   56] Training loss: 0.02194451, Validation loss: 0.02204018, Gradient norm: 0.32866490
INFO:root:[   57] Training loss: 0.02255568, Validation loss: 0.02275295, Gradient norm: 0.34383891
INFO:root:[   58] Training loss: 0.02234017, Validation loss: 0.02197939, Gradient norm: 0.35200418
INFO:root:[   59] Training loss: 0.02165575, Validation loss: 0.02178883, Gradient norm: 0.31397765
INFO:root:[   60] Training loss: 0.02179726, Validation loss: 0.02425988, Gradient norm: 0.27958035
INFO:root:[   61] Training loss: 0.02239828, Validation loss: 0.02166145, Gradient norm: 0.36663371
INFO:root:[   62] Training loss: 0.02108201, Validation loss: 0.02170059, Gradient norm: 0.22892999
INFO:root:[   63] Training loss: 0.02239643, Validation loss: 0.02297776, Gradient norm: 0.45036723
INFO:root:[   64] Training loss: 0.02256482, Validation loss: 0.02177504, Gradient norm: 0.42158296
INFO:root:[   65] Training loss: 0.02159340, Validation loss: 0.02154272, Gradient norm: 0.34034296
INFO:root:[   66] Training loss: 0.02213015, Validation loss: 0.02436622, Gradient norm: 0.43952797
INFO:root:[   67] Training loss: 0.02129902, Validation loss: 0.02084184, Gradient norm: 0.28562226
INFO:root:[   68] Training loss: 0.02050615, Validation loss: 0.02058350, Gradient norm: 0.21842824
INFO:root:[   69] Training loss: 0.02123867, Validation loss: 0.02126434, Gradient norm: 0.26292554
INFO:root:[   70] Training loss: 0.02131933, Validation loss: 0.02163929, Gradient norm: 0.35525878
INFO:root:[   71] Training loss: 0.02179422, Validation loss: 0.02108752, Gradient norm: 0.38785583
INFO:root:[   72] Training loss: 0.02078618, Validation loss: 0.02102651, Gradient norm: 0.26243732
INFO:root:[   73] Training loss: 0.02106388, Validation loss: 0.02052495, Gradient norm: 0.30399521
INFO:root:[   74] Training loss: 0.02023628, Validation loss: 0.02088746, Gradient norm: 0.26268318
INFO:root:[   75] Training loss: 0.02176560, Validation loss: 0.02041153, Gradient norm: 0.47821160
INFO:root:[   76] Training loss: 0.02085242, Validation loss: 0.02105734, Gradient norm: 0.33957062
INFO:root:[   77] Training loss: 0.02109092, Validation loss: 0.02024282, Gradient norm: 0.34762316
INFO:root:[   78] Training loss: 0.02076916, Validation loss: 0.02057463, Gradient norm: 0.32203031
INFO:root:[   79] Training loss: 0.02075301, Validation loss: 0.02069147, Gradient norm: 0.26718912
INFO:root:[   80] Training loss: 0.02114179, Validation loss: 0.02087304, Gradient norm: 0.31502967
INFO:root:[   81] Training loss: 0.02042184, Validation loss: 0.01985009, Gradient norm: 0.26868944
INFO:root:[   82] Training loss: 0.02003063, Validation loss: 0.02086231, Gradient norm: 0.31864107
INFO:root:[   83] Training loss: 0.02023734, Validation loss: 0.02006043, Gradient norm: 0.29243527
INFO:root:[   84] Training loss: 0.02006005, Validation loss: 0.02082225, Gradient norm: 0.25797230
INFO:root:[   85] Training loss: 0.02023359, Validation loss: 0.02028919, Gradient norm: 0.39806793
INFO:root:[   86] Training loss: 0.02067735, Validation loss: 0.02207811, Gradient norm: 0.34698848
INFO:root:[   87] Training loss: 0.02002205, Validation loss: 0.02203879, Gradient norm: 0.26699999
INFO:root:[   88] Training loss: 0.02153473, Validation loss: 0.01970677, Gradient norm: 0.37034328
INFO:root:[   89] Training loss: 0.01993435, Validation loss: 0.02026179, Gradient norm: 0.31896993
INFO:root:[   90] Training loss: 0.01959506, Validation loss: 0.02023296, Gradient norm: 0.28408272
INFO:root:[   91] Training loss: 0.02023338, Validation loss: 0.01982100, Gradient norm: 0.35327085
INFO:root:[   92] Training loss: 0.01986974, Validation loss: 0.01991935, Gradient norm: 0.26421330
INFO:root:[   93] Training loss: 0.02021427, Validation loss: 0.02397259, Gradient norm: 0.35425592
INFO:root:[   94] Training loss: 0.02085650, Validation loss: 0.02256334, Gradient norm: 0.47914635
INFO:root:[   95] Training loss: 0.02032088, Validation loss: 0.02048587, Gradient norm: 0.40512644
INFO:root:[   96] Training loss: 0.02011797, Validation loss: 0.01953073, Gradient norm: 0.36743056
INFO:root:[   97] Training loss: 0.02050736, Validation loss: 0.02060035, Gradient norm: 0.40930813
INFO:root:[   98] Training loss: 0.01962031, Validation loss: 0.01996789, Gradient norm: 0.34720620
INFO:root:[   99] Training loss: 0.01928398, Validation loss: 0.02032188, Gradient norm: 0.33495410
INFO:root:[  100] Training loss: 0.01956787, Validation loss: 0.01923439, Gradient norm: 0.31090455
INFO:root:[  101] Training loss: 0.01919263, Validation loss: 0.01910861, Gradient norm: 0.23861593
INFO:root:[  102] Training loss: 0.01910570, Validation loss: 0.01880583, Gradient norm: 0.29617165
INFO:root:[  103] Training loss: 0.01968644, Validation loss: 0.02030035, Gradient norm: 0.42966380
INFO:root:[  104] Training loss: 0.01947164, Validation loss: 0.02130190, Gradient norm: 0.32615020
INFO:root:[  105] Training loss: 0.01991967, Validation loss: 0.01913394, Gradient norm: 0.39850386
INFO:root:[  106] Training loss: 0.01945834, Validation loss: 0.01934630, Gradient norm: 0.31318852
INFO:root:[  107] Training loss: 0.01961759, Validation loss: 0.02186771, Gradient norm: 0.38142894
INFO:root:[  108] Training loss: 0.01919080, Validation loss: 0.01954284, Gradient norm: 0.37524708
INFO:root:[  109] Training loss: 0.01883306, Validation loss: 0.01912299, Gradient norm: 0.28337373
INFO:root:[  110] Training loss: 0.01906442, Validation loss: 0.01895018, Gradient norm: 0.31138865
INFO:root:[  111] Training loss: 0.01944709, Validation loss: 0.02112084, Gradient norm: 0.30004573
INFO:root:EP 111: Early stopping
INFO:root:Training the model took 576.104s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.73257
INFO:root:EnergyScoreTrain: 0.583
INFO:root:CoverageTrain: 0.98546
INFO:root:IntervalWidthTrain: 0.14945
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.76654
INFO:root:EnergyScoreValidation: 0.60252
INFO:root:CoverageValidation: 0.98269
INFO:root:IntervalWidthValidation: 0.14984
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.74016
INFO:root:EnergyScoreTest: 1.25237
INFO:root:CoverageTest: 0.86693
INFO:root:IntervalWidthTest: 0.16939
INFO:root:###4 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07363621, Validation loss: 0.06702582, Gradient norm: 0.36295804
INFO:root:[    2] Training loss: 0.05863837, Validation loss: 0.05312029, Gradient norm: 0.29025234
INFO:root:[    3] Training loss: 0.05059653, Validation loss: 0.04654601, Gradient norm: 0.44519164
INFO:root:[    4] Training loss: 0.04340031, Validation loss: 0.04077027, Gradient norm: 0.48006504
INFO:root:[    5] Training loss: 0.03812844, Validation loss: 0.03655899, Gradient norm: 0.34047764
INFO:root:[    6] Training loss: 0.03728881, Validation loss: 0.03510119, Gradient norm: 0.44457342
INFO:root:[    7] Training loss: 0.03641172, Validation loss: 0.03553998, Gradient norm: 0.39077816
INFO:root:[    8] Training loss: 0.03463087, Validation loss: 0.03312104, Gradient norm: 0.41675905
INFO:root:[    9] Training loss: 0.03424558, Validation loss: 0.03451780, Gradient norm: 0.28235648
INFO:root:[   10] Training loss: 0.03304972, Validation loss: 0.03277528, Gradient norm: 0.45067170
INFO:root:[   11] Training loss: 0.03252334, Validation loss: 0.03149813, Gradient norm: 0.42993980
INFO:root:[   12] Training loss: 0.03194034, Validation loss: 0.03050392, Gradient norm: 0.36237720
INFO:root:[   13] Training loss: 0.03146553, Validation loss: 0.02971057, Gradient norm: 0.44106530
INFO:root:[   14] Training loss: 0.03046523, Validation loss: 0.02994379, Gradient norm: 0.32446730
INFO:root:[   15] Training loss: 0.03062903, Validation loss: 0.02964428, Gradient norm: 0.41046809
INFO:root:[   16] Training loss: 0.02980916, Validation loss: 0.03174052, Gradient norm: 0.47761631
INFO:root:[   17] Training loss: 0.03006667, Validation loss: 0.02981473, Gradient norm: 0.40592971
INFO:root:[   18] Training loss: 0.02969797, Validation loss: 0.03160875, Gradient norm: 0.45504374
INFO:root:[   19] Training loss: 0.02968948, Validation loss: 0.02888821, Gradient norm: 0.48816034
INFO:root:[   20] Training loss: 0.02806729, Validation loss: 0.03075360, Gradient norm: 0.29382475
INFO:root:[   21] Training loss: 0.02908615, Validation loss: 0.02707633, Gradient norm: 0.48085295
INFO:root:[   22] Training loss: 0.02742734, Validation loss: 0.02753640, Gradient norm: 0.28849316
INFO:root:[   23] Training loss: 0.02732579, Validation loss: 0.02731875, Gradient norm: 0.27173242
INFO:root:[   24] Training loss: 0.02761373, Validation loss: 0.02807204, Gradient norm: 0.33101978
INFO:root:[   25] Training loss: 0.02781120, Validation loss: 0.02750290, Gradient norm: 0.32571270
INFO:root:[   26] Training loss: 0.02636598, Validation loss: 0.02680197, Gradient norm: 0.30804987
INFO:root:[   27] Training loss: 0.02621986, Validation loss: 0.02667107, Gradient norm: 0.33793427
INFO:root:[   28] Training loss: 0.02799885, Validation loss: 0.02688158, Gradient norm: 0.53266496
INFO:root:[   29] Training loss: 0.02607194, Validation loss: 0.02565623, Gradient norm: 0.27370441
INFO:root:[   30] Training loss: 0.02614998, Validation loss: 0.02617248, Gradient norm: 0.30205040
INFO:root:[   31] Training loss: 0.02613183, Validation loss: 0.02664815, Gradient norm: 0.34819958
INFO:root:[   32] Training loss: 0.02766962, Validation loss: 0.02554768, Gradient norm: 0.49111964
INFO:root:[   33] Training loss: 0.02595017, Validation loss: 0.02654466, Gradient norm: 0.26606488
INFO:root:[   34] Training loss: 0.02659041, Validation loss: 0.02629315, Gradient norm: 0.39337415
INFO:root:[   35] Training loss: 0.02566887, Validation loss: 0.02641738, Gradient norm: 0.31464115
INFO:root:[   36] Training loss: 0.02560527, Validation loss: 0.02541486, Gradient norm: 0.29761885
INFO:root:[   37] Training loss: 0.02583382, Validation loss: 0.02476702, Gradient norm: 0.32164724
INFO:root:[   38] Training loss: 0.02517970, Validation loss: 0.02511297, Gradient norm: 0.21892083
INFO:root:[   39] Training loss: 0.02523844, Validation loss: 0.02508150, Gradient norm: 0.34027203
INFO:root:[   40] Training loss: 0.02466553, Validation loss: 0.02470414, Gradient norm: 0.28560373
INFO:root:[   41] Training loss: 0.02495360, Validation loss: 0.02465531, Gradient norm: 0.30686183
INFO:root:[   42] Training loss: 0.02486371, Validation loss: 0.02401733, Gradient norm: 0.29092948
INFO:root:[   43] Training loss: 0.02449817, Validation loss: 0.02385561, Gradient norm: 0.33755367
INFO:root:[   44] Training loss: 0.02489931, Validation loss: 0.02542500, Gradient norm: 0.32799013
INFO:root:[   45] Training loss: 0.02495484, Validation loss: 0.02387608, Gradient norm: 0.48814958
INFO:root:[   46] Training loss: 0.02464610, Validation loss: 0.02404176, Gradient norm: 0.35237183
INFO:root:[   47] Training loss: 0.02484328, Validation loss: 0.02421081, Gradient norm: 0.33386186
INFO:root:[   48] Training loss: 0.02393996, Validation loss: 0.02353975, Gradient norm: 0.29014591
INFO:root:[   49] Training loss: 0.02382761, Validation loss: 0.02452300, Gradient norm: 0.37965532
INFO:root:[   50] Training loss: 0.02349755, Validation loss: 0.02411170, Gradient norm: 0.32556813
INFO:root:[   51] Training loss: 0.02333235, Validation loss: 0.02553584, Gradient norm: 0.26376044
INFO:root:[   52] Training loss: 0.02374540, Validation loss: 0.02581677, Gradient norm: 0.36115044
INFO:root:[   53] Training loss: 0.02427064, Validation loss: 0.02259574, Gradient norm: 0.36214682
INFO:root:[   54] Training loss: 0.02410624, Validation loss: 0.02404499, Gradient norm: 0.39602923
INFO:root:[   55] Training loss: 0.02345624, Validation loss: 0.02372001, Gradient norm: 0.27211845
INFO:root:[   56] Training loss: 0.02303307, Validation loss: 0.02250355, Gradient norm: 0.26636146
INFO:root:[   57] Training loss: 0.02298966, Validation loss: 0.02368310, Gradient norm: 0.34999371
INFO:root:[   58] Training loss: 0.02277892, Validation loss: 0.02248711, Gradient norm: 0.29130016
INFO:root:[   59] Training loss: 0.02376547, Validation loss: 0.02270470, Gradient norm: 0.31080377
INFO:root:[   60] Training loss: 0.02329615, Validation loss: 0.02242576, Gradient norm: 0.36224133
INFO:root:[   61] Training loss: 0.02241606, Validation loss: 0.02198490, Gradient norm: 0.33304296
INFO:root:[   62] Training loss: 0.02249895, Validation loss: 0.02225902, Gradient norm: 0.28591399
INFO:root:[   63] Training loss: 0.02258548, Validation loss: 0.02224967, Gradient norm: 0.25116612
INFO:root:[   64] Training loss: 0.02322243, Validation loss: 0.02260848, Gradient norm: 0.37388427
INFO:root:[   65] Training loss: 0.02247438, Validation loss: 0.02294241, Gradient norm: 0.26628279
INFO:root:[   66] Training loss: 0.02304774, Validation loss: 0.02332142, Gradient norm: 0.47854627
INFO:root:[   67] Training loss: 0.02240040, Validation loss: 0.02204212, Gradient norm: 0.31200204
INFO:root:[   68] Training loss: 0.02187145, Validation loss: 0.02171955, Gradient norm: 0.26030535
INFO:root:[   69] Training loss: 0.02196701, Validation loss: 0.02211452, Gradient norm: 0.30376879
INFO:root:[   70] Training loss: 0.02187921, Validation loss: 0.02212827, Gradient norm: 0.26797678
INFO:root:[   71] Training loss: 0.02182134, Validation loss: 0.02184665, Gradient norm: 0.30000635
INFO:root:[   72] Training loss: 0.02172515, Validation loss: 0.02170217, Gradient norm: 0.30072726
INFO:root:[   73] Training loss: 0.02179712, Validation loss: 0.02238190, Gradient norm: 0.34551176
INFO:root:[   74] Training loss: 0.02136386, Validation loss: 0.02172126, Gradient norm: 0.26516311
INFO:root:[   75] Training loss: 0.02095052, Validation loss: 0.02147457, Gradient norm: 0.28847566
INFO:root:[   76] Training loss: 0.02116947, Validation loss: 0.02134027, Gradient norm: 0.28527645
INFO:root:[   77] Training loss: 0.02146502, Validation loss: 0.02257834, Gradient norm: 0.33641692
INFO:root:[   78] Training loss: 0.02100439, Validation loss: 0.02151509, Gradient norm: 0.34060866
INFO:root:[   79] Training loss: 0.02211774, Validation loss: 0.02103656, Gradient norm: 0.30939267
INFO:root:[   80] Training loss: 0.02156937, Validation loss: 0.02239915, Gradient norm: 0.32970129
INFO:root:[   81] Training loss: 0.02244500, Validation loss: 0.02068399, Gradient norm: 0.48255523
INFO:root:[   82] Training loss: 0.02133376, Validation loss: 0.02115893, Gradient norm: 0.26287491
INFO:root:[   83] Training loss: 0.02165863, Validation loss: 0.02069646, Gradient norm: 0.25476998
INFO:root:[   84] Training loss: 0.02076854, Validation loss: 0.02086517, Gradient norm: 0.31621395
INFO:root:[   85] Training loss: 0.02141650, Validation loss: 0.02054034, Gradient norm: 0.33967232
INFO:root:[   86] Training loss: 0.02119533, Validation loss: 0.02073826, Gradient norm: 0.34463240
INFO:root:[   87] Training loss: 0.02144560, Validation loss: 0.02271527, Gradient norm: 0.33454856
INFO:root:[   88] Training loss: 0.02112051, Validation loss: 0.02048834, Gradient norm: 0.23845406
INFO:root:[   89] Training loss: 0.02079883, Validation loss: 0.02028555, Gradient norm: 0.29997877
INFO:root:[   90] Training loss: 0.02119081, Validation loss: 0.02055047, Gradient norm: 0.31571684
INFO:root:[   91] Training loss: 0.02102405, Validation loss: 0.02070532, Gradient norm: 0.35656340
INFO:root:[   92] Training loss: 0.02072459, Validation loss: 0.02045923, Gradient norm: 0.27269421
INFO:root:[   93] Training loss: 0.02102190, Validation loss: 0.02085565, Gradient norm: 0.42312830
INFO:root:[   94] Training loss: 0.02047220, Validation loss: 0.02044187, Gradient norm: 0.28916772
INFO:root:[   95] Training loss: 0.02095279, Validation loss: 0.02089313, Gradient norm: 0.33676345
INFO:root:[   96] Training loss: 0.01981129, Validation loss: 0.01988622, Gradient norm: 0.26424642
INFO:root:[   97] Training loss: 0.01999325, Validation loss: 0.02050158, Gradient norm: 0.25897569
INFO:root:[   98] Training loss: 0.02037874, Validation loss: 0.02011221, Gradient norm: 0.35737625
INFO:root:[   99] Training loss: 0.02005410, Validation loss: 0.02157045, Gradient norm: 0.25403714
INFO:root:[  100] Training loss: 0.02093304, Validation loss: 0.02007144, Gradient norm: 0.39315723
INFO:root:[  101] Training loss: 0.02019218, Validation loss: 0.02060117, Gradient norm: 0.35569930
INFO:root:[  102] Training loss: 0.02002532, Validation loss: 0.01992884, Gradient norm: 0.32484844
INFO:root:[  103] Training loss: 0.01975682, Validation loss: 0.02020804, Gradient norm: 0.27724770
INFO:root:[  104] Training loss: 0.01965100, Validation loss: 0.02012421, Gradient norm: 0.24558787
INFO:root:[  105] Training loss: 0.02032697, Validation loss: 0.02079050, Gradient norm: 0.39019019
INFO:root:EP 105: Early stopping
INFO:root:Training the model took 545.692s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.78266
INFO:root:EnergyScoreTrain: 0.61857
INFO:root:CoverageTrain: 0.98107
INFO:root:IntervalWidthTrain: 0.15687
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.80951
INFO:root:EnergyScoreValidation: 0.63537
INFO:root:CoverageValidation: 0.97926
INFO:root:IntervalWidthValidation: 0.15747
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.52467
INFO:root:EnergyScoreTest: 1.07332
INFO:root:CoverageTest: 0.93818
INFO:root:IntervalWidthTest: 0.18222
INFO:root:###5 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07495720, Validation loss: 0.06675984, Gradient norm: 0.29714771
INFO:root:[    2] Training loss: 0.06269983, Validation loss: 0.06267382, Gradient norm: 0.23438539
INFO:root:[    3] Training loss: 0.05652572, Validation loss: 0.05509241, Gradient norm: 0.24367259
INFO:root:[    4] Training loss: 0.05047634, Validation loss: 0.04863746, Gradient norm: 0.20491783
INFO:root:[    5] Training loss: 0.04809854, Validation loss: 0.04792911, Gradient norm: 0.43422573
INFO:root:[    6] Training loss: 0.04257493, Validation loss: 0.03922845, Gradient norm: 0.35951107
INFO:root:[    7] Training loss: 0.03738005, Validation loss: 0.03570697, Gradient norm: 0.24481613
INFO:root:[    8] Training loss: 0.03702802, Validation loss: 0.03645444, Gradient norm: 0.40416271
INFO:root:[    9] Training loss: 0.03603219, Validation loss: 0.03320476, Gradient norm: 0.39573876
INFO:root:[   10] Training loss: 0.03464509, Validation loss: 0.03738897, Gradient norm: 0.38736933
INFO:root:[   11] Training loss: 0.03411000, Validation loss: 0.03247680, Gradient norm: 0.36255973
INFO:root:[   12] Training loss: 0.03274710, Validation loss: 0.03082353, Gradient norm: 0.37993928
INFO:root:[   13] Training loss: 0.03196825, Validation loss: 0.03383826, Gradient norm: 0.34408402
INFO:root:[   14] Training loss: 0.03323009, Validation loss: 0.03145897, Gradient norm: 0.45689264
INFO:root:[   15] Training loss: 0.03074810, Validation loss: 0.03040490, Gradient norm: 0.26177181
INFO:root:[   16] Training loss: 0.02997577, Validation loss: 0.02957237, Gradient norm: 0.26991159
INFO:root:[   17] Training loss: 0.03001636, Validation loss: 0.02931948, Gradient norm: 0.21431422
INFO:root:[   18] Training loss: 0.02909437, Validation loss: 0.02878520, Gradient norm: 0.32253886
INFO:root:[   19] Training loss: 0.02906316, Validation loss: 0.02879314, Gradient norm: 0.30840800
INFO:root:[   20] Training loss: 0.02817906, Validation loss: 0.02836801, Gradient norm: 0.28971274
INFO:root:[   21] Training loss: 0.02856656, Validation loss: 0.02859741, Gradient norm: 0.28597210
INFO:root:[   22] Training loss: 0.02897269, Validation loss: 0.02998925, Gradient norm: 0.37979611
INFO:root:[   23] Training loss: 0.02841436, Validation loss: 0.02884867, Gradient norm: 0.38779239
INFO:root:[   24] Training loss: 0.02789932, Validation loss: 0.02873251, Gradient norm: 0.25422618
INFO:root:[   25] Training loss: 0.02823222, Validation loss: 0.02881265, Gradient norm: 0.36342947
INFO:root:[   26] Training loss: 0.02894470, Validation loss: 0.02900462, Gradient norm: 0.46784415
INFO:root:[   27] Training loss: 0.02761064, Validation loss: 0.02727072, Gradient norm: 0.32077403
INFO:root:[   28] Training loss: 0.02769512, Validation loss: 0.02659025, Gradient norm: 0.28623150
INFO:root:[   29] Training loss: 0.02650628, Validation loss: 0.02626411, Gradient norm: 0.26605257
INFO:root:[   30] Training loss: 0.02671764, Validation loss: 0.02595112, Gradient norm: 0.37113387
INFO:root:[   31] Training loss: 0.02630079, Validation loss: 0.02592155, Gradient norm: 0.32245824
INFO:root:[   32] Training loss: 0.02563604, Validation loss: 0.02569232, Gradient norm: 0.23718136
INFO:root:[   33] Training loss: 0.02650801, Validation loss: 0.02682894, Gradient norm: 0.29503707
INFO:root:[   34] Training loss: 0.02533191, Validation loss: 0.02579951, Gradient norm: 0.29490442
INFO:root:[   35] Training loss: 0.02514481, Validation loss: 0.02457253, Gradient norm: 0.27754273
INFO:root:[   36] Training loss: 0.02535375, Validation loss: 0.02516510, Gradient norm: 0.24823241
INFO:root:[   37] Training loss: 0.02536236, Validation loss: 0.02575425, Gradient norm: 0.28998633
INFO:root:[   38] Training loss: 0.02515084, Validation loss: 0.02501132, Gradient norm: 0.32537058
INFO:root:[   39] Training loss: 0.02508802, Validation loss: 0.02530235, Gradient norm: 0.27047807
INFO:root:[   40] Training loss: 0.02469464, Validation loss: 0.02559545, Gradient norm: 0.28005944
INFO:root:[   41] Training loss: 0.02504619, Validation loss: 0.02486283, Gradient norm: 0.31884733
INFO:root:[   42] Training loss: 0.02497915, Validation loss: 0.02541661, Gradient norm: 0.36984947
INFO:root:[   43] Training loss: 0.02541864, Validation loss: 0.02378725, Gradient norm: 0.27185935
INFO:root:[   44] Training loss: 0.02489732, Validation loss: 0.02460311, Gradient norm: 0.31856801
INFO:root:[   45] Training loss: 0.02498084, Validation loss: 0.02586012, Gradient norm: 0.31465153
INFO:root:[   46] Training loss: 0.02434871, Validation loss: 0.02425661, Gradient norm: 0.28935939
INFO:root:[   47] Training loss: 0.02411471, Validation loss: 0.02397261, Gradient norm: 0.33881251
INFO:root:[   48] Training loss: 0.02396614, Validation loss: 0.02391430, Gradient norm: 0.28366524
INFO:root:[   49] Training loss: 0.02395969, Validation loss: 0.02535975, Gradient norm: 0.32430511
INFO:root:[   50] Training loss: 0.02381711, Validation loss: 0.02347195, Gradient norm: 0.28387523
INFO:root:[   51] Training loss: 0.02418459, Validation loss: 0.02412008, Gradient norm: 0.33992399
INFO:root:[   52] Training loss: 0.02413387, Validation loss: 0.02383264, Gradient norm: 0.33820928
INFO:root:[   53] Training loss: 0.02392388, Validation loss: 0.02337484, Gradient norm: 0.25918837
INFO:root:[   54] Training loss: 0.02311122, Validation loss: 0.02496261, Gradient norm: 0.23860716
INFO:root:[   55] Training loss: 0.02358313, Validation loss: 0.02421400, Gradient norm: 0.27585089
INFO:root:[   56] Training loss: 0.02355529, Validation loss: 0.02345629, Gradient norm: 0.35923127
INFO:root:[   57] Training loss: 0.02293055, Validation loss: 0.02308345, Gradient norm: 0.26893603
INFO:root:[   58] Training loss: 0.02311265, Validation loss: 0.02369090, Gradient norm: 0.21603704
INFO:root:[   59] Training loss: 0.02328411, Validation loss: 0.02281402, Gradient norm: 0.32042134
INFO:root:[   60] Training loss: 0.02290089, Validation loss: 0.02299174, Gradient norm: 0.32195746
INFO:root:[   61] Training loss: 0.02307368, Validation loss: 0.02264384, Gradient norm: 0.28295932
INFO:root:[   62] Training loss: 0.02262197, Validation loss: 0.02249912, Gradient norm: 0.27238550
INFO:root:[   63] Training loss: 0.02254243, Validation loss: 0.02263378, Gradient norm: 0.31030382
INFO:root:[   64] Training loss: 0.02335900, Validation loss: 0.02261989, Gradient norm: 0.25424700
INFO:root:[   65] Training loss: 0.02199650, Validation loss: 0.02194698, Gradient norm: 0.25032126
INFO:root:[   66] Training loss: 0.02307973, Validation loss: 0.02227026, Gradient norm: 0.28209053
INFO:root:[   67] Training loss: 0.02303963, Validation loss: 0.02257596, Gradient norm: 0.32959233
INFO:root:[   68] Training loss: 0.02205008, Validation loss: 0.02256199, Gradient norm: 0.28322441
INFO:root:[   69] Training loss: 0.02198917, Validation loss: 0.02234519, Gradient norm: 0.24337276
INFO:root:[   70] Training loss: 0.02212123, Validation loss: 0.02197860, Gradient norm: 0.25221056
INFO:root:[   71] Training loss: 0.02240094, Validation loss: 0.02217377, Gradient norm: 0.21932853
INFO:root:[   72] Training loss: 0.02226794, Validation loss: 0.02460948, Gradient norm: 0.29214105
INFO:root:[   73] Training loss: 0.02208234, Validation loss: 0.02348374, Gradient norm: 0.32557905
INFO:root:[   74] Training loss: 0.02267345, Validation loss: 0.02171709, Gradient norm: 0.28759961
INFO:root:[   75] Training loss: 0.02127960, Validation loss: 0.02181441, Gradient norm: 0.28196059
INFO:root:[   76] Training loss: 0.02208463, Validation loss: 0.02184522, Gradient norm: 0.25852004
INFO:root:[   77] Training loss: 0.02238112, Validation loss: 0.02207617, Gradient norm: 0.26619407
INFO:root:[   78] Training loss: 0.02163938, Validation loss: 0.02225497, Gradient norm: 0.32244585
INFO:root:[   79] Training loss: 0.02225690, Validation loss: 0.02240414, Gradient norm: 0.29743983
INFO:root:[   80] Training loss: 0.02198473, Validation loss: 0.02160341, Gradient norm: 0.29097954
INFO:root:[   81] Training loss: 0.02151765, Validation loss: 0.02147653, Gradient norm: 0.32615874
INFO:root:[   82] Training loss: 0.02118143, Validation loss: 0.02149238, Gradient norm: 0.22926683
INFO:root:[   83] Training loss: 0.02176010, Validation loss: 0.02219156, Gradient norm: 0.32492044
INFO:root:[   84] Training loss: 0.02185857, Validation loss: 0.02120994, Gradient norm: 0.31309495
INFO:root:[   85] Training loss: 0.02172295, Validation loss: 0.02144980, Gradient norm: 0.20171041
INFO:root:[   86] Training loss: 0.02083928, Validation loss: 0.02131244, Gradient norm: 0.25691796
INFO:root:[   87] Training loss: 0.02103490, Validation loss: 0.02147385, Gradient norm: 0.24213775
INFO:root:[   88] Training loss: 0.02110295, Validation loss: 0.02228004, Gradient norm: 0.29854593
INFO:root:[   89] Training loss: 0.02171017, Validation loss: 0.02124818, Gradient norm: 0.25744276
INFO:root:[   90] Training loss: 0.02107894, Validation loss: 0.02151379, Gradient norm: 0.30969348
INFO:root:[   91] Training loss: 0.02128272, Validation loss: 0.02097107, Gradient norm: 0.25052988
INFO:root:[   92] Training loss: 0.02101983, Validation loss: 0.02082327, Gradient norm: 0.20100158
INFO:root:[   93] Training loss: 0.02102467, Validation loss: 0.02185617, Gradient norm: 0.22004169
INFO:root:[   94] Training loss: 0.02165749, Validation loss: 0.02266130, Gradient norm: 0.41827389
INFO:root:[   95] Training loss: 0.02131297, Validation loss: 0.02084270, Gradient norm: 0.32146328
INFO:root:[   96] Training loss: 0.02041563, Validation loss: 0.02116682, Gradient norm: 0.27840091
INFO:root:[   97] Training loss: 0.02064173, Validation loss: 0.02272548, Gradient norm: 0.29065135
INFO:root:[   98] Training loss: 0.02137188, Validation loss: 0.02034917, Gradient norm: 0.36691750
INFO:root:[   99] Training loss: 0.02043972, Validation loss: 0.02108484, Gradient norm: 0.22414440
INFO:root:[  100] Training loss: 0.02043302, Validation loss: 0.02098654, Gradient norm: 0.26038037
INFO:root:[  101] Training loss: 0.02020916, Validation loss: 0.02039479, Gradient norm: 0.26105674
INFO:root:[  102] Training loss: 0.02012640, Validation loss: 0.02044740, Gradient norm: 0.22144585
INFO:root:[  103] Training loss: 0.02009016, Validation loss: 0.02046147, Gradient norm: 0.19344687
INFO:root:[  104] Training loss: 0.02046129, Validation loss: 0.02024664, Gradient norm: 0.33964648
INFO:root:[  105] Training loss: 0.02034765, Validation loss: 0.02126821, Gradient norm: 0.24533110
INFO:root:[  106] Training loss: 0.02024191, Validation loss: 0.02060424, Gradient norm: 0.23865052
INFO:root:[  107] Training loss: 0.01954651, Validation loss: 0.02037700, Gradient norm: 0.20072353
INFO:root:[  108] Training loss: 0.02058641, Validation loss: 0.02039215, Gradient norm: 0.31904113
INFO:root:[  109] Training loss: 0.02048273, Validation loss: 0.02060505, Gradient norm: 0.24719542
INFO:root:[  110] Training loss: 0.02010138, Validation loss: 0.02025046, Gradient norm: 0.19876066
INFO:root:[  111] Training loss: 0.02032068, Validation loss: 0.02007468, Gradient norm: 0.25555582
INFO:root:[  112] Training loss: 0.01980875, Validation loss: 0.02073980, Gradient norm: 0.23148968
INFO:root:[  113] Training loss: 0.02019042, Validation loss: 0.01988452, Gradient norm: 0.32343517
INFO:root:[  114] Training loss: 0.02048202, Validation loss: 0.02089383, Gradient norm: 0.29223164
INFO:root:[  115] Training loss: 0.01982861, Validation loss: 0.02207912, Gradient norm: 0.25319703
INFO:root:[  116] Training loss: 0.01942288, Validation loss: 0.02078698, Gradient norm: 0.31573828
INFO:root:[  117] Training loss: 0.02056120, Validation loss: 0.01966733, Gradient norm: 0.39276989
INFO:root:[  118] Training loss: 0.01950862, Validation loss: 0.01996575, Gradient norm: 0.20078991
INFO:root:[  119] Training loss: 0.01927474, Validation loss: 0.01973107, Gradient norm: 0.19417893
INFO:root:[  120] Training loss: 0.01965060, Validation loss: 0.02028915, Gradient norm: 0.26531602
INFO:root:[  121] Training loss: 0.01996745, Validation loss: 0.01938587, Gradient norm: 0.27916424
INFO:root:[  122] Training loss: 0.01937943, Validation loss: 0.01981580, Gradient norm: 0.26324859
INFO:root:[  123] Training loss: 0.01938805, Validation loss: 0.01968820, Gradient norm: 0.18359922
INFO:root:[  124] Training loss: 0.01910937, Validation loss: 0.02014520, Gradient norm: 0.18740239
INFO:root:[  125] Training loss: 0.01955589, Validation loss: 0.02016718, Gradient norm: 0.24842801
INFO:root:[  126] Training loss: 0.01912117, Validation loss: 0.01985113, Gradient norm: 0.21417024
INFO:root:[  127] Training loss: 0.01972329, Validation loss: 0.01952685, Gradient norm: 0.37922803
INFO:root:[  128] Training loss: 0.01949327, Validation loss: 0.01953466, Gradient norm: 0.31244168
INFO:root:[  129] Training loss: 0.01935856, Validation loss: 0.01919633, Gradient norm: 0.21022015
INFO:root:[  130] Training loss: 0.02001741, Validation loss: 0.02037411, Gradient norm: 0.33812067
INFO:root:[  131] Training loss: 0.01993975, Validation loss: 0.01936928, Gradient norm: 0.32261995
INFO:root:[  132] Training loss: 0.02011908, Validation loss: 0.01961588, Gradient norm: 0.42355511
INFO:root:[  133] Training loss: 0.01908803, Validation loss: 0.01925125, Gradient norm: 0.22939396
INFO:root:[  134] Training loss: 0.01867980, Validation loss: 0.01951537, Gradient norm: 0.21725187
INFO:root:[  135] Training loss: 0.01874628, Validation loss: 0.01966484, Gradient norm: 0.22250111
INFO:root:[  136] Training loss: 0.01939620, Validation loss: 0.02124617, Gradient norm: 0.23755617
INFO:root:[  137] Training loss: 0.01946535, Validation loss: 0.02012967, Gradient norm: 0.36036908
INFO:root:[  138] Training loss: 0.01915453, Validation loss: 0.01961971, Gradient norm: 0.30169280
INFO:root:[  139] Training loss: 0.02003856, Validation loss: 0.01903789, Gradient norm: 0.28779875
INFO:root:[  140] Training loss: 0.01927495, Validation loss: 0.01905959, Gradient norm: 0.26339102
INFO:root:[  141] Training loss: 0.01886942, Validation loss: 0.01935598, Gradient norm: 0.19677850
INFO:root:[  142] Training loss: 0.01964190, Validation loss: 0.01927640, Gradient norm: 0.32222081
INFO:root:[  143] Training loss: 0.01902748, Validation loss: 0.01911106, Gradient norm: 0.23295570
INFO:root:[  144] Training loss: 0.01865255, Validation loss: 0.01927725, Gradient norm: 0.21057815
INFO:root:[  145] Training loss: 0.01866671, Validation loss: 0.01965507, Gradient norm: 0.21776478
INFO:root:[  146] Training loss: 0.01864772, Validation loss: 0.01880135, Gradient norm: 0.21814394
INFO:root:[  147] Training loss: 0.01916183, Validation loss: 0.01895816, Gradient norm: 0.39003465
INFO:root:[  148] Training loss: 0.01908240, Validation loss: 0.01925437, Gradient norm: 0.28426641
INFO:root:[  149] Training loss: 0.01847775, Validation loss: 0.01890582, Gradient norm: 0.24257212
INFO:root:[  150] Training loss: 0.01914897, Validation loss: 0.01912948, Gradient norm: 0.32998549
INFO:root:[  151] Training loss: 0.01904688, Validation loss: 0.02004046, Gradient norm: 0.25035585
INFO:root:[  152] Training loss: 0.01905452, Validation loss: 0.01938367, Gradient norm: 0.26945101
INFO:root:[  153] Training loss: 0.01852509, Validation loss: 0.01895885, Gradient norm: 0.22721484
INFO:root:[  154] Training loss: 0.01879503, Validation loss: 0.01923588, Gradient norm: 0.25531532
INFO:root:[  155] Training loss: 0.01941550, Validation loss: 0.01884312, Gradient norm: 0.29813636
INFO:root:EP 155: Early stopping
INFO:root:Training the model took 802.367s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.71606
INFO:root:EnergyScoreTrain: 0.57475
INFO:root:CoverageTrain: 0.98141
INFO:root:IntervalWidthTrain: 0.14476
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.74893
INFO:root:EnergyScoreValidation: 0.59556
INFO:root:CoverageValidation: 0.97854
INFO:root:IntervalWidthValidation: 0.14544
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.23273
INFO:root:EnergyScoreTest: 0.88702
INFO:root:CoverageTest: 0.93453
INFO:root:IntervalWidthTest: 0.15722
INFO:root:###6 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07947642, Validation loss: 0.06862232, Gradient norm: 0.27278400
INFO:root:[    2] Training loss: 0.06246046, Validation loss: 0.05902416, Gradient norm: 0.21063901
INFO:root:[    3] Training loss: 0.05471768, Validation loss: 0.05113734, Gradient norm: 0.24433579
INFO:root:[    4] Training loss: 0.04840672, Validation loss: 0.04500768, Gradient norm: 0.29951459
INFO:root:[    5] Training loss: 0.04279656, Validation loss: 0.04252431, Gradient norm: 0.26097926
INFO:root:[    6] Training loss: 0.04143319, Validation loss: 0.04042370, Gradient norm: 0.19840207
INFO:root:[    7] Training loss: 0.04021604, Validation loss: 0.03954113, Gradient norm: 0.36990860
INFO:root:[    8] Training loss: 0.03882973, Validation loss: 0.03778642, Gradient norm: 0.27830645
INFO:root:[    9] Training loss: 0.03822432, Validation loss: 0.03760247, Gradient norm: 0.23763242
INFO:root:[   10] Training loss: 0.03626737, Validation loss: 0.03680074, Gradient norm: 0.30000195
INFO:root:[   11] Training loss: 0.03627147, Validation loss: 0.03618225, Gradient norm: 0.29316356
INFO:root:[   12] Training loss: 0.03519061, Validation loss: 0.03656186, Gradient norm: 0.27697767
INFO:root:[   13] Training loss: 0.03459360, Validation loss: 0.03420227, Gradient norm: 0.27512079
INFO:root:[   14] Training loss: 0.03369474, Validation loss: 0.03390587, Gradient norm: 0.30243187
INFO:root:[   15] Training loss: 0.03469007, Validation loss: 0.03384237, Gradient norm: 0.27732572
INFO:root:[   16] Training loss: 0.03363954, Validation loss: 0.03273527, Gradient norm: 0.21276700
INFO:root:[   17] Training loss: 0.03261682, Validation loss: 0.03275964, Gradient norm: 0.22486001
INFO:root:[   18] Training loss: 0.03230042, Validation loss: 0.03264792, Gradient norm: 0.21385656
INFO:root:[   19] Training loss: 0.03266043, Validation loss: 0.03364040, Gradient norm: 0.33878190
INFO:root:[   20] Training loss: 0.03216266, Validation loss: 0.03200428, Gradient norm: 0.24732971
INFO:root:[   21] Training loss: 0.03213644, Validation loss: 0.03116539, Gradient norm: 0.29385490
INFO:root:[   22] Training loss: 0.03179972, Validation loss: 0.03179980, Gradient norm: 0.28759413
INFO:root:[   23] Training loss: 0.03195493, Validation loss: 0.03284669, Gradient norm: 0.38253791
INFO:root:[   24] Training loss: 0.03110763, Validation loss: 0.02993242, Gradient norm: 0.24575312
INFO:root:[   25] Training loss: 0.03062001, Validation loss: 0.02979788, Gradient norm: 0.26216308
INFO:root:[   26] Training loss: 0.02939236, Validation loss: 0.02960446, Gradient norm: 0.26973137
INFO:root:[   27] Training loss: 0.03022385, Validation loss: 0.02996353, Gradient norm: 0.23412550
INFO:root:[   28] Training loss: 0.02945001, Validation loss: 0.02945359, Gradient norm: 0.35895808
INFO:root:[   29] Training loss: 0.02934329, Validation loss: 0.02907370, Gradient norm: 0.27573119
INFO:root:[   30] Training loss: 0.02901338, Validation loss: 0.03077404, Gradient norm: 0.24118865
INFO:root:[   31] Training loss: 0.02967845, Validation loss: 0.02907245, Gradient norm: 0.30575764
INFO:root:[   32] Training loss: 0.02975065, Validation loss: 0.03039277, Gradient norm: 0.32030477
INFO:root:[   33] Training loss: 0.02958095, Validation loss: 0.02955105, Gradient norm: 0.30505711
INFO:root:[   34] Training loss: 0.02969139, Validation loss: 0.02880990, Gradient norm: 0.36056762
INFO:root:[   35] Training loss: 0.02883773, Validation loss: 0.02819472, Gradient norm: 0.30338814
INFO:root:[   36] Training loss: 0.02808646, Validation loss: 0.02804543, Gradient norm: 0.29903190
INFO:root:[   37] Training loss: 0.02764722, Validation loss: 0.02727279, Gradient norm: 0.26765485
INFO:root:[   38] Training loss: 0.02763059, Validation loss: 0.02814841, Gradient norm: 0.26732615
INFO:root:[   39] Training loss: 0.02841735, Validation loss: 0.02719113, Gradient norm: 0.31467679
INFO:root:[   40] Training loss: 0.02700908, Validation loss: 0.02699065, Gradient norm: 0.27897517
INFO:root:[   41] Training loss: 0.02671533, Validation loss: 0.02746389, Gradient norm: 0.26637420
INFO:root:[   42] Training loss: 0.02724332, Validation loss: 0.02769904, Gradient norm: 0.33968570
INFO:root:[   43] Training loss: 0.02721170, Validation loss: 0.02751403, Gradient norm: 0.36064350
INFO:root:[   44] Training loss: 0.02648637, Validation loss: 0.02631675, Gradient norm: 0.26360330
INFO:root:[   45] Training loss: 0.02717587, Validation loss: 0.02697434, Gradient norm: 0.34484344
INFO:root:[   46] Training loss: 0.02737756, Validation loss: 0.02623798, Gradient norm: 0.40669797
INFO:root:[   47] Training loss: 0.02597094, Validation loss: 0.02749806, Gradient norm: 0.30153142
INFO:root:[   48] Training loss: 0.02653034, Validation loss: 0.02549678, Gradient norm: 0.31195469
INFO:root:[   49] Training loss: 0.02625695, Validation loss: 0.02528504, Gradient norm: 0.30493761
INFO:root:[   50] Training loss: 0.02577607, Validation loss: 0.02544536, Gradient norm: 0.35719100
INFO:root:[   51] Training loss: 0.02598669, Validation loss: 0.02584501, Gradient norm: 0.28651800
INFO:root:[   52] Training loss: 0.02475343, Validation loss: 0.02623948, Gradient norm: 0.29926659
INFO:root:[   53] Training loss: 0.02505794, Validation loss: 0.02550921, Gradient norm: 0.25117352
INFO:root:[   54] Training loss: 0.02527111, Validation loss: 0.02577992, Gradient norm: 0.36151588
INFO:root:[   55] Training loss: 0.02491097, Validation loss: 0.02522288, Gradient norm: 0.30551556
INFO:root:[   56] Training loss: 0.02419288, Validation loss: 0.02551738, Gradient norm: 0.21839570
INFO:root:[   57] Training loss: 0.02467581, Validation loss: 0.02474345, Gradient norm: 0.28748214
INFO:root:[   58] Training loss: 0.02587282, Validation loss: 0.02552107, Gradient norm: 0.42763245
INFO:root:[   59] Training loss: 0.02428236, Validation loss: 0.02554249, Gradient norm: 0.30023342
INFO:root:[   60] Training loss: 0.02469107, Validation loss: 0.02509666, Gradient norm: 0.28766368
INFO:root:[   61] Training loss: 0.02441942, Validation loss: 0.02479157, Gradient norm: 0.25313320
INFO:root:[   62] Training loss: 0.02462725, Validation loss: 0.02446837, Gradient norm: 0.34678743
INFO:root:[   63] Training loss: 0.02479072, Validation loss: 0.02393378, Gradient norm: 0.27231803
INFO:root:[   64] Training loss: 0.02428218, Validation loss: 0.02446094, Gradient norm: 0.31086582
INFO:root:[   65] Training loss: 0.02443905, Validation loss: 0.02480211, Gradient norm: 0.25566652
INFO:root:[   66] Training loss: 0.02416130, Validation loss: 0.02492344, Gradient norm: 0.31578555
INFO:root:[   67] Training loss: 0.02441415, Validation loss: 0.02444826, Gradient norm: 0.29505619
INFO:root:[   68] Training loss: 0.02470518, Validation loss: 0.02354798, Gradient norm: 0.36833962
INFO:root:[   69] Training loss: 0.02367247, Validation loss: 0.02458163, Gradient norm: 0.28791357
INFO:root:[   70] Training loss: 0.02357968, Validation loss: 0.02441407, Gradient norm: 0.26920895
INFO:root:[   71] Training loss: 0.02402989, Validation loss: 0.02372386, Gradient norm: 0.29777232
INFO:root:[   72] Training loss: 0.02362610, Validation loss: 0.02371765, Gradient norm: 0.28719691
INFO:root:[   73] Training loss: 0.02348964, Validation loss: 0.02368866, Gradient norm: 0.25488112
INFO:root:[   74] Training loss: 0.02379418, Validation loss: 0.02333401, Gradient norm: 0.26164416
INFO:root:[   75] Training loss: 0.02417204, Validation loss: 0.02418837, Gradient norm: 0.36855078
INFO:root:[   76] Training loss: 0.02328704, Validation loss: 0.02319323, Gradient norm: 0.22732801
INFO:root:[   77] Training loss: 0.02293637, Validation loss: 0.02300404, Gradient norm: 0.22024336
INFO:root:[   78] Training loss: 0.02356215, Validation loss: 0.02381071, Gradient norm: 0.24388995
INFO:root:[   79] Training loss: 0.02371009, Validation loss: 0.02439403, Gradient norm: 0.34691226
INFO:root:[   80] Training loss: 0.02322535, Validation loss: 0.02299959, Gradient norm: 0.28982359
INFO:root:[   81] Training loss: 0.02316234, Validation loss: 0.02349138, Gradient norm: 0.30042468
INFO:root:[   82] Training loss: 0.02346174, Validation loss: 0.02275626, Gradient norm: 0.33551217
INFO:root:[   83] Training loss: 0.02349816, Validation loss: 0.02393152, Gradient norm: 0.33205503
INFO:root:[   84] Training loss: 0.02369805, Validation loss: 0.02322585, Gradient norm: 0.36107109
INFO:root:[   85] Training loss: 0.02324845, Validation loss: 0.02447815, Gradient norm: 0.31873558
INFO:root:[   86] Training loss: 0.02310203, Validation loss: 0.02337962, Gradient norm: 0.37496135
INFO:root:[   87] Training loss: 0.02274861, Validation loss: 0.02259497, Gradient norm: 0.22393596
INFO:root:[   88] Training loss: 0.02283737, Validation loss: 0.02295979, Gradient norm: 0.32629339
INFO:root:[   89] Training loss: 0.02262139, Validation loss: 0.02227380, Gradient norm: 0.28095738
INFO:root:[   90] Training loss: 0.02241774, Validation loss: 0.02298897, Gradient norm: 0.23208022
INFO:root:[   91] Training loss: 0.02299865, Validation loss: 0.02254909, Gradient norm: 0.27149181
INFO:root:[   92] Training loss: 0.02279056, Validation loss: 0.02193131, Gradient norm: 0.31564466
INFO:root:[   93] Training loss: 0.02271086, Validation loss: 0.02275228, Gradient norm: 0.30329262
INFO:root:[   94] Training loss: 0.02313296, Validation loss: 0.02229313, Gradient norm: 0.35803393
INFO:root:[   95] Training loss: 0.02165014, Validation loss: 0.02253608, Gradient norm: 0.20441619
INFO:root:[   96] Training loss: 0.02189261, Validation loss: 0.02235927, Gradient norm: 0.27687814
INFO:root:[   97] Training loss: 0.02213614, Validation loss: 0.02213698, Gradient norm: 0.24662197
INFO:root:[   98] Training loss: 0.02211123, Validation loss: 0.02299936, Gradient norm: 0.21373485
INFO:root:[   99] Training loss: 0.02235729, Validation loss: 0.02183874, Gradient norm: 0.29994248
INFO:root:[  100] Training loss: 0.02262411, Validation loss: 0.02216661, Gradient norm: 0.39055096
INFO:root:[  101] Training loss: 0.02175546, Validation loss: 0.02259880, Gradient norm: 0.28373264
INFO:root:[  102] Training loss: 0.02230664, Validation loss: 0.02175653, Gradient norm: 0.24382920
INFO:root:[  103] Training loss: 0.02241829, Validation loss: 0.02249067, Gradient norm: 0.33346317
INFO:root:[  104] Training loss: 0.02219237, Validation loss: 0.02175980, Gradient norm: 0.28534250
INFO:root:[  105] Training loss: 0.02144707, Validation loss: 0.02224264, Gradient norm: 0.27104864
INFO:root:[  106] Training loss: 0.02234639, Validation loss: 0.02204320, Gradient norm: 0.35183492
INFO:root:[  107] Training loss: 0.02146871, Validation loss: 0.02170611, Gradient norm: 0.28043581
INFO:root:[  108] Training loss: 0.02116796, Validation loss: 0.02176659, Gradient norm: 0.20926982
INFO:root:[  109] Training loss: 0.02160081, Validation loss: 0.02143135, Gradient norm: 0.26831087
INFO:root:[  110] Training loss: 0.02196789, Validation loss: 0.02196744, Gradient norm: 0.31258056
INFO:root:[  111] Training loss: 0.02150381, Validation loss: 0.02124479, Gradient norm: 0.33321577
INFO:root:[  112] Training loss: 0.02166336, Validation loss: 0.02228260, Gradient norm: 0.25000328
INFO:root:[  113] Training loss: 0.02181791, Validation loss: 0.02167640, Gradient norm: 0.19688808
INFO:root:[  114] Training loss: 0.02124305, Validation loss: 0.02144396, Gradient norm: 0.26063875
INFO:root:[  115] Training loss: 0.02168941, Validation loss: 0.02137655, Gradient norm: 0.25280676
INFO:root:[  116] Training loss: 0.02151635, Validation loss: 0.02157922, Gradient norm: 0.35995359
INFO:root:[  117] Training loss: 0.02082353, Validation loss: 0.02177217, Gradient norm: 0.27815934
INFO:root:[  118] Training loss: 0.02131374, Validation loss: 0.02107480, Gradient norm: 0.29577223
INFO:root:[  119] Training loss: 0.02097063, Validation loss: 0.02204755, Gradient norm: 0.26236359
INFO:root:[  120] Training loss: 0.02181157, Validation loss: 0.02224965, Gradient norm: 0.42561855
INFO:root:[  121] Training loss: 0.02141537, Validation loss: 0.02131606, Gradient norm: 0.29301516
INFO:root:[  122] Training loss: 0.02244245, Validation loss: 0.02207828, Gradient norm: 0.37833872
INFO:root:[  123] Training loss: 0.02157028, Validation loss: 0.02172909, Gradient norm: 0.33246207
INFO:root:[  124] Training loss: 0.02124672, Validation loss: 0.02183495, Gradient norm: 0.28543384
INFO:root:[  125] Training loss: 0.02135072, Validation loss: 0.02112542, Gradient norm: 0.26150508
INFO:root:[  126] Training loss: 0.02123029, Validation loss: 0.02189602, Gradient norm: 0.32943426
INFO:root:[  127] Training loss: 0.02070506, Validation loss: 0.02111800, Gradient norm: 0.28704848
INFO:root:EP 127: Early stopping
INFO:root:Training the model took 657.976s.
INFO:root:Emptying the cuda cache took 0.016s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82182
INFO:root:EnergyScoreTrain: 0.65427
INFO:root:CoverageTrain: 0.98639
INFO:root:IntervalWidthTrain: 0.16989
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.84413
INFO:root:EnergyScoreValidation: 0.67166
INFO:root:CoverageValidation: 0.98385
INFO:root:IntervalWidthValidation: 0.17131
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.40079
INFO:root:EnergyScoreTest: 0.99742
INFO:root:CoverageTest: 0.96853
INFO:root:IntervalWidthTest: 0.20463
INFO:root:###7 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.16955607, Validation loss: 0.13690592, Gradient norm: 0.57204183
INFO:root:[    2] Training loss: 0.11660569, Validation loss: 0.10357645, Gradient norm: 0.35203379
INFO:root:[    3] Training loss: 0.09508187, Validation loss: 0.08808739, Gradient norm: 0.27791415
INFO:root:[    4] Training loss: 0.08157529, Validation loss: 0.07925634, Gradient norm: 0.22191439
INFO:root:[    5] Training loss: 0.07502591, Validation loss: 0.07304796, Gradient norm: 0.21165159
INFO:root:[    6] Training loss: 0.06964018, Validation loss: 0.06875383, Gradient norm: 0.19620935
INFO:root:[    7] Training loss: 0.06599480, Validation loss: 0.06714812, Gradient norm: 0.19950015
INFO:root:[    8] Training loss: 0.06307653, Validation loss: 0.06261769, Gradient norm: 0.18071769
INFO:root:[    9] Training loss: 0.06148782, Validation loss: 0.06115129, Gradient norm: 0.18947839
INFO:root:[   10] Training loss: 0.06047405, Validation loss: 0.05981642, Gradient norm: 0.22221388
INFO:root:[   11] Training loss: 0.05773980, Validation loss: 0.05777364, Gradient norm: 0.17621799
INFO:root:[   12] Training loss: 0.05588801, Validation loss: 0.05644239, Gradient norm: 0.16206002
INFO:root:[   13] Training loss: 0.05590643, Validation loss: 0.05579118, Gradient norm: 0.19699191
INFO:root:[   14] Training loss: 0.05509662, Validation loss: 0.05481436, Gradient norm: 0.19145746
INFO:root:[   15] Training loss: 0.05338862, Validation loss: 0.05340493, Gradient norm: 0.21411950
INFO:root:[   16] Training loss: 0.05197842, Validation loss: 0.05205459, Gradient norm: 0.18297949
INFO:root:[   17] Training loss: 0.05034079, Validation loss: 0.05078260, Gradient norm: 0.19187154
INFO:root:[   18] Training loss: 0.05054150, Validation loss: 0.04980473, Gradient norm: 0.19247431
INFO:root:[   19] Training loss: 0.04835745, Validation loss: 0.04933600, Gradient norm: 0.21054998
INFO:root:[   20] Training loss: 0.04843924, Validation loss: 0.04790752, Gradient norm: 0.20605879
INFO:root:[   21] Training loss: 0.04729115, Validation loss: 0.04624421, Gradient norm: 0.20347510
INFO:root:[   22] Training loss: 0.04573113, Validation loss: 0.04758939, Gradient norm: 0.19589314
INFO:root:[   23] Training loss: 0.04402369, Validation loss: 0.04484927, Gradient norm: 0.22502348
INFO:root:[   24] Training loss: 0.04436129, Validation loss: 0.04452032, Gradient norm: 0.20592206
INFO:root:[   25] Training loss: 0.04205594, Validation loss: 0.04230389, Gradient norm: 0.25285020
INFO:root:[   26] Training loss: 0.04113277, Validation loss: 0.04199069, Gradient norm: 0.21294099
INFO:root:[   27] Training loss: 0.04108620, Validation loss: 0.04030109, Gradient norm: 0.25705987
INFO:root:[   28] Training loss: 0.03787421, Validation loss: 0.04022276, Gradient norm: 0.22311480
INFO:root:[   29] Training loss: 0.03886488, Validation loss: 0.03807715, Gradient norm: 0.23900142
INFO:root:[   30] Training loss: 0.03761587, Validation loss: 0.03912594, Gradient norm: 0.20166382
INFO:root:[   31] Training loss: 0.03797097, Validation loss: 0.03776087, Gradient norm: 0.22791926
INFO:root:[   32] Training loss: 0.03756201, Validation loss: 0.03651626, Gradient norm: 0.30474693
INFO:root:[   33] Training loss: 0.03731298, Validation loss: 0.03579941, Gradient norm: 0.26867434
INFO:root:[   34] Training loss: 0.03593332, Validation loss: 0.03549642, Gradient norm: 0.22012266
INFO:root:[   35] Training loss: 0.03668206, Validation loss: 0.03490822, Gradient norm: 0.25543852
INFO:root:[   36] Training loss: 0.03582804, Validation loss: 0.03469107, Gradient norm: 0.24258522
INFO:root:[   37] Training loss: 0.03511058, Validation loss: 0.03463793, Gradient norm: 0.32390535
INFO:root:[   38] Training loss: 0.03491377, Validation loss: 0.03463985, Gradient norm: 0.28381134
INFO:root:[   39] Training loss: 0.03426194, Validation loss: 0.03409210, Gradient norm: 0.26766387
INFO:root:[   40] Training loss: 0.03320823, Validation loss: 0.03328214, Gradient norm: 0.22952343
INFO:root:[   41] Training loss: 0.03357512, Validation loss: 0.03336864, Gradient norm: 0.22321599
INFO:root:[   42] Training loss: 0.03261687, Validation loss: 0.03376995, Gradient norm: 0.24822239
INFO:root:[   43] Training loss: 0.03289093, Validation loss: 0.03391492, Gradient norm: 0.23218199
INFO:root:[   44] Training loss: 0.03344984, Validation loss: 0.03315304, Gradient norm: 0.28403191
INFO:root:[   45] Training loss: 0.03272963, Validation loss: 0.03292602, Gradient norm: 0.22228980
INFO:root:[   46] Training loss: 0.03219574, Validation loss: 0.03262940, Gradient norm: 0.24006166
INFO:root:[   47] Training loss: 0.03221383, Validation loss: 0.03174043, Gradient norm: 0.32211035
INFO:root:[   48] Training loss: 0.03354192, Validation loss: 0.03278344, Gradient norm: 0.31472488
INFO:root:[   49] Training loss: 0.03173252, Validation loss: 0.03236331, Gradient norm: 0.33997983
INFO:root:[   50] Training loss: 0.03212782, Validation loss: 0.03239869, Gradient norm: 0.23327430
INFO:root:[   51] Training loss: 0.03198557, Validation loss: 0.03349398, Gradient norm: 0.31984012
INFO:root:[   52] Training loss: 0.03172796, Validation loss: 0.03160604, Gradient norm: 0.24482608
INFO:root:[   53] Training loss: 0.03159825, Validation loss: 0.03113021, Gradient norm: 0.22941646
INFO:root:[   54] Training loss: 0.03042675, Validation loss: 0.03147443, Gradient norm: 0.24574258
INFO:root:[   55] Training loss: 0.03090767, Validation loss: 0.03076933, Gradient norm: 0.27578479
INFO:root:[   56] Training loss: 0.03067089, Validation loss: 0.03083021, Gradient norm: 0.27566763
INFO:root:[   57] Training loss: 0.03067879, Validation loss: 0.03045798, Gradient norm: 0.24936571
INFO:root:[   58] Training loss: 0.03046471, Validation loss: 0.03107377, Gradient norm: 0.24315529
INFO:root:[   59] Training loss: 0.03054559, Validation loss: 0.02999038, Gradient norm: 0.30219248
INFO:root:[   60] Training loss: 0.03052775, Validation loss: 0.02976321, Gradient norm: 0.25752046
INFO:root:[   61] Training loss: 0.03104808, Validation loss: 0.03188159, Gradient norm: 0.32162472
INFO:root:[   62] Training loss: 0.03073429, Validation loss: 0.02978244, Gradient norm: 0.28468402
INFO:root:[   63] Training loss: 0.02971712, Validation loss: 0.03042708, Gradient norm: 0.25139961
INFO:root:[   64] Training loss: 0.03071806, Validation loss: 0.02949260, Gradient norm: 0.35687275
INFO:root:[   65] Training loss: 0.03033395, Validation loss: 0.02961847, Gradient norm: 0.30821111
INFO:root:[   66] Training loss: 0.02921575, Validation loss: 0.02997421, Gradient norm: 0.21407687
INFO:root:[   67] Training loss: 0.02935040, Validation loss: 0.02999640, Gradient norm: 0.28738699
INFO:root:[   68] Training loss: 0.02964600, Validation loss: 0.02914921, Gradient norm: 0.26651401
INFO:root:[   69] Training loss: 0.02950374, Validation loss: 0.02935821, Gradient norm: 0.31309842
INFO:root:[   70] Training loss: 0.02958405, Validation loss: 0.02906485, Gradient norm: 0.24704281
INFO:root:[   71] Training loss: 0.02865138, Validation loss: 0.02910250, Gradient norm: 0.24021769
INFO:root:[   72] Training loss: 0.02958949, Validation loss: 0.02910277, Gradient norm: 0.29348853
INFO:root:[   73] Training loss: 0.03002352, Validation loss: 0.03029457, Gradient norm: 0.32061664
INFO:root:[   74] Training loss: 0.02999544, Validation loss: 0.02950998, Gradient norm: 0.32667521
INFO:root:[   75] Training loss: 0.02879336, Validation loss: 0.02923754, Gradient norm: 0.28833431
INFO:root:[   76] Training loss: 0.02926398, Validation loss: 0.02887365, Gradient norm: 0.28275938
INFO:root:[   77] Training loss: 0.02790654, Validation loss: 0.02850916, Gradient norm: 0.21989225
INFO:root:[   78] Training loss: 0.02828576, Validation loss: 0.02912094, Gradient norm: 0.24899281
INFO:root:[   79] Training loss: 0.02870988, Validation loss: 0.02933595, Gradient norm: 0.32286567
INFO:root:[   80] Training loss: 0.02805036, Validation loss: 0.02821949, Gradient norm: 0.21333909
INFO:root:[   81] Training loss: 0.02881275, Validation loss: 0.02828927, Gradient norm: 0.29858087
INFO:root:[   82] Training loss: 0.02843278, Validation loss: 0.02828383, Gradient norm: 0.24471159
INFO:root:[   83] Training loss: 0.02795873, Validation loss: 0.02848852, Gradient norm: 0.25247004
INFO:root:[   84] Training loss: 0.02755849, Validation loss: 0.02846901, Gradient norm: 0.30056244
INFO:root:[   85] Training loss: 0.02819045, Validation loss: 0.02823425, Gradient norm: 0.35041963
INFO:root:[   86] Training loss: 0.02807910, Validation loss: 0.02762446, Gradient norm: 0.24800225
INFO:root:[   87] Training loss: 0.02848597, Validation loss: 0.02997104, Gradient norm: 0.30636508
INFO:root:[   88] Training loss: 0.02808849, Validation loss: 0.02775552, Gradient norm: 0.33261675
INFO:root:[   89] Training loss: 0.02756093, Validation loss: 0.02820004, Gradient norm: 0.29424420
INFO:root:[   90] Training loss: 0.02823817, Validation loss: 0.02781999, Gradient norm: 0.29113593
INFO:root:[   91] Training loss: 0.02779443, Validation loss: 0.02818555, Gradient norm: 0.32618833
INFO:root:[   92] Training loss: 0.02770132, Validation loss: 0.02745009, Gradient norm: 0.35680642
INFO:root:[   93] Training loss: 0.02764358, Validation loss: 0.02785479, Gradient norm: 0.31398044
INFO:root:[   94] Training loss: 0.02797811, Validation loss: 0.02796025, Gradient norm: 0.26254114
INFO:root:[   95] Training loss: 0.02680768, Validation loss: 0.02790997, Gradient norm: 0.24844287
INFO:root:[   96] Training loss: 0.02747900, Validation loss: 0.02768301, Gradient norm: 0.23936055
INFO:root:[   97] Training loss: 0.02789292, Validation loss: 0.02710199, Gradient norm: 0.22523120
INFO:root:[   98] Training loss: 0.02739992, Validation loss: 0.02739008, Gradient norm: 0.28017155
INFO:root:[   99] Training loss: 0.02725089, Validation loss: 0.02779780, Gradient norm: 0.24844756
INFO:root:[  100] Training loss: 0.02735839, Validation loss: 0.02719244, Gradient norm: 0.26846088
INFO:root:[  101] Training loss: 0.02719709, Validation loss: 0.02728063, Gradient norm: 0.28158526
INFO:root:[  102] Training loss: 0.02727583, Validation loss: 0.02733363, Gradient norm: 0.28920933
INFO:root:[  103] Training loss: 0.02625850, Validation loss: 0.02702268, Gradient norm: 0.25723825
INFO:root:[  104] Training loss: 0.02699583, Validation loss: 0.02740757, Gradient norm: 0.31728796
INFO:root:[  105] Training loss: 0.02764470, Validation loss: 0.02680231, Gradient norm: 0.28043515
INFO:root:[  106] Training loss: 0.02701306, Validation loss: 0.02681757, Gradient norm: 0.26861723
INFO:root:[  107] Training loss: 0.02657973, Validation loss: 0.02773349, Gradient norm: 0.32277872
INFO:root:[  108] Training loss: 0.02724973, Validation loss: 0.02685216, Gradient norm: 0.30386785
INFO:root:[  109] Training loss: 0.02673318, Validation loss: 0.02740534, Gradient norm: 0.35872282
INFO:root:[  110] Training loss: 0.02612908, Validation loss: 0.02636644, Gradient norm: 0.27212984
INFO:root:[  111] Training loss: 0.02649303, Validation loss: 0.02683082, Gradient norm: 0.24463797
INFO:root:[  112] Training loss: 0.02674190, Validation loss: 0.02633460, Gradient norm: 0.29213179
INFO:root:[  113] Training loss: 0.02655132, Validation loss: 0.02653877, Gradient norm: 0.31879354
INFO:root:[  114] Training loss: 0.02606117, Validation loss: 0.02686544, Gradient norm: 0.29895248
INFO:root:[  115] Training loss: 0.02667484, Validation loss: 0.02728211, Gradient norm: 0.30421024
INFO:root:[  116] Training loss: 0.02625786, Validation loss: 0.02628106, Gradient norm: 0.37959406
INFO:root:[  117] Training loss: 0.02625640, Validation loss: 0.02658604, Gradient norm: 0.32510432
INFO:root:[  118] Training loss: 0.02662069, Validation loss: 0.02603921, Gradient norm: 0.30253816
INFO:root:[  119] Training loss: 0.02546971, Validation loss: 0.02594392, Gradient norm: 0.23894104
INFO:root:[  120] Training loss: 0.02589117, Validation loss: 0.02631763, Gradient norm: 0.26825304
INFO:root:[  121] Training loss: 0.02587108, Validation loss: 0.02647061, Gradient norm: 0.24489841
INFO:root:[  122] Training loss: 0.02567712, Validation loss: 0.02567981, Gradient norm: 0.25179721
INFO:root:[  123] Training loss: 0.02593976, Validation loss: 0.02587898, Gradient norm: 0.28083916
INFO:root:[  124] Training loss: 0.02651643, Validation loss: 0.02620441, Gradient norm: 0.27876014
INFO:root:[  125] Training loss: 0.02585439, Validation loss: 0.02666330, Gradient norm: 0.21605481
INFO:root:[  126] Training loss: 0.02570071, Validation loss: 0.02719432, Gradient norm: 0.26311422
INFO:root:[  127] Training loss: 0.02641016, Validation loss: 0.02664218, Gradient norm: 0.39093275
INFO:root:[  128] Training loss: 0.02668424, Validation loss: 0.02635820, Gradient norm: 0.35293390
INFO:root:[  129] Training loss: 0.02551382, Validation loss: 0.02573666, Gradient norm: 0.27102261
INFO:root:[  130] Training loss: 0.02576781, Validation loss: 0.02661280, Gradient norm: 0.26998353
INFO:root:[  131] Training loss: 0.02528317, Validation loss: 0.02563466, Gradient norm: 0.30719525
INFO:root:[  132] Training loss: 0.02609373, Validation loss: 0.02545366, Gradient norm: 0.27224967
INFO:root:[  133] Training loss: 0.02561694, Validation loss: 0.02533211, Gradient norm: 0.23215318
INFO:root:[  134] Training loss: 0.02530220, Validation loss: 0.02643657, Gradient norm: 0.26798546
INFO:root:[  135] Training loss: 0.02632074, Validation loss: 0.02620055, Gradient norm: 0.43193435
INFO:root:[  136] Training loss: 0.02496164, Validation loss: 0.02507436, Gradient norm: 0.22068918
INFO:root:[  137] Training loss: 0.02533591, Validation loss: 0.02672417, Gradient norm: 0.27250346
INFO:root:[  138] Training loss: 0.02550386, Validation loss: 0.02554256, Gradient norm: 0.32588087
INFO:root:[  139] Training loss: 0.02597412, Validation loss: 0.02518271, Gradient norm: 0.24176938
INFO:root:[  140] Training loss: 0.02533351, Validation loss: 0.02579082, Gradient norm: 0.26032091
INFO:root:[  141] Training loss: 0.02569884, Validation loss: 0.02507015, Gradient norm: 0.28712819
INFO:root:[  142] Training loss: 0.02556611, Validation loss: 0.02516589, Gradient norm: 0.31192548
INFO:root:[  143] Training loss: 0.02456385, Validation loss: 0.02460022, Gradient norm: 0.24654644
INFO:root:[  144] Training loss: 0.02554029, Validation loss: 0.02513662, Gradient norm: 0.28212742
INFO:root:[  145] Training loss: 0.02567817, Validation loss: 0.02684056, Gradient norm: 0.28053690
INFO:root:[  146] Training loss: 0.02566492, Validation loss: 0.02481334, Gradient norm: 0.34950014
INFO:root:[  147] Training loss: 0.02455815, Validation loss: 0.02460779, Gradient norm: 0.24847732
INFO:root:[  148] Training loss: 0.02477827, Validation loss: 0.02476773, Gradient norm: 0.33597798
INFO:root:[  149] Training loss: 0.02456160, Validation loss: 0.02473505, Gradient norm: 0.32609034
INFO:root:[  150] Training loss: 0.02453171, Validation loss: 0.02478379, Gradient norm: 0.25957443
INFO:root:[  151] Training loss: 0.02467768, Validation loss: 0.02467807, Gradient norm: 0.31706315
INFO:root:[  152] Training loss: 0.02466384, Validation loss: 0.02505984, Gradient norm: 0.31711578
INFO:root:[  153] Training loss: 0.02442644, Validation loss: 0.02458750, Gradient norm: 0.27913533
INFO:root:[  154] Training loss: 0.02500142, Validation loss: 0.02500313, Gradient norm: 0.33889729
INFO:root:[  155] Training loss: 0.02426610, Validation loss: 0.02489429, Gradient norm: 0.24814665
INFO:root:[  156] Training loss: 0.02443750, Validation loss: 0.02474185, Gradient norm: 0.24998391
INFO:root:[  157] Training loss: 0.02498196, Validation loss: 0.02480170, Gradient norm: 0.28173430
INFO:root:[  158] Training loss: 0.02598671, Validation loss: 0.02450288, Gradient norm: 0.43697575
INFO:root:[  159] Training loss: 0.02513214, Validation loss: 0.02466443, Gradient norm: 0.33783847
INFO:root:[  160] Training loss: 0.02458817, Validation loss: 0.02435121, Gradient norm: 0.31975994
INFO:root:[  161] Training loss: 0.02461151, Validation loss: 0.02449086, Gradient norm: 0.38033350
INFO:root:[  162] Training loss: 0.02397592, Validation loss: 0.02565252, Gradient norm: 0.26010767
INFO:root:[  163] Training loss: 0.02395156, Validation loss: 0.02442043, Gradient norm: 0.22466576
INFO:root:[  164] Training loss: 0.02465883, Validation loss: 0.02505531, Gradient norm: 0.30521148
INFO:root:[  165] Training loss: 0.02434463, Validation loss: 0.02405334, Gradient norm: 0.39714661
INFO:root:[  166] Training loss: 0.02418535, Validation loss: 0.02512555, Gradient norm: 0.32513323
INFO:root:[  167] Training loss: 0.02546139, Validation loss: 0.02498103, Gradient norm: 0.29289170
INFO:root:[  168] Training loss: 0.02386953, Validation loss: 0.02480959, Gradient norm: 0.27223339
INFO:root:[  169] Training loss: 0.02453583, Validation loss: 0.02468746, Gradient norm: 0.26836094
INFO:root:[  170] Training loss: 0.02394297, Validation loss: 0.02586865, Gradient norm: 0.29739559
INFO:root:[  171] Training loss: 0.02416956, Validation loss: 0.02439015, Gradient norm: 0.37125437
INFO:root:[  172] Training loss: 0.02418123, Validation loss: 0.02402178, Gradient norm: 0.25106654
INFO:root:[  173] Training loss: 0.02378895, Validation loss: 0.02465070, Gradient norm: 0.29060824
INFO:root:[  174] Training loss: 0.02338269, Validation loss: 0.02395884, Gradient norm: 0.18404748
INFO:root:[  175] Training loss: 0.02390739, Validation loss: 0.02352979, Gradient norm: 0.27215632
INFO:root:[  176] Training loss: 0.02389847, Validation loss: 0.02403586, Gradient norm: 0.26625818
INFO:root:[  177] Training loss: 0.02347783, Validation loss: 0.02396876, Gradient norm: 0.24987458
INFO:root:[  178] Training loss: 0.02422504, Validation loss: 0.02354243, Gradient norm: 0.25674044
INFO:root:[  179] Training loss: 0.02358421, Validation loss: 0.02427831, Gradient norm: 0.26238861
INFO:root:[  180] Training loss: 0.02394563, Validation loss: 0.02406114, Gradient norm: 0.26684412
INFO:root:[  181] Training loss: 0.02452926, Validation loss: 0.02371670, Gradient norm: 0.42368926
INFO:root:[  182] Training loss: 0.02350376, Validation loss: 0.02386489, Gradient norm: 0.23925186
INFO:root:[  183] Training loss: 0.02387450, Validation loss: 0.02362628, Gradient norm: 0.33935000
INFO:root:[  184] Training loss: 0.02365942, Validation loss: 0.02346292, Gradient norm: 0.25457417
INFO:root:[  185] Training loss: 0.02376976, Validation loss: 0.02402947, Gradient norm: 0.30896113
INFO:root:[  186] Training loss: 0.02385202, Validation loss: 0.02308783, Gradient norm: 0.29540559
INFO:root:[  187] Training loss: 0.02332442, Validation loss: 0.02326681, Gradient norm: 0.29202824
INFO:root:[  188] Training loss: 0.02313012, Validation loss: 0.02361157, Gradient norm: 0.27888279
INFO:root:[  189] Training loss: 0.02309784, Validation loss: 0.02343673, Gradient norm: 0.28491058
INFO:root:[  190] Training loss: 0.02290269, Validation loss: 0.02330138, Gradient norm: 0.31261557
INFO:root:[  191] Training loss: 0.02356740, Validation loss: 0.02361869, Gradient norm: 0.28854107
INFO:root:[  192] Training loss: 0.02365847, Validation loss: 0.02330709, Gradient norm: 0.30309474
INFO:root:[  193] Training loss: 0.02337385, Validation loss: 0.02339737, Gradient norm: 0.29713318
INFO:root:[  194] Training loss: 0.02397851, Validation loss: 0.02340732, Gradient norm: 0.25871415
INFO:root:[  195] Training loss: 0.02332275, Validation loss: 0.02303345, Gradient norm: 0.27454714
INFO:root:[  196] Training loss: 0.02371949, Validation loss: 0.02318373, Gradient norm: 0.29286891
INFO:root:[  197] Training loss: 0.02289578, Validation loss: 0.02308501, Gradient norm: 0.25074407
INFO:root:[  198] Training loss: 0.02292205, Validation loss: 0.02321725, Gradient norm: 0.29425876
INFO:root:[  199] Training loss: 0.02269751, Validation loss: 0.02283015, Gradient norm: 0.23892349
INFO:root:[  200] Training loss: 0.02292439, Validation loss: 0.02425280, Gradient norm: 0.25444519
INFO:root:[  201] Training loss: 0.02289011, Validation loss: 0.02310777, Gradient norm: 0.32188941
INFO:root:[  202] Training loss: 0.02245958, Validation loss: 0.02320795, Gradient norm: 0.25325490
INFO:root:[  203] Training loss: 0.02229655, Validation loss: 0.02292434, Gradient norm: 0.22146708
INFO:root:[  204] Training loss: 0.02247242, Validation loss: 0.02413267, Gradient norm: 0.25273283
INFO:root:[  205] Training loss: 0.02311520, Validation loss: 0.02365935, Gradient norm: 0.33158295
INFO:root:[  206] Training loss: 0.02251625, Validation loss: 0.02314577, Gradient norm: 0.22936630
INFO:root:[  207] Training loss: 0.02344090, Validation loss: 0.02371792, Gradient norm: 0.34798225
INFO:root:[  208] Training loss: 0.02310958, Validation loss: 0.02284238, Gradient norm: 0.33344241
INFO:root:[  209] Training loss: 0.02327326, Validation loss: 0.02262394, Gradient norm: 0.33850195
INFO:root:[  210] Training loss: 0.02260938, Validation loss: 0.02303002, Gradient norm: 0.23350630
INFO:root:[  211] Training loss: 0.02293094, Validation loss: 0.02298087, Gradient norm: 0.29194580
INFO:root:[  212] Training loss: 0.02238536, Validation loss: 0.02302831, Gradient norm: 0.25852013
INFO:root:[  213] Training loss: 0.02278803, Validation loss: 0.02288429, Gradient norm: 0.28523940
INFO:root:[  214] Training loss: 0.02279217, Validation loss: 0.02413129, Gradient norm: 0.26986386
INFO:root:[  215] Training loss: 0.02309410, Validation loss: 0.02275551, Gradient norm: 0.40216281
INFO:root:[  216] Training loss: 0.02279986, Validation loss: 0.02246374, Gradient norm: 0.25560781
INFO:root:[  217] Training loss: 0.02310414, Validation loss: 0.02257790, Gradient norm: 0.29180296
INFO:root:[  218] Training loss: 0.02297373, Validation loss: 0.02235444, Gradient norm: 0.26185555
INFO:root:[  219] Training loss: 0.02267201, Validation loss: 0.02252308, Gradient norm: 0.34466080
INFO:root:[  220] Training loss: 0.02217690, Validation loss: 0.02233470, Gradient norm: 0.26522943
INFO:root:[  221] Training loss: 0.02224228, Validation loss: 0.02273828, Gradient norm: 0.28110855
INFO:root:[  222] Training loss: 0.02212579, Validation loss: 0.02233715, Gradient norm: 0.31673415
INFO:root:[  223] Training loss: 0.02257650, Validation loss: 0.02250625, Gradient norm: 0.28515905
INFO:root:[  224] Training loss: 0.02166237, Validation loss: 0.02213599, Gradient norm: 0.24328229
INFO:root:[  225] Training loss: 0.02250379, Validation loss: 0.02262812, Gradient norm: 0.26275489
INFO:root:[  226] Training loss: 0.02175725, Validation loss: 0.02244165, Gradient norm: 0.23095830
INFO:root:[  227] Training loss: 0.02220700, Validation loss: 0.02228372, Gradient norm: 0.28291684
INFO:root:[  228] Training loss: 0.02172076, Validation loss: 0.02192069, Gradient norm: 0.19869022
INFO:root:[  229] Training loss: 0.02252428, Validation loss: 0.02223593, Gradient norm: 0.21808243
INFO:root:[  230] Training loss: 0.02194451, Validation loss: 0.02227809, Gradient norm: 0.24525560
INFO:root:[  231] Training loss: 0.02257436, Validation loss: 0.02198652, Gradient norm: 0.24906193
INFO:root:[  232] Training loss: 0.02204611, Validation loss: 0.02189729, Gradient norm: 0.26395423
INFO:root:[  233] Training loss: 0.02229334, Validation loss: 0.02285120, Gradient norm: 0.24717472
INFO:root:[  234] Training loss: 0.02185444, Validation loss: 0.02184900, Gradient norm: 0.26937497
INFO:root:[  235] Training loss: 0.02189837, Validation loss: 0.02308366, Gradient norm: 0.23603131
INFO:root:[  236] Training loss: 0.02214255, Validation loss: 0.02180507, Gradient norm: 0.24849785
INFO:root:[  237] Training loss: 0.02164586, Validation loss: 0.02198552, Gradient norm: 0.26507019
INFO:root:[  238] Training loss: 0.02123367, Validation loss: 0.02184139, Gradient norm: 0.23088209
INFO:root:[  239] Training loss: 0.02134993, Validation loss: 0.02186711, Gradient norm: 0.22712761
INFO:root:[  240] Training loss: 0.02182006, Validation loss: 0.02197439, Gradient norm: 0.22450991
INFO:root:[  241] Training loss: 0.02136639, Validation loss: 0.02150775, Gradient norm: 0.22610045
INFO:root:[  242] Training loss: 0.02191990, Validation loss: 0.02169196, Gradient norm: 0.25270867
INFO:root:[  243] Training loss: 0.02177959, Validation loss: 0.02145960, Gradient norm: 0.22913273
INFO:root:[  244] Training loss: 0.02142213, Validation loss: 0.02150975, Gradient norm: 0.23433574
INFO:root:[  245] Training loss: 0.02159042, Validation loss: 0.02167315, Gradient norm: 0.23078264
INFO:root:[  246] Training loss: 0.02165632, Validation loss: 0.02203248, Gradient norm: 0.29350349
INFO:root:[  247] Training loss: 0.02141148, Validation loss: 0.02171212, Gradient norm: 0.23321100
INFO:root:[  248] Training loss: 0.02125821, Validation loss: 0.02154397, Gradient norm: 0.19872033
INFO:root:[  249] Training loss: 0.02118117, Validation loss: 0.02161043, Gradient norm: 0.24676346
INFO:root:[  250] Training loss: 0.02145336, Validation loss: 0.02162173, Gradient norm: 0.31317639
INFO:root:[  251] Training loss: 0.02165159, Validation loss: 0.02232938, Gradient norm: 0.29200126
INFO:root:[  252] Training loss: 0.02161301, Validation loss: 0.02212782, Gradient norm: 0.29683470
INFO:root:EP 252: Early stopping
INFO:root:Training the model took 1301.697s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.81865
INFO:root:EnergyScoreTrain: 0.67132
INFO:root:CoverageTrain: 0.99038
INFO:root:IntervalWidthTrain: 0.17376
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.84342
INFO:root:EnergyScoreValidation: 0.68913
INFO:root:CoverageValidation: 0.98994
INFO:root:IntervalWidthValidation: 0.17536
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.38785
INFO:root:EnergyScoreTest: 1.03285
INFO:root:CoverageTest: 0.94403
INFO:root:IntervalWidthTest: 0.17099
INFO:root:###8 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09919436, Validation loss: 0.08531466, Gradient norm: 0.31023269
INFO:root:[    2] Training loss: 0.07797492, Validation loss: 0.06940224, Gradient norm: 0.57184931
INFO:root:[    3] Training loss: 0.06558271, Validation loss: 0.05763975, Gradient norm: 0.48974848
INFO:root:[    4] Training loss: 0.05723863, Validation loss: 0.04884757, Gradient norm: 0.94646741
INFO:root:[    5] Training loss: 0.05239734, Validation loss: 0.04336454, Gradient norm: 0.81296385
INFO:root:[    6] Training loss: 0.04837070, Validation loss: 0.04334874, Gradient norm: 0.74955468
INFO:root:[    7] Training loss: 0.04758410, Validation loss: 0.04026460, Gradient norm: 0.84167723
INFO:root:[    8] Training loss: 0.04596201, Validation loss: 0.04625857, Gradient norm: 0.71750910
INFO:root:[    9] Training loss: 0.04549652, Validation loss: 0.03820146, Gradient norm: 0.61821057
INFO:root:[   10] Training loss: 0.04374317, Validation loss: 0.04297780, Gradient norm: 0.55230767
INFO:root:[   11] Training loss: 0.04324588, Validation loss: 0.04017871, Gradient norm: 0.64460184
INFO:root:[   12] Training loss: 0.04294586, Validation loss: 0.03824026, Gradient norm: 0.75262411
INFO:root:[   13] Training loss: 0.04268746, Validation loss: 0.03612362, Gradient norm: 0.79821910
INFO:root:[   14] Training loss: 0.04096041, Validation loss: 0.03561327, Gradient norm: 0.77727112
INFO:root:[   15] Training loss: 0.04014243, Validation loss: 0.03564379, Gradient norm: 0.55753802
INFO:root:[   16] Training loss: 0.03939718, Validation loss: 0.03651600, Gradient norm: 0.60847943
INFO:root:[   17] Training loss: 0.03928996, Validation loss: 0.03572211, Gradient norm: 0.55519531
INFO:root:[   18] Training loss: 0.04006945, Validation loss: 0.03536902, Gradient norm: 0.81499587
INFO:root:[   19] Training loss: 0.03821176, Validation loss: 0.03351745, Gradient norm: 0.51601103
INFO:root:[   20] Training loss: 0.03988073, Validation loss: 0.03302779, Gradient norm: 0.99985301
INFO:root:[   21] Training loss: 0.04002749, Validation loss: 0.03596533, Gradient norm: 1.05930639
INFO:root:[   22] Training loss: 0.03687354, Validation loss: 0.03271932, Gradient norm: 0.69163658
INFO:root:[   23] Training loss: 0.03800900, Validation loss: 0.03566668, Gradient norm: 0.92375981
INFO:root:[   24] Training loss: 0.03771105, Validation loss: 0.03215796, Gradient norm: 0.68452305
INFO:root:[   25] Training loss: 0.03753849, Validation loss: 0.03308248, Gradient norm: 0.92398597
INFO:root:[   26] Training loss: 0.03528702, Validation loss: 0.03258185, Gradient norm: 0.60722863
INFO:root:[   27] Training loss: 0.03646685, Validation loss: 0.03064282, Gradient norm: 0.85497426
INFO:root:[   28] Training loss: 0.03599884, Validation loss: 0.03297377, Gradient norm: 0.73099152
INFO:root:[   29] Training loss: 0.03519508, Validation loss: 0.03030765, Gradient norm: 0.59675917
INFO:root:[   30] Training loss: 0.03586794, Validation loss: 0.03075344, Gradient norm: 0.68318864
INFO:root:[   31] Training loss: 0.03451566, Validation loss: 0.03235292, Gradient norm: 0.61188125
INFO:root:[   32] Training loss: 0.03539666, Validation loss: 0.02980531, Gradient norm: 0.77786503
INFO:root:[   33] Training loss: 0.03487384, Validation loss: 0.03117168, Gradient norm: 0.58749709
INFO:root:[   34] Training loss: 0.03548741, Validation loss: 0.03002145, Gradient norm: 0.85528327
INFO:root:[   35] Training loss: 0.03404718, Validation loss: 0.03071933, Gradient norm: 0.67200700
INFO:root:[   36] Training loss: 0.03359213, Validation loss: 0.02996444, Gradient norm: 0.47161156
INFO:root:[   37] Training loss: 0.03441770, Validation loss: 0.03203582, Gradient norm: 0.78852067
INFO:root:[   38] Training loss: 0.03403046, Validation loss: 0.03031382, Gradient norm: 0.73814775
INFO:root:[   39] Training loss: 0.03386548, Validation loss: 0.03011077, Gradient norm: 0.56436260
INFO:root:[   40] Training loss: 0.03368022, Validation loss: 0.02921280, Gradient norm: 0.56902680
INFO:root:[   41] Training loss: 0.03347668, Validation loss: 0.02851969, Gradient norm: 0.57670680
INFO:root:[   42] Training loss: 0.03262875, Validation loss: 0.02944862, Gradient norm: 0.53995418
INFO:root:[   43] Training loss: 0.03297011, Validation loss: 0.02894865, Gradient norm: 0.70936590
INFO:root:[   44] Training loss: 0.03245104, Validation loss: 0.02932094, Gradient norm: 0.49210526
INFO:root:[   45] Training loss: 0.03285662, Validation loss: 0.02892368, Gradient norm: 0.59684343
INFO:root:[   46] Training loss: 0.03252078, Validation loss: 0.02797204, Gradient norm: 0.49875686
INFO:root:[   47] Training loss: 0.03316800, Validation loss: 0.02919658, Gradient norm: 0.68389523
INFO:root:[   48] Training loss: 0.03340193, Validation loss: 0.03046560, Gradient norm: 0.68019133
INFO:root:[   49] Training loss: 0.03245105, Validation loss: 0.02935751, Gradient norm: 0.73917298
INFO:root:[   50] Training loss: 0.03204562, Validation loss: 0.02760796, Gradient norm: 0.79119433
INFO:root:[   51] Training loss: 0.03222782, Validation loss: 0.02870636, Gradient norm: 0.72829452
INFO:root:[   52] Training loss: 0.03203724, Validation loss: 0.03017556, Gradient norm: 0.71489718
INFO:root:[   53] Training loss: 0.03261595, Validation loss: 0.02894698, Gradient norm: 0.66319696
INFO:root:[   54] Training loss: 0.03165412, Validation loss: 0.02923859, Gradient norm: 0.62240307
INFO:root:[   55] Training loss: 0.03100190, Validation loss: 0.02720843, Gradient norm: 0.63338815
INFO:root:[   56] Training loss: 0.03139211, Validation loss: 0.02770132, Gradient norm: 0.57331120
INFO:root:[   57] Training loss: 0.03120915, Validation loss: 0.02712786, Gradient norm: 0.62601977
INFO:root:[   58] Training loss: 0.03151311, Validation loss: 0.02768720, Gradient norm: 0.59456524
INFO:root:[   59] Training loss: 0.03119994, Validation loss: 0.02830477, Gradient norm: 0.72161177
INFO:root:[   60] Training loss: 0.03192898, Validation loss: 0.02828182, Gradient norm: 0.87119257
INFO:root:[   61] Training loss: 0.03060976, Validation loss: 0.02981219, Gradient norm: 0.50813512
INFO:root:[   62] Training loss: 0.03155274, Validation loss: 0.02697560, Gradient norm: 0.68840286
INFO:root:[   63] Training loss: 0.03036762, Validation loss: 0.02726890, Gradient norm: 0.62101537
INFO:root:[   64] Training loss: 0.03093404, Validation loss: 0.02646793, Gradient norm: 0.57337463
INFO:root:[   65] Training loss: 0.03112935, Validation loss: 0.02826325, Gradient norm: 0.50594954
INFO:root:[   66] Training loss: 0.03070046, Validation loss: 0.02617503, Gradient norm: 0.65301786
INFO:root:[   67] Training loss: 0.02995959, Validation loss: 0.02756076, Gradient norm: 0.63889648
INFO:root:[   68] Training loss: 0.03129726, Validation loss: 0.02684728, Gradient norm: 0.76588362
INFO:root:[   69] Training loss: 0.02988257, Validation loss: 0.02801273, Gradient norm: 0.55947783
INFO:root:[   70] Training loss: 0.03092313, Validation loss: 0.02643180, Gradient norm: 0.67076981
INFO:root:[   71] Training loss: 0.02987694, Validation loss: 0.02865173, Gradient norm: 0.57985212
INFO:root:[   72] Training loss: 0.03095011, Validation loss: 0.02751789, Gradient norm: 0.57533296
INFO:root:[   73] Training loss: 0.03056294, Validation loss: 0.02769611, Gradient norm: 0.70756118
INFO:root:[   74] Training loss: 0.03054086, Validation loss: 0.02651570, Gradient norm: 0.87154490
INFO:root:[   75] Training loss: 0.02994121, Validation loss: 0.02623864, Gradient norm: 0.63887591
INFO:root:[   76] Training loss: 0.02952629, Validation loss: 0.02555622, Gradient norm: 0.66653340
INFO:root:[   77] Training loss: 0.02974372, Validation loss: 0.02623490, Gradient norm: 0.73192532
INFO:root:[   78] Training loss: 0.03045374, Validation loss: 0.02866532, Gradient norm: 0.83553437
INFO:root:[   79] Training loss: 0.02944150, Validation loss: 0.02793785, Gradient norm: 0.51841695
INFO:root:[   80] Training loss: 0.02971418, Validation loss: 0.02596426, Gradient norm: 0.61071228
INFO:root:[   81] Training loss: 0.02983416, Validation loss: 0.02567967, Gradient norm: 0.69654886
INFO:root:[   82] Training loss: 0.02901050, Validation loss: 0.02591451, Gradient norm: 0.57936913
INFO:root:[   83] Training loss: 0.02909839, Validation loss: 0.02635769, Gradient norm: 0.46976445
INFO:root:[   84] Training loss: 0.02897458, Validation loss: 0.02562355, Gradient norm: 0.51055971
INFO:root:[   85] Training loss: 0.02991078, Validation loss: 0.02551148, Gradient norm: 0.86782004
INFO:root:[   86] Training loss: 0.02896003, Validation loss: 0.02619027, Gradient norm: 0.50107211
INFO:root:[   87] Training loss: 0.02959456, Validation loss: 0.02846374, Gradient norm: 0.51477456
INFO:root:[   88] Training loss: 0.02930858, Validation loss: 0.02675153, Gradient norm: 0.74964462
INFO:root:[   89] Training loss: 0.02945600, Validation loss: 0.02564496, Gradient norm: 0.75433283
INFO:root:[   90] Training loss: 0.02884182, Validation loss: 0.02550688, Gradient norm: 0.44445789
INFO:root:[   91] Training loss: 0.02856394, Validation loss: 0.02816140, Gradient norm: 0.42654297
INFO:root:[   92] Training loss: 0.02805112, Validation loss: 0.02486074, Gradient norm: 0.59868627
INFO:root:[   93] Training loss: 0.02877590, Validation loss: 0.02692614, Gradient norm: 0.65729564
INFO:root:[   94] Training loss: 0.02890573, Validation loss: 0.02588245, Gradient norm: 0.75887740
INFO:root:[   95] Training loss: 0.02933738, Validation loss: 0.02524451, Gradient norm: 0.73756684
INFO:root:[   96] Training loss: 0.02818718, Validation loss: 0.02586757, Gradient norm: 0.49588640
INFO:root:[   97] Training loss: 0.02808738, Validation loss: 0.02647720, Gradient norm: 0.51734057
INFO:root:[   98] Training loss: 0.02831998, Validation loss: 0.02497192, Gradient norm: 0.39505963
INFO:root:[   99] Training loss: 0.02804021, Validation loss: 0.02532231, Gradient norm: 0.50084056
INFO:root:[  100] Training loss: 0.02846843, Validation loss: 0.02483363, Gradient norm: 0.58882464
INFO:root:[  101] Training loss: 0.02782671, Validation loss: 0.02617269, Gradient norm: 0.63869243
INFO:root:[  102] Training loss: 0.02865884, Validation loss: 0.02458886, Gradient norm: 0.69904852
INFO:root:[  103] Training loss: 0.02843921, Validation loss: 0.02440203, Gradient norm: 0.64650186
INFO:root:[  104] Training loss: 0.02760947, Validation loss: 0.02504677, Gradient norm: 0.59628929
INFO:root:[  105] Training loss: 0.02849967, Validation loss: 0.03167759, Gradient norm: 0.63869031
INFO:root:[  106] Training loss: 0.02919744, Validation loss: 0.02438176, Gradient norm: 0.78564622
INFO:root:[  107] Training loss: 0.02779118, Validation loss: 0.02689785, Gradient norm: 0.54015024
INFO:root:[  108] Training loss: 0.02893267, Validation loss: 0.02610748, Gradient norm: 0.84380963
INFO:root:[  109] Training loss: 0.02830993, Validation loss: 0.02520987, Gradient norm: 0.70833744
INFO:root:[  110] Training loss: 0.02706054, Validation loss: 0.02473662, Gradient norm: 0.43856557
INFO:root:[  111] Training loss: 0.02679275, Validation loss: 0.02500316, Gradient norm: 0.43827358
INFO:root:[  112] Training loss: 0.02739931, Validation loss: 0.02421293, Gradient norm: 0.49105610
INFO:root:[  113] Training loss: 0.02726064, Validation loss: 0.02465732, Gradient norm: 0.48290039
INFO:root:[  114] Training loss: 0.02728910, Validation loss: 0.02460015, Gradient norm: 0.53545424
INFO:root:[  115] Training loss: 0.02709009, Validation loss: 0.02471289, Gradient norm: 0.53266663
INFO:root:[  116] Training loss: 0.02707261, Validation loss: 0.02393221, Gradient norm: 0.48540386
INFO:root:[  117] Training loss: 0.02709081, Validation loss: 0.02448204, Gradient norm: 0.53945158
INFO:root:[  118] Training loss: 0.02793296, Validation loss: 0.02728489, Gradient norm: 0.62058467
INFO:root:[  119] Training loss: 0.02771013, Validation loss: 0.02479934, Gradient norm: 0.67617345
INFO:root:[  120] Training loss: 0.02751469, Validation loss: 0.02419113, Gradient norm: 0.56963081
INFO:root:[  121] Training loss: 0.02735305, Validation loss: 0.02425251, Gradient norm: 0.39533763
INFO:root:[  122] Training loss: 0.02706101, Validation loss: 0.02358613, Gradient norm: 0.61875976
INFO:root:[  123] Training loss: 0.02607073, Validation loss: 0.02395355, Gradient norm: 0.47465375
INFO:root:[  124] Training loss: 0.02688430, Validation loss: 0.02517114, Gradient norm: 0.60115102
INFO:root:[  125] Training loss: 0.02625904, Validation loss: 0.02560275, Gradient norm: 0.48650806
INFO:root:[  126] Training loss: 0.02703924, Validation loss: 0.02618256, Gradient norm: 0.60532525
INFO:root:[  127] Training loss: 0.02691222, Validation loss: 0.02404256, Gradient norm: 0.60817224
INFO:root:[  128] Training loss: 0.02689018, Validation loss: 0.02417625, Gradient norm: 0.60931835
INFO:root:[  129] Training loss: 0.02790048, Validation loss: 0.02388259, Gradient norm: 0.63888167
INFO:root:[  130] Training loss: 0.02738479, Validation loss: 0.02514697, Gradient norm: 0.60519435
INFO:root:[  131] Training loss: 0.02664950, Validation loss: 0.02403066, Gradient norm: 0.57851612
INFO:root:EP 131: Early stopping
INFO:root:Training the model took 253.897s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.72711
INFO:root:EnergyScoreTrain: 0.62478
INFO:root:CoverageTrain: 0.2479
INFO:root:IntervalWidthTrain: 0.0095
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.75923
INFO:root:EnergyScoreValidation: 0.65128
INFO:root:CoverageValidation: 0.24885
INFO:root:IntervalWidthValidation: 0.00987
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.66092
INFO:root:EnergyScoreTest: 1.52838
INFO:root:CoverageTest: 0.05639
INFO:root:IntervalWidthTest: 0.0107
INFO:root:###9 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 211812352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09423843, Validation loss: 0.07567205, Gradient norm: 0.28571895
INFO:root:[    2] Training loss: 0.07760078, Validation loss: 0.06138607, Gradient norm: 0.38625101
INFO:root:[    3] Training loss: 0.06854617, Validation loss: 0.06724998, Gradient norm: 0.55374814
INFO:root:[    4] Training loss: 0.06297043, Validation loss: 0.05129076, Gradient norm: 0.47332243
INFO:root:[    5] Training loss: 0.05919666, Validation loss: 0.04944650, Gradient norm: 0.47676865
INFO:root:[    6] Training loss: 0.05749988, Validation loss: 0.05069051, Gradient norm: 0.39541952
INFO:root:[    7] Training loss: 0.05526927, Validation loss: 0.04587301, Gradient norm: 0.37016909
INFO:root:[    8] Training loss: 0.05462268, Validation loss: 0.04580924, Gradient norm: 0.36705603
INFO:root:[    9] Training loss: 0.05338370, Validation loss: 0.04698154, Gradient norm: 0.37389313
INFO:root:[   10] Training loss: 0.05136801, Validation loss: 0.04358172, Gradient norm: 0.27623946
INFO:root:[   11] Training loss: 0.05226049, Validation loss: 0.04611654, Gradient norm: 0.36637462
INFO:root:[   12] Training loss: 0.05067165, Validation loss: 0.04303108, Gradient norm: 0.26495078
INFO:root:[   13] Training loss: 0.05170152, Validation loss: 0.04301985, Gradient norm: 0.38540263
INFO:root:[   14] Training loss: 0.05023059, Validation loss: 0.04335786, Gradient norm: 0.31005766
INFO:root:[   15] Training loss: 0.04897490, Validation loss: 0.04163405, Gradient norm: 0.35707919
INFO:root:[   16] Training loss: 0.04965663, Validation loss: 0.04275376, Gradient norm: 0.36395243
INFO:root:[   17] Training loss: 0.04741848, Validation loss: 0.03980250, Gradient norm: 0.29493069
INFO:root:[   18] Training loss: 0.04695679, Validation loss: 0.04054684, Gradient norm: 0.25046053
INFO:root:[   19] Training loss: 0.04714131, Validation loss: 0.04063090, Gradient norm: 0.38638564
INFO:root:[   20] Training loss: 0.04719066, Validation loss: 0.03985564, Gradient norm: 0.40208448
INFO:root:[   21] Training loss: 0.04608805, Validation loss: 0.04033875, Gradient norm: 0.33130744
INFO:root:[   22] Training loss: 0.04625290, Validation loss: 0.03778183, Gradient norm: 0.46047005
INFO:root:[   23] Training loss: 0.04514878, Validation loss: 0.03702478, Gradient norm: 0.33651217
INFO:root:[   24] Training loss: 0.04477974, Validation loss: 0.03694305, Gradient norm: 0.27521517
INFO:root:[   25] Training loss: 0.04460597, Validation loss: 0.03659649, Gradient norm: 0.30216476
INFO:root:[   26] Training loss: 0.04391662, Validation loss: 0.03667617, Gradient norm: 0.42291258
INFO:root:[   27] Training loss: 0.04379844, Validation loss: 0.03547790, Gradient norm: 0.34067967
INFO:root:[   28] Training loss: 0.04348132, Validation loss: 0.03538359, Gradient norm: 0.36839654
INFO:root:[   29] Training loss: 0.04275721, Validation loss: 0.03438909, Gradient norm: 0.31656399
INFO:root:[   30] Training loss: 0.04124133, Validation loss: 0.03452457, Gradient norm: 0.28152078
INFO:root:[   31] Training loss: 0.04190158, Validation loss: 0.03498387, Gradient norm: 0.46380225
INFO:root:[   32] Training loss: 0.04097827, Validation loss: 0.03504558, Gradient norm: 0.24813528
INFO:root:[   33] Training loss: 0.04230811, Validation loss: 0.03546948, Gradient norm: 0.42342963
INFO:root:[   34] Training loss: 0.04190365, Validation loss: 0.03388648, Gradient norm: 0.50425960
INFO:root:[   35] Training loss: 0.04069122, Validation loss: 0.03483000, Gradient norm: 0.27401366
INFO:root:[   36] Training loss: 0.04150436, Validation loss: 0.03519372, Gradient norm: 0.38901183
INFO:root:[   37] Training loss: 0.04057798, Validation loss: 0.03378405, Gradient norm: 0.35556012
INFO:root:[   38] Training loss: 0.03988515, Validation loss: 0.03357663, Gradient norm: 0.28322597
INFO:root:[   39] Training loss: 0.04002274, Validation loss: 0.03388307, Gradient norm: 0.30074817
INFO:root:[   40] Training loss: 0.03994235, Validation loss: 0.03485271, Gradient norm: 0.41589414
INFO:root:[   41] Training loss: 0.03982532, Validation loss: 0.03268125, Gradient norm: 0.36835084
INFO:root:[   42] Training loss: 0.03955809, Validation loss: 0.03412607, Gradient norm: 0.41093608
INFO:root:[   43] Training loss: 0.04101566, Validation loss: 0.03359330, Gradient norm: 0.43496125
INFO:root:[   44] Training loss: 0.03972186, Validation loss: 0.03311925, Gradient norm: 0.44351505
INFO:root:[   45] Training loss: 0.03959443, Validation loss: 0.03238449, Gradient norm: 0.38810417
INFO:root:[   46] Training loss: 0.03824227, Validation loss: 0.03363958, Gradient norm: 0.37253459
INFO:root:[   47] Training loss: 0.03916169, Validation loss: 0.03297375, Gradient norm: 0.39040960
INFO:root:[   48] Training loss: 0.03947241, Validation loss: 0.03268050, Gradient norm: 0.37889750
INFO:root:[   49] Training loss: 0.03841626, Validation loss: 0.03622442, Gradient norm: 0.35875993
INFO:root:[   50] Training loss: 0.03911848, Validation loss: 0.03164714, Gradient norm: 0.40905194
INFO:root:[   51] Training loss: 0.03865883, Validation loss: 0.03167186, Gradient norm: 0.41735968
INFO:root:[   52] Training loss: 0.03903024, Validation loss: 0.03160216, Gradient norm: 0.46336627
INFO:root:[   53] Training loss: 0.03762808, Validation loss: 0.03264218, Gradient norm: 0.33414251
INFO:root:[   54] Training loss: 0.03741928, Validation loss: 0.03308654, Gradient norm: 0.36903802
INFO:root:[   55] Training loss: 0.03710981, Validation loss: 0.03214199, Gradient norm: 0.32266805
INFO:root:[   56] Training loss: 0.03745843, Validation loss: 0.03129405, Gradient norm: 0.33141160
INFO:root:[   57] Training loss: 0.03751093, Validation loss: 0.03285947, Gradient norm: 0.36420956
INFO:root:[   58] Training loss: 0.03731020, Validation loss: 0.03187432, Gradient norm: 0.40357685
INFO:root:[   59] Training loss: 0.03698720, Validation loss: 0.03038320, Gradient norm: 0.34619626
INFO:root:[   60] Training loss: 0.03637553, Validation loss: 0.03102672, Gradient norm: 0.30435432
INFO:root:[   61] Training loss: 0.03698798, Validation loss: 0.03071560, Gradient norm: 0.41693777
INFO:root:[   62] Training loss: 0.03638426, Validation loss: 0.03057049, Gradient norm: 0.35964743
INFO:root:[   63] Training loss: 0.03646251, Validation loss: 0.03167853, Gradient norm: 0.33002540
INFO:root:[   64] Training loss: 0.03593345, Validation loss: 0.03098025, Gradient norm: 0.31036293
INFO:root:[   65] Training loss: 0.03633776, Validation loss: 0.03006124, Gradient norm: 0.37186310
INFO:root:[   66] Training loss: 0.03689637, Validation loss: 0.03005495, Gradient norm: 0.48705394
INFO:root:[   67] Training loss: 0.03673224, Validation loss: 0.02956245, Gradient norm: 0.44786088
INFO:root:[   68] Training loss: 0.03546974, Validation loss: 0.03068481, Gradient norm: 0.31889430
INFO:root:[   69] Training loss: 0.03555086, Validation loss: 0.02989600, Gradient norm: 0.30322387
INFO:root:[   70] Training loss: 0.03604421, Validation loss: 0.02979597, Gradient norm: 0.34977616
INFO:root:[   71] Training loss: 0.03477983, Validation loss: 0.02962400, Gradient norm: 0.35738307
INFO:root:[   72] Training loss: 0.03639501, Validation loss: 0.02923632, Gradient norm: 0.39577426
INFO:root:[   73] Training loss: 0.03516636, Validation loss: 0.02965092, Gradient norm: 0.35261786
INFO:root:[   74] Training loss: 0.03512628, Validation loss: 0.02982761, Gradient norm: 0.36094241
INFO:root:[   75] Training loss: 0.03524891, Validation loss: 0.03128546, Gradient norm: 0.34989745
INFO:root:[   76] Training loss: 0.03535088, Validation loss: 0.03078540, Gradient norm: 0.48437073
INFO:root:[   77] Training loss: 0.03593358, Validation loss: 0.03075315, Gradient norm: 0.50904494
INFO:root:[   78] Training loss: 0.03456526, Validation loss: 0.02869823, Gradient norm: 0.26941314
INFO:root:[   79] Training loss: 0.03438740, Validation loss: 0.02865333, Gradient norm: 0.27923307
INFO:root:[   80] Training loss: 0.03440971, Validation loss: 0.02854683, Gradient norm: 0.29857132
INFO:root:[   81] Training loss: 0.03398418, Validation loss: 0.02805369, Gradient norm: 0.28339632
INFO:root:[   82] Training loss: 0.03421505, Validation loss: 0.02831959, Gradient norm: 0.36961129
INFO:root:[   83] Training loss: 0.03395872, Validation loss: 0.02809426, Gradient norm: 0.31146576
INFO:root:[   84] Training loss: 0.03464358, Validation loss: 0.02791036, Gradient norm: 0.46814559
INFO:root:[   85] Training loss: 0.03418538, Validation loss: 0.03212827, Gradient norm: 0.43062083
INFO:root:[   86] Training loss: 0.03461988, Validation loss: 0.02761859, Gradient norm: 0.49550194
INFO:root:[   87] Training loss: 0.03340759, Validation loss: 0.02829939, Gradient norm: 0.30847819
INFO:root:[   88] Training loss: 0.03347604, Validation loss: 0.02795519, Gradient norm: 0.31225902
INFO:root:[   89] Training loss: 0.03333445, Validation loss: 0.02952543, Gradient norm: 0.31539032
INFO:root:[   90] Training loss: 0.03375787, Validation loss: 0.02847226, Gradient norm: 0.47991013
INFO:root:[   91] Training loss: 0.03302744, Validation loss: 0.02840351, Gradient norm: 0.32064677
INFO:root:[   92] Training loss: 0.03338444, Validation loss: 0.02737886, Gradient norm: 0.42516529
INFO:root:[   93] Training loss: 0.03265749, Validation loss: 0.02806015, Gradient norm: 0.30665614
INFO:root:[   94] Training loss: 0.03245038, Validation loss: 0.02727203, Gradient norm: 0.34957291
INFO:root:[   95] Training loss: 0.03376883, Validation loss: 0.02769152, Gradient norm: 0.34718796
INFO:root:[   96] Training loss: 0.03216077, Validation loss: 0.02802858, Gradient norm: 0.30663904
INFO:root:[   97] Training loss: 0.03301845, Validation loss: 0.02886095, Gradient norm: 0.45431909
INFO:root:[   98] Training loss: 0.03306758, Validation loss: 0.02662761, Gradient norm: 0.34735174
INFO:root:[   99] Training loss: 0.03204326, Validation loss: 0.02691651, Gradient norm: 0.28911973
INFO:root:[  100] Training loss: 0.03178755, Validation loss: 0.02762497, Gradient norm: 0.30441921
INFO:root:[  101] Training loss: 0.03267642, Validation loss: 0.02720390, Gradient norm: 0.34434619
INFO:root:[  102] Training loss: 0.03255238, Validation loss: 0.02854286, Gradient norm: 0.42510384
INFO:root:[  103] Training loss: 0.03220725, Validation loss: 0.02757246, Gradient norm: 0.43167962
INFO:root:[  104] Training loss: 0.03207182, Validation loss: 0.02694965, Gradient norm: 0.32064632
INFO:root:[  105] Training loss: 0.03198390, Validation loss: 0.02742165, Gradient norm: 0.35437077
INFO:root:[  106] Training loss: 0.03180972, Validation loss: 0.02698674, Gradient norm: 0.34154531
INFO:root:[  107] Training loss: 0.03199143, Validation loss: 0.02814563, Gradient norm: 0.31557331
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 208.848s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.83521
INFO:root:EnergyScoreTrain: 0.70191
INFO:root:CoverageTrain: 0.28423
INFO:root:IntervalWidthTrain: 0.01275
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.85891
INFO:root:EnergyScoreValidation: 0.7198
INFO:root:CoverageValidation: 0.27693
INFO:root:IntervalWidthValidation: 0.01294
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.64104
INFO:root:EnergyScoreTest: 2.40542
INFO:root:CoverageTest: 0.03674
INFO:root:IntervalWidthTest: 0.01884
INFO:root:###10 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 251658240
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.10533272, Validation loss: 0.08598569, Gradient norm: 0.49885272
INFO:root:[    2] Training loss: 0.08447936, Validation loss: 0.06670729, Gradient norm: 0.32585549
INFO:root:[    3] Training loss: 0.07594713, Validation loss: 0.06579085, Gradient norm: 0.39934799
INFO:root:[    4] Training loss: 0.07101464, Validation loss: 0.05786036, Gradient norm: 0.30529222
INFO:root:[    5] Training loss: 0.06486174, Validation loss: 0.05124731, Gradient norm: 0.35573984
INFO:root:[    6] Training loss: 0.06252974, Validation loss: 0.04689530, Gradient norm: 0.28719878
INFO:root:[    7] Training loss: 0.05940037, Validation loss: 0.04708493, Gradient norm: 0.33496611
INFO:root:[    8] Training loss: 0.05622497, Validation loss: 0.04689724, Gradient norm: 0.31324263
INFO:root:[    9] Training loss: 0.05523003, Validation loss: 0.04302745, Gradient norm: 0.28086059
INFO:root:[   10] Training loss: 0.05377767, Validation loss: 0.04188443, Gradient norm: 0.28657562
INFO:root:[   11] Training loss: 0.05349022, Validation loss: 0.04507628, Gradient norm: 0.41741179
INFO:root:[   12] Training loss: 0.05184397, Validation loss: 0.04176330, Gradient norm: 0.27599954
INFO:root:[   13] Training loss: 0.05210409, Validation loss: 0.04092499, Gradient norm: 0.30636228
INFO:root:[   14] Training loss: 0.05133018, Validation loss: 0.04044963, Gradient norm: 0.27911069
INFO:root:[   15] Training loss: 0.04982636, Validation loss: 0.04097418, Gradient norm: 0.25120169
INFO:root:[   16] Training loss: 0.05007432, Validation loss: 0.04024631, Gradient norm: 0.30705575
INFO:root:[   17] Training loss: 0.04972979, Validation loss: 0.04351661, Gradient norm: 0.29866344
INFO:root:[   18] Training loss: 0.04840184, Validation loss: 0.04105368, Gradient norm: 0.25514830
INFO:root:[   19] Training loss: 0.04871796, Validation loss: 0.03899435, Gradient norm: 0.29898635
INFO:root:[   20] Training loss: 0.04748413, Validation loss: 0.03946744, Gradient norm: 0.22319065
INFO:root:[   21] Training loss: 0.04875292, Validation loss: 0.03992612, Gradient norm: 0.32379291
INFO:root:[   22] Training loss: 0.04688638, Validation loss: 0.04060399, Gradient norm: 0.27745481
INFO:root:[   23] Training loss: 0.04824393, Validation loss: 0.03944319, Gradient norm: 0.30547121
INFO:root:[   24] Training loss: 0.04620479, Validation loss: 0.03901025, Gradient norm: 0.30005788
INFO:root:[   25] Training loss: 0.04603706, Validation loss: 0.04000266, Gradient norm: 0.25703572
INFO:root:[   26] Training loss: 0.04590010, Validation loss: 0.04046790, Gradient norm: 0.28206683
INFO:root:[   27] Training loss: 0.04494596, Validation loss: 0.04204106, Gradient norm: 0.32753533
INFO:root:[   28] Training loss: 0.04621563, Validation loss: 0.04023230, Gradient norm: 0.34293155
INFO:root:[   29] Training loss: 0.04448160, Validation loss: 0.03619812, Gradient norm: 0.32026522
INFO:root:[   30] Training loss: 0.04446790, Validation loss: 0.03835145, Gradient norm: 0.30758212
INFO:root:[   31] Training loss: 0.04318441, Validation loss: 0.03754950, Gradient norm: 0.21009715
INFO:root:[   32] Training loss: 0.04420970, Validation loss: 0.03617152, Gradient norm: 0.31818485
INFO:root:[   33] Training loss: 0.04319021, Validation loss: 0.03570601, Gradient norm: 0.37488383
INFO:root:[   34] Training loss: 0.04342586, Validation loss: 0.03608934, Gradient norm: 0.31109062
INFO:root:[   35] Training loss: 0.04257239, Validation loss: 0.03490533, Gradient norm: 0.26951673
INFO:root:[   36] Training loss: 0.04276650, Validation loss: 0.03645884, Gradient norm: 0.32537397
INFO:root:[   37] Training loss: 0.04192162, Validation loss: 0.03449539, Gradient norm: 0.22218495
INFO:root:[   38] Training loss: 0.04251238, Validation loss: 0.03460648, Gradient norm: 0.33308415
INFO:root:[   39] Training loss: 0.04198430, Validation loss: 0.03477346, Gradient norm: 0.38161498
INFO:root:[   40] Training loss: 0.04217621, Validation loss: 0.03862726, Gradient norm: 0.40430722
INFO:root:[   41] Training loss: 0.04213823, Validation loss: 0.03510669, Gradient norm: 0.41306259
INFO:root:[   42] Training loss: 0.04126847, Validation loss: 0.03624068, Gradient norm: 0.39644400
INFO:root:[   43] Training loss: 0.04119976, Validation loss: 0.03527233, Gradient norm: 0.28893204
INFO:root:[   44] Training loss: 0.04033548, Validation loss: 0.03441185, Gradient norm: 0.32467902
INFO:root:[   45] Training loss: 0.04119564, Validation loss: 0.03250997, Gradient norm: 0.38330458
INFO:root:[   46] Training loss: 0.03928998, Validation loss: 0.03237068, Gradient norm: 0.30088299
INFO:root:[   47] Training loss: 0.03973659, Validation loss: 0.03305698, Gradient norm: 0.28956133
INFO:root:[   48] Training loss: 0.03960120, Validation loss: 0.03385699, Gradient norm: 0.36749135
INFO:root:[   49] Training loss: 0.03946986, Validation loss: 0.03327682, Gradient norm: 0.28277644
INFO:root:[   50] Training loss: 0.04026287, Validation loss: 0.03236858, Gradient norm: 0.35424853
INFO:root:[   51] Training loss: 0.03939490, Validation loss: 0.03313423, Gradient norm: 0.33789561
INFO:root:[   52] Training loss: 0.03883009, Validation loss: 0.03156426, Gradient norm: 0.33212827
INFO:root:[   53] Training loss: 0.03850485, Validation loss: 0.03245413, Gradient norm: 0.29373612
INFO:root:[   54] Training loss: 0.03845874, Validation loss: 0.03137542, Gradient norm: 0.29942356
INFO:root:[   55] Training loss: 0.03946712, Validation loss: 0.03225489, Gradient norm: 0.42992774
INFO:root:[   56] Training loss: 0.03845266, Validation loss: 0.03239356, Gradient norm: 0.28853568
INFO:root:[   57] Training loss: 0.03834882, Validation loss: 0.03240372, Gradient norm: 0.30982110
INFO:root:[   58] Training loss: 0.03874901, Validation loss: 0.03101867, Gradient norm: 0.31409084
INFO:root:[   59] Training loss: 0.03778272, Validation loss: 0.03203062, Gradient norm: 0.33880837
INFO:root:[   60] Training loss: 0.03854188, Validation loss: 0.03124550, Gradient norm: 0.39168255
INFO:root:[   61] Training loss: 0.03817604, Validation loss: 0.03329365, Gradient norm: 0.30425542
INFO:root:[   62] Training loss: 0.03804905, Validation loss: 0.03298822, Gradient norm: 0.28813236
INFO:root:[   63] Training loss: 0.03691786, Validation loss: 0.03167691, Gradient norm: 0.31014364
INFO:root:[   64] Training loss: 0.03795118, Validation loss: 0.03120223, Gradient norm: 0.29938679
INFO:root:[   65] Training loss: 0.03770633, Validation loss: 0.02975771, Gradient norm: 0.37218511
INFO:root:[   66] Training loss: 0.03679245, Validation loss: 0.02997077, Gradient norm: 0.35938998
INFO:root:[   67] Training loss: 0.03723696, Validation loss: 0.03072969, Gradient norm: 0.39620171
INFO:root:[   68] Training loss: 0.03673591, Validation loss: 0.03020718, Gradient norm: 0.29674886
INFO:root:[   69] Training loss: 0.03685399, Validation loss: 0.02989335, Gradient norm: 0.34961200
INFO:root:[   70] Training loss: 0.03703546, Validation loss: 0.02945183, Gradient norm: 0.31928716
INFO:root:[   71] Training loss: 0.03762559, Validation loss: 0.02972553, Gradient norm: 0.35559271
INFO:root:[   72] Training loss: 0.03717785, Validation loss: 0.02982714, Gradient norm: 0.40711121
INFO:root:[   73] Training loss: 0.03620837, Validation loss: 0.02997596, Gradient norm: 0.27856469
INFO:root:[   74] Training loss: 0.03581443, Validation loss: 0.03078447, Gradient norm: 0.31624112
INFO:root:[   75] Training loss: 0.03645488, Validation loss: 0.03018676, Gradient norm: 0.34013567
INFO:root:[   76] Training loss: 0.03549085, Validation loss: 0.03019573, Gradient norm: 0.29480073
INFO:root:[   77] Training loss: 0.03621270, Validation loss: 0.02987144, Gradient norm: 0.27504630
INFO:root:[   78] Training loss: 0.03567963, Validation loss: 0.03317512, Gradient norm: 0.33382295
INFO:root:[   79] Training loss: 0.03530067, Validation loss: 0.03230963, Gradient norm: 0.28321402
INFO:root:[   80] Training loss: 0.03557162, Validation loss: 0.02898386, Gradient norm: 0.35065251
INFO:root:[   81] Training loss: 0.03602524, Validation loss: 0.02848003, Gradient norm: 0.37760703
INFO:root:[   82] Training loss: 0.03500287, Validation loss: 0.02931562, Gradient norm: 0.32891970
INFO:root:[   83] Training loss: 0.03664498, Validation loss: 0.02904229, Gradient norm: 0.32796366
INFO:root:[   84] Training loss: 0.03547499, Validation loss: 0.02845135, Gradient norm: 0.36465531
INFO:root:[   85] Training loss: 0.03529606, Validation loss: 0.03253307, Gradient norm: 0.32017857
INFO:root:[   86] Training loss: 0.03480726, Validation loss: 0.03057623, Gradient norm: 0.33948969
INFO:root:[   87] Training loss: 0.03528472, Validation loss: 0.02950235, Gradient norm: 0.32339780
INFO:root:[   88] Training loss: 0.03522898, Validation loss: 0.03215240, Gradient norm: 0.32924035
INFO:root:[   89] Training loss: 0.03534143, Validation loss: 0.02881470, Gradient norm: 0.37740218
INFO:root:[   90] Training loss: 0.03404608, Validation loss: 0.02897629, Gradient norm: 0.29293543
INFO:root:[   91] Training loss: 0.03423572, Validation loss: 0.02784168, Gradient norm: 0.31364081
INFO:root:[   92] Training loss: 0.03530117, Validation loss: 0.02763973, Gradient norm: 0.35995362
INFO:root:[   93] Training loss: 0.03452825, Validation loss: 0.03045217, Gradient norm: 0.31873482
INFO:root:[   94] Training loss: 0.03414034, Validation loss: 0.02880077, Gradient norm: 0.30008747
INFO:root:[   95] Training loss: 0.03417342, Validation loss: 0.02766030, Gradient norm: 0.31744649
INFO:root:[   96] Training loss: 0.03440550, Validation loss: 0.02802065, Gradient norm: 0.36612845
INFO:root:[   97] Training loss: 0.03349357, Validation loss: 0.02833174, Gradient norm: 0.26758218
INFO:root:[   98] Training loss: 0.03444814, Validation loss: 0.02898161, Gradient norm: 0.36566080
INFO:root:[   99] Training loss: 0.03416335, Validation loss: 0.03034104, Gradient norm: 0.33778217
INFO:root:[  100] Training loss: 0.03484042, Validation loss: 0.02937731, Gradient norm: 0.38272180
INFO:root:[  101] Training loss: 0.03431036, Validation loss: 0.02720533, Gradient norm: 0.36416717
INFO:root:[  102] Training loss: 0.03357451, Validation loss: 0.02780768, Gradient norm: 0.29316970
INFO:root:[  103] Training loss: 0.03330953, Validation loss: 0.02900365, Gradient norm: 0.28941412
INFO:root:[  104] Training loss: 0.03309753, Validation loss: 0.03142619, Gradient norm: 0.30325838
INFO:root:[  105] Training loss: 0.03318793, Validation loss: 0.02885665, Gradient norm: 0.33021655
INFO:root:[  106] Training loss: 0.03275289, Validation loss: 0.02757168, Gradient norm: 0.23627685
INFO:root:[  107] Training loss: 0.03292610, Validation loss: 0.02751214, Gradient norm: 0.27642983
INFO:root:[  108] Training loss: 0.03383194, Validation loss: 0.02768673, Gradient norm: 0.36435538
INFO:root:[  109] Training loss: 0.03350208, Validation loss: 0.02995637, Gradient norm: 0.38279696
INFO:root:[  110] Training loss: 0.03302215, Validation loss: 0.02761563, Gradient norm: 0.38108708
INFO:root:EP 110: Early stopping
INFO:root:Training the model took 214.286s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.89452
INFO:root:EnergyScoreTrain: 0.67501
INFO:root:CoverageTrain: 0.40542
INFO:root:IntervalWidthTrain: 0.0251
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91074
INFO:root:EnergyScoreValidation: 0.69274
INFO:root:CoverageValidation: 0.39025
INFO:root:IntervalWidthValidation: 0.02395
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.18314
INFO:root:EnergyScoreTest: 0.93168
INFO:root:CoverageTest: 0.29655
INFO:root:IntervalWidthTest: 0.02498
INFO:root:###11 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 211812352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.12076053, Validation loss: 0.08940664, Gradient norm: 0.46912529
INFO:root:[    2] Training loss: 0.09570428, Validation loss: 0.07345978, Gradient norm: 0.25630801
INFO:root:[    3] Training loss: 0.08297615, Validation loss: 0.07288269, Gradient norm: 0.22594566
INFO:root:[    4] Training loss: 0.07856792, Validation loss: 0.05845321, Gradient norm: 0.27575937
INFO:root:[    5] Training loss: 0.07102861, Validation loss: 0.05345778, Gradient norm: 0.28075770
INFO:root:[    6] Training loss: 0.06854412, Validation loss: 0.05281695, Gradient norm: 0.29060636
INFO:root:[    7] Training loss: 0.06663392, Validation loss: 0.05916995, Gradient norm: 0.31637935
INFO:root:[    8] Training loss: 0.06307260, Validation loss: 0.05062075, Gradient norm: 0.28414350
INFO:root:[    9] Training loss: 0.06238552, Validation loss: 0.05735868, Gradient norm: 0.32762514
INFO:root:[   10] Training loss: 0.06093460, Validation loss: 0.05041037, Gradient norm: 0.25923453
INFO:root:[   11] Training loss: 0.05940799, Validation loss: 0.04757826, Gradient norm: 0.27998667
INFO:root:[   12] Training loss: 0.05925347, Validation loss: 0.04275754, Gradient norm: 0.38461670
INFO:root:[   13] Training loss: 0.05846509, Validation loss: 0.04762239, Gradient norm: 0.38131762
INFO:root:[   14] Training loss: 0.05645502, Validation loss: 0.04893383, Gradient norm: 0.34014881
INFO:root:[   15] Training loss: 0.05693636, Validation loss: 0.04207218, Gradient norm: 0.39693919
INFO:root:[   16] Training loss: 0.05545475, Validation loss: 0.04462045, Gradient norm: 0.36262061
INFO:root:[   17] Training loss: 0.05478145, Validation loss: 0.04548081, Gradient norm: 0.25274295
INFO:root:[   18] Training loss: 0.05389475, Validation loss: 0.05014752, Gradient norm: 0.27610728
INFO:root:[   19] Training loss: 0.05340534, Validation loss: 0.04515287, Gradient norm: 0.25079707
INFO:root:[   20] Training loss: 0.05299123, Validation loss: 0.05349232, Gradient norm: 0.30751447
INFO:root:[   21] Training loss: 0.05309634, Validation loss: 0.05173246, Gradient norm: 0.46173208
INFO:root:[   22] Training loss: 0.05238036, Validation loss: 0.04784352, Gradient norm: 0.39976131
INFO:root:[   23] Training loss: 0.05083075, Validation loss: 0.04420054, Gradient norm: 0.25476070
INFO:root:[   24] Training loss: 0.05184641, Validation loss: 0.04334516, Gradient norm: 0.37011392
INFO:root:[   25] Training loss: 0.05032491, Validation loss: 0.04332305, Gradient norm: 0.35782553
INFO:root:[   26] Training loss: 0.05032971, Validation loss: 0.04290792, Gradient norm: 0.32283230
INFO:root:[   27] Training loss: 0.05002061, Validation loss: 0.04394036, Gradient norm: 0.33051828
INFO:root:[   28] Training loss: 0.04935034, Validation loss: 0.04071517, Gradient norm: 0.37254196
INFO:root:[   29] Training loss: 0.04909783, Validation loss: 0.04097538, Gradient norm: 0.31806622
INFO:root:[   30] Training loss: 0.04908816, Validation loss: 0.04244571, Gradient norm: 0.34205562
INFO:root:[   31] Training loss: 0.04852838, Validation loss: 0.03930496, Gradient norm: 0.29784177
INFO:root:[   32] Training loss: 0.04834938, Validation loss: 0.04683036, Gradient norm: 0.33541490
INFO:root:[   33] Training loss: 0.04880232, Validation loss: 0.04326469, Gradient norm: 0.38151350
INFO:root:[   34] Training loss: 0.04706922, Validation loss: 0.04414286, Gradient norm: 0.32462134
INFO:root:[   35] Training loss: 0.04703509, Validation loss: 0.04945366, Gradient norm: 0.41403669
INFO:root:[   36] Training loss: 0.04731288, Validation loss: 0.04226174, Gradient norm: 0.37710312
INFO:root:[   37] Training loss: 0.04674524, Validation loss: 0.04295034, Gradient norm: 0.35304073
INFO:root:[   38] Training loss: 0.04610057, Validation loss: 0.04379173, Gradient norm: 0.32154250
INFO:root:[   39] Training loss: 0.04680363, Validation loss: 0.04052728, Gradient norm: 0.33573643
INFO:root:[   40] Training loss: 0.04615213, Validation loss: 0.03940383, Gradient norm: 0.32884391
INFO:root:[   41] Training loss: 0.04609339, Validation loss: 0.03862307, Gradient norm: 0.41904226
INFO:root:[   42] Training loss: 0.04541885, Validation loss: 0.03824834, Gradient norm: 0.27439178
INFO:root:[   43] Training loss: 0.04682906, Validation loss: 0.03971688, Gradient norm: 0.42659458
INFO:root:[   44] Training loss: 0.04482544, Validation loss: 0.03751232, Gradient norm: 0.36372047
INFO:root:[   45] Training loss: 0.04548369, Validation loss: 0.03817937, Gradient norm: 0.38727342
INFO:root:[   46] Training loss: 0.04416954, Validation loss: 0.04304443, Gradient norm: 0.34197592
INFO:root:[   47] Training loss: 0.04433140, Validation loss: 0.03834416, Gradient norm: 0.34547396
INFO:root:[   48] Training loss: 0.04381470, Validation loss: 0.04076551, Gradient norm: 0.34682645
INFO:root:[   49] Training loss: 0.04512086, Validation loss: 0.04560924, Gradient norm: 0.34122378
INFO:root:[   50] Training loss: 0.04399243, Validation loss: 0.03860949, Gradient norm: 0.31797035
INFO:root:[   51] Training loss: 0.04359107, Validation loss: 0.04115865, Gradient norm: 0.30209467
INFO:root:[   52] Training loss: 0.04395738, Validation loss: 0.03876436, Gradient norm: 0.30259713
INFO:root:[   53] Training loss: 0.04343508, Validation loss: 0.03996227, Gradient norm: 0.31114767
INFO:root:[   54] Training loss: 0.04357628, Validation loss: 0.03777696, Gradient norm: 0.30384259
INFO:root:[   55] Training loss: 0.04386983, Validation loss: 0.04189075, Gradient norm: 0.34011580
INFO:root:[   56] Training loss: 0.04278820, Validation loss: 0.04422502, Gradient norm: 0.31788978
INFO:root:[   57] Training loss: 0.04266386, Validation loss: 0.03975333, Gradient norm: 0.34174985
INFO:root:[   58] Training loss: 0.04286169, Validation loss: 0.03882594, Gradient norm: 0.27490148
INFO:root:[   59] Training loss: 0.04271256, Validation loss: 0.04022158, Gradient norm: 0.29858263
INFO:root:[   60] Training loss: 0.04189949, Validation loss: 0.03823857, Gradient norm: 0.27891571
INFO:root:[   61] Training loss: 0.04321777, Validation loss: 0.04162739, Gradient norm: 0.39275238
INFO:root:[   62] Training loss: 0.04275237, Validation loss: 0.03966086, Gradient norm: 0.33375949
INFO:root:[   63] Training loss: 0.04251296, Validation loss: 0.04517286, Gradient norm: 0.34685497
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 123.779s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.22909
INFO:root:EnergyScoreTrain: 0.94178
INFO:root:CoverageTrain: 0.38955
INFO:root:IntervalWidthTrain: 0.03196
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.27155
INFO:root:EnergyScoreValidation: 0.97133
INFO:root:CoverageValidation: 0.37922
INFO:root:IntervalWidthValidation: 0.03259
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.68243
INFO:root:EnergyScoreTest: 1.26806
INFO:root:CoverageTest: 0.31381
INFO:root:IntervalWidthTest: 0.04358
INFO:root:###12 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 251658240
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.13591153, Validation loss: 0.08760479, Gradient norm: 0.44616457
INFO:root:[    2] Training loss: 0.10500865, Validation loss: 0.07114754, Gradient norm: 0.33900242
INFO:root:[    3] Training loss: 0.09434691, Validation loss: 0.06532428, Gradient norm: 0.32214112
INFO:root:[    4] Training loss: 0.08582846, Validation loss: 0.06278597, Gradient norm: 0.30277540
INFO:root:[    5] Training loss: 0.08257818, Validation loss: 0.06186393, Gradient norm: 0.36867052
INFO:root:[    6] Training loss: 0.07659470, Validation loss: 0.05567448, Gradient norm: 0.27502368
INFO:root:[    7] Training loss: 0.07462369, Validation loss: 0.05442648, Gradient norm: 0.46247252
INFO:root:[    8] Training loss: 0.07126703, Validation loss: 0.04844481, Gradient norm: 0.34871011
INFO:root:[    9] Training loss: 0.06968118, Validation loss: 0.05147213, Gradient norm: 0.43208455
INFO:root:[   10] Training loss: 0.06804456, Validation loss: 0.04708593, Gradient norm: 0.33178190
INFO:root:[   11] Training loss: 0.06618886, Validation loss: 0.04791673, Gradient norm: 0.23922209
INFO:root:[   12] Training loss: 0.06538338, Validation loss: 0.04723076, Gradient norm: 0.33257909
INFO:root:[   13] Training loss: 0.06294782, Validation loss: 0.04795064, Gradient norm: 0.29171575
INFO:root:[   14] Training loss: 0.06293894, Validation loss: 0.04705562, Gradient norm: 0.41481082
INFO:root:[   15] Training loss: 0.06208352, Validation loss: 0.04422703, Gradient norm: 0.32469120
INFO:root:[   16] Training loss: 0.06131944, Validation loss: 0.04431378, Gradient norm: 0.29344645
INFO:root:[   17] Training loss: 0.06051381, Validation loss: 0.04692566, Gradient norm: 0.23805150
INFO:root:[   18] Training loss: 0.05962822, Validation loss: 0.04468146, Gradient norm: 0.31992325
INFO:root:[   19] Training loss: 0.05922803, Validation loss: 0.04291632, Gradient norm: 0.34843110
INFO:root:[   20] Training loss: 0.05768701, Validation loss: 0.04469646, Gradient norm: 0.27257147
INFO:root:[   21] Training loss: 0.05807485, Validation loss: 0.04274871, Gradient norm: 0.26069436
INFO:root:[   22] Training loss: 0.05841909, Validation loss: 0.04426945, Gradient norm: 0.26032418
INFO:root:[   23] Training loss: 0.05548064, Validation loss: 0.04261794, Gradient norm: 0.22081524
INFO:root:[   24] Training loss: 0.05563704, Validation loss: 0.04382694, Gradient norm: 0.22825875
INFO:root:[   25] Training loss: 0.05471551, Validation loss: 0.04319652, Gradient norm: 0.26140896
INFO:root:[   26] Training loss: 0.05553197, Validation loss: 0.04686975, Gradient norm: 0.34113656
INFO:root:[   27] Training loss: 0.05583727, Validation loss: 0.04159298, Gradient norm: 0.37317170
INFO:root:[   28] Training loss: 0.05444861, Validation loss: 0.04146947, Gradient norm: 0.35274630
INFO:root:[   29] Training loss: 0.05330005, Validation loss: 0.04219024, Gradient norm: 0.29985244
INFO:root:[   30] Training loss: 0.05327204, Validation loss: 0.04072858, Gradient norm: 0.29015055
INFO:root:[   31] Training loss: 0.05258115, Validation loss: 0.04170638, Gradient norm: 0.27413424
INFO:root:[   32] Training loss: 0.05272166, Validation loss: 0.04278582, Gradient norm: 0.35463329
INFO:root:[   33] Training loss: 0.05257090, Validation loss: 0.04129067, Gradient norm: 0.31878516
INFO:root:[   34] Training loss: 0.05193785, Validation loss: 0.03976517, Gradient norm: 0.34471102
INFO:root:[   35] Training loss: 0.05219264, Validation loss: 0.03988401, Gradient norm: 0.30991351
INFO:root:[   36] Training loss: 0.05130583, Validation loss: 0.04154443, Gradient norm: 0.42319826
INFO:root:[   37] Training loss: 0.05107898, Validation loss: 0.03879878, Gradient norm: 0.30306862
INFO:root:[   38] Training loss: 0.05032071, Validation loss: 0.03925455, Gradient norm: 0.27109992
INFO:root:[   39] Training loss: 0.05045101, Validation loss: 0.04194827, Gradient norm: 0.27644200
INFO:root:[   40] Training loss: 0.05028596, Validation loss: 0.04005931, Gradient norm: 0.37114394
INFO:root:[   41] Training loss: 0.04958648, Validation loss: 0.03823131, Gradient norm: 0.28986695
INFO:root:[   42] Training loss: 0.04996131, Validation loss: 0.03877151, Gradient norm: 0.28923586
INFO:root:[   43] Training loss: 0.04848855, Validation loss: 0.03737280, Gradient norm: 0.25961519
INFO:root:[   44] Training loss: 0.04871177, Validation loss: 0.03784219, Gradient norm: 0.31399988
INFO:root:[   45] Training loss: 0.04942772, Validation loss: 0.03868445, Gradient norm: 0.26640601
INFO:root:[   46] Training loss: 0.04805100, Validation loss: 0.03812064, Gradient norm: 0.29803254
INFO:root:[   47] Training loss: 0.04746796, Validation loss: 0.03930866, Gradient norm: 0.30129817
INFO:root:[   48] Training loss: 0.04704070, Validation loss: 0.03673033, Gradient norm: 0.35988178
INFO:root:[   49] Training loss: 0.04810014, Validation loss: 0.03753526, Gradient norm: 0.41940535
INFO:root:[   50] Training loss: 0.04678411, Validation loss: 0.03702186, Gradient norm: 0.31038508
INFO:root:[   51] Training loss: 0.04630125, Validation loss: 0.03771678, Gradient norm: 0.33197645
INFO:root:[   52] Training loss: 0.04593355, Validation loss: 0.03922603, Gradient norm: 0.29628128
INFO:root:[   53] Training loss: 0.04705814, Validation loss: 0.03763083, Gradient norm: 0.39338845
INFO:root:[   54] Training loss: 0.04616252, Validation loss: 0.03669828, Gradient norm: 0.28430022
INFO:root:[   55] Training loss: 0.04498476, Validation loss: 0.03517306, Gradient norm: 0.32277837
INFO:root:[   56] Training loss: 0.04600260, Validation loss: 0.03751630, Gradient norm: 0.33261493
INFO:root:[   57] Training loss: 0.04532038, Validation loss: 0.03564790, Gradient norm: 0.32709148
INFO:root:[   58] Training loss: 0.04515566, Validation loss: 0.04028995, Gradient norm: 0.39917125
INFO:root:[   59] Training loss: 0.04547246, Validation loss: 0.03483006, Gradient norm: 0.41062775
INFO:root:[   60] Training loss: 0.04431456, Validation loss: 0.03654750, Gradient norm: 0.35257338
INFO:root:[   61] Training loss: 0.04476367, Validation loss: 0.03658956, Gradient norm: 0.24716096
INFO:root:[   62] Training loss: 0.04405471, Validation loss: 0.03690988, Gradient norm: 0.33789947
INFO:root:[   63] Training loss: 0.04383928, Validation loss: 0.03519511, Gradient norm: 0.33714072
INFO:root:[   64] Training loss: 0.04365887, Validation loss: 0.03632088, Gradient norm: 0.29926872
INFO:root:[   65] Training loss: 0.04374236, Validation loss: 0.03437448, Gradient norm: 0.30888820
INFO:root:[   66] Training loss: 0.04379481, Validation loss: 0.03668128, Gradient norm: 0.31971195
INFO:root:[   67] Training loss: 0.04311925, Validation loss: 0.03820285, Gradient norm: 0.25807551
INFO:root:[   68] Training loss: 0.04420597, Validation loss: 0.03389609, Gradient norm: 0.30213717
INFO:root:[   69] Training loss: 0.04307702, Validation loss: 0.03485588, Gradient norm: 0.35794028
INFO:root:[   70] Training loss: 0.04243823, Validation loss: 0.03591597, Gradient norm: 0.31305692
INFO:root:[   71] Training loss: 0.04359004, Validation loss: 0.03630538, Gradient norm: 0.31003800
INFO:root:[   72] Training loss: 0.04340802, Validation loss: 0.03573660, Gradient norm: 0.38678866
INFO:root:[   73] Training loss: 0.04230640, Validation loss: 0.03543191, Gradient norm: 0.27094458
INFO:root:[   74] Training loss: 0.04291386, Validation loss: 0.03555909, Gradient norm: 0.27887902
INFO:root:[   75] Training loss: 0.04264987, Validation loss: 0.03582576, Gradient norm: 0.40363231
INFO:root:[   76] Training loss: 0.04309145, Validation loss: 0.03517276, Gradient norm: 0.34585007
INFO:root:[   77] Training loss: 0.04219859, Validation loss: 0.03435816, Gradient norm: 0.34082308
INFO:root:EP 77: Early stopping
INFO:root:Training the model took 150.779s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.19167
INFO:root:EnergyScoreTrain: 0.82193
INFO:root:CoverageTrain: 0.48978
INFO:root:IntervalWidthTrain: 0.04538
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.20136
INFO:root:EnergyScoreValidation: 0.80651
INFO:root:CoverageValidation: 0.49972
INFO:root:IntervalWidthValidation: 0.04786
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.49069
INFO:root:EnergyScoreTest: 1.04098
INFO:root:CoverageTest: 0.40279
INFO:root:IntervalWidthTest: 0.0508
INFO:root:###13 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 211812352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.18911381, Validation loss: 0.10699792, Gradient norm: 0.65610573
INFO:root:[    2] Training loss: 0.13562055, Validation loss: 0.09773279, Gradient norm: 0.32704413
INFO:root:[    3] Training loss: 0.11756872, Validation loss: 0.08987800, Gradient norm: 0.29188660
INFO:root:[    4] Training loss: 0.10724504, Validation loss: 0.08884346, Gradient norm: 0.26299313
INFO:root:[    5] Training loss: 0.10055596, Validation loss: 0.08434681, Gradient norm: 0.22178023
INFO:root:[    6] Training loss: 0.09430004, Validation loss: 0.08805724, Gradient norm: 0.24790009
INFO:root:[    7] Training loss: 0.09034437, Validation loss: 0.08516644, Gradient norm: 0.22775140
INFO:root:[    8] Training loss: 0.08862408, Validation loss: 0.08449480, Gradient norm: 0.22049522
INFO:root:[    9] Training loss: 0.08465799, Validation loss: 0.07664129, Gradient norm: 0.24286379
INFO:root:[   10] Training loss: 0.08261192, Validation loss: 0.07623883, Gradient norm: 0.22680364
INFO:root:[   11] Training loss: 0.08008778, Validation loss: 0.07960893, Gradient norm: 0.27205906
INFO:root:[   12] Training loss: 0.07851474, Validation loss: 0.07316109, Gradient norm: 0.19825670
INFO:root:[   13] Training loss: 0.07698487, Validation loss: 0.08144376, Gradient norm: 0.23225882
INFO:root:[   14] Training loss: 0.07641605, Validation loss: 0.08606527, Gradient norm: 0.23424547
INFO:root:[   15] Training loss: 0.07477801, Validation loss: 0.08217737, Gradient norm: 0.30183899
INFO:root:[   16] Training loss: 0.07451535, Validation loss: 0.07947985, Gradient norm: 0.28927377
INFO:root:[   17] Training loss: 0.07263974, Validation loss: 0.08396145, Gradient norm: 0.25957070
INFO:root:[   18] Training loss: 0.07132722, Validation loss: 0.08740061, Gradient norm: 0.18532733
INFO:root:[   19] Training loss: 0.06962135, Validation loss: 0.08385698, Gradient norm: 0.23826853
INFO:root:[   20] Training loss: 0.07075985, Validation loss: 0.07875072, Gradient norm: 0.22640976
INFO:root:[   21] Training loss: 0.06831623, Validation loss: 0.07792760, Gradient norm: 0.24921096
INFO:root:[   22] Training loss: 0.06809758, Validation loss: 0.08145657, Gradient norm: 0.30343705
INFO:root:[   23] Training loss: 0.06774953, Validation loss: 0.08374086, Gradient norm: 0.25731636
INFO:root:[   24] Training loss: 0.06614898, Validation loss: 0.08140730, Gradient norm: 0.29047390
INFO:root:[   25] Training loss: 0.06556307, Validation loss: 0.07926577, Gradient norm: 0.26251774
INFO:root:[   26] Training loss: 0.06428331, Validation loss: 0.07903653, Gradient norm: 0.26003253
INFO:root:[   27] Training loss: 0.06319894, Validation loss: 0.07586912, Gradient norm: 0.21002815
INFO:root:[   28] Training loss: 0.06396021, Validation loss: 0.07922159, Gradient norm: 0.30303491
INFO:root:[   29] Training loss: 0.06239537, Validation loss: 0.07760565, Gradient norm: 0.27941477
INFO:root:[   30] Training loss: 0.06188336, Validation loss: 0.07509561, Gradient norm: 0.23022233
INFO:root:[   31] Training loss: 0.06108271, Validation loss: 0.07549171, Gradient norm: 0.28684150
INFO:root:[   32] Training loss: 0.06123042, Validation loss: 0.07666438, Gradient norm: 0.22384211
INFO:root:[   33] Training loss: 0.05983259, Validation loss: 0.07979668, Gradient norm: 0.23907852
INFO:root:[   34] Training loss: 0.06027205, Validation loss: 0.07542225, Gradient norm: 0.37744275
INFO:root:[   35] Training loss: 0.05947306, Validation loss: 0.07253674, Gradient norm: 0.24550933
INFO:root:[   36] Training loss: 0.05952355, Validation loss: 0.06744679, Gradient norm: 0.27862916
INFO:root:[   37] Training loss: 0.05837777, Validation loss: 0.07430957, Gradient norm: 0.31829469
INFO:root:[   38] Training loss: 0.05909148, Validation loss: 0.07389102, Gradient norm: 0.24399598
INFO:root:[   39] Training loss: 0.05859962, Validation loss: 0.07152953, Gradient norm: 0.24232786
INFO:root:[   40] Training loss: 0.05734356, Validation loss: 0.07603696, Gradient norm: 0.25490025
INFO:root:[   41] Training loss: 0.05703801, Validation loss: 0.07057703, Gradient norm: 0.25411387
INFO:root:[   42] Training loss: 0.05688351, Validation loss: 0.07318243, Gradient norm: 0.25073868
INFO:root:[   43] Training loss: 0.05598583, Validation loss: 0.07025150, Gradient norm: 0.21392321
INFO:root:[   44] Training loss: 0.05594173, Validation loss: 0.07131965, Gradient norm: 0.23246826
INFO:root:[   45] Training loss: 0.05458172, Validation loss: 0.06936151, Gradient norm: 0.24571685
INFO:root:[   46] Training loss: 0.05554193, Validation loss: 0.06828308, Gradient norm: 0.25146996
INFO:root:[   47] Training loss: 0.05389178, Validation loss: 0.06828784, Gradient norm: 0.25306175
INFO:root:[   48] Training loss: 0.05450441, Validation loss: 0.07385631, Gradient norm: 0.37534248
INFO:root:[   49] Training loss: 0.05363215, Validation loss: 0.06863698, Gradient norm: 0.28171688
INFO:root:[   50] Training loss: 0.05409071, Validation loss: 0.07179680, Gradient norm: 0.27735242
INFO:root:[   51] Training loss: 0.05240447, Validation loss: 0.07143848, Gradient norm: 0.30204058
INFO:root:[   52] Training loss: 0.05267874, Validation loss: 0.07792013, Gradient norm: 0.32611025
INFO:root:[   53] Training loss: 0.05305050, Validation loss: 0.06924371, Gradient norm: 0.45940275
INFO:root:[   54] Training loss: 0.05250895, Validation loss: 0.06649643, Gradient norm: 0.31165771
INFO:root:[   55] Training loss: 0.05101556, Validation loss: 0.06410378, Gradient norm: 0.27255291
INFO:root:[   56] Training loss: 0.05253246, Validation loss: 0.07072839, Gradient norm: 0.30302283
INFO:root:[   57] Training loss: 0.05101894, Validation loss: 0.06545736, Gradient norm: 0.42394637
INFO:root:[   58] Training loss: 0.05060291, Validation loss: 0.07361632, Gradient norm: 0.33895683
INFO:root:[   59] Training loss: 0.05077595, Validation loss: 0.06773533, Gradient norm: 0.34320930
INFO:root:[   60] Training loss: 0.04976691, Validation loss: 0.06948443, Gradient norm: 0.30400300
INFO:root:[   61] Training loss: 0.05023461, Validation loss: 0.06938833, Gradient norm: 0.32239193
INFO:root:[   62] Training loss: 0.04960220, Validation loss: 0.07335243, Gradient norm: 0.39762263
INFO:root:[   63] Training loss: 0.04975344, Validation loss: 0.07281082, Gradient norm: 0.37626053
INFO:root:[   64] Training loss: 0.04881931, Validation loss: 0.06907141, Gradient norm: 0.38455437
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 124.972s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 2.07136
INFO:root:EnergyScoreTrain: 1.41013
INFO:root:CoverageTrain: 0.32542
INFO:root:IntervalWidthTrain: 0.07709
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 2.11177
INFO:root:EnergyScoreValidation: 1.4638
INFO:root:CoverageValidation: 0.29435
INFO:root:IntervalWidthValidation: 0.07115
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.15275
INFO:root:EnergyScoreTest: 1.47718
INFO:root:CoverageTest: 0.33889
INFO:root:IntervalWidthTest: 0.07531
INFO:root:###14 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 251658240
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.39934752, Validation loss: 0.11747917, Gradient norm: 1.46772895
INFO:root:[    2] Training loss: 0.21453474, Validation loss: 0.11853347, Gradient norm: 0.92002172
INFO:root:[    3] Training loss: 0.14787423, Validation loss: 0.11918708, Gradient norm: 0.64288831
INFO:root:[    4] Training loss: 0.12852537, Validation loss: 0.11976863, Gradient norm: 0.42410040
INFO:root:[    5] Training loss: 0.11992712, Validation loss: 0.11765683, Gradient norm: 0.37951148
INFO:root:[    6] Training loss: 0.11624992, Validation loss: 0.11610780, Gradient norm: 0.38131818
INFO:root:[    7] Training loss: 0.11425953, Validation loss: 0.11660572, Gradient norm: 0.37904405
INFO:root:[    8] Training loss: 0.11493505, Validation loss: 0.11518862, Gradient norm: 0.55251411
INFO:root:[    9] Training loss: 0.11252588, Validation loss: 0.11477313, Gradient norm: 0.45490819
INFO:root:[   10] Training loss: 0.11462649, Validation loss: 0.11558650, Gradient norm: 0.48831639
INFO:root:[   11] Training loss: 0.11238039, Validation loss: 0.11376490, Gradient norm: 0.61575868
INFO:root:[   12] Training loss: 0.11089927, Validation loss: 0.11021543, Gradient norm: 0.52821528
INFO:root:[   13] Training loss: 0.10932553, Validation loss: 0.10735770, Gradient norm: 0.57857900
INFO:root:[   14] Training loss: 0.10959766, Validation loss: 0.10630283, Gradient norm: 0.56699248
INFO:root:[   15] Training loss: 0.10753904, Validation loss: 0.10591544, Gradient norm: 0.60163246
INFO:root:[   16] Training loss: 0.10500750, Validation loss: 0.10310348, Gradient norm: 0.64945818
INFO:root:[   17] Training loss: 0.10443606, Validation loss: 0.10281124, Gradient norm: 0.58904023
INFO:root:[   18] Training loss: 0.10271890, Validation loss: 0.09970968, Gradient norm: 0.54677707
INFO:root:[   19] Training loss: 0.10182386, Validation loss: 0.09940953, Gradient norm: 0.57750380
INFO:root:[   20] Training loss: 0.10044716, Validation loss: 0.09511232, Gradient norm: 0.51937632
INFO:root:[   21] Training loss: 0.09919465, Validation loss: 0.09449108, Gradient norm: 0.52064190
INFO:root:[   22] Training loss: 0.09810769, Validation loss: 0.09135753, Gradient norm: 0.62494882
INFO:root:[   23] Training loss: 0.09697439, Validation loss: 0.08890899, Gradient norm: 0.44009191
INFO:root:[   24] Training loss: 0.09556696, Validation loss: 0.08840587, Gradient norm: 0.49725774
INFO:root:[   25] Training loss: 0.09551058, Validation loss: 0.08890364, Gradient norm: 0.51912792
INFO:root:[   26] Training loss: 0.09406346, Validation loss: 0.08762991, Gradient norm: 0.51402790
INFO:root:[   27] Training loss: 0.09408678, Validation loss: 0.08718353, Gradient norm: 0.48353598
INFO:root:[   28] Training loss: 0.09188017, Validation loss: 0.08454459, Gradient norm: 0.50727747
INFO:root:[   29] Training loss: 0.09182226, Validation loss: 0.08184655, Gradient norm: 0.50743976
INFO:root:[   30] Training loss: 0.08953297, Validation loss: 0.07882105, Gradient norm: 0.50793273
INFO:root:[   31] Training loss: 0.08664740, Validation loss: 0.07269903, Gradient norm: 0.49487758
INFO:root:[   32] Training loss: 0.08658957, Validation loss: 0.07107554, Gradient norm: 0.46375033
INFO:root:[   33] Training loss: 0.08257436, Validation loss: 0.06823784, Gradient norm: 0.49993605
INFO:root:[   34] Training loss: 0.08086492, Validation loss: 0.06973591, Gradient norm: 0.55679285
INFO:root:[   35] Training loss: 0.08040914, Validation loss: 0.06700832, Gradient norm: 0.49097876
INFO:root:[   36] Training loss: 0.07876413, Validation loss: 0.06678273, Gradient norm: 0.61996653
INFO:root:[   37] Training loss: 0.07827141, Validation loss: 0.06376156, Gradient norm: 0.65067559
INFO:root:[   38] Training loss: 0.07711015, Validation loss: 0.06436836, Gradient norm: 0.54693634
INFO:root:[   39] Training loss: 0.07709822, Validation loss: 0.06280828, Gradient norm: 0.63908573
INFO:root:[   40] Training loss: 0.07618978, Validation loss: 0.06259103, Gradient norm: 0.59031813
INFO:root:[   41] Training loss: 0.07588719, Validation loss: 0.06084084, Gradient norm: 0.71236895
INFO:root:[   42] Training loss: 0.07579818, Validation loss: 0.06319163, Gradient norm: 0.80347960
INFO:root:[   43] Training loss: 0.07446042, Validation loss: 0.05898157, Gradient norm: 0.66970092
INFO:root:[   44] Training loss: 0.07442620, Validation loss: 0.05840030, Gradient norm: 0.73716884
INFO:root:[   45] Training loss: 0.07465616, Validation loss: 0.06018533, Gradient norm: 0.86859864
INFO:root:[   46] Training loss: 0.07392546, Validation loss: 0.06035164, Gradient norm: 0.92448407
INFO:root:[   47] Training loss: 0.07343285, Validation loss: 0.05908430, Gradient norm: 0.97814341
INFO:root:[   48] Training loss: 0.07268599, Validation loss: 0.05782720, Gradient norm: 0.85149778
INFO:root:[   49] Training loss: 0.07197576, Validation loss: 0.05849036, Gradient norm: 0.74832094
INFO:root:[   50] Training loss: 0.07218642, Validation loss: 0.05837152, Gradient norm: 1.02412915
INFO:root:[   51] Training loss: 0.07080166, Validation loss: 0.05818570, Gradient norm: 0.72784047
INFO:root:[   52] Training loss: 0.07195803, Validation loss: 0.05981522, Gradient norm: 0.62483732
INFO:root:[   53] Training loss: 0.07044961, Validation loss: 0.05835519, Gradient norm: 1.01819434
INFO:root:[   54] Training loss: 0.07096799, Validation loss: 0.05912565, Gradient norm: 0.85808220
INFO:root:[   55] Training loss: 0.07232332, Validation loss: 0.05952538, Gradient norm: 1.00986341
INFO:root:[   56] Training loss: 0.07027103, Validation loss: 0.05691828, Gradient norm: 0.94825336
INFO:root:[   57] Training loss: 0.06912068, Validation loss: 0.05926640, Gradient norm: 0.99539123
INFO:root:[   58] Training loss: 0.06967572, Validation loss: 0.05385626, Gradient norm: 0.89400575
INFO:root:[   59] Training loss: 0.07005699, Validation loss: 0.05880043, Gradient norm: 1.01246572
INFO:root:[   60] Training loss: 0.06886176, Validation loss: 0.05775115, Gradient norm: 1.08733959
INFO:root:[   61] Training loss: 0.06965633, Validation loss: 0.05549835, Gradient norm: 1.29027652
INFO:root:[   62] Training loss: 0.06822734, Validation loss: 0.05484879, Gradient norm: 1.17598552
INFO:root:[   63] Training loss: 0.06836930, Validation loss: 0.05383102, Gradient norm: 1.17367260
INFO:root:[   64] Training loss: 0.06965134, Validation loss: 0.05698936, Gradient norm: 1.07121423
INFO:root:[   65] Training loss: 0.06865772, Validation loss: 0.05733452, Gradient norm: 1.00212061
INFO:root:[   66] Training loss: 0.06770509, Validation loss: 0.05656142, Gradient norm: 1.38134577
INFO:root:[   67] Training loss: 0.06793038, Validation loss: 0.05478615, Gradient norm: 1.39769795
INFO:root:[   68] Training loss: 0.06781549, Validation loss: 0.05667075, Gradient norm: 0.99731321
INFO:root:[   69] Training loss: 0.06723433, Validation loss: 0.05245984, Gradient norm: 1.12864050
INFO:root:[   70] Training loss: 0.06692253, Validation loss: 0.05734318, Gradient norm: 1.33837708
INFO:root:[   71] Training loss: 0.06658312, Validation loss: 0.05709596, Gradient norm: 1.08364531
INFO:root:[   72] Training loss: 0.06684819, Validation loss: 0.05404154, Gradient norm: 1.36696728
INFO:root:[   73] Training loss: 0.06713736, Validation loss: 0.05923779, Gradient norm: 1.11112349
INFO:root:[   74] Training loss: 0.06694562, Validation loss: 0.05489790, Gradient norm: 1.06806464
INFO:root:[   75] Training loss: 0.06637789, Validation loss: 0.05571126, Gradient norm: 1.07135142
INFO:root:[   76] Training loss: 0.06650067, Validation loss: 0.05595338, Gradient norm: 1.31664995
INFO:root:[   77] Training loss: 0.06611484, Validation loss: 0.05424204, Gradient norm: 1.57003620
INFO:root:[   78] Training loss: 0.06675249, Validation loss: 0.05568480, Gradient norm: 1.33629349
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 153.713s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.85666
INFO:root:EnergyScoreTrain: 1.23534
INFO:root:CoverageTrain: 0.4324
INFO:root:IntervalWidthTrain: 0.0761
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.89403
INFO:root:EnergyScoreValidation: 1.31685
INFO:root:CoverageValidation: 0.37428
INFO:root:IntervalWidthValidation: 0.0666
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.03478
INFO:root:EnergyScoreTest: 1.27274
INFO:root:CoverageTest: 0.46514
INFO:root:IntervalWidthTest: 0.09139
INFO:root:###15 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 211812352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.10875587, Validation loss: 0.09023229, Gradient norm: 1.00147700
INFO:root:[    2] Training loss: 0.07739111, Validation loss: 0.07350483, Gradient norm: 0.68954791
INFO:root:[    3] Training loss: 0.06683247, Validation loss: 0.06502549, Gradient norm: 1.23717681
INFO:root:[    4] Training loss: 0.06553939, Validation loss: 0.06727206, Gradient norm: 1.45588217
INFO:root:[    5] Training loss: 0.06177370, Validation loss: 0.05619065, Gradient norm: 1.31927422
INFO:root:[    6] Training loss: 0.05708716, Validation loss: 0.05329883, Gradient norm: 0.92697956
INFO:root:[    7] Training loss: 0.05213983, Validation loss: 0.05049082, Gradient norm: 0.67174466
INFO:root:[    8] Training loss: 0.05037737, Validation loss: 0.04815842, Gradient norm: 0.81821935
INFO:root:[    9] Training loss: 0.04870286, Validation loss: 0.04791589, Gradient norm: 0.67826538
INFO:root:[   10] Training loss: 0.04951309, Validation loss: 0.04997935, Gradient norm: 0.93312074
INFO:root:[   11] Training loss: 0.04659531, Validation loss: 0.04723817, Gradient norm: 0.78425807
INFO:root:[   12] Training loss: 0.04594560, Validation loss: 0.04601667, Gradient norm: 0.66281100
INFO:root:[   13] Training loss: 0.04554171, Validation loss: 0.04366138, Gradient norm: 0.69694071
INFO:root:[   14] Training loss: 0.04535957, Validation loss: 0.04488726, Gradient norm: 0.77544418
INFO:root:[   15] Training loss: 0.04572531, Validation loss: 0.04273077, Gradient norm: 0.99689311
INFO:root:[   16] Training loss: 0.04391971, Validation loss: 0.04199973, Gradient norm: 0.64178989
INFO:root:[   17] Training loss: 0.04159764, Validation loss: 0.04321632, Gradient norm: 0.55361285
INFO:root:[   18] Training loss: 0.04136710, Validation loss: 0.04043646, Gradient norm: 0.70687561
INFO:root:[   19] Training loss: 0.04218872, Validation loss: 0.05063924, Gradient norm: 0.85513694
INFO:root:[   20] Training loss: 0.04466572, Validation loss: 0.04185841, Gradient norm: 1.22821669
INFO:root:[   21] Training loss: 0.04101647, Validation loss: 0.04221116, Gradient norm: 0.82024549
INFO:root:[   22] Training loss: 0.03960684, Validation loss: 0.03952047, Gradient norm: 0.57776351
INFO:root:[   23] Training loss: 0.03883181, Validation loss: 0.03913346, Gradient norm: 0.70876593
INFO:root:[   24] Training loss: 0.03867620, Validation loss: 0.04047460, Gradient norm: 0.61588343
INFO:root:[   25] Training loss: 0.03848273, Validation loss: 0.04112297, Gradient norm: 0.74585069
INFO:root:[   26] Training loss: 0.03799494, Validation loss: 0.03750251, Gradient norm: 0.72071846
INFO:root:[   27] Training loss: 0.03736687, Validation loss: 0.03701784, Gradient norm: 0.53698646
INFO:root:[   28] Training loss: 0.03788119, Validation loss: 0.03929746, Gradient norm: 0.81538915
INFO:root:[   29] Training loss: 0.03744423, Validation loss: 0.03605187, Gradient norm: 0.66402060
INFO:root:[   30] Training loss: 0.03808702, Validation loss: 0.03994295, Gradient norm: 0.77041904
INFO:root:[   31] Training loss: 0.03679567, Validation loss: 0.03563736, Gradient norm: 0.56575289
INFO:root:[   32] Training loss: 0.03702710, Validation loss: 0.03607111, Gradient norm: 0.54751135
INFO:root:[   33] Training loss: 0.03690331, Validation loss: 0.03879516, Gradient norm: 0.64895942
INFO:root:[   34] Training loss: 0.03633270, Validation loss: 0.03529400, Gradient norm: 0.68192565
INFO:root:[   35] Training loss: 0.03469221, Validation loss: 0.03552980, Gradient norm: 0.62448297
INFO:root:[   36] Training loss: 0.03532768, Validation loss: 0.03605243, Gradient norm: 0.68016998
INFO:root:[   37] Training loss: 0.03693475, Validation loss: 0.03509512, Gradient norm: 0.83438935
INFO:root:[   38] Training loss: 0.03504770, Validation loss: 0.03536832, Gradient norm: 0.72072845
INFO:root:[   39] Training loss: 0.03519366, Validation loss: 0.03401350, Gradient norm: 0.75723772
INFO:root:[   40] Training loss: 0.03435948, Validation loss: 0.03420725, Gradient norm: 0.45358813
INFO:root:[   41] Training loss: 0.03551071, Validation loss: 0.03592744, Gradient norm: 0.65213820
INFO:root:[   42] Training loss: 0.03524151, Validation loss: 0.03509763, Gradient norm: 0.62091888
INFO:root:[   43] Training loss: 0.03403374, Validation loss: 0.03371663, Gradient norm: 0.61888956
INFO:root:[   44] Training loss: 0.03489426, Validation loss: 0.03435645, Gradient norm: 0.72987670
INFO:root:[   45] Training loss: 0.03415718, Validation loss: 0.03410018, Gradient norm: 0.57557180
INFO:root:[   46] Training loss: 0.03348368, Validation loss: 0.03374757, Gradient norm: 0.49318011
INFO:root:[   47] Training loss: 0.03460331, Validation loss: 0.03463042, Gradient norm: 0.76311674
INFO:root:[   48] Training loss: 0.03326769, Validation loss: 0.03266490, Gradient norm: 0.50060967
INFO:root:[   49] Training loss: 0.03288894, Validation loss: 0.03374133, Gradient norm: 0.47423291
INFO:root:[   50] Training loss: 0.03347324, Validation loss: 0.03628871, Gradient norm: 0.58809278
INFO:root:[   51] Training loss: 0.03359223, Validation loss: 0.03366225, Gradient norm: 0.62543591
INFO:root:[   52] Training loss: 0.03390914, Validation loss: 0.03481883, Gradient norm: 0.82668229
INFO:root:[   53] Training loss: 0.03369250, Validation loss: 0.03238069, Gradient norm: 0.76219898
INFO:root:[   54] Training loss: 0.03237777, Validation loss: 0.03241791, Gradient norm: 0.57738198
INFO:root:[   55] Training loss: 0.03269349, Validation loss: 0.03248290, Gradient norm: 0.66318634
INFO:root:[   56] Training loss: 0.03290471, Validation loss: 0.03257415, Gradient norm: 0.51983047
INFO:root:[   57] Training loss: 0.03176526, Validation loss: 0.03256258, Gradient norm: 0.51339755
INFO:root:[   58] Training loss: 0.03156409, Validation loss: 0.03239774, Gradient norm: 0.50432359
INFO:root:[   59] Training loss: 0.03242729, Validation loss: 0.03216234, Gradient norm: 0.51110810
INFO:root:[   60] Training loss: 0.03255337, Validation loss: 0.03375112, Gradient norm: 0.47848667
INFO:root:[   61] Training loss: 0.03321135, Validation loss: 0.03280100, Gradient norm: 0.67073309
INFO:root:[   62] Training loss: 0.03205509, Validation loss: 0.03115753, Gradient norm: 0.54609695
INFO:root:[   63] Training loss: 0.03241008, Validation loss: 0.03261712, Gradient norm: 0.61159578
INFO:root:[   64] Training loss: 0.03205208, Validation loss: 0.03184301, Gradient norm: 0.70857060
INFO:root:[   65] Training loss: 0.03196027, Validation loss: 0.03633405, Gradient norm: 0.59268969
INFO:root:[   66] Training loss: 0.03277289, Validation loss: 0.03259336, Gradient norm: 0.74396342
INFO:root:[   67] Training loss: 0.03113880, Validation loss: 0.03039392, Gradient norm: 0.60192753
INFO:root:[   68] Training loss: 0.03049569, Validation loss: 0.03090148, Gradient norm: 0.34410025
INFO:root:[   69] Training loss: 0.03151997, Validation loss: 0.03180245, Gradient norm: 0.64359616
INFO:root:[   70] Training loss: 0.03177206, Validation loss: 0.03060416, Gradient norm: 0.64907322
INFO:root:[   71] Training loss: 0.03081584, Validation loss: 0.03317164, Gradient norm: 0.50583504
INFO:root:[   72] Training loss: 0.03213541, Validation loss: 0.03149504, Gradient norm: 0.66477088
INFO:root:[   73] Training loss: 0.03124437, Validation loss: 0.03179677, Gradient norm: 0.57459753
INFO:root:[   74] Training loss: 0.03153839, Validation loss: 0.03093426, Gradient norm: 0.62788100
INFO:root:[   75] Training loss: 0.03071528, Validation loss: 0.02995505, Gradient norm: 0.45026957
INFO:root:[   76] Training loss: 0.03047958, Validation loss: 0.03025665, Gradient norm: 0.60764780
INFO:root:[   77] Training loss: 0.03121187, Validation loss: 0.03045915, Gradient norm: 0.57275994
INFO:root:[   78] Training loss: 0.03080451, Validation loss: 0.03011976, Gradient norm: 0.56096766
INFO:root:[   79] Training loss: 0.02960073, Validation loss: 0.03047244, Gradient norm: 0.48811800
INFO:root:[   80] Training loss: 0.03031970, Validation loss: 0.03005258, Gradient norm: 0.67898768
INFO:root:[   81] Training loss: 0.03064703, Validation loss: 0.03032145, Gradient norm: 0.65962371
INFO:root:[   82] Training loss: 0.03145016, Validation loss: 0.03168592, Gradient norm: 0.74101258
INFO:root:[   83] Training loss: 0.03028989, Validation loss: 0.03003204, Gradient norm: 0.65839968
INFO:root:[   84] Training loss: 0.03115176, Validation loss: 0.03160194, Gradient norm: 0.84786452
INFO:root:EP 84: Early stopping
INFO:root:Training the model took 165.386s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.80224
INFO:root:EnergyScoreTrain: 0.62588
INFO:root:CoverageTrain: 0.79041
INFO:root:IntervalWidthTrain: 0.05609
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.82973
INFO:root:EnergyScoreValidation: 0.64767
INFO:root:CoverageValidation: 0.782
INFO:root:IntervalWidthValidation: 0.05648
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.46461
INFO:root:EnergyScoreTest: 1.21254
INFO:root:CoverageTest: 0.49694
INFO:root:IntervalWidthTest: 0.06083
INFO:root:###16 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 207618048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09645397, Validation loss: 0.08771655, Gradient norm: 0.21092855
INFO:root:[    2] Training loss: 0.08022777, Validation loss: 0.07348569, Gradient norm: 0.30977673
INFO:root:[    3] Training loss: 0.06797408, Validation loss: 0.06138200, Gradient norm: 0.57899437
INFO:root:[    4] Training loss: 0.05872212, Validation loss: 0.05642735, Gradient norm: 0.53482674
INFO:root:[    5] Training loss: 0.05456965, Validation loss: 0.05365632, Gradient norm: 0.61016261
INFO:root:[    6] Training loss: 0.05300121, Validation loss: 0.05455601, Gradient norm: 0.58471453
INFO:root:[    7] Training loss: 0.05328823, Validation loss: 0.05129677, Gradient norm: 0.92487854
INFO:root:[    8] Training loss: 0.05001976, Validation loss: 0.04936617, Gradient norm: 0.46249165
INFO:root:[    9] Training loss: 0.04978408, Validation loss: 0.04920240, Gradient norm: 0.63604820
INFO:root:[   10] Training loss: 0.04828808, Validation loss: 0.04800017, Gradient norm: 0.58911792
INFO:root:[   11] Training loss: 0.04899489, Validation loss: 0.04661643, Gradient norm: 0.81366102
INFO:root:[   12] Training loss: 0.04914003, Validation loss: 0.04949439, Gradient norm: 1.00986144
INFO:root:[   13] Training loss: 0.04633658, Validation loss: 0.04569936, Gradient norm: 0.48325172
INFO:root:[   14] Training loss: 0.04590317, Validation loss: 0.04590722, Gradient norm: 0.57851463
INFO:root:[   15] Training loss: 0.04503898, Validation loss: 0.04506403, Gradient norm: 0.61396775
INFO:root:[   16] Training loss: 0.04552388, Validation loss: 0.04499999, Gradient norm: 0.56664289
INFO:root:[   17] Training loss: 0.04481565, Validation loss: 0.04424524, Gradient norm: 0.51228618
INFO:root:[   18] Training loss: 0.04387520, Validation loss: 0.04359948, Gradient norm: 0.52042671
INFO:root:[   19] Training loss: 0.04391347, Validation loss: 0.04252821, Gradient norm: 0.69956175
INFO:root:[   20] Training loss: 0.04311209, Validation loss: 0.04306185, Gradient norm: 0.54828282
INFO:root:[   21] Training loss: 0.04281510, Validation loss: 0.04268086, Gradient norm: 0.59370218
INFO:root:[   22] Training loss: 0.04248032, Validation loss: 0.04166595, Gradient norm: 0.59316386
INFO:root:[   23] Training loss: 0.04227384, Validation loss: 0.04150141, Gradient norm: 0.71092787
INFO:root:[   24] Training loss: 0.04162235, Validation loss: 0.04066109, Gradient norm: 0.46370277
INFO:root:[   25] Training loss: 0.04119581, Validation loss: 0.04063539, Gradient norm: 0.45860099
INFO:root:[   26] Training loss: 0.04127638, Validation loss: 0.04382132, Gradient norm: 0.54788049
INFO:root:[   27] Training loss: 0.04150783, Validation loss: 0.03965464, Gradient norm: 0.68911522
INFO:root:[   28] Training loss: 0.03988266, Validation loss: 0.04034940, Gradient norm: 0.42363618
INFO:root:[   29] Training loss: 0.04036420, Validation loss: 0.04085704, Gradient norm: 0.76213080
INFO:root:[   30] Training loss: 0.04053520, Validation loss: 0.04133611, Gradient norm: 0.87754191
INFO:root:[   31] Training loss: 0.03991236, Validation loss: 0.03859206, Gradient norm: 0.62633694
INFO:root:[   32] Training loss: 0.04035298, Validation loss: 0.03817039, Gradient norm: 0.79503268
INFO:root:[   33] Training loss: 0.03991647, Validation loss: 0.04089191, Gradient norm: 0.80513328
INFO:root:[   34] Training loss: 0.03959425, Validation loss: 0.03958597, Gradient norm: 0.61684484
INFO:root:[   35] Training loss: 0.03971957, Validation loss: 0.03876493, Gradient norm: 0.64885486
INFO:root:[   36] Training loss: 0.03860316, Validation loss: 0.03846934, Gradient norm: 0.63661135
INFO:root:[   37] Training loss: 0.03801620, Validation loss: 0.03910718, Gradient norm: 0.54263423
INFO:root:[   38] Training loss: 0.03965011, Validation loss: 0.03701728, Gradient norm: 0.69646580
INFO:root:[   39] Training loss: 0.03765403, Validation loss: 0.03738088, Gradient norm: 0.52883664
INFO:root:[   40] Training loss: 0.03666276, Validation loss: 0.03702486, Gradient norm: 0.39420193
INFO:root:[   41] Training loss: 0.03775760, Validation loss: 0.03679767, Gradient norm: 0.46626695
INFO:root:[   42] Training loss: 0.03799109, Validation loss: 0.03969516, Gradient norm: 0.63798176
INFO:root:[   43] Training loss: 0.03770632, Validation loss: 0.03697070, Gradient norm: 0.46415880
INFO:root:[   44] Training loss: 0.03710733, Validation loss: 0.03855123, Gradient norm: 0.47231026
INFO:root:[   45] Training loss: 0.03757585, Validation loss: 0.03664653, Gradient norm: 0.65809564
INFO:root:[   46] Training loss: 0.03635258, Validation loss: 0.03702234, Gradient norm: 0.44957648
INFO:root:[   47] Training loss: 0.03751003, Validation loss: 0.03624500, Gradient norm: 0.50548359
INFO:root:[   48] Training loss: 0.03736680, Validation loss: 0.03922578, Gradient norm: 0.65751582
INFO:root:[   49] Training loss: 0.03641886, Validation loss: 0.03624967, Gradient norm: 0.60778923
INFO:root:[   50] Training loss: 0.03603403, Validation loss: 0.03621590, Gradient norm: 0.50541845
INFO:root:[   51] Training loss: 0.03682603, Validation loss: 0.03827426, Gradient norm: 0.75047913
INFO:root:[   52] Training loss: 0.03631322, Validation loss: 0.03779995, Gradient norm: 0.70371651
INFO:root:[   53] Training loss: 0.03598384, Validation loss: 0.03536676, Gradient norm: 0.51627803
INFO:root:[   54] Training loss: 0.03562649, Validation loss: 0.03579954, Gradient norm: 0.63755892
INFO:root:[   55] Training loss: 0.03656518, Validation loss: 0.03645631, Gradient norm: 0.71286085
INFO:root:[   56] Training loss: 0.03609987, Validation loss: 0.03597175, Gradient norm: 0.75140665
INFO:root:[   57] Training loss: 0.03606089, Validation loss: 0.03681822, Gradient norm: 0.53438106
INFO:root:[   58] Training loss: 0.03646998, Validation loss: 0.03534564, Gradient norm: 0.68565649
INFO:root:[   59] Training loss: 0.03513265, Validation loss: 0.03478779, Gradient norm: 0.54530430
INFO:root:[   60] Training loss: 0.03548731, Validation loss: 0.03481963, Gradient norm: 0.54860889
INFO:root:[   61] Training loss: 0.03549319, Validation loss: 0.03543325, Gradient norm: 0.55297698
INFO:root:[   62] Training loss: 0.03507517, Validation loss: 0.03460170, Gradient norm: 0.44075192
INFO:root:[   63] Training loss: 0.03503815, Validation loss: 0.03567846, Gradient norm: 0.54182175
INFO:root:[   64] Training loss: 0.03447924, Validation loss: 0.03548609, Gradient norm: 0.46242482
INFO:root:[   65] Training loss: 0.03454269, Validation loss: 0.03556620, Gradient norm: 0.54089128
INFO:root:[   66] Training loss: 0.03476271, Validation loss: 0.03643367, Gradient norm: 0.53939275
INFO:root:[   67] Training loss: 0.03450188, Validation loss: 0.03409556, Gradient norm: 0.53035839
INFO:root:[   68] Training loss: 0.03406591, Validation loss: 0.03573015, Gradient norm: 0.62604081
INFO:root:[   69] Training loss: 0.03425901, Validation loss: 0.03515136, Gradient norm: 0.56187192
INFO:root:[   70] Training loss: 0.03465768, Validation loss: 0.03435294, Gradient norm: 0.59310523
INFO:root:[   71] Training loss: 0.03431713, Validation loss: 0.03424959, Gradient norm: 0.51169881
INFO:root:[   72] Training loss: 0.03396565, Validation loss: 0.03439358, Gradient norm: 0.53551483
INFO:root:[   73] Training loss: 0.03377524, Validation loss: 0.03452846, Gradient norm: 0.49073543
INFO:root:[   74] Training loss: 0.03399738, Validation loss: 0.03378671, Gradient norm: 0.61413633
INFO:root:[   75] Training loss: 0.03403582, Validation loss: 0.03618651, Gradient norm: 0.56396133
INFO:root:[   76] Training loss: 0.03538725, Validation loss: 0.03363986, Gradient norm: 0.55557421
INFO:root:[   77] Training loss: 0.03351094, Validation loss: 0.03417302, Gradient norm: 0.50601445
INFO:root:[   78] Training loss: 0.03429001, Validation loss: 0.03378469, Gradient norm: 0.65716437
INFO:root:[   79] Training loss: 0.03281731, Validation loss: 0.03402823, Gradient norm: 0.44528422
INFO:root:[   80] Training loss: 0.03285805, Validation loss: 0.03338308, Gradient norm: 0.41641852
INFO:root:[   81] Training loss: 0.03350290, Validation loss: 0.03424839, Gradient norm: 0.52783386
INFO:root:[   82] Training loss: 0.03276560, Validation loss: 0.03385727, Gradient norm: 0.38611420
INFO:root:[   83] Training loss: 0.03320266, Validation loss: 0.03265043, Gradient norm: 0.56384062
INFO:root:[   84] Training loss: 0.03266293, Validation loss: 0.03547708, Gradient norm: 0.51452988
INFO:root:[   85] Training loss: 0.03353973, Validation loss: 0.03485508, Gradient norm: 0.57946185
INFO:root:[   86] Training loss: 0.03319139, Validation loss: 0.03306537, Gradient norm: 0.58973401
INFO:root:[   87] Training loss: 0.03354025, Validation loss: 0.03341044, Gradient norm: 0.66862686
INFO:root:[   88] Training loss: 0.03376255, Validation loss: 0.03270922, Gradient norm: 0.66389157
INFO:root:[   89] Training loss: 0.03277822, Validation loss: 0.03284983, Gradient norm: 0.54450234
INFO:root:[   90] Training loss: 0.03325088, Validation loss: 0.03216062, Gradient norm: 0.60563116
INFO:root:[   91] Training loss: 0.03284952, Validation loss: 0.03369362, Gradient norm: 0.61144286
INFO:root:[   92] Training loss: 0.03277942, Validation loss: 0.03314657, Gradient norm: 0.64924698
INFO:root:[   93] Training loss: 0.03338121, Validation loss: 0.03549876, Gradient norm: 0.78753216
INFO:root:[   94] Training loss: 0.03215946, Validation loss: 0.03298665, Gradient norm: 0.49510265
INFO:root:[   95] Training loss: 0.03296541, Validation loss: 0.03416344, Gradient norm: 0.72101314
INFO:root:[   96] Training loss: 0.03238996, Validation loss: 0.03216508, Gradient norm: 0.54176342
INFO:root:[   97] Training loss: 0.03090749, Validation loss: 0.03180726, Gradient norm: 0.32823758
INFO:root:[   98] Training loss: 0.03238039, Validation loss: 0.03189711, Gradient norm: 0.75149299
INFO:root:[   99] Training loss: 0.03161554, Validation loss: 0.03203539, Gradient norm: 0.50800222
INFO:root:[  100] Training loss: 0.03164474, Validation loss: 0.03165721, Gradient norm: 0.48849936
INFO:root:[  101] Training loss: 0.03179750, Validation loss: 0.03164383, Gradient norm: 0.61985013
INFO:root:[  102] Training loss: 0.03212236, Validation loss: 0.03139516, Gradient norm: 0.68711063
INFO:root:[  103] Training loss: 0.03104235, Validation loss: 0.03135739, Gradient norm: 0.33291786
INFO:root:[  104] Training loss: 0.03086821, Validation loss: 0.03278003, Gradient norm: 0.44857518
INFO:root:[  105] Training loss: 0.03234932, Validation loss: 0.03115131, Gradient norm: 0.55959791
INFO:root:[  106] Training loss: 0.03113240, Validation loss: 0.03214260, Gradient norm: 0.53791807
INFO:root:[  107] Training loss: 0.03228144, Validation loss: 0.03232051, Gradient norm: 0.76243412
INFO:root:[  108] Training loss: 0.03159162, Validation loss: 0.03085144, Gradient norm: 0.51288800
INFO:root:[  109] Training loss: 0.03061842, Validation loss: 0.03140154, Gradient norm: 0.38879330
INFO:root:[  110] Training loss: 0.03119341, Validation loss: 0.03119588, Gradient norm: 0.49337134
INFO:root:[  111] Training loss: 0.03146936, Validation loss: 0.03101309, Gradient norm: 0.57429876
INFO:root:[  112] Training loss: 0.03104566, Validation loss: 0.03251255, Gradient norm: 0.63982564
INFO:root:[  113] Training loss: 0.03039728, Validation loss: 0.03144654, Gradient norm: 0.49061113
INFO:root:[  114] Training loss: 0.03088513, Validation loss: 0.03055957, Gradient norm: 0.54155750
INFO:root:[  115] Training loss: 0.03020660, Validation loss: 0.03150760, Gradient norm: 0.52385888
INFO:root:[  116] Training loss: 0.02973986, Validation loss: 0.03052854, Gradient norm: 0.43617168
INFO:root:[  117] Training loss: 0.03057342, Validation loss: 0.03122444, Gradient norm: 0.46568467
INFO:root:[  118] Training loss: 0.03142831, Validation loss: 0.03071870, Gradient norm: 0.61178652
INFO:root:[  119] Training loss: 0.03018956, Validation loss: 0.03212959, Gradient norm: 0.53302222
INFO:root:[  120] Training loss: 0.03032514, Validation loss: 0.03024783, Gradient norm: 0.61393196
INFO:root:[  121] Training loss: 0.03010574, Validation loss: 0.03094418, Gradient norm: 0.42955339
INFO:root:[  122] Training loss: 0.03056795, Validation loss: 0.03071703, Gradient norm: 0.63283181
INFO:root:[  123] Training loss: 0.02944923, Validation loss: 0.03118600, Gradient norm: 0.52136715
INFO:root:[  124] Training loss: 0.02986715, Validation loss: 0.03002279, Gradient norm: 0.41771783
INFO:root:[  125] Training loss: 0.02973606, Validation loss: 0.03036670, Gradient norm: 0.43019853
INFO:root:[  126] Training loss: 0.02993858, Validation loss: 0.03001304, Gradient norm: 0.46472709
INFO:root:[  127] Training loss: 0.02950841, Validation loss: 0.02967669, Gradient norm: 0.48012514
INFO:root:[  128] Training loss: 0.02967787, Validation loss: 0.03043720, Gradient norm: 0.57860689
INFO:root:[  129] Training loss: 0.02954099, Validation loss: 0.03001743, Gradient norm: 0.41247747
INFO:root:[  130] Training loss: 0.02920636, Validation loss: 0.03055443, Gradient norm: 0.41891494
INFO:root:[  131] Training loss: 0.02968027, Validation loss: 0.02965915, Gradient norm: 0.50994665
INFO:root:[  132] Training loss: 0.02894916, Validation loss: 0.02923327, Gradient norm: 0.47230007
INFO:root:[  133] Training loss: 0.02881578, Validation loss: 0.02956704, Gradient norm: 0.47058287
INFO:root:[  134] Training loss: 0.02889930, Validation loss: 0.02933548, Gradient norm: 0.62022148
INFO:root:[  135] Training loss: 0.02951546, Validation loss: 0.02968534, Gradient norm: 0.45068668
INFO:root:[  136] Training loss: 0.02910468, Validation loss: 0.02921086, Gradient norm: 0.46922142
INFO:root:[  137] Training loss: 0.02942892, Validation loss: 0.02885135, Gradient norm: 0.44319603
INFO:root:[  138] Training loss: 0.02875859, Validation loss: 0.02885706, Gradient norm: 0.63932874
INFO:root:[  139] Training loss: 0.02943730, Validation loss: 0.02913007, Gradient norm: 0.43909955
INFO:root:[  140] Training loss: 0.02932776, Validation loss: 0.03026667, Gradient norm: 0.62844813
INFO:root:[  141] Training loss: 0.02903802, Validation loss: 0.02881671, Gradient norm: 0.64737439
INFO:root:[  142] Training loss: 0.03012119, Validation loss: 0.02887564, Gradient norm: 0.69935457
INFO:root:[  143] Training loss: 0.02968410, Validation loss: 0.02905064, Gradient norm: 0.61970456
INFO:root:[  144] Training loss: 0.02858984, Validation loss: 0.02914956, Gradient norm: 0.49837134
INFO:root:[  145] Training loss: 0.02867038, Validation loss: 0.02943064, Gradient norm: 0.42946015
INFO:root:[  146] Training loss: 0.02908451, Validation loss: 0.03001058, Gradient norm: 0.56593630
INFO:root:[  147] Training loss: 0.02891090, Validation loss: 0.02872365, Gradient norm: 0.56327390
INFO:root:[  148] Training loss: 0.02800735, Validation loss: 0.02852587, Gradient norm: 0.43574338
INFO:root:[  149] Training loss: 0.02831825, Validation loss: 0.02891760, Gradient norm: 0.43821091
INFO:root:[  150] Training loss: 0.02898711, Validation loss: 0.02899934, Gradient norm: 0.63782075
INFO:root:[  151] Training loss: 0.02833510, Validation loss: 0.02987444, Gradient norm: 0.59277883
INFO:root:[  152] Training loss: 0.02931063, Validation loss: 0.02917241, Gradient norm: 0.64369343
INFO:root:[  153] Training loss: 0.02872184, Validation loss: 0.02949935, Gradient norm: 0.59843987
INFO:root:[  154] Training loss: 0.02784461, Validation loss: 0.02838829, Gradient norm: 0.36531682
INFO:root:[  155] Training loss: 0.02801668, Validation loss: 0.02861332, Gradient norm: 0.43225349
INFO:root:[  156] Training loss: 0.02797270, Validation loss: 0.02947901, Gradient norm: 0.43538921
INFO:root:[  157] Training loss: 0.02794187, Validation loss: 0.02809617, Gradient norm: 0.52711238
INFO:root:[  158] Training loss: 0.02797463, Validation loss: 0.03076250, Gradient norm: 0.39923129
INFO:root:[  159] Training loss: 0.02828417, Validation loss: 0.02828432, Gradient norm: 0.48017214
INFO:root:[  160] Training loss: 0.02771104, Validation loss: 0.02780760, Gradient norm: 0.40209749
INFO:root:[  161] Training loss: 0.02769837, Validation loss: 0.02873882, Gradient norm: 0.43308540
INFO:root:[  162] Training loss: 0.02816585, Validation loss: 0.02908791, Gradient norm: 0.54628134
INFO:root:[  163] Training loss: 0.02771946, Validation loss: 0.02803054, Gradient norm: 0.54269776
INFO:root:[  164] Training loss: 0.02793936, Validation loss: 0.02832586, Gradient norm: 0.51173491
INFO:root:[  165] Training loss: 0.02794169, Validation loss: 0.02894487, Gradient norm: 0.46006189
INFO:root:[  166] Training loss: 0.02812248, Validation loss: 0.02833175, Gradient norm: 0.57460576
INFO:root:[  167] Training loss: 0.02767892, Validation loss: 0.02811429, Gradient norm: 0.46422251
INFO:root:[  168] Training loss: 0.02783594, Validation loss: 0.02807843, Gradient norm: 0.53939437
INFO:root:[  169] Training loss: 0.02800594, Validation loss: 0.02760012, Gradient norm: 0.44083168
INFO:root:[  170] Training loss: 0.02759846, Validation loss: 0.02834777, Gradient norm: 0.54749402
INFO:root:[  171] Training loss: 0.02731343, Validation loss: 0.02787098, Gradient norm: 0.51228005
INFO:root:[  172] Training loss: 0.02749888, Validation loss: 0.02752721, Gradient norm: 0.41792804
INFO:root:[  173] Training loss: 0.02799030, Validation loss: 0.02847719, Gradient norm: 0.50315410
INFO:root:[  174] Training loss: 0.02735477, Validation loss: 0.02826420, Gradient norm: 0.50552350
INFO:root:[  175] Training loss: 0.02719955, Validation loss: 0.02746809, Gradient norm: 0.42872156
INFO:root:[  176] Training loss: 0.02747449, Validation loss: 0.02768403, Gradient norm: 0.45475360
INFO:root:[  177] Training loss: 0.02690829, Validation loss: 0.02825679, Gradient norm: 0.37805574
INFO:root:[  178] Training loss: 0.02741877, Validation loss: 0.02739688, Gradient norm: 0.43562233
INFO:root:[  179] Training loss: 0.02698327, Validation loss: 0.02780117, Gradient norm: 0.46010184
INFO:root:[  180] Training loss: 0.02686768, Validation loss: 0.02784555, Gradient norm: 0.49726742
INFO:root:[  181] Training loss: 0.02863252, Validation loss: 0.02765863, Gradient norm: 0.53041274
INFO:root:[  182] Training loss: 0.02767641, Validation loss: 0.02762333, Gradient norm: 0.62415857
INFO:root:[  183] Training loss: 0.02656122, Validation loss: 0.02795186, Gradient norm: 0.44427938
INFO:root:[  184] Training loss: 0.02757545, Validation loss: 0.02706528, Gradient norm: 0.49468315
INFO:root:[  185] Training loss: 0.02665603, Validation loss: 0.02694726, Gradient norm: 0.50540262
INFO:root:[  186] Training loss: 0.02656845, Validation loss: 0.02721363, Gradient norm: 0.45432702
INFO:root:[  187] Training loss: 0.02692574, Validation loss: 0.02708659, Gradient norm: 0.42547937
INFO:root:[  188] Training loss: 0.02617933, Validation loss: 0.02710362, Gradient norm: 0.38587528
INFO:root:[  189] Training loss: 0.02654935, Validation loss: 0.02739621, Gradient norm: 0.42529090
INFO:root:[  190] Training loss: 0.02645511, Validation loss: 0.02709112, Gradient norm: 0.41691384
INFO:root:[  191] Training loss: 0.02608450, Validation loss: 0.02744139, Gradient norm: 0.45087156
INFO:root:[  192] Training loss: 0.02644897, Validation loss: 0.02720769, Gradient norm: 0.41544246
INFO:root:[  193] Training loss: 0.02691277, Validation loss: 0.02691404, Gradient norm: 0.46748253
INFO:root:[  194] Training loss: 0.02736171, Validation loss: 0.02726076, Gradient norm: 0.62458417
INFO:root:[  195] Training loss: 0.02670904, Validation loss: 0.02753118, Gradient norm: 0.61628107
INFO:root:[  196] Training loss: 0.02674571, Validation loss: 0.02710540, Gradient norm: 0.39729872
INFO:root:[  197] Training loss: 0.02659259, Validation loss: 0.02721736, Gradient norm: 0.46983951
INFO:root:[  198] Training loss: 0.02650824, Validation loss: 0.02837021, Gradient norm: 0.44855097
INFO:root:[  199] Training loss: 0.02641919, Validation loss: 0.02676539, Gradient norm: 0.46680745
INFO:root:[  200] Training loss: 0.02653602, Validation loss: 0.02661080, Gradient norm: 0.46385045
INFO:root:[  201] Training loss: 0.02595552, Validation loss: 0.02695618, Gradient norm: 0.41185211
INFO:root:[  202] Training loss: 0.02597767, Validation loss: 0.02744553, Gradient norm: 0.39725697
INFO:root:[  203] Training loss: 0.02609713, Validation loss: 0.02919369, Gradient norm: 0.41414388
INFO:root:[  204] Training loss: 0.02686261, Validation loss: 0.02647821, Gradient norm: 0.52476831
INFO:root:[  205] Training loss: 0.02582711, Validation loss: 0.02633789, Gradient norm: 0.42318848
INFO:root:[  206] Training loss: 0.02592320, Validation loss: 0.02715224, Gradient norm: 0.43764723
INFO:root:[  207] Training loss: 0.02692187, Validation loss: 0.02752960, Gradient norm: 0.56787942
INFO:root:[  208] Training loss: 0.02658870, Validation loss: 0.02715015, Gradient norm: 0.40979576
INFO:root:[  209] Training loss: 0.02637874, Validation loss: 0.02650567, Gradient norm: 0.40193613
INFO:root:[  210] Training loss: 0.02640706, Validation loss: 0.02745685, Gradient norm: 0.58614582
INFO:root:[  211] Training loss: 0.02552730, Validation loss: 0.02670143, Gradient norm: 0.43097387
INFO:root:[  212] Training loss: 0.02593058, Validation loss: 0.02616920, Gradient norm: 0.45808881
INFO:root:[  213] Training loss: 0.02613685, Validation loss: 0.02719376, Gradient norm: 0.43859312
INFO:root:[  214] Training loss: 0.02573046, Validation loss: 0.02671661, Gradient norm: 0.47557751
INFO:root:[  215] Training loss: 0.02598717, Validation loss: 0.02626531, Gradient norm: 0.47025869
INFO:root:[  216] Training loss: 0.02552597, Validation loss: 0.02626226, Gradient norm: 0.43626969
INFO:root:[  217] Training loss: 0.02619487, Validation loss: 0.02740021, Gradient norm: 0.51309643
INFO:root:[  218] Training loss: 0.02529801, Validation loss: 0.02702606, Gradient norm: 0.49781771
INFO:root:[  219] Training loss: 0.02569136, Validation loss: 0.02709646, Gradient norm: 0.40505485
INFO:root:[  220] Training loss: 0.02657404, Validation loss: 0.02641411, Gradient norm: 0.52837823
INFO:root:[  221] Training loss: 0.02595052, Validation loss: 0.02664824, Gradient norm: 0.45308673
INFO:root:[  222] Training loss: 0.02564798, Validation loss: 0.02600604, Gradient norm: 0.44861581
INFO:root:[  223] Training loss: 0.02528927, Validation loss: 0.02583313, Gradient norm: 0.36930398
INFO:root:[  224] Training loss: 0.02536554, Validation loss: 0.02600248, Gradient norm: 0.48037383
INFO:root:[  225] Training loss: 0.02611182, Validation loss: 0.02693209, Gradient norm: 0.48987589
INFO:root:[  226] Training loss: 0.02565974, Validation loss: 0.02656278, Gradient norm: 0.49785727
INFO:root:[  227] Training loss: 0.02495454, Validation loss: 0.02584302, Gradient norm: 0.34256783
INFO:root:[  228] Training loss: 0.02550791, Validation loss: 0.02657541, Gradient norm: 0.45775338
INFO:root:[  229] Training loss: 0.02583213, Validation loss: 0.02568412, Gradient norm: 0.51736822
INFO:root:[  230] Training loss: 0.02573020, Validation loss: 0.02649102, Gradient norm: 0.53010898
INFO:root:[  231] Training loss: 0.02583161, Validation loss: 0.02630163, Gradient norm: 0.54004684
INFO:root:[  232] Training loss: 0.02534812, Validation loss: 0.02618627, Gradient norm: 0.39980174
INFO:root:[  233] Training loss: 0.02609647, Validation loss: 0.02642061, Gradient norm: 0.62300232
INFO:root:[  234] Training loss: 0.02573651, Validation loss: 0.02574752, Gradient norm: 0.63939653
INFO:root:[  235] Training loss: 0.02477444, Validation loss: 0.02572564, Gradient norm: 0.43527319
INFO:root:[  236] Training loss: 0.02550511, Validation loss: 0.02611033, Gradient norm: 0.46222744
INFO:root:[  237] Training loss: 0.02572386, Validation loss: 0.02678902, Gradient norm: 0.47055408
INFO:root:[  238] Training loss: 0.02541334, Validation loss: 0.02557246, Gradient norm: 0.49297178
INFO:root:[  239] Training loss: 0.02519345, Validation loss: 0.02581912, Gradient norm: 0.44011630
INFO:root:[  240] Training loss: 0.02469455, Validation loss: 0.02557247, Gradient norm: 0.40278728
INFO:root:[  241] Training loss: 0.02518604, Validation loss: 0.02638110, Gradient norm: 0.50716806
INFO:root:[  242] Training loss: 0.02523029, Validation loss: 0.02592760, Gradient norm: 0.44630762
INFO:root:[  243] Training loss: 0.02531401, Validation loss: 0.02550874, Gradient norm: 0.37382165
INFO:root:[  244] Training loss: 0.02472654, Validation loss: 0.02773944, Gradient norm: 0.36277775
INFO:root:[  245] Training loss: 0.02515164, Validation loss: 0.02593303, Gradient norm: 0.44147046
INFO:root:[  246] Training loss: 0.02425210, Validation loss: 0.02610625, Gradient norm: 0.36423775
INFO:root:[  247] Training loss: 0.02543535, Validation loss: 0.02612444, Gradient norm: 0.45310528
INFO:root:[  248] Training loss: 0.02475757, Validation loss: 0.02535697, Gradient norm: 0.44978121
INFO:root:[  249] Training loss: 0.02512060, Validation loss: 0.02538946, Gradient norm: 0.42923796
INFO:root:[  250] Training loss: 0.02516326, Validation loss: 0.02531813, Gradient norm: 0.46552042
INFO:root:[  251] Training loss: 0.02488435, Validation loss: 0.02547434, Gradient norm: 0.48606925
INFO:root:[  252] Training loss: 0.02554757, Validation loss: 0.02561457, Gradient norm: 0.51960027
INFO:root:[  253] Training loss: 0.02663875, Validation loss: 0.02683750, Gradient norm: 0.60099976
INFO:root:[  254] Training loss: 0.02520702, Validation loss: 0.02536702, Gradient norm: 0.51704603
INFO:root:[  255] Training loss: 0.02471322, Validation loss: 0.02518512, Gradient norm: 0.51191702
INFO:root:[  256] Training loss: 0.02433121, Validation loss: 0.02593456, Gradient norm: 0.29327521
INFO:root:[  257] Training loss: 0.02511649, Validation loss: 0.02580600, Gradient norm: 0.60880127
INFO:root:[  258] Training loss: 0.02514514, Validation loss: 0.02520772, Gradient norm: 0.53711450
INFO:root:[  259] Training loss: 0.02485222, Validation loss: 0.02589416, Gradient norm: 0.34458086
INFO:root:[  260] Training loss: 0.02504029, Validation loss: 0.02634474, Gradient norm: 0.50784198
INFO:root:[  261] Training loss: 0.02471475, Validation loss: 0.02567343, Gradient norm: 0.46356976
INFO:root:[  262] Training loss: 0.02401712, Validation loss: 0.02685613, Gradient norm: 0.33629506
INFO:root:[  263] Training loss: 0.02472627, Validation loss: 0.02514233, Gradient norm: 0.42764929
INFO:root:[  264] Training loss: 0.02443548, Validation loss: 0.02565896, Gradient norm: 0.41654833
INFO:root:[  265] Training loss: 0.02452796, Validation loss: 0.02522810, Gradient norm: 0.42235198
INFO:root:[  266] Training loss: 0.02455707, Validation loss: 0.02607746, Gradient norm: 0.38256220
INFO:root:[  267] Training loss: 0.02390734, Validation loss: 0.02558361, Gradient norm: 0.37068213
INFO:root:[  268] Training loss: 0.02458295, Validation loss: 0.02511998, Gradient norm: 0.37860342
INFO:root:[  269] Training loss: 0.02493046, Validation loss: 0.02488329, Gradient norm: 0.54246011
INFO:root:[  270] Training loss: 0.02496061, Validation loss: 0.02504575, Gradient norm: 0.39538534
INFO:root:[  271] Training loss: 0.02458203, Validation loss: 0.02511898, Gradient norm: 0.47083823
INFO:root:[  272] Training loss: 0.02418538, Validation loss: 0.02581030, Gradient norm: 0.38921187
INFO:root:[  273] Training loss: 0.02392680, Validation loss: 0.02548946, Gradient norm: 0.31991010
INFO:root:[  274] Training loss: 0.02389224, Validation loss: 0.02505852, Gradient norm: 0.36781516
INFO:root:[  275] Training loss: 0.02522627, Validation loss: 0.02563107, Gradient norm: 0.59004543
INFO:root:[  276] Training loss: 0.02461600, Validation loss: 0.02556776, Gradient norm: 0.40755063
INFO:root:[  277] Training loss: 0.02413879, Validation loss: 0.02571591, Gradient norm: 0.39452864
INFO:root:[  278] Training loss: 0.02415725, Validation loss: 0.02525009, Gradient norm: 0.42279791
INFO:root:EP 278: Early stopping
INFO:root:Training the model took 537.561s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.58731
INFO:root:EnergyScoreTrain: 0.43828
INFO:root:CoverageTrain: 0.85953
INFO:root:IntervalWidthTrain: 0.05124
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.63609
INFO:root:EnergyScoreValidation: 0.47646
INFO:root:CoverageValidation: 0.84253
INFO:root:IntervalWidthValidation: 0.05181
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.07967
INFO:root:EnergyScoreTest: 0.85318
INFO:root:CoverageTest: 0.6066
INFO:root:IntervalWidthTest: 0.05553
INFO:root:###17 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 207618048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26882963, Validation loss: 0.12719758, Gradient norm: 1.58467374
INFO:root:[    2] Training loss: 0.11302247, Validation loss: 0.10558181, Gradient norm: 0.43357655
INFO:root:[    3] Training loss: 0.09854364, Validation loss: 0.09473672, Gradient norm: 0.40357381
INFO:root:[    4] Training loss: 0.09145563, Validation loss: 0.09388830, Gradient norm: 0.41797667
INFO:root:[    5] Training loss: 0.08496732, Validation loss: 0.08287824, Gradient norm: 0.32822159
INFO:root:[    6] Training loss: 0.08003086, Validation loss: 0.07667912, Gradient norm: 0.46954556
INFO:root:[    7] Training loss: 0.07256500, Validation loss: 0.07320447, Gradient norm: 0.45827736
INFO:root:[    8] Training loss: 0.06977296, Validation loss: 0.06665198, Gradient norm: 0.43566430
INFO:root:[    9] Training loss: 0.06786495, Validation loss: 0.06543216, Gradient norm: 0.57163568
INFO:root:[   10] Training loss: 0.06394670, Validation loss: 0.06305961, Gradient norm: 0.30362475
INFO:root:[   11] Training loss: 0.06237153, Validation loss: 0.06189501, Gradient norm: 0.37777246
INFO:root:[   12] Training loss: 0.06232408, Validation loss: 0.06079036, Gradient norm: 0.44370062
INFO:root:[   13] Training loss: 0.06045861, Validation loss: 0.05867547, Gradient norm: 0.48113690
INFO:root:[   14] Training loss: 0.05863338, Validation loss: 0.05813128, Gradient norm: 0.36646342
INFO:root:[   15] Training loss: 0.05902684, Validation loss: 0.05693786, Gradient norm: 0.33160848
INFO:root:[   16] Training loss: 0.05787185, Validation loss: 0.05660227, Gradient norm: 0.41220879
INFO:root:[   17] Training loss: 0.05634047, Validation loss: 0.05904300, Gradient norm: 0.28847159
INFO:root:[   18] Training loss: 0.05605029, Validation loss: 0.05798759, Gradient norm: 0.38692470
INFO:root:[   19] Training loss: 0.05577345, Validation loss: 0.05507521, Gradient norm: 0.47542128
INFO:root:[   20] Training loss: 0.05457560, Validation loss: 0.05491027, Gradient norm: 0.27523772
INFO:root:[   21] Training loss: 0.05351715, Validation loss: 0.05414747, Gradient norm: 0.32902961
INFO:root:[   22] Training loss: 0.05428102, Validation loss: 0.05358471, Gradient norm: 0.26850899
INFO:root:[   23] Training loss: 0.05425713, Validation loss: 0.05263410, Gradient norm: 0.30545973
INFO:root:[   24] Training loss: 0.05203215, Validation loss: 0.05441815, Gradient norm: 0.36603726
INFO:root:[   25] Training loss: 0.05294800, Validation loss: 0.05238290, Gradient norm: 0.40149203
INFO:root:[   26] Training loss: 0.05170675, Validation loss: 0.05267663, Gradient norm: 0.40298038
INFO:root:[   27] Training loss: 0.05176390, Validation loss: 0.05062068, Gradient norm: 0.41062883
INFO:root:[   28] Training loss: 0.05058334, Validation loss: 0.05048189, Gradient norm: 0.29017370
INFO:root:[   29] Training loss: 0.05023357, Validation loss: 0.04954104, Gradient norm: 0.36173255
INFO:root:[   30] Training loss: 0.05080367, Validation loss: 0.05097438, Gradient norm: 0.40828164
INFO:root:[   31] Training loss: 0.04939996, Validation loss: 0.04971520, Gradient norm: 0.44072483
INFO:root:[   32] Training loss: 0.05053658, Validation loss: 0.04994587, Gradient norm: 0.53633313
INFO:root:[   33] Training loss: 0.04950555, Validation loss: 0.04764270, Gradient norm: 0.47187760
INFO:root:[   34] Training loss: 0.04774730, Validation loss: 0.04972325, Gradient norm: 0.29499296
INFO:root:[   35] Training loss: 0.04761241, Validation loss: 0.04759467, Gradient norm: 0.40878444
INFO:root:[   36] Training loss: 0.04736530, Validation loss: 0.04691582, Gradient norm: 0.37185397
INFO:root:[   37] Training loss: 0.04714071, Validation loss: 0.04734358, Gradient norm: 0.36739324
INFO:root:[   38] Training loss: 0.04656554, Validation loss: 0.04681592, Gradient norm: 0.31275997
INFO:root:[   39] Training loss: 0.04624533, Validation loss: 0.04630911, Gradient norm: 0.32642449
INFO:root:[   40] Training loss: 0.04628886, Validation loss: 0.04646205, Gradient norm: 0.31808093
INFO:root:[   41] Training loss: 0.04533131, Validation loss: 0.04636286, Gradient norm: 0.38632884
INFO:root:[   42] Training loss: 0.04574185, Validation loss: 0.04509987, Gradient norm: 0.35536674
INFO:root:[   43] Training loss: 0.04563056, Validation loss: 0.04512131, Gradient norm: 0.42630222
INFO:root:[   44] Training loss: 0.04529735, Validation loss: 0.04490758, Gradient norm: 0.31308801
INFO:root:[   45] Training loss: 0.04449453, Validation loss: 0.04587966, Gradient norm: 0.33508486
INFO:root:[   46] Training loss: 0.04483089, Validation loss: 0.04542678, Gradient norm: 0.38523611
INFO:root:[   47] Training loss: 0.04387471, Validation loss: 0.04485587, Gradient norm: 0.41667895
INFO:root:[   48] Training loss: 0.04443238, Validation loss: 0.04368824, Gradient norm: 0.34965139
INFO:root:[   49] Training loss: 0.04347699, Validation loss: 0.04527480, Gradient norm: 0.31600304
INFO:root:[   50] Training loss: 0.04443329, Validation loss: 0.04262337, Gradient norm: 0.45393768
INFO:root:[   51] Training loss: 0.04272448, Validation loss: 0.04297030, Gradient norm: 0.28549625
INFO:root:[   52] Training loss: 0.04420807, Validation loss: 0.04289600, Gradient norm: 0.36374722
INFO:root:[   53] Training loss: 0.04291150, Validation loss: 0.04323153, Gradient norm: 0.27562481
INFO:root:[   54] Training loss: 0.04242450, Validation loss: 0.04362001, Gradient norm: 0.36562004
INFO:root:[   55] Training loss: 0.04258252, Validation loss: 0.04202840, Gradient norm: 0.42771981
INFO:root:[   56] Training loss: 0.04179158, Validation loss: 0.04241097, Gradient norm: 0.35027003
INFO:root:[   57] Training loss: 0.04155900, Validation loss: 0.04230679, Gradient norm: 0.40791514
INFO:root:[   58] Training loss: 0.04204071, Validation loss: 0.04184154, Gradient norm: 0.33300117
INFO:root:[   59] Training loss: 0.04125147, Validation loss: 0.04414759, Gradient norm: 0.34869275
INFO:root:[   60] Training loss: 0.04188498, Validation loss: 0.04158273, Gradient norm: 0.36309493
INFO:root:[   61] Training loss: 0.04085808, Validation loss: 0.04245485, Gradient norm: 0.30099951
INFO:root:[   62] Training loss: 0.04059254, Validation loss: 0.04108239, Gradient norm: 0.29583613
INFO:root:[   63] Training loss: 0.04111321, Validation loss: 0.04168482, Gradient norm: 0.47239755
INFO:root:[   64] Training loss: 0.04209692, Validation loss: 0.04221242, Gradient norm: 0.36591494
INFO:root:[   65] Training loss: 0.04092178, Validation loss: 0.04095399, Gradient norm: 0.38091135
INFO:root:[   66] Training loss: 0.04106947, Validation loss: 0.04137582, Gradient norm: 0.51221348
INFO:root:[   67] Training loss: 0.04005429, Validation loss: 0.04004432, Gradient norm: 0.39642296
INFO:root:[   68] Training loss: 0.03952466, Validation loss: 0.03943313, Gradient norm: 0.31793057
INFO:root:[   69] Training loss: 0.03954426, Validation loss: 0.03935258, Gradient norm: 0.36043726
INFO:root:[   70] Training loss: 0.03967685, Validation loss: 0.04039432, Gradient norm: 0.41131264
INFO:root:[   71] Training loss: 0.04052520, Validation loss: 0.03949833, Gradient norm: 0.44175951
INFO:root:[   72] Training loss: 0.04044130, Validation loss: 0.03988539, Gradient norm: 0.46121452
INFO:root:[   73] Training loss: 0.03982137, Validation loss: 0.03951872, Gradient norm: 0.36128017
INFO:root:[   74] Training loss: 0.03977549, Validation loss: 0.04021726, Gradient norm: 0.36128992
INFO:root:[   75] Training loss: 0.03870234, Validation loss: 0.03887382, Gradient norm: 0.34463638
INFO:root:[   76] Training loss: 0.03889103, Validation loss: 0.03923760, Gradient norm: 0.31192400
INFO:root:[   77] Training loss: 0.03889486, Validation loss: 0.03855272, Gradient norm: 0.26838319
INFO:root:[   78] Training loss: 0.03945791, Validation loss: 0.03808902, Gradient norm: 0.36648839
INFO:root:[   79] Training loss: 0.03814391, Validation loss: 0.03861072, Gradient norm: 0.32472840
INFO:root:[   80] Training loss: 0.03848082, Validation loss: 0.03820699, Gradient norm: 0.29468167
INFO:root:[   81] Training loss: 0.03891694, Validation loss: 0.03830721, Gradient norm: 0.43109943
INFO:root:[   82] Training loss: 0.03798237, Validation loss: 0.03804962, Gradient norm: 0.36558765
INFO:root:[   83] Training loss: 0.03722417, Validation loss: 0.03785292, Gradient norm: 0.35252724
INFO:root:[   84] Training loss: 0.03795377, Validation loss: 0.03815782, Gradient norm: 0.41046608
INFO:root:[   85] Training loss: 0.03750973, Validation loss: 0.03803810, Gradient norm: 0.29614270
INFO:root:[   86] Training loss: 0.03795946, Validation loss: 0.03743929, Gradient norm: 0.36112899
INFO:root:[   87] Training loss: 0.03832479, Validation loss: 0.03766379, Gradient norm: 0.32705412
INFO:root:[   88] Training loss: 0.03778214, Validation loss: 0.03751550, Gradient norm: 0.34709720
INFO:root:[   89] Training loss: 0.03734553, Validation loss: 0.03723797, Gradient norm: 0.35339729
INFO:root:[   90] Training loss: 0.03748130, Validation loss: 0.03738746, Gradient norm: 0.30014185
INFO:root:[   91] Training loss: 0.03711058, Validation loss: 0.03717111, Gradient norm: 0.31246167
INFO:root:[   92] Training loss: 0.03716051, Validation loss: 0.03672836, Gradient norm: 0.38075944
INFO:root:[   93] Training loss: 0.03745034, Validation loss: 0.03728256, Gradient norm: 0.44018502
INFO:root:[   94] Training loss: 0.03678518, Validation loss: 0.03641314, Gradient norm: 0.29269333
INFO:root:[   95] Training loss: 0.03606033, Validation loss: 0.03720754, Gradient norm: 0.28215649
INFO:root:[   96] Training loss: 0.03785599, Validation loss: 0.03813650, Gradient norm: 0.36327540
INFO:root:[   97] Training loss: 0.03723875, Validation loss: 0.03734264, Gradient norm: 0.41056826
INFO:root:[   98] Training loss: 0.03747700, Validation loss: 0.03692345, Gradient norm: 0.34833408
INFO:root:[   99] Training loss: 0.03757349, Validation loss: 0.03671520, Gradient norm: 0.40713913
INFO:root:[  100] Training loss: 0.03730720, Validation loss: 0.03642589, Gradient norm: 0.49888750
INFO:root:[  101] Training loss: 0.03673259, Validation loss: 0.03614203, Gradient norm: 0.45895354
INFO:root:[  102] Training loss: 0.03634934, Validation loss: 0.03647759, Gradient norm: 0.33779288
INFO:root:[  103] Training loss: 0.03586571, Validation loss: 0.03769706, Gradient norm: 0.32425125
INFO:root:[  104] Training loss: 0.03592803, Validation loss: 0.03639312, Gradient norm: 0.29878743
INFO:root:[  105] Training loss: 0.03636510, Validation loss: 0.03742757, Gradient norm: 0.40875462
INFO:root:[  106] Training loss: 0.03580239, Validation loss: 0.03688605, Gradient norm: 0.36628754
INFO:root:[  107] Training loss: 0.03568939, Validation loss: 0.03574774, Gradient norm: 0.39912365
INFO:root:[  108] Training loss: 0.03633589, Validation loss: 0.03614302, Gradient norm: 0.41322143
INFO:root:[  109] Training loss: 0.03545102, Validation loss: 0.03588855, Gradient norm: 0.34798470
INFO:root:[  110] Training loss: 0.03487125, Validation loss: 0.03574087, Gradient norm: 0.31336733
INFO:root:[  111] Training loss: 0.03496681, Validation loss: 0.03509734, Gradient norm: 0.29687347
INFO:root:[  112] Training loss: 0.03587293, Validation loss: 0.03604220, Gradient norm: 0.30794122
INFO:root:[  113] Training loss: 0.03595203, Validation loss: 0.03585003, Gradient norm: 0.43222080
INFO:root:[  114] Training loss: 0.03637185, Validation loss: 0.03528378, Gradient norm: 0.48147692
INFO:root:[  115] Training loss: 0.03486995, Validation loss: 0.03593889, Gradient norm: 0.43151420
INFO:root:[  116] Training loss: 0.03534075, Validation loss: 0.03526523, Gradient norm: 0.39768235
INFO:root:[  117] Training loss: 0.03484358, Validation loss: 0.03607842, Gradient norm: 0.36329722
INFO:root:[  118] Training loss: 0.03499511, Validation loss: 0.03472027, Gradient norm: 0.31743781
INFO:root:[  119] Training loss: 0.03436482, Validation loss: 0.03517803, Gradient norm: 0.34661332
INFO:root:[  120] Training loss: 0.03510533, Validation loss: 0.03494699, Gradient norm: 0.38600891
INFO:root:[  121] Training loss: 0.03471990, Validation loss: 0.03508310, Gradient norm: 0.30744155
INFO:root:[  122] Training loss: 0.03403733, Validation loss: 0.03497437, Gradient norm: 0.31044665
INFO:root:[  123] Training loss: 0.03458284, Validation loss: 0.03429187, Gradient norm: 0.34631544
INFO:root:[  124] Training loss: 0.03499063, Validation loss: 0.03529071, Gradient norm: 0.33695287
INFO:root:[  125] Training loss: 0.03538607, Validation loss: 0.03433765, Gradient norm: 0.45924022
INFO:root:[  126] Training loss: 0.03408040, Validation loss: 0.03512098, Gradient norm: 0.28122065
INFO:root:[  127] Training loss: 0.03372639, Validation loss: 0.03483569, Gradient norm: 0.27667348
INFO:root:[  128] Training loss: 0.03447791, Validation loss: 0.03611206, Gradient norm: 0.44277516
INFO:root:[  129] Training loss: 0.03403506, Validation loss: 0.03472059, Gradient norm: 0.36626379
INFO:root:[  130] Training loss: 0.03395907, Validation loss: 0.03445726, Gradient norm: 0.26279912
INFO:root:[  131] Training loss: 0.03348284, Validation loss: 0.03405207, Gradient norm: 0.27054607
INFO:root:[  132] Training loss: 0.03431564, Validation loss: 0.03426808, Gradient norm: 0.28175188
INFO:root:[  133] Training loss: 0.03392987, Validation loss: 0.03436557, Gradient norm: 0.30828504
INFO:root:[  134] Training loss: 0.03400843, Validation loss: 0.03378681, Gradient norm: 0.30099557
INFO:root:[  135] Training loss: 0.03331589, Validation loss: 0.03356286, Gradient norm: 0.34531397
INFO:root:[  136] Training loss: 0.03443499, Validation loss: 0.03437792, Gradient norm: 0.27261504
INFO:root:[  137] Training loss: 0.03405548, Validation loss: 0.03401654, Gradient norm: 0.39148856
INFO:root:[  138] Training loss: 0.03329127, Validation loss: 0.03366048, Gradient norm: 0.40364797
INFO:root:[  139] Training loss: 0.03401963, Validation loss: 0.03421103, Gradient norm: 0.29928256
INFO:root:[  140] Training loss: 0.03374873, Validation loss: 0.03575997, Gradient norm: 0.41995090
INFO:root:[  141] Training loss: 0.03376625, Validation loss: 0.03386018, Gradient norm: 0.34632508
INFO:root:[  142] Training loss: 0.03460999, Validation loss: 0.03382185, Gradient norm: 0.50234044
INFO:root:[  143] Training loss: 0.03330816, Validation loss: 0.03326629, Gradient norm: 0.29094542
INFO:root:[  144] Training loss: 0.03375936, Validation loss: 0.03428769, Gradient norm: 0.41065262
INFO:root:[  145] Training loss: 0.03352154, Validation loss: 0.03314350, Gradient norm: 0.38771571
INFO:root:[  146] Training loss: 0.03394623, Validation loss: 0.03370270, Gradient norm: 0.41302500
INFO:root:[  147] Training loss: 0.03283230, Validation loss: 0.03378645, Gradient norm: 0.25520927
INFO:root:[  148] Training loss: 0.03323864, Validation loss: 0.03474603, Gradient norm: 0.37527944
INFO:root:[  149] Training loss: 0.03329434, Validation loss: 0.03439275, Gradient norm: 0.38551350
INFO:root:[  150] Training loss: 0.03282554, Validation loss: 0.03453131, Gradient norm: 0.40024375
INFO:root:[  151] Training loss: 0.03387630, Validation loss: 0.03385722, Gradient norm: 0.47855497
INFO:root:[  152] Training loss: 0.03275999, Validation loss: 0.03338184, Gradient norm: 0.34862956
INFO:root:[  153] Training loss: 0.03303321, Validation loss: 0.03318906, Gradient norm: 0.38479601
INFO:root:[  154] Training loss: 0.03362044, Validation loss: 0.03278859, Gradient norm: 0.48361935
INFO:root:[  155] Training loss: 0.03326989, Validation loss: 0.03279595, Gradient norm: 0.40185992
INFO:root:[  156] Training loss: 0.03260823, Validation loss: 0.03262503, Gradient norm: 0.29696065
INFO:root:[  157] Training loss: 0.03247899, Validation loss: 0.03264165, Gradient norm: 0.32982932
INFO:root:[  158] Training loss: 0.03309589, Validation loss: 0.03303797, Gradient norm: 0.34750120
INFO:root:[  159] Training loss: 0.03300154, Validation loss: 0.03262501, Gradient norm: 0.36559058
INFO:root:[  160] Training loss: 0.03267476, Validation loss: 0.03323414, Gradient norm: 0.30421635
INFO:root:[  161] Training loss: 0.03243032, Validation loss: 0.03310408, Gradient norm: 0.34111159
INFO:root:[  162] Training loss: 0.03295974, Validation loss: 0.03279855, Gradient norm: 0.38416069
INFO:root:[  163] Training loss: 0.03213810, Validation loss: 0.03267746, Gradient norm: 0.34486189
INFO:root:[  164] Training loss: 0.03274674, Validation loss: 0.03256334, Gradient norm: 0.34699275
INFO:root:[  165] Training loss: 0.03275766, Validation loss: 0.03488463, Gradient norm: 0.33149644
INFO:root:[  166] Training loss: 0.03275560, Validation loss: 0.03230208, Gradient norm: 0.46147189
INFO:root:[  167] Training loss: 0.03293829, Validation loss: 0.03318157, Gradient norm: 0.37480065
INFO:root:[  168] Training loss: 0.03215230, Validation loss: 0.03229390, Gradient norm: 0.31193191
INFO:root:[  169] Training loss: 0.03305714, Validation loss: 0.03208238, Gradient norm: 0.45228271
INFO:root:[  170] Training loss: 0.03244099, Validation loss: 0.03329425, Gradient norm: 0.46982164
INFO:root:[  171] Training loss: 0.03204312, Validation loss: 0.03213675, Gradient norm: 0.39461994
INFO:root:[  172] Training loss: 0.03222530, Validation loss: 0.03291688, Gradient norm: 0.32606701
INFO:root:[  173] Training loss: 0.03146745, Validation loss: 0.03346285, Gradient norm: 0.28698654
INFO:root:[  174] Training loss: 0.03126781, Validation loss: 0.03212982, Gradient norm: 0.33444930
INFO:root:[  175] Training loss: 0.03220485, Validation loss: 0.03352317, Gradient norm: 0.39509051
INFO:root:[  176] Training loss: 0.03195988, Validation loss: 0.03224009, Gradient norm: 0.45807687
INFO:root:[  177] Training loss: 0.03167103, Validation loss: 0.03247693, Gradient norm: 0.34113586
INFO:root:[  178] Training loss: 0.03191828, Validation loss: 0.03372576, Gradient norm: 0.37513224
INFO:root:EP 178: Early stopping
INFO:root:Training the model took 345.806s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.72443
INFO:root:EnergyScoreTrain: 0.55005
INFO:root:CoverageTrain: 0.88577
INFO:root:IntervalWidthTrain: 0.07254
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.75531
INFO:root:EnergyScoreValidation: 0.57218
INFO:root:CoverageValidation: 0.88229
INFO:root:IntervalWidthValidation: 0.07305
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.19426
INFO:root:EnergyScoreTest: 1.67679
INFO:root:CoverageTest: 0.64946
INFO:root:IntervalWidthTest: 0.12021
INFO:root:###18 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 207618048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.10873314, Validation loss: 0.09929429, Gradient norm: 0.23876106
INFO:root:[    2] Training loss: 0.09097359, Validation loss: 0.08596844, Gradient norm: 0.20454558
INFO:root:[    3] Training loss: 0.08150256, Validation loss: 0.08150463, Gradient norm: 0.23149198
INFO:root:[    4] Training loss: 0.07713662, Validation loss: 0.07604657, Gradient norm: 0.30447536
INFO:root:[    5] Training loss: 0.07342168, Validation loss: 0.07308816, Gradient norm: 0.33187194
INFO:root:[    6] Training loss: 0.06830678, Validation loss: 0.06726043, Gradient norm: 0.37887999
INFO:root:[    7] Training loss: 0.06411685, Validation loss: 0.06198576, Gradient norm: 0.37839732
INFO:root:[    8] Training loss: 0.06026476, Validation loss: 0.06182081, Gradient norm: 0.45766568
INFO:root:[    9] Training loss: 0.05936815, Validation loss: 0.05755620, Gradient norm: 0.38197470
INFO:root:[   10] Training loss: 0.05714695, Validation loss: 0.05646009, Gradient norm: 0.34137686
INFO:root:[   11] Training loss: 0.05543902, Validation loss: 0.05539347, Gradient norm: 0.26203422
INFO:root:[   12] Training loss: 0.05432582, Validation loss: 0.05678304, Gradient norm: 0.36230613
INFO:root:[   13] Training loss: 0.05318681, Validation loss: 0.05282084, Gradient norm: 0.33929018
INFO:root:[   14] Training loss: 0.05320575, Validation loss: 0.05325959, Gradient norm: 0.37363432
INFO:root:[   15] Training loss: 0.05157543, Validation loss: 0.05243029, Gradient norm: 0.31989339
INFO:root:[   16] Training loss: 0.05268855, Validation loss: 0.05133987, Gradient norm: 0.35604038
INFO:root:[   17] Training loss: 0.05202182, Validation loss: 0.05119976, Gradient norm: 0.33664253
INFO:root:[   18] Training loss: 0.05053668, Validation loss: 0.05218679, Gradient norm: 0.35698206
INFO:root:[   19] Training loss: 0.05069899, Validation loss: 0.05039794, Gradient norm: 0.37540785
INFO:root:[   20] Training loss: 0.04934165, Validation loss: 0.04885562, Gradient norm: 0.36711435
INFO:root:[   21] Training loss: 0.04986607, Validation loss: 0.04954049, Gradient norm: 0.49076284
INFO:root:[   22] Training loss: 0.04966074, Validation loss: 0.04845545, Gradient norm: 0.39417451
INFO:root:[   23] Training loss: 0.04809301, Validation loss: 0.04852597, Gradient norm: 0.37650116
INFO:root:[   24] Training loss: 0.04812639, Validation loss: 0.04762886, Gradient norm: 0.42592170
INFO:root:[   25] Training loss: 0.04755209, Validation loss: 0.05071296, Gradient norm: 0.39578289
INFO:root:[   26] Training loss: 0.04746596, Validation loss: 0.04840860, Gradient norm: 0.32011637
INFO:root:[   27] Training loss: 0.04725340, Validation loss: 0.04685665, Gradient norm: 0.30166845
INFO:root:[   28] Training loss: 0.04740847, Validation loss: 0.04721264, Gradient norm: 0.39537521
INFO:root:[   29] Training loss: 0.04649829, Validation loss: 0.04697247, Gradient norm: 0.41499334
INFO:root:[   30] Training loss: 0.04586220, Validation loss: 0.04590917, Gradient norm: 0.31606502
INFO:root:[   31] Training loss: 0.04577896, Validation loss: 0.04620551, Gradient norm: 0.30699413
INFO:root:[   32] Training loss: 0.04674327, Validation loss: 0.04863468, Gradient norm: 0.36057419
INFO:root:[   33] Training loss: 0.04531591, Validation loss: 0.04508870, Gradient norm: 0.43351849
INFO:root:[   34] Training loss: 0.04562000, Validation loss: 0.04593928, Gradient norm: 0.53470420
INFO:root:[   35] Training loss: 0.04531487, Validation loss: 0.04507355, Gradient norm: 0.34286194
INFO:root:[   36] Training loss: 0.04498327, Validation loss: 0.04430431, Gradient norm: 0.43195375
INFO:root:[   37] Training loss: 0.04451786, Validation loss: 0.04463959, Gradient norm: 0.42377779
INFO:root:[   38] Training loss: 0.04417226, Validation loss: 0.04543987, Gradient norm: 0.38492276
INFO:root:[   39] Training loss: 0.04460275, Validation loss: 0.04585516, Gradient norm: 0.41031339
INFO:root:[   40] Training loss: 0.04497759, Validation loss: 0.04404712, Gradient norm: 0.50714146
INFO:root:[   41] Training loss: 0.04369396, Validation loss: 0.04380598, Gradient norm: 0.50519711
INFO:root:[   42] Training loss: 0.04321910, Validation loss: 0.04307790, Gradient norm: 0.40109014
INFO:root:[   43] Training loss: 0.04283026, Validation loss: 0.04611752, Gradient norm: 0.37556049
INFO:root:[   44] Training loss: 0.04192475, Validation loss: 0.04330007, Gradient norm: 0.33895036
INFO:root:[   45] Training loss: 0.04248762, Validation loss: 0.04335507, Gradient norm: 0.43934736
INFO:root:[   46] Training loss: 0.04244666, Validation loss: 0.04166763, Gradient norm: 0.37175892
INFO:root:[   47] Training loss: 0.04168971, Validation loss: 0.04261822, Gradient norm: 0.35897153
INFO:root:[   48] Training loss: 0.04239904, Validation loss: 0.04177493, Gradient norm: 0.33147125
INFO:root:[   49] Training loss: 0.04216179, Validation loss: 0.04321726, Gradient norm: 0.43272260
INFO:root:[   50] Training loss: 0.04267892, Validation loss: 0.04185342, Gradient norm: 0.40083424
INFO:root:[   51] Training loss: 0.04139186, Validation loss: 0.04166768, Gradient norm: 0.46286675
INFO:root:[   52] Training loss: 0.04111400, Validation loss: 0.04178834, Gradient norm: 0.27380635
INFO:root:[   53] Training loss: 0.04155818, Validation loss: 0.04097383, Gradient norm: 0.36227706
INFO:root:[   54] Training loss: 0.04119947, Validation loss: 0.04055609, Gradient norm: 0.35082412
INFO:root:[   55] Training loss: 0.04055333, Validation loss: 0.04133142, Gradient norm: 0.32293574
INFO:root:[   56] Training loss: 0.04142806, Validation loss: 0.04070704, Gradient norm: 0.44242469
INFO:root:[   57] Training loss: 0.04118931, Validation loss: 0.04008999, Gradient norm: 0.42207516
INFO:root:[   58] Training loss: 0.04091310, Validation loss: 0.04024117, Gradient norm: 0.38641295
INFO:root:[   59] Training loss: 0.03964422, Validation loss: 0.04025673, Gradient norm: 0.34030537
INFO:root:[   60] Training loss: 0.03997535, Validation loss: 0.04191699, Gradient norm: 0.32495234
INFO:root:[   61] Training loss: 0.03971291, Validation loss: 0.03970168, Gradient norm: 0.39141246
INFO:root:[   62] Training loss: 0.03946230, Validation loss: 0.04017994, Gradient norm: 0.42262316
INFO:root:[   63] Training loss: 0.03947967, Validation loss: 0.04098758, Gradient norm: 0.37540080
INFO:root:[   64] Training loss: 0.04058379, Validation loss: 0.04044996, Gradient norm: 0.43042503
INFO:root:[   65] Training loss: 0.03940602, Validation loss: 0.03916395, Gradient norm: 0.36569310
INFO:root:[   66] Training loss: 0.03936039, Validation loss: 0.03991240, Gradient norm: 0.35565255
INFO:root:[   67] Training loss: 0.04021984, Validation loss: 0.03951606, Gradient norm: 0.46605649
INFO:root:[   68] Training loss: 0.03944716, Validation loss: 0.03880464, Gradient norm: 0.48340441
INFO:root:[   69] Training loss: 0.03879615, Validation loss: 0.03883502, Gradient norm: 0.34551230
INFO:root:[   70] Training loss: 0.03812884, Validation loss: 0.03916500, Gradient norm: 0.34990998
INFO:root:[   71] Training loss: 0.03860955, Validation loss: 0.03860405, Gradient norm: 0.42549879
INFO:root:[   72] Training loss: 0.03903777, Validation loss: 0.03838480, Gradient norm: 0.34937388
INFO:root:[   73] Training loss: 0.03835201, Validation loss: 0.03835379, Gradient norm: 0.28378554
INFO:root:[   74] Training loss: 0.03803586, Validation loss: 0.03962476, Gradient norm: 0.37118040
INFO:root:[   75] Training loss: 0.03914622, Validation loss: 0.03804481, Gradient norm: 0.35436999
INFO:root:[   76] Training loss: 0.03788347, Validation loss: 0.03760058, Gradient norm: 0.28997548
INFO:root:[   77] Training loss: 0.03777644, Validation loss: 0.03885547, Gradient norm: 0.44411964
INFO:root:[   78] Training loss: 0.03812524, Validation loss: 0.03849797, Gradient norm: 0.35353374
INFO:root:[   79] Training loss: 0.03732088, Validation loss: 0.03733736, Gradient norm: 0.37884843
INFO:root:[   80] Training loss: 0.03802248, Validation loss: 0.03768567, Gradient norm: 0.47919916
INFO:root:[   81] Training loss: 0.03679379, Validation loss: 0.03729768, Gradient norm: 0.31795029
INFO:root:[   82] Training loss: 0.03674424, Validation loss: 0.03790367, Gradient norm: 0.33715223
INFO:root:[   83] Training loss: 0.03810502, Validation loss: 0.03707942, Gradient norm: 0.48368643
INFO:root:[   84] Training loss: 0.03753492, Validation loss: 0.03790865, Gradient norm: 0.40739934
INFO:root:[   85] Training loss: 0.03692622, Validation loss: 0.03684774, Gradient norm: 0.39173358
INFO:root:[   86] Training loss: 0.03858581, Validation loss: 0.03681553, Gradient norm: 0.31202041
INFO:root:[   87] Training loss: 0.03780620, Validation loss: 0.03705592, Gradient norm: 0.40259444
INFO:root:[   88] Training loss: 0.03669194, Validation loss: 0.03704822, Gradient norm: 0.39425683
INFO:root:[   89] Training loss: 0.03686991, Validation loss: 0.03641735, Gradient norm: 0.45423549
INFO:root:[   90] Training loss: 0.03726618, Validation loss: 0.03679266, Gradient norm: 0.39832314
INFO:root:[   91] Training loss: 0.03641288, Validation loss: 0.03700311, Gradient norm: 0.35657391
INFO:root:[   92] Training loss: 0.03651935, Validation loss: 0.03616397, Gradient norm: 0.40603687
INFO:root:[   93] Training loss: 0.03647021, Validation loss: 0.03815074, Gradient norm: 0.41760183
INFO:root:[   94] Training loss: 0.03679503, Validation loss: 0.03644662, Gradient norm: 0.47789080
INFO:root:[   95] Training loss: 0.03601660, Validation loss: 0.03740316, Gradient norm: 0.38063046
INFO:root:[   96] Training loss: 0.03573556, Validation loss: 0.03655776, Gradient norm: 0.28675477
INFO:root:[   97] Training loss: 0.03637422, Validation loss: 0.03764480, Gradient norm: 0.40493608
INFO:root:[   98] Training loss: 0.03619234, Validation loss: 0.03595017, Gradient norm: 0.39582326
INFO:root:[   99] Training loss: 0.03567557, Validation loss: 0.03606917, Gradient norm: 0.31046975
INFO:root:[  100] Training loss: 0.03626159, Validation loss: 0.03613666, Gradient norm: 0.40885332
INFO:root:[  101] Training loss: 0.03545778, Validation loss: 0.03580766, Gradient norm: 0.35669507
INFO:root:[  102] Training loss: 0.03608949, Validation loss: 0.03643391, Gradient norm: 0.30994514
INFO:root:[  103] Training loss: 0.03516778, Validation loss: 0.03527042, Gradient norm: 0.38455095
INFO:root:[  104] Training loss: 0.03594815, Validation loss: 0.03561020, Gradient norm: 0.40380107
INFO:root:[  105] Training loss: 0.03479959, Validation loss: 0.03689762, Gradient norm: 0.32961660
INFO:root:[  106] Training loss: 0.03666657, Validation loss: 0.03520188, Gradient norm: 0.43225070
INFO:root:[  107] Training loss: 0.03538324, Validation loss: 0.03639248, Gradient norm: 0.45809672
INFO:root:[  108] Training loss: 0.03530294, Validation loss: 0.03534147, Gradient norm: 0.34699369
INFO:root:[  109] Training loss: 0.03464554, Validation loss: 0.03474644, Gradient norm: 0.28293203
INFO:root:[  110] Training loss: 0.03548826, Validation loss: 0.03497959, Gradient norm: 0.43537403
INFO:root:[  111] Training loss: 0.03544593, Validation loss: 0.03529077, Gradient norm: 0.34601564
INFO:root:[  112] Training loss: 0.03522250, Validation loss: 0.03522662, Gradient norm: 0.37465426
INFO:root:[  113] Training loss: 0.03460758, Validation loss: 0.03519264, Gradient norm: 0.34479947
INFO:root:[  114] Training loss: 0.03440446, Validation loss: 0.03476207, Gradient norm: 0.31625770
INFO:root:[  115] Training loss: 0.03447661, Validation loss: 0.03495215, Gradient norm: 0.38896640
INFO:root:[  116] Training loss: 0.03550549, Validation loss: 0.03497103, Gradient norm: 0.46183816
INFO:root:[  117] Training loss: 0.03410190, Validation loss: 0.03491117, Gradient norm: 0.37046112
INFO:root:[  118] Training loss: 0.03401468, Validation loss: 0.03446192, Gradient norm: 0.38282372
INFO:root:[  119] Training loss: 0.03442188, Validation loss: 0.03701950, Gradient norm: 0.39676679
INFO:root:[  120] Training loss: 0.03494225, Validation loss: 0.03471156, Gradient norm: 0.41736884
INFO:root:[  121] Training loss: 0.03440465, Validation loss: 0.03501598, Gradient norm: 0.34617042
INFO:root:[  122] Training loss: 0.03460020, Validation loss: 0.03501646, Gradient norm: 0.36410868
INFO:root:[  123] Training loss: 0.03431688, Validation loss: 0.03493519, Gradient norm: 0.32138616
INFO:root:[  124] Training loss: 0.03495054, Validation loss: 0.03419422, Gradient norm: 0.40976197
INFO:root:[  125] Training loss: 0.03466884, Validation loss: 0.03411095, Gradient norm: 0.44913058
INFO:root:[  126] Training loss: 0.03424503, Validation loss: 0.03432585, Gradient norm: 0.45310661
INFO:root:[  127] Training loss: 0.03329515, Validation loss: 0.03448398, Gradient norm: 0.29714525
INFO:root:[  128] Training loss: 0.03460819, Validation loss: 0.03471669, Gradient norm: 0.38356563
INFO:root:[  129] Training loss: 0.03367280, Validation loss: 0.03440727, Gradient norm: 0.34489441
INFO:root:[  130] Training loss: 0.03339718, Validation loss: 0.03396599, Gradient norm: 0.30234762
INFO:root:[  131] Training loss: 0.03289346, Validation loss: 0.03372445, Gradient norm: 0.26038333
INFO:root:[  132] Training loss: 0.03363266, Validation loss: 0.03484565, Gradient norm: 0.37684273
INFO:root:[  133] Training loss: 0.03394284, Validation loss: 0.03432636, Gradient norm: 0.44818728
INFO:root:[  134] Training loss: 0.03378407, Validation loss: 0.03474661, Gradient norm: 0.44403186
INFO:root:[  135] Training loss: 0.03314865, Validation loss: 0.03304340, Gradient norm: 0.39836441
INFO:root:[  136] Training loss: 0.03270216, Validation loss: 0.03382904, Gradient norm: 0.25546010
INFO:root:[  137] Training loss: 0.03357979, Validation loss: 0.03351298, Gradient norm: 0.38906733
INFO:root:[  138] Training loss: 0.03286944, Validation loss: 0.03338692, Gradient norm: 0.30956445
INFO:root:[  139] Training loss: 0.03347178, Validation loss: 0.03323162, Gradient norm: 0.46963900
INFO:root:[  140] Training loss: 0.03313916, Validation loss: 0.03350636, Gradient norm: 0.39565216
INFO:root:[  141] Training loss: 0.03357881, Validation loss: 0.03374132, Gradient norm: 0.40889553
INFO:root:[  142] Training loss: 0.03284836, Validation loss: 0.03340946, Gradient norm: 0.37197729
INFO:root:[  143] Training loss: 0.03325765, Validation loss: 0.03282321, Gradient norm: 0.36574537
INFO:root:[  144] Training loss: 0.03279498, Validation loss: 0.03291430, Gradient norm: 0.30395687
INFO:root:[  145] Training loss: 0.03312323, Validation loss: 0.03249956, Gradient norm: 0.45686212
INFO:root:[  146] Training loss: 0.03320819, Validation loss: 0.03303534, Gradient norm: 0.39414126
INFO:root:[  147] Training loss: 0.03284022, Validation loss: 0.03251298, Gradient norm: 0.41577967
INFO:root:[  148] Training loss: 0.03248018, Validation loss: 0.03261347, Gradient norm: 0.38298176
INFO:root:[  149] Training loss: 0.03213051, Validation loss: 0.03242250, Gradient norm: 0.36301971
INFO:root:[  150] Training loss: 0.03206746, Validation loss: 0.03288874, Gradient norm: 0.43535418
INFO:root:[  151] Training loss: 0.03256005, Validation loss: 0.03266416, Gradient norm: 0.30994150
INFO:root:[  152] Training loss: 0.03280933, Validation loss: 0.03297868, Gradient norm: 0.32431136
INFO:root:[  153] Training loss: 0.03224410, Validation loss: 0.03253648, Gradient norm: 0.37779872
INFO:root:[  154] Training loss: 0.03184226, Validation loss: 0.03299981, Gradient norm: 0.34214580
INFO:root:[  155] Training loss: 0.03230162, Validation loss: 0.03353169, Gradient norm: 0.28269737
INFO:root:[  156] Training loss: 0.03236983, Validation loss: 0.03183528, Gradient norm: 0.35216981
INFO:root:[  157] Training loss: 0.03162129, Validation loss: 0.03375338, Gradient norm: 0.36556654
INFO:root:[  158] Training loss: 0.03262894, Validation loss: 0.03277714, Gradient norm: 0.40847279
INFO:root:[  159] Training loss: 0.03244237, Validation loss: 0.03224116, Gradient norm: 0.37107991
INFO:root:[  160] Training loss: 0.03128981, Validation loss: 0.03217403, Gradient norm: 0.30179017
INFO:root:[  161] Training loss: 0.03100473, Validation loss: 0.03260852, Gradient norm: 0.36910304
INFO:root:[  162] Training loss: 0.03166900, Validation loss: 0.03152148, Gradient norm: 0.33591230
INFO:root:[  163] Training loss: 0.03164598, Validation loss: 0.03158143, Gradient norm: 0.34704373
INFO:root:[  164] Training loss: 0.03203348, Validation loss: 0.03268212, Gradient norm: 0.49121091
INFO:root:[  165] Training loss: 0.03178291, Validation loss: 0.03202679, Gradient norm: 0.42073272
INFO:root:[  166] Training loss: 0.03148952, Validation loss: 0.03162376, Gradient norm: 0.35017043
INFO:root:[  167] Training loss: 0.03111997, Validation loss: 0.03136142, Gradient norm: 0.32949737
INFO:root:[  168] Training loss: 0.03111243, Validation loss: 0.03150053, Gradient norm: 0.40627504
INFO:root:[  169] Training loss: 0.03099044, Validation loss: 0.03158179, Gradient norm: 0.39024116
INFO:root:[  170] Training loss: 0.03024110, Validation loss: 0.03098909, Gradient norm: 0.28590803
INFO:root:[  171] Training loss: 0.03068571, Validation loss: 0.03082429, Gradient norm: 0.38100485
INFO:root:[  172] Training loss: 0.03034581, Validation loss: 0.03079698, Gradient norm: 0.27042929
INFO:root:[  173] Training loss: 0.03128900, Validation loss: 0.03274764, Gradient norm: 0.35016831
INFO:root:[  174] Training loss: 0.03218570, Validation loss: 0.03092883, Gradient norm: 0.55868598
INFO:root:[  175] Training loss: 0.03089751, Validation loss: 0.03154801, Gradient norm: 0.34706464
INFO:root:[  176] Training loss: 0.03062670, Validation loss: 0.03154221, Gradient norm: 0.37433113
INFO:root:[  177] Training loss: 0.03086503, Validation loss: 0.03084692, Gradient norm: 0.31167945
INFO:root:[  178] Training loss: 0.03099573, Validation loss: 0.03016710, Gradient norm: 0.28228414
INFO:root:[  179] Training loss: 0.03024841, Validation loss: 0.03079002, Gradient norm: 0.37010726
INFO:root:[  180] Training loss: 0.03037916, Validation loss: 0.03057253, Gradient norm: 0.27617367
INFO:root:[  181] Training loss: 0.02984197, Validation loss: 0.03070563, Gradient norm: 0.31854982
INFO:root:[  182] Training loss: 0.03028604, Validation loss: 0.03068775, Gradient norm: 0.30243506
INFO:root:[  183] Training loss: 0.03142546, Validation loss: 0.03064162, Gradient norm: 0.56249288
INFO:root:[  184] Training loss: 0.03023655, Validation loss: 0.02997224, Gradient norm: 0.42921934
INFO:root:[  185] Training loss: 0.03004241, Validation loss: 0.03086197, Gradient norm: 0.36429317
INFO:root:[  186] Training loss: 0.03028002, Validation loss: 0.02976901, Gradient norm: 0.32355862
INFO:root:[  187] Training loss: 0.03004624, Validation loss: 0.03011206, Gradient norm: 0.35886132
INFO:root:[  188] Training loss: 0.02995971, Validation loss: 0.03074106, Gradient norm: 0.29473247
INFO:root:[  189] Training loss: 0.03075821, Validation loss: 0.03176284, Gradient norm: 0.49472489
INFO:root:[  190] Training loss: 0.03081126, Validation loss: 0.03065331, Gradient norm: 0.31630321
INFO:root:[  191] Training loss: 0.02949003, Validation loss: 0.03052344, Gradient norm: 0.34585965
INFO:root:[  192] Training loss: 0.02955461, Validation loss: 0.03004186, Gradient norm: 0.38115849
INFO:root:[  193] Training loss: 0.02936068, Validation loss: 0.03059778, Gradient norm: 0.38751378
INFO:root:[  194] Training loss: 0.02967371, Validation loss: 0.03180554, Gradient norm: 0.42609108
INFO:root:[  195] Training loss: 0.03000661, Validation loss: 0.02975403, Gradient norm: 0.49102388
INFO:root:[  196] Training loss: 0.03014041, Validation loss: 0.03054011, Gradient norm: 0.30534546
INFO:root:[  197] Training loss: 0.02999946, Validation loss: 0.03061453, Gradient norm: 0.42513310
INFO:root:[  198] Training loss: 0.02957513, Validation loss: 0.02925538, Gradient norm: 0.32584320
INFO:root:[  199] Training loss: 0.02902442, Validation loss: 0.02943318, Gradient norm: 0.26399034
INFO:root:[  200] Training loss: 0.02972882, Validation loss: 0.03052077, Gradient norm: 0.37186774
INFO:root:[  201] Training loss: 0.02965673, Validation loss: 0.03088883, Gradient norm: 0.40839938
INFO:root:[  202] Training loss: 0.02986487, Validation loss: 0.03024383, Gradient norm: 0.39617459
INFO:root:[  203] Training loss: 0.02964526, Validation loss: 0.03021395, Gradient norm: 0.38417375
INFO:root:[  204] Training loss: 0.02860459, Validation loss: 0.02932524, Gradient norm: 0.30104444
INFO:root:[  205] Training loss: 0.02916471, Validation loss: 0.02966130, Gradient norm: 0.33141083
INFO:root:[  206] Training loss: 0.02910109, Validation loss: 0.02929721, Gradient norm: 0.33316003
INFO:root:[  207] Training loss: 0.02943607, Validation loss: 0.03029319, Gradient norm: 0.37620021
INFO:root:[  208] Training loss: 0.02960953, Validation loss: 0.02907674, Gradient norm: 0.36166624
INFO:root:[  209] Training loss: 0.02893496, Validation loss: 0.03088115, Gradient norm: 0.35238935
INFO:root:[  210] Training loss: 0.02937540, Validation loss: 0.03017542, Gradient norm: 0.39037885
INFO:root:[  211] Training loss: 0.02986935, Validation loss: 0.02925864, Gradient norm: 0.46815479
INFO:root:[  212] Training loss: 0.02954760, Validation loss: 0.02893857, Gradient norm: 0.38799273
INFO:root:[  213] Training loss: 0.02866958, Validation loss: 0.02885569, Gradient norm: 0.36553934
INFO:root:[  214] Training loss: 0.02889540, Validation loss: 0.02889858, Gradient norm: 0.30195920
INFO:root:[  215] Training loss: 0.02873079, Validation loss: 0.02919638, Gradient norm: 0.35648424
INFO:root:[  216] Training loss: 0.02845613, Validation loss: 0.02918053, Gradient norm: 0.39620521
INFO:root:[  217] Training loss: 0.02838581, Validation loss: 0.02880385, Gradient norm: 0.29458803
INFO:root:[  218] Training loss: 0.02839231, Validation loss: 0.02918439, Gradient norm: 0.29126578
INFO:root:[  219] Training loss: 0.02878858, Validation loss: 0.02934048, Gradient norm: 0.34574314
INFO:root:[  220] Training loss: 0.02902080, Validation loss: 0.03086983, Gradient norm: 0.44921754
INFO:root:[  221] Training loss: 0.02959981, Validation loss: 0.02893337, Gradient norm: 0.49054015
INFO:root:[  222] Training loss: 0.02819583, Validation loss: 0.02917589, Gradient norm: 0.33422414
INFO:root:[  223] Training loss: 0.02839151, Validation loss: 0.02888802, Gradient norm: 0.32062054
INFO:root:[  224] Training loss: 0.02802975, Validation loss: 0.02885988, Gradient norm: 0.33247947
INFO:root:[  225] Training loss: 0.02771677, Validation loss: 0.02891679, Gradient norm: 0.30506721
INFO:root:[  226] Training loss: 0.02822346, Validation loss: 0.02864195, Gradient norm: 0.31825078
INFO:root:[  227] Training loss: 0.02829614, Validation loss: 0.02935073, Gradient norm: 0.37235302
INFO:root:[  228] Training loss: 0.02815051, Validation loss: 0.02846090, Gradient norm: 0.40804392
INFO:root:[  229] Training loss: 0.02806216, Validation loss: 0.02912747, Gradient norm: 0.35111241
INFO:root:[  230] Training loss: 0.02782018, Validation loss: 0.02829997, Gradient norm: 0.33509317
INFO:root:[  231] Training loss: 0.02781771, Validation loss: 0.02872079, Gradient norm: 0.32730218
INFO:root:[  232] Training loss: 0.02809790, Validation loss: 0.02839219, Gradient norm: 0.42635400
INFO:root:[  233] Training loss: 0.02815838, Validation loss: 0.03019061, Gradient norm: 0.44739486
INFO:root:[  234] Training loss: 0.02819222, Validation loss: 0.02836939, Gradient norm: 0.35027525
INFO:root:[  235] Training loss: 0.02744439, Validation loss: 0.02869790, Gradient norm: 0.26820323
INFO:root:[  236] Training loss: 0.02768869, Validation loss: 0.02917795, Gradient norm: 0.33199334
INFO:root:[  237] Training loss: 0.02753268, Validation loss: 0.02847224, Gradient norm: 0.37688847
INFO:root:[  238] Training loss: 0.02734693, Validation loss: 0.02976467, Gradient norm: 0.35579816
INFO:root:[  239] Training loss: 0.02856995, Validation loss: 0.02857961, Gradient norm: 0.51319003
INFO:root:EP 239: Early stopping
INFO:root:Training the model took 462.927s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.65101
INFO:root:EnergyScoreTrain: 0.47938
INFO:root:CoverageTrain: 0.8828
INFO:root:IntervalWidthTrain: 0.06353
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.68181
INFO:root:EnergyScoreValidation: 0.5017
INFO:root:CoverageValidation: 0.87909
INFO:root:IntervalWidthValidation: 0.0645
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.82857
INFO:root:EnergyScoreTest: 1.53139
INFO:root:CoverageTest: 0.44808
INFO:root:IntervalWidthTest: 0.063
INFO:root:###19 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 207618048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.15248043, Validation loss: 0.12502395, Gradient norm: 0.59277811
INFO:root:[    2] Training loss: 0.11529763, Validation loss: 0.10890451, Gradient norm: 0.28509441
INFO:root:[    3] Training loss: 0.10307537, Validation loss: 0.10014396, Gradient norm: 0.29301442
INFO:root:[    4] Training loss: 0.09317920, Validation loss: 0.09319258, Gradient norm: 0.27109799
INFO:root:[    5] Training loss: 0.08914294, Validation loss: 0.08972282, Gradient norm: 0.26096771
INFO:root:[    6] Training loss: 0.08206238, Validation loss: 0.08192190, Gradient norm: 0.29118554
INFO:root:[    7] Training loss: 0.08092683, Validation loss: 0.07951276, Gradient norm: 0.29856658
INFO:root:[    8] Training loss: 0.07664653, Validation loss: 0.07506923, Gradient norm: 0.27748748
INFO:root:[    9] Training loss: 0.07572653, Validation loss: 0.07560225, Gradient norm: 0.30697590
INFO:root:[   10] Training loss: 0.07324328, Validation loss: 0.07173678, Gradient norm: 0.42882189
INFO:root:[   11] Training loss: 0.07082536, Validation loss: 0.06936747, Gradient norm: 0.21575660
INFO:root:[   12] Training loss: 0.07033847, Validation loss: 0.06821508, Gradient norm: 0.34554745
INFO:root:[   13] Training loss: 0.07074421, Validation loss: 0.06741506, Gradient norm: 0.53432813
INFO:root:[   14] Training loss: 0.06612058, Validation loss: 0.06628852, Gradient norm: 0.25096358
INFO:root:[   15] Training loss: 0.06692729, Validation loss: 0.06520665, Gradient norm: 0.26182559
INFO:root:[   16] Training loss: 0.06459164, Validation loss: 0.06455470, Gradient norm: 0.22061629
INFO:root:[   17] Training loss: 0.06331964, Validation loss: 0.06400564, Gradient norm: 0.25011623
INFO:root:[   18] Training loss: 0.06291434, Validation loss: 0.06273385, Gradient norm: 0.25944867
INFO:root:[   19] Training loss: 0.06248421, Validation loss: 0.06226432, Gradient norm: 0.28215115
INFO:root:[   20] Training loss: 0.06196265, Validation loss: 0.06111182, Gradient norm: 0.32820885
INFO:root:[   21] Training loss: 0.06160834, Validation loss: 0.05989821, Gradient norm: 0.27647370
INFO:root:[   22] Training loss: 0.05986421, Validation loss: 0.05955556, Gradient norm: 0.27140687
INFO:root:[   23] Training loss: 0.05956115, Validation loss: 0.05962490, Gradient norm: 0.28923479
INFO:root:[   24] Training loss: 0.06009639, Validation loss: 0.05897635, Gradient norm: 0.31414297
INFO:root:[   25] Training loss: 0.05831393, Validation loss: 0.05882798, Gradient norm: 0.25421698
INFO:root:[   26] Training loss: 0.05755781, Validation loss: 0.05896584, Gradient norm: 0.23996982
INFO:root:[   27] Training loss: 0.05676810, Validation loss: 0.05804271, Gradient norm: 0.25309669
INFO:root:[   28] Training loss: 0.05722037, Validation loss: 0.05754109, Gradient norm: 0.23124785
INFO:root:[   29] Training loss: 0.05722725, Validation loss: 0.05588751, Gradient norm: 0.24001116
INFO:root:[   30] Training loss: 0.05608532, Validation loss: 0.05599654, Gradient norm: 0.23685931
INFO:root:[   31] Training loss: 0.05699615, Validation loss: 0.05660617, Gradient norm: 0.24983508
INFO:root:[   32] Training loss: 0.05587426, Validation loss: 0.05533892, Gradient norm: 0.26158676
INFO:root:[   33] Training loss: 0.05488481, Validation loss: 0.05500661, Gradient norm: 0.30437934
INFO:root:[   34] Training loss: 0.05443673, Validation loss: 0.05418224, Gradient norm: 0.28402011
INFO:root:[   35] Training loss: 0.05411986, Validation loss: 0.05431941, Gradient norm: 0.28123580
INFO:root:[   36] Training loss: 0.05350860, Validation loss: 0.05310685, Gradient norm: 0.28308600
INFO:root:[   37] Training loss: 0.05302182, Validation loss: 0.05315403, Gradient norm: 0.21923889
INFO:root:[   38] Training loss: 0.05369310, Validation loss: 0.05314540, Gradient norm: 0.31197599
INFO:root:[   39] Training loss: 0.05260180, Validation loss: 0.05219888, Gradient norm: 0.24232863
INFO:root:[   40] Training loss: 0.05212924, Validation loss: 0.05170461, Gradient norm: 0.27211574
INFO:root:[   41] Training loss: 0.05303098, Validation loss: 0.05190583, Gradient norm: 0.34082051
INFO:root:[   42] Training loss: 0.05143474, Validation loss: 0.05247515, Gradient norm: 0.28129903
INFO:root:[   43] Training loss: 0.05151442, Validation loss: 0.05168825, Gradient norm: 0.26693911
INFO:root:[   44] Training loss: 0.05150023, Validation loss: 0.05133612, Gradient norm: 0.36501677
INFO:root:[   45] Training loss: 0.05116928, Validation loss: 0.05077403, Gradient norm: 0.27545765
INFO:root:[   46] Training loss: 0.05104654, Validation loss: 0.05094195, Gradient norm: 0.31243467
INFO:root:[   47] Training loss: 0.04998873, Validation loss: 0.05037100, Gradient norm: 0.30081897
INFO:root:[   48] Training loss: 0.04998780, Validation loss: 0.05040090, Gradient norm: 0.28356170
INFO:root:[   49] Training loss: 0.05077581, Validation loss: 0.04988350, Gradient norm: 0.26680884
INFO:root:[   50] Training loss: 0.04955368, Validation loss: 0.04889873, Gradient norm: 0.32549557
INFO:root:[   51] Training loss: 0.04948900, Validation loss: 0.04942765, Gradient norm: 0.31932709
INFO:root:[   52] Training loss: 0.04963744, Validation loss: 0.05213856, Gradient norm: 0.31673382
INFO:root:[   53] Training loss: 0.04971313, Validation loss: 0.04810941, Gradient norm: 0.45106608
INFO:root:[   54] Training loss: 0.04809117, Validation loss: 0.04899948, Gradient norm: 0.31024621
INFO:root:[   55] Training loss: 0.04821477, Validation loss: 0.04844583, Gradient norm: 0.27880604
INFO:root:[   56] Training loss: 0.04790124, Validation loss: 0.04877330, Gradient norm: 0.26263555
INFO:root:[   57] Training loss: 0.04739881, Validation loss: 0.04840455, Gradient norm: 0.35058616
INFO:root:[   58] Training loss: 0.04787075, Validation loss: 0.04750060, Gradient norm: 0.31411829
INFO:root:[   59] Training loss: 0.04667436, Validation loss: 0.04608490, Gradient norm: 0.32874946
INFO:root:[   60] Training loss: 0.04675709, Validation loss: 0.04710264, Gradient norm: 0.28685825
INFO:root:[   61] Training loss: 0.04717414, Validation loss: 0.04684950, Gradient norm: 0.38854981
INFO:root:[   62] Training loss: 0.04667726, Validation loss: 0.04710175, Gradient norm: 0.29309646
INFO:root:[   63] Training loss: 0.04653164, Validation loss: 0.04627556, Gradient norm: 0.31963059
INFO:root:[   64] Training loss: 0.04596238, Validation loss: 0.04608008, Gradient norm: 0.33784619
INFO:root:[   65] Training loss: 0.04616604, Validation loss: 0.04594082, Gradient norm: 0.36601651
INFO:root:[   66] Training loss: 0.04630547, Validation loss: 0.04645876, Gradient norm: 0.35180138
INFO:root:[   67] Training loss: 0.04554605, Validation loss: 0.04581776, Gradient norm: 0.30195378
INFO:root:[   68] Training loss: 0.04625064, Validation loss: 0.04565664, Gradient norm: 0.43769431
INFO:root:[   69] Training loss: 0.04532046, Validation loss: 0.04524933, Gradient norm: 0.32780600
INFO:root:[   70] Training loss: 0.04515707, Validation loss: 0.04571517, Gradient norm: 0.32799081
INFO:root:[   71] Training loss: 0.04558770, Validation loss: 0.04519609, Gradient norm: 0.24937310
INFO:root:[   72] Training loss: 0.04486737, Validation loss: 0.04484665, Gradient norm: 0.34148336
INFO:root:[   73] Training loss: 0.04571607, Validation loss: 0.04633981, Gradient norm: 0.42510566
INFO:root:[   74] Training loss: 0.04609100, Validation loss: 0.04434149, Gradient norm: 0.46458697
INFO:root:[   75] Training loss: 0.04418648, Validation loss: 0.04431630, Gradient norm: 0.27357984
INFO:root:[   76] Training loss: 0.04524507, Validation loss: 0.04399537, Gradient norm: 0.36072934
INFO:root:[   77] Training loss: 0.04344357, Validation loss: 0.04445773, Gradient norm: 0.33854379
INFO:root:[   78] Training loss: 0.04468128, Validation loss: 0.04380099, Gradient norm: 0.40899170
INFO:root:[   79] Training loss: 0.04399098, Validation loss: 0.04363544, Gradient norm: 0.32567491
INFO:root:[   80] Training loss: 0.04432440, Validation loss: 0.04489598, Gradient norm: 0.32235556
INFO:root:[   81] Training loss: 0.04385692, Validation loss: 0.04337499, Gradient norm: 0.34866164
INFO:root:[   82] Training loss: 0.04385733, Validation loss: 0.04281241, Gradient norm: 0.40028426
INFO:root:[   83] Training loss: 0.04258275, Validation loss: 0.04435028, Gradient norm: 0.30779927
INFO:root:[   84] Training loss: 0.04296875, Validation loss: 0.04341480, Gradient norm: 0.33179134
INFO:root:[   85] Training loss: 0.04363550, Validation loss: 0.04277382, Gradient norm: 0.34532106
INFO:root:[   86] Training loss: 0.04300393, Validation loss: 0.04308062, Gradient norm: 0.44081294
INFO:root:[   87] Training loss: 0.04342808, Validation loss: 0.04265341, Gradient norm: 0.34457965
INFO:root:[   88] Training loss: 0.04220900, Validation loss: 0.04282374, Gradient norm: 0.29280826
INFO:root:[   89] Training loss: 0.04360770, Validation loss: 0.04245819, Gradient norm: 0.34763018
INFO:root:[   90] Training loss: 0.04113915, Validation loss: 0.04217753, Gradient norm: 0.28283854
INFO:root:[   91] Training loss: 0.04292952, Validation loss: 0.04247943, Gradient norm: 0.39105491
INFO:root:[   92] Training loss: 0.04159703, Validation loss: 0.04306920, Gradient norm: 0.30956509
INFO:root:[   93] Training loss: 0.04256708, Validation loss: 0.04263078, Gradient norm: 0.36614522
INFO:root:[   94] Training loss: 0.04048827, Validation loss: 0.04172856, Gradient norm: 0.28272965
INFO:root:[   95] Training loss: 0.04199274, Validation loss: 0.04206485, Gradient norm: 0.32989130
INFO:root:[   96] Training loss: 0.04072427, Validation loss: 0.04176389, Gradient norm: 0.35281965
INFO:root:[   97] Training loss: 0.04087100, Validation loss: 0.04086108, Gradient norm: 0.30251213
INFO:root:[   98] Training loss: 0.04134112, Validation loss: 0.04115948, Gradient norm: 0.29287751
INFO:root:[   99] Training loss: 0.04276948, Validation loss: 0.04306779, Gradient norm: 0.43718620
INFO:root:[  100] Training loss: 0.04175718, Validation loss: 0.04097478, Gradient norm: 0.35414541
INFO:root:[  101] Training loss: 0.04069041, Validation loss: 0.04104917, Gradient norm: 0.29880384
INFO:root:[  102] Training loss: 0.03991027, Validation loss: 0.04093639, Gradient norm: 0.28360849
INFO:root:[  103] Training loss: 0.04136052, Validation loss: 0.04013657, Gradient norm: 0.32745814
INFO:root:[  104] Training loss: 0.04043983, Validation loss: 0.04084292, Gradient norm: 0.35250590
INFO:root:[  105] Training loss: 0.04019837, Validation loss: 0.04001599, Gradient norm: 0.27947397
INFO:root:[  106] Training loss: 0.03961956, Validation loss: 0.04141177, Gradient norm: 0.27857999
INFO:root:[  107] Training loss: 0.04009760, Validation loss: 0.04092578, Gradient norm: 0.32923372
INFO:root:[  108] Training loss: 0.04029517, Validation loss: 0.03964554, Gradient norm: 0.43032658
INFO:root:[  109] Training loss: 0.03989055, Validation loss: 0.04009935, Gradient norm: 0.31019510
INFO:root:[  110] Training loss: 0.03977996, Validation loss: 0.04047213, Gradient norm: 0.36460733
INFO:root:[  111] Training loss: 0.04018781, Validation loss: 0.03974044, Gradient norm: 0.38633133
INFO:root:[  112] Training loss: 0.04009021, Validation loss: 0.03936350, Gradient norm: 0.35046541
INFO:root:[  113] Training loss: 0.04070511, Validation loss: 0.04157794, Gradient norm: 0.43090386
INFO:root:[  114] Training loss: 0.03987585, Validation loss: 0.03944073, Gradient norm: 0.35238397
INFO:root:[  115] Training loss: 0.03878757, Validation loss: 0.03947231, Gradient norm: 0.38163513
INFO:root:[  116] Training loss: 0.03992203, Validation loss: 0.03879767, Gradient norm: 0.28925479
INFO:root:[  117] Training loss: 0.03907487, Validation loss: 0.04043509, Gradient norm: 0.29805403
INFO:root:[  118] Training loss: 0.03920388, Validation loss: 0.03901613, Gradient norm: 0.37608213
INFO:root:[  119] Training loss: 0.03878021, Validation loss: 0.03904176, Gradient norm: 0.33143573
INFO:root:[  120] Training loss: 0.03919834, Validation loss: 0.03807862, Gradient norm: 0.42639393
INFO:root:[  121] Training loss: 0.03843898, Validation loss: 0.03962046, Gradient norm: 0.37758065
INFO:root:[  122] Training loss: 0.03807051, Validation loss: 0.03848268, Gradient norm: 0.32790367
INFO:root:[  123] Training loss: 0.03804210, Validation loss: 0.03820364, Gradient norm: 0.30224921
INFO:root:[  124] Training loss: 0.03841800, Validation loss: 0.03825188, Gradient norm: 0.41366771
INFO:root:[  125] Training loss: 0.03792820, Validation loss: 0.03921031, Gradient norm: 0.28083181
INFO:root:[  126] Training loss: 0.03852965, Validation loss: 0.03800539, Gradient norm: 0.33540326
INFO:root:[  127] Training loss: 0.03744200, Validation loss: 0.03817416, Gradient norm: 0.36907785
INFO:root:[  128] Training loss: 0.03768831, Validation loss: 0.03754080, Gradient norm: 0.37068651
INFO:root:[  129] Training loss: 0.03765800, Validation loss: 0.03789105, Gradient norm: 0.31363935
INFO:root:[  130] Training loss: 0.03695880, Validation loss: 0.03785870, Gradient norm: 0.24487298
INFO:root:[  131] Training loss: 0.03706872, Validation loss: 0.03738759, Gradient norm: 0.30490177
INFO:root:[  132] Training loss: 0.03823734, Validation loss: 0.03742464, Gradient norm: 0.47157457
INFO:root:[  133] Training loss: 0.03723645, Validation loss: 0.03856420, Gradient norm: 0.33964644
INFO:root:[  134] Training loss: 0.03809967, Validation loss: 0.03744001, Gradient norm: 0.38607751
INFO:root:[  135] Training loss: 0.03781745, Validation loss: 0.03833859, Gradient norm: 0.35273518
INFO:root:[  136] Training loss: 0.03683094, Validation loss: 0.03755246, Gradient norm: 0.28461063
INFO:root:[  137] Training loss: 0.03762975, Validation loss: 0.03691122, Gradient norm: 0.30157206
INFO:root:[  138] Training loss: 0.03697431, Validation loss: 0.03707686, Gradient norm: 0.33717467
INFO:root:[  139] Training loss: 0.03625792, Validation loss: 0.03749998, Gradient norm: 0.30774510
INFO:root:[  140] Training loss: 0.03660495, Validation loss: 0.03684980, Gradient norm: 0.35792085
INFO:root:[  141] Training loss: 0.03647154, Validation loss: 0.03728321, Gradient norm: 0.27628717
INFO:root:[  142] Training loss: 0.03656434, Validation loss: 0.03704559, Gradient norm: 0.40792352
INFO:root:[  143] Training loss: 0.03571966, Validation loss: 0.03671272, Gradient norm: 0.28978085
INFO:root:[  144] Training loss: 0.03652834, Validation loss: 0.03640226, Gradient norm: 0.29171687
INFO:root:[  145] Training loss: 0.03616125, Validation loss: 0.03615872, Gradient norm: 0.23094321
INFO:root:[  146] Training loss: 0.03607546, Validation loss: 0.03607544, Gradient norm: 0.35798511
INFO:root:[  147] Training loss: 0.03616670, Validation loss: 0.03645430, Gradient norm: 0.30841567
INFO:root:[  148] Training loss: 0.03667738, Validation loss: 0.03625623, Gradient norm: 0.30955629
INFO:root:[  149] Training loss: 0.03599352, Validation loss: 0.03638885, Gradient norm: 0.40164558
INFO:root:[  150] Training loss: 0.03568352, Validation loss: 0.03643351, Gradient norm: 0.31476249
INFO:root:[  151] Training loss: 0.03641932, Validation loss: 0.03594987, Gradient norm: 0.27845713
INFO:root:[  152] Training loss: 0.03561277, Validation loss: 0.03614285, Gradient norm: 0.33692783
INFO:root:[  153] Training loss: 0.03522561, Validation loss: 0.03567273, Gradient norm: 0.28719433
INFO:root:[  154] Training loss: 0.03575712, Validation loss: 0.03536098, Gradient norm: 0.29471145
INFO:root:[  155] Training loss: 0.03552225, Validation loss: 0.03536189, Gradient norm: 0.29469749
INFO:root:[  156] Training loss: 0.03511167, Validation loss: 0.03575906, Gradient norm: 0.28531806
INFO:root:[  157] Training loss: 0.03526545, Validation loss: 0.03647936, Gradient norm: 0.33975281
INFO:root:[  158] Training loss: 0.03509105, Validation loss: 0.03476718, Gradient norm: 0.33466109
INFO:root:[  159] Training loss: 0.03554093, Validation loss: 0.03545013, Gradient norm: 0.26804480
INFO:root:[  160] Training loss: 0.03496424, Validation loss: 0.03514974, Gradient norm: 0.30686622
INFO:root:[  161] Training loss: 0.03512735, Validation loss: 0.03512228, Gradient norm: 0.32173807
INFO:root:[  162] Training loss: 0.03455292, Validation loss: 0.03537093, Gradient norm: 0.29400397
INFO:root:[  163] Training loss: 0.03413332, Validation loss: 0.03459992, Gradient norm: 0.30348689
INFO:root:[  164] Training loss: 0.03413728, Validation loss: 0.03459858, Gradient norm: 0.25989585
INFO:root:[  165] Training loss: 0.03460366, Validation loss: 0.03445343, Gradient norm: 0.35371291
INFO:root:[  166] Training loss: 0.03558986, Validation loss: 0.03544137, Gradient norm: 0.45050938
INFO:root:[  167] Training loss: 0.03472902, Validation loss: 0.03542024, Gradient norm: 0.34954911
INFO:root:[  168] Training loss: 0.03389127, Validation loss: 0.03431731, Gradient norm: 0.29560718
INFO:root:[  169] Training loss: 0.03458898, Validation loss: 0.03561316, Gradient norm: 0.33913840
INFO:root:[  170] Training loss: 0.03393486, Validation loss: 0.03528030, Gradient norm: 0.38362281
INFO:root:[  171] Training loss: 0.03367858, Validation loss: 0.03490516, Gradient norm: 0.26131536
INFO:root:[  172] Training loss: 0.03439707, Validation loss: 0.03435211, Gradient norm: 0.42841988
INFO:root:[  173] Training loss: 0.03464287, Validation loss: 0.03532092, Gradient norm: 0.41975748
INFO:root:[  174] Training loss: 0.03384513, Validation loss: 0.03363053, Gradient norm: 0.30491094
INFO:root:[  175] Training loss: 0.03357858, Validation loss: 0.03392680, Gradient norm: 0.28879524
INFO:root:[  176] Training loss: 0.03348124, Validation loss: 0.03348749, Gradient norm: 0.31808797
INFO:root:[  177] Training loss: 0.03400333, Validation loss: 0.03363682, Gradient norm: 0.33562266
INFO:root:[  178] Training loss: 0.03300624, Validation loss: 0.03392061, Gradient norm: 0.26331371
INFO:root:[  179] Training loss: 0.03289411, Validation loss: 0.03422915, Gradient norm: 0.35587487
INFO:root:[  180] Training loss: 0.03340560, Validation loss: 0.03402208, Gradient norm: 0.40360194
INFO:root:[  181] Training loss: 0.03342790, Validation loss: 0.03350235, Gradient norm: 0.27079577
INFO:root:[  182] Training loss: 0.03294246, Validation loss: 0.03459629, Gradient norm: 0.30039292
INFO:root:[  183] Training loss: 0.03280488, Validation loss: 0.03468120, Gradient norm: 0.37656296
INFO:root:[  184] Training loss: 0.03321923, Validation loss: 0.03314773, Gradient norm: 0.34514903
INFO:root:[  185] Training loss: 0.03250084, Validation loss: 0.03387249, Gradient norm: 0.29863824
INFO:root:[  186] Training loss: 0.03272871, Validation loss: 0.03314111, Gradient norm: 0.25053092
INFO:root:[  187] Training loss: 0.03242634, Validation loss: 0.03361677, Gradient norm: 0.23714046
INFO:root:[  188] Training loss: 0.03302944, Validation loss: 0.03251732, Gradient norm: 0.28072133
INFO:root:[  189] Training loss: 0.03309379, Validation loss: 0.03318307, Gradient norm: 0.31741869
INFO:root:[  190] Training loss: 0.03205923, Validation loss: 0.03306139, Gradient norm: 0.32340934
INFO:root:[  191] Training loss: 0.03234004, Validation loss: 0.03274443, Gradient norm: 0.25058963
INFO:root:[  192] Training loss: 0.03204202, Validation loss: 0.03277087, Gradient norm: 0.35854201
INFO:root:[  193] Training loss: 0.03170830, Validation loss: 0.03254631, Gradient norm: 0.27353704
INFO:root:[  194] Training loss: 0.03285070, Validation loss: 0.03348638, Gradient norm: 0.34247981
INFO:root:[  195] Training loss: 0.03222862, Validation loss: 0.03239953, Gradient norm: 0.35955093
INFO:root:[  196] Training loss: 0.03181646, Validation loss: 0.03204650, Gradient norm: 0.28949261
INFO:root:[  197] Training loss: 0.03184555, Validation loss: 0.03323830, Gradient norm: 0.32880797
INFO:root:[  198] Training loss: 0.03226161, Validation loss: 0.03263975, Gradient norm: 0.36244642
INFO:root:[  199] Training loss: 0.03215403, Validation loss: 0.03256351, Gradient norm: 0.32010834
INFO:root:[  200] Training loss: 0.03121668, Validation loss: 0.03274293, Gradient norm: 0.28740838
INFO:root:[  201] Training loss: 0.03179766, Validation loss: 0.03416375, Gradient norm: 0.30873135
INFO:root:[  202] Training loss: 0.03157897, Validation loss: 0.03260655, Gradient norm: 0.32278014
INFO:root:[  203] Training loss: 0.03179192, Validation loss: 0.03255962, Gradient norm: 0.35089301
INFO:root:[  204] Training loss: 0.03207218, Validation loss: 0.03213398, Gradient norm: 0.31098709
INFO:root:[  205] Training loss: 0.03161845, Validation loss: 0.03237367, Gradient norm: 0.28262776
INFO:root:EP 205: Early stopping
INFO:root:Training the model took 398.149s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.73714
INFO:root:EnergyScoreTrain: 0.54508
INFO:root:CoverageTrain: 0.88723
INFO:root:IntervalWidthTrain: 0.07308
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.76446
INFO:root:EnergyScoreValidation: 0.56504
INFO:root:CoverageValidation: 0.88772
INFO:root:IntervalWidthValidation: 0.07402
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.55751
INFO:root:EnergyScoreTest: 1.22222
INFO:root:CoverageTest: 0.57847
INFO:root:IntervalWidthTest: 0.08069
INFO:root:###20 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 207618048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.22887391, Validation loss: 0.16473953, Gradient norm: 0.91857197
INFO:root:[    2] Training loss: 0.14894290, Validation loss: 0.13816635, Gradient norm: 0.51663344
INFO:root:[    3] Training loss: 0.12933140, Validation loss: 0.12643154, Gradient norm: 0.43525701
INFO:root:[    4] Training loss: 0.11855529, Validation loss: 0.11717782, Gradient norm: 0.36360798
INFO:root:[    5] Training loss: 0.11461441, Validation loss: 0.11244607, Gradient norm: 0.34692976
INFO:root:[    6] Training loss: 0.10794218, Validation loss: 0.10747295, Gradient norm: 0.26647455
INFO:root:[    7] Training loss: 0.10541921, Validation loss: 0.10479536, Gradient norm: 0.25862568
INFO:root:[    8] Training loss: 0.10139289, Validation loss: 0.10045970, Gradient norm: 0.26792806
INFO:root:[    9] Training loss: 0.09893521, Validation loss: 0.09785152, Gradient norm: 0.23291213
INFO:root:[   10] Training loss: 0.09462306, Validation loss: 0.09321741, Gradient norm: 0.34444378
INFO:root:[   11] Training loss: 0.09092607, Validation loss: 0.09158563, Gradient norm: 0.28403161
INFO:root:[   12] Training loss: 0.08895482, Validation loss: 0.08836316, Gradient norm: 0.42718085
INFO:root:[   13] Training loss: 0.08537955, Validation loss: 0.08455789, Gradient norm: 0.34221933
INFO:root:[   14] Training loss: 0.08164679, Validation loss: 0.08264302, Gradient norm: 0.26297866
INFO:root:[   15] Training loss: 0.08178117, Validation loss: 0.08017929, Gradient norm: 0.34156130
INFO:root:[   16] Training loss: 0.07989365, Validation loss: 0.07973916, Gradient norm: 0.46410960
INFO:root:[   17] Training loss: 0.07747471, Validation loss: 0.07733692, Gradient norm: 0.38890267
INFO:root:[   18] Training loss: 0.07599252, Validation loss: 0.07633122, Gradient norm: 0.32968137
INFO:root:[   19] Training loss: 0.07585456, Validation loss: 0.07525956, Gradient norm: 0.25783124
INFO:root:[   20] Training loss: 0.07415297, Validation loss: 0.07376304, Gradient norm: 0.29437296
INFO:root:[   21] Training loss: 0.07315797, Validation loss: 0.07253822, Gradient norm: 0.30071022
INFO:root:[   22] Training loss: 0.07212213, Validation loss: 0.07163175, Gradient norm: 0.30944274
INFO:root:[   23] Training loss: 0.07273922, Validation loss: 0.07111474, Gradient norm: 0.41910951
INFO:root:[   24] Training loss: 0.07036419, Validation loss: 0.06970376, Gradient norm: 0.43538160
INFO:root:[   25] Training loss: 0.06943976, Validation loss: 0.07003117, Gradient norm: 0.38731290
INFO:root:[   26] Training loss: 0.06842374, Validation loss: 0.06826689, Gradient norm: 0.31772285
INFO:root:[   27] Training loss: 0.06902084, Validation loss: 0.06809191, Gradient norm: 0.35595935
INFO:root:[   28] Training loss: 0.06722134, Validation loss: 0.06682918, Gradient norm: 0.42853568
INFO:root:[   29] Training loss: 0.06727987, Validation loss: 0.06629146, Gradient norm: 0.37539082
INFO:root:[   30] Training loss: 0.06619342, Validation loss: 0.06538887, Gradient norm: 0.33563792
INFO:root:[   31] Training loss: 0.06494182, Validation loss: 0.06468051, Gradient norm: 0.29905272
INFO:root:[   32] Training loss: 0.06445542, Validation loss: 0.06385492, Gradient norm: 0.25962070
INFO:root:[   33] Training loss: 0.06417118, Validation loss: 0.06372910, Gradient norm: 0.36301825
INFO:root:[   34] Training loss: 0.06268376, Validation loss: 0.06350055, Gradient norm: 0.34634312
INFO:root:[   35] Training loss: 0.06213822, Validation loss: 0.06284555, Gradient norm: 0.34402547
INFO:root:[   36] Training loss: 0.06148727, Validation loss: 0.06180120, Gradient norm: 0.40269774
INFO:root:[   37] Training loss: 0.06118259, Validation loss: 0.06154786, Gradient norm: 0.37676975
INFO:root:[   38] Training loss: 0.06079508, Validation loss: 0.06106504, Gradient norm: 0.34092168
INFO:root:[   39] Training loss: 0.06088391, Validation loss: 0.06010069, Gradient norm: 0.34500311
INFO:root:[   40] Training loss: 0.06084340, Validation loss: 0.05970515, Gradient norm: 0.34862320
INFO:root:[   41] Training loss: 0.05877377, Validation loss: 0.06026253, Gradient norm: 0.29000224
INFO:root:[   42] Training loss: 0.05946187, Validation loss: 0.05990923, Gradient norm: 0.39471590
INFO:root:[   43] Training loss: 0.05822242, Validation loss: 0.05857768, Gradient norm: 0.30904366
INFO:root:[   44] Training loss: 0.05806539, Validation loss: 0.05755911, Gradient norm: 0.48026021
INFO:root:[   45] Training loss: 0.05741883, Validation loss: 0.05703491, Gradient norm: 0.45019161
INFO:root:[   46] Training loss: 0.05705789, Validation loss: 0.05720970, Gradient norm: 0.39877993
INFO:root:[   47] Training loss: 0.05757907, Validation loss: 0.05654265, Gradient norm: 0.28524733
INFO:root:[   48] Training loss: 0.05590457, Validation loss: 0.05656690, Gradient norm: 0.36174500
INFO:root:[   49] Training loss: 0.05616953, Validation loss: 0.05555296, Gradient norm: 0.46500854
INFO:root:[   50] Training loss: 0.05556519, Validation loss: 0.05602943, Gradient norm: 0.33040495
INFO:root:[   51] Training loss: 0.05539252, Validation loss: 0.05488132, Gradient norm: 0.41118851
INFO:root:[   52] Training loss: 0.05495940, Validation loss: 0.05444044, Gradient norm: 0.40606505
INFO:root:[   53] Training loss: 0.05372332, Validation loss: 0.05512032, Gradient norm: 0.31729050
INFO:root:[   54] Training loss: 0.05419586, Validation loss: 0.05450366, Gradient norm: 0.43671998
INFO:root:[   55] Training loss: 0.05565902, Validation loss: 0.05367995, Gradient norm: 0.48815424
INFO:root:[   56] Training loss: 0.05322153, Validation loss: 0.05417187, Gradient norm: 0.31080122
INFO:root:[   57] Training loss: 0.05343293, Validation loss: 0.05304408, Gradient norm: 0.42123054
INFO:root:[   58] Training loss: 0.05236031, Validation loss: 0.05303233, Gradient norm: 0.29819919
INFO:root:[   59] Training loss: 0.05299227, Validation loss: 0.05219419, Gradient norm: 0.38174138
INFO:root:[   60] Training loss: 0.05177767, Validation loss: 0.05223960, Gradient norm: 0.44819872
INFO:root:[   61] Training loss: 0.05257326, Validation loss: 0.05249547, Gradient norm: 0.35937976
INFO:root:[   62] Training loss: 0.05222853, Validation loss: 0.05197470, Gradient norm: 0.37764748
INFO:root:[   63] Training loss: 0.05157153, Validation loss: 0.05117570, Gradient norm: 0.47702577
INFO:root:[   64] Training loss: 0.05051306, Validation loss: 0.05066192, Gradient norm: 0.37634301
INFO:root:[   65] Training loss: 0.05099982, Validation loss: 0.05131779, Gradient norm: 0.40682191
INFO:root:[   66] Training loss: 0.05064063, Validation loss: 0.05046293, Gradient norm: 0.46649799
INFO:root:[   67] Training loss: 0.05139570, Validation loss: 0.05072696, Gradient norm: 0.42591002
INFO:root:[   68] Training loss: 0.05078662, Validation loss: 0.04958108, Gradient norm: 0.36384272
INFO:root:[   69] Training loss: 0.04891311, Validation loss: 0.04942322, Gradient norm: 0.27982344
INFO:root:[   70] Training loss: 0.04955159, Validation loss: 0.05020287, Gradient norm: 0.30919129
INFO:root:[   71] Training loss: 0.05009752, Validation loss: 0.04960496, Gradient norm: 0.33371506
INFO:root:[   72] Training loss: 0.05017527, Validation loss: 0.04947400, Gradient norm: 0.49425583
INFO:root:[   73] Training loss: 0.04884818, Validation loss: 0.04983054, Gradient norm: 0.32303185
INFO:root:[   74] Training loss: 0.04885579, Validation loss: 0.05003378, Gradient norm: 0.44595165
INFO:root:[   75] Training loss: 0.04923528, Validation loss: 0.04832779, Gradient norm: 0.41406585
INFO:root:[   76] Training loss: 0.04821995, Validation loss: 0.04837709, Gradient norm: 0.31074875
INFO:root:[   77] Training loss: 0.04742902, Validation loss: 0.04775697, Gradient norm: 0.38004327
INFO:root:[   78] Training loss: 0.04860733, Validation loss: 0.04772311, Gradient norm: 0.43762873
INFO:root:[   79] Training loss: 0.04805752, Validation loss: 0.04753957, Gradient norm: 0.43395752
INFO:root:[   80] Training loss: 0.04815465, Validation loss: 0.04902566, Gradient norm: 0.48071825
INFO:root:[   81] Training loss: 0.04752340, Validation loss: 0.04824382, Gradient norm: 0.53796172
INFO:root:[   82] Training loss: 0.04673486, Validation loss: 0.04686203, Gradient norm: 0.34314022
INFO:root:[   83] Training loss: 0.04687986, Validation loss: 0.04789533, Gradient norm: 0.33899918
INFO:root:[   84] Training loss: 0.04624738, Validation loss: 0.04742105, Gradient norm: 0.36230272
INFO:root:[   85] Training loss: 0.04703421, Validation loss: 0.04663490, Gradient norm: 0.39022056
INFO:root:[   86] Training loss: 0.04673275, Validation loss: 0.04610174, Gradient norm: 0.40735930
INFO:root:[   87] Training loss: 0.04569053, Validation loss: 0.04609982, Gradient norm: 0.32796205
INFO:root:[   88] Training loss: 0.04585867, Validation loss: 0.04574522, Gradient norm: 0.33501896
INFO:root:[   89] Training loss: 0.04585918, Validation loss: 0.04781182, Gradient norm: 0.31315328
INFO:root:[   90] Training loss: 0.04561925, Validation loss: 0.04537906, Gradient norm: 0.40838379
INFO:root:[   91] Training loss: 0.04640596, Validation loss: 0.04563261, Gradient norm: 0.36781895
INFO:root:[   92] Training loss: 0.04490935, Validation loss: 0.04448886, Gradient norm: 0.36449079
INFO:root:[   93] Training loss: 0.04564065, Validation loss: 0.04525547, Gradient norm: 0.36767351
INFO:root:[   94] Training loss: 0.04537862, Validation loss: 0.04473228, Gradient norm: 0.43751077
INFO:root:[   95] Training loss: 0.04488339, Validation loss: 0.04524252, Gradient norm: 0.32237858
INFO:root:[   96] Training loss: 0.04479588, Validation loss: 0.04500114, Gradient norm: 0.39653384
INFO:root:[   97] Training loss: 0.04517099, Validation loss: 0.04406956, Gradient norm: 0.31469164
INFO:root:[   98] Training loss: 0.04521763, Validation loss: 0.04430380, Gradient norm: 0.41259948
INFO:root:[   99] Training loss: 0.04458105, Validation loss: 0.04548413, Gradient norm: 0.41004497
INFO:root:[  100] Training loss: 0.04477734, Validation loss: 0.04498516, Gradient norm: 0.46738384
INFO:root:[  101] Training loss: 0.04441119, Validation loss: 0.04389101, Gradient norm: 0.38090240
INFO:root:[  102] Training loss: 0.04420627, Validation loss: 0.04519783, Gradient norm: 0.46750654
INFO:root:[  103] Training loss: 0.04386195, Validation loss: 0.04371650, Gradient norm: 0.37431617
INFO:root:[  104] Training loss: 0.04343986, Validation loss: 0.04410308, Gradient norm: 0.41316450
INFO:root:[  105] Training loss: 0.04462815, Validation loss: 0.04371331, Gradient norm: 0.41299018
INFO:root:[  106] Training loss: 0.04384870, Validation loss: 0.04330290, Gradient norm: 0.42456955
INFO:root:[  107] Training loss: 0.04306330, Validation loss: 0.04382028, Gradient norm: 0.45885448
INFO:root:[  108] Training loss: 0.04414298, Validation loss: 0.04409481, Gradient norm: 0.46668018
INFO:root:[  109] Training loss: 0.04330523, Validation loss: 0.04326924, Gradient norm: 0.46616607
INFO:root:[  110] Training loss: 0.04335732, Validation loss: 0.04307452, Gradient norm: 0.42348760
INFO:root:[  111] Training loss: 0.04321435, Validation loss: 0.04462288, Gradient norm: 0.48199399
INFO:root:[  112] Training loss: 0.04292982, Validation loss: 0.04240873, Gradient norm: 0.34830559
INFO:root:[  113] Training loss: 0.04206520, Validation loss: 0.04272739, Gradient norm: 0.28798741
INFO:root:[  114] Training loss: 0.04299254, Validation loss: 0.04282773, Gradient norm: 0.36512611
INFO:root:[  115] Training loss: 0.04253246, Validation loss: 0.04236596, Gradient norm: 0.40096627
INFO:root:[  116] Training loss: 0.04241458, Validation loss: 0.04287219, Gradient norm: 0.31742041
INFO:root:[  117] Training loss: 0.04220347, Validation loss: 0.04251419, Gradient norm: 0.35384302
INFO:root:[  118] Training loss: 0.04273396, Validation loss: 0.04301774, Gradient norm: 0.35813351
INFO:root:[  119] Training loss: 0.04244066, Validation loss: 0.04297849, Gradient norm: 0.37663794
INFO:root:[  120] Training loss: 0.04169252, Validation loss: 0.04276523, Gradient norm: 0.38742297
INFO:root:[  121] Training loss: 0.04172737, Validation loss: 0.04219160, Gradient norm: 0.37603293
INFO:root:[  122] Training loss: 0.04231688, Validation loss: 0.04279585, Gradient norm: 0.28481204
INFO:root:[  123] Training loss: 0.04247861, Validation loss: 0.04199644, Gradient norm: 0.33014887
INFO:root:[  124] Training loss: 0.04273618, Validation loss: 0.04237004, Gradient norm: 0.32190965
INFO:root:[  125] Training loss: 0.04132602, Validation loss: 0.04210004, Gradient norm: 0.33812828
INFO:root:[  126] Training loss: 0.04157952, Validation loss: 0.04146100, Gradient norm: 0.34482664
INFO:root:[  127] Training loss: 0.04201498, Validation loss: 0.04176690, Gradient norm: 0.48027334
INFO:root:[  128] Training loss: 0.04164373, Validation loss: 0.04221910, Gradient norm: 0.35608346
INFO:root:[  129] Training loss: 0.04117315, Validation loss: 0.04264734, Gradient norm: 0.42091489
INFO:root:[  130] Training loss: 0.04135720, Validation loss: 0.04166297, Gradient norm: 0.37702845
INFO:root:[  131] Training loss: 0.04144113, Validation loss: 0.04134913, Gradient norm: 0.37216201
INFO:root:[  132] Training loss: 0.04157042, Validation loss: 0.04182137, Gradient norm: 0.42305876
INFO:root:[  133] Training loss: 0.04127087, Validation loss: 0.04289303, Gradient norm: 0.41246589
INFO:root:[  134] Training loss: 0.04122305, Validation loss: 0.04331813, Gradient norm: 0.36378988
INFO:root:[  135] Training loss: 0.04113219, Validation loss: 0.04165351, Gradient norm: 0.31646282
INFO:root:[  136] Training loss: 0.04095584, Validation loss: 0.04092835, Gradient norm: 0.40363012
INFO:root:[  137] Training loss: 0.04137281, Validation loss: 0.04123909, Gradient norm: 0.47589982
INFO:root:[  138] Training loss: 0.04073936, Validation loss: 0.04039967, Gradient norm: 0.31178880
INFO:root:[  139] Training loss: 0.04093016, Validation loss: 0.04126482, Gradient norm: 0.34965241
INFO:root:[  140] Training loss: 0.04037985, Validation loss: 0.04090336, Gradient norm: 0.33278731
INFO:root:[  141] Training loss: 0.04043147, Validation loss: 0.04032285, Gradient norm: 0.33739422
INFO:root:[  142] Training loss: 0.04026854, Validation loss: 0.04073128, Gradient norm: 0.37880322
INFO:root:[  143] Training loss: 0.04011943, Validation loss: 0.04016096, Gradient norm: 0.41122744
INFO:root:[  144] Training loss: 0.04085417, Validation loss: 0.04075215, Gradient norm: 0.33367039
INFO:root:[  145] Training loss: 0.04002334, Validation loss: 0.04112211, Gradient norm: 0.39613905
INFO:root:[  146] Training loss: 0.04026464, Validation loss: 0.04040277, Gradient norm: 0.36289609
INFO:root:[  147] Training loss: 0.03984661, Validation loss: 0.04069224, Gradient norm: 0.43337674
INFO:root:[  148] Training loss: 0.03993644, Validation loss: 0.04009792, Gradient norm: 0.32143844
INFO:root:[  149] Training loss: 0.03984608, Validation loss: 0.04005241, Gradient norm: 0.32283648
INFO:root:[  150] Training loss: 0.03983341, Validation loss: 0.03976885, Gradient norm: 0.28146070
INFO:root:[  151] Training loss: 0.03973626, Validation loss: 0.04013902, Gradient norm: 0.35669741
INFO:root:[  152] Training loss: 0.03966223, Validation loss: 0.04036717, Gradient norm: 0.28094686
INFO:root:[  153] Training loss: 0.03999933, Validation loss: 0.03968299, Gradient norm: 0.40199390
INFO:root:[  154] Training loss: 0.04018439, Validation loss: 0.04055838, Gradient norm: 0.42990626
INFO:root:[  155] Training loss: 0.03983773, Validation loss: 0.03963949, Gradient norm: 0.43794827
INFO:root:[  156] Training loss: 0.03977385, Validation loss: 0.03937206, Gradient norm: 0.38778102
INFO:root:[  157] Training loss: 0.03975606, Validation loss: 0.03962000, Gradient norm: 0.40541355
INFO:root:[  158] Training loss: 0.03915984, Validation loss: 0.03918198, Gradient norm: 0.37415511
INFO:root:[  159] Training loss: 0.03941357, Validation loss: 0.03974845, Gradient norm: 0.40488259
INFO:root:[  160] Training loss: 0.03956677, Validation loss: 0.03947373, Gradient norm: 0.35999886
INFO:root:[  161] Training loss: 0.03920281, Validation loss: 0.03890115, Gradient norm: 0.38873080
INFO:root:[  162] Training loss: 0.03920072, Validation loss: 0.03957022, Gradient norm: 0.35731664
INFO:root:[  163] Training loss: 0.04006714, Validation loss: 0.03940083, Gradient norm: 0.35369345
INFO:root:[  164] Training loss: 0.03904548, Validation loss: 0.03893977, Gradient norm: 0.31648369
INFO:root:[  165] Training loss: 0.03855016, Validation loss: 0.03896993, Gradient norm: 0.35434301
INFO:root:[  166] Training loss: 0.03904516, Validation loss: 0.03898206, Gradient norm: 0.31178974
INFO:root:[  167] Training loss: 0.03952509, Validation loss: 0.03922649, Gradient norm: 0.41292737
INFO:root:[  168] Training loss: 0.03856915, Validation loss: 0.03865120, Gradient norm: 0.37261483
INFO:root:[  169] Training loss: 0.03883980, Validation loss: 0.03871145, Gradient norm: 0.37070487
INFO:root:[  170] Training loss: 0.03881693, Validation loss: 0.03938470, Gradient norm: 0.37859281
INFO:root:[  171] Training loss: 0.03876104, Validation loss: 0.03908936, Gradient norm: 0.35197004
INFO:root:[  172] Training loss: 0.03857252, Validation loss: 0.03999714, Gradient norm: 0.37632917
INFO:root:[  173] Training loss: 0.03908183, Validation loss: 0.03847475, Gradient norm: 0.43349641
INFO:root:[  174] Training loss: 0.03852952, Validation loss: 0.04026059, Gradient norm: 0.38087879
INFO:root:[  175] Training loss: 0.03891977, Validation loss: 0.03859985, Gradient norm: 0.38714588
INFO:root:[  176] Training loss: 0.03918960, Validation loss: 0.03920958, Gradient norm: 0.45344619
INFO:root:[  177] Training loss: 0.03820342, Validation loss: 0.03889102, Gradient norm: 0.38417874
INFO:root:[  178] Training loss: 0.03892613, Validation loss: 0.03800213, Gradient norm: 0.40507974
INFO:root:[  179] Training loss: 0.03794134, Validation loss: 0.03851399, Gradient norm: 0.39843273
INFO:root:[  180] Training loss: 0.03828217, Validation loss: 0.03836194, Gradient norm: 0.38678194
INFO:root:[  181] Training loss: 0.03745879, Validation loss: 0.03871499, Gradient norm: 0.33378740
INFO:root:[  182] Training loss: 0.03793200, Validation loss: 0.03858843, Gradient norm: 0.42061723
INFO:root:[  183] Training loss: 0.03751671, Validation loss: 0.03808297, Gradient norm: 0.27409177
INFO:root:[  184] Training loss: 0.03804222, Validation loss: 0.03820302, Gradient norm: 0.39741481
INFO:root:[  185] Training loss: 0.03779370, Validation loss: 0.03822635, Gradient norm: 0.40651108
INFO:root:[  186] Training loss: 0.03762919, Validation loss: 0.03846023, Gradient norm: 0.28211697
INFO:root:[  187] Training loss: 0.03783253, Validation loss: 0.03818994, Gradient norm: 0.32958590
INFO:root:[  188] Training loss: 0.03740414, Validation loss: 0.03787964, Gradient norm: 0.29002189
INFO:root:[  189] Training loss: 0.03774809, Validation loss: 0.03780816, Gradient norm: 0.38592097
INFO:root:[  190] Training loss: 0.03794284, Validation loss: 0.03858825, Gradient norm: 0.31756083
INFO:root:[  191] Training loss: 0.03736870, Validation loss: 0.03844821, Gradient norm: 0.30648091
INFO:root:[  192] Training loss: 0.03730322, Validation loss: 0.03763574, Gradient norm: 0.33887763
INFO:root:[  193] Training loss: 0.03763665, Validation loss: 0.03934342, Gradient norm: 0.36648992
INFO:root:[  194] Training loss: 0.03750887, Validation loss: 0.03762962, Gradient norm: 0.40547114
INFO:root:[  195] Training loss: 0.03683121, Validation loss: 0.04016886, Gradient norm: 0.34318571
INFO:root:[  196] Training loss: 0.03823449, Validation loss: 0.03778195, Gradient norm: 0.40801288
INFO:root:[  197] Training loss: 0.03752961, Validation loss: 0.03770574, Gradient norm: 0.35404377
INFO:root:[  198] Training loss: 0.03780287, Validation loss: 0.03827044, Gradient norm: 0.33921928
INFO:root:[  199] Training loss: 0.03761393, Validation loss: 0.03723148, Gradient norm: 0.43086247
INFO:root:[  200] Training loss: 0.03767179, Validation loss: 0.03768237, Gradient norm: 0.38520167
INFO:root:[  201] Training loss: 0.03776068, Validation loss: 0.03800728, Gradient norm: 0.40034140
INFO:root:[  202] Training loss: 0.03774536, Validation loss: 0.03715514, Gradient norm: 0.43133339
INFO:root:[  203] Training loss: 0.03749061, Validation loss: 0.03825970, Gradient norm: 0.40186296
INFO:root:[  204] Training loss: 0.03793969, Validation loss: 0.03805423, Gradient norm: 0.36057892
INFO:root:[  205] Training loss: 0.03726958, Validation loss: 0.03936075, Gradient norm: 0.43154114
INFO:root:[  206] Training loss: 0.03708826, Validation loss: 0.03789057, Gradient norm: 0.32712265
INFO:root:[  207] Training loss: 0.03716923, Validation loss: 0.03792875, Gradient norm: 0.41349018
INFO:root:[  208] Training loss: 0.03722759, Validation loss: 0.03719444, Gradient norm: 0.36336884
INFO:root:[  209] Training loss: 0.03724180, Validation loss: 0.03755195, Gradient norm: 0.38678215
INFO:root:[  210] Training loss: 0.03660896, Validation loss: 0.03704882, Gradient norm: 0.35606473
INFO:root:[  211] Training loss: 0.03664399, Validation loss: 0.03738965, Gradient norm: 0.28585985
INFO:root:[  212] Training loss: 0.03671258, Validation loss: 0.03734981, Gradient norm: 0.35027934
INFO:root:[  213] Training loss: 0.03609492, Validation loss: 0.03807110, Gradient norm: 0.31034214
INFO:root:[  214] Training loss: 0.03705136, Validation loss: 0.03765394, Gradient norm: 0.33421245
INFO:root:[  215] Training loss: 0.03648560, Validation loss: 0.03662121, Gradient norm: 0.27254352
INFO:root:[  216] Training loss: 0.03690439, Validation loss: 0.03708763, Gradient norm: 0.26432003
INFO:root:[  217] Training loss: 0.03692389, Validation loss: 0.03727667, Gradient norm: 0.35799257
INFO:root:[  218] Training loss: 0.03693326, Validation loss: 0.03750184, Gradient norm: 0.36452413
INFO:root:[  219] Training loss: 0.03641973, Validation loss: 0.03683906, Gradient norm: 0.36454200
INFO:root:[  220] Training loss: 0.03651280, Validation loss: 0.03682847, Gradient norm: 0.32149718
INFO:root:[  221] Training loss: 0.03688990, Validation loss: 0.03661964, Gradient norm: 0.42286162
INFO:root:[  222] Training loss: 0.03748758, Validation loss: 0.03703896, Gradient norm: 0.43848887
INFO:root:[  223] Training loss: 0.03648436, Validation loss: 0.03651987, Gradient norm: 0.27262050
INFO:root:[  224] Training loss: 0.03589904, Validation loss: 0.03719118, Gradient norm: 0.34010399
INFO:root:[  225] Training loss: 0.03601263, Validation loss: 0.03746011, Gradient norm: 0.31967416
INFO:root:[  226] Training loss: 0.03618745, Validation loss: 0.03726824, Gradient norm: 0.37311269
INFO:root:[  227] Training loss: 0.03625391, Validation loss: 0.03630732, Gradient norm: 0.31904882
INFO:root:[  228] Training loss: 0.03561380, Validation loss: 0.03617823, Gradient norm: 0.27697023
INFO:root:[  229] Training loss: 0.03563800, Validation loss: 0.03651566, Gradient norm: 0.29009267
INFO:root:[  230] Training loss: 0.03643441, Validation loss: 0.03645234, Gradient norm: 0.40327446
INFO:root:[  231] Training loss: 0.03566883, Validation loss: 0.03670568, Gradient norm: 0.33226385
INFO:root:[  232] Training loss: 0.03586970, Validation loss: 0.03631289, Gradient norm: 0.31548701
INFO:root:[  233] Training loss: 0.03553793, Validation loss: 0.03691874, Gradient norm: 0.35360434
INFO:root:[  234] Training loss: 0.03549418, Validation loss: 0.03632326, Gradient norm: 0.29357551
INFO:root:[  235] Training loss: 0.03492869, Validation loss: 0.03641114, Gradient norm: 0.23729995
INFO:root:[  236] Training loss: 0.03545282, Validation loss: 0.03611764, Gradient norm: 0.30002856
INFO:root:[  237] Training loss: 0.03623922, Validation loss: 0.03623224, Gradient norm: 0.33979955
INFO:root:[  238] Training loss: 0.03573495, Validation loss: 0.03631184, Gradient norm: 0.33512019
INFO:root:[  239] Training loss: 0.03571250, Validation loss: 0.03599655, Gradient norm: 0.33251257
INFO:root:[  240] Training loss: 0.03592766, Validation loss: 0.03603286, Gradient norm: 0.34246817
INFO:root:[  241] Training loss: 0.03609147, Validation loss: 0.03640985, Gradient norm: 0.27636377
INFO:root:[  242] Training loss: 0.03584031, Validation loss: 0.03626098, Gradient norm: 0.35718143
INFO:root:[  243] Training loss: 0.03563349, Validation loss: 0.03573618, Gradient norm: 0.30074819
INFO:root:[  244] Training loss: 0.03573232, Validation loss: 0.03564055, Gradient norm: 0.41840559
INFO:root:[  245] Training loss: 0.03512652, Validation loss: 0.03591805, Gradient norm: 0.32840126
INFO:root:[  246] Training loss: 0.03484154, Validation loss: 0.03640198, Gradient norm: 0.27740783
INFO:root:[  247] Training loss: 0.03560152, Validation loss: 0.03600544, Gradient norm: 0.30690583
INFO:root:[  248] Training loss: 0.03533732, Validation loss: 0.03538925, Gradient norm: 0.35621472
INFO:root:[  249] Training loss: 0.03481587, Validation loss: 0.03556362, Gradient norm: 0.31270625
INFO:root:[  250] Training loss: 0.03546357, Validation loss: 0.03539503, Gradient norm: 0.27758880
INFO:root:[  251] Training loss: 0.03509819, Validation loss: 0.03561403, Gradient norm: 0.39686227
INFO:root:[  252] Training loss: 0.03520588, Validation loss: 0.03560784, Gradient norm: 0.38706526
INFO:root:[  253] Training loss: 0.03508106, Validation loss: 0.03605181, Gradient norm: 0.29165740
INFO:root:[  254] Training loss: 0.03478324, Validation loss: 0.03585485, Gradient norm: 0.29176416
INFO:root:[  255] Training loss: 0.03535170, Validation loss: 0.03502917, Gradient norm: 0.36117044
INFO:root:[  256] Training loss: 0.03550941, Validation loss: 0.03532101, Gradient norm: 0.40939518
INFO:root:[  257] Training loss: 0.03466310, Validation loss: 0.03527615, Gradient norm: 0.35535956
INFO:root:[  258] Training loss: 0.03471248, Validation loss: 0.03540158, Gradient norm: 0.23666778
INFO:root:[  259] Training loss: 0.03487651, Validation loss: 0.03518966, Gradient norm: 0.39226674
INFO:root:[  260] Training loss: 0.03495493, Validation loss: 0.03602810, Gradient norm: 0.30530507
INFO:root:[  261] Training loss: 0.03512819, Validation loss: 0.03497382, Gradient norm: 0.34432935
INFO:root:[  262] Training loss: 0.03504507, Validation loss: 0.03500837, Gradient norm: 0.28958867
INFO:root:[  263] Training loss: 0.03485729, Validation loss: 0.03587395, Gradient norm: 0.30036994
INFO:root:[  264] Training loss: 0.03552439, Validation loss: 0.03651998, Gradient norm: 0.45221393
INFO:root:[  265] Training loss: 0.03546486, Validation loss: 0.03463606, Gradient norm: 0.39494213
INFO:root:[  266] Training loss: 0.03508879, Validation loss: 0.03466203, Gradient norm: 0.26882507
INFO:root:[  267] Training loss: 0.03439522, Validation loss: 0.03509386, Gradient norm: 0.22173847
INFO:root:[  268] Training loss: 0.03389235, Validation loss: 0.03559709, Gradient norm: 0.32440566
INFO:root:[  269] Training loss: 0.03515155, Validation loss: 0.03622673, Gradient norm: 0.46776679
INFO:root:[  270] Training loss: 0.03430788, Validation loss: 0.03632201, Gradient norm: 0.30975783
INFO:root:[  271] Training loss: 0.03469124, Validation loss: 0.03468247, Gradient norm: 0.30617656
INFO:root:[  272] Training loss: 0.03401425, Validation loss: 0.03504962, Gradient norm: 0.29004988
INFO:root:[  273] Training loss: 0.03406420, Validation loss: 0.03551452, Gradient norm: 0.27372814
INFO:root:[  274] Training loss: 0.03493431, Validation loss: 0.03486636, Gradient norm: 0.43817712
INFO:root:EP 274: Early stopping
INFO:root:Training the model took 532.262s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.77225
INFO:root:EnergyScoreTrain: 0.57759
INFO:root:CoverageTrain: 0.89448
INFO:root:IntervalWidthTrain: 0.07984
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.80399
INFO:root:EnergyScoreValidation: 0.60041
INFO:root:CoverageValidation: 0.89326
INFO:root:IntervalWidthValidation: 0.08056
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.65358
INFO:root:EnergyScoreTest: 1.29439
INFO:root:CoverageTest: 0.63547
INFO:root:IntervalWidthTest: 0.09083
INFO:root:###21 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[0.75, 0.75], [0.67, 0.67], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [1.5, 1.5], [1.33, 1.33]], 'uno_n_modes': [[20, 20], [14, 14], [6, 6], [6, 6], [6, 6], [14, 14], [20, 20]]}
INFO:root:NumberParameters: 1964521
INFO:root:Memory allocated: 207618048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28449767, Validation loss: 0.21731889, Gradient norm: 0.91182824
INFO:root:[    2] Training loss: 0.18630717, Validation loss: 0.16502989, Gradient norm: 0.53842586
INFO:root:[    3] Training loss: 0.14740291, Validation loss: 0.13788025, Gradient norm: 0.35796599
INFO:root:[    4] Training loss: 0.12957312, Validation loss: 0.12464226, Gradient norm: 0.24300559
INFO:root:[    5] Training loss: 0.12035683, Validation loss: 0.11910303, Gradient norm: 0.18725797
INFO:root:[    6] Training loss: 0.11508198, Validation loss: 0.11496013, Gradient norm: 0.17957763
INFO:root:[    7] Training loss: 0.11182836, Validation loss: 0.11210283, Gradient norm: 0.16858911
INFO:root:[    8] Training loss: 0.11027216, Validation loss: 0.10985325, Gradient norm: 0.15568675
INFO:root:[    9] Training loss: 0.10637043, Validation loss: 0.10831518, Gradient norm: 0.15326885
INFO:root:[   10] Training loss: 0.10357217, Validation loss: 0.10614905, Gradient norm: 0.16201400
INFO:root:[   11] Training loss: 0.10317779, Validation loss: 0.10285585, Gradient norm: 0.18267390
INFO:root:[   12] Training loss: 0.09945823, Validation loss: 0.10072856, Gradient norm: 0.17933332
INFO:root:[   13] Training loss: 0.09806339, Validation loss: 0.09664378, Gradient norm: 0.22859887
INFO:root:[   14] Training loss: 0.09551297, Validation loss: 0.09334646, Gradient norm: 0.21369524
INFO:root:[   15] Training loss: 0.09112171, Validation loss: 0.09056865, Gradient norm: 0.22284385
INFO:root:[   16] Training loss: 0.08654904, Validation loss: 0.08783851, Gradient norm: 0.25484856
INFO:root:[   17] Training loss: 0.08420759, Validation loss: 0.08560289, Gradient norm: 0.24803037
INFO:root:[   18] Training loss: 0.08309949, Validation loss: 0.08429680, Gradient norm: 0.31663875
INFO:root:[   19] Training loss: 0.08098089, Validation loss: 0.08108173, Gradient norm: 0.31554389
INFO:root:[   20] Training loss: 0.07933203, Validation loss: 0.07960531, Gradient norm: 0.29015480
INFO:root:[   21] Training loss: 0.07804857, Validation loss: 0.08023483, Gradient norm: 0.23710158
INFO:root:[   22] Training loss: 0.07727849, Validation loss: 0.07771177, Gradient norm: 0.24655793
INFO:root:[   23] Training loss: 0.07576538, Validation loss: 0.07893378, Gradient norm: 0.30259020
INFO:root:[   24] Training loss: 0.07750099, Validation loss: 0.07521753, Gradient norm: 0.28376020
INFO:root:[   25] Training loss: 0.07484373, Validation loss: 0.07474846, Gradient norm: 0.29267719
INFO:root:[   26] Training loss: 0.07323674, Validation loss: 0.07448308, Gradient norm: 0.34194641
INFO:root:[   27] Training loss: 0.07136841, Validation loss: 0.07246197, Gradient norm: 0.33181238
INFO:root:[   28] Training loss: 0.07278825, Validation loss: 0.07122486, Gradient norm: 0.28191450
INFO:root:[   29] Training loss: 0.07247871, Validation loss: 0.07162048, Gradient norm: 0.31917325
INFO:root:[   30] Training loss: 0.07136415, Validation loss: 0.07131617, Gradient norm: 0.31577694
INFO:root:[   31] Training loss: 0.06979106, Validation loss: 0.07047084, Gradient norm: 0.29623896
INFO:root:[   32] Training loss: 0.06841865, Validation loss: 0.06931131, Gradient norm: 0.29486210
INFO:root:[   33] Training loss: 0.06882996, Validation loss: 0.06937066, Gradient norm: 0.34958453
INFO:root:[   34] Training loss: 0.06889766, Validation loss: 0.06778793, Gradient norm: 0.30387217
INFO:root:[   35] Training loss: 0.06741650, Validation loss: 0.06755522, Gradient norm: 0.28936375
INFO:root:[   36] Training loss: 0.06732988, Validation loss: 0.06773912, Gradient norm: 0.37923890
INFO:root:[   37] Training loss: 0.06660494, Validation loss: 0.06752510, Gradient norm: 0.41879191
INFO:root:[   38] Training loss: 0.06581193, Validation loss: 0.06651069, Gradient norm: 0.38822676
INFO:root:[   39] Training loss: 0.06618611, Validation loss: 0.06528394, Gradient norm: 0.36252821
INFO:root:[   40] Training loss: 0.06561249, Validation loss: 0.06626466, Gradient norm: 0.38648778
INFO:root:[   41] Training loss: 0.06548018, Validation loss: 0.06476095, Gradient norm: 0.41433503
INFO:root:[   42] Training loss: 0.06402819, Validation loss: 0.06658152, Gradient norm: 0.31748316
INFO:root:[   43] Training loss: 0.06421069, Validation loss: 0.06437649, Gradient norm: 0.32216373
INFO:root:[   44] Training loss: 0.06300279, Validation loss: 0.06365755, Gradient norm: 0.32328463
INFO:root:[   45] Training loss: 0.06345650, Validation loss: 0.06395992, Gradient norm: 0.29399057
INFO:root:[   46] Training loss: 0.06400535, Validation loss: 0.06260823, Gradient norm: 0.36552158
INFO:root:[   47] Training loss: 0.06275930, Validation loss: 0.06272292, Gradient norm: 0.36897029
INFO:root:[   48] Training loss: 0.06216083, Validation loss: 0.06222452, Gradient norm: 0.36605278
INFO:root:[   49] Training loss: 0.06183116, Validation loss: 0.06170997, Gradient norm: 0.39013794
INFO:root:[   50] Training loss: 0.06042282, Validation loss: 0.06204110, Gradient norm: 0.34274354
INFO:root:[   51] Training loss: 0.06188307, Validation loss: 0.06120628, Gradient norm: 0.33361595
INFO:root:[   52] Training loss: 0.06051697, Validation loss: 0.06145946, Gradient norm: 0.34510520
INFO:root:[   53] Training loss: 0.05978720, Validation loss: 0.06120684, Gradient norm: 0.39711684
INFO:root:[   54] Training loss: 0.06183990, Validation loss: 0.06036732, Gradient norm: 0.55417094
INFO:root:[   55] Training loss: 0.06026151, Validation loss: 0.06011302, Gradient norm: 0.38739932
INFO:root:[   56] Training loss: 0.05994901, Validation loss: 0.06127788, Gradient norm: 0.34727931
INFO:root:[   57] Training loss: 0.06004692, Validation loss: 0.06008929, Gradient norm: 0.39972960
INFO:root:[   58] Training loss: 0.05952867, Validation loss: 0.05949924, Gradient norm: 0.37015592
INFO:root:[   59] Training loss: 0.05814416, Validation loss: 0.05922781, Gradient norm: 0.35788759
INFO:root:[   60] Training loss: 0.05936388, Validation loss: 0.05835050, Gradient norm: 0.31775084
INFO:root:[   61] Training loss: 0.05852011, Validation loss: 0.05936952, Gradient norm: 0.43175507
INFO:root:[   62] Training loss: 0.05827598, Validation loss: 0.05943680, Gradient norm: 0.37845830
INFO:root:[   63] Training loss: 0.05804442, Validation loss: 0.06035568, Gradient norm: 0.36815159
INFO:root:[   64] Training loss: 0.05849814, Validation loss: 0.05835356, Gradient norm: 0.50145280
INFO:root:[   65] Training loss: 0.05777628, Validation loss: 0.05828318, Gradient norm: 0.36201516
INFO:root:[   66] Training loss: 0.05773449, Validation loss: 0.05750274, Gradient norm: 0.37154927
INFO:root:[   67] Training loss: 0.05758565, Validation loss: 0.05757024, Gradient norm: 0.48604665
INFO:root:[   68] Training loss: 0.05772910, Validation loss: 0.05837825, Gradient norm: 0.39678305
INFO:root:[   69] Training loss: 0.05742992, Validation loss: 0.05813915, Gradient norm: 0.40762009
INFO:root:[   70] Training loss: 0.05740597, Validation loss: 0.05722889, Gradient norm: 0.42347838
INFO:root:[   71] Training loss: 0.05670962, Validation loss: 0.05716070, Gradient norm: 0.32190555
INFO:root:[   72] Training loss: 0.05666790, Validation loss: 0.05699527, Gradient norm: 0.40031049
INFO:root:[   73] Training loss: 0.05813017, Validation loss: 0.05729048, Gradient norm: 0.61895728
INFO:root:[   74] Training loss: 0.05571292, Validation loss: 0.05628352, Gradient norm: 0.32798558
INFO:root:[   75] Training loss: 0.05646067, Validation loss: 0.05642084, Gradient norm: 0.39308521
INFO:root:[   76] Training loss: 0.05661742, Validation loss: 0.05677190, Gradient norm: 0.44802430
INFO:root:[   77] Training loss: 0.05627451, Validation loss: 0.05617762, Gradient norm: 0.43165192
INFO:root:[   78] Training loss: 0.05547635, Validation loss: 0.05606176, Gradient norm: 0.32150501
INFO:root:[   79] Training loss: 0.05569734, Validation loss: 0.05678362, Gradient norm: 0.41137694
INFO:root:[   80] Training loss: 0.05530602, Validation loss: 0.05578427, Gradient norm: 0.37408521
INFO:root:[   81] Training loss: 0.05590516, Validation loss: 0.05545222, Gradient norm: 0.31283051
INFO:root:[   82] Training loss: 0.05514786, Validation loss: 0.05669991, Gradient norm: 0.40962482
INFO:root:[   83] Training loss: 0.05557854, Validation loss: 0.05502046, Gradient norm: 0.53301351
INFO:root:[   84] Training loss: 0.05521166, Validation loss: 0.05667401, Gradient norm: 0.42183379
INFO:root:[   85] Training loss: 0.05490149, Validation loss: 0.05497179, Gradient norm: 0.47717335
INFO:root:[   86] Training loss: 0.05426436, Validation loss: 0.05588350, Gradient norm: 0.42361073
INFO:root:[   87] Training loss: 0.05519552, Validation loss: 0.05460114, Gradient norm: 0.44779780
INFO:root:[   88] Training loss: 0.05495138, Validation loss: 0.05489176, Gradient norm: 0.50671313
INFO:root:[   89] Training loss: 0.05415858, Validation loss: 0.05457791, Gradient norm: 0.37734991
INFO:root:[   90] Training loss: 0.05447983, Validation loss: 0.05455786, Gradient norm: 0.46987145
INFO:root:[   91] Training loss: 0.05472872, Validation loss: 0.05472681, Gradient norm: 0.43297084
INFO:root:[   92] Training loss: 0.05431672, Validation loss: 0.05411909, Gradient norm: 0.47354126
INFO:root:[   93] Training loss: 0.05279802, Validation loss: 0.05458933, Gradient norm: 0.40644261
INFO:root:[   94] Training loss: 0.05433729, Validation loss: 0.05420434, Gradient norm: 0.47444511
INFO:root:[   95] Training loss: 0.05329040, Validation loss: 0.05374212, Gradient norm: 0.39705112
INFO:root:[   96] Training loss: 0.05340997, Validation loss: 0.05367313, Gradient norm: 0.40582716
INFO:root:[   97] Training loss: 0.05274014, Validation loss: 0.05522311, Gradient norm: 0.34805466
INFO:root:[   98] Training loss: 0.05291590, Validation loss: 0.05459774, Gradient norm: 0.42922628
INFO:root:[   99] Training loss: 0.05485914, Validation loss: 0.05409227, Gradient norm: 0.55799665
INFO:root:[  100] Training loss: 0.05265540, Validation loss: 0.05316234, Gradient norm: 0.36800076
INFO:root:[  101] Training loss: 0.05325537, Validation loss: 0.05240191, Gradient norm: 0.54151043
INFO:root:[  102] Training loss: 0.05332522, Validation loss: 0.05383337, Gradient norm: 0.46904636
INFO:root:[  103] Training loss: 0.05250261, Validation loss: 0.05327417, Gradient norm: 0.40002447
INFO:root:[  104] Training loss: 0.05289436, Validation loss: 0.05383507, Gradient norm: 0.42573578
INFO:root:[  105] Training loss: 0.05289900, Validation loss: 0.05405924, Gradient norm: 0.37296372
INFO:root:[  106] Training loss: 0.05233311, Validation loss: 0.05264100, Gradient norm: 0.48281310
INFO:root:[  107] Training loss: 0.05201278, Validation loss: 0.05217024, Gradient norm: 0.40858036
INFO:root:[  108] Training loss: 0.05200328, Validation loss: 0.05254873, Gradient norm: 0.35530403
INFO:root:[  109] Training loss: 0.05254505, Validation loss: 0.05338245, Gradient norm: 0.55560919
INFO:root:[  110] Training loss: 0.05170464, Validation loss: 0.05247364, Gradient norm: 0.39490567
INFO:root:[  111] Training loss: 0.05187884, Validation loss: 0.05363233, Gradient norm: 0.39339618
INFO:root:[  112] Training loss: 0.05224594, Validation loss: 0.05205727, Gradient norm: 0.49264419
INFO:root:[  113] Training loss: 0.05102402, Validation loss: 0.05439156, Gradient norm: 0.43491561
INFO:root:[  114] Training loss: 0.05164344, Validation loss: 0.05175336, Gradient norm: 0.50293844
INFO:root:[  115] Training loss: 0.05143885, Validation loss: 0.05163554, Gradient norm: 0.43350951
INFO:root:[  116] Training loss: 0.05133502, Validation loss: 0.05158448, Gradient norm: 0.42601228
INFO:root:[  117] Training loss: 0.05162349, Validation loss: 0.05137319, Gradient norm: 0.44892566
INFO:root:[  118] Training loss: 0.05307626, Validation loss: 0.05461495, Gradient norm: 0.50111577
INFO:root:[  119] Training loss: 0.05097405, Validation loss: 0.05170434, Gradient norm: 0.48594477
INFO:root:[  120] Training loss: 0.05115842, Validation loss: 0.05184508, Gradient norm: 0.37823436
INFO:root:[  121] Training loss: 0.05073243, Validation loss: 0.05127795, Gradient norm: 0.36363981
INFO:root:[  122] Training loss: 0.05157551, Validation loss: 0.05111832, Gradient norm: 0.41237450
INFO:root:[  123] Training loss: 0.05147755, Validation loss: 0.05107080, Gradient norm: 0.38052050
INFO:root:[  124] Training loss: 0.05088622, Validation loss: 0.05148729, Gradient norm: 0.31423891
INFO:root:[  125] Training loss: 0.05191127, Validation loss: 0.05174367, Gradient norm: 0.49331344
INFO:root:[  126] Training loss: 0.05097265, Validation loss: 0.05110385, Gradient norm: 0.48962867
INFO:root:[  127] Training loss: 0.05076384, Validation loss: 0.05105868, Gradient norm: 0.41418649
INFO:root:[  128] Training loss: 0.05019575, Validation loss: 0.05068269, Gradient norm: 0.30914466
INFO:root:[  129] Training loss: 0.05019610, Validation loss: 0.05121213, Gradient norm: 0.47729007
INFO:root:[  130] Training loss: 0.04972883, Validation loss: 0.05064922, Gradient norm: 0.40723758
INFO:root:[  131] Training loss: 0.05073780, Validation loss: 0.05098964, Gradient norm: 0.46857586
INFO:root:[  132] Training loss: 0.05089878, Validation loss: 0.05035285, Gradient norm: 0.41776452
INFO:root:[  133] Training loss: 0.05163526, Validation loss: 0.05063327, Gradient norm: 0.44284612
INFO:root:[  134] Training loss: 0.04945546, Validation loss: 0.05052454, Gradient norm: 0.32079168
INFO:root:[  135] Training loss: 0.04950927, Validation loss: 0.05108870, Gradient norm: 0.41338910
INFO:root:[  136] Training loss: 0.04969807, Validation loss: 0.05016832, Gradient norm: 0.35874546
INFO:root:[  137] Training loss: 0.04942162, Validation loss: 0.05055097, Gradient norm: 0.33891691
INFO:root:[  138] Training loss: 0.05051807, Validation loss: 0.05081596, Gradient norm: 0.39370114
INFO:root:[  139] Training loss: 0.04982784, Validation loss: 0.05011405, Gradient norm: 0.38071235
INFO:root:[  140] Training loss: 0.05063078, Validation loss: 0.05011957, Gradient norm: 0.50046027
INFO:root:[  141] Training loss: 0.04956783, Validation loss: 0.05025642, Gradient norm: 0.30143152
INFO:root:[  142] Training loss: 0.05130400, Validation loss: 0.05050423, Gradient norm: 0.59036763
INFO:root:[  143] Training loss: 0.04927808, Validation loss: 0.04940489, Gradient norm: 0.43522512
INFO:root:[  144] Training loss: 0.04885764, Validation loss: 0.05000629, Gradient norm: 0.33503296
INFO:root:[  145] Training loss: 0.05007700, Validation loss: 0.04953977, Gradient norm: 0.38944630
INFO:root:[  146] Training loss: 0.04949932, Validation loss: 0.04962374, Gradient norm: 0.43231725
INFO:root:[  147] Training loss: 0.04861484, Validation loss: 0.04970689, Gradient norm: 0.32373545
INFO:root:[  148] Training loss: 0.04864318, Validation loss: 0.04906485, Gradient norm: 0.26403089
INFO:root:[  149] Training loss: 0.04930952, Validation loss: 0.05061208, Gradient norm: 0.41761577
INFO:root:[  150] Training loss: 0.04871972, Validation loss: 0.04988348, Gradient norm: 0.32301108
INFO:root:[  151] Training loss: 0.04930530, Validation loss: 0.04956766, Gradient norm: 0.38119886
INFO:root:[  152] Training loss: 0.04920196, Validation loss: 0.04970066, Gradient norm: 0.33173237
INFO:root:[  153] Training loss: 0.04860795, Validation loss: 0.04940416, Gradient norm: 0.34506460
INFO:root:[  154] Training loss: 0.04931318, Validation loss: 0.04906109, Gradient norm: 0.33823557
INFO:root:[  155] Training loss: 0.04898290, Validation loss: 0.04941822, Gradient norm: 0.34138010
INFO:root:[  156] Training loss: 0.04908296, Validation loss: 0.05011185, Gradient norm: 0.46251932
INFO:root:[  157] Training loss: 0.04877584, Validation loss: 0.04955471, Gradient norm: 0.47744735
INFO:root:[  158] Training loss: 0.04904814, Validation loss: 0.04937379, Gradient norm: 0.34366707
INFO:root:[  159] Training loss: 0.04839933, Validation loss: 0.04888425, Gradient norm: 0.38216994
INFO:root:[  160] Training loss: 0.04870655, Validation loss: 0.04973924, Gradient norm: 0.42581771
INFO:root:[  161] Training loss: 0.04819783, Validation loss: 0.05131874, Gradient norm: 0.42683361
INFO:root:[  162] Training loss: 0.04850557, Validation loss: 0.04945172, Gradient norm: 0.40748776
INFO:root:[  163] Training loss: 0.04856886, Validation loss: 0.04875531, Gradient norm: 0.33095083
INFO:root:[  164] Training loss: 0.04888421, Validation loss: 0.04904984, Gradient norm: 0.51995346
INFO:root:[  165] Training loss: 0.04891894, Validation loss: 0.04951352, Gradient norm: 0.45281208
INFO:root:[  166] Training loss: 0.04847532, Validation loss: 0.04820092, Gradient norm: 0.36170967
INFO:root:[  167] Training loss: 0.04859395, Validation loss: 0.04834423, Gradient norm: 0.49943411
INFO:root:[  168] Training loss: 0.04773542, Validation loss: 0.04857389, Gradient norm: 0.31688929
INFO:root:[  169] Training loss: 0.04769706, Validation loss: 0.04948266, Gradient norm: 0.35704503
INFO:root:[  170] Training loss: 0.04746406, Validation loss: 0.04866099, Gradient norm: 0.37619838
INFO:root:[  171] Training loss: 0.04761785, Validation loss: 0.04746805, Gradient norm: 0.30706630
INFO:root:[  172] Training loss: 0.04748129, Validation loss: 0.04781888, Gradient norm: 0.36930479
INFO:root:[  173] Training loss: 0.04805006, Validation loss: 0.04736571, Gradient norm: 0.31832926
INFO:root:[  174] Training loss: 0.04723981, Validation loss: 0.04809183, Gradient norm: 0.38295167
INFO:root:[  175] Training loss: 0.04713480, Validation loss: 0.04725563, Gradient norm: 0.34415634
INFO:root:[  176] Training loss: 0.04774340, Validation loss: 0.04836848, Gradient norm: 0.51595614
INFO:root:[  177] Training loss: 0.04788585, Validation loss: 0.04783548, Gradient norm: 0.41788936
INFO:root:[  178] Training loss: 0.04673259, Validation loss: 0.05002812, Gradient norm: 0.33466604
INFO:root:[  179] Training loss: 0.04771559, Validation loss: 0.04766965, Gradient norm: 0.38760822
INFO:root:[  180] Training loss: 0.04780835, Validation loss: 0.04745533, Gradient norm: 0.46637778
INFO:root:[  181] Training loss: 0.04756739, Validation loss: 0.04719645, Gradient norm: 0.38164497
INFO:root:[  182] Training loss: 0.04811625, Validation loss: 0.04757227, Gradient norm: 0.51270853
INFO:root:[  183] Training loss: 0.04714602, Validation loss: 0.04799153, Gradient norm: 0.38819742
INFO:root:[  184] Training loss: 0.04767519, Validation loss: 0.04778357, Gradient norm: 0.49719291
INFO:root:[  185] Training loss: 0.04699913, Validation loss: 0.04726236, Gradient norm: 0.36834708
INFO:root:[  186] Training loss: 0.04718067, Validation loss: 0.04711201, Gradient norm: 0.38593263
INFO:root:[  187] Training loss: 0.04669787, Validation loss: 0.04824788, Gradient norm: 0.34122248
INFO:root:[  188] Training loss: 0.04749261, Validation loss: 0.04718827, Gradient norm: 0.39893373
INFO:root:[  189] Training loss: 0.04662593, Validation loss: 0.04732929, Gradient norm: 0.28027562
INFO:root:[  190] Training loss: 0.04684260, Validation loss: 0.04716485, Gradient norm: 0.43812246
INFO:root:[  191] Training loss: 0.04752941, Validation loss: 0.04727010, Gradient norm: 0.47436443
INFO:root:[  192] Training loss: 0.04653887, Validation loss: 0.04727822, Gradient norm: 0.38555610
INFO:root:[  193] Training loss: 0.04727833, Validation loss: 0.04742867, Gradient norm: 0.37706958
INFO:root:[  194] Training loss: 0.04664494, Validation loss: 0.04709545, Gradient norm: 0.38551766
INFO:root:[  195] Training loss: 0.04644803, Validation loss: 0.04680969, Gradient norm: 0.35186544
INFO:root:[  196] Training loss: 0.04656680, Validation loss: 0.04671731, Gradient norm: 0.27394742
INFO:root:[  197] Training loss: 0.04650751, Validation loss: 0.04664578, Gradient norm: 0.46881199
INFO:root:[  198] Training loss: 0.04618948, Validation loss: 0.04666883, Gradient norm: 0.29893070
INFO:root:[  199] Training loss: 0.04596744, Validation loss: 0.04714660, Gradient norm: 0.29364214
INFO:root:[  200] Training loss: 0.04602167, Validation loss: 0.04653219, Gradient norm: 0.36831339
INFO:root:[  201] Training loss: 0.04642852, Validation loss: 0.04725535, Gradient norm: 0.40246838
INFO:root:[  202] Training loss: 0.04618666, Validation loss: 0.04700703, Gradient norm: 0.38462666
INFO:root:[  203] Training loss: 0.04625906, Validation loss: 0.04729528, Gradient norm: 0.37371811
INFO:root:[  204] Training loss: 0.04614596, Validation loss: 0.04701956, Gradient norm: 0.32987310
INFO:root:[  205] Training loss: 0.04626888, Validation loss: 0.04652875, Gradient norm: 0.35406663
INFO:root:[  206] Training loss: 0.04644730, Validation loss: 0.04684184, Gradient norm: 0.35473327
INFO:root:[  207] Training loss: 0.04692548, Validation loss: 0.04856995, Gradient norm: 0.44345830
INFO:root:[  208] Training loss: 0.04646884, Validation loss: 0.04791257, Gradient norm: 0.48523610
INFO:root:[  209] Training loss: 0.04546199, Validation loss: 0.04813502, Gradient norm: 0.45191995
INFO:root:[  210] Training loss: 0.04600770, Validation loss: 0.04764954, Gradient norm: 0.45359964
INFO:root:[  211] Training loss: 0.04668687, Validation loss: 0.04662443, Gradient norm: 0.41906629
INFO:root:[  212] Training loss: 0.04709946, Validation loss: 0.04598770, Gradient norm: 0.50908886
INFO:root:[  213] Training loss: 0.04612759, Validation loss: 0.04674200, Gradient norm: 0.39933998
INFO:root:[  214] Training loss: 0.04600686, Validation loss: 0.04650965, Gradient norm: 0.44381017
INFO:root:[  215] Training loss: 0.04539641, Validation loss: 0.04628423, Gradient norm: 0.36805360
INFO:root:[  216] Training loss: 0.04643987, Validation loss: 0.04680685, Gradient norm: 0.37716596
INFO:root:[  217] Training loss: 0.04570716, Validation loss: 0.04575067, Gradient norm: 0.36214517
INFO:root:[  218] Training loss: 0.04557528, Validation loss: 0.04661145, Gradient norm: 0.39925130
INFO:root:[  219] Training loss: 0.04559219, Validation loss: 0.04569659, Gradient norm: 0.45419609
INFO:root:[  220] Training loss: 0.04547387, Validation loss: 0.04578684, Gradient norm: 0.39241009
INFO:root:[  221] Training loss: 0.04610410, Validation loss: 0.04543827, Gradient norm: 0.42139961
INFO:root:[  222] Training loss: 0.04583316, Validation loss: 0.04624281, Gradient norm: 0.26393521
INFO:root:[  223] Training loss: 0.04573170, Validation loss: 0.04598290, Gradient norm: 0.43228651
INFO:root:[  224] Training loss: 0.04616784, Validation loss: 0.04621591, Gradient norm: 0.41481364
INFO:root:[  225] Training loss: 0.04636027, Validation loss: 0.04566146, Gradient norm: 0.33191526
INFO:root:[  226] Training loss: 0.04502085, Validation loss: 0.04622332, Gradient norm: 0.30505967
INFO:root:[  227] Training loss: 0.04521997, Validation loss: 0.04574491, Gradient norm: 0.36579339
INFO:root:[  228] Training loss: 0.04571396, Validation loss: 0.04571814, Gradient norm: 0.35722622
INFO:root:[  229] Training loss: 0.04543186, Validation loss: 0.04533638, Gradient norm: 0.32680279
INFO:root:[  230] Training loss: 0.04504157, Validation loss: 0.04527401, Gradient norm: 0.30531782
INFO:root:[  231] Training loss: 0.04534595, Validation loss: 0.04543633, Gradient norm: 0.36187157
INFO:root:[  232] Training loss: 0.04531289, Validation loss: 0.04675925, Gradient norm: 0.31523387
INFO:root:[  233] Training loss: 0.04629038, Validation loss: 0.04550229, Gradient norm: 0.49584503
INFO:root:[  234] Training loss: 0.04501236, Validation loss: 0.04614000, Gradient norm: 0.39223220
INFO:root:[  235] Training loss: 0.04470964, Validation loss: 0.04869491, Gradient norm: 0.37981431
INFO:root:[  236] Training loss: 0.04568417, Validation loss: 0.04510549, Gradient norm: 0.41611342
INFO:root:[  237] Training loss: 0.04493093, Validation loss: 0.04539572, Gradient norm: 0.35389648
INFO:root:[  238] Training loss: 0.04533698, Validation loss: 0.04551146, Gradient norm: 0.36653348
INFO:root:[  239] Training loss: 0.04552121, Validation loss: 0.04630165, Gradient norm: 0.40026393
INFO:root:[  240] Training loss: 0.04495679, Validation loss: 0.04535954, Gradient norm: 0.36797944
INFO:root:[  241] Training loss: 0.04550006, Validation loss: 0.04741520, Gradient norm: 0.39312234
INFO:root:[  242] Training loss: 0.04496997, Validation loss: 0.04562026, Gradient norm: 0.44450462
INFO:root:[  243] Training loss: 0.04553926, Validation loss: 0.04497229, Gradient norm: 0.46228940
INFO:root:[  244] Training loss: 0.04456759, Validation loss: 0.04496641, Gradient norm: 0.35122559
INFO:root:[  245] Training loss: 0.04385494, Validation loss: 0.04567182, Gradient norm: 0.38312875
INFO:root:[  246] Training loss: 0.04433018, Validation loss: 0.04646411, Gradient norm: 0.41078436
INFO:root:[  247] Training loss: 0.04504425, Validation loss: 0.04503499, Gradient norm: 0.37065084
INFO:root:[  248] Training loss: 0.04490300, Validation loss: 0.04467156, Gradient norm: 0.31123718
INFO:root:[  249] Training loss: 0.04480065, Validation loss: 0.04567877, Gradient norm: 0.40734475
INFO:root:[  250] Training loss: 0.04573185, Validation loss: 0.04539416, Gradient norm: 0.36926201
INFO:root:[  251] Training loss: 0.04487911, Validation loss: 0.04480749, Gradient norm: 0.32217827
INFO:root:[  252] Training loss: 0.04502347, Validation loss: 0.04483620, Gradient norm: 0.37455996
INFO:root:[  253] Training loss: 0.04420816, Validation loss: 0.04464810, Gradient norm: 0.29993924
INFO:root:[  254] Training loss: 0.04513555, Validation loss: 0.04569974, Gradient norm: 0.37735887
INFO:root:[  255] Training loss: 0.04524681, Validation loss: 0.04466873, Gradient norm: 0.36922192
INFO:root:[  256] Training loss: 0.04450373, Validation loss: 0.04456276, Gradient norm: 0.35809735
INFO:root:[  257] Training loss: 0.04419457, Validation loss: 0.04460669, Gradient norm: 0.34917919
INFO:root:[  258] Training loss: 0.04405215, Validation loss: 0.04460873, Gradient norm: 0.38465896
INFO:root:[  259] Training loss: 0.04391689, Validation loss: 0.04444666, Gradient norm: 0.39409847
INFO:root:[  260] Training loss: 0.04465433, Validation loss: 0.04420015, Gradient norm: 0.37271344
INFO:root:[  261] Training loss: 0.04412331, Validation loss: 0.04489471, Gradient norm: 0.35831455
INFO:root:[  262] Training loss: 0.04499524, Validation loss: 0.04469973, Gradient norm: 0.34517090
INFO:root:[  263] Training loss: 0.04515967, Validation loss: 0.04468676, Gradient norm: 0.52991039
INFO:root:[  264] Training loss: 0.04409737, Validation loss: 0.04451079, Gradient norm: 0.32148416
INFO:root:[  265] Training loss: 0.04382462, Validation loss: 0.04425818, Gradient norm: 0.28582169
INFO:root:[  266] Training loss: 0.04372921, Validation loss: 0.04434836, Gradient norm: 0.35702225
INFO:root:[  267] Training loss: 0.04391263, Validation loss: 0.04571278, Gradient norm: 0.32878559
INFO:root:[  268] Training loss: 0.04363421, Validation loss: 0.04448857, Gradient norm: 0.37920030
INFO:root:[  269] Training loss: 0.04391705, Validation loss: 0.04474211, Gradient norm: 0.42537412
INFO:root:EP 269: Early stopping
INFO:root:Training the model took 521.791s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.95807
INFO:root:EnergyScoreTrain: 0.72637
INFO:root:CoverageTrain: 0.86303
INFO:root:IntervalWidthTrain: 0.10208
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.98038
INFO:root:EnergyScoreValidation: 0.74381
INFO:root:CoverageValidation: 0.86625
INFO:root:IntervalWidthValidation: 0.10326
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.62919
INFO:root:EnergyScoreTest: 1.21108
INFO:root:CoverageTest: 0.82363
INFO:root:IntervalWidthTest: 0.14402
