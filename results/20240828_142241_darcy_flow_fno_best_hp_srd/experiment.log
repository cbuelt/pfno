INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno_srd.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.19270888, Validation loss: 0.17491065, Gradient norm: 3.39807477
INFO:root:[    2] Training loss: 0.08673121, Validation loss: 0.16996156, Gradient norm: 2.83117480
INFO:root:[    3] Training loss: 0.08162941, Validation loss: 0.12878617, Gradient norm: 3.31780942
INFO:root:[    4] Training loss: 0.07097109, Validation loss: 0.11157009, Gradient norm: 2.99666270
INFO:root:[    5] Training loss: 0.06085456, Validation loss: 0.15614265, Gradient norm: 2.39912566
INFO:root:[    6] Training loss: 0.05957464, Validation loss: 0.11026201, Gradient norm: 2.66029578
INFO:root:[    7] Training loss: 0.05849369, Validation loss: 0.12169206, Gradient norm: 2.85131165
INFO:root:[    8] Training loss: 0.05295938, Validation loss: 0.09665089, Gradient norm: 2.39617738
INFO:root:[    9] Training loss: 0.05560697, Validation loss: 0.09158127, Gradient norm: 2.66128875
INFO:root:[   10] Training loss: 0.05184157, Validation loss: 0.08810855, Gradient norm: 2.43798396
INFO:root:[   11] Training loss: 0.05277200, Validation loss: 0.08836945, Gradient norm: 2.48786130
INFO:root:[   12] Training loss: 0.04842178, Validation loss: 0.09492768, Gradient norm: 2.09853422
INFO:root:[   13] Training loss: 0.04700625, Validation loss: 0.08611061, Gradient norm: 2.05900575
INFO:root:[   14] Training loss: 0.04541791, Validation loss: 0.09268674, Gradient norm: 2.20748391
INFO:root:[   15] Training loss: 0.04572283, Validation loss: 0.07848159, Gradient norm: 2.24590809
INFO:root:[   16] Training loss: 0.04272224, Validation loss: 0.07956303, Gradient norm: 1.94793331
INFO:root:[   17] Training loss: 0.04575445, Validation loss: 0.07059423, Gradient norm: 2.21580373
INFO:root:[   18] Training loss: 0.04971784, Validation loss: 0.07181339, Gradient norm: 2.51701399
INFO:root:[   19] Training loss: 0.04210046, Validation loss: 0.07344501, Gradient norm: 2.01343250
INFO:root:[   20] Training loss: 0.04146776, Validation loss: 0.07394835, Gradient norm: 1.98938856
INFO:root:[   21] Training loss: 0.04263763, Validation loss: 0.06844256, Gradient norm: 2.19037145
INFO:root:[   22] Training loss: 0.04179458, Validation loss: 0.07313022, Gradient norm: 2.08910621
INFO:root:[   23] Training loss: 0.04305181, Validation loss: 0.07025683, Gradient norm: 2.24188534
INFO:root:[   24] Training loss: 0.04414292, Validation loss: 0.06854733, Gradient norm: 2.24300931
INFO:root:[   25] Training loss: 0.04093796, Validation loss: 0.07382960, Gradient norm: 2.03231485
INFO:root:[   26] Training loss: 0.04039793, Validation loss: 0.06375701, Gradient norm: 2.14725971
INFO:root:[   27] Training loss: 0.03810887, Validation loss: 0.06396728, Gradient norm: 1.63957872
INFO:root:[   28] Training loss: 0.04017862, Validation loss: 0.07827140, Gradient norm: 2.09264771
INFO:root:[   29] Training loss: 0.03645222, Validation loss: 0.06705670, Gradient norm: 1.61161703
INFO:root:[   30] Training loss: 0.03766543, Validation loss: 0.06925088, Gradient norm: 1.87581905
INFO:root:[   31] Training loss: 0.03782176, Validation loss: 0.06010905, Gradient norm: 1.79172224
INFO:root:[   32] Training loss: 0.03675920, Validation loss: 0.06338297, Gradient norm: 1.93258223
INFO:root:[   33] Training loss: 0.03656864, Validation loss: 0.06962632, Gradient norm: 1.72957227
INFO:root:[   34] Training loss: 0.03775613, Validation loss: 0.06664110, Gradient norm: 1.77649214
INFO:root:[   35] Training loss: 0.03404884, Validation loss: 0.07198575, Gradient norm: 1.54582146
INFO:root:[   36] Training loss: 0.03648299, Validation loss: 0.06551015, Gradient norm: 1.66952371
INFO:root:[   37] Training loss: 0.03572047, Validation loss: 0.06878009, Gradient norm: 1.86783307
INFO:root:[   38] Training loss: 0.03252664, Validation loss: 0.06568096, Gradient norm: 1.55688049
INFO:root:[   39] Training loss: 0.03421044, Validation loss: 0.07575264, Gradient norm: 1.65215585
INFO:root:[   40] Training loss: 0.03361327, Validation loss: 0.06520226, Gradient norm: 1.79103409
INFO:root:[   41] Training loss: 0.03443595, Validation loss: 0.05914485, Gradient norm: 1.59269743
INFO:root:[   42] Training loss: 0.03195176, Validation loss: 0.06107323, Gradient norm: 1.41407347
INFO:root:[   43] Training loss: 0.03250091, Validation loss: 0.06515008, Gradient norm: 1.72411668
INFO:root:[   44] Training loss: 0.03221713, Validation loss: 0.06493177, Gradient norm: 1.68849751
INFO:root:[   45] Training loss: 0.03303556, Validation loss: 0.07023501, Gradient norm: 1.72524138
INFO:root:[   46] Training loss: 0.03064583, Validation loss: 0.07800640, Gradient norm: 1.56685135
INFO:root:[   47] Training loss: 0.03175481, Validation loss: 0.07233335, Gradient norm: 1.68556126
INFO:root:[   48] Training loss: 0.03016861, Validation loss: 0.06494440, Gradient norm: 1.47129043
INFO:root:[   49] Training loss: 0.03069359, Validation loss: 0.06113331, Gradient norm: 1.56214867
INFO:root:[   50] Training loss: 0.03332809, Validation loss: 0.05934094, Gradient norm: 1.84301288
INFO:root:[   51] Training loss: 0.03099979, Validation loss: 0.06305713, Gradient norm: 1.81458684
INFO:root:[   52] Training loss: 0.03200846, Validation loss: 0.06023683, Gradient norm: 1.67040750
INFO:root:[   53] Training loss: 0.03051501, Validation loss: 0.06563711, Gradient norm: 1.88111054
INFO:root:[   54] Training loss: 0.02924084, Validation loss: 0.06265979, Gradient norm: 1.57790392
INFO:root:[   55] Training loss: 0.02972912, Validation loss: 0.06332996, Gradient norm: 1.53980521
INFO:root:[   56] Training loss: 0.02924504, Validation loss: 0.06825763, Gradient norm: 1.61720486
INFO:root:[   57] Training loss: 0.02955436, Validation loss: 0.06299963, Gradient norm: 1.56623418
INFO:root:[   58] Training loss: 0.02796347, Validation loss: 0.06344391, Gradient norm: 1.39542690
INFO:root:[   59] Training loss: 0.02818314, Validation loss: 0.07582410, Gradient norm: 1.50515697
INFO:root:[   60] Training loss: 0.02826663, Validation loss: 0.06815245, Gradient norm: 1.35475555
INFO:root:[   61] Training loss: 0.02783029, Validation loss: 0.06095290, Gradient norm: 1.48148383
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 2788.427s.
INFO:root:Emptying the cuda cache took 0.053s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03639
INFO:root:EnergyScoreTrain: 0.02862
INFO:root:CoverageTrain: 0.98539
INFO:root:IntervalWidthTrain: 0.04473
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08143
INFO:root:EnergyScoreValidation: 0.05857
INFO:root:CoverageValidation: 0.85083
INFO:root:IntervalWidthValidation: 0.0467
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08281
INFO:root:EnergyScoreTest: 0.05973
INFO:root:CoverageTest: 0.84401
INFO:root:IntervalWidthTest: 0.04674
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.19844366, Validation loss: 0.17764352, Gradient norm: 3.65112631
INFO:root:[    2] Training loss: 0.09205175, Validation loss: 0.13998320, Gradient norm: 3.54884623
INFO:root:[    3] Training loss: 0.07638957, Validation loss: 0.10789796, Gradient norm: 3.03083852
INFO:root:[    4] Training loss: 0.06778794, Validation loss: 0.10648615, Gradient norm: 2.90725786
INFO:root:[    5] Training loss: 0.06771916, Validation loss: 0.10835901, Gradient norm: 3.11219737
INFO:root:[    6] Training loss: 0.06229830, Validation loss: 0.08203475, Gradient norm: 2.92668381
INFO:root:[    7] Training loss: 0.05658175, Validation loss: 0.08830459, Gradient norm: 2.41987448
INFO:root:[    8] Training loss: 0.05503487, Validation loss: 0.08522754, Gradient norm: 2.54046930
INFO:root:[    9] Training loss: 0.05581625, Validation loss: 0.08902686, Gradient norm: 2.86966513
INFO:root:[   10] Training loss: 0.05360684, Validation loss: 0.07870759, Gradient norm: 2.56553755
INFO:root:[   11] Training loss: 0.05017891, Validation loss: 0.07648404, Gradient norm: 2.50474598
INFO:root:[   12] Training loss: 0.04947950, Validation loss: 0.07936182, Gradient norm: 2.20627495
INFO:root:[   13] Training loss: 0.04689252, Validation loss: 0.08064445, Gradient norm: 2.31316680
INFO:root:[   14] Training loss: 0.04638538, Validation loss: 0.06941985, Gradient norm: 2.30482209
INFO:root:[   15] Training loss: 0.04207984, Validation loss: 0.07368318, Gradient norm: 1.78293028
INFO:root:[   16] Training loss: 0.04276899, Validation loss: 0.05930348, Gradient norm: 2.19643720
INFO:root:[   17] Training loss: 0.04316174, Validation loss: 0.05906010, Gradient norm: 2.21118417
INFO:root:[   18] Training loss: 0.04220826, Validation loss: 0.07554554, Gradient norm: 2.08981823
INFO:root:[   19] Training loss: 0.04307523, Validation loss: 0.06974798, Gradient norm: 2.32370205
INFO:root:[   20] Training loss: 0.04248019, Validation loss: 0.05793237, Gradient norm: 2.37142126
INFO:root:[   21] Training loss: 0.03942224, Validation loss: 0.06584603, Gradient norm: 1.86893891
INFO:root:[   22] Training loss: 0.04169791, Validation loss: 0.06233422, Gradient norm: 2.03834837
INFO:root:[   23] Training loss: 0.03871403, Validation loss: 0.05420078, Gradient norm: 1.90989429
INFO:root:[   24] Training loss: 0.03979533, Validation loss: 0.06857855, Gradient norm: 2.13709066
INFO:root:[   25] Training loss: 0.04073123, Validation loss: 0.06851928, Gradient norm: 2.13660341
INFO:root:[   26] Training loss: 0.03811204, Validation loss: 0.05195102, Gradient norm: 1.88228458
INFO:root:[   27] Training loss: 0.03686348, Validation loss: 0.07122090, Gradient norm: 1.84555457
INFO:root:[   28] Training loss: 0.03737306, Validation loss: 0.05685593, Gradient norm: 1.88052353
INFO:root:[   29] Training loss: 0.03730252, Validation loss: 0.05163039, Gradient norm: 1.83592630
INFO:root:[   30] Training loss: 0.03522220, Validation loss: 0.07634494, Gradient norm: 1.61215879
INFO:root:[   31] Training loss: 0.03544281, Validation loss: 0.06342794, Gradient norm: 1.82045328
INFO:root:[   32] Training loss: 0.03431720, Validation loss: 0.07782124, Gradient norm: 1.65737549
INFO:root:[   33] Training loss: 0.03406073, Validation loss: 0.06086360, Gradient norm: 1.59932320
INFO:root:[   34] Training loss: 0.03457141, Validation loss: 0.05239314, Gradient norm: 1.86234717
INFO:root:[   35] Training loss: 0.03498180, Validation loss: 0.04640630, Gradient norm: 1.90104222
INFO:root:[   36] Training loss: 0.03358566, Validation loss: 0.05115514, Gradient norm: 1.67345262
INFO:root:[   37] Training loss: 0.03331013, Validation loss: 0.05909584, Gradient norm: 1.69036403
INFO:root:[   38] Training loss: 0.03469518, Validation loss: 0.05477859, Gradient norm: 1.85839507
INFO:root:[   39] Training loss: 0.03366114, Validation loss: 0.05148264, Gradient norm: 1.70401803
INFO:root:[   40] Training loss: 0.03156824, Validation loss: 0.04869424, Gradient norm: 1.79869699
INFO:root:[   41] Training loss: 0.03133142, Validation loss: 0.05098554, Gradient norm: 1.53837277
INFO:root:[   42] Training loss: 0.03332692, Validation loss: 0.05529228, Gradient norm: 1.82845965
INFO:root:[   43] Training loss: 0.03041740, Validation loss: 0.06513535, Gradient norm: 1.57606269
INFO:root:[   44] Training loss: 0.03256505, Validation loss: 0.06231963, Gradient norm: 1.78919056
INFO:root:[   45] Training loss: 0.03105944, Validation loss: 0.05164341, Gradient norm: 1.62184780
INFO:root:[   46] Training loss: 0.03167292, Validation loss: 0.06434602, Gradient norm: 1.75703317
INFO:root:[   47] Training loss: 0.03200410, Validation loss: 0.06280901, Gradient norm: 1.56424659
INFO:root:[   48] Training loss: 0.03010012, Validation loss: 0.06200974, Gradient norm: 1.62457434
INFO:root:[   49] Training loss: 0.03001864, Validation loss: 0.04422425, Gradient norm: 1.60062350
INFO:root:[   50] Training loss: 0.03017934, Validation loss: 0.05187096, Gradient norm: 1.72249007
INFO:root:[   51] Training loss: 0.03072211, Validation loss: 0.04600425, Gradient norm: 1.88470391
INFO:root:[   52] Training loss: 0.03057206, Validation loss: 0.05306775, Gradient norm: 1.60458385
INFO:root:[   53] Training loss: 0.02951503, Validation loss: 0.05652916, Gradient norm: 1.49038212
INFO:root:[   54] Training loss: 0.02855234, Validation loss: 0.04309276, Gradient norm: 1.65192823
INFO:root:[   55] Training loss: 0.02872703, Validation loss: 0.07078994, Gradient norm: 1.64721782
INFO:root:[   56] Training loss: 0.02805845, Validation loss: 0.06314660, Gradient norm: 1.42892084
INFO:root:[   57] Training loss: 0.02849064, Validation loss: 0.04751607, Gradient norm: 1.56554144
INFO:root:[   58] Training loss: 0.02949484, Validation loss: 0.06494808, Gradient norm: 1.74416550
INFO:root:[   59] Training loss: 0.02864361, Validation loss: 0.05123691, Gradient norm: 1.62128764
INFO:root:[   60] Training loss: 0.02877591, Validation loss: 0.04651437, Gradient norm: 1.70586949
INFO:root:[   61] Training loss: 0.02791618, Validation loss: 0.06304165, Gradient norm: 1.62015631
INFO:root:[   62] Training loss: 0.02890963, Validation loss: 0.04937361, Gradient norm: 1.75418013
INFO:root:[   63] Training loss: 0.02830172, Validation loss: 0.05938593, Gradient norm: 1.69117729
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 2744.074s.
INFO:root:Emptying the cuda cache took 0.049s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05208
INFO:root:EnergyScoreTrain: 0.03669
INFO:root:CoverageTrain: 0.98325
INFO:root:IntervalWidthTrain: 0.04216
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06017
INFO:root:EnergyScoreValidation: 0.04288
INFO:root:CoverageValidation: 0.92231
INFO:root:IntervalWidthValidation: 0.04174
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06022
INFO:root:EnergyScoreTest: 0.04295
INFO:root:CoverageTest: 0.92014
INFO:root:IntervalWidthTest: 0.04183
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.21293175, Validation loss: 0.15227511, Gradient norm: 3.50311693
INFO:root:[    2] Training loss: 0.10025949, Validation loss: 0.12624131, Gradient norm: 3.75981092
INFO:root:[    3] Training loss: 0.08393698, Validation loss: 0.11894976, Gradient norm: 3.07940471
INFO:root:[    4] Training loss: 0.06905526, Validation loss: 0.13039865, Gradient norm: 2.43749984
INFO:root:[    5] Training loss: 0.06770575, Validation loss: 0.11577471, Gradient norm: 2.79758158
INFO:root:[    6] Training loss: 0.06549038, Validation loss: 0.11200850, Gradient norm: 2.76608741
INFO:root:[    7] Training loss: 0.05739797, Validation loss: 0.11339236, Gradient norm: 2.41048325
INFO:root:[    8] Training loss: 0.05841050, Validation loss: 0.10705129, Gradient norm: 2.77021840
INFO:root:[    9] Training loss: 0.05660762, Validation loss: 0.11984513, Gradient norm: 2.66414340
INFO:root:[   10] Training loss: 0.05431575, Validation loss: 0.09778863, Gradient norm: 2.64858734
INFO:root:[   11] Training loss: 0.05138349, Validation loss: 0.10120586, Gradient norm: 2.54971259
INFO:root:[   12] Training loss: 0.05352379, Validation loss: 0.09656462, Gradient norm: 2.81879885
INFO:root:[   13] Training loss: 0.04774609, Validation loss: 0.09553524, Gradient norm: 2.16121447
INFO:root:[   14] Training loss: 0.04814968, Validation loss: 0.08947569, Gradient norm: 2.49097740
INFO:root:[   15] Training loss: 0.04915687, Validation loss: 0.08579592, Gradient norm: 2.36786027
INFO:root:[   16] Training loss: 0.04478206, Validation loss: 0.09281133, Gradient norm: 2.15801006
INFO:root:[   17] Training loss: 0.04340434, Validation loss: 0.08451556, Gradient norm: 2.20866978
INFO:root:[   18] Training loss: 0.04458090, Validation loss: 0.08118116, Gradient norm: 2.20814422
INFO:root:[   19] Training loss: 0.04370856, Validation loss: 0.07959543, Gradient norm: 2.24029785
INFO:root:[   20] Training loss: 0.04154104, Validation loss: 0.08957263, Gradient norm: 1.93838794
INFO:root:[   21] Training loss: 0.04694215, Validation loss: 0.08062942, Gradient norm: 2.85563920
INFO:root:[   22] Training loss: 0.04177883, Validation loss: 0.09299493, Gradient norm: 2.14903001
INFO:root:[   23] Training loss: 0.04311802, Validation loss: 0.09198744, Gradient norm: 2.36621547
INFO:root:[   24] Training loss: 0.03930980, Validation loss: 0.07933489, Gradient norm: 1.80339738
INFO:root:[   25] Training loss: 0.04152950, Validation loss: 0.08545337, Gradient norm: 2.36205118
INFO:root:[   26] Training loss: 0.03909721, Validation loss: 0.08222785, Gradient norm: 2.12857613
INFO:root:[   27] Training loss: 0.03971255, Validation loss: 0.08224902, Gradient norm: 2.08087555
INFO:root:[   28] Training loss: 0.03969255, Validation loss: 0.08360902, Gradient norm: 1.89476143
INFO:root:[   29] Training loss: 0.03917111, Validation loss: 0.07461530, Gradient norm: 2.27977713
INFO:root:[   30] Training loss: 0.03777692, Validation loss: 0.07120560, Gradient norm: 2.00475023
INFO:root:[   31] Training loss: 0.03606179, Validation loss: 0.09059036, Gradient norm: 1.68110798
INFO:root:[   32] Training loss: 0.03557284, Validation loss: 0.06966460, Gradient norm: 1.83634843
INFO:root:[   33] Training loss: 0.03836266, Validation loss: 0.07051671, Gradient norm: 2.19425663
INFO:root:[   34] Training loss: 0.03727402, Validation loss: 0.07037301, Gradient norm: 2.10922896
INFO:root:[   35] Training loss: 0.03873206, Validation loss: 0.08074362, Gradient norm: 2.28370179
INFO:root:[   36] Training loss: 0.03373479, Validation loss: 0.07599289, Gradient norm: 1.65145380
INFO:root:[   37] Training loss: 0.03340883, Validation loss: 0.08838674, Gradient norm: 1.54901553
INFO:root:[   38] Training loss: 0.03423578, Validation loss: 0.06348902, Gradient norm: 1.90709028
INFO:root:[   39] Training loss: 0.03585583, Validation loss: 0.08929930, Gradient norm: 1.91394728
INFO:root:[   40] Training loss: 0.03561466, Validation loss: 0.07443655, Gradient norm: 1.94409054
INFO:root:[   41] Training loss: 0.03479633, Validation loss: 0.07783506, Gradient norm: 1.97698227
INFO:root:[   42] Training loss: 0.03281054, Validation loss: 0.06853526, Gradient norm: 2.00110268
INFO:root:[   43] Training loss: 0.03408234, Validation loss: 0.06213740, Gradient norm: 2.05262520
INFO:root:[   44] Training loss: 0.03324528, Validation loss: 0.06676560, Gradient norm: 1.85065944
INFO:root:[   45] Training loss: 0.03129946, Validation loss: 0.08022188, Gradient norm: 1.72966966
INFO:root:[   46] Training loss: 0.03267029, Validation loss: 0.07631290, Gradient norm: 1.58156888
INFO:root:[   47] Training loss: 0.03076455, Validation loss: 0.07039819, Gradient norm: 1.72971345
INFO:root:[   48] Training loss: 0.03064552, Validation loss: 0.06701678, Gradient norm: 1.66227886
INFO:root:[   49] Training loss: 0.03242728, Validation loss: 0.06897505, Gradient norm: 1.73177481
INFO:root:[   50] Training loss: 0.03193409, Validation loss: 0.06853990, Gradient norm: 1.94285266
INFO:root:[   51] Training loss: 0.03021942, Validation loss: 0.07421936, Gradient norm: 1.61582910
INFO:root:[   52] Training loss: 0.03025059, Validation loss: 0.08349501, Gradient norm: 1.77136340
INFO:root:[   53] Training loss: 0.03224257, Validation loss: 0.06320495, Gradient norm: 2.02117212
INFO:root:[   54] Training loss: 0.03086296, Validation loss: 0.06203017, Gradient norm: 1.78274924
INFO:root:[   55] Training loss: 0.03232886, Validation loss: 0.06973157, Gradient norm: 1.76644389
INFO:root:[   56] Training loss: 0.03013756, Validation loss: 0.07494797, Gradient norm: 1.81429849
INFO:root:[   57] Training loss: 0.02944313, Validation loss: 0.08802698, Gradient norm: 1.92482322
INFO:root:[   58] Training loss: 0.03141807, Validation loss: 0.07543126, Gradient norm: 2.07243127
INFO:root:[   59] Training loss: 0.02999914, Validation loss: 0.06193167, Gradient norm: 1.81181397
INFO:root:[   60] Training loss: 0.03118035, Validation loss: 0.08067513, Gradient norm: 1.67139987
INFO:root:[   61] Training loss: 0.02721487, Validation loss: 0.07078068, Gradient norm: 1.59738654
INFO:root:[   62] Training loss: 0.02920188, Validation loss: 0.07096215, Gradient norm: 1.77659622
INFO:root:[   63] Training loss: 0.02864030, Validation loss: 0.06900775, Gradient norm: 1.71901055
INFO:root:[   64] Training loss: 0.03025233, Validation loss: 0.07678600, Gradient norm: 1.91034206
INFO:root:[   65] Training loss: 0.02865686, Validation loss: 0.06049976, Gradient norm: 1.79457307
INFO:root:[   66] Training loss: 0.02936906, Validation loss: 0.08587805, Gradient norm: 1.98648576
INFO:root:[   67] Training loss: 0.02905214, Validation loss: 0.08416650, Gradient norm: 1.90417828
INFO:root:[   68] Training loss: 0.02908936, Validation loss: 0.06628689, Gradient norm: 1.72042380
INFO:root:[   69] Training loss: 0.02751316, Validation loss: 0.07456300, Gradient norm: 1.56287861
INFO:root:[   70] Training loss: 0.02888545, Validation loss: 0.06597137, Gradient norm: 1.70584900
INFO:root:[   71] Training loss: 0.02625876, Validation loss: 0.08076886, Gradient norm: 1.58173881
INFO:root:[   72] Training loss: 0.02639844, Validation loss: 0.07358174, Gradient norm: 1.72328439
INFO:root:[   73] Training loss: 0.02776301, Validation loss: 0.08018005, Gradient norm: 1.89065067
INFO:root:[   74] Training loss: 0.02792369, Validation loss: 0.06440487, Gradient norm: 1.80677473
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 3239.604s.
INFO:root:Emptying the cuda cache took 0.05s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04615
INFO:root:EnergyScoreTrain: 0.03297
INFO:root:CoverageTrain: 0.98743
INFO:root:IntervalWidthTrain: 0.04225
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08281
INFO:root:EnergyScoreValidation: 0.06075
INFO:root:CoverageValidation: 0.82748
INFO:root:IntervalWidthValidation: 0.04308
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08338
INFO:root:EnergyScoreTest: 0.06125
INFO:root:CoverageTest: 0.82564
INFO:root:IntervalWidthTest: 0.04333
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.21067589, Validation loss: 0.18629291, Gradient norm: 3.49635577
INFO:root:[    2] Training loss: 0.09501053, Validation loss: 0.11910390, Gradient norm: 3.21868323
INFO:root:[    3] Training loss: 0.08529066, Validation loss: 0.12923637, Gradient norm: 3.44840363
INFO:root:[    4] Training loss: 0.07572361, Validation loss: 0.12280853, Gradient norm: 3.26996236
INFO:root:[    5] Training loss: 0.06509715, Validation loss: 0.11351741, Gradient norm: 2.70639971
INFO:root:[    6] Training loss: 0.06328677, Validation loss: 0.10197430, Gradient norm: 2.82783342
INFO:root:[    7] Training loss: 0.05964350, Validation loss: 0.09941873, Gradient norm: 2.62285833
INFO:root:[    8] Training loss: 0.05775436, Validation loss: 0.10345406, Gradient norm: 2.69278656
INFO:root:[    9] Training loss: 0.05694037, Validation loss: 0.10212629, Gradient norm: 2.67173847
INFO:root:[   10] Training loss: 0.05322106, Validation loss: 0.11820142, Gradient norm: 2.51526298
INFO:root:[   11] Training loss: 0.05120228, Validation loss: 0.09298860, Gradient norm: 2.37633904
INFO:root:[   12] Training loss: 0.04840459, Validation loss: 0.08731435, Gradient norm: 2.30021199
INFO:root:[   13] Training loss: 0.04975633, Validation loss: 0.10672637, Gradient norm: 2.56127523
INFO:root:[   14] Training loss: 0.04897083, Validation loss: 0.08374294, Gradient norm: 2.36084975
INFO:root:[   15] Training loss: 0.04545144, Validation loss: 0.08137196, Gradient norm: 2.23526902
INFO:root:[   16] Training loss: 0.04246628, Validation loss: 0.08064937, Gradient norm: 1.93100071
INFO:root:[   17] Training loss: 0.04336171, Validation loss: 0.07809262, Gradient norm: 2.13212352
INFO:root:[   18] Training loss: 0.04513940, Validation loss: 0.07422556, Gradient norm: 2.41025741
INFO:root:[   19] Training loss: 0.04523214, Validation loss: 0.09780795, Gradient norm: 2.44122030
INFO:root:[   20] Training loss: 0.04315908, Validation loss: 0.08502271, Gradient norm: 1.91944267
INFO:root:[   21] Training loss: 0.04118684, Validation loss: 0.07298451, Gradient norm: 2.07095122
INFO:root:[   22] Training loss: 0.03998545, Validation loss: 0.07962982, Gradient norm: 2.00191190
INFO:root:[   23] Training loss: 0.04161514, Validation loss: 0.07116747, Gradient norm: 2.39139369
INFO:root:[   24] Training loss: 0.03953212, Validation loss: 0.08447969, Gradient norm: 2.10644637
INFO:root:[   25] Training loss: 0.04056764, Validation loss: 0.08520378, Gradient norm: 2.36185515
INFO:root:[   26] Training loss: 0.04178601, Validation loss: 0.08075254, Gradient norm: 2.43486018
INFO:root:[   27] Training loss: 0.03841665, Validation loss: 0.07572108, Gradient norm: 2.03178274
INFO:root:[   28] Training loss: 0.03698226, Validation loss: 0.06391141, Gradient norm: 1.94544539
INFO:root:[   29] Training loss: 0.03704265, Validation loss: 0.06573787, Gradient norm: 1.85943396
INFO:root:[   30] Training loss: 0.03760714, Validation loss: 0.05824572, Gradient norm: 2.01507054
INFO:root:[   31] Training loss: 0.03558572, Validation loss: 0.07659799, Gradient norm: 1.85882451
INFO:root:[   32] Training loss: 0.03747905, Validation loss: 0.06140840, Gradient norm: 2.23574607
INFO:root:[   33] Training loss: 0.03559518, Validation loss: 0.07069703, Gradient norm: 1.88863777
INFO:root:[   34] Training loss: 0.03507070, Validation loss: 0.06136833, Gradient norm: 1.85220621
INFO:root:[   35] Training loss: 0.03400253, Validation loss: 0.06626254, Gradient norm: 1.90311912
INFO:root:[   36] Training loss: 0.03443246, Validation loss: 0.05352013, Gradient norm: 1.90453951
INFO:root:[   37] Training loss: 0.03493341, Validation loss: 0.07228937, Gradient norm: 1.87816682
INFO:root:[   38] Training loss: 0.03342109, Validation loss: 0.05731838, Gradient norm: 1.70998815
INFO:root:[   39] Training loss: 0.03433069, Validation loss: 0.07033012, Gradient norm: 1.95159222
INFO:root:[   40] Training loss: 0.03595056, Validation loss: 0.05653197, Gradient norm: 2.07617416
INFO:root:[   41] Training loss: 0.03339461, Validation loss: 0.04834060, Gradient norm: 1.98784203
INFO:root:[   42] Training loss: 0.03309135, Validation loss: 0.05142565, Gradient norm: 1.87228813
INFO:root:[   43] Training loss: 0.03440359, Validation loss: 0.06152835, Gradient norm: 2.00871786
INFO:root:[   44] Training loss: 0.03251533, Validation loss: 0.05217028, Gradient norm: 1.88673016
INFO:root:[   45] Training loss: 0.03309147, Validation loss: 0.05416008, Gradient norm: 1.91454038
INFO:root:[   46] Training loss: 0.03106804, Validation loss: 0.06430723, Gradient norm: 1.55141956
INFO:root:[   47] Training loss: 0.03208263, Validation loss: 0.06699003, Gradient norm: 1.75608074
INFO:root:[   48] Training loss: 0.03105702, Validation loss: 0.05645451, Gradient norm: 1.62473722
INFO:root:[   49] Training loss: 0.03199860, Validation loss: 0.05048939, Gradient norm: 1.81962118
INFO:root:[   50] Training loss: 0.02968667, Validation loss: 0.05155391, Gradient norm: 1.67220574
INFO:root:[   51] Training loss: 0.03121014, Validation loss: 0.06738007, Gradient norm: 1.88542546
INFO:root:[   52] Training loss: 0.03278507, Validation loss: 0.05350045, Gradient norm: 1.78344029
INFO:root:[   53] Training loss: 0.02956164, Validation loss: 0.05091467, Gradient norm: 1.82732777
INFO:root:[   54] Training loss: 0.03049975, Validation loss: 0.06579041, Gradient norm: 1.71365075
INFO:root:[   55] Training loss: 0.03142392, Validation loss: 0.06847408, Gradient norm: 1.82653470
INFO:root:[   56] Training loss: 0.03111356, Validation loss: 0.05796249, Gradient norm: 1.66981584
INFO:root:[   57] Training loss: 0.02887899, Validation loss: 0.04709579, Gradient norm: 1.82975955
INFO:root:[   58] Training loss: 0.03065885, Validation loss: 0.07013186, Gradient norm: 1.91663741
INFO:root:[   59] Training loss: 0.02872282, Validation loss: 0.05298428, Gradient norm: 1.74346527
INFO:root:[   60] Training loss: 0.03012936, Validation loss: 0.05779443, Gradient norm: 1.62917167
INFO:root:[   61] Training loss: 0.02930700, Validation loss: 0.06035915, Gradient norm: 1.69792774
INFO:root:[   62] Training loss: 0.02940818, Validation loss: 0.05664857, Gradient norm: 1.98030347
INFO:root:[   63] Training loss: 0.02715101, Validation loss: 0.04660920, Gradient norm: 1.50756318
INFO:root:[   64] Training loss: 0.02966016, Validation loss: 0.04668610, Gradient norm: 1.78560672
INFO:root:[   65] Training loss: 0.02924140, Validation loss: 0.06375177, Gradient norm: 1.79435984
INFO:root:[   66] Training loss: 0.02866003, Validation loss: 0.04541516, Gradient norm: 1.75663551
INFO:root:[   67] Training loss: 0.02717206, Validation loss: 0.05712585, Gradient norm: 1.62713928
INFO:root:[   68] Training loss: 0.02791088, Validation loss: 0.06041073, Gradient norm: 1.56906771
INFO:root:[   69] Training loss: 0.02854223, Validation loss: 0.06416435, Gradient norm: 1.80875298
INFO:root:[   70] Training loss: 0.02913298, Validation loss: 0.05563252, Gradient norm: 1.76944861
INFO:root:[   71] Training loss: 0.02782343, Validation loss: 0.05881325, Gradient norm: 1.69335770
INFO:root:[   72] Training loss: 0.02841769, Validation loss: 0.05617222, Gradient norm: 1.73096178
INFO:root:[   73] Training loss: 0.02693083, Validation loss: 0.04991946, Gradient norm: 1.68430714
INFO:root:[   74] Training loss: 0.02766385, Validation loss: 0.05073901, Gradient norm: 1.62630313
INFO:root:[   75] Training loss: 0.02817675, Validation loss: 0.04704048, Gradient norm: 1.80771185
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 3271.766s.
INFO:root:Emptying the cuda cache took 0.049s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03751
INFO:root:EnergyScoreTrain: 0.02802
INFO:root:CoverageTrain: 0.99358
INFO:root:IntervalWidthTrain: 0.04082
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06414
INFO:root:EnergyScoreValidation: 0.04557
INFO:root:CoverageValidation: 0.89507
INFO:root:IntervalWidthValidation: 0.04091
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0645
INFO:root:EnergyScoreTest: 0.04589
INFO:root:CoverageTest: 0.88802
INFO:root:IntervalWidthTest: 0.04104
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.20393610, Validation loss: 0.16342147, Gradient norm: 3.46387263
INFO:root:[    2] Training loss: 0.09894633, Validation loss: 0.15712076, Gradient norm: 3.32748807
INFO:root:[    3] Training loss: 0.08413577, Validation loss: 0.11714462, Gradient norm: 3.25781465
INFO:root:[    4] Training loss: 0.06966752, Validation loss: 0.10855310, Gradient norm: 2.60648135
INFO:root:[    5] Training loss: 0.06196117, Validation loss: 0.10360389, Gradient norm: 2.50033648
INFO:root:[    6] Training loss: 0.06000233, Validation loss: 0.10163553, Gradient norm: 2.75927523
INFO:root:[    7] Training loss: 0.05994801, Validation loss: 0.13966089, Gradient norm: 2.64802177
INFO:root:[    8] Training loss: 0.05522156, Validation loss: 0.10360472, Gradient norm: 2.56204038
INFO:root:[    9] Training loss: 0.05793599, Validation loss: 0.12525565, Gradient norm: 3.09542271
INFO:root:[   10] Training loss: 0.05096370, Validation loss: 0.09056704, Gradient norm: 2.38607753
INFO:root:[   11] Training loss: 0.05182977, Validation loss: 0.10334658, Gradient norm: 2.63448022
INFO:root:[   12] Training loss: 0.05223185, Validation loss: 0.10843329, Gradient norm: 2.66284617
INFO:root:[   13] Training loss: 0.04771519, Validation loss: 0.08872778, Gradient norm: 2.17173490
INFO:root:[   14] Training loss: 0.04577749, Validation loss: 0.11097780, Gradient norm: 2.12113393
INFO:root:[   15] Training loss: 0.04651717, Validation loss: 0.09030171, Gradient norm: 2.37677498
INFO:root:[   16] Training loss: 0.04505458, Validation loss: 0.12145749, Gradient norm: 2.20202253
INFO:root:[   17] Training loss: 0.04264980, Validation loss: 0.09726790, Gradient norm: 2.09884586
INFO:root:[   18] Training loss: 0.04554754, Validation loss: 0.10338595, Gradient norm: 2.41566652
INFO:root:[   19] Training loss: 0.04221846, Validation loss: 0.08942759, Gradient norm: 2.07160796
INFO:root:[   20] Training loss: 0.04653913, Validation loss: 0.09362775, Gradient norm: 2.63842842
INFO:root:[   21] Training loss: 0.04257548, Validation loss: 0.09043276, Gradient norm: 2.13903272
INFO:root:[   22] Training loss: 0.04016733, Validation loss: 0.08092220, Gradient norm: 1.95377565
INFO:root:[   23] Training loss: 0.04088135, Validation loss: 0.10393455, Gradient norm: 2.10468682
INFO:root:[   24] Training loss: 0.04344937, Validation loss: 0.07663526, Gradient norm: 2.34301814
INFO:root:[   25] Training loss: 0.03759074, Validation loss: 0.07429889, Gradient norm: 1.84180001
INFO:root:[   26] Training loss: 0.03971798, Validation loss: 0.08726835, Gradient norm: 2.03234110
INFO:root:[   27] Training loss: 0.03770385, Validation loss: 0.07555755, Gradient norm: 1.90977624
INFO:root:[   28] Training loss: 0.03653104, Validation loss: 0.07672571, Gradient norm: 1.85911401
INFO:root:[   29] Training loss: 0.03597370, Validation loss: 0.07342923, Gradient norm: 1.79744834
INFO:root:[   30] Training loss: 0.03688088, Validation loss: 0.07467402, Gradient norm: 1.82986131
INFO:root:[   31] Training loss: 0.03600681, Validation loss: 0.06691073, Gradient norm: 1.78126985
INFO:root:[   32] Training loss: 0.03597195, Validation loss: 0.07792479, Gradient norm: 1.99718999
INFO:root:[   33] Training loss: 0.03626126, Validation loss: 0.07549740, Gradient norm: 1.91423459
INFO:root:[   34] Training loss: 0.03734490, Validation loss: 0.07172062, Gradient norm: 2.08891792
INFO:root:[   35] Training loss: 0.03426991, Validation loss: 0.06967910, Gradient norm: 1.74461818
INFO:root:[   36] Training loss: 0.03262548, Validation loss: 0.06755010, Gradient norm: 1.58067797
INFO:root:[   37] Training loss: 0.03402793, Validation loss: 0.06877419, Gradient norm: 1.84891080
INFO:root:[   38] Training loss: 0.03357653, Validation loss: 0.06393854, Gradient norm: 1.98863933
INFO:root:[   39] Training loss: 0.03277640, Validation loss: 0.06503872, Gradient norm: 1.75267267
INFO:root:[   40] Training loss: 0.03424868, Validation loss: 0.06514460, Gradient norm: 1.72851370
INFO:root:[   41] Training loss: 0.03099539, Validation loss: 0.06340074, Gradient norm: 1.52625940
INFO:root:[   42] Training loss: 0.03377620, Validation loss: 0.06430943, Gradient norm: 1.93826114
INFO:root:[   43] Training loss: 0.03084985, Validation loss: 0.07995408, Gradient norm: 1.46453018
INFO:root:[   44] Training loss: 0.03283309, Validation loss: 0.06540538, Gradient norm: 1.75891920
INFO:root:[   45] Training loss: 0.03158736, Validation loss: 0.07272420, Gradient norm: 1.66924647
INFO:root:[   46] Training loss: 0.03247825, Validation loss: 0.06854077, Gradient norm: 1.96649407
INFO:root:[   47] Training loss: 0.03145184, Validation loss: 0.06171293, Gradient norm: 1.88134750
INFO:root:[   48] Training loss: 0.03185363, Validation loss: 0.05809055, Gradient norm: 1.86873642
INFO:root:[   49] Training loss: 0.02923393, Validation loss: 0.05930274, Gradient norm: 1.46520306
INFO:root:[   50] Training loss: 0.03032762, Validation loss: 0.06716752, Gradient norm: 1.73316656
INFO:root:[   51] Training loss: 0.03014413, Validation loss: 0.05771217, Gradient norm: 1.53591059
INFO:root:[   52] Training loss: 0.02948512, Validation loss: 0.06439426, Gradient norm: 1.74055136
INFO:root:[   53] Training loss: 0.02904177, Validation loss: 0.05826459, Gradient norm: 1.71972525
INFO:root:[   54] Training loss: 0.03014977, Validation loss: 0.05778435, Gradient norm: 1.52794326
INFO:root:[   55] Training loss: 0.02994756, Validation loss: 0.06226593, Gradient norm: 1.61254941
INFO:root:[   56] Training loss: 0.02789086, Validation loss: 0.05751088, Gradient norm: 1.61413043
INFO:root:[   57] Training loss: 0.02701124, Validation loss: 0.05790852, Gradient norm: 1.39552571
INFO:root:[   58] Training loss: 0.02826042, Validation loss: 0.05822657, Gradient norm: 1.36591544
INFO:root:[   59] Training loss: 0.02816155, Validation loss: 0.05952822, Gradient norm: 1.66904164
INFO:root:[   60] Training loss: 0.02830708, Validation loss: 0.06089384, Gradient norm: 1.68847080
INFO:root:[   61] Training loss: 0.02813416, Validation loss: 0.05903837, Gradient norm: 1.80052890
