INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 3 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.37458288, Validation loss: 0.30870457, Gradient norm: 2.24837319
INFO:root:[    2] Training loss: 0.30269166, Validation loss: 0.30003644, Gradient norm: 1.78186941
INFO:root:[    3] Training loss: 0.29163939, Validation loss: 0.27180591, Gradient norm: 1.75204381
INFO:root:[    4] Training loss: 0.28305594, Validation loss: 0.27440844, Gradient norm: 1.62216315
INFO:root:[    5] Training loss: 0.27735110, Validation loss: 0.28105853, Gradient norm: 1.57922417
INFO:root:[    6] Training loss: 0.27190319, Validation loss: 0.26229500, Gradient norm: 1.59601585
INFO:root:[    7] Training loss: 0.26536199, Validation loss: 0.25189654, Gradient norm: 1.50753207
INFO:root:[    8] Training loss: 0.25973553, Validation loss: 0.26020621, Gradient norm: 1.49741633
INFO:root:[    9] Training loss: 0.25436008, Validation loss: 0.24753121, Gradient norm: 1.50408292
INFO:root:[   10] Training loss: 0.24934842, Validation loss: 0.24057523, Gradient norm: 1.49564412
INFO:root:[   11] Training loss: 0.24563764, Validation loss: 0.23981059, Gradient norm: 1.55515208
INFO:root:[   12] Training loss: 0.24083754, Validation loss: 0.23583167, Gradient norm: 1.51256985
INFO:root:[   13] Training loss: 0.23740689, Validation loss: 0.23105098, Gradient norm: 1.48354948
INFO:root:[   14] Training loss: 0.23483230, Validation loss: 0.22948583, Gradient norm: 1.50510658
INFO:root:[   15] Training loss: 0.23161179, Validation loss: 0.23662509, Gradient norm: 1.51945975
INFO:root:[   16] Training loss: 0.22918420, Validation loss: 0.22767566, Gradient norm: 1.48880806
INFO:root:[   17] Training loss: 0.22706573, Validation loss: 0.22907993, Gradient norm: 1.49197029
INFO:root:[   18] Training loss: 0.22430038, Validation loss: 0.22960454, Gradient norm: 1.40893296
INFO:root:[   19] Training loss: 0.22317571, Validation loss: 0.22496580, Gradient norm: 1.52641824
INFO:root:[   20] Training loss: 0.22174923, Validation loss: 0.21947145, Gradient norm: 1.51992813
INFO:root:[   21] Training loss: 0.21978518, Validation loss: 0.21633014, Gradient norm: 1.45743533
INFO:root:[   22] Training loss: 0.21828584, Validation loss: 0.21641593, Gradient norm: 1.43031802
INFO:root:[   23] Training loss: 0.21719014, Validation loss: 0.21747666, Gradient norm: 1.42901089
INFO:root:[   24] Training loss: 0.21566488, Validation loss: 0.21443755, Gradient norm: 1.43879178
INFO:root:[   25] Training loss: 0.21460825, Validation loss: 0.21252106, Gradient norm: 1.43086460
INFO:root:[   26] Training loss: 0.21331726, Validation loss: 0.21225185, Gradient norm: 1.40160349
INFO:root:[   27] Training loss: 0.21273074, Validation loss: 0.20899742, Gradient norm: 1.43954360
INFO:root:[   28] Training loss: 0.21192049, Validation loss: 0.20926391, Gradient norm: 1.45666567
INFO:root:[   29] Training loss: 0.21030392, Validation loss: 0.21421326, Gradient norm: 1.34493383
INFO:root:[   30] Training loss: 0.20974533, Validation loss: 0.20549568, Gradient norm: 1.39409040
INFO:root:[   31] Training loss: 0.20879617, Validation loss: 0.20839101, Gradient norm: 1.39905926
INFO:root:[   32] Training loss: 0.20812757, Validation loss: 0.21152158, Gradient norm: 1.42310024
INFO:root:[   33] Training loss: 0.20752120, Validation loss: 0.20871378, Gradient norm: 1.43052969
INFO:root:[   34] Training loss: 0.20645328, Validation loss: 0.20528917, Gradient norm: 1.37123990
INFO:root:[   35] Training loss: 0.20598749, Validation loss: 0.20048660, Gradient norm: 1.39774219
INFO:root:[   36] Training loss: 0.20534181, Validation loss: 0.21003686, Gradient norm: 1.38834006
INFO:root:[   37] Training loss: 0.20421793, Validation loss: 0.20521697, Gradient norm: 1.32556119
INFO:root:[   38] Training loss: 0.20385760, Validation loss: 0.20549420, Gradient norm: 1.42035018
INFO:root:[   39] Training loss: 0.20286414, Validation loss: 0.20390995, Gradient norm: 1.34754324
INFO:root:[   40] Training loss: 0.20285336, Validation loss: 0.20192558, Gradient norm: 1.37304328
INFO:root:[   41] Training loss: 0.20201554, Validation loss: 0.20841426, Gradient norm: 1.31519567
INFO:root:[   42] Training loss: 0.20118092, Validation loss: 0.21028024, Gradient norm: 1.32129402
INFO:root:[   43] Training loss: 0.20051207, Validation loss: 0.19629339, Gradient norm: 1.30909615
INFO:root:[   44] Training loss: 0.20014006, Validation loss: 0.19764084, Gradient norm: 1.30344488
INFO:root:[   45] Training loss: 0.19995997, Validation loss: 0.19981320, Gradient norm: 1.37810199
INFO:root:[   46] Training loss: 0.19905414, Validation loss: 0.19474536, Gradient norm: 1.31529756
INFO:root:[   47] Training loss: 0.19837300, Validation loss: 0.19531450, Gradient norm: 1.29633832
INFO:root:[   48] Training loss: 0.19867157, Validation loss: 0.20239630, Gradient norm: 1.34001323
INFO:root:[   49] Training loss: 0.19760317, Validation loss: 0.19328875, Gradient norm: 1.25484833
INFO:root:[   50] Training loss: 0.19774081, Validation loss: 0.20612636, Gradient norm: 1.35697638
