INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 3 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.37458288, Validation loss: 0.30870457, Gradient norm: 2.24837319
INFO:root:[    2] Training loss: 0.30269166, Validation loss: 0.30003644, Gradient norm: 1.78186941
INFO:root:[    3] Training loss: 0.29163939, Validation loss: 0.27180591, Gradient norm: 1.75204381
INFO:root:[    4] Training loss: 0.28305594, Validation loss: 0.27440844, Gradient norm: 1.62216315
INFO:root:[    5] Training loss: 0.27735110, Validation loss: 0.28105853, Gradient norm: 1.57922417
INFO:root:[    6] Training loss: 0.27190319, Validation loss: 0.26229500, Gradient norm: 1.59601585
INFO:root:[    7] Training loss: 0.26536199, Validation loss: 0.25189654, Gradient norm: 1.50753207
INFO:root:[    8] Training loss: 0.25973553, Validation loss: 0.26020621, Gradient norm: 1.49741633
INFO:root:[    9] Training loss: 0.25436008, Validation loss: 0.24753121, Gradient norm: 1.50408292
INFO:root:[   10] Training loss: 0.24934842, Validation loss: 0.24057523, Gradient norm: 1.49564412
INFO:root:[   11] Training loss: 0.24563764, Validation loss: 0.23981059, Gradient norm: 1.55515208
INFO:root:[   12] Training loss: 0.24083754, Validation loss: 0.23583167, Gradient norm: 1.51256985
INFO:root:[   13] Training loss: 0.23740689, Validation loss: 0.23105098, Gradient norm: 1.48354948
INFO:root:[   14] Training loss: 0.23483230, Validation loss: 0.22948583, Gradient norm: 1.50510658
INFO:root:[   15] Training loss: 0.23161179, Validation loss: 0.23662509, Gradient norm: 1.51945975
INFO:root:[   16] Training loss: 0.22918420, Validation loss: 0.22767566, Gradient norm: 1.48880806
INFO:root:[   17] Training loss: 0.22706573, Validation loss: 0.22907993, Gradient norm: 1.49197029
INFO:root:[   18] Training loss: 0.22430038, Validation loss: 0.22960454, Gradient norm: 1.40893296
INFO:root:[   19] Training loss: 0.22317571, Validation loss: 0.22496580, Gradient norm: 1.52641824
INFO:root:[   20] Training loss: 0.22174923, Validation loss: 0.21947145, Gradient norm: 1.51992813
INFO:root:[   21] Training loss: 0.21978518, Validation loss: 0.21633014, Gradient norm: 1.45743533
INFO:root:[   22] Training loss: 0.21828584, Validation loss: 0.21641593, Gradient norm: 1.43031802
INFO:root:[   23] Training loss: 0.21719014, Validation loss: 0.21747666, Gradient norm: 1.42901089
INFO:root:[   24] Training loss: 0.21566488, Validation loss: 0.21443755, Gradient norm: 1.43879178
INFO:root:[   25] Training loss: 0.21460825, Validation loss: 0.21252106, Gradient norm: 1.43086460
INFO:root:[   26] Training loss: 0.21331726, Validation loss: 0.21225185, Gradient norm: 1.40160349
INFO:root:[   27] Training loss: 0.21273074, Validation loss: 0.20899742, Gradient norm: 1.43954360
INFO:root:[   28] Training loss: 0.21192049, Validation loss: 0.20926391, Gradient norm: 1.45666567
INFO:root:[   29] Training loss: 0.21030392, Validation loss: 0.21421326, Gradient norm: 1.34493383
INFO:root:[   30] Training loss: 0.20974533, Validation loss: 0.20549568, Gradient norm: 1.39409040
INFO:root:[   31] Training loss: 0.20879617, Validation loss: 0.20839101, Gradient norm: 1.39905926
INFO:root:[   32] Training loss: 0.20812757, Validation loss: 0.21152158, Gradient norm: 1.42310024
INFO:root:[   33] Training loss: 0.20752120, Validation loss: 0.20871378, Gradient norm: 1.43052969
INFO:root:[   34] Training loss: 0.20645328, Validation loss: 0.20528917, Gradient norm: 1.37123990
INFO:root:[   35] Training loss: 0.20598749, Validation loss: 0.20048660, Gradient norm: 1.39774219
INFO:root:[   36] Training loss: 0.20534181, Validation loss: 0.21003686, Gradient norm: 1.38834006
INFO:root:[   37] Training loss: 0.20421793, Validation loss: 0.20521697, Gradient norm: 1.32556119
INFO:root:[   38] Training loss: 0.20385760, Validation loss: 0.20549420, Gradient norm: 1.42035018
INFO:root:[   39] Training loss: 0.20286414, Validation loss: 0.20390995, Gradient norm: 1.34754324
INFO:root:[   40] Training loss: 0.20285336, Validation loss: 0.20192558, Gradient norm: 1.37304328
INFO:root:[   41] Training loss: 0.20201554, Validation loss: 0.20841426, Gradient norm: 1.31519567
INFO:root:[   42] Training loss: 0.20118092, Validation loss: 0.21028024, Gradient norm: 1.32129402
INFO:root:[   43] Training loss: 0.20051207, Validation loss: 0.19629339, Gradient norm: 1.30909615
INFO:root:[   44] Training loss: 0.20014006, Validation loss: 0.19764084, Gradient norm: 1.30344488
INFO:root:[   45] Training loss: 0.19995997, Validation loss: 0.19981320, Gradient norm: 1.37810199
INFO:root:[   46] Training loss: 0.19905414, Validation loss: 0.19474536, Gradient norm: 1.31529756
INFO:root:[   47] Training loss: 0.19837300, Validation loss: 0.19531450, Gradient norm: 1.29633832
INFO:root:[   48] Training loss: 0.19867157, Validation loss: 0.20239630, Gradient norm: 1.34001323
INFO:root:[   49] Training loss: 0.19760317, Validation loss: 0.19328875, Gradient norm: 1.25484833
INFO:root:[   50] Training loss: 0.19774081, Validation loss: 0.20612636, Gradient norm: 1.35697638
INFO:root:[   51] Training loss: 0.19685642, Validation loss: 0.19150332, Gradient norm: 1.26637630
INFO:root:[   52] Training loss: 0.19689996, Validation loss: 0.20639577, Gradient norm: 1.37442248
INFO:root:[   53] Training loss: 0.19598011, Validation loss: 0.19329671, Gradient norm: 1.23624652
INFO:root:[   54] Training loss: 0.19574772, Validation loss: 0.19862357, Gradient norm: 1.29664174
INFO:root:[   55] Training loss: 0.19568626, Validation loss: 0.19755722, Gradient norm: 1.30667143
INFO:root:[   56] Training loss: 0.19488436, Validation loss: 0.18996145, Gradient norm: 1.26232853
INFO:root:[   57] Training loss: 0.19437152, Validation loss: 0.18903063, Gradient norm: 1.26661888
INFO:root:[   58] Training loss: 0.19418356, Validation loss: 0.19422639, Gradient norm: 1.30734872
INFO:root:[   59] Training loss: 0.19418089, Validation loss: 0.19043855, Gradient norm: 1.27787411
INFO:root:[   60] Training loss: 0.19379420, Validation loss: 0.18917281, Gradient norm: 1.24497569
INFO:root:[   61] Training loss: 0.19330772, Validation loss: 0.18753203, Gradient norm: 1.26759559
INFO:root:[   62] Training loss: 0.19290762, Validation loss: 0.19297552, Gradient norm: 1.29197086
INFO:root:[   63] Training loss: 0.19274490, Validation loss: 0.18882146, Gradient norm: 1.26154225
INFO:root:[   64] Training loss: 0.19268223, Validation loss: 0.18886942, Gradient norm: 1.27240822
INFO:root:[   65] Training loss: 0.19236068, Validation loss: 0.19363430, Gradient norm: 1.28475036
INFO:root:[   66] Training loss: 0.19184051, Validation loss: 0.19603665, Gradient norm: 1.24335522
INFO:root:[   67] Training loss: 0.19163733, Validation loss: 0.18613603, Gradient norm: 1.26660266
INFO:root:[   68] Training loss: 0.19132250, Validation loss: 0.18643371, Gradient norm: 1.23079546
INFO:root:[   69] Training loss: 0.19069029, Validation loss: 0.18960472, Gradient norm: 1.21971613
INFO:root:[   70] Training loss: 0.19076391, Validation loss: 0.19156493, Gradient norm: 1.23789130
INFO:root:[   71] Training loss: 0.19070215, Validation loss: 0.19467240, Gradient norm: 1.28139788
INFO:root:[   72] Training loss: 0.19057474, Validation loss: 0.19334241, Gradient norm: 1.24917710
INFO:root:[   73] Training loss: 0.19007485, Validation loss: 0.18909829, Gradient norm: 1.28073457
INFO:root:[   74] Training loss: 0.18999077, Validation loss: 0.19118518, Gradient norm: 1.26919342
INFO:root:[   75] Training loss: 0.18930402, Validation loss: 0.18793969, Gradient norm: 1.18807032
INFO:root:[   76] Training loss: 0.18934030, Validation loss: 0.19909691, Gradient norm: 1.22797646
INFO:root:[   77] Training loss: 0.18902535, Validation loss: 0.18447886, Gradient norm: 1.19861092
INFO:root:[   78] Training loss: 0.18896573, Validation loss: 0.18616061, Gradient norm: 1.25313591
INFO:root:[   79] Training loss: 0.18885065, Validation loss: 0.18441498, Gradient norm: 1.27442028
INFO:root:[   80] Training loss: 0.18882099, Validation loss: 0.18501640, Gradient norm: 1.25426640
INFO:root:[   81] Training loss: 0.18858810, Validation loss: 0.18751745, Gradient norm: 1.26509795
INFO:root:[   82] Training loss: 0.18810611, Validation loss: 0.18464586, Gradient norm: 1.21498446
INFO:root:[   83] Training loss: 0.18823052, Validation loss: 0.18786699, Gradient norm: 1.19128320
INFO:root:[   84] Training loss: 0.18778589, Validation loss: 0.18411267, Gradient norm: 1.23687492
INFO:root:[   85] Training loss: 0.18775974, Validation loss: 0.18811991, Gradient norm: 1.24196934
INFO:root:[   86] Training loss: 0.18745870, Validation loss: 0.18151081, Gradient norm: 1.19888268
INFO:root:[   87] Training loss: 0.18706066, Validation loss: 0.19069410, Gradient norm: 1.18148311
INFO:root:[   88] Training loss: 0.18757628, Validation loss: 0.18187294, Gradient norm: 1.28876869
INFO:root:[   89] Training loss: 0.18678090, Validation loss: 0.18232852, Gradient norm: 1.19342847
INFO:root:[   90] Training loss: 0.18734360, Validation loss: 0.19387988, Gradient norm: 1.26692224
INFO:root:[   91] Training loss: 0.18639821, Validation loss: 0.18379590, Gradient norm: 1.18734442
INFO:root:[   92] Training loss: 0.18698969, Validation loss: 0.18827710, Gradient norm: 1.27876807
INFO:root:[   93] Training loss: 0.18647382, Validation loss: 0.18419718, Gradient norm: 1.23449760
INFO:root:[   94] Training loss: 0.18649176, Validation loss: 0.18766497, Gradient norm: 1.28541952
INFO:root:[   95] Training loss: 0.18615715, Validation loss: 0.18483608, Gradient norm: 1.20530847
INFO:root:EP 95: Early stopping
INFO:root:Training the model took 42985.587s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.19099
INFO:root:EnergyScoreValidation: 0.14214
INFO:root:CoverageValidation: 0.32311
INFO:root:IntervalWidthValidation: 0.01978
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.31456
INFO:root:EnergyScoreTest: 0.24914
INFO:root:CoverageTest: 0.23877
INFO:root:IntervalWidthTest: 0.01803
INFO:root:###2 out of 3 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 3246391296
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.36785520, Validation loss: 0.31252085, Gradient norm: 2.17677648
INFO:root:[    2] Training loss: 0.30275875, Validation loss: 0.30772046, Gradient norm: 1.72335836
INFO:root:[    3] Training loss: 0.29180700, Validation loss: 0.29096393, Gradient norm: 1.67197043
INFO:root:[    4] Training loss: 0.28425893, Validation loss: 0.28250076, Gradient norm: 1.60394072
INFO:root:[    5] Training loss: 0.27808558, Validation loss: 0.27733335, Gradient norm: 1.58473641
INFO:root:[    6] Training loss: 0.27180217, Validation loss: 0.26767065, Gradient norm: 1.50578265
INFO:root:[    7] Training loss: 0.26639142, Validation loss: 0.26547658, Gradient norm: 1.48563295
INFO:root:[    8] Training loss: 0.26081066, Validation loss: 0.26109383, Gradient norm: 1.49215700
INFO:root:[    9] Training loss: 0.25491144, Validation loss: 0.25726795, Gradient norm: 1.47182238
INFO:root:[   10] Training loss: 0.24929109, Validation loss: 0.25737625, Gradient norm: 1.45834526
INFO:root:[   11] Training loss: 0.24434932, Validation loss: 0.25010174, Gradient norm: 1.46733378
INFO:root:[   12] Training loss: 0.24026275, Validation loss: 0.25022913, Gradient norm: 1.42648248
INFO:root:[   13] Training loss: 0.23705070, Validation loss: 0.25568278, Gradient norm: 1.47382312
INFO:root:[   14] Training loss: 0.23405330, Validation loss: 0.24271226, Gradient norm: 1.45541175
INFO:root:[   15] Training loss: 0.23087023, Validation loss: 0.23965605, Gradient norm: 1.42362241
INFO:root:[   16] Training loss: 0.22885804, Validation loss: 0.24111689, Gradient norm: 1.45696191
INFO:root:[   17] Training loss: 0.22659133, Validation loss: 0.23798897, Gradient norm: 1.45775636
INFO:root:[   18] Training loss: 0.22495372, Validation loss: 0.23202431, Gradient norm: 1.50294766
INFO:root:[   19] Training loss: 0.22306346, Validation loss: 0.23712802, Gradient norm: 1.44620486
INFO:root:[   20] Training loss: 0.22081281, Validation loss: 0.23202014, Gradient norm: 1.38398406
INFO:root:[   21] Training loss: 0.21961239, Validation loss: 0.23491755, Gradient norm: 1.41556689
INFO:root:[   22] Training loss: 0.21847032, Validation loss: 0.23385868, Gradient norm: 1.43334391
INFO:root:[   23] Training loss: 0.21705455, Validation loss: 0.23833159, Gradient norm: 1.40383093
INFO:root:[   24] Training loss: 0.21590793, Validation loss: 0.22719181, Gradient norm: 1.38820668
INFO:root:[   25] Training loss: 0.21491589, Validation loss: 0.22649500, Gradient norm: 1.39872322
INFO:root:[   26] Training loss: 0.21286485, Validation loss: 0.22816082, Gradient norm: 1.31548883
INFO:root:[   27] Training loss: 0.21268446, Validation loss: 0.22273991, Gradient norm: 1.42800860
INFO:root:[   28] Training loss: 0.21206061, Validation loss: 0.22245430, Gradient norm: 1.41673003
INFO:root:[   29] Training loss: 0.21007445, Validation loss: 0.22334498, Gradient norm: 1.29328330
INFO:root:[   30] Training loss: 0.20948930, Validation loss: 0.22237285, Gradient norm: 1.33631435
INFO:root:[   31] Training loss: 0.20848074, Validation loss: 0.22223403, Gradient norm: 1.29234681
INFO:root:[   32] Training loss: 0.20839905, Validation loss: 0.22463010, Gradient norm: 1.38543600
INFO:root:[   33] Training loss: 0.20656527, Validation loss: 0.22063813, Gradient norm: 1.26697163
INFO:root:[   34] Training loss: 0.20645573, Validation loss: 0.22105640, Gradient norm: 1.33969077
INFO:root:[   35] Training loss: 0.20567284, Validation loss: 0.21787459, Gradient norm: 1.33449910
INFO:root:[   36] Training loss: 0.20481986, Validation loss: 0.21913027, Gradient norm: 1.32240643
INFO:root:[   37] Training loss: 0.20491016, Validation loss: 0.22038477, Gradient norm: 1.39740158
INFO:root:[   38] Training loss: 0.20336858, Validation loss: 0.21752780, Gradient norm: 1.27961884
INFO:root:[   39] Training loss: 0.20319500, Validation loss: 0.22211427, Gradient norm: 1.31836234
INFO:root:[   40] Training loss: 0.20251296, Validation loss: 0.21507088, Gradient norm: 1.28301598
INFO:root:[   41] Training loss: 0.20191802, Validation loss: 0.21605602, Gradient norm: 1.27958911
INFO:root:[   42] Training loss: 0.20193642, Validation loss: 0.21772552, Gradient norm: 1.32573866
INFO:root:[   43] Training loss: 0.20115078, Validation loss: 0.22027908, Gradient norm: 1.28166545
INFO:root:[   44] Training loss: 0.20064011, Validation loss: 0.21827199, Gradient norm: 1.28996756
INFO:root:[   45] Training loss: 0.20003535, Validation loss: 0.22159752, Gradient norm: 1.26206207
INFO:root:[   46] Training loss: 0.19977772, Validation loss: 0.21607847, Gradient norm: 1.28738945
INFO:root:[   47] Training loss: 0.19898837, Validation loss: 0.21560211, Gradient norm: 1.25125032
INFO:root:[   48] Training loss: 0.19875089, Validation loss: 0.21554993, Gradient norm: 1.28675850
INFO:root:[   49] Training loss: 0.19849294, Validation loss: 0.22344287, Gradient norm: 1.29061969
INFO:root:[   50] Training loss: 0.19790986, Validation loss: 0.21070500, Gradient norm: 1.26035927
INFO:root:[   51] Training loss: 0.19812155, Validation loss: 0.21574467, Gradient norm: 1.32994837
INFO:root:[   52] Training loss: 0.19720232, Validation loss: 0.21268224, Gradient norm: 1.25537942
INFO:root:[   53] Training loss: 0.19655604, Validation loss: 0.21029417, Gradient norm: 1.22822786
INFO:root:[   54] Training loss: 0.19674592, Validation loss: 0.21412209, Gradient norm: 1.31510122
INFO:root:[   55] Training loss: 0.19622775, Validation loss: 0.21426385, Gradient norm: 1.24363580
INFO:root:[   56] Training loss: 0.19591157, Validation loss: 0.21519487, Gradient norm: 1.23724103
INFO:root:[   57] Training loss: 0.19564536, Validation loss: 0.20892708, Gradient norm: 1.29221317
INFO:root:[   58] Training loss: 0.19523555, Validation loss: 0.21146413, Gradient norm: 1.22598285
INFO:root:[   59] Training loss: 0.19542335, Validation loss: 0.21574244, Gradient norm: 1.31171435
INFO:root:[   60] Training loss: 0.19510465, Validation loss: 0.21060669, Gradient norm: 1.29006587
INFO:root:[   61] Training loss: 0.19462892, Validation loss: 0.20993458, Gradient norm: 1.26572524
INFO:root:[   62] Training loss: 0.19434664, Validation loss: 0.20941368, Gradient norm: 1.29931772
INFO:root:[   63] Training loss: 0.19421307, Validation loss: 0.21291316, Gradient norm: 1.27541775
INFO:root:[   64] Training loss: 0.19395375, Validation loss: 0.20672617, Gradient norm: 1.25521476
INFO:root:[   65] Training loss: 0.19347565, Validation loss: 0.20768309, Gradient norm: 1.24609121
INFO:root:[   66] Training loss: 0.19308391, Validation loss: 0.20904770, Gradient norm: 1.20042507
INFO:root:[   67] Training loss: 0.19315765, Validation loss: 0.20934889, Gradient norm: 1.29037049
INFO:root:[   68] Training loss: 0.19265295, Validation loss: 0.20504720, Gradient norm: 1.22558856
INFO:root:[   69] Training loss: 0.19245682, Validation loss: 0.21344929, Gradient norm: 1.21791018
INFO:root:[   70] Training loss: 0.19233841, Validation loss: 0.20722463, Gradient norm: 1.29739368
INFO:root:[   71] Training loss: 0.19215797, Validation loss: 0.20532872, Gradient norm: 1.21407845
INFO:root:[   72] Training loss: 0.19189388, Validation loss: 0.20645093, Gradient norm: 1.25669774
INFO:root:[   73] Training loss: 0.19191671, Validation loss: 0.21296718, Gradient norm: 1.26270447
INFO:root:[   74] Training loss: 0.19153011, Validation loss: 0.21110043, Gradient norm: 1.24661104
INFO:root:[   75] Training loss: 0.19169698, Validation loss: 0.20395995, Gradient norm: 1.28638092
INFO:root:[   76] Training loss: 0.19070233, Validation loss: 0.20745801, Gradient norm: 1.19127302
INFO:root:[   77] Training loss: 0.19102568, Validation loss: 0.21059066, Gradient norm: 1.25380918
INFO:root:[   78] Training loss: 0.19105963, Validation loss: 0.20948096, Gradient norm: 1.27947462
INFO:root:[   79] Training loss: 0.19046447, Validation loss: 0.20731321, Gradient norm: 1.20124469
INFO:root:[   80] Training loss: 0.19066749, Validation loss: 0.20465950, Gradient norm: 1.26608071
INFO:root:[   81] Training loss: 0.19036966, Validation loss: 0.21147166, Gradient norm: 1.23790031
INFO:root:[   82] Training loss: 0.18996618, Validation loss: 0.20938546, Gradient norm: 1.23779246
INFO:root:[   83] Training loss: 0.19010663, Validation loss: 0.20419673, Gradient norm: 1.24030354
INFO:root:[   84] Training loss: 0.18991151, Validation loss: 0.20461028, Gradient norm: 1.24129869
INFO:root:[   85] Training loss: 0.18953005, Validation loss: 0.20242693, Gradient norm: 1.22116463
INFO:root:[   86] Training loss: 0.18942853, Validation loss: 0.20717596, Gradient norm: 1.26865914
INFO:root:[   87] Training loss: 0.18991709, Validation loss: 0.20309867, Gradient norm: 1.28074971
INFO:root:[   88] Training loss: 0.18942779, Validation loss: 0.20204897, Gradient norm: 1.27177285
INFO:root:[   89] Training loss: 0.18862037, Validation loss: 0.20208337, Gradient norm: 1.20481829
INFO:root:[   90] Training loss: 0.18877053, Validation loss: 0.20203596, Gradient norm: 1.21612542
INFO:root:[   91] Training loss: 0.18898832, Validation loss: 0.20238579, Gradient norm: 1.24527318
INFO:root:[   92] Training loss: 0.18829439, Validation loss: 0.20643605, Gradient norm: 1.20830741
INFO:root:[   93] Training loss: 0.18872697, Validation loss: 0.20436804, Gradient norm: 1.26787239
INFO:root:[   94] Training loss: 0.18846751, Validation loss: 0.21009182, Gradient norm: 1.22039359
INFO:root:[   95] Training loss: 0.18850545, Validation loss: 0.20057491, Gradient norm: 1.27185982
INFO:root:[   96] Training loss: 0.18793807, Validation loss: 0.20833290, Gradient norm: 1.22319668
INFO:root:[   97] Training loss: 0.18772433, Validation loss: 0.20329480, Gradient norm: 1.19388422
INFO:root:[   98] Training loss: 0.18775619, Validation loss: 0.20675965, Gradient norm: 1.22164325
INFO:root:[   99] Training loss: 0.18757757, Validation loss: 0.20415631, Gradient norm: 1.22647324
INFO:root:[  100] Training loss: 0.18770382, Validation loss: 0.20503499, Gradient norm: 1.25475582
INFO:root:[  101] Training loss: 0.18736545, Validation loss: 0.20928678, Gradient norm: 1.25796864
INFO:root:[  102] Training loss: 0.18719746, Validation loss: 0.20185003, Gradient norm: 1.19275720
INFO:root:[  103] Training loss: 0.18712867, Validation loss: 0.20333729, Gradient norm: 1.22636298
INFO:root:[  104] Training loss: 0.18716021, Validation loss: 0.20147904, Gradient norm: 1.23990732
INFO:root:EP 104: Early stopping
INFO:root:Training the model took 47286.257s.
INFO:root:Emptying the cuda cache took 0.05s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.18336
INFO:root:EnergyScoreValidation: 0.14265
INFO:root:CoverageValidation: 0.63245
INFO:root:IntervalWidthValidation: 0.0383
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.30357
INFO:root:EnergyScoreTest: 0.25359
INFO:root:CoverageTest: 0.42843
INFO:root:IntervalWidthTest: 0.03068
INFO:root:###3 out of 3 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 364904448
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.25751967, Validation loss: 0.20671116, Gradient norm: 3.47014825
INFO:root:[    2] Training loss: 0.20914960, Validation loss: 0.19594381, Gradient norm: 2.17658353
INFO:root:[    3] Training loss: 0.20110639, Validation loss: 0.19763317, Gradient norm: 2.02717849
INFO:root:[    4] Training loss: 0.19409847, Validation loss: 0.18959258, Gradient norm: 1.66460652
INFO:root:[    5] Training loss: 0.18801287, Validation loss: 0.18890919, Gradient norm: 1.48041843
INFO:root:[    6] Training loss: 0.18115811, Validation loss: 0.18188995, Gradient norm: 1.36216614
INFO:root:[    7] Training loss: 0.17417164, Validation loss: 0.17539420, Gradient norm: 1.31746906
INFO:root:[    8] Training loss: 0.16774075, Validation loss: 0.16976428, Gradient norm: 1.22188389
INFO:root:[    9] Training loss: 0.16300589, Validation loss: 0.16808209, Gradient norm: 1.19109938
INFO:root:[   10] Training loss: 0.15777347, Validation loss: 0.16715086, Gradient norm: 1.14341631
INFO:root:[   11] Training loss: 0.15421844, Validation loss: 0.16625896, Gradient norm: 1.10413191
INFO:root:[   12] Training loss: 0.15137518, Validation loss: 0.16028345, Gradient norm: 1.15842648
INFO:root:[   13] Training loss: 0.14874066, Validation loss: 0.15838445, Gradient norm: 1.11007308
INFO:root:[   14] Training loss: 0.14678261, Validation loss: 0.15415157, Gradient norm: 1.10729613
INFO:root:[   15] Training loss: 0.14427897, Validation loss: 0.15448152, Gradient norm: 1.05551381
INFO:root:[   16] Training loss: 0.14293106, Validation loss: 0.15227614, Gradient norm: 1.12030824
INFO:root:[   17] Training loss: 0.14107883, Validation loss: 0.14995827, Gradient norm: 1.10443449
INFO:root:[   18] Training loss: 0.13956709, Validation loss: 0.15287690, Gradient norm: 1.03274464
INFO:root:[   19] Training loss: 0.13825412, Validation loss: 0.14944496, Gradient norm: 1.05552359
INFO:root:[   20] Training loss: 0.13715850, Validation loss: 0.14950711, Gradient norm: 1.05617573
INFO:root:[   21] Training loss: 0.13584263, Validation loss: 0.14676475, Gradient norm: 0.97453055
INFO:root:[   22] Training loss: 0.13483060, Validation loss: 0.14534091, Gradient norm: 0.99169600
INFO:root:[   23] Training loss: 0.13367266, Validation loss: 0.14828370, Gradient norm: 0.98938456
INFO:root:[   24] Training loss: 0.13328567, Validation loss: 0.14541913, Gradient norm: 1.05512487
INFO:root:[   25] Training loss: 0.13191832, Validation loss: 0.14272617, Gradient norm: 0.94580268
INFO:root:[   26] Training loss: 0.13142745, Validation loss: 0.14282468, Gradient norm: 0.98167607
INFO:root:[   27] Training loss: 0.13031088, Validation loss: 0.14307533, Gradient norm: 0.95485326
INFO:root:[   28] Training loss: 0.12991839, Validation loss: 0.14164130, Gradient norm: 0.95316388
INFO:root:[   29] Training loss: 0.12926797, Validation loss: 0.14096091, Gradient norm: 0.99241606
INFO:root:[   30] Training loss: 0.12869435, Validation loss: 0.14134428, Gradient norm: 0.96165789
INFO:root:[   31] Training loss: 0.12792416, Validation loss: 0.14349165, Gradient norm: 0.95715256
INFO:root:[   32] Training loss: 0.12747655, Validation loss: 0.13985185, Gradient norm: 0.95218321
INFO:root:[   33] Training loss: 0.12685965, Validation loss: 0.13963911, Gradient norm: 0.97049218
INFO:root:[   34] Training loss: 0.12622803, Validation loss: 0.14582339, Gradient norm: 0.94387426
INFO:root:[   35] Training loss: 0.12540839, Validation loss: 0.14270428, Gradient norm: 0.87751412
INFO:root:[   36] Training loss: 0.12518796, Validation loss: 0.13593310, Gradient norm: 0.91876152
INFO:root:[   37] Training loss: 0.12482607, Validation loss: 0.13934145, Gradient norm: 0.93907729
INFO:root:[   38] Training loss: 0.12452033, Validation loss: 0.13570282, Gradient norm: 0.94214684
INFO:root:[   39] Training loss: 0.12391061, Validation loss: 0.13633469, Gradient norm: 0.93997041
INFO:root:[   40] Training loss: 0.12366630, Validation loss: 0.13609678, Gradient norm: 0.89106895
INFO:root:[   41] Training loss: 0.12343367, Validation loss: 0.13562990, Gradient norm: 0.91998935
INFO:root:[   42] Training loss: 0.12252850, Validation loss: 0.13412943, Gradient norm: 0.91679911
INFO:root:[   43] Training loss: 0.12246082, Validation loss: 0.13829413, Gradient norm: 0.95219049
INFO:root:[   44] Training loss: 0.12206208, Validation loss: 0.13370996, Gradient norm: 0.88612905
INFO:root:[   45] Training loss: 0.12136280, Validation loss: 0.13514991, Gradient norm: 0.85543588
INFO:root:[   46] Training loss: 0.12163661, Validation loss: 0.13449532, Gradient norm: 0.94533716
INFO:root:[   47] Training loss: 0.12111867, Validation loss: 0.13303709, Gradient norm: 0.90323230
INFO:root:[   48] Training loss: 0.12107689, Validation loss: 0.13367487, Gradient norm: 0.93080452
INFO:root:[   49] Training loss: 0.12072823, Validation loss: 0.13463705, Gradient norm: 0.91036828
INFO:root:[   50] Training loss: 0.12005161, Validation loss: 0.13573731, Gradient norm: 0.87875422
INFO:root:[   51] Training loss: 0.11989950, Validation loss: 0.13584513, Gradient norm: 0.86903593
INFO:root:[   52] Training loss: 0.11946455, Validation loss: 0.13852432, Gradient norm: 0.86580117
INFO:root:[   53] Training loss: 0.11940575, Validation loss: 0.13338120, Gradient norm: 0.87158296
INFO:root:[   54] Training loss: 0.11941283, Validation loss: 0.13512673, Gradient norm: 0.91628702
INFO:root:[   55] Training loss: 0.11879017, Validation loss: 0.13083815, Gradient norm: 0.86778404
INFO:root:[   56] Training loss: 0.11883667, Validation loss: 0.13039515, Gradient norm: 0.89983928
INFO:root:[   57] Training loss: 0.11808663, Validation loss: 0.12986840, Gradient norm: 0.85570818
INFO:root:[   58] Training loss: 0.11827941, Validation loss: 0.13551398, Gradient norm: 0.86519222
INFO:root:[   59] Training loss: 0.11800444, Validation loss: 0.13489052, Gradient norm: 0.88180179
INFO:root:[   60] Training loss: 0.11763604, Validation loss: 0.12941184, Gradient norm: 0.88480217
INFO:root:[   61] Training loss: 0.11781588, Validation loss: 0.12968319, Gradient norm: 0.88549984
INFO:root:[   62] Training loss: 0.11721207, Validation loss: 0.13128647, Gradient norm: 0.87674662
INFO:root:[   63] Training loss: 0.11706921, Validation loss: 0.12978858, Gradient norm: 0.84248664
INFO:root:[   64] Training loss: 0.11690716, Validation loss: 0.12990770, Gradient norm: 0.86591560
INFO:root:[   65] Training loss: 0.11663678, Validation loss: 0.12856114, Gradient norm: 0.81084011
INFO:root:[   66] Training loss: 0.11681134, Validation loss: 0.13052995, Gradient norm: 0.91027593
INFO:root:[   67] Training loss: 0.11645563, Validation loss: 0.12901136, Gradient norm: 0.88131160
INFO:root:[   68] Training loss: 0.11613843, Validation loss: 0.12923980, Gradient norm: 0.87575858
INFO:root:[   69] Training loss: 0.11601221, Validation loss: 0.13071320, Gradient norm: 0.82643983
INFO:root:[   70] Training loss: 0.11596945, Validation loss: 0.12811126, Gradient norm: 0.88334787
INFO:root:[   71] Training loss: 0.11582232, Validation loss: 0.13208620, Gradient norm: 0.86663028
INFO:root:[   72] Training loss: 0.11547906, Validation loss: 0.13217636, Gradient norm: 0.85454588
INFO:root:[   73] Training loss: 0.11522082, Validation loss: 0.12810195, Gradient norm: 0.80399863
INFO:root:[   74] Training loss: 0.11512125, Validation loss: 0.12675782, Gradient norm: 0.85778862
INFO:root:[   75] Training loss: 0.11516698, Validation loss: 0.12798146, Gradient norm: 0.86868387
INFO:root:[   76] Training loss: 0.11494872, Validation loss: 0.12963510, Gradient norm: 0.86181619
INFO:root:[   77] Training loss: 0.11476318, Validation loss: 0.12835105, Gradient norm: 0.88808898
INFO:root:[   78] Training loss: 0.11434919, Validation loss: 0.12702044, Gradient norm: 0.82360477
INFO:root:[   79] Training loss: 0.11477820, Validation loss: 0.12918733, Gradient norm: 0.88090683
INFO:root:[   80] Training loss: 0.11443657, Validation loss: 0.12715043, Gradient norm: 0.85777217
INFO:root:[   81] Training loss: 0.11420104, Validation loss: 0.12770234, Gradient norm: 0.87916622
INFO:root:[   82] Training loss: 0.11388432, Validation loss: 0.12728009, Gradient norm: 0.81847329
INFO:root:[   83] Training loss: 0.11369144, Validation loss: 0.12619559, Gradient norm: 0.83178590
INFO:root:[   84] Training loss: 0.11394939, Validation loss: 0.12641757, Gradient norm: 0.87260007
INFO:root:[   85] Training loss: 0.11341659, Validation loss: 0.12492983, Gradient norm: 0.80806629
INFO:root:[   86] Training loss: 0.11352296, Validation loss: 0.12650388, Gradient norm: 0.87175285
INFO:root:[   87] Training loss: 0.11339011, Validation loss: 0.12608093, Gradient norm: 0.86412556
INFO:root:[   88] Training loss: 0.11318473, Validation loss: 0.12545980, Gradient norm: 0.81573358
INFO:root:[   89] Training loss: 0.11317417, Validation loss: 0.12731117, Gradient norm: 0.87165127
INFO:root:[   90] Training loss: 0.11318189, Validation loss: 0.12792663, Gradient norm: 0.85276613
INFO:root:[   91] Training loss: 0.11293898, Validation loss: 0.12523154, Gradient norm: 0.87027611
INFO:root:[   92] Training loss: 0.11244727, Validation loss: 0.12907865, Gradient norm: 0.81491604
INFO:root:[   93] Training loss: 0.11232344, Validation loss: 0.12571782, Gradient norm: 0.80512218
INFO:root:[   94] Training loss: 0.11257382, Validation loss: 0.12550579, Gradient norm: 0.86541479
INFO:root:[   95] Training loss: 0.11216467, Validation loss: 0.12412014, Gradient norm: 0.80598786
INFO:root:[   96] Training loss: 0.11207870, Validation loss: 0.12390682, Gradient norm: 0.82892223
INFO:root:[   97] Training loss: 0.11209463, Validation loss: 0.12620761, Gradient norm: 0.85860981
INFO:root:[   98] Training loss: 0.11221604, Validation loss: 0.12679635, Gradient norm: 0.87610463
