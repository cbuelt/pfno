INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.84054751, Validation loss: 0.74378716, Gradient norm: 4.79246172
INFO:root:[    2] Training loss: 0.73459191, Validation loss: 0.72597918, Gradient norm: 2.38425875
INFO:root:[    3] Training loss: 0.72030867, Validation loss: 0.71765851, Gradient norm: 2.30674595
INFO:root:[    4] Training loss: 0.71654306, Validation loss: 0.72172976, Gradient norm: 2.61877626
INFO:root:[    5] Training loss: 0.71324476, Validation loss: 0.71487715, Gradient norm: 2.89962613
INFO:root:[    6] Training loss: 0.70904846, Validation loss: 0.70193858, Gradient norm: 2.90556164
INFO:root:[    7] Training loss: 0.70366180, Validation loss: 0.70206407, Gradient norm: 2.02209469
INFO:root:[    8] Training loss: 0.70178663, Validation loss: 0.70440335, Gradient norm: 1.39342083
INFO:root:[    9] Training loss: 0.70087927, Validation loss: 0.69816923, Gradient norm: 1.40462641
INFO:root:[   10] Training loss: 0.69889667, Validation loss: 0.69881708, Gradient norm: 1.29329563
INFO:root:[   11] Training loss: 0.69778557, Validation loss: 0.69976909, Gradient norm: 1.26839036
INFO:root:[   12] Training loss: 0.69763166, Validation loss: 0.70047030, Gradient norm: 1.23329931
INFO:root:[   13] Training loss: 0.69672650, Validation loss: 0.69453490, Gradient norm: 1.27860864
INFO:root:[   14] Training loss: 0.69643804, Validation loss: 0.69747863, Gradient norm: 1.07162434
INFO:root:[   15] Training loss: 0.69558704, Validation loss: 0.69287483, Gradient norm: 1.22558846
INFO:root:[   16] Training loss: 0.69433977, Validation loss: 0.69783861, Gradient norm: 1.05684784
INFO:root:[   17] Training loss: 0.69334870, Validation loss: 0.69381715, Gradient norm: 1.01678135
INFO:root:[   18] Training loss: 0.69350936, Validation loss: 0.69471254, Gradient norm: 1.09663717
INFO:root:[   19] Training loss: 0.69250018, Validation loss: 0.69349486, Gradient norm: 1.08193222
INFO:root:[   20] Training loss: 0.69166165, Validation loss: 0.69033431, Gradient norm: 0.94371276
INFO:root:[   21] Training loss: 0.69129360, Validation loss: 0.69346730, Gradient norm: 0.97168248
INFO:root:[   22] Training loss: 0.69017002, Validation loss: 0.69145495, Gradient norm: 0.85366442
INFO:root:[   23] Training loss: 0.68961424, Validation loss: 0.69091866, Gradient norm: 0.91411315
INFO:root:[   24] Training loss: 0.68838801, Validation loss: 0.68854005, Gradient norm: 0.70705610
INFO:root:[   25] Training loss: 0.68839872, Validation loss: 0.68634342, Gradient norm: 0.78672734
INFO:root:[   26] Training loss: 0.68772898, Validation loss: 0.68933167, Gradient norm: 0.80234724
INFO:root:[   27] Training loss: 0.68653066, Validation loss: 0.68628167, Gradient norm: 0.76244461
INFO:root:[   28] Training loss: 0.68655911, Validation loss: 0.68586867, Gradient norm: 0.71962100
INFO:root:[   29] Training loss: 0.68636696, Validation loss: 0.68531769, Gradient norm: 0.77841623
INFO:root:[   30] Training loss: 0.68488920, Validation loss: 0.68792021, Gradient norm: 0.69958382
INFO:root:[   31] Training loss: 0.68569426, Validation loss: 0.68561125, Gradient norm: 0.77273316
INFO:root:[   32] Training loss: 0.68392666, Validation loss: 0.68499371, Gradient norm: 0.74048718
INFO:root:[   33] Training loss: 0.68330191, Validation loss: 0.68880554, Gradient norm: 0.71443225
INFO:root:[   34] Training loss: 0.68243667, Validation loss: 0.68376497, Gradient norm: 0.71999690
INFO:root:[   35] Training loss: 0.68261858, Validation loss: 0.68165514, Gradient norm: 0.72446876
INFO:root:[   36] Training loss: 0.68287594, Validation loss: 0.68344127, Gradient norm: 0.75786441
INFO:root:[   37] Training loss: 0.68094318, Validation loss: 0.68488917, Gradient norm: 0.69939491
INFO:root:[   38] Training loss: 0.68115084, Validation loss: 0.68462294, Gradient norm: 0.64720033
INFO:root:[   39] Training loss: 0.68073111, Validation loss: 0.68289198, Gradient norm: 0.68528413
INFO:root:[   40] Training loss: 0.67952625, Validation loss: 0.68000099, Gradient norm: 0.68083304
INFO:root:[   41] Training loss: 0.68001464, Validation loss: 0.67988474, Gradient norm: 0.71679622
INFO:root:[   42] Training loss: 0.67955865, Validation loss: 0.68275998, Gradient norm: 0.71555562
INFO:root:[   43] Training loss: 0.67910050, Validation loss: 0.68116915, Gradient norm: 0.70305620
INFO:root:[   44] Training loss: 0.67898002, Validation loss: 0.67732294, Gradient norm: 0.79932886
INFO:root:[   45] Training loss: 0.67727523, Validation loss: 0.67929037, Gradient norm: 0.67216998
INFO:root:[   46] Training loss: 0.67753521, Validation loss: 0.67768137, Gradient norm: 0.68370868
INFO:root:[   47] Training loss: 0.67750863, Validation loss: 0.67716837, Gradient norm: 0.74642974
INFO:root:[   48] Training loss: 0.67661493, Validation loss: 0.67787071, Gradient norm: 0.72099422
INFO:root:[   49] Training loss: 0.67602663, Validation loss: 0.67775031, Gradient norm: 0.67044516
INFO:root:[   50] Training loss: 0.67615869, Validation loss: 0.68008392, Gradient norm: 0.75229367
INFO:root:[   51] Training loss: 0.67572059, Validation loss: 0.67760283, Gradient norm: 0.70017608
INFO:root:[   52] Training loss: 0.67472140, Validation loss: 0.67701691, Gradient norm: 0.69945677
INFO:root:[   53] Training loss: 0.67466876, Validation loss: 0.67497424, Gradient norm: 0.75889513
INFO:root:[   54] Training loss: 0.67454259, Validation loss: 0.67681398, Gradient norm: 0.66496691
INFO:root:[   55] Training loss: 0.67464381, Validation loss: 0.67628537, Gradient norm: 0.74666078
INFO:root:[   56] Training loss: 0.67364063, Validation loss: 0.67450868, Gradient norm: 0.68476041
INFO:root:[   57] Training loss: 0.67332874, Validation loss: 0.67585965, Gradient norm: 0.65549747
INFO:root:[   58] Training loss: 0.67380284, Validation loss: 0.67587348, Gradient norm: 0.79491311
INFO:root:[   59] Training loss: 0.67378105, Validation loss: 0.67432459, Gradient norm: 0.73864631
INFO:root:[   60] Training loss: 0.67311806, Validation loss: 0.67745001, Gradient norm: 0.68230341
INFO:root:[   61] Training loss: 0.67321909, Validation loss: 0.67327517, Gradient norm: 0.74889954
INFO:root:[   62] Training loss: 0.67265373, Validation loss: 0.67432722, Gradient norm: 0.75490048
INFO:root:[   63] Training loss: 0.67175579, Validation loss: 0.67455270, Gradient norm: 0.70608728
INFO:root:[   64] Training loss: 0.67172750, Validation loss: 0.67681518, Gradient norm: 0.72856399
INFO:root:[   65] Training loss: 0.67138947, Validation loss: 0.67135659, Gradient norm: 0.80125934
INFO:root:[   66] Training loss: 0.67170550, Validation loss: 0.67250207, Gradient norm: 0.74147380
INFO:root:[   67] Training loss: 0.67181146, Validation loss: 0.67274334, Gradient norm: 0.66872986
INFO:root:[   68] Training loss: 0.67186316, Validation loss: 0.67174217, Gradient norm: 0.74832977
INFO:root:[   69] Training loss: 0.67034571, Validation loss: 0.67135656, Gradient norm: 0.63695960
INFO:root:[   70] Training loss: 0.67132173, Validation loss: 0.67311758, Gradient norm: 0.79313773
INFO:root:[   71] Training loss: 0.67100522, Validation loss: 0.67321937, Gradient norm: 0.79326357
INFO:root:[   72] Training loss: 0.66948074, Validation loss: 0.67236314, Gradient norm: 0.62365412
INFO:root:[   73] Training loss: 0.67056038, Validation loss: 0.67375572, Gradient norm: 0.82267169
INFO:root:[   74] Training loss: 0.67039975, Validation loss: 0.67034344, Gradient norm: 0.71769863
INFO:root:[   75] Training loss: 0.66904270, Validation loss: 0.67189636, Gradient norm: 0.64777381
INFO:root:[   76] Training loss: 0.67042637, Validation loss: 0.66874421, Gradient norm: 0.77315141
INFO:root:[   77] Training loss: 0.67008705, Validation loss: 0.67112723, Gradient norm: 0.78888869
INFO:root:[   78] Training loss: 0.66885621, Validation loss: 0.67000251, Gradient norm: 0.71224608
INFO:root:[   79] Training loss: 0.66936719, Validation loss: 0.67017684, Gradient norm: 0.74568199
INFO:root:[   80] Training loss: 0.66950768, Validation loss: 0.67116199, Gradient norm: 0.70740918
INFO:root:[   81] Training loss: 0.66809000, Validation loss: 0.67116076, Gradient norm: 0.81338919
INFO:root:[   82] Training loss: 0.66844408, Validation loss: 0.66863126, Gradient norm: 0.74891391
INFO:root:[   83] Training loss: 0.66756931, Validation loss: 0.66999354, Gradient norm: 0.74761326
INFO:root:[   84] Training loss: 0.66835624, Validation loss: 0.66715114, Gradient norm: 0.74677444
INFO:root:[   85] Training loss: 0.66723085, Validation loss: 0.67182641, Gradient norm: 0.74216103
INFO:root:[   86] Training loss: 0.66681479, Validation loss: 0.66789814, Gradient norm: 0.73795239
INFO:root:[   87] Training loss: 0.66793342, Validation loss: 0.67028848, Gradient norm: 0.80778005
INFO:root:[   88] Training loss: 0.66635679, Validation loss: 0.66997224, Gradient norm: 0.76890439
INFO:root:[   89] Training loss: 0.66725695, Validation loss: 0.66998732, Gradient norm: 0.81031542
INFO:root:[   90] Training loss: 0.66665621, Validation loss: 0.66702504, Gradient norm: 0.72358611
INFO:root:[   91] Training loss: 0.66677079, Validation loss: 0.66916067, Gradient norm: 0.72816839
INFO:root:[   92] Training loss: 0.66684077, Validation loss: 0.66754499, Gradient norm: 0.77615994
INFO:root:[   93] Training loss: 0.66637388, Validation loss: 0.66909878, Gradient norm: 0.73779678
INFO:root:[   94] Training loss: 0.66553692, Validation loss: 0.66776512, Gradient norm: 0.63373074
INFO:root:[   95] Training loss: 0.66586925, Validation loss: 0.66568518, Gradient norm: 0.82185068
INFO:root:[   96] Training loss: 0.66600400, Validation loss: 0.66626387, Gradient norm: 0.80713813
INFO:root:[   97] Training loss: 0.66530591, Validation loss: 0.66531275, Gradient norm: 0.72713313
INFO:root:[   98] Training loss: 0.66552313, Validation loss: 0.67008287, Gradient norm: 0.74508823
INFO:root:[   99] Training loss: 0.66548121, Validation loss: 0.66664415, Gradient norm: 0.77362456
INFO:root:[  100] Training loss: 0.66510191, Validation loss: 0.66764410, Gradient norm: 0.77104254
INFO:root:[  101] Training loss: 0.66567071, Validation loss: 0.66779448, Gradient norm: 0.73456995
INFO:root:[  102] Training loss: 0.66580655, Validation loss: 0.66867698, Gradient norm: 0.79362583
INFO:root:[  103] Training loss: 0.66470772, Validation loss: 0.66745856, Gradient norm: 0.73570873
INFO:root:[  104] Training loss: 0.66533780, Validation loss: 0.66627832, Gradient norm: 0.74546918
INFO:root:[  105] Training loss: 0.66503934, Validation loss: 0.66627607, Gradient norm: 0.70624246
INFO:root:[  106] Training loss: 0.66471011, Validation loss: 0.66553146, Gradient norm: 0.76092596
INFO:root:EP 106: Early stopping
INFO:root:Training the model took 6717.946s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91855
INFO:root:EnergyScoreTrain: 0.66356
INFO:root:CRPSTrain: 0.61887
INFO:root:Gaussian NLLTrain: 2.7977
INFO:root:CoverageTrain: 0.74172
INFO:root:IntervalWidthTrain: 3.35926
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.92092
INFO:root:EnergyScoreValidation: 0.66542
INFO:root:CRPSValidation: 0.6205
INFO:root:Gaussian NLLValidation: 2.79207
INFO:root:CoverageValidation: 0.74165
INFO:root:IntervalWidthValidation: 3.36253
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.92067
INFO:root:EnergyScoreTest: 0.66538
INFO:root:CRPSTest: 0.62052
INFO:root:Gaussian NLLTest: 2.81258
INFO:root:CoverageTest: 0.74103
INFO:root:IntervalWidthTest: 3.35902
INFO:root:###2 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 310378496
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.76250815, Validation loss: 0.72610735, Gradient norm: 8.10872271
INFO:root:[    2] Training loss: 0.71771220, Validation loss: 0.72215342, Gradient norm: 7.15456429
INFO:root:[    3] Training loss: 0.71036564, Validation loss: 0.70557062, Gradient norm: 6.06970313
INFO:root:[    4] Training loss: 0.70709656, Validation loss: 0.70766101, Gradient norm: 5.59697156
INFO:root:[    5] Training loss: 0.70593589, Validation loss: 0.70114287, Gradient norm: 6.16276335
INFO:root:[    6] Training loss: 0.70357209, Validation loss: 0.70377802, Gradient norm: 5.12515003
INFO:root:[    7] Training loss: 0.70228918, Validation loss: 0.70618269, Gradient norm: 4.98853571
INFO:root:[    8] Training loss: 0.70120097, Validation loss: 0.70031418, Gradient norm: 5.20772232
INFO:root:[    9] Training loss: 0.69981723, Validation loss: 0.69875099, Gradient norm: 4.76327141
INFO:root:[   10] Training loss: 0.69854953, Validation loss: 0.69856042, Gradient norm: 4.27411670
INFO:root:[   11] Training loss: 0.69794445, Validation loss: 0.69885053, Gradient norm: 4.65791000
INFO:root:[   12] Training loss: 0.69748638, Validation loss: 0.69786412, Gradient norm: 4.84259970
INFO:root:[   13] Training loss: 0.69675834, Validation loss: 0.69810810, Gradient norm: 4.80052653
INFO:root:[   14] Training loss: 0.69576331, Validation loss: 0.69826972, Gradient norm: 4.36117155
INFO:root:[   15] Training loss: 0.69421064, Validation loss: 0.69382896, Gradient norm: 3.74202975
INFO:root:[   16] Training loss: 0.69264828, Validation loss: 0.69413036, Gradient norm: 3.80869321
INFO:root:[   17] Training loss: 0.69152563, Validation loss: 0.69161213, Gradient norm: 3.77301749
INFO:root:[   18] Training loss: 0.69063112, Validation loss: 0.69281489, Gradient norm: 3.59462388
INFO:root:[   19] Training loss: 0.69002600, Validation loss: 0.68960466, Gradient norm: 3.20531908
INFO:root:[   20] Training loss: 0.68920532, Validation loss: 0.68964963, Gradient norm: 3.27682612
INFO:root:[   21] Training loss: 0.68881261, Validation loss: 0.69101491, Gradient norm: 3.24994597
INFO:root:[   22] Training loss: 0.68852624, Validation loss: 0.68941905, Gradient norm: 3.21722219
INFO:root:[   23] Training loss: 0.68754108, Validation loss: 0.68748395, Gradient norm: 3.16915776
INFO:root:[   24] Training loss: 0.68742805, Validation loss: 0.68852522, Gradient norm: 3.15768435
INFO:root:[   25] Training loss: 0.68642685, Validation loss: 0.68644510, Gradient norm: 3.09163616
INFO:root:[   26] Training loss: 0.68598699, Validation loss: 0.68532753, Gradient norm: 3.02605388
INFO:root:[   27] Training loss: 0.68534483, Validation loss: 0.68603753, Gradient norm: 3.16497975
INFO:root:[   28] Training loss: 0.68482729, Validation loss: 0.68494461, Gradient norm: 3.19644817
INFO:root:[   29] Training loss: 0.68415353, Validation loss: 0.68446901, Gradient norm: 3.09641224
INFO:root:[   30] Training loss: 0.68346959, Validation loss: 0.68537265, Gradient norm: 3.00118179
INFO:root:[   31] Training loss: 0.68325340, Validation loss: 0.68450110, Gradient norm: 2.93860258
INFO:root:[   32] Training loss: 0.68275488, Validation loss: 0.68275453, Gradient norm: 2.89266656
INFO:root:[   33] Training loss: 0.68164432, Validation loss: 0.68439099, Gradient norm: 2.84638165
INFO:root:[   34] Training loss: 0.68134242, Validation loss: 0.68192000, Gradient norm: 2.99614569
INFO:root:[   35] Training loss: 0.68007278, Validation loss: 0.68073377, Gradient norm: 3.10672363
INFO:root:[   36] Training loss: 0.67965540, Validation loss: 0.68073684, Gradient norm: 3.15111445
INFO:root:[   37] Training loss: 0.67907673, Validation loss: 0.67879684, Gradient norm: 2.91915941
INFO:root:[   38] Training loss: 0.67862486, Validation loss: 0.68077678, Gradient norm: 2.66656181
INFO:root:[   39] Training loss: 0.67793143, Validation loss: 0.67986656, Gradient norm: 3.01738009
INFO:root:[   40] Training loss: 0.67772315, Validation loss: 0.67760353, Gradient norm: 2.67042561
INFO:root:[   41] Training loss: 0.67750607, Validation loss: 0.67814417, Gradient norm: 2.70334570
INFO:root:[   42] Training loss: 0.67649355, Validation loss: 0.67915181, Gradient norm: 2.72356497
INFO:root:[   43] Training loss: 0.67650102, Validation loss: 0.67570399, Gradient norm: 2.64087026
INFO:root:[   44] Training loss: 0.67537101, Validation loss: 0.68027163, Gradient norm: 2.66019574
INFO:root:[   45] Training loss: 0.67543312, Validation loss: 0.67678173, Gradient norm: 2.75201060
INFO:root:[   46] Training loss: 0.67499706, Validation loss: 0.67665330, Gradient norm: 2.69182214
INFO:root:[   47] Training loss: 0.67494472, Validation loss: 0.67655163, Gradient norm: 2.49182489
INFO:root:[   48] Training loss: 0.67443170, Validation loss: 0.67834559, Gradient norm: 2.49678623
INFO:root:[   49] Training loss: 0.67384961, Validation loss: 0.67529072, Gradient norm: 2.40478066
INFO:root:[   50] Training loss: 0.67394998, Validation loss: 0.67367834, Gradient norm: 2.68480761
INFO:root:[   51] Training loss: 0.67326969, Validation loss: 0.67554148, Gradient norm: 2.55831374
INFO:root:[   52] Training loss: 0.67338921, Validation loss: 0.67510967, Gradient norm: 2.59433641
INFO:root:[   53] Training loss: 0.67290290, Validation loss: 0.67315042, Gradient norm: 2.48229672
INFO:root:[   54] Training loss: 0.67291964, Validation loss: 0.67347297, Gradient norm: 2.44338461
INFO:root:[   55] Training loss: 0.67241883, Validation loss: 0.67331241, Gradient norm: 2.50401936
INFO:root:[   56] Training loss: 0.67203629, Validation loss: 0.67365364, Gradient norm: 2.46024317
INFO:root:[   57] Training loss: 0.67142842, Validation loss: 0.67610978, Gradient norm: 2.20900777
INFO:root:[   58] Training loss: 0.67088673, Validation loss: 0.67332675, Gradient norm: 2.32997975
INFO:root:[   59] Training loss: 0.67065656, Validation loss: 0.67258894, Gradient norm: 2.48454909
INFO:root:[   60] Training loss: 0.67058460, Validation loss: 0.67042296, Gradient norm: 2.37174576
INFO:root:[   61] Training loss: 0.67009426, Validation loss: 0.67276526, Gradient norm: 2.45691152
INFO:root:[   62] Training loss: 0.67007367, Validation loss: 0.67317489, Gradient norm: 2.43588009
INFO:root:[   63] Training loss: 0.66994413, Validation loss: 0.67099298, Gradient norm: 2.28167970
INFO:root:[   64] Training loss: 0.66893895, Validation loss: 0.66951450, Gradient norm: 2.51136222
INFO:root:[   65] Training loss: 0.66893567, Validation loss: 0.66915726, Gradient norm: 2.43318085
INFO:root:[   66] Training loss: 0.66858532, Validation loss: 0.67271371, Gradient norm: 2.27415030
INFO:root:[   67] Training loss: 0.66885798, Validation loss: 0.67384129, Gradient norm: 2.32604744
INFO:root:[   68] Training loss: 0.66826243, Validation loss: 0.67033127, Gradient norm: 2.35289542
INFO:root:[   69] Training loss: 0.66812677, Validation loss: 0.67021792, Gradient norm: 2.31387770
INFO:root:[   70] Training loss: 0.66804999, Validation loss: 0.66947659, Gradient norm: 2.27120807
INFO:root:[   71] Training loss: 0.66742062, Validation loss: 0.66762379, Gradient norm: 2.36419297
INFO:root:[   72] Training loss: 0.66767735, Validation loss: 0.67014825, Gradient norm: 2.22444264
INFO:root:[   73] Training loss: 0.66727776, Validation loss: 0.66850239, Gradient norm: 2.23326723
INFO:root:[   74] Training loss: 0.66733694, Validation loss: 0.67051324, Gradient norm: 2.35541351
INFO:root:[   75] Training loss: 0.66689492, Validation loss: 0.66734556, Gradient norm: 2.35573815
INFO:root:[   76] Training loss: 0.66713862, Validation loss: 0.67126519, Gradient norm: 2.29496459
INFO:root:[   77] Training loss: 0.66638414, Validation loss: 0.67016933, Gradient norm: 2.27429905
INFO:root:[   78] Training loss: 0.66664816, Validation loss: 0.66785832, Gradient norm: 2.34898449
INFO:root:[   79] Training loss: 0.66642037, Validation loss: 0.66802083, Gradient norm: 2.41536586
INFO:root:[   80] Training loss: 0.66616962, Validation loss: 0.66793188, Gradient norm: 2.24584336
INFO:root:[   81] Training loss: 0.66614341, Validation loss: 0.66830948, Gradient norm: 2.25992285
INFO:root:[   82] Training loss: 0.66590539, Validation loss: 0.66886646, Gradient norm: 2.10133408
INFO:root:[   83] Training loss: 0.66575261, Validation loss: 0.66615611, Gradient norm: 2.23318902
INFO:root:[   84] Training loss: 0.66554249, Validation loss: 0.66671675, Gradient norm: 2.15298554
INFO:root:[   85] Training loss: 0.66578470, Validation loss: 0.66827150, Gradient norm: 2.23541049
INFO:root:[   86] Training loss: 0.66567790, Validation loss: 0.66583941, Gradient norm: 2.18070079
INFO:root:[   87] Training loss: 0.66494944, Validation loss: 0.66660410, Gradient norm: 2.13706529
INFO:root:[   88] Training loss: 0.66535102, Validation loss: 0.66590549, Gradient norm: 2.27979475
INFO:root:[   89] Training loss: 0.66505311, Validation loss: 0.66586689, Gradient norm: 2.18807017
INFO:root:[   90] Training loss: 0.66496656, Validation loss: 0.66684236, Gradient norm: 2.19325926
INFO:root:[   91] Training loss: 0.66490396, Validation loss: 0.66726994, Gradient norm: 2.14818989
INFO:root:[   92] Training loss: 0.66479041, Validation loss: 0.66540029, Gradient norm: 2.18462837
INFO:root:[   93] Training loss: 0.66484020, Validation loss: 0.66636600, Gradient norm: 2.18552797
INFO:root:[   94] Training loss: 0.66451749, Validation loss: 0.66424491, Gradient norm: 2.15065561
INFO:root:[   95] Training loss: 0.66426599, Validation loss: 0.66728804, Gradient norm: 2.18308862
INFO:root:[   96] Training loss: 0.66476733, Validation loss: 0.66773706, Gradient norm: 2.20897668
INFO:root:[   97] Training loss: 0.66356766, Validation loss: 0.66775374, Gradient norm: 2.10362486
INFO:root:[   98] Training loss: 0.66435049, Validation loss: 0.66435208, Gradient norm: 2.26665420
INFO:root:[   99] Training loss: 0.66414998, Validation loss: 0.66671362, Gradient norm: 2.20850776
INFO:root:[  100] Training loss: 0.66415101, Validation loss: 0.66568506, Gradient norm: 2.02619475
INFO:root:[  101] Training loss: 0.66462894, Validation loss: 0.66796323, Gradient norm: 2.24253954
INFO:root:[  102] Training loss: 0.66363574, Validation loss: 0.66523461, Gradient norm: 2.07876138
INFO:root:[  103] Training loss: 0.66421061, Validation loss: 0.66816100, Gradient norm: 1.96659015
INFO:root:EP 103: Early stopping
INFO:root:Training the model took 6456.75s.
INFO:root:Emptying the cuda cache took 0.017s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91645
INFO:root:EnergyScoreTrain: 0.66177
INFO:root:CRPSTrain: 0.61087
INFO:root:Gaussian NLLTrain: 2.51006
INFO:root:CoverageTrain: 0.60807
INFO:root:IntervalWidthTrain: 1.86835
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91969
INFO:root:EnergyScoreValidation: 0.66429
INFO:root:CRPSValidation: 0.61358
INFO:root:Gaussian NLLValidation: 2.52151
INFO:root:CoverageValidation: 0.60591
INFO:root:IntervalWidthValidation: 1.86666
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91888
INFO:root:EnergyScoreTest: 0.66383
INFO:root:CRPSTest: 0.61323
INFO:root:Gaussian NLLTest: 2.52015
INFO:root:CoverageTest: 0.60647
INFO:root:IntervalWidthTest: 1.86866
INFO:root:###3 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.75416940, Validation loss: 0.72571777, Gradient norm: 6.07155195
INFO:root:[    2] Training loss: 0.72146651, Validation loss: 0.71559128, Gradient norm: 4.56967401
INFO:root:[    3] Training loss: 0.71589970, Validation loss: 0.71418588, Gradient norm: 3.98439027
INFO:root:[    4] Training loss: 0.71175408, Validation loss: 0.70587610, Gradient norm: 4.23069894
INFO:root:[    5] Training loss: 0.70577550, Validation loss: 0.70835798, Gradient norm: 4.14297077
INFO:root:[    6] Training loss: 0.70262207, Validation loss: 0.70422286, Gradient norm: 3.60311132
INFO:root:[    7] Training loss: 0.70186070, Validation loss: 0.69986911, Gradient norm: 3.70542181
INFO:root:[    8] Training loss: 0.70045475, Validation loss: 0.69973589, Gradient norm: 3.23131009
INFO:root:[    9] Training loss: 0.69910079, Validation loss: 0.69822509, Gradient norm: 3.26825920
INFO:root:[   10] Training loss: 0.69804445, Validation loss: 0.69938791, Gradient norm: 3.08622907
INFO:root:[   11] Training loss: 0.69771119, Validation loss: 0.69648347, Gradient norm: 3.03347787
INFO:root:[   12] Training loss: 0.69661599, Validation loss: 0.69780504, Gradient norm: 2.84224546
INFO:root:[   13] Training loss: 0.69579995, Validation loss: 0.69581885, Gradient norm: 2.73311303
INFO:root:[   14] Training loss: 0.69457303, Validation loss: 0.69559484, Gradient norm: 2.55319965
INFO:root:[   15] Training loss: 0.69363944, Validation loss: 0.69403176, Gradient norm: 2.71203591
INFO:root:[   16] Training loss: 0.69264041, Validation loss: 0.69399536, Gradient norm: 2.53649229
INFO:root:[   17] Training loss: 0.69099521, Validation loss: 0.69019340, Gradient norm: 2.40191151
INFO:root:[   18] Training loss: 0.68947767, Validation loss: 0.68977758, Gradient norm: 2.47244780
INFO:root:[   19] Training loss: 0.68742087, Validation loss: 0.68587406, Gradient norm: 2.28593636
INFO:root:[   20] Training loss: 0.68564147, Validation loss: 0.68576781, Gradient norm: 2.37049447
INFO:root:[   21] Training loss: 0.68415361, Validation loss: 0.68515095, Gradient norm: 2.26596411
INFO:root:[   22] Training loss: 0.68300359, Validation loss: 0.68340887, Gradient norm: 2.34850971
INFO:root:[   23] Training loss: 0.68170514, Validation loss: 0.68250002, Gradient norm: 2.29964627
INFO:root:[   24] Training loss: 0.68053735, Validation loss: 0.67912286, Gradient norm: 2.17167997
INFO:root:[   25] Training loss: 0.67923238, Validation loss: 0.67945920, Gradient norm: 2.15998914
INFO:root:[   26] Training loss: 0.67846082, Validation loss: 0.68016295, Gradient norm: 2.11561805
INFO:root:[   27] Training loss: 0.67751132, Validation loss: 0.67755725, Gradient norm: 1.95855694
INFO:root:[   28] Training loss: 0.67695834, Validation loss: 0.67772608, Gradient norm: 2.00391125
INFO:root:[   29] Training loss: 0.67632970, Validation loss: 0.67845701, Gradient norm: 2.08486281
INFO:root:[   30] Training loss: 0.67594854, Validation loss: 0.67679685, Gradient norm: 2.07758768
INFO:root:[   31] Training loss: 0.67533872, Validation loss: 0.67622997, Gradient norm: 1.90717307
INFO:root:[   32] Training loss: 0.67573886, Validation loss: 0.67768478, Gradient norm: 2.11422254
INFO:root:[   33] Training loss: 0.67505163, Validation loss: 0.67617617, Gradient norm: 2.14200206
INFO:root:[   34] Training loss: 0.67426318, Validation loss: 0.67779380, Gradient norm: 2.11389286
INFO:root:[   35] Training loss: 0.67403549, Validation loss: 0.67549856, Gradient norm: 1.99006582
INFO:root:[   36] Training loss: 0.67365676, Validation loss: 0.67566893, Gradient norm: 1.97943517
INFO:root:[   37] Training loss: 0.67325727, Validation loss: 0.67628699, Gradient norm: 1.90954016
INFO:root:[   38] Training loss: 0.67366749, Validation loss: 0.67221476, Gradient norm: 2.03679976
INFO:root:[   39] Training loss: 0.67294407, Validation loss: 0.67353852, Gradient norm: 1.99318064
INFO:root:[   40] Training loss: 0.67270856, Validation loss: 0.67332190, Gradient norm: 1.90445046
INFO:root:[   41] Training loss: 0.67221344, Validation loss: 0.67143906, Gradient norm: 2.00840482
INFO:root:[   42] Training loss: 0.67229316, Validation loss: 0.67252310, Gradient norm: 2.05817329
INFO:root:[   43] Training loss: 0.67217288, Validation loss: 0.67167922, Gradient norm: 2.13543546
INFO:root:[   44] Training loss: 0.67204576, Validation loss: 0.67200719, Gradient norm: 2.03796861
INFO:root:[   45] Training loss: 0.67152377, Validation loss: 0.67410862, Gradient norm: 2.00839809
INFO:root:[   46] Training loss: 0.67144678, Validation loss: 0.67109492, Gradient norm: 1.99111054
INFO:root:[   47] Training loss: 0.67098706, Validation loss: 0.67133254, Gradient norm: 1.96572330
INFO:root:[   48] Training loss: 0.67098597, Validation loss: 0.67337145, Gradient norm: 2.02090076
INFO:root:[   49] Training loss: 0.67109175, Validation loss: 0.67187666, Gradient norm: 2.05907167
INFO:root:[   50] Training loss: 0.67074604, Validation loss: 0.67250465, Gradient norm: 1.92045979
INFO:root:[   51] Training loss: 0.67034121, Validation loss: 0.67276728, Gradient norm: 2.01496661
INFO:root:[   52] Training loss: 0.67005889, Validation loss: 0.67351298, Gradient norm: 1.93049701
INFO:root:[   53] Training loss: 0.66994194, Validation loss: 0.66994429, Gradient norm: 1.87765364
INFO:root:[   54] Training loss: 0.66969921, Validation loss: 0.66999173, Gradient norm: 1.82963131
INFO:root:[   55] Training loss: 0.66948078, Validation loss: 0.67104234, Gradient norm: 1.98331317
INFO:root:[   56] Training loss: 0.66868044, Validation loss: 0.66837282, Gradient norm: 1.86644352
INFO:root:[   57] Training loss: 0.66882947, Validation loss: 0.66896303, Gradient norm: 1.87416947
INFO:root:[   58] Training loss: 0.66768837, Validation loss: 0.66879514, Gradient norm: 1.76569179
INFO:root:[   59] Training loss: 0.66770095, Validation loss: 0.66955255, Gradient norm: 1.63107617
INFO:root:[   60] Training loss: 0.66641543, Validation loss: 0.66742793, Gradient norm: 1.70563763
INFO:root:[   61] Training loss: 0.66622518, Validation loss: 0.66875965, Gradient norm: 1.55069987
INFO:root:[   62] Training loss: 0.66552152, Validation loss: 0.66602563, Gradient norm: 1.54528792
INFO:root:[   63] Training loss: 0.66483032, Validation loss: 0.66724503, Gradient norm: 1.59553331
INFO:root:[   64] Training loss: 0.66491019, Validation loss: 0.66614758, Gradient norm: 1.59035973
INFO:root:[   65] Training loss: 0.66432215, Validation loss: 0.66358884, Gradient norm: 1.54734001
INFO:root:[   66] Training loss: 0.66367116, Validation loss: 0.66478328, Gradient norm: 1.49722713
INFO:root:[   67] Training loss: 0.66428371, Validation loss: 0.66462835, Gradient norm: 1.47370225
INFO:root:[   68] Training loss: 0.66269184, Validation loss: 0.66357712, Gradient norm: 1.44432652
INFO:root:[   69] Training loss: 0.66365039, Validation loss: 0.66406751, Gradient norm: 1.50721701
INFO:root:[   70] Training loss: 0.66293589, Validation loss: 0.66375801, Gradient norm: 1.45082872
INFO:root:[   71] Training loss: 0.66206763, Validation loss: 0.66194052, Gradient norm: 1.34014130
INFO:root:[   72] Training loss: 0.66186656, Validation loss: 0.66244172, Gradient norm: 1.39219147
INFO:root:[   73] Training loss: 0.66116647, Validation loss: 0.66282936, Gradient norm: 1.30816390
INFO:root:[   74] Training loss: 0.66183666, Validation loss: 0.66412877, Gradient norm: 1.34689078
INFO:root:[   75] Training loss: 0.66136464, Validation loss: 0.66251604, Gradient norm: 1.34519027
INFO:root:[   76] Training loss: 0.66061873, Validation loss: 0.66225903, Gradient norm: 1.31173050
INFO:root:[   77] Training loss: 0.66095233, Validation loss: 0.66140249, Gradient norm: 1.28081744
INFO:root:[   78] Training loss: 0.66080877, Validation loss: 0.66114091, Gradient norm: 1.31246847
INFO:root:[   79] Training loss: 0.66007230, Validation loss: 0.66085029, Gradient norm: 1.30652550
INFO:root:[   80] Training loss: 0.66035007, Validation loss: 0.65928827, Gradient norm: 1.26511973
INFO:root:[   81] Training loss: 0.65973319, Validation loss: 0.66160764, Gradient norm: 1.22677159
INFO:root:[   82] Training loss: 0.66010417, Validation loss: 0.65973146, Gradient norm: 1.18738068
INFO:root:[   83] Training loss: 0.65982080, Validation loss: 0.66056807, Gradient norm: 1.26214934
INFO:root:[   84] Training loss: 0.65933569, Validation loss: 0.66224043, Gradient norm: 1.15590799
INFO:root:[   85] Training loss: 0.65950684, Validation loss: 0.66002869, Gradient norm: 1.20964507
INFO:root:[   86] Training loss: 0.65888404, Validation loss: 0.66403403, Gradient norm: 1.18627473
INFO:root:[   87] Training loss: 0.65941408, Validation loss: 0.66280236, Gradient norm: 1.11745766
INFO:root:[   88] Training loss: 0.65876426, Validation loss: 0.65962749, Gradient norm: 1.17239140
INFO:root:[   89] Training loss: 0.65919337, Validation loss: 0.66398212, Gradient norm: 1.10014596
INFO:root:EP 89: Early stopping
INFO:root:Training the model took 5574.191s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91037
INFO:root:EnergyScoreTrain: 0.65767
INFO:root:CRPSTrain: 0.6104
INFO:root:Gaussian NLLTrain: 1.9085
INFO:root:CoverageTrain: 0.77743
INFO:root:IntervalWidthTrain: 3.32798
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91268
INFO:root:EnergyScoreValidation: 0.65948
INFO:root:CRPSValidation: 0.61214
INFO:root:Gaussian NLLValidation: 1.91397
INFO:root:CoverageValidation: 0.77673
INFO:root:IntervalWidthValidation: 3.32884
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91288
INFO:root:EnergyScoreTest: 0.65974
INFO:root:CRPSTest: 0.61212
INFO:root:Gaussian NLLTest: 1.90733
INFO:root:CoverageTest: 0.77741
INFO:root:IntervalWidthTest: 3.33001
INFO:root:###4 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.74619326, Validation loss: 0.72922074, Gradient norm: 3.33466334
INFO:root:[    2] Training loss: 0.71696748, Validation loss: 0.71008010, Gradient norm: 2.87444386
INFO:root:[    3] Training loss: 0.70704504, Validation loss: 0.70406116, Gradient norm: 2.18317535
INFO:root:[    4] Training loss: 0.70431279, Validation loss: 0.70239067, Gradient norm: 2.37562907
INFO:root:[    5] Training loss: 0.70155399, Validation loss: 0.69910095, Gradient norm: 2.37358392
INFO:root:[    6] Training loss: 0.69817358, Validation loss: 0.69490658, Gradient norm: 1.89913282
INFO:root:[    7] Training loss: 0.69539717, Validation loss: 0.69458384, Gradient norm: 2.18613065
INFO:root:[    8] Training loss: 0.69355385, Validation loss: 0.69572186, Gradient norm: 2.38446153
INFO:root:[    9] Training loss: 0.69131817, Validation loss: 0.68962468, Gradient norm: 2.14590387
INFO:root:[   10] Training loss: 0.68994571, Validation loss: 0.69226012, Gradient norm: 2.06769750
INFO:root:[   11] Training loss: 0.68901473, Validation loss: 0.68814761, Gradient norm: 2.05658269
INFO:root:[   12] Training loss: 0.68795286, Validation loss: 0.68729296, Gradient norm: 2.03575524
INFO:root:[   13] Training loss: 0.68720972, Validation loss: 0.68763950, Gradient norm: 1.94961049
INFO:root:[   14] Training loss: 0.68644593, Validation loss: 0.68549215, Gradient norm: 1.92941125
INFO:root:[   15] Training loss: 0.68529559, Validation loss: 0.68727745, Gradient norm: 2.03540395
INFO:root:[   16] Training loss: 0.68471457, Validation loss: 0.68488836, Gradient norm: 1.87206001
INFO:root:[   17] Training loss: 0.68346405, Validation loss: 0.68561658, Gradient norm: 1.93404092
INFO:root:[   18] Training loss: 0.68319312, Validation loss: 0.68234870, Gradient norm: 1.93799812
INFO:root:[   19] Training loss: 0.68221025, Validation loss: 0.68053803, Gradient norm: 1.88136915
INFO:root:[   20] Training loss: 0.68148070, Validation loss: 0.68058309, Gradient norm: 1.83678415
INFO:root:[   21] Training loss: 0.68075114, Validation loss: 0.68136804, Gradient norm: 1.84892183
INFO:root:[   22] Training loss: 0.68011393, Validation loss: 0.68121713, Gradient norm: 1.92023882
INFO:root:[   23] Training loss: 0.67947519, Validation loss: 0.67914849, Gradient norm: 1.91305268
INFO:root:[   24] Training loss: 0.67927318, Validation loss: 0.68021571, Gradient norm: 1.86619468
INFO:root:[   25] Training loss: 0.67804576, Validation loss: 0.67843754, Gradient norm: 1.84598997
INFO:root:[   26] Training loss: 0.67792274, Validation loss: 0.67738836, Gradient norm: 1.79288191
INFO:root:[   27] Training loss: 0.67743200, Validation loss: 0.67777347, Gradient norm: 1.89470474
INFO:root:[   28] Training loss: 0.67697869, Validation loss: 0.67867626, Gradient norm: 1.99624660
INFO:root:[   29] Training loss: 0.67670612, Validation loss: 0.67633835, Gradient norm: 1.73085887
INFO:root:[   30] Training loss: 0.67579542, Validation loss: 0.67570923, Gradient norm: 1.92158980
INFO:root:[   31] Training loss: 0.67604681, Validation loss: 0.67505897, Gradient norm: 1.86122041
INFO:root:[   32] Training loss: 0.67535220, Validation loss: 0.67806303, Gradient norm: 1.83836921
INFO:root:[   33] Training loss: 0.67463697, Validation loss: 0.67605604, Gradient norm: 1.78839860
INFO:root:[   34] Training loss: 0.67442209, Validation loss: 0.67687147, Gradient norm: 1.71153951
INFO:root:[   35] Training loss: 0.67379318, Validation loss: 0.67664995, Gradient norm: 1.75984167
INFO:root:[   36] Training loss: 0.67383365, Validation loss: 0.67326511, Gradient norm: 1.76904454
INFO:root:[   37] Training loss: 0.67347654, Validation loss: 0.67414526, Gradient norm: 1.83955623
INFO:root:[   38] Training loss: 0.67347201, Validation loss: 0.67622017, Gradient norm: 1.80047506
INFO:root:[   39] Training loss: 0.67328926, Validation loss: 0.67251334, Gradient norm: 1.85881918
INFO:root:[   40] Training loss: 0.67256257, Validation loss: 0.67276537, Gradient norm: 1.81293981
INFO:root:[   41] Training loss: 0.67226910, Validation loss: 0.67195527, Gradient norm: 1.68214727
INFO:root:[   42] Training loss: 0.67219173, Validation loss: 0.67404655, Gradient norm: 1.77314753
INFO:root:[   43] Training loss: 0.67211056, Validation loss: 0.67308504, Gradient norm: 1.73179392
INFO:root:[   44] Training loss: 0.67170656, Validation loss: 0.67519381, Gradient norm: 1.77048544
INFO:root:[   45] Training loss: 0.67170687, Validation loss: 0.67373312, Gradient norm: 1.71612748
INFO:root:[   46] Training loss: 0.67087111, Validation loss: 0.67112199, Gradient norm: 1.70673904
INFO:root:[   47] Training loss: 0.67088934, Validation loss: 0.67138428, Gradient norm: 1.68670891
INFO:root:[   48] Training loss: 0.67100541, Validation loss: 0.67235268, Gradient norm: 1.74671387
INFO:root:[   49] Training loss: 0.67060463, Validation loss: 0.67034784, Gradient norm: 1.70079584
INFO:root:[   50] Training loss: 0.67010623, Validation loss: 0.67142661, Gradient norm: 1.71985007
INFO:root:[   51] Training loss: 0.66989670, Validation loss: 0.67240933, Gradient norm: 1.70742167
INFO:root:[   52] Training loss: 0.67038092, Validation loss: 0.66985349, Gradient norm: 1.66654172
INFO:root:[   53] Training loss: 0.66919599, Validation loss: 0.67105184, Gradient norm: 1.74796306
INFO:root:[   54] Training loss: 0.66880963, Validation loss: 0.67092741, Gradient norm: 1.76161283
INFO:root:[   55] Training loss: 0.66859864, Validation loss: 0.66801418, Gradient norm: 1.60907698
INFO:root:[   56] Training loss: 0.66777277, Validation loss: 0.66748462, Gradient norm: 1.64560144
INFO:root:[   57] Training loss: 0.66790384, Validation loss: 0.66851584, Gradient norm: 1.74170502
INFO:root:[   58] Training loss: 0.66748881, Validation loss: 0.66885675, Gradient norm: 1.59133996
INFO:root:[   59] Training loss: 0.66647514, Validation loss: 0.67005602, Gradient norm: 1.52165356
INFO:root:[   60] Training loss: 0.66580040, Validation loss: 0.66605941, Gradient norm: 1.47112857
INFO:root:[   61] Training loss: 0.66566439, Validation loss: 0.66454896, Gradient norm: 1.53072375
INFO:root:[   62] Training loss: 0.66476921, Validation loss: 0.66489715, Gradient norm: 1.51357616
INFO:root:[   63] Training loss: 0.66492135, Validation loss: 0.66489257, Gradient norm: 1.36893224
INFO:root:[   64] Training loss: 0.66369950, Validation loss: 0.66668020, Gradient norm: 1.46110152
INFO:root:[   65] Training loss: 0.66387722, Validation loss: 0.66502808, Gradient norm: 1.34680395
INFO:root:[   66] Training loss: 0.66301415, Validation loss: 0.66535082, Gradient norm: 1.38014626
INFO:root:[   67] Training loss: 0.66307236, Validation loss: 0.66598990, Gradient norm: 1.36355293
INFO:root:[   68] Training loss: 0.66257483, Validation loss: 0.66406269, Gradient norm: 1.31109755
INFO:root:[   69] Training loss: 0.66197456, Validation loss: 0.66339189, Gradient norm: 1.30379660
INFO:root:[   70] Training loss: 0.66191599, Validation loss: 0.66400065, Gradient norm: 1.26779738
INFO:root:[   71] Training loss: 0.66152927, Validation loss: 0.66093389, Gradient norm: 1.18058916
INFO:root:[   72] Training loss: 0.66169420, Validation loss: 0.66337557, Gradient norm: 1.30249470
INFO:root:[   73] Training loss: 0.66122492, Validation loss: 0.66336786, Gradient norm: 1.25669467
INFO:root:[   74] Training loss: 0.66122779, Validation loss: 0.66351027, Gradient norm: 1.16452223
INFO:root:[   75] Training loss: 0.66028811, Validation loss: 0.66184485, Gradient norm: 1.21559391
INFO:root:[   76] Training loss: 0.66038116, Validation loss: 0.66630796, Gradient norm: 1.15444883
INFO:root:[   77] Training loss: 0.66058342, Validation loss: 0.66477814, Gradient norm: 1.18373249
INFO:root:[   78] Training loss: 0.65977571, Validation loss: 0.66583338, Gradient norm: 1.13356341
INFO:root:[   79] Training loss: 0.65956662, Validation loss: 0.66217552, Gradient norm: 1.09557262
INFO:root:[   80] Training loss: 0.65933569, Validation loss: 0.66133504, Gradient norm: 1.09369600
INFO:root:[   81] Training loss: 0.65998200, Validation loss: 0.65994351, Gradient norm: 1.09677004
INFO:root:[   82] Training loss: 0.65914712, Validation loss: 0.66048627, Gradient norm: 1.06362698
INFO:root:[   83] Training loss: 0.65931222, Validation loss: 0.66015261, Gradient norm: 1.03058560
INFO:root:[   84] Training loss: 0.65923313, Validation loss: 0.66119153, Gradient norm: 1.02548189
INFO:root:[   85] Training loss: 0.65884394, Validation loss: 0.65878428, Gradient norm: 1.06478306
INFO:root:[   86] Training loss: 0.65931112, Validation loss: 0.66134150, Gradient norm: 1.05397787
INFO:root:[   87] Training loss: 0.65919354, Validation loss: 0.66012545, Gradient norm: 1.00072033
INFO:root:[   88] Training loss: 0.65881261, Validation loss: 0.65814733, Gradient norm: 0.98485057
INFO:root:[   89] Training loss: 0.65893976, Validation loss: 0.65778610, Gradient norm: 0.97907448
INFO:root:[   90] Training loss: 0.65814658, Validation loss: 0.66055209, Gradient norm: 0.92521386
INFO:root:[   91] Training loss: 0.65836346, Validation loss: 0.66123804, Gradient norm: 0.93471384
INFO:root:[   92] Training loss: 0.65830793, Validation loss: 0.66084719, Gradient norm: 0.87508414
INFO:root:[   93] Training loss: 0.65854365, Validation loss: 0.65961196, Gradient norm: 0.91099904
INFO:root:[   94] Training loss: 0.65742316, Validation loss: 0.65752585, Gradient norm: 0.90485513
INFO:root:[   95] Training loss: 0.65775695, Validation loss: 0.66129789, Gradient norm: 0.91148121
INFO:root:[   96] Training loss: 0.65758350, Validation loss: 0.66107513, Gradient norm: 0.87025801
INFO:root:[   97] Training loss: 0.65747042, Validation loss: 0.65956920, Gradient norm: 0.85571408
INFO:root:[   98] Training loss: 0.65724743, Validation loss: 0.65808347, Gradient norm: 0.83124603
INFO:root:[   99] Training loss: 0.65733769, Validation loss: 0.65670357, Gradient norm: 0.84552199
INFO:root:[  100] Training loss: 0.65704352, Validation loss: 0.65949178, Gradient norm: 0.79134675
INFO:root:[  101] Training loss: 0.65708638, Validation loss: 0.66042873, Gradient norm: 0.79268401
INFO:root:[  102] Training loss: 0.65677902, Validation loss: 0.65734345, Gradient norm: 0.77604457
INFO:root:[  103] Training loss: 0.65731751, Validation loss: 0.65986405, Gradient norm: 0.85118534
INFO:root:[  104] Training loss: 0.65711562, Validation loss: 0.66069808, Gradient norm: 0.84203132
INFO:root:[  105] Training loss: 0.65713043, Validation loss: 0.65823398, Gradient norm: 0.76156966
INFO:root:[  106] Training loss: 0.65651625, Validation loss: 0.65801487, Gradient norm: 0.77746856
INFO:root:[  107] Training loss: 0.65661328, Validation loss: 0.66085713, Gradient norm: 0.71097520
INFO:root:[  108] Training loss: 0.65625453, Validation loss: 0.66095623, Gradient norm: 0.77500768
INFO:root:EP 108: Early stopping
INFO:root:Training the model took 6766.906s.
INFO:root:Emptying the cuda cache took 0.021s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.90547
INFO:root:EnergyScoreTrain: 0.65422
INFO:root:CRPSTrain: 0.60263
INFO:root:Gaussian NLLTrain: 2.337
INFO:root:CoverageTrain: 0.74986
INFO:root:IntervalWidthTrain: 3.22086
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.90785
INFO:root:EnergyScoreValidation: 0.65613
INFO:root:CRPSValidation: 0.60445
INFO:root:Gaussian NLLValidation: 2.34465
INFO:root:CoverageValidation: 0.74904
INFO:root:IntervalWidthValidation: 3.22093
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.90757
INFO:root:EnergyScoreTest: 0.656
INFO:root:CRPSTest: 0.60417
INFO:root:Gaussian NLLTest: 2.33532
INFO:root:CoverageTest: 0.75007
INFO:root:IntervalWidthTest: 3.22554
INFO:root:###5 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.73465138, Validation loss: 0.71457803, Gradient norm: 2.03380540
INFO:root:[    2] Training loss: 0.71053798, Validation loss: 0.70509026, Gradient norm: 1.80232990
INFO:root:[    3] Training loss: 0.70181068, Validation loss: 0.69957219, Gradient norm: 1.44869195
INFO:root:[    4] Training loss: 0.69720518, Validation loss: 0.69848919, Gradient norm: 1.37402419
INFO:root:[    5] Training loss: 0.69318736, Validation loss: 0.69409245, Gradient norm: 1.25944056
INFO:root:[    6] Training loss: 0.69175678, Validation loss: 0.68981144, Gradient norm: 1.27765941
INFO:root:[    7] Training loss: 0.69039677, Validation loss: 0.69148772, Gradient norm: 1.33215847
INFO:root:[    8] Training loss: 0.68942645, Validation loss: 0.69066881, Gradient norm: 1.25218463
INFO:root:[    9] Training loss: 0.68794443, Validation loss: 0.69013748, Gradient norm: 1.09583586
INFO:root:[   10] Training loss: 0.68743993, Validation loss: 0.68637553, Gradient norm: 1.16205665
INFO:root:[   11] Training loss: 0.68620374, Validation loss: 0.68561819, Gradient norm: 1.03760534
INFO:root:[   12] Training loss: 0.68518244, Validation loss: 0.68626322, Gradient norm: 1.12958384
INFO:root:[   13] Training loss: 0.68418265, Validation loss: 0.68424815, Gradient norm: 1.01812582
INFO:root:[   14] Training loss: 0.68319099, Validation loss: 0.68397266, Gradient norm: 1.03256496
INFO:root:[   15] Training loss: 0.68191855, Validation loss: 0.68323887, Gradient norm: 1.14567409
INFO:root:[   16] Training loss: 0.68086703, Validation loss: 0.68641678, Gradient norm: 1.10057833
INFO:root:[   17] Training loss: 0.68031810, Validation loss: 0.68202422, Gradient norm: 1.08704505
INFO:root:[   18] Training loss: 0.67977259, Validation loss: 0.68070836, Gradient norm: 1.07658122
INFO:root:[   19] Training loss: 0.67919609, Validation loss: 0.68033684, Gradient norm: 1.10196155
INFO:root:[   20] Training loss: 0.67822138, Validation loss: 0.68011179, Gradient norm: 1.04724779
INFO:root:[   21] Training loss: 0.67767526, Validation loss: 0.67840225, Gradient norm: 1.10463016
INFO:root:[   22] Training loss: 0.67669598, Validation loss: 0.67739788, Gradient norm: 1.02916655
INFO:root:[   23] Training loss: 0.67635466, Validation loss: 0.67968327, Gradient norm: 1.07376160
INFO:root:[   24] Training loss: 0.67561903, Validation loss: 0.67499679, Gradient norm: 1.03131844
INFO:root:[   25] Training loss: 0.67486044, Validation loss: 0.67794627, Gradient norm: 1.04103375
INFO:root:[   26] Training loss: 0.67461456, Validation loss: 0.67674239, Gradient norm: 1.00541270
INFO:root:[   27] Training loss: 0.67406217, Validation loss: 0.67574274, Gradient norm: 1.05245096
INFO:root:[   28] Training loss: 0.67355910, Validation loss: 0.67308867, Gradient norm: 1.09110062
INFO:root:[   29] Training loss: 0.67341572, Validation loss: 0.67519778, Gradient norm: 1.02960785
INFO:root:[   30] Training loss: 0.67280342, Validation loss: 0.67302877, Gradient norm: 1.05409236
INFO:root:[   31] Training loss: 0.67274140, Validation loss: 0.67245144, Gradient norm: 1.03176711
INFO:root:[   32] Training loss: 0.67207550, Validation loss: 0.67377533, Gradient norm: 0.97879065
INFO:root:[   33] Training loss: 0.67151382, Validation loss: 0.67513316, Gradient norm: 0.99763456
INFO:root:[   34] Training loss: 0.67161748, Validation loss: 0.67229796, Gradient norm: 1.06983926
INFO:root:[   35] Training loss: 0.67114666, Validation loss: 0.67198409, Gradient norm: 0.97552824
INFO:root:[   36] Training loss: 0.67097991, Validation loss: 0.67116340, Gradient norm: 0.99553047
INFO:root:[   37] Training loss: 0.67084502, Validation loss: 0.67056347, Gradient norm: 1.01839121
INFO:root:[   38] Training loss: 0.67067827, Validation loss: 0.67361024, Gradient norm: 0.97059343
INFO:root:[   39] Training loss: 0.67033680, Validation loss: 0.67084940, Gradient norm: 0.98219650
INFO:root:[   40] Training loss: 0.66983731, Validation loss: 0.66959010, Gradient norm: 0.99899252
INFO:root:[   41] Training loss: 0.66953651, Validation loss: 0.66920232, Gradient norm: 1.04406361
INFO:root:[   42] Training loss: 0.66977434, Validation loss: 0.67199067, Gradient norm: 0.97447989
INFO:root:[   43] Training loss: 0.66949542, Validation loss: 0.67159672, Gradient norm: 0.97156250
INFO:root:[   44] Training loss: 0.66882777, Validation loss: 0.67010242, Gradient norm: 0.93841169
INFO:root:[   45] Training loss: 0.66875143, Validation loss: 0.66967180, Gradient norm: 1.04641829
INFO:root:[   46] Training loss: 0.66859173, Validation loss: 0.66946174, Gradient norm: 0.94390807
INFO:root:[   47] Training loss: 0.66821364, Validation loss: 0.66779869, Gradient norm: 1.01355363
INFO:root:[   48] Training loss: 0.66822812, Validation loss: 0.67070371, Gradient norm: 0.96430745
INFO:root:[   49] Training loss: 0.66816243, Validation loss: 0.66963545, Gradient norm: 0.96997527
INFO:root:[   50] Training loss: 0.66775591, Validation loss: 0.67007047, Gradient norm: 0.95482964
INFO:root:[   51] Training loss: 0.66731003, Validation loss: 0.66786492, Gradient norm: 0.91375810
INFO:root:[   52] Training loss: 0.66740194, Validation loss: 0.67120978, Gradient norm: 0.91824295
INFO:root:[   53] Training loss: 0.66711277, Validation loss: 0.66842946, Gradient norm: 1.00476094
INFO:root:[   54] Training loss: 0.66733379, Validation loss: 0.66891761, Gradient norm: 0.97262185
INFO:root:[   55] Training loss: 0.66712901, Validation loss: 0.66970176, Gradient norm: 0.94881031
INFO:root:[   56] Training loss: 0.66708408, Validation loss: 0.66821492, Gradient norm: 0.93055288
INFO:root:[   57] Training loss: 0.66649568, Validation loss: 0.66684138, Gradient norm: 0.94505408
INFO:root:[   58] Training loss: 0.66635794, Validation loss: 0.66949327, Gradient norm: 0.93237369
INFO:root:[   59] Training loss: 0.66620221, Validation loss: 0.66628598, Gradient norm: 0.85309177
INFO:root:[   60] Training loss: 0.66601446, Validation loss: 0.66993511, Gradient norm: 0.86583535
INFO:root:[   61] Training loss: 0.66614869, Validation loss: 0.66686793, Gradient norm: 0.94069195
INFO:root:[   62] Training loss: 0.66598989, Validation loss: 0.66708681, Gradient norm: 0.96537569
INFO:root:[   63] Training loss: 0.66578170, Validation loss: 0.66783197, Gradient norm: 0.93042329
INFO:root:[   64] Training loss: 0.66532176, Validation loss: 0.66643940, Gradient norm: 0.83387102
INFO:root:[   65] Training loss: 0.66579549, Validation loss: 0.66705238, Gradient norm: 0.93937117
INFO:root:[   66] Training loss: 0.66558208, Validation loss: 0.66582166, Gradient norm: 0.91449528
INFO:root:[   67] Training loss: 0.66507622, Validation loss: 0.66552720, Gradient norm: 0.89296749
INFO:root:[   68] Training loss: 0.66551992, Validation loss: 0.66504716, Gradient norm: 0.93537975
INFO:root:[   69] Training loss: 0.66497476, Validation loss: 0.66706627, Gradient norm: 0.89415437
INFO:root:[   70] Training loss: 0.66491153, Validation loss: 0.66656787, Gradient norm: 0.90794847
INFO:root:[   71] Training loss: 0.66510257, Validation loss: 0.66705566, Gradient norm: 0.95738847
INFO:root:[   72] Training loss: 0.66483549, Validation loss: 0.66520622, Gradient norm: 0.86402762
INFO:root:[   73] Training loss: 0.66463254, Validation loss: 0.66582770, Gradient norm: 0.87944395
INFO:root:[   74] Training loss: 0.66470035, Validation loss: 0.66471638, Gradient norm: 0.81195593
INFO:root:[   75] Training loss: 0.66477306, Validation loss: 0.66515605, Gradient norm: 0.97014749
INFO:root:[   76] Training loss: 0.66452833, Validation loss: 0.66751665, Gradient norm: 0.92648892
INFO:root:[   77] Training loss: 0.66475706, Validation loss: 0.66529833, Gradient norm: 0.87604152
INFO:root:[   78] Training loss: 0.66460406, Validation loss: 0.66493793, Gradient norm: 0.84890450
INFO:root:[   79] Training loss: 0.66412338, Validation loss: 0.66635022, Gradient norm: 0.90991956
INFO:root:[   80] Training loss: 0.66443393, Validation loss: 0.66799909, Gradient norm: 0.91351951
INFO:root:[   81] Training loss: 0.66395155, Validation loss: 0.66686480, Gradient norm: 0.82551758
INFO:root:[   82] Training loss: 0.66407644, Validation loss: 0.66712094, Gradient norm: 0.87182415
INFO:root:[   83] Training loss: 0.66385644, Validation loss: 0.66679187, Gradient norm: 0.89765414
INFO:root:EP 83: Early stopping
INFO:root:Training the model took 5205.558s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91625
INFO:root:EnergyScoreTrain: 0.66176
INFO:root:CRPSTrain: 0.59171
INFO:root:Gaussian NLLTrain: 1.61092
INFO:root:CoverageTrain: 0.86543
INFO:root:IntervalWidthTrain: 3.97319
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91922
INFO:root:EnergyScoreValidation: 0.66409
INFO:root:CRPSValidation: 0.59419
INFO:root:Gaussian NLLValidation: 1.61931
INFO:root:CoverageValidation: 0.86398
INFO:root:IntervalWidthValidation: 3.96977
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91805
INFO:root:EnergyScoreTest: 0.66334
INFO:root:CRPSTest: 0.59341
INFO:root:Gaussian NLLTest: 1.61689
INFO:root:CoverageTest: 0.86423
INFO:root:IntervalWidthTest: 3.96994
INFO:root:###6 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.73474791, Validation loss: 0.71609160, Gradient norm: 1.49408099
INFO:root:[    2] Training loss: 0.70930729, Validation loss: 0.70320764, Gradient norm: 0.94204967
INFO:root:[    3] Training loss: 0.69906734, Validation loss: 0.69621028, Gradient norm: 0.91182732
INFO:root:[    4] Training loss: 0.69563918, Validation loss: 0.69633773, Gradient norm: 0.87238077
INFO:root:[    5] Training loss: 0.69309249, Validation loss: 0.69282258, Gradient norm: 0.81886924
INFO:root:[    6] Training loss: 0.69106087, Validation loss: 0.69163911, Gradient norm: 0.77814936
INFO:root:[    7] Training loss: 0.68965874, Validation loss: 0.69090347, Gradient norm: 0.77309880
INFO:root:[    8] Training loss: 0.68896954, Validation loss: 0.69022003, Gradient norm: 0.77738682
INFO:root:[    9] Training loss: 0.68763360, Validation loss: 0.68697740, Gradient norm: 0.70974995
INFO:root:[   10] Training loss: 0.68686875, Validation loss: 0.68621369, Gradient norm: 0.78829554
INFO:root:[   11] Training loss: 0.68607090, Validation loss: 0.68604489, Gradient norm: 0.74876896
INFO:root:[   12] Training loss: 0.68483159, Validation loss: 0.68675332, Gradient norm: 0.68625794
INFO:root:[   13] Training loss: 0.68399944, Validation loss: 0.68561992, Gradient norm: 0.71587382
INFO:root:[   14] Training loss: 0.68320763, Validation loss: 0.68232336, Gradient norm: 0.76559058
INFO:root:[   15] Training loss: 0.68225462, Validation loss: 0.68367302, Gradient norm: 0.78984536
INFO:root:[   16] Training loss: 0.68106938, Validation loss: 0.68131935, Gradient norm: 0.74719336
INFO:root:[   17] Training loss: 0.68059190, Validation loss: 0.68198557, Gradient norm: 0.77818736
INFO:root:[   18] Training loss: 0.67989817, Validation loss: 0.68321451, Gradient norm: 0.80464619
INFO:root:[   19] Training loss: 0.67930115, Validation loss: 0.67860665, Gradient norm: 0.78721568
INFO:root:[   20] Training loss: 0.67859085, Validation loss: 0.67826474, Gradient norm: 0.68484060
INFO:root:[   21] Training loss: 0.67804271, Validation loss: 0.67836242, Gradient norm: 0.75021916
INFO:root:[   22] Training loss: 0.67748149, Validation loss: 0.67666965, Gradient norm: 0.77738119
INFO:root:[   23] Training loss: 0.67722428, Validation loss: 0.67952210, Gradient norm: 0.80645861
INFO:root:[   24] Training loss: 0.67645640, Validation loss: 0.67784545, Gradient norm: 0.77037998
INFO:root:[   25] Training loss: 0.67607872, Validation loss: 0.67702426, Gradient norm: 0.79388021
INFO:root:[   26] Training loss: 0.67552022, Validation loss: 0.67642113, Gradient norm: 0.79995742
INFO:root:[   27] Training loss: 0.67491081, Validation loss: 0.67691899, Gradient norm: 0.79262227
INFO:root:[   28] Training loss: 0.67484405, Validation loss: 0.67461660, Gradient norm: 0.76838335
INFO:root:[   29] Training loss: 0.67422683, Validation loss: 0.67640509, Gradient norm: 0.79683660
INFO:root:[   30] Training loss: 0.67369101, Validation loss: 0.67327418, Gradient norm: 0.79810340
INFO:root:[   31] Training loss: 0.67348952, Validation loss: 0.67432493, Gradient norm: 0.75485608
INFO:root:[   32] Training loss: 0.67261366, Validation loss: 0.67386709, Gradient norm: 0.74736377
INFO:root:[   33] Training loss: 0.67212552, Validation loss: 0.67555374, Gradient norm: 0.73901974
INFO:root:[   34] Training loss: 0.67202525, Validation loss: 0.67357297, Gradient norm: 0.77709146
INFO:root:[   35] Training loss: 0.67136657, Validation loss: 0.67044356, Gradient norm: 0.75194043
INFO:root:[   36] Training loss: 0.67074093, Validation loss: 0.67193288, Gradient norm: 0.74349978
INFO:root:[   37] Training loss: 0.66977622, Validation loss: 0.67093645, Gradient norm: 0.59800174
INFO:root:[   38] Training loss: 0.67058909, Validation loss: 0.67204403, Gradient norm: 0.80123426
INFO:root:[   39] Training loss: 0.67002367, Validation loss: 0.67084596, Gradient norm: 0.81206468
INFO:root:[   40] Training loss: 0.66966895, Validation loss: 0.67144002, Gradient norm: 0.74733901
INFO:root:[   41] Training loss: 0.66913596, Validation loss: 0.67254461, Gradient norm: 0.74727325
INFO:root:[   42] Training loss: 0.66915143, Validation loss: 0.66893460, Gradient norm: 0.70698476
INFO:root:[   43] Training loss: 0.66909045, Validation loss: 0.67065775, Gradient norm: 0.73820106
INFO:root:[   44] Training loss: 0.66843023, Validation loss: 0.66969828, Gradient norm: 0.64787418
INFO:root:[   45] Training loss: 0.66860054, Validation loss: 0.66791115, Gradient norm: 0.76268171
INFO:root:[   46] Training loss: 0.66841719, Validation loss: 0.67107284, Gradient norm: 0.71487840
INFO:root:[   47] Training loss: 0.66799114, Validation loss: 0.66946207, Gradient norm: 0.77116985
INFO:root:[   48] Training loss: 0.66768933, Validation loss: 0.66865064, Gradient norm: 0.70368082
INFO:root:[   49] Training loss: 0.66707696, Validation loss: 0.66721640, Gradient norm: 0.67201826
INFO:root:[   50] Training loss: 0.66682580, Validation loss: 0.66794118, Gradient norm: 0.60344172
INFO:root:[   51] Training loss: 0.66700072, Validation loss: 0.66786661, Gradient norm: 0.73458306
INFO:root:[   52] Training loss: 0.66617592, Validation loss: 0.66754361, Gradient norm: 0.51703740
INFO:root:[   53] Training loss: 0.66678226, Validation loss: 0.66847268, Gradient norm: 0.76465446
INFO:root:[   54] Training loss: 0.66634977, Validation loss: 0.66805701, Gradient norm: 0.74352102
INFO:root:[   55] Training loss: 0.66601316, Validation loss: 0.66703904, Gradient norm: 0.64179990
INFO:root:[   56] Training loss: 0.66602475, Validation loss: 0.66809336, Gradient norm: 0.71402062
INFO:root:[   57] Training loss: 0.66555057, Validation loss: 0.66707090, Gradient norm: 0.69187411
INFO:root:[   58] Training loss: 0.66557441, Validation loss: 0.66596204, Gradient norm: 0.66416214
INFO:root:[   59] Training loss: 0.66595666, Validation loss: 0.66816352, Gradient norm: 0.71360024
INFO:root:[   60] Training loss: 0.66502467, Validation loss: 0.66600832, Gradient norm: 0.59066370
INFO:root:[   61] Training loss: 0.66523520, Validation loss: 0.66826795, Gradient norm: 0.67696204
INFO:root:[   62] Training loss: 0.66515508, Validation loss: 0.66754245, Gradient norm: 0.67791196
INFO:root:[   63] Training loss: 0.66522646, Validation loss: 0.66928925, Gradient norm: 0.70134140
INFO:root:[   64] Training loss: 0.66496073, Validation loss: 0.66599636, Gradient norm: 0.71437681
INFO:root:[   65] Training loss: 0.66439912, Validation loss: 0.66606770, Gradient norm: 0.54145364
INFO:root:[   66] Training loss: 0.66450712, Validation loss: 0.67001333, Gradient norm: 0.69933414
INFO:root:[   67] Training loss: 0.66485019, Validation loss: 0.66628686, Gradient norm: 0.73157906
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 4204.822s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91757
INFO:root:EnergyScoreTrain: 0.66294
INFO:root:CRPSTrain: 0.55837
INFO:root:Gaussian NLLTrain: 1.53732
INFO:root:CoverageTrain: 0.87629
INFO:root:IntervalWidthTrain: 3.62648
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.92064
INFO:root:EnergyScoreValidation: 0.66531
INFO:root:CRPSValidation: 0.56048
INFO:root:Gaussian NLLValidation: 1.54174
INFO:root:CoverageValidation: 0.87537
INFO:root:IntervalWidthValidation: 3.62289
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91992
INFO:root:EnergyScoreTest: 0.66486
INFO:root:CRPSTest: 0.56021
INFO:root:Gaussian NLLTest: 1.54151
INFO:root:CoverageTest: 0.87553
INFO:root:IntervalWidthTest: 3.62535
INFO:root:###7 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.73266317, Validation loss: 0.71292403, Gradient norm: 0.92453435
INFO:root:[    2] Training loss: 0.70591898, Validation loss: 0.70527151, Gradient norm: 0.63454538
INFO:root:[    3] Training loss: 0.69790607, Validation loss: 0.69560636, Gradient norm: 0.53801727
INFO:root:[    4] Training loss: 0.69408648, Validation loss: 0.69368321, Gradient norm: 0.37835143
INFO:root:[    5] Training loss: 0.69237761, Validation loss: 0.69345481, Gradient norm: 0.51675191
INFO:root:[    6] Training loss: 0.69038543, Validation loss: 0.69107505, Gradient norm: 0.42173466
INFO:root:[    7] Training loss: 0.68953656, Validation loss: 0.68872336, Gradient norm: 0.44745334
INFO:root:[    8] Training loss: 0.68836768, Validation loss: 0.68947686, Gradient norm: 0.36668343
INFO:root:[    9] Training loss: 0.68792816, Validation loss: 0.68827425, Gradient norm: 0.45163155
INFO:root:[   10] Training loss: 0.68674396, Validation loss: 0.68747501, Gradient norm: 0.42209621
INFO:root:[   11] Training loss: 0.68638885, Validation loss: 0.68828274, Gradient norm: 0.48692078
INFO:root:[   12] Training loss: 0.68425688, Validation loss: 0.68267690, Gradient norm: 0.36551766
INFO:root:[   13] Training loss: 0.68296183, Validation loss: 0.68288562, Gradient norm: 0.39874263
INFO:root:[   14] Training loss: 0.68090386, Validation loss: 0.68088528, Gradient norm: 0.34635302
INFO:root:[   15] Training loss: 0.67948734, Validation loss: 0.68208554, Gradient norm: 0.37669287
INFO:root:[   16] Training loss: 0.67750022, Validation loss: 0.67968056, Gradient norm: 0.30924235
INFO:root:[   17] Training loss: 0.67725330, Validation loss: 0.67779430, Gradient norm: 0.38077586
INFO:root:[   18] Training loss: 0.67620026, Validation loss: 0.67679807, Gradient norm: 0.36163546
INFO:root:[   19] Training loss: 0.67479461, Validation loss: 0.67657989, Gradient norm: 0.31003710
INFO:root:[   20] Training loss: 0.67446691, Validation loss: 0.67383826, Gradient norm: 0.38552034
INFO:root:[   21] Training loss: 0.67405618, Validation loss: 0.67371123, Gradient norm: 0.36970506
INFO:root:[   22] Training loss: 0.67313932, Validation loss: 0.67230595, Gradient norm: 0.34240583
INFO:root:[   23] Training loss: 0.67180909, Validation loss: 0.67133462, Gradient norm: 0.30024286
INFO:root:[   24] Training loss: 0.67217524, Validation loss: 0.67121913, Gradient norm: 0.36442654
INFO:root:[   25] Training loss: 0.67088628, Validation loss: 0.67227822, Gradient norm: 0.33514812
INFO:root:[   26] Training loss: 0.67095045, Validation loss: 0.67092307, Gradient norm: 0.36041370
INFO:root:[   27] Training loss: 0.67011096, Validation loss: 0.66961636, Gradient norm: 0.33542186
INFO:root:[   28] Training loss: 0.66989590, Validation loss: 0.66911094, Gradient norm: 0.35439958
INFO:root:[   29] Training loss: 0.66971278, Validation loss: 0.67015166, Gradient norm: 0.33143934
INFO:root:[   30] Training loss: 0.66954574, Validation loss: 0.66721390, Gradient norm: 0.35372884
INFO:root:[   31] Training loss: 0.66867569, Validation loss: 0.66902039, Gradient norm: 0.35744941
INFO:root:[   32] Training loss: 0.66883355, Validation loss: 0.66853726, Gradient norm: 0.38203690
INFO:root:[   33] Training loss: 0.66775435, Validation loss: 0.67014664, Gradient norm: 0.31658445
INFO:root:[   34] Training loss: 0.66803437, Validation loss: 0.66692854, Gradient norm: 0.38605046
INFO:root:[   35] Training loss: 0.66744889, Validation loss: 0.66632813, Gradient norm: 0.34159194
INFO:root:[   36] Training loss: 0.66768376, Validation loss: 0.66755459, Gradient norm: 0.36392270
INFO:root:[   37] Training loss: 0.66702837, Validation loss: 0.66860318, Gradient norm: 0.35597461
INFO:root:[   38] Training loss: 0.66720913, Validation loss: 0.66679901, Gradient norm: 0.33153013
INFO:root:[   39] Training loss: 0.66763672, Validation loss: 0.66922306, Gradient norm: 0.43746707
INFO:root:[   40] Training loss: 0.66603208, Validation loss: 0.66789806, Gradient norm: 0.33008470
INFO:root:[   41] Training loss: 0.66725327, Validation loss: 0.66685254, Gradient norm: 0.42410897
INFO:root:[   42] Training loss: 0.66604350, Validation loss: 0.67079508, Gradient norm: 0.35438494
INFO:root:[   43] Training loss: 0.66653379, Validation loss: 0.66901031, Gradient norm: 0.40907214
INFO:root:[   44] Training loss: 0.66610307, Validation loss: 0.66648546, Gradient norm: 0.40476584
INFO:root:[   45] Training loss: 0.66560817, Validation loss: 0.66732144, Gradient norm: 0.34186037
INFO:root:[   46] Training loss: 0.66556821, Validation loss: 0.66510320, Gradient norm: 0.37000391
INFO:root:[   47] Training loss: 0.66593678, Validation loss: 0.66512604, Gradient norm: 0.45804764
INFO:root:[   48] Training loss: 0.66606240, Validation loss: 0.66431798, Gradient norm: 0.44690725
INFO:root:[   49] Training loss: 0.66552852, Validation loss: 0.66666558, Gradient norm: 0.38218046
INFO:root:[   50] Training loss: 0.66591652, Validation loss: 0.66536224, Gradient norm: 0.42142008
INFO:root:[   51] Training loss: 0.66477994, Validation loss: 0.66539224, Gradient norm: 0.38264015
INFO:root:[   52] Training loss: 0.66498140, Validation loss: 0.66741560, Gradient norm: 0.44410124
INFO:root:[   53] Training loss: 0.66513106, Validation loss: 0.66482628, Gradient norm: 0.44248924
INFO:root:[   54] Training loss: 0.66472215, Validation loss: 0.66390803, Gradient norm: 0.42435098
INFO:root:[   55] Training loss: 0.66440175, Validation loss: 0.66407183, Gradient norm: 0.36110157
INFO:root:[   56] Training loss: 0.66484773, Validation loss: 0.66537746, Gradient norm: 0.45367684
INFO:root:[   57] Training loss: 0.66420116, Validation loss: 0.66927748, Gradient norm: 0.42789647
INFO:root:[   58] Training loss: 0.66472492, Validation loss: 0.66273890, Gradient norm: 0.47474292
INFO:root:[   59] Training loss: 0.66415963, Validation loss: 0.66453406, Gradient norm: 0.42051462
INFO:root:[   60] Training loss: 0.66388442, Validation loss: 0.66414486, Gradient norm: 0.44616385
INFO:root:[   61] Training loss: 0.66399254, Validation loss: 0.66355114, Gradient norm: 0.44024048
INFO:root:[   62] Training loss: 0.66380782, Validation loss: 0.66491892, Gradient norm: 0.42543667
INFO:root:[   63] Training loss: 0.66403690, Validation loss: 0.66349368, Gradient norm: 0.50264747
INFO:root:[   64] Training loss: 0.66372135, Validation loss: 0.66509396, Gradient norm: 0.45858958
INFO:root:[   65] Training loss: 0.66382217, Validation loss: 0.66317801, Gradient norm: 0.50447215
INFO:root:[   66] Training loss: 0.66277920, Validation loss: 0.66271188, Gradient norm: 0.38885940
INFO:root:[   67] Training loss: 0.66302892, Validation loss: 0.66410199, Gradient norm: 0.45066948
INFO:root:[   68] Training loss: 0.66326058, Validation loss: 0.66450242, Gradient norm: 0.50496879
INFO:root:[   69] Training loss: 0.66328454, Validation loss: 0.66696413, Gradient norm: 0.49342120
INFO:root:[   70] Training loss: 0.66277285, Validation loss: 0.66225381, Gradient norm: 0.46739790
INFO:root:[   71] Training loss: 0.66232738, Validation loss: 0.66195256, Gradient norm: 0.45635492
INFO:root:[   72] Training loss: 0.66213589, Validation loss: 0.66742972, Gradient norm: 0.41837415
INFO:root:[   73] Training loss: 0.66280055, Validation loss: 0.66273473, Gradient norm: 0.57066856
INFO:root:[   74] Training loss: 0.66182852, Validation loss: 0.66597568, Gradient norm: 0.52511722
INFO:root:[   75] Training loss: 0.66180543, Validation loss: 0.66293367, Gradient norm: 0.42678857
INFO:root:[   76] Training loss: 0.66144526, Validation loss: 0.66206593, Gradient norm: 0.44450600
INFO:root:[   77] Training loss: 0.66081118, Validation loss: 0.66581383, Gradient norm: 0.43377046
INFO:root:[   78] Training loss: 0.66248535, Validation loss: 0.66618412, Gradient norm: 0.60077749
INFO:root:[   79] Training loss: 0.66126306, Validation loss: 0.66081620, Gradient norm: 0.48745647
INFO:root:[   80] Training loss: 0.66144338, Validation loss: 0.66675715, Gradient norm: 0.52781771
INFO:root:[   81] Training loss: 0.66108374, Validation loss: 0.66145376, Gradient norm: 0.44558082
INFO:root:[   82] Training loss: 0.66159461, Validation loss: 0.66373790, Gradient norm: 0.52996918
INFO:root:[   83] Training loss: 0.66147190, Validation loss: 0.66217139, Gradient norm: 0.56954147
INFO:root:[   84] Training loss: 0.66075013, Validation loss: 0.66212813, Gradient norm: 0.48336725
INFO:root:[   85] Training loss: 0.66094865, Validation loss: 0.66081145, Gradient norm: 0.54237645
INFO:root:[   86] Training loss: 0.66064345, Validation loss: 0.66202412, Gradient norm: 0.47180764
INFO:root:[   87] Training loss: 0.66086858, Validation loss: 0.66169284, Gradient norm: 0.55863048
INFO:root:[   88] Training loss: 0.66045527, Validation loss: 0.66067724, Gradient norm: 0.40516363
INFO:root:[   89] Training loss: 0.66136274, Validation loss: 0.66350660, Gradient norm: 0.60851104
INFO:root:[   90] Training loss: 0.66002593, Validation loss: 0.66122348, Gradient norm: 0.50151854
INFO:root:[   91] Training loss: 0.66057861, Validation loss: 0.66520598, Gradient norm: 0.51253875
INFO:root:[   92] Training loss: 0.66025639, Validation loss: 0.66202766, Gradient norm: 0.49792918
INFO:root:[   93] Training loss: 0.66078944, Validation loss: 0.66059015, Gradient norm: 0.57770054
INFO:root:[   94] Training loss: 0.66008019, Validation loss: 0.66077485, Gradient norm: 0.52923975
INFO:root:[   95] Training loss: 0.66021599, Validation loss: 0.66274763, Gradient norm: 0.52176379
INFO:root:[   96] Training loss: 0.65965552, Validation loss: 0.66070190, Gradient norm: 0.47528938
INFO:root:[   97] Training loss: 0.66021592, Validation loss: 0.65928159, Gradient norm: 0.52118802
INFO:root:[   98] Training loss: 0.65985665, Validation loss: 0.66140305, Gradient norm: 0.54819431
INFO:root:[   99] Training loss: 0.65940959, Validation loss: 0.65976079, Gradient norm: 0.49901585
INFO:root:[  100] Training loss: 0.66004688, Validation loss: 0.66043323, Gradient norm: 0.50922024
INFO:root:[  101] Training loss: 0.65982706, Validation loss: 0.66046634, Gradient norm: 0.50109204
INFO:root:[  102] Training loss: 0.66002033, Validation loss: 0.66175351, Gradient norm: 0.57757546
INFO:root:[  103] Training loss: 0.65915856, Validation loss: 0.66044767, Gradient norm: 0.47113687
INFO:root:[  104] Training loss: 0.65916151, Validation loss: 0.66218447, Gradient norm: 0.47186023
INFO:root:[  105] Training loss: 0.65911411, Validation loss: 0.65940601, Gradient norm: 0.51938369
INFO:root:[  106] Training loss: 0.66005772, Validation loss: 0.66122106, Gradient norm: 0.61422262
INFO:root:EP 106: Early stopping
INFO:root:Training the model took 6624.818s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.9101
INFO:root:EnergyScoreTrain: 0.65799
INFO:root:CRPSTrain: 0.67264
INFO:root:Gaussian NLLTrain: 94.89589
INFO:root:CoverageTrain: 0.51107
INFO:root:IntervalWidthTrain: 2.52324
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91167
INFO:root:EnergyScoreValidation: 0.65925
INFO:root:CRPSValidation: 0.67393
INFO:root:Gaussian NLLValidation: 176545.44375
INFO:root:CoverageValidation: 0.50996
INFO:root:IntervalWidthValidation: 2.51926
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91187
INFO:root:EnergyScoreTest: 0.65953
INFO:root:CRPSTest: 0.67413
INFO:root:Gaussian NLLTest: 31.4825
INFO:root:CoverageTest: 0.51019
INFO:root:IntervalWidthTest: 2.52261
INFO:root:###8 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72789234, Validation loss: 0.70676428, Gradient norm: 0.77311118
INFO:root:[    2] Training loss: 0.70288957, Validation loss: 0.70054103, Gradient norm: 0.35278491
INFO:root:[    3] Training loss: 0.69735116, Validation loss: 0.69540041, Gradient norm: 0.35236133
INFO:root:[    4] Training loss: 0.69446603, Validation loss: 0.69327381, Gradient norm: 0.29354297
INFO:root:[    5] Training loss: 0.69216018, Validation loss: 0.69160795, Gradient norm: 0.31720231
INFO:root:[    6] Training loss: 0.68979692, Validation loss: 0.68988188, Gradient norm: 0.27751964
INFO:root:[    7] Training loss: 0.68851789, Validation loss: 0.68946655, Gradient norm: 0.36036318
INFO:root:[    8] Training loss: 0.68575855, Validation loss: 0.68788001, Gradient norm: 0.27041025
INFO:root:[    9] Training loss: 0.68509120, Validation loss: 0.68533070, Gradient norm: 0.37488131
INFO:root:[   10] Training loss: 0.68266924, Validation loss: 0.68449627, Gradient norm: 0.26317462
INFO:root:[   11] Training loss: 0.68171834, Validation loss: 0.68283069, Gradient norm: 0.29690055
INFO:root:[   12] Training loss: 0.68125356, Validation loss: 0.67912360, Gradient norm: 0.29833576
INFO:root:[   13] Training loss: 0.67952354, Validation loss: 0.68063591, Gradient norm: 0.34266965
INFO:root:[   14] Training loss: 0.67874363, Validation loss: 0.67980759, Gradient norm: 0.36527801
INFO:root:[   15] Training loss: 0.67660782, Validation loss: 0.67684282, Gradient norm: 0.29385327
INFO:root:[   16] Training loss: 0.67601179, Validation loss: 0.67773633, Gradient norm: 0.35304030
INFO:root:[   17] Training loss: 0.67558714, Validation loss: 0.67630709, Gradient norm: 0.37110295
INFO:root:[   18] Training loss: 0.67423818, Validation loss: 0.67603203, Gradient norm: 0.35158231
INFO:root:[   19] Training loss: 0.67363103, Validation loss: 0.67310331, Gradient norm: 0.31551862
INFO:root:[   20] Training loss: 0.67406811, Validation loss: 0.67537305, Gradient norm: 0.39944417
INFO:root:[   21] Training loss: 0.67261469, Validation loss: 0.67196581, Gradient norm: 0.32266266
INFO:root:[   22] Training loss: 0.67258304, Validation loss: 0.67113965, Gradient norm: 0.40529102
INFO:root:[   23] Training loss: 0.67258077, Validation loss: 0.67194085, Gradient norm: 0.39726048
INFO:root:[   24] Training loss: 0.67140919, Validation loss: 0.67136764, Gradient norm: 0.35159315
INFO:root:[   25] Training loss: 0.67183055, Validation loss: 0.67179206, Gradient norm: 0.46890108
INFO:root:[   26] Training loss: 0.67134715, Validation loss: 0.66949052, Gradient norm: 0.42255907
INFO:root:[   27] Training loss: 0.67008914, Validation loss: 0.67316467, Gradient norm: 0.35672785
INFO:root:[   28] Training loss: 0.67038776, Validation loss: 0.67410401, Gradient norm: 0.41628721
INFO:root:[   29] Training loss: 0.67151440, Validation loss: 0.66974129, Gradient norm: 0.50573109
INFO:root:[   30] Training loss: 0.66971706, Validation loss: 0.66922366, Gradient norm: 0.42132309
INFO:root:[   31] Training loss: 0.66980362, Validation loss: 0.66854141, Gradient norm: 0.49581345
INFO:root:[   32] Training loss: 0.66936883, Validation loss: 0.67099878, Gradient norm: 0.44896991
INFO:root:[   33] Training loss: 0.66972589, Validation loss: 0.67205492, Gradient norm: 0.48178712
INFO:root:[   34] Training loss: 0.66885495, Validation loss: 0.66914471, Gradient norm: 0.46599341
INFO:root:[   35] Training loss: 0.66928831, Validation loss: 0.66692472, Gradient norm: 0.49138879
INFO:root:[   36] Training loss: 0.66881347, Validation loss: 0.67192644, Gradient norm: 0.51005019
INFO:root:[   37] Training loss: 0.66876856, Validation loss: 0.66825968, Gradient norm: 0.48112338
INFO:root:[   38] Training loss: 0.66882095, Validation loss: 0.66826475, Gradient norm: 0.51950241
INFO:root:[   39] Training loss: 0.66816413, Validation loss: 0.67026491, Gradient norm: 0.48721832
INFO:root:[   40] Training loss: 0.66694533, Validation loss: 0.66965527, Gradient norm: 0.45460023
INFO:root:[   41] Training loss: 0.66848544, Validation loss: 0.66768883, Gradient norm: 0.57263757
INFO:root:[   42] Training loss: 0.66817613, Validation loss: 0.66857187, Gradient norm: 0.58884893
INFO:root:[   43] Training loss: 0.66778162, Validation loss: 0.67276819, Gradient norm: 0.50249242
INFO:root:[   44] Training loss: 0.66821167, Validation loss: 0.66908147, Gradient norm: 0.58375953
INFO:root:[   45] Training loss: 0.66697977, Validation loss: 0.67154007, Gradient norm: 0.52499622
INFO:root:[   46] Training loss: 0.66695240, Validation loss: 0.66836254, Gradient norm: 0.51688310
INFO:root:[   47] Training loss: 0.66701583, Validation loss: 0.66710353, Gradient norm: 0.58119900
INFO:root:[   48] Training loss: 0.66653741, Validation loss: 0.66629673, Gradient norm: 0.49120328
INFO:root:[   49] Training loss: 0.66739775, Validation loss: 0.66610663, Gradient norm: 0.63984534
INFO:root:[   50] Training loss: 0.66690774, Validation loss: 0.66838390, Gradient norm: 0.55326679
INFO:root:[   51] Training loss: 0.66613830, Validation loss: 0.66891736, Gradient norm: 0.51828559
INFO:root:[   52] Training loss: 0.66586681, Validation loss: 0.66983979, Gradient norm: 0.59714065
INFO:root:[   53] Training loss: 0.66657076, Validation loss: 0.66479677, Gradient norm: 0.56674678
INFO:root:[   54] Training loss: 0.66600551, Validation loss: 0.66788559, Gradient norm: 0.54293908
INFO:root:[   55] Training loss: 0.66612505, Validation loss: 0.66731207, Gradient norm: 0.61192364
INFO:root:[   56] Training loss: 0.66507771, Validation loss: 0.66735278, Gradient norm: 0.49263938
INFO:root:[   57] Training loss: 0.66546018, Validation loss: 0.66518211, Gradient norm: 0.61118195
INFO:root:[   58] Training loss: 0.66528891, Validation loss: 0.66555181, Gradient norm: 0.47584809
INFO:root:[   59] Training loss: 0.66539420, Validation loss: 0.66572265, Gradient norm: 0.61140900
INFO:root:[   60] Training loss: 0.66472591, Validation loss: 0.66519152, Gradient norm: 0.56172570
INFO:root:[   61] Training loss: 0.66494611, Validation loss: 0.66699679, Gradient norm: 0.58168902
INFO:root:[   62] Training loss: 0.66472066, Validation loss: 0.66406620, Gradient norm: 0.53314391
INFO:root:[   63] Training loss: 0.66449449, Validation loss: 0.66438807, Gradient norm: 0.51145661
INFO:root:[   64] Training loss: 0.66444425, Validation loss: 0.66388684, Gradient norm: 0.57385979
INFO:root:[   65] Training loss: 0.66430129, Validation loss: 0.66494428, Gradient norm: 0.58360389
INFO:root:[   66] Training loss: 0.66388654, Validation loss: 0.66445451, Gradient norm: 0.52742607
INFO:root:[   67] Training loss: 0.66436877, Validation loss: 0.66443138, Gradient norm: 0.54589189
INFO:root:[   68] Training loss: 0.66418607, Validation loss: 0.66526744, Gradient norm: 0.63778500
INFO:root:[   69] Training loss: 0.66357304, Validation loss: 0.66737203, Gradient norm: 0.50318167
INFO:root:[   70] Training loss: 0.66416046, Validation loss: 0.66404551, Gradient norm: 0.56117340
INFO:root:[   71] Training loss: 0.66284200, Validation loss: 0.66417918, Gradient norm: 0.52220948
INFO:root:[   72] Training loss: 0.66382287, Validation loss: 0.66394726, Gradient norm: 0.56231018
INFO:root:[   73] Training loss: 0.66377672, Validation loss: 0.66682448, Gradient norm: 0.55961513
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 4582.906s.
INFO:root:Emptying the cuda cache took 0.021s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91605
INFO:root:EnergyScoreTrain: 0.66308
INFO:root:CRPSTrain: 0.67704
INFO:root:Gaussian NLLTrain: 8713.28028
INFO:root:CoverageTrain: 0.51579
INFO:root:IntervalWidthTrain: 2.4394
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91756
INFO:root:EnergyScoreValidation: 0.66441
INFO:root:CRPSValidation: 0.67829
INFO:root:Gaussian NLLValidation: 57264.86933
INFO:root:CoverageValidation: 0.51451
INFO:root:IntervalWidthValidation: 2.43505
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91747
INFO:root:EnergyScoreTest: 0.66438
INFO:root:CRPSTest: 0.67826
INFO:root:Gaussian NLLTest: 239229.68528
INFO:root:CoverageTest: 0.51545
INFO:root:IntervalWidthTest: 2.44122
INFO:root:###9 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1687601
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72547425, Validation loss: 0.70863513, Gradient norm: 0.41848757
INFO:root:[    2] Training loss: 0.70351963, Validation loss: 0.70635059, Gradient norm: 0.20348040
INFO:root:[    3] Training loss: 0.69979810, Validation loss: 0.69896775, Gradient norm: 0.22462734
INFO:root:[    4] Training loss: 0.69755968, Validation loss: 0.69708073, Gradient norm: 0.20139476
INFO:root:[    5] Training loss: 0.69568730, Validation loss: 0.69532785, Gradient norm: 0.23790020
INFO:root:[    6] Training loss: 0.69384188, Validation loss: 0.69405880, Gradient norm: 0.21636413
INFO:root:[    7] Training loss: 0.69213785, Validation loss: 0.69296129, Gradient norm: 0.24336156
INFO:root:[    8] Training loss: 0.69113683, Validation loss: 0.68968940, Gradient norm: 0.34084815
INFO:root:[    9] Training loss: 0.68866831, Validation loss: 0.68776513, Gradient norm: 0.30450050
INFO:root:[   10] Training loss: 0.68656568, Validation loss: 0.68776941, Gradient norm: 0.29639883
INFO:root:[   11] Training loss: 0.68521169, Validation loss: 0.68542698, Gradient norm: 0.32715766
INFO:root:[   12] Training loss: 0.68402109, Validation loss: 0.68663270, Gradient norm: 0.36003792
INFO:root:[   13] Training loss: 0.68163379, Validation loss: 0.68173864, Gradient norm: 0.33277394
INFO:root:[   14] Training loss: 0.68130724, Validation loss: 0.68267293, Gradient norm: 0.36482113
INFO:root:[   15] Training loss: 0.68027925, Validation loss: 0.68172860, Gradient norm: 0.35663938
INFO:root:[   16] Training loss: 0.67953893, Validation loss: 0.67869604, Gradient norm: 0.39983944
INFO:root:[   17] Training loss: 0.67883165, Validation loss: 0.68323897, Gradient norm: 0.37261399
INFO:root:[   18] Training loss: 0.67797164, Validation loss: 0.68076817, Gradient norm: 0.41194495
INFO:root:[   19] Training loss: 0.67686308, Validation loss: 0.67673814, Gradient norm: 0.34202824
INFO:root:[   20] Training loss: 0.67745299, Validation loss: 0.67601606, Gradient norm: 0.44930824
INFO:root:[   21] Training loss: 0.67649383, Validation loss: 0.67906727, Gradient norm: 0.40070837
INFO:root:[   22] Training loss: 0.67673187, Validation loss: 0.67678748, Gradient norm: 0.49947585
INFO:root:[   23] Training loss: 0.67579592, Validation loss: 0.67814565, Gradient norm: 0.49319762
INFO:root:[   24] Training loss: 0.67600543, Validation loss: 0.67481092, Gradient norm: 0.53316413
INFO:root:[   25] Training loss: 0.67494999, Validation loss: 0.67403860, Gradient norm: 0.49164020
INFO:root:[   26] Training loss: 0.67553565, Validation loss: 0.67457856, Gradient norm: 0.55322344
INFO:root:[   27] Training loss: 0.67499204, Validation loss: 0.67514844, Gradient norm: 0.54656465
INFO:root:[   28] Training loss: 0.67520649, Validation loss: 0.67290198, Gradient norm: 0.62909780
INFO:root:[   29] Training loss: 0.67427602, Validation loss: 0.67487792, Gradient norm: 0.63189322
INFO:root:[   30] Training loss: 0.67381999, Validation loss: 0.67461402, Gradient norm: 0.60268417
INFO:root:[   31] Training loss: 0.67339438, Validation loss: 0.67911934, Gradient norm: 0.59714231
INFO:root:[   32] Training loss: 0.67379935, Validation loss: 0.67339636, Gradient norm: 0.69681740
INFO:root:[   33] Training loss: 0.67331248, Validation loss: 0.67209408, Gradient norm: 0.65123964
INFO:root:[   34] Training loss: 0.67299433, Validation loss: 0.67589417, Gradient norm: 0.61130734
INFO:root:[   35] Training loss: 0.67251254, Validation loss: 0.67266900, Gradient norm: 0.61300728
INFO:root:[   36] Training loss: 0.67354939, Validation loss: 0.67203309, Gradient norm: 0.72474198
INFO:root:[   37] Training loss: 0.67246084, Validation loss: 0.67576400, Gradient norm: 0.61202375
INFO:root:[   38] Training loss: 0.67330616, Validation loss: 0.67585904, Gradient norm: 0.71489246
INFO:root:[   39] Training loss: 0.67277124, Validation loss: 0.67371115, Gradient norm: 0.71735780
INFO:root:[   40] Training loss: 0.67239705, Validation loss: 0.67389478, Gradient norm: 0.68271306
INFO:root:[   41] Training loss: 0.67167573, Validation loss: 0.66968922, Gradient norm: 0.60415342
INFO:root:[   42] Training loss: 0.67140625, Validation loss: 0.67114204, Gradient norm: 0.65394963
INFO:root:[   43] Training loss: 0.67222396, Validation loss: 0.67292667, Gradient norm: 0.68258024
INFO:root:[   44] Training loss: 0.67170967, Validation loss: 0.67188083, Gradient norm: 0.68151192
INFO:root:[   45] Training loss: 0.67101965, Validation loss: 0.67341304, Gradient norm: 0.58895902
INFO:root:[   46] Training loss: 0.67145483, Validation loss: 0.67126422, Gradient norm: 0.72529443
INFO:root:[   47] Training loss: 0.67086404, Validation loss: 0.67563787, Gradient norm: 0.60504948
INFO:root:[   48] Training loss: 0.67122069, Validation loss: 0.67315852, Gradient norm: 0.67970407
INFO:root:[   49] Training loss: 0.67087527, Validation loss: 0.67000052, Gradient norm: 0.65164840
INFO:root:[   50] Training loss: 0.67063725, Validation loss: 0.67161900, Gradient norm: 0.60335417
INFO:root:[   51] Training loss: 0.67103956, Validation loss: 0.67135787, Gradient norm: 0.70483204
INFO:root:[   52] Training loss: 0.66971645, Validation loss: 0.67238665, Gradient norm: 0.59388253
INFO:root:[   53] Training loss: 0.67074596, Validation loss: 0.67111321, Gradient norm: 0.67832980
INFO:root:[   54] Training loss: 0.67054288, Validation loss: 0.67002488, Gradient norm: 0.66125202
INFO:root:[   55] Training loss: 0.67000331, Validation loss: 0.67178921, Gradient norm: 0.62822256
INFO:root:[   56] Training loss: 0.66984760, Validation loss: 0.67133075, Gradient norm: 0.58651460
INFO:root:[   57] Training loss: 0.66968292, Validation loss: 0.66968415, Gradient norm: 0.63426438
INFO:root:[   58] Training loss: 0.66967119, Validation loss: 0.66798896, Gradient norm: 0.56836893
INFO:root:[   59] Training loss: 0.66974208, Validation loss: 0.66843008, Gradient norm: 0.61998418
INFO:root:[   60] Training loss: 0.66954041, Validation loss: 0.66805910, Gradient norm: 0.65031043
INFO:root:[   61] Training loss: 0.66954673, Validation loss: 0.66880137, Gradient norm: 0.67130962
INFO:root:[   62] Training loss: 0.66964469, Validation loss: 0.67017166, Gradient norm: 0.64534727
INFO:root:[   63] Training loss: 0.66969140, Validation loss: 0.66883581, Gradient norm: 0.65774205
INFO:root:[   64] Training loss: 0.66908495, Validation loss: 0.66967820, Gradient norm: 0.56360445
INFO:root:[   65] Training loss: 0.66890920, Validation loss: 0.66965568, Gradient norm: 0.60966904
INFO:root:[   66] Training loss: 0.66849614, Validation loss: 0.66778764, Gradient norm: 0.54341667
INFO:root:[   67] Training loss: 0.66923048, Validation loss: 0.67174395, Gradient norm: 0.65205388
INFO:root:[   68] Training loss: 0.66838973, Validation loss: 0.66791270, Gradient norm: 0.61007768
INFO:root:[   69] Training loss: 0.66863224, Validation loss: 0.67092628, Gradient norm: 0.64056796
INFO:root:[   70] Training loss: 0.66797182, Validation loss: 0.66807355, Gradient norm: 0.60341512
INFO:root:[   71] Training loss: 0.66770890, Validation loss: 0.66839397, Gradient norm: 0.56909799
INFO:root:[   72] Training loss: 0.66794753, Validation loss: 0.66649997, Gradient norm: 0.63762164
INFO:root:[   73] Training loss: 0.66821827, Validation loss: 0.66971304, Gradient norm: 0.66313647
INFO:root:[   74] Training loss: 0.66758430, Validation loss: 0.66815756, Gradient norm: 0.62806917
INFO:root:[   75] Training loss: 0.66829907, Validation loss: 0.67120810, Gradient norm: 0.63988669
INFO:root:[   76] Training loss: 0.66763119, Validation loss: 0.66814753, Gradient norm: 0.64623872
INFO:root:[   77] Training loss: 0.66698478, Validation loss: 0.66768505, Gradient norm: 0.57264728
INFO:root:[   78] Training loss: 0.66779611, Validation loss: 0.66728562, Gradient norm: 0.63776530
INFO:root:[   79] Training loss: 0.66734935, Validation loss: 0.66798295, Gradient norm: 0.69454948
INFO:root:[   80] Training loss: 0.66623504, Validation loss: 0.67020161, Gradient norm: 0.57831584
INFO:root:[   81] Training loss: 0.66713749, Validation loss: 0.66903828, Gradient norm: 0.67729420
INFO:root:EP 81: Early stopping
INFO:root:Training the model took 5059.31s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91921
INFO:root:EnergyScoreTrain: 0.66512
INFO:root:CRPSTrain: 0.68851
INFO:root:Gaussian NLLTrain: 234343272.76667
INFO:root:CoverageTrain: 0.52037
INFO:root:IntervalWidthTrain: 2.63964
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.92053
INFO:root:EnergyScoreValidation: 0.66625
INFO:root:CRPSValidation: 0.68953
INFO:root:Gaussian NLLValidation: 252161193.3
INFO:root:CoverageValidation: 0.51938
INFO:root:IntervalWidthValidation: 2.63374
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.9202
INFO:root:EnergyScoreTest: 0.66608
INFO:root:CRPSTest: 0.68965
INFO:root:Gaussian NLLTest: 224186387.888
INFO:root:CoverageTest: 0.51966
INFO:root:IntervalWidthTest: 2.63731
INFO:root:###10 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 176160768
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.70940571, Validation loss: 0.69849473, Gradient norm: 0.07885451
INFO:root:[    2] Training loss: 0.69533402, Validation loss: 0.69397553, Gradient norm: 0.14268536
INFO:root:[    3] Training loss: 0.69195998, Validation loss: 0.69051539, Gradient norm: 0.16028905
INFO:root:[    4] Training loss: 0.68953717, Validation loss: 0.68964390, Gradient norm: 0.20965335
INFO:root:[    5] Training loss: 0.68803159, Validation loss: 0.68778457, Gradient norm: 0.21479224
INFO:root:[    6] Training loss: 0.68663090, Validation loss: 0.68635352, Gradient norm: 0.22056125
INFO:root:[    7] Training loss: 0.68566029, Validation loss: 0.68745199, Gradient norm: 0.22693259
INFO:root:[    8] Training loss: 0.68478302, Validation loss: 0.68451474, Gradient norm: 0.25312064
INFO:root:[    9] Training loss: 0.68369513, Validation loss: 0.68515406, Gradient norm: 0.21512740
INFO:root:[   10] Training loss: 0.68288117, Validation loss: 0.68330633, Gradient norm: 0.23934631
INFO:root:[   11] Training loss: 0.68201104, Validation loss: 0.68233884, Gradient norm: 0.26440760
INFO:root:[   12] Training loss: 0.68126931, Validation loss: 0.68129631, Gradient norm: 0.26941167
INFO:root:[   13] Training loss: 0.68010129, Validation loss: 0.67972984, Gradient norm: 0.26096809
INFO:root:[   14] Training loss: 0.67909472, Validation loss: 0.67874790, Gradient norm: 0.25329372
INFO:root:[   15] Training loss: 0.67842753, Validation loss: 0.67818778, Gradient norm: 0.27758846
INFO:root:[   16] Training loss: 0.67762825, Validation loss: 0.67755906, Gradient norm: 0.27655148
INFO:root:[   17] Training loss: 0.67692858, Validation loss: 0.67742650, Gradient norm: 0.23516374
INFO:root:[   18] Training loss: 0.67644572, Validation loss: 0.67648256, Gradient norm: 0.28496754
INFO:root:[   19] Training loss: 0.67567701, Validation loss: 0.67677160, Gradient norm: 0.23551986
INFO:root:[   20] Training loss: 0.67573461, Validation loss: 0.67671851, Gradient norm: 0.26529712
INFO:root:[   21] Training loss: 0.67487799, Validation loss: 0.67664231, Gradient norm: 0.26286603
INFO:root:[   22] Training loss: 0.67489179, Validation loss: 0.67488382, Gradient norm: 0.26062468
INFO:root:[   23] Training loss: 0.67403928, Validation loss: 0.67339837, Gradient norm: 0.26477673
INFO:root:[   24] Training loss: 0.67381072, Validation loss: 0.67448231, Gradient norm: 0.25099460
INFO:root:[   25] Training loss: 0.67347766, Validation loss: 0.67458843, Gradient norm: 0.23644594
INFO:root:[   26] Training loss: 0.67362938, Validation loss: 0.67513150, Gradient norm: 0.27907886
INFO:root:[   27] Training loss: 0.67295191, Validation loss: 0.67307082, Gradient norm: 0.23566564
INFO:root:[   28] Training loss: 0.67272131, Validation loss: 0.67409277, Gradient norm: 0.28076549
INFO:root:[   29] Training loss: 0.67251816, Validation loss: 0.67326471, Gradient norm: 0.23364713
INFO:root:[   30] Training loss: 0.67225870, Validation loss: 0.67268776, Gradient norm: 0.24105729
INFO:root:[   31] Training loss: 0.67210080, Validation loss: 0.67353162, Gradient norm: 0.26041701
INFO:root:[   32] Training loss: 0.67202820, Validation loss: 0.67342883, Gradient norm: 0.24818756
INFO:root:[   33] Training loss: 0.67154818, Validation loss: 0.67166813, Gradient norm: 0.24845258
INFO:root:[   34] Training loss: 0.67147391, Validation loss: 0.67136208, Gradient norm: 0.22941568
INFO:root:[   35] Training loss: 0.67139407, Validation loss: 0.67216843, Gradient norm: 0.24960596
INFO:root:[   36] Training loss: 0.67116691, Validation loss: 0.67153820, Gradient norm: 0.20997342
INFO:root:[   37] Training loss: 0.67131591, Validation loss: 0.67179211, Gradient norm: 0.25683327
INFO:root:[   38] Training loss: 0.67063571, Validation loss: 0.67092418, Gradient norm: 0.22095144
INFO:root:[   39] Training loss: 0.67059044, Validation loss: 0.67128802, Gradient norm: 0.24793236
INFO:root:[   40] Training loss: 0.67042100, Validation loss: 0.67095518, Gradient norm: 0.23179473
INFO:root:[   41] Training loss: 0.67008767, Validation loss: 0.67067315, Gradient norm: 0.23125050
INFO:root:[   42] Training loss: 0.66998291, Validation loss: 0.67038451, Gradient norm: 0.24250774
INFO:root:[   43] Training loss: 0.66928998, Validation loss: 0.66990344, Gradient norm: 0.22179803
INFO:root:[   44] Training loss: 0.66904208, Validation loss: 0.66977676, Gradient norm: 0.19631391
INFO:root:[   45] Training loss: 0.66883367, Validation loss: 0.66885907, Gradient norm: 0.23721484
INFO:root:[   46] Training loss: 0.66859662, Validation loss: 0.66912856, Gradient norm: 0.25866466
INFO:root:[   47] Training loss: 0.66818065, Validation loss: 0.66879682, Gradient norm: 0.20643127
INFO:root:[   48] Training loss: 0.66772082, Validation loss: 0.66939936, Gradient norm: 0.21848841
INFO:root:[   49] Training loss: 0.66794917, Validation loss: 0.66889718, Gradient norm: 0.24040618
INFO:root:[   50] Training loss: 0.66735140, Validation loss: 0.66840555, Gradient norm: 0.23842895
INFO:root:[   51] Training loss: 0.66723274, Validation loss: 0.66762390, Gradient norm: 0.22619661
INFO:root:[   52] Training loss: 0.66713930, Validation loss: 0.66692423, Gradient norm: 0.22409943
INFO:root:[   53] Training loss: 0.66659042, Validation loss: 0.66864944, Gradient norm: 0.22778649
INFO:root:[   54] Training loss: 0.66641765, Validation loss: 0.66828785, Gradient norm: 0.20572865
INFO:root:[   55] Training loss: 0.66634039, Validation loss: 0.66709944, Gradient norm: 0.21411680
INFO:root:[   56] Training loss: 0.66652090, Validation loss: 0.66798679, Gradient norm: 0.23659674
INFO:root:[   57] Training loss: 0.66601391, Validation loss: 0.66742063, Gradient norm: 0.19111686
INFO:root:[   58] Training loss: 0.66612910, Validation loss: 0.66692082, Gradient norm: 0.22763844
INFO:root:[   59] Training loss: 0.66578760, Validation loss: 0.66699450, Gradient norm: 0.23339200
INFO:root:[   60] Training loss: 0.66578496, Validation loss: 0.66718861, Gradient norm: 0.20618649
INFO:root:[   61] Training loss: 0.66580400, Validation loss: 0.66680672, Gradient norm: 0.22327025
INFO:root:[   62] Training loss: 0.66575331, Validation loss: 0.66630921, Gradient norm: 0.21888347
INFO:root:[   63] Training loss: 0.66514296, Validation loss: 0.66674353, Gradient norm: 0.19201089
INFO:root:[   64] Training loss: 0.66556519, Validation loss: 0.66610180, Gradient norm: 0.22926085
INFO:root:[   65] Training loss: 0.66529885, Validation loss: 0.66673386, Gradient norm: 0.20360892
INFO:root:[   66] Training loss: 0.66497986, Validation loss: 0.66715558, Gradient norm: 0.22568899
INFO:root:[   67] Training loss: 0.66498462, Validation loss: 0.66747155, Gradient norm: 0.20542507
INFO:root:[   68] Training loss: 0.66475356, Validation loss: 0.66584671, Gradient norm: 0.19351328
INFO:root:[   69] Training loss: 0.66499058, Validation loss: 0.66510893, Gradient norm: 0.23258321
INFO:root:[   70] Training loss: 0.66458410, Validation loss: 0.66482450, Gradient norm: 0.22685512
INFO:root:[   71] Training loss: 0.66454553, Validation loss: 0.66528177, Gradient norm: 0.19650314
INFO:root:[   72] Training loss: 0.66434370, Validation loss: 0.66511554, Gradient norm: 0.20435251
INFO:root:[   73] Training loss: 0.66389422, Validation loss: 0.66483413, Gradient norm: 0.18345958
INFO:root:[   74] Training loss: 0.66394032, Validation loss: 0.66555488, Gradient norm: 0.21785610
INFO:root:[   75] Training loss: 0.66383683, Validation loss: 0.66458255, Gradient norm: 0.22877524
INFO:root:[   76] Training loss: 0.66347487, Validation loss: 0.66384083, Gradient norm: 0.19455685
INFO:root:[   77] Training loss: 0.66353118, Validation loss: 0.66438789, Gradient norm: 0.21444082
INFO:root:[   78] Training loss: 0.66360941, Validation loss: 0.66448563, Gradient norm: 0.21898908
INFO:root:[   79] Training loss: 0.66316096, Validation loss: 0.66397945, Gradient norm: 0.21518133
INFO:root:[   80] Training loss: 0.66282766, Validation loss: 0.66460142, Gradient norm: 0.19539246
INFO:root:[   81] Training loss: 0.66311384, Validation loss: 0.66455307, Gradient norm: 0.20795273
INFO:root:[   82] Training loss: 0.66282032, Validation loss: 0.66351803, Gradient norm: 0.22034902
INFO:root:[   83] Training loss: 0.66258152, Validation loss: 0.66292723, Gradient norm: 0.19608153
INFO:root:[   84] Training loss: 0.66268054, Validation loss: 0.66235730, Gradient norm: 0.20921931
INFO:root:[   85] Training loss: 0.66258233, Validation loss: 0.66358265, Gradient norm: 0.20609199
INFO:root:[   86] Training loss: 0.66230467, Validation loss: 0.66336652, Gradient norm: 0.21479177
INFO:root:[   87] Training loss: 0.66215576, Validation loss: 0.66332115, Gradient norm: 0.18496464
INFO:root:[   88] Training loss: 0.66215874, Validation loss: 0.66327200, Gradient norm: 0.21540498
INFO:root:[   89] Training loss: 0.66207633, Validation loss: 0.66339440, Gradient norm: 0.20618662
INFO:root:[   90] Training loss: 0.66203195, Validation loss: 0.66333686, Gradient norm: 0.20773362
INFO:root:[   91] Training loss: 0.66179689, Validation loss: 0.66395088, Gradient norm: 0.19462015
INFO:root:[   92] Training loss: 0.66204476, Validation loss: 0.66300232, Gradient norm: 0.21772305
INFO:root:[   93] Training loss: 0.66177738, Validation loss: 0.66225101, Gradient norm: 0.20196875
INFO:root:[   94] Training loss: 0.66172623, Validation loss: 0.66258750, Gradient norm: 0.20012210
INFO:root:[   95] Training loss: 0.66163505, Validation loss: 0.66237443, Gradient norm: 0.21629756
INFO:root:[   96] Training loss: 0.66165471, Validation loss: 0.66274122, Gradient norm: 0.19621562
INFO:root:[   97] Training loss: 0.66144712, Validation loss: 0.66315556, Gradient norm: 0.20665758
INFO:root:[   98] Training loss: 0.66155117, Validation loss: 0.66292980, Gradient norm: 0.19267525
INFO:root:[   99] Training loss: 0.66153849, Validation loss: 0.66202305, Gradient norm: 0.21661829
INFO:root:[  100] Training loss: 0.66102005, Validation loss: 0.66260705, Gradient norm: 0.18389407
INFO:root:[  101] Training loss: 0.66139683, Validation loss: 0.66273743, Gradient norm: 0.21648974
INFO:root:[  102] Training loss: 0.66093202, Validation loss: 0.66274817, Gradient norm: 0.20241661
INFO:root:[  103] Training loss: 0.66111489, Validation loss: 0.66222751, Gradient norm: 0.21749569
INFO:root:[  104] Training loss: 0.66116633, Validation loss: 0.66193742, Gradient norm: 0.19565695
INFO:root:[  105] Training loss: 0.66098928, Validation loss: 0.66199087, Gradient norm: 0.19826004
INFO:root:[  106] Training loss: 0.66093810, Validation loss: 0.66164001, Gradient norm: 0.21156658
INFO:root:[  107] Training loss: 0.66110667, Validation loss: 0.66139482, Gradient norm: 0.21699054
INFO:root:[  108] Training loss: 0.66092694, Validation loss: 0.66257546, Gradient norm: 0.19158330
INFO:root:[  109] Training loss: 0.66107771, Validation loss: 0.66195550, Gradient norm: 0.19914291
INFO:root:[  110] Training loss: 0.66083661, Validation loss: 0.66190170, Gradient norm: 0.21213672
INFO:root:[  111] Training loss: 0.66076601, Validation loss: 0.66325757, Gradient norm: 0.20062229
INFO:root:[  112] Training loss: 0.66064898, Validation loss: 0.66174625, Gradient norm: 0.20754242
INFO:root:[  113] Training loss: 0.66065342, Validation loss: 0.66148391, Gradient norm: 0.19154953
INFO:root:[  114] Training loss: 0.66061842, Validation loss: 0.66138217, Gradient norm: 0.21174787
INFO:root:[  115] Training loss: 0.66064235, Validation loss: 0.66111285, Gradient norm: 0.19785915
INFO:root:[  116] Training loss: 0.66046627, Validation loss: 0.66055720, Gradient norm: 0.19858447
INFO:root:[  117] Training loss: 0.66012806, Validation loss: 0.66215760, Gradient norm: 0.18734487
INFO:root:[  118] Training loss: 0.66053944, Validation loss: 0.66156114, Gradient norm: 0.22209084
INFO:root:[  119] Training loss: 0.66013363, Validation loss: 0.66127315, Gradient norm: 0.18641883
INFO:root:[  120] Training loss: 0.66009842, Validation loss: 0.66218842, Gradient norm: 0.19138552
INFO:root:[  121] Training loss: 0.66023070, Validation loss: 0.66105550, Gradient norm: 0.19952404
INFO:root:[  122] Training loss: 0.66023097, Validation loss: 0.66092205, Gradient norm: 0.21112379
INFO:root:[  123] Training loss: 0.66014660, Validation loss: 0.66159842, Gradient norm: 0.20995496
INFO:root:[  124] Training loss: 0.65995042, Validation loss: 0.66130110, Gradient norm: 0.18843389
INFO:root:[  125] Training loss: 0.65999358, Validation loss: 0.66098449, Gradient norm: 0.21579318
INFO:root:EP 125: Early stopping
INFO:root:Training the model took 5190.723s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.9104
INFO:root:EnergyScoreTrain: 0.65725
INFO:root:CRPSTrain: 0.53246
INFO:root:Gaussian NLLTrain: 293.631
INFO:root:CoverageTrain: 0.89641
INFO:root:IntervalWidthTrain: 3.39097
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91487
INFO:root:EnergyScoreValidation: 0.66063
INFO:root:CRPSValidation: 0.53559
INFO:root:Gaussian NLLValidation: 177.11055
INFO:root:CoverageValidation: 0.89514
INFO:root:IntervalWidthValidation: 3.39371
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91432
INFO:root:EnergyScoreTest: 0.66035
INFO:root:CRPSTest: 0.53549
INFO:root:Gaussian NLLTest: 173.65494
INFO:root:CoverageTest: 0.89508
INFO:root:IntervalWidthTest: 3.39222
INFO:root:###11 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.70975237, Validation loss: 0.69858788, Gradient norm: 0.07167073
INFO:root:[    2] Training loss: 0.69642214, Validation loss: 0.69432130, Gradient norm: 0.10991941
INFO:root:[    3] Training loss: 0.69295710, Validation loss: 0.69001970, Gradient norm: 0.13840926
INFO:root:[    4] Training loss: 0.69031004, Validation loss: 0.68852381, Gradient norm: 0.14283447
INFO:root:[    5] Training loss: 0.68864104, Validation loss: 0.68794747, Gradient norm: 0.13937788
INFO:root:[    6] Training loss: 0.68723539, Validation loss: 0.68681930, Gradient norm: 0.13991012
INFO:root:[    7] Training loss: 0.68636418, Validation loss: 0.68586413, Gradient norm: 0.16644479
INFO:root:[    8] Training loss: 0.68523004, Validation loss: 0.68399536, Gradient norm: 0.16670964
INFO:root:[    9] Training loss: 0.68421621, Validation loss: 0.68392661, Gradient norm: 0.17137807
INFO:root:[   10] Training loss: 0.68317891, Validation loss: 0.68266528, Gradient norm: 0.17752852
INFO:root:[   11] Training loss: 0.68223957, Validation loss: 0.68093586, Gradient norm: 0.18161739
INFO:root:[   12] Training loss: 0.68140884, Validation loss: 0.67917523, Gradient norm: 0.17126594
INFO:root:[   13] Training loss: 0.68045623, Validation loss: 0.67955029, Gradient norm: 0.19021736
INFO:root:[   14] Training loss: 0.67975129, Validation loss: 0.67895704, Gradient norm: 0.18414250
INFO:root:[   15] Training loss: 0.67918898, Validation loss: 0.67809482, Gradient norm: 0.19573692
INFO:root:[   16] Training loss: 0.67862153, Validation loss: 0.67715752, Gradient norm: 0.19915902
INFO:root:[   17] Training loss: 0.67816774, Validation loss: 0.67641882, Gradient norm: 0.18311449
INFO:root:[   18] Training loss: 0.67758237, Validation loss: 0.67698198, Gradient norm: 0.19633644
INFO:root:[   19] Training loss: 0.67716697, Validation loss: 0.67652118, Gradient norm: 0.19777570
INFO:root:[   20] Training loss: 0.67683163, Validation loss: 0.67512609, Gradient norm: 0.20379377
INFO:root:[   21] Training loss: 0.67600237, Validation loss: 0.67886920, Gradient norm: 0.19282839
INFO:root:[   22] Training loss: 0.67635157, Validation loss: 0.67480652, Gradient norm: 0.21416618
INFO:root:[   23] Training loss: 0.67551202, Validation loss: 0.67473004, Gradient norm: 0.19328115
INFO:root:[   24] Training loss: 0.67547286, Validation loss: 0.67596916, Gradient norm: 0.19360655
INFO:root:[   25] Training loss: 0.67541279, Validation loss: 0.67394158, Gradient norm: 0.20616117
INFO:root:[   26] Training loss: 0.67529290, Validation loss: 0.67407468, Gradient norm: 0.19466856
INFO:root:[   27] Training loss: 0.67488864, Validation loss: 0.67316459, Gradient norm: 0.19832537
INFO:root:[   28] Training loss: 0.67453043, Validation loss: 0.67303567, Gradient norm: 0.20251191
INFO:root:[   29] Training loss: 0.67419876, Validation loss: 0.67327849, Gradient norm: 0.19226740
INFO:root:[   30] Training loss: 0.67410323, Validation loss: 0.67349511, Gradient norm: 0.20648618
INFO:root:[   31] Training loss: 0.67397350, Validation loss: 0.67347786, Gradient norm: 0.18408693
INFO:root:[   32] Training loss: 0.67366216, Validation loss: 0.67326437, Gradient norm: 0.20774399
INFO:root:[   33] Training loss: 0.67361063, Validation loss: 0.67282776, Gradient norm: 0.20179276
INFO:root:[   34] Training loss: 0.67352337, Validation loss: 0.67293818, Gradient norm: 0.22661755
INFO:root:[   35] Training loss: 0.67306978, Validation loss: 0.67251336, Gradient norm: 0.22009847
INFO:root:[   36] Training loss: 0.67316604, Validation loss: 0.67487771, Gradient norm: 0.20316848
INFO:root:[   37] Training loss: 0.67305666, Validation loss: 0.67204151, Gradient norm: 0.24204547
INFO:root:[   38] Training loss: 0.67247460, Validation loss: 0.67069781, Gradient norm: 0.21697103
INFO:root:[   39] Training loss: 0.67260164, Validation loss: 0.67103799, Gradient norm: 0.25196297
INFO:root:[   40] Training loss: 0.67177880, Validation loss: 0.67038651, Gradient norm: 0.20468675
INFO:root:[   41] Training loss: 0.67166240, Validation loss: 0.67031897, Gradient norm: 0.23547333
INFO:root:[   42] Training loss: 0.67154862, Validation loss: 0.67193048, Gradient norm: 0.23701453
INFO:root:[   43] Training loss: 0.67091265, Validation loss: 0.66985038, Gradient norm: 0.24803971
INFO:root:[   44] Training loss: 0.67051464, Validation loss: 0.66961552, Gradient norm: 0.24781056
INFO:root:[   45] Training loss: 0.67018252, Validation loss: 0.66958157, Gradient norm: 0.23881226
INFO:root:[   46] Training loss: 0.67033668, Validation loss: 0.67041379, Gradient norm: 0.26376450
INFO:root:[   47] Training loss: 0.66984290, Validation loss: 0.66887170, Gradient norm: 0.27107569
INFO:root:[   48] Training loss: 0.66952539, Validation loss: 0.66965386, Gradient norm: 0.26386143
INFO:root:[   49] Training loss: 0.66931245, Validation loss: 0.66926748, Gradient norm: 0.26584060
INFO:root:[   50] Training loss: 0.66888579, Validation loss: 0.66967468, Gradient norm: 0.25322279
INFO:root:[   51] Training loss: 0.66907075, Validation loss: 0.66766115, Gradient norm: 0.23825764
INFO:root:[   52] Training loss: 0.66889703, Validation loss: 0.67033682, Gradient norm: 0.28734556
INFO:root:[   53] Training loss: 0.66880405, Validation loss: 0.66674109, Gradient norm: 0.30835626
INFO:root:[   54] Training loss: 0.66844825, Validation loss: 0.66740931, Gradient norm: 0.28474521
INFO:root:[   55] Training loss: 0.66829296, Validation loss: 0.66610902, Gradient norm: 0.32433356
INFO:root:[   56] Training loss: 0.66794140, Validation loss: 0.66685181, Gradient norm: 0.28138364
INFO:root:[   57] Training loss: 0.66783119, Validation loss: 0.66670641, Gradient norm: 0.28817151
INFO:root:[   58] Training loss: 0.66747208, Validation loss: 0.66723128, Gradient norm: 0.28374419
INFO:root:[   59] Training loss: 0.66742124, Validation loss: 0.66656848, Gradient norm: 0.32121180
INFO:root:[   60] Training loss: 0.66742041, Validation loss: 0.66571106, Gradient norm: 0.31782170
INFO:root:[   61] Training loss: 0.66746925, Validation loss: 0.66869048, Gradient norm: 0.32377061
INFO:root:[   62] Training loss: 0.66717707, Validation loss: 0.66566953, Gradient norm: 0.28574840
INFO:root:[   63] Training loss: 0.66696300, Validation loss: 0.66729747, Gradient norm: 0.34447209
INFO:root:[   64] Training loss: 0.66714770, Validation loss: 0.66586362, Gradient norm: 0.33908021
INFO:root:[   65] Training loss: 0.66682553, Validation loss: 0.66571671, Gradient norm: 0.30486215
INFO:root:[   66] Training loss: 0.66664152, Validation loss: 0.66751852, Gradient norm: 0.32420303
INFO:root:[   67] Training loss: 0.66668304, Validation loss: 0.66692662, Gradient norm: 0.32194667
INFO:root:[   68] Training loss: 0.66676358, Validation loss: 0.66783801, Gradient norm: 0.35340673
INFO:root:[   69] Training loss: 0.66650151, Validation loss: 0.66542797, Gradient norm: 0.33651337
INFO:root:[   70] Training loss: 0.66631582, Validation loss: 0.66466033, Gradient norm: 0.34710255
INFO:root:[   71] Training loss: 0.66644440, Validation loss: 0.66559161, Gradient norm: 0.31318118
INFO:root:[   72] Training loss: 0.66600593, Validation loss: 0.66494476, Gradient norm: 0.27116196
INFO:root:[   73] Training loss: 0.66638593, Validation loss: 0.66578750, Gradient norm: 0.39945379
INFO:root:[   74] Training loss: 0.66583081, Validation loss: 0.66489982, Gradient norm: 0.30191732
INFO:root:[   75] Training loss: 0.66621718, Validation loss: 0.66681768, Gradient norm: 0.34657486
INFO:root:[   76] Training loss: 0.66591832, Validation loss: 0.66476507, Gradient norm: 0.36259814
INFO:root:[   77] Training loss: 0.66598874, Validation loss: 0.66471493, Gradient norm: 0.37072159
INFO:root:[   78] Training loss: 0.66589623, Validation loss: 0.66529611, Gradient norm: 0.34625303
INFO:root:[   79] Training loss: 0.66573163, Validation loss: 0.66440530, Gradient norm: 0.35333682
INFO:root:[   80] Training loss: 0.66530242, Validation loss: 0.66451341, Gradient norm: 0.30447342
INFO:root:[   81] Training loss: 0.66570887, Validation loss: 0.66417729, Gradient norm: 0.36600626
INFO:root:[   82] Training loss: 0.66546925, Validation loss: 0.66379021, Gradient norm: 0.33756086
INFO:root:[   83] Training loss: 0.66516576, Validation loss: 0.66529461, Gradient norm: 0.31595752
INFO:root:[   84] Training loss: 0.66530433, Validation loss: 0.66406879, Gradient norm: 0.33181028
INFO:root:[   85] Training loss: 0.66573184, Validation loss: 0.66464296, Gradient norm: 0.41668073
INFO:root:[   86] Training loss: 0.66504539, Validation loss: 0.66516483, Gradient norm: 0.30441076
INFO:root:[   87] Training loss: 0.66502571, Validation loss: 0.66459930, Gradient norm: 0.33005661
INFO:root:[   88] Training loss: 0.66474464, Validation loss: 0.66376703, Gradient norm: 0.32207189
INFO:root:[   89] Training loss: 0.66481114, Validation loss: 0.66409480, Gradient norm: 0.33585068
INFO:root:[   90] Training loss: 0.66501924, Validation loss: 0.66346033, Gradient norm: 0.40544618
INFO:root:[   91] Training loss: 0.66461537, Validation loss: 0.66467952, Gradient norm: 0.31483630
INFO:root:[   92] Training loss: 0.66468758, Validation loss: 0.66543774, Gradient norm: 0.34016268
INFO:root:[   93] Training loss: 0.66440180, Validation loss: 0.66405643, Gradient norm: 0.34224117
INFO:root:[   94] Training loss: 0.66457424, Validation loss: 0.66414148, Gradient norm: 0.36923983
INFO:root:[   95] Training loss: 0.66405214, Validation loss: 0.66287322, Gradient norm: 0.32386657
INFO:root:[   96] Training loss: 0.66388485, Validation loss: 0.66353174, Gradient norm: 0.26178148
INFO:root:[   97] Training loss: 0.66437988, Validation loss: 0.66417299, Gradient norm: 0.40558141
INFO:root:[   98] Training loss: 0.66406716, Validation loss: 0.66390614, Gradient norm: 0.32444047
INFO:root:[   99] Training loss: 0.66375399, Validation loss: 0.66287948, Gradient norm: 0.31945997
INFO:root:[  100] Training loss: 0.66404227, Validation loss: 0.66305001, Gradient norm: 0.38516449
INFO:root:[  101] Training loss: 0.66358087, Validation loss: 0.66310519, Gradient norm: 0.27960686
INFO:root:[  102] Training loss: 0.66322640, Validation loss: 0.66276815, Gradient norm: 0.31789081
INFO:root:[  103] Training loss: 0.66378088, Validation loss: 0.66391692, Gradient norm: 0.37473021
INFO:root:[  104] Training loss: 0.66355351, Validation loss: 0.66274500, Gradient norm: 0.30383503
INFO:root:[  105] Training loss: 0.66368843, Validation loss: 0.66218485, Gradient norm: 0.35784344
INFO:root:[  106] Training loss: 0.66354856, Validation loss: 0.66223666, Gradient norm: 0.37984394
INFO:root:[  107] Training loss: 0.66333484, Validation loss: 0.66231413, Gradient norm: 0.29442256
INFO:root:[  108] Training loss: 0.66351397, Validation loss: 0.66295714, Gradient norm: 0.35150560
INFO:root:[  109] Training loss: 0.66355917, Validation loss: 0.66250203, Gradient norm: 0.31263780
INFO:root:[  110] Training loss: 0.66352944, Validation loss: 0.66331400, Gradient norm: 0.38820002
INFO:root:[  111] Training loss: 0.66328818, Validation loss: 0.66414703, Gradient norm: 0.34636042
INFO:root:[  112] Training loss: 0.66282645, Validation loss: 0.66222827, Gradient norm: 0.28633384
INFO:root:[  113] Training loss: 0.66325873, Validation loss: 0.66243526, Gradient norm: 0.37393827
INFO:root:[  114] Training loss: 0.66306570, Validation loss: 0.66155052, Gradient norm: 0.34446670
INFO:root:[  115] Training loss: 0.66311972, Validation loss: 0.66295268, Gradient norm: 0.31890644
INFO:root:[  116] Training loss: 0.66284658, Validation loss: 0.66132009, Gradient norm: 0.31347244
INFO:root:[  117] Training loss: 0.66273640, Validation loss: 0.66147214, Gradient norm: 0.33762210
INFO:root:[  118] Training loss: 0.66291434, Validation loss: 0.66110146, Gradient norm: 0.34289185
INFO:root:[  119] Training loss: 0.66256266, Validation loss: 0.66269981, Gradient norm: 0.32006783
INFO:root:[  120] Training loss: 0.66285659, Validation loss: 0.66200563, Gradient norm: 0.37803930
INFO:root:[  121] Training loss: 0.66246475, Validation loss: 0.66128404, Gradient norm: 0.28584247
INFO:root:[  122] Training loss: 0.66269778, Validation loss: 0.66291265, Gradient norm: 0.34147632
INFO:root:[  123] Training loss: 0.66251805, Validation loss: 0.66233102, Gradient norm: 0.34092550
INFO:root:[  124] Training loss: 0.66251040, Validation loss: 0.66177047, Gradient norm: 0.35700311
INFO:root:[  125] Training loss: 0.66229247, Validation loss: 0.66136682, Gradient norm: 0.32191697
INFO:root:[  126] Training loss: 0.66238523, Validation loss: 0.66224331, Gradient norm: 0.35499942
INFO:root:[  127] Training loss: 0.66240035, Validation loss: 0.66140207, Gradient norm: 0.33610597
INFO:root:EP 127: Early stopping
INFO:root:Training the model took 5275.982s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.91176
INFO:root:EnergyScoreTrain: 0.65827
INFO:root:CRPSTrain: 0.54328
INFO:root:Gaussian NLLTrain: 125208776.79569
INFO:root:CoverageTrain: 0.84786
INFO:root:IntervalWidthTrain: 3.27077
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91535
INFO:root:EnergyScoreValidation: 0.66104
INFO:root:CRPSValidation: 0.54589
INFO:root:Gaussian NLLValidation: 126476266.64556
INFO:root:CoverageValidation: 0.8478
INFO:root:IntervalWidthValidation: 3.27481
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.915
INFO:root:EnergyScoreTest: 0.6609
INFO:root:CRPSTest: 0.5459
INFO:root:Gaussian NLLTest: 146475498.341
INFO:root:CoverageTest: 0.84722
INFO:root:IntervalWidthTest: 3.27202
INFO:root:###12 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75], [1.0, 0.667], [1.0, 0.5], [1.0, 1.0], [1.0, 2.0], [1.0, 1.5], [1.0, 1.334]], 'uno_n_modes': [[4, 20], [4, 14], [4, 6], [7, 6], [7, 6], [10, 14], [10, 20]]}
INFO:root:NumberParameters: 1688178
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.71004188, Validation loss: 0.69915002, Gradient norm: 0.06548769
INFO:root:[    2] Training loss: 0.69699429, Validation loss: 0.69341782, Gradient norm: 0.09942233
INFO:root:[    3] Training loss: 0.69344301, Validation loss: 0.69025062, Gradient norm: 0.11702966
INFO:root:[    4] Training loss: 0.69084927, Validation loss: 0.68879660, Gradient norm: 0.12872994
INFO:root:[    5] Training loss: 0.68921509, Validation loss: 0.68729140, Gradient norm: 0.13336729
INFO:root:[    6] Training loss: 0.68760103, Validation loss: 0.68611098, Gradient norm: 0.12602465
INFO:root:[    7] Training loss: 0.68671231, Validation loss: 0.68550543, Gradient norm: 0.14789306
INFO:root:[    8] Training loss: 0.68531342, Validation loss: 0.68330326, Gradient norm: 0.15271783
INFO:root:[    9] Training loss: 0.68480774, Validation loss: 0.68647912, Gradient norm: 0.19582215
INFO:root:[   10] Training loss: 0.68326695, Validation loss: 0.68326124, Gradient norm: 0.17848299
INFO:root:[   11] Training loss: 0.68231210, Validation loss: 0.68253840, Gradient norm: 0.18026536
INFO:root:[   12] Training loss: 0.68218509, Validation loss: 0.68105614, Gradient norm: 0.23507573
INFO:root:[   13] Training loss: 0.68111700, Validation loss: 0.67966699, Gradient norm: 0.25097278
INFO:root:[   14] Training loss: 0.68014523, Validation loss: 0.67772269, Gradient norm: 0.21919874
INFO:root:[   15] Training loss: 0.67990046, Validation loss: 0.67758424, Gradient norm: 0.27141764
INFO:root:[   16] Training loss: 0.67900995, Validation loss: 0.67771081, Gradient norm: 0.24832966
INFO:root:[   17] Training loss: 0.67872920, Validation loss: 0.67640875, Gradient norm: 0.25717590
INFO:root:[   18] Training loss: 0.67858642, Validation loss: 0.67632274, Gradient norm: 0.30977647
INFO:root:[   19] Training loss: 0.67781056, Validation loss: 0.67538952, Gradient norm: 0.27440469
INFO:root:[   20] Training loss: 0.67790725, Validation loss: 0.67730049, Gradient norm: 0.33111507
INFO:root:[   21] Training loss: 0.67680373, Validation loss: 0.67792573, Gradient norm: 0.28101508
INFO:root:[   22] Training loss: 0.67674630, Validation loss: 0.67406730, Gradient norm: 0.27624698
INFO:root:[   23] Training loss: 0.67656080, Validation loss: 0.67372320, Gradient norm: 0.36456795
INFO:root:[   24] Training loss: 0.67620147, Validation loss: 0.67408708, Gradient norm: 0.28672165
INFO:root:[   25] Training loss: 0.67629491, Validation loss: 0.67528331, Gradient norm: 0.36269717
INFO:root:[   26] Training loss: 0.67638941, Validation loss: 0.67436857, Gradient norm: 0.34136727
INFO:root:[   27] Training loss: 0.67562473, Validation loss: 0.67581282, Gradient norm: 0.31094004
INFO:root:[   28] Training loss: 0.67563254, Validation loss: 0.67385436, Gradient norm: 0.36475618
INFO:root:[   29] Training loss: 0.67545347, Validation loss: 0.67299093, Gradient norm: 0.36725596
INFO:root:[   30] Training loss: 0.67513856, Validation loss: 0.67222441, Gradient norm: 0.33937565
INFO:root:[   31] Training loss: 0.67533749, Validation loss: 0.67308880, Gradient norm: 0.38104248
INFO:root:[   32] Training loss: 0.67445102, Validation loss: 0.67300132, Gradient norm: 0.27628605
INFO:root:[   33] Training loss: 0.67495054, Validation loss: 0.67418664, Gradient norm: 0.40270921
INFO:root:[   34] Training loss: 0.67490644, Validation loss: 0.67382860, Gradient norm: 0.36100004
INFO:root:[   35] Training loss: 0.67460194, Validation loss: 0.67212095, Gradient norm: 0.34827615
INFO:root:[   36] Training loss: 0.67464475, Validation loss: 0.67248129, Gradient norm: 0.36992927
INFO:root:[   37] Training loss: 0.67453195, Validation loss: 0.67247263, Gradient norm: 0.40229814
INFO:root:[   38] Training loss: 0.67386922, Validation loss: 0.67249486, Gradient norm: 0.30348032
INFO:root:[   39] Training loss: 0.67416766, Validation loss: 0.67223001, Gradient norm: 0.38735306
INFO:root:[   40] Training loss: 0.67382961, Validation loss: 0.67166792, Gradient norm: 0.34416846
INFO:root:[   41] Training loss: 0.67390600, Validation loss: 0.67442311, Gradient norm: 0.38177003
INFO:root:[   42] Training loss: 0.67353871, Validation loss: 0.67193356, Gradient norm: 0.33184302
INFO:root:[   43] Training loss: 0.67381559, Validation loss: 0.67170552, Gradient norm: 0.41956308
INFO:root:[   44] Training loss: 0.67300401, Validation loss: 0.67097827, Gradient norm: 0.33284219
INFO:root:[   45] Training loss: 0.67307653, Validation loss: 0.67053378, Gradient norm: 0.37891697
INFO:root:[   46] Training loss: 0.67258273, Validation loss: 0.67070449, Gradient norm: 0.32392821
INFO:root:[   47] Training loss: 0.67269104, Validation loss: 0.67074146, Gradient norm: 0.42990964
INFO:root:[   48] Training loss: 0.67194887, Validation loss: 0.67258265, Gradient norm: 0.34135983
INFO:root:[   49] Training loss: 0.67233816, Validation loss: 0.67083406, Gradient norm: 0.41988971
INFO:root:[   50] Training loss: 0.67121727, Validation loss: 0.67225131, Gradient norm: 0.34131604
INFO:root:[   51] Training loss: 0.67145284, Validation loss: 0.66851290, Gradient norm: 0.38739720
INFO:root:[   52] Training loss: 0.67069298, Validation loss: 0.66960670, Gradient norm: 0.28513463
INFO:root:[   53] Training loss: 0.67067899, Validation loss: 0.66800915, Gradient norm: 0.34967169
INFO:root:[   54] Training loss: 0.67067155, Validation loss: 0.67002074, Gradient norm: 0.37142145
INFO:root:[   55] Training loss: 0.67072761, Validation loss: 0.66828803, Gradient norm: 0.41323231
INFO:root:[   56] Training loss: 0.67019621, Validation loss: 0.66811188, Gradient norm: 0.30858206
INFO:root:[   57] Training loss: 0.67045100, Validation loss: 0.66815190, Gradient norm: 0.37519691
INFO:root:[   58] Training loss: 0.66995810, Validation loss: 0.66743052, Gradient norm: 0.33367989
INFO:root:[   59] Training loss: 0.66946399, Validation loss: 0.66843527, Gradient norm: 0.29908108
INFO:root:[   60] Training loss: 0.66994267, Validation loss: 0.66708650, Gradient norm: 0.38429723
INFO:root:[   61] Training loss: 0.66999748, Validation loss: 0.66837576, Gradient norm: 0.36692437
INFO:root:[   62] Training loss: 0.66964401, Validation loss: 0.66879123, Gradient norm: 0.31176922
INFO:root:[   63] Training loss: 0.66925489, Validation loss: 0.66818702, Gradient norm: 0.33135737
INFO:root:[   64] Training loss: 0.66970924, Validation loss: 0.66661629, Gradient norm: 0.38141618
INFO:root:[   65] Training loss: 0.66911011, Validation loss: 0.66769111, Gradient norm: 0.30303023
INFO:root:[   66] Training loss: 0.66901935, Validation loss: 0.66714318, Gradient norm: 0.35813715
INFO:root:[   67] Training loss: 0.66866675, Validation loss: 0.66682138, Gradient norm: 0.30772755
INFO:root:[   68] Training loss: 0.66909755, Validation loss: 0.66675138, Gradient norm: 0.39190485
INFO:root:[   69] Training loss: 0.66875306, Validation loss: 0.66679921, Gradient norm: 0.30267060
INFO:root:[   70] Training loss: 0.66869665, Validation loss: 0.66632627, Gradient norm: 0.32410511
INFO:root:[   71] Training loss: 0.66860748, Validation loss: 0.66597850, Gradient norm: 0.31709930
INFO:root:[   72] Training loss: 0.66848927, Validation loss: 0.66567368, Gradient norm: 0.35529299
INFO:root:[   73] Training loss: 0.66815753, Validation loss: 0.66693471, Gradient norm: 0.32947557
INFO:root:[   74] Training loss: 0.66826689, Validation loss: 0.66690639, Gradient norm: 0.35353738
INFO:root:[   75] Training loss: 0.66824277, Validation loss: 0.66573431, Gradient norm: 0.35593132
INFO:root:[   76] Training loss: 0.66787457, Validation loss: 0.66638897, Gradient norm: 0.29866189
INFO:root:[   77] Training loss: 0.66824967, Validation loss: 0.66570073, Gradient norm: 0.36738330
INFO:root:[   78] Training loss: 0.66803394, Validation loss: 0.66631276, Gradient norm: 0.32711504
INFO:root:[   79] Training loss: 0.66786008, Validation loss: 0.66825569, Gradient norm: 0.33965241
INFO:root:[   80] Training loss: 0.66737289, Validation loss: 0.66639435, Gradient norm: 0.31633277
INFO:root:[   81] Training loss: 0.66746107, Validation loss: 0.66509585, Gradient norm: 0.33039940
INFO:root:[   82] Training loss: 0.66721905, Validation loss: 0.66527238, Gradient norm: 0.31622209
INFO:root:[   83] Training loss: 0.66728793, Validation loss: 0.66473926, Gradient norm: 0.31572517
INFO:root:[   84] Training loss: 0.66731723, Validation loss: 0.66407178, Gradient norm: 0.34192791
INFO:root:[   85] Training loss: 0.66691109, Validation loss: 0.66521474, Gradient norm: 0.27721416
INFO:root:[   86] Training loss: 0.66723414, Validation loss: 0.66589725, Gradient norm: 0.35386922
INFO:root:[   87] Training loss: 0.66671740, Validation loss: 0.66444823, Gradient norm: 0.30494146
INFO:root:[   88] Training loss: 0.66700155, Validation loss: 0.66437251, Gradient norm: 0.37447071
INFO:root:[   89] Training loss: 0.66640791, Validation loss: 0.66445086, Gradient norm: 0.25225081
INFO:root:[   90] Training loss: 0.66689250, Validation loss: 0.66513819, Gradient norm: 0.35575329
INFO:root:[   91] Training loss: 0.66662113, Validation loss: 0.66597985, Gradient norm: 0.32847912
INFO:root:[   92] Training loss: 0.66651054, Validation loss: 0.66567660, Gradient norm: 0.30873620
INFO:root:[   93] Training loss: 0.66609573, Validation loss: 0.66466103, Gradient norm: 0.26553404
INFO:root:[   94] Training loss: 0.66669028, Validation loss: 0.66355710, Gradient norm: 0.35748117
INFO:root:[   95] Training loss: 0.66602432, Validation loss: 0.66402832, Gradient norm: 0.31520195
INFO:root:[   96] Training loss: 0.66601591, Validation loss: 0.66386150, Gradient norm: 0.28830031
INFO:root:[   97] Training loss: 0.66625902, Validation loss: 0.66501339, Gradient norm: 0.35588847
INFO:root:[   98] Training loss: 0.66601605, Validation loss: 0.66398609, Gradient norm: 0.27778806
INFO:root:[   99] Training loss: 0.66617939, Validation loss: 0.66294547, Gradient norm: 0.32301464
INFO:root:[  100] Training loss: 0.66605563, Validation loss: 0.66387290, Gradient norm: 0.36014665
INFO:root:[  101] Training loss: 0.66569899, Validation loss: 0.66372085, Gradient norm: 0.26963878
INFO:root:[  102] Training loss: 0.66587394, Validation loss: 0.66372221, Gradient norm: 0.36287138
INFO:root:[  103] Training loss: 0.66570993, Validation loss: 0.66337267, Gradient norm: 0.31336866
INFO:root:[  104] Training loss: 0.66561177, Validation loss: 0.66265268, Gradient norm: 0.28894891
INFO:root:[  105] Training loss: 0.66546132, Validation loss: 0.66487510, Gradient norm: 0.29352482
INFO:root:[  106] Training loss: 0.66553469, Validation loss: 0.66317025, Gradient norm: 0.30519897
