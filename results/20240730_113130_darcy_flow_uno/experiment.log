INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05150203, Validation loss: 0.03109207, Gradient norm: 0.81786168
INFO:root:[    2] Training loss: 0.03447086, Validation loss: 0.02501567, Gradient norm: 0.84320479
INFO:root:[    3] Training loss: 0.02927670, Validation loss: 0.03056702, Gradient norm: 0.77997822
INFO:root:[    4] Training loss: 0.02501328, Validation loss: 0.02747732, Gradient norm: 0.66439009
INFO:root:[    5] Training loss: 0.02214913, Validation loss: 0.02799362, Gradient norm: 0.55590154
INFO:root:[    6] Training loss: 0.02404194, Validation loss: 0.03119944, Gradient norm: 0.67974999
INFO:root:[    7] Training loss: 0.02127331, Validation loss: 0.02169173, Gradient norm: 0.55771874
INFO:root:[    8] Training loss: 0.02029044, Validation loss: 0.02534407, Gradient norm: 0.52260956
INFO:root:[    9] Training loss: 0.01939256, Validation loss: 0.02149095, Gradient norm: 0.53362479
INFO:root:[   10] Training loss: 0.01782803, Validation loss: 0.02311288, Gradient norm: 0.45732904
INFO:root:[   11] Training loss: 0.01960412, Validation loss: 0.03326938, Gradient norm: 0.55785305
INFO:root:[   12] Training loss: 0.01704675, Validation loss: 0.02195058, Gradient norm: 0.40398832
INFO:root:[   13] Training loss: 0.01913378, Validation loss: 0.03102908, Gradient norm: 0.54486289
INFO:root:[   14] Training loss: 0.01844231, Validation loss: 0.02702183, Gradient norm: 0.54720735
INFO:root:[   15] Training loss: 0.01701257, Validation loss: 0.02507479, Gradient norm: 0.45621183
INFO:root:[   16] Training loss: 0.01655927, Validation loss: 0.02622275, Gradient norm: 0.46578995
INFO:root:[   17] Training loss: 0.01595554, Validation loss: 0.02432812, Gradient norm: 0.45041204
INFO:root:[   18] Training loss: 0.01527955, Validation loss: 0.02317727, Gradient norm: 0.42248704
INFO:root:[   19] Training loss: 0.01458266, Validation loss: 0.02160929, Gradient norm: 0.36005979
INFO:root:[   20] Training loss: 0.01428594, Validation loss: 0.02417338, Gradient norm: 0.36287904
INFO:root:[   21] Training loss: 0.01490952, Validation loss: 0.02730487, Gradient norm: 0.37835929
INFO:root:[   22] Training loss: 0.01656618, Validation loss: 0.02669309, Gradient norm: 0.50060522
INFO:root:[   23] Training loss: 0.01420779, Validation loss: 0.02624595, Gradient norm: 0.35908449
INFO:root:[   24] Training loss: 0.01424686, Validation loss: 0.03140604, Gradient norm: 0.43655617
INFO:root:[   25] Training loss: 0.01347463, Validation loss: 0.02513497, Gradient norm: 0.34988781
INFO:root:[   26] Training loss: 0.01419920, Validation loss: 0.02397412, Gradient norm: 0.39360154
INFO:root:[   27] Training loss: 0.01295798, Validation loss: 0.02930929, Gradient norm: 0.36450936
INFO:root:[   28] Training loss: 0.01284384, Validation loss: 0.02577242, Gradient norm: 0.35857767
INFO:root:[   29] Training loss: 0.01443557, Validation loss: 0.02389494, Gradient norm: 0.46339781
INFO:root:[   30] Training loss: 0.01309399, Validation loss: 0.02448288, Gradient norm: 0.36337305
INFO:root:[   31] Training loss: 0.01380523, Validation loss: 0.02569641, Gradient norm: 0.42782269
INFO:root:[   32] Training loss: 0.01281626, Validation loss: 0.02392553, Gradient norm: 0.33230515
INFO:root:[   33] Training loss: 0.01279119, Validation loss: 0.02586215, Gradient norm: 0.36789009
INFO:root:[   34] Training loss: 0.01187928, Validation loss: 0.02346660, Gradient norm: 0.33575426
INFO:root:[   35] Training loss: 0.01305210, Validation loss: 0.02807867, Gradient norm: 0.38714356
INFO:root:[   36] Training loss: 0.01240481, Validation loss: 0.02447418, Gradient norm: 0.37866822
INFO:root:[   37] Training loss: 0.01194493, Validation loss: 0.02664183, Gradient norm: 0.35204591
INFO:root:[   38] Training loss: 0.01200169, Validation loss: 0.03011746, Gradient norm: 0.32415290
INFO:root:[   39] Training loss: 0.01176316, Validation loss: 0.02510035, Gradient norm: 0.31401655
INFO:root:[   40] Training loss: 0.01179948, Validation loss: 0.02538531, Gradient norm: 0.30998423
INFO:root:[   41] Training loss: 0.01191963, Validation loss: 0.03257874, Gradient norm: 0.37235215
INFO:root:[   42] Training loss: 0.01221504, Validation loss: 0.02836670, Gradient norm: 0.36384868
INFO:root:[   43] Training loss: 0.01218429, Validation loss: 0.02748972, Gradient norm: 0.39401340
INFO:root:[   44] Training loss: 0.01066595, Validation loss: 0.02459179, Gradient norm: 0.30254309
INFO:root:[   45] Training loss: 0.01205042, Validation loss: 0.02757241, Gradient norm: 0.31873262
INFO:root:[   46] Training loss: 0.01117227, Validation loss: 0.02570309, Gradient norm: 0.30684241
INFO:root:[   47] Training loss: 0.01218589, Validation loss: 0.02367037, Gradient norm: 0.40261107
INFO:root:[   48] Training loss: 0.01092168, Validation loss: 0.02676528, Gradient norm: 0.32728672
INFO:root:[   49] Training loss: 0.01104655, Validation loss: 0.02950550, Gradient norm: 0.35233617
INFO:root:[   50] Training loss: 0.01104817, Validation loss: 0.02483158, Gradient norm: 0.33664473
INFO:root:[   51] Training loss: 0.01123617, Validation loss: 0.02891052, Gradient norm: 0.36420060
INFO:root:[   52] Training loss: 0.01119179, Validation loss: 0.02618705, Gradient norm: 0.33788569
INFO:root:[   53] Training loss: 0.01056991, Validation loss: 0.02885804, Gradient norm: 0.31014234
INFO:root:[   54] Training loss: 0.01091029, Validation loss: 0.02973985, Gradient norm: 0.33201992
INFO:root:[   55] Training loss: 0.01127595, Validation loss: 0.02638729, Gradient norm: 0.33043523
INFO:root:[   56] Training loss: 0.01120335, Validation loss: 0.02759835, Gradient norm: 0.34713924
INFO:root:[   57] Training loss: 0.01101026, Validation loss: 0.02894335, Gradient norm: 0.34897699
INFO:root:[   58] Training loss: 0.01079858, Validation loss: 0.02681840, Gradient norm: 0.32774848
INFO:root:[   59] Training loss: 0.00998710, Validation loss: 0.02700025, Gradient norm: 0.30104827
INFO:root:[   60] Training loss: 0.01060777, Validation loss: 0.02552971, Gradient norm: 0.35814253
INFO:root:[   61] Training loss: 0.01075314, Validation loss: 0.02420711, Gradient norm: 0.30049651
INFO:root:[   62] Training loss: 0.01031336, Validation loss: 0.02954339, Gradient norm: 0.28947443
INFO:root:[   63] Training loss: 0.01060225, Validation loss: 0.02735459, Gradient norm: 0.32901265
INFO:root:[   64] Training loss: 0.01067571, Validation loss: 0.02632465, Gradient norm: 0.34048350
INFO:root:[   65] Training loss: 0.01032984, Validation loss: 0.02761399, Gradient norm: 0.28280772
INFO:root:[   66] Training loss: 0.01088897, Validation loss: 0.02500934, Gradient norm: 0.34435165
INFO:root:[   67] Training loss: 0.01039685, Validation loss: 0.02614059, Gradient norm: 0.33229355
INFO:root:[   68] Training loss: 0.01013731, Validation loss: 0.02785559, Gradient norm: 0.29345703
INFO:root:[   69] Training loss: 0.00996304, Validation loss: 0.02723481, Gradient norm: 0.27488847
INFO:root:[   70] Training loss: 0.00991662, Validation loss: 0.03128931, Gradient norm: 0.27967969
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 4657.091s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02111
INFO:root:EnergyScoreTrain: 0.01545
INFO:root:CoverageTrain: 0.92728
INFO:root:IntervalWidthTrain: 0.08633
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02752
INFO:root:EnergyScoreValidation: 0.02169
INFO:root:CoverageValidation: 0.5912
INFO:root:IntervalWidthValidation: 0.04631
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02816
INFO:root:EnergyScoreTest: 0.02228
INFO:root:CoverageTest: 0.58303
INFO:root:IntervalWidthTest: 0.04646
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 150994944
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05889843, Validation loss: 0.03490156, Gradient norm: 1.17198299
INFO:root:[    2] Training loss: 0.03241879, Validation loss: 0.03156418, Gradient norm: 0.67208127
INFO:root:[    3] Training loss: 0.02878513, Validation loss: 0.02452674, Gradient norm: 0.67559429
INFO:root:[    4] Training loss: 0.02562810, Validation loss: 0.02213879, Gradient norm: 0.60002867
INFO:root:[    5] Training loss: 0.02384567, Validation loss: 0.02417981, Gradient norm: 0.53424908
INFO:root:[    6] Training loss: 0.02070094, Validation loss: 0.02376647, Gradient norm: 0.44771233
INFO:root:[    7] Training loss: 0.02192967, Validation loss: 0.02094561, Gradient norm: 0.54422372
INFO:root:[    8] Training loss: 0.02059335, Validation loss: 0.02525069, Gradient norm: 0.51019470
INFO:root:[    9] Training loss: 0.01936877, Validation loss: 0.02098291, Gradient norm: 0.46381282
INFO:root:[   10] Training loss: 0.02017402, Validation loss: 0.02693796, Gradient norm: 0.53653580
INFO:root:[   11] Training loss: 0.01980401, Validation loss: 0.02266809, Gradient norm: 0.49673540
INFO:root:[   12] Training loss: 0.01577983, Validation loss: 0.02059656, Gradient norm: 0.29159133
INFO:root:[   13] Training loss: 0.01740437, Validation loss: 0.01989256, Gradient norm: 0.42429429
INFO:root:[   14] Training loss: 0.01624714, Validation loss: 0.01873211, Gradient norm: 0.39695495
INFO:root:[   15] Training loss: 0.01684305, Validation loss: 0.02084754, Gradient norm: 0.42576701
INFO:root:[   16] Training loss: 0.01612172, Validation loss: 0.02242572, Gradient norm: 0.36426692
INFO:root:[   17] Training loss: 0.01692838, Validation loss: 0.02178351, Gradient norm: 0.44146379
INFO:root:[   18] Training loss: 0.01525253, Validation loss: 0.01797515, Gradient norm: 0.29867368
INFO:root:[   19] Training loss: 0.01559507, Validation loss: 0.02176939, Gradient norm: 0.40392185
INFO:root:[   20] Training loss: 0.01461241, Validation loss: 0.02100701, Gradient norm: 0.34388320
INFO:root:[   21] Training loss: 0.01481359, Validation loss: 0.02641838, Gradient norm: 0.39985608
INFO:root:[   22] Training loss: 0.01471347, Validation loss: 0.02092240, Gradient norm: 0.39226857
INFO:root:[   23] Training loss: 0.01391339, Validation loss: 0.02087385, Gradient norm: 0.32157126
INFO:root:[   24] Training loss: 0.01445842, Validation loss: 0.02018556, Gradient norm: 0.37421134
INFO:root:[   25] Training loss: 0.01489320, Validation loss: 0.02410770, Gradient norm: 0.39783102
INFO:root:[   26] Training loss: 0.01482760, Validation loss: 0.01945432, Gradient norm: 0.37638251
INFO:root:[   27] Training loss: 0.01311043, Validation loss: 0.01973693, Gradient norm: 0.33268173
INFO:root:[   28] Training loss: 0.01334459, Validation loss: 0.02134586, Gradient norm: 0.34221830
INFO:root:[   29] Training loss: 0.01283726, Validation loss: 0.02233484, Gradient norm: 0.28244014
INFO:root:[   30] Training loss: 0.01336700, Validation loss: 0.02453156, Gradient norm: 0.35007265
INFO:root:[   31] Training loss: 0.01334849, Validation loss: 0.02755206, Gradient norm: 0.34233640
INFO:root:[   32] Training loss: 0.01275775, Validation loss: 0.02182601, Gradient norm: 0.32184125
INFO:root:[   33] Training loss: 0.01298423, Validation loss: 0.02213168, Gradient norm: 0.34574045
INFO:root:[   34] Training loss: 0.01275062, Validation loss: 0.02211654, Gradient norm: 0.30893095
INFO:root:[   35] Training loss: 0.01251553, Validation loss: 0.02378831, Gradient norm: 0.31952807
INFO:root:[   36] Training loss: 0.01249233, Validation loss: 0.02074426, Gradient norm: 0.34619280
INFO:root:[   37] Training loss: 0.01178373, Validation loss: 0.02102927, Gradient norm: 0.27113659
INFO:root:[   38] Training loss: 0.01242734, Validation loss: 0.02220473, Gradient norm: 0.30894356
INFO:root:[   39] Training loss: 0.01239906, Validation loss: 0.02135641, Gradient norm: 0.28448693
INFO:root:[   40] Training loss: 0.01221800, Validation loss: 0.02235698, Gradient norm: 0.29999273
INFO:root:[   41] Training loss: 0.01277890, Validation loss: 0.02235662, Gradient norm: 0.34526691
INFO:root:[   42] Training loss: 0.01225137, Validation loss: 0.02278710, Gradient norm: 0.32802635
INFO:root:[   43] Training loss: 0.01199885, Validation loss: 0.02352539, Gradient norm: 0.28675538
INFO:root:[   44] Training loss: 0.01250061, Validation loss: 0.02794471, Gradient norm: 0.36373470
INFO:root:[   45] Training loss: 0.01250138, Validation loss: 0.02118236, Gradient norm: 0.36685050
INFO:root:[   46] Training loss: 0.01211546, Validation loss: 0.02211413, Gradient norm: 0.33884957
INFO:root:[   47] Training loss: 0.01120316, Validation loss: 0.02386698, Gradient norm: 0.21131007
INFO:root:[   48] Training loss: 0.01157129, Validation loss: 0.02883204, Gradient norm: 0.27791812
INFO:root:[   49] Training loss: 0.01122541, Validation loss: 0.02492430, Gradient norm: 0.27412050
INFO:root:[   50] Training loss: 0.01144548, Validation loss: 0.02443855, Gradient norm: 0.30247702
INFO:root:[   51] Training loss: 0.01142806, Validation loss: 0.02693629, Gradient norm: 0.30265540
INFO:root:[   52] Training loss: 0.01161241, Validation loss: 0.02547946, Gradient norm: 0.26246857
INFO:root:[   53] Training loss: 0.01148026, Validation loss: 0.02321428, Gradient norm: 0.28559720
INFO:root:[   54] Training loss: 0.01153697, Validation loss: 0.02520353, Gradient norm: 0.31352679
INFO:root:[   55] Training loss: 0.01132914, Validation loss: 0.02425199, Gradient norm: 0.29216098
INFO:root:[   56] Training loss: 0.01109608, Validation loss: 0.02538464, Gradient norm: 0.28598919
INFO:root:[   57] Training loss: 0.01068714, Validation loss: 0.02530983, Gradient norm: 0.25804894
INFO:root:[   58] Training loss: 0.01100945, Validation loss: 0.02183190, Gradient norm: 0.30382854
INFO:root:[   59] Training loss: 0.01081753, Validation loss: 0.02748475, Gradient norm: 0.23846594
INFO:root:[   60] Training loss: 0.01075627, Validation loss: 0.02303729, Gradient norm: 0.25305372
INFO:root:[   61] Training loss: 0.01076559, Validation loss: 0.02389898, Gradient norm: 0.28043752
INFO:root:[   62] Training loss: 0.01048746, Validation loss: 0.02200605, Gradient norm: 0.25210598
INFO:root:[   63] Training loss: 0.06186304, Validation loss: 0.03737921, Gradient norm: 2.09592134
INFO:root:[   64] Training loss: 0.02744684, Validation loss: 0.02467863, Gradient norm: 2.11082206
INFO:root:[   65] Training loss: 0.02365159, Validation loss: 0.02848621, Gradient norm: 1.67621484
INFO:root:[   66] Training loss: 0.02268419, Validation loss: 0.02271729, Gradient norm: 1.64832862
INFO:root:[   67] Training loss: 0.01969382, Validation loss: 0.03469665, Gradient norm: 1.17166876
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 4376.382s.
INFO:root:Emptying the cuda cache took 0.014s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02235
INFO:root:EnergyScoreTrain: 0.016
INFO:root:CoverageTrain: 0.97221
INFO:root:IntervalWidthTrain: 0.08477
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02502
INFO:root:EnergyScoreValidation: 0.01814
INFO:root:CoverageValidation: 0.86766
INFO:root:IntervalWidthValidation: 0.07625
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02539
INFO:root:EnergyScoreTest: 0.01844
INFO:root:CoverageTest: 0.86245
INFO:root:IntervalWidthTest: 0.07617
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05393209, Validation loss: 0.03881075, Gradient norm: 0.79163345
INFO:root:[    2] Training loss: 0.03107935, Validation loss: 0.02606592, Gradient norm: 0.53782424
INFO:root:[    3] Training loss: 0.02758880, Validation loss: 0.02092939, Gradient norm: 0.49872078
INFO:root:[    4] Training loss: 0.02254658, Validation loss: 0.02105183, Gradient norm: 0.43109662
INFO:root:[    5] Training loss: 0.02398155, Validation loss: 0.02502849, Gradient norm: 0.52130354
INFO:root:[    6] Training loss: 0.02185838, Validation loss: 0.01901992, Gradient norm: 0.43810670
INFO:root:[    7] Training loss: 0.01973638, Validation loss: 0.02424413, Gradient norm: 0.39828387
INFO:root:[    8] Training loss: 0.01915900, Validation loss: 0.02094918, Gradient norm: 0.36258846
INFO:root:[    9] Training loss: 0.01773243, Validation loss: 0.01975153, Gradient norm: 0.34336387
INFO:root:[   10] Training loss: 0.01983915, Validation loss: 0.02385747, Gradient norm: 0.42293625
INFO:root:[   11] Training loss: 0.01739574, Validation loss: 0.02031080, Gradient norm: 0.33267705
INFO:root:[   12] Training loss: 0.01728627, Validation loss: 0.02267868, Gradient norm: 0.36607668
INFO:root:[   13] Training loss: 0.01804933, Validation loss: 0.02246249, Gradient norm: 0.42481729
INFO:root:[   14] Training loss: 0.01557560, Validation loss: 0.01788759, Gradient norm: 0.30425744
INFO:root:[   15] Training loss: 0.01766091, Validation loss: 0.02150769, Gradient norm: 0.41536709
INFO:root:[   16] Training loss: 0.01500063, Validation loss: 0.01980843, Gradient norm: 0.29540941
INFO:root:[   17] Training loss: 0.01598379, Validation loss: 0.02739601, Gradient norm: 0.36722667
INFO:root:[   18] Training loss: 0.01398405, Validation loss: 0.01993862, Gradient norm: 0.25641665
INFO:root:[   19] Training loss: 0.01607366, Validation loss: 0.01957839, Gradient norm: 0.39203640
INFO:root:[   20] Training loss: 0.01475332, Validation loss: 0.02314718, Gradient norm: 0.35645722
INFO:root:[   21] Training loss: 0.01409620, Validation loss: 0.02060597, Gradient norm: 0.28701879
INFO:root:[   22] Training loss: 0.01357269, Validation loss: 0.02095102, Gradient norm: 0.28682085
INFO:root:[   23] Training loss: 0.01430003, Validation loss: 0.02963996, Gradient norm: 0.34147280
INFO:root:[   24] Training loss: 0.01542251, Validation loss: 0.02019057, Gradient norm: 0.38262975
INFO:root:[   25] Training loss: 0.01386722, Validation loss: 0.02733953, Gradient norm: 0.31207493
INFO:root:[   26] Training loss: 0.01335778, Validation loss: 0.02087900, Gradient norm: 0.30525276
INFO:root:[   27] Training loss: 0.01252690, Validation loss: 0.02151368, Gradient norm: 0.24569232
INFO:root:[   28] Training loss: 0.01366063, Validation loss: 0.02129579, Gradient norm: 0.31299342
INFO:root:[   29] Training loss: 0.01287474, Validation loss: 0.02055870, Gradient norm: 0.27631174
INFO:root:[   30] Training loss: 0.01318420, Validation loss: 0.02385532, Gradient norm: 0.33332457
INFO:root:[   31] Training loss: 0.01270827, Validation loss: 0.02229983, Gradient norm: 0.28735636
INFO:root:[   32] Training loss: 0.01300006, Validation loss: 0.02274536, Gradient norm: 0.33149258
INFO:root:[   33] Training loss: 0.01294554, Validation loss: 0.02293861, Gradient norm: 0.31034431
INFO:root:[   34] Training loss: 0.01227501, Validation loss: 0.02893851, Gradient norm: 0.27784919
INFO:root:[   35] Training loss: 0.01205042, Validation loss: 0.02368773, Gradient norm: 0.24767643
INFO:root:[   36] Training loss: 0.01210914, Validation loss: 0.02473878, Gradient norm: 0.26693911
INFO:root:[   37] Training loss: 0.01229538, Validation loss: 0.02345622, Gradient norm: 0.27759750
INFO:root:[   38] Training loss: 0.01230343, Validation loss: 0.02335730, Gradient norm: 0.26466451
INFO:root:[   39] Training loss: 0.01215668, Validation loss: 0.02364411, Gradient norm: 0.28742090
INFO:root:[   40] Training loss: 0.01179764, Validation loss: 0.02425033, Gradient norm: 0.27261726
INFO:root:[   41] Training loss: 0.01225495, Validation loss: 0.02649791, Gradient norm: 0.28184293
INFO:root:[   42] Training loss: 0.01209678, Validation loss: 0.02343016, Gradient norm: 0.29578461
INFO:root:[   43] Training loss: 0.01212423, Validation loss: 0.02232119, Gradient norm: 0.28833913
INFO:root:[   44] Training loss: 0.01120991, Validation loss: 0.02502748, Gradient norm: 0.22445282
INFO:root:[   45] Training loss: 0.01170447, Validation loss: 0.02357424, Gradient norm: 0.28617640
INFO:root:[   46] Training loss: 0.01206509, Validation loss: 0.02529121, Gradient norm: 0.29832266
INFO:root:[   47] Training loss: 0.01189856, Validation loss: 0.02097420, Gradient norm: 0.27389991
INFO:root:[   48] Training loss: 0.01151816, Validation loss: 0.02213989, Gradient norm: 0.25956168
INFO:root:[   49] Training loss: 0.01161931, Validation loss: 0.02221466, Gradient norm: 0.29744493
INFO:root:[   50] Training loss: 0.01156405, Validation loss: 0.02429267, Gradient norm: 0.24553273
INFO:root:[   51] Training loss: 0.01147505, Validation loss: 0.02442685, Gradient norm: 0.29218968
INFO:root:[   52] Training loss: 0.01082935, Validation loss: 0.02714240, Gradient norm: 0.23113636
INFO:root:[   53] Training loss: 0.01169494, Validation loss: 0.02211232, Gradient norm: 0.29327970
INFO:root:[   54] Training loss: 0.01087716, Validation loss: 0.02366082, Gradient norm: 0.23325439
INFO:root:[   55] Training loss: 0.01151131, Validation loss: 0.02380848, Gradient norm: 0.28483665
INFO:root:[   56] Training loss: 0.01147546, Validation loss: 0.02872485, Gradient norm: 0.28251790
INFO:root:[   57] Training loss: 0.01101919, Validation loss: 0.02354278, Gradient norm: 0.24610987
INFO:root:[   58] Training loss: 0.01140502, Validation loss: 0.02632084, Gradient norm: 0.29480749
INFO:root:[   59] Training loss: 0.01089942, Validation loss: 0.02271185, Gradient norm: 0.23744166
INFO:root:[   60] Training loss: 0.01102367, Validation loss: 0.02619017, Gradient norm: 0.25794608
INFO:root:[   61] Training loss: 0.01118212, Validation loss: 0.02689512, Gradient norm: 0.27489555
INFO:root:[   62] Training loss: 0.01081277, Validation loss: 0.02787680, Gradient norm: 0.24908803
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 4041.483s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02094
INFO:root:EnergyScoreTrain: 0.01528
INFO:root:CoverageTrain: 0.98305
INFO:root:IntervalWidthTrain: 0.09851
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02485
INFO:root:EnergyScoreValidation: 0.01797
INFO:root:CoverageValidation: 0.91066
INFO:root:IntervalWidthValidation: 0.09018
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02534
INFO:root:EnergyScoreTest: 0.01829
INFO:root:CoverageTest: 0.90774
INFO:root:IntervalWidthTest: 0.09008
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05089689, Validation loss: 0.03256374, Gradient norm: 0.75700733
INFO:root:[    2] Training loss: 0.03192976, Validation loss: 0.02354070, Gradient norm: 0.51106614
INFO:root:[    3] Training loss: 0.02687483, Validation loss: 0.02935068, Gradient norm: 0.46747987
INFO:root:[    4] Training loss: 0.02698929, Validation loss: 0.02227810, Gradient norm: 0.51320586
INFO:root:[    5] Training loss: 0.02300132, Validation loss: 0.02005044, Gradient norm: 0.42947849
INFO:root:[    6] Training loss: 0.02111046, Validation loss: 0.02148355, Gradient norm: 0.37804921
INFO:root:[    7] Training loss: 0.01938974, Validation loss: 0.03077897, Gradient norm: 0.30682932
INFO:root:[    8] Training loss: 0.02127755, Validation loss: 0.02236001, Gradient norm: 0.41312604
INFO:root:[    9] Training loss: 0.01929445, Validation loss: 0.02426334, Gradient norm: 0.36128104
INFO:root:[   10] Training loss: 0.02074281, Validation loss: 0.02091590, Gradient norm: 0.44184721
INFO:root:[   11] Training loss: 0.01723354, Validation loss: 0.02699705, Gradient norm: 0.31176312
INFO:root:[   12] Training loss: 0.01764284, Validation loss: 0.02469566, Gradient norm: 0.34845965
INFO:root:[   13] Training loss: 0.01642240, Validation loss: 0.02671862, Gradient norm: 0.31518412
INFO:root:[   14] Training loss: 0.01641848, Validation loss: 0.03112716, Gradient norm: 0.31139170
INFO:root:[   15] Training loss: 0.01654283, Validation loss: 0.02712244, Gradient norm: 0.33373327
INFO:root:[   16] Training loss: 0.01573629, Validation loss: 0.02066177, Gradient norm: 0.30014738
INFO:root:[   17] Training loss: 0.01649858, Validation loss: 0.02211624, Gradient norm: 0.36216484
INFO:root:[   18] Training loss: 0.01477340, Validation loss: 0.02745824, Gradient norm: 0.28762941
INFO:root:[   19] Training loss: 0.01474612, Validation loss: 0.02330960, Gradient norm: 0.29389047
INFO:root:[   20] Training loss: 0.01537834, Validation loss: 0.02281014, Gradient norm: 0.33046068
INFO:root:[   21] Training loss: 0.01522604, Validation loss: 0.02217967, Gradient norm: 0.34916380
INFO:root:[   22] Training loss: 0.01474209, Validation loss: 0.02389886, Gradient norm: 0.31989935
INFO:root:[   23] Training loss: 0.01507184, Validation loss: 0.02200720, Gradient norm: 0.35225228
INFO:root:[   24] Training loss: 0.01460895, Validation loss: 0.02430340, Gradient norm: 0.36227775
INFO:root:[   25] Training loss: 0.01309268, Validation loss: 0.02580017, Gradient norm: 0.25899634
INFO:root:[   26] Training loss: 0.01324816, Validation loss: 0.02143985, Gradient norm: 0.26191210
INFO:root:[   27] Training loss: 0.01340135, Validation loss: 0.02793511, Gradient norm: 0.29477504
INFO:root:[   28] Training loss: 0.01348934, Validation loss: 0.02697663, Gradient norm: 0.30558267
INFO:root:[   29] Training loss: 0.01300812, Validation loss: 0.02483241, Gradient norm: 0.25378844
INFO:root:[   30] Training loss: 0.01421217, Validation loss: 0.02859402, Gradient norm: 0.34290674
INFO:root:[   31] Training loss: 0.01320098, Validation loss: 0.02360012, Gradient norm: 0.28085072
INFO:root:[   32] Training loss: 0.01261686, Validation loss: 0.02468792, Gradient norm: 0.22246113
INFO:root:[   33] Training loss: 0.01389385, Validation loss: 0.02034690, Gradient norm: 0.33789569
INFO:root:[   34] Training loss: 0.01267760, Validation loss: 0.02293130, Gradient norm: 0.28350155
INFO:root:[   35] Training loss: 0.01271916, Validation loss: 0.02176075, Gradient norm: 0.29510466
INFO:root:[   36] Training loss: 0.01205788, Validation loss: 0.02144826, Gradient norm: 0.23176735
INFO:root:[   37] Training loss: 0.01255732, Validation loss: 0.02069744, Gradient norm: 0.25281223
INFO:root:[   38] Training loss: 0.01301882, Validation loss: 0.02655444, Gradient norm: 0.30580734
INFO:root:[   39] Training loss: 0.01353575, Validation loss: 0.03076610, Gradient norm: 0.30872329
INFO:root:[   40] Training loss: 0.01266057, Validation loss: 0.02154131, Gradient norm: 0.27685857
INFO:root:[   41] Training loss: 0.01192251, Validation loss: 0.02271952, Gradient norm: 0.24177661
INFO:root:[   42] Training loss: 0.01184884, Validation loss: 0.02778414, Gradient norm: 0.23692204
INFO:root:[   43] Training loss: 0.01220495, Validation loss: 0.02054346, Gradient norm: 0.27186717
INFO:root:[   44] Training loss: 0.01303314, Validation loss: 0.02369207, Gradient norm: 0.30508587
INFO:root:[   45] Training loss: 0.01209393, Validation loss: 0.01975916, Gradient norm: 0.26906301
INFO:root:[   46] Training loss: 0.01194286, Validation loss: 0.02520408, Gradient norm: 0.23299072
INFO:root:[   47] Training loss: 0.01186453, Validation loss: 0.02287980, Gradient norm: 0.26304108
INFO:root:[   48] Training loss: 0.01161903, Validation loss: 0.02554419, Gradient norm: 0.22502409
INFO:root:[   49] Training loss: 0.01228249, Validation loss: 0.02791786, Gradient norm: 0.28581613
INFO:root:[   50] Training loss: 0.01273626, Validation loss: 0.02718502, Gradient norm: 0.30741297
INFO:root:[   51] Training loss: 0.01246147, Validation loss: 0.02221430, Gradient norm: 0.28342076
INFO:root:[   52] Training loss: 0.01175468, Validation loss: 0.02678995, Gradient norm: 0.25828720
INFO:root:[   53] Training loss: 0.01155567, Validation loss: 0.02692746, Gradient norm: 0.22184212
INFO:root:[   54] Training loss: 0.01234041, Validation loss: 0.02658422, Gradient norm: 0.30491025
INFO:root:[   55] Training loss: 0.01126520, Validation loss: 0.02163097, Gradient norm: 0.20916935
INFO:root:[   56] Training loss: 0.01167370, Validation loss: 0.02457080, Gradient norm: 0.24232199
INFO:root:[   57] Training loss: 0.01148892, Validation loss: 0.02501587, Gradient norm: 0.24724520
INFO:root:[   58] Training loss: 0.01133626, Validation loss: 0.02897090, Gradient norm: 0.23147512
INFO:root:[   59] Training loss: 0.01143350, Validation loss: 0.02592465, Gradient norm: 0.23279326
