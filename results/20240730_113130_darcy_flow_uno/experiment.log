INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05150203, Validation loss: 0.03109207, Gradient norm: 0.81786168
INFO:root:[    2] Training loss: 0.03447086, Validation loss: 0.02501567, Gradient norm: 0.84320479
INFO:root:[    3] Training loss: 0.02927670, Validation loss: 0.03056702, Gradient norm: 0.77997822
INFO:root:[    4] Training loss: 0.02501328, Validation loss: 0.02747732, Gradient norm: 0.66439009
INFO:root:[    5] Training loss: 0.02214913, Validation loss: 0.02799362, Gradient norm: 0.55590154
INFO:root:[    6] Training loss: 0.02404194, Validation loss: 0.03119944, Gradient norm: 0.67974999
INFO:root:[    7] Training loss: 0.02127331, Validation loss: 0.02169173, Gradient norm: 0.55771874
INFO:root:[    8] Training loss: 0.02029044, Validation loss: 0.02534407, Gradient norm: 0.52260956
INFO:root:[    9] Training loss: 0.01939256, Validation loss: 0.02149095, Gradient norm: 0.53362479
INFO:root:[   10] Training loss: 0.01782803, Validation loss: 0.02311288, Gradient norm: 0.45732904
INFO:root:[   11] Training loss: 0.01960412, Validation loss: 0.03326938, Gradient norm: 0.55785305
INFO:root:[   12] Training loss: 0.01704675, Validation loss: 0.02195058, Gradient norm: 0.40398832
INFO:root:[   13] Training loss: 0.01913378, Validation loss: 0.03102908, Gradient norm: 0.54486289
INFO:root:[   14] Training loss: 0.01844231, Validation loss: 0.02702183, Gradient norm: 0.54720735
INFO:root:[   15] Training loss: 0.01701257, Validation loss: 0.02507479, Gradient norm: 0.45621183
INFO:root:[   16] Training loss: 0.01655927, Validation loss: 0.02622275, Gradient norm: 0.46578995
INFO:root:[   17] Training loss: 0.01595554, Validation loss: 0.02432812, Gradient norm: 0.45041204
INFO:root:[   18] Training loss: 0.01527955, Validation loss: 0.02317727, Gradient norm: 0.42248704
INFO:root:[   19] Training loss: 0.01458266, Validation loss: 0.02160929, Gradient norm: 0.36005979
INFO:root:[   20] Training loss: 0.01428594, Validation loss: 0.02417338, Gradient norm: 0.36287904
INFO:root:[   21] Training loss: 0.01490952, Validation loss: 0.02730487, Gradient norm: 0.37835929
INFO:root:[   22] Training loss: 0.01656618, Validation loss: 0.02669309, Gradient norm: 0.50060522
INFO:root:[   23] Training loss: 0.01420779, Validation loss: 0.02624595, Gradient norm: 0.35908449
INFO:root:[   24] Training loss: 0.01424686, Validation loss: 0.03140604, Gradient norm: 0.43655617
INFO:root:[   25] Training loss: 0.01347463, Validation loss: 0.02513497, Gradient norm: 0.34988781
INFO:root:[   26] Training loss: 0.01419920, Validation loss: 0.02397412, Gradient norm: 0.39360154
INFO:root:[   27] Training loss: 0.01295798, Validation loss: 0.02930929, Gradient norm: 0.36450936
INFO:root:[   28] Training loss: 0.01284384, Validation loss: 0.02577242, Gradient norm: 0.35857767
INFO:root:[   29] Training loss: 0.01443557, Validation loss: 0.02389494, Gradient norm: 0.46339781
INFO:root:[   30] Training loss: 0.01309399, Validation loss: 0.02448288, Gradient norm: 0.36337305
INFO:root:[   31] Training loss: 0.01380523, Validation loss: 0.02569641, Gradient norm: 0.42782269
INFO:root:[   32] Training loss: 0.01281626, Validation loss: 0.02392553, Gradient norm: 0.33230515
INFO:root:[   33] Training loss: 0.01279119, Validation loss: 0.02586215, Gradient norm: 0.36789009
INFO:root:[   34] Training loss: 0.01187928, Validation loss: 0.02346660, Gradient norm: 0.33575426
INFO:root:[   35] Training loss: 0.01305210, Validation loss: 0.02807867, Gradient norm: 0.38714356
INFO:root:[   36] Training loss: 0.01240481, Validation loss: 0.02447418, Gradient norm: 0.37866822
INFO:root:[   37] Training loss: 0.01194493, Validation loss: 0.02664183, Gradient norm: 0.35204591
INFO:root:[   38] Training loss: 0.01200169, Validation loss: 0.03011746, Gradient norm: 0.32415290
INFO:root:[   39] Training loss: 0.01176316, Validation loss: 0.02510035, Gradient norm: 0.31401655
INFO:root:[   40] Training loss: 0.01179948, Validation loss: 0.02538531, Gradient norm: 0.30998423
INFO:root:[   41] Training loss: 0.01191963, Validation loss: 0.03257874, Gradient norm: 0.37235215
INFO:root:[   42] Training loss: 0.01221504, Validation loss: 0.02836670, Gradient norm: 0.36384868
INFO:root:[   43] Training loss: 0.01218429, Validation loss: 0.02748972, Gradient norm: 0.39401340
INFO:root:[   44] Training loss: 0.01066595, Validation loss: 0.02459179, Gradient norm: 0.30254309
INFO:root:[   45] Training loss: 0.01205042, Validation loss: 0.02757241, Gradient norm: 0.31873262
INFO:root:[   46] Training loss: 0.01117227, Validation loss: 0.02570309, Gradient norm: 0.30684241
INFO:root:[   47] Training loss: 0.01218589, Validation loss: 0.02367037, Gradient norm: 0.40261107
INFO:root:[   48] Training loss: 0.01092168, Validation loss: 0.02676528, Gradient norm: 0.32728672
INFO:root:[   49] Training loss: 0.01104655, Validation loss: 0.02950550, Gradient norm: 0.35233617
INFO:root:[   50] Training loss: 0.01104817, Validation loss: 0.02483158, Gradient norm: 0.33664473
INFO:root:[   51] Training loss: 0.01123617, Validation loss: 0.02891052, Gradient norm: 0.36420060
INFO:root:[   52] Training loss: 0.01119179, Validation loss: 0.02618705, Gradient norm: 0.33788569
INFO:root:[   53] Training loss: 0.01056991, Validation loss: 0.02885804, Gradient norm: 0.31014234
INFO:root:[   54] Training loss: 0.01091029, Validation loss: 0.02973985, Gradient norm: 0.33201992
INFO:root:[   55] Training loss: 0.01127595, Validation loss: 0.02638729, Gradient norm: 0.33043523
INFO:root:[   56] Training loss: 0.01120335, Validation loss: 0.02759835, Gradient norm: 0.34713924
INFO:root:[   57] Training loss: 0.01101026, Validation loss: 0.02894335, Gradient norm: 0.34897699
INFO:root:[   58] Training loss: 0.01079858, Validation loss: 0.02681840, Gradient norm: 0.32774848
INFO:root:[   59] Training loss: 0.00998710, Validation loss: 0.02700025, Gradient norm: 0.30104827
INFO:root:[   60] Training loss: 0.01060777, Validation loss: 0.02552971, Gradient norm: 0.35814253
INFO:root:[   61] Training loss: 0.01075314, Validation loss: 0.02420711, Gradient norm: 0.30049651
INFO:root:[   62] Training loss: 0.01031336, Validation loss: 0.02954339, Gradient norm: 0.28947443
INFO:root:[   63] Training loss: 0.01060225, Validation loss: 0.02735459, Gradient norm: 0.32901265
INFO:root:[   64] Training loss: 0.01067571, Validation loss: 0.02632465, Gradient norm: 0.34048350
INFO:root:[   65] Training loss: 0.01032984, Validation loss: 0.02761399, Gradient norm: 0.28280772
INFO:root:[   66] Training loss: 0.01088897, Validation loss: 0.02500934, Gradient norm: 0.34435165
INFO:root:[   67] Training loss: 0.01039685, Validation loss: 0.02614059, Gradient norm: 0.33229355
INFO:root:[   68] Training loss: 0.01013731, Validation loss: 0.02785559, Gradient norm: 0.29345703
INFO:root:[   69] Training loss: 0.00996304, Validation loss: 0.02723481, Gradient norm: 0.27488847
INFO:root:[   70] Training loss: 0.00991662, Validation loss: 0.03128931, Gradient norm: 0.27967969
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 4657.091s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02111
INFO:root:EnergyScoreTrain: 0.01545
INFO:root:CoverageTrain: 0.92728
INFO:root:IntervalWidthTrain: 0.08633
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02752
INFO:root:EnergyScoreValidation: 0.02169
INFO:root:CoverageValidation: 0.5912
INFO:root:IntervalWidthValidation: 0.04631
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02816
INFO:root:EnergyScoreTest: 0.02228
INFO:root:CoverageTest: 0.58303
INFO:root:IntervalWidthTest: 0.04646
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 150994944
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05889843, Validation loss: 0.03490156, Gradient norm: 1.17198299
INFO:root:[    2] Training loss: 0.03241879, Validation loss: 0.03156418, Gradient norm: 0.67208127
INFO:root:[    3] Training loss: 0.02878513, Validation loss: 0.02452674, Gradient norm: 0.67559429
INFO:root:[    4] Training loss: 0.02562810, Validation loss: 0.02213879, Gradient norm: 0.60002867
INFO:root:[    5] Training loss: 0.02384567, Validation loss: 0.02417981, Gradient norm: 0.53424908
INFO:root:[    6] Training loss: 0.02070094, Validation loss: 0.02376647, Gradient norm: 0.44771233
INFO:root:[    7] Training loss: 0.02192967, Validation loss: 0.02094561, Gradient norm: 0.54422372
INFO:root:[    8] Training loss: 0.02059335, Validation loss: 0.02525069, Gradient norm: 0.51019470
INFO:root:[    9] Training loss: 0.01936877, Validation loss: 0.02098291, Gradient norm: 0.46381282
INFO:root:[   10] Training loss: 0.02017402, Validation loss: 0.02693796, Gradient norm: 0.53653580
INFO:root:[   11] Training loss: 0.01980401, Validation loss: 0.02266809, Gradient norm: 0.49673540
INFO:root:[   12] Training loss: 0.01577983, Validation loss: 0.02059656, Gradient norm: 0.29159133
INFO:root:[   13] Training loss: 0.01740437, Validation loss: 0.01989256, Gradient norm: 0.42429429
INFO:root:[   14] Training loss: 0.01624714, Validation loss: 0.01873211, Gradient norm: 0.39695495
INFO:root:[   15] Training loss: 0.01684305, Validation loss: 0.02084754, Gradient norm: 0.42576701
INFO:root:[   16] Training loss: 0.01612172, Validation loss: 0.02242572, Gradient norm: 0.36426692
INFO:root:[   17] Training loss: 0.01692838, Validation loss: 0.02178351, Gradient norm: 0.44146379
INFO:root:[   18] Training loss: 0.01525253, Validation loss: 0.01797515, Gradient norm: 0.29867368
INFO:root:[   19] Training loss: 0.01559507, Validation loss: 0.02176939, Gradient norm: 0.40392185
INFO:root:[   20] Training loss: 0.01461241, Validation loss: 0.02100701, Gradient norm: 0.34388320
INFO:root:[   21] Training loss: 0.01481359, Validation loss: 0.02641838, Gradient norm: 0.39985608
INFO:root:[   22] Training loss: 0.01471347, Validation loss: 0.02092240, Gradient norm: 0.39226857
INFO:root:[   23] Training loss: 0.01391339, Validation loss: 0.02087385, Gradient norm: 0.32157126
INFO:root:[   24] Training loss: 0.01445842, Validation loss: 0.02018556, Gradient norm: 0.37421134
INFO:root:[   25] Training loss: 0.01489320, Validation loss: 0.02410770, Gradient norm: 0.39783102
INFO:root:[   26] Training loss: 0.01482760, Validation loss: 0.01945432, Gradient norm: 0.37638251
INFO:root:[   27] Training loss: 0.01311043, Validation loss: 0.01973693, Gradient norm: 0.33268173
INFO:root:[   28] Training loss: 0.01334459, Validation loss: 0.02134586, Gradient norm: 0.34221830
INFO:root:[   29] Training loss: 0.01283726, Validation loss: 0.02233484, Gradient norm: 0.28244014
INFO:root:[   30] Training loss: 0.01336700, Validation loss: 0.02453156, Gradient norm: 0.35007265
INFO:root:[   31] Training loss: 0.01334849, Validation loss: 0.02755206, Gradient norm: 0.34233640
INFO:root:[   32] Training loss: 0.01275775, Validation loss: 0.02182601, Gradient norm: 0.32184125
INFO:root:[   33] Training loss: 0.01298423, Validation loss: 0.02213168, Gradient norm: 0.34574045
INFO:root:[   34] Training loss: 0.01275062, Validation loss: 0.02211654, Gradient norm: 0.30893095
INFO:root:[   35] Training loss: 0.01251553, Validation loss: 0.02378831, Gradient norm: 0.31952807
INFO:root:[   36] Training loss: 0.01249233, Validation loss: 0.02074426, Gradient norm: 0.34619280
INFO:root:[   37] Training loss: 0.01178373, Validation loss: 0.02102927, Gradient norm: 0.27113659
INFO:root:[   38] Training loss: 0.01242734, Validation loss: 0.02220473, Gradient norm: 0.30894356
INFO:root:[   39] Training loss: 0.01239906, Validation loss: 0.02135641, Gradient norm: 0.28448693
INFO:root:[   40] Training loss: 0.01221800, Validation loss: 0.02235698, Gradient norm: 0.29999273
INFO:root:[   41] Training loss: 0.01277890, Validation loss: 0.02235662, Gradient norm: 0.34526691
INFO:root:[   42] Training loss: 0.01225137, Validation loss: 0.02278710, Gradient norm: 0.32802635
INFO:root:[   43] Training loss: 0.01199885, Validation loss: 0.02352539, Gradient norm: 0.28675538
INFO:root:[   44] Training loss: 0.01250061, Validation loss: 0.02794471, Gradient norm: 0.36373470
INFO:root:[   45] Training loss: 0.01250138, Validation loss: 0.02118236, Gradient norm: 0.36685050
INFO:root:[   46] Training loss: 0.01211546, Validation loss: 0.02211413, Gradient norm: 0.33884957
INFO:root:[   47] Training loss: 0.01120316, Validation loss: 0.02386698, Gradient norm: 0.21131007
INFO:root:[   48] Training loss: 0.01157129, Validation loss: 0.02883204, Gradient norm: 0.27791812
INFO:root:[   49] Training loss: 0.01122541, Validation loss: 0.02492430, Gradient norm: 0.27412050
INFO:root:[   50] Training loss: 0.01144548, Validation loss: 0.02443855, Gradient norm: 0.30247702
INFO:root:[   51] Training loss: 0.01142806, Validation loss: 0.02693629, Gradient norm: 0.30265540
INFO:root:[   52] Training loss: 0.01161241, Validation loss: 0.02547946, Gradient norm: 0.26246857
INFO:root:[   53] Training loss: 0.01148026, Validation loss: 0.02321428, Gradient norm: 0.28559720
INFO:root:[   54] Training loss: 0.01153697, Validation loss: 0.02520353, Gradient norm: 0.31352679
INFO:root:[   55] Training loss: 0.01132914, Validation loss: 0.02425199, Gradient norm: 0.29216098
INFO:root:[   56] Training loss: 0.01109608, Validation loss: 0.02538464, Gradient norm: 0.28598919
INFO:root:[   57] Training loss: 0.01068714, Validation loss: 0.02530983, Gradient norm: 0.25804894
INFO:root:[   58] Training loss: 0.01100945, Validation loss: 0.02183190, Gradient norm: 0.30382854
INFO:root:[   59] Training loss: 0.01081753, Validation loss: 0.02748475, Gradient norm: 0.23846594
INFO:root:[   60] Training loss: 0.01075627, Validation loss: 0.02303729, Gradient norm: 0.25305372
INFO:root:[   61] Training loss: 0.01076559, Validation loss: 0.02389898, Gradient norm: 0.28043752
INFO:root:[   62] Training loss: 0.01048746, Validation loss: 0.02200605, Gradient norm: 0.25210598
INFO:root:[   63] Training loss: 0.06186304, Validation loss: 0.03737921, Gradient norm: 2.09592134
INFO:root:[   64] Training loss: 0.02744684, Validation loss: 0.02467863, Gradient norm: 2.11082206
INFO:root:[   65] Training loss: 0.02365159, Validation loss: 0.02848621, Gradient norm: 1.67621484
INFO:root:[   66] Training loss: 0.02268419, Validation loss: 0.02271729, Gradient norm: 1.64832862
INFO:root:[   67] Training loss: 0.01969382, Validation loss: 0.03469665, Gradient norm: 1.17166876
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 4376.382s.
INFO:root:Emptying the cuda cache took 0.014s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02235
INFO:root:EnergyScoreTrain: 0.016
INFO:root:CoverageTrain: 0.97221
INFO:root:IntervalWidthTrain: 0.08477
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02502
INFO:root:EnergyScoreValidation: 0.01814
INFO:root:CoverageValidation: 0.86766
INFO:root:IntervalWidthValidation: 0.07625
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02539
INFO:root:EnergyScoreTest: 0.01844
INFO:root:CoverageTest: 0.86245
INFO:root:IntervalWidthTest: 0.07617
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05393209, Validation loss: 0.03881075, Gradient norm: 0.79163345
INFO:root:[    2] Training loss: 0.03107935, Validation loss: 0.02606592, Gradient norm: 0.53782424
INFO:root:[    3] Training loss: 0.02758880, Validation loss: 0.02092939, Gradient norm: 0.49872078
INFO:root:[    4] Training loss: 0.02254658, Validation loss: 0.02105183, Gradient norm: 0.43109662
INFO:root:[    5] Training loss: 0.02398155, Validation loss: 0.02502849, Gradient norm: 0.52130354
INFO:root:[    6] Training loss: 0.02185838, Validation loss: 0.01901992, Gradient norm: 0.43810670
INFO:root:[    7] Training loss: 0.01973638, Validation loss: 0.02424413, Gradient norm: 0.39828387
INFO:root:[    8] Training loss: 0.01915900, Validation loss: 0.02094918, Gradient norm: 0.36258846
INFO:root:[    9] Training loss: 0.01773243, Validation loss: 0.01975153, Gradient norm: 0.34336387
INFO:root:[   10] Training loss: 0.01983915, Validation loss: 0.02385747, Gradient norm: 0.42293625
INFO:root:[   11] Training loss: 0.01739574, Validation loss: 0.02031080, Gradient norm: 0.33267705
INFO:root:[   12] Training loss: 0.01728627, Validation loss: 0.02267868, Gradient norm: 0.36607668
INFO:root:[   13] Training loss: 0.01804933, Validation loss: 0.02246249, Gradient norm: 0.42481729
INFO:root:[   14] Training loss: 0.01557560, Validation loss: 0.01788759, Gradient norm: 0.30425744
INFO:root:[   15] Training loss: 0.01766091, Validation loss: 0.02150769, Gradient norm: 0.41536709
INFO:root:[   16] Training loss: 0.01500063, Validation loss: 0.01980843, Gradient norm: 0.29540941
INFO:root:[   17] Training loss: 0.01598379, Validation loss: 0.02739601, Gradient norm: 0.36722667
INFO:root:[   18] Training loss: 0.01398405, Validation loss: 0.01993862, Gradient norm: 0.25641665
INFO:root:[   19] Training loss: 0.01607366, Validation loss: 0.01957839, Gradient norm: 0.39203640
INFO:root:[   20] Training loss: 0.01475332, Validation loss: 0.02314718, Gradient norm: 0.35645722
INFO:root:[   21] Training loss: 0.01409620, Validation loss: 0.02060597, Gradient norm: 0.28701879
INFO:root:[   22] Training loss: 0.01357269, Validation loss: 0.02095102, Gradient norm: 0.28682085
INFO:root:[   23] Training loss: 0.01430003, Validation loss: 0.02963996, Gradient norm: 0.34147280
INFO:root:[   24] Training loss: 0.01542251, Validation loss: 0.02019057, Gradient norm: 0.38262975
INFO:root:[   25] Training loss: 0.01386722, Validation loss: 0.02733953, Gradient norm: 0.31207493
INFO:root:[   26] Training loss: 0.01335778, Validation loss: 0.02087900, Gradient norm: 0.30525276
INFO:root:[   27] Training loss: 0.01252690, Validation loss: 0.02151368, Gradient norm: 0.24569232
INFO:root:[   28] Training loss: 0.01366063, Validation loss: 0.02129579, Gradient norm: 0.31299342
INFO:root:[   29] Training loss: 0.01287474, Validation loss: 0.02055870, Gradient norm: 0.27631174
INFO:root:[   30] Training loss: 0.01318420, Validation loss: 0.02385532, Gradient norm: 0.33332457
INFO:root:[   31] Training loss: 0.01270827, Validation loss: 0.02229983, Gradient norm: 0.28735636
INFO:root:[   32] Training loss: 0.01300006, Validation loss: 0.02274536, Gradient norm: 0.33149258
INFO:root:[   33] Training loss: 0.01294554, Validation loss: 0.02293861, Gradient norm: 0.31034431
INFO:root:[   34] Training loss: 0.01227501, Validation loss: 0.02893851, Gradient norm: 0.27784919
INFO:root:[   35] Training loss: 0.01205042, Validation loss: 0.02368773, Gradient norm: 0.24767643
INFO:root:[   36] Training loss: 0.01210914, Validation loss: 0.02473878, Gradient norm: 0.26693911
INFO:root:[   37] Training loss: 0.01229538, Validation loss: 0.02345622, Gradient norm: 0.27759750
INFO:root:[   38] Training loss: 0.01230343, Validation loss: 0.02335730, Gradient norm: 0.26466451
INFO:root:[   39] Training loss: 0.01215668, Validation loss: 0.02364411, Gradient norm: 0.28742090
INFO:root:[   40] Training loss: 0.01179764, Validation loss: 0.02425033, Gradient norm: 0.27261726
INFO:root:[   41] Training loss: 0.01225495, Validation loss: 0.02649791, Gradient norm: 0.28184293
INFO:root:[   42] Training loss: 0.01209678, Validation loss: 0.02343016, Gradient norm: 0.29578461
INFO:root:[   43] Training loss: 0.01212423, Validation loss: 0.02232119, Gradient norm: 0.28833913
INFO:root:[   44] Training loss: 0.01120991, Validation loss: 0.02502748, Gradient norm: 0.22445282
INFO:root:[   45] Training loss: 0.01170447, Validation loss: 0.02357424, Gradient norm: 0.28617640
INFO:root:[   46] Training loss: 0.01206509, Validation loss: 0.02529121, Gradient norm: 0.29832266
INFO:root:[   47] Training loss: 0.01189856, Validation loss: 0.02097420, Gradient norm: 0.27389991
INFO:root:[   48] Training loss: 0.01151816, Validation loss: 0.02213989, Gradient norm: 0.25956168
INFO:root:[   49] Training loss: 0.01161931, Validation loss: 0.02221466, Gradient norm: 0.29744493
INFO:root:[   50] Training loss: 0.01156405, Validation loss: 0.02429267, Gradient norm: 0.24553273
INFO:root:[   51] Training loss: 0.01147505, Validation loss: 0.02442685, Gradient norm: 0.29218968
INFO:root:[   52] Training loss: 0.01082935, Validation loss: 0.02714240, Gradient norm: 0.23113636
INFO:root:[   53] Training loss: 0.01169494, Validation loss: 0.02211232, Gradient norm: 0.29327970
INFO:root:[   54] Training loss: 0.01087716, Validation loss: 0.02366082, Gradient norm: 0.23325439
INFO:root:[   55] Training loss: 0.01151131, Validation loss: 0.02380848, Gradient norm: 0.28483665
INFO:root:[   56] Training loss: 0.01147546, Validation loss: 0.02872485, Gradient norm: 0.28251790
INFO:root:[   57] Training loss: 0.01101919, Validation loss: 0.02354278, Gradient norm: 0.24610987
INFO:root:[   58] Training loss: 0.01140502, Validation loss: 0.02632084, Gradient norm: 0.29480749
INFO:root:[   59] Training loss: 0.01089942, Validation loss: 0.02271185, Gradient norm: 0.23744166
INFO:root:[   60] Training loss: 0.01102367, Validation loss: 0.02619017, Gradient norm: 0.25794608
INFO:root:[   61] Training loss: 0.01118212, Validation loss: 0.02689512, Gradient norm: 0.27489555
INFO:root:[   62] Training loss: 0.01081277, Validation loss: 0.02787680, Gradient norm: 0.24908803
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 4041.483s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02094
INFO:root:EnergyScoreTrain: 0.01528
INFO:root:CoverageTrain: 0.98305
INFO:root:IntervalWidthTrain: 0.09851
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02485
INFO:root:EnergyScoreValidation: 0.01797
INFO:root:CoverageValidation: 0.91066
INFO:root:IntervalWidthValidation: 0.09018
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02534
INFO:root:EnergyScoreTest: 0.01829
INFO:root:CoverageTest: 0.90774
INFO:root:IntervalWidthTest: 0.09008
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05089689, Validation loss: 0.03256374, Gradient norm: 0.75700733
INFO:root:[    2] Training loss: 0.03192976, Validation loss: 0.02354070, Gradient norm: 0.51106614
INFO:root:[    3] Training loss: 0.02687483, Validation loss: 0.02935068, Gradient norm: 0.46747987
INFO:root:[    4] Training loss: 0.02698929, Validation loss: 0.02227810, Gradient norm: 0.51320586
INFO:root:[    5] Training loss: 0.02300132, Validation loss: 0.02005044, Gradient norm: 0.42947849
INFO:root:[    6] Training loss: 0.02111046, Validation loss: 0.02148355, Gradient norm: 0.37804921
INFO:root:[    7] Training loss: 0.01938974, Validation loss: 0.03077897, Gradient norm: 0.30682932
INFO:root:[    8] Training loss: 0.02127755, Validation loss: 0.02236001, Gradient norm: 0.41312604
INFO:root:[    9] Training loss: 0.01929445, Validation loss: 0.02426334, Gradient norm: 0.36128104
INFO:root:[   10] Training loss: 0.02074281, Validation loss: 0.02091590, Gradient norm: 0.44184721
INFO:root:[   11] Training loss: 0.01723354, Validation loss: 0.02699705, Gradient norm: 0.31176312
INFO:root:[   12] Training loss: 0.01764284, Validation loss: 0.02469566, Gradient norm: 0.34845965
INFO:root:[   13] Training loss: 0.01642240, Validation loss: 0.02671862, Gradient norm: 0.31518412
INFO:root:[   14] Training loss: 0.01641848, Validation loss: 0.03112716, Gradient norm: 0.31139170
INFO:root:[   15] Training loss: 0.01654283, Validation loss: 0.02712244, Gradient norm: 0.33373327
INFO:root:[   16] Training loss: 0.01573629, Validation loss: 0.02066177, Gradient norm: 0.30014738
INFO:root:[   17] Training loss: 0.01649858, Validation loss: 0.02211624, Gradient norm: 0.36216484
INFO:root:[   18] Training loss: 0.01477340, Validation loss: 0.02745824, Gradient norm: 0.28762941
INFO:root:[   19] Training loss: 0.01474612, Validation loss: 0.02330960, Gradient norm: 0.29389047
INFO:root:[   20] Training loss: 0.01537834, Validation loss: 0.02281014, Gradient norm: 0.33046068
INFO:root:[   21] Training loss: 0.01522604, Validation loss: 0.02217967, Gradient norm: 0.34916380
INFO:root:[   22] Training loss: 0.01474209, Validation loss: 0.02389886, Gradient norm: 0.31989935
INFO:root:[   23] Training loss: 0.01507184, Validation loss: 0.02200720, Gradient norm: 0.35225228
INFO:root:[   24] Training loss: 0.01460895, Validation loss: 0.02430340, Gradient norm: 0.36227775
INFO:root:[   25] Training loss: 0.01309268, Validation loss: 0.02580017, Gradient norm: 0.25899634
INFO:root:[   26] Training loss: 0.01324816, Validation loss: 0.02143985, Gradient norm: 0.26191210
INFO:root:[   27] Training loss: 0.01340135, Validation loss: 0.02793511, Gradient norm: 0.29477504
INFO:root:[   28] Training loss: 0.01348934, Validation loss: 0.02697663, Gradient norm: 0.30558267
INFO:root:[   29] Training loss: 0.01300812, Validation loss: 0.02483241, Gradient norm: 0.25378844
INFO:root:[   30] Training loss: 0.01421217, Validation loss: 0.02859402, Gradient norm: 0.34290674
INFO:root:[   31] Training loss: 0.01320098, Validation loss: 0.02360012, Gradient norm: 0.28085072
INFO:root:[   32] Training loss: 0.01261686, Validation loss: 0.02468792, Gradient norm: 0.22246113
INFO:root:[   33] Training loss: 0.01389385, Validation loss: 0.02034690, Gradient norm: 0.33789569
INFO:root:[   34] Training loss: 0.01267760, Validation loss: 0.02293130, Gradient norm: 0.28350155
INFO:root:[   35] Training loss: 0.01271916, Validation loss: 0.02176075, Gradient norm: 0.29510466
INFO:root:[   36] Training loss: 0.01205788, Validation loss: 0.02144826, Gradient norm: 0.23176735
INFO:root:[   37] Training loss: 0.01255732, Validation loss: 0.02069744, Gradient norm: 0.25281223
INFO:root:[   38] Training loss: 0.01301882, Validation loss: 0.02655444, Gradient norm: 0.30580734
INFO:root:[   39] Training loss: 0.01353575, Validation loss: 0.03076610, Gradient norm: 0.30872329
INFO:root:[   40] Training loss: 0.01266057, Validation loss: 0.02154131, Gradient norm: 0.27685857
INFO:root:[   41] Training loss: 0.01192251, Validation loss: 0.02271952, Gradient norm: 0.24177661
INFO:root:[   42] Training loss: 0.01184884, Validation loss: 0.02778414, Gradient norm: 0.23692204
INFO:root:[   43] Training loss: 0.01220495, Validation loss: 0.02054346, Gradient norm: 0.27186717
INFO:root:[   44] Training loss: 0.01303314, Validation loss: 0.02369207, Gradient norm: 0.30508587
INFO:root:[   45] Training loss: 0.01209393, Validation loss: 0.01975916, Gradient norm: 0.26906301
INFO:root:[   46] Training loss: 0.01194286, Validation loss: 0.02520408, Gradient norm: 0.23299072
INFO:root:[   47] Training loss: 0.01186453, Validation loss: 0.02287980, Gradient norm: 0.26304108
INFO:root:[   48] Training loss: 0.01161903, Validation loss: 0.02554419, Gradient norm: 0.22502409
INFO:root:[   49] Training loss: 0.01228249, Validation loss: 0.02791786, Gradient norm: 0.28581613
INFO:root:[   50] Training loss: 0.01273626, Validation loss: 0.02718502, Gradient norm: 0.30741297
INFO:root:[   51] Training loss: 0.01246147, Validation loss: 0.02221430, Gradient norm: 0.28342076
INFO:root:[   52] Training loss: 0.01175468, Validation loss: 0.02678995, Gradient norm: 0.25828720
INFO:root:[   53] Training loss: 0.01155567, Validation loss: 0.02692746, Gradient norm: 0.22184212
INFO:root:[   54] Training loss: 0.01234041, Validation loss: 0.02658422, Gradient norm: 0.30491025
INFO:root:[   55] Training loss: 0.01126520, Validation loss: 0.02163097, Gradient norm: 0.20916935
INFO:root:[   56] Training loss: 0.01167370, Validation loss: 0.02457080, Gradient norm: 0.24232199
INFO:root:[   57] Training loss: 0.01148892, Validation loss: 0.02501587, Gradient norm: 0.24724520
INFO:root:[   58] Training loss: 0.01133626, Validation loss: 0.02897090, Gradient norm: 0.23147512
INFO:root:[   59] Training loss: 0.01143350, Validation loss: 0.02592465, Gradient norm: 0.23279326
INFO:root:[   60] Training loss: 0.01190722, Validation loss: 0.02732639, Gradient norm: 0.27215529
INFO:root:[   61] Training loss: 0.01150200, Validation loss: 0.02128715, Gradient norm: 0.23749870
INFO:root:[   62] Training loss: 0.01129622, Validation loss: 0.02408884, Gradient norm: 0.25166882
INFO:root:[   63] Training loss: 0.01136229, Validation loss: 0.02406472, Gradient norm: 0.24140688
INFO:root:[   64] Training loss: 0.01126361, Validation loss: 0.02541582, Gradient norm: 0.22302982
INFO:root:[   65] Training loss: 0.01131485, Validation loss: 0.02422007, Gradient norm: 0.25950190
INFO:root:[   66] Training loss: 0.01151669, Validation loss: 0.02580998, Gradient norm: 0.27349371
INFO:root:[   67] Training loss: 0.01091040, Validation loss: 0.03283838, Gradient norm: 0.20429439
INFO:root:[   68] Training loss: 0.01157771, Validation loss: 0.02756445, Gradient norm: 0.22196421
INFO:root:[   69] Training loss: 0.01194351, Validation loss: 0.02478768, Gradient norm: 0.27715491
INFO:root:[   70] Training loss: 0.01123895, Validation loss: 0.02861341, Gradient norm: 0.24232204
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 4561.076s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01964
INFO:root:EnergyScoreTrain: 0.01416
INFO:root:CoverageTrain: 0.987
INFO:root:IntervalWidthTrain: 0.09002
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02716
INFO:root:EnergyScoreValidation: 0.01992
INFO:root:CoverageValidation: 0.79741
INFO:root:IntervalWidthValidation: 0.07631
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02796
INFO:root:EnergyScoreTest: 0.0206
INFO:root:CoverageTest: 0.78861
INFO:root:IntervalWidthTest: 0.07605
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 268435456
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04922358, Validation loss: 0.03027285, Gradient norm: 0.66683615
INFO:root:[    2] Training loss: 0.02768062, Validation loss: 0.02761674, Gradient norm: 0.42148164
INFO:root:[    3] Training loss: 0.02628461, Validation loss: 0.02739120, Gradient norm: 0.44421180
INFO:root:[    4] Training loss: 0.02300536, Validation loss: 0.02042321, Gradient norm: 0.38126504
INFO:root:[    5] Training loss: 0.02019018, Validation loss: 0.02434816, Gradient norm: 0.32192630
INFO:root:[    6] Training loss: 0.02093589, Validation loss: 0.02204618, Gradient norm: 0.36311728
INFO:root:[    7] Training loss: 0.01997235, Validation loss: 0.02043279, Gradient norm: 0.34507329
INFO:root:[    8] Training loss: 0.01840856, Validation loss: 0.01858247, Gradient norm: 0.32255949
INFO:root:[    9] Training loss: 0.01748190, Validation loss: 0.02181089, Gradient norm: 0.24548543
INFO:root:[   10] Training loss: 0.01869388, Validation loss: 0.01840903, Gradient norm: 0.35550492
INFO:root:[   11] Training loss: 0.01814713, Validation loss: 0.01995814, Gradient norm: 0.35093084
INFO:root:[   12] Training loss: 0.01805767, Validation loss: 0.01971519, Gradient norm: 0.33318984
INFO:root:[   13] Training loss: 0.01898688, Validation loss: 0.02019797, Gradient norm: 0.40694397
INFO:root:[   14] Training loss: 0.01604522, Validation loss: 0.02099285, Gradient norm: 0.25490333
INFO:root:[   15] Training loss: 0.01707122, Validation loss: 0.02096493, Gradient norm: 0.32484920
INFO:root:[   16] Training loss: 0.01594763, Validation loss: 0.01872239, Gradient norm: 0.29044486
INFO:root:[   17] Training loss: 0.01699359, Validation loss: 0.02091304, Gradient norm: 0.36470516
INFO:root:[   18] Training loss: 0.01531262, Validation loss: 0.02746703, Gradient norm: 0.26166080
INFO:root:[   19] Training loss: 0.01558277, Validation loss: 0.01900988, Gradient norm: 0.30719694
INFO:root:[   20] Training loss: 0.01628008, Validation loss: 0.01850578, Gradient norm: 0.34898870
INFO:root:[   21] Training loss: 0.01615688, Validation loss: 0.02280619, Gradient norm: 0.34738284
INFO:root:[   22] Training loss: 0.01556653, Validation loss: 0.02084055, Gradient norm: 0.33194382
INFO:root:[   23] Training loss: 0.01529781, Validation loss: 0.02031072, Gradient norm: 0.32546820
INFO:root:[   24] Training loss: 0.01435606, Validation loss: 0.01902003, Gradient norm: 0.26617896
INFO:root:[   25] Training loss: 0.01494241, Validation loss: 0.01802708, Gradient norm: 0.32801076
INFO:root:[   26] Training loss: 0.01450202, Validation loss: 0.02410529, Gradient norm: 0.29225575
INFO:root:[   27] Training loss: 0.01437267, Validation loss: 0.02651466, Gradient norm: 0.32407252
INFO:root:[   28] Training loss: 0.01427685, Validation loss: 0.02267032, Gradient norm: 0.28801062
INFO:root:[   29] Training loss: 0.01372428, Validation loss: 0.02461607, Gradient norm: 0.23625443
INFO:root:[   30] Training loss: 0.01413984, Validation loss: 0.02022884, Gradient norm: 0.27216362
INFO:root:[   31] Training loss: 0.01512815, Validation loss: 0.02394401, Gradient norm: 0.35565114
INFO:root:[   32] Training loss: 0.01361962, Validation loss: 0.02634262, Gradient norm: 0.28652246
INFO:root:[   33] Training loss: 0.01396901, Validation loss: 0.02326802, Gradient norm: 0.28374576
INFO:root:[   34] Training loss: 0.01372673, Validation loss: 0.02442678, Gradient norm: 0.28313859
INFO:root:[   35] Training loss: 0.01353035, Validation loss: 0.02768736, Gradient norm: 0.27295977
INFO:root:[   36] Training loss: 0.01345598, Validation loss: 0.02560419, Gradient norm: 0.24553504
INFO:root:[   37] Training loss: 0.01472907, Validation loss: 0.02339677, Gradient norm: 0.33016279
INFO:root:[   38] Training loss: 0.01321194, Validation loss: 0.02282894, Gradient norm: 0.27640216
INFO:root:[   39] Training loss: 0.01356139, Validation loss: 0.02786613, Gradient norm: 0.28295591
INFO:root:[   40] Training loss: 0.01281444, Validation loss: 0.02873738, Gradient norm: 0.23834150
INFO:root:[   41] Training loss: 0.01285357, Validation loss: 0.02372605, Gradient norm: 0.26222769
INFO:root:[   42] Training loss: 0.01246363, Validation loss: 0.02167185, Gradient norm: 0.21852043
INFO:root:[   43] Training loss: 0.01399954, Validation loss: 0.02521368, Gradient norm: 0.31635315
INFO:root:[   44] Training loss: 0.01343348, Validation loss: 0.02430183, Gradient norm: 0.27545452
INFO:root:[   45] Training loss: 0.01269548, Validation loss: 0.02543683, Gradient norm: 0.22637846
INFO:root:[   46] Training loss: 0.01233808, Validation loss: 0.02589563, Gradient norm: 0.25270679
INFO:root:[   47] Training loss: 0.01255858, Validation loss: 0.02421331, Gradient norm: 0.24921743
INFO:root:[   48] Training loss: 0.01288820, Validation loss: 0.03048744, Gradient norm: 0.27738925
INFO:root:[   49] Training loss: 0.01278080, Validation loss: 0.02109623, Gradient norm: 0.26732198
INFO:root:[   50] Training loss: 0.01208822, Validation loss: 0.02365476, Gradient norm: 0.21505787
INFO:root:[   51] Training loss: 0.01250013, Validation loss: 0.03134252, Gradient norm: 0.25099333
INFO:root:[   52] Training loss: 0.01325389, Validation loss: 0.02353396, Gradient norm: 0.28095906
INFO:root:[   53] Training loss: 0.01227412, Validation loss: 0.02849354, Gradient norm: 0.22309147
INFO:root:[   54] Training loss: 0.01210980, Validation loss: 0.03264850, Gradient norm: 0.25514318
INFO:root:[   55] Training loss: 0.01257475, Validation loss: 0.02697318, Gradient norm: 0.27642730
INFO:root:[   56] Training loss: 0.01269829, Validation loss: 0.02655944, Gradient norm: 0.28567800
INFO:root:[   57] Training loss: 0.01195875, Validation loss: 0.02860687, Gradient norm: 0.22822880
INFO:root:[   58] Training loss: 0.01243943, Validation loss: 0.02735057, Gradient norm: 0.26037307
INFO:root:[   59] Training loss: 0.01244616, Validation loss: 0.02778985, Gradient norm: 0.24896522
INFO:root:[   60] Training loss: 0.01254695, Validation loss: 0.02627522, Gradient norm: 0.28069934
INFO:root:[   61] Training loss: 0.01216133, Validation loss: 0.02208974, Gradient norm: 0.23610112
INFO:root:[   62] Training loss: 0.01212365, Validation loss: 0.02764707, Gradient norm: 0.23229757
INFO:root:[   63] Training loss: 0.01204793, Validation loss: 0.02701923, Gradient norm: 0.23124855
INFO:root:[   64] Training loss: 0.01194971, Validation loss: 0.02283795, Gradient norm: 0.25244036
INFO:root:[   65] Training loss: 0.01197459, Validation loss: 0.02512942, Gradient norm: 0.24264333
INFO:root:[   66] Training loss: 0.01223805, Validation loss: 0.02575424, Gradient norm: 0.26080354
INFO:root:[   67] Training loss: 0.01190562, Validation loss: 0.03138205, Gradient norm: 0.24107159
INFO:root:[   68] Training loss: 0.01176233, Validation loss: 0.02251260, Gradient norm: 0.20545584
INFO:root:[   69] Training loss: 0.01206945, Validation loss: 0.02535030, Gradient norm: 0.27325849
INFO:root:[   70] Training loss: 0.01189392, Validation loss: 0.02407814, Gradient norm: 0.23729557
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 4582.183s.
INFO:root:Emptying the cuda cache took 0.009s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02028
INFO:root:EnergyScoreTrain: 0.01509
INFO:root:CoverageTrain: 0.99327
INFO:root:IntervalWidthTrain: 0.11001
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02486
INFO:root:EnergyScoreValidation: 0.01823
INFO:root:CoverageValidation: 0.91566
INFO:root:IntervalWidthValidation: 0.09393
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02555
INFO:root:EnergyScoreTest: 0.01877
INFO:root:CoverageTest: 0.90924
INFO:root:IntervalWidthTest: 0.0938
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05152338, Validation loss: 0.03575528, Gradient norm: 0.51710893
INFO:root:[    2] Training loss: 0.03446231, Validation loss: 0.02593504, Gradient norm: 0.44807971
INFO:root:[    3] Training loss: 0.02920907, Validation loss: 0.02572447, Gradient norm: 0.35787518
INFO:root:[    4] Training loss: 0.02677526, Validation loss: 0.02240257, Gradient norm: 0.35511006
INFO:root:[    5] Training loss: 0.02378270, Validation loss: 0.02171385, Gradient norm: 0.32010648
INFO:root:[    6] Training loss: 0.02323179, Validation loss: 0.02578424, Gradient norm: 0.32224875
INFO:root:[    7] Training loss: 0.02241875, Validation loss: 0.02237571, Gradient norm: 0.32372210
INFO:root:[    8] Training loss: 0.02188913, Validation loss: 0.02306471, Gradient norm: 0.37411480
INFO:root:[    9] Training loss: 0.02015174, Validation loss: 0.02233740, Gradient norm: 0.29336887
INFO:root:[   10] Training loss: 0.01872076, Validation loss: 0.02044998, Gradient norm: 0.23050801
INFO:root:[   11] Training loss: 0.01927291, Validation loss: 0.02033570, Gradient norm: 0.29322500
INFO:root:[   12] Training loss: 0.01972238, Validation loss: 0.02445272, Gradient norm: 0.36828859
INFO:root:[   13] Training loss: 0.01742546, Validation loss: 0.01970880, Gradient norm: 0.26525761
INFO:root:[   14] Training loss: 0.01888747, Validation loss: 0.01988395, Gradient norm: 0.34522688
INFO:root:[   15] Training loss: 0.01682841, Validation loss: 0.02912634, Gradient norm: 0.24397507
INFO:root:[   16] Training loss: 0.01685468, Validation loss: 0.02141116, Gradient norm: 0.26087911
INFO:root:[   17] Training loss: 0.01749995, Validation loss: 0.02369280, Gradient norm: 0.29649288
INFO:root:[   18] Training loss: 0.01612157, Validation loss: 0.02083584, Gradient norm: 0.26428421
INFO:root:[   19] Training loss: 0.01595771, Validation loss: 0.01976112, Gradient norm: 0.24928371
INFO:root:[   20] Training loss: 0.01631761, Validation loss: 0.02136120, Gradient norm: 0.28648225
INFO:root:[   21] Training loss: 0.01519103, Validation loss: 0.02213911, Gradient norm: 0.18854483
INFO:root:[   22] Training loss: 0.01620512, Validation loss: 0.02569433, Gradient norm: 0.27505375
INFO:root:[   23] Training loss: 0.01675421, Validation loss: 0.02727671, Gradient norm: 0.30306222
INFO:root:[   24] Training loss: 0.01538722, Validation loss: 0.01927356, Gradient norm: 0.22930537
INFO:root:[   25] Training loss: 0.01580658, Validation loss: 0.03118415, Gradient norm: 0.26596339
INFO:root:[   26] Training loss: 0.01588268, Validation loss: 0.02399279, Gradient norm: 0.27289239
INFO:root:[   27] Training loss: 0.01624846, Validation loss: 0.02258100, Gradient norm: 0.30668376
INFO:root:[   28] Training loss: 0.01471772, Validation loss: 0.02742980, Gradient norm: 0.23055507
INFO:root:[   29] Training loss: 0.01464143, Validation loss: 0.02584014, Gradient norm: 0.23272908
INFO:root:[   30] Training loss: 0.01461448, Validation loss: 0.02723006, Gradient norm: 0.22021456
INFO:root:[   31] Training loss: 0.01499813, Validation loss: 0.02550547, Gradient norm: 0.25372864
INFO:root:[   32] Training loss: 0.01471096, Validation loss: 0.02890416, Gradient norm: 0.22702601
INFO:root:[   33] Training loss: 0.01453007, Validation loss: 0.02633715, Gradient norm: 0.23192705
INFO:root:[   34] Training loss: 0.01530118, Validation loss: 0.02047335, Gradient norm: 0.28418586
INFO:root:[   35] Training loss: 0.01466943, Validation loss: 0.02620348, Gradient norm: 0.26257855
INFO:root:[   36] Training loss: 0.01429641, Validation loss: 0.02493033, Gradient norm: 0.22301087
INFO:root:[   37] Training loss: 0.01416816, Validation loss: 0.02905504, Gradient norm: 0.22115963
INFO:root:[   38] Training loss: 0.01441906, Validation loss: 0.02421970, Gradient norm: 0.24814271
INFO:root:[   39] Training loss: 0.01494725, Validation loss: 0.02179706, Gradient norm: 0.26263613
INFO:root:[   40] Training loss: 0.01450615, Validation loss: 0.02247304, Gradient norm: 0.25030393
INFO:root:[   41] Training loss: 0.01347468, Validation loss: 0.02243824, Gradient norm: 0.20951418
INFO:root:[   42] Training loss: 0.01417810, Validation loss: 0.02790545, Gradient norm: 0.25865344
INFO:root:[   43] Training loss: 0.01354294, Validation loss: 0.02294269, Gradient norm: 0.20491153
INFO:root:[   44] Training loss: 0.01336748, Validation loss: 0.02916051, Gradient norm: 0.18232005
INFO:root:[   45] Training loss: 0.01404625, Validation loss: 0.02144575, Gradient norm: 0.24386817
INFO:root:[   46] Training loss: 0.01396939, Validation loss: 0.02423194, Gradient norm: 0.24348190
INFO:root:[   47] Training loss: 0.01255714, Validation loss: 0.02756221, Gradient norm: 0.16770587
INFO:root:[   48] Training loss: 0.01368450, Validation loss: 0.02424349, Gradient norm: 0.22851817
INFO:root:[   49] Training loss: 0.01427612, Validation loss: 0.02318783, Gradient norm: 0.26614279
INFO:root:[   50] Training loss: 0.01279701, Validation loss: 0.02308663, Gradient norm: 0.17522217
INFO:root:[   51] Training loss: 0.01289235, Validation loss: 0.02819382, Gradient norm: 0.19595920
INFO:root:[   52] Training loss: 0.01413628, Validation loss: 0.02557067, Gradient norm: 0.25730311
INFO:root:[   53] Training loss: 0.01310712, Validation loss: 0.02684475, Gradient norm: 0.21598080
INFO:root:[   54] Training loss: 0.01328699, Validation loss: 0.02182832, Gradient norm: 0.22150098
INFO:root:[   55] Training loss: 0.01297453, Validation loss: 0.02103135, Gradient norm: 0.20270599
INFO:root:[   56] Training loss: 0.01353065, Validation loss: 0.02239474, Gradient norm: 0.23947844
INFO:root:[   57] Training loss: 0.01306397, Validation loss: 0.02272176, Gradient norm: 0.23274834
INFO:root:[   58] Training loss: 0.01301608, Validation loss: 0.02390207, Gradient norm: 0.21809251
INFO:root:[   59] Training loss: 0.01395314, Validation loss: 0.02462677, Gradient norm: 0.25224571
INFO:root:[   60] Training loss: 0.01231960, Validation loss: 0.02648502, Gradient norm: 0.17330213
INFO:root:[   61] Training loss: 0.01302266, Validation loss: 0.02691680, Gradient norm: 0.19365494
INFO:root:[   62] Training loss: 0.01303053, Validation loss: 0.02392533, Gradient norm: 0.21784632
INFO:root:[   63] Training loss: 0.01246886, Validation loss: 0.03176334, Gradient norm: 0.19052973
INFO:root:[   64] Training loss: 0.01359887, Validation loss: 0.02660663, Gradient norm: 0.26586832
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 4132.313s.
INFO:root:Emptying the cuda cache took 0.011s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02671
INFO:root:EnergyScoreTrain: 0.01949
INFO:root:CoverageTrain: 0.98926
INFO:root:IntervalWidthTrain: 0.12891
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02633
INFO:root:EnergyScoreValidation: 0.01915
INFO:root:CoverageValidation: 0.92579
INFO:root:IntervalWidthValidation: 0.10655
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02677
INFO:root:EnergyScoreTest: 0.0195
INFO:root:CoverageTest: 0.92307
INFO:root:IntervalWidthTest: 0.10642
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05561550, Validation loss: 0.03603849, Gradient norm: 0.86327586
INFO:root:[    2] Training loss: 0.03260549, Validation loss: 0.03269812, Gradient norm: 0.62085971
INFO:root:[    3] Training loss: 0.02773507, Validation loss: 0.02269593, Gradient norm: 0.53097213
INFO:root:[    4] Training loss: 0.02579545, Validation loss: 0.02356113, Gradient norm: 0.51193343
INFO:root:[    5] Training loss: 0.02266605, Validation loss: 0.02100649, Gradient norm: 0.47250350
INFO:root:[    6] Training loss: 0.02138938, Validation loss: 0.02215795, Gradient norm: 0.46832686
INFO:root:[    7] Training loss: 0.02066416, Validation loss: 0.01905694, Gradient norm: 0.40066735
INFO:root:[    8] Training loss: 0.01980798, Validation loss: 0.02031254, Gradient norm: 0.42765157
INFO:root:[    9] Training loss: 0.01883348, Validation loss: 0.02101763, Gradient norm: 0.36775163
INFO:root:[   10] Training loss: 0.01778160, Validation loss: 0.01902820, Gradient norm: 0.35680927
INFO:root:[   11] Training loss: 0.01874140, Validation loss: 0.02303628, Gradient norm: 0.37464757
INFO:root:[   12] Training loss: 0.01902718, Validation loss: 0.02247030, Gradient norm: 0.42907659
INFO:root:[   13] Training loss: 0.01761109, Validation loss: 0.02129897, Gradient norm: 0.36122596
INFO:root:[   14] Training loss: 0.01875721, Validation loss: 0.01972158, Gradient norm: 0.41465181
INFO:root:[   15] Training loss: 0.01628068, Validation loss: 0.01982077, Gradient norm: 0.35432829
INFO:root:[   16] Training loss: 0.01757546, Validation loss: 0.02708933, Gradient norm: 0.42519201
INFO:root:[   17] Training loss: 0.01646849, Validation loss: 0.01981581, Gradient norm: 0.34981511
INFO:root:[   18] Training loss: 0.01580956, Validation loss: 0.02239823, Gradient norm: 0.36966522
INFO:root:[   19] Training loss: 0.01437192, Validation loss: 0.02031896, Gradient norm: 0.29597983
INFO:root:[   20] Training loss: 0.01467168, Validation loss: 0.02370037, Gradient norm: 0.33016032
INFO:root:[   21] Training loss: 0.01529461, Validation loss: 0.02021872, Gradient norm: 0.35304138
INFO:root:[   22] Training loss: 0.01545469, Validation loss: 0.02192299, Gradient norm: 0.40656492
INFO:root:[   23] Training loss: 0.01482230, Validation loss: 0.02268064, Gradient norm: 0.35071499
INFO:root:[   24] Training loss: 0.01534126, Validation loss: 0.02429584, Gradient norm: 0.37002249
INFO:root:[   25] Training loss: 0.01403968, Validation loss: 0.02147142, Gradient norm: 0.36382018
INFO:root:[   26] Training loss: 0.01467706, Validation loss: 0.02034660, Gradient norm: 0.36676637
INFO:root:[   27] Training loss: 0.01376433, Validation loss: 0.02264372, Gradient norm: 0.31368029
INFO:root:[   28] Training loss: 0.01386126, Validation loss: 0.02134419, Gradient norm: 0.35901928
INFO:root:[   29] Training loss: 0.01383822, Validation loss: 0.02537401, Gradient norm: 0.33607284
INFO:root:[   30] Training loss: 0.01342116, Validation loss: 0.02311856, Gradient norm: 0.36747553
INFO:root:[   31] Training loss: 0.01315979, Validation loss: 0.02257250, Gradient norm: 0.32144815
INFO:root:[   32] Training loss: 0.01293732, Validation loss: 0.02106935, Gradient norm: 0.28650427
INFO:root:[   33] Training loss: 0.01336829, Validation loss: 0.02326768, Gradient norm: 0.32793702
INFO:root:[   34] Training loss: 0.01334167, Validation loss: 0.02494428, Gradient norm: 0.33618525
INFO:root:[   35] Training loss: 0.01244188, Validation loss: 0.02495403, Gradient norm: 0.29195098
INFO:root:[   36] Training loss: 0.01247841, Validation loss: 0.02414100, Gradient norm: 0.29055697
INFO:root:[   37] Training loss: 0.01314657, Validation loss: 0.02012863, Gradient norm: 0.34563197
INFO:root:[   38] Training loss: 0.01320751, Validation loss: 0.02412236, Gradient norm: 0.30432547
INFO:root:[   39] Training loss: 0.01341019, Validation loss: 0.02652245, Gradient norm: 0.38368553
INFO:root:[   40] Training loss: 0.01256869, Validation loss: 0.02095359, Gradient norm: 0.33166912
INFO:root:[   41] Training loss: 0.01269412, Validation loss: 0.02366654, Gradient norm: 0.32182274
INFO:root:[   42] Training loss: 0.01241466, Validation loss: 0.02580284, Gradient norm: 0.30792828
INFO:root:[   43] Training loss: 0.01225954, Validation loss: 0.02607234, Gradient norm: 0.32709426
INFO:root:[   44] Training loss: 0.01234034, Validation loss: 0.02219307, Gradient norm: 0.31045940
INFO:root:[   45] Training loss: 0.01232602, Validation loss: 0.02471668, Gradient norm: 0.34370617
INFO:root:[   46] Training loss: 0.01253398, Validation loss: 0.02409458, Gradient norm: 0.32626167
INFO:root:[   47] Training loss: 0.01238249, Validation loss: 0.02379961, Gradient norm: 0.29381450
INFO:root:[   48] Training loss: 0.01190925, Validation loss: 0.02220153, Gradient norm: 0.31415747
INFO:root:[   49] Training loss: 0.01195130, Validation loss: 0.02330675, Gradient norm: 0.31372035
INFO:root:[   50] Training loss: 0.01112477, Validation loss: 0.02168371, Gradient norm: 0.23647686
INFO:root:[   51] Training loss: 0.01186170, Validation loss: 0.02346756, Gradient norm: 0.29176215
INFO:root:[   52] Training loss: 0.01146777, Validation loss: 0.02477146, Gradient norm: 0.29297142
INFO:root:[   53] Training loss: 0.22970490, Validation loss: 0.11373314, Gradient norm: 6.38387918
INFO:root:[   54] Training loss: 0.11370301, Validation loss: 0.11216778, Gradient norm: 0.05521981
INFO:root:[   55] Training loss: 0.11363737, Validation loss: 0.11321675, Gradient norm: 0.05148523
INFO:root:[   56] Training loss: 0.11364711, Validation loss: 0.11203539, Gradient norm: 0.05053778
INFO:root:[   57] Training loss: 0.11358445, Validation loss: 0.11261050, Gradient norm: 0.05393938
INFO:root:[   58] Training loss: 0.11379639, Validation loss: 0.11200915, Gradient norm: 0.05256419
INFO:root:[   59] Training loss: 0.11363013, Validation loss: 0.11200628, Gradient norm: 0.05123297
INFO:root:[   60] Training loss: 0.11367089, Validation loss: 0.11210252, Gradient norm: 0.05163038
INFO:root:[   61] Training loss: 0.11368121, Validation loss: 0.11441593, Gradient norm: 0.04978776
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1885.3s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02075
INFO:root:EnergyScoreTrain: 0.0154
INFO:root:CoverageTrain: 0.79087
INFO:root:IntervalWidthTrain: 0.06751
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02565
INFO:root:EnergyScoreValidation: 0.01908
INFO:root:CoverageValidation: 0.70243
INFO:root:IntervalWidthValidation: 0.06634
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02605
INFO:root:EnergyScoreTest: 0.01946
INFO:root:CoverageTest: 0.69988
INFO:root:IntervalWidthTest: 0.06645
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06115870, Validation loss: 0.03269427, Gradient norm: 0.76431743
INFO:root:[    2] Training loss: 0.03552530, Validation loss: 0.02451507, Gradient norm: 0.53753791
INFO:root:[    3] Training loss: 0.03021333, Validation loss: 0.03393024, Gradient norm: 0.46496701
INFO:root:[    4] Training loss: 0.02631381, Validation loss: 0.02135180, Gradient norm: 0.38828221
INFO:root:[    5] Training loss: 0.02398426, Validation loss: 0.02172292, Gradient norm: 0.33985830
INFO:root:[    6] Training loss: 0.02397232, Validation loss: 0.02079982, Gradient norm: 0.37120962
INFO:root:[    7] Training loss: 0.02368142, Validation loss: 0.02088667, Gradient norm: 0.37622189
INFO:root:[    8] Training loss: 0.02129855, Validation loss: 0.01982577, Gradient norm: 0.29829423
INFO:root:[    9] Training loss: 0.02112726, Validation loss: 0.02398233, Gradient norm: 0.34476351
INFO:root:[   10] Training loss: 0.02071305, Validation loss: 0.01927405, Gradient norm: 0.35125589
INFO:root:[   11] Training loss: 0.02033787, Validation loss: 0.02080098, Gradient norm: 0.36461049
INFO:root:[   12] Training loss: 0.01877039, Validation loss: 0.01966563, Gradient norm: 0.29910445
INFO:root:[   13] Training loss: 0.01842943, Validation loss: 0.02284218, Gradient norm: 0.32547944
INFO:root:[   14] Training loss: 0.01945369, Validation loss: 0.02233974, Gradient norm: 0.34397842
INFO:root:[   15] Training loss: 0.01880887, Validation loss: 0.02110843, Gradient norm: 0.31840070
INFO:root:[   16] Training loss: 0.01886626, Validation loss: 0.01908036, Gradient norm: 0.33389963
INFO:root:[   17] Training loss: 0.01806705, Validation loss: 0.02590773, Gradient norm: 0.33823161
INFO:root:[   18] Training loss: 0.01757228, Validation loss: 0.02306033, Gradient norm: 0.30929344
INFO:root:[   19] Training loss: 0.01776832, Validation loss: 0.02141431, Gradient norm: 0.30447109
INFO:root:[   20] Training loss: 0.01667491, Validation loss: 0.02719697, Gradient norm: 0.30654000
INFO:root:[   21] Training loss: 0.01720942, Validation loss: 0.02460104, Gradient norm: 0.35269472
INFO:root:[   22] Training loss: 0.01657749, Validation loss: 0.02366662, Gradient norm: 0.30658575
INFO:root:[   23] Training loss: 0.01669342, Validation loss: 0.02038635, Gradient norm: 0.28744091
INFO:root:[   24] Training loss: 0.01724810, Validation loss: 0.02624322, Gradient norm: 0.36071076
INFO:root:[   25] Training loss: 0.01642358, Validation loss: 0.02806597, Gradient norm: 0.31048405
INFO:root:[   26] Training loss: 0.01595323, Validation loss: 0.02369374, Gradient norm: 0.30520567
INFO:root:[   27] Training loss: 0.01553130, Validation loss: 0.02812589, Gradient norm: 0.30253794
INFO:root:[   28] Training loss: 0.01589125, Validation loss: 0.02446887, Gradient norm: 0.31736860
INFO:root:[   29] Training loss: 0.01637932, Validation loss: 0.02330240, Gradient norm: 0.33700529
INFO:root:[   30] Training loss: 0.01626437, Validation loss: 0.02347440, Gradient norm: 0.35627687
INFO:root:[   31] Training loss: 0.01516567, Validation loss: 0.02768401, Gradient norm: 0.26699480
INFO:root:[   32] Training loss: 0.01545179, Validation loss: 0.02354881, Gradient norm: 0.33522735
INFO:root:[   33] Training loss: 0.01502082, Validation loss: 0.02477086, Gradient norm: 0.28244101
INFO:root:[   34] Training loss: 0.01530248, Validation loss: 0.02867989, Gradient norm: 0.30740627
INFO:root:[   35] Training loss: 0.01523760, Validation loss: 0.02550659, Gradient norm: 0.31324205
INFO:root:[   36] Training loss: 0.01556399, Validation loss: 0.02716841, Gradient norm: 0.31987946
INFO:root:[   37] Training loss: 0.01445535, Validation loss: 0.02863993, Gradient norm: 0.29254393
INFO:root:[   38] Training loss: 0.01502043, Validation loss: 0.02501640, Gradient norm: 0.31885161
INFO:root:[   39] Training loss: 0.01512718, Validation loss: 0.02314240, Gradient norm: 0.32038207
INFO:root:[   40] Training loss: 0.01448459, Validation loss: 0.02406937, Gradient norm: 0.33923614
INFO:root:[   41] Training loss: 0.01449213, Validation loss: 0.02382535, Gradient norm: 0.28335031
INFO:root:[   42] Training loss: 0.01435593, Validation loss: 0.02751290, Gradient norm: 0.30068617
INFO:root:[   43] Training loss: 0.01387351, Validation loss: 0.02578978, Gradient norm: 0.27339360
INFO:root:[   44] Training loss: 0.01469118, Validation loss: 0.02560152, Gradient norm: 0.31365037
INFO:root:[   45] Training loss: 0.01419053, Validation loss: 0.02258328, Gradient norm: 0.29615366
INFO:root:[   46] Training loss: 0.01449516, Validation loss: 0.02633120, Gradient norm: 0.30874588
INFO:root:[   47] Training loss: 0.01371003, Validation loss: 0.02445157, Gradient norm: 0.24746947
INFO:root:[   48] Training loss: 0.01362355, Validation loss: 0.02586479, Gradient norm: 0.27361106
INFO:root:[   49] Training loss: 0.01393486, Validation loss: 0.02673129, Gradient norm: 0.30565189
INFO:root:[   50] Training loss: 0.01385403, Validation loss: 0.02846270, Gradient norm: 0.26294985
INFO:root:[   51] Training loss: 0.06163837, Validation loss: 0.11376151, Gradient norm: 0.55551315
INFO:root:[   52] Training loss: 0.11367773, Validation loss: 0.11208877, Gradient norm: 0.04741165
INFO:root:[   53] Training loss: 0.11361406, Validation loss: 0.11271376, Gradient norm: 0.04804865
INFO:root:[   54] Training loss: 0.11360855, Validation loss: 0.11184703, Gradient norm: 0.05528083
INFO:root:[   55] Training loss: 0.11362877, Validation loss: 0.11414294, Gradient norm: 0.05678821
INFO:root:[   56] Training loss: 0.11366995, Validation loss: 0.11362254, Gradient norm: 0.05260346
INFO:root:[   57] Training loss: 0.11366013, Validation loss: 0.11504767, Gradient norm: 0.05019653
INFO:root:[   58] Training loss: 0.11366816, Validation loss: 0.11275241, Gradient norm: 0.05906226
INFO:root:[   59] Training loss: 0.11356446, Validation loss: 0.11343840, Gradient norm: 0.05318635
INFO:root:[   60] Training loss: 0.11371989, Validation loss: 0.11175503, Gradient norm: 0.05461586
INFO:root:[   61] Training loss: 0.11362818, Validation loss: 0.11345734, Gradient norm: 0.05641700
INFO:root:[   62] Training loss: 0.11366246, Validation loss: 0.11297843, Gradient norm: 0.05670523
INFO:root:[   63] Training loss: 0.11362297, Validation loss: 0.11275728, Gradient norm: 0.05610057
INFO:root:[   64] Training loss: 0.11376534, Validation loss: 0.11315403, Gradient norm: 0.06024107
INFO:root:[   65] Training loss: 0.11371854, Validation loss: 0.11316694, Gradient norm: 0.05336366
INFO:root:[   66] Training loss: 0.11360769, Validation loss: 0.11179021, Gradient norm: 0.05591913
INFO:root:[   67] Training loss: 0.11373147, Validation loss: 0.11254637, Gradient norm: 0.05032384
INFO:root:[   68] Training loss: 0.11362682, Validation loss: 0.11243841, Gradient norm: 0.04784203
INFO:root:[   69] Training loss: 0.11372869, Validation loss: 0.11324215, Gradient norm: 0.06314166
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2125.46s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02345
INFO:root:EnergyScoreTrain: 0.01718
INFO:root:CoverageTrain: 0.63925
INFO:root:IntervalWidthTrain: 0.05818
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02539
INFO:root:EnergyScoreValidation: 0.01866
INFO:root:CoverageValidation: 0.66515
INFO:root:IntervalWidthValidation: 0.06319
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0257
INFO:root:EnergyScoreTest: 0.01887
INFO:root:CoverageTest: 0.66724
INFO:root:IntervalWidthTest: 0.06338
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07161020, Validation loss: 0.04211346, Gradient norm: 0.96482416
INFO:root:[    2] Training loss: 0.04021601, Validation loss: 0.03271985, Gradient norm: 0.62993050
INFO:root:[    3] Training loss: 0.03351196, Validation loss: 0.03038315, Gradient norm: 0.44987260
INFO:root:[    4] Training loss: 0.03163700, Validation loss: 0.02333315, Gradient norm: 0.49421413
INFO:root:[    5] Training loss: 0.02774553, Validation loss: 0.02794861, Gradient norm: 0.37350412
INFO:root:[    6] Training loss: 0.02620629, Validation loss: 0.02520127, Gradient norm: 0.35624003
INFO:root:[    7] Training loss: 0.02639221, Validation loss: 0.02779069, Gradient norm: 0.42268950
INFO:root:[    8] Training loss: 0.02501600, Validation loss: 0.02071822, Gradient norm: 0.36893154
INFO:root:[    9] Training loss: 0.02405359, Validation loss: 0.02359870, Gradient norm: 0.35250226
INFO:root:[   10] Training loss: 0.02291453, Validation loss: 0.02877937, Gradient norm: 0.33980213
INFO:root:[   11] Training loss: 0.02318850, Validation loss: 0.03236080, Gradient norm: 0.37143556
INFO:root:[   12] Training loss: 0.02174844, Validation loss: 0.02181883, Gradient norm: 0.28810642
INFO:root:[   13] Training loss: 0.02106961, Validation loss: 0.03212805, Gradient norm: 0.31726030
INFO:root:[   14] Training loss: 0.02146609, Validation loss: 0.02137064, Gradient norm: 0.30457682
INFO:root:[   15] Training loss: 0.02074133, Validation loss: 0.02314706, Gradient norm: 0.32937248
INFO:root:[   16] Training loss: 0.01927791, Validation loss: 0.02516193, Gradient norm: 0.27690776
INFO:root:[   17] Training loss: 0.01999579, Validation loss: 0.02181768, Gradient norm: 0.31494338
INFO:root:[   18] Training loss: 0.02003823, Validation loss: 0.02534999, Gradient norm: 0.33614566
INFO:root:[   19] Training loss: 0.02005082, Validation loss: 0.02449286, Gradient norm: 0.35408994
INFO:root:[   20] Training loss: 0.01807372, Validation loss: 0.02220405, Gradient norm: 0.26194634
INFO:root:[   21] Training loss: 0.01891963, Validation loss: 0.02566373, Gradient norm: 0.28889870
INFO:root:[   22] Training loss: 0.01848507, Validation loss: 0.02454979, Gradient norm: 0.28968035
INFO:root:[   23] Training loss: 0.01786420, Validation loss: 0.03083900, Gradient norm: 0.28037057
INFO:root:[   24] Training loss: 0.01852495, Validation loss: 0.02481047, Gradient norm: 0.31384975
INFO:root:[   25] Training loss: 0.01853958, Validation loss: 0.02524595, Gradient norm: 0.33187851
INFO:root:[   26] Training loss: 0.01746936, Validation loss: 0.02731946, Gradient norm: 0.27270956
INFO:root:[   27] Training loss: 0.01868151, Validation loss: 0.02377460, Gradient norm: 0.33854672
INFO:root:[   28] Training loss: 0.01739248, Validation loss: 0.02461802, Gradient norm: 0.25899658
INFO:root:[   29] Training loss: 0.01764908, Validation loss: 0.03174701, Gradient norm: 0.27436989
INFO:root:[   30] Training loss: 0.01650958, Validation loss: 0.02818300, Gradient norm: 0.22553971
INFO:root:[   31] Training loss: 0.01700368, Validation loss: 0.02925350, Gradient norm: 0.26235061
INFO:root:[   32] Training loss: 0.01679643, Validation loss: 0.03264608, Gradient norm: 0.26558866
INFO:root:[   33] Training loss: 0.01811088, Validation loss: 0.02737636, Gradient norm: 0.30319530
INFO:root:[   34] Training loss: 0.01584696, Validation loss: 0.02565079, Gradient norm: 0.19625141
INFO:root:[   35] Training loss: 0.01756649, Validation loss: 0.02429970, Gradient norm: 0.27497410
INFO:root:[   36] Training loss: 0.01650101, Validation loss: 0.02490835, Gradient norm: 0.23766016
INFO:root:[   37] Training loss: 0.01680166, Validation loss: 0.02763372, Gradient norm: 0.27134301
INFO:root:[   38] Training loss: 0.01714655, Validation loss: 0.02468351, Gradient norm: 0.26339535
INFO:root:[   39] Training loss: 0.01741087, Validation loss: 0.02623103, Gradient norm: 0.29846969
INFO:root:[   40] Training loss: 0.01654898, Validation loss: 0.02658906, Gradient norm: 0.27148784
INFO:root:[   41] Training loss: 0.01661294, Validation loss: 0.02617494, Gradient norm: 0.27971171
INFO:root:[   42] Training loss: 0.01912065, Validation loss: 0.04453973, Gradient norm: 0.32017047
INFO:root:[   43] Training loss: 0.02634826, Validation loss: 0.02448450, Gradient norm: 0.44420600
INFO:root:[   44] Training loss: 0.01648544, Validation loss: 0.02646857, Gradient norm: 0.24831276
INFO:root:[   45] Training loss: 0.01713420, Validation loss: 0.02380193, Gradient norm: 0.32417693
INFO:root:[   46] Training loss: 0.01589317, Validation loss: 0.02694373, Gradient norm: 0.27839487
INFO:root:[   47] Training loss: 0.01526598, Validation loss: 0.02706386, Gradient norm: 0.22791132
INFO:root:[   48] Training loss: 0.01630429, Validation loss: 0.02755584, Gradient norm: 0.29527475
INFO:root:[   49] Training loss: 0.01568732, Validation loss: 0.02495803, Gradient norm: 0.24566460
INFO:root:[   50] Training loss: 0.01621989, Validation loss: 0.02628001, Gradient norm: 0.29377971
INFO:root:[   51] Training loss: 0.01548538, Validation loss: 0.02612787, Gradient norm: 0.24612392
INFO:root:[   52] Training loss: 0.01504917, Validation loss: 0.02521611, Gradient norm: 0.21226134
INFO:root:[   53] Training loss: 0.01554063, Validation loss: 0.03248608, Gradient norm: 0.23408747
INFO:root:[   54] Training loss: 0.01609013, Validation loss: 0.02580492, Gradient norm: 0.25561650
INFO:root:[   55] Training loss: 0.01621809, Validation loss: 0.02769106, Gradient norm: 0.28543429
INFO:root:[   56] Training loss: 0.01565407, Validation loss: 0.02507959, Gradient norm: 0.26673041
INFO:root:[   57] Training loss: 0.01509941, Validation loss: 0.02437949, Gradient norm: 0.22731661
INFO:root:[   58] Training loss: 0.01575790, Validation loss: 0.02685197, Gradient norm: 0.24301786
INFO:root:[   59] Training loss: 0.01560209, Validation loss: 0.02632429, Gradient norm: 0.23247222
INFO:root:[   60] Training loss: 0.01579108, Validation loss: 0.02682595, Gradient norm: 0.28066350
INFO:root:[   61] Training loss: 0.01531966, Validation loss: 0.02417147, Gradient norm: 0.24310562
INFO:root:[   62] Training loss: 0.01565048, Validation loss: 0.02572574, Gradient norm: 0.26634279
INFO:root:[   63] Training loss: 0.01520119, Validation loss: 0.02452057, Gradient norm: 0.22522561
INFO:root:[   64] Training loss: 0.01466456, Validation loss: 0.02897094, Gradient norm: 0.18699328
INFO:root:[   65] Training loss: 0.01489195, Validation loss: 0.02415266, Gradient norm: 0.20956384
INFO:root:[   66] Training loss: 0.01541292, Validation loss: 0.02629046, Gradient norm: 0.26339460
INFO:root:[   67] Training loss: 0.01507263, Validation loss: 0.02521069, Gradient norm: 0.21705415
INFO:root:[   68] Training loss: 0.01492588, Validation loss: 0.02944470, Gradient norm: 0.23263201
INFO:root:[   69] Training loss: 0.01471462, Validation loss: 0.02788724, Gradient norm: 0.19904183
INFO:root:[   70] Training loss: 0.01589400, Validation loss: 0.02637990, Gradient norm: 0.26801103
INFO:root:[   71] Training loss: 0.01531082, Validation loss: 0.02717470, Gradient norm: 0.22097909
INFO:root:[   72] Training loss: 0.01529547, Validation loss: 0.02787475, Gradient norm: 0.23934486
INFO:root:[   73] Training loss: 0.01539646, Validation loss: 0.02681827, Gradient norm: 0.24499803
INFO:root:[   74] Training loss: 0.01505389, Validation loss: 0.02641004, Gradient norm: 0.23032500
INFO:root:[   75] Training loss: 0.01551762, Validation loss: 0.02376773, Gradient norm: 0.26248002
INFO:root:[   76] Training loss: 0.01532430, Validation loss: 0.02472332, Gradient norm: 0.24375199
INFO:root:[   77] Training loss: 0.01471547, Validation loss: 0.03207207, Gradient norm: 0.18231893
INFO:root:[   78] Training loss: 0.01512660, Validation loss: 0.02824211, Gradient norm: 0.26164712
INFO:root:[   79] Training loss: 0.01488094, Validation loss: 0.02872208, Gradient norm: 0.22723607
INFO:root:[   80] Training loss: 0.01459070, Validation loss: 0.02470187, Gradient norm: 0.23335556
INFO:root:[   81] Training loss: 0.01512315, Validation loss: 0.02551965, Gradient norm: 0.26776966
INFO:root:[   82] Training loss: 0.01498821, Validation loss: 0.02671997, Gradient norm: 0.25166246
INFO:root:[   83] Training loss: 0.01457027, Validation loss: 0.02500135, Gradient norm: 0.22126935
INFO:root:[   84] Training loss: 0.01461308, Validation loss: 0.02636351, Gradient norm: 0.22313731
INFO:root:EP 84: Early stopping
INFO:root:Training the model took 2590.628s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02563
INFO:root:EnergyScoreTrain: 0.01879
INFO:root:CoverageTrain: 0.75767
INFO:root:IntervalWidthTrain: 0.07394
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0284
INFO:root:EnergyScoreValidation: 0.02091
INFO:root:CoverageValidation: 0.71949
INFO:root:IntervalWidthValidation: 0.07338
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02887
INFO:root:EnergyScoreTest: 0.02128
INFO:root:CoverageTest: 0.71091
INFO:root:IntervalWidthTest: 0.07326
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07014591, Validation loss: 0.03240527, Gradient norm: 0.75487439
INFO:root:[    2] Training loss: 0.03864140, Validation loss: 0.02841646, Gradient norm: 0.45163092
INFO:root:[    3] Training loss: 0.03404305, Validation loss: 0.02805104, Gradient norm: 0.40035020
INFO:root:[    4] Training loss: 0.03157026, Validation loss: 0.02793669, Gradient norm: 0.41453517
INFO:root:[    5] Training loss: 0.02924357, Validation loss: 0.02208978, Gradient norm: 0.39793855
INFO:root:[    6] Training loss: 0.02454771, Validation loss: 0.02397701, Gradient norm: 0.27546643
INFO:root:[    7] Training loss: 0.02720207, Validation loss: 0.02123115, Gradient norm: 0.40361461
INFO:root:[    8] Training loss: 0.02415547, Validation loss: 0.02413964, Gradient norm: 0.30882011
INFO:root:[    9] Training loss: 0.02345751, Validation loss: 0.02614891, Gradient norm: 0.28143712
INFO:root:[   10] Training loss: 0.02197309, Validation loss: 0.03316225, Gradient norm: 0.25950023
INFO:root:[   11] Training loss: 0.02401659, Validation loss: 0.02952924, Gradient norm: 0.36190489
INFO:root:[   12] Training loss: 0.02123543, Validation loss: 0.02465244, Gradient norm: 0.23475947
INFO:root:[   13] Training loss: 0.02311740, Validation loss: 0.02159975, Gradient norm: 0.34676378
INFO:root:[   14] Training loss: 0.02205307, Validation loss: 0.02325094, Gradient norm: 0.29739379
INFO:root:[   15] Training loss: 0.02178778, Validation loss: 0.02720379, Gradient norm: 0.30141658
INFO:root:[   16] Training loss: 0.02121240, Validation loss: 0.02614250, Gradient norm: 0.26987428
INFO:root:[   17] Training loss: 0.02162583, Validation loss: 0.02257908, Gradient norm: 0.31758131
INFO:root:[   18] Training loss: 0.02103830, Validation loss: 0.02210209, Gradient norm: 0.29714720
INFO:root:[   19] Training loss: 0.02035061, Validation loss: 0.02326553, Gradient norm: 0.27177044
INFO:root:[   20] Training loss: 0.02148526, Validation loss: 0.02594640, Gradient norm: 0.33406744
INFO:root:[   21] Training loss: 0.02050564, Validation loss: 0.02739253, Gradient norm: 0.27372274
INFO:root:[   22] Training loss: 0.02042793, Validation loss: 0.02482540, Gradient norm: 0.28936061
INFO:root:[   23] Training loss: 0.01968929, Validation loss: 0.02903643, Gradient norm: 0.26131116
INFO:root:[   24] Training loss: 0.01934130, Validation loss: 0.02491024, Gradient norm: 0.26209778
INFO:root:[   25] Training loss: 0.01910816, Validation loss: 0.02526080, Gradient norm: 0.26728603
INFO:root:[   26] Training loss: 0.01930696, Validation loss: 0.02776694, Gradient norm: 0.24764318
INFO:root:[   27] Training loss: 0.01866202, Validation loss: 0.03110605, Gradient norm: 0.23474895
INFO:root:[   28] Training loss: 0.01920314, Validation loss: 0.03415649, Gradient norm: 0.27757950
INFO:root:[   29] Training loss: 0.01847090, Validation loss: 0.04368310, Gradient norm: 0.25452310
INFO:root:[   30] Training loss: 0.01856485, Validation loss: 0.02753584, Gradient norm: 0.23178608
INFO:root:[   31] Training loss: 0.01877114, Validation loss: 0.02490942, Gradient norm: 0.25731420
INFO:root:[   32] Training loss: 0.01885266, Validation loss: 0.02899417, Gradient norm: 0.26395287
INFO:root:[   33] Training loss: 0.01925616, Validation loss: 0.03408145, Gradient norm: 0.31715686
INFO:root:[   34] Training loss: 0.01799302, Validation loss: 0.03254661, Gradient norm: 0.26670360
INFO:root:[   35] Training loss: 0.01826340, Validation loss: 0.02998860, Gradient norm: 0.26545994
INFO:root:[   36] Training loss: 0.01819572, Validation loss: 0.02442302, Gradient norm: 0.25904043
INFO:root:[   37] Training loss: 0.01811207, Validation loss: 0.02348983, Gradient norm: 0.25753453
INFO:root:[   38] Training loss: 0.01776966, Validation loss: 0.03732779, Gradient norm: 0.24282949
INFO:root:[   39] Training loss: 0.01832220, Validation loss: 0.02938636, Gradient norm: 0.26584708
INFO:root:[   40] Training loss: 0.01779701, Validation loss: 0.03087485, Gradient norm: 0.25385185
INFO:root:[   41] Training loss: 0.01811100, Validation loss: 0.02635100, Gradient norm: 0.28666598
INFO:root:[   42] Training loss: 0.01796881, Validation loss: 0.02616306, Gradient norm: 0.26216654
INFO:root:[   43] Training loss: 0.01727875, Validation loss: 0.03508682, Gradient norm: 0.22185564
INFO:root:[   44] Training loss: 0.01789422, Validation loss: 0.03155522, Gradient norm: 0.28012904
INFO:root:[   45] Training loss: 0.01715923, Validation loss: 0.02634670, Gradient norm: 0.23609827
INFO:root:[   46] Training loss: 0.01757161, Validation loss: 0.03053574, Gradient norm: 0.27239697
INFO:root:[   47] Training loss: 0.01681891, Validation loss: 0.02843952, Gradient norm: 0.23688039
INFO:root:[   48] Training loss: 0.01732024, Validation loss: 0.03210538, Gradient norm: 0.26208750
INFO:root:[   49] Training loss: 0.01668059, Validation loss: 0.03543550, Gradient norm: 0.23363925
INFO:root:[   50] Training loss: 0.01716966, Validation loss: 0.02913234, Gradient norm: 0.26267710
INFO:root:[   51] Training loss: 0.01699878, Validation loss: 0.03564131, Gradient norm: 0.24431539
INFO:root:[   52] Training loss: 0.01718373, Validation loss: 0.02995003, Gradient norm: 0.26815484
INFO:root:[   53] Training loss: 0.01712689, Validation loss: 0.03080145, Gradient norm: 0.24162913
INFO:root:[   54] Training loss: 0.01684737, Validation loss: 0.02735948, Gradient norm: 0.22666494
INFO:root:[   55] Training loss: 0.01684057, Validation loss: 0.03377317, Gradient norm: 0.24515196
INFO:root:[   56] Training loss: 0.01733654, Validation loss: 0.03449111, Gradient norm: 0.24831612
INFO:root:[   57] Training loss: 0.01654976, Validation loss: 0.03294855, Gradient norm: 0.23808339
INFO:root:[   58] Training loss: 0.01666459, Validation loss: 0.03144597, Gradient norm: 0.24889939
INFO:root:[   59] Training loss: 0.01688025, Validation loss: 0.03054166, Gradient norm: 0.23870995
INFO:root:[   60] Training loss: 0.01634207, Validation loss: 0.03043822, Gradient norm: 0.20056603
INFO:root:[   61] Training loss: 0.01609399, Validation loss: 0.03113042, Gradient norm: 0.20638589
INFO:root:[   62] Training loss: 0.01650909, Validation loss: 0.02876964, Gradient norm: 0.23151319
INFO:root:[   63] Training loss: 0.01644641, Validation loss: 0.03048491, Gradient norm: 0.24060659
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 1941.085s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02666
INFO:root:EnergyScoreTrain: 0.01958
INFO:root:CoverageTrain: 0.83742
INFO:root:IntervalWidthTrain: 0.09906
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02835
INFO:root:EnergyScoreValidation: 0.02075
INFO:root:CoverageValidation: 0.81643
INFO:root:IntervalWidthValidation: 0.09827
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02883
INFO:root:EnergyScoreTest: 0.02112
INFO:root:CoverageTest: 0.80987
INFO:root:IntervalWidthTest: 0.0981
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05675932, Validation loss: 0.03611434, Gradient norm: 0.53269466
INFO:root:[    2] Training loss: 0.03645662, Validation loss: 0.03062275, Gradient norm: 0.37462593
INFO:root:[    3] Training loss: 0.03184974, Validation loss: 0.02948622, Gradient norm: 0.33293531
INFO:root:[    4] Training loss: 0.03008122, Validation loss: 0.02264666, Gradient norm: 0.34618268
INFO:root:[    5] Training loss: 0.02845861, Validation loss: 0.02511165, Gradient norm: 0.32108032
INFO:root:[    6] Training loss: 0.02839022, Validation loss: 0.02592282, Gradient norm: 0.33691385
INFO:root:[    7] Training loss: 0.02587654, Validation loss: 0.02387848, Gradient norm: 0.25309452
INFO:root:[    8] Training loss: 0.02566613, Validation loss: 0.02535965, Gradient norm: 0.28130990
INFO:root:[    9] Training loss: 0.02504981, Validation loss: 0.02569657, Gradient norm: 0.25565658
INFO:root:[   10] Training loss: 0.02529931, Validation loss: 0.02503478, Gradient norm: 0.27383399
INFO:root:[   11] Training loss: 0.02520588, Validation loss: 0.02466101, Gradient norm: 0.29964849
INFO:root:[   12] Training loss: 0.02399247, Validation loss: 0.02483819, Gradient norm: 0.26241646
INFO:root:[   13] Training loss: 0.02456464, Validation loss: 0.03147229, Gradient norm: 0.30182462
INFO:root:[   14] Training loss: 0.02345859, Validation loss: 0.02368351, Gradient norm: 0.26027886
INFO:root:[   15] Training loss: 0.02314758, Validation loss: 0.02359796, Gradient norm: 0.26363942
INFO:root:[   16] Training loss: 0.02346095, Validation loss: 0.02840695, Gradient norm: 0.28588306
INFO:root:[   17] Training loss: 0.02277638, Validation loss: 0.02552621, Gradient norm: 0.27217286
INFO:root:[   18] Training loss: 0.02216917, Validation loss: 0.03028144, Gradient norm: 0.25006503
INFO:root:[   19] Training loss: 0.02187818, Validation loss: 0.02558430, Gradient norm: 0.24373802
INFO:root:[   20] Training loss: 0.02178888, Validation loss: 0.02675078, Gradient norm: 0.27893424
INFO:root:[   21] Training loss: 0.02151649, Validation loss: 0.02661666, Gradient norm: 0.25336621
INFO:root:[   22] Training loss: 0.02163360, Validation loss: 0.02483305, Gradient norm: 0.27656584
INFO:root:[   23] Training loss: 0.02138997, Validation loss: 0.03230820, Gradient norm: 0.28213994
INFO:root:[   24] Training loss: 0.02067976, Validation loss: 0.02708649, Gradient norm: 0.26633688
INFO:root:[   25] Training loss: 0.02030866, Validation loss: 0.02628127, Gradient norm: 0.24706539
INFO:root:[   26] Training loss: 0.02066200, Validation loss: 0.02869753, Gradient norm: 0.26354237
INFO:root:[   27] Training loss: 0.02042585, Validation loss: 0.02732187, Gradient norm: 0.26031119
INFO:root:[   28] Training loss: 0.02017734, Validation loss: 0.02533048, Gradient norm: 0.26799112
INFO:root:[   29] Training loss: 0.02052724, Validation loss: 0.02871082, Gradient norm: 0.27973081
INFO:root:[   30] Training loss: 0.02024610, Validation loss: 0.02835392, Gradient norm: 0.25483065
INFO:root:[   31] Training loss: 0.01931996, Validation loss: 0.02853749, Gradient norm: 0.22655152
INFO:root:[   32] Training loss: 0.01948440, Validation loss: 0.03112343, Gradient norm: 0.23638455
INFO:root:[   33] Training loss: 0.01927038, Validation loss: 0.02616698, Gradient norm: 0.21600972
INFO:root:[   34] Training loss: 0.01973478, Validation loss: 0.02729577, Gradient norm: 0.24174540
INFO:root:[   35] Training loss: 0.01887882, Validation loss: 0.03073154, Gradient norm: 0.23127849
INFO:root:[   36] Training loss: 0.01981176, Validation loss: 0.02618970, Gradient norm: 0.26928055
INFO:root:[   37] Training loss: 0.01994192, Validation loss: 0.02530542, Gradient norm: 0.27939692
INFO:root:[   38] Training loss: 0.01990774, Validation loss: 0.03247433, Gradient norm: 0.26626232
INFO:root:[   39] Training loss: 0.01899238, Validation loss: 0.02654437, Gradient norm: 0.24392821
INFO:root:[   40] Training loss: 0.01904116, Validation loss: 0.03078378, Gradient norm: 0.22911361
INFO:root:[   41] Training loss: 0.01917382, Validation loss: 0.02587391, Gradient norm: 0.23512888
INFO:root:[   42] Training loss: 0.01915004, Validation loss: 0.02994586, Gradient norm: 0.28048826
INFO:root:[   43] Training loss: 0.01888339, Validation loss: 0.03083777, Gradient norm: 0.24644920
INFO:root:[   44] Training loss: 0.01889310, Validation loss: 0.02996093, Gradient norm: 0.25464997
INFO:root:[   45] Training loss: 0.01841786, Validation loss: 0.03114355, Gradient norm: 0.21539559
INFO:root:[   46] Training loss: 0.01940860, Validation loss: 0.03492492, Gradient norm: 0.27711995
INFO:root:[   47] Training loss: 0.01869360, Validation loss: 0.02844627, Gradient norm: 0.22432206
INFO:root:[   48] Training loss: 0.01863451, Validation loss: 0.02782327, Gradient norm: 0.21306989
INFO:root:[   49] Training loss: 0.01782895, Validation loss: 0.03161984, Gradient norm: 0.19063844
INFO:root:[   50] Training loss: 0.01863970, Validation loss: 0.02576988, Gradient norm: 0.22824252
INFO:root:[   51] Training loss: 0.01836370, Validation loss: 0.03094348, Gradient norm: 0.23486435
INFO:root:[   52] Training loss: 0.01844246, Validation loss: 0.02523460, Gradient norm: 0.22676322
INFO:root:[   53] Training loss: 0.01859149, Validation loss: 0.03296455, Gradient norm: 0.23681331
INFO:root:[   54] Training loss: 0.01870562, Validation loss: 0.02982361, Gradient norm: 0.25326841
INFO:root:[   55] Training loss: 0.01894257, Validation loss: 0.02935807, Gradient norm: 0.26938271
INFO:root:[   56] Training loss: 0.01851808, Validation loss: 0.02675071, Gradient norm: 0.26089990
INFO:root:[   57] Training loss: 0.01803142, Validation loss: 0.03013218, Gradient norm: 0.23175346
INFO:root:[   58] Training loss: 0.01804575, Validation loss: 0.02670264, Gradient norm: 0.23248394
INFO:root:[   59] Training loss: 0.01827089, Validation loss: 0.02918414, Gradient norm: 0.24615019
INFO:root:[   60] Training loss: 0.01765066, Validation loss: 0.02830148, Gradient norm: 0.19102591
INFO:root:[   61] Training loss: 0.01840829, Validation loss: 0.02851960, Gradient norm: 0.25276586
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1888.215s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02967
INFO:root:EnergyScoreTrain: 0.0218
INFO:root:CoverageTrain: 0.82655
INFO:root:IntervalWidthTrain: 0.09346
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03111
INFO:root:EnergyScoreValidation: 0.02289
INFO:root:CoverageValidation: 0.81006
INFO:root:IntervalWidthValidation: 0.09435
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03169
INFO:root:EnergyScoreTest: 0.02335
INFO:root:CoverageTest: 0.80426
INFO:root:IntervalWidthTest: 0.09397
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5816002
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05939538, Validation loss: 0.03711919, Gradient norm: 0.49740146
INFO:root:[    2] Training loss: 0.03944759, Validation loss: 0.03817833, Gradient norm: 0.39449241
INFO:root:[    3] Training loss: 0.03614772, Validation loss: 0.03854743, Gradient norm: 0.39022269
INFO:root:[    4] Training loss: 0.03275935, Validation loss: 0.03377119, Gradient norm: 0.35816789
INFO:root:[    5] Training loss: 0.03086616, Validation loss: 0.05141974, Gradient norm: 0.30885057
INFO:root:[    6] Training loss: 0.03134773, Validation loss: 0.04182287, Gradient norm: 0.32426098
INFO:root:[    7] Training loss: 0.02938704, Validation loss: 0.04735539, Gradient norm: 0.28888690
INFO:root:[    8] Training loss: 0.02888560, Validation loss: 0.03582450, Gradient norm: 0.28643175
INFO:root:[    9] Training loss: 0.02820273, Validation loss: 0.04720110, Gradient norm: 0.25859471
INFO:root:[   10] Training loss: 0.02717728, Validation loss: 0.03343599, Gradient norm: 0.24200074
INFO:root:[   11] Training loss: 0.02941408, Validation loss: 0.03078563, Gradient norm: 0.36820640
INFO:root:[   12] Training loss: 0.02701428, Validation loss: 0.03642568, Gradient norm: 0.24470455
INFO:root:[   13] Training loss: 0.02755925, Validation loss: 0.03049394, Gradient norm: 0.31299467
INFO:root:[   14] Training loss: 0.02646325, Validation loss: 0.04787785, Gradient norm: 0.28777149
INFO:root:[   15] Training loss: 0.02666792, Validation loss: 0.05566426, Gradient norm: 0.30304215
INFO:root:[   16] Training loss: 0.02503341, Validation loss: 0.03123117, Gradient norm: 0.24805661
INFO:root:[   17] Training loss: 0.02465263, Validation loss: 0.03436968, Gradient norm: 0.25436998
INFO:root:[   18] Training loss: 0.02442813, Validation loss: 0.04267645, Gradient norm: 0.25527033
INFO:root:[   19] Training loss: 0.02504647, Validation loss: 0.04724931, Gradient norm: 0.28526648
INFO:root:[   20] Training loss: 0.02541157, Validation loss: 0.05237373, Gradient norm: 0.29563422
INFO:root:[   21] Training loss: 0.02359865, Validation loss: 0.03590432, Gradient norm: 0.25442663
INFO:root:[   22] Training loss: 0.02359673, Validation loss: 0.03703320, Gradient norm: 0.24052765
INFO:root:[   23] Training loss: 0.02423926, Validation loss: 0.03651711, Gradient norm: 0.28108403
INFO:root:[   24] Training loss: 0.02353865, Validation loss: 0.04986224, Gradient norm: 0.22617187
INFO:root:[   25] Training loss: 0.02398883, Validation loss: 0.03811161, Gradient norm: 0.29883476
INFO:root:[   26] Training loss: 0.02296951, Validation loss: 0.03328689, Gradient norm: 0.24036854
INFO:root:[   27] Training loss: 0.02275045, Validation loss: 0.03873325, Gradient norm: 0.23685836
INFO:root:[   28] Training loss: 0.02256220, Validation loss: 0.03281469, Gradient norm: 0.21981863
INFO:root:[   29] Training loss: 0.02250486, Validation loss: 0.03513947, Gradient norm: 0.22535829
INFO:root:[   30] Training loss: 0.02245058, Validation loss: 0.04555979, Gradient norm: 0.24326178
INFO:root:[   31] Training loss: 0.02305196, Validation loss: 0.03546332, Gradient norm: 0.27727833
INFO:root:[   32] Training loss: 0.02181721, Validation loss: 0.04067056, Gradient norm: 0.23296091
INFO:root:[   33] Training loss: 0.02185297, Validation loss: 0.04571791, Gradient norm: 0.22711306
INFO:root:[   34] Training loss: 0.02208619, Validation loss: 0.05021863, Gradient norm: 0.22084661
INFO:root:[   35] Training loss: 0.02172519, Validation loss: 0.03916579, Gradient norm: 0.21566249
INFO:root:[   36] Training loss: 0.02120741, Validation loss: 0.03923237, Gradient norm: 0.16397167
INFO:root:[   37] Training loss: 0.02213068, Validation loss: 0.03900010, Gradient norm: 0.23807536
INFO:root:[   38] Training loss: 0.02186516, Validation loss: 0.04859711, Gradient norm: 0.24547969
INFO:root:[   39] Training loss: 0.02192288, Validation loss: 0.04453377, Gradient norm: 0.23806400
INFO:root:[   40] Training loss: 0.02245366, Validation loss: 0.05106666, Gradient norm: 0.25433589
INFO:root:[   41] Training loss: 0.02092829, Validation loss: 0.04081847, Gradient norm: 0.18989831
INFO:root:[   42] Training loss: 0.02203657, Validation loss: 0.03892721, Gradient norm: 0.24997791
INFO:root:[   43] Training loss: 0.02179658, Validation loss: 0.03722872, Gradient norm: 0.24479795
INFO:root:[   44] Training loss: 0.02134160, Validation loss: 0.04960696, Gradient norm: 0.23322623
INFO:root:[   45] Training loss: 0.02091206, Validation loss: 0.04448659, Gradient norm: 0.19144963
INFO:root:[   46] Training loss: 0.02090986, Validation loss: 0.04818822, Gradient norm: 0.21165815
INFO:root:[   47] Training loss: 0.02115968, Validation loss: 0.05680874, Gradient norm: 0.20705571
INFO:root:[   48] Training loss: 0.02070600, Validation loss: 0.04389205, Gradient norm: 0.19868472
INFO:root:[   49] Training loss: 0.02096256, Validation loss: 0.03518022, Gradient norm: 0.21841413
INFO:root:[   50] Training loss: 0.02101621, Validation loss: 0.03976495, Gradient norm: 0.23036347
INFO:root:[   51] Training loss: 0.02089004, Validation loss: 0.05633986, Gradient norm: 0.20010461
INFO:root:[   52] Training loss: 0.02113094, Validation loss: 0.03853704, Gradient norm: 0.23007365
INFO:root:[   53] Training loss: 0.02071168, Validation loss: 0.04264108, Gradient norm: 0.20808773
INFO:root:[   54] Training loss: 0.02124840, Validation loss: 0.05723347, Gradient norm: 0.25317370
INFO:root:[   55] Training loss: 0.02152109, Validation loss: 0.04460243, Gradient norm: 0.26311059
INFO:root:[   56] Training loss: 0.02093184, Validation loss: 0.05938669, Gradient norm: 0.21883573
INFO:root:[   57] Training loss: 0.02100768, Validation loss: 0.05622103, Gradient norm: 0.23082344
INFO:root:[   58] Training loss: 0.02047213, Validation loss: 0.04857838, Gradient norm: 0.20031421
INFO:root:[   59] Training loss: 0.01978538, Validation loss: 0.04627187, Gradient norm: 0.13802962
INFO:root:[   60] Training loss: 0.02117631, Validation loss: 0.05598007, Gradient norm: 0.24232740
INFO:root:[   61] Training loss: 0.02044860, Validation loss: 0.05152593, Gradient norm: 0.23025827
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1900.028s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04122
INFO:root:EnergyScoreTrain: 0.03432
INFO:root:CoverageTrain: 0.31559
INFO:root:IntervalWidthTrain: 0.0433
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03727
INFO:root:EnergyScoreValidation: 0.03049
INFO:root:CoverageValidation: 0.37302
INFO:root:IntervalWidthValidation: 0.04514
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03734
INFO:root:EnergyScoreTest: 0.03054
INFO:root:CoverageTest: 0.37488
INFO:root:IntervalWidthTest: 0.04511
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05745274, Validation loss: 0.04674978, Gradient norm: 0.74449415
INFO:root:[    2] Training loss: 0.04131980, Validation loss: 0.02961600, Gradient norm: 0.69961571
INFO:root:[    3] Training loss: 0.03348445, Validation loss: 0.02659603, Gradient norm: 0.55903999
INFO:root:[    4] Training loss: 0.03024845, Validation loss: 0.02807182, Gradient norm: 0.55626707
INFO:root:[    5] Training loss: 0.02980034, Validation loss: 0.03000843, Gradient norm: 0.58832941
INFO:root:[    6] Training loss: 0.02827184, Validation loss: 0.03097362, Gradient norm: 0.53904523
INFO:root:[    7] Training loss: 0.02892350, Validation loss: 0.03114536, Gradient norm: 0.59178745
INFO:root:[    8] Training loss: 0.02591621, Validation loss: 0.03802260, Gradient norm: 0.56088468
INFO:root:[    9] Training loss: 0.02529649, Validation loss: 0.02628359, Gradient norm: 0.50716730
INFO:root:[   10] Training loss: 0.02390306, Validation loss: 0.02482162, Gradient norm: 0.49750695
INFO:root:[   11] Training loss: 0.02368001, Validation loss: 0.03299755, Gradient norm: 0.50918460
INFO:root:[   12] Training loss: 0.02226614, Validation loss: 0.02477562, Gradient norm: 0.43645639
INFO:root:[   13] Training loss: 0.02437194, Validation loss: 0.03012842, Gradient norm: 0.54668991
INFO:root:[   14] Training loss: 0.02420505, Validation loss: 0.03680220, Gradient norm: 0.57732944
INFO:root:[   15] Training loss: 0.02251621, Validation loss: 0.02668840, Gradient norm: 0.52271222
INFO:root:[   16] Training loss: 0.02266881, Validation loss: 0.02955095, Gradient norm: 0.50545179
INFO:root:[   17] Training loss: 0.02180410, Validation loss: 0.02729072, Gradient norm: 0.51944089
INFO:root:[   18] Training loss: 0.02110922, Validation loss: 0.03159843, Gradient norm: 0.47794525
INFO:root:[   19] Training loss: 0.01997652, Validation loss: 0.02872039, Gradient norm: 0.40958973
INFO:root:[   20] Training loss: 0.02190509, Validation loss: 0.02854497, Gradient norm: 0.51594359
INFO:root:[   21] Training loss: 0.02116805, Validation loss: 0.03264296, Gradient norm: 0.50155200
INFO:root:[   22] Training loss: 0.02069322, Validation loss: 0.03573721, Gradient norm: 0.46938290
INFO:root:[   23] Training loss: 0.01956156, Validation loss: 0.02761679, Gradient norm: 0.43939373
INFO:root:[   24] Training loss: 0.01930361, Validation loss: 0.03398550, Gradient norm: 0.43936978
INFO:root:[   25] Training loss: 0.02075210, Validation loss: 0.03743039, Gradient norm: 0.51522535
INFO:root:[   26] Training loss: 0.01982358, Validation loss: 0.02936175, Gradient norm: 0.50163816
INFO:root:[   27] Training loss: 0.01893002, Validation loss: 0.03175425, Gradient norm: 0.47607480
INFO:root:[   28] Training loss: 0.01843653, Validation loss: 0.02807856, Gradient norm: 0.44060991
INFO:root:[   29] Training loss: 0.01945454, Validation loss: 0.03552693, Gradient norm: 0.50251270
INFO:root:[   30] Training loss: 0.01909803, Validation loss: 0.03659811, Gradient norm: 0.46236783
INFO:root:[   31] Training loss: 0.01922533, Validation loss: 0.03459712, Gradient norm: 0.47719504
INFO:root:[   32] Training loss: 0.01922722, Validation loss: 0.03093757, Gradient norm: 0.49751317
INFO:root:[   33] Training loss: 0.01881823, Validation loss: 0.02891318, Gradient norm: 0.46810070
INFO:root:[   34] Training loss: 0.01779652, Validation loss: 0.02827589, Gradient norm: 0.44729151
INFO:root:[   35] Training loss: 0.01758666, Validation loss: 0.03544564, Gradient norm: 0.44745725
INFO:root:[   36] Training loss: 0.01725710, Validation loss: 0.03166854, Gradient norm: 0.40362415
INFO:root:[   37] Training loss: 0.01757386, Validation loss: 0.03607739, Gradient norm: 0.43722580
INFO:root:[   38] Training loss: 0.01765176, Validation loss: 0.03383565, Gradient norm: 0.48379468
INFO:root:[   39] Training loss: 0.01753581, Validation loss: 0.03025916, Gradient norm: 0.43278654
INFO:root:[   40] Training loss: 0.01653542, Validation loss: 0.03016210, Gradient norm: 0.43315997
INFO:root:[   41] Training loss: 0.01731093, Validation loss: 0.03524817, Gradient norm: 0.43706634
INFO:root:[   42] Training loss: 0.01704605, Validation loss: 0.03212691, Gradient norm: 0.45872960
INFO:root:[   43] Training loss: 0.06479474, Validation loss: 0.11373860, Gradient norm: 1.40582296
INFO:root:[   44] Training loss: 0.08373196, Validation loss: 0.04195456, Gradient norm: 2.83520556
INFO:root:[   45] Training loss: 0.04027351, Validation loss: 0.03331495, Gradient norm: 2.59523554
INFO:root:[   46] Training loss: 0.03475322, Validation loss: 0.03339537, Gradient norm: 1.97275050
INFO:root:[   47] Training loss: 0.03163995, Validation loss: 0.03618622, Gradient norm: 1.46030783
INFO:root:[   48] Training loss: 0.02953489, Validation loss: 0.04032698, Gradient norm: 1.44919676
INFO:root:[   49] Training loss: 0.02720061, Validation loss: 0.03372703, Gradient norm: 1.18696900
INFO:root:[   50] Training loss: 0.02639675, Validation loss: 0.03397125, Gradient norm: 1.18225278
INFO:root:[   51] Training loss: 0.02212110, Validation loss: 0.03592484, Gradient norm: 0.82176063
INFO:root:[   52] Training loss: 0.02176156, Validation loss: 0.03078551, Gradient norm: 0.90439715
INFO:root:[   53] Training loss: 0.02146012, Validation loss: 0.02857399, Gradient norm: 0.83034240
INFO:root:[   54] Training loss: 0.02072339, Validation loss: 0.03501377, Gradient norm: 0.76640965
INFO:root:[   55] Training loss: 0.02028059, Validation loss: 0.03256106, Gradient norm: 0.74216498
INFO:root:[   56] Training loss: 0.01976896, Validation loss: 0.03864755, Gradient norm: 0.71248419
INFO:root:[   57] Training loss: 0.01873769, Validation loss: 0.03410265, Gradient norm: 0.58720457
INFO:root:[   58] Training loss: 0.01772357, Validation loss: 0.03128076, Gradient norm: 0.53589249
INFO:root:[   59] Training loss: 0.01786334, Validation loss: 0.03445156, Gradient norm: 0.56105621
INFO:root:[   60] Training loss: 0.01757742, Validation loss: 0.03116478, Gradient norm: 0.50856890
INFO:root:[   61] Training loss: 0.01754546, Validation loss: 0.03040451, Gradient norm: 0.57519955
INFO:root:[   62] Training loss: 0.01774417, Validation loss: 0.03373150, Gradient norm: 0.57018689
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1886.086s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0226
INFO:root:EnergyScoreTrain: 0.02163
INFO:root:CoverageTrain: 0.06777
INFO:root:IntervalWidthTrain: 0.00229
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0243
INFO:root:EnergyScoreValidation: 0.02347
INFO:root:CoverageValidation: 0.04837
INFO:root:IntervalWidthValidation: 0.00198
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02444
INFO:root:EnergyScoreTest: 0.02359
INFO:root:CoverageTest: 0.04989
INFO:root:IntervalWidthTest: 0.00201
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 473956352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05996172, Validation loss: 0.03506593, Gradient norm: 0.58647938
INFO:root:[    2] Training loss: 0.04163108, Validation loss: 0.04201940, Gradient norm: 0.49987849
INFO:root:[    3] Training loss: 0.03761270, Validation loss: 0.02793498, Gradient norm: 0.46597414
INFO:root:[    4] Training loss: 0.03511842, Validation loss: 0.02773557, Gradient norm: 0.49244876
INFO:root:[    5] Training loss: 0.03301853, Validation loss: 0.03089066, Gradient norm: 0.49326892
INFO:root:[    6] Training loss: 0.03056905, Validation loss: 0.02857618, Gradient norm: 0.42091510
INFO:root:[    7] Training loss: 0.03158739, Validation loss: 0.03642912, Gradient norm: 0.46766084
INFO:root:[    8] Training loss: 0.02859084, Validation loss: 0.03034510, Gradient norm: 0.39210733
INFO:root:[    9] Training loss: 0.02866089, Validation loss: 0.03483378, Gradient norm: 0.44605731
INFO:root:[   10] Training loss: 0.02768042, Validation loss: 0.03484951, Gradient norm: 0.43578208
INFO:root:[   11] Training loss: 0.02675874, Validation loss: 0.02599731, Gradient norm: 0.41938059
INFO:root:[   12] Training loss: 0.02512114, Validation loss: 0.03817857, Gradient norm: 0.36058154
INFO:root:[   13] Training loss: 0.02638732, Validation loss: 0.02962328, Gradient norm: 0.44683693
INFO:root:[   14] Training loss: 0.02604946, Validation loss: 0.02677292, Gradient norm: 0.45068681
INFO:root:[   15] Training loss: 0.02494161, Validation loss: 0.02648603, Gradient norm: 0.42127609
INFO:root:[   16] Training loss: 0.02567219, Validation loss: 0.03282464, Gradient norm: 0.43570848
INFO:root:[   17] Training loss: 0.02505276, Validation loss: 0.03493300, Gradient norm: 0.45971502
INFO:root:[   18] Training loss: 0.02381836, Validation loss: 0.03237720, Gradient norm: 0.41406150
INFO:root:[   19] Training loss: 0.02308931, Validation loss: 0.02815541, Gradient norm: 0.41103799
INFO:root:[   20] Training loss: 0.02343740, Validation loss: 0.03976569, Gradient norm: 0.38430454
INFO:root:[   21] Training loss: 0.02410746, Validation loss: 0.03891352, Gradient norm: 0.45312171
INFO:root:[   22] Training loss: 0.02156965, Validation loss: 0.03354011, Gradient norm: 0.36469119
INFO:root:[   23] Training loss: 0.02320027, Validation loss: 0.03322405, Gradient norm: 0.46102870
INFO:root:[   24] Training loss: 0.02291020, Validation loss: 0.04132958, Gradient norm: 0.44487332
INFO:root:[   25] Training loss: 0.02218529, Validation loss: 0.03232652, Gradient norm: 0.39264844
INFO:root:[   26] Training loss: 0.02092545, Validation loss: 0.03724955, Gradient norm: 0.37893494
INFO:root:[   27] Training loss: 0.02140812, Validation loss: 0.03267184, Gradient norm: 0.38076781
INFO:root:[   28] Training loss: 0.02146002, Validation loss: 0.03224756, Gradient norm: 0.38531671
INFO:root:[   29] Training loss: 0.02079507, Validation loss: 0.02752724, Gradient norm: 0.36043609
INFO:root:[   30] Training loss: 0.02116304, Validation loss: 0.03341952, Gradient norm: 0.42372330
INFO:root:[   31] Training loss: 0.02124307, Validation loss: 0.03737400, Gradient norm: 0.42782298
INFO:root:[   32] Training loss: 0.02019704, Validation loss: 0.03121122, Gradient norm: 0.37161591
INFO:root:[   33] Training loss: 0.02094109, Validation loss: 0.03402030, Gradient norm: 0.40838770
INFO:root:[   34] Training loss: 0.02164358, Validation loss: 0.03275327, Gradient norm: 0.45652534
INFO:root:[   35] Training loss: 0.02057286, Validation loss: 0.03644423, Gradient norm: 0.39499615
INFO:root:[   36] Training loss: 0.02153621, Validation loss: 0.03650302, Gradient norm: 0.42125232
INFO:root:[   37] Training loss: 0.01996002, Validation loss: 0.03439956, Gradient norm: 0.38182464
INFO:root:[   38] Training loss: 0.02065351, Validation loss: 0.03248459, Gradient norm: 0.46132725
INFO:root:[   39] Training loss: 0.01980221, Validation loss: 0.03619187, Gradient norm: 0.38009072
INFO:root:[   40] Training loss: 0.01955324, Validation loss: 0.02699141, Gradient norm: 0.35399216
INFO:root:[   41] Training loss: 0.02055401, Validation loss: 0.03134150, Gradient norm: 0.41788511
INFO:root:[   42] Training loss: 0.01910855, Validation loss: 0.02815065, Gradient norm: 0.36533827
INFO:root:[   43] Training loss: 0.02011680, Validation loss: 0.02975248, Gradient norm: 0.41918468
INFO:root:[   44] Training loss: 0.01924318, Validation loss: 0.02923096, Gradient norm: 0.37479040
INFO:root:[   45] Training loss: 0.02010523, Validation loss: 0.03690251, Gradient norm: 0.44554207
INFO:root:[   46] Training loss: 0.01952735, Validation loss: 0.03103516, Gradient norm: 0.39404882
INFO:root:[   47] Training loss: 0.01931202, Validation loss: 0.03327828, Gradient norm: 0.38074895
INFO:root:[   48] Training loss: 0.01940781, Validation loss: 0.03194457, Gradient norm: 0.39061005
INFO:root:[   49] Training loss: 0.01923606, Validation loss: 0.03201604, Gradient norm: 0.41019703
INFO:root:[   50] Training loss: 0.01850767, Validation loss: 0.03345813, Gradient norm: 0.36100861
INFO:root:[   51] Training loss: 0.01963544, Validation loss: 0.03060024, Gradient norm: 0.44655690
INFO:root:[   52] Training loss: 0.01954610, Validation loss: 0.03272287, Gradient norm: 0.40593930
INFO:root:[   53] Training loss: 0.01857933, Validation loss: 0.03319325, Gradient norm: 0.36250810
INFO:root:[   54] Training loss: 0.01808908, Validation loss: 0.03325075, Gradient norm: 0.36604450
INFO:root:[   55] Training loss: 0.01859169, Validation loss: 0.03294807, Gradient norm: 0.38551764
INFO:root:[   56] Training loss: 0.01863705, Validation loss: 0.03464288, Gradient norm: 0.36885318
INFO:root:[   57] Training loss: 0.03046979, Validation loss: 0.03935684, Gradient norm: 0.60663021
INFO:root:[   58] Training loss: 0.02061601, Validation loss: 0.03210037, Gradient norm: 0.48657691
INFO:root:[   59] Training loss: 0.01821793, Validation loss: 0.03064869, Gradient norm: 0.35525922
INFO:root:[   60] Training loss: 0.01867428, Validation loss: 0.03132644, Gradient norm: 0.45120507
INFO:root:[   61] Training loss: 0.01952450, Validation loss: 0.03469610, Gradient norm: 0.47509105
INFO:root:[   62] Training loss: 0.01788162, Validation loss: 0.03618550, Gradient norm: 0.31787844
INFO:root:[   63] Training loss: 0.01803382, Validation loss: 0.03747952, Gradient norm: 0.35436522
INFO:root:[   64] Training loss: 0.01793607, Validation loss: 0.02864440, Gradient norm: 0.36792696
INFO:root:[   65] Training loss: 0.01860665, Validation loss: 0.03112338, Gradient norm: 0.40196720
INFO:root:[   66] Training loss: 0.01784051, Validation loss: 0.03163650, Gradient norm: 0.30381273
INFO:root:[   67] Training loss: 0.01867768, Validation loss: 0.03802706, Gradient norm: 0.40598063
INFO:root:[   68] Training loss: 0.01832802, Validation loss: 0.03633338, Gradient norm: 0.37509590
INFO:root:[   69] Training loss: 0.01913502, Validation loss: 0.03992063, Gradient norm: 0.38378546
INFO:root:[   70] Training loss: 0.01836359, Validation loss: 0.03303169, Gradient norm: 0.36188588
INFO:root:[   71] Training loss: 0.01874804, Validation loss: 0.03056358, Gradient norm: 0.43354574
INFO:root:[   72] Training loss: 0.01883044, Validation loss: 0.03937867, Gradient norm: 0.45156848
INFO:root:[   73] Training loss: 0.01842052, Validation loss: 0.03125082, Gradient norm: 0.36668059
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 2209.26s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02165
INFO:root:EnergyScoreTrain: 0.02073
INFO:root:CoverageTrain: 0.0539
INFO:root:IntervalWidthTrain: 0.00216
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02607
INFO:root:EnergyScoreValidation: 0.02525
INFO:root:CoverageValidation: 0.03983
INFO:root:IntervalWidthValidation: 0.00194
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02669
INFO:root:EnergyScoreTest: 0.0259
INFO:root:CoverageTest: 0.03477
INFO:root:IntervalWidthTest: 0.00181
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 406847488
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08595676, Validation loss: 0.04657588, Gradient norm: 1.02023304
INFO:root:[    2] Training loss: 0.05079587, Validation loss: 0.05367984, Gradient norm: 0.53830039
INFO:root:[    3] Training loss: 0.04821835, Validation loss: 0.03960187, Gradient norm: 0.63180069
INFO:root:[    4] Training loss: 0.04478699, Validation loss: 0.03436995, Gradient norm: 0.54649296
INFO:root:[    5] Training loss: 0.04041449, Validation loss: 0.03381984, Gradient norm: 0.55926096
INFO:root:[    6] Training loss: 0.03760202, Validation loss: 0.03472652, Gradient norm: 0.49292440
INFO:root:[    7] Training loss: 0.03466043, Validation loss: 0.04483748, Gradient norm: 0.48146997
INFO:root:[    8] Training loss: 0.03466098, Validation loss: 0.03382437, Gradient norm: 0.51380722
INFO:root:[    9] Training loss: 0.03259505, Validation loss: 0.04315544, Gradient norm: 0.48983597
INFO:root:[   10] Training loss: 0.03103069, Validation loss: 0.02747518, Gradient norm: 0.40863998
INFO:root:[   11] Training loss: 0.03038604, Validation loss: 0.03235467, Gradient norm: 0.45872326
INFO:root:[   12] Training loss: 0.02987997, Validation loss: 0.03094523, Gradient norm: 0.42950370
INFO:root:[   13] Training loss: 0.02751876, Validation loss: 0.03046106, Gradient norm: 0.36616103
INFO:root:[   14] Training loss: 0.02984933, Validation loss: 0.02974810, Gradient norm: 0.49616372
INFO:root:[   15] Training loss: 0.02799197, Validation loss: 0.03417995, Gradient norm: 0.45277835
INFO:root:[   16] Training loss: 0.02852329, Validation loss: 0.04194824, Gradient norm: 0.46949212
INFO:root:[   17] Training loss: 0.02610419, Validation loss: 0.02677472, Gradient norm: 0.34900721
INFO:root:[   18] Training loss: 0.02691734, Validation loss: 0.03646636, Gradient norm: 0.40286243
INFO:root:[   19] Training loss: 0.02622285, Validation loss: 0.03634286, Gradient norm: 0.42804346
INFO:root:[   20] Training loss: 0.02624283, Validation loss: 0.03211877, Gradient norm: 0.39805126
INFO:root:[   21] Training loss: 0.02566799, Validation loss: 0.03515823, Gradient norm: 0.39447419
INFO:root:[   22] Training loss: 0.02621476, Validation loss: 0.03444444, Gradient norm: 0.43809193
INFO:root:[   23] Training loss: 0.02397058, Validation loss: 0.03001031, Gradient norm: 0.35553107
INFO:root:[   24] Training loss: 0.02532601, Validation loss: 0.02704867, Gradient norm: 0.43651018
INFO:root:[   25] Training loss: 0.02459042, Validation loss: 0.03037023, Gradient norm: 0.43633921
INFO:root:[   26] Training loss: 0.02439513, Validation loss: 0.03355813, Gradient norm: 0.37996315
INFO:root:[   27] Training loss: 0.02363211, Validation loss: 0.03121300, Gradient norm: 0.36152650
INFO:root:[   28] Training loss: 0.02265384, Validation loss: 0.03772690, Gradient norm: 0.31687124
INFO:root:[   29] Training loss: 0.02411858, Validation loss: 0.03131178, Gradient norm: 0.37660300
INFO:root:[   30] Training loss: 0.02491535, Validation loss: 0.03261455, Gradient norm: 0.45398951
INFO:root:[   31] Training loss: 0.02303971, Validation loss: 0.03371248, Gradient norm: 0.37414874
INFO:root:[   32] Training loss: 0.02324536, Validation loss: 0.03289551, Gradient norm: 0.36356209
INFO:root:[   33] Training loss: 0.02293237, Validation loss: 0.03534833, Gradient norm: 0.37100858
INFO:root:[   34] Training loss: 0.02401754, Validation loss: 0.03225355, Gradient norm: 0.44952181
INFO:root:[   35] Training loss: 0.02198282, Validation loss: 0.03207719, Gradient norm: 0.34451402
INFO:root:[   36] Training loss: 0.02273979, Validation loss: 0.04314510, Gradient norm: 0.35469908
INFO:root:[   37] Training loss: 0.02280127, Validation loss: 0.03251587, Gradient norm: 0.41769705
INFO:root:[   38] Training loss: 0.02322069, Validation loss: 0.03624805, Gradient norm: 0.40037773
INFO:root:[   39] Training loss: 0.02172272, Validation loss: 0.03656395, Gradient norm: 0.34301924
INFO:root:[   40] Training loss: 0.02234965, Validation loss: 0.03776484, Gradient norm: 0.37206368
INFO:root:[   41] Training loss: 0.02180860, Validation loss: 0.03486071, Gradient norm: 0.37483736
INFO:root:[   42] Training loss: 0.02237053, Validation loss: 0.03229740, Gradient norm: 0.37034534
INFO:root:[   43] Training loss: 0.02197353, Validation loss: 0.03640688, Gradient norm: 0.37903306
INFO:root:[   44] Training loss: 0.02215848, Validation loss: 0.03032356, Gradient norm: 0.38184114
INFO:root:[   45] Training loss: 0.02172444, Validation loss: 0.03179838, Gradient norm: 0.34689124
INFO:root:[   46] Training loss: 0.02126352, Validation loss: 0.03465119, Gradient norm: 0.37073956
INFO:root:[   47] Training loss: 0.02254230, Validation loss: 0.03309351, Gradient norm: 0.37667681
INFO:root:[   48] Training loss: 0.02048721, Validation loss: 0.03597166, Gradient norm: 0.27295628
INFO:root:[   49] Training loss: 0.02137476, Validation loss: 0.03619422, Gradient norm: 0.34786933
INFO:root:[   50] Training loss: 0.02178115, Validation loss: 0.03379499, Gradient norm: 0.40164368
INFO:root:[   51] Training loss: 0.02329794, Validation loss: 0.03110991, Gradient norm: 0.37660139
INFO:root:[   52] Training loss: 0.02139266, Validation loss: 0.03773743, Gradient norm: 0.38583661
INFO:root:[   53] Training loss: 0.02079699, Validation loss: 0.03285611, Gradient norm: 0.31600095
INFO:root:[   54] Training loss: 0.02145503, Validation loss: 0.03703993, Gradient norm: 0.34499517
INFO:root:[   55] Training loss: 0.02200623, Validation loss: 0.03260267, Gradient norm: 0.41431332
INFO:root:[   56] Training loss: 0.02085183, Validation loss: 0.03451481, Gradient norm: 0.34576512
INFO:root:[   57] Training loss: 0.02163402, Validation loss: 0.03433234, Gradient norm: 0.39325075
INFO:root:[   58] Training loss: 0.02148589, Validation loss: 0.03429613, Gradient norm: 0.39644019
INFO:root:[   59] Training loss: 0.02035745, Validation loss: 0.03693248, Gradient norm: 0.31369521
INFO:root:[   60] Training loss: 0.02082838, Validation loss: 0.03231282, Gradient norm: 0.35983100
INFO:root:[   61] Training loss: 0.02105958, Validation loss: 0.03389819, Gradient norm: 0.38163559
INFO:root:[   62] Training loss: 0.02095704, Validation loss: 0.03639606, Gradient norm: 0.36696090
INFO:root:[   63] Training loss: 0.02074977, Validation loss: 0.03685975, Gradient norm: 0.34510339
INFO:root:[   64] Training loss: 0.02072149, Validation loss: 0.03691235, Gradient norm: 0.37637121
INFO:root:[   65] Training loss: 0.02017492, Validation loss: 0.03665294, Gradient norm: 0.30169925
INFO:root:[   66] Training loss: 0.02083130, Validation loss: 0.03465678, Gradient norm: 0.37039007
INFO:root:[   67] Training loss: 0.02083775, Validation loss: 0.03052265, Gradient norm: 0.32128344
INFO:root:[   68] Training loss: 0.02035608, Validation loss: 0.03342959, Gradient norm: 0.35176452
INFO:root:[   69] Training loss: 0.02083071, Validation loss: 0.03110650, Gradient norm: 0.37670828
INFO:root:[   70] Training loss: 0.02030398, Validation loss: 0.03283249, Gradient norm: 0.35226225
INFO:root:[   71] Training loss: 0.02022850, Validation loss: 0.03504586, Gradient norm: 0.34452515
INFO:root:[   72] Training loss: 0.02122426, Validation loss: 0.03634605, Gradient norm: 0.36113778
INFO:root:[   73] Training loss: 0.02049408, Validation loss: 0.03244701, Gradient norm: 0.33438759
INFO:root:[   74] Training loss: 0.02038699, Validation loss: 0.03539548, Gradient norm: 0.33969628
INFO:root:[   75] Training loss: 0.02043305, Validation loss: 0.03773176, Gradient norm: 0.34082290
INFO:root:[   76] Training loss: 0.01975877, Validation loss: 0.03185963, Gradient norm: 0.33012925
INFO:root:EP 76: Early stopping
INFO:root:Training the model took 2309.086s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02818
INFO:root:EnergyScoreTrain: 0.02663
INFO:root:CoverageTrain: 0.04751
INFO:root:IntervalWidthTrain: 0.00363
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02669
INFO:root:EnergyScoreValidation: 0.02532
INFO:root:CoverageValidation: 0.05296
INFO:root:IntervalWidthValidation: 0.00322
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02717
INFO:root:EnergyScoreTest: 0.02587
INFO:root:CoverageTest: 0.05308
INFO:root:IntervalWidthTest: 0.00308
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 639631360
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07214645, Validation loss: 0.05954473, Gradient norm: 0.64548090
INFO:root:[    2] Training loss: 0.04930956, Validation loss: 0.04425986, Gradient norm: 0.47509612
INFO:root:[    3] Training loss: 0.04433024, Validation loss: 0.04158894, Gradient norm: 0.50949386
INFO:root:[    4] Training loss: 0.04027827, Validation loss: 0.03205694, Gradient norm: 0.44820066
INFO:root:[    5] Training loss: 0.03861288, Validation loss: 0.03210857, Gradient norm: 0.50461426
INFO:root:[    6] Training loss: 0.03624177, Validation loss: 0.03170059, Gradient norm: 0.48249455
INFO:root:[    7] Training loss: 0.03323503, Validation loss: 0.03400953, Gradient norm: 0.35145989
INFO:root:[    8] Training loss: 0.03371211, Validation loss: 0.03644075, Gradient norm: 0.44492671
INFO:root:[    9] Training loss: 0.03195974, Validation loss: 0.03121565, Gradient norm: 0.38460618
INFO:root:[   10] Training loss: 0.03193969, Validation loss: 0.03377047, Gradient norm: 0.41942192
INFO:root:[   11] Training loss: 0.03078049, Validation loss: 0.03499109, Gradient norm: 0.37430892
INFO:root:[   12] Training loss: 0.03256077, Validation loss: 0.03353249, Gradient norm: 0.42170789
INFO:root:[   13] Training loss: 0.03071488, Validation loss: 0.04252866, Gradient norm: 0.40818457
INFO:root:[   14] Training loss: 0.03086328, Validation loss: 0.03867208, Gradient norm: 0.42847864
INFO:root:[   15] Training loss: 0.02944706, Validation loss: 0.03668339, Gradient norm: 0.39653408
INFO:root:[   16] Training loss: 0.02769676, Validation loss: 0.02827847, Gradient norm: 0.33153217
INFO:root:[   17] Training loss: 0.02988458, Validation loss: 0.04187212, Gradient norm: 0.43028617
INFO:root:[   18] Training loss: 0.02798189, Validation loss: 0.02872602, Gradient norm: 0.37217234
INFO:root:[   19] Training loss: 0.02790739, Validation loss: 0.03163110, Gradient norm: 0.37565881
INFO:root:[   20] Training loss: 0.02799487, Validation loss: 0.03338588, Gradient norm: 0.41916247
INFO:root:[   21] Training loss: 0.02997846, Validation loss: 0.03571049, Gradient norm: 0.48197638
INFO:root:[   22] Training loss: 0.02767717, Validation loss: 0.03746634, Gradient norm: 0.40005760
INFO:root:[   23] Training loss: 0.02733727, Validation loss: 0.03489592, Gradient norm: 0.41605103
INFO:root:[   24] Training loss: 0.02638243, Validation loss: 0.03445437, Gradient norm: 0.34300652
INFO:root:[   25] Training loss: 0.02659653, Validation loss: 0.03163596, Gradient norm: 0.38826104
INFO:root:[   26] Training loss: 0.02766058, Validation loss: 0.02993377, Gradient norm: 0.42268230
INFO:root:[   27] Training loss: 0.02699867, Validation loss: 0.03132860, Gradient norm: 0.43086381
INFO:root:[   28] Training loss: 0.02635284, Validation loss: 0.04379994, Gradient norm: 0.35710393
INFO:root:[   29] Training loss: 0.02485366, Validation loss: 0.03659575, Gradient norm: 0.30633973
INFO:root:[   30] Training loss: 0.02553549, Validation loss: 0.03820450, Gradient norm: 0.33805113
INFO:root:[   31] Training loss: 0.02650638, Validation loss: 0.03617356, Gradient norm: 0.42643827
INFO:root:[   32] Training loss: 0.02476721, Validation loss: 0.03690021, Gradient norm: 0.33322039
INFO:root:[   33] Training loss: 0.02561319, Validation loss: 0.03448068, Gradient norm: 0.37768564
INFO:root:[   34] Training loss: 0.02533882, Validation loss: 0.02928555, Gradient norm: 0.33499087
INFO:root:[   35] Training loss: 0.02569201, Validation loss: 0.03195245, Gradient norm: 0.38959606
INFO:root:[   36] Training loss: 0.02499706, Validation loss: 0.03439844, Gradient norm: 0.41940920
INFO:root:[   37] Training loss: 0.02551314, Validation loss: 0.03687702, Gradient norm: 0.41910426
INFO:root:[   38] Training loss: 0.02522430, Validation loss: 0.02912294, Gradient norm: 0.38636865
INFO:root:[   39] Training loss: 0.02513713, Validation loss: 0.03585284, Gradient norm: 0.36268244
INFO:root:[   40] Training loss: 0.02488142, Validation loss: 0.03086610, Gradient norm: 0.35783072
INFO:root:[   41] Training loss: 0.02540797, Validation loss: 0.03702717, Gradient norm: 0.37183086
INFO:root:[   42] Training loss: 0.02458438, Validation loss: 0.03393255, Gradient norm: 0.34982730
INFO:root:[   43] Training loss: 0.02470660, Validation loss: 0.03247439, Gradient norm: 0.31191439
INFO:root:[   44] Training loss: 0.02407791, Validation loss: 0.02924159, Gradient norm: 0.32949300
INFO:root:[   45] Training loss: 0.02460700, Validation loss: 0.03160880, Gradient norm: 0.36127745
INFO:root:[   46] Training loss: 0.02399920, Validation loss: 0.03685615, Gradient norm: 0.34660555
INFO:root:[   47] Training loss: 0.02414276, Validation loss: 0.03453469, Gradient norm: 0.35930103
INFO:root:[   48] Training loss: 0.02385443, Validation loss: 0.03737069, Gradient norm: 0.33664972
INFO:root:[   49] Training loss: 0.02364936, Validation loss: 0.04203914, Gradient norm: 0.34146579
INFO:root:[   50] Training loss: 0.02458911, Validation loss: 0.03450814, Gradient norm: 0.33884888
INFO:root:[   51] Training loss: 0.02318088, Validation loss: 0.03477052, Gradient norm: 0.29621765
INFO:root:[   52] Training loss: 0.02419361, Validation loss: 0.03154600, Gradient norm: 0.38655563
INFO:root:[   53] Training loss: 0.02428895, Validation loss: 0.03251009, Gradient norm: 0.39506865
INFO:root:[   54] Training loss: 0.02272621, Validation loss: 0.03348711, Gradient norm: 0.28469880
INFO:root:[   55] Training loss: 0.02381488, Validation loss: 0.03880709, Gradient norm: 0.34027461
INFO:root:[   56] Training loss: 0.02350357, Validation loss: 0.03714971, Gradient norm: 0.31947773
INFO:root:[   57] Training loss: 0.02313107, Validation loss: 0.03545722, Gradient norm: 0.32358860
INFO:root:[   58] Training loss: 0.02307833, Validation loss: 0.04029799, Gradient norm: 0.33783037
INFO:root:[   59] Training loss: 0.02318773, Validation loss: 0.03078574, Gradient norm: 0.33632460
INFO:root:[   60] Training loss: 0.02233278, Validation loss: 0.03238276, Gradient norm: 0.28455922
INFO:root:[   61] Training loss: 0.02318296, Validation loss: 0.03606313, Gradient norm: 0.31528482
INFO:root:[   62] Training loss: 0.02297626, Validation loss: 0.03692291, Gradient norm: 0.32605082
INFO:root:[   63] Training loss: 0.02378858, Validation loss: 0.03454618, Gradient norm: 0.35662133
INFO:root:[   64] Training loss: 0.02321046, Validation loss: 0.03598778, Gradient norm: 0.31846425
INFO:root:[   65] Training loss: 0.02295340, Validation loss: 0.02964496, Gradient norm: 0.28454725
INFO:root:[   66] Training loss: 0.02280006, Validation loss: 0.03366225, Gradient norm: 0.31420217
INFO:root:[   67] Training loss: 0.02348731, Validation loss: 0.03936274, Gradient norm: 0.34075834
INFO:root:[   68] Training loss: 0.02252561, Validation loss: 0.03753992, Gradient norm: 0.31106980
INFO:root:[   69] Training loss: 0.02274614, Validation loss: 0.03896608, Gradient norm: 0.33318146
INFO:root:[   70] Training loss: 0.02374921, Validation loss: 0.04176671, Gradient norm: 0.36573686
INFO:root:[   71] Training loss: 0.02264794, Validation loss: 0.03992956, Gradient norm: 0.35808162
INFO:root:[   72] Training loss: 0.02258693, Validation loss: 0.04256547, Gradient norm: 0.32101629
INFO:root:[   73] Training loss: 0.02315488, Validation loss: 0.03097090, Gradient norm: 0.33878323
INFO:root:[   74] Training loss: 0.02344740, Validation loss: 0.03484268, Gradient norm: 0.38299626
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 2273.824s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0287
INFO:root:EnergyScoreTrain: 0.02684
INFO:root:CoverageTrain: 0.08596
INFO:root:IntervalWidthTrain: 0.00453
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02866
INFO:root:EnergyScoreValidation: 0.02697
INFO:root:CoverageValidation: 0.06994
INFO:root:IntervalWidthValidation: 0.00413
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02921
INFO:root:EnergyScoreTest: 0.0277
INFO:root:CoverageTest: 0.05835
INFO:root:IntervalWidthTest: 0.00366
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 622854144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07319511, Validation loss: 0.06156462, Gradient norm: 0.61128803
INFO:root:[    2] Training loss: 0.04966757, Validation loss: 0.03829885, Gradient norm: 0.34200670
INFO:root:[    3] Training loss: 0.04498971, Validation loss: 0.04434467, Gradient norm: 0.38287244
INFO:root:[    4] Training loss: 0.04163699, Validation loss: 0.03557702, Gradient norm: 0.39416223
INFO:root:[    5] Training loss: 0.04068655, Validation loss: 0.03443727, Gradient norm: 0.46538159
INFO:root:[    6] Training loss: 0.03838919, Validation loss: 0.03520179, Gradient norm: 0.40337250
INFO:root:[    7] Training loss: 0.03473501, Validation loss: 0.03535180, Gradient norm: 0.32186508
INFO:root:[    8] Training loss: 0.03525920, Validation loss: 0.03598829, Gradient norm: 0.37631995
INFO:root:[    9] Training loss: 0.03485155, Validation loss: 0.04021724, Gradient norm: 0.41246470
INFO:root:[   10] Training loss: 0.03366798, Validation loss: 0.03783566, Gradient norm: 0.37926843
INFO:root:[   11] Training loss: 0.03306882, Validation loss: 0.04648236, Gradient norm: 0.38898086
INFO:root:[   12] Training loss: 0.03290629, Validation loss: 0.03425505, Gradient norm: 0.33540332
INFO:root:[   13] Training loss: 0.03256545, Validation loss: 0.03889291, Gradient norm: 0.39357537
INFO:root:[   14] Training loss: 0.03112780, Validation loss: 0.03804098, Gradient norm: 0.33567205
INFO:root:[   15] Training loss: 0.03239911, Validation loss: 0.03612285, Gradient norm: 0.37678187
INFO:root:[   16] Training loss: 0.03203439, Validation loss: 0.03347312, Gradient norm: 0.38763906
INFO:root:[   17] Training loss: 0.03034889, Validation loss: 0.03670368, Gradient norm: 0.34367827
INFO:root:[   18] Training loss: 0.03108396, Validation loss: 0.03850344, Gradient norm: 0.37232718
INFO:root:[   19] Training loss: 0.03061396, Validation loss: 0.03700588, Gradient norm: 0.37812467
INFO:root:[   20] Training loss: 0.02914705, Validation loss: 0.04731766, Gradient norm: 0.32203254
INFO:root:[   21] Training loss: 0.03002996, Validation loss: 0.03798356, Gradient norm: 0.33291586
INFO:root:[   22] Training loss: 0.02948340, Validation loss: 0.03451680, Gradient norm: 0.34395641
INFO:root:[   23] Training loss: 0.02805757, Validation loss: 0.03517225, Gradient norm: 0.29323932
INFO:root:[   24] Training loss: 0.02972994, Validation loss: 0.03219937, Gradient norm: 0.37832787
INFO:root:[   25] Training loss: 0.02921684, Validation loss: 0.03366412, Gradient norm: 0.31568603
INFO:root:[   26] Training loss: 0.02866716, Validation loss: 0.03848736, Gradient norm: 0.34768232
INFO:root:[   27] Training loss: 0.02970219, Validation loss: 0.03788137, Gradient norm: 0.35412174
INFO:root:[   28] Training loss: 0.02915789, Validation loss: 0.03342538, Gradient norm: 0.34417585
INFO:root:[   29] Training loss: 0.02869396, Validation loss: 0.03349103, Gradient norm: 0.39025162
INFO:root:[   30] Training loss: 0.02869534, Validation loss: 0.03876179, Gradient norm: 0.31871763
INFO:root:[   31] Training loss: 0.02757083, Validation loss: 0.03150471, Gradient norm: 0.29635604
INFO:root:[   32] Training loss: 0.02734390, Validation loss: 0.04035743, Gradient norm: 0.26498585
INFO:root:[   33] Training loss: 0.02785959, Validation loss: 0.04023737, Gradient norm: 0.29823251
INFO:root:[   34] Training loss: 0.02761499, Validation loss: 0.03839861, Gradient norm: 0.30649084
INFO:root:[   35] Training loss: 0.02812601, Validation loss: 0.04312685, Gradient norm: 0.30462108
INFO:root:[   36] Training loss: 0.02737389, Validation loss: 0.04116374, Gradient norm: 0.33459619
INFO:root:[   37] Training loss: 0.02775307, Validation loss: 0.04482943, Gradient norm: 0.35274761
INFO:root:[   38] Training loss: 0.02722416, Validation loss: 0.03635456, Gradient norm: 0.35036479
INFO:root:[   39] Training loss: 0.02737595, Validation loss: 0.03460670, Gradient norm: 0.31054349
INFO:root:[   40] Training loss: 0.02822451, Validation loss: 0.03857932, Gradient norm: 0.33908292
INFO:root:[   41] Training loss: 0.02725973, Validation loss: 0.04314889, Gradient norm: 0.32424171
INFO:root:[   42] Training loss: 0.02711917, Validation loss: 0.03681334, Gradient norm: 0.28915102
INFO:root:[   43] Training loss: 0.02638100, Validation loss: 0.03231966, Gradient norm: 0.27654540
INFO:root:[   44] Training loss: 0.02722396, Validation loss: 0.04212458, Gradient norm: 0.33703821
INFO:root:[   45] Training loss: 0.02738548, Validation loss: 0.03733810, Gradient norm: 0.36863780
INFO:root:[   46] Training loss: 0.02642895, Validation loss: 0.03321720, Gradient norm: 0.27982842
INFO:root:[   47] Training loss: 0.02756772, Validation loss: 0.03923087, Gradient norm: 0.34562372
INFO:root:[   48] Training loss: 0.02643049, Validation loss: 0.03168190, Gradient norm: 0.27905083
INFO:root:[   49] Training loss: 0.02659321, Validation loss: 0.04279776, Gradient norm: 0.27010403
INFO:root:[   50] Training loss: 0.02625074, Validation loss: 0.03266445, Gradient norm: 0.26734231
INFO:root:[   51] Training loss: 0.02623716, Validation loss: 0.03394894, Gradient norm: 0.30607084
INFO:root:[   52] Training loss: 0.02656572, Validation loss: 0.04129865, Gradient norm: 0.33215861
INFO:root:[   53] Training loss: 0.02640956, Validation loss: 0.03555578, Gradient norm: 0.33083852
INFO:root:[   54] Training loss: 0.02650182, Validation loss: 0.04106736, Gradient norm: 0.29841695
INFO:root:[   55] Training loss: 0.02584056, Validation loss: 0.03477009, Gradient norm: 0.28353885
INFO:root:[   56] Training loss: 0.02585253, Validation loss: 0.03134267, Gradient norm: 0.29079038
INFO:root:[   57] Training loss: 0.02561046, Validation loss: 0.03241324, Gradient norm: 0.24730455
INFO:root:[   58] Training loss: 0.02586570, Validation loss: 0.03845655, Gradient norm: 0.31626850
INFO:root:[   59] Training loss: 0.02640212, Validation loss: 0.03828485, Gradient norm: 0.34669736
INFO:root:[   60] Training loss: 0.02587639, Validation loss: 0.04215485, Gradient norm: 0.29573852
INFO:root:[   61] Training loss: 0.02537496, Validation loss: 0.03892890, Gradient norm: 0.26044387
INFO:root:[   62] Training loss: 0.02467757, Validation loss: 0.03778190, Gradient norm: 0.23573500
INFO:root:[   63] Training loss: 0.02641816, Validation loss: 0.03771088, Gradient norm: 0.34342893
INFO:root:[   64] Training loss: 0.02625452, Validation loss: 0.03704132, Gradient norm: 0.32562306
INFO:root:[   65] Training loss: 0.02538896, Validation loss: 0.03876431, Gradient norm: 0.27689909
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 2099.747s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02066
INFO:root:EnergyScoreTrain: 0.01615
INFO:root:CoverageTrain: 0.27478
INFO:root:IntervalWidthTrain: 0.01367
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03138
INFO:root:EnergyScoreValidation: 0.02762
INFO:root:CoverageValidation: 0.18409
INFO:root:IntervalWidthValidation: 0.01072
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03299
INFO:root:EnergyScoreTest: 0.02901
INFO:root:CoverageTest: 0.18405
INFO:root:IntervalWidthTest: 0.01126
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 488636416
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06448757, Validation loss: 0.05076507, Gradient norm: 0.45478578
INFO:root:[    2] Training loss: 0.04985308, Validation loss: 0.05735255, Gradient norm: 0.35923867
INFO:root:[    3] Training loss: 0.04471919, Validation loss: 0.04855825, Gradient norm: 0.36524661
INFO:root:[    4] Training loss: 0.04359331, Validation loss: 0.04170964, Gradient norm: 0.37126634
INFO:root:[    5] Training loss: 0.04206371, Validation loss: 0.04583270, Gradient norm: 0.32213844
INFO:root:[    6] Training loss: 0.04193143, Validation loss: 0.04673896, Gradient norm: 0.36832787
INFO:root:[    7] Training loss: 0.03973813, Validation loss: 0.04329704, Gradient norm: 0.35794597
INFO:root:[    8] Training loss: 0.03817066, Validation loss: 0.04807454, Gradient norm: 0.38734395
INFO:root:[    9] Training loss: 0.03851961, Validation loss: 0.05008359, Gradient norm: 0.41265480
INFO:root:[   10] Training loss: 0.03643995, Validation loss: 0.04245015, Gradient norm: 0.34427991
INFO:root:[   11] Training loss: 0.03662935, Validation loss: 0.03952111, Gradient norm: 0.35844305
INFO:root:[   12] Training loss: 0.03507340, Validation loss: 0.04804887, Gradient norm: 0.33608485
INFO:root:[   13] Training loss: 0.03543336, Validation loss: 0.04123147, Gradient norm: 0.39895401
INFO:root:[   14] Training loss: 0.03389674, Validation loss: 0.04056288, Gradient norm: 0.32608268
INFO:root:[   15] Training loss: 0.03254427, Validation loss: 0.03800841, Gradient norm: 0.28713205
INFO:root:[   16] Training loss: 0.03343764, Validation loss: 0.04313920, Gradient norm: 0.30398276
INFO:root:[   17] Training loss: 0.03322363, Validation loss: 0.04139921, Gradient norm: 0.33945147
INFO:root:[   18] Training loss: 0.03238144, Validation loss: 0.04414291, Gradient norm: 0.30443670
INFO:root:[   19] Training loss: 0.03389332, Validation loss: 0.03666262, Gradient norm: 0.34255635
INFO:root:[   20] Training loss: 0.03261360, Validation loss: 0.04587655, Gradient norm: 0.29339707
INFO:root:[   21] Training loss: 0.03228219, Validation loss: 0.03637296, Gradient norm: 0.31107541
INFO:root:[   22] Training loss: 0.03286716, Validation loss: 0.04379889, Gradient norm: 0.36585978
INFO:root:[   23] Training loss: 0.03124298, Validation loss: 0.03735162, Gradient norm: 0.28875105
INFO:root:[   24] Training loss: 0.03208290, Validation loss: 0.03799705, Gradient norm: 0.34930042
INFO:root:[   25] Training loss: 0.03131144, Validation loss: 0.03732357, Gradient norm: 0.28071096
INFO:root:[   26] Training loss: 0.03164590, Validation loss: 0.04143143, Gradient norm: 0.29737253
INFO:root:[   27] Training loss: 0.03165435, Validation loss: 0.04225285, Gradient norm: 0.30479896
INFO:root:[   28] Training loss: 0.03098607, Validation loss: 0.04280305, Gradient norm: 0.31893517
INFO:root:[   29] Training loss: 0.03147131, Validation loss: 0.03631577, Gradient norm: 0.31542184
INFO:root:[   30] Training loss: 0.03063899, Validation loss: 0.04622460, Gradient norm: 0.26945990
INFO:root:[   31] Training loss: 0.03109917, Validation loss: 0.04006801, Gradient norm: 0.32093495
INFO:root:[   32] Training loss: 0.03058486, Validation loss: 0.03531413, Gradient norm: 0.29554703
INFO:root:[   33] Training loss: 0.03246423, Validation loss: 0.04302902, Gradient norm: 0.38095613
INFO:root:[   34] Training loss: 0.03045252, Validation loss: 0.03912653, Gradient norm: 0.28055461
INFO:root:[   35] Training loss: 0.02990784, Validation loss: 0.03643499, Gradient norm: 0.29391489
INFO:root:[   36] Training loss: 0.02987481, Validation loss: 0.04290011, Gradient norm: 0.27127832
INFO:root:[   37] Training loss: 0.03078775, Validation loss: 0.03874102, Gradient norm: 0.31788371
INFO:root:[   38] Training loss: 0.03016270, Validation loss: 0.03777131, Gradient norm: 0.32238796
INFO:root:[   39] Training loss: 0.02959290, Validation loss: 0.04131265, Gradient norm: 0.29798194
INFO:root:[   40] Training loss: 0.02995603, Validation loss: 0.04309047, Gradient norm: 0.25501389
INFO:root:[   41] Training loss: 0.03017499, Validation loss: 0.03478473, Gradient norm: 0.34490525
INFO:root:[   42] Training loss: 0.02972533, Validation loss: 0.03922014, Gradient norm: 0.26939739
INFO:root:[   43] Training loss: 0.02945005, Validation loss: 0.03994047, Gradient norm: 0.26026979
INFO:root:[   44] Training loss: 0.03004928, Validation loss: 0.03739248, Gradient norm: 0.28700042
INFO:root:[   45] Training loss: 0.02960097, Validation loss: 0.03928385, Gradient norm: 0.29175257
INFO:root:[   46] Training loss: 0.02902029, Validation loss: 0.03642526, Gradient norm: 0.25096227
INFO:root:[   47] Training loss: 0.02943068, Validation loss: 0.03664644, Gradient norm: 0.28113866
INFO:root:[   48] Training loss: 0.02892752, Validation loss: 0.03735356, Gradient norm: 0.26382839
INFO:root:[   49] Training loss: 0.03019119, Validation loss: 0.03638501, Gradient norm: 0.34152832
INFO:root:[   50] Training loss: 0.03005175, Validation loss: 0.03877039, Gradient norm: 0.29197611
INFO:root:[   51] Training loss: 0.02884959, Validation loss: 0.04532644, Gradient norm: 0.29525175
INFO:root:[   52] Training loss: 0.02912897, Validation loss: 0.03928668, Gradient norm: 0.28941986
INFO:root:[   53] Training loss: 0.02900132, Validation loss: 0.03959870, Gradient norm: 0.26315700
INFO:root:[   54] Training loss: 0.02914161, Validation loss: 0.04800957, Gradient norm: 0.30465059
INFO:root:[   55] Training loss: 0.02947630, Validation loss: 0.03415295, Gradient norm: 0.31977613
INFO:root:[   56] Training loss: 0.02852853, Validation loss: 0.03722644, Gradient norm: 0.26659585
INFO:root:[   57] Training loss: 0.02886157, Validation loss: 0.03554833, Gradient norm: 0.30138985
INFO:root:[   58] Training loss: 0.02955415, Validation loss: 0.03666396, Gradient norm: 0.32570701
INFO:root:[   59] Training loss: 0.02860020, Validation loss: 0.03599519, Gradient norm: 0.26789369
INFO:root:[   60] Training loss: 0.02849167, Validation loss: 0.03520230, Gradient norm: 0.28219489
INFO:root:[   61] Training loss: 0.02885812, Validation loss: 0.03722995, Gradient norm: 0.29815121
INFO:root:[   62] Training loss: 0.02819219, Validation loss: 0.03564927, Gradient norm: 0.25343975
INFO:root:[   63] Training loss: 0.02808101, Validation loss: 0.04039562, Gradient norm: 0.26718615
INFO:root:[   64] Training loss: 0.02815149, Validation loss: 0.03669311, Gradient norm: 0.27257790
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 2143.042s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03325
INFO:root:EnergyScoreTrain: 0.02427
INFO:root:CoverageTrain: 0.29865
INFO:root:IntervalWidthTrain: 0.02777
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03568
INFO:root:EnergyScoreValidation: 0.02866
INFO:root:CoverageValidation: 0.25833
INFO:root:IntervalWidthValidation: 0.02176
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03582
INFO:root:EnergyScoreTest: 0.02858
INFO:root:CoverageTest: 0.27206
INFO:root:IntervalWidthTest: 0.02304
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 289406976
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06603967, Validation loss: 0.04558786, Gradient norm: 0.88334917
INFO:root:[    2] Training loss: 0.04160807, Validation loss: 0.03456008, Gradient norm: 0.66250225
INFO:root:[    3] Training loss: 0.03687712, Validation loss: 0.03354515, Gradient norm: 0.66383461
INFO:root:[    4] Training loss: 0.03308085, Validation loss: 0.03292011, Gradient norm: 0.69463562
INFO:root:[    5] Training loss: 0.03073528, Validation loss: 0.02894246, Gradient norm: 0.59296863
INFO:root:[    6] Training loss: 0.02860449, Validation loss: 0.02581679, Gradient norm: 0.51751036
INFO:root:[    7] Training loss: 0.02668984, Validation loss: 0.03310453, Gradient norm: 0.49800758
INFO:root:[    8] Training loss: 0.02599497, Validation loss: 0.03369214, Gradient norm: 0.49116621
INFO:root:[    9] Training loss: 0.02680639, Validation loss: 0.02897217, Gradient norm: 0.54358682
INFO:root:[   10] Training loss: 0.02547797, Validation loss: 0.02969475, Gradient norm: 0.50803780
INFO:root:[   11] Training loss: 0.02401014, Validation loss: 0.03053833, Gradient norm: 0.47247489
INFO:root:[   12] Training loss: 0.02303834, Validation loss: 0.03020429, Gradient norm: 0.45094034
INFO:root:[   13] Training loss: 0.02457280, Validation loss: 0.02892277, Gradient norm: 0.52077208
INFO:root:[   14] Training loss: 0.02217158, Validation loss: 0.03725248, Gradient norm: 0.40594582
INFO:root:[   15] Training loss: 0.02398489, Validation loss: 0.04112818, Gradient norm: 0.49324402
INFO:root:[   16] Training loss: 0.02296121, Validation loss: 0.02908819, Gradient norm: 0.46320120
INFO:root:[   17] Training loss: 0.02100564, Validation loss: 0.03682662, Gradient norm: 0.39715462
INFO:root:[   18] Training loss: 0.02211298, Validation loss: 0.03599728, Gradient norm: 0.49612425
INFO:root:[   19] Training loss: 0.01951729, Validation loss: 0.02955708, Gradient norm: 0.33227110
INFO:root:[   20] Training loss: 0.02221534, Validation loss: 0.03059797, Gradient norm: 0.53787838
INFO:root:[   21] Training loss: 0.02070304, Validation loss: 0.03472640, Gradient norm: 0.44344177
INFO:root:[   22] Training loss: 0.02030324, Validation loss: 0.03685253, Gradient norm: 0.45468376
INFO:root:[   23] Training loss: 0.02008529, Validation loss: 0.03113198, Gradient norm: 0.46322599
INFO:root:[   24] Training loss: 0.02018460, Validation loss: 0.03361998, Gradient norm: 0.48297296
INFO:root:[   25] Training loss: 0.01942498, Validation loss: 0.03265617, Gradient norm: 0.42716649
INFO:root:[   26] Training loss: 0.01979157, Validation loss: 0.03299951, Gradient norm: 0.46070215
INFO:root:[   27] Training loss: 0.01956274, Validation loss: 0.03585598, Gradient norm: 0.45718534
INFO:root:[   28] Training loss: 0.02010419, Validation loss: 0.03014409, Gradient norm: 0.45966252
INFO:root:[   29] Training loss: 0.01794614, Validation loss: 0.03543237, Gradient norm: 0.36479302
INFO:root:[   30] Training loss: 0.01830287, Validation loss: 0.03214601, Gradient norm: 0.43403059
INFO:root:[   31] Training loss: 0.01851813, Validation loss: 0.03708496, Gradient norm: 0.45947200
INFO:root:[   32] Training loss: 0.01801246, Validation loss: 0.03183476, Gradient norm: 0.41825997
INFO:root:[   33] Training loss: 0.01837577, Validation loss: 0.03078987, Gradient norm: 0.46855516
INFO:root:[   34] Training loss: 0.01797896, Validation loss: 0.03154610, Gradient norm: 0.42279233
INFO:root:[   35] Training loss: 0.01803957, Validation loss: 0.03462981, Gradient norm: 0.40664022
INFO:root:[   36] Training loss: 0.01697885, Validation loss: 0.03499366, Gradient norm: 0.38193151
INFO:root:[   37] Training loss: 0.01705693, Validation loss: 0.03402877, Gradient norm: 0.37599288
INFO:root:[   38] Training loss: 0.01810578, Validation loss: 0.03281714, Gradient norm: 0.42934482
INFO:root:[   39] Training loss: 0.01669583, Validation loss: 0.03261781, Gradient norm: 0.36557423
INFO:root:[   40] Training loss: 0.01716086, Validation loss: 0.03412242, Gradient norm: 0.37890172
INFO:root:[   41] Training loss: 0.01734328, Validation loss: 0.03474455, Gradient norm: 0.42068501
INFO:root:[   42] Training loss: 0.01740719, Validation loss: 0.03261848, Gradient norm: 0.43159877
INFO:root:[   43] Training loss: 0.01694947, Validation loss: 0.03083647, Gradient norm: 0.40474566
INFO:root:[   44] Training loss: 0.01718725, Validation loss: 0.03293080, Gradient norm: 0.40803224
INFO:root:[   45] Training loss: 0.01647669, Validation loss: 0.03795998, Gradient norm: 0.42046910
INFO:root:[   46] Training loss: 0.01683335, Validation loss: 0.03806622, Gradient norm: 0.43188716
INFO:root:[   47] Training loss: 0.01702343, Validation loss: 0.03222121, Gradient norm: 0.42092642
INFO:root:[   48] Training loss: 0.01649097, Validation loss: 0.03533856, Gradient norm: 0.39046231
INFO:root:[   49] Training loss: 0.01590286, Validation loss: 0.03232537, Gradient norm: 0.36742127
INFO:root:[   50] Training loss: 0.01556350, Validation loss: 0.03216730, Gradient norm: 0.33059207
INFO:root:[   51] Training loss: 0.01582200, Validation loss: 0.03229227, Gradient norm: 0.37358866
INFO:root:[   52] Training loss: 0.01637076, Validation loss: 0.03377190, Gradient norm: 0.40863543
INFO:root:[   53] Training loss: 0.01609296, Validation loss: 0.03194996, Gradient norm: 0.34495350
INFO:root:[   54] Training loss: 0.01542717, Validation loss: 0.03409378, Gradient norm: 0.36605636
INFO:root:[   55] Training loss: 0.01592531, Validation loss: 0.03487132, Gradient norm: 0.40117482
INFO:root:[   56] Training loss: 0.01597002, Validation loss: 0.03226781, Gradient norm: 0.41472656
INFO:root:[   57] Training loss: 0.01603283, Validation loss: 0.03225724, Gradient norm: 0.40123682
INFO:root:[   58] Training loss: 0.01639316, Validation loss: 0.03569817, Gradient norm: 0.41062104
INFO:root:[   59] Training loss: 0.01527399, Validation loss: 0.04072472, Gradient norm: 0.41172909
INFO:root:[   60] Training loss: 0.01513494, Validation loss: 0.03283867, Gradient norm: 0.36154107
INFO:root:[   61] Training loss: 0.01520008, Validation loss: 0.03230890, Gradient norm: 0.42640982
INFO:root:[   62] Training loss: 0.01555897, Validation loss: 0.03596338, Gradient norm: 0.36096515
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1905.284s.
INFO:root:Emptying the cuda cache took 0.007s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02207
INFO:root:EnergyScoreTrain: 0.01779
INFO:root:CoverageTrain: 0.67652
INFO:root:IntervalWidthTrain: 0.03536
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.024
INFO:root:EnergyScoreValidation: 0.01976
INFO:root:CoverageValidation: 0.5791
INFO:root:IntervalWidthValidation: 0.03353
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0246
INFO:root:EnergyScoreTest: 0.02033
INFO:root:CoverageTest: 0.57201
INFO:root:IntervalWidthTest: 0.03355
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 184549376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07739649, Validation loss: 0.06175712, Gradient norm: 0.99304380
INFO:root:[    2] Training loss: 0.04867713, Validation loss: 0.04313745, Gradient norm: 0.66179238
INFO:root:[    3] Training loss: 0.04244998, Validation loss: 0.04093631, Gradient norm: 0.60045866
INFO:root:[    4] Training loss: 0.04225578, Validation loss: 0.03599422, Gradient norm: 0.70159780
INFO:root:[    5] Training loss: 0.03493046, Validation loss: 0.03205593, Gradient norm: 0.53901590
INFO:root:[    6] Training loss: 0.03245064, Validation loss: 0.03064454, Gradient norm: 0.49873864
INFO:root:[    7] Training loss: 0.03206093, Validation loss: 0.03302616, Gradient norm: 0.53010687
INFO:root:[    8] Training loss: 0.03015137, Validation loss: 0.03067486, Gradient norm: 0.42218893
INFO:root:[    9] Training loss: 0.02899895, Validation loss: 0.03014147, Gradient norm: 0.46786487
INFO:root:[   10] Training loss: 0.02823174, Validation loss: 0.04025611, Gradient norm: 0.43027950
INFO:root:[   11] Training loss: 0.03098640, Validation loss: 0.03109421, Gradient norm: 0.60786152
INFO:root:[   12] Training loss: 0.02656861, Validation loss: 0.03041951, Gradient norm: 0.43358238
INFO:root:[   13] Training loss: 0.02599061, Validation loss: 0.03753608, Gradient norm: 0.46687532
INFO:root:[   14] Training loss: 0.02522488, Validation loss: 0.03000091, Gradient norm: 0.42440047
INFO:root:[   15] Training loss: 0.02578868, Validation loss: 0.02851589, Gradient norm: 0.46287839
INFO:root:[   16] Training loss: 0.02485823, Validation loss: 0.03042117, Gradient norm: 0.45474755
INFO:root:[   17] Training loss: 0.02442561, Validation loss: 0.02910721, Gradient norm: 0.42650410
INFO:root:[   18] Training loss: 0.02430353, Validation loss: 0.03167785, Gradient norm: 0.44383494
INFO:root:[   19] Training loss: 0.02416427, Validation loss: 0.03008753, Gradient norm: 0.46040911
INFO:root:[   20] Training loss: 0.02270284, Validation loss: 0.03120591, Gradient norm: 0.38758551
INFO:root:[   21] Training loss: 0.02418845, Validation loss: 0.03410349, Gradient norm: 0.45902397
INFO:root:[   22] Training loss: 0.02276650, Validation loss: 0.03232894, Gradient norm: 0.41619538
INFO:root:[   23] Training loss: 0.02349961, Validation loss: 0.03260020, Gradient norm: 0.48610149
INFO:root:[   24] Training loss: 0.02133703, Validation loss: 0.03223693, Gradient norm: 0.38061346
INFO:root:[   25] Training loss: 0.02163999, Validation loss: 0.03338708, Gradient norm: 0.36364491
INFO:root:[   26] Training loss: 0.02210642, Validation loss: 0.03038003, Gradient norm: 0.44822382
INFO:root:[   27] Training loss: 0.02080644, Validation loss: 0.03202409, Gradient norm: 0.38834639
INFO:root:[   28] Training loss: 0.02024619, Validation loss: 0.03922306, Gradient norm: 0.32837224
INFO:root:[   29] Training loss: 0.02203831, Validation loss: 0.03639295, Gradient norm: 0.41941703
INFO:root:[   30] Training loss: 0.02139819, Validation loss: 0.03469141, Gradient norm: 0.40576680
INFO:root:[   31] Training loss: 0.02183458, Validation loss: 0.03256911, Gradient norm: 0.43361104
INFO:root:[   32] Training loss: 0.02025892, Validation loss: 0.03758963, Gradient norm: 0.35595311
INFO:root:[   33] Training loss: 0.02132892, Validation loss: 0.03802754, Gradient norm: 0.41909378
INFO:root:[   34] Training loss: 0.02080046, Validation loss: 0.04139218, Gradient norm: 0.42988236
INFO:root:[   35] Training loss: 0.02079221, Validation loss: 0.03630288, Gradient norm: 0.38111200
INFO:root:[   36] Training loss: 0.02042665, Validation loss: 0.03510116, Gradient norm: 0.41726016
INFO:root:[   37] Training loss: 0.02076113, Validation loss: 0.03320783, Gradient norm: 0.39603604
INFO:root:[   38] Training loss: 0.01978826, Validation loss: 0.03384775, Gradient norm: 0.35950226
INFO:root:[   39] Training loss: 0.01894794, Validation loss: 0.03456332, Gradient norm: 0.29166431
INFO:root:[   40] Training loss: 0.02060689, Validation loss: 0.03286594, Gradient norm: 0.41587003
INFO:root:[   41] Training loss: 0.02009825, Validation loss: 0.03693607, Gradient norm: 0.37276735
INFO:root:[   42] Training loss: 0.01970361, Validation loss: 0.03727755, Gradient norm: 0.41208952
INFO:root:[   43] Training loss: 0.02059882, Validation loss: 0.03327620, Gradient norm: 0.43712717
INFO:root:[   44] Training loss: 0.01988098, Validation loss: 0.03507906, Gradient norm: 0.41335394
INFO:root:[   45] Training loss: 0.01896158, Validation loss: 0.03658239, Gradient norm: 0.36019827
INFO:root:[   46] Training loss: 0.01941839, Validation loss: 0.03040782, Gradient norm: 0.38182746
INFO:root:[   47] Training loss: 0.01992718, Validation loss: 0.03684396, Gradient norm: 0.36196060
INFO:root:[   48] Training loss: 0.01901292, Validation loss: 0.02937733, Gradient norm: 0.30647283
INFO:root:[   49] Training loss: 0.01914207, Validation loss: 0.03605984, Gradient norm: 0.35524942
INFO:root:[   50] Training loss: 0.01876385, Validation loss: 0.03063010, Gradient norm: 0.35160608
INFO:root:[   51] Training loss: 0.01988521, Validation loss: 0.03358132, Gradient norm: 0.41462046
INFO:root:[   52] Training loss: 0.02003396, Validation loss: 0.03721312, Gradient norm: 0.40137438
INFO:root:[   53] Training loss: 0.01941840, Validation loss: 0.03312570, Gradient norm: 0.39893994
INFO:root:[   54] Training loss: 0.01843161, Validation loss: 0.03551527, Gradient norm: 0.31980142
INFO:root:[   55] Training loss: 0.01822653, Validation loss: 0.03605386, Gradient norm: 0.35472677
INFO:root:[   56] Training loss: 0.01860842, Validation loss: 0.03037846, Gradient norm: 0.35131961
INFO:root:[   57] Training loss: 0.01862993, Validation loss: 0.03357118, Gradient norm: 0.35156783
INFO:root:[   58] Training loss: 0.01851290, Validation loss: 0.03422613, Gradient norm: 0.33841836
INFO:root:[   59] Training loss: 0.01860739, Validation loss: 0.03577210, Gradient norm: 0.36081678
INFO:root:[   60] Training loss: 0.01887883, Validation loss: 0.03485276, Gradient norm: 0.36182461
INFO:root:[   61] Training loss: 0.01875198, Validation loss: 0.03738689, Gradient norm: 0.37707105
INFO:root:[   62] Training loss: 0.01883920, Validation loss: 0.03404737, Gradient norm: 0.39678352
INFO:root:[   63] Training loss: 0.01863619, Validation loss: 0.03321088, Gradient norm: 0.37982876
INFO:root:[   64] Training loss: 0.01823089, Validation loss: 0.03315561, Gradient norm: 0.36766194
INFO:root:[   65] Training loss: 0.01767021, Validation loss: 0.02920350, Gradient norm: 0.29229320
INFO:root:[   66] Training loss: 0.01839594, Validation loss: 0.03559930, Gradient norm: 0.33852587
INFO:root:[   67] Training loss: 0.01881474, Validation loss: 0.03235740, Gradient norm: 0.39852791
INFO:root:[   68] Training loss: 0.01944273, Validation loss: 0.03414247, Gradient norm: 0.43498192
INFO:root:[   69] Training loss: 0.01725094, Validation loss: 0.03619708, Gradient norm: 0.27086951
INFO:root:[   70] Training loss: 0.01773799, Validation loss: 0.03363242, Gradient norm: 0.34317405
INFO:root:[   71] Training loss: 0.01776143, Validation loss: 0.03702616, Gradient norm: 0.34939373
INFO:root:[   72] Training loss: 0.01778951, Validation loss: 0.03012786, Gradient norm: 0.33383559
INFO:root:[   73] Training loss: 0.01863988, Validation loss: 0.03573482, Gradient norm: 0.38866728
INFO:root:[   74] Training loss: 0.01801601, Validation loss: 0.03224334, Gradient norm: 0.38709517
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 2261.737s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02262
INFO:root:EnergyScoreTrain: 0.01729
INFO:root:CoverageTrain: 0.78651
INFO:root:IntervalWidthTrain: 0.04657
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02606
INFO:root:EnergyScoreValidation: 0.02059
INFO:root:CoverageValidation: 0.5676
INFO:root:IntervalWidthValidation: 0.04167
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02632
INFO:root:EnergyScoreTest: 0.02084
INFO:root:CoverageTest: 0.56658
INFO:root:IntervalWidthTest: 0.04161
INFO:root:###21 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07320935, Validation loss: 0.05637818, Gradient norm: 0.90857335
INFO:root:[    2] Training loss: 0.04920923, Validation loss: 0.04322939, Gradient norm: 0.60884705
INFO:root:[    3] Training loss: 0.04350894, Validation loss: 0.04020324, Gradient norm: 0.50819829
INFO:root:[    4] Training loss: 0.04140445, Validation loss: 0.04125068, Gradient norm: 0.58152898
INFO:root:[    5] Training loss: 0.03876321, Validation loss: 0.03559586, Gradient norm: 0.60235333
INFO:root:[    6] Training loss: 0.03321028, Validation loss: 0.03750801, Gradient norm: 0.42257534
INFO:root:[    7] Training loss: 0.03343973, Validation loss: 0.03321215, Gradient norm: 0.52076649
INFO:root:[    8] Training loss: 0.03389427, Validation loss: 0.03422801, Gradient norm: 0.56764291
INFO:root:[    9] Training loss: 0.02981841, Validation loss: 0.03412712, Gradient norm: 0.42935989
INFO:root:[   10] Training loss: 0.02950801, Validation loss: 0.03970023, Gradient norm: 0.41316911
INFO:root:[   11] Training loss: 0.02910065, Validation loss: 0.03263537, Gradient norm: 0.42470827
INFO:root:[   12] Training loss: 0.02654152, Validation loss: 0.03080537, Gradient norm: 0.32318376
INFO:root:[   13] Training loss: 0.02892483, Validation loss: 0.03362349, Gradient norm: 0.43658799
INFO:root:[   14] Training loss: 0.02722105, Validation loss: 0.03189192, Gradient norm: 0.39211317
INFO:root:[   15] Training loss: 0.02706562, Validation loss: 0.03220517, Gradient norm: 0.44809432
INFO:root:[   16] Training loss: 0.02654607, Validation loss: 0.03293878, Gradient norm: 0.41754522
INFO:root:[   17] Training loss: 0.02516370, Validation loss: 0.03309018, Gradient norm: 0.37372115
INFO:root:[   18] Training loss: 0.02574421, Validation loss: 0.03473278, Gradient norm: 0.39760552
INFO:root:[   19] Training loss: 0.02634238, Validation loss: 0.03569023, Gradient norm: 0.45681996
INFO:root:[   20] Training loss: 0.02434512, Validation loss: 0.03706140, Gradient norm: 0.36226048
INFO:root:[   21] Training loss: 0.02598937, Validation loss: 0.03454946, Gradient norm: 0.43251143
INFO:root:[   22] Training loss: 0.02500234, Validation loss: 0.03293075, Gradient norm: 0.41592102
INFO:root:[   23] Training loss: 0.02533804, Validation loss: 0.03903314, Gradient norm: 0.46691844
INFO:root:[   24] Training loss: 0.02447131, Validation loss: 0.03294086, Gradient norm: 0.37766025
INFO:root:[   25] Training loss: 0.02332815, Validation loss: 0.03187680, Gradient norm: 0.36810379
INFO:root:[   26] Training loss: 0.02403373, Validation loss: 0.03095501, Gradient norm: 0.39249444
INFO:root:[   27] Training loss: 0.02381223, Validation loss: 0.03176493, Gradient norm: 0.36925086
INFO:root:[   28] Training loss: 0.02352260, Validation loss: 0.03276429, Gradient norm: 0.38425071
INFO:root:[   29] Training loss: 0.02349373, Validation loss: 0.03050742, Gradient norm: 0.41402313
INFO:root:[   30] Training loss: 0.02358194, Validation loss: 0.03364037, Gradient norm: 0.40953083
INFO:root:[   31] Training loss: 0.02298035, Validation loss: 0.03574801, Gradient norm: 0.37590984
INFO:root:[   32] Training loss: 0.02203492, Validation loss: 0.03477592, Gradient norm: 0.34267826
INFO:root:[   33] Training loss: 0.02316511, Validation loss: 0.03673049, Gradient norm: 0.38372949
INFO:root:[   34] Training loss: 0.02433729, Validation loss: 0.03862656, Gradient norm: 0.43024112
INFO:root:[   35] Training loss: 0.02355949, Validation loss: 0.03674175, Gradient norm: 0.37262813
INFO:root:[   36] Training loss: 0.02234957, Validation loss: 0.04022247, Gradient norm: 0.37555652
INFO:root:[   37] Training loss: 0.02342776, Validation loss: 0.03662972, Gradient norm: 0.41187327
INFO:root:[   38] Training loss: 0.02249301, Validation loss: 0.03971523, Gradient norm: 0.37131649
INFO:root:[   39] Training loss: 0.02244475, Validation loss: 0.03869088, Gradient norm: 0.31109836
INFO:root:[   40] Training loss: 0.02223436, Validation loss: 0.03447055, Gradient norm: 0.36757361
INFO:root:[   41] Training loss: 0.02255639, Validation loss: 0.03564216, Gradient norm: 0.38103601
INFO:root:[   42] Training loss: 0.02195070, Validation loss: 0.03156411, Gradient norm: 0.35821945
INFO:root:[   43] Training loss: 0.02140876, Validation loss: 0.03390062, Gradient norm: 0.35363018
INFO:root:[   44] Training loss: 0.02156498, Validation loss: 0.03771265, Gradient norm: 0.34418254
INFO:root:[   45] Training loss: 0.02197525, Validation loss: 0.03201569, Gradient norm: 0.38395857
INFO:root:[   46] Training loss: 0.02085882, Validation loss: 0.03268976, Gradient norm: 0.25683789
INFO:root:[   47] Training loss: 0.02191909, Validation loss: 0.03645602, Gradient norm: 0.38837206
INFO:root:[   48] Training loss: 0.02164341, Validation loss: 0.03763452, Gradient norm: 0.35140634
INFO:root:[   49] Training loss: 0.02226389, Validation loss: 0.03342324, Gradient norm: 0.37166405
INFO:root:[   50] Training loss: 0.02208096, Validation loss: 0.03486355, Gradient norm: 0.37181817
INFO:root:[   51] Training loss: 0.02145243, Validation loss: 0.03832178, Gradient norm: 0.35096495
INFO:root:[   52] Training loss: 0.02139821, Validation loss: 0.04243196, Gradient norm: 0.37387090
INFO:root:[   53] Training loss: 0.02116790, Validation loss: 0.04706139, Gradient norm: 0.37256141
INFO:root:[   54] Training loss: 0.02156776, Validation loss: 0.03467991, Gradient norm: 0.32887246
INFO:root:[   55] Training loss: 0.02084278, Validation loss: 0.03317314, Gradient norm: 0.35748464
INFO:root:[   56] Training loss: 0.02017042, Validation loss: 0.04060181, Gradient norm: 0.30749530
INFO:root:[   57] Training loss: 0.02163614, Validation loss: 0.04009794, Gradient norm: 0.39510491
INFO:root:[   58] Training loss: 0.02066081, Validation loss: 0.03748961, Gradient norm: 0.32080161
INFO:root:[   59] Training loss: 0.02074807, Validation loss: 0.04264609, Gradient norm: 0.37095080
INFO:root:[   60] Training loss: 0.02057769, Validation loss: 0.04047374, Gradient norm: 0.30411657
INFO:root:[   61] Training loss: 0.02054485, Validation loss: 0.03530468, Gradient norm: 0.31139920
INFO:root:[   62] Training loss: 0.02014172, Validation loss: 0.03389841, Gradient norm: 0.31531239
INFO:root:[   63] Training loss: 0.02160436, Validation loss: 0.03669568, Gradient norm: 0.35688789
INFO:root:[   64] Training loss: 0.02102285, Validation loss: 0.03460817, Gradient norm: 0.35332050
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 1962.027s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0193
INFO:root:EnergyScoreTrain: 0.01444
INFO:root:CoverageTrain: 0.88497
INFO:root:IntervalWidthTrain: 0.0516
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02766
INFO:root:EnergyScoreValidation: 0.02191
INFO:root:CoverageValidation: 0.51596
INFO:root:IntervalWidthValidation: 0.04321
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02842
INFO:root:EnergyScoreTest: 0.02263
INFO:root:CoverageTest: 0.51106
INFO:root:IntervalWidthTest: 0.04318
INFO:root:###22 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 201326592
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07679352, Validation loss: 0.06178759, Gradient norm: 0.67633732
INFO:root:[    2] Training loss: 0.05454080, Validation loss: 0.05151538, Gradient norm: 0.52848423
INFO:root:[    3] Training loss: 0.04833365, Validation loss: 0.04531839, Gradient norm: 0.50475899
INFO:root:[    4] Training loss: 0.04411636, Validation loss: 0.04368723, Gradient norm: 0.46506356
INFO:root:[    5] Training loss: 0.03987837, Validation loss: 0.04327614, Gradient norm: 0.40056015
INFO:root:[    6] Training loss: 0.03689097, Validation loss: 0.03777239, Gradient norm: 0.38651402
INFO:root:[    7] Training loss: 0.03564447, Validation loss: 0.04196761, Gradient norm: 0.36252493
INFO:root:[    8] Training loss: 0.03501024, Validation loss: 0.04387781, Gradient norm: 0.41824611
INFO:root:[    9] Training loss: 0.03359642, Validation loss: 0.04206907, Gradient norm: 0.39508629
INFO:root:[   10] Training loss: 0.03263764, Validation loss: 0.03928810, Gradient norm: 0.37689654
INFO:root:[   11] Training loss: 0.03117848, Validation loss: 0.03647109, Gradient norm: 0.36503864
INFO:root:[   12] Training loss: 0.03025073, Validation loss: 0.04037346, Gradient norm: 0.33995511
INFO:root:[   13] Training loss: 0.03018958, Validation loss: 0.03695059, Gradient norm: 0.38098152
INFO:root:[   14] Training loss: 0.03044619, Validation loss: 0.04317205, Gradient norm: 0.39183534
INFO:root:[   15] Training loss: 0.03007285, Validation loss: 0.04117169, Gradient norm: 0.39610609
INFO:root:[   16] Training loss: 0.02745413, Validation loss: 0.03539869, Gradient norm: 0.30005762
INFO:root:[   17] Training loss: 0.02882728, Validation loss: 0.03391676, Gradient norm: 0.39002604
INFO:root:[   18] Training loss: 0.02863869, Validation loss: 0.03476240, Gradient norm: 0.36625301
INFO:root:[   19] Training loss: 0.02681772, Validation loss: 0.03907943, Gradient norm: 0.34789291
INFO:root:[   20] Training loss: 0.02727844, Validation loss: 0.03883442, Gradient norm: 0.34965031
INFO:root:[   21] Training loss: 0.02646587, Validation loss: 0.03524344, Gradient norm: 0.33432286
INFO:root:[   22] Training loss: 0.02710160, Validation loss: 0.04015349, Gradient norm: 0.37969381
INFO:root:[   23] Training loss: 0.02510434, Validation loss: 0.03327250, Gradient norm: 0.29486641
INFO:root:[   24] Training loss: 0.02664782, Validation loss: 0.04009592, Gradient norm: 0.35203446
INFO:root:[   25] Training loss: 0.02638573, Validation loss: 0.03505059, Gradient norm: 0.33526045
INFO:root:[   26] Training loss: 0.02558978, Validation loss: 0.04009917, Gradient norm: 0.34348935
INFO:root:[   27] Training loss: 0.02537362, Validation loss: 0.04090787, Gradient norm: 0.33145210
INFO:root:[   28] Training loss: 0.02563592, Validation loss: 0.03906651, Gradient norm: 0.37390518
INFO:root:[   29] Training loss: 0.02498784, Validation loss: 0.04151268, Gradient norm: 0.32621173
INFO:root:[   30] Training loss: 0.02469836, Validation loss: 0.03512289, Gradient norm: 0.30450743
INFO:root:[   31] Training loss: 0.02479890, Validation loss: 0.03662097, Gradient norm: 0.32519408
INFO:root:[   32] Training loss: 0.02535456, Validation loss: 0.03620967, Gradient norm: 0.37278467
INFO:root:[   33] Training loss: 0.02473591, Validation loss: 0.03758044, Gradient norm: 0.32677720
INFO:root:[   34] Training loss: 0.02402812, Validation loss: 0.03597702, Gradient norm: 0.31470086
INFO:root:[   35] Training loss: 0.02626098, Validation loss: 0.04375213, Gradient norm: 0.40136579
INFO:root:[   36] Training loss: 0.02453890, Validation loss: 0.03986778, Gradient norm: 0.32535801
INFO:root:[   37] Training loss: 0.02442377, Validation loss: 0.04222156, Gradient norm: 0.32068916
INFO:root:[   38] Training loss: 0.02471386, Validation loss: 0.03763882, Gradient norm: 0.34060821
INFO:root:[   39] Training loss: 0.02402653, Validation loss: 0.03507579, Gradient norm: 0.34972537
INFO:root:[   40] Training loss: 0.02394435, Validation loss: 0.03856537, Gradient norm: 0.31387988
INFO:root:[   41] Training loss: 0.02363368, Validation loss: 0.03450518, Gradient norm: 0.32560083
INFO:root:[   42] Training loss: 0.02408426, Validation loss: 0.03765158, Gradient norm: 0.34649181
INFO:root:[   43] Training loss: 0.02350531, Validation loss: 0.03445634, Gradient norm: 0.27163610
INFO:root:[   44] Training loss: 0.02411952, Validation loss: 0.03367204, Gradient norm: 0.33918646
INFO:root:[   45] Training loss: 0.02332789, Validation loss: 0.03792472, Gradient norm: 0.29196530
INFO:root:[   46] Training loss: 0.02419638, Validation loss: 0.03564906, Gradient norm: 0.33888283
INFO:root:[   47] Training loss: 0.02407489, Validation loss: 0.03528263, Gradient norm: 0.35985590
INFO:root:[   48] Training loss: 0.02386080, Validation loss: 0.03628173, Gradient norm: 0.31042604
INFO:root:[   49] Training loss: 0.02277085, Validation loss: 0.03988888, Gradient norm: 0.28815678
INFO:root:[   50] Training loss: 0.02337417, Validation loss: 0.03713241, Gradient norm: 0.32440504
INFO:root:[   51] Training loss: 0.02401722, Validation loss: 0.03748592, Gradient norm: 0.32708021
INFO:root:[   52] Training loss: 0.02335594, Validation loss: 0.03502995, Gradient norm: 0.32693661
INFO:root:[   53] Training loss: 0.02274728, Validation loss: 0.03742942, Gradient norm: 0.32305251
INFO:root:[   54] Training loss: 0.02274478, Validation loss: 0.03489836, Gradient norm: 0.32472278
INFO:root:[   55] Training loss: 0.02256937, Validation loss: 0.03820124, Gradient norm: 0.26863998
INFO:root:[   56] Training loss: 0.02298726, Validation loss: 0.04039804, Gradient norm: 0.30702015
INFO:root:[   57] Training loss: 0.02240764, Validation loss: 0.03464282, Gradient norm: 0.25824751
INFO:root:[   58] Training loss: 0.02296303, Validation loss: 0.03316600, Gradient norm: 0.29548756
INFO:root:[   59] Training loss: 0.02334054, Validation loss: 0.03795095, Gradient norm: 0.32841371
INFO:root:[   60] Training loss: 0.02258232, Validation loss: 0.03441227, Gradient norm: 0.30678654
INFO:root:[   61] Training loss: 0.02287893, Validation loss: 0.03459511, Gradient norm: 0.29346982
INFO:root:[   62] Training loss: 0.02259383, Validation loss: 0.03905894, Gradient norm: 0.31392297
INFO:root:[   63] Training loss: 0.02250576, Validation loss: 0.03533070, Gradient norm: 0.31377307
INFO:root:[   64] Training loss: 0.02332660, Validation loss: 0.03569552, Gradient norm: 0.34545060
INFO:root:[   65] Training loss: 0.02312887, Validation loss: 0.03365543, Gradient norm: 0.34112758
INFO:root:[   66] Training loss: 0.02325622, Validation loss: 0.03899135, Gradient norm: 0.32908311
INFO:root:[   67] Training loss: 0.02269290, Validation loss: 0.03736373, Gradient norm: 0.31868170
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 2056.166s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01792
INFO:root:EnergyScoreTrain: 0.01313
INFO:root:CoverageTrain: 0.92107
INFO:root:IntervalWidthTrain: 0.05709
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02971
INFO:root:EnergyScoreValidation: 0.02342
INFO:root:CoverageValidation: 0.51141
INFO:root:IntervalWidthValidation: 0.04812
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03045
INFO:root:EnergyScoreTest: 0.02411
INFO:root:CoverageTest: 0.50457
INFO:root:IntervalWidthTest: 0.04802
INFO:root:###23 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08168358, Validation loss: 0.06120832, Gradient norm: 0.70935527
INFO:root:[    2] Training loss: 0.05492080, Validation loss: 0.05451737, Gradient norm: 0.42229149
INFO:root:[    3] Training loss: 0.05027000, Validation loss: 0.04810687, Gradient norm: 0.48221515
INFO:root:[    4] Training loss: 0.04581399, Validation loss: 0.04912492, Gradient norm: 0.49522997
INFO:root:[    5] Training loss: 0.04194121, Validation loss: 0.04033375, Gradient norm: 0.42851836
INFO:root:[    6] Training loss: 0.03749019, Validation loss: 0.03977830, Gradient norm: 0.34015234
INFO:root:[    7] Training loss: 0.03621322, Validation loss: 0.03713255, Gradient norm: 0.41412970
INFO:root:[    8] Training loss: 0.03475947, Validation loss: 0.03680873, Gradient norm: 0.38432637
INFO:root:[    9] Training loss: 0.03447352, Validation loss: 0.04393984, Gradient norm: 0.40976081
INFO:root:[   10] Training loss: 0.03413912, Validation loss: 0.04296828, Gradient norm: 0.42540765
INFO:root:[   11] Training loss: 0.03215033, Validation loss: 0.04052322, Gradient norm: 0.36533379
INFO:root:[   12] Training loss: 0.03073146, Validation loss: 0.04125294, Gradient norm: 0.31536508
INFO:root:[   13] Training loss: 0.03089788, Validation loss: 0.03645610, Gradient norm: 0.37476347
INFO:root:[   14] Training loss: 0.03109663, Validation loss: 0.03999206, Gradient norm: 0.37999439
INFO:root:[   15] Training loss: 0.02999912, Validation loss: 0.03743926, Gradient norm: 0.33767806
INFO:root:[   16] Training loss: 0.02984957, Validation loss: 0.04460785, Gradient norm: 0.36134084
INFO:root:[   17] Training loss: 0.02866548, Validation loss: 0.04102282, Gradient norm: 0.31920396
INFO:root:[   18] Training loss: 0.02883232, Validation loss: 0.04327820, Gradient norm: 0.35663403
INFO:root:[   19] Training loss: 0.02856514, Validation loss: 0.04232267, Gradient norm: 0.33698724
INFO:root:[   20] Training loss: 0.02863413, Validation loss: 0.04680730, Gradient norm: 0.37792486
INFO:root:[   21] Training loss: 0.02874502, Validation loss: 0.03814997, Gradient norm: 0.37992168
INFO:root:[   22] Training loss: 0.02806343, Validation loss: 0.04250652, Gradient norm: 0.34498819
INFO:root:[   23] Training loss: 0.02782192, Validation loss: 0.03692983, Gradient norm: 0.32346738
INFO:root:[   24] Training loss: 0.02843269, Validation loss: 0.03654200, Gradient norm: 0.34988017
INFO:root:[   25] Training loss: 0.02787634, Validation loss: 0.03587584, Gradient norm: 0.35779626
INFO:root:[   26] Training loss: 0.02678526, Validation loss: 0.03567181, Gradient norm: 0.30415694
INFO:root:[   27] Training loss: 0.02714264, Validation loss: 0.03702277, Gradient norm: 0.35871470
INFO:root:[   28] Training loss: 0.02573766, Validation loss: 0.03775568, Gradient norm: 0.28417207
INFO:root:[   29] Training loss: 0.02785643, Validation loss: 0.04373958, Gradient norm: 0.39124521
INFO:root:[   30] Training loss: 0.02666947, Validation loss: 0.04200991, Gradient norm: 0.32888933
INFO:root:[   31] Training loss: 0.02620275, Validation loss: 0.03865053, Gradient norm: 0.30643679
INFO:root:[   32] Training loss: 0.02699434, Validation loss: 0.03597736, Gradient norm: 0.31979358
INFO:root:[   33] Training loss: 0.02616005, Validation loss: 0.04414815, Gradient norm: 0.32661423
INFO:root:[   34] Training loss: 0.02603683, Validation loss: 0.03445430, Gradient norm: 0.32282630
INFO:root:[   35] Training loss: 0.02597475, Validation loss: 0.03472714, Gradient norm: 0.33571796
INFO:root:[   36] Training loss: 0.02714686, Validation loss: 0.03524566, Gradient norm: 0.36230109
INFO:root:[   37] Training loss: 0.02609600, Validation loss: 0.03884339, Gradient norm: 0.33860361
INFO:root:[   38] Training loss: 0.02708133, Validation loss: 0.04215352, Gradient norm: 0.40666346
INFO:root:[   39] Training loss: 0.02581410, Validation loss: 0.03595328, Gradient norm: 0.32873073
INFO:root:[   40] Training loss: 0.02607197, Validation loss: 0.04056117, Gradient norm: 0.33477321
INFO:root:[   41] Training loss: 0.02565120, Validation loss: 0.03810649, Gradient norm: 0.33176206
INFO:root:[   42] Training loss: 0.02524356, Validation loss: 0.04249347, Gradient norm: 0.30077826
INFO:root:[   43] Training loss: 0.02480830, Validation loss: 0.03424067, Gradient norm: 0.26248544
INFO:root:[   44] Training loss: 0.02558678, Validation loss: 0.04311442, Gradient norm: 0.33849797
INFO:root:[   45] Training loss: 0.02632411, Validation loss: 0.03507290, Gradient norm: 0.37483076
INFO:root:[   46] Training loss: 0.02539421, Validation loss: 0.03939773, Gradient norm: 0.33260605
INFO:root:[   47] Training loss: 0.02532275, Validation loss: 0.03982473, Gradient norm: 0.29138688
INFO:root:[   48] Training loss: 0.02521702, Validation loss: 0.03551447, Gradient norm: 0.33127911
INFO:root:[   49] Training loss: 0.02582233, Validation loss: 0.03458623, Gradient norm: 0.34612459
INFO:root:[   50] Training loss: 0.02458802, Validation loss: 0.03745686, Gradient norm: 0.28651750
INFO:root:[   51] Training loss: 0.02508134, Validation loss: 0.03896316, Gradient norm: 0.30964291
INFO:root:[   52] Training loss: 0.02441108, Validation loss: 0.03969866, Gradient norm: 0.28815095
INFO:root:[   53] Training loss: 0.02554143, Validation loss: 0.04672863, Gradient norm: 0.36816854
INFO:root:[   54] Training loss: 0.02531426, Validation loss: 0.04014770, Gradient norm: 0.34494591
INFO:root:[   55] Training loss: 0.02456513, Validation loss: 0.03776715, Gradient norm: 0.29119487
INFO:root:[   56] Training loss: 0.02430357, Validation loss: 0.03754046, Gradient norm: 0.27826797
INFO:root:[   57] Training loss: 0.02506378, Validation loss: 0.03899099, Gradient norm: 0.33812929
INFO:root:[   58] Training loss: 0.02485938, Validation loss: 0.03700341, Gradient norm: 0.31583502
INFO:root:[   59] Training loss: 0.02458576, Validation loss: 0.04452884, Gradient norm: 0.30312393
INFO:root:[   60] Training loss: 0.02447253, Validation loss: 0.04750247, Gradient norm: 0.28753911
INFO:root:[   61] Training loss: 0.02453383, Validation loss: 0.04425163, Gradient norm: 0.30080513
INFO:root:[   62] Training loss: 0.02366219, Validation loss: 0.03732812, Gradient norm: 0.24321092
INFO:root:[   63] Training loss: 0.02465881, Validation loss: 0.03954840, Gradient norm: 0.30522810
INFO:root:[   64] Training loss: 0.02429504, Validation loss: 0.03566814, Gradient norm: 0.28898110
INFO:root:[   65] Training loss: 0.02428725, Validation loss: 0.03846933, Gradient norm: 0.29699372
INFO:root:[   66] Training loss: 0.02547747, Validation loss: 0.04225693, Gradient norm: 0.39372722
INFO:root:[   67] Training loss: 0.02418326, Validation loss: 0.03391107, Gradient norm: 0.29531997
INFO:root:[   68] Training loss: 0.02387167, Validation loss: 0.03600015, Gradient norm: 0.25378364
INFO:root:[   69] Training loss: 0.02414433, Validation loss: 0.03823762, Gradient norm: 0.30208789
INFO:root:[   70] Training loss: 0.02377604, Validation loss: 0.03827582, Gradient norm: 0.29520953
INFO:root:[   71] Training loss: 0.02489751, Validation loss: 0.03995224, Gradient norm: 0.32296194
INFO:root:[   72] Training loss: 0.02455073, Validation loss: 0.03694828, Gradient norm: 0.32042339
INFO:root:[   73] Training loss: 0.02366816, Validation loss: 0.04178465, Gradient norm: 0.26386637
INFO:root:[   74] Training loss: 0.02441491, Validation loss: 0.03583986, Gradient norm: 0.32530950
INFO:root:[   75] Training loss: 0.02407533, Validation loss: 0.03726928, Gradient norm: 0.30910395
INFO:root:[   76] Training loss: 0.02372428, Validation loss: 0.03895604, Gradient norm: 0.28001366
INFO:root:EP 76: Early stopping
INFO:root:Training the model took 2329.631s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01802
INFO:root:EnergyScoreTrain: 0.01322
INFO:root:CoverageTrain: 0.92341
INFO:root:IntervalWidthTrain: 0.06358
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02959
INFO:root:EnergyScoreValidation: 0.02323
INFO:root:CoverageValidation: 0.5967
INFO:root:IntervalWidthValidation: 0.05129
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03061
INFO:root:EnergyScoreTest: 0.02419
INFO:root:CoverageTest: 0.58607
INFO:root:IntervalWidthTest: 0.05117
INFO:root:###24 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 58720256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07717408, Validation loss: 0.06151180, Gradient norm: 0.53425797
INFO:root:[    2] Training loss: 0.05615514, Validation loss: 0.04812041, Gradient norm: 0.40486823
INFO:root:[    3] Training loss: 0.04999193, Validation loss: 0.04633387, Gradient norm: 0.43859064
INFO:root:[    4] Training loss: 0.04635678, Validation loss: 0.04464047, Gradient norm: 0.37767178
INFO:root:[    5] Training loss: 0.04291605, Validation loss: 0.04679411, Gradient norm: 0.33738662
INFO:root:[    6] Training loss: 0.04028940, Validation loss: 0.04474469, Gradient norm: 0.29035324
INFO:root:[    7] Training loss: 0.04096286, Validation loss: 0.04212363, Gradient norm: 0.35718123
INFO:root:[    8] Training loss: 0.03930560, Validation loss: 0.04338831, Gradient norm: 0.34707514
INFO:root:[    9] Training loss: 0.03957907, Validation loss: 0.04641014, Gradient norm: 0.41688733
INFO:root:[   10] Training loss: 0.03595967, Validation loss: 0.04331706, Gradient norm: 0.28828450
INFO:root:[   11] Training loss: 0.03693606, Validation loss: 0.03910844, Gradient norm: 0.40249767
INFO:root:[   12] Training loss: 0.03499493, Validation loss: 0.04302644, Gradient norm: 0.34781854
INFO:root:[   13] Training loss: 0.03526823, Validation loss: 0.04369247, Gradient norm: 0.35456626
INFO:root:[   14] Training loss: 0.03465134, Validation loss: 0.04916908, Gradient norm: 0.30339022
INFO:root:[   15] Training loss: 0.03488777, Validation loss: 0.04469004, Gradient norm: 0.33552405
INFO:root:[   16] Training loss: 0.03325866, Validation loss: 0.04321402, Gradient norm: 0.31009705
INFO:root:[   17] Training loss: 0.03415642, Validation loss: 0.04930874, Gradient norm: 0.34758063
INFO:root:[   18] Training loss: 0.03398331, Validation loss: 0.03955062, Gradient norm: 0.34890227
INFO:root:[   19] Training loss: 0.03190096, Validation loss: 0.04275351, Gradient norm: 0.26273939
INFO:root:[   20] Training loss: 0.03308722, Validation loss: 0.05409920, Gradient norm: 0.31947655
INFO:root:[   21] Training loss: 0.03358574, Validation loss: 0.04677826, Gradient norm: 0.34810082
INFO:root:[   22] Training loss: 0.03139500, Validation loss: 0.03821515, Gradient norm: 0.24466638
INFO:root:[   23] Training loss: 0.03358081, Validation loss: 0.03669959, Gradient norm: 0.37165040
INFO:root:[   24] Training loss: 0.03104696, Validation loss: 0.04615371, Gradient norm: 0.25347702
INFO:root:[   25] Training loss: 0.03195955, Validation loss: 0.04994756, Gradient norm: 0.34363310
INFO:root:[   26] Training loss: 0.03174411, Validation loss: 0.04516599, Gradient norm: 0.29296784
INFO:root:[   27] Training loss: 0.03106775, Validation loss: 0.04607415, Gradient norm: 0.26301945
INFO:root:[   28] Training loss: 0.03258919, Validation loss: 0.05097001, Gradient norm: 0.38303552
INFO:root:[   29] Training loss: 0.03136723, Validation loss: 0.03818374, Gradient norm: 0.30693947
INFO:root:[   30] Training loss: 0.03012585, Validation loss: 0.04572536, Gradient norm: 0.25601021
INFO:root:[   31] Training loss: 0.03059177, Validation loss: 0.04110559, Gradient norm: 0.30882815
