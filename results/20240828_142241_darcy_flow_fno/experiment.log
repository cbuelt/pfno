INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno_srr.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Starting the logger.
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno_laplace.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:Training parameters: {'seed': 1, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 4194304
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.24213845, Validation loss: 0.18548126, Gradient norm: 3.56904573
INFO:root:[    1] Training loss: 0.28904687, Validation loss: 0.11849912, Gradient norm: 3.12887401
INFO:root:[    2] Training loss: 0.12623191, Validation loss: 0.17556398, Gradient norm: 3.71788937
INFO:root:[    2] Training loss: 0.12694979, Validation loss: 0.12536167, Gradient norm: 2.77982840
INFO:root:[    3] Training loss: 0.11650277, Validation loss: 0.15316167, Gradient norm: 4.06183105
INFO:root:[    3] Training loss: 0.10650456, Validation loss: 0.10378422, Gradient norm: 2.67951688
INFO:root:[    4] Training loss: 0.09546900, Validation loss: 0.13524344, Gradient norm: 3.74230482
INFO:root:[    4] Training loss: 0.09537110, Validation loss: 0.08453483, Gradient norm: 2.61824809
INFO:root:[    5] Training loss: 0.08915696, Validation loss: 0.12838778, Gradient norm: 3.45540680
INFO:root:[    5] Training loss: 0.08133256, Validation loss: 0.08895847, Gradient norm: 1.97448875
INFO:root:[    6] Training loss: 0.08413389, Validation loss: 0.11073760, Gradient norm: 3.29545325
INFO:root:[    6] Training loss: 0.08251494, Validation loss: 0.08859267, Gradient norm: 2.45112865
INFO:root:[    7] Training loss: 0.07967565, Validation loss: 0.11766948, Gradient norm: 3.56641811
INFO:root:[    7] Training loss: 0.07813705, Validation loss: 0.07724085, Gradient norm: 2.30387500
INFO:root:[    8] Training loss: 0.07537277, Validation loss: 0.10460217, Gradient norm: 3.34726690
INFO:root:[    8] Training loss: 0.07406648, Validation loss: 0.07878723, Gradient norm: 2.20608996
INFO:root:[    9] Training loss: 0.07217751, Validation loss: 0.11046668, Gradient norm: 2.89755173
INFO:root:[    9] Training loss: 0.07103575, Validation loss: 0.08221719, Gradient norm: 2.17957143
INFO:root:[   10] Training loss: 0.07283662, Validation loss: 0.10603441, Gradient norm: 3.49579265
INFO:root:[   10] Training loss: 0.06774091, Validation loss: 0.07126868, Gradient norm: 2.08431684
INFO:root:[   11] Training loss: 0.07079870, Validation loss: 0.10322953, Gradient norm: 3.20042219
INFO:root:[   11] Training loss: 0.06632681, Validation loss: 0.07865120, Gradient norm: 2.05595162
INFO:root:[   12] Training loss: 0.06931823, Validation loss: 0.09542678, Gradient norm: 3.23060274
INFO:root:[   12] Training loss: 0.06427604, Validation loss: 0.07238709, Gradient norm: 2.06080018
INFO:root:[   13] Training loss: 0.06779675, Validation loss: 0.09094517, Gradient norm: 3.37336730
INFO:root:[   13] Training loss: 0.06264978, Validation loss: 0.07187576, Gradient norm: 2.12269725
INFO:root:[   14] Training loss: 0.06814274, Validation loss: 0.09418128, Gradient norm: 3.23795514
INFO:root:[   14] Training loss: 0.06219316, Validation loss: 0.07443187, Gradient norm: 1.98970843
INFO:root:[   15] Training loss: 0.06639992, Validation loss: 0.09898575, Gradient norm: 3.10879354
INFO:root:[   15] Training loss: 0.06380701, Validation loss: 0.07849813, Gradient norm: 2.15932332
INFO:root:[   16] Training loss: 0.05985108, Validation loss: 0.08572558, Gradient norm: 2.76639114
INFO:root:[   16] Training loss: 0.06158919, Validation loss: 0.07753764, Gradient norm: 2.05704726
INFO:root:[   17] Training loss: 0.06047150, Validation loss: 0.09315720, Gradient norm: 2.72609648
INFO:root:[   17] Training loss: 0.06043288, Validation loss: 0.07049609, Gradient norm: 2.06079929
INFO:root:[   18] Training loss: 0.06220348, Validation loss: 0.09041876, Gradient norm: 3.04270163
INFO:root:[   18] Training loss: 0.05759309, Validation loss: 0.06819005, Gradient norm: 1.73092389
INFO:root:[   19] Training loss: 0.05624633, Validation loss: 0.08501007, Gradient norm: 2.50760139
INFO:root:[   19] Training loss: 0.05614578, Validation loss: 0.07597392, Gradient norm: 1.94110650
INFO:root:[   20] Training loss: 0.05899416, Validation loss: 0.08575482, Gradient norm: 3.02472234
INFO:root:[   20] Training loss: 0.06016048, Validation loss: 0.06964259, Gradient norm: 1.80693471
INFO:root:[   21] Training loss: 0.05783191, Validation loss: 0.09134638, Gradient norm: 2.93892470
INFO:root:[   21] Training loss: 0.05687513, Validation loss: 0.07464778, Gradient norm: 1.94499504
INFO:root:[   22] Training loss: 0.05461025, Validation loss: 0.07920772, Gradient norm: 2.67060920
INFO:root:[   22] Training loss: 0.05515059, Validation loss: 0.07926612, Gradient norm: 2.12087281
INFO:root:[   23] Training loss: 0.05752071, Validation loss: 0.09618637, Gradient norm: 2.88242813
INFO:root:[   23] Training loss: 0.05244268, Validation loss: 0.06983006, Gradient norm: 1.76680197
INFO:root:[   24] Training loss: 0.05525935, Validation loss: 0.09360578, Gradient norm: 2.93896286
INFO:root:[   24] Training loss: 0.05594126, Validation loss: 0.07453725, Gradient norm: 2.11629520
INFO:root:[   25] Training loss: 0.05695094, Validation loss: 0.08931364, Gradient norm: 3.05742328
INFO:root:[   25] Training loss: 0.05351800, Validation loss: 0.07503209, Gradient norm: 1.95554743
INFO:root:[   26] Training loss: 0.05191344, Validation loss: 0.10027813, Gradient norm: 2.45302155
INFO:root:[   26] Training loss: 0.05259835, Validation loss: 0.06578685, Gradient norm: 2.02446030
INFO:root:[   27] Training loss: 0.05312802, Validation loss: 0.08942324, Gradient norm: 2.58224976
INFO:root:[   27] Training loss: 0.04948763, Validation loss: 0.07039272, Gradient norm: 1.71806750
INFO:root:[   28] Training loss: 0.05141155, Validation loss: 0.08727484, Gradient norm: 2.40124502
INFO:root:[   28] Training loss: 0.05123310, Validation loss: 0.08606391, Gradient norm: 1.86107769
INFO:root:[   29] Training loss: 0.05260265, Validation loss: 0.09165607, Gradient norm: 2.89623077
INFO:root:[   29] Training loss: 0.05022058, Validation loss: 0.07243161, Gradient norm: 1.94976319
INFO:root:[   30] Training loss: 0.05244415, Validation loss: 0.08735956, Gradient norm: 2.53740270
INFO:root:[   30] Training loss: 0.05137637, Validation loss: 0.07206075, Gradient norm: 1.88038782
INFO:root:[   31] Training loss: 0.05294920, Validation loss: 0.08399224, Gradient norm: 3.02977809
INFO:root:[   31] Training loss: 0.05028307, Validation loss: 0.07349616, Gradient norm: 1.98777412
INFO:root:[   32] Training loss: 0.05299039, Validation loss: 0.08025471, Gradient norm: 2.80764104
INFO:root:[   32] Training loss: 0.04976464, Validation loss: 0.07590810, Gradient norm: 1.79389315
INFO:root:[   33] Training loss: 0.04893885, Validation loss: 0.08544738, Gradient norm: 2.49480503
INFO:root:[   33] Training loss: 0.04958366, Validation loss: 0.07098979, Gradient norm: 2.00451383
INFO:root:[   34] Training loss: 0.05096816, Validation loss: 0.07908027, Gradient norm: 2.87533056
INFO:root:[   34] Training loss: 0.04912716, Validation loss: 0.08094648, Gradient norm: 2.00693327
INFO:root:[   35] Training loss: 0.04866901, Validation loss: 0.07978017, Gradient norm: 2.42779292
INFO:root:[   36] Training loss: 0.04888668, Validation loss: 0.09126811, Gradient norm: 2.52193066
INFO:root:[   35] Training loss: 0.04924057, Validation loss: 0.06613558, Gradient norm: 2.04401303
INFO:root:[   37] Training loss: 0.04533540, Validation loss: 0.08941836, Gradient norm: 2.29695349
INFO:root:[   36] Training loss: 0.04746715, Validation loss: 0.06997371, Gradient norm: 1.75249297
INFO:root:[   38] Training loss: 0.04795066, Validation loss: 0.08307926, Gradient norm: 2.68013617
INFO:root:[   37] Training loss: 0.04570462, Validation loss: 0.07790991, Gradient norm: 1.76211646
INFO:root:[   39] Training loss: 0.04821485, Validation loss: 0.09146586, Gradient norm: 2.43433531
INFO:root:[   38] Training loss: 0.04739730, Validation loss: 0.07347768, Gradient norm: 1.89547175
INFO:root:[   40] Training loss: 0.04688929, Validation loss: 0.08774833, Gradient norm: 2.62148049
INFO:root:[   39] Training loss: 0.04387019, Validation loss: 0.07013443, Gradient norm: 1.70597053
INFO:root:[   41] Training loss: 0.04963186, Validation loss: 0.07413154, Gradient norm: 2.68921927
INFO:root:[   40] Training loss: 0.04631811, Validation loss: 0.07783842, Gradient norm: 1.88568780
INFO:root:[   42] Training loss: 0.04439365, Validation loss: 0.08136691, Gradient norm: 2.20912153
INFO:root:[   41] Training loss: 0.04634571, Validation loss: 0.07256946, Gradient norm: 1.69220914
INFO:root:[   43] Training loss: 0.04692795, Validation loss: 0.09127044, Gradient norm: 2.78576918
INFO:root:[   42] Training loss: 0.04411863, Validation loss: 0.07014433, Gradient norm: 1.86635425
INFO:root:[   44] Training loss: 0.04569382, Validation loss: 0.07558051, Gradient norm: 2.41060324
INFO:root:[   43] Training loss: 0.04767306, Validation loss: 0.08620273, Gradient norm: 1.96364740
INFO:root:[   45] Training loss: 0.04666712, Validation loss: 0.08260850, Gradient norm: 2.29312377
INFO:root:[   44] Training loss: 0.04538783, Validation loss: 0.06943200, Gradient norm: 1.84234416
INFO:root:[   46] Training loss: 0.04460798, Validation loss: 0.07795792, Gradient norm: 2.54813113
INFO:root:[   45] Training loss: 0.04479904, Validation loss: 0.07304521, Gradient norm: 1.92471336
INFO:root:[   47] Training loss: 0.04563201, Validation loss: 0.07637240, Gradient norm: 2.66433269
INFO:root:[   46] Training loss: 0.04246294, Validation loss: 0.06913608, Gradient norm: 1.80149381
INFO:root:[   48] Training loss: 0.04238847, Validation loss: 0.07517976, Gradient norm: 2.40875044
INFO:root:[   47] Training loss: 0.04371989, Validation loss: 0.06710123, Gradient norm: 1.79766329
INFO:root:[   49] Training loss: 0.04524765, Validation loss: 0.07912810, Gradient norm: 2.49158293
INFO:root:[   48] Training loss: 0.04484384, Validation loss: 0.07780488, Gradient norm: 1.80337173
INFO:root:[   50] Training loss: 0.04326652, Validation loss: 0.08495614, Gradient norm: 2.46684256
INFO:root:[   49] Training loss: 0.04387663, Validation loss: 0.07349386, Gradient norm: 1.96198653
INFO:root:[   51] Training loss: 0.04214266, Validation loss: 0.08482364, Gradient norm: 2.38978474
INFO:root:[   50] Training loss: 0.04350413, Validation loss: 0.07464718, Gradient norm: 1.81598632
INFO:root:[   52] Training loss: 0.04304990, Validation loss: 0.09324477, Gradient norm: 2.37461930
INFO:root:[   51] Training loss: 0.04293936, Validation loss: 0.07688327, Gradient norm: 1.86385417
INFO:root:[   53] Training loss: 0.04176217, Validation loss: 0.09032494, Gradient norm: 2.51969095
INFO:root:[   52] Training loss: 0.04419672, Validation loss: 0.08446668, Gradient norm: 1.69294598
INFO:root:[   54] Training loss: 0.04141104, Validation loss: 0.08970874, Gradient norm: 2.27092277
INFO:root:[   53] Training loss: 0.04506679, Validation loss: 0.07859077, Gradient norm: 1.67147737
INFO:root:[   55] Training loss: 0.04123467, Validation loss: 0.08081021, Gradient norm: 2.28035930
INFO:root:[   54] Training loss: 0.04201683, Validation loss: 0.08313422, Gradient norm: 1.72157412
INFO:root:[   56] Training loss: 0.04124688, Validation loss: 0.07603714, Gradient norm: 2.56679449
INFO:root:[   55] Training loss: 0.04129352, Validation loss: 0.07252197, Gradient norm: 1.95073241
INFO:root:[   57] Training loss: 0.04106218, Validation loss: 0.08815424, Gradient norm: 2.51898713
INFO:root:[   56] Training loss: 0.04246604, Validation loss: 0.08560202, Gradient norm: 2.01065449
INFO:root:[   58] Training loss: 0.04072460, Validation loss: 0.08695782, Gradient norm: 2.69149181
INFO:root:[   57] Training loss: 0.04270370, Validation loss: 0.08485605, Gradient norm: 1.78095212
INFO:root:[   59] Training loss: 0.04059956, Validation loss: 0.08414527, Gradient norm: 2.28850228
INFO:root:[   58] Training loss: 0.03991035, Validation loss: 0.07363159, Gradient norm: 1.76035814
INFO:root:[   60] Training loss: 0.04122846, Validation loss: 0.09132617, Gradient norm: 2.48323759
INFO:root:[   61] Training loss: 0.04044678, Validation loss: 0.07901848, Gradient norm: 2.50391313
INFO:root:[   59] Training loss: 0.04131216, Validation loss: 0.08762371, Gradient norm: 1.81753760
INFO:root:[   62] Training loss: 0.03957210, Validation loss: 0.08331784, Gradient norm: 2.23498889
INFO:root:[   60] Training loss: 0.04192096, Validation loss: 0.07692253, Gradient norm: 1.58671585
INFO:root:[   63] Training loss: 0.03892415, Validation loss: 0.07936670, Gradient norm: 2.36976795
INFO:root:[   61] Training loss: 0.04150126, Validation loss: 0.08326605, Gradient norm: 1.73968126
INFO:root:[   64] Training loss: 0.03944326, Validation loss: 0.08207617, Gradient norm: 2.55769668
INFO:root:[   62] Training loss: 0.04061660, Validation loss: 0.08208178, Gradient norm: 1.88514405
INFO:root:[   65] Training loss: 0.03904921, Validation loss: 0.08019420, Gradient norm: 2.45043516
INFO:root:[   63] Training loss: 0.04184060, Validation loss: 0.07944098, Gradient norm: 1.70486099
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 2426.966s.
INFO:root:Emptying the cuda cache took 0.027s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:[   64] Training loss: 0.03998221, Validation loss: 0.07207042, Gradient norm: 1.89808903
INFO:root:[   65] Training loss: 0.03862599, Validation loss: 0.07359498, Gradient norm: 1.87528568
INFO:root:Evaluating the model on Train data.
INFO:root:[   66] Training loss: 0.04018673, Validation loss: 0.07301811, Gradient norm: 1.78197080
INFO:root:MSETrain: 0.203
INFO:root:EnergyScoreTrain: 0.10321
INFO:root:CoverageTrain: 0.73455
INFO:root:IntervalWidthTrain: 0.10325
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.22524
INFO:root:EnergyScoreValidation: 0.11601
INFO:root:CoverageValidation: 0.69591
INFO:root:IntervalWidthValidation: 0.1051
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21556
INFO:root:EnergyScoreTest: 0.10917
INFO:root:CoverageTest: 0.71191
INFO:root:IntervalWidthTest: 0.10036
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1247805440
INFO:root:Training starts now.
INFO:root:[   67] Training loss: 0.04048419, Validation loss: 0.08008063, Gradient norm: 1.99254532
INFO:root:[    1] Training loss: 0.25962591, Validation loss: 0.22121459, Gradient norm: 4.50476787
INFO:root:[   68] Training loss: 0.04044167, Validation loss: 0.08073601, Gradient norm: 1.86757731
INFO:root:[    2] Training loss: 0.13411824, Validation loss: 0.14915565, Gradient norm: 4.69816453
INFO:root:[   69] Training loss: 0.03969083, Validation loss: 0.08316721, Gradient norm: 1.97870926
INFO:root:[    3] Training loss: 0.10992271, Validation loss: 0.13586863, Gradient norm: 3.88724552
INFO:root:[   70] Training loss: 0.04069963, Validation loss: 0.07408015, Gradient norm: 1.97174713
INFO:root:[    4] Training loss: 0.10027673, Validation loss: 0.12730807, Gradient norm: 4.23024971
INFO:root:[   71] Training loss: 0.03923876, Validation loss: 0.07992087, Gradient norm: 1.84118866
INFO:root:[    5] Training loss: 0.09482019, Validation loss: 0.12783088, Gradient norm: 4.12134742
INFO:root:[   72] Training loss: 0.03998776, Validation loss: 0.07740953, Gradient norm: 1.91913420
INFO:root:[    6] Training loss: 0.08736536, Validation loss: 0.12462194, Gradient norm: 4.11901468
INFO:root:[   73] Training loss: 0.03924018, Validation loss: 0.07965582, Gradient norm: 1.93505784
INFO:root:[    7] Training loss: 0.08444784, Validation loss: 0.10222505, Gradient norm: 3.80163102
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 2805.195s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:[    8] Training loss: 0.07497516, Validation loss: 0.11610476, Gradient norm: 3.04060616
INFO:root:[    9] Training loss: 0.07312004, Validation loss: 0.09571857, Gradient norm: 3.38540742
INFO:root:MSETrain: 0.04707
INFO:root:EnergyScoreTrain: 0.03468
INFO:root:CoverageTrain: 0.85851
INFO:root:IntervalWidthTrain: 0.03581
INFO:root:Evaluating the model on Validation data.
INFO:root:[   10] Training loss: 0.07163597, Validation loss: 0.10774797, Gradient norm: 3.41271912
INFO:root:MSEValidation: 0.08987
INFO:root:EnergyScoreValidation: 0.06623
INFO:root:CoverageValidation: 0.63419
INFO:root:IntervalWidthValidation: 0.03481
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0909
INFO:root:EnergyScoreTest: 0.06716
INFO:root:CoverageTest: 0.62886
INFO:root:IntervalWidthTest: 0.03489
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[   11] Training loss: 0.07052711, Validation loss: 0.10577209, Gradient norm: 3.41270967
INFO:root:[    1] Training loss: 0.31570366, Validation loss: 0.19689974, Gradient norm: 3.39895786
INFO:root:[   12] Training loss: 0.06742972, Validation loss: 0.09487803, Gradient norm: 3.30732035
INFO:root:[    2] Training loss: 0.13737231, Validation loss: 0.10934154, Gradient norm: 3.23610452
INFO:root:[   13] Training loss: 0.06534440, Validation loss: 0.08548147, Gradient norm: 3.23594989
INFO:root:[    3] Training loss: 0.10775464, Validation loss: 0.11095645, Gradient norm: 2.66726384
INFO:root:[   14] Training loss: 0.06818196, Validation loss: 0.08077164, Gradient norm: 3.55838014
INFO:root:[    4] Training loss: 0.08997906, Validation loss: 0.09744760, Gradient norm: 2.19849058
INFO:root:[   15] Training loss: 0.05970015, Validation loss: 0.10603204, Gradient norm: 2.75516213
INFO:root:[    5] Training loss: 0.08589843, Validation loss: 0.09135735, Gradient norm: 2.53271650
INFO:root:[   16] Training loss: 0.06323418, Validation loss: 0.08321732, Gradient norm: 3.26706642
INFO:root:[    6] Training loss: 0.08385692, Validation loss: 0.10593926, Gradient norm: 2.66456165
INFO:root:[   17] Training loss: 0.05876894, Validation loss: 0.09683599, Gradient norm: 2.80415573
INFO:root:[    7] Training loss: 0.07749384, Validation loss: 0.07337718, Gradient norm: 2.34133504
INFO:root:[   18] Training loss: 0.06157965, Validation loss: 0.07820120, Gradient norm: 3.29106692
INFO:root:[    8] Training loss: 0.07212917, Validation loss: 0.07337189, Gradient norm: 2.13741627
INFO:root:[   19] Training loss: 0.05649912, Validation loss: 0.07543850, Gradient norm: 2.76150078
INFO:root:[    9] Training loss: 0.07032829, Validation loss: 0.07252756, Gradient norm: 2.25826934
INFO:root:[   20] Training loss: 0.05586241, Validation loss: 0.08137362, Gradient norm: 2.87323469
INFO:root:[   10] Training loss: 0.06794091, Validation loss: 0.07142458, Gradient norm: 2.12919878
INFO:root:[   21] Training loss: 0.05516869, Validation loss: 0.09313765, Gradient norm: 2.55767213
INFO:root:[   11] Training loss: 0.06602194, Validation loss: 0.07899552, Gradient norm: 2.12589300
INFO:root:[   22] Training loss: 0.05452941, Validation loss: 0.07452118, Gradient norm: 2.57288462
INFO:root:[   12] Training loss: 0.06578615, Validation loss: 0.07915708, Gradient norm: 2.18581356
INFO:root:[   23] Training loss: 0.05545460, Validation loss: 0.08103366, Gradient norm: 2.96092372
INFO:root:[   24] Training loss: 0.05288631, Validation loss: 0.07416093, Gradient norm: 2.70155655
INFO:root:[   13] Training loss: 0.06505361, Validation loss: 0.08046210, Gradient norm: 2.06945990
INFO:root:[   25] Training loss: 0.05189291, Validation loss: 0.09360535, Gradient norm: 2.59170783
INFO:root:[   14] Training loss: 0.06502806, Validation loss: 0.07973589, Gradient norm: 2.09859140
INFO:root:[   26] Training loss: 0.05430186, Validation loss: 0.07473797, Gradient norm: 2.90127224
INFO:root:[   15] Training loss: 0.06338590, Validation loss: 0.07314757, Gradient norm: 2.11876899
INFO:root:[   27] Training loss: 0.05510576, Validation loss: 0.09045457, Gradient norm: 2.85288988
INFO:root:[   16] Training loss: 0.06192909, Validation loss: 0.07718652, Gradient norm: 2.15455982
INFO:root:[   28] Training loss: 0.05379642, Validation loss: 0.09352426, Gradient norm: 3.02943316
INFO:root:[   17] Training loss: 0.05978757, Validation loss: 0.10071779, Gradient norm: 2.05465021
INFO:root:[   29] Training loss: 0.04941944, Validation loss: 0.08992875, Gradient norm: 2.62821511
INFO:root:[   18] Training loss: 0.06043657, Validation loss: 0.08891524, Gradient norm: 1.99113808
INFO:root:[   30] Training loss: 0.04872917, Validation loss: 0.07008776, Gradient norm: 2.62538231
INFO:root:[   19] Training loss: 0.05715062, Validation loss: 0.07523959, Gradient norm: 2.01752508
INFO:root:[   31] Training loss: 0.05096069, Validation loss: 0.08977680, Gradient norm: 2.93779711
INFO:root:[   20] Training loss: 0.05498566, Validation loss: 0.07267123, Gradient norm: 1.79149383
INFO:root:[   32] Training loss: 0.05003769, Validation loss: 0.06982965, Gradient norm: 2.78764199
INFO:root:[   21] Training loss: 0.05749318, Validation loss: 0.08245284, Gradient norm: 2.17750822
INFO:root:[   33] Training loss: 0.04889280, Validation loss: 0.07817520, Gradient norm: 2.52345701
INFO:root:[   22] Training loss: 0.05646937, Validation loss: 0.07657065, Gradient norm: 2.09055131
INFO:root:[   34] Training loss: 0.05133097, Validation loss: 0.08895365, Gradient norm: 2.86963799
INFO:root:[   23] Training loss: 0.05478844, Validation loss: 0.07801953, Gradient norm: 1.82274808
INFO:root:[   35] Training loss: 0.04806511, Validation loss: 0.07004293, Gradient norm: 2.82610459
INFO:root:[   24] Training loss: 0.05482594, Validation loss: 0.07485754, Gradient norm: 2.00063723
INFO:root:[   36] Training loss: 0.04660876, Validation loss: 0.07795725, Gradient norm: 2.55873301
INFO:root:[   25] Training loss: 0.05434026, Validation loss: 0.07502445, Gradient norm: 2.02722550
INFO:root:[   37] Training loss: 0.04692697, Validation loss: 0.07877591, Gradient norm: 2.66685708
INFO:root:[   26] Training loss: 0.05212935, Validation loss: 0.09053026, Gradient norm: 1.81298048
INFO:root:[   38] Training loss: 0.04608511, Validation loss: 0.07244961, Gradient norm: 2.48563693
INFO:root:[   27] Training loss: 0.05059925, Validation loss: 0.08241203, Gradient norm: 1.72119428
INFO:root:[   39] Training loss: 0.04525102, Validation loss: 0.07908719, Gradient norm: 2.37314933
INFO:root:[   28] Training loss: 0.05182208, Validation loss: 0.08270060, Gradient norm: 1.76712675
INFO:root:[   40] Training loss: 0.04390105, Validation loss: 0.06711956, Gradient norm: 2.26428281
INFO:root:[   29] Training loss: 0.05071009, Validation loss: 0.07217525, Gradient norm: 1.97066611
INFO:root:[   41] Training loss: 0.04527065, Validation loss: 0.07318092, Gradient norm: 2.35504694
INFO:root:[   30] Training loss: 0.04948706, Validation loss: 0.07138822, Gradient norm: 1.80095896
INFO:root:[   42] Training loss: 0.04359100, Validation loss: 0.06615335, Gradient norm: 2.29246662
INFO:root:[   31] Training loss: 0.05027780, Validation loss: 0.07224617, Gradient norm: 2.03969620
INFO:root:[   43] Training loss: 0.04428047, Validation loss: 0.08197955, Gradient norm: 2.50122898
INFO:root:[   32] Training loss: 0.04942372, Validation loss: 0.06852301, Gradient norm: 1.90213013
INFO:root:[   44] Training loss: 0.04462097, Validation loss: 0.07236404, Gradient norm: 2.73075261
INFO:root:[   33] Training loss: 0.04900719, Validation loss: 0.07347313, Gradient norm: 1.98786167
INFO:root:[   45] Training loss: 0.04716412, Validation loss: 0.07144573, Gradient norm: 2.67549302
INFO:root:[   34] Training loss: 0.04895204, Validation loss: 0.06925728, Gradient norm: 1.86039862
INFO:root:[   46] Training loss: 0.04359005, Validation loss: 0.07702066, Gradient norm: 2.71811899
INFO:root:[   35] Training loss: 0.04859804, Validation loss: 0.08120904, Gradient norm: 1.85932803
INFO:root:[   47] Training loss: 0.04443241, Validation loss: 0.07155818, Gradient norm: 2.78261497
INFO:root:[   36] Training loss: 0.05020176, Validation loss: 0.08473988, Gradient norm: 1.65368120
INFO:root:[   48] Training loss: 0.04235797, Validation loss: 0.06707804, Gradient norm: 2.52140802
INFO:root:[   37] Training loss: 0.04986975, Validation loss: 0.06398438, Gradient norm: 1.96468790
INFO:root:[   49] Training loss: 0.04272020, Validation loss: 0.06291479, Gradient norm: 2.21167956
INFO:root:[   38] Training loss: 0.04786802, Validation loss: 0.06509929, Gradient norm: 1.93826431
INFO:root:[   50] Training loss: 0.04337587, Validation loss: 0.06314620, Gradient norm: 2.63443193
INFO:root:[   51] Training loss: 0.03981462, Validation loss: 0.07336892, Gradient norm: 2.34987003
INFO:root:[   39] Training loss: 0.04631798, Validation loss: 0.07326121, Gradient norm: 1.96702737
INFO:root:[   52] Training loss: 0.04025711, Validation loss: 0.08676904, Gradient norm: 2.39799804
INFO:root:[   40] Training loss: 0.04609120, Validation loss: 0.08054716, Gradient norm: 1.85975434
INFO:root:[   53] Training loss: 0.04466644, Validation loss: 0.07523980, Gradient norm: 2.69284666
INFO:root:[   41] Training loss: 0.04375706, Validation loss: 0.06927116, Gradient norm: 1.75245272
INFO:root:[   54] Training loss: 0.04163049, Validation loss: 0.07505239, Gradient norm: 2.80250240
INFO:root:[   42] Training loss: 0.04600611, Validation loss: 0.07481671, Gradient norm: 1.89956354
INFO:root:[   55] Training loss: 0.04037035, Validation loss: 0.06821949, Gradient norm: 2.30945705
INFO:root:[   43] Training loss: 0.04797819, Validation loss: 0.08415891, Gradient norm: 2.00481910
INFO:root:[   56] Training loss: 0.04099164, Validation loss: 0.07456159, Gradient norm: 2.49525116
INFO:root:[   44] Training loss: 0.04624182, Validation loss: 0.07085732, Gradient norm: 1.98953715
INFO:root:[   57] Training loss: 0.04058608, Validation loss: 0.06700762, Gradient norm: 2.61168226
INFO:root:[   45] Training loss: 0.04355784, Validation loss: 0.07254829, Gradient norm: 1.86489599
INFO:root:[   58] Training loss: 0.04094464, Validation loss: 0.08926426, Gradient norm: 2.54135943
INFO:root:[   46] Training loss: 0.04565244, Validation loss: 0.07425321, Gradient norm: 1.82471415
INFO:root:[   59] Training loss: 0.04127148, Validation loss: 0.06698722, Gradient norm: 2.62191151
INFO:root:[   47] Training loss: 0.04567736, Validation loss: 0.08586729, Gradient norm: 1.68321738
INFO:root:[   60] Training loss: 0.04208479, Validation loss: 0.06617421, Gradient norm: 2.81780090
INFO:root:[   48] Training loss: 0.04316717, Validation loss: 0.07240342, Gradient norm: 1.85462917
INFO:root:[   61] Training loss: 0.03918443, Validation loss: 0.06364413, Gradient norm: 2.44682366
INFO:root:[   49] Training loss: 0.04386139, Validation loss: 0.08141907, Gradient norm: 1.99731238
INFO:root:[   62] Training loss: 0.04077050, Validation loss: 0.06755083, Gradient norm: 2.84614263
INFO:root:[   50] Training loss: 0.04506863, Validation loss: 0.07734931, Gradient norm: 1.54048466
INFO:root:[   63] Training loss: 0.03755143, Validation loss: 0.06518169, Gradient norm: 2.50180388
INFO:root:[   51] Training loss: 0.04391913, Validation loss: 0.07484671, Gradient norm: 2.09300745
INFO:root:[   64] Training loss: 0.03921788, Validation loss: 0.06461463, Gradient norm: 2.47545976
INFO:root:[   52] Training loss: 0.04210207, Validation loss: 0.07209000, Gradient norm: 1.96467495
INFO:root:[   65] Training loss: 0.03902461, Validation loss: 0.07146102, Gradient norm: 2.71075645
INFO:root:[   53] Training loss: 0.04345999, Validation loss: 0.08090690, Gradient norm: 1.96736821
INFO:root:[   66] Training loss: 0.03821084, Validation loss: 0.07486974, Gradient norm: 2.53995481
INFO:root:[   54] Training loss: 0.04272558, Validation loss: 0.07598925, Gradient norm: 2.00663566
INFO:root:[   67] Training loss: 0.04060736, Validation loss: 0.08071316, Gradient norm: 2.43809342
INFO:root:[   55] Training loss: 0.04233256, Validation loss: 0.07080173, Gradient norm: 1.78900089
INFO:root:[   68] Training loss: 0.03766398, Validation loss: 0.07909904, Gradient norm: 2.35762259
INFO:root:[   56] Training loss: 0.04341897, Validation loss: 0.07202103, Gradient norm: 1.93745865
INFO:root:[   69] Training loss: 0.03786973, Validation loss: 0.06294185, Gradient norm: 2.49484460
INFO:root:[   57] Training loss: 0.04141849, Validation loss: 0.07682095, Gradient norm: 1.81567900
INFO:root:[   70] Training loss: 0.03731277, Validation loss: 0.06278746, Gradient norm: 2.53429755
INFO:root:[   58] Training loss: 0.04324282, Validation loss: 0.08047728, Gradient norm: 2.00372659
INFO:root:[   71] Training loss: 0.03698228, Validation loss: 0.08231503, Gradient norm: 2.73457950
INFO:root:[   59] Training loss: 0.04221464, Validation loss: 0.06956631, Gradient norm: 1.87196640
INFO:root:[   72] Training loss: 0.03721881, Validation loss: 0.06817679, Gradient norm: 2.52158975
INFO:root:[   60] Training loss: 0.04291946, Validation loss: 0.07509582, Gradient norm: 1.87052677
INFO:root:[   73] Training loss: 0.03727846, Validation loss: 0.08310957, Gradient norm: 2.42340994
INFO:root:[   61] Training loss: 0.04137718, Validation loss: 0.06841551, Gradient norm: 1.66164526
INFO:root:[   74] Training loss: 0.03660956, Validation loss: 0.06203037, Gradient norm: 2.36549985
INFO:root:[   62] Training loss: 0.04128081, Validation loss: 0.07274602, Gradient norm: 2.00331634
INFO:root:[   75] Training loss: 0.03554387, Validation loss: 0.06799754, Gradient norm: 2.45288283
INFO:root:[   63] Training loss: 0.04236281, Validation loss: 0.07105441, Gradient norm: 1.81117603
INFO:root:[   76] Training loss: 0.03650988, Validation loss: 0.07599507, Gradient norm: 2.39849446
INFO:root:[   64] Training loss: 0.03891226, Validation loss: 0.08649292, Gradient norm: 1.80175360
INFO:root:[   77] Training loss: 0.03614453, Validation loss: 0.06681303, Gradient norm: 2.38316410
INFO:root:[   65] Training loss: 0.04077713, Validation loss: 0.08591520, Gradient norm: 2.00865979
INFO:root:[   78] Training loss: 0.03675158, Validation loss: 0.06515489, Gradient norm: 2.40181919
INFO:root:[   79] Training loss: 0.03442395, Validation loss: 0.06293237, Gradient norm: 2.47464705
INFO:root:[   66] Training loss: 0.04176771, Validation loss: 0.07616695, Gradient norm: 1.81752407
INFO:root:[   80] Training loss: 0.03618614, Validation loss: 0.06510684, Gradient norm: 2.88836596
INFO:root:[   67] Training loss: 0.04080354, Validation loss: 0.07993104, Gradient norm: 1.89206696
INFO:root:[   81] Training loss: 0.03581893, Validation loss: 0.07486653, Gradient norm: 2.63433132
INFO:root:[   68] Training loss: 0.04066639, Validation loss: 0.07728608, Gradient norm: 1.93186628
INFO:root:[   82] Training loss: 0.03571641, Validation loss: 0.06529383, Gradient norm: 2.39274391
INFO:root:[   69] Training loss: 0.04045213, Validation loss: 0.07675003, Gradient norm: 1.89369411
INFO:root:[   83] Training loss: 0.03489674, Validation loss: 0.07104426, Gradient norm: 2.81369695
INFO:root:[   70] Training loss: 0.04036913, Validation loss: 0.08839128, Gradient norm: 1.91151704
INFO:root:EP 83: Early stopping
INFO:root:Training the model took 2932.666s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 2586.08s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05357
INFO:root:EnergyScoreTrain: 0.03865
INFO:root:CoverageTrain: 0.80897
INFO:root:IntervalWidthTrain: 0.03224
INFO:root:Evaluating the model on Validation data.
INFO:root:MSETrain: 0.20782
INFO:root:EnergyScoreTrain: 0.10926
INFO:root:CoverageTrain: 0.73343
INFO:root:IntervalWidthTrain: 0.10838
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.20494
INFO:root:EnergyScoreValidation: 0.10577
INFO:root:CoverageValidation: 0.74614
INFO:root:IntervalWidthValidation: 0.10363
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.20374
INFO:root:EnergyScoreTest: 0.10133
INFO:root:CoverageTest: 0.74346
INFO:root:IntervalWidthTest: 0.10442
INFO:root:MSEValidation: 0.08588
INFO:root:EnergyScoreValidation: 0.06426
INFO:root:CoverageValidation: 0.60826
INFO:root:IntervalWidthValidation: 0.03164
INFO:root:Evaluating the model on Test data.
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1239416832
INFO:root:Training starts now.
INFO:root:MSETest: 0.08618
INFO:root:EnergyScoreTest: 0.06455
INFO:root:CoverageTest: 0.6049
INFO:root:IntervalWidthTest: 0.0317
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.26696883, Validation loss: 0.19019706, Gradient norm: 4.04301263
INFO:root:[    1] Training loss: 0.29606808, Validation loss: 0.11908919, Gradient norm: 2.98127800
INFO:root:[    2] Training loss: 0.13110766, Validation loss: 0.16957212, Gradient norm: 4.07483578
INFO:root:[    2] Training loss: 0.12974554, Validation loss: 0.13882196, Gradient norm: 2.88073493
INFO:root:[    3] Training loss: 0.11202498, Validation loss: 0.15640765, Gradient norm: 4.25276806
INFO:root:[    3] Training loss: 0.10870353, Validation loss: 0.09895163, Gradient norm: 2.89558641
INFO:root:[    4] Training loss: 0.09759576, Validation loss: 0.15063010, Gradient norm: 4.20804101
INFO:root:[    4] Training loss: 0.09571870, Validation loss: 0.09787240, Gradient norm: 2.57890410
INFO:root:[    5] Training loss: 0.09087587, Validation loss: 0.13820994, Gradient norm: 3.70034394
INFO:root:[    6] Training loss: 0.08596556, Validation loss: 0.14493152, Gradient norm: 3.84485612
INFO:root:[    5] Training loss: 0.08400343, Validation loss: 0.10567947, Gradient norm: 2.35377572
INFO:root:[    7] Training loss: 0.08028375, Validation loss: 0.13702995, Gradient norm: 3.64912269
INFO:root:[    6] Training loss: 0.08182068, Validation loss: 0.07414569, Gradient norm: 2.54807045
INFO:root:[    8] Training loss: 0.08000618, Validation loss: 0.14203093, Gradient norm: 3.94689917
INFO:root:[    7] Training loss: 0.08061834, Validation loss: 0.07953555, Gradient norm: 2.76846851
INFO:root:[    9] Training loss: 0.07354239, Validation loss: 0.14135330, Gradient norm: 3.52793671
INFO:root:[    8] Training loss: 0.07730203, Validation loss: 0.09393815, Gradient norm: 2.70333321
INFO:root:[   10] Training loss: 0.07384620, Validation loss: 0.12840594, Gradient norm: 3.47632766
INFO:root:[    9] Training loss: 0.07288291, Validation loss: 0.09577474, Gradient norm: 2.54381711
INFO:root:[   11] Training loss: 0.07266457, Validation loss: 0.13241175, Gradient norm: 3.57899406
INFO:root:[   10] Training loss: 0.06721024, Validation loss: 0.08184593, Gradient norm: 2.11920391
INFO:root:[   12] Training loss: 0.06902778, Validation loss: 0.12646877, Gradient norm: 3.53929378
INFO:root:[   11] Training loss: 0.06865381, Validation loss: 0.07838328, Gradient norm: 2.59295909
INFO:root:[   13] Training loss: 0.06758586, Validation loss: 0.11419211, Gradient norm: 3.49619327
INFO:root:[   12] Training loss: 0.06826039, Validation loss: 0.10586294, Gradient norm: 2.60346558
INFO:root:[   14] Training loss: 0.06875085, Validation loss: 0.12251813, Gradient norm: 3.58752950
INFO:root:[   13] Training loss: 0.06377163, Validation loss: 0.07347703, Gradient norm: 2.34921171
INFO:root:[   15] Training loss: 0.06586001, Validation loss: 0.11805200, Gradient norm: 3.44935588
INFO:root:[   14] Training loss: 0.06279084, Validation loss: 0.07599404, Gradient norm: 2.33362018
INFO:root:[   16] Training loss: 0.06263435, Validation loss: 0.12616847, Gradient norm: 3.10954651
INFO:root:[   15] Training loss: 0.06087618, Validation loss: 0.07830366, Gradient norm: 2.22090206
INFO:root:[   17] Training loss: 0.06273473, Validation loss: 0.11201177, Gradient norm: 2.94765511
INFO:root:[   16] Training loss: 0.06293069, Validation loss: 0.08958370, Gradient norm: 2.41352309
INFO:root:[   18] Training loss: 0.06493851, Validation loss: 0.12780702, Gradient norm: 3.36953495
INFO:root:[   17] Training loss: 0.06114241, Validation loss: 0.08093981, Gradient norm: 2.13135588
INFO:root:[   19] Training loss: 0.06323364, Validation loss: 0.10761943, Gradient norm: 3.33594865
INFO:root:[   18] Training loss: 0.06022745, Validation loss: 0.08674965, Gradient norm: 2.29148587
INFO:root:[   20] Training loss: 0.05997051, Validation loss: 0.11202452, Gradient norm: 3.14388750
INFO:root:[   19] Training loss: 0.06106948, Validation loss: 0.09226848, Gradient norm: 2.43097953
INFO:root:[   21] Training loss: 0.05910031, Validation loss: 0.10274579, Gradient norm: 3.13430830
INFO:root:[   20] Training loss: 0.05518406, Validation loss: 0.07348818, Gradient norm: 2.01444324
INFO:root:[   22] Training loss: 0.05525060, Validation loss: 0.10150568, Gradient norm: 2.65852666
INFO:root:[   23] Training loss: 0.05579106, Validation loss: 0.11247855, Gradient norm: 3.07865869
INFO:root:[   21] Training loss: 0.05646968, Validation loss: 0.08584204, Gradient norm: 2.21146310
INFO:root:[   24] Training loss: 0.05747834, Validation loss: 0.10608579, Gradient norm: 3.18290943
INFO:root:[   22] Training loss: 0.05562530, Validation loss: 0.07916879, Gradient norm: 2.02628713
INFO:root:[   25] Training loss: 0.05473843, Validation loss: 0.10403926, Gradient norm: 2.73600233
INFO:root:[   23] Training loss: 0.05566276, Validation loss: 0.07393012, Gradient norm: 2.05463049
INFO:root:[   26] Training loss: 0.05357024, Validation loss: 0.09855622, Gradient norm: 3.06085890
INFO:root:[   24] Training loss: 0.05415259, Validation loss: 0.09113622, Gradient norm: 2.16333919
INFO:root:[   27] Training loss: 0.05631881, Validation loss: 0.10739077, Gradient norm: 3.34091979
INFO:root:[   25] Training loss: 0.05633890, Validation loss: 0.07207533, Gradient norm: 2.25917616
INFO:root:[   28] Training loss: 0.05446109, Validation loss: 0.10360242, Gradient norm: 3.15619446
INFO:root:[   26] Training loss: 0.05164959, Validation loss: 0.08274260, Gradient norm: 1.90993004
INFO:root:[   29] Training loss: 0.05285755, Validation loss: 0.09621468, Gradient norm: 2.72650509
INFO:root:[   27] Training loss: 0.05286234, Validation loss: 0.08596408, Gradient norm: 2.06484500
INFO:root:[   30] Training loss: 0.05258848, Validation loss: 0.09394035, Gradient norm: 3.07610957
INFO:root:[   28] Training loss: 0.05395280, Validation loss: 0.08370017, Gradient norm: 1.92989207
INFO:root:[   31] Training loss: 0.05116828, Validation loss: 0.09502229, Gradient norm: 2.88192981
INFO:root:[   29] Training loss: 0.05117900, Validation loss: 0.08906115, Gradient norm: 2.04199367
INFO:root:[   32] Training loss: 0.05050423, Validation loss: 0.09696002, Gradient norm: 2.92207443
INFO:root:[   30] Training loss: 0.04972619, Validation loss: 0.07610599, Gradient norm: 1.87996121
INFO:root:[   33] Training loss: 0.05197187, Validation loss: 0.09414852, Gradient norm: 3.17169583
INFO:root:[   31] Training loss: 0.05286120, Validation loss: 0.07324401, Gradient norm: 2.19724820
INFO:root:[   34] Training loss: 0.05090350, Validation loss: 0.10092252, Gradient norm: 3.07696449
INFO:root:[   32] Training loss: 0.05028574, Validation loss: 0.08448589, Gradient norm: 2.02071670
INFO:root:[   35] Training loss: 0.04916286, Validation loss: 0.10445132, Gradient norm: 2.98316646
INFO:root:[   33] Training loss: 0.04861151, Validation loss: 0.07471978, Gradient norm: 1.97693826
INFO:root:[   36] Training loss: 0.04922569, Validation loss: 0.09338392, Gradient norm: 2.75649287
INFO:root:[   34] Training loss: 0.04846985, Validation loss: 0.08812187, Gradient norm: 1.86616842
INFO:root:[   37] Training loss: 0.04633047, Validation loss: 0.09006438, Gradient norm: 2.58174881
INFO:root:[   35] Training loss: 0.04826977, Validation loss: 0.07604559, Gradient norm: 2.03102552
INFO:root:[   38] Training loss: 0.04928555, Validation loss: 0.09732868, Gradient norm: 2.94553520
INFO:root:[   36] Training loss: 0.04922888, Validation loss: 0.07332252, Gradient norm: 2.00543790
INFO:root:[   39] Training loss: 0.04554028, Validation loss: 0.10343994, Gradient norm: 2.37407480
INFO:root:[   37] Training loss: 0.04785106, Validation loss: 0.07869984, Gradient norm: 1.96954084
INFO:root:[   40] Training loss: 0.04954545, Validation loss: 0.09353371, Gradient norm: 3.21914113
INFO:root:[   41] Training loss: 0.04549594, Validation loss: 0.09105770, Gradient norm: 2.41317405
INFO:root:[   38] Training loss: 0.04743021, Validation loss: 0.07596979, Gradient norm: 1.85003447
INFO:root:[   42] Training loss: 0.04562038, Validation loss: 0.08650494, Gradient norm: 2.77299865
INFO:root:[   39] Training loss: 0.04783597, Validation loss: 0.08030484, Gradient norm: 2.06264297
INFO:root:[   43] Training loss: 0.04763801, Validation loss: 0.09475710, Gradient norm: 2.93126272
INFO:root:[   40] Training loss: 0.04619046, Validation loss: 0.07434718, Gradient norm: 1.89902032
INFO:root:[   44] Training loss: 0.04670480, Validation loss: 0.11438432, Gradient norm: 3.01566108
INFO:root:[   41] Training loss: 0.04457068, Validation loss: 0.09083332, Gradient norm: 1.72365882
INFO:root:[   45] Training loss: 0.04622849, Validation loss: 0.09373516, Gradient norm: 2.95351020
INFO:root:[   42] Training loss: 0.04775317, Validation loss: 0.08467613, Gradient norm: 2.02596061
INFO:root:[   46] Training loss: 0.04697737, Validation loss: 0.10914087, Gradient norm: 3.06525063
INFO:root:[   43] Training loss: 0.04668331, Validation loss: 0.08370803, Gradient norm: 2.19673977
INFO:root:[   47] Training loss: 0.04485594, Validation loss: 0.09957736, Gradient norm: 2.78182917
INFO:root:[   44] Training loss: 0.04708865, Validation loss: 0.07951656, Gradient norm: 2.00153586
INFO:root:[   48] Training loss: 0.04368904, Validation loss: 0.10688969, Gradient norm: 2.68049699
INFO:root:[   45] Training loss: 0.04570454, Validation loss: 0.08706183, Gradient norm: 2.00183730
INFO:root:[   49] Training loss: 0.04312120, Validation loss: 0.11077129, Gradient norm: 2.83303279
INFO:root:[   46] Training loss: 0.04532031, Validation loss: 0.08229392, Gradient norm: 1.88566592
INFO:root:[   50] Training loss: 0.04691069, Validation loss: 0.09960575, Gradient norm: 2.97461748
INFO:root:[   47] Training loss: 0.04389452, Validation loss: 0.07552896, Gradient norm: 1.86254853
INFO:root:[   51] Training loss: 0.04399922, Validation loss: 0.09904164, Gradient norm: 2.87535364
INFO:root:[   48] Training loss: 0.04357526, Validation loss: 0.08826443, Gradient norm: 2.10571553
INFO:root:[   52] Training loss: 0.04388985, Validation loss: 0.09180411, Gradient norm: 2.79198266
INFO:root:[   49] Training loss: 0.04590027, Validation loss: 0.07896992, Gradient norm: 1.99614083
INFO:root:[   53] Training loss: 0.04246171, Validation loss: 0.09439439, Gradient norm: 2.76642981
INFO:root:[   50] Training loss: 0.04461647, Validation loss: 0.07846930, Gradient norm: 1.91280148
INFO:root:[   54] Training loss: 0.04570106, Validation loss: 0.09718504, Gradient norm: 3.11965391
INFO:root:[   51] Training loss: 0.04690393, Validation loss: 0.07836073, Gradient norm: 2.10599101
INFO:root:[   55] Training loss: 0.04207629, Validation loss: 0.09117852, Gradient norm: 2.57132031
INFO:root:[   52] Training loss: 0.04320982, Validation loss: 0.07876915, Gradient norm: 1.92009787
INFO:root:[   56] Training loss: 0.04287580, Validation loss: 0.10320641, Gradient norm: 2.81297063
INFO:root:[   53] Training loss: 0.04237201, Validation loss: 0.07998554, Gradient norm: 1.70538525
INFO:root:[   57] Training loss: 0.04329884, Validation loss: 0.10440134, Gradient norm: 3.13748725
INFO:root:[   54] Training loss: 0.04166075, Validation loss: 0.08043862, Gradient norm: 1.88637653
INFO:root:[   58] Training loss: 0.04029046, Validation loss: 0.09770688, Gradient norm: 2.53281137
INFO:root:[   55] Training loss: 0.04574079, Validation loss: 0.07016493, Gradient norm: 2.01949788
INFO:root:[   59] Training loss: 0.04249594, Validation loss: 0.10735598, Gradient norm: 2.59583727
INFO:root:[   56] Training loss: 0.04180558, Validation loss: 0.07366341, Gradient norm: 1.92738561
INFO:root:[   60] Training loss: 0.04190336, Validation loss: 0.09269834, Gradient norm: 2.56135275
INFO:root:[   57] Training loss: 0.04286728, Validation loss: 0.06324438, Gradient norm: 1.95303167
INFO:root:[   61] Training loss: 0.04160512, Validation loss: 0.09656055, Gradient norm: 2.70097226
INFO:root:[   58] Training loss: 0.04257407, Validation loss: 0.07824182, Gradient norm: 1.72203824
INFO:root:[   62] Training loss: 0.04175678, Validation loss: 0.10052829, Gradient norm: 2.64242591
INFO:root:[   63] Training loss: 0.03906677, Validation loss: 0.10682148, Gradient norm: 2.51206283
INFO:root:[   59] Training loss: 0.04014288, Validation loss: 0.06892285, Gradient norm: 1.91870353
INFO:root:[   64] Training loss: 0.04004787, Validation loss: 0.09761059, Gradient norm: 2.48920476
INFO:root:[   60] Training loss: 0.04312524, Validation loss: 0.07274192, Gradient norm: 2.05805884
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 2225.441s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:[   61] Training loss: 0.04203528, Validation loss: 0.07050304, Gradient norm: 2.00556366
INFO:root:[   62] Training loss: 0.04127866, Validation loss: 0.07232834, Gradient norm: 1.93867131
INFO:root:Evaluating the model on Train data.
INFO:root:[   63] Training loss: 0.04250155, Validation loss: 0.08201853, Gradient norm: 1.89742920
INFO:root:MSETrain: 0.19439
INFO:root:EnergyScoreTrain: 0.10004
INFO:root:CoverageTrain: 0.71773
INFO:root:IntervalWidthTrain: 0.09748
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.20496
INFO:root:EnergyScoreValidation: 0.10625
INFO:root:CoverageValidation: 0.70893
INFO:root:IntervalWidthValidation: 0.09682
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.2052
INFO:root:EnergyScoreTest: 0.10132
INFO:root:CoverageTest: 0.74859
INFO:root:IntervalWidthTest: 0.10577
INFO:root:[   64] Training loss: 0.04094553, Validation loss: 0.07443528, Gradient norm: 1.68452906
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1247805440
INFO:root:Training starts now.
INFO:root:[   65] Training loss: 0.04014899, Validation loss: 0.06638697, Gradient norm: 2.01810746
INFO:root:[    1] Training loss: 0.26310572, Validation loss: 0.17678567, Gradient norm: 4.25025220
INFO:root:[   66] Training loss: 0.04065480, Validation loss: 0.07168197, Gradient norm: 1.95269429
INFO:root:[    2] Training loss: 0.13553237, Validation loss: 0.14882060, Gradient norm: 4.42875875
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 2420.758s.
INFO:root:Emptying the cuda cache took 0.027s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:[    3] Training loss: 0.11307925, Validation loss: 0.14427958, Gradient norm: 4.28243951
INFO:root:[    4] Training loss: 0.10066335, Validation loss: 0.14215510, Gradient norm: 4.14225472
INFO:root:MSETrain: 0.04154
INFO:root:EnergyScoreTrain: 0.03005
INFO:root:CoverageTrain: 0.80015
INFO:root:IntervalWidthTrain: 0.02381
INFO:root:Evaluating the model on Validation data.
INFO:root:[    5] Training loss: 0.09085648, Validation loss: 0.13047995, Gradient norm: 3.93487526
INFO:root:MSEValidation: 0.08134
INFO:root:EnergyScoreValidation: 0.06352
INFO:root:CoverageValidation: 0.48696
INFO:root:IntervalWidthValidation: 0.02334
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08177
INFO:root:EnergyScoreTest: 0.06406
INFO:root:CoverageTest: 0.48851
INFO:root:IntervalWidthTest: 0.02321
INFO:root:[    6] Training loss: 0.08544354, Validation loss: 0.13710773, Gradient norm: 3.84344351
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    7] Training loss: 0.08049133, Validation loss: 0.12459074, Gradient norm: 3.61212754
INFO:root:[    1] Training loss: 0.30220299, Validation loss: 0.13104242, Gradient norm: 3.50097801
INFO:root:[    8] Training loss: 0.07824947, Validation loss: 0.14977072, Gradient norm: 3.21899674
INFO:root:[    2] Training loss: 0.12909247, Validation loss: 0.10588967, Gradient norm: 2.71824033
INFO:root:[    9] Training loss: 0.07846661, Validation loss: 0.10866966, Gradient norm: 4.04143724
INFO:root:[    3] Training loss: 0.10957442, Validation loss: 0.09209464, Gradient norm: 2.89091413
INFO:root:[   10] Training loss: 0.07300208, Validation loss: 0.12332826, Gradient norm: 3.69570723
INFO:root:[    4] Training loss: 0.09525792, Validation loss: 0.10234265, Gradient norm: 2.49955119
INFO:root:[   11] Training loss: 0.06857801, Validation loss: 0.10869668, Gradient norm: 3.46629815
INFO:root:[    5] Training loss: 0.08947233, Validation loss: 0.08699559, Gradient norm: 2.69196469
INFO:root:[   12] Training loss: 0.06886950, Validation loss: 0.10313951, Gradient norm: 3.60205798
INFO:root:[    6] Training loss: 0.08082993, Validation loss: 0.08920177, Gradient norm: 2.29361625
INFO:root:[   13] Training loss: 0.06519315, Validation loss: 0.10890639, Gradient norm: 2.99058242
INFO:root:[    7] Training loss: 0.07784434, Validation loss: 0.10333421, Gradient norm: 2.56921606
INFO:root:[   14] Training loss: 0.06287083, Validation loss: 0.11136896, Gradient norm: 3.27109999
INFO:root:[    8] Training loss: 0.07332761, Validation loss: 0.07950285, Gradient norm: 2.31793229
INFO:root:[   15] Training loss: 0.06206604, Validation loss: 0.09880900, Gradient norm: 3.33908851
INFO:root:[    9] Training loss: 0.07031031, Validation loss: 0.10145077, Gradient norm: 2.32831645
INFO:root:[   16] Training loss: 0.05967470, Validation loss: 0.11213724, Gradient norm: 2.68920913
INFO:root:[   10] Training loss: 0.07106626, Validation loss: 0.09254168, Gradient norm: 2.57340912
INFO:root:[   17] Training loss: 0.06338288, Validation loss: 0.10028333, Gradient norm: 3.64622949
INFO:root:[   18] Training loss: 0.05951443, Validation loss: 0.09224616, Gradient norm: 3.12902372
INFO:root:[   11] Training loss: 0.06814268, Validation loss: 0.09039672, Gradient norm: 2.53853514
INFO:root:[   19] Training loss: 0.05951598, Validation loss: 0.09624265, Gradient norm: 3.21579907
INFO:root:[   12] Training loss: 0.06820437, Validation loss: 0.10181230, Gradient norm: 2.55098338
INFO:root:[   20] Training loss: 0.05862911, Validation loss: 0.10201996, Gradient norm: 3.20888595
INFO:root:[   13] Training loss: 0.06375233, Validation loss: 0.07527431, Gradient norm: 2.22417045
INFO:root:[   21] Training loss: 0.05819363, Validation loss: 0.10146191, Gradient norm: 3.14592652
INFO:root:[   14] Training loss: 0.06298539, Validation loss: 0.07172630, Gradient norm: 2.25563195
INFO:root:[   22] Training loss: 0.05544353, Validation loss: 0.09821093, Gradient norm: 2.69647427
INFO:root:[   15] Training loss: 0.06311842, Validation loss: 0.08138395, Gradient norm: 2.40977549
INFO:root:[   23] Training loss: 0.05648447, Validation loss: 0.08180645, Gradient norm: 3.19713611
INFO:root:[   16] Training loss: 0.05802846, Validation loss: 0.06873551, Gradient norm: 1.97221768
INFO:root:[   24] Training loss: 0.05591719, Validation loss: 0.08553948, Gradient norm: 3.28527157
INFO:root:[   17] Training loss: 0.05691557, Validation loss: 0.07585350, Gradient norm: 1.90110095
INFO:root:[   25] Training loss: 0.05358240, Validation loss: 0.08719190, Gradient norm: 2.99760278
INFO:root:[   18] Training loss: 0.05745141, Validation loss: 0.07860271, Gradient norm: 2.11561771
INFO:root:[   26] Training loss: 0.05488460, Validation loss: 0.08193023, Gradient norm: 3.17594435
INFO:root:[   19] Training loss: 0.05853656, Validation loss: 0.08245653, Gradient norm: 2.25137211
INFO:root:[   27] Training loss: 0.05171749, Validation loss: 0.08496314, Gradient norm: 2.84058446
INFO:root:[   20] Training loss: 0.05584587, Validation loss: 0.07747571, Gradient norm: 2.17545045
INFO:root:[   28] Training loss: 0.05032722, Validation loss: 0.09439096, Gradient norm: 2.53754089
INFO:root:[   21] Training loss: 0.05530374, Validation loss: 0.07472686, Gradient norm: 2.06671168
INFO:root:[   29] Training loss: 0.05275842, Validation loss: 0.09035414, Gradient norm: 3.10739301
INFO:root:[   22] Training loss: 0.05491920, Validation loss: 0.07680800, Gradient norm: 2.03288732
INFO:root:[   30] Training loss: 0.04999921, Validation loss: 0.07623359, Gradient norm: 2.68458111
INFO:root:[   23] Training loss: 0.05429422, Validation loss: 0.08408362, Gradient norm: 2.07894208
INFO:root:[   31] Training loss: 0.05084168, Validation loss: 0.08230327, Gradient norm: 2.98830657
INFO:root:[   24] Training loss: 0.05590162, Validation loss: 0.08258539, Gradient norm: 2.12542047
INFO:root:[   32] Training loss: 0.04951237, Validation loss: 0.08456345, Gradient norm: 2.91705539
INFO:root:[   25] Training loss: 0.05253510, Validation loss: 0.07298504, Gradient norm: 1.97498637
INFO:root:[   33] Training loss: 0.04866122, Validation loss: 0.07932054, Gradient norm: 2.93734807
INFO:root:[   26] Training loss: 0.05403679, Validation loss: 0.07004132, Gradient norm: 2.17592983
INFO:root:[   34] Training loss: 0.04735241, Validation loss: 0.07535630, Gradient norm: 2.77463646
INFO:root:[   27] Training loss: 0.05342212, Validation loss: 0.07813777, Gradient norm: 2.03293188
INFO:root:[   35] Training loss: 0.04732690, Validation loss: 0.09637708, Gradient norm: 2.69655157
INFO:root:[   28] Training loss: 0.05421991, Validation loss: 0.08473838, Gradient norm: 2.19962577
INFO:root:[   36] Training loss: 0.04719849, Validation loss: 0.08075139, Gradient norm: 2.75156998
INFO:root:[   29] Training loss: 0.05578923, Validation loss: 0.07777200, Gradient norm: 2.10917763
INFO:root:[   37] Training loss: 0.04805632, Validation loss: 0.08572941, Gradient norm: 3.03911976
INFO:root:[   30] Training loss: 0.05049579, Validation loss: 0.07717872, Gradient norm: 2.17550395
INFO:root:[   38] Training loss: 0.05026047, Validation loss: 0.07581831, Gradient norm: 2.81500786
INFO:root:[   31] Training loss: 0.04878197, Validation loss: 0.08021816, Gradient norm: 1.78389403
INFO:root:[   39] Training loss: 0.04730245, Validation loss: 0.07944550, Gradient norm: 3.08078101
INFO:root:[   40] Training loss: 0.04581692, Validation loss: 0.09348739, Gradient norm: 2.51825669
INFO:root:[   32] Training loss: 0.04978472, Validation loss: 0.07308184, Gradient norm: 2.00279756
INFO:root:[   41] Training loss: 0.04762824, Validation loss: 0.08681962, Gradient norm: 2.97276980
INFO:root:[   33] Training loss: 0.05011685, Validation loss: 0.08129603, Gradient norm: 2.05440429
INFO:root:[   42] Training loss: 0.04610797, Validation loss: 0.09122453, Gradient norm: 2.78683391
INFO:root:[   34] Training loss: 0.04863118, Validation loss: 0.07335507, Gradient norm: 1.91239385
INFO:root:[   43] Training loss: 0.04479837, Validation loss: 0.07368899, Gradient norm: 2.59439921
INFO:root:[   35] Training loss: 0.04897812, Validation loss: 0.07819475, Gradient norm: 1.91542250
INFO:root:[   44] Training loss: 0.04469519, Validation loss: 0.08297573, Gradient norm: 2.55350235
INFO:root:[   36] Training loss: 0.04746644, Validation loss: 0.08304945, Gradient norm: 1.85510665
INFO:root:[   45] Training loss: 0.04293951, Validation loss: 0.07416396, Gradient norm: 2.35608829
INFO:root:[   37] Training loss: 0.04680929, Validation loss: 0.08643120, Gradient norm: 1.79441163
INFO:root:[   46] Training loss: 0.04514197, Validation loss: 0.08548574, Gradient norm: 2.75371894
INFO:root:[   38] Training loss: 0.04968579, Validation loss: 0.07552961, Gradient norm: 2.02064684
INFO:root:[   47] Training loss: 0.04474907, Validation loss: 0.07129390, Gradient norm: 2.79314344
INFO:root:[   39] Training loss: 0.04790021, Validation loss: 0.08143183, Gradient norm: 2.11566111
INFO:root:[   48] Training loss: 0.04626803, Validation loss: 0.07471555, Gradient norm: 2.81452418
INFO:root:[   40] Training loss: 0.04718494, Validation loss: 0.08775622, Gradient norm: 1.91064108
INFO:root:[   49] Training loss: 0.04445554, Validation loss: 0.07754077, Gradient norm: 2.74502102
INFO:root:[   41] Training loss: 0.04879527, Validation loss: 0.08475249, Gradient norm: 2.16272097
INFO:root:[   50] Training loss: 0.04314938, Validation loss: 0.06855549, Gradient norm: 2.70909118
INFO:root:[   42] Training loss: 0.04707050, Validation loss: 0.07611407, Gradient norm: 2.00386229
INFO:root:[   51] Training loss: 0.04400074, Validation loss: 0.07207094, Gradient norm: 2.96453293
INFO:root:[   43] Training loss: 0.04539892, Validation loss: 0.07936260, Gradient norm: 1.89590295
INFO:root:[   52] Training loss: 0.04251239, Validation loss: 0.08074268, Gradient norm: 2.53068127
INFO:root:[   44] Training loss: 0.04561360, Validation loss: 0.08477068, Gradient norm: 2.00022035
INFO:root:[   53] Training loss: 0.04350262, Validation loss: 0.07217570, Gradient norm: 2.82212614
INFO:root:[   45] Training loss: 0.04409565, Validation loss: 0.07927766, Gradient norm: 2.10394125
INFO:root:[   54] Training loss: 0.04300599, Validation loss: 0.06770408, Gradient norm: 2.61032049
INFO:root:[   46] Training loss: 0.04556582, Validation loss: 0.08011416, Gradient norm: 1.73739574
INFO:root:[   47] Training loss: 0.04357815, Validation loss: 0.08123003, Gradient norm: 1.76777447
INFO:root:[   55] Training loss: 0.04394297, Validation loss: 0.08537058, Gradient norm: 2.82859494
INFO:root:[   48] Training loss: 0.04381914, Validation loss: 0.08565111, Gradient norm: 1.86785038
INFO:root:[   56] Training loss: 0.04296578, Validation loss: 0.07098591, Gradient norm: 2.98802219
INFO:root:[   49] Training loss: 0.04497264, Validation loss: 0.09309575, Gradient norm: 1.88947901
INFO:root:[   57] Training loss: 0.04204731, Validation loss: 0.08342161, Gradient norm: 2.88862753
INFO:root:[   50] Training loss: 0.04485362, Validation loss: 0.08616914, Gradient norm: 1.92956304
INFO:root:[   58] Training loss: 0.04268386, Validation loss: 0.06895531, Gradient norm: 2.71376137
INFO:root:[   59] Training loss: 0.04170671, Validation loss: 0.06749323, Gradient norm: 2.67325733
INFO:root:[   51] Training loss: 0.04444197, Validation loss: 0.08630353, Gradient norm: 2.07489980
INFO:root:[   60] Training loss: 0.04183601, Validation loss: 0.06643810, Gradient norm: 2.63347126
INFO:root:[   52] Training loss: 0.04247810, Validation loss: 0.08312989, Gradient norm: 1.85825915
INFO:root:[   61] Training loss: 0.03939637, Validation loss: 0.06758240, Gradient norm: 2.40788258
INFO:root:[   53] Training loss: 0.04350252, Validation loss: 0.09017473, Gradient norm: 1.87900093
INFO:root:[   62] Training loss: 0.04042631, Validation loss: 0.07658943, Gradient norm: 2.63168294
INFO:root:[   63] Training loss: 0.04026114, Validation loss: 0.06773540, Gradient norm: 2.74157261
INFO:root:[   54] Training loss: 0.04335161, Validation loss: 0.08562821, Gradient norm: 2.14515740
INFO:root:[   64] Training loss: 0.03993518, Validation loss: 0.07440079, Gradient norm: 2.36209518
INFO:root:[   55] Training loss: 0.04338815, Validation loss: 0.09672005, Gradient norm: 1.86809411
INFO:root:[   65] Training loss: 0.03995386, Validation loss: 0.06917687, Gradient norm: 2.42388836
INFO:root:[   56] Training loss: 0.04519752, Validation loss: 0.09295458, Gradient norm: 1.72201147
INFO:root:[   66] Training loss: 0.03986227, Validation loss: 0.08785669, Gradient norm: 2.79383545
INFO:root:[   57] Training loss: 0.04229692, Validation loss: 0.08471516, Gradient norm: 1.86772337
INFO:root:[   67] Training loss: 0.03844908, Validation loss: 0.07318239, Gradient norm: 2.48398578
INFO:root:[   58] Training loss: 0.04199216, Validation loss: 0.08506977, Gradient norm: 2.09810463
INFO:root:[   68] Training loss: 0.03975389, Validation loss: 0.06346273, Gradient norm: 2.57292880
INFO:root:[   59] Training loss: 0.04171419, Validation loss: 0.08670224, Gradient norm: 1.78391273
INFO:root:[   69] Training loss: 0.03719471, Validation loss: 0.07379731, Gradient norm: 2.51334551
INFO:root:[   60] Training loss: 0.04233744, Validation loss: 0.09237519, Gradient norm: 1.93229732
INFO:root:[   70] Training loss: 0.03897544, Validation loss: 0.07171142, Gradient norm: 2.51920587
INFO:root:[   61] Training loss: 0.04068938, Validation loss: 0.08512937, Gradient norm: 1.89197940
INFO:root:[   71] Training loss: 0.03736565, Validation loss: 0.06436892, Gradient norm: 2.37114151
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 2237.647s.
INFO:root:Emptying the cuda cache took 0.026s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:[   72] Training loss: 0.03815992, Validation loss: 0.06557162, Gradient norm: 2.65404164
INFO:root:MSETrain: 0.05306
INFO:root:EnergyScoreTrain: 0.03976
INFO:root:CoverageTrain: 0.87574
INFO:root:IntervalWidthTrain: 0.04423
INFO:root:Evaluating the model on Validation data.
INFO:root:[   73] Training loss: 0.03782854, Validation loss: 0.08118601, Gradient norm: 2.63588746
INFO:root:MSEValidation: 0.09571
INFO:root:EnergyScoreValidation: 0.06884
INFO:root:CoverageValidation: 0.70286
INFO:root:IntervalWidthValidation: 0.04396
INFO:root:Evaluating the model on Test data.
INFO:root:[   74] Training loss: 0.04039187, Validation loss: 0.06985901, Gradient norm: 2.77702886
INFO:root:MSETest: 0.0959
INFO:root:EnergyScoreTest: 0.06906
INFO:root:CoverageTest: 0.70414
INFO:root:IntervalWidthTest: 0.04406
INFO:root:[   75] Training loss: 0.04008428, Validation loss: 0.08206183, Gradient norm: 2.68755408
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[   76] Training loss: 0.03684475, Validation loss: 0.07099356, Gradient norm: 2.71887772
INFO:root:[    1] Training loss: 0.30472652, Validation loss: 0.18060484, Gradient norm: 3.15479115
INFO:root:[   77] Training loss: 0.03916317, Validation loss: 0.07689220, Gradient norm: 2.88124616
INFO:root:[    2] Training loss: 0.13541830, Validation loss: 0.14296626, Gradient norm: 2.65727742
INFO:root:EP 77: Early stopping
INFO:root:Training the model took 2679.118s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:[    3] Training loss: 0.10817777, Validation loss: 0.09709757, Gradient norm: 2.72102398
INFO:root:[    4] Training loss: 0.09361919, Validation loss: 0.08938009, Gradient norm: 2.38052050
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.19409
INFO:root:EnergyScoreTrain: 0.10005
INFO:root:CoverageTrain: 0.72016
INFO:root:IntervalWidthTrain: 0.09733
INFO:root:Evaluating the model on Validation data.
INFO:root:[    5] Training loss: 0.08370328, Validation loss: 0.08487073, Gradient norm: 2.42540529
INFO:root:MSEValidation: 0.2
INFO:root:EnergyScoreValidation: 0.10275
INFO:root:CoverageValidation: 0.705
INFO:root:IntervalWidthValidation: 0.09209
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.1914
INFO:root:EnergyScoreTest: 0.09814
INFO:root:CoverageTest: 0.74749
INFO:root:IntervalWidthTest: 0.09852
INFO:root:[    6] Training loss: 0.08001233, Validation loss: 0.09175357, Gradient norm: 2.48818007
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1243611136
INFO:root:Training starts now.
INFO:root:[    7] Training loss: 0.07573935, Validation loss: 0.07591999, Gradient norm: 2.33355798
INFO:root:[    1] Training loss: 0.24840342, Validation loss: 0.24811473, Gradient norm: 3.40415918
INFO:root:[    8] Training loss: 0.07431679, Validation loss: 0.08443089, Gradient norm: 2.46342575
INFO:root:[    2] Training loss: 0.13617499, Validation loss: 0.18717763, Gradient norm: 4.54727257
INFO:root:[    9] Training loss: 0.07109979, Validation loss: 0.08789718, Gradient norm: 2.33659584
INFO:root:[    3] Training loss: 0.11379279, Validation loss: 0.16535262, Gradient norm: 4.06579923
INFO:root:[   10] Training loss: 0.06879135, Validation loss: 0.06475291, Gradient norm: 2.38088416
INFO:root:[    4] Training loss: 0.09978184, Validation loss: 0.14526438, Gradient norm: 3.82648319
INFO:root:[   11] Training loss: 0.06721585, Validation loss: 0.06867452, Gradient norm: 2.34569883
INFO:root:[    5] Training loss: 0.09107108, Validation loss: 0.16770355, Gradient norm: 3.80360015
INFO:root:[   12] Training loss: 0.06804180, Validation loss: 0.06873158, Gradient norm: 2.48945326
INFO:root:[    6] Training loss: 0.08376187, Validation loss: 0.15159450, Gradient norm: 3.71676745
INFO:root:[    7] Training loss: 0.08302897, Validation loss: 0.14139964, Gradient norm: 3.91605976
INFO:root:[   13] Training loss: 0.06272190, Validation loss: 0.06978762, Gradient norm: 2.03281677
INFO:root:[    8] Training loss: 0.07531381, Validation loss: 0.13329584, Gradient norm: 3.34862850
INFO:root:[   14] Training loss: 0.06237515, Validation loss: 0.07882929, Gradient norm: 2.16664980
INFO:root:[    9] Training loss: 0.07496834, Validation loss: 0.14882740, Gradient norm: 3.32476717
INFO:root:[   15] Training loss: 0.06122183, Validation loss: 0.06181304, Gradient norm: 2.25143521
INFO:root:[   10] Training loss: 0.07067670, Validation loss: 0.15124919, Gradient norm: 3.59600380
INFO:root:[   16] Training loss: 0.05765692, Validation loss: 0.05495771, Gradient norm: 2.03234221
INFO:root:[   11] Training loss: 0.07012250, Validation loss: 0.13722080, Gradient norm: 3.71348800
INFO:root:[   17] Training loss: 0.06177056, Validation loss: 0.05840332, Gradient norm: 2.18510737
INFO:root:[   12] Training loss: 0.07206768, Validation loss: 0.12654760, Gradient norm: 3.55308513
INFO:root:[   18] Training loss: 0.05762438, Validation loss: 0.06105141, Gradient norm: 2.13088966
INFO:root:[   13] Training loss: 0.06643494, Validation loss: 0.12172362, Gradient norm: 3.28348307
INFO:root:[   19] Training loss: 0.05276835, Validation loss: 0.05866955, Gradient norm: 1.75716708
INFO:root:[   14] Training loss: 0.06565089, Validation loss: 0.10861901, Gradient norm: 3.22345146
INFO:root:[   20] Training loss: 0.05432835, Validation loss: 0.05503915, Gradient norm: 1.93687438
INFO:root:[   15] Training loss: 0.06251565, Validation loss: 0.10794122, Gradient norm: 3.11213359
INFO:root:[   21] Training loss: 0.05676747, Validation loss: 0.06481115, Gradient norm: 2.09629396
INFO:root:[   16] Training loss: 0.06221438, Validation loss: 0.10364687, Gradient norm: 3.05564617
INFO:root:[   22] Training loss: 0.05700156, Validation loss: 0.07201169, Gradient norm: 2.19166247
INFO:root:[   17] Training loss: 0.05912173, Validation loss: 0.09850716, Gradient norm: 2.84537315
INFO:root:[   23] Training loss: 0.05537735, Validation loss: 0.06057695, Gradient norm: 2.13991539
INFO:root:[   18] Training loss: 0.06113955, Validation loss: 0.10801928, Gradient norm: 3.18046207
INFO:root:[   24] Training loss: 0.05347854, Validation loss: 0.05718062, Gradient norm: 1.88644625
INFO:root:[   19] Training loss: 0.05766303, Validation loss: 0.10223987, Gradient norm: 2.98101139
INFO:root:[   25] Training loss: 0.05340919, Validation loss: 0.05123139, Gradient norm: 2.05789710
INFO:root:[   20] Training loss: 0.05484452, Validation loss: 0.10284250, Gradient norm: 2.72348023
INFO:root:[   26] Training loss: 0.05088258, Validation loss: 0.05702996, Gradient norm: 1.91893915
INFO:root:[   21] Training loss: 0.05507043, Validation loss: 0.08939779, Gradient norm: 3.02500153
INFO:root:[   27] Training loss: 0.05211642, Validation loss: 0.07459319, Gradient norm: 2.03344869
INFO:root:[   22] Training loss: 0.05499921, Validation loss: 0.10144446, Gradient norm: 2.66482180
INFO:root:[   28] Training loss: 0.05152160, Validation loss: 0.05341384, Gradient norm: 1.99949201
INFO:root:[   23] Training loss: 0.05595032, Validation loss: 0.08801967, Gradient norm: 2.94185012
INFO:root:[   29] Training loss: 0.05024517, Validation loss: 0.05626003, Gradient norm: 1.96675367
INFO:root:[   24] Training loss: 0.05230162, Validation loss: 0.09846824, Gradient norm: 2.53351361
INFO:root:[   30] Training loss: 0.05049831, Validation loss: 0.05304933, Gradient norm: 1.92367964
INFO:root:[   25] Training loss: 0.05235984, Validation loss: 0.09439653, Gradient norm: 2.90245604
INFO:root:[   31] Training loss: 0.05033898, Validation loss: 0.06512010, Gradient norm: 1.90146118
INFO:root:[   26] Training loss: 0.05129907, Validation loss: 0.08193430, Gradient norm: 2.85155740
INFO:root:[   32] Training loss: 0.04832293, Validation loss: 0.05122021, Gradient norm: 1.97072525
INFO:root:[   27] Training loss: 0.05394635, Validation loss: 0.08260692, Gradient norm: 3.14220416
INFO:root:[   33] Training loss: 0.04700985, Validation loss: 0.05864967, Gradient norm: 1.75418627
INFO:root:[   28] Training loss: 0.05174493, Validation loss: 0.09385824, Gradient norm: 2.76506972
INFO:root:[   34] Training loss: 0.04874430, Validation loss: 0.05615526, Gradient norm: 2.00503283
INFO:root:[   29] Training loss: 0.05462984, Validation loss: 0.08215230, Gradient norm: 2.80887938
INFO:root:[   35] Training loss: 0.04656695, Validation loss: 0.07384651, Gradient norm: 1.94119844
INFO:root:[   30] Training loss: 0.04963109, Validation loss: 0.07507099, Gradient norm: 2.65511599
INFO:root:[   36] Training loss: 0.04959550, Validation loss: 0.06072141, Gradient norm: 1.97622267
INFO:root:[   31] Training loss: 0.05025004, Validation loss: 0.07831607, Gradient norm: 2.47368658
INFO:root:[   32] Training loss: 0.04881876, Validation loss: 0.08354830, Gradient norm: 2.78563137
INFO:root:[   37] Training loss: 0.04673227, Validation loss: 0.05319406, Gradient norm: 1.65409320
INFO:root:[   33] Training loss: 0.04674296, Validation loss: 0.07185429, Gradient norm: 2.36997945
INFO:root:[   38] Training loss: 0.04611201, Validation loss: 0.05364659, Gradient norm: 1.81739653
INFO:root:[   34] Training loss: 0.04981389, Validation loss: 0.07790838, Gradient norm: 3.01032637
INFO:root:[   39] Training loss: 0.04888812, Validation loss: 0.06491443, Gradient norm: 2.10497571
INFO:root:[   35] Training loss: 0.04942463, Validation loss: 0.08278443, Gradient norm: 2.66021446
INFO:root:[   40] Training loss: 0.04435997, Validation loss: 0.05271923, Gradient norm: 1.82065276
INFO:root:[   36] Training loss: 0.04840537, Validation loss: 0.08293502, Gradient norm: 2.54428934
INFO:root:[   41] Training loss: 0.04489760, Validation loss: 0.04913720, Gradient norm: 1.83840092
INFO:root:[   37] Training loss: 0.04846920, Validation loss: 0.07194193, Gradient norm: 3.01079049
INFO:root:[   42] Training loss: 0.04476819, Validation loss: 0.04879242, Gradient norm: 1.77006824
INFO:root:[   38] Training loss: 0.04624386, Validation loss: 0.08218561, Gradient norm: 2.56690736
INFO:root:[   43] Training loss: 0.04632104, Validation loss: 0.06135511, Gradient norm: 1.91025816
INFO:root:[   39] Training loss: 0.04629214, Validation loss: 0.07352739, Gradient norm: 2.93602964
INFO:root:[   44] Training loss: 0.04603443, Validation loss: 0.06198665, Gradient norm: 2.12527631
INFO:root:[   40] Training loss: 0.04590817, Validation loss: 0.08038572, Gradient norm: 2.53239953
INFO:root:[   45] Training loss: 0.04419294, Validation loss: 0.05670720, Gradient norm: 1.94168725
INFO:root:[   41] Training loss: 0.04750827, Validation loss: 0.07728091, Gradient norm: 2.68706822
INFO:root:[   46] Training loss: 0.04756662, Validation loss: 0.06016389, Gradient norm: 1.91038928
INFO:root:[   42] Training loss: 0.04417784, Validation loss: 0.07427224, Gradient norm: 2.50026554
INFO:root:[   47] Training loss: 0.04264718, Validation loss: 0.05252521, Gradient norm: 1.77077662
INFO:root:[   43] Training loss: 0.04256980, Validation loss: 0.07939299, Gradient norm: 2.50244429
INFO:root:[   48] Training loss: 0.04362082, Validation loss: 0.05264429, Gradient norm: 1.99092414
INFO:root:[   44] Training loss: 0.04274097, Validation loss: 0.07858728, Gradient norm: 2.26419641
INFO:root:[   49] Training loss: 0.04371304, Validation loss: 0.05663151, Gradient norm: 2.00743394
INFO:root:[   45] Training loss: 0.04541124, Validation loss: 0.07272323, Gradient norm: 2.62350367
INFO:root:[   50] Training loss: 0.04620824, Validation loss: 0.05705973, Gradient norm: 1.94839918
INFO:root:[   46] Training loss: 0.04532610, Validation loss: 0.07708184, Gradient norm: 2.67039667
INFO:root:[   51] Training loss: 0.04293087, Validation loss: 0.06568020, Gradient norm: 1.79843298
INFO:root:[   47] Training loss: 0.04421169, Validation loss: 0.06661561, Gradient norm: 2.69240261
INFO:root:[   52] Training loss: 0.04079610, Validation loss: 0.04789492, Gradient norm: 1.86527869
INFO:root:[   48] Training loss: 0.04385140, Validation loss: 0.06620126, Gradient norm: 2.58832217
INFO:root:[   53] Training loss: 0.04374618, Validation loss: 0.06596500, Gradient norm: 1.82193893
INFO:root:[   49] Training loss: 0.04190553, Validation loss: 0.07888265, Gradient norm: 2.53832202
INFO:root:[   54] Training loss: 0.04152271, Validation loss: 0.05437367, Gradient norm: 1.89509707
INFO:root:[   50] Training loss: 0.04173343, Validation loss: 0.08486621, Gradient norm: 2.43672298
INFO:root:[   55] Training loss: 0.04066038, Validation loss: 0.05567494, Gradient norm: 1.91029483
INFO:root:[   51] Training loss: 0.04250470, Validation loss: 0.07111684, Gradient norm: 2.62431986
INFO:root:[   56] Training loss: 0.04173635, Validation loss: 0.06683603, Gradient norm: 1.82071946
INFO:root:[   52] Training loss: 0.04347869, Validation loss: 0.06824155, Gradient norm: 2.56954260
INFO:root:[   57] Training loss: 0.04166208, Validation loss: 0.05133179, Gradient norm: 1.83923933
INFO:root:[   53] Training loss: 0.04159383, Validation loss: 0.07633052, Gradient norm: 2.77331007
INFO:root:[   58] Training loss: 0.04055317, Validation loss: 0.06041925, Gradient norm: 1.60206997
INFO:root:[   54] Training loss: 0.04076938, Validation loss: 0.06541861, Gradient norm: 2.60785234
INFO:root:[   59] Training loss: 0.04001862, Validation loss: 0.05237335, Gradient norm: 1.85810867
INFO:root:[   55] Training loss: 0.04070957, Validation loss: 0.07441900, Gradient norm: 2.29184494
INFO:root:[   60] Training loss: 0.04263520, Validation loss: 0.05519570, Gradient norm: 1.95295375
INFO:root:[   56] Training loss: 0.04053840, Validation loss: 0.06261903, Gradient norm: 2.48385539
INFO:root:[   57] Training loss: 0.04058848, Validation loss: 0.07709315, Gradient norm: 2.43141703
INFO:root:[   61] Training loss: 0.04022887, Validation loss: 0.05642880, Gradient norm: 1.87144273
INFO:root:[   58] Training loss: 0.03971063, Validation loss: 0.07116995, Gradient norm: 2.83531498
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 2206.879s.
INFO:root:Emptying the cuda cache took 0.028s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:[   59] Training loss: 0.04036640, Validation loss: 0.06653611, Gradient norm: 2.52421014
INFO:root:MSETrain: 0.042
INFO:root:EnergyScoreTrain: 0.03038
INFO:root:CoverageTrain: 0.85932
INFO:root:IntervalWidthTrain: 0.02788
INFO:root:Evaluating the model on Validation data.
INFO:root:[   60] Training loss: 0.04090340, Validation loss: 0.06775696, Gradient norm: 2.43157909
INFO:root:MSEValidation: 0.06475
INFO:root:EnergyScoreValidation: 0.04792
INFO:root:CoverageValidation: 0.68326
INFO:root:IntervalWidthValidation: 0.02689
INFO:root:Evaluating the model on Test data.
INFO:root:[   61] Training loss: 0.03910144, Validation loss: 0.07045342, Gradient norm: 2.55879795
INFO:root:MSETest: 0.06476
INFO:root:EnergyScoreTest: 0.04803
INFO:root:CoverageTest: 0.6807
INFO:root:IntervalWidthTest: 0.02676
INFO:root:[   62] Training loss: 0.03954596, Validation loss: 0.07410298, Gradient norm: 2.45205621
INFO:root:###6 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[   63] Training loss: 0.03896563, Validation loss: 0.07412818, Gradient norm: 2.54472029
INFO:root:[    1] Training loss: 0.31893402, Validation loss: 0.17422729, Gradient norm: 3.49403378
INFO:root:[   64] Training loss: 0.04067808, Validation loss: 0.06598284, Gradient norm: 2.98293250
INFO:root:[    2] Training loss: 0.12847622, Validation loss: 0.09778210, Gradient norm: 2.36259063
INFO:root:[   65] Training loss: 0.03883531, Validation loss: 0.06905092, Gradient norm: 2.50559146
INFO:root:[    3] Training loss: 0.10711261, Validation loss: 0.09053497, Gradient norm: 2.83397096
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 2251.919s.
INFO:root:Emptying the cuda cache took 0.012s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:[    4] Training loss: 0.09612737, Validation loss: 0.08795760, Gradient norm: 2.62810254
INFO:root:[    5] Training loss: 0.08765051, Validation loss: 0.08550730, Gradient norm: 2.42455333
INFO:root:Evaluating the model on Train data.
INFO:root:[    6] Training loss: 0.08433143, Validation loss: 0.10112709, Gradient norm: 2.53019056
INFO:root:MSETrain: 0.19065
INFO:root:EnergyScoreTrain: 0.09818
INFO:root:CoverageTrain: 0.7385
INFO:root:IntervalWidthTrain: 0.10079
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.19306
INFO:root:EnergyScoreValidation: 0.09558
INFO:root:CoverageValidation: 0.78003
INFO:root:IntervalWidthValidation: 0.1059
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.22656
INFO:root:EnergyScoreTest: 0.11297
INFO:root:CoverageTest: 0.69843
INFO:root:IntervalWidthTest: 0.10367
INFO:root:[    7] Training loss: 0.07970595, Validation loss: 0.08054375, Gradient norm: 2.33482108
INFO:root:###6 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1245708288
INFO:root:Training starts now.
INFO:root:[    8] Training loss: 0.07840627, Validation loss: 0.07242135, Gradient norm: 2.66379183
INFO:root:[    1] Training loss: 0.25224652, Validation loss: 0.17542720, Gradient norm: 3.93726132
INFO:root:[    9] Training loss: 0.07096003, Validation loss: 0.10016257, Gradient norm: 2.17875925
INFO:root:[    2] Training loss: 0.12950183, Validation loss: 0.14535457, Gradient norm: 4.04012476
INFO:root:[   10] Training loss: 0.07148257, Validation loss: 0.06886364, Gradient norm: 2.48378737
INFO:root:[    3] Training loss: 0.11366010, Validation loss: 0.12789129, Gradient norm: 4.60226803
INFO:root:[   11] Training loss: 0.06873725, Validation loss: 0.09233039, Gradient norm: 2.36268042
INFO:root:[    4] Training loss: 0.09870990, Validation loss: 0.12742332, Gradient norm: 3.95689565
INFO:root:[   12] Training loss: 0.06457779, Validation loss: 0.06617249, Gradient norm: 2.19634176
INFO:root:[    5] Training loss: 0.09228138, Validation loss: 0.11016803, Gradient norm: 3.75329479
INFO:root:[   13] Training loss: 0.06636275, Validation loss: 0.06172590, Gradient norm: 2.50361344
INFO:root:[    6] Training loss: 0.08604087, Validation loss: 0.10134098, Gradient norm: 4.34025689
INFO:root:[   14] Training loss: 0.06064951, Validation loss: 0.08051542, Gradient norm: 2.07152617
INFO:root:[    7] Training loss: 0.08247691, Validation loss: 0.10277416, Gradient norm: 4.35462101
INFO:root:[   15] Training loss: 0.06012967, Validation loss: 0.07485481, Gradient norm: 1.98758590
INFO:root:[    8] Training loss: 0.07809815, Validation loss: 0.10665197, Gradient norm: 3.69442769
INFO:root:[   16] Training loss: 0.06157431, Validation loss: 0.06287187, Gradient norm: 2.25503950
INFO:root:[    9] Training loss: 0.07409658, Validation loss: 0.13056524, Gradient norm: 3.80386935
INFO:root:[   17] Training loss: 0.06012366, Validation loss: 0.06938945, Gradient norm: 2.30778130
INFO:root:[   10] Training loss: 0.07790281, Validation loss: 0.09239769, Gradient norm: 3.80105304
INFO:root:[   18] Training loss: 0.05866967, Validation loss: 0.06173804, Gradient norm: 2.23715138
INFO:root:[   11] Training loss: 0.07473548, Validation loss: 0.09456336, Gradient norm: 3.67178673
INFO:root:[   12] Training loss: 0.06902847, Validation loss: 0.09413310, Gradient norm: 3.14231667
INFO:root:[   19] Training loss: 0.05654895, Validation loss: 0.08002247, Gradient norm: 1.89453559
INFO:root:[   13] Training loss: 0.06552379, Validation loss: 0.08353999, Gradient norm: 3.31831063
INFO:root:[   20] Training loss: 0.05962609, Validation loss: 0.06396428, Gradient norm: 2.24003592
INFO:root:[   14] Training loss: 0.06803903, Validation loss: 0.09349860, Gradient norm: 3.43900774
INFO:root:[   21] Training loss: 0.05529475, Validation loss: 0.07983599, Gradient norm: 1.95538309
INFO:root:[   15] Training loss: 0.06488372, Validation loss: 0.09355565, Gradient norm: 3.42789078
INFO:root:[   22] Training loss: 0.05713845, Validation loss: 0.06520870, Gradient norm: 2.17243858
INFO:root:[   16] Training loss: 0.06420918, Validation loss: 0.10575776, Gradient norm: 3.50168051
INFO:root:[   23] Training loss: 0.05776308, Validation loss: 0.06098676, Gradient norm: 2.28143940
INFO:root:[   17] Training loss: 0.06118054, Validation loss: 0.08074695, Gradient norm: 3.11526595
INFO:root:[   24] Training loss: 0.05592143, Validation loss: 0.06424596, Gradient norm: 2.13260816
INFO:root:[   18] Training loss: 0.05830882, Validation loss: 0.08348633, Gradient norm: 2.71498639
INFO:root:[   25] Training loss: 0.05326176, Validation loss: 0.06541187, Gradient norm: 2.05733913
INFO:root:[   19] Training loss: 0.05885634, Validation loss: 0.07913884, Gradient norm: 3.36444756
INFO:root:[   26] Training loss: 0.05160798, Validation loss: 0.06371960, Gradient norm: 1.91927543
INFO:root:[   20] Training loss: 0.06042126, Validation loss: 0.09081255, Gradient norm: 3.17527769
INFO:root:[   27] Training loss: 0.05106325, Validation loss: 0.07857971, Gradient norm: 2.01951892
INFO:root:[   21] Training loss: 0.05828826, Validation loss: 0.09292833, Gradient norm: 3.14589265
INFO:root:[   28] Training loss: 0.05312884, Validation loss: 0.06689011, Gradient norm: 2.05100391
INFO:root:[   22] Training loss: 0.05680946, Validation loss: 0.08114516, Gradient norm: 3.07364206
INFO:root:[   29] Training loss: 0.04862412, Validation loss: 0.06419280, Gradient norm: 1.89523428
INFO:root:[   23] Training loss: 0.05598997, Validation loss: 0.08164925, Gradient norm: 2.98949068
INFO:root:[   30] Training loss: 0.05106028, Validation loss: 0.06565202, Gradient norm: 1.91923899
INFO:root:[   24] Training loss: 0.05373240, Validation loss: 0.07776664, Gradient norm: 2.89138245
INFO:root:[   31] Training loss: 0.05109486, Validation loss: 0.06534032, Gradient norm: 1.92399965
INFO:root:[   25] Training loss: 0.05783989, Validation loss: 0.08983511, Gradient norm: 3.31185699
INFO:root:[   32] Training loss: 0.04877391, Validation loss: 0.06584770, Gradient norm: 1.91089291
INFO:root:[   26] Training loss: 0.05361667, Validation loss: 0.09329944, Gradient norm: 2.69114815
INFO:root:[   33] Training loss: 0.05052847, Validation loss: 0.06832592, Gradient norm: 2.17328545
INFO:root:[   27] Training loss: 0.05136352, Validation loss: 0.08550625, Gradient norm: 2.44478040
INFO:root:[   34] Training loss: 0.04933491, Validation loss: 0.06481903, Gradient norm: 2.14748238
INFO:root:[   28] Training loss: 0.05350963, Validation loss: 0.07914898, Gradient norm: 3.37877057
INFO:root:[   35] Training loss: 0.05119481, Validation loss: 0.06145686, Gradient norm: 2.11667710
INFO:root:[   29] Training loss: 0.05121412, Validation loss: 0.09588578, Gradient norm: 2.80621189
INFO:root:[   36] Training loss: 0.04744968, Validation loss: 0.06477416, Gradient norm: 1.86314506
INFO:root:[   30] Training loss: 0.05368953, Validation loss: 0.08037813, Gradient norm: 3.24285784
INFO:root:[   37] Training loss: 0.04793791, Validation loss: 0.06292180, Gradient norm: 1.82198892
INFO:root:[   31] Training loss: 0.04912568, Validation loss: 0.08273835, Gradient norm: 2.50470888
INFO:root:[   38] Training loss: 0.04498995, Validation loss: 0.06012999, Gradient norm: 1.88594108
INFO:root:[   32] Training loss: 0.04940740, Validation loss: 0.09070497, Gradient norm: 2.79509326
INFO:root:[   39] Training loss: 0.04759989, Validation loss: 0.06397610, Gradient norm: 1.92787925
INFO:root:[   33] Training loss: 0.04937205, Validation loss: 0.09594354, Gradient norm: 2.44789776
INFO:root:[   34] Training loss: 0.04853465, Validation loss: 0.08074525, Gradient norm: 2.69782172
INFO:root:[   40] Training loss: 0.04599461, Validation loss: 0.07506460, Gradient norm: 1.83252099
INFO:root:[   35] Training loss: 0.04829332, Validation loss: 0.07876541, Gradient norm: 2.78196776
INFO:root:[   41] Training loss: 0.04658745, Validation loss: 0.06453628, Gradient norm: 1.70348945
INFO:root:[   36] Training loss: 0.04763924, Validation loss: 0.08842623, Gradient norm: 2.77787445
INFO:root:[   42] Training loss: 0.04580757, Validation loss: 0.06241033, Gradient norm: 1.99482338
INFO:root:[   37] Training loss: 0.05182935, Validation loss: 0.07925879, Gradient norm: 3.24651456
INFO:root:[   43] Training loss: 0.04555864, Validation loss: 0.07666929, Gradient norm: 1.92522039
INFO:root:[   38] Training loss: 0.04861542, Validation loss: 0.07295509, Gradient norm: 3.16995693
INFO:root:[   44] Training loss: 0.04581911, Validation loss: 0.06597432, Gradient norm: 2.03899781
INFO:root:[   39] Training loss: 0.04727353, Validation loss: 0.07822007, Gradient norm: 2.86134708
INFO:root:[   45] Training loss: 0.04633928, Validation loss: 0.06461291, Gradient norm: 1.90038092
INFO:root:[   40] Training loss: 0.05002437, Validation loss: 0.07867693, Gradient norm: 2.94062379
INFO:root:[   46] Training loss: 0.04397322, Validation loss: 0.05899723, Gradient norm: 1.97123755
INFO:root:[   41] Training loss: 0.04955425, Validation loss: 0.07463152, Gradient norm: 3.10227217
INFO:root:[   47] Training loss: 0.04470355, Validation loss: 0.06175598, Gradient norm: 1.96791905
INFO:root:[   42] Training loss: 0.04544469, Validation loss: 0.07294472, Gradient norm: 2.72268901
INFO:root:[   48] Training loss: 0.04422256, Validation loss: 0.05891674, Gradient norm: 2.05262149
INFO:root:[   43] Training loss: 0.04503743, Validation loss: 0.07828456, Gradient norm: 2.66698511
INFO:root:[   49] Training loss: 0.04562880, Validation loss: 0.08140691, Gradient norm: 1.63302083
INFO:root:[   44] Training loss: 0.04497476, Validation loss: 0.06719471, Gradient norm: 2.65437386
INFO:root:[   50] Training loss: 0.04151201, Validation loss: 0.06125283, Gradient norm: 1.67970015
INFO:root:[   45] Training loss: 0.04678494, Validation loss: 0.08766560, Gradient norm: 3.03315239
INFO:root:[   51] Training loss: 0.04540582, Validation loss: 0.08090240, Gradient norm: 2.00401746
INFO:root:[   46] Training loss: 0.04627168, Validation loss: 0.08228185, Gradient norm: 2.66734597
INFO:root:[   52] Training loss: 0.04191873, Validation loss: 0.05865462, Gradient norm: 1.75369850
INFO:root:[   47] Training loss: 0.04699440, Validation loss: 0.08002597, Gradient norm: 2.99300559
INFO:root:[   53] Training loss: 0.04326493, Validation loss: 0.07568676, Gradient norm: 2.07944419
INFO:root:[   48] Training loss: 0.04525504, Validation loss: 0.07674141, Gradient norm: 2.90845504
INFO:root:[   54] Training loss: 0.04394330, Validation loss: 0.05910511, Gradient norm: 2.02350272
INFO:root:[   49] Training loss: 0.04625578, Validation loss: 0.07557494, Gradient norm: 2.69442788
INFO:root:[   50] Training loss: 0.04280207, Validation loss: 0.07362585, Gradient norm: 2.36656457
INFO:root:[   55] Training loss: 0.04273496, Validation loss: 0.07770432, Gradient norm: 1.88092989
INFO:root:[   51] Training loss: 0.04313249, Validation loss: 0.06701823, Gradient norm: 2.78082123
INFO:root:[   56] Training loss: 0.04202261, Validation loss: 0.06176039, Gradient norm: 1.86378120
INFO:root:[   57] Training loss: 0.04140601, Validation loss: 0.06348698, Gradient norm: 1.77339693
INFO:root:[   58] Training loss: 0.04279451, Validation loss: 0.05953605, Gradient norm: 2.05768454
INFO:root:[   59] Training loss: 0.04250791, Validation loss: 0.07086227, Gradient norm: 1.83533409
INFO:root:[   60] Training loss: 0.04189816, Validation loss: 0.07460032, Gradient norm: 2.00229615
INFO:root:[   61] Training loss: 0.04148436, Validation loss: 0.06086010, Gradient norm: 1.92537997
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 245297.758s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03731
INFO:root:EnergyScoreTrain: 0.02744
INFO:root:CoverageTrain: 0.88826
INFO:root:IntervalWidthTrain: 0.02825
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07772
INFO:root:EnergyScoreValidation: 0.059
INFO:root:CoverageValidation: 0.59906
INFO:root:IntervalWidthValidation: 0.02709
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07929
INFO:root:EnergyScoreTest: 0.06051
INFO:root:CoverageTest: 0.59307
INFO:root:IntervalWidthTest: 0.02708
INFO:root:###7 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234567, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.30215981, Validation loss: 0.13511725, Gradient norm: 3.25393127
INFO:root:[    2] Training loss: 0.13326830, Validation loss: 0.09696797, Gradient norm: 2.85137904
INFO:root:[    3] Training loss: 0.10567864, Validation loss: 0.09093662, Gradient norm: 2.45892235
INFO:root:[    4] Training loss: 0.09311078, Validation loss: 0.09580761, Gradient norm: 2.55756992
INFO:root:[    5] Training loss: 0.08989507, Validation loss: 0.08364280, Gradient norm: 2.77521212
INFO:root:[    6] Training loss: 0.08179166, Validation loss: 0.07739828, Gradient norm: 2.53068698
INFO:root:[    7] Training loss: 0.07997643, Validation loss: 0.08246140, Gradient norm: 2.66533104
INFO:root:[    8] Training loss: 0.07620505, Validation loss: 0.08860713, Gradient norm: 2.52001511
INFO:root:[    9] Training loss: 0.07500072, Validation loss: 0.07141006, Gradient norm: 2.71904669
INFO:root:[   10] Training loss: 0.06706903, Validation loss: 0.07453752, Gradient norm: 2.22207208
INFO:root:[   11] Training loss: 0.06807515, Validation loss: 0.07200269, Gradient norm: 2.43486807
INFO:root:[   12] Training loss: 0.06691874, Validation loss: 0.08899800, Gradient norm: 2.43941483
INFO:root:[   13] Training loss: 0.06613488, Validation loss: 0.07880724, Gradient norm: 2.47269964
INFO:root:[   14] Training loss: 0.06090437, Validation loss: 0.08372851, Gradient norm: 2.03458758
INFO:root:[   15] Training loss: 0.06241182, Validation loss: 0.07178766, Gradient norm: 2.36937602
INFO:root:[   16] Training loss: 0.06039372, Validation loss: 0.09182517, Gradient norm: 2.19472921
INFO:root:[   17] Training loss: 0.06095351, Validation loss: 0.07036041, Gradient norm: 2.33249334
INFO:root:[   18] Training loss: 0.05768676, Validation loss: 0.07333775, Gradient norm: 2.16630559
INFO:root:[   19] Training loss: 0.06072549, Validation loss: 0.07883197, Gradient norm: 2.30944972
INFO:root:[   20] Training loss: 0.05632696, Validation loss: 0.07202929, Gradient norm: 1.99043626
INFO:root:[   21] Training loss: 0.05761437, Validation loss: 0.08722935, Gradient norm: 2.20370241
INFO:root:[   22] Training loss: 0.05502217, Validation loss: 0.07957349, Gradient norm: 2.01502420
INFO:root:[   23] Training loss: 0.05718046, Validation loss: 0.08758719, Gradient norm: 2.20518018
INFO:root:[   24] Training loss: 0.05594051, Validation loss: 0.08165349, Gradient norm: 2.16689622
INFO:root:[   25] Training loss: 0.05243800, Validation loss: 0.07784625, Gradient norm: 1.87105890
