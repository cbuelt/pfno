INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 12, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.25927435, Validation loss: 0.21198577, Gradient norm: 3.38584274
INFO:root:[    2] Training loss: 0.21029575, Validation loss: 0.19987594, Gradient norm: 2.27722370
INFO:root:[    3] Training loss: 0.20159924, Validation loss: 0.20114550, Gradient norm: 1.84038306
INFO:root:[    4] Training loss: 0.19578027, Validation loss: 0.19347475, Gradient norm: 1.60691462
INFO:root:[    5] Training loss: 0.19012951, Validation loss: 0.18880542, Gradient norm: 1.36384325
INFO:root:[    6] Training loss: 0.18472152, Validation loss: 0.18410619, Gradient norm: 1.30642088
INFO:root:[    7] Training loss: 0.17935621, Validation loss: 0.18353310, Gradient norm: 1.20804485
INFO:root:[    8] Training loss: 0.17300892, Validation loss: 0.17642098, Gradient norm: 1.09551224
INFO:root:[    9] Training loss: 0.16793522, Validation loss: 0.17603943, Gradient norm: 1.14543658
INFO:root:[   10] Training loss: 0.16267313, Validation loss: 0.17094684, Gradient norm: 1.07649689
INFO:root:[   11] Training loss: 0.15890896, Validation loss: 0.16486903, Gradient norm: 1.09930456
INFO:root:[   12] Training loss: 0.15517198, Validation loss: 0.16436779, Gradient norm: 1.07322577
INFO:root:[   13] Training loss: 0.15223352, Validation loss: 0.16640067, Gradient norm: 1.06804533
INFO:root:[   14] Training loss: 0.14935890, Validation loss: 0.15902499, Gradient norm: 0.98685913
INFO:root:[   15] Training loss: 0.14737073, Validation loss: 0.15683690, Gradient norm: 1.00258942
INFO:root:[   16] Training loss: 0.14513811, Validation loss: 0.15723447, Gradient norm: 1.04008802
INFO:root:[   17] Training loss: 0.14297625, Validation loss: 0.16920360, Gradient norm: 1.00442274
INFO:root:[   18] Training loss: 0.14180840, Validation loss: 0.15415131, Gradient norm: 1.07081212
INFO:root:[   19] Training loss: 0.13987153, Validation loss: 0.15205926, Gradient norm: 0.97792511
INFO:root:[   20] Training loss: 0.13860541, Validation loss: 0.15217291, Gradient norm: 0.93267775
INFO:root:[   21] Training loss: 0.13736104, Validation loss: 0.14833523, Gradient norm: 1.00202548
INFO:root:[   22] Training loss: 0.13627894, Validation loss: 0.14826416, Gradient norm: 0.96609848
