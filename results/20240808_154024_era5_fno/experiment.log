INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file era5/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'era5', 'max_training_set_size': 1000, 'init_steps': 10, 'pred_horizon': 10}
INFO:root:###1 out of 4 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.29552443, Validation loss: 0.20776778, Gradient norm: 1.94024488
INFO:root:[    2] Training loss: 0.21668781, Validation loss: 0.19823373, Gradient norm: 1.33844880
INFO:root:[    3] Training loss: 0.20849729, Validation loss: 0.20171057, Gradient norm: 1.24380915
INFO:root:[    4] Training loss: 0.20317960, Validation loss: 0.18903103, Gradient norm: 1.18788608
INFO:root:[    5] Training loss: 0.19937563, Validation loss: 0.19265450, Gradient norm: 1.20842280
INFO:root:[    6] Training loss: 0.19452862, Validation loss: 0.18522579, Gradient norm: 1.10083815
INFO:root:[    7] Training loss: 0.19074899, Validation loss: 0.18765947, Gradient norm: 1.07638278
INFO:root:[    8] Training loss: 0.18671509, Validation loss: 0.17975612, Gradient norm: 1.04838278
INFO:root:[    9] Training loss: 0.18316170, Validation loss: 0.18880311, Gradient norm: 1.05014811
INFO:root:[   10] Training loss: 0.17935187, Validation loss: 0.18053007, Gradient norm: 1.08063765
INFO:root:[   11] Training loss: 0.17664132, Validation loss: 0.18560057, Gradient norm: 1.11551548
INFO:root:[   12] Training loss: 0.17303205, Validation loss: 0.17309091, Gradient norm: 1.04077973
INFO:root:[   13] Training loss: 0.17046092, Validation loss: 0.17153688, Gradient norm: 1.08276433
INFO:root:[   14] Training loss: 0.16877445, Validation loss: 0.16569570, Gradient norm: 1.13295884
INFO:root:[   15] Training loss: 0.16590919, Validation loss: 0.17750374, Gradient norm: 1.03770623
INFO:root:[   16] Training loss: 0.16485304, Validation loss: 0.16858972, Gradient norm: 1.11837124
INFO:root:[   17] Training loss: 0.16310069, Validation loss: 0.17103797, Gradient norm: 1.08286093
INFO:root:[   18] Training loss: 0.16152916, Validation loss: 0.15980853, Gradient norm: 1.07245790
INFO:root:[   19] Training loss: 0.16007679, Validation loss: 0.15922777, Gradient norm: 1.08114571
INFO:root:[   20] Training loss: 0.15866161, Validation loss: 0.16897332, Gradient norm: 1.01831053
INFO:root:[   21] Training loss: 0.15752664, Validation loss: 0.15745040, Gradient norm: 1.04878071
INFO:root:[   22] Training loss: 0.15697928, Validation loss: 0.15700144, Gradient norm: 1.10509200
INFO:root:[   23] Training loss: 0.15506633, Validation loss: 0.15543131, Gradient norm: 1.00850616
INFO:root:[   24] Training loss: 0.15431797, Validation loss: 0.15563278, Gradient norm: 1.03516253
INFO:root:[   25] Training loss: 0.15368743, Validation loss: 0.16517748, Gradient norm: 1.04116435
INFO:root:[   26] Training loss: 0.15334800, Validation loss: 0.14958002, Gradient norm: 1.08541573
INFO:root:[   27] Training loss: 0.15238435, Validation loss: 0.15316060, Gradient norm: 1.03974549
INFO:root:[   28] Training loss: 0.15190146, Validation loss: 0.15385738, Gradient norm: 1.09048779
INFO:root:[   29] Training loss: 0.15062914, Validation loss: 0.15845124, Gradient norm: 1.00920457
INFO:root:[   30] Training loss: 0.15013984, Validation loss: 0.14960324, Gradient norm: 1.02065569
INFO:root:[   31] Training loss: 0.15010097, Validation loss: 0.15020370, Gradient norm: 1.08871182
INFO:root:[   32] Training loss: 0.14850040, Validation loss: 0.14761578, Gradient norm: 0.96145467
INFO:root:[   33] Training loss: 0.14854932, Validation loss: 0.15016018, Gradient norm: 1.04556995
INFO:root:[   34] Training loss: 0.14812709, Validation loss: 0.14623039, Gradient norm: 1.05499995
INFO:root:[   35] Training loss: 0.14797325, Validation loss: 0.14628151, Gradient norm: 1.05505465
INFO:root:[   36] Training loss: 0.14667873, Validation loss: 0.14605358, Gradient norm: 0.99129333
INFO:root:[   37] Training loss: 0.14649169, Validation loss: 0.14524552, Gradient norm: 1.01177493
INFO:root:[   38] Training loss: 0.14608241, Validation loss: 0.14816678, Gradient norm: 0.99453408
INFO:root:[   39] Training loss: 0.14576122, Validation loss: 0.14459665, Gradient norm: 1.02344130
INFO:root:[   40] Training loss: 0.14486466, Validation loss: 0.14278158, Gradient norm: 0.94243159
INFO:root:[   41] Training loss: 0.14451630, Validation loss: 0.14974785, Gradient norm: 0.94781738
INFO:root:[   42] Training loss: 0.14486488, Validation loss: 0.14472931, Gradient norm: 1.06221731
INFO:root:[   43] Training loss: 0.14374948, Validation loss: 0.15031578, Gradient norm: 0.94514076
INFO:root:[   44] Training loss: 0.14381858, Validation loss: 0.14602749, Gradient norm: 0.95223922
INFO:root:[   45] Training loss: 0.14257454, Validation loss: 0.14439740, Gradient norm: 0.93921634
INFO:root:[   46] Training loss: 0.14326581, Validation loss: 0.14724641, Gradient norm: 1.01143613
INFO:root:[   47] Training loss: 0.14270281, Validation loss: 0.14379112, Gradient norm: 1.01710029
INFO:root:[   48] Training loss: 0.14226365, Validation loss: 0.14271594, Gradient norm: 0.97872241
INFO:root:[   49] Training loss: 0.14193758, Validation loss: 0.14158390, Gradient norm: 0.95836712
INFO:root:[   50] Training loss: 0.14114312, Validation loss: 0.14107395, Gradient norm: 0.95757921
INFO:root:[   51] Training loss: 0.14157278, Validation loss: 0.14398458, Gradient norm: 0.98977160
INFO:root:[   52] Training loss: 0.14075524, Validation loss: 0.14096998, Gradient norm: 0.96770817
INFO:root:[   53] Training loss: 0.14112309, Validation loss: 0.14748577, Gradient norm: 1.00667641
INFO:root:[   54] Training loss: 0.14032625, Validation loss: 0.14037498, Gradient norm: 0.94796188
INFO:root:[   55] Training loss: 0.14021514, Validation loss: 0.13870154, Gradient norm: 0.94807689
INFO:root:[   56] Training loss: 0.13985472, Validation loss: 0.13998145, Gradient norm: 0.97705300
INFO:root:[   57] Training loss: 0.13974236, Validation loss: 0.13810087, Gradient norm: 0.93957537
INFO:root:[   58] Training loss: 0.13923067, Validation loss: 0.13982693, Gradient norm: 0.91535699
INFO:root:[   59] Training loss: 0.13880127, Validation loss: 0.14089601, Gradient norm: 0.93192464
INFO:root:[   60] Training loss: 0.13921442, Validation loss: 0.13847251, Gradient norm: 0.96135180
INFO:root:[   61] Training loss: 0.13902682, Validation loss: 0.14510360, Gradient norm: 0.97400076
INFO:root:[   62] Training loss: 0.13830703, Validation loss: 0.13870174, Gradient norm: 0.90091453
INFO:root:[   63] Training loss: 0.13806510, Validation loss: 0.14117750, Gradient norm: 0.90385528
INFO:root:[   64] Training loss: 0.13792388, Validation loss: 0.13598309, Gradient norm: 0.91707866
INFO:root:[   65] Training loss: 0.13764969, Validation loss: 0.14317033, Gradient norm: 0.94059153
INFO:root:[   66] Training loss: 0.13771403, Validation loss: 0.13655901, Gradient norm: 0.88323694
INFO:root:[   67] Training loss: 0.13734670, Validation loss: 0.14802484, Gradient norm: 0.93407621
INFO:root:[   68] Training loss: 0.13698304, Validation loss: 0.14077783, Gradient norm: 0.90060469
INFO:root:[   69] Training loss: 0.13700266, Validation loss: 0.14311969, Gradient norm: 0.91303572
INFO:root:[   70] Training loss: 0.13703835, Validation loss: 0.13584424, Gradient norm: 0.97412412
INFO:root:[   71] Training loss: 0.13669743, Validation loss: 0.13854401, Gradient norm: 0.92076572
INFO:root:[   72] Training loss: 0.13626117, Validation loss: 0.13622609, Gradient norm: 0.91030135
INFO:root:[   73] Training loss: 0.13620426, Validation loss: 0.13500375, Gradient norm: 0.91693865
INFO:root:[   74] Training loss: 0.13607125, Validation loss: 0.14132578, Gradient norm: 0.87549787
INFO:root:[   75] Training loss: 0.13601271, Validation loss: 0.14380908, Gradient norm: 0.94313204
INFO:root:[   76] Training loss: 0.13577831, Validation loss: 0.13661548, Gradient norm: 0.89483041
INFO:root:[   77] Training loss: 0.13556208, Validation loss: 0.13881027, Gradient norm: 0.91200078
INFO:root:[   78] Training loss: 0.13557750, Validation loss: 0.13524032, Gradient norm: 0.93252716
INFO:root:[   79] Training loss: 0.13533199, Validation loss: 0.13690268, Gradient norm: 0.91139882
INFO:root:[   80] Training loss: 0.13490357, Validation loss: 0.13310261, Gradient norm: 0.91296656
INFO:root:[   81] Training loss: 0.13499136, Validation loss: 0.13368086, Gradient norm: 0.87295805
INFO:root:[   82] Training loss: 0.13489242, Validation loss: 0.13593523, Gradient norm: 0.93125709
INFO:root:[   83] Training loss: 0.13458852, Validation loss: 0.13750608, Gradient norm: 0.86442608
INFO:root:[   84] Training loss: 0.13410989, Validation loss: 0.14194595, Gradient norm: 0.87516995
INFO:root:[   85] Training loss: 0.13436458, Validation loss: 0.13153856, Gradient norm: 0.91993625
INFO:root:[   86] Training loss: 0.13444498, Validation loss: 0.13512253, Gradient norm: 0.91186474
INFO:root:[   87] Training loss: 0.13426885, Validation loss: 0.14267251, Gradient norm: 0.88215659
INFO:root:[   88] Training loss: 0.13374037, Validation loss: 0.13642862, Gradient norm: 0.89267034
INFO:root:[   89] Training loss: 0.13394466, Validation loss: 0.13512909, Gradient norm: 0.90097332
INFO:root:[   90] Training loss: 0.13369340, Validation loss: 0.13409851, Gradient norm: 0.87648213
INFO:root:[   91] Training loss: 0.13354628, Validation loss: 0.13211711, Gradient norm: 0.87470126
INFO:root:[   92] Training loss: 0.13349695, Validation loss: 0.13113247, Gradient norm: 0.87700783
INFO:root:[   93] Training loss: 0.13344922, Validation loss: 0.13313528, Gradient norm: 0.92794952
INFO:root:[   94] Training loss: 0.13315477, Validation loss: 0.13636270, Gradient norm: 0.85057770
INFO:root:[   95] Training loss: 0.13303960, Validation loss: 0.13114391, Gradient norm: 0.88956905
INFO:root:[   96] Training loss: 0.13284666, Validation loss: 0.14130972, Gradient norm: 0.85972562
INFO:root:[   97] Training loss: 0.13293189, Validation loss: 0.13209180, Gradient norm: 0.87876964
INFO:root:[   98] Training loss: 0.13265958, Validation loss: 0.13009538, Gradient norm: 0.84604660
INFO:root:[   99] Training loss: 0.13268387, Validation loss: 0.13116792, Gradient norm: 0.88430489
INFO:root:[  100] Training loss: 0.13220624, Validation loss: 0.13312755, Gradient norm: 0.83848444
INFO:root:[  101] Training loss: 0.13264724, Validation loss: 0.13177490, Gradient norm: 0.92395445
INFO:root:[  102] Training loss: 0.13233311, Validation loss: 0.13085243, Gradient norm: 0.86192899
INFO:root:[  103] Training loss: 0.13242336, Validation loss: 0.13223687, Gradient norm: 0.86652077
INFO:root:[  104] Training loss: 0.13217739, Validation loss: 0.13588862, Gradient norm: 0.90507815
INFO:root:[  105] Training loss: 0.13195045, Validation loss: 0.13390321, Gradient norm: 0.84684050
INFO:root:[  106] Training loss: 0.13192717, Validation loss: 0.13111826, Gradient norm: 0.86911201
INFO:root:[  107] Training loss: 0.13175191, Validation loss: 0.13383834, Gradient norm: 0.85276254
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 56549.928s.
INFO:root:Emptying the cuda cache took 0.047s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.18239
INFO:root:EnergyScoreValidation: 0.12996
INFO:root:CoverageValidation: 0.85543
INFO:root:IntervalWidthValidation: 0.06782
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.30536
INFO:root:EnergyScoreTest: 0.22704
INFO:root:CoverageTest: 0.74912
INFO:root:IntervalWidthTest: 0.06283
INFO:root:###2 out of 4 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 587202560
INFO:root:Training starts now.
