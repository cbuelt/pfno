INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 2097152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 134.00781871, Validation loss: 124.66554050, Gradient norm: 939.34000551
INFO:root:[    2] Training loss: 122.58259967, Validation loss: 123.68938472, Gradient norm: 351.69042336
INFO:root:[    3] Training loss: 122.38156243, Validation loss: 122.78156780, Gradient norm: 240.21500659
INFO:root:[    4] Training loss: 122.11293334, Validation loss: 122.26446586, Gradient norm: 160.09584811
INFO:root:[    5] Training loss: 122.04135949, Validation loss: 122.17050039, Gradient norm: 143.69071439
INFO:root:[    6] Training loss: 121.87339742, Validation loss: 122.46416210, Gradient norm: 108.64086935
INFO:root:[    7] Training loss: 121.71175621, Validation loss: 122.04785419, Gradient norm: 89.81834856
INFO:root:[    8] Training loss: 121.83505958, Validation loss: 122.00260425, Gradient norm: 71.30437118
INFO:root:[    9] Training loss: 121.75957509, Validation loss: 122.43143884, Gradient norm: 52.60890071
INFO:root:[   10] Training loss: 121.68012190, Validation loss: 121.81785662, Gradient norm: 51.62909079
INFO:root:[   11] Training loss: 121.74881292, Validation loss: 122.05649646, Gradient norm: 46.51677540
INFO:root:[   12] Training loss: 121.72707441, Validation loss: 121.86782942, Gradient norm: 45.76459460
INFO:root:[   13] Training loss: 121.64320090, Validation loss: 121.80729754, Gradient norm: 38.71052005
INFO:root:[   14] Training loss: 121.66378501, Validation loss: 121.57277469, Gradient norm: 34.57140044
INFO:root:[   15] Training loss: 121.71438315, Validation loss: 121.50431876, Gradient norm: 29.45226479
INFO:root:[   16] Training loss: 121.49362392, Validation loss: 121.60237832, Gradient norm: 23.84386943
INFO:root:[   17] Training loss: 121.31540275, Validation loss: 121.20846821, Gradient norm: 26.90112193
INFO:root:[   18] Training loss: 120.83708177, Validation loss: 120.89129928, Gradient norm: 34.74921654
INFO:root:[   19] Training loss: 120.70013745, Validation loss: 120.64024406, Gradient norm: 42.75544051
INFO:root:[   20] Training loss: 120.70850588, Validation loss: 120.79426391, Gradient norm: 39.57344717
INFO:root:[   21] Training loss: 120.58568249, Validation loss: 120.80471670, Gradient norm: 40.38013044
INFO:root:[   22] Training loss: 120.50537602, Validation loss: 120.61878967, Gradient norm: 32.93153384
INFO:root:[   23] Training loss: 120.58467608, Validation loss: 120.78833797, Gradient norm: 34.22964937
INFO:root:[   24] Training loss: 120.48868284, Validation loss: 121.01014289, Gradient norm: 34.32909962
INFO:root:[   25] Training loss: 120.50530040, Validation loss: 120.57194808, Gradient norm: 34.79372616
INFO:root:[   26] Training loss: 120.46662174, Validation loss: 120.47773979, Gradient norm: 36.36149245
INFO:root:[   27] Training loss: 120.57095344, Validation loss: 120.50995531, Gradient norm: 33.12003642
INFO:root:[   28] Training loss: 120.47962837, Validation loss: 120.38411739, Gradient norm: 33.66677304
INFO:root:[   29] Training loss: 120.42432316, Validation loss: 120.39643649, Gradient norm: 32.17397080
INFO:root:[   30] Training loss: 120.49362378, Validation loss: 121.13637516, Gradient norm: 31.31149320
INFO:root:[   31] Training loss: 120.53760191, Validation loss: 120.28947107, Gradient norm: 30.58738781
INFO:root:[   32] Training loss: 120.44098569, Validation loss: 120.58174107, Gradient norm: 27.84887699
INFO:root:[   33] Training loss: 120.48041906, Validation loss: 120.29755296, Gradient norm: 31.11594453
INFO:root:[   34] Training loss: 120.34216106, Validation loss: 120.54604234, Gradient norm: 32.07339917
INFO:root:[   35] Training loss: 120.46558988, Validation loss: 120.35076010, Gradient norm: 20.47664704
INFO:root:[   36] Training loss: 120.23858710, Validation loss: 120.54066546, Gradient norm: 24.01635205
INFO:root:[   37] Training loss: 120.32786520, Validation loss: 120.46353702, Gradient norm: 20.61115009
INFO:root:[   38] Training loss: 120.18949897, Validation loss: 120.59791933, Gradient norm: 21.24135692
INFO:root:[   39] Training loss: 120.26395673, Validation loss: 120.23057793, Gradient norm: 24.08588704
INFO:root:[   40] Training loss: 120.01831527, Validation loss: 119.85571842, Gradient norm: 21.85545354
INFO:root:[   41] Training loss: 119.56878392, Validation loss: 119.15598981, Gradient norm: 25.87684874
INFO:root:[   42] Training loss: 118.58296359, Validation loss: 117.79867343, Gradient norm: 30.64317026
INFO:root:[   43] Training loss: 117.44347692, Validation loss: 117.04948320, Gradient norm: 34.46884561
INFO:root:[   44] Training loss: 116.42249717, Validation loss: 115.58500172, Gradient norm: 44.58749474
INFO:root:[   45] Training loss: 115.30370682, Validation loss: 115.05680584, Gradient norm: 61.57444400
INFO:root:[   46] Training loss: 114.30059254, Validation loss: 114.16981033, Gradient norm: 54.17130265
INFO:root:[   47] Training loss: 113.51086176, Validation loss: 113.31143399, Gradient norm: 58.75481439
INFO:root:[   48] Training loss: 112.83179987, Validation loss: 112.72427131, Gradient norm: 62.95034264
INFO:root:[   49] Training loss: 112.21722034, Validation loss: 111.84878303, Gradient norm: 57.90992172
INFO:root:[   50] Training loss: 111.70491717, Validation loss: 111.53349094, Gradient norm: 65.59582327
INFO:root:[   51] Training loss: 111.12205910, Validation loss: 111.21543542, Gradient norm: 52.27958619
INFO:root:[   52] Training loss: 110.69653172, Validation loss: 110.45572189, Gradient norm: 59.78844362
INFO:root:[   53] Training loss: 110.43305746, Validation loss: 110.50789511, Gradient norm: 65.24422315
INFO:root:[   54] Training loss: 109.89863202, Validation loss: 109.86981517, Gradient norm: 59.45232513
INFO:root:[   55] Training loss: 109.76274359, Validation loss: 109.73730837, Gradient norm: 65.37353539
INFO:root:[   56] Training loss: 109.27375436, Validation loss: 109.91954172, Gradient norm: 77.21630044
INFO:root:[   57] Training loss: 108.90240789, Validation loss: 108.90320613, Gradient norm: 79.85469301
INFO:root:[   58] Training loss: 108.54996315, Validation loss: 108.80577692, Gradient norm: 72.14200382
INFO:root:[   59] Training loss: 108.32789247, Validation loss: 108.34117468, Gradient norm: 78.39074875
INFO:root:[   60] Training loss: 108.15014561, Validation loss: 108.01122337, Gradient norm: 83.21176883
INFO:root:[   61] Training loss: 107.69988008, Validation loss: 107.69564135, Gradient norm: 80.80547266
INFO:root:[   62] Training loss: 107.72930159, Validation loss: 107.97971765, Gradient norm: 86.49401007
INFO:root:[   63] Training loss: 107.37345954, Validation loss: 107.23409245, Gradient norm: 93.27294492
INFO:root:[   64] Training loss: 107.11750348, Validation loss: 107.33962276, Gradient norm: 75.22057603
INFO:root:[   65] Training loss: 106.89189283, Validation loss: 107.05127637, Gradient norm: 98.80117670
INFO:root:[   66] Training loss: 106.64532126, Validation loss: 107.03946291, Gradient norm: 92.54455579
INFO:root:[   67] Training loss: 106.43922256, Validation loss: 106.81303379, Gradient norm: 82.80477743
INFO:root:[   68] Training loss: 106.20676395, Validation loss: 106.23517004, Gradient norm: 74.07040818
INFO:root:[   69] Training loss: 106.23691154, Validation loss: 106.08495199, Gradient norm: 98.70003141
INFO:root:[   70] Training loss: 106.03243148, Validation loss: 106.55787159, Gradient norm: 96.06479708
INFO:root:[   71] Training loss: 105.80338119, Validation loss: 105.95974653, Gradient norm: 98.07264477
INFO:root:[   72] Training loss: 105.72770266, Validation loss: 105.78001220, Gradient norm: 96.42598885
INFO:root:[   73] Training loss: 105.49161145, Validation loss: 105.61580711, Gradient norm: 106.69841776
INFO:root:[   74] Training loss: 105.31893928, Validation loss: 105.77919822, Gradient norm: 95.55578827
INFO:root:[   75] Training loss: 105.27649459, Validation loss: 105.11187139, Gradient norm: 115.52758854
INFO:root:[   76] Training loss: 105.04026943, Validation loss: 105.20367247, Gradient norm: 87.93217198
INFO:root:[   77] Training loss: 105.05786592, Validation loss: 105.36241255, Gradient norm: 116.68268731
INFO:root:[   78] Training loss: 104.99572632, Validation loss: 105.24521111, Gradient norm: 100.18459235
INFO:root:[   79] Training loss: 104.72331143, Validation loss: 105.18163799, Gradient norm: 115.07184445
INFO:root:[   80] Training loss: 104.55812073, Validation loss: 104.81817706, Gradient norm: 98.10936357
INFO:root:[   81] Training loss: 104.60304902, Validation loss: 104.87857029, Gradient norm: 99.45824359
INFO:root:[   82] Training loss: 104.37459024, Validation loss: 105.05338840, Gradient norm: 108.15038455
INFO:root:[   83] Training loss: 104.31610850, Validation loss: 104.74104178, Gradient norm: 98.40960022
INFO:root:[   84] Training loss: 104.09478179, Validation loss: 104.24586934, Gradient norm: 108.01610434
INFO:root:[   85] Training loss: 104.13788807, Validation loss: 104.41260187, Gradient norm: 100.96181388
INFO:root:[   86] Training loss: 103.97108196, Validation loss: 104.19542905, Gradient norm: 104.41453221
INFO:root:[   87] Training loss: 103.87537944, Validation loss: 104.10503019, Gradient norm: 99.37958639
INFO:root:[   88] Training loss: 103.75264031, Validation loss: 103.75748075, Gradient norm: 110.41521520
INFO:root:[   89] Training loss: 103.81992151, Validation loss: 103.99799847, Gradient norm: 112.41859510
INFO:root:[   90] Training loss: 103.66162123, Validation loss: 104.00286129, Gradient norm: 113.23056871
INFO:root:[   91] Training loss: 103.52150051, Validation loss: 104.09080400, Gradient norm: 95.67996638
INFO:root:[   92] Training loss: 103.57366741, Validation loss: 103.75594698, Gradient norm: 119.19867349
INFO:root:[   93] Training loss: 103.36627899, Validation loss: 104.27289897, Gradient norm: 108.84345611
INFO:root:[   94] Training loss: 103.45974542, Validation loss: 103.92003921, Gradient norm: 121.17772455
INFO:root:[   95] Training loss: 103.19697105, Validation loss: 103.78024923, Gradient norm: 108.48376411
INFO:root:[   96] Training loss: 103.19208284, Validation loss: 103.94555269, Gradient norm: 107.81117629
INFO:root:[   97] Training loss: 103.26778115, Validation loss: 103.70611178, Gradient norm: 121.73001680
INFO:root:[   98] Training loss: 102.96818792, Validation loss: 103.67948361, Gradient norm: 106.90941547
INFO:root:[   99] Training loss: 103.14420163, Validation loss: 103.63627151, Gradient norm: 110.09843216
INFO:root:[  100] Training loss: 102.98931142, Validation loss: 103.60430093, Gradient norm: 117.69267511
INFO:root:[  101] Training loss: 102.95943741, Validation loss: 103.50733159, Gradient norm: 116.79619258
INFO:root:[  102] Training loss: 102.85141504, Validation loss: 103.55859954, Gradient norm: 115.47756258
INFO:root:[  103] Training loss: 102.64335626, Validation loss: 102.86597337, Gradient norm: 96.71435079
INFO:root:[  104] Training loss: 102.73172936, Validation loss: 103.08808873, Gradient norm: 126.64734537
INFO:root:[  105] Training loss: 102.68305719, Validation loss: 102.82043273, Gradient norm: 102.11544356
INFO:root:[  106] Training loss: 102.60663321, Validation loss: 103.15484172, Gradient norm: 116.96083025
INFO:root:[  107] Training loss: 102.66983161, Validation loss: 103.41433058, Gradient norm: 123.29953627
INFO:root:[  108] Training loss: 102.32643343, Validation loss: 102.54748351, Gradient norm: 106.10052832
INFO:root:[  109] Training loss: 102.50285603, Validation loss: 102.91688301, Gradient norm: 124.93778618
INFO:root:[  110] Training loss: 102.26318683, Validation loss: 102.62580977, Gradient norm: 106.41068608
INFO:root:[  111] Training loss: 102.33627103, Validation loss: 102.87285561, Gradient norm: 126.32735791
INFO:root:[  112] Training loss: 102.20359033, Validation loss: 103.02223100, Gradient norm: 116.28699860
INFO:root:[  113] Training loss: 102.09223958, Validation loss: 102.79284458, Gradient norm: 107.92564866
INFO:root:[  114] Training loss: 102.23497819, Validation loss: 102.29731330, Gradient norm: 120.48453335
INFO:root:[  115] Training loss: 102.19362256, Validation loss: 102.58734525, Gradient norm: 142.54341673
INFO:root:[  116] Training loss: 102.09388368, Validation loss: 102.68489364, Gradient norm: 97.60798695
INFO:root:[  117] Training loss: 102.06114210, Validation loss: 102.42941547, Gradient norm: 134.38677742
INFO:root:[  118] Training loss: 101.97237423, Validation loss: 102.73157238, Gradient norm: 124.99065035
INFO:root:[  119] Training loss: 101.83284989, Validation loss: 102.13801890, Gradient norm: 115.69751165
INFO:root:[  120] Training loss: 101.88411996, Validation loss: 102.31516187, Gradient norm: 137.45729292
INFO:root:[  121] Training loss: 101.77803850, Validation loss: 102.53591735, Gradient norm: 137.12493093
INFO:root:[  122] Training loss: 101.67908660, Validation loss: 102.37585870, Gradient norm: 104.62591367
INFO:root:[  123] Training loss: 101.65583511, Validation loss: 102.42333669, Gradient norm: 112.29571572
INFO:root:[  124] Training loss: 101.59812718, Validation loss: 102.57692587, Gradient norm: 129.43762140
INFO:root:[  125] Training loss: 101.69197177, Validation loss: 102.35532879, Gradient norm: 129.58515880
INFO:root:[  126] Training loss: 101.44765547, Validation loss: 101.89519106, Gradient norm: 111.76509170
INFO:root:[  127] Training loss: 101.51369578, Validation loss: 102.56096860, Gradient norm: 131.50641923
INFO:root:[  128] Training loss: 101.45792882, Validation loss: 101.89256076, Gradient norm: 127.13177676
INFO:root:[  129] Training loss: 101.41977658, Validation loss: 101.96901519, Gradient norm: 129.04650598
INFO:root:[  130] Training loss: 101.29373176, Validation loss: 101.74834232, Gradient norm: 119.90279252
INFO:root:[  131] Training loss: 101.32293283, Validation loss: 101.77518305, Gradient norm: 126.62762855
INFO:root:[  132] Training loss: 101.28059151, Validation loss: 102.00197812, Gradient norm: 130.82934262
INFO:root:[  133] Training loss: 101.29718126, Validation loss: 101.86704728, Gradient norm: 123.63263497
INFO:root:[  134] Training loss: 101.28917296, Validation loss: 102.32681985, Gradient norm: 126.90245162
INFO:root:[  135] Training loss: 101.11164323, Validation loss: 101.54840640, Gradient norm: 127.83927162
INFO:root:[  136] Training loss: 101.09074247, Validation loss: 102.18347378, Gradient norm: 126.36736289
INFO:root:[  137] Training loss: 100.97853034, Validation loss: 102.06200830, Gradient norm: 135.61709881
INFO:root:[  138] Training loss: 100.88936102, Validation loss: 101.42375288, Gradient norm: 118.00947266
INFO:root:[  139] Training loss: 101.03668152, Validation loss: 101.81416084, Gradient norm: 156.41479487
INFO:root:[  140] Training loss: 100.95620113, Validation loss: 101.47811521, Gradient norm: 133.13692905
INFO:root:[  141] Training loss: 100.89512135, Validation loss: 101.66549683, Gradient norm: 136.25957962
INFO:root:[  142] Training loss: 100.92566904, Validation loss: 101.59679465, Gradient norm: 143.07990260
INFO:root:[  143] Training loss: 100.74432663, Validation loss: 101.49771250, Gradient norm: 109.06955277
INFO:root:[  144] Training loss: 100.71498776, Validation loss: 101.10460531, Gradient norm: 137.34808406
INFO:root:[  145] Training loss: 100.68170888, Validation loss: 101.06120826, Gradient norm: 135.97858891
INFO:root:[  146] Training loss: 100.64559653, Validation loss: 101.07319246, Gradient norm: 126.94256747
INFO:root:[  147] Training loss: 100.80957457, Validation loss: 101.71198246, Gradient norm: 139.12595755
INFO:root:[  148] Training loss: 100.46809104, Validation loss: 101.52418992, Gradient norm: 118.62340788
INFO:root:[  149] Training loss: 100.50366954, Validation loss: 101.19301184, Gradient norm: 138.94821259
INFO:root:[  150] Training loss: 100.60812412, Validation loss: 101.31782926, Gradient norm: 146.68233529
INFO:root:[  151] Training loss: 100.57855144, Validation loss: 103.46979391, Gradient norm: 153.70687674
INFO:root:[  152] Training loss: 100.47280843, Validation loss: 101.00905109, Gradient norm: 113.38936743
INFO:root:[  153] Training loss: 100.41918236, Validation loss: 101.31950957, Gradient norm: 131.96783692
INFO:root:[  154] Training loss: 100.35396488, Validation loss: 101.22219349, Gradient norm: 139.88352135
INFO:root:[  155] Training loss: 100.37508487, Validation loss: 100.82995869, Gradient norm: 155.76774270
INFO:root:[  156] Training loss: 100.27851253, Validation loss: 101.10672286, Gradient norm: 124.93657684
INFO:root:[  157] Training loss: 100.29875453, Validation loss: 101.33891481, Gradient norm: 158.08024852
INFO:root:[  158] Training loss: 100.29134254, Validation loss: 101.26759891, Gradient norm: 148.06051964
INFO:root:[  159] Training loss: 100.24171157, Validation loss: 101.03487002, Gradient norm: 146.61312597
INFO:root:[  160] Training loss: 100.06224411, Validation loss: 100.79360225, Gradient norm: 132.85729992
INFO:root:[  161] Training loss: 100.23467748, Validation loss: 100.96949689, Gradient norm: 153.62030449
INFO:root:[  162] Training loss: 100.22420826, Validation loss: 100.94910194, Gradient norm: 138.81528892
INFO:root:[  163] Training loss: 100.13195254, Validation loss: 100.69577579, Gradient norm: 151.41370730
INFO:root:[  164] Training loss: 100.00315452, Validation loss: 101.03863788, Gradient norm: 144.61317122
INFO:root:[  165] Training loss: 100.12761067, Validation loss: 100.85144543, Gradient norm: 139.21148932
INFO:root:[  166] Training loss: 100.03064350, Validation loss: 100.77751949, Gradient norm: 151.21827221
INFO:root:[  167] Training loss: 100.10818853, Validation loss: 100.86179273, Gradient norm: 149.90377676
INFO:root:[  168] Training loss: 99.94387817, Validation loss: 100.82938701, Gradient norm: 139.01149799
INFO:root:[  169] Training loss: 100.00363706, Validation loss: 101.47248446, Gradient norm: 161.61116157
INFO:root:[  170] Training loss: 99.96579398, Validation loss: 101.49893609, Gradient norm: 159.35894492
INFO:root:[  171] Training loss: 99.86637190, Validation loss: 100.70144153, Gradient norm: 150.13893575
INFO:root:[  172] Training loss: 99.80512602, Validation loss: 101.10326412, Gradient norm: 129.25015548
INFO:root:[  173] Training loss: 99.75720654, Validation loss: 100.58889034, Gradient norm: 175.55374742
INFO:root:[  174] Training loss: 99.68605089, Validation loss: 100.26981301, Gradient norm: 134.61135925
INFO:root:[  175] Training loss: 99.62589230, Validation loss: 100.43449902, Gradient norm: 150.23218889
INFO:root:[  176] Training loss: 99.69226790, Validation loss: 100.63705523, Gradient norm: 151.41388595
INFO:root:[  177] Training loss: 99.72031740, Validation loss: 100.76992061, Gradient norm: 158.43138064
INFO:root:[  178] Training loss: 99.68305436, Validation loss: 100.29861819, Gradient norm: 144.39928097
INFO:root:[  179] Training loss: 99.59532355, Validation loss: 100.73316561, Gradient norm: 163.30569265
INFO:root:[  180] Training loss: 99.56831448, Validation loss: 100.52101661, Gradient norm: 155.85388239
INFO:root:[  181] Training loss: 99.49722648, Validation loss: 100.53761318, Gradient norm: 139.17151869
INFO:root:[  182] Training loss: 99.40861234, Validation loss: 100.47016854, Gradient norm: 139.58858945
INFO:root:[  183] Training loss: 99.64708602, Validation loss: 100.39595321, Gradient norm: 186.70793289
INFO:root:EP 183: Early stopping
INFO:root:Training the model took 3546.699s.
INFO:root:Emptying the cuda cache took 0.048s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 140.99716
INFO:root:EnergyScoreTrain: 99.51497
INFO:root:CoverageTrain: 0.74642
INFO:root:IntervalWidthTrain: 7.76101
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 142.11343
INFO:root:EnergyScoreValidation: 100.31878
INFO:root:CoverageValidation: 0.74459
INFO:root:IntervalWidthValidation: 7.75444
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 142.28037
INFO:root:EnergyScoreTest: 100.42441
INFO:root:CoverageTest: 0.74401
INFO:root:IntervalWidthTest: 7.75938
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 125.63817063, Validation loss: 123.59088714, Gradient norm: 352.16191220
INFO:root:[    2] Training loss: 122.43583753, Validation loss: 121.80371594, Gradient norm: 225.26404607
INFO:root:[    3] Training loss: 122.02875937, Validation loss: 121.87590500, Gradient norm: 95.08892297
INFO:root:[    4] Training loss: 121.82434838, Validation loss: 121.71977892, Gradient norm: 81.22063476
INFO:root:[    5] Training loss: 121.89995723, Validation loss: 121.67444742, Gradient norm: 65.24310539
INFO:root:[    6] Training loss: 121.70188951, Validation loss: 121.60840949, Gradient norm: 52.81669127
INFO:root:[    7] Training loss: 121.72792242, Validation loss: 121.86407892, Gradient norm: 62.81753212
INFO:root:[    8] Training loss: 121.45089256, Validation loss: 121.94495050, Gradient norm: 50.90985191
INFO:root:[    9] Training loss: 121.40359031, Validation loss: 121.37876340, Gradient norm: 40.83122254
INFO:root:[   10] Training loss: 121.23615508, Validation loss: 121.61251410, Gradient norm: 49.38899930
INFO:root:[   11] Training loss: 121.00183038, Validation loss: 121.74793585, Gradient norm: 46.88764777
INFO:root:[   12] Training loss: 121.00476898, Validation loss: 121.43424409, Gradient norm: 38.62667702
INFO:root:[   13] Training loss: 120.78701640, Validation loss: 120.96140894, Gradient norm: 38.22935639
INFO:root:[   14] Training loss: 120.70714859, Validation loss: 121.02213814, Gradient norm: 34.31893668
INFO:root:[   15] Training loss: 120.67867765, Validation loss: 120.85627773, Gradient norm: 33.82698332
INFO:root:[   16] Training loss: 120.60515500, Validation loss: 120.72788291, Gradient norm: 32.65593564
INFO:root:[   17] Training loss: 120.47955748, Validation loss: 120.61242807, Gradient norm: 29.43381648
INFO:root:[   18] Training loss: 120.52373444, Validation loss: 120.81632022, Gradient norm: 29.25305233
INFO:root:[   19] Training loss: 120.46905754, Validation loss: 120.75098103, Gradient norm: 30.23537526
INFO:root:[   20] Training loss: 120.52731391, Validation loss: 120.50039094, Gradient norm: 37.37329368
INFO:root:[   21] Training loss: 120.54793150, Validation loss: 120.52255223, Gradient norm: 34.74260071
INFO:root:[   22] Training loss: 120.35479581, Validation loss: 120.51851233, Gradient norm: 26.46257271
INFO:root:[   23] Training loss: 120.47359602, Validation loss: 120.50438374, Gradient norm: 26.06326589
INFO:root:[   24] Training loss: 120.33797374, Validation loss: 120.23301881, Gradient norm: 21.87061842
INFO:root:[   25] Training loss: 120.25864363, Validation loss: 120.53564243, Gradient norm: 22.97443236
INFO:root:[   26] Training loss: 120.30281769, Validation loss: 120.45961051, Gradient norm: 25.08094969
INFO:root:[   27] Training loss: 120.40297746, Validation loss: 120.27119472, Gradient norm: 27.61459181
INFO:root:[   28] Training loss: 120.34904649, Validation loss: 120.48829519, Gradient norm: 23.85168455
INFO:root:[   29] Training loss: 120.23373973, Validation loss: 120.08259688, Gradient norm: 24.46261548
INFO:root:[   30] Training loss: 119.75862034, Validation loss: 119.52309812, Gradient norm: 24.41010425
INFO:root:[   31] Training loss: 119.04051006, Validation loss: 118.46700129, Gradient norm: 21.94229352
INFO:root:[   32] Training loss: 118.06871114, Validation loss: 117.45922694, Gradient norm: 25.83000773
INFO:root:[   33] Training loss: 116.97005915, Validation loss: 116.56273704, Gradient norm: 22.43170591
INFO:root:[   34] Training loss: 116.07731851, Validation loss: 115.90263551, Gradient norm: 38.27597067
INFO:root:[   35] Training loss: 115.03364948, Validation loss: 114.41770146, Gradient norm: 33.95002650
INFO:root:[   36] Training loss: 114.08861218, Validation loss: 113.75361633, Gradient norm: 43.26816086
INFO:root:[   37] Training loss: 113.42339345, Validation loss: 113.17684595, Gradient norm: 61.45799922
INFO:root:[   38] Training loss: 112.54131783, Validation loss: 112.59453793, Gradient norm: 57.77918840
INFO:root:[   39] Training loss: 111.88193107, Validation loss: 111.96192643, Gradient norm: 63.68510150
INFO:root:[   40] Training loss: 111.28740982, Validation loss: 111.16887323, Gradient norm: 58.59468028
INFO:root:[   41] Training loss: 110.66626929, Validation loss: 110.94605176, Gradient norm: 59.61853352
INFO:root:[   42] Training loss: 110.30985949, Validation loss: 110.27308392, Gradient norm: 67.66271302
INFO:root:[   43] Training loss: 109.83144304, Validation loss: 109.73694847, Gradient norm: 63.30064784
INFO:root:[   44] Training loss: 109.28818174, Validation loss: 110.01652001, Gradient norm: 53.61222954
INFO:root:[   45] Training loss: 108.96689464, Validation loss: 108.96149050, Gradient norm: 60.45919324
INFO:root:[   46] Training loss: 108.59222729, Validation loss: 108.56817074, Gradient norm: 55.05472958
INFO:root:[   47] Training loss: 108.16584920, Validation loss: 108.19918850, Gradient norm: 66.30885342
INFO:root:[   48] Training loss: 107.81091295, Validation loss: 108.01019577, Gradient norm: 59.05564278
INFO:root:[   49] Training loss: 107.43304140, Validation loss: 107.99693667, Gradient norm: 51.13816169
INFO:root:[   50] Training loss: 107.13670835, Validation loss: 107.30229634, Gradient norm: 54.22920072
INFO:root:[   51] Training loss: 106.89239813, Validation loss: 107.38157654, Gradient norm: 53.25426679
INFO:root:[   52] Training loss: 106.75499030, Validation loss: 106.95785996, Gradient norm: 64.49724335
INFO:root:[   53] Training loss: 106.38000043, Validation loss: 107.13531520, Gradient norm: 57.06199777
INFO:root:[   54] Training loss: 106.24556847, Validation loss: 106.49961090, Gradient norm: 57.53885236
INFO:root:[   55] Training loss: 106.07636862, Validation loss: 106.35663447, Gradient norm: 63.84501592
INFO:root:[   56] Training loss: 105.93094655, Validation loss: 106.05738357, Gradient norm: 60.92587038
INFO:root:[   57] Training loss: 105.71817347, Validation loss: 105.97370490, Gradient norm: 52.69041164
INFO:root:[   58] Training loss: 105.55952967, Validation loss: 105.79613363, Gradient norm: 58.09632481
INFO:root:[   59] Training loss: 105.31978317, Validation loss: 105.65436633, Gradient norm: 57.41130967
INFO:root:[   60] Training loss: 105.15871693, Validation loss: 105.56520896, Gradient norm: 58.58625394
INFO:root:[   61] Training loss: 105.09908484, Validation loss: 105.29667295, Gradient norm: 58.78759284
INFO:root:[   62] Training loss: 104.94403225, Validation loss: 105.14703159, Gradient norm: 65.94575778
INFO:root:[   63] Training loss: 104.66010041, Validation loss: 105.29264779, Gradient norm: 58.10895047
INFO:root:[   64] Training loss: 104.53550086, Validation loss: 104.91301017, Gradient norm: 59.64897508
INFO:root:[   65] Training loss: 104.39801464, Validation loss: 105.16018729, Gradient norm: 57.23323924
INFO:root:[   66] Training loss: 104.31435225, Validation loss: 104.58313830, Gradient norm: 51.65888446
INFO:root:[   67] Training loss: 104.28457216, Validation loss: 104.52551033, Gradient norm: 59.75780099
INFO:root:[   68] Training loss: 104.05217270, Validation loss: 104.09121467, Gradient norm: 61.06314538
INFO:root:[   69] Training loss: 104.00178818, Validation loss: 104.43575024, Gradient norm: 67.73624865
INFO:root:[   70] Training loss: 103.86895948, Validation loss: 104.78964602, Gradient norm: 60.60361768
INFO:root:[   71] Training loss: 103.82055840, Validation loss: 104.63277988, Gradient norm: 54.29158762
INFO:root:[   72] Training loss: 103.69890770, Validation loss: 104.11557217, Gradient norm: 59.39475823
INFO:root:[   73] Training loss: 103.66020972, Validation loss: 104.29992597, Gradient norm: 68.63420374
INFO:root:[   74] Training loss: 103.39046120, Validation loss: 104.03688917, Gradient norm: 51.67187459
INFO:root:[   75] Training loss: 103.35674495, Validation loss: 104.26710721, Gradient norm: 62.55176875
INFO:root:[   76] Training loss: 103.37428729, Validation loss: 103.43852944, Gradient norm: 69.97221542
INFO:root:[   77] Training loss: 103.16127001, Validation loss: 103.97886079, Gradient norm: 59.49916658
INFO:root:[   78] Training loss: 103.25413054, Validation loss: 103.69491867, Gradient norm: 71.76473096
INFO:root:[   79] Training loss: 103.06396572, Validation loss: 103.65144848, Gradient norm: 52.17754058
INFO:root:[   80] Training loss: 102.94779118, Validation loss: 103.59496860, Gradient norm: 72.20165114
INFO:root:[   81] Training loss: 102.86958624, Validation loss: 103.43370872, Gradient norm: 53.52838971
INFO:root:[   82] Training loss: 102.93366667, Validation loss: 103.39104698, Gradient norm: 68.08529628
INFO:root:[   83] Training loss: 102.70301488, Validation loss: 103.63444045, Gradient norm: 62.68176269
INFO:root:[   84] Training loss: 102.70154956, Validation loss: 103.16611481, Gradient norm: 70.69668005
INFO:root:[   85] Training loss: 102.58821173, Validation loss: 103.71795917, Gradient norm: 59.75756429
INFO:root:[   86] Training loss: 102.60781212, Validation loss: 103.14313560, Gradient norm: 66.44377398
INFO:root:[   87] Training loss: 102.42406396, Validation loss: 103.18059145, Gradient norm: 64.54362221
INFO:root:[   88] Training loss: 102.45982982, Validation loss: 102.87967945, Gradient norm: 66.85863046
INFO:root:[   89] Training loss: 102.37231729, Validation loss: 103.00588805, Gradient norm: 73.85514996
INFO:root:[   90] Training loss: 102.37080694, Validation loss: 102.87604549, Gradient norm: 64.86456807
INFO:root:[   91] Training loss: 102.12224113, Validation loss: 102.88141343, Gradient norm: 62.69794337
INFO:root:[   92] Training loss: 102.13740283, Validation loss: 102.90718421, Gradient norm: 75.54364895
INFO:root:[   93] Training loss: 102.04329425, Validation loss: 102.75394913, Gradient norm: 64.46570311
INFO:root:[   94] Training loss: 101.98371847, Validation loss: 102.85292027, Gradient norm: 48.57700812
INFO:root:[   95] Training loss: 101.94424411, Validation loss: 103.08254400, Gradient norm: 74.18915968
INFO:root:[   96] Training loss: 101.88118926, Validation loss: 102.61403472, Gradient norm: 62.09154556
INFO:root:[   97] Training loss: 101.91627739, Validation loss: 102.87178960, Gradient norm: 76.76290003
INFO:root:[   98] Training loss: 101.80512906, Validation loss: 102.33501566, Gradient norm: 73.49809635
INFO:root:[   99] Training loss: 101.65991002, Validation loss: 102.32600824, Gradient norm: 62.14197540
INFO:root:[  100] Training loss: 101.62552697, Validation loss: 102.55223031, Gradient norm: 78.41790105
INFO:root:[  101] Training loss: 101.62211076, Validation loss: 102.43822321, Gradient norm: 70.30184478
INFO:root:[  102] Training loss: 101.46817260, Validation loss: 102.36128156, Gradient norm: 63.99096736
INFO:root:[  103] Training loss: 101.62640145, Validation loss: 101.77259774, Gradient norm: 76.53198913
INFO:root:[  104] Training loss: 101.49226298, Validation loss: 103.06979554, Gradient norm: 65.87586348
INFO:root:[  105] Training loss: 101.54524852, Validation loss: 102.99367970, Gradient norm: 80.62203635
INFO:root:[  106] Training loss: 101.40593551, Validation loss: 102.95011165, Gradient norm: 72.47460101
INFO:root:[  107] Training loss: 101.28141238, Validation loss: 102.05429577, Gradient norm: 81.93152065
INFO:root:[  108] Training loss: 101.44361006, Validation loss: 102.04380772, Gradient norm: 62.46975621
INFO:root:[  109] Training loss: 101.19106853, Validation loss: 101.96339206, Gradient norm: 64.80833773
INFO:root:[  110] Training loss: 101.20175002, Validation loss: 101.95920563, Gradient norm: 78.74360975
INFO:root:[  111] Training loss: 101.07750668, Validation loss: 101.94918613, Gradient norm: 68.63628822
INFO:root:[  112] Training loss: 100.98661730, Validation loss: 101.32724367, Gradient norm: 72.68947059
INFO:root:[  113] Training loss: 101.11749990, Validation loss: 101.77404259, Gradient norm: 83.14765317
INFO:root:[  114] Training loss: 101.00016467, Validation loss: 101.78850845, Gradient norm: 66.72765204
INFO:root:[  115] Training loss: 101.06100801, Validation loss: 101.48384094, Gradient norm: 79.19275824
INFO:root:[  116] Training loss: 100.93031014, Validation loss: 101.60477711, Gradient norm: 77.51113987
INFO:root:[  117] Training loss: 100.79864765, Validation loss: 101.59373158, Gradient norm: 77.94434868
INFO:root:[  118] Training loss: 100.69389553, Validation loss: 101.42541688, Gradient norm: 67.06791774
INFO:root:[  119] Training loss: 100.74644632, Validation loss: 101.59739133, Gradient norm: 75.85801800
INFO:root:[  120] Training loss: 100.73198349, Validation loss: 101.45797887, Gradient norm: 89.07839173
INFO:root:[  121] Training loss: 100.59245543, Validation loss: 101.44196214, Gradient norm: 67.54875791
INFO:root:EP 121: Early stopping
INFO:root:Training the model took 2178.625s.
INFO:root:Emptying the cuda cache took 0.046s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 142.99652
INFO:root:EnergyScoreTrain: 100.87311
INFO:root:CoverageTrain: 0.7479
INFO:root:IntervalWidthTrain: 8.1886
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 143.86861
INFO:root:EnergyScoreValidation: 101.48974
INFO:root:CoverageValidation: 0.74643
INFO:root:IntervalWidthValidation: 8.17663
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 144.04965
INFO:root:EnergyScoreTest: 101.60482
INFO:root:CoverageTest: 0.74637
INFO:root:IntervalWidthTest: 8.18994
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.58686748, Validation loss: 122.70284955, Gradient norm: 372.12673660
INFO:root:[    2] Training loss: 121.75387654, Validation loss: 121.72493402, Gradient norm: 311.64531043
INFO:root:[    3] Training loss: 121.55752185, Validation loss: 122.27677365, Gradient norm: 299.43658423
INFO:root:[    4] Training loss: 121.21780524, Validation loss: 121.00194655, Gradient norm: 263.36867131
INFO:root:[    5] Training loss: 120.77471330, Validation loss: 120.72002253, Gradient norm: 216.53005865
INFO:root:[    6] Training loss: 120.60747130, Validation loss: 121.09133253, Gradient norm: 196.53058548
INFO:root:[    7] Training loss: 120.55008637, Validation loss: 120.47448994, Gradient norm: 185.91446531
INFO:root:[    8] Training loss: 120.44703533, Validation loss: 120.50170030, Gradient norm: 155.41042309
INFO:root:[    9] Training loss: 120.48796595, Validation loss: 120.74390069, Gradient norm: 218.30601046
INFO:root:[   10] Training loss: 120.43123883, Validation loss: 120.49069424, Gradient norm: 225.76917849
INFO:root:[   11] Training loss: 120.33235688, Validation loss: 120.40346185, Gradient norm: 180.28991388
INFO:root:[   12] Training loss: 120.28816871, Validation loss: 120.49780273, Gradient norm: 173.86821746
INFO:root:[   13] Training loss: 120.29135355, Validation loss: 121.10131441, Gradient norm: 194.50053652
INFO:root:[   14] Training loss: 120.17592290, Validation loss: 120.25462867, Gradient norm: 150.81260198
INFO:root:[   15] Training loss: 120.12450173, Validation loss: 120.42089712, Gradient norm: 193.08010111
INFO:root:[   16] Training loss: 119.64074592, Validation loss: 119.98485723, Gradient norm: 152.46079464
INFO:root:[   17] Training loss: 118.68777810, Validation loss: 118.28939477, Gradient norm: 157.62016346
INFO:root:[   18] Training loss: 117.67772047, Validation loss: 117.49086525, Gradient norm: 151.21259018
INFO:root:[   19] Training loss: 116.69802397, Validation loss: 116.44309813, Gradient norm: 133.58657400
INFO:root:[   20] Training loss: 115.89730896, Validation loss: 115.60092979, Gradient norm: 126.01079498
INFO:root:[   21] Training loss: 115.12141608, Validation loss: 115.02189505, Gradient norm: 100.81872033
INFO:root:[   22] Training loss: 114.46737846, Validation loss: 114.24412089, Gradient norm: 137.01348762
INFO:root:[   23] Training loss: 113.80127460, Validation loss: 113.72814889, Gradient norm: 113.21257743
INFO:root:[   24] Training loss: 113.24295219, Validation loss: 113.35640795, Gradient norm: 124.28341708
INFO:root:[   25] Training loss: 112.79749912, Validation loss: 113.10269612, Gradient norm: 113.87063242
INFO:root:[   26] Training loss: 112.39899850, Validation loss: 112.33696563, Gradient norm: 105.61062549
INFO:root:[   27] Training loss: 112.01935098, Validation loss: 112.24888085, Gradient norm: 104.19551292
INFO:root:[   28] Training loss: 111.65728037, Validation loss: 111.90413666, Gradient norm: 90.96198371
INFO:root:[   29] Training loss: 111.43414759, Validation loss: 111.41356843, Gradient norm: 123.51083729
INFO:root:[   30] Training loss: 111.00887130, Validation loss: 111.14870374, Gradient norm: 84.51141033
INFO:root:[   31] Training loss: 110.76847049, Validation loss: 110.81130850, Gradient norm: 107.92149652
INFO:root:[   32] Training loss: 110.50783795, Validation loss: 110.75127753, Gradient norm: 115.21079532
INFO:root:[   33] Training loss: 110.24692792, Validation loss: 110.37835536, Gradient norm: 79.55011765
INFO:root:[   34] Training loss: 110.04468989, Validation loss: 110.08552499, Gradient norm: 123.76197084
INFO:root:[   35] Training loss: 109.77973087, Validation loss: 110.12061152, Gradient norm: 84.12439183
INFO:root:[   36] Training loss: 109.62518243, Validation loss: 109.68107105, Gradient norm: 118.20283840
INFO:root:[   37] Training loss: 109.40331248, Validation loss: 109.51212100, Gradient norm: 100.19005407
INFO:root:[   38] Training loss: 109.16589545, Validation loss: 109.36851817, Gradient norm: 109.14813044
INFO:root:[   39] Training loss: 108.96902074, Validation loss: 109.29297349, Gradient norm: 112.73294335
INFO:root:[   40] Training loss: 108.77852435, Validation loss: 109.05847773, Gradient norm: 102.36911288
INFO:root:[   41] Training loss: 108.57943131, Validation loss: 108.94283926, Gradient norm: 90.08766091
INFO:root:[   42] Training loss: 108.37747618, Validation loss: 109.16294335, Gradient norm: 105.20066743
INFO:root:[   43] Training loss: 108.27759363, Validation loss: 108.62219633, Gradient norm: 96.24674627
INFO:root:[   44] Training loss: 108.09514159, Validation loss: 109.05984181, Gradient norm: 100.07267705
INFO:root:[   45] Training loss: 107.99937290, Validation loss: 108.16271841, Gradient norm: 120.08264925
INFO:root:[   46] Training loss: 107.79395179, Validation loss: 108.09169112, Gradient norm: 101.62177554
INFO:root:[   47] Training loss: 107.70012091, Validation loss: 108.09675072, Gradient norm: 119.90129694
INFO:root:[   48] Training loss: 107.49039500, Validation loss: 108.00054853, Gradient norm: 104.42200226
INFO:root:[   49] Training loss: 107.39837464, Validation loss: 107.74032146, Gradient norm: 107.82034820
INFO:root:[   50] Training loss: 107.17272490, Validation loss: 107.88068916, Gradient norm: 100.87757123
INFO:root:[   51] Training loss: 107.07530557, Validation loss: 107.46867423, Gradient norm: 128.32284124
INFO:root:[   52] Training loss: 106.94498005, Validation loss: 107.95665978, Gradient norm: 117.12267102
INFO:root:[   53] Training loss: 106.77915671, Validation loss: 107.27605570, Gradient norm: 117.09832598
INFO:root:[   54] Training loss: 106.71450110, Validation loss: 107.34671731, Gradient norm: 124.49818502
INFO:root:[   55] Training loss: 106.57198219, Validation loss: 107.01273346, Gradient norm: 101.34530186
INFO:root:[   56] Training loss: 106.42506672, Validation loss: 107.00265292, Gradient norm: 123.09959085
INFO:root:[   57] Training loss: 106.28858293, Validation loss: 106.71204350, Gradient norm: 122.87077379
INFO:root:[   58] Training loss: 106.24144380, Validation loss: 106.83061192, Gradient norm: 122.08875094
INFO:root:[   59] Training loss: 106.11553253, Validation loss: 106.85100319, Gradient norm: 129.84637950
INFO:root:[   60] Training loss: 105.99737778, Validation loss: 106.61519439, Gradient norm: 119.68934078
INFO:root:[   61] Training loss: 105.83249185, Validation loss: 106.56044769, Gradient norm: 115.28654696
INFO:root:[   62] Training loss: 105.78436745, Validation loss: 106.82932571, Gradient norm: 125.19906832
INFO:root:[   63] Training loss: 105.66872838, Validation loss: 106.59812664, Gradient norm: 145.99378973
INFO:root:[   64] Training loss: 105.52231085, Validation loss: 106.45399659, Gradient norm: 130.54455720
INFO:root:[   65] Training loss: 105.61441492, Validation loss: 106.28587026, Gradient norm: 152.65032600
INFO:root:[   66] Training loss: 105.41010440, Validation loss: 106.26924107, Gradient norm: 123.32341620
INFO:root:[   67] Training loss: 105.40599897, Validation loss: 105.97717285, Gradient norm: 142.53348681
INFO:root:[   68] Training loss: 105.26674396, Validation loss: 105.97593952, Gradient norm: 126.29158915
INFO:root:[   69] Training loss: 105.13717186, Validation loss: 106.55825332, Gradient norm: 134.31322241
INFO:root:[   70] Training loss: 105.15190199, Validation loss: 105.99904501, Gradient norm: 162.22145895
INFO:root:[   71] Training loss: 104.94831274, Validation loss: 105.94892489, Gradient norm: 116.75043220
INFO:root:[   72] Training loss: 104.95152897, Validation loss: 105.50785275, Gradient norm: 160.44003138
INFO:root:[   73] Training loss: 104.78209335, Validation loss: 105.79044026, Gradient norm: 137.01498936
INFO:root:[   74] Training loss: 104.79082280, Validation loss: 105.60965387, Gradient norm: 136.77285764
INFO:root:[   75] Training loss: 104.65298334, Validation loss: 105.83074372, Gradient norm: 162.50264083
INFO:root:[   76] Training loss: 104.61609251, Validation loss: 105.58233616, Gradient norm: 141.33438281
INFO:root:[   77] Training loss: 104.48925106, Validation loss: 105.22914702, Gradient norm: 154.42115503
INFO:root:[   78] Training loss: 104.40789511, Validation loss: 105.76030310, Gradient norm: 135.53634798
INFO:root:[   79] Training loss: 104.38904328, Validation loss: 105.39193699, Gradient norm: 180.68565826
INFO:root:[   80] Training loss: 104.25914670, Validation loss: 105.48643625, Gradient norm: 147.09698192
INFO:root:[   81] Training loss: 104.28615111, Validation loss: 105.43059250, Gradient norm: 163.09364762
INFO:root:[   82] Training loss: 104.21629394, Validation loss: 105.35965413, Gradient norm: 178.42862841
INFO:root:[   83] Training loss: 104.09551664, Validation loss: 105.16191890, Gradient norm: 137.45567685
INFO:root:[   84] Training loss: 103.98490271, Validation loss: 105.27558373, Gradient norm: 146.66423368
INFO:root:[   85] Training loss: 103.92621626, Validation loss: 104.87748797, Gradient norm: 158.49906175
INFO:root:[   86] Training loss: 103.86819343, Validation loss: 105.11694283, Gradient norm: 146.30673760
INFO:root:[   87] Training loss: 103.85393666, Validation loss: 104.98025881, Gradient norm: 171.46791180
INFO:root:[   88] Training loss: 103.79117794, Validation loss: 104.97520605, Gradient norm: 154.69033442
INFO:root:[   89] Training loss: 103.69671698, Validation loss: 105.13717783, Gradient norm: 158.55450124
INFO:root:[   90] Training loss: 103.68511254, Validation loss: 105.36990251, Gradient norm: 167.37502428
INFO:root:[   91] Training loss: 103.62932074, Validation loss: 105.00414381, Gradient norm: 172.48860358
INFO:root:[   92] Training loss: 103.63644645, Validation loss: 104.95532411, Gradient norm: 217.28855839
INFO:root:[   93] Training loss: 103.48004265, Validation loss: 104.66180551, Gradient norm: 170.22215553
INFO:root:[   94] Training loss: 103.34830191, Validation loss: 104.97486062, Gradient norm: 147.32060868
INFO:root:[   95] Training loss: 103.41371310, Validation loss: 104.77047387, Gradient norm: 166.00487316
INFO:root:[   96] Training loss: 103.39218862, Validation loss: 104.62268145, Gradient norm: 197.96043581
INFO:root:[   97] Training loss: 103.21454647, Validation loss: 104.71845061, Gradient norm: 160.02482236
INFO:root:[   98] Training loss: 103.23949831, Validation loss: 105.27430146, Gradient norm: 196.27694363
INFO:root:[   99] Training loss: 103.16826339, Validation loss: 104.51094950, Gradient norm: 172.71705393
INFO:root:[  100] Training loss: 103.07431712, Validation loss: 104.83543317, Gradient norm: 168.59462389
INFO:root:[  101] Training loss: 103.01860283, Validation loss: 104.58135197, Gradient norm: 182.51194638
INFO:root:[  102] Training loss: 103.01244861, Validation loss: 104.67120940, Gradient norm: 172.95744954
INFO:root:[  103] Training loss: 102.99025058, Validation loss: 104.49972192, Gradient norm: 213.88260906
INFO:root:[  104] Training loss: 102.89620567, Validation loss: 104.61028395, Gradient norm: 173.71222894
INFO:root:[  105] Training loss: 102.80953676, Validation loss: 104.54768924, Gradient norm: 209.86437635
INFO:root:[  106] Training loss: 102.75672190, Validation loss: 104.40045298, Gradient norm: 172.05138915
INFO:root:[  107] Training loss: 102.74806605, Validation loss: 104.14284594, Gradient norm: 181.64428869
INFO:root:[  108] Training loss: 102.69088684, Validation loss: 104.40315773, Gradient norm: 222.09323982
INFO:root:[  109] Training loss: 102.63382606, Validation loss: 104.55770295, Gradient norm: 200.23058442
INFO:root:[  110] Training loss: 102.59565120, Validation loss: 104.31927306, Gradient norm: 172.22175786
INFO:root:[  111] Training loss: 102.57818955, Validation loss: 104.88287827, Gradient norm: 228.01783885
INFO:root:[  112] Training loss: 102.45190673, Validation loss: 104.21360147, Gradient norm: 193.65559219
INFO:root:[  113] Training loss: 102.48802421, Validation loss: 104.54369486, Gradient norm: 210.94365085
INFO:root:[  114] Training loss: 102.39057382, Validation loss: 104.48920625, Gradient norm: 176.67075862
INFO:root:[  115] Training loss: 102.38509673, Validation loss: 104.95061204, Gradient norm: 184.01925082
INFO:root:[  116] Training loss: 102.31096467, Validation loss: 104.03726433, Gradient norm: 218.40607413
INFO:root:[  117] Training loss: 102.30828864, Validation loss: 103.93396891, Gradient norm: 211.57228286
INFO:root:[  118] Training loss: 102.29412923, Validation loss: 103.81511399, Gradient norm: 209.16473627
INFO:root:[  119] Training loss: 102.12481804, Validation loss: 104.51260902, Gradient norm: 201.95500750
INFO:root:[  120] Training loss: 102.04740015, Validation loss: 104.13702051, Gradient norm: 206.59165661
INFO:root:[  121] Training loss: 102.06722786, Validation loss: 104.08727264, Gradient norm: 204.29842764
INFO:root:[  122] Training loss: 102.04691483, Validation loss: 104.07766645, Gradient norm: 191.13659364
INFO:root:[  123] Training loss: 102.02778686, Validation loss: 104.07379571, Gradient norm: 219.60218928
INFO:root:[  124] Training loss: 101.93822115, Validation loss: 104.34499727, Gradient norm: 211.63627547
INFO:root:[  125] Training loss: 101.86454354, Validation loss: 104.09994638, Gradient norm: 198.18474820
INFO:root:[  126] Training loss: 101.90676097, Validation loss: 104.11451669, Gradient norm: 216.14231877
INFO:root:[  127] Training loss: 101.89253707, Validation loss: 104.37616625, Gradient norm: 216.05240676
INFO:root:EP 127: Early stopping
INFO:root:Training the model took 2267.163s.
INFO:root:Emptying the cuda cache took 0.047s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 144.55266
INFO:root:EnergyScoreTrain: 101.80949
INFO:root:CoverageTrain: 0.77493
INFO:root:IntervalWidthTrain: 8.16361
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.61374
INFO:root:EnergyScoreValidation: 103.96
INFO:root:CoverageValidation: 0.76991
INFO:root:IntervalWidthValidation: 8.15439
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.8974
INFO:root:EnergyScoreTest: 104.16143
INFO:root:CoverageTest: 0.76909
INFO:root:IntervalWidthTest: 8.15193
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.99962933, Validation loss: 122.14773349, Gradient norm: 233.32521113
INFO:root:[    2] Training loss: 121.67214763, Validation loss: 121.64066551, Gradient norm: 207.58973716
INFO:root:[    3] Training loss: 121.61242406, Validation loss: 121.36082932, Gradient norm: 213.55073614
INFO:root:[    4] Training loss: 121.21496319, Validation loss: 121.18811035, Gradient norm: 158.51866673
INFO:root:[    5] Training loss: 120.91417789, Validation loss: 120.77568212, Gradient norm: 136.48765196
INFO:root:[    6] Training loss: 120.71121425, Validation loss: 120.71366304, Gradient norm: 116.20686173
INFO:root:[    7] Training loss: 120.63133760, Validation loss: 120.58879879, Gradient norm: 125.74780616
INFO:root:[    8] Training loss: 120.54383978, Validation loss: 121.10316783, Gradient norm: 115.02603476
INFO:root:[    9] Training loss: 120.43429714, Validation loss: 120.45053390, Gradient norm: 104.48711771
INFO:root:[   10] Training loss: 120.33101998, Validation loss: 120.37337994, Gradient norm: 94.64955035
INFO:root:[   11] Training loss: 120.24670667, Validation loss: 120.00898059, Gradient norm: 91.46997722
INFO:root:[   12] Training loss: 119.67546325, Validation loss: 119.18103922, Gradient norm: 101.70947872
INFO:root:[   13] Training loss: 118.18362697, Validation loss: 117.51292735, Gradient norm: 82.18041522
INFO:root:[   14] Training loss: 116.69534491, Validation loss: 116.03040577, Gradient norm: 84.75282565
INFO:root:[   15] Training loss: 115.39118566, Validation loss: 114.77900933, Gradient norm: 84.89510942
INFO:root:[   16] Training loss: 114.39063411, Validation loss: 114.22731649, Gradient norm: 65.95719477
INFO:root:[   17] Training loss: 113.50526766, Validation loss: 113.46686186, Gradient norm: 66.84099037
INFO:root:[   18] Training loss: 112.88632803, Validation loss: 112.61643351, Gradient norm: 67.40472050
INFO:root:[   19] Training loss: 112.25781270, Validation loss: 112.32966877, Gradient norm: 75.11859773
INFO:root:[   20] Training loss: 111.78496477, Validation loss: 111.75597618, Gradient norm: 64.43712480
INFO:root:[   21] Training loss: 111.44165761, Validation loss: 111.42640344, Gradient norm: 64.62741148
INFO:root:[   22] Training loss: 110.98328636, Validation loss: 110.85179296, Gradient norm: 59.00047201
INFO:root:[   23] Training loss: 110.66894524, Validation loss: 110.68379238, Gradient norm: 68.24608720
INFO:root:[   24] Training loss: 110.36818351, Validation loss: 110.42856098, Gradient norm: 63.25168019
INFO:root:[   25] Training loss: 110.05404663, Validation loss: 110.14774007, Gradient norm: 67.14720043
INFO:root:[   26] Training loss: 109.80992045, Validation loss: 109.86008901, Gradient norm: 69.97483539
INFO:root:[   27] Training loss: 109.61678172, Validation loss: 109.71644461, Gradient norm: 72.27383556
INFO:root:[   28] Training loss: 109.30787625, Validation loss: 109.59449216, Gradient norm: 61.88513351
INFO:root:[   29] Training loss: 109.11573751, Validation loss: 109.27467741, Gradient norm: 68.60683816
INFO:root:[   30] Training loss: 108.91872683, Validation loss: 109.29844929, Gradient norm: 77.22598139
INFO:root:[   31] Training loss: 108.79594462, Validation loss: 108.74533370, Gradient norm: 77.78476514
INFO:root:[   32] Training loss: 108.48473655, Validation loss: 108.71040029, Gradient norm: 59.51591776
INFO:root:[   33] Training loss: 108.35815673, Validation loss: 108.68280161, Gradient norm: 72.04878255
INFO:root:[   34] Training loss: 108.18389204, Validation loss: 108.66855200, Gradient norm: 85.22142300
INFO:root:[   35] Training loss: 108.04232045, Validation loss: 108.35231465, Gradient norm: 79.43923415
INFO:root:[   36] Training loss: 107.85394503, Validation loss: 108.10438353, Gradient norm: 72.39507631
INFO:root:[   37] Training loss: 107.66324892, Validation loss: 108.02524172, Gradient norm: 81.08589118
INFO:root:[   38] Training loss: 107.54352090, Validation loss: 107.83820211, Gradient norm: 72.80543941
INFO:root:[   39] Training loss: 107.44190263, Validation loss: 107.84199866, Gradient norm: 81.08303988
INFO:root:[   40] Training loss: 107.19717272, Validation loss: 107.76300312, Gradient norm: 79.31149580
INFO:root:[   41] Training loss: 107.07993972, Validation loss: 107.64452178, Gradient norm: 74.33051418
INFO:root:[   42] Training loss: 107.00145728, Validation loss: 107.46527126, Gradient norm: 88.93046518
INFO:root:[   43] Training loss: 106.87198409, Validation loss: 107.36508152, Gradient norm: 97.69645534
INFO:root:[   44] Training loss: 106.74471202, Validation loss: 107.22007146, Gradient norm: 78.79334173
INFO:root:[   45] Training loss: 106.60300202, Validation loss: 107.09356847, Gradient norm: 68.60657117
INFO:root:[   46] Training loss: 106.48971996, Validation loss: 107.13088147, Gradient norm: 94.84152965
INFO:root:[   47] Training loss: 106.43590694, Validation loss: 106.84642318, Gradient norm: 94.21997118
INFO:root:[   48] Training loss: 106.24452945, Validation loss: 106.81320112, Gradient norm: 89.39516229
INFO:root:[   49] Training loss: 106.15133647, Validation loss: 106.78981439, Gradient norm: 96.41244401
INFO:root:[   50] Training loss: 106.03659577, Validation loss: 106.63825173, Gradient norm: 88.45009332
INFO:root:[   51] Training loss: 105.97406701, Validation loss: 106.69855052, Gradient norm: 101.98949501
INFO:root:[   52] Training loss: 105.85564342, Validation loss: 106.49594116, Gradient norm: 100.59860997
INFO:root:[   53] Training loss: 105.80563327, Validation loss: 106.49364182, Gradient norm: 92.57845231
INFO:root:[   54] Training loss: 105.59857590, Validation loss: 106.48693137, Gradient norm: 94.11952970
INFO:root:[   55] Training loss: 105.56126357, Validation loss: 106.26295261, Gradient norm: 95.38080704
INFO:root:[   56] Training loss: 105.45817613, Validation loss: 106.26804668, Gradient norm: 113.42225931
INFO:root:[   57] Training loss: 105.33241238, Validation loss: 106.13458699, Gradient norm: 101.36964442
INFO:root:[   58] Training loss: 105.26552670, Validation loss: 106.35138860, Gradient norm: 98.02465311
INFO:root:[   59] Training loss: 105.19959246, Validation loss: 105.94532355, Gradient norm: 113.04359307
INFO:root:[   60] Training loss: 105.18434015, Validation loss: 106.02232282, Gradient norm: 109.68995226
INFO:root:[   61] Training loss: 105.07014654, Validation loss: 106.30100171, Gradient norm: 118.79735047
INFO:root:[   62] Training loss: 105.02374436, Validation loss: 105.77279005, Gradient norm: 126.73157617
INFO:root:[   63] Training loss: 104.90572708, Validation loss: 105.89492982, Gradient norm: 102.41807715
INFO:root:[   64] Training loss: 104.75578882, Validation loss: 105.93615460, Gradient norm: 123.65395009
INFO:root:[   65] Training loss: 104.79376207, Validation loss: 105.61674210, Gradient norm: 129.72767130
INFO:root:[   66] Training loss: 104.68101231, Validation loss: 105.89608160, Gradient norm: 121.08264241
INFO:root:[   67] Training loss: 104.60219108, Validation loss: 105.71785578, Gradient norm: 127.60067485
INFO:root:[   68] Training loss: 104.55751287, Validation loss: 105.54741695, Gradient norm: 138.63159920
INFO:root:[   69] Training loss: 104.49127501, Validation loss: 105.51620431, Gradient norm: 109.15720759
INFO:root:[   70] Training loss: 104.44848727, Validation loss: 105.41631317, Gradient norm: 132.49143577
INFO:root:[   71] Training loss: 104.41609543, Validation loss: 105.19654346, Gradient norm: 149.61341443
INFO:root:[   72] Training loss: 104.21433670, Validation loss: 105.24527556, Gradient norm: 126.21871225
INFO:root:[   73] Training loss: 104.21410046, Validation loss: 105.36414469, Gradient norm: 142.44738978
INFO:root:[   74] Training loss: 104.12717951, Validation loss: 105.19889647, Gradient norm: 136.08411611
INFO:root:[   75] Training loss: 104.11379093, Validation loss: 105.22958769, Gradient norm: 156.48944925
INFO:root:[   76] Training loss: 104.05956606, Validation loss: 105.24466442, Gradient norm: 132.82507170
INFO:root:[   77] Training loss: 104.04872975, Validation loss: 105.09040543, Gradient norm: 178.19723863
INFO:root:[   78] Training loss: 103.90008133, Validation loss: 104.87635067, Gradient norm: 157.92774643
INFO:root:[   79] Training loss: 103.80812795, Validation loss: 104.98850434, Gradient norm: 148.17683392
INFO:root:[   80] Training loss: 103.79026052, Validation loss: 105.23912995, Gradient norm: 153.88151102
INFO:root:[   81] Training loss: 103.80609266, Validation loss: 104.97250393, Gradient norm: 176.12906208
INFO:root:[   82] Training loss: 103.64253863, Validation loss: 104.91022334, Gradient norm: 159.36708483
INFO:root:[   83] Training loss: 103.66878442, Validation loss: 105.42111364, Gradient norm: 165.52782070
INFO:root:[   84] Training loss: 103.62035397, Validation loss: 104.77104582, Gradient norm: 187.68638437
INFO:root:[   85] Training loss: 103.54669068, Validation loss: 105.01546715, Gradient norm: 170.03134832
INFO:root:[   86] Training loss: 103.52420253, Validation loss: 104.64988682, Gradient norm: 196.50734531
INFO:root:[   87] Training loss: 103.42982969, Validation loss: 104.48642573, Gradient norm: 170.36045273
INFO:root:[   88] Training loss: 103.40265514, Validation loss: 105.08511747, Gradient norm: 192.93492113
INFO:root:[   89] Training loss: 103.37872348, Validation loss: 104.54318527, Gradient norm: 188.07765424
INFO:root:[   90] Training loss: 103.32831864, Validation loss: 104.63491874, Gradient norm: 212.45729273
INFO:root:[   91] Training loss: 103.23189943, Validation loss: 104.47014697, Gradient norm: 179.00027371
INFO:root:[   92] Training loss: 103.19084397, Validation loss: 104.28670686, Gradient norm: 210.09566464
INFO:root:[   93] Training loss: 103.18237737, Validation loss: 104.63273936, Gradient norm: 210.77007131
INFO:root:[   94] Training loss: 103.06580569, Validation loss: 104.53695573, Gradient norm: 189.46626420
INFO:root:[   95] Training loss: 103.02440745, Validation loss: 104.41977876, Gradient norm: 213.17594132
INFO:root:[   96] Training loss: 102.92020592, Validation loss: 104.37308397, Gradient norm: 190.20177262
INFO:root:[   97] Training loss: 102.97230429, Validation loss: 104.87434598, Gradient norm: 209.65452026
INFO:root:[   98] Training loss: 102.90074124, Validation loss: 104.63259151, Gradient norm: 233.26150811
INFO:root:[   99] Training loss: 102.80864689, Validation loss: 104.05992258, Gradient norm: 214.66585097
INFO:root:[  100] Training loss: 102.89620573, Validation loss: 104.49876904, Gradient norm: 240.79529373
INFO:root:[  101] Training loss: 102.73394742, Validation loss: 104.72941510, Gradient norm: 209.57892039
INFO:root:[  102] Training loss: 102.73752101, Validation loss: 104.22754301, Gradient norm: 203.49872879
INFO:root:[  103] Training loss: 102.66531291, Validation loss: 104.74340347, Gradient norm: 216.99921653
INFO:root:[  104] Training loss: 102.55943676, Validation loss: 104.93022393, Gradient norm: 213.44257413
INFO:root:[  105] Training loss: 102.57814471, Validation loss: 104.41758544, Gradient norm: 240.37711082
INFO:root:[  106] Training loss: 102.56221670, Validation loss: 104.24932546, Gradient norm: 246.28936541
INFO:root:[  107] Training loss: 102.48398280, Validation loss: 104.59230357, Gradient norm: 213.03942660
INFO:root:[  108] Training loss: 102.42698622, Validation loss: 103.93896327, Gradient norm: 216.54611667
INFO:root:[  109] Training loss: 102.44908378, Validation loss: 104.01221755, Gradient norm: 219.61759860
INFO:root:[  110] Training loss: 102.30175768, Validation loss: 104.36759265, Gradient norm: 215.54088610
INFO:root:[  111] Training loss: 102.29999826, Validation loss: 104.52880886, Gradient norm: 227.77997161
INFO:root:[  112] Training loss: 102.33241096, Validation loss: 104.12381297, Gradient norm: 252.34254470
INFO:root:[  113] Training loss: 102.23551543, Validation loss: 103.82362997, Gradient norm: 237.69047345
INFO:root:[  114] Training loss: 102.21209872, Validation loss: 104.08728738, Gradient norm: 242.76193250
INFO:root:[  115] Training loss: 102.22346605, Validation loss: 104.88643015, Gradient norm: 262.91559945
INFO:root:[  116] Training loss: 102.17655783, Validation loss: 104.03583895, Gradient norm: 238.09353126
INFO:root:[  117] Training loss: 102.12227880, Validation loss: 104.04663586, Gradient norm: 239.42899242
INFO:root:[  118] Training loss: 102.16834333, Validation loss: 104.18020998, Gradient norm: 243.29935734
INFO:root:[  119] Training loss: 102.09971578, Validation loss: 104.18440457, Gradient norm: 267.04433607
INFO:root:[  120] Training loss: 102.03920503, Validation loss: 104.42546844, Gradient norm: 287.94293890
INFO:root:[  121] Training loss: 102.07024694, Validation loss: 104.18506438, Gradient norm: 266.02344202
INFO:root:[  122] Training loss: 101.95792105, Validation loss: 103.81772350, Gradient norm: 237.42673047
INFO:root:[  123] Training loss: 101.97680414, Validation loss: 103.80760061, Gradient norm: 241.44584412
INFO:root:[  124] Training loss: 101.93770363, Validation loss: 103.81967084, Gradient norm: 308.24374582
INFO:root:[  125] Training loss: 101.86466771, Validation loss: 104.05519709, Gradient norm: 256.65417305
INFO:root:[  126] Training loss: 101.81531228, Validation loss: 104.50317804, Gradient norm: 242.15174183
INFO:root:[  127] Training loss: 101.76563114, Validation loss: 104.23781060, Gradient norm: 279.29693433
INFO:root:[  128] Training loss: 101.78399584, Validation loss: 104.48012280, Gradient norm: 276.77260463
INFO:root:[  129] Training loss: 101.67063377, Validation loss: 103.93252458, Gradient norm: 244.84201545
INFO:root:[  130] Training loss: 101.70994257, Validation loss: 104.05838407, Gradient norm: 287.56564285
INFO:root:[  131] Training loss: 101.60683758, Validation loss: 103.99223354, Gradient norm: 270.04821008
INFO:root:[  132] Training loss: 101.70420736, Validation loss: 103.87910830, Gradient norm: 295.95342022
INFO:root:[  133] Training loss: 101.57932687, Validation loss: 103.76930026, Gradient norm: 255.33690810
INFO:root:[  134] Training loss: 101.56376479, Validation loss: 104.30617786, Gradient norm: 288.09289167
INFO:root:[  135] Training loss: 101.57483200, Validation loss: 103.83405672, Gradient norm: 265.14354426
INFO:root:[  136] Training loss: 101.50231785, Validation loss: 104.06249079, Gradient norm: 294.27693707
INFO:root:[  137] Training loss: 101.32396104, Validation loss: 104.14480880, Gradient norm: 266.50695797
INFO:root:[  138] Training loss: 101.41064041, Validation loss: 104.19307525, Gradient norm: 290.89933475
INFO:root:[  139] Training loss: 101.39048943, Validation loss: 103.53640142, Gradient norm: 288.97144324
INFO:root:[  140] Training loss: 101.35447990, Validation loss: 103.40197570, Gradient norm: 237.02762406
INFO:root:[  141] Training loss: 101.35395651, Validation loss: 103.76374028, Gradient norm: 302.82545454
INFO:root:[  142] Training loss: 101.37246171, Validation loss: 103.92916975, Gradient norm: 283.73331879
INFO:root:[  143] Training loss: 101.28498591, Validation loss: 103.83585831, Gradient norm: 276.39873891
INFO:root:[  144] Training loss: 101.24879948, Validation loss: 103.61866155, Gradient norm: 295.55291752
INFO:root:[  145] Training loss: 101.26697932, Validation loss: 104.02345618, Gradient norm: 265.92726911
INFO:root:[  146] Training loss: 101.19877246, Validation loss: 103.85555557, Gradient norm: 272.80254677
INFO:root:[  147] Training loss: 101.10963960, Validation loss: 104.04463906, Gradient norm: 274.15067477
INFO:root:[  148] Training loss: 101.21537754, Validation loss: 104.02845133, Gradient norm: 304.78787884
INFO:root:[  149] Training loss: 101.11328652, Validation loss: 103.64154868, Gradient norm: 290.41051052
INFO:root:EP 149: Early stopping
INFO:root:Training the model took 2651.721s.
INFO:root:Emptying the cuda cache took 0.048s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 143.60922
INFO:root:EnergyScoreTrain: 101.14791
INFO:root:CoverageTrain: 0.78358
INFO:root:IntervalWidthTrain: 8.17517
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.16999
INFO:root:EnergyScoreValidation: 103.66114
INFO:root:CoverageValidation: 0.77727
INFO:root:IntervalWidthValidation: 8.16928
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.46139
INFO:root:EnergyScoreTest: 103.87714
INFO:root:CoverageTest: 0.77601
INFO:root:IntervalWidthTest: 8.15178
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.43020981, Validation loss: 121.63912859, Gradient norm: 157.70896343
INFO:root:[    2] Training loss: 121.61200343, Validation loss: 121.62412262, Gradient norm: 143.09541677
INFO:root:[    3] Training loss: 121.42021503, Validation loss: 121.37338941, Gradient norm: 118.53249164
INFO:root:[    4] Training loss: 121.24649169, Validation loss: 120.88892443, Gradient norm: 138.08636253
INFO:root:[    5] Training loss: 120.84491946, Validation loss: 120.76877015, Gradient norm: 88.62494895
INFO:root:[    6] Training loss: 120.72318403, Validation loss: 120.68800933, Gradient norm: 93.93146146
INFO:root:[    7] Training loss: 120.60997293, Validation loss: 120.73808920, Gradient norm: 82.12208094
INFO:root:[    8] Training loss: 120.44569451, Validation loss: 120.35447088, Gradient norm: 61.26939709
INFO:root:[    9] Training loss: 120.19033564, Validation loss: 119.95992332, Gradient norm: 67.65600701
INFO:root:[   10] Training loss: 119.23823102, Validation loss: 118.49214067, Gradient norm: 56.44311332
INFO:root:[   11] Training loss: 117.92197810, Validation loss: 117.40962798, Gradient norm: 59.18241166
INFO:root:[   12] Training loss: 116.66350522, Validation loss: 116.15786822, Gradient norm: 55.02932639
INFO:root:[   13] Training loss: 115.63045887, Validation loss: 115.31597716, Gradient norm: 54.62668526
INFO:root:[   14] Training loss: 114.75451363, Validation loss: 114.63321633, Gradient norm: 48.66582906
INFO:root:[   15] Training loss: 114.01740258, Validation loss: 113.78716725, Gradient norm: 52.12209393
INFO:root:[   16] Training loss: 113.43569339, Validation loss: 113.19015029, Gradient norm: 68.53410462
INFO:root:[   17] Training loss: 112.87454926, Validation loss: 113.14640124, Gradient norm: 71.33499521
INFO:root:[   18] Training loss: 112.36081081, Validation loss: 112.18825768, Gradient norm: 61.28443183
INFO:root:[   19] Training loss: 111.90672559, Validation loss: 111.84091476, Gradient norm: 63.92551718
INFO:root:[   20] Training loss: 111.47328665, Validation loss: 111.62807280, Gradient norm: 76.24568720
INFO:root:[   21] Training loss: 111.16846844, Validation loss: 111.34213020, Gradient norm: 75.33004971
INFO:root:[   22] Training loss: 110.77287630, Validation loss: 110.93151724, Gradient norm: 68.56873941
INFO:root:[   23] Training loss: 110.48907957, Validation loss: 110.57095968, Gradient norm: 71.15062886
INFO:root:[   24] Training loss: 110.24618645, Validation loss: 110.35389157, Gradient norm: 70.49689178
INFO:root:[   25] Training loss: 110.00315074, Validation loss: 110.21799469, Gradient norm: 71.25862925
INFO:root:[   26] Training loss: 109.79297840, Validation loss: 109.97045925, Gradient norm: 89.15995089
INFO:root:[   27] Training loss: 109.56220293, Validation loss: 109.73018357, Gradient norm: 87.28154917
INFO:root:[   28] Training loss: 109.26116565, Validation loss: 109.71765216, Gradient norm: 63.36345338
INFO:root:[   29] Training loss: 109.17807817, Validation loss: 109.31274072, Gradient norm: 103.75984010
INFO:root:[   30] Training loss: 108.92481191, Validation loss: 109.29726463, Gradient norm: 70.23011288
INFO:root:[   31] Training loss: 108.78952236, Validation loss: 109.29442728, Gradient norm: 81.61729188
INFO:root:[   32] Training loss: 108.59852965, Validation loss: 109.16841099, Gradient norm: 98.61962456
INFO:root:[   33] Training loss: 108.43504502, Validation loss: 108.62534595, Gradient norm: 93.96338202
INFO:root:[   34] Training loss: 108.28940994, Validation loss: 108.55320293, Gradient norm: 91.87070009
INFO:root:[   35] Training loss: 108.19584602, Validation loss: 108.42399781, Gradient norm: 105.66592229
INFO:root:[   36] Training loss: 108.01843890, Validation loss: 108.31791792, Gradient norm: 93.54552955
INFO:root:[   37] Training loss: 107.88836150, Validation loss: 108.19884701, Gradient norm: 101.24528719
INFO:root:[   38] Training loss: 107.78674593, Validation loss: 108.33011285, Gradient norm: 103.99536169
INFO:root:[   39] Training loss: 107.66792223, Validation loss: 108.06992945, Gradient norm: 105.40933196
INFO:root:[   40] Training loss: 107.56263024, Validation loss: 107.84466711, Gradient norm: 112.51524947
INFO:root:[   41] Training loss: 107.35871752, Validation loss: 107.76511383, Gradient norm: 127.75283162
INFO:root:[   42] Training loss: 107.24800974, Validation loss: 107.77601860, Gradient norm: 98.69546623
INFO:root:[   43] Training loss: 107.21437336, Validation loss: 107.78497604, Gradient norm: 129.50060574
INFO:root:[   44] Training loss: 107.14609251, Validation loss: 107.58586647, Gradient norm: 120.21565489
INFO:root:[   45] Training loss: 107.01643183, Validation loss: 107.34757864, Gradient norm: 140.92380985
INFO:root:[   46] Training loss: 106.92656174, Validation loss: 107.32996132, Gradient norm: 121.50138475
INFO:root:[   47] Training loss: 106.78148064, Validation loss: 107.17989586, Gradient norm: 121.11244493
INFO:root:[   48] Training loss: 106.73503964, Validation loss: 107.10371373, Gradient norm: 169.09661311
INFO:root:[   49] Training loss: 106.58085443, Validation loss: 107.37811043, Gradient norm: 145.51424454
INFO:root:[   50] Training loss: 106.52010258, Validation loss: 107.11746689, Gradient norm: 141.72889570
INFO:root:[   51] Training loss: 106.45743304, Validation loss: 106.94192742, Gradient norm: 172.74246508
INFO:root:[   52] Training loss: 106.33572644, Validation loss: 107.03493815, Gradient norm: 146.01866870
INFO:root:[   53] Training loss: 106.18055820, Validation loss: 106.67517037, Gradient norm: 146.94436941
INFO:root:[   54] Training loss: 106.10205530, Validation loss: 106.88150051, Gradient norm: 153.00949970
INFO:root:[   55] Training loss: 106.17397241, Validation loss: 106.76209548, Gradient norm: 216.62014873
INFO:root:[   56] Training loss: 105.97548655, Validation loss: 106.56717050, Gradient norm: 141.65261806
INFO:root:[   57] Training loss: 105.95554163, Validation loss: 107.67118914, Gradient norm: 189.49405665
INFO:root:[   58] Training loss: 105.85034396, Validation loss: 106.85848762, Gradient norm: 181.34419437
INFO:root:[   59] Training loss: 105.72688908, Validation loss: 106.57684010, Gradient norm: 169.69651050
INFO:root:[   60] Training loss: 105.67443625, Validation loss: 106.20033027, Gradient norm: 222.47686009
INFO:root:[   61] Training loss: 105.71670937, Validation loss: 106.57401460, Gradient norm: 206.44737955
INFO:root:[   62] Training loss: 105.57831114, Validation loss: 106.19146676, Gradient norm: 195.08216489
INFO:root:[   63] Training loss: 105.52828757, Validation loss: 106.09428616, Gradient norm: 216.27018798
INFO:root:[   64] Training loss: 105.44564975, Validation loss: 105.99945516, Gradient norm: 224.03500360
INFO:root:[   65] Training loss: 105.44210923, Validation loss: 105.88060313, Gradient norm: 226.41882500
INFO:root:[   66] Training loss: 105.31517191, Validation loss: 105.88385746, Gradient norm: 187.98295088
INFO:root:[   67] Training loss: 105.29429836, Validation loss: 106.05893181, Gradient norm: 234.83855903
INFO:root:[   68] Training loss: 105.18171786, Validation loss: 106.20988412, Gradient norm: 212.84132797
INFO:root:[   69] Training loss: 105.17433767, Validation loss: 105.92246694, Gradient norm: 251.46941691
INFO:root:[   70] Training loss: 105.05192134, Validation loss: 105.93323833, Gradient norm: 234.26670498
INFO:root:[   71] Training loss: 105.00566061, Validation loss: 105.70860001, Gradient norm: 219.73240648
INFO:root:[   72] Training loss: 104.97020559, Validation loss: 105.76025838, Gradient norm: 271.78930780
INFO:root:[   73] Training loss: 104.95120685, Validation loss: 105.54873000, Gradient norm: 272.11070368
INFO:root:[   74] Training loss: 104.90647794, Validation loss: 105.36055098, Gradient norm: 282.94403606
INFO:root:[   75] Training loss: 104.78594289, Validation loss: 105.38737225, Gradient norm: 234.85360680
INFO:root:[   76] Training loss: 104.88634234, Validation loss: 105.62222921, Gradient norm: 321.17947567
INFO:root:[   77] Training loss: 104.74600929, Validation loss: 105.36821642, Gradient norm: 239.61004576
INFO:root:[   78] Training loss: 104.74019231, Validation loss: 105.21320685, Gradient norm: 293.69939969
INFO:root:[   79] Training loss: 104.66569033, Validation loss: 105.40599481, Gradient norm: 285.87892819
INFO:root:[   80] Training loss: 104.54706627, Validation loss: 105.73730232, Gradient norm: 261.22008933
INFO:root:[   81] Training loss: 104.58987447, Validation loss: 105.43218731, Gradient norm: 313.76087672
INFO:root:[   82] Training loss: 104.53121334, Validation loss: 105.35900037, Gradient norm: 314.49269162
INFO:root:[   83] Training loss: 104.46883797, Validation loss: 105.46808045, Gradient norm: 283.42502954
INFO:root:[   84] Training loss: 104.50479632, Validation loss: 105.21074782, Gradient norm: 327.23180422
INFO:root:[   85] Training loss: 104.35214301, Validation loss: 104.88067837, Gradient norm: 323.32402791
INFO:root:[   86] Training loss: 104.31646452, Validation loss: 105.04785814, Gradient norm: 274.89372985
INFO:root:[   87] Training loss: 104.21945764, Validation loss: 104.83776882, Gradient norm: 311.98111083
INFO:root:[   88] Training loss: 104.27783743, Validation loss: 105.07499642, Gradient norm: 322.13282479
INFO:root:[   89] Training loss: 104.17462273, Validation loss: 105.10018553, Gradient norm: 330.20637824
INFO:root:[   90] Training loss: 104.05515006, Validation loss: 105.05370620, Gradient norm: 294.79352439
INFO:root:[   91] Training loss: 104.10042005, Validation loss: 105.19752476, Gradient norm: 341.43010016
INFO:root:[   92] Training loss: 104.13370426, Validation loss: 105.57719079, Gradient norm: 347.79461501
INFO:root:[   93] Training loss: 104.01829610, Validation loss: 105.23205461, Gradient norm: 337.87725626
INFO:root:[   94] Training loss: 103.96244306, Validation loss: 104.84056486, Gradient norm: 286.73631642
INFO:root:[   95] Training loss: 103.91043341, Validation loss: 104.72185332, Gradient norm: 320.73450056
INFO:root:[   96] Training loss: 103.98055254, Validation loss: 104.71740591, Gradient norm: 325.77300133
INFO:root:[   97] Training loss: 103.88213497, Validation loss: 104.91838311, Gradient norm: 324.01346243
INFO:root:[   98] Training loss: 103.92328907, Validation loss: 104.67293364, Gradient norm: 355.16195207
INFO:root:[   99] Training loss: 103.78090580, Validation loss: 104.75292890, Gradient norm: 336.58332202
INFO:root:[  100] Training loss: 103.78793632, Validation loss: 105.04663402, Gradient norm: 340.99176057
INFO:root:[  101] Training loss: 103.69965781, Validation loss: 104.93109447, Gradient norm: 325.86530586
INFO:root:[  102] Training loss: 103.63698409, Validation loss: 104.79602024, Gradient norm: 296.79110185
INFO:root:[  103] Training loss: 103.70007945, Validation loss: 104.46167045, Gradient norm: 394.72581742
INFO:root:[  104] Training loss: 103.63155230, Validation loss: 104.79073755, Gradient norm: 341.36848517
INFO:root:[  105] Training loss: 103.54203628, Validation loss: 104.42728082, Gradient norm: 344.57788543
INFO:root:[  106] Training loss: 103.61400098, Validation loss: 104.80121297, Gradient norm: 366.22666729
INFO:root:[  107] Training loss: 103.57438194, Validation loss: 104.50265661, Gradient norm: 345.03312341
INFO:root:[  108] Training loss: 103.39465474, Validation loss: 105.55075968, Gradient norm: 297.80494589
INFO:root:[  109] Training loss: 103.51520957, Validation loss: 105.06867455, Gradient norm: 349.34371478
INFO:root:[  110] Training loss: 103.41238174, Validation loss: 105.62223947, Gradient norm: 341.70837894
INFO:root:[  111] Training loss: 103.34502654, Validation loss: 104.29986993, Gradient norm: 317.76302634
INFO:root:[  112] Training loss: 103.42647114, Validation loss: 104.34315438, Gradient norm: 380.20071590
INFO:root:[  113] Training loss: 103.32046110, Validation loss: 104.35159328, Gradient norm: 367.33846863
INFO:root:[  114] Training loss: 103.31315363, Validation loss: 104.59442639, Gradient norm: 363.24844342
INFO:root:[  115] Training loss: 103.16476380, Validation loss: 104.33343979, Gradient norm: 349.32072860
INFO:root:[  116] Training loss: 103.25246456, Validation loss: 104.33986795, Gradient norm: 362.60100579
INFO:root:[  117] Training loss: 103.19098157, Validation loss: 104.61267800, Gradient norm: 394.29508265
INFO:root:[  118] Training loss: 103.24122856, Validation loss: 104.06451495, Gradient norm: 366.58100016
INFO:root:[  119] Training loss: 103.14901882, Validation loss: 104.46866660, Gradient norm: 364.48800285
INFO:root:[  120] Training loss: 103.02302187, Validation loss: 104.28341280, Gradient norm: 360.32771135
INFO:root:[  121] Training loss: 103.07535539, Validation loss: 104.13406267, Gradient norm: 409.84105679
INFO:root:[  122] Training loss: 102.98714899, Validation loss: 104.24183129, Gradient norm: 355.28935219
INFO:root:[  123] Training loss: 103.00245221, Validation loss: 104.33459367, Gradient norm: 365.65858647
INFO:root:[  124] Training loss: 103.02470303, Validation loss: 104.53714936, Gradient norm: 378.08692665
INFO:root:[  125] Training loss: 103.02343838, Validation loss: 104.01744422, Gradient norm: 407.51492920
INFO:root:[  126] Training loss: 102.96394564, Validation loss: 104.22254891, Gradient norm: 362.20327613
INFO:root:[  127] Training loss: 102.91104518, Validation loss: 103.89726152, Gradient norm: 344.38875932
INFO:root:[  128] Training loss: 102.96480655, Validation loss: 104.80347416, Gradient norm: 406.70690176
INFO:root:[  129] Training loss: 102.91622000, Validation loss: 108.40837913, Gradient norm: 402.82400876
INFO:root:[  130] Training loss: 102.90328304, Validation loss: 104.33281603, Gradient norm: 391.50002800
INFO:root:[  131] Training loss: 102.83820741, Validation loss: 103.78636433, Gradient norm: 373.80166829
INFO:root:[  132] Training loss: 102.76687021, Validation loss: 104.08690985, Gradient norm: 351.70179369
INFO:root:[  133] Training loss: 102.74751937, Validation loss: 104.82045299, Gradient norm: 368.89542116
INFO:root:[  134] Training loss: 102.74233394, Validation loss: 104.22416056, Gradient norm: 405.84397879
INFO:root:[  135] Training loss: 102.76583869, Validation loss: 103.81529262, Gradient norm: 384.31323517
INFO:root:[  136] Training loss: 102.72551390, Validation loss: 104.40716395, Gradient norm: 392.72899094
INFO:root:[  137] Training loss: 102.71873717, Validation loss: 103.96114086, Gradient norm: 416.41654209
INFO:root:[  138] Training loss: 102.61338840, Validation loss: 104.26452821, Gradient norm: 378.69777709
INFO:root:[  139] Training loss: 102.66179542, Validation loss: 104.07289518, Gradient norm: 415.56003347
INFO:root:[  140] Training loss: 102.62445244, Validation loss: 103.87490924, Gradient norm: 358.96730047
INFO:root:EP 140: Early stopping
INFO:root:Training the model took 2502.356s.
INFO:root:Emptying the cuda cache took 0.051s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 145.54378
INFO:root:EnergyScoreTrain: 102.50966
INFO:root:CoverageTrain: 0.74654
INFO:root:IntervalWidthTrain: 8.16636
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.60832
INFO:root:EnergyScoreValidation: 103.95775
INFO:root:CoverageValidation: 0.74281
INFO:root:IntervalWidthValidation: 8.16176
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.72433
INFO:root:EnergyScoreTest: 104.05077
INFO:root:CoverageTest: 0.74171
INFO:root:IntervalWidthTest: 8.14642
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.32653532, Validation loss: 121.47527234, Gradient norm: 100.58461483
INFO:root:[    2] Training loss: 121.58725070, Validation loss: 121.42499148, Gradient norm: 118.64234559
INFO:root:[    3] Training loss: 121.42403830, Validation loss: 121.56793555, Gradient norm: 69.34244906
INFO:root:[    4] Training loss: 121.37062255, Validation loss: 122.02164170, Gradient norm: 78.59666795
INFO:root:[    5] Training loss: 121.43473168, Validation loss: 121.37507524, Gradient norm: 81.05939011
INFO:root:[    6] Training loss: 121.16715139, Validation loss: 121.20857397, Gradient norm: 58.28399399
INFO:root:[    7] Training loss: 120.87851857, Validation loss: 120.73737940, Gradient norm: 57.77742352
INFO:root:[    8] Training loss: 120.74429038, Validation loss: 120.82026436, Gradient norm: 51.52950042
INFO:root:[    9] Training loss: 120.60327088, Validation loss: 120.69868995, Gradient norm: 52.83683863
INFO:root:[   10] Training loss: 120.53405640, Validation loss: 120.53177406, Gradient norm: 42.11590272
INFO:root:[   11] Training loss: 120.39329245, Validation loss: 120.37581608, Gradient norm: 46.85859753
INFO:root:[   12] Training loss: 120.20194035, Validation loss: 120.05401822, Gradient norm: 50.01815880
INFO:root:[   13] Training loss: 119.59736654, Validation loss: 119.20251333, Gradient norm: 34.47974794
INFO:root:[   14] Training loss: 118.51711273, Validation loss: 118.36491605, Gradient norm: 40.63344353
INFO:root:[   15] Training loss: 117.54644012, Validation loss: 117.12103771, Gradient norm: 44.37507537
INFO:root:[   16] Training loss: 116.63026928, Validation loss: 116.26837605, Gradient norm: 40.13333284
INFO:root:[   17] Training loss: 115.85337006, Validation loss: 115.61670132, Gradient norm: 41.25972800
INFO:root:[   18] Training loss: 115.26318785, Validation loss: 115.19048441, Gradient norm: 40.17586679
INFO:root:[   19] Training loss: 114.75419812, Validation loss: 114.48536630, Gradient norm: 41.77942677
INFO:root:[   20] Training loss: 114.27215745, Validation loss: 114.10897511, Gradient norm: 52.78996989
INFO:root:[   21] Training loss: 113.83560322, Validation loss: 113.54756717, Gradient norm: 57.76377546
INFO:root:[   22] Training loss: 113.30457914, Validation loss: 113.17598225, Gradient norm: 52.62292350
INFO:root:[   23] Training loss: 112.92561226, Validation loss: 112.62140919, Gradient norm: 47.39854789
INFO:root:[   24] Training loss: 112.45531416, Validation loss: 112.68265849, Gradient norm: 57.96134697
INFO:root:[   25] Training loss: 112.09129894, Validation loss: 112.44923375, Gradient norm: 65.66071676
INFO:root:[   26] Training loss: 111.65441678, Validation loss: 111.63391140, Gradient norm: 67.48092483
INFO:root:[   27] Training loss: 111.38521103, Validation loss: 111.34044595, Gradient norm: 63.09087777
INFO:root:[   28] Training loss: 111.06086717, Validation loss: 111.03125158, Gradient norm: 69.79656216
INFO:root:[   29] Training loss: 110.80335668, Validation loss: 110.79753613, Gradient norm: 88.24508186
INFO:root:[   30] Training loss: 110.49427397, Validation loss: 110.61787625, Gradient norm: 67.70124046
INFO:root:[   31] Training loss: 110.17368904, Validation loss: 110.29217819, Gradient norm: 70.00177965
INFO:root:[   32] Training loss: 110.12454325, Validation loss: 109.99320168, Gradient norm: 90.30408257
INFO:root:[   33] Training loss: 109.78949893, Validation loss: 109.83558023, Gradient norm: 80.17263180
INFO:root:[   34] Training loss: 109.59414200, Validation loss: 110.22833647, Gradient norm: 80.00703913
INFO:root:[   35] Training loss: 109.44844757, Validation loss: 109.59109576, Gradient norm: 83.55053551
INFO:root:[   36] Training loss: 109.25904049, Validation loss: 109.31389960, Gradient norm: 94.50572992
INFO:root:[   37] Training loss: 108.99350475, Validation loss: 109.36685891, Gradient norm: 80.51107286
INFO:root:[   38] Training loss: 108.89361896, Validation loss: 109.14621892, Gradient norm: 84.55330585
INFO:root:[   39] Training loss: 108.72947436, Validation loss: 108.74701033, Gradient norm: 92.15977478
INFO:root:[   40] Training loss: 108.56429426, Validation loss: 108.75296047, Gradient norm: 106.73460372
INFO:root:[   41] Training loss: 108.38054812, Validation loss: 108.44499417, Gradient norm: 95.38307011
INFO:root:[   42] Training loss: 108.21903202, Validation loss: 108.48848146, Gradient norm: 93.75117724
INFO:root:[   43] Training loss: 108.06309118, Validation loss: 108.27508966, Gradient norm: 97.93728586
INFO:root:[   44] Training loss: 107.98618978, Validation loss: 108.16986531, Gradient norm: 105.60221702
INFO:root:[   45] Training loss: 107.83084194, Validation loss: 108.18866098, Gradient norm: 111.41874999
INFO:root:[   46] Training loss: 107.67711990, Validation loss: 108.16650575, Gradient norm: 117.60064932
INFO:root:[   47] Training loss: 107.59282110, Validation loss: 107.66425376, Gradient norm: 125.26898236
INFO:root:[   48] Training loss: 107.43982372, Validation loss: 107.70500815, Gradient norm: 114.37127486
INFO:root:[   49] Training loss: 107.34546006, Validation loss: 108.87859713, Gradient norm: 130.29382189
INFO:root:[   50] Training loss: 107.34103225, Validation loss: 107.67606696, Gradient norm: 146.68129750
INFO:root:[   51] Training loss: 107.14073782, Validation loss: 107.37771501, Gradient norm: 124.43291447
INFO:root:[   52] Training loss: 106.97018635, Validation loss: 107.25007919, Gradient norm: 140.99868593
INFO:root:[   53] Training loss: 106.93836820, Validation loss: 107.76299759, Gradient norm: 182.36828386
INFO:root:[   54] Training loss: 106.81550099, Validation loss: 107.11022265, Gradient norm: 156.83424957
INFO:root:[   55] Training loss: 106.69578593, Validation loss: 107.59904427, Gradient norm: 156.27764629
INFO:root:[   56] Training loss: 106.63347045, Validation loss: 106.74634578, Gradient norm: 185.63162535
INFO:root:[   57] Training loss: 106.60028549, Validation loss: 106.80888419, Gradient norm: 183.87654198
INFO:root:[   58] Training loss: 106.41205550, Validation loss: 106.63737672, Gradient norm: 171.26241209
INFO:root:[   59] Training loss: 106.38873824, Validation loss: 106.66219593, Gradient norm: 218.74813144
INFO:root:[   60] Training loss: 106.30825691, Validation loss: 106.56308799, Gradient norm: 215.22803046
INFO:root:[   61] Training loss: 106.16178854, Validation loss: 106.44011583, Gradient norm: 186.27958819
INFO:root:[   62] Training loss: 106.16916521, Validation loss: 106.37079857, Gradient norm: 255.98042822
INFO:root:[   63] Training loss: 106.05807191, Validation loss: 106.36718461, Gradient norm: 184.62133560
INFO:root:[   64] Training loss: 105.98661757, Validation loss: 106.44155936, Gradient norm: 227.05123232
INFO:root:[   65] Training loss: 105.89620580, Validation loss: 106.26573418, Gradient norm: 251.90405541
INFO:root:[   66] Training loss: 105.85420544, Validation loss: 106.24925705, Gradient norm: 257.59018916
INFO:root:[   67] Training loss: 105.81767671, Validation loss: 106.02741715, Gradient norm: 276.72210272
INFO:root:[   68] Training loss: 105.75074903, Validation loss: 106.87352121, Gradient norm: 249.89279279
INFO:root:[   69] Training loss: 105.69321361, Validation loss: 105.90331005, Gradient norm: 298.36385703
INFO:root:[   70] Training loss: 105.62173381, Validation loss: 106.16252057, Gradient norm: 260.61547984
INFO:root:[   71] Training loss: 105.63007935, Validation loss: 105.70935558, Gradient norm: 318.16893646
INFO:root:[   72] Training loss: 105.49151692, Validation loss: 106.12996279, Gradient norm: 278.18943123
INFO:root:[   73] Training loss: 105.57781888, Validation loss: 106.00476285, Gradient norm: 335.32043803
INFO:root:[   74] Training loss: 105.44319133, Validation loss: 105.77458112, Gradient norm: 338.13057239
INFO:root:[   75] Training loss: 105.43230188, Validation loss: 105.68771257, Gradient norm: 369.78696174
INFO:root:[   76] Training loss: 105.33595877, Validation loss: 105.82698112, Gradient norm: 316.06937919
INFO:root:[   77] Training loss: 105.34288133, Validation loss: 106.00032175, Gradient norm: 334.33944374
INFO:root:[   78] Training loss: 105.37046713, Validation loss: 105.82055585, Gradient norm: 354.26864218
INFO:root:[   79] Training loss: 105.25764506, Validation loss: 105.53347910, Gradient norm: 365.27235029
INFO:root:[   80] Training loss: 105.15781362, Validation loss: 105.69932451, Gradient norm: 333.51170308
INFO:root:[   81] Training loss: 105.21494219, Validation loss: 105.38535335, Gradient norm: 366.14239608
INFO:root:[   82] Training loss: 105.13789152, Validation loss: 105.34598489, Gradient norm: 359.65822029
INFO:root:[   83] Training loss: 105.12113669, Validation loss: 105.32429373, Gradient norm: 395.01980226
INFO:root:[   84] Training loss: 105.02434958, Validation loss: 105.20633329, Gradient norm: 348.02156711
INFO:root:[   85] Training loss: 105.04602051, Validation loss: 105.59486863, Gradient norm: 389.54695030
INFO:root:[   86] Training loss: 105.07307164, Validation loss: 105.46999359, Gradient norm: 416.43528418
INFO:root:[   87] Training loss: 105.09148616, Validation loss: 105.04552539, Gradient norm: 401.46434777
INFO:root:[   88] Training loss: 105.00964268, Validation loss: 105.70967655, Gradient norm: 412.45021988
INFO:root:[   89] Training loss: 104.99083879, Validation loss: 105.41894163, Gradient norm: 436.94826288
INFO:root:[   90] Training loss: 104.93913418, Validation loss: 105.18410150, Gradient norm: 366.46413794
INFO:root:[   91] Training loss: 105.03300402, Validation loss: 105.31681376, Gradient norm: 440.72193359
INFO:root:[   92] Training loss: 104.97337429, Validation loss: 105.43132019, Gradient norm: 426.60601943
INFO:root:[   93] Training loss: 104.97314750, Validation loss: 105.27499811, Gradient norm: 434.52940423
INFO:root:[   94] Training loss: 104.87116984, Validation loss: 105.07722684, Gradient norm: 444.06075273
INFO:root:[   95] Training loss: 104.89030403, Validation loss: 105.24316853, Gradient norm: 376.08653962
INFO:root:[   96] Training loss: 104.93067696, Validation loss: 105.12926746, Gradient norm: 451.97849375
INFO:root:EP 96: Early stopping
INFO:root:Training the model took 1699.202s.
INFO:root:Emptying the cuda cache took 0.051s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 148.63463
INFO:root:EnergyScoreTrain: 104.73463
INFO:root:CoverageTrain: 0.67543
INFO:root:IntervalWidthTrain: 7.94869
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 149.29175
INFO:root:EnergyScoreValidation: 105.18392
INFO:root:CoverageValidation: 0.67477
INFO:root:IntervalWidthValidation: 7.95775
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 149.56321
INFO:root:EnergyScoreTest: 105.38731
INFO:root:CoverageTest: 0.67316
INFO:root:IntervalWidthTest: 7.93409
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.37553818, Validation loss: 121.31231611, Gradient norm: 36.51106766
INFO:root:[    2] Training loss: 121.25337071, Validation loss: 120.96154733, Gradient norm: 6.18254697
INFO:root:[    3] Training loss: 120.76771680, Validation loss: 120.46081438, Gradient norm: 6.62655066
INFO:root:[    4] Training loss: 120.26110577, Validation loss: 119.45336230, Gradient norm: 6.83149462
INFO:root:[    5] Training loss: 118.11932846, Validation loss: 116.64196277, Gradient norm: 11.93345263
INFO:root:[    6] Training loss: 115.98460692, Validation loss: 114.79296244, Gradient norm: 14.87384857
INFO:root:[    7] Training loss: 114.43448835, Validation loss: 113.57373336, Gradient norm: 16.96256357
INFO:root:[    8] Training loss: 113.35232942, Validation loss: 112.20387400, Gradient norm: 18.03984494
INFO:root:[    9] Training loss: 112.48006095, Validation loss: 111.62030766, Gradient norm: 17.78411830
INFO:root:[   10] Training loss: 111.80408863, Validation loss: 110.92304177, Gradient norm: 20.38705719
INFO:root:[   11] Training loss: 111.27416378, Validation loss: 110.76838395, Gradient norm: 22.52141006
INFO:root:[   12] Training loss: 110.82890374, Validation loss: 110.16657652, Gradient norm: 23.37863555
INFO:root:[   13] Training loss: 110.40648172, Validation loss: 109.72541441, Gradient norm: 27.61529230
INFO:root:[   14] Training loss: 109.93969740, Validation loss: 109.28901067, Gradient norm: 27.88924857
INFO:root:[   15] Training loss: 109.62060554, Validation loss: 109.21746247, Gradient norm: 26.13160642
INFO:root:[   16] Training loss: 109.31310927, Validation loss: 108.63988732, Gradient norm: 32.98102975
INFO:root:[   17] Training loss: 109.01498757, Validation loss: 108.50309832, Gradient norm: 32.95698736
INFO:root:[   18] Training loss: 108.79137245, Validation loss: 108.18965649, Gradient norm: 37.36672207
INFO:root:[   19] Training loss: 108.50101768, Validation loss: 108.00727160, Gradient norm: 32.99624356
INFO:root:[   20] Training loss: 108.33076767, Validation loss: 107.73653070, Gradient norm: 40.26457654
INFO:root:[   21] Training loss: 108.09456000, Validation loss: 107.58855175, Gradient norm: 36.77882023
INFO:root:[   22] Training loss: 107.95268459, Validation loss: 107.34416620, Gradient norm: 42.74955771
INFO:root:[   23] Training loss: 107.74096815, Validation loss: 107.28705702, Gradient norm: 42.15369713
INFO:root:[   24] Training loss: 107.56994197, Validation loss: 107.10718799, Gradient norm: 46.78485232
INFO:root:[   25] Training loss: 107.36654670, Validation loss: 107.01945838, Gradient norm: 45.23116900
INFO:root:[   26] Training loss: 107.28274388, Validation loss: 106.78953052, Gradient norm: 57.17143317
INFO:root:[   27] Training loss: 107.07119170, Validation loss: 106.67232014, Gradient norm: 50.67963611
INFO:root:[   28] Training loss: 106.94009123, Validation loss: 106.49444554, Gradient norm: 58.59359655
INFO:root:[   29] Training loss: 106.80453701, Validation loss: 106.51170928, Gradient norm: 64.69663355
INFO:root:[   30] Training loss: 106.62703104, Validation loss: 106.38312083, Gradient norm: 64.87623759
INFO:root:[   31] Training loss: 106.57310958, Validation loss: 105.95464088, Gradient norm: 66.51436783
INFO:root:[   32] Training loss: 106.39125237, Validation loss: 105.87685763, Gradient norm: 71.36761532
INFO:root:[   33] Training loss: 106.23656956, Validation loss: 106.00286208, Gradient norm: 75.12045239
INFO:root:[   34] Training loss: 106.17379869, Validation loss: 106.11599889, Gradient norm: 78.10316639
INFO:root:[   35] Training loss: 106.06228928, Validation loss: 105.82626895, Gradient norm: 80.98445157
INFO:root:[   36] Training loss: 105.93317373, Validation loss: 105.72683374, Gradient norm: 84.63518677
INFO:root:[   37] Training loss: 105.89218754, Validation loss: 105.59775964, Gradient norm: 92.98102873
INFO:root:[   38] Training loss: 105.71771369, Validation loss: 105.39802551, Gradient norm: 92.71781032
INFO:root:[   39] Training loss: 105.58299255, Validation loss: 105.39175099, Gradient norm: 88.14233976
INFO:root:[   40] Training loss: 105.56260290, Validation loss: 105.46793313, Gradient norm: 100.53083022
INFO:root:[   41] Training loss: 105.45625866, Validation loss: 105.62652956, Gradient norm: 97.47668437
INFO:root:[   42] Training loss: 105.35600159, Validation loss: 105.33718872, Gradient norm: 115.89022302
INFO:root:[   43] Training loss: 105.24771476, Validation loss: 105.25160059, Gradient norm: 91.28479638
INFO:root:[   44] Training loss: 105.21851457, Validation loss: 104.95533752, Gradient norm: 122.20270229
INFO:root:[   45] Training loss: 105.10200217, Validation loss: 104.88057788, Gradient norm: 114.04670774
INFO:root:[   46] Training loss: 104.99915064, Validation loss: 105.22577904, Gradient norm: 119.50429236
INFO:root:[   47] Training loss: 104.97592413, Validation loss: 104.85317967, Gradient norm: 125.62889210
INFO:root:[   48] Training loss: 104.83638122, Validation loss: 105.17819793, Gradient norm: 127.44202985
INFO:root:[   49] Training loss: 104.84498630, Validation loss: 105.03767921, Gradient norm: 145.68678785
INFO:root:[   50] Training loss: 104.68746874, Validation loss: 104.97939800, Gradient norm: 122.97456273
INFO:root:[   51] Training loss: 104.63706065, Validation loss: 104.68262429, Gradient norm: 148.84700692
INFO:root:[   52] Training loss: 104.61654461, Validation loss: 104.62505051, Gradient norm: 153.00971372
INFO:root:[   53] Training loss: 104.49573672, Validation loss: 104.94799936, Gradient norm: 155.47276502
INFO:root:[   54] Training loss: 104.38846554, Validation loss: 104.69544720, Gradient norm: 137.82649330
INFO:root:[   55] Training loss: 104.39314898, Validation loss: 104.47066840, Gradient norm: 165.07769699
INFO:root:[   56] Training loss: 104.31131312, Validation loss: 104.60701094, Gradient norm: 175.26971038
INFO:root:[   57] Training loss: 104.16856641, Validation loss: 104.64013724, Gradient norm: 146.49548583
INFO:root:[   58] Training loss: 104.16001750, Validation loss: 104.51762522, Gradient norm: 163.16632825
INFO:root:[   59] Training loss: 104.13835556, Validation loss: 104.48896395, Gradient norm: 178.97448261
INFO:root:[   60] Training loss: 104.06353557, Validation loss: 104.63177332, Gradient norm: 163.20393771
INFO:root:[   61] Training loss: 103.94314400, Validation loss: 104.31467333, Gradient norm: 144.12669000
INFO:root:[   62] Training loss: 103.90693530, Validation loss: 104.48949827, Gradient norm: 176.91870825
INFO:root:[   63] Training loss: 103.83699758, Validation loss: 104.38368146, Gradient norm: 188.26699630
INFO:root:[   64] Training loss: 103.86003363, Validation loss: 104.46911568, Gradient norm: 170.55018501
INFO:root:[   65] Training loss: 103.71753915, Validation loss: 104.08622216, Gradient norm: 178.12432244
INFO:root:[   66] Training loss: 103.67860163, Validation loss: 104.40584354, Gradient norm: 185.01053726
INFO:root:[   67] Training loss: 103.67001593, Validation loss: 104.28867051, Gradient norm: 175.08179739
INFO:root:[   68] Training loss: 103.59485086, Validation loss: 104.34639214, Gradient norm: 204.02168649
INFO:root:[   69] Training loss: 103.48181665, Validation loss: 104.06041060, Gradient norm: 179.41166228
INFO:root:[   70] Training loss: 103.48414335, Validation loss: 104.14358836, Gradient norm: 208.09223612
INFO:root:[   71] Training loss: 103.39270438, Validation loss: 103.98172628, Gradient norm: 204.15687768
INFO:root:[   72] Training loss: 103.37181051, Validation loss: 104.18074956, Gradient norm: 233.78167021
INFO:root:[   73] Training loss: 103.35304625, Validation loss: 104.27065277, Gradient norm: 229.24275659
INFO:root:[   74] Training loss: 103.19068132, Validation loss: 104.04728251, Gradient norm: 188.23806873
INFO:root:[   75] Training loss: 103.18049716, Validation loss: 104.17483310, Gradient norm: 205.28638589
INFO:root:[   76] Training loss: 103.11688624, Validation loss: 104.34948099, Gradient norm: 206.36109393
INFO:root:[   77] Training loss: 103.10193897, Validation loss: 104.00174476, Gradient norm: 231.90352257
INFO:root:[   78] Training loss: 103.00921253, Validation loss: 103.92923263, Gradient norm: 214.09119044
INFO:root:[   79] Training loss: 102.96977909, Validation loss: 104.32303804, Gradient norm: 241.05377399
INFO:root:[   80] Training loss: 102.94198332, Validation loss: 104.46265911, Gradient norm: 230.73025454
INFO:root:[   81] Training loss: 102.85343602, Validation loss: 103.64478723, Gradient norm: 232.83532844
INFO:root:[   82] Training loss: 102.83413399, Validation loss: 103.98265628, Gradient norm: 236.65992953
INFO:root:[   83] Training loss: 102.84402925, Validation loss: 103.84280948, Gradient norm: 240.48269782
INFO:root:[   84] Training loss: 102.70707190, Validation loss: 104.08772041, Gradient norm: 235.62940962
INFO:root:[   85] Training loss: 102.62917341, Validation loss: 103.82537947, Gradient norm: 192.28048093
INFO:root:[   86] Training loss: 102.71852179, Validation loss: 104.05186094, Gradient norm: 263.31849192
INFO:root:[   87] Training loss: 102.58677788, Validation loss: 104.01677441, Gradient norm: 254.52023488
INFO:root:[   88] Training loss: 102.55622972, Validation loss: 104.11232705, Gradient norm: 234.50340610
INFO:root:[   89] Training loss: 102.56201921, Validation loss: 104.33973483, Gradient norm: 294.24431777
INFO:root:[   90] Training loss: 102.43932451, Validation loss: 103.88426919, Gradient norm: 243.41088250
INFO:root:EP 90: Early stopping
INFO:root:Training the model took 1250.874s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 143.26076
INFO:root:EnergyScoreTrain: 100.83837
INFO:root:CoverageTrain: 0.34338
INFO:root:IntervalWidthTrain: 4.859
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.35377
INFO:root:EnergyScoreValidation: 103.75302
INFO:root:CoverageValidation: 0.34309
INFO:root:IntervalWidthValidation: 4.86482
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.57289
INFO:root:EnergyScoreTest: 103.90343
INFO:root:CoverageTest: 0.34478
INFO:root:IntervalWidthTest: 4.88358
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.99824666, Validation loss: 121.47056290, Gradient norm: 18.43084605
INFO:root:[    2] Training loss: 121.34094063, Validation loss: 121.31117485, Gradient norm: 7.18437130
INFO:root:[    3] Training loss: 121.32403605, Validation loss: 121.29323657, Gradient norm: 6.56947624
INFO:root:[    4] Training loss: 121.30607166, Validation loss: 121.28692890, Gradient norm: 6.60648919
INFO:root:[    5] Training loss: 121.30020675, Validation loss: 121.44686363, Gradient norm: 6.57423032
INFO:root:[    6] Training loss: 121.28595133, Validation loss: 121.29580899, Gradient norm: 6.97111495
INFO:root:[    7] Training loss: 121.22640222, Validation loss: 121.12860634, Gradient norm: 5.44382494
INFO:root:[    8] Training loss: 120.87715358, Validation loss: 120.47393115, Gradient norm: 7.58060197
INFO:root:[    9] Training loss: 119.49042396, Validation loss: 117.70034001, Gradient norm: 10.56753420
INFO:root:[   10] Training loss: 117.05306426, Validation loss: 115.37489056, Gradient norm: 18.89050752
INFO:root:[   11] Training loss: 115.04006458, Validation loss: 113.06340658, Gradient norm: 28.26264357
INFO:root:[   12] Training loss: 113.67095157, Validation loss: 112.29892283, Gradient norm: 29.25580542
INFO:root:[   13] Training loss: 112.81329872, Validation loss: 111.05939273, Gradient norm: 35.78085824
INFO:root:[   14] Training loss: 112.19529677, Validation loss: 110.31217220, Gradient norm: 43.33666390
INFO:root:[   15] Training loss: 111.65667455, Validation loss: 109.90096099, Gradient norm: 43.70760102
INFO:root:[   16] Training loss: 111.25720282, Validation loss: 109.73047796, Gradient norm: 39.91450932
INFO:root:[   17] Training loss: 110.91647899, Validation loss: 109.58916868, Gradient norm: 59.27537647
INFO:root:[   18] Training loss: 110.63473781, Validation loss: 109.12585607, Gradient norm: 61.34033213
INFO:root:[   19] Training loss: 110.31711106, Validation loss: 108.70299635, Gradient norm: 58.46006559
INFO:root:[   20] Training loss: 110.09332512, Validation loss: 109.22553253, Gradient norm: 62.57868300
INFO:root:[   21] Training loss: 109.83962662, Validation loss: 108.47323582, Gradient norm: 65.11051498
INFO:root:[   22] Training loss: 109.70112833, Validation loss: 107.89862429, Gradient norm: 70.91664729
INFO:root:[   23] Training loss: 109.50789615, Validation loss: 107.89709788, Gradient norm: 75.73850810
INFO:root:[   24] Training loss: 109.29363257, Validation loss: 107.65365469, Gradient norm: 70.03103491
INFO:root:[   25] Training loss: 109.11485797, Validation loss: 108.96975234, Gradient norm: 73.26631352
INFO:root:[   26] Training loss: 109.05024341, Validation loss: 107.86585051, Gradient norm: 86.53009249
INFO:root:[   27] Training loss: 108.90609978, Validation loss: 107.34310466, Gradient norm: 85.36700342
INFO:root:[   28] Training loss: 108.70057975, Validation loss: 107.53339386, Gradient norm: 85.21546791
INFO:root:[   29] Training loss: 108.60328485, Validation loss: 108.85484814, Gradient norm: 90.80229469
INFO:root:[   30] Training loss: 108.52515027, Validation loss: 108.07174525, Gradient norm: 105.08070358
INFO:root:[   31] Training loss: 108.40521017, Validation loss: 107.56812207, Gradient norm: 104.83069304
INFO:root:[   32] Training loss: 108.28998215, Validation loss: 110.46927301, Gradient norm: 116.31695792
INFO:root:[   33] Training loss: 108.23124621, Validation loss: 107.58171950, Gradient norm: 117.98490750
INFO:root:[   34] Training loss: 108.17786434, Validation loss: 108.59536191, Gradient norm: 115.83962654
INFO:root:[   35] Training loss: 108.00088866, Validation loss: 109.47742673, Gradient norm: 126.47867405
INFO:root:[   36] Training loss: 108.01135348, Validation loss: 106.93599543, Gradient norm: 149.84667795
INFO:root:[   37] Training loss: 107.87018221, Validation loss: 109.79753218, Gradient norm: 136.46837262
INFO:root:[   38] Training loss: 107.82909920, Validation loss: 107.98147609, Gradient norm: 152.93300587
INFO:root:[   39] Training loss: 107.73398583, Validation loss: 108.55815861, Gradient norm: 147.12264011
INFO:root:[   40] Training loss: 107.70027701, Validation loss: 106.80729728, Gradient norm: 169.83398998
INFO:root:[   41] Training loss: 107.54080180, Validation loss: 108.90602822, Gradient norm: 154.88923585
INFO:root:[   42] Training loss: 107.51224241, Validation loss: 107.86849055, Gradient norm: 168.72393788
INFO:root:[   43] Training loss: 107.52013816, Validation loss: 111.76139332, Gradient norm: 172.38845378
INFO:root:[   44] Training loss: 107.42671305, Validation loss: 110.42235513, Gradient norm: 183.50554792
INFO:root:[   45] Training loss: 107.39249528, Validation loss: 112.17516353, Gradient norm: 176.38842029
INFO:root:[   46] Training loss: 107.26348512, Validation loss: 109.22144370, Gradient norm: 191.57642437
INFO:root:[   47] Training loss: 107.27361743, Validation loss: 111.12574452, Gradient norm: 184.27867358
INFO:root:[   48] Training loss: 107.16843036, Validation loss: 113.37359645, Gradient norm: 188.17562949
INFO:root:[   49] Training loss: 107.20791916, Validation loss: 110.48089284, Gradient norm: 199.28857806
INFO:root:[   50] Training loss: 107.05161528, Validation loss: 113.34666759, Gradient norm: 182.34263818
INFO:root:[   51] Training loss: 107.11014759, Validation loss: 114.92101209, Gradient norm: 202.96204857
INFO:root:[   52] Training loss: 107.10797139, Validation loss: 112.72175467, Gradient norm: 211.09269001
INFO:root:[   53] Training loss: 107.02305373, Validation loss: 108.99780142, Gradient norm: 259.78437367
INFO:root:[   54] Training loss: 106.98386579, Validation loss: 114.35745160, Gradient norm: 203.33978842
INFO:root:[   55] Training loss: 106.88650884, Validation loss: 117.38936405, Gradient norm: 199.31883810
INFO:root:[   56] Training loss: 106.85192925, Validation loss: 117.06529894, Gradient norm: 229.04707954
INFO:root:[   57] Training loss: 106.90551049, Validation loss: 112.75970433, Gradient norm: 266.05369368
INFO:root:[   58] Training loss: 106.87219927, Validation loss: 115.43600306, Gradient norm: 231.11063765
INFO:root:[   59] Training loss: 106.76937940, Validation loss: 120.94143203, Gradient norm: 223.92556489
INFO:root:[   60] Training loss: 106.81895933, Validation loss: 115.01137622, Gradient norm: 247.55921269
INFO:root:[   61] Training loss: 106.75110856, Validation loss: 112.09074507, Gradient norm: 263.62489242
INFO:root:[   62] Training loss: 106.74820432, Validation loss: 115.51571129, Gradient norm: 247.48584642
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 863.096s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 149.23962
INFO:root:EnergyScoreTrain: 106.17074
INFO:root:CoverageTrain: 0.23958
INFO:root:IntervalWidthTrain: 3.34514
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 150.0538
INFO:root:EnergyScoreValidation: 106.82613
INFO:root:CoverageValidation: 0.2392
INFO:root:IntervalWidthValidation: 3.33711
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 150.31389
INFO:root:EnergyScoreTest: 107.01387
INFO:root:CoverageTest: 0.23886
INFO:root:IntervalWidthTest: 3.33769
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.63708408, Validation loss: 121.56134165, Gradient norm: 22.06643111
INFO:root:[    2] Training loss: 121.39609318, Validation loss: 121.81761169, Gradient norm: 7.09986148
INFO:root:[    3] Training loss: 121.35079389, Validation loss: 121.61906196, Gradient norm: 7.36843153
INFO:root:[    4] Training loss: 121.32608377, Validation loss: 121.54323578, Gradient norm: 5.84118450
INFO:root:[    5] Training loss: 121.31058901, Validation loss: 121.31139269, Gradient norm: 4.16471569
INFO:root:[    6] Training loss: 121.29183541, Validation loss: 121.39880739, Gradient norm: 4.92492245
INFO:root:[    7] Training loss: 121.12697898, Validation loss: 121.00004578, Gradient norm: 5.87288697
INFO:root:[    8] Training loss: 120.59820125, Validation loss: 119.84180082, Gradient norm: 7.04672300
INFO:root:[    9] Training loss: 119.44274639, Validation loss: 118.24033592, Gradient norm: 10.23563638
INFO:root:[   10] Training loss: 118.29279705, Validation loss: 117.10643689, Gradient norm: 12.82560700
INFO:root:[   11] Training loss: 117.22176530, Validation loss: 115.47075706, Gradient norm: 19.76353510
INFO:root:[   12] Training loss: 116.13034091, Validation loss: 114.82734128, Gradient norm: 22.41908988
INFO:root:[   13] Training loss: 115.35053199, Validation loss: 113.84242196, Gradient norm: 28.27447288
INFO:root:[   14] Training loss: 114.81878730, Validation loss: 113.87110427, Gradient norm: 32.00654640
INFO:root:[   15] Training loss: 114.31990537, Validation loss: 112.59231120, Gradient norm: 34.63320285
INFO:root:[   16] Training loss: 113.94678470, Validation loss: 112.59550555, Gradient norm: 39.25345676
INFO:root:[   17] Training loss: 113.54758933, Validation loss: 112.52662974, Gradient norm: 42.09738603
INFO:root:[   18] Training loss: 113.26223222, Validation loss: 112.91804320, Gradient norm: 45.33711295
INFO:root:[   19] Training loss: 113.02423670, Validation loss: 112.21150970, Gradient norm: 59.32390354
INFO:root:[   20] Training loss: 112.74713884, Validation loss: 111.84603224, Gradient norm: 58.63742106
INFO:root:[   21] Training loss: 112.53542281, Validation loss: 114.22753827, Gradient norm: 57.79700536
INFO:root:[   22] Training loss: 112.43011643, Validation loss: 111.69568634, Gradient norm: 73.16453172
INFO:root:[   23] Training loss: 112.25275468, Validation loss: 112.26982669, Gradient norm: 78.87567274
INFO:root:[   24] Training loss: 112.07627666, Validation loss: 111.51786304, Gradient norm: 77.20783862
INFO:root:[   25] Training loss: 111.92929219, Validation loss: 116.16241797, Gradient norm: 90.54638017
INFO:root:[   26] Training loss: 111.75187312, Validation loss: 120.22109512, Gradient norm: 82.32707482
INFO:root:[   27] Training loss: 111.65846934, Validation loss: 118.00148563, Gradient norm: 101.93629923
INFO:root:[   28] Training loss: 111.49010535, Validation loss: 115.04273119, Gradient norm: 98.24829805
INFO:root:[   29] Training loss: 111.44057998, Validation loss: 115.14954955, Gradient norm: 120.62169305
INFO:root:[   30] Training loss: 111.25391908, Validation loss: 122.00303624, Gradient norm: 108.88321828
INFO:root:[   31] Training loss: 111.25395520, Validation loss: 118.70501814, Gradient norm: 114.04705253
INFO:root:[   32] Training loss: 111.12672620, Validation loss: 120.53200110, Gradient norm: 114.70712043
INFO:root:[   33] Training loss: 111.00174416, Validation loss: 115.84478102, Gradient norm: 127.56734249
INFO:root:[   34] Training loss: 111.02094897, Validation loss: 118.99190206, Gradient norm: 131.52471706
INFO:root:[   35] Training loss: 110.89635332, Validation loss: 117.21198378, Gradient norm: 135.21204978
INFO:root:[   36] Training loss: 110.76565977, Validation loss: 123.03322943, Gradient norm: 140.47527095
INFO:root:[   37] Training loss: 110.69055264, Validation loss: 123.92005657, Gradient norm: 156.02376018
INFO:root:[   38] Training loss: 110.67186602, Validation loss: 115.86394632, Gradient norm: 157.92469298
INFO:root:[   39] Training loss: 110.63599213, Validation loss: 119.36427649, Gradient norm: 172.84605515
INFO:root:[   40] Training loss: 110.62138414, Validation loss: 122.72270755, Gradient norm: 162.96869878
INFO:root:[   41] Training loss: 110.57047886, Validation loss: 122.24979138, Gradient norm: 182.78417406
INFO:root:[   42] Training loss: 110.45339189, Validation loss: 121.30457885, Gradient norm: 176.99254125
INFO:root:[   43] Training loss: 110.42382785, Validation loss: 126.97928172, Gradient norm: 193.22276609
INFO:root:[   44] Training loss: 110.27502057, Validation loss: 122.44445696, Gradient norm: 185.59805175
INFO:root:[   45] Training loss: 110.24606931, Validation loss: 125.41303543, Gradient norm: 198.45291098
INFO:root:[   46] Training loss: 110.12155921, Validation loss: 118.30187094, Gradient norm: 206.94070869
INFO:root:[   47] Training loss: 110.11522459, Validation loss: 120.69435725, Gradient norm: 219.18616816
INFO:root:[   48] Training loss: 110.01700038, Validation loss: 119.84657709, Gradient norm: 217.04777304
INFO:root:[   49] Training loss: 109.94694580, Validation loss: 120.61755240, Gradient norm: 210.72013385
INFO:root:[   50] Training loss: 109.99806578, Validation loss: 118.07002969, Gradient norm: 248.91126614
INFO:root:[   51] Training loss: 109.91296745, Validation loss: 127.69144519, Gradient norm: 243.88736034
INFO:root:[   52] Training loss: 110.00683817, Validation loss: 115.40752385, Gradient norm: 278.06433879
INFO:root:[   53] Training loss: 109.75803328, Validation loss: 112.39854694, Gradient norm: 248.05071673
INFO:root:[   54] Training loss: 109.83981073, Validation loss: 112.79569718, Gradient norm: 281.85545746
INFO:root:[   55] Training loss: 109.78731692, Validation loss: 112.01601489, Gradient norm: 282.96953764
INFO:root:[   56] Training loss: 109.72946748, Validation loss: 117.40146821, Gradient norm: 297.36872404
INFO:root:[   57] Training loss: 109.66335499, Validation loss: 113.33259004, Gradient norm: 305.53380870
INFO:root:[   58] Training loss: 109.66052023, Validation loss: 115.27760762, Gradient norm: 297.89492912
INFO:root:[   59] Training loss: 109.64364604, Validation loss: 112.46336917, Gradient norm: 335.93107360
INFO:root:[   60] Training loss: 109.59893191, Validation loss: 110.13464803, Gradient norm: 311.28220877
INFO:root:[   61] Training loss: 109.55223327, Validation loss: 110.53060334, Gradient norm: 359.69010626
INFO:root:[   62] Training loss: 109.60315157, Validation loss: 113.97149527, Gradient norm: 368.33464045
INFO:root:[   63] Training loss: 109.58262837, Validation loss: 108.77171273, Gradient norm: 351.35872925
INFO:root:[   64] Training loss: 109.51545013, Validation loss: 109.74045984, Gradient norm: 344.56463151
INFO:root:[   65] Training loss: 109.44369149, Validation loss: 109.42766782, Gradient norm: 353.03361837
INFO:root:[   66] Training loss: 109.44670139, Validation loss: 108.18650818, Gradient norm: 385.91074390
INFO:root:[   67] Training loss: 109.39163957, Validation loss: 107.64011252, Gradient norm: 375.87350090
INFO:root:[   68] Training loss: 109.43750695, Validation loss: 106.68341354, Gradient norm: 365.32421907
INFO:root:[   69] Training loss: 109.31974826, Validation loss: 106.65150715, Gradient norm: 418.19298951
INFO:root:[   70] Training loss: 109.40939615, Validation loss: 106.95719541, Gradient norm: 417.11618347
INFO:root:[   71] Training loss: 109.63762955, Validation loss: 107.30095462, Gradient norm: 422.59808574
INFO:root:[   72] Training loss: 109.86525247, Validation loss: 111.22705131, Gradient norm: 459.32223146
INFO:root:[   73] Training loss: 109.86898669, Validation loss: 118.24178551, Gradient norm: 436.33157945
INFO:root:[   74] Training loss: 110.22554084, Validation loss: 127.87957501, Gradient norm: 525.71466653
INFO:root:[   75] Training loss: 109.92626332, Validation loss: 139.65595166, Gradient norm: 507.50541898
INFO:root:[   76] Training loss: 109.74307481, Validation loss: 146.92553237, Gradient norm: 520.54890191
INFO:root:[   77] Training loss: 109.68922120, Validation loss: 145.33738603, Gradient norm: 511.27526991
INFO:root:[   78] Training loss: 109.85255594, Validation loss: 150.40522240, Gradient norm: 532.43139161
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 1305.553s.
INFO:root:Emptying the cuda cache took 0.079s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 150.39642
INFO:root:EnergyScoreTrain: 106.30764
INFO:root:CoverageTrain: 0.21353
INFO:root:IntervalWidthTrain: 3.51589
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 150.94347
INFO:root:EnergyScoreValidation: 106.72794
INFO:root:CoverageValidation: 0.21326
INFO:root:IntervalWidthValidation: 3.50935
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 151.33099
INFO:root:EnergyScoreTest: 107.00194
INFO:root:CoverageTest: 0.21447
INFO:root:IntervalWidthTest: 3.52968
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.45616940, Validation loss: 121.72160392, Gradient norm: 19.56060151
INFO:root:[    2] Training loss: 121.41559196, Validation loss: 121.80086938, Gradient norm: 6.29473694
INFO:root:[    3] Training loss: 121.33315635, Validation loss: 121.68762944, Gradient norm: 5.34064465
INFO:root:[    4] Training loss: 121.31297586, Validation loss: 121.93388051, Gradient norm: 5.25831422
INFO:root:[    5] Training loss: 121.29959647, Validation loss: 121.70084250, Gradient norm: 6.42577648
INFO:root:[    6] Training loss: 121.27750788, Validation loss: 121.70426467, Gradient norm: 5.14668627
INFO:root:[    7] Training loss: 121.30074877, Validation loss: 121.41477861, Gradient norm: 4.74620001
INFO:root:[    8] Training loss: 121.31352720, Validation loss: 121.43292342, Gradient norm: 4.30206353
INFO:root:[    9] Training loss: 121.27401119, Validation loss: 121.49365655, Gradient norm: 3.95790509
INFO:root:[   10] Training loss: 121.30835245, Validation loss: 121.35642111, Gradient norm: 3.61362917
INFO:root:[   11] Training loss: 121.29571952, Validation loss: 121.34228253, Gradient norm: 4.84528626
INFO:root:[   12] Training loss: 121.27496189, Validation loss: 121.26756392, Gradient norm: 4.25147855
INFO:root:[   13] Training loss: 121.26922290, Validation loss: 121.37857477, Gradient norm: 4.16574631
INFO:root:[   14] Training loss: 121.28729322, Validation loss: 121.23881399, Gradient norm: 3.39121337
INFO:root:[   15] Training loss: 121.28861054, Validation loss: 121.29099432, Gradient norm: 3.28182841
INFO:root:[   16] Training loss: 121.26207220, Validation loss: 121.36455062, Gradient norm: 2.91202866
INFO:root:[   17] Training loss: 121.28784821, Validation loss: 121.20758872, Gradient norm: 2.52904603
INFO:root:[   18] Training loss: 121.26189301, Validation loss: 121.24890189, Gradient norm: 3.02834377
INFO:root:[   19] Training loss: 121.28100701, Validation loss: 121.31437736, Gradient norm: 2.15310768
INFO:root:[   20] Training loss: 121.25701783, Validation loss: 121.37514548, Gradient norm: 2.43380739
INFO:root:[   21] Training loss: 121.27445646, Validation loss: 121.39299353, Gradient norm: 2.20159311
INFO:root:[   22] Training loss: 121.25517192, Validation loss: 121.47664064, Gradient norm: 2.27136875
INFO:root:[   23] Training loss: 121.19380802, Validation loss: 120.94230257, Gradient norm: 2.05666482
INFO:root:[   24] Training loss: 120.69979170, Validation loss: 120.16620294, Gradient norm: 6.38369366
INFO:root:[   25] Training loss: 119.69260373, Validation loss: 118.22991786, Gradient norm: 8.09753506
INFO:root:[   26] Training loss: 118.27334628, Validation loss: 116.57372705, Gradient norm: 12.96007703
INFO:root:[   27] Training loss: 117.06976480, Validation loss: 115.33702692, Gradient norm: 20.04731778
INFO:root:[   28] Training loss: 116.15375505, Validation loss: 114.86083248, Gradient norm: 24.76240936
INFO:root:[   29] Training loss: 115.48372637, Validation loss: 114.29794706, Gradient norm: 28.99210750
INFO:root:[   30] Training loss: 114.97359581, Validation loss: 113.72977263, Gradient norm: 34.07099219
INFO:root:[   31] Training loss: 114.67700931, Validation loss: 115.29184723, Gradient norm: 41.29029158
INFO:root:[   32] Training loss: 114.38069605, Validation loss: 116.92711429, Gradient norm: 41.95233136
INFO:root:[   33] Training loss: 114.11052528, Validation loss: 115.42702695, Gradient norm: 49.25072051
INFO:root:[   34] Training loss: 113.84691444, Validation loss: 118.23640626, Gradient norm: 52.70987068
INFO:root:[   35] Training loss: 113.71733613, Validation loss: 120.21171728, Gradient norm: 58.27021630
INFO:root:[   36] Training loss: 113.56454414, Validation loss: 129.79971261, Gradient norm: 68.61700046
INFO:root:[   37] Training loss: 113.41502522, Validation loss: 141.31652464, Gradient norm: 70.95497503
INFO:root:[   38] Training loss: 113.34505989, Validation loss: 137.66099180, Gradient norm: 74.61304973
INFO:root:[   39] Training loss: 113.20986122, Validation loss: 130.98187887, Gradient norm: 80.15951550
INFO:root:[   40] Training loss: 113.13079456, Validation loss: 139.66305332, Gradient norm: 80.18000990
INFO:root:[   41] Training loss: 112.88002743, Validation loss: 145.77279137, Gradient norm: 85.41220603
INFO:root:[   42] Training loss: 112.87152397, Validation loss: 143.74452209, Gradient norm: 96.41858825
INFO:root:[   43] Training loss: 112.83678612, Validation loss: 136.26773229, Gradient norm: 91.55383827
INFO:root:[   44] Training loss: 112.71109961, Validation loss: 133.88415738, Gradient norm: 103.10833484
INFO:root:[   45] Training loss: 112.66237971, Validation loss: 141.47397008, Gradient norm: 112.21656187
INFO:root:[   46] Training loss: 112.58105698, Validation loss: 131.72831884, Gradient norm: 108.27490107
INFO:root:[   47] Training loss: 112.50075862, Validation loss: 138.26717456, Gradient norm: 113.37353745
INFO:root:[   48] Training loss: 112.42933945, Validation loss: 131.70094510, Gradient norm: 113.86317149
INFO:root:[   49] Training loss: 112.40614332, Validation loss: 120.66459761, Gradient norm: 120.71856702
INFO:root:[   50] Training loss: 112.38676264, Validation loss: 139.01974172, Gradient norm: 126.54428927
INFO:root:[   51] Training loss: 112.27187638, Validation loss: 131.59144434, Gradient norm: 120.22775821
INFO:root:[   52] Training loss: 112.40466984, Validation loss: 131.12718306, Gradient norm: 158.98016657
INFO:root:[   53] Training loss: 112.22064864, Validation loss: 134.67510986, Gradient norm: 151.01382290
INFO:root:[   54] Training loss: 112.04433542, Validation loss: 136.39467437, Gradient norm: 127.80925446
INFO:root:[   55] Training loss: 112.05222361, Validation loss: 131.94757606, Gradient norm: 154.99544230
INFO:root:[   56] Training loss: 112.00287824, Validation loss: 130.55405558, Gradient norm: 152.59296230
INFO:root:[   57] Training loss: 111.85284566, Validation loss: 126.88477089, Gradient norm: 153.27494180
INFO:root:[   58] Training loss: 111.98749178, Validation loss: 127.51678230, Gradient norm: 176.14822534
INFO:root:[   59] Training loss: 111.89947780, Validation loss: 130.08257794, Gradient norm: 198.82012351
INFO:root:[   60] Training loss: 111.82331720, Validation loss: 132.80551411, Gradient norm: 167.62059240
INFO:root:[   61] Training loss: 111.80840869, Validation loss: 123.94028894, Gradient norm: 185.24496245
INFO:root:[   62] Training loss: 111.72986508, Validation loss: 128.82392647, Gradient norm: 199.95422014
INFO:root:[   63] Training loss: 111.75059611, Validation loss: 128.04417840, Gradient norm: 196.34847235
INFO:root:[   64] Training loss: 111.76670318, Validation loss: 124.05474222, Gradient norm: 201.78629583
INFO:root:[   65] Training loss: 111.70538107, Validation loss: 126.45187957, Gradient norm: 226.28879522
INFO:root:[   66] Training loss: 111.59420756, Validation loss: 121.96035451, Gradient norm: 221.90742867
INFO:root:[   67] Training loss: 111.64463077, Validation loss: 116.43573919, Gradient norm: 217.28477921
INFO:root:[   68] Training loss: 111.45480522, Validation loss: 116.33643157, Gradient norm: 216.22538353
INFO:root:[   69] Training loss: 111.55937370, Validation loss: 120.17414882, Gradient norm: 255.00695686
INFO:root:[   70] Training loss: 111.51825154, Validation loss: 126.04341573, Gradient norm: 229.03919627
INFO:root:[   71] Training loss: 111.46357828, Validation loss: 111.22491166, Gradient norm: 246.06553318
INFO:root:[   72] Training loss: 111.42622430, Validation loss: 114.06885187, Gradient norm: 259.07134033
INFO:root:[   73] Training loss: 111.38475030, Validation loss: 110.98048164, Gradient norm: 268.41795438
INFO:root:[   74] Training loss: 111.45550429, Validation loss: 111.23046954, Gradient norm: 303.47285842
INFO:root:[   75] Training loss: 111.31296303, Validation loss: 111.87169752, Gradient norm: 267.94052322
INFO:root:[   76] Training loss: 111.25348326, Validation loss: 114.47872504, Gradient norm: 271.63161826
INFO:root:[   77] Training loss: 111.32250281, Validation loss: 113.54669584, Gradient norm: 274.93849747
INFO:root:[   78] Training loss: 111.34266804, Validation loss: 118.10360165, Gradient norm: 301.41203052
INFO:root:[   79] Training loss: 111.27313793, Validation loss: 109.33826473, Gradient norm: 330.56319503
INFO:root:[   80] Training loss: 111.24619259, Validation loss: 110.29512839, Gradient norm: 364.94050741
INFO:root:[   81] Training loss: 111.23075610, Validation loss: 110.43297445, Gradient norm: 335.93247476
INFO:root:[   82] Training loss: 111.23589737, Validation loss: 115.96669006, Gradient norm: 361.32862614
INFO:root:[   83] Training loss: 111.19748080, Validation loss: 121.01428459, Gradient norm: 335.79861196
INFO:root:[   84] Training loss: 111.46310378, Validation loss: 113.84053013, Gradient norm: 348.25025697
INFO:root:[   85] Training loss: 111.28055093, Validation loss: 116.06359942, Gradient norm: 384.88711509
INFO:root:[   86] Training loss: 111.26138258, Validation loss: 110.22978894, Gradient norm: 405.25677259
INFO:root:[   87] Training loss: 111.15367302, Validation loss: 109.10098556, Gradient norm: 476.35484802
INFO:root:[   88] Training loss: 111.14603100, Validation loss: 108.68993614, Gradient norm: 446.49576275
INFO:root:[   89] Training loss: 111.00363814, Validation loss: 108.60175955, Gradient norm: 440.76804877
INFO:root:[   90] Training loss: 110.96403267, Validation loss: 108.60894276, Gradient norm: 506.01020450
INFO:root:[   91] Training loss: 110.92365825, Validation loss: 108.27371716, Gradient norm: 462.10427666
INFO:root:[   92] Training loss: 110.88472822, Validation loss: 108.28184746, Gradient norm: 439.53180201
INFO:root:[   93] Training loss: 110.82269787, Validation loss: 107.92915239, Gradient norm: 429.92461102
INFO:root:[   94] Training loss: 110.97485898, Validation loss: 109.43248302, Gradient norm: 535.67605421
INFO:root:[   95] Training loss: 110.92381597, Validation loss: 108.24538579, Gradient norm: 511.22029477
INFO:root:[   96] Training loss: 110.87757124, Validation loss: 108.54640250, Gradient norm: 536.74635426
INFO:root:[   97] Training loss: 110.87613313, Validation loss: 108.28795019, Gradient norm: 527.52951032
INFO:root:[   98] Training loss: 110.75012396, Validation loss: 108.02060226, Gradient norm: 520.03024542
INFO:root:[   99] Training loss: 110.84692133, Validation loss: 108.28801543, Gradient norm: 572.99617862
INFO:root:[  100] Training loss: 111.20764329, Validation loss: 152.48622394, Gradient norm: 461.81223142
INFO:root:[  101] Training loss: 111.07487123, Validation loss: 152.37259122, Gradient norm: 439.61562894
INFO:root:[  102] Training loss: 111.24043699, Validation loss: 152.58008181, Gradient norm: 462.39812883
INFO:root:EP 102: Early stopping
INFO:root:Training the model took 106927.191s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 152.54931
INFO:root:EnergyScoreTrain: 107.49943
INFO:root:CoverageTrain: 0.34593
INFO:root:IntervalWidthTrain: 4.90172
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 153.10958
INFO:root:EnergyScoreValidation: 107.9101
INFO:root:CoverageValidation: 0.34614
INFO:root:IntervalWidthValidation: 4.90326
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 153.34206
INFO:root:EnergyScoreTest: 108.07403
INFO:root:CoverageTest: 0.34733
INFO:root:IntervalWidthTest: 4.91641
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.00883828, Validation loss: 122.02317784, Gradient norm: 18.99032779
INFO:root:[    2] Training loss: 121.39635785, Validation loss: 121.61141257, Gradient norm: 5.94769080
INFO:root:[    3] Training loss: 121.35955770, Validation loss: 121.96131213, Gradient norm: 6.41864810
INFO:root:[    4] Training loss: 121.31977979, Validation loss: 122.04739038, Gradient norm: 6.13194623
INFO:root:[    5] Training loss: 121.31528284, Validation loss: 121.51144409, Gradient norm: 4.54351007
INFO:root:[    6] Training loss: 121.32067149, Validation loss: 121.56250631, Gradient norm: 5.55549047
INFO:root:[    7] Training loss: 121.28652124, Validation loss: 121.67213782, Gradient norm: 4.20459366
INFO:root:[    8] Training loss: 121.30753016, Validation loss: 121.68913927, Gradient norm: 4.63259853
INFO:root:[    9] Training loss: 121.30592056, Validation loss: 121.35659685, Gradient norm: 3.45034552
INFO:root:[   10] Training loss: 121.29468307, Validation loss: 121.48453995, Gradient norm: 4.14564586
INFO:root:[   11] Training loss: 121.30156444, Validation loss: 121.34196446, Gradient norm: 4.05265197
INFO:root:[   12] Training loss: 121.29351239, Validation loss: 121.49929283, Gradient norm: 2.82297182
INFO:root:[   13] Training loss: 121.29327210, Validation loss: 121.28837927, Gradient norm: 3.45843843
INFO:root:[   14] Training loss: 121.26141270, Validation loss: 121.23406956, Gradient norm: 2.75124263
INFO:root:[   15] Training loss: 121.28672676, Validation loss: 121.27406179, Gradient norm: 2.63492773
INFO:root:[   16] Training loss: 121.27585980, Validation loss: 121.23711974, Gradient norm: 2.96525533
INFO:root:[   17] Training loss: 121.29542501, Validation loss: 121.26458845, Gradient norm: 2.63287090
INFO:root:[   18] Training loss: 121.26609282, Validation loss: 121.32214408, Gradient norm: 2.02521193
INFO:root:[   19] Training loss: 121.26795217, Validation loss: 121.39737544, Gradient norm: 2.19372546
INFO:root:[   20] Training loss: 121.27010102, Validation loss: 121.34983615, Gradient norm: 2.11370507
INFO:root:[   21] Training loss: 121.25674985, Validation loss: 121.45079803, Gradient norm: 1.62798875
INFO:root:[   22] Training loss: 121.26971618, Validation loss: 121.28395107, Gradient norm: 1.53566485
INFO:root:[   23] Training loss: 121.26919785, Validation loss: 121.62236865, Gradient norm: 1.56132678
INFO:root:[   24] Training loss: 121.28529338, Validation loss: 121.37811385, Gradient norm: 1.83390300
INFO:root:[   25] Training loss: 121.26553054, Validation loss: 121.44099216, Gradient norm: 1.53962287
INFO:root:[   26] Training loss: 121.27491801, Validation loss: 121.63155049, Gradient norm: 1.39969492
INFO:root:[   27] Training loss: 121.27647785, Validation loss: 121.48024487, Gradient norm: 1.37731675
INFO:root:[   28] Training loss: 121.27171440, Validation loss: 121.45969864, Gradient norm: 1.37255566
INFO:root:[   29] Training loss: 121.28586491, Validation loss: 121.53886414, Gradient norm: 1.46470856
INFO:root:[   30] Training loss: 121.26848859, Validation loss: 121.48252737, Gradient norm: 1.17422693
INFO:root:[   31] Training loss: 121.29188038, Validation loss: 121.52482815, Gradient norm: 1.13290587
INFO:root:[   32] Training loss: 121.25961297, Validation loss: 121.43305917, Gradient norm: 0.98132374
INFO:root:[   33] Training loss: 121.26722035, Validation loss: 121.40803423, Gradient norm: 0.86710781
INFO:root:[   34] Training loss: 121.27429982, Validation loss: 121.41968221, Gradient norm: 0.83533198
INFO:root:[   35] Training loss: 121.27509598, Validation loss: 121.53766869, Gradient norm: 0.89882412
INFO:root:[   36] Training loss: 121.28199167, Validation loss: 121.46258755, Gradient norm: 0.89088190
INFO:root:[   37] Training loss: 121.27082946, Validation loss: 121.46294982, Gradient norm: 0.73975703
INFO:root:[   38] Training loss: 121.25049497, Validation loss: 121.42643738, Gradient norm: 0.70604218
INFO:root:[   39] Training loss: 121.26367235, Validation loss: 121.39482485, Gradient norm: 0.61444381
INFO:root:[   40] Training loss: 121.27227378, Validation loss: 121.48725286, Gradient norm: 0.57922555
INFO:root:[   41] Training loss: 121.27490653, Validation loss: 121.44745057, Gradient norm: 0.47207079
INFO:root:[   42] Training loss: 121.25353720, Validation loss: 121.41133828, Gradient norm: 0.59464181
INFO:root:[   43] Training loss: 121.26020354, Validation loss: 121.45971548, Gradient norm: 0.56653037
INFO:root:[   44] Training loss: 121.27026766, Validation loss: 121.37947319, Gradient norm: 0.55988911
INFO:root:[   45] Training loss: 121.28666592, Validation loss: 121.45924588, Gradient norm: 0.42742938
INFO:root:[   46] Training loss: 121.26144139, Validation loss: 121.32186206, Gradient norm: 0.58256213
INFO:root:[   47] Training loss: 121.26286397, Validation loss: 121.38947191, Gradient norm: 0.49000217
INFO:root:[   48] Training loss: 121.28658423, Validation loss: 121.39224901, Gradient norm: 0.53531346
INFO:root:[   49] Training loss: 121.25685997, Validation loss: 121.32426400, Gradient norm: 0.43608182
INFO:root:[   50] Training loss: 121.26889794, Validation loss: 121.40805659, Gradient norm: 0.48660735
INFO:root:[   51] Training loss: 121.26311432, Validation loss: 121.38389587, Gradient norm: 0.42765935
INFO:root:[   52] Training loss: 121.29009456, Validation loss: 121.40103965, Gradient norm: 0.43224867
INFO:root:[   53] Training loss: 121.27151023, Validation loss: 121.45783655, Gradient norm: 0.32825136
INFO:root:[   54] Training loss: 121.24806166, Validation loss: 121.42985929, Gradient norm: 0.53168330
INFO:root:[   55] Training loss: 121.23850642, Validation loss: 121.55398375, Gradient norm: 0.46427550
INFO:root:[   56] Training loss: 121.25847275, Validation loss: 121.61886623, Gradient norm: 0.52052522
INFO:root:[   57] Training loss: 121.26259350, Validation loss: 121.48939277, Gradient norm: 0.44155783
INFO:root:[   58] Training loss: 121.26595050, Validation loss: 121.41293887, Gradient norm: 0.44050424
INFO:root:[   59] Training loss: 121.26403140, Validation loss: 121.47422159, Gradient norm: 0.39269565
INFO:root:[   60] Training loss: 121.26897052, Validation loss: 121.37098220, Gradient norm: 0.48210876
INFO:root:[   61] Training loss: 121.26804365, Validation loss: 121.44995170, Gradient norm: 0.48700336
INFO:root:[   62] Training loss: 121.28438642, Validation loss: 121.49327298, Gradient norm: 0.39927159
INFO:root:[   63] Training loss: 121.27779645, Validation loss: 121.49240323, Gradient norm: 0.41817759
INFO:root:[   64] Training loss: 121.27787139, Validation loss: 121.53529858, Gradient norm: 0.43123583
INFO:root:[   65] Training loss: 121.26749258, Validation loss: 121.44448063, Gradient norm: 0.45154769
INFO:root:[   66] Training loss: 121.28104239, Validation loss: 121.47357151, Gradient norm: 0.35599686
INFO:root:[   67] Training loss: 121.26300906, Validation loss: 121.41895399, Gradient norm: 0.33443547
INFO:root:[   68] Training loss: 121.26010442, Validation loss: 121.51789409, Gradient norm: 0.44473663
INFO:root:[   69] Training loss: 121.25626684, Validation loss: 121.57842623, Gradient norm: 0.40378538
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 983.549s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 172.29554
INFO:root:EnergyScoreTrain: 121.26843
INFO:root:CoverageTrain: 0.91364
INFO:root:IntervalWidthTrain: 9.90007
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 172.3161
INFO:root:EnergyScoreValidation: 121.28366
INFO:root:CoverageValidation: 0.91363
INFO:root:IntervalWidthValidation: 9.898
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 172.35432
INFO:root:EnergyScoreTest: 121.30921
INFO:root:CoverageTest: 0.91359
INFO:root:IntervalWidthTest: 9.89944
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.42045890, Validation loss: 123.72505267, Gradient norm: 21.47703846
INFO:root:[    2] Training loss: 121.50941076, Validation loss: 123.64554622, Gradient norm: 6.63524608
INFO:root:[    3] Training loss: 121.41058188, Validation loss: 122.46332892, Gradient norm: 5.98039287
INFO:root:[    4] Training loss: 121.37746720, Validation loss: 123.65527633, Gradient norm: 7.36958299
INFO:root:[    5] Training loss: 121.34435380, Validation loss: 123.25851861, Gradient norm: 7.57673437
INFO:root:[    6] Training loss: 121.33032760, Validation loss: 123.24660334, Gradient norm: 7.01806116
INFO:root:[    7] Training loss: 121.32168046, Validation loss: 123.02812432, Gradient norm: 10.86651432
INFO:root:[    8] Training loss: 121.31449262, Validation loss: 123.39772954, Gradient norm: 11.47599734
INFO:root:[    9] Training loss: 121.33441722, Validation loss: 123.65689902, Gradient norm: 11.75661759
INFO:root:[   10] Training loss: 121.31394256, Validation loss: 122.90977162, Gradient norm: 12.44017179
INFO:root:[   11] Training loss: 121.32525824, Validation loss: 122.48260445, Gradient norm: 10.59833845
INFO:root:[   12] Training loss: 121.34850230, Validation loss: 122.76933920, Gradient norm: 10.43411632
INFO:root:[   13] Training loss: 121.29645032, Validation loss: 123.40448472, Gradient norm: 11.79743507
INFO:root:[   14] Training loss: 121.30230537, Validation loss: 122.79303136, Gradient norm: 11.43483263
INFO:root:[   15] Training loss: 121.29438532, Validation loss: 122.77436934, Gradient norm: 12.28209426
INFO:root:[   16] Training loss: 121.29128333, Validation loss: 123.13197485, Gradient norm: 9.47269804
INFO:root:[   17] Training loss: 121.28503567, Validation loss: 122.60952706, Gradient norm: 10.40336223
INFO:root:[   18] Training loss: 121.29397772, Validation loss: 122.79742642, Gradient norm: 13.77095651
INFO:root:[   19] Training loss: 121.29170720, Validation loss: 123.21689711, Gradient norm: 13.63417767
INFO:root:[   20] Training loss: 121.28665053, Validation loss: 122.28683866, Gradient norm: 11.82834001
INFO:root:[   21] Training loss: 121.28354334, Validation loss: 122.90995631, Gradient norm: 10.19204744
INFO:root:[   22] Training loss: 121.29461872, Validation loss: 123.04160467, Gradient norm: 12.48907956
INFO:root:[   23] Training loss: 121.30538799, Validation loss: 122.58448555, Gradient norm: 13.55099082
INFO:root:[   24] Training loss: 121.28432910, Validation loss: 122.73454048, Gradient norm: 11.91350764
INFO:root:[   25] Training loss: 121.30589267, Validation loss: 122.61940844, Gradient norm: 12.29193973
INFO:root:[   26] Training loss: 121.29367909, Validation loss: 122.49333112, Gradient norm: 12.12098975
INFO:root:[   27] Training loss: 121.28449756, Validation loss: 122.72672772, Gradient norm: 11.30300228
INFO:root:[   28] Training loss: 121.32191805, Validation loss: 122.91575017, Gradient norm: 11.85701034
INFO:root:[   29] Training loss: 121.29484491, Validation loss: 123.37863817, Gradient norm: 11.57838775
INFO:root:[   30] Training loss: 121.29372217, Validation loss: 122.70345333, Gradient norm: 15.51001743
INFO:root:[   31] Training loss: 121.29340619, Validation loss: 122.44951314, Gradient norm: 12.11342582
INFO:root:[   32] Training loss: 121.28816993, Validation loss: 122.09346824, Gradient norm: 10.73850352
INFO:root:[   33] Training loss: 121.29468280, Validation loss: 122.36705833, Gradient norm: 9.89841801
INFO:root:[   34] Training loss: 121.29541191, Validation loss: 122.94438277, Gradient norm: 9.44149605
INFO:root:[   35] Training loss: 121.32734950, Validation loss: 122.11282480, Gradient norm: 11.45566581
INFO:root:[   36] Training loss: 121.29868850, Validation loss: 122.10772837, Gradient norm: 9.85892343
INFO:root:[   37] Training loss: 121.29063416, Validation loss: 122.02959810, Gradient norm: 12.85475248
INFO:root:[   38] Training loss: 121.29364648, Validation loss: 121.85733848, Gradient norm: 8.90242701
INFO:root:[   39] Training loss: 121.27893897, Validation loss: 122.48018804, Gradient norm: 9.66846689
INFO:root:[   40] Training loss: 121.27968132, Validation loss: 121.96471694, Gradient norm: 9.06779035
INFO:root:[   41] Training loss: 121.27394388, Validation loss: 122.48176733, Gradient norm: 8.92898410
INFO:root:[   42] Training loss: 121.29423793, Validation loss: 122.64411084, Gradient norm: 12.17700935
INFO:root:[   43] Training loss: 121.28980633, Validation loss: 122.59459581, Gradient norm: 8.43048226
INFO:root:[   44] Training loss: 121.29077979, Validation loss: 122.51662813, Gradient norm: 10.03277786
INFO:root:[   45] Training loss: 121.26901468, Validation loss: 122.70119134, Gradient norm: 8.72942622
INFO:root:[   46] Training loss: 121.31974414, Validation loss: 122.44958312, Gradient norm: 10.37541120
INFO:root:[   47] Training loss: 121.27265721, Validation loss: 122.15841648, Gradient norm: 7.71100661
INFO:root:[   48] Training loss: 121.30028156, Validation loss: 121.70932875, Gradient norm: 13.02918322
INFO:root:[   49] Training loss: 121.28380335, Validation loss: 122.29936823, Gradient norm: 10.96039528
INFO:root:[   50] Training loss: 121.28969912, Validation loss: 122.69798779, Gradient norm: 12.35566687
INFO:root:[   51] Training loss: 121.30702419, Validation loss: 122.82201596, Gradient norm: 15.16794916
INFO:root:[   52] Training loss: 121.31183759, Validation loss: 122.39432341, Gradient norm: 15.00936866
INFO:root:[   53] Training loss: 121.28715252, Validation loss: 122.35046518, Gradient norm: 10.30778049
INFO:root:[   54] Training loss: 121.27373106, Validation loss: 122.50564602, Gradient norm: 11.19843133
INFO:root:[   55] Training loss: 121.29453419, Validation loss: 121.92762598, Gradient norm: 10.96537911
INFO:root:[   56] Training loss: 121.30696383, Validation loss: 122.13376591, Gradient norm: 12.81834231
INFO:root:[   57] Training loss: 121.30301605, Validation loss: 122.07191046, Gradient norm: 10.90454246
INFO:root:[   58] Training loss: 121.29528228, Validation loss: 122.03344148, Gradient norm: 10.33426603
INFO:root:[   59] Training loss: 121.28364820, Validation loss: 122.14364308, Gradient norm: 11.41792905
INFO:root:[   60] Training loss: 121.26451854, Validation loss: 122.04477744, Gradient norm: 10.09888386
INFO:root:[   61] Training loss: 121.28631315, Validation loss: 121.78360564, Gradient norm: 10.87922274
INFO:root:[   62] Training loss: 121.28765180, Validation loss: 122.00899663, Gradient norm: 9.37153084
INFO:root:[   63] Training loss: 121.26673025, Validation loss: 121.96121005, Gradient norm: 8.66200231
INFO:root:[   64] Training loss: 121.27545456, Validation loss: 121.96059418, Gradient norm: 10.49058426
INFO:root:[   65] Training loss: 121.29053700, Validation loss: 122.17343745, Gradient norm: 9.04251468
INFO:root:[   66] Training loss: 121.28289120, Validation loss: 121.95788022, Gradient norm: 9.60270624
INFO:root:[   67] Training loss: 121.28221542, Validation loss: 121.79180803, Gradient norm: 11.61559560
INFO:root:[   68] Training loss: 121.29482411, Validation loss: 121.90322481, Gradient norm: 11.55574042
INFO:root:[   69] Training loss: 121.28122839, Validation loss: 121.90085497, Gradient norm: 10.81585982
INFO:root:[   70] Training loss: 121.28841393, Validation loss: 122.14810128, Gradient norm: 12.02899660
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 974.181s.
INFO:root:Emptying the cuda cache took 0.023s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 172.10675
INFO:root:EnergyScoreTrain: 121.79991
INFO:root:CoverageTrain: 0.84847
INFO:root:IntervalWidthTrain: 8.63058
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 172.13673
INFO:root:EnergyScoreValidation: 121.8257
INFO:root:CoverageValidation: 0.84822
INFO:root:IntervalWidthValidation: 8.62785
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 172.1682
INFO:root:EnergyScoreTest: 121.84525
INFO:root:CoverageTest: 0.84824
INFO:root:IntervalWidthTest: 8.63078
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.28404614, Validation loss: 170.46749036, Gradient norm: 4.19292470
INFO:root:[    2] Training loss: 169.88420740, Validation loss: 167.96522154, Gradient norm: 6.07117121
INFO:root:[    3] Training loss: 166.19800642, Validation loss: 163.91336375, Gradient norm: 17.05341985
INFO:root:[    4] Training loss: 163.03185806, Validation loss: 161.14118694, Gradient norm: 24.30115586
INFO:root:[    5] Training loss: 160.65962192, Validation loss: 158.97074890, Gradient norm: 27.77220532
INFO:root:[    6] Training loss: 159.01592491, Validation loss: 157.66790771, Gradient norm: 31.66750596
INFO:root:[    7] Training loss: 157.80197751, Validation loss: 156.48023829, Gradient norm: 34.43325486
INFO:root:[    8] Training loss: 156.82227791, Validation loss: 155.67948598, Gradient norm: 36.83663987
INFO:root:[    9] Training loss: 156.01510620, Validation loss: 154.74154032, Gradient norm: 39.41130381
INFO:root:[   10] Training loss: 155.30384435, Validation loss: 154.16764358, Gradient norm: 44.60917383
INFO:root:[   11] Training loss: 154.69436348, Validation loss: 153.84679176, Gradient norm: 45.29665086
INFO:root:[   12] Training loss: 154.19113915, Validation loss: 153.18960571, Gradient norm: 57.95236024
INFO:root:[   13] Training loss: 153.73613678, Validation loss: 153.06907285, Gradient norm: 57.93226087
INFO:root:[   14] Training loss: 153.30235047, Validation loss: 152.41806820, Gradient norm: 61.47336452
INFO:root:[   15] Training loss: 152.89133568, Validation loss: 151.88190697, Gradient norm: 62.73016123
INFO:root:[   16] Training loss: 152.60109852, Validation loss: 151.53916510, Gradient norm: 82.24761025
INFO:root:[   17] Training loss: 152.26032966, Validation loss: 151.78368930, Gradient norm: 95.57957944
INFO:root:[   18] Training loss: 151.97574609, Validation loss: 150.87071281, Gradient norm: 98.62202166
INFO:root:[   19] Training loss: 151.65992224, Validation loss: 150.69255013, Gradient norm: 91.45120207
INFO:root:[   20] Training loss: 151.40718430, Validation loss: 150.85423384, Gradient norm: 101.94939788
INFO:root:[   21] Training loss: 151.14712200, Validation loss: 150.53216658, Gradient norm: 120.15548979
INFO:root:[   22] Training loss: 150.93023520, Validation loss: 150.19658529, Gradient norm: 134.93241983
INFO:root:[   23] Training loss: 150.67528851, Validation loss: 149.87144944, Gradient norm: 131.83853936
INFO:root:[   24] Training loss: 150.46398953, Validation loss: 149.54368170, Gradient norm: 138.15388698
INFO:root:[   25] Training loss: 150.30224015, Validation loss: 149.68020262, Gradient norm: 146.78947475
INFO:root:[   26] Training loss: 150.03998167, Validation loss: 149.42511302, Gradient norm: 151.32391071
INFO:root:[   27] Training loss: 149.87557335, Validation loss: 149.06318507, Gradient norm: 174.38437547
INFO:root:[   28] Training loss: 149.63064224, Validation loss: 150.07171947, Gradient norm: 173.33081593
INFO:root:[   29] Training loss: 149.53789568, Validation loss: 149.02714433, Gradient norm: 211.56643236
INFO:root:[   30] Training loss: 149.28932352, Validation loss: 148.72770691, Gradient norm: 203.76261117
INFO:root:[   31] Training loss: 149.15595279, Validation loss: 148.49624792, Gradient norm: 206.76281220
INFO:root:[   32] Training loss: 149.02006747, Validation loss: 149.06520817, Gradient norm: 217.28001497
INFO:root:[   33] Training loss: 148.97228909, Validation loss: 148.27371216, Gradient norm: 258.83542856
INFO:root:[   34] Training loss: 148.74136447, Validation loss: 149.31495246, Gradient norm: 240.19765551
INFO:root:[   35] Training loss: 148.59632563, Validation loss: 148.32452866, Gradient norm: 261.15694655
INFO:root:[   36] Training loss: 148.49025220, Validation loss: 147.93944102, Gradient norm: 269.02172870
INFO:root:[   37] Training loss: 148.36843332, Validation loss: 147.80126427, Gradient norm: 264.33280701
INFO:root:[   38] Training loss: 148.18693515, Validation loss: 147.62445595, Gradient norm: 298.87326594
INFO:root:[   39] Training loss: 148.00918620, Validation loss: 147.98519003, Gradient norm: 265.41817316
INFO:root:[   40] Training loss: 147.93648158, Validation loss: 147.57028409, Gradient norm: 294.83072916
INFO:root:[   41] Training loss: 147.84643892, Validation loss: 147.39510793, Gradient norm: 281.58683984
INFO:root:[   42] Training loss: 147.79717572, Validation loss: 147.71524942, Gradient norm: 360.91992536
INFO:root:[   43] Training loss: 147.55817663, Validation loss: 147.71513051, Gradient norm: 289.45933485
INFO:root:[   44] Training loss: 147.46637003, Validation loss: 147.47167127, Gradient norm: 313.45664408
INFO:root:[   45] Training loss: 147.40892596, Validation loss: 147.13185277, Gradient norm: 330.90175925
INFO:root:[   46] Training loss: 147.38505392, Validation loss: 147.57655492, Gradient norm: 339.72057377
INFO:root:[   47] Training loss: 147.16269569, Validation loss: 146.84314438, Gradient norm: 360.21063025
INFO:root:[   48] Training loss: 147.07168174, Validation loss: 147.36902803, Gradient norm: 372.29246275
INFO:root:[   49] Training loss: 147.00644799, Validation loss: 146.85598123, Gradient norm: 367.06957319
INFO:root:[   50] Training loss: 146.85518410, Validation loss: 146.99073265, Gradient norm: 352.97269730
INFO:root:[   51] Training loss: 146.87960559, Validation loss: 147.06855563, Gradient norm: 386.56839731
INFO:root:[   52] Training loss: 146.70122062, Validation loss: 147.20478926, Gradient norm: 356.71935895
INFO:root:[   53] Training loss: 146.69684094, Validation loss: 148.03552246, Gradient norm: 404.32291282
INFO:root:[   54] Training loss: 146.50364415, Validation loss: 147.71341048, Gradient norm: 352.05012089
INFO:root:[   55] Training loss: 146.47625570, Validation loss: 146.66576675, Gradient norm: 348.19511456
INFO:root:[   56] Training loss: 146.36825886, Validation loss: 146.72660038, Gradient norm: 388.13110345
INFO:root:[   57] Training loss: 146.32762605, Validation loss: 146.75686856, Gradient norm: 419.19632245
INFO:root:[   58] Training loss: 146.23026147, Validation loss: 146.49923233, Gradient norm: 370.77792392
INFO:root:[   59] Training loss: 146.06250513, Validation loss: 146.54342073, Gradient norm: 389.03031386
INFO:root:[   60] Training loss: 146.06856247, Validation loss: 146.19782546, Gradient norm: 387.39552640
INFO:root:[   61] Training loss: 146.03118856, Validation loss: 146.60060277, Gradient norm: 457.14380025
INFO:root:[   62] Training loss: 145.76613354, Validation loss: 148.69879992, Gradient norm: 371.18580232
INFO:root:[   63] Training loss: 145.81000120, Validation loss: 146.41420772, Gradient norm: 441.61280810
INFO:root:[   64] Training loss: 145.71043720, Validation loss: 147.93875543, Gradient norm: 411.36367451
INFO:root:[   65] Training loss: 145.67197384, Validation loss: 146.07230246, Gradient norm: 455.19680008
INFO:root:[   66] Training loss: 145.60068316, Validation loss: 146.66889270, Gradient norm: 430.29438842
INFO:root:[   67] Training loss: 145.48806263, Validation loss: 145.93360112, Gradient norm: 421.45243845
INFO:root:[   68] Training loss: 145.42035007, Validation loss: 146.01391233, Gradient norm: 453.92043394
INFO:root:[   69] Training loss: 145.32204039, Validation loss: 145.94421650, Gradient norm: 400.97884859
INFO:root:[   70] Training loss: 145.18452845, Validation loss: 146.28055283, Gradient norm: 433.37686123
INFO:root:[   71] Training loss: 145.21080571, Validation loss: 146.54852768, Gradient norm: 467.32843313
INFO:root:[   72] Training loss: 145.11747404, Validation loss: 146.59677019, Gradient norm: 453.92672408
INFO:root:[   73] Training loss: 145.10718597, Validation loss: 145.94138941, Gradient norm: 439.62647896
INFO:root:[   74] Training loss: 144.99182034, Validation loss: 145.88182962, Gradient norm: 466.82463267
INFO:root:[   75] Training loss: 144.94734354, Validation loss: 145.93057514, Gradient norm: 484.09768699
INFO:root:[   76] Training loss: 144.79095445, Validation loss: 145.87906042, Gradient norm: 387.08296431
INFO:root:[   77] Training loss: 144.82271286, Validation loss: 146.17830632, Gradient norm: 461.66108574
INFO:root:[   78] Training loss: 144.70699331, Validation loss: 145.81718077, Gradient norm: 452.57531392
INFO:root:[   79] Training loss: 144.61653988, Validation loss: 146.23378780, Gradient norm: 469.18394969
INFO:root:[   80] Training loss: 144.58455462, Validation loss: 145.95389320, Gradient norm: 475.49535842
INFO:root:[   81] Training loss: 144.55877551, Validation loss: 145.90192387, Gradient norm: 442.18164094
INFO:root:[   82] Training loss: 144.35064927, Validation loss: 147.07240085, Gradient norm: 425.68201249
INFO:root:[   83] Training loss: 144.57397258, Validation loss: 145.66238561, Gradient norm: 504.44678578
INFO:root:[   84] Training loss: 144.35997576, Validation loss: 146.26646423, Gradient norm: 453.49513511
INFO:root:[   85] Training loss: 144.21798355, Validation loss: 146.01859152, Gradient norm: 444.93352565
INFO:root:[   86] Training loss: 144.22555731, Validation loss: 145.79966262, Gradient norm: 476.99517613
INFO:root:[   87] Training loss: 144.13850889, Validation loss: 145.85783912, Gradient norm: 437.67354816
INFO:root:[   88] Training loss: 144.10493037, Validation loss: 146.18671444, Gradient norm: 472.73080792
INFO:root:[   89] Training loss: 144.09942708, Validation loss: 145.93869597, Gradient norm: 489.89079934
INFO:root:[   90] Training loss: 143.90202872, Validation loss: 146.18474237, Gradient norm: 424.42588761
INFO:root:[   91] Training loss: 143.88988677, Validation loss: 146.58884088, Gradient norm: 468.93147025
INFO:root:[   92] Training loss: 143.81287863, Validation loss: 145.51858363, Gradient norm: 432.10582014
INFO:root:[   93] Training loss: 143.83632721, Validation loss: 145.39426448, Gradient norm: 498.52842431
INFO:root:[   94] Training loss: 143.73890200, Validation loss: 145.64751724, Gradient norm: 482.05273069
INFO:root:[   95] Training loss: 143.67805940, Validation loss: 145.69355827, Gradient norm: 457.60185758
INFO:root:[   96] Training loss: 143.63618996, Validation loss: 145.37930035, Gradient norm: 489.89424567
INFO:root:[   97] Training loss: 143.59029233, Validation loss: 146.28435701, Gradient norm: 461.71830010
INFO:root:[   98] Training loss: 143.53282476, Validation loss: 145.54865817, Gradient norm: 472.64130211
INFO:root:[   99] Training loss: 143.51930642, Validation loss: 145.55111957, Gradient norm: 481.24734371
INFO:root:[  100] Training loss: 143.36523302, Validation loss: 146.08209649, Gradient norm: 442.91330035
INFO:root:[  101] Training loss: 143.41555151, Validation loss: 146.23543417, Gradient norm: 496.21841379
INFO:root:[  102] Training loss: 143.33972411, Validation loss: 145.54853295, Gradient norm: 505.46329711
INFO:root:[  103] Training loss: 143.25069839, Validation loss: 145.64454546, Gradient norm: 451.77255053
INFO:root:[  104] Training loss: 143.23302791, Validation loss: 145.63566221, Gradient norm: 463.84272324
INFO:root:[  105] Training loss: 143.24541723, Validation loss: 145.75116703, Gradient norm: 497.73211362
INFO:root:EP 105: Early stopping
INFO:root:Training the model took 1390.282s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 140.91608
INFO:root:EnergyScoreTrain: 140.15822
INFO:root:CoverageTrain: 0.00882
INFO:root:IntervalWidthTrain: 0.02673
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 145.36927
INFO:root:EnergyScoreValidation: 144.64997
INFO:root:CoverageValidation: 0.00792
INFO:root:IntervalWidthValidation: 0.02535
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 145.39495
INFO:root:EnergyScoreTest: 144.74802
INFO:root:CoverageTest: 0.00706
INFO:root:IntervalWidthTest: 0.0226
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.58114935, Validation loss: 171.51804220, Gradient norm: 3.20311491
INFO:root:[    2] Training loss: 170.59047490, Validation loss: 169.46397716, Gradient norm: 4.34261232
INFO:root:[    3] Training loss: 167.79526378, Validation loss: 164.82915313, Gradient norm: 9.56496085
INFO:root:[    4] Training loss: 164.07230857, Validation loss: 161.36480923, Gradient norm: 14.47439598
INFO:root:[    5] Training loss: 161.49321902, Validation loss: 158.84593569, Gradient norm: 22.46386713
INFO:root:[    6] Training loss: 159.94940429, Validation loss: 157.43807615, Gradient norm: 21.27617902
INFO:root:[    7] Training loss: 158.80028500, Validation loss: 156.42950124, Gradient norm: 24.28308693
INFO:root:[    8] Training loss: 157.96764637, Validation loss: 155.63299140, Gradient norm: 29.78845897
INFO:root:[    9] Training loss: 157.23898261, Validation loss: 155.02380055, Gradient norm: 31.07586813
INFO:root:[   10] Training loss: 156.70532362, Validation loss: 154.40817787, Gradient norm: 35.65292573
INFO:root:[   11] Training loss: 156.24926434, Validation loss: 153.97000280, Gradient norm: 44.85766404
INFO:root:[   12] Training loss: 155.78989998, Validation loss: 153.36798990, Gradient norm: 51.43524258
INFO:root:[   13] Training loss: 155.45176805, Validation loss: 153.13884919, Gradient norm: 58.98520582
INFO:root:[   14] Training loss: 155.08319389, Validation loss: 152.77025578, Gradient norm: 55.49378780
INFO:root:[   15] Training loss: 154.76572614, Validation loss: 152.55652592, Gradient norm: 66.86812250
INFO:root:[   16] Training loss: 154.44936351, Validation loss: 152.27910482, Gradient norm: 72.25954037
INFO:root:[   17] Training loss: 154.23040285, Validation loss: 151.85790647, Gradient norm: 91.65410572
INFO:root:[   18] Training loss: 154.00136748, Validation loss: 151.83301834, Gradient norm: 104.19923159
INFO:root:[   19] Training loss: 153.71256479, Validation loss: 151.33557760, Gradient norm: 102.29929717
INFO:root:[   20] Training loss: 153.56743426, Validation loss: 151.27018527, Gradient norm: 104.40309997
INFO:root:[   21] Training loss: 153.40877621, Validation loss: 150.95373114, Gradient norm: 123.50824651
INFO:root:[   22] Training loss: 153.10721163, Validation loss: 150.72255417, Gradient norm: 115.23020186
INFO:root:[   23] Training loss: 152.93500431, Validation loss: 150.59713377, Gradient norm: 129.89885726
INFO:root:[   24] Training loss: 152.84404586, Validation loss: 150.38799681, Gradient norm: 158.20093825
INFO:root:[   25] Training loss: 152.66962764, Validation loss: 150.04935692, Gradient norm: 164.74440572
INFO:root:[   26] Training loss: 152.48118443, Validation loss: 150.04351123, Gradient norm: 159.69614975
INFO:root:[   27] Training loss: 152.37534028, Validation loss: 149.87049076, Gradient norm: 181.19342710
INFO:root:[   28] Training loss: 152.22134926, Validation loss: 150.04234051, Gradient norm: 194.39285366
INFO:root:[   29] Training loss: 152.11298512, Validation loss: 149.65326033, Gradient norm: 196.44569317
INFO:root:[   30] Training loss: 151.98378706, Validation loss: 149.57061873, Gradient norm: 198.40260576
INFO:root:[   31] Training loss: 151.82845016, Validation loss: 149.31689558, Gradient norm: 223.56127799
INFO:root:[   32] Training loss: 151.73497374, Validation loss: 149.48685271, Gradient norm: 243.20776207
INFO:root:[   33] Training loss: 151.59681243, Validation loss: 149.27829821, Gradient norm: 240.43568906
INFO:root:[   34] Training loss: 151.49716538, Validation loss: 149.45474559, Gradient norm: 249.78023109
INFO:root:[   35] Training loss: 151.46900643, Validation loss: 149.39779400, Gradient norm: 280.25644957
INFO:root:[   36] Training loss: 151.30869354, Validation loss: 149.08212070, Gradient norm: 261.26572150
INFO:root:[   37] Training loss: 151.32099780, Validation loss: 148.73336213, Gradient norm: 278.28775972
INFO:root:[   38] Training loss: 151.12877257, Validation loss: 148.68568578, Gradient norm: 286.45962325
INFO:root:[   39] Training loss: 151.17532497, Validation loss: 148.86456983, Gradient norm: 293.40017655
INFO:root:[   40] Training loss: 150.95832177, Validation loss: 148.50866068, Gradient norm: 290.91554445
INFO:root:[   41] Training loss: 150.93750122, Validation loss: 148.61516229, Gradient norm: 326.89531214
INFO:root:[   42] Training loss: 150.76594530, Validation loss: 148.20888072, Gradient norm: 324.79572929
INFO:root:[   43] Training loss: 150.80415898, Validation loss: 148.53343779, Gradient norm: 342.67847858
INFO:root:[   44] Training loss: 150.69361742, Validation loss: 148.40451629, Gradient norm: 316.45426477
INFO:root:[   45] Training loss: 150.67506814, Validation loss: 148.48301855, Gradient norm: 314.03349286
INFO:root:[   46] Training loss: 150.51883772, Validation loss: 148.18073404, Gradient norm: 332.43434999
INFO:root:[   47] Training loss: 150.46815045, Validation loss: 148.11391738, Gradient norm: 351.17284017
INFO:root:[   48] Training loss: 150.46634114, Validation loss: 148.06925596, Gradient norm: 390.71733588
INFO:root:[   49] Training loss: 150.31561144, Validation loss: 148.59761521, Gradient norm: 342.05479179
INFO:root:[   50] Training loss: 150.30096949, Validation loss: 147.72336289, Gradient norm: 367.85402537
INFO:root:[   51] Training loss: 150.23278755, Validation loss: 148.30666009, Gradient norm: 352.02503674
INFO:root:[   52] Training loss: 150.19563996, Validation loss: 148.03152729, Gradient norm: 410.36594687
INFO:root:[   53] Training loss: 150.09015514, Validation loss: 147.86556481, Gradient norm: 403.95378657
INFO:root:[   54] Training loss: 149.99362399, Validation loss: 147.62908620, Gradient norm: 386.44115915
INFO:root:[   55] Training loss: 150.01211170, Validation loss: 147.89368255, Gradient norm: 413.88070913
INFO:root:[   56] Training loss: 149.89197858, Validation loss: 147.51487101, Gradient norm: 404.32108189
INFO:root:[   57] Training loss: 149.83165856, Validation loss: 147.37735038, Gradient norm: 426.24056199
INFO:root:[   58] Training loss: 149.81238279, Validation loss: 147.28154676, Gradient norm: 418.61873628
INFO:root:[   59] Training loss: 149.84209800, Validation loss: 148.35032549, Gradient norm: 442.05151190
INFO:root:[   60] Training loss: 149.76079215, Validation loss: 147.28862052, Gradient norm: 427.86475070
INFO:root:[   61] Training loss: 149.68400925, Validation loss: 147.24257371, Gradient norm: 443.38305720
INFO:root:[   62] Training loss: 149.62224599, Validation loss: 147.16392570, Gradient norm: 451.45003019
INFO:root:[   63] Training loss: 149.64692904, Validation loss: 147.23363784, Gradient norm: 459.55621406
INFO:root:[   64] Training loss: 149.58151961, Validation loss: 147.56272309, Gradient norm: 500.56817638
INFO:root:[   65] Training loss: 149.50936944, Validation loss: 147.25660337, Gradient norm: 463.72157213
INFO:root:[   66] Training loss: 149.40456451, Validation loss: 146.93577260, Gradient norm: 443.99648313
INFO:root:[   67] Training loss: 149.51657739, Validation loss: 147.47671982, Gradient norm: 503.30889275
INFO:root:[   68] Training loss: 149.34110361, Validation loss: 147.13264728, Gradient norm: 458.75707696
INFO:root:[   69] Training loss: 149.28947017, Validation loss: 147.00918947, Gradient norm: 506.64022258
INFO:root:[   70] Training loss: 149.25774512, Validation loss: 147.16454499, Gradient norm: 482.50777017
INFO:root:[   71] Training loss: 149.34980774, Validation loss: 146.96357096, Gradient norm: 497.14934283
INFO:root:[   72] Training loss: 149.23157711, Validation loss: 146.67757863, Gradient norm: 467.76950774
INFO:root:[   73] Training loss: 149.15129670, Validation loss: 149.39624918, Gradient norm: 493.77356643
INFO:root:[   74] Training loss: 149.20547458, Validation loss: 147.23616501, Gradient norm: 490.97729683
INFO:root:[   75] Training loss: 149.03751691, Validation loss: 146.51400441, Gradient norm: 509.90511004
INFO:root:[   76] Training loss: 149.10337195, Validation loss: 146.76907086, Gradient norm: 528.62280392
INFO:root:[   77] Training loss: 149.02291060, Validation loss: 146.60660369, Gradient norm: 502.74769165
INFO:root:[   78] Training loss: 149.00550424, Validation loss: 146.55392246, Gradient norm: 467.59309667
INFO:root:[   79] Training loss: 148.99198360, Validation loss: 146.54606418, Gradient norm: 525.07441837
INFO:root:[   80] Training loss: 148.99524074, Validation loss: 146.35969280, Gradient norm: 523.67077076
INFO:root:[   81] Training loss: 148.86662509, Validation loss: 146.38760955, Gradient norm: 528.72058227
INFO:root:[   82] Training loss: 148.87503281, Validation loss: 146.61162909, Gradient norm: 526.58411569
INFO:root:[   83] Training loss: 148.81318813, Validation loss: 146.63323080, Gradient norm: 553.72486962
INFO:root:[   84] Training loss: 148.68300770, Validation loss: 146.17475260, Gradient norm: 529.37593350
INFO:root:[   85] Training loss: 148.75423330, Validation loss: 146.42046067, Gradient norm: 541.32195196
INFO:root:[   86] Training loss: 148.65885210, Validation loss: 146.52302130, Gradient norm: 517.51684248
INFO:root:[   87] Training loss: 148.80854676, Validation loss: 146.23174207, Gradient norm: 580.44843060
INFO:root:[   88] Training loss: 148.61775883, Validation loss: 146.00280709, Gradient norm: 543.22364265
INFO:root:[   89] Training loss: 148.72210086, Validation loss: 146.39545677, Gradient norm: 628.51109434
INFO:root:[   90] Training loss: 148.61354929, Validation loss: 146.28663898, Gradient norm: 516.96833445
INFO:root:[   91] Training loss: 148.62767164, Validation loss: 147.36563479, Gradient norm: 587.58477451
INFO:root:[   92] Training loss: 148.49640973, Validation loss: 146.11680129, Gradient norm: 564.96994195
INFO:root:[   93] Training loss: 148.49318567, Validation loss: 146.04729435, Gradient norm: 592.29410207
INFO:root:[   94] Training loss: 148.55976206, Validation loss: 146.24387859, Gradient norm: 597.04933792
INFO:root:[   95] Training loss: 148.42908012, Validation loss: 146.16114807, Gradient norm: 569.01356013
INFO:root:[   96] Training loss: 148.52250239, Validation loss: 145.96923407, Gradient norm: 611.26683766
INFO:root:[   97] Training loss: 148.25243600, Validation loss: 146.50961462, Gradient norm: 562.10559107
INFO:root:[   98] Training loss: 148.41837088, Validation loss: 146.14315165, Gradient norm: 589.79195684
INFO:root:[   99] Training loss: 148.34918902, Validation loss: 146.13669349, Gradient norm: 567.39463873
INFO:root:[  100] Training loss: 148.29827314, Validation loss: 146.40108674, Gradient norm: 585.84858683
INFO:root:[  101] Training loss: 148.26890577, Validation loss: 145.81562069, Gradient norm: 608.71349225
INFO:root:[  102] Training loss: 148.32983074, Validation loss: 145.85745081, Gradient norm: 605.51023446
INFO:root:[  103] Training loss: 148.25923805, Validation loss: 146.02121340, Gradient norm: 639.53645318
INFO:root:[  104] Training loss: 148.16200891, Validation loss: 145.99101152, Gradient norm: 625.17983923
INFO:root:[  105] Training loss: 148.09814116, Validation loss: 146.23523581, Gradient norm: 610.14431424
INFO:root:[  106] Training loss: 148.11940691, Validation loss: 145.70816514, Gradient norm: 641.64343280
INFO:root:[  107] Training loss: 148.26589034, Validation loss: 146.71902098, Gradient norm: 620.80045063
INFO:root:[  108] Training loss: 148.27025706, Validation loss: 145.67620376, Gradient norm: 621.90656522
INFO:root:[  109] Training loss: 148.23157792, Validation loss: 145.49528714, Gradient norm: 654.28607011
INFO:root:[  110] Training loss: 148.05862022, Validation loss: 146.13534967, Gradient norm: 635.16952195
INFO:root:[  111] Training loss: 148.22041105, Validation loss: 145.78490264, Gradient norm: 655.82075365
INFO:root:[  112] Training loss: 148.03448135, Validation loss: 147.87382613, Gradient norm: 602.60934384
INFO:root:[  113] Training loss: 148.10511253, Validation loss: 145.96338732, Gradient norm: 638.10814986
INFO:root:[  114] Training loss: 148.00026629, Validation loss: 145.52677865, Gradient norm: 654.54678895
INFO:root:[  115] Training loss: 148.07351185, Validation loss: 146.62624017, Gradient norm: 647.68679227
INFO:root:[  116] Training loss: 148.04216476, Validation loss: 146.53044339, Gradient norm: 663.48560493
INFO:root:[  117] Training loss: 148.02795532, Validation loss: 146.13551751, Gradient norm: 658.37625387
INFO:root:[  118] Training loss: 147.96404759, Validation loss: 146.10008345, Gradient norm: 663.54784815
INFO:root:EP 118: Early stopping
INFO:root:Training the model took 1563.463s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 142.96128
INFO:root:EnergyScoreTrain: 142.1719
INFO:root:CoverageTrain: 0.00871
INFO:root:IntervalWidthTrain: 0.02766
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 145.49037
INFO:root:EnergyScoreValidation: 144.78354
INFO:root:CoverageValidation: 0.00745
INFO:root:IntervalWidthValidation: 0.02465
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 145.78703
INFO:root:EnergyScoreTest: 145.0813
INFO:root:CoverageTest: 0.00744
INFO:root:IntervalWidthTest: 0.02464
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.78292199, Validation loss: 171.40297725, Gradient norm: 6.17064640
INFO:root:[    2] Training loss: 170.96742086, Validation loss: 170.07629605, Gradient norm: 3.12195894
INFO:root:[    3] Training loss: 168.96906854, Validation loss: 166.12512312, Gradient norm: 6.51784714
INFO:root:[    4] Training loss: 165.38983937, Validation loss: 162.19872679, Gradient norm: 10.45997195
INFO:root:[    5] Training loss: 163.04099537, Validation loss: 160.27756737, Gradient norm: 13.62631582
INFO:root:[    6] Training loss: 161.69299343, Validation loss: 159.01237014, Gradient norm: 17.89424113
INFO:root:[    7] Training loss: 160.76616135, Validation loss: 157.85199396, Gradient norm: 27.18283537
INFO:root:[    8] Training loss: 159.96882926, Validation loss: 157.15442210, Gradient norm: 39.09418613
INFO:root:[    9] Training loss: 159.36791358, Validation loss: 156.88779844, Gradient norm: 56.69254627
INFO:root:[   10] Training loss: 158.87160701, Validation loss: 156.03166725, Gradient norm: 74.05847452
INFO:root:[   11] Training loss: 158.43921506, Validation loss: 155.45456722, Gradient norm: 101.05492325
INFO:root:[   12] Training loss: 158.03016852, Validation loss: 154.94604229, Gradient norm: 121.92455446
INFO:root:[   13] Training loss: 157.71057385, Validation loss: 154.46711363, Gradient norm: 162.60022446
INFO:root:[   14] Training loss: 157.47538379, Validation loss: 154.27752107, Gradient norm: 187.87191357
INFO:root:[   15] Training loss: 157.19233744, Validation loss: 154.34545530, Gradient norm: 222.96383973
INFO:root:[   16] Training loss: 157.08991572, Validation loss: 154.20255674, Gradient norm: 282.22748817
INFO:root:[   17] Training loss: 156.73693105, Validation loss: 153.68557529, Gradient norm: 252.70705601
INFO:root:[   18] Training loss: 156.67923014, Validation loss: 153.65318404, Gradient norm: 294.63831057
INFO:root:[   19] Training loss: 156.55391632, Validation loss: 153.65678353, Gradient norm: 331.57784210
INFO:root:[   20] Training loss: 156.30555698, Validation loss: 154.05917464, Gradient norm: 362.76627403
INFO:root:[   21] Training loss: 156.27492314, Validation loss: 152.53532673, Gradient norm: 362.44771283
INFO:root:[   22] Training loss: 156.23144126, Validation loss: 152.66952409, Gradient norm: 419.30978886
INFO:root:[   23] Training loss: 156.12649361, Validation loss: 152.84842919, Gradient norm: 465.57766920
INFO:root:[   24] Training loss: 155.90150492, Validation loss: 152.25533268, Gradient norm: 506.24797845
INFO:root:[   25] Training loss: 156.00205886, Validation loss: 152.05790658, Gradient norm: 498.84310722
INFO:root:[   26] Training loss: 155.74503981, Validation loss: 152.10354562, Gradient norm: 548.17572286
INFO:root:[   27] Training loss: 155.83776234, Validation loss: 151.71327788, Gradient norm: 567.93628393
INFO:root:[   28] Training loss: 155.70239312, Validation loss: 152.49217645, Gradient norm: 628.14591136
INFO:root:[   29] Training loss: 155.61677970, Validation loss: 151.54677713, Gradient norm: 691.05963008
INFO:root:[   30] Training loss: 155.71313531, Validation loss: 151.74784798, Gradient norm: 688.62220987
INFO:root:[   31] Training loss: 155.88283276, Validation loss: 151.35321834, Gradient norm: 631.73322596
INFO:root:[   32] Training loss: 155.37621800, Validation loss: 151.53535988, Gradient norm: 664.01166033
INFO:root:[   33] Training loss: 155.50033137, Validation loss: 158.33026281, Gradient norm: 736.85337747
INFO:root:[   34] Training loss: 155.51040055, Validation loss: 151.54374011, Gradient norm: 680.93148479
INFO:root:[   35] Training loss: 155.59239210, Validation loss: 151.36762685, Gradient norm: 814.36171886
INFO:root:[   36] Training loss: 155.38703378, Validation loss: 151.50189788, Gradient norm: 744.07581218
INFO:root:[   37] Training loss: 155.53541065, Validation loss: 153.38791157, Gradient norm: 801.69354488
INFO:root:[   38] Training loss: 155.43339876, Validation loss: 155.09487284, Gradient norm: 752.85162188
INFO:root:[   39] Training loss: 155.31009789, Validation loss: 151.81244423, Gradient norm: 760.80119595
INFO:root:[   40] Training loss: 155.02689301, Validation loss: 151.43154749, Gradient norm: 820.91064230
INFO:root:[   41] Training loss: 155.50674614, Validation loss: 151.01330882, Gradient norm: 875.16637268
INFO:root:[   42] Training loss: 155.50675289, Validation loss: 150.62446805, Gradient norm: 767.93631088
INFO:root:[   43] Training loss: 155.63515884, Validation loss: 150.79105930, Gradient norm: 852.76295342
INFO:root:[   44] Training loss: 155.51172604, Validation loss: 151.03661110, Gradient norm: 788.66492309
INFO:root:[   45] Training loss: 155.32818577, Validation loss: 150.92450635, Gradient norm: 897.79148953
INFO:root:[   46] Training loss: 155.32071071, Validation loss: 150.69714934, Gradient norm: 880.94446001
INFO:root:[   47] Training loss: 155.46775359, Validation loss: 151.36038103, Gradient norm: 822.26356573
INFO:root:[   48] Training loss: 155.38110379, Validation loss: 153.73929675, Gradient norm: 936.23992744
INFO:root:[   49] Training loss: 155.27950415, Validation loss: 153.44112265, Gradient norm: 925.14119586
INFO:root:[   50] Training loss: 155.38673077, Validation loss: 153.37733986, Gradient norm: 919.93417242
INFO:root:[   51] Training loss: 155.68527073, Validation loss: 155.13517761, Gradient norm: 829.23688323
INFO:root:[   52] Training loss: 155.03922332, Validation loss: 159.72690135, Gradient norm: 903.74645035
INFO:root:[   53] Training loss: 156.02955871, Validation loss: 153.23617238, Gradient norm: 905.23975147
INFO:root:[   54] Training loss: 155.26942930, Validation loss: 150.66851017, Gradient norm: 851.60232459
INFO:root:[   55] Training loss: 155.42064349, Validation loss: 150.95229997, Gradient norm: 915.61378866
INFO:root:[   56] Training loss: 156.12109389, Validation loss: 151.89204986, Gradient norm: 905.24757490
INFO:root:[   57] Training loss: 155.26772464, Validation loss: 154.57457865, Gradient norm: 973.15941056
INFO:root:[   58] Training loss: 155.37879107, Validation loss: 151.36455931, Gradient norm: 1078.54770643
INFO:root:[   59] Training loss: 156.00797657, Validation loss: 153.32729471, Gradient norm: 1073.79322732
INFO:root:[   60] Training loss: 155.77070145, Validation loss: 151.19289793, Gradient norm: 982.91667407
INFO:root:[   61] Training loss: 155.63520246, Validation loss: 152.73660331, Gradient norm: 1017.75601806
INFO:root:[   62] Training loss: 155.44128512, Validation loss: 151.46207244, Gradient norm: 1133.51420447
INFO:root:[   63] Training loss: 155.51461292, Validation loss: 151.90724287, Gradient norm: 1202.33085721
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 862.47s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 150.08588
INFO:root:EnergyScoreTrain: 149.27464
INFO:root:CoverageTrain: 0.00693
INFO:root:IntervalWidthTrain: 0.02667
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 150.6947
INFO:root:EnergyScoreValidation: 150.05709
INFO:root:CoverageValidation: 0.00534
INFO:root:IntervalWidthValidation: 0.02079
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 150.99827
INFO:root:EnergyScoreTest: 150.35657
INFO:root:CoverageTest: 0.00536
INFO:root:IntervalWidthTest: 0.02084
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.74906287, Validation loss: 171.44461744, Gradient norm: 6.16668079
INFO:root:[    2] Training loss: 171.12940304, Validation loss: 170.37681685, Gradient norm: 2.46190292
INFO:root:[    3] Training loss: 169.86566635, Validation loss: 167.62379245, Gradient norm: 4.48730307
INFO:root:[    4] Training loss: 167.07328364, Validation loss: 163.41359000, Gradient norm: 9.16819066
INFO:root:[    5] Training loss: 164.33745150, Validation loss: 161.10711617, Gradient norm: 19.28442776
INFO:root:[    6] Training loss: 162.90439491, Validation loss: 160.11053519, Gradient norm: 23.69565247
INFO:root:[    7] Training loss: 161.99680011, Validation loss: 159.05529259, Gradient norm: 35.07253509
INFO:root:[    8] Training loss: 161.30929592, Validation loss: 158.59630716, Gradient norm: 51.23008399
INFO:root:[    9] Training loss: 160.75350047, Validation loss: 157.60049491, Gradient norm: 64.91739521
INFO:root:[   10] Training loss: 160.24070227, Validation loss: 157.35725824, Gradient norm: 91.19677904
INFO:root:[   11] Training loss: 159.81990564, Validation loss: 156.54406054, Gradient norm: 100.32784560
INFO:root:[   12] Training loss: 159.43562898, Validation loss: 155.77941842, Gradient norm: 128.46101479
INFO:root:[   13] Training loss: 159.05273086, Validation loss: 155.79063205, Gradient norm: 139.53443623
INFO:root:[   14] Training loss: 158.81542348, Validation loss: 155.56855616, Gradient norm: 187.75228330
INFO:root:[   15] Training loss: 158.53570057, Validation loss: 154.68804090, Gradient norm: 185.98656981
INFO:root:[   16] Training loss: 158.24233293, Validation loss: 154.51305153, Gradient norm: 214.11677318
INFO:root:[   17] Training loss: 158.02662726, Validation loss: 154.13464882, Gradient norm: 233.23158949
INFO:root:[   18] Training loss: 157.85845394, Validation loss: 153.76478682, Gradient norm: 272.66294561
INFO:root:[   19] Training loss: 157.64402312, Validation loss: 153.57534316, Gradient norm: 312.64653784
INFO:root:[   20] Training loss: 157.45307153, Validation loss: 153.48270074, Gradient norm: 330.65177142
INFO:root:[   21] Training loss: 157.38723161, Validation loss: 153.13422367, Gradient norm: 347.02199770
INFO:root:[   22] Training loss: 157.12439856, Validation loss: 152.80773978, Gradient norm: 392.40596775
INFO:root:[   23] Training loss: 157.00783964, Validation loss: 153.86724222, Gradient norm: 418.78461838
INFO:root:[   24] Training loss: 157.09173165, Validation loss: 153.05416239, Gradient norm: 458.90229641
INFO:root:[   25] Training loss: 156.80899480, Validation loss: 152.76133097, Gradient norm: 440.81953824
INFO:root:[   26] Training loss: 156.95443293, Validation loss: 152.79030372, Gradient norm: 449.36406629
INFO:root:[   27] Training loss: 156.70343247, Validation loss: 152.35460373, Gradient norm: 501.20807937
INFO:root:[   28] Training loss: 156.54141073, Validation loss: 151.98836807, Gradient norm: 531.96949045
INFO:root:[   29] Training loss: 156.67805400, Validation loss: 152.23578039, Gradient norm: 587.01002534
INFO:root:[   30] Training loss: 156.73639902, Validation loss: 152.17302835, Gradient norm: 538.64187431
INFO:root:[   31] Training loss: 156.54268018, Validation loss: 151.70301714, Gradient norm: 548.51833102
INFO:root:[   32] Training loss: 156.31524780, Validation loss: 151.94142572, Gradient norm: 639.15314919
INFO:root:[   33] Training loss: 156.35447450, Validation loss: 151.86145283, Gradient norm: 674.67398697
INFO:root:[   34] Training loss: 156.37229379, Validation loss: 152.39114906, Gradient norm: 665.25832062
INFO:root:[   35] Training loss: 156.21430510, Validation loss: 151.66247190, Gradient norm: 687.22923753
INFO:root:[   36] Training loss: 156.49596290, Validation loss: 153.84243880, Gradient norm: 648.80139259
INFO:root:[   37] Training loss: 156.21447443, Validation loss: 151.78715094, Gradient norm: 690.27465584
INFO:root:[   38] Training loss: 156.04499250, Validation loss: 155.45346175, Gradient norm: 723.60827687
INFO:root:[   39] Training loss: 156.60819481, Validation loss: 151.28541828, Gradient norm: 669.85416953
INFO:root:[   40] Training loss: 156.45234950, Validation loss: 151.84840235, Gradient norm: 687.71536881
INFO:root:[   41] Training loss: 156.13542351, Validation loss: 151.28825957, Gradient norm: 689.60055698
INFO:root:[   42] Training loss: 155.79673267, Validation loss: 151.47129506, Gradient norm: 785.13034141
INFO:root:[   43] Training loss: 155.99879429, Validation loss: 150.80739041, Gradient norm: 810.29584070
INFO:root:[   44] Training loss: 156.23399934, Validation loss: 151.28061544, Gradient norm: 747.25041718
INFO:root:[   45] Training loss: 156.08091466, Validation loss: 155.59430300, Gradient norm: 846.43412827
INFO:root:[   46] Training loss: 156.14801998, Validation loss: 151.95532700, Gradient norm: 752.39698691
INFO:root:[   47] Training loss: 156.02072940, Validation loss: 152.05026245, Gradient norm: 798.70062822
INFO:root:[   48] Training loss: 155.85496467, Validation loss: 151.31282675, Gradient norm: 852.74330473
INFO:root:[   49] Training loss: 156.34782086, Validation loss: 151.46389823, Gradient norm: 770.36932578
INFO:root:[   50] Training loss: 156.19884471, Validation loss: 151.81521712, Gradient norm: 747.82299984
INFO:root:[   51] Training loss: 156.11834771, Validation loss: 151.05044871, Gradient norm: 814.97003427
INFO:root:[   52] Training loss: 156.07112581, Validation loss: 152.10560713, Gradient norm: 814.86797314
INFO:root:[   53] Training loss: 155.76531604, Validation loss: 151.32935675, Gradient norm: 878.51921455
INFO:root:[   54] Training loss: 157.71992857, Validation loss: 153.40078525, Gradient norm: 774.06872439
INFO:root:[   55] Training loss: 156.32875142, Validation loss: 152.26840052, Gradient norm: 810.14984296
INFO:root:[   56] Training loss: 156.11617650, Validation loss: 151.67715507, Gradient norm: 903.13877041
INFO:root:[   57] Training loss: 155.86962594, Validation loss: 150.98307590, Gradient norm: 894.70809830
INFO:root:[   58] Training loss: 156.10760620, Validation loss: 150.82849068, Gradient norm: 890.29040748
INFO:root:[   59] Training loss: 156.31006724, Validation loss: 150.94725510, Gradient norm: 836.68117675
INFO:root:[   60] Training loss: 155.95380098, Validation loss: 152.54118873, Gradient norm: 974.80512460
INFO:root:[   61] Training loss: 156.07926063, Validation loss: 151.36540064, Gradient norm: 900.26446018
INFO:root:[   62] Training loss: 156.09882713, Validation loss: 152.30633440, Gradient norm: 1018.88564870
INFO:root:[   63] Training loss: 155.84806864, Validation loss: 152.91585462, Gradient norm: 911.11090624
INFO:root:[   64] Training loss: 155.78663649, Validation loss: 151.65983213, Gradient norm: 1038.12797580
INFO:root:[   65] Training loss: 156.02403921, Validation loss: 152.38615207, Gradient norm: 1059.11195928
INFO:root:[   66] Training loss: 156.00528400, Validation loss: 151.88386693, Gradient norm: 1049.28148897
INFO:root:[   67] Training loss: 156.11758517, Validation loss: 151.20497605, Gradient norm: 1046.14800007
INFO:root:[   68] Training loss: 155.97827351, Validation loss: 150.77113290, Gradient norm: 1037.87189851
INFO:root:[   69] Training loss: 156.41559743, Validation loss: 152.56558385, Gradient norm: 1000.54443640
INFO:root:[   70] Training loss: 156.31201766, Validation loss: 151.57286019, Gradient norm: 1075.06156555
INFO:root:[   71] Training loss: 156.12169073, Validation loss: 150.88064628, Gradient norm: 1107.51998687
INFO:root:[   72] Training loss: 156.10326136, Validation loss: 151.27419044, Gradient norm: 1200.95440601
INFO:root:[   73] Training loss: 156.10401025, Validation loss: 151.10746870, Gradient norm: 1157.89781618
INFO:root:[   74] Training loss: 156.09612686, Validation loss: 151.78660847, Gradient norm: 1187.52382930
INFO:root:[   75] Training loss: 156.12863146, Validation loss: 151.24557127, Gradient norm: 1243.85507593
INFO:root:[   76] Training loss: 155.92651151, Validation loss: 151.74686616, Gradient norm: 1211.54068271
INFO:root:[   77] Training loss: 156.03747775, Validation loss: 152.42386180, Gradient norm: 1234.77116913
INFO:root:[   78] Training loss: 155.89815528, Validation loss: 150.59421197, Gradient norm: 1261.52989445
INFO:root:[   79] Training loss: 156.12591418, Validation loss: 154.77284820, Gradient norm: 1356.04574051
INFO:root:[   80] Training loss: 156.31736148, Validation loss: 152.00705903, Gradient norm: 1233.07647048
INFO:root:[   81] Training loss: 156.74180427, Validation loss: 153.20740641, Gradient norm: 1348.10313227
INFO:root:[   82] Training loss: 156.61387661, Validation loss: 152.24114832, Gradient norm: 1392.56292308
INFO:root:[   83] Training loss: 156.38699381, Validation loss: 152.36570845, Gradient norm: 1420.65046329
INFO:root:[   84] Training loss: 156.24063529, Validation loss: 156.66154164, Gradient norm: 1376.59519534
INFO:root:[   85] Training loss: 156.65414753, Validation loss: 151.32129117, Gradient norm: 1470.47643910
INFO:root:[   86] Training loss: 156.53898445, Validation loss: 151.35903194, Gradient norm: 1371.40767708
INFO:root:[   87] Training loss: 156.34773754, Validation loss: 151.01065590, Gradient norm: 1331.10233028
INFO:root:EP 87: Early stopping
INFO:root:Training the model took 1139.557s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 150.0306
INFO:root:EnergyScoreTrain: 149.04281
INFO:root:CoverageTrain: 0.0079
INFO:root:IntervalWidthTrain: 0.02835
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 150.59567
INFO:root:EnergyScoreValidation: 149.44164
INFO:root:CoverageValidation: 0.0092
INFO:root:IntervalWidthValidation: 0.03322
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 150.71414
INFO:root:EnergyScoreTest: 149.53187
INFO:root:CoverageTest: 0.00942
INFO:root:IntervalWidthTest: 0.03405
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.59171329, Validation loss: 171.39823650, Gradient norm: 2.48348679
INFO:root:[    2] Training loss: 171.46465619, Validation loss: 171.52532959, Gradient norm: 0.28755053
INFO:root:[    3] Training loss: 171.39855741, Validation loss: 170.91313382, Gradient norm: 1.04765786
INFO:root:[    4] Training loss: 170.36067672, Validation loss: 169.01634216, Gradient norm: 3.76548053
INFO:root:[    5] Training loss: 167.90759763, Validation loss: 164.27853709, Gradient norm: 10.05924524
INFO:root:[    6] Training loss: 165.17400394, Validation loss: 162.02644401, Gradient norm: 19.50273537
INFO:root:[    7] Training loss: 163.74091143, Validation loss: 160.57270550, Gradient norm: 36.82387590
INFO:root:[    8] Training loss: 162.96580883, Validation loss: 159.73270864, Gradient norm: 63.77103885
INFO:root:[    9] Training loss: 162.39342533, Validation loss: 159.21524258, Gradient norm: 99.39264991
INFO:root:[   10] Training loss: 161.97783276, Validation loss: 159.00902110, Gradient norm: 127.82724889
INFO:root:[   11] Training loss: 161.60461318, Validation loss: 158.10977909, Gradient norm: 158.83513861
INFO:root:[   12] Training loss: 161.22475805, Validation loss: 158.14463070, Gradient norm: 198.71816290
INFO:root:[   13] Training loss: 160.95903407, Validation loss: 157.32589301, Gradient norm: 205.53788573
INFO:root:[   14] Training loss: 160.77013458, Validation loss: 156.97831726, Gradient norm: 238.97282533
INFO:root:[   15] Training loss: 160.44422224, Validation loss: 156.85532458, Gradient norm: 277.26519734
INFO:root:[   16] Training loss: 160.33149233, Validation loss: 156.55407820, Gradient norm: 269.19139537
INFO:root:[   17] Training loss: 160.11063351, Validation loss: 156.69091218, Gradient norm: 338.23249802
INFO:root:[   18] Training loss: 160.02320079, Validation loss: 156.00495332, Gradient norm: 360.98310404
INFO:root:[   19] Training loss: 159.69726914, Validation loss: 156.45068465, Gradient norm: 337.65608064
INFO:root:[   20] Training loss: 159.64449074, Validation loss: 155.29617941, Gradient norm: 394.10767315
INFO:root:[   21] Training loss: 159.43852355, Validation loss: 156.67386075, Gradient norm: 415.20152704
INFO:root:[   22] Training loss: 159.30722397, Validation loss: 154.93618353, Gradient norm: 447.27224286
INFO:root:[   23] Training loss: 159.42260810, Validation loss: 154.96529310, Gradient norm: 461.06695299
INFO:root:[   24] Training loss: 158.97387034, Validation loss: 154.76697040, Gradient norm: 437.57154260
INFO:root:[   25] Training loss: 158.98786224, Validation loss: 154.23386357, Gradient norm: 480.40152762
INFO:root:[   26] Training loss: 158.83428428, Validation loss: 154.48436448, Gradient norm: 520.72402152
INFO:root:[   27] Training loss: 158.82078390, Validation loss: 154.44389343, Gradient norm: 505.74505310
INFO:root:[   28] Training loss: 158.70920218, Validation loss: 156.00673334, Gradient norm: 576.65489908
INFO:root:[   29] Training loss: 158.65717241, Validation loss: 155.74372916, Gradient norm: 521.21211579
INFO:root:[   30] Training loss: 158.41060051, Validation loss: 155.04259044, Gradient norm: 569.88769410
INFO:root:[   31] Training loss: 158.38316251, Validation loss: 153.91241087, Gradient norm: 569.35613094
INFO:root:[   32] Training loss: 158.27815787, Validation loss: 153.36570792, Gradient norm: 619.23649194
INFO:root:[   33] Training loss: 158.31749503, Validation loss: 158.94220129, Gradient norm: 624.79088034
INFO:root:[   34] Training loss: 158.63958254, Validation loss: 153.74398856, Gradient norm: 548.96162286
INFO:root:[   35] Training loss: 158.18990373, Validation loss: 153.47539441, Gradient norm: 636.60631516
INFO:root:[   36] Training loss: 158.05023247, Validation loss: 153.14740779, Gradient norm: 602.90042192
INFO:root:[   37] Training loss: 158.32829514, Validation loss: 153.41657389, Gradient norm: 548.07781178
INFO:root:[   38] Training loss: 158.10772435, Validation loss: 153.10915769, Gradient norm: 669.32447435
INFO:root:[   39] Training loss: 158.06163592, Validation loss: 153.50515484, Gradient norm: 660.07053869
INFO:root:[   40] Training loss: 157.91428098, Validation loss: 152.81913021, Gradient norm: 660.61741970
INFO:root:[   41] Training loss: 158.07371899, Validation loss: 153.88447781, Gradient norm: 693.79997086
INFO:root:[   42] Training loss: 157.80663819, Validation loss: 152.66299018, Gradient norm: 651.54501160
INFO:root:[   43] Training loss: 157.78147983, Validation loss: 152.65746702, Gradient norm: 685.01756292
INFO:root:[   44] Training loss: 157.91140166, Validation loss: 154.75812294, Gradient norm: 732.69874632
INFO:root:[   45] Training loss: 157.69432540, Validation loss: 154.82863696, Gradient norm: 783.02637526
INFO:root:[   46] Training loss: 157.77122200, Validation loss: 152.84587729, Gradient norm: 699.24887210
INFO:root:[   47] Training loss: 157.96197618, Validation loss: 152.91938834, Gradient norm: 784.71879456
INFO:root:[   48] Training loss: 157.56053054, Validation loss: 152.67507829, Gradient norm: 777.44807441
INFO:root:[   49] Training loss: 158.04860181, Validation loss: 152.79939796, Gradient norm: 740.99050924
INFO:root:[   50] Training loss: 157.70563446, Validation loss: 152.61393527, Gradient norm: 806.71730274
INFO:root:[   51] Training loss: 157.77459298, Validation loss: 152.27724167, Gradient norm: 804.23772879
INFO:root:[   52] Training loss: 157.99465348, Validation loss: 153.25758257, Gradient norm: 785.83380373
INFO:root:[   53] Training loss: 157.58256490, Validation loss: 154.52728482, Gradient norm: 860.98069914
INFO:root:[   54] Training loss: 158.06039429, Validation loss: 153.10114209, Gradient norm: 760.21133354
INFO:root:[   55] Training loss: 157.60101318, Validation loss: 152.71114744, Gradient norm: 807.19003618
INFO:root:[   56] Training loss: 157.82576704, Validation loss: 152.90187073, Gradient norm: 913.99447934
INFO:root:[   57] Training loss: 157.61829748, Validation loss: 154.48972820, Gradient norm: 913.41361921
INFO:root:[   58] Training loss: 158.14651138, Validation loss: 152.69407706, Gradient norm: 885.81747300
INFO:root:[   59] Training loss: 157.72019432, Validation loss: 152.78252911, Gradient norm: 882.08342787
INFO:root:[   60] Training loss: 157.58025394, Validation loss: 152.97003858, Gradient norm: 918.94343174
INFO:root:[   61] Training loss: 157.88428396, Validation loss: 153.88049790, Gradient norm: 953.87682438
INFO:root:[   62] Training loss: 157.97876206, Validation loss: 152.29785156, Gradient norm: 872.09177855
INFO:root:[   63] Training loss: 158.06789013, Validation loss: 152.94637009, Gradient norm: 966.74716126
INFO:root:[   64] Training loss: 157.69512656, Validation loss: 152.89305220, Gradient norm: 988.85687850
INFO:root:[   65] Training loss: 158.06307875, Validation loss: 152.67782224, Gradient norm: 968.65231129
INFO:root:[   66] Training loss: 157.70777664, Validation loss: 156.30941088, Gradient norm: 1054.24263827
INFO:root:[   67] Training loss: 158.34129212, Validation loss: 153.54766635, Gradient norm: 1013.25121985
INFO:root:[   68] Training loss: 157.95332215, Validation loss: 153.67517826, Gradient norm: 1038.32350810
INFO:root:[   69] Training loss: 158.05739668, Validation loss: 155.84293918, Gradient norm: 1056.69083699
INFO:root:[   70] Training loss: 158.10916057, Validation loss: 153.82005257, Gradient norm: 1111.12909016
INFO:root:[   71] Training loss: 158.31781235, Validation loss: 154.17325355, Gradient norm: 1152.27216361
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 944.751s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 152.04797
INFO:root:EnergyScoreTrain: 151.57598
INFO:root:CoverageTrain: 0.00332
INFO:root:IntervalWidthTrain: 0.01262
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 152.36899
INFO:root:EnergyScoreValidation: 151.89683
INFO:root:CoverageValidation: 0.0033
INFO:root:IntervalWidthValidation: 0.01265
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 152.46691
INFO:root:EnergyScoreTest: 151.85475
INFO:root:CoverageTest: 0.00436
INFO:root:IntervalWidthTest: 0.0168
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.75269108, Validation loss: 171.47330606, Gradient norm: 4.20030425
INFO:root:[    2] Training loss: 171.46895512, Validation loss: 171.51816217, Gradient norm: 0.39305122
INFO:root:[    3] Training loss: 171.48519114, Validation loss: 171.41959934, Gradient norm: 0.68431224
INFO:root:[    4] Training loss: 171.45377141, Validation loss: 171.52552217, Gradient norm: 0.16264613
INFO:root:[    5] Training loss: 171.45439756, Validation loss: 171.52701069, Gradient norm: 0.16658145
INFO:root:[    6] Training loss: 171.45337522, Validation loss: 171.49778642, Gradient norm: 0.14910975
INFO:root:[    7] Training loss: 171.45727053, Validation loss: 171.53221867, Gradient norm: 0.12660354
INFO:root:[    8] Training loss: 171.45083173, Validation loss: 171.51261113, Gradient norm: 0.15838452
INFO:root:[    9] Training loss: 171.45424524, Validation loss: 171.53729090, Gradient norm: 0.14317894
INFO:root:[   10] Training loss: 171.45201448, Validation loss: 171.46578979, Gradient norm: 0.12655671
INFO:root:[   11] Training loss: 171.45238191, Validation loss: 171.51080270, Gradient norm: 0.13688967
INFO:root:[   12] Training loss: 171.45305019, Validation loss: 171.43177585, Gradient norm: 0.12739347
INFO:root:[   13] Training loss: 171.45377681, Validation loss: 171.51140621, Gradient norm: 0.13620833
INFO:root:[   14] Training loss: 171.45873767, Validation loss: 171.43442036, Gradient norm: 0.12689951
INFO:root:[   15] Training loss: 171.45897263, Validation loss: 171.36259987, Gradient norm: 0.13296518
INFO:root:[   16] Training loss: 171.45455825, Validation loss: 171.44953024, Gradient norm: 0.11970926
INFO:root:[   17] Training loss: 171.44666729, Validation loss: 171.44940764, Gradient norm: 0.11858422
INFO:root:[   18] Training loss: 171.45747389, Validation loss: 171.52026367, Gradient norm: 0.13619877
INFO:root:[   19] Training loss: 171.45368066, Validation loss: 171.52259721, Gradient norm: 0.12993747
INFO:root:[   20] Training loss: 171.45278512, Validation loss: 171.47553437, Gradient norm: 0.11347679
INFO:root:[   21] Training loss: 171.45360072, Validation loss: 171.51366819, Gradient norm: 0.12304611
INFO:root:[   22] Training loss: 171.44882364, Validation loss: 171.49882928, Gradient norm: 0.11496635
INFO:root:[   23] Training loss: 171.45712591, Validation loss: 171.50521219, Gradient norm: 0.12713659
INFO:root:[   24] Training loss: 171.45888499, Validation loss: 171.49668200, Gradient norm: 0.13205319
INFO:root:[   25] Training loss: 171.45146652, Validation loss: 171.40711396, Gradient norm: 0.12407154
INFO:root:[   26] Training loss: 171.45544150, Validation loss: 171.48201831, Gradient norm: 0.12087870
INFO:root:[   27] Training loss: 171.45040623, Validation loss: 171.49559442, Gradient norm: 0.11097975
INFO:root:[   28] Training loss: 171.45286101, Validation loss: 171.43368215, Gradient norm: 0.10056586
INFO:root:[   29] Training loss: 171.45706123, Validation loss: 171.43777150, Gradient norm: 0.10598433
INFO:root:[   30] Training loss: 171.45264550, Validation loss: 171.40543707, Gradient norm: 0.10181025
INFO:root:[   31] Training loss: 171.45275312, Validation loss: 171.45121344, Gradient norm: 0.11653052
INFO:root:[   32] Training loss: 171.45461374, Validation loss: 171.42154141, Gradient norm: 0.11405613
INFO:root:[   33] Training loss: 171.46141430, Validation loss: 171.51455162, Gradient norm: 0.10319211
INFO:root:[   34] Training loss: 171.45753857, Validation loss: 171.66882745, Gradient norm: 0.10133513
INFO:root:[   35] Training loss: 171.44765169, Validation loss: 171.50697485, Gradient norm: 0.10617442
INFO:root:[   36] Training loss: 171.45489799, Validation loss: 171.52193267, Gradient norm: 0.10843825
INFO:root:[   37] Training loss: 171.46131222, Validation loss: 171.58141511, Gradient norm: 0.09388974
INFO:root:[   38] Training loss: 171.45798999, Validation loss: 171.55075547, Gradient norm: 0.10613871
INFO:root:[   39] Training loss: 171.45411696, Validation loss: 171.51621588, Gradient norm: 0.11732019
INFO:root:[   40] Training loss: 171.45146409, Validation loss: 171.43427934, Gradient norm: 0.09867627
INFO:root:[   41] Training loss: 171.45275906, Validation loss: 171.46940771, Gradient norm: 0.09949610
INFO:root:[   42] Training loss: 171.45448465, Validation loss: 171.49235272, Gradient norm: 0.10220384
INFO:root:[   43] Training loss: 171.45079405, Validation loss: 171.50410514, Gradient norm: 0.09555504
INFO:root:[   44] Training loss: 171.45184961, Validation loss: 171.52634509, Gradient norm: 0.09515795
INFO:root:[   45] Training loss: 171.45214695, Validation loss: 171.58075793, Gradient norm: 0.10368153
INFO:root:[   46] Training loss: 171.45342086, Validation loss: 171.45980414, Gradient norm: 0.09786215
INFO:root:[   47] Training loss: 171.45973192, Validation loss: 171.47145133, Gradient norm: 0.11085561
INFO:root:[   48] Training loss: 171.45671406, Validation loss: 171.49405960, Gradient norm: 0.10918590
INFO:root:[   49] Training loss: 171.45593221, Validation loss: 171.56125351, Gradient norm: 0.09533375
INFO:root:[   50] Training loss: 171.45364988, Validation loss: 171.41392307, Gradient norm: 0.08928179
INFO:root:[   51] Training loss: 171.45158103, Validation loss: 171.41456814, Gradient norm: 0.09466313
INFO:root:[   52] Training loss: 171.45823994, Validation loss: 171.52142913, Gradient norm: 0.09556167
INFO:root:[   53] Training loss: 171.45412168, Validation loss: 171.44186191, Gradient norm: 0.09730154
INFO:root:[   54] Training loss: 171.45940906, Validation loss: 171.48441446, Gradient norm: 0.10402473
INFO:root:[   55] Training loss: 171.45860709, Validation loss: 171.49269683, Gradient norm: 0.10198108
INFO:root:[   56] Training loss: 171.45716088, Validation loss: 171.51703986, Gradient norm: 0.08448067
INFO:root:[   57] Training loss: 171.46161132, Validation loss: 171.45726697, Gradient norm: 0.09519361
INFO:root:[   58] Training loss: 171.44940793, Validation loss: 171.54096353, Gradient norm: 0.09258092
INFO:root:[   59] Training loss: 171.45451436, Validation loss: 171.46400189, Gradient norm: 0.10383709
INFO:root:[   60] Training loss: 171.46266134, Validation loss: 171.43520645, Gradient norm: 0.10798782
INFO:root:[   61] Training loss: 171.45616217, Validation loss: 171.40870561, Gradient norm: 0.10247231
INFO:root:[   62] Training loss: 171.45469990, Validation loss: 171.38611840, Gradient norm: 0.10652557
INFO:root:[   63] Training loss: 171.46816733, Validation loss: 171.42238801, Gradient norm: 0.10622242
INFO:root:[   64] Training loss: 171.44735326, Validation loss: 171.58202388, Gradient norm: 0.08890329
INFO:root:[   65] Training loss: 171.45913588, Validation loss: 171.46566983, Gradient norm: 0.09602197
INFO:root:[   66] Training loss: 171.45286276, Validation loss: 171.41878431, Gradient norm: 0.10187802
INFO:root:[   67] Training loss: 171.45487517, Validation loss: 171.38898442, Gradient norm: 0.10964909
INFO:root:[   68] Training loss: 171.45775449, Validation loss: 171.44694361, Gradient norm: 0.07454070
INFO:root:[   69] Training loss: 171.45541854, Validation loss: 171.60107685, Gradient norm: 0.09599726
INFO:root:[   70] Training loss: 171.45748051, Validation loss: 171.54520232, Gradient norm: 0.10671425
INFO:root:[   71] Training loss: 171.45311622, Validation loss: 171.47845459, Gradient norm: 0.11682987
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 944.726s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: nan
INFO:root:EnergyScoreTrain: nan
INFO:root:CoverageTrain: 0.0
INFO:root:IntervalWidthTrain: nan
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: nan
INFO:root:EnergyScoreValidation: nan
INFO:root:CoverageValidation: 0.0
INFO:root:IntervalWidthValidation: nan
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: nan
INFO:root:EnergyScoreTest: nan
INFO:root:CoverageTest: 0.0
INFO:root:IntervalWidthTest: nan
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.44740606, Validation loss: 170.64318269, Gradient norm: 7.63875184
INFO:root:[    2] Training loss: 170.25854492, Validation loss: 169.43550636, Gradient norm: 5.56843030
INFO:root:[    3] Training loss: 166.78893287, Validation loss: 164.66689537, Gradient norm: 13.81852455
INFO:root:[    4] Training loss: 162.87269349, Validation loss: 161.66162109, Gradient norm: 21.16155001
INFO:root:[    5] Training loss: 160.53651671, Validation loss: 159.85936342, Gradient norm: 28.06646138
INFO:root:[    6] Training loss: 158.98942498, Validation loss: 158.66518534, Gradient norm: 30.40199275
INFO:root:[    7] Training loss: 157.81959709, Validation loss: 157.69317416, Gradient norm: 30.67252537
INFO:root:[    8] Training loss: 156.90366370, Validation loss: 156.91395464, Gradient norm: 35.09100100
INFO:root:[    9] Training loss: 156.16331563, Validation loss: 156.25000894, Gradient norm: 39.69149064
INFO:root:[   10] Training loss: 155.50056404, Validation loss: 155.71141947, Gradient norm: 43.15043711
INFO:root:[   11] Training loss: 154.94514114, Validation loss: 155.11118343, Gradient norm: 39.97170799
INFO:root:[   12] Training loss: 154.42612397, Validation loss: 154.77926425, Gradient norm: 54.71069923
INFO:root:[   13] Training loss: 153.99832450, Validation loss: 154.24510193, Gradient norm: 55.31715766
INFO:root:[   14] Training loss: 153.59354677, Validation loss: 153.87386506, Gradient norm: 59.29932141
INFO:root:[   15] Training loss: 153.21264405, Validation loss: 153.55988496, Gradient norm: 75.77129034
INFO:root:[   16] Training loss: 152.85887105, Validation loss: 153.60419859, Gradient norm: 75.30759042
INFO:root:[   17] Training loss: 152.47829322, Validation loss: 153.05294852, Gradient norm: 78.25859674
INFO:root:[   18] Training loss: 152.20369376, Validation loss: 152.77833662, Gradient norm: 93.97640973
INFO:root:[   19] Training loss: 151.96517742, Validation loss: 152.52289397, Gradient norm: 105.39081071
INFO:root:[   20] Training loss: 151.64971397, Validation loss: 152.15357655, Gradient norm: 97.95352810
INFO:root:[   21] Training loss: 151.37091186, Validation loss: 152.48339686, Gradient norm: 112.46224991
INFO:root:[   22] Training loss: 151.20524665, Validation loss: 151.95772000, Gradient norm: 151.15017443
INFO:root:[   23] Training loss: 150.84734540, Validation loss: 151.70267881, Gradient norm: 110.96028882
INFO:root:[   24] Training loss: 150.65634695, Validation loss: 151.28876732, Gradient norm: 132.43946762
INFO:root:[   25] Training loss: 150.49525762, Validation loss: 151.10284476, Gradient norm: 151.20250286
INFO:root:[   26] Training loss: 150.25071487, Validation loss: 151.21760506, Gradient norm: 171.49958222
INFO:root:[   27] Training loss: 150.05099339, Validation loss: 150.79658350, Gradient norm: 164.55190431
INFO:root:[   28] Training loss: 149.88148809, Validation loss: 150.69002980, Gradient norm: 178.26823291
INFO:root:[   29] Training loss: 149.72025211, Validation loss: 150.97175546, Gradient norm: 195.67188924
INFO:root:[   30] Training loss: 149.51396665, Validation loss: 150.99943227, Gradient norm: 209.59654806
INFO:root:[   31] Training loss: 149.34019821, Validation loss: 150.57126907, Gradient norm: 210.94184909
INFO:root:[   32] Training loss: 149.19442668, Validation loss: 150.24591643, Gradient norm: 213.04448594
INFO:root:[   33] Training loss: 149.01274892, Validation loss: 150.23999602, Gradient norm: 236.83157830
INFO:root:[   34] Training loss: 148.93607405, Validation loss: 149.86909958, Gradient norm: 262.97709606
INFO:root:[   35] Training loss: 148.70353739, Validation loss: 149.96016930, Gradient norm: 229.18246350
INFO:root:[   36] Training loss: 148.63334494, Validation loss: 151.01706985, Gradient norm: 286.76708369
INFO:root:[   37] Training loss: 148.52853042, Validation loss: 149.62975495, Gradient norm: 238.03787860
INFO:root:[   38] Training loss: 148.29553452, Validation loss: 149.57389095, Gradient norm: 283.48027836
INFO:root:[   39] Training loss: 148.21300378, Validation loss: 149.62755453, Gradient norm: 306.63379527
INFO:root:[   40] Training loss: 148.12898457, Validation loss: 149.51893616, Gradient norm: 297.12222822
INFO:root:[   41] Training loss: 148.07053976, Validation loss: 149.37031923, Gradient norm: 341.79193388
INFO:root:[   42] Training loss: 147.89512297, Validation loss: 150.29041317, Gradient norm: 305.62818871
INFO:root:[   43] Training loss: 147.79670054, Validation loss: 149.10933080, Gradient norm: 348.83048290
INFO:root:[   44] Training loss: 147.66022336, Validation loss: 149.30879211, Gradient norm: 322.84399730
INFO:root:[   45] Training loss: 147.56778224, Validation loss: 149.16056034, Gradient norm: 324.21663391
INFO:root:[   46] Training loss: 147.48352388, Validation loss: 148.98030669, Gradient norm: 387.61753828
INFO:root:[   47] Training loss: 147.36413534, Validation loss: 149.25696380, Gradient norm: 381.91135155
INFO:root:[   48] Training loss: 147.31519919, Validation loss: 149.10097320, Gradient norm: 368.22117780
INFO:root:[   49] Training loss: 147.16318262, Validation loss: 149.10036337, Gradient norm: 402.92339002
INFO:root:[   50] Training loss: 147.02722641, Validation loss: 148.66564257, Gradient norm: 356.89321097
INFO:root:[   51] Training loss: 146.96703021, Validation loss: 148.73194201, Gradient norm: 392.54169225
INFO:root:[   52] Training loss: 146.89700533, Validation loss: 148.79660192, Gradient norm: 360.95034799
INFO:root:[   53] Training loss: 146.81255037, Validation loss: 149.39571144, Gradient norm: 429.70153910
INFO:root:[   54] Training loss: 146.71723749, Validation loss: 148.96901782, Gradient norm: 431.93121656
INFO:root:[   55] Training loss: 146.63980116, Validation loss: 148.52795463, Gradient norm: 397.54306381
INFO:root:[   56] Training loss: 146.55866505, Validation loss: 148.58952542, Gradient norm: 428.43301218
INFO:root:[   57] Training loss: 146.43507858, Validation loss: 148.70537962, Gradient norm: 417.67423872
INFO:root:[   58] Training loss: 146.46828400, Validation loss: 148.65176865, Gradient norm: 479.29553505
INFO:root:[   59] Training loss: 146.32427519, Validation loss: 148.40118040, Gradient norm: 441.64407566
INFO:root:[   60] Training loss: 146.24172947, Validation loss: 148.94182455, Gradient norm: 472.19674724
INFO:root:[   61] Training loss: 146.15094143, Validation loss: 148.63551331, Gradient norm: 404.84239199
INFO:root:[   62] Training loss: 146.07633905, Validation loss: 149.01375554, Gradient norm: 469.68030417
INFO:root:[   63] Training loss: 145.98876048, Validation loss: 148.27199265, Gradient norm: 465.17397939
INFO:root:[   64] Training loss: 145.87624582, Validation loss: 148.40187336, Gradient norm: 455.24113521
INFO:root:[   65] Training loss: 145.89625320, Validation loss: 149.22126244, Gradient norm: 514.01442408
INFO:root:[   66] Training loss: 145.76502964, Validation loss: 148.41306542, Gradient norm: 468.46242000
INFO:root:[   67] Training loss: 145.71350557, Validation loss: 148.57926152, Gradient norm: 505.89624394
INFO:root:[   68] Training loss: 145.63770044, Validation loss: 148.65670303, Gradient norm: 479.06370323
INFO:root:[   69] Training loss: 145.60630245, Validation loss: 148.23956562, Gradient norm: 477.84285327
INFO:root:[   70] Training loss: 145.61312191, Validation loss: 148.48170419, Gradient norm: 516.01767172
INFO:root:[   71] Training loss: 145.36719668, Validation loss: 148.47433945, Gradient norm: 447.25061349
INFO:root:[   72] Training loss: 145.34419318, Validation loss: 148.26391075, Gradient norm: 508.59318163
INFO:root:[   73] Training loss: 145.26672849, Validation loss: 148.39785293, Gradient norm: 481.20122372
INFO:root:[   74] Training loss: 145.34972037, Validation loss: 148.03309789, Gradient norm: 511.88003774
INFO:root:[   75] Training loss: 145.20578314, Validation loss: 148.15592483, Gradient norm: 467.58710619
INFO:root:[   76] Training loss: 145.11344883, Validation loss: 148.09112444, Gradient norm: 483.43884672
INFO:root:[   77] Training loss: 144.96734336, Validation loss: 148.42598961, Gradient norm: 464.99829090
INFO:root:[   78] Training loss: 145.04778729, Validation loss: 148.09612143, Gradient norm: 523.01748183
INFO:root:[   79] Training loss: 144.91855236, Validation loss: 148.09930946, Gradient norm: 499.71799928
INFO:root:[   80] Training loss: 144.83536833, Validation loss: 148.13107668, Gradient norm: 560.69433401
INFO:root:[   81] Training loss: 144.84796642, Validation loss: 148.04577321, Gradient norm: 506.10091397
INFO:root:[   82] Training loss: 144.72791000, Validation loss: 148.38919962, Gradient norm: 516.63429388
INFO:root:[   83] Training loss: 144.67893280, Validation loss: 148.12060757, Gradient norm: 559.64438286
INFO:root:[   84] Training loss: 144.64482360, Validation loss: 148.01570287, Gradient norm: 527.56973838
INFO:root:[   85] Training loss: 144.62254158, Validation loss: 147.97180755, Gradient norm: 540.86552598
INFO:root:[   86] Training loss: 144.50981275, Validation loss: 147.87530307, Gradient norm: 522.07033386
INFO:root:[   87] Training loss: 144.36392023, Validation loss: 150.85676154, Gradient norm: 538.66821544
INFO:root:[   88] Training loss: 144.40503362, Validation loss: 148.20096404, Gradient norm: 552.90101802
INFO:root:[   89] Training loss: 144.35862300, Validation loss: 147.75370473, Gradient norm: 478.44241267
INFO:root:[   90] Training loss: 144.30684574, Validation loss: 150.61143125, Gradient norm: 529.20767377
INFO:root:[   91] Training loss: 144.20711416, Validation loss: 148.39879898, Gradient norm: 512.51923851
INFO:root:[   92] Training loss: 144.28061737, Validation loss: 148.06985000, Gradient norm: 611.08755169
INFO:root:[   93] Training loss: 144.00173194, Validation loss: 148.14082915, Gradient norm: 484.91678932
INFO:root:[   94] Training loss: 144.08203084, Validation loss: 148.13707128, Gradient norm: 589.76458497
INFO:root:[   95] Training loss: 143.89883018, Validation loss: 148.54491977, Gradient norm: 541.26611629
INFO:root:[   96] Training loss: 143.87578252, Validation loss: 148.17516985, Gradient norm: 595.68318096
INFO:root:[   97] Training loss: 143.85435135, Validation loss: 148.36918956, Gradient norm: 577.84465621
INFO:root:[   98] Training loss: 143.78663703, Validation loss: 148.41122279, Gradient norm: 570.48809236
INFO:root:EP 98: Early stopping
INFO:root:Training the model took 1304.915s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 141.56759
INFO:root:EnergyScoreTrain: 126.74982
INFO:root:CoverageTrain: 0.31242
INFO:root:IntervalWidthTrain: 1.28504
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 145.82951
INFO:root:EnergyScoreValidation: 130.94846
INFO:root:CoverageValidation: 0.30103
INFO:root:IntervalWidthValidation: 1.28559
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 146.06979
INFO:root:EnergyScoreTest: 131.20745
INFO:root:CoverageTest: 0.29937
INFO:root:IntervalWidthTest: 1.28259
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.82559933, Validation loss: 171.46263491, Gradient norm: 9.81263057
INFO:root:[    2] Training loss: 170.80603351, Validation loss: 170.29544488, Gradient norm: 4.04306943
INFO:root:[    3] Training loss: 168.96528166, Validation loss: 167.43252353, Gradient norm: 7.69616006
INFO:root:[    4] Training loss: 165.82269044, Validation loss: 164.58854833, Gradient norm: 11.23181950
INFO:root:[    5] Training loss: 163.20185163, Validation loss: 162.49279785, Gradient norm: 13.55589851
INFO:root:[    6] Training loss: 161.36864235, Validation loss: 160.83365342, Gradient norm: 14.78188856
INFO:root:[    7] Training loss: 160.03089702, Validation loss: 159.52696859, Gradient norm: 19.77900257
INFO:root:[    8] Training loss: 158.97574514, Validation loss: 158.55910150, Gradient norm: 21.67710723
INFO:root:[    9] Training loss: 158.12951255, Validation loss: 158.07596983, Gradient norm: 32.32731209
INFO:root:[   10] Training loss: 157.42521938, Validation loss: 157.30815019, Gradient norm: 37.95235459
INFO:root:[   11] Training loss: 156.82678803, Validation loss: 156.75989296, Gradient norm: 45.17229489
INFO:root:[   12] Training loss: 156.34874232, Validation loss: 156.44511150, Gradient norm: 52.80245512
INFO:root:[   13] Training loss: 155.95603214, Validation loss: 156.07643548, Gradient norm: 61.44772834
INFO:root:[   14] Training loss: 155.59128456, Validation loss: 155.80304323, Gradient norm: 68.89513516
INFO:root:[   15] Training loss: 155.24105146, Validation loss: 155.46107956, Gradient norm: 78.41477331
INFO:root:[   16] Training loss: 154.95362908, Validation loss: 155.44397183, Gradient norm: 90.12857429
INFO:root:[   17] Training loss: 154.77380574, Validation loss: 155.05189409, Gradient norm: 124.35518195
INFO:root:[   18] Training loss: 154.42868353, Validation loss: 154.55575456, Gradient norm: 118.18485333
INFO:root:[   19] Training loss: 154.21814289, Validation loss: 154.64654909, Gradient norm: 148.69629586
INFO:root:[   20] Training loss: 153.99227217, Validation loss: 154.27668657, Gradient norm: 167.52520639
INFO:root:[   21] Training loss: 153.83946809, Validation loss: 154.10216654, Gradient norm: 186.37358466
INFO:root:[   22] Training loss: 153.58351527, Validation loss: 154.54786419, Gradient norm: 196.24750534
INFO:root:[   23] Training loss: 153.45808181, Validation loss: 154.23009307, Gradient norm: 199.21868088
INFO:root:[   24] Training loss: 153.19896124, Validation loss: 153.55745355, Gradient norm: 212.37725874
INFO:root:[   25] Training loss: 153.01834701, Validation loss: 153.27921742, Gradient norm: 214.12672549
INFO:root:[   26] Training loss: 152.85348227, Validation loss: 153.13456358, Gradient norm: 265.30432608
INFO:root:[   27] Training loss: 152.76799727, Validation loss: 153.09320226, Gradient norm: 274.93386189
INFO:root:[   28] Training loss: 152.56380105, Validation loss: 152.92066956, Gradient norm: 282.75082104
INFO:root:[   29] Training loss: 152.40175284, Validation loss: 152.77815931, Gradient norm: 308.13815347
INFO:root:[   30] Training loss: 152.27224178, Validation loss: 152.65416691, Gradient norm: 319.26429063
INFO:root:[   31] Training loss: 152.18811346, Validation loss: 152.49289098, Gradient norm: 322.57976062
INFO:root:[   32] Training loss: 152.01977458, Validation loss: 152.66095549, Gradient norm: 345.62639501
INFO:root:[   33] Training loss: 151.93398656, Validation loss: 153.16881377, Gradient norm: 366.92104633
INFO:root:[   34] Training loss: 151.82659872, Validation loss: 152.47720337, Gradient norm: 376.92650246
INFO:root:[   35] Training loss: 151.71662295, Validation loss: 152.23497799, Gradient norm: 408.22646332
INFO:root:[   36] Training loss: 151.54135753, Validation loss: 153.49642892, Gradient norm: 381.39600908
INFO:root:[   37] Training loss: 151.45675983, Validation loss: 151.76711405, Gradient norm: 396.59591123
INFO:root:[   38] Training loss: 151.35680855, Validation loss: 151.85139413, Gradient norm: 409.01955530
INFO:root:[   39] Training loss: 151.22322231, Validation loss: 151.71444071, Gradient norm: 399.44643194
INFO:root:[   40] Training loss: 151.24713945, Validation loss: 151.76791119, Gradient norm: 444.27670422
INFO:root:[   41] Training loss: 151.04579082, Validation loss: 152.01459740, Gradient norm: 408.15229875
INFO:root:[   42] Training loss: 151.04910211, Validation loss: 151.50469550, Gradient norm: 465.04873583
INFO:root:[   43] Training loss: 150.99605702, Validation loss: 151.71403451, Gradient norm: 434.37945866
INFO:root:[   44] Training loss: 150.83408835, Validation loss: 151.55792447, Gradient norm: 467.75093019
INFO:root:[   45] Training loss: 150.79519910, Validation loss: 150.94741927, Gradient norm: 471.94710769
INFO:root:[   46] Training loss: 150.66812944, Validation loss: 151.75449924, Gradient norm: 452.90402095
INFO:root:[   47] Training loss: 150.64135621, Validation loss: 151.37983388, Gradient norm: 516.78846480
INFO:root:[   48] Training loss: 150.55045906, Validation loss: 151.00735842, Gradient norm: 477.00235404
INFO:root:[   49] Training loss: 150.62668603, Validation loss: 151.42084266, Gradient norm: 511.14201171
INFO:root:[   50] Training loss: 150.44874613, Validation loss: 151.18339065, Gradient norm: 492.44525228
INFO:root:[   51] Training loss: 150.42370227, Validation loss: 150.99623476, Gradient norm: 503.44386530
INFO:root:[   52] Training loss: 150.18183899, Validation loss: 150.64915835, Gradient norm: 500.71138286
INFO:root:[   53] Training loss: 150.28339028, Validation loss: 150.76314203, Gradient norm: 474.51406941
INFO:root:[   54] Training loss: 150.09875920, Validation loss: 155.45163490, Gradient norm: 530.71951433
INFO:root:[   55] Training loss: 150.12062329, Validation loss: 151.30673218, Gradient norm: 502.08811637
INFO:root:[   56] Training loss: 149.97655723, Validation loss: 150.73883425, Gradient norm: 523.54004219
INFO:root:[   57] Training loss: 149.95082740, Validation loss: 150.53983702, Gradient norm: 540.75430852
INFO:root:[   58] Training loss: 149.88400269, Validation loss: 150.38808467, Gradient norm: 574.58491171
INFO:root:[   59] Training loss: 149.89581582, Validation loss: 150.58808110, Gradient norm: 596.79724591
INFO:root:[   60] Training loss: 149.79649042, Validation loss: 150.27170273, Gradient norm: 568.38963333
INFO:root:[   61] Training loss: 149.77034375, Validation loss: 150.71450911, Gradient norm: 564.00475541
INFO:root:[   62] Training loss: 149.76949911, Validation loss: 150.48115276, Gradient norm: 595.14913806
INFO:root:[   63] Training loss: 149.63116226, Validation loss: 150.15410167, Gradient norm: 565.54622953
INFO:root:[   64] Training loss: 149.72247841, Validation loss: 152.47403217, Gradient norm: 575.49515427
INFO:root:[   65] Training loss: 149.59144592, Validation loss: 150.61268879, Gradient norm: 588.87580679
