INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 2097152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 134.00781871, Validation loss: 124.66554050, Gradient norm: 939.34000551
INFO:root:[    2] Training loss: 122.58259967, Validation loss: 123.68938472, Gradient norm: 351.69042336
INFO:root:[    3] Training loss: 122.38156243, Validation loss: 122.78156780, Gradient norm: 240.21500659
INFO:root:[    4] Training loss: 122.11293334, Validation loss: 122.26446586, Gradient norm: 160.09584811
INFO:root:[    5] Training loss: 122.04135949, Validation loss: 122.17050039, Gradient norm: 143.69071439
INFO:root:[    6] Training loss: 121.87339742, Validation loss: 122.46416210, Gradient norm: 108.64086935
INFO:root:[    7] Training loss: 121.71175621, Validation loss: 122.04785419, Gradient norm: 89.81834856
INFO:root:[    8] Training loss: 121.83505958, Validation loss: 122.00260425, Gradient norm: 71.30437118
INFO:root:[    9] Training loss: 121.75957509, Validation loss: 122.43143884, Gradient norm: 52.60890071
INFO:root:[   10] Training loss: 121.68012190, Validation loss: 121.81785662, Gradient norm: 51.62909079
INFO:root:[   11] Training loss: 121.74881292, Validation loss: 122.05649646, Gradient norm: 46.51677540
INFO:root:[   12] Training loss: 121.72707441, Validation loss: 121.86782942, Gradient norm: 45.76459460
INFO:root:[   13] Training loss: 121.64320090, Validation loss: 121.80729754, Gradient norm: 38.71052005
INFO:root:[   14] Training loss: 121.66378501, Validation loss: 121.57277469, Gradient norm: 34.57140044
INFO:root:[   15] Training loss: 121.71438315, Validation loss: 121.50431876, Gradient norm: 29.45226479
INFO:root:[   16] Training loss: 121.49362392, Validation loss: 121.60237832, Gradient norm: 23.84386943
INFO:root:[   17] Training loss: 121.31540275, Validation loss: 121.20846821, Gradient norm: 26.90112193
INFO:root:[   18] Training loss: 120.83708177, Validation loss: 120.89129928, Gradient norm: 34.74921654
INFO:root:[   19] Training loss: 120.70013745, Validation loss: 120.64024406, Gradient norm: 42.75544051
INFO:root:[   20] Training loss: 120.70850588, Validation loss: 120.79426391, Gradient norm: 39.57344717
INFO:root:[   21] Training loss: 120.58568249, Validation loss: 120.80471670, Gradient norm: 40.38013044
INFO:root:[   22] Training loss: 120.50537602, Validation loss: 120.61878967, Gradient norm: 32.93153384
INFO:root:[   23] Training loss: 120.58467608, Validation loss: 120.78833797, Gradient norm: 34.22964937
INFO:root:[   24] Training loss: 120.48868284, Validation loss: 121.01014289, Gradient norm: 34.32909962
INFO:root:[   25] Training loss: 120.50530040, Validation loss: 120.57194808, Gradient norm: 34.79372616
INFO:root:[   26] Training loss: 120.46662174, Validation loss: 120.47773979, Gradient norm: 36.36149245
INFO:root:[   27] Training loss: 120.57095344, Validation loss: 120.50995531, Gradient norm: 33.12003642
INFO:root:[   28] Training loss: 120.47962837, Validation loss: 120.38411739, Gradient norm: 33.66677304
INFO:root:[   29] Training loss: 120.42432316, Validation loss: 120.39643649, Gradient norm: 32.17397080
INFO:root:[   30] Training loss: 120.49362378, Validation loss: 121.13637516, Gradient norm: 31.31149320
INFO:root:[   31] Training loss: 120.53760191, Validation loss: 120.28947107, Gradient norm: 30.58738781
INFO:root:[   32] Training loss: 120.44098569, Validation loss: 120.58174107, Gradient norm: 27.84887699
INFO:root:[   33] Training loss: 120.48041906, Validation loss: 120.29755296, Gradient norm: 31.11594453
INFO:root:[   34] Training loss: 120.34216106, Validation loss: 120.54604234, Gradient norm: 32.07339917
INFO:root:[   35] Training loss: 120.46558988, Validation loss: 120.35076010, Gradient norm: 20.47664704
INFO:root:[   36] Training loss: 120.23858710, Validation loss: 120.54066546, Gradient norm: 24.01635205
INFO:root:[   37] Training loss: 120.32786520, Validation loss: 120.46353702, Gradient norm: 20.61115009
INFO:root:[   38] Training loss: 120.18949897, Validation loss: 120.59791933, Gradient norm: 21.24135692
INFO:root:[   39] Training loss: 120.26395673, Validation loss: 120.23057793, Gradient norm: 24.08588704
INFO:root:[   40] Training loss: 120.01831527, Validation loss: 119.85571842, Gradient norm: 21.85545354
INFO:root:[   41] Training loss: 119.56878392, Validation loss: 119.15598981, Gradient norm: 25.87684874
INFO:root:[   42] Training loss: 118.58296359, Validation loss: 117.79867343, Gradient norm: 30.64317026
INFO:root:[   43] Training loss: 117.44347692, Validation loss: 117.04948320, Gradient norm: 34.46884561
INFO:root:[   44] Training loss: 116.42249717, Validation loss: 115.58500172, Gradient norm: 44.58749474
INFO:root:[   45] Training loss: 115.30370682, Validation loss: 115.05680584, Gradient norm: 61.57444400
INFO:root:[   46] Training loss: 114.30059254, Validation loss: 114.16981033, Gradient norm: 54.17130265
INFO:root:[   47] Training loss: 113.51086176, Validation loss: 113.31143399, Gradient norm: 58.75481439
INFO:root:[   48] Training loss: 112.83179987, Validation loss: 112.72427131, Gradient norm: 62.95034264
INFO:root:[   49] Training loss: 112.21722034, Validation loss: 111.84878303, Gradient norm: 57.90992172
INFO:root:[   50] Training loss: 111.70491717, Validation loss: 111.53349094, Gradient norm: 65.59582327
INFO:root:[   51] Training loss: 111.12205910, Validation loss: 111.21543542, Gradient norm: 52.27958619
INFO:root:[   52] Training loss: 110.69653172, Validation loss: 110.45572189, Gradient norm: 59.78844362
INFO:root:[   53] Training loss: 110.43305746, Validation loss: 110.50789511, Gradient norm: 65.24422315
INFO:root:[   54] Training loss: 109.89863202, Validation loss: 109.86981517, Gradient norm: 59.45232513
INFO:root:[   55] Training loss: 109.76274359, Validation loss: 109.73730837, Gradient norm: 65.37353539
INFO:root:[   56] Training loss: 109.27375436, Validation loss: 109.91954172, Gradient norm: 77.21630044
INFO:root:[   57] Training loss: 108.90240789, Validation loss: 108.90320613, Gradient norm: 79.85469301
INFO:root:[   58] Training loss: 108.54996315, Validation loss: 108.80577692, Gradient norm: 72.14200382
INFO:root:[   59] Training loss: 108.32789247, Validation loss: 108.34117468, Gradient norm: 78.39074875
INFO:root:[   60] Training loss: 108.15014561, Validation loss: 108.01122337, Gradient norm: 83.21176883
INFO:root:[   61] Training loss: 107.69988008, Validation loss: 107.69564135, Gradient norm: 80.80547266
INFO:root:[   62] Training loss: 107.72930159, Validation loss: 107.97971765, Gradient norm: 86.49401007
INFO:root:[   63] Training loss: 107.37345954, Validation loss: 107.23409245, Gradient norm: 93.27294492
INFO:root:[   64] Training loss: 107.11750348, Validation loss: 107.33962276, Gradient norm: 75.22057603
INFO:root:[   65] Training loss: 106.89189283, Validation loss: 107.05127637, Gradient norm: 98.80117670
INFO:root:[   66] Training loss: 106.64532126, Validation loss: 107.03946291, Gradient norm: 92.54455579
INFO:root:[   67] Training loss: 106.43922256, Validation loss: 106.81303379, Gradient norm: 82.80477743
INFO:root:[   68] Training loss: 106.20676395, Validation loss: 106.23517004, Gradient norm: 74.07040818
INFO:root:[   69] Training loss: 106.23691154, Validation loss: 106.08495199, Gradient norm: 98.70003141
INFO:root:[   70] Training loss: 106.03243148, Validation loss: 106.55787159, Gradient norm: 96.06479708
INFO:root:[   71] Training loss: 105.80338119, Validation loss: 105.95974653, Gradient norm: 98.07264477
INFO:root:[   72] Training loss: 105.72770266, Validation loss: 105.78001220, Gradient norm: 96.42598885
INFO:root:[   73] Training loss: 105.49161145, Validation loss: 105.61580711, Gradient norm: 106.69841776
INFO:root:[   74] Training loss: 105.31893928, Validation loss: 105.77919822, Gradient norm: 95.55578827
INFO:root:[   75] Training loss: 105.27649459, Validation loss: 105.11187139, Gradient norm: 115.52758854
INFO:root:[   76] Training loss: 105.04026943, Validation loss: 105.20367247, Gradient norm: 87.93217198
INFO:root:[   77] Training loss: 105.05786592, Validation loss: 105.36241255, Gradient norm: 116.68268731
INFO:root:[   78] Training loss: 104.99572632, Validation loss: 105.24521111, Gradient norm: 100.18459235
INFO:root:[   79] Training loss: 104.72331143, Validation loss: 105.18163799, Gradient norm: 115.07184445
INFO:root:[   80] Training loss: 104.55812073, Validation loss: 104.81817706, Gradient norm: 98.10936357
INFO:root:[   81] Training loss: 104.60304902, Validation loss: 104.87857029, Gradient norm: 99.45824359
INFO:root:[   82] Training loss: 104.37459024, Validation loss: 105.05338840, Gradient norm: 108.15038455
INFO:root:[   83] Training loss: 104.31610850, Validation loss: 104.74104178, Gradient norm: 98.40960022
INFO:root:[   84] Training loss: 104.09478179, Validation loss: 104.24586934, Gradient norm: 108.01610434
INFO:root:[   85] Training loss: 104.13788807, Validation loss: 104.41260187, Gradient norm: 100.96181388
INFO:root:[   86] Training loss: 103.97108196, Validation loss: 104.19542905, Gradient norm: 104.41453221
INFO:root:[   87] Training loss: 103.87537944, Validation loss: 104.10503019, Gradient norm: 99.37958639
INFO:root:[   88] Training loss: 103.75264031, Validation loss: 103.75748075, Gradient norm: 110.41521520
INFO:root:[   89] Training loss: 103.81992151, Validation loss: 103.99799847, Gradient norm: 112.41859510
INFO:root:[   90] Training loss: 103.66162123, Validation loss: 104.00286129, Gradient norm: 113.23056871
INFO:root:[   91] Training loss: 103.52150051, Validation loss: 104.09080400, Gradient norm: 95.67996638
INFO:root:[   92] Training loss: 103.57366741, Validation loss: 103.75594698, Gradient norm: 119.19867349
INFO:root:[   93] Training loss: 103.36627899, Validation loss: 104.27289897, Gradient norm: 108.84345611
INFO:root:[   94] Training loss: 103.45974542, Validation loss: 103.92003921, Gradient norm: 121.17772455
INFO:root:[   95] Training loss: 103.19697105, Validation loss: 103.78024923, Gradient norm: 108.48376411
INFO:root:[   96] Training loss: 103.19208284, Validation loss: 103.94555269, Gradient norm: 107.81117629
INFO:root:[   97] Training loss: 103.26778115, Validation loss: 103.70611178, Gradient norm: 121.73001680
INFO:root:[   98] Training loss: 102.96818792, Validation loss: 103.67948361, Gradient norm: 106.90941547
INFO:root:[   99] Training loss: 103.14420163, Validation loss: 103.63627151, Gradient norm: 110.09843216
INFO:root:[  100] Training loss: 102.98931142, Validation loss: 103.60430093, Gradient norm: 117.69267511
INFO:root:[  101] Training loss: 102.95943741, Validation loss: 103.50733159, Gradient norm: 116.79619258
INFO:root:[  102] Training loss: 102.85141504, Validation loss: 103.55859954, Gradient norm: 115.47756258
INFO:root:[  103] Training loss: 102.64335626, Validation loss: 102.86597337, Gradient norm: 96.71435079
INFO:root:[  104] Training loss: 102.73172936, Validation loss: 103.08808873, Gradient norm: 126.64734537
INFO:root:[  105] Training loss: 102.68305719, Validation loss: 102.82043273, Gradient norm: 102.11544356
INFO:root:[  106] Training loss: 102.60663321, Validation loss: 103.15484172, Gradient norm: 116.96083025
INFO:root:[  107] Training loss: 102.66983161, Validation loss: 103.41433058, Gradient norm: 123.29953627
INFO:root:[  108] Training loss: 102.32643343, Validation loss: 102.54748351, Gradient norm: 106.10052832
INFO:root:[  109] Training loss: 102.50285603, Validation loss: 102.91688301, Gradient norm: 124.93778618
INFO:root:[  110] Training loss: 102.26318683, Validation loss: 102.62580977, Gradient norm: 106.41068608
INFO:root:[  111] Training loss: 102.33627103, Validation loss: 102.87285561, Gradient norm: 126.32735791
INFO:root:[  112] Training loss: 102.20359033, Validation loss: 103.02223100, Gradient norm: 116.28699860
INFO:root:[  113] Training loss: 102.09223958, Validation loss: 102.79284458, Gradient norm: 107.92564866
INFO:root:[  114] Training loss: 102.23497819, Validation loss: 102.29731330, Gradient norm: 120.48453335
INFO:root:[  115] Training loss: 102.19362256, Validation loss: 102.58734525, Gradient norm: 142.54341673
INFO:root:[  116] Training loss: 102.09388368, Validation loss: 102.68489364, Gradient norm: 97.60798695
INFO:root:[  117] Training loss: 102.06114210, Validation loss: 102.42941547, Gradient norm: 134.38677742
INFO:root:[  118] Training loss: 101.97237423, Validation loss: 102.73157238, Gradient norm: 124.99065035
INFO:root:[  119] Training loss: 101.83284989, Validation loss: 102.13801890, Gradient norm: 115.69751165
INFO:root:[  120] Training loss: 101.88411996, Validation loss: 102.31516187, Gradient norm: 137.45729292
INFO:root:[  121] Training loss: 101.77803850, Validation loss: 102.53591735, Gradient norm: 137.12493093
INFO:root:[  122] Training loss: 101.67908660, Validation loss: 102.37585870, Gradient norm: 104.62591367
INFO:root:[  123] Training loss: 101.65583511, Validation loss: 102.42333669, Gradient norm: 112.29571572
INFO:root:[  124] Training loss: 101.59812718, Validation loss: 102.57692587, Gradient norm: 129.43762140
INFO:root:[  125] Training loss: 101.69197177, Validation loss: 102.35532879, Gradient norm: 129.58515880
INFO:root:[  126] Training loss: 101.44765547, Validation loss: 101.89519106, Gradient norm: 111.76509170
INFO:root:[  127] Training loss: 101.51369578, Validation loss: 102.56096860, Gradient norm: 131.50641923
INFO:root:[  128] Training loss: 101.45792882, Validation loss: 101.89256076, Gradient norm: 127.13177676
INFO:root:[  129] Training loss: 101.41977658, Validation loss: 101.96901519, Gradient norm: 129.04650598
INFO:root:[  130] Training loss: 101.29373176, Validation loss: 101.74834232, Gradient norm: 119.90279252
INFO:root:[  131] Training loss: 101.32293283, Validation loss: 101.77518305, Gradient norm: 126.62762855
INFO:root:[  132] Training loss: 101.28059151, Validation loss: 102.00197812, Gradient norm: 130.82934262
INFO:root:[  133] Training loss: 101.29718126, Validation loss: 101.86704728, Gradient norm: 123.63263497
INFO:root:[  134] Training loss: 101.28917296, Validation loss: 102.32681985, Gradient norm: 126.90245162
INFO:root:[  135] Training loss: 101.11164323, Validation loss: 101.54840640, Gradient norm: 127.83927162
INFO:root:[  136] Training loss: 101.09074247, Validation loss: 102.18347378, Gradient norm: 126.36736289
INFO:root:[  137] Training loss: 100.97853034, Validation loss: 102.06200830, Gradient norm: 135.61709881
INFO:root:[  138] Training loss: 100.88936102, Validation loss: 101.42375288, Gradient norm: 118.00947266
INFO:root:[  139] Training loss: 101.03668152, Validation loss: 101.81416084, Gradient norm: 156.41479487
INFO:root:[  140] Training loss: 100.95620113, Validation loss: 101.47811521, Gradient norm: 133.13692905
INFO:root:[  141] Training loss: 100.89512135, Validation loss: 101.66549683, Gradient norm: 136.25957962
INFO:root:[  142] Training loss: 100.92566904, Validation loss: 101.59679465, Gradient norm: 143.07990260
INFO:root:[  143] Training loss: 100.74432663, Validation loss: 101.49771250, Gradient norm: 109.06955277
INFO:root:[  144] Training loss: 100.71498776, Validation loss: 101.10460531, Gradient norm: 137.34808406
INFO:root:[  145] Training loss: 100.68170888, Validation loss: 101.06120826, Gradient norm: 135.97858891
INFO:root:[  146] Training loss: 100.64559653, Validation loss: 101.07319246, Gradient norm: 126.94256747
INFO:root:[  147] Training loss: 100.80957457, Validation loss: 101.71198246, Gradient norm: 139.12595755
INFO:root:[  148] Training loss: 100.46809104, Validation loss: 101.52418992, Gradient norm: 118.62340788
INFO:root:[  149] Training loss: 100.50366954, Validation loss: 101.19301184, Gradient norm: 138.94821259
INFO:root:[  150] Training loss: 100.60812412, Validation loss: 101.31782926, Gradient norm: 146.68233529
INFO:root:[  151] Training loss: 100.57855144, Validation loss: 103.46979391, Gradient norm: 153.70687674
INFO:root:[  152] Training loss: 100.47280843, Validation loss: 101.00905109, Gradient norm: 113.38936743
INFO:root:[  153] Training loss: 100.41918236, Validation loss: 101.31950957, Gradient norm: 131.96783692
INFO:root:[  154] Training loss: 100.35396488, Validation loss: 101.22219349, Gradient norm: 139.88352135
INFO:root:[  155] Training loss: 100.37508487, Validation loss: 100.82995869, Gradient norm: 155.76774270
INFO:root:[  156] Training loss: 100.27851253, Validation loss: 101.10672286, Gradient norm: 124.93657684
INFO:root:[  157] Training loss: 100.29875453, Validation loss: 101.33891481, Gradient norm: 158.08024852
INFO:root:[  158] Training loss: 100.29134254, Validation loss: 101.26759891, Gradient norm: 148.06051964
INFO:root:[  159] Training loss: 100.24171157, Validation loss: 101.03487002, Gradient norm: 146.61312597
INFO:root:[  160] Training loss: 100.06224411, Validation loss: 100.79360225, Gradient norm: 132.85729992
INFO:root:[  161] Training loss: 100.23467748, Validation loss: 100.96949689, Gradient norm: 153.62030449
INFO:root:[  162] Training loss: 100.22420826, Validation loss: 100.94910194, Gradient norm: 138.81528892
INFO:root:[  163] Training loss: 100.13195254, Validation loss: 100.69577579, Gradient norm: 151.41370730
INFO:root:[  164] Training loss: 100.00315452, Validation loss: 101.03863788, Gradient norm: 144.61317122
INFO:root:[  165] Training loss: 100.12761067, Validation loss: 100.85144543, Gradient norm: 139.21148932
INFO:root:[  166] Training loss: 100.03064350, Validation loss: 100.77751949, Gradient norm: 151.21827221
INFO:root:[  167] Training loss: 100.10818853, Validation loss: 100.86179273, Gradient norm: 149.90377676
INFO:root:[  168] Training loss: 99.94387817, Validation loss: 100.82938701, Gradient norm: 139.01149799
INFO:root:[  169] Training loss: 100.00363706, Validation loss: 101.47248446, Gradient norm: 161.61116157
INFO:root:[  170] Training loss: 99.96579398, Validation loss: 101.49893609, Gradient norm: 159.35894492
INFO:root:[  171] Training loss: 99.86637190, Validation loss: 100.70144153, Gradient norm: 150.13893575
INFO:root:[  172] Training loss: 99.80512602, Validation loss: 101.10326412, Gradient norm: 129.25015548
INFO:root:[  173] Training loss: 99.75720654, Validation loss: 100.58889034, Gradient norm: 175.55374742
INFO:root:[  174] Training loss: 99.68605089, Validation loss: 100.26981301, Gradient norm: 134.61135925
INFO:root:[  175] Training loss: 99.62589230, Validation loss: 100.43449902, Gradient norm: 150.23218889
INFO:root:[  176] Training loss: 99.69226790, Validation loss: 100.63705523, Gradient norm: 151.41388595
INFO:root:[  177] Training loss: 99.72031740, Validation loss: 100.76992061, Gradient norm: 158.43138064
INFO:root:[  178] Training loss: 99.68305436, Validation loss: 100.29861819, Gradient norm: 144.39928097
INFO:root:[  179] Training loss: 99.59532355, Validation loss: 100.73316561, Gradient norm: 163.30569265
INFO:root:[  180] Training loss: 99.56831448, Validation loss: 100.52101661, Gradient norm: 155.85388239
INFO:root:[  181] Training loss: 99.49722648, Validation loss: 100.53761318, Gradient norm: 139.17151869
INFO:root:[  182] Training loss: 99.40861234, Validation loss: 100.47016854, Gradient norm: 139.58858945
INFO:root:[  183] Training loss: 99.64708602, Validation loss: 100.39595321, Gradient norm: 186.70793289
INFO:root:EP 183: Early stopping
INFO:root:Training the model took 3546.699s.
INFO:root:Emptying the cuda cache took 0.048s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 140.99716
INFO:root:EnergyScoreTrain: 99.51497
INFO:root:CoverageTrain: 0.74642
INFO:root:IntervalWidthTrain: 7.76101
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 142.11343
INFO:root:EnergyScoreValidation: 100.31878
INFO:root:CoverageValidation: 0.74459
INFO:root:IntervalWidthValidation: 7.75444
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 142.28037
INFO:root:EnergyScoreTest: 100.42441
INFO:root:CoverageTest: 0.74401
INFO:root:IntervalWidthTest: 7.75938
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 125.63817063, Validation loss: 123.59088714, Gradient norm: 352.16191220
INFO:root:[    2] Training loss: 122.43583753, Validation loss: 121.80371594, Gradient norm: 225.26404607
INFO:root:[    3] Training loss: 122.02875937, Validation loss: 121.87590500, Gradient norm: 95.08892297
INFO:root:[    4] Training loss: 121.82434838, Validation loss: 121.71977892, Gradient norm: 81.22063476
INFO:root:[    5] Training loss: 121.89995723, Validation loss: 121.67444742, Gradient norm: 65.24310539
INFO:root:[    6] Training loss: 121.70188951, Validation loss: 121.60840949, Gradient norm: 52.81669127
INFO:root:[    7] Training loss: 121.72792242, Validation loss: 121.86407892, Gradient norm: 62.81753212
INFO:root:[    8] Training loss: 121.45089256, Validation loss: 121.94495050, Gradient norm: 50.90985191
INFO:root:[    9] Training loss: 121.40359031, Validation loss: 121.37876340, Gradient norm: 40.83122254
INFO:root:[   10] Training loss: 121.23615508, Validation loss: 121.61251410, Gradient norm: 49.38899930
INFO:root:[   11] Training loss: 121.00183038, Validation loss: 121.74793585, Gradient norm: 46.88764777
INFO:root:[   12] Training loss: 121.00476898, Validation loss: 121.43424409, Gradient norm: 38.62667702
INFO:root:[   13] Training loss: 120.78701640, Validation loss: 120.96140894, Gradient norm: 38.22935639
INFO:root:[   14] Training loss: 120.70714859, Validation loss: 121.02213814, Gradient norm: 34.31893668
INFO:root:[   15] Training loss: 120.67867765, Validation loss: 120.85627773, Gradient norm: 33.82698332
INFO:root:[   16] Training loss: 120.60515500, Validation loss: 120.72788291, Gradient norm: 32.65593564
INFO:root:[   17] Training loss: 120.47955748, Validation loss: 120.61242807, Gradient norm: 29.43381648
INFO:root:[   18] Training loss: 120.52373444, Validation loss: 120.81632022, Gradient norm: 29.25305233
INFO:root:[   19] Training loss: 120.46905754, Validation loss: 120.75098103, Gradient norm: 30.23537526
INFO:root:[   20] Training loss: 120.52731391, Validation loss: 120.50039094, Gradient norm: 37.37329368
INFO:root:[   21] Training loss: 120.54793150, Validation loss: 120.52255223, Gradient norm: 34.74260071
INFO:root:[   22] Training loss: 120.35479581, Validation loss: 120.51851233, Gradient norm: 26.46257271
INFO:root:[   23] Training loss: 120.47359602, Validation loss: 120.50438374, Gradient norm: 26.06326589
INFO:root:[   24] Training loss: 120.33797374, Validation loss: 120.23301881, Gradient norm: 21.87061842
INFO:root:[   25] Training loss: 120.25864363, Validation loss: 120.53564243, Gradient norm: 22.97443236
INFO:root:[   26] Training loss: 120.30281769, Validation loss: 120.45961051, Gradient norm: 25.08094969
INFO:root:[   27] Training loss: 120.40297746, Validation loss: 120.27119472, Gradient norm: 27.61459181
INFO:root:[   28] Training loss: 120.34904649, Validation loss: 120.48829519, Gradient norm: 23.85168455
INFO:root:[   29] Training loss: 120.23373973, Validation loss: 120.08259688, Gradient norm: 24.46261548
INFO:root:[   30] Training loss: 119.75862034, Validation loss: 119.52309812, Gradient norm: 24.41010425
INFO:root:[   31] Training loss: 119.04051006, Validation loss: 118.46700129, Gradient norm: 21.94229352
INFO:root:[   32] Training loss: 118.06871114, Validation loss: 117.45922694, Gradient norm: 25.83000773
INFO:root:[   33] Training loss: 116.97005915, Validation loss: 116.56273704, Gradient norm: 22.43170591
INFO:root:[   34] Training loss: 116.07731851, Validation loss: 115.90263551, Gradient norm: 38.27597067
INFO:root:[   35] Training loss: 115.03364948, Validation loss: 114.41770146, Gradient norm: 33.95002650
INFO:root:[   36] Training loss: 114.08861218, Validation loss: 113.75361633, Gradient norm: 43.26816086
INFO:root:[   37] Training loss: 113.42339345, Validation loss: 113.17684595, Gradient norm: 61.45799922
INFO:root:[   38] Training loss: 112.54131783, Validation loss: 112.59453793, Gradient norm: 57.77918840
INFO:root:[   39] Training loss: 111.88193107, Validation loss: 111.96192643, Gradient norm: 63.68510150
INFO:root:[   40] Training loss: 111.28740982, Validation loss: 111.16887323, Gradient norm: 58.59468028
INFO:root:[   41] Training loss: 110.66626929, Validation loss: 110.94605176, Gradient norm: 59.61853352
INFO:root:[   42] Training loss: 110.30985949, Validation loss: 110.27308392, Gradient norm: 67.66271302
INFO:root:[   43] Training loss: 109.83144304, Validation loss: 109.73694847, Gradient norm: 63.30064784
INFO:root:[   44] Training loss: 109.28818174, Validation loss: 110.01652001, Gradient norm: 53.61222954
INFO:root:[   45] Training loss: 108.96689464, Validation loss: 108.96149050, Gradient norm: 60.45919324
INFO:root:[   46] Training loss: 108.59222729, Validation loss: 108.56817074, Gradient norm: 55.05472958
INFO:root:[   47] Training loss: 108.16584920, Validation loss: 108.19918850, Gradient norm: 66.30885342
INFO:root:[   48] Training loss: 107.81091295, Validation loss: 108.01019577, Gradient norm: 59.05564278
INFO:root:[   49] Training loss: 107.43304140, Validation loss: 107.99693667, Gradient norm: 51.13816169
INFO:root:[   50] Training loss: 107.13670835, Validation loss: 107.30229634, Gradient norm: 54.22920072
INFO:root:[   51] Training loss: 106.89239813, Validation loss: 107.38157654, Gradient norm: 53.25426679
INFO:root:[   52] Training loss: 106.75499030, Validation loss: 106.95785996, Gradient norm: 64.49724335
INFO:root:[   53] Training loss: 106.38000043, Validation loss: 107.13531520, Gradient norm: 57.06199777
INFO:root:[   54] Training loss: 106.24556847, Validation loss: 106.49961090, Gradient norm: 57.53885236
INFO:root:[   55] Training loss: 106.07636862, Validation loss: 106.35663447, Gradient norm: 63.84501592
INFO:root:[   56] Training loss: 105.93094655, Validation loss: 106.05738357, Gradient norm: 60.92587038
INFO:root:[   57] Training loss: 105.71817347, Validation loss: 105.97370490, Gradient norm: 52.69041164
INFO:root:[   58] Training loss: 105.55952967, Validation loss: 105.79613363, Gradient norm: 58.09632481
INFO:root:[   59] Training loss: 105.31978317, Validation loss: 105.65436633, Gradient norm: 57.41130967
INFO:root:[   60] Training loss: 105.15871693, Validation loss: 105.56520896, Gradient norm: 58.58625394
INFO:root:[   61] Training loss: 105.09908484, Validation loss: 105.29667295, Gradient norm: 58.78759284
INFO:root:[   62] Training loss: 104.94403225, Validation loss: 105.14703159, Gradient norm: 65.94575778
INFO:root:[   63] Training loss: 104.66010041, Validation loss: 105.29264779, Gradient norm: 58.10895047
INFO:root:[   64] Training loss: 104.53550086, Validation loss: 104.91301017, Gradient norm: 59.64897508
INFO:root:[   65] Training loss: 104.39801464, Validation loss: 105.16018729, Gradient norm: 57.23323924
INFO:root:[   66] Training loss: 104.31435225, Validation loss: 104.58313830, Gradient norm: 51.65888446
INFO:root:[   67] Training loss: 104.28457216, Validation loss: 104.52551033, Gradient norm: 59.75780099
INFO:root:[   68] Training loss: 104.05217270, Validation loss: 104.09121467, Gradient norm: 61.06314538
INFO:root:[   69] Training loss: 104.00178818, Validation loss: 104.43575024, Gradient norm: 67.73624865
INFO:root:[   70] Training loss: 103.86895948, Validation loss: 104.78964602, Gradient norm: 60.60361768
INFO:root:[   71] Training loss: 103.82055840, Validation loss: 104.63277988, Gradient norm: 54.29158762
INFO:root:[   72] Training loss: 103.69890770, Validation loss: 104.11557217, Gradient norm: 59.39475823
INFO:root:[   73] Training loss: 103.66020972, Validation loss: 104.29992597, Gradient norm: 68.63420374
INFO:root:[   74] Training loss: 103.39046120, Validation loss: 104.03688917, Gradient norm: 51.67187459
INFO:root:[   75] Training loss: 103.35674495, Validation loss: 104.26710721, Gradient norm: 62.55176875
INFO:root:[   76] Training loss: 103.37428729, Validation loss: 103.43852944, Gradient norm: 69.97221542
INFO:root:[   77] Training loss: 103.16127001, Validation loss: 103.97886079, Gradient norm: 59.49916658
INFO:root:[   78] Training loss: 103.25413054, Validation loss: 103.69491867, Gradient norm: 71.76473096
INFO:root:[   79] Training loss: 103.06396572, Validation loss: 103.65144848, Gradient norm: 52.17754058
INFO:root:[   80] Training loss: 102.94779118, Validation loss: 103.59496860, Gradient norm: 72.20165114
INFO:root:[   81] Training loss: 102.86958624, Validation loss: 103.43370872, Gradient norm: 53.52838971
INFO:root:[   82] Training loss: 102.93366667, Validation loss: 103.39104698, Gradient norm: 68.08529628
INFO:root:[   83] Training loss: 102.70301488, Validation loss: 103.63444045, Gradient norm: 62.68176269
INFO:root:[   84] Training loss: 102.70154956, Validation loss: 103.16611481, Gradient norm: 70.69668005
INFO:root:[   85] Training loss: 102.58821173, Validation loss: 103.71795917, Gradient norm: 59.75756429
INFO:root:[   86] Training loss: 102.60781212, Validation loss: 103.14313560, Gradient norm: 66.44377398
INFO:root:[   87] Training loss: 102.42406396, Validation loss: 103.18059145, Gradient norm: 64.54362221
INFO:root:[   88] Training loss: 102.45982982, Validation loss: 102.87967945, Gradient norm: 66.85863046
INFO:root:[   89] Training loss: 102.37231729, Validation loss: 103.00588805, Gradient norm: 73.85514996
INFO:root:[   90] Training loss: 102.37080694, Validation loss: 102.87604549, Gradient norm: 64.86456807
INFO:root:[   91] Training loss: 102.12224113, Validation loss: 102.88141343, Gradient norm: 62.69794337
INFO:root:[   92] Training loss: 102.13740283, Validation loss: 102.90718421, Gradient norm: 75.54364895
INFO:root:[   93] Training loss: 102.04329425, Validation loss: 102.75394913, Gradient norm: 64.46570311
INFO:root:[   94] Training loss: 101.98371847, Validation loss: 102.85292027, Gradient norm: 48.57700812
INFO:root:[   95] Training loss: 101.94424411, Validation loss: 103.08254400, Gradient norm: 74.18915968
INFO:root:[   96] Training loss: 101.88118926, Validation loss: 102.61403472, Gradient norm: 62.09154556
INFO:root:[   97] Training loss: 101.91627739, Validation loss: 102.87178960, Gradient norm: 76.76290003
INFO:root:[   98] Training loss: 101.80512906, Validation loss: 102.33501566, Gradient norm: 73.49809635
INFO:root:[   99] Training loss: 101.65991002, Validation loss: 102.32600824, Gradient norm: 62.14197540
INFO:root:[  100] Training loss: 101.62552697, Validation loss: 102.55223031, Gradient norm: 78.41790105
INFO:root:[  101] Training loss: 101.62211076, Validation loss: 102.43822321, Gradient norm: 70.30184478
INFO:root:[  102] Training loss: 101.46817260, Validation loss: 102.36128156, Gradient norm: 63.99096736
INFO:root:[  103] Training loss: 101.62640145, Validation loss: 101.77259774, Gradient norm: 76.53198913
INFO:root:[  104] Training loss: 101.49226298, Validation loss: 103.06979554, Gradient norm: 65.87586348
INFO:root:[  105] Training loss: 101.54524852, Validation loss: 102.99367970, Gradient norm: 80.62203635
INFO:root:[  106] Training loss: 101.40593551, Validation loss: 102.95011165, Gradient norm: 72.47460101
INFO:root:[  107] Training loss: 101.28141238, Validation loss: 102.05429577, Gradient norm: 81.93152065
INFO:root:[  108] Training loss: 101.44361006, Validation loss: 102.04380772, Gradient norm: 62.46975621
INFO:root:[  109] Training loss: 101.19106853, Validation loss: 101.96339206, Gradient norm: 64.80833773
INFO:root:[  110] Training loss: 101.20175002, Validation loss: 101.95920563, Gradient norm: 78.74360975
INFO:root:[  111] Training loss: 101.07750668, Validation loss: 101.94918613, Gradient norm: 68.63628822
INFO:root:[  112] Training loss: 100.98661730, Validation loss: 101.32724367, Gradient norm: 72.68947059
INFO:root:[  113] Training loss: 101.11749990, Validation loss: 101.77404259, Gradient norm: 83.14765317
INFO:root:[  114] Training loss: 101.00016467, Validation loss: 101.78850845, Gradient norm: 66.72765204
INFO:root:[  115] Training loss: 101.06100801, Validation loss: 101.48384094, Gradient norm: 79.19275824
INFO:root:[  116] Training loss: 100.93031014, Validation loss: 101.60477711, Gradient norm: 77.51113987
INFO:root:[  117] Training loss: 100.79864765, Validation loss: 101.59373158, Gradient norm: 77.94434868
INFO:root:[  118] Training loss: 100.69389553, Validation loss: 101.42541688, Gradient norm: 67.06791774
INFO:root:[  119] Training loss: 100.74644632, Validation loss: 101.59739133, Gradient norm: 75.85801800
INFO:root:[  120] Training loss: 100.73198349, Validation loss: 101.45797887, Gradient norm: 89.07839173
INFO:root:[  121] Training loss: 100.59245543, Validation loss: 101.44196214, Gradient norm: 67.54875791
INFO:root:EP 121: Early stopping
INFO:root:Training the model took 2178.625s.
INFO:root:Emptying the cuda cache took 0.046s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 142.99652
INFO:root:EnergyScoreTrain: 100.87311
INFO:root:CoverageTrain: 0.7479
INFO:root:IntervalWidthTrain: 8.1886
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 143.86861
INFO:root:EnergyScoreValidation: 101.48974
INFO:root:CoverageValidation: 0.74643
INFO:root:IntervalWidthValidation: 8.17663
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 144.04965
INFO:root:EnergyScoreTest: 101.60482
INFO:root:CoverageTest: 0.74637
INFO:root:IntervalWidthTest: 8.18994
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.58686748, Validation loss: 122.70284955, Gradient norm: 372.12673660
INFO:root:[    2] Training loss: 121.75387654, Validation loss: 121.72493402, Gradient norm: 311.64531043
INFO:root:[    3] Training loss: 121.55752185, Validation loss: 122.27677365, Gradient norm: 299.43658423
INFO:root:[    4] Training loss: 121.21780524, Validation loss: 121.00194655, Gradient norm: 263.36867131
INFO:root:[    5] Training loss: 120.77471330, Validation loss: 120.72002253, Gradient norm: 216.53005865
INFO:root:[    6] Training loss: 120.60747130, Validation loss: 121.09133253, Gradient norm: 196.53058548
INFO:root:[    7] Training loss: 120.55008637, Validation loss: 120.47448994, Gradient norm: 185.91446531
INFO:root:[    8] Training loss: 120.44703533, Validation loss: 120.50170030, Gradient norm: 155.41042309
INFO:root:[    9] Training loss: 120.48796595, Validation loss: 120.74390069, Gradient norm: 218.30601046
INFO:root:[   10] Training loss: 120.43123883, Validation loss: 120.49069424, Gradient norm: 225.76917849
INFO:root:[   11] Training loss: 120.33235688, Validation loss: 120.40346185, Gradient norm: 180.28991388
INFO:root:[   12] Training loss: 120.28816871, Validation loss: 120.49780273, Gradient norm: 173.86821746
INFO:root:[   13] Training loss: 120.29135355, Validation loss: 121.10131441, Gradient norm: 194.50053652
INFO:root:[   14] Training loss: 120.17592290, Validation loss: 120.25462867, Gradient norm: 150.81260198
INFO:root:[   15] Training loss: 120.12450173, Validation loss: 120.42089712, Gradient norm: 193.08010111
INFO:root:[   16] Training loss: 119.64074592, Validation loss: 119.98485723, Gradient norm: 152.46079464
INFO:root:[   17] Training loss: 118.68777810, Validation loss: 118.28939477, Gradient norm: 157.62016346
INFO:root:[   18] Training loss: 117.67772047, Validation loss: 117.49086525, Gradient norm: 151.21259018
INFO:root:[   19] Training loss: 116.69802397, Validation loss: 116.44309813, Gradient norm: 133.58657400
INFO:root:[   20] Training loss: 115.89730896, Validation loss: 115.60092979, Gradient norm: 126.01079498
INFO:root:[   21] Training loss: 115.12141608, Validation loss: 115.02189505, Gradient norm: 100.81872033
INFO:root:[   22] Training loss: 114.46737846, Validation loss: 114.24412089, Gradient norm: 137.01348762
INFO:root:[   23] Training loss: 113.80127460, Validation loss: 113.72814889, Gradient norm: 113.21257743
INFO:root:[   24] Training loss: 113.24295219, Validation loss: 113.35640795, Gradient norm: 124.28341708
INFO:root:[   25] Training loss: 112.79749912, Validation loss: 113.10269612, Gradient norm: 113.87063242
INFO:root:[   26] Training loss: 112.39899850, Validation loss: 112.33696563, Gradient norm: 105.61062549
INFO:root:[   27] Training loss: 112.01935098, Validation loss: 112.24888085, Gradient norm: 104.19551292
INFO:root:[   28] Training loss: 111.65728037, Validation loss: 111.90413666, Gradient norm: 90.96198371
INFO:root:[   29] Training loss: 111.43414759, Validation loss: 111.41356843, Gradient norm: 123.51083729
INFO:root:[   30] Training loss: 111.00887130, Validation loss: 111.14870374, Gradient norm: 84.51141033
INFO:root:[   31] Training loss: 110.76847049, Validation loss: 110.81130850, Gradient norm: 107.92149652
INFO:root:[   32] Training loss: 110.50783795, Validation loss: 110.75127753, Gradient norm: 115.21079532
INFO:root:[   33] Training loss: 110.24692792, Validation loss: 110.37835536, Gradient norm: 79.55011765
INFO:root:[   34] Training loss: 110.04468989, Validation loss: 110.08552499, Gradient norm: 123.76197084
INFO:root:[   35] Training loss: 109.77973087, Validation loss: 110.12061152, Gradient norm: 84.12439183
INFO:root:[   36] Training loss: 109.62518243, Validation loss: 109.68107105, Gradient norm: 118.20283840
INFO:root:[   37] Training loss: 109.40331248, Validation loss: 109.51212100, Gradient norm: 100.19005407
INFO:root:[   38] Training loss: 109.16589545, Validation loss: 109.36851817, Gradient norm: 109.14813044
INFO:root:[   39] Training loss: 108.96902074, Validation loss: 109.29297349, Gradient norm: 112.73294335
INFO:root:[   40] Training loss: 108.77852435, Validation loss: 109.05847773, Gradient norm: 102.36911288
INFO:root:[   41] Training loss: 108.57943131, Validation loss: 108.94283926, Gradient norm: 90.08766091
INFO:root:[   42] Training loss: 108.37747618, Validation loss: 109.16294335, Gradient norm: 105.20066743
INFO:root:[   43] Training loss: 108.27759363, Validation loss: 108.62219633, Gradient norm: 96.24674627
INFO:root:[   44] Training loss: 108.09514159, Validation loss: 109.05984181, Gradient norm: 100.07267705
INFO:root:[   45] Training loss: 107.99937290, Validation loss: 108.16271841, Gradient norm: 120.08264925
INFO:root:[   46] Training loss: 107.79395179, Validation loss: 108.09169112, Gradient norm: 101.62177554
INFO:root:[   47] Training loss: 107.70012091, Validation loss: 108.09675072, Gradient norm: 119.90129694
INFO:root:[   48] Training loss: 107.49039500, Validation loss: 108.00054853, Gradient norm: 104.42200226
INFO:root:[   49] Training loss: 107.39837464, Validation loss: 107.74032146, Gradient norm: 107.82034820
INFO:root:[   50] Training loss: 107.17272490, Validation loss: 107.88068916, Gradient norm: 100.87757123
INFO:root:[   51] Training loss: 107.07530557, Validation loss: 107.46867423, Gradient norm: 128.32284124
INFO:root:[   52] Training loss: 106.94498005, Validation loss: 107.95665978, Gradient norm: 117.12267102
INFO:root:[   53] Training loss: 106.77915671, Validation loss: 107.27605570, Gradient norm: 117.09832598
INFO:root:[   54] Training loss: 106.71450110, Validation loss: 107.34671731, Gradient norm: 124.49818502
INFO:root:[   55] Training loss: 106.57198219, Validation loss: 107.01273346, Gradient norm: 101.34530186
INFO:root:[   56] Training loss: 106.42506672, Validation loss: 107.00265292, Gradient norm: 123.09959085
INFO:root:[   57] Training loss: 106.28858293, Validation loss: 106.71204350, Gradient norm: 122.87077379
INFO:root:[   58] Training loss: 106.24144380, Validation loss: 106.83061192, Gradient norm: 122.08875094
INFO:root:[   59] Training loss: 106.11553253, Validation loss: 106.85100319, Gradient norm: 129.84637950
INFO:root:[   60] Training loss: 105.99737778, Validation loss: 106.61519439, Gradient norm: 119.68934078
INFO:root:[   61] Training loss: 105.83249185, Validation loss: 106.56044769, Gradient norm: 115.28654696
INFO:root:[   62] Training loss: 105.78436745, Validation loss: 106.82932571, Gradient norm: 125.19906832
INFO:root:[   63] Training loss: 105.66872838, Validation loss: 106.59812664, Gradient norm: 145.99378973
INFO:root:[   64] Training loss: 105.52231085, Validation loss: 106.45399659, Gradient norm: 130.54455720
INFO:root:[   65] Training loss: 105.61441492, Validation loss: 106.28587026, Gradient norm: 152.65032600
INFO:root:[   66] Training loss: 105.41010440, Validation loss: 106.26924107, Gradient norm: 123.32341620
INFO:root:[   67] Training loss: 105.40599897, Validation loss: 105.97717285, Gradient norm: 142.53348681
INFO:root:[   68] Training loss: 105.26674396, Validation loss: 105.97593952, Gradient norm: 126.29158915
INFO:root:[   69] Training loss: 105.13717186, Validation loss: 106.55825332, Gradient norm: 134.31322241
INFO:root:[   70] Training loss: 105.15190199, Validation loss: 105.99904501, Gradient norm: 162.22145895
INFO:root:[   71] Training loss: 104.94831274, Validation loss: 105.94892489, Gradient norm: 116.75043220
INFO:root:[   72] Training loss: 104.95152897, Validation loss: 105.50785275, Gradient norm: 160.44003138
INFO:root:[   73] Training loss: 104.78209335, Validation loss: 105.79044026, Gradient norm: 137.01498936
INFO:root:[   74] Training loss: 104.79082280, Validation loss: 105.60965387, Gradient norm: 136.77285764
INFO:root:[   75] Training loss: 104.65298334, Validation loss: 105.83074372, Gradient norm: 162.50264083
INFO:root:[   76] Training loss: 104.61609251, Validation loss: 105.58233616, Gradient norm: 141.33438281
INFO:root:[   77] Training loss: 104.48925106, Validation loss: 105.22914702, Gradient norm: 154.42115503
INFO:root:[   78] Training loss: 104.40789511, Validation loss: 105.76030310, Gradient norm: 135.53634798
INFO:root:[   79] Training loss: 104.38904328, Validation loss: 105.39193699, Gradient norm: 180.68565826
INFO:root:[   80] Training loss: 104.25914670, Validation loss: 105.48643625, Gradient norm: 147.09698192
INFO:root:[   81] Training loss: 104.28615111, Validation loss: 105.43059250, Gradient norm: 163.09364762
INFO:root:[   82] Training loss: 104.21629394, Validation loss: 105.35965413, Gradient norm: 178.42862841
INFO:root:[   83] Training loss: 104.09551664, Validation loss: 105.16191890, Gradient norm: 137.45567685
INFO:root:[   84] Training loss: 103.98490271, Validation loss: 105.27558373, Gradient norm: 146.66423368
INFO:root:[   85] Training loss: 103.92621626, Validation loss: 104.87748797, Gradient norm: 158.49906175
INFO:root:[   86] Training loss: 103.86819343, Validation loss: 105.11694283, Gradient norm: 146.30673760
INFO:root:[   87] Training loss: 103.85393666, Validation loss: 104.98025881, Gradient norm: 171.46791180
INFO:root:[   88] Training loss: 103.79117794, Validation loss: 104.97520605, Gradient norm: 154.69033442
INFO:root:[   89] Training loss: 103.69671698, Validation loss: 105.13717783, Gradient norm: 158.55450124
INFO:root:[   90] Training loss: 103.68511254, Validation loss: 105.36990251, Gradient norm: 167.37502428
INFO:root:[   91] Training loss: 103.62932074, Validation loss: 105.00414381, Gradient norm: 172.48860358
INFO:root:[   92] Training loss: 103.63644645, Validation loss: 104.95532411, Gradient norm: 217.28855839
INFO:root:[   93] Training loss: 103.48004265, Validation loss: 104.66180551, Gradient norm: 170.22215553
INFO:root:[   94] Training loss: 103.34830191, Validation loss: 104.97486062, Gradient norm: 147.32060868
INFO:root:[   95] Training loss: 103.41371310, Validation loss: 104.77047387, Gradient norm: 166.00487316
INFO:root:[   96] Training loss: 103.39218862, Validation loss: 104.62268145, Gradient norm: 197.96043581
INFO:root:[   97] Training loss: 103.21454647, Validation loss: 104.71845061, Gradient norm: 160.02482236
INFO:root:[   98] Training loss: 103.23949831, Validation loss: 105.27430146, Gradient norm: 196.27694363
INFO:root:[   99] Training loss: 103.16826339, Validation loss: 104.51094950, Gradient norm: 172.71705393
INFO:root:[  100] Training loss: 103.07431712, Validation loss: 104.83543317, Gradient norm: 168.59462389
INFO:root:[  101] Training loss: 103.01860283, Validation loss: 104.58135197, Gradient norm: 182.51194638
INFO:root:[  102] Training loss: 103.01244861, Validation loss: 104.67120940, Gradient norm: 172.95744954
INFO:root:[  103] Training loss: 102.99025058, Validation loss: 104.49972192, Gradient norm: 213.88260906
INFO:root:[  104] Training loss: 102.89620567, Validation loss: 104.61028395, Gradient norm: 173.71222894
INFO:root:[  105] Training loss: 102.80953676, Validation loss: 104.54768924, Gradient norm: 209.86437635
INFO:root:[  106] Training loss: 102.75672190, Validation loss: 104.40045298, Gradient norm: 172.05138915
INFO:root:[  107] Training loss: 102.74806605, Validation loss: 104.14284594, Gradient norm: 181.64428869
INFO:root:[  108] Training loss: 102.69088684, Validation loss: 104.40315773, Gradient norm: 222.09323982
INFO:root:[  109] Training loss: 102.63382606, Validation loss: 104.55770295, Gradient norm: 200.23058442
INFO:root:[  110] Training loss: 102.59565120, Validation loss: 104.31927306, Gradient norm: 172.22175786
INFO:root:[  111] Training loss: 102.57818955, Validation loss: 104.88287827, Gradient norm: 228.01783885
INFO:root:[  112] Training loss: 102.45190673, Validation loss: 104.21360147, Gradient norm: 193.65559219
INFO:root:[  113] Training loss: 102.48802421, Validation loss: 104.54369486, Gradient norm: 210.94365085
INFO:root:[  114] Training loss: 102.39057382, Validation loss: 104.48920625, Gradient norm: 176.67075862
INFO:root:[  115] Training loss: 102.38509673, Validation loss: 104.95061204, Gradient norm: 184.01925082
INFO:root:[  116] Training loss: 102.31096467, Validation loss: 104.03726433, Gradient norm: 218.40607413
INFO:root:[  117] Training loss: 102.30828864, Validation loss: 103.93396891, Gradient norm: 211.57228286
INFO:root:[  118] Training loss: 102.29412923, Validation loss: 103.81511399, Gradient norm: 209.16473627
INFO:root:[  119] Training loss: 102.12481804, Validation loss: 104.51260902, Gradient norm: 201.95500750
INFO:root:[  120] Training loss: 102.04740015, Validation loss: 104.13702051, Gradient norm: 206.59165661
INFO:root:[  121] Training loss: 102.06722786, Validation loss: 104.08727264, Gradient norm: 204.29842764
INFO:root:[  122] Training loss: 102.04691483, Validation loss: 104.07766645, Gradient norm: 191.13659364
INFO:root:[  123] Training loss: 102.02778686, Validation loss: 104.07379571, Gradient norm: 219.60218928
INFO:root:[  124] Training loss: 101.93822115, Validation loss: 104.34499727, Gradient norm: 211.63627547
INFO:root:[  125] Training loss: 101.86454354, Validation loss: 104.09994638, Gradient norm: 198.18474820
INFO:root:[  126] Training loss: 101.90676097, Validation loss: 104.11451669, Gradient norm: 216.14231877
INFO:root:[  127] Training loss: 101.89253707, Validation loss: 104.37616625, Gradient norm: 216.05240676
INFO:root:EP 127: Early stopping
INFO:root:Training the model took 2267.163s.
INFO:root:Emptying the cuda cache took 0.047s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 144.55266
INFO:root:EnergyScoreTrain: 101.80949
INFO:root:CoverageTrain: 0.77493
INFO:root:IntervalWidthTrain: 8.16361
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.61374
INFO:root:EnergyScoreValidation: 103.96
INFO:root:CoverageValidation: 0.76991
INFO:root:IntervalWidthValidation: 8.15439
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.8974
INFO:root:EnergyScoreTest: 104.16143
INFO:root:CoverageTest: 0.76909
INFO:root:IntervalWidthTest: 8.15193
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.99962933, Validation loss: 122.14773349, Gradient norm: 233.32521113
INFO:root:[    2] Training loss: 121.67214763, Validation loss: 121.64066551, Gradient norm: 207.58973716
INFO:root:[    3] Training loss: 121.61242406, Validation loss: 121.36082932, Gradient norm: 213.55073614
INFO:root:[    4] Training loss: 121.21496319, Validation loss: 121.18811035, Gradient norm: 158.51866673
INFO:root:[    5] Training loss: 120.91417789, Validation loss: 120.77568212, Gradient norm: 136.48765196
INFO:root:[    6] Training loss: 120.71121425, Validation loss: 120.71366304, Gradient norm: 116.20686173
INFO:root:[    7] Training loss: 120.63133760, Validation loss: 120.58879879, Gradient norm: 125.74780616
INFO:root:[    8] Training loss: 120.54383978, Validation loss: 121.10316783, Gradient norm: 115.02603476
INFO:root:[    9] Training loss: 120.43429714, Validation loss: 120.45053390, Gradient norm: 104.48711771
INFO:root:[   10] Training loss: 120.33101998, Validation loss: 120.37337994, Gradient norm: 94.64955035
INFO:root:[   11] Training loss: 120.24670667, Validation loss: 120.00898059, Gradient norm: 91.46997722
INFO:root:[   12] Training loss: 119.67546325, Validation loss: 119.18103922, Gradient norm: 101.70947872
INFO:root:[   13] Training loss: 118.18362697, Validation loss: 117.51292735, Gradient norm: 82.18041522
INFO:root:[   14] Training loss: 116.69534491, Validation loss: 116.03040577, Gradient norm: 84.75282565
INFO:root:[   15] Training loss: 115.39118566, Validation loss: 114.77900933, Gradient norm: 84.89510942
INFO:root:[   16] Training loss: 114.39063411, Validation loss: 114.22731649, Gradient norm: 65.95719477
INFO:root:[   17] Training loss: 113.50526766, Validation loss: 113.46686186, Gradient norm: 66.84099037
INFO:root:[   18] Training loss: 112.88632803, Validation loss: 112.61643351, Gradient norm: 67.40472050
INFO:root:[   19] Training loss: 112.25781270, Validation loss: 112.32966877, Gradient norm: 75.11859773
INFO:root:[   20] Training loss: 111.78496477, Validation loss: 111.75597618, Gradient norm: 64.43712480
INFO:root:[   21] Training loss: 111.44165761, Validation loss: 111.42640344, Gradient norm: 64.62741148
INFO:root:[   22] Training loss: 110.98328636, Validation loss: 110.85179296, Gradient norm: 59.00047201
INFO:root:[   23] Training loss: 110.66894524, Validation loss: 110.68379238, Gradient norm: 68.24608720
INFO:root:[   24] Training loss: 110.36818351, Validation loss: 110.42856098, Gradient norm: 63.25168019
INFO:root:[   25] Training loss: 110.05404663, Validation loss: 110.14774007, Gradient norm: 67.14720043
INFO:root:[   26] Training loss: 109.80992045, Validation loss: 109.86008901, Gradient norm: 69.97483539
INFO:root:[   27] Training loss: 109.61678172, Validation loss: 109.71644461, Gradient norm: 72.27383556
INFO:root:[   28] Training loss: 109.30787625, Validation loss: 109.59449216, Gradient norm: 61.88513351
INFO:root:[   29] Training loss: 109.11573751, Validation loss: 109.27467741, Gradient norm: 68.60683816
INFO:root:[   30] Training loss: 108.91872683, Validation loss: 109.29844929, Gradient norm: 77.22598139
INFO:root:[   31] Training loss: 108.79594462, Validation loss: 108.74533370, Gradient norm: 77.78476514
INFO:root:[   32] Training loss: 108.48473655, Validation loss: 108.71040029, Gradient norm: 59.51591776
INFO:root:[   33] Training loss: 108.35815673, Validation loss: 108.68280161, Gradient norm: 72.04878255
INFO:root:[   34] Training loss: 108.18389204, Validation loss: 108.66855200, Gradient norm: 85.22142300
INFO:root:[   35] Training loss: 108.04232045, Validation loss: 108.35231465, Gradient norm: 79.43923415
INFO:root:[   36] Training loss: 107.85394503, Validation loss: 108.10438353, Gradient norm: 72.39507631
INFO:root:[   37] Training loss: 107.66324892, Validation loss: 108.02524172, Gradient norm: 81.08589118
INFO:root:[   38] Training loss: 107.54352090, Validation loss: 107.83820211, Gradient norm: 72.80543941
INFO:root:[   39] Training loss: 107.44190263, Validation loss: 107.84199866, Gradient norm: 81.08303988
INFO:root:[   40] Training loss: 107.19717272, Validation loss: 107.76300312, Gradient norm: 79.31149580
INFO:root:[   41] Training loss: 107.07993972, Validation loss: 107.64452178, Gradient norm: 74.33051418
INFO:root:[   42] Training loss: 107.00145728, Validation loss: 107.46527126, Gradient norm: 88.93046518
INFO:root:[   43] Training loss: 106.87198409, Validation loss: 107.36508152, Gradient norm: 97.69645534
INFO:root:[   44] Training loss: 106.74471202, Validation loss: 107.22007146, Gradient norm: 78.79334173
INFO:root:[   45] Training loss: 106.60300202, Validation loss: 107.09356847, Gradient norm: 68.60657117
INFO:root:[   46] Training loss: 106.48971996, Validation loss: 107.13088147, Gradient norm: 94.84152965
INFO:root:[   47] Training loss: 106.43590694, Validation loss: 106.84642318, Gradient norm: 94.21997118
INFO:root:[   48] Training loss: 106.24452945, Validation loss: 106.81320112, Gradient norm: 89.39516229
INFO:root:[   49] Training loss: 106.15133647, Validation loss: 106.78981439, Gradient norm: 96.41244401
INFO:root:[   50] Training loss: 106.03659577, Validation loss: 106.63825173, Gradient norm: 88.45009332
INFO:root:[   51] Training loss: 105.97406701, Validation loss: 106.69855052, Gradient norm: 101.98949501
INFO:root:[   52] Training loss: 105.85564342, Validation loss: 106.49594116, Gradient norm: 100.59860997
INFO:root:[   53] Training loss: 105.80563327, Validation loss: 106.49364182, Gradient norm: 92.57845231
INFO:root:[   54] Training loss: 105.59857590, Validation loss: 106.48693137, Gradient norm: 94.11952970
INFO:root:[   55] Training loss: 105.56126357, Validation loss: 106.26295261, Gradient norm: 95.38080704
INFO:root:[   56] Training loss: 105.45817613, Validation loss: 106.26804668, Gradient norm: 113.42225931
INFO:root:[   57] Training loss: 105.33241238, Validation loss: 106.13458699, Gradient norm: 101.36964442
INFO:root:[   58] Training loss: 105.26552670, Validation loss: 106.35138860, Gradient norm: 98.02465311
INFO:root:[   59] Training loss: 105.19959246, Validation loss: 105.94532355, Gradient norm: 113.04359307
INFO:root:[   60] Training loss: 105.18434015, Validation loss: 106.02232282, Gradient norm: 109.68995226
INFO:root:[   61] Training loss: 105.07014654, Validation loss: 106.30100171, Gradient norm: 118.79735047
INFO:root:[   62] Training loss: 105.02374436, Validation loss: 105.77279005, Gradient norm: 126.73157617
INFO:root:[   63] Training loss: 104.90572708, Validation loss: 105.89492982, Gradient norm: 102.41807715
INFO:root:[   64] Training loss: 104.75578882, Validation loss: 105.93615460, Gradient norm: 123.65395009
INFO:root:[   65] Training loss: 104.79376207, Validation loss: 105.61674210, Gradient norm: 129.72767130
INFO:root:[   66] Training loss: 104.68101231, Validation loss: 105.89608160, Gradient norm: 121.08264241
INFO:root:[   67] Training loss: 104.60219108, Validation loss: 105.71785578, Gradient norm: 127.60067485
INFO:root:[   68] Training loss: 104.55751287, Validation loss: 105.54741695, Gradient norm: 138.63159920
INFO:root:[   69] Training loss: 104.49127501, Validation loss: 105.51620431, Gradient norm: 109.15720759
INFO:root:[   70] Training loss: 104.44848727, Validation loss: 105.41631317, Gradient norm: 132.49143577
INFO:root:[   71] Training loss: 104.41609543, Validation loss: 105.19654346, Gradient norm: 149.61341443
INFO:root:[   72] Training loss: 104.21433670, Validation loss: 105.24527556, Gradient norm: 126.21871225
INFO:root:[   73] Training loss: 104.21410046, Validation loss: 105.36414469, Gradient norm: 142.44738978
INFO:root:[   74] Training loss: 104.12717951, Validation loss: 105.19889647, Gradient norm: 136.08411611
INFO:root:[   75] Training loss: 104.11379093, Validation loss: 105.22958769, Gradient norm: 156.48944925
INFO:root:[   76] Training loss: 104.05956606, Validation loss: 105.24466442, Gradient norm: 132.82507170
INFO:root:[   77] Training loss: 104.04872975, Validation loss: 105.09040543, Gradient norm: 178.19723863
INFO:root:[   78] Training loss: 103.90008133, Validation loss: 104.87635067, Gradient norm: 157.92774643
INFO:root:[   79] Training loss: 103.80812795, Validation loss: 104.98850434, Gradient norm: 148.17683392
INFO:root:[   80] Training loss: 103.79026052, Validation loss: 105.23912995, Gradient norm: 153.88151102
INFO:root:[   81] Training loss: 103.80609266, Validation loss: 104.97250393, Gradient norm: 176.12906208
INFO:root:[   82] Training loss: 103.64253863, Validation loss: 104.91022334, Gradient norm: 159.36708483
INFO:root:[   83] Training loss: 103.66878442, Validation loss: 105.42111364, Gradient norm: 165.52782070
INFO:root:[   84] Training loss: 103.62035397, Validation loss: 104.77104582, Gradient norm: 187.68638437
INFO:root:[   85] Training loss: 103.54669068, Validation loss: 105.01546715, Gradient norm: 170.03134832
INFO:root:[   86] Training loss: 103.52420253, Validation loss: 104.64988682, Gradient norm: 196.50734531
INFO:root:[   87] Training loss: 103.42982969, Validation loss: 104.48642573, Gradient norm: 170.36045273
INFO:root:[   88] Training loss: 103.40265514, Validation loss: 105.08511747, Gradient norm: 192.93492113
INFO:root:[   89] Training loss: 103.37872348, Validation loss: 104.54318527, Gradient norm: 188.07765424
INFO:root:[   90] Training loss: 103.32831864, Validation loss: 104.63491874, Gradient norm: 212.45729273
INFO:root:[   91] Training loss: 103.23189943, Validation loss: 104.47014697, Gradient norm: 179.00027371
INFO:root:[   92] Training loss: 103.19084397, Validation loss: 104.28670686, Gradient norm: 210.09566464
INFO:root:[   93] Training loss: 103.18237737, Validation loss: 104.63273936, Gradient norm: 210.77007131
INFO:root:[   94] Training loss: 103.06580569, Validation loss: 104.53695573, Gradient norm: 189.46626420
INFO:root:[   95] Training loss: 103.02440745, Validation loss: 104.41977876, Gradient norm: 213.17594132
INFO:root:[   96] Training loss: 102.92020592, Validation loss: 104.37308397, Gradient norm: 190.20177262
INFO:root:[   97] Training loss: 102.97230429, Validation loss: 104.87434598, Gradient norm: 209.65452026
INFO:root:[   98] Training loss: 102.90074124, Validation loss: 104.63259151, Gradient norm: 233.26150811
INFO:root:[   99] Training loss: 102.80864689, Validation loss: 104.05992258, Gradient norm: 214.66585097
INFO:root:[  100] Training loss: 102.89620573, Validation loss: 104.49876904, Gradient norm: 240.79529373
INFO:root:[  101] Training loss: 102.73394742, Validation loss: 104.72941510, Gradient norm: 209.57892039
INFO:root:[  102] Training loss: 102.73752101, Validation loss: 104.22754301, Gradient norm: 203.49872879
INFO:root:[  103] Training loss: 102.66531291, Validation loss: 104.74340347, Gradient norm: 216.99921653
INFO:root:[  104] Training loss: 102.55943676, Validation loss: 104.93022393, Gradient norm: 213.44257413
INFO:root:[  105] Training loss: 102.57814471, Validation loss: 104.41758544, Gradient norm: 240.37711082
INFO:root:[  106] Training loss: 102.56221670, Validation loss: 104.24932546, Gradient norm: 246.28936541
INFO:root:[  107] Training loss: 102.48398280, Validation loss: 104.59230357, Gradient norm: 213.03942660
INFO:root:[  108] Training loss: 102.42698622, Validation loss: 103.93896327, Gradient norm: 216.54611667
INFO:root:[  109] Training loss: 102.44908378, Validation loss: 104.01221755, Gradient norm: 219.61759860
INFO:root:[  110] Training loss: 102.30175768, Validation loss: 104.36759265, Gradient norm: 215.54088610
INFO:root:[  111] Training loss: 102.29999826, Validation loss: 104.52880886, Gradient norm: 227.77997161
INFO:root:[  112] Training loss: 102.33241096, Validation loss: 104.12381297, Gradient norm: 252.34254470
INFO:root:[  113] Training loss: 102.23551543, Validation loss: 103.82362997, Gradient norm: 237.69047345
INFO:root:[  114] Training loss: 102.21209872, Validation loss: 104.08728738, Gradient norm: 242.76193250
INFO:root:[  115] Training loss: 102.22346605, Validation loss: 104.88643015, Gradient norm: 262.91559945
INFO:root:[  116] Training loss: 102.17655783, Validation loss: 104.03583895, Gradient norm: 238.09353126
INFO:root:[  117] Training loss: 102.12227880, Validation loss: 104.04663586, Gradient norm: 239.42899242
INFO:root:[  118] Training loss: 102.16834333, Validation loss: 104.18020998, Gradient norm: 243.29935734
INFO:root:[  119] Training loss: 102.09971578, Validation loss: 104.18440457, Gradient norm: 267.04433607
INFO:root:[  120] Training loss: 102.03920503, Validation loss: 104.42546844, Gradient norm: 287.94293890
INFO:root:[  121] Training loss: 102.07024694, Validation loss: 104.18506438, Gradient norm: 266.02344202
INFO:root:[  122] Training loss: 101.95792105, Validation loss: 103.81772350, Gradient norm: 237.42673047
INFO:root:[  123] Training loss: 101.97680414, Validation loss: 103.80760061, Gradient norm: 241.44584412
INFO:root:[  124] Training loss: 101.93770363, Validation loss: 103.81967084, Gradient norm: 308.24374582
INFO:root:[  125] Training loss: 101.86466771, Validation loss: 104.05519709, Gradient norm: 256.65417305
INFO:root:[  126] Training loss: 101.81531228, Validation loss: 104.50317804, Gradient norm: 242.15174183
INFO:root:[  127] Training loss: 101.76563114, Validation loss: 104.23781060, Gradient norm: 279.29693433
INFO:root:[  128] Training loss: 101.78399584, Validation loss: 104.48012280, Gradient norm: 276.77260463
INFO:root:[  129] Training loss: 101.67063377, Validation loss: 103.93252458, Gradient norm: 244.84201545
INFO:root:[  130] Training loss: 101.70994257, Validation loss: 104.05838407, Gradient norm: 287.56564285
INFO:root:[  131] Training loss: 101.60683758, Validation loss: 103.99223354, Gradient norm: 270.04821008
INFO:root:[  132] Training loss: 101.70420736, Validation loss: 103.87910830, Gradient norm: 295.95342022
INFO:root:[  133] Training loss: 101.57932687, Validation loss: 103.76930026, Gradient norm: 255.33690810
INFO:root:[  134] Training loss: 101.56376479, Validation loss: 104.30617786, Gradient norm: 288.09289167
INFO:root:[  135] Training loss: 101.57483200, Validation loss: 103.83405672, Gradient norm: 265.14354426
INFO:root:[  136] Training loss: 101.50231785, Validation loss: 104.06249079, Gradient norm: 294.27693707
INFO:root:[  137] Training loss: 101.32396104, Validation loss: 104.14480880, Gradient norm: 266.50695797
INFO:root:[  138] Training loss: 101.41064041, Validation loss: 104.19307525, Gradient norm: 290.89933475
INFO:root:[  139] Training loss: 101.39048943, Validation loss: 103.53640142, Gradient norm: 288.97144324
INFO:root:[  140] Training loss: 101.35447990, Validation loss: 103.40197570, Gradient norm: 237.02762406
INFO:root:[  141] Training loss: 101.35395651, Validation loss: 103.76374028, Gradient norm: 302.82545454
INFO:root:[  142] Training loss: 101.37246171, Validation loss: 103.92916975, Gradient norm: 283.73331879
INFO:root:[  143] Training loss: 101.28498591, Validation loss: 103.83585831, Gradient norm: 276.39873891
INFO:root:[  144] Training loss: 101.24879948, Validation loss: 103.61866155, Gradient norm: 295.55291752
INFO:root:[  145] Training loss: 101.26697932, Validation loss: 104.02345618, Gradient norm: 265.92726911
INFO:root:[  146] Training loss: 101.19877246, Validation loss: 103.85555557, Gradient norm: 272.80254677
INFO:root:[  147] Training loss: 101.10963960, Validation loss: 104.04463906, Gradient norm: 274.15067477
INFO:root:[  148] Training loss: 101.21537754, Validation loss: 104.02845133, Gradient norm: 304.78787884
INFO:root:[  149] Training loss: 101.11328652, Validation loss: 103.64154868, Gradient norm: 290.41051052
INFO:root:EP 149: Early stopping
INFO:root:Training the model took 2651.721s.
INFO:root:Emptying the cuda cache took 0.048s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 143.60922
INFO:root:EnergyScoreTrain: 101.14791
INFO:root:CoverageTrain: 0.78358
INFO:root:IntervalWidthTrain: 8.17517
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.16999
INFO:root:EnergyScoreValidation: 103.66114
INFO:root:CoverageValidation: 0.77727
INFO:root:IntervalWidthValidation: 8.16928
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.46139
INFO:root:EnergyScoreTest: 103.87714
INFO:root:CoverageTest: 0.77601
INFO:root:IntervalWidthTest: 8.15178
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.43020981, Validation loss: 121.63912859, Gradient norm: 157.70896343
INFO:root:[    2] Training loss: 121.61200343, Validation loss: 121.62412262, Gradient norm: 143.09541677
INFO:root:[    3] Training loss: 121.42021503, Validation loss: 121.37338941, Gradient norm: 118.53249164
INFO:root:[    4] Training loss: 121.24649169, Validation loss: 120.88892443, Gradient norm: 138.08636253
INFO:root:[    5] Training loss: 120.84491946, Validation loss: 120.76877015, Gradient norm: 88.62494895
INFO:root:[    6] Training loss: 120.72318403, Validation loss: 120.68800933, Gradient norm: 93.93146146
INFO:root:[    7] Training loss: 120.60997293, Validation loss: 120.73808920, Gradient norm: 82.12208094
INFO:root:[    8] Training loss: 120.44569451, Validation loss: 120.35447088, Gradient norm: 61.26939709
INFO:root:[    9] Training loss: 120.19033564, Validation loss: 119.95992332, Gradient norm: 67.65600701
INFO:root:[   10] Training loss: 119.23823102, Validation loss: 118.49214067, Gradient norm: 56.44311332
INFO:root:[   11] Training loss: 117.92197810, Validation loss: 117.40962798, Gradient norm: 59.18241166
INFO:root:[   12] Training loss: 116.66350522, Validation loss: 116.15786822, Gradient norm: 55.02932639
INFO:root:[   13] Training loss: 115.63045887, Validation loss: 115.31597716, Gradient norm: 54.62668526
INFO:root:[   14] Training loss: 114.75451363, Validation loss: 114.63321633, Gradient norm: 48.66582906
INFO:root:[   15] Training loss: 114.01740258, Validation loss: 113.78716725, Gradient norm: 52.12209393
INFO:root:[   16] Training loss: 113.43569339, Validation loss: 113.19015029, Gradient norm: 68.53410462
INFO:root:[   17] Training loss: 112.87454926, Validation loss: 113.14640124, Gradient norm: 71.33499521
INFO:root:[   18] Training loss: 112.36081081, Validation loss: 112.18825768, Gradient norm: 61.28443183
INFO:root:[   19] Training loss: 111.90672559, Validation loss: 111.84091476, Gradient norm: 63.92551718
INFO:root:[   20] Training loss: 111.47328665, Validation loss: 111.62807280, Gradient norm: 76.24568720
INFO:root:[   21] Training loss: 111.16846844, Validation loss: 111.34213020, Gradient norm: 75.33004971
INFO:root:[   22] Training loss: 110.77287630, Validation loss: 110.93151724, Gradient norm: 68.56873941
INFO:root:[   23] Training loss: 110.48907957, Validation loss: 110.57095968, Gradient norm: 71.15062886
INFO:root:[   24] Training loss: 110.24618645, Validation loss: 110.35389157, Gradient norm: 70.49689178
INFO:root:[   25] Training loss: 110.00315074, Validation loss: 110.21799469, Gradient norm: 71.25862925
INFO:root:[   26] Training loss: 109.79297840, Validation loss: 109.97045925, Gradient norm: 89.15995089
INFO:root:[   27] Training loss: 109.56220293, Validation loss: 109.73018357, Gradient norm: 87.28154917
INFO:root:[   28] Training loss: 109.26116565, Validation loss: 109.71765216, Gradient norm: 63.36345338
INFO:root:[   29] Training loss: 109.17807817, Validation loss: 109.31274072, Gradient norm: 103.75984010
INFO:root:[   30] Training loss: 108.92481191, Validation loss: 109.29726463, Gradient norm: 70.23011288
INFO:root:[   31] Training loss: 108.78952236, Validation loss: 109.29442728, Gradient norm: 81.61729188
INFO:root:[   32] Training loss: 108.59852965, Validation loss: 109.16841099, Gradient norm: 98.61962456
INFO:root:[   33] Training loss: 108.43504502, Validation loss: 108.62534595, Gradient norm: 93.96338202
INFO:root:[   34] Training loss: 108.28940994, Validation loss: 108.55320293, Gradient norm: 91.87070009
INFO:root:[   35] Training loss: 108.19584602, Validation loss: 108.42399781, Gradient norm: 105.66592229
INFO:root:[   36] Training loss: 108.01843890, Validation loss: 108.31791792, Gradient norm: 93.54552955
INFO:root:[   37] Training loss: 107.88836150, Validation loss: 108.19884701, Gradient norm: 101.24528719
INFO:root:[   38] Training loss: 107.78674593, Validation loss: 108.33011285, Gradient norm: 103.99536169
INFO:root:[   39] Training loss: 107.66792223, Validation loss: 108.06992945, Gradient norm: 105.40933196
INFO:root:[   40] Training loss: 107.56263024, Validation loss: 107.84466711, Gradient norm: 112.51524947
INFO:root:[   41] Training loss: 107.35871752, Validation loss: 107.76511383, Gradient norm: 127.75283162
INFO:root:[   42] Training loss: 107.24800974, Validation loss: 107.77601860, Gradient norm: 98.69546623
INFO:root:[   43] Training loss: 107.21437336, Validation loss: 107.78497604, Gradient norm: 129.50060574
INFO:root:[   44] Training loss: 107.14609251, Validation loss: 107.58586647, Gradient norm: 120.21565489
INFO:root:[   45] Training loss: 107.01643183, Validation loss: 107.34757864, Gradient norm: 140.92380985
INFO:root:[   46] Training loss: 106.92656174, Validation loss: 107.32996132, Gradient norm: 121.50138475
INFO:root:[   47] Training loss: 106.78148064, Validation loss: 107.17989586, Gradient norm: 121.11244493
INFO:root:[   48] Training loss: 106.73503964, Validation loss: 107.10371373, Gradient norm: 169.09661311
INFO:root:[   49] Training loss: 106.58085443, Validation loss: 107.37811043, Gradient norm: 145.51424454
INFO:root:[   50] Training loss: 106.52010258, Validation loss: 107.11746689, Gradient norm: 141.72889570
INFO:root:[   51] Training loss: 106.45743304, Validation loss: 106.94192742, Gradient norm: 172.74246508
INFO:root:[   52] Training loss: 106.33572644, Validation loss: 107.03493815, Gradient norm: 146.01866870
INFO:root:[   53] Training loss: 106.18055820, Validation loss: 106.67517037, Gradient norm: 146.94436941
INFO:root:[   54] Training loss: 106.10205530, Validation loss: 106.88150051, Gradient norm: 153.00949970
INFO:root:[   55] Training loss: 106.17397241, Validation loss: 106.76209548, Gradient norm: 216.62014873
INFO:root:[   56] Training loss: 105.97548655, Validation loss: 106.56717050, Gradient norm: 141.65261806
INFO:root:[   57] Training loss: 105.95554163, Validation loss: 107.67118914, Gradient norm: 189.49405665
INFO:root:[   58] Training loss: 105.85034396, Validation loss: 106.85848762, Gradient norm: 181.34419437
INFO:root:[   59] Training loss: 105.72688908, Validation loss: 106.57684010, Gradient norm: 169.69651050
INFO:root:[   60] Training loss: 105.67443625, Validation loss: 106.20033027, Gradient norm: 222.47686009
INFO:root:[   61] Training loss: 105.71670937, Validation loss: 106.57401460, Gradient norm: 206.44737955
INFO:root:[   62] Training loss: 105.57831114, Validation loss: 106.19146676, Gradient norm: 195.08216489
INFO:root:[   63] Training loss: 105.52828757, Validation loss: 106.09428616, Gradient norm: 216.27018798
INFO:root:[   64] Training loss: 105.44564975, Validation loss: 105.99945516, Gradient norm: 224.03500360
INFO:root:[   65] Training loss: 105.44210923, Validation loss: 105.88060313, Gradient norm: 226.41882500
INFO:root:[   66] Training loss: 105.31517191, Validation loss: 105.88385746, Gradient norm: 187.98295088
INFO:root:[   67] Training loss: 105.29429836, Validation loss: 106.05893181, Gradient norm: 234.83855903
INFO:root:[   68] Training loss: 105.18171786, Validation loss: 106.20988412, Gradient norm: 212.84132797
INFO:root:[   69] Training loss: 105.17433767, Validation loss: 105.92246694, Gradient norm: 251.46941691
INFO:root:[   70] Training loss: 105.05192134, Validation loss: 105.93323833, Gradient norm: 234.26670498
INFO:root:[   71] Training loss: 105.00566061, Validation loss: 105.70860001, Gradient norm: 219.73240648
INFO:root:[   72] Training loss: 104.97020559, Validation loss: 105.76025838, Gradient norm: 271.78930780
INFO:root:[   73] Training loss: 104.95120685, Validation loss: 105.54873000, Gradient norm: 272.11070368
INFO:root:[   74] Training loss: 104.90647794, Validation loss: 105.36055098, Gradient norm: 282.94403606
INFO:root:[   75] Training loss: 104.78594289, Validation loss: 105.38737225, Gradient norm: 234.85360680
INFO:root:[   76] Training loss: 104.88634234, Validation loss: 105.62222921, Gradient norm: 321.17947567
INFO:root:[   77] Training loss: 104.74600929, Validation loss: 105.36821642, Gradient norm: 239.61004576
INFO:root:[   78] Training loss: 104.74019231, Validation loss: 105.21320685, Gradient norm: 293.69939969
INFO:root:[   79] Training loss: 104.66569033, Validation loss: 105.40599481, Gradient norm: 285.87892819
INFO:root:[   80] Training loss: 104.54706627, Validation loss: 105.73730232, Gradient norm: 261.22008933
INFO:root:[   81] Training loss: 104.58987447, Validation loss: 105.43218731, Gradient norm: 313.76087672
INFO:root:[   82] Training loss: 104.53121334, Validation loss: 105.35900037, Gradient norm: 314.49269162
INFO:root:[   83] Training loss: 104.46883797, Validation loss: 105.46808045, Gradient norm: 283.42502954
INFO:root:[   84] Training loss: 104.50479632, Validation loss: 105.21074782, Gradient norm: 327.23180422
INFO:root:[   85] Training loss: 104.35214301, Validation loss: 104.88067837, Gradient norm: 323.32402791
INFO:root:[   86] Training loss: 104.31646452, Validation loss: 105.04785814, Gradient norm: 274.89372985
INFO:root:[   87] Training loss: 104.21945764, Validation loss: 104.83776882, Gradient norm: 311.98111083
INFO:root:[   88] Training loss: 104.27783743, Validation loss: 105.07499642, Gradient norm: 322.13282479
INFO:root:[   89] Training loss: 104.17462273, Validation loss: 105.10018553, Gradient norm: 330.20637824
INFO:root:[   90] Training loss: 104.05515006, Validation loss: 105.05370620, Gradient norm: 294.79352439
INFO:root:[   91] Training loss: 104.10042005, Validation loss: 105.19752476, Gradient norm: 341.43010016
INFO:root:[   92] Training loss: 104.13370426, Validation loss: 105.57719079, Gradient norm: 347.79461501
INFO:root:[   93] Training loss: 104.01829610, Validation loss: 105.23205461, Gradient norm: 337.87725626
INFO:root:[   94] Training loss: 103.96244306, Validation loss: 104.84056486, Gradient norm: 286.73631642
INFO:root:[   95] Training loss: 103.91043341, Validation loss: 104.72185332, Gradient norm: 320.73450056
INFO:root:[   96] Training loss: 103.98055254, Validation loss: 104.71740591, Gradient norm: 325.77300133
INFO:root:[   97] Training loss: 103.88213497, Validation loss: 104.91838311, Gradient norm: 324.01346243
INFO:root:[   98] Training loss: 103.92328907, Validation loss: 104.67293364, Gradient norm: 355.16195207
INFO:root:[   99] Training loss: 103.78090580, Validation loss: 104.75292890, Gradient norm: 336.58332202
INFO:root:[  100] Training loss: 103.78793632, Validation loss: 105.04663402, Gradient norm: 340.99176057
INFO:root:[  101] Training loss: 103.69965781, Validation loss: 104.93109447, Gradient norm: 325.86530586
INFO:root:[  102] Training loss: 103.63698409, Validation loss: 104.79602024, Gradient norm: 296.79110185
INFO:root:[  103] Training loss: 103.70007945, Validation loss: 104.46167045, Gradient norm: 394.72581742
INFO:root:[  104] Training loss: 103.63155230, Validation loss: 104.79073755, Gradient norm: 341.36848517
INFO:root:[  105] Training loss: 103.54203628, Validation loss: 104.42728082, Gradient norm: 344.57788543
INFO:root:[  106] Training loss: 103.61400098, Validation loss: 104.80121297, Gradient norm: 366.22666729
INFO:root:[  107] Training loss: 103.57438194, Validation loss: 104.50265661, Gradient norm: 345.03312341
INFO:root:[  108] Training loss: 103.39465474, Validation loss: 105.55075968, Gradient norm: 297.80494589
INFO:root:[  109] Training loss: 103.51520957, Validation loss: 105.06867455, Gradient norm: 349.34371478
INFO:root:[  110] Training loss: 103.41238174, Validation loss: 105.62223947, Gradient norm: 341.70837894
INFO:root:[  111] Training loss: 103.34502654, Validation loss: 104.29986993, Gradient norm: 317.76302634
INFO:root:[  112] Training loss: 103.42647114, Validation loss: 104.34315438, Gradient norm: 380.20071590
INFO:root:[  113] Training loss: 103.32046110, Validation loss: 104.35159328, Gradient norm: 367.33846863
INFO:root:[  114] Training loss: 103.31315363, Validation loss: 104.59442639, Gradient norm: 363.24844342
INFO:root:[  115] Training loss: 103.16476380, Validation loss: 104.33343979, Gradient norm: 349.32072860
INFO:root:[  116] Training loss: 103.25246456, Validation loss: 104.33986795, Gradient norm: 362.60100579
INFO:root:[  117] Training loss: 103.19098157, Validation loss: 104.61267800, Gradient norm: 394.29508265
INFO:root:[  118] Training loss: 103.24122856, Validation loss: 104.06451495, Gradient norm: 366.58100016
INFO:root:[  119] Training loss: 103.14901882, Validation loss: 104.46866660, Gradient norm: 364.48800285
INFO:root:[  120] Training loss: 103.02302187, Validation loss: 104.28341280, Gradient norm: 360.32771135
INFO:root:[  121] Training loss: 103.07535539, Validation loss: 104.13406267, Gradient norm: 409.84105679
INFO:root:[  122] Training loss: 102.98714899, Validation loss: 104.24183129, Gradient norm: 355.28935219
INFO:root:[  123] Training loss: 103.00245221, Validation loss: 104.33459367, Gradient norm: 365.65858647
INFO:root:[  124] Training loss: 103.02470303, Validation loss: 104.53714936, Gradient norm: 378.08692665
INFO:root:[  125] Training loss: 103.02343838, Validation loss: 104.01744422, Gradient norm: 407.51492920
INFO:root:[  126] Training loss: 102.96394564, Validation loss: 104.22254891, Gradient norm: 362.20327613
INFO:root:[  127] Training loss: 102.91104518, Validation loss: 103.89726152, Gradient norm: 344.38875932
INFO:root:[  128] Training loss: 102.96480655, Validation loss: 104.80347416, Gradient norm: 406.70690176
INFO:root:[  129] Training loss: 102.91622000, Validation loss: 108.40837913, Gradient norm: 402.82400876
INFO:root:[  130] Training loss: 102.90328304, Validation loss: 104.33281603, Gradient norm: 391.50002800
INFO:root:[  131] Training loss: 102.83820741, Validation loss: 103.78636433, Gradient norm: 373.80166829
INFO:root:[  132] Training loss: 102.76687021, Validation loss: 104.08690985, Gradient norm: 351.70179369
INFO:root:[  133] Training loss: 102.74751937, Validation loss: 104.82045299, Gradient norm: 368.89542116
INFO:root:[  134] Training loss: 102.74233394, Validation loss: 104.22416056, Gradient norm: 405.84397879
INFO:root:[  135] Training loss: 102.76583869, Validation loss: 103.81529262, Gradient norm: 384.31323517
INFO:root:[  136] Training loss: 102.72551390, Validation loss: 104.40716395, Gradient norm: 392.72899094
INFO:root:[  137] Training loss: 102.71873717, Validation loss: 103.96114086, Gradient norm: 416.41654209
INFO:root:[  138] Training loss: 102.61338840, Validation loss: 104.26452821, Gradient norm: 378.69777709
INFO:root:[  139] Training loss: 102.66179542, Validation loss: 104.07289518, Gradient norm: 415.56003347
INFO:root:[  140] Training loss: 102.62445244, Validation loss: 103.87490924, Gradient norm: 358.96730047
INFO:root:EP 140: Early stopping
INFO:root:Training the model took 2502.356s.
INFO:root:Emptying the cuda cache took 0.051s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 145.54378
INFO:root:EnergyScoreTrain: 102.50966
INFO:root:CoverageTrain: 0.74654
INFO:root:IntervalWidthTrain: 8.16636
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.60832
INFO:root:EnergyScoreValidation: 103.95775
INFO:root:CoverageValidation: 0.74281
INFO:root:IntervalWidthValidation: 8.16176
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.72433
INFO:root:EnergyScoreTest: 104.05077
INFO:root:CoverageTest: 0.74171
INFO:root:IntervalWidthTest: 8.14642
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.32653532, Validation loss: 121.47527234, Gradient norm: 100.58461483
INFO:root:[    2] Training loss: 121.58725070, Validation loss: 121.42499148, Gradient norm: 118.64234559
INFO:root:[    3] Training loss: 121.42403830, Validation loss: 121.56793555, Gradient norm: 69.34244906
INFO:root:[    4] Training loss: 121.37062255, Validation loss: 122.02164170, Gradient norm: 78.59666795
INFO:root:[    5] Training loss: 121.43473168, Validation loss: 121.37507524, Gradient norm: 81.05939011
INFO:root:[    6] Training loss: 121.16715139, Validation loss: 121.20857397, Gradient norm: 58.28399399
INFO:root:[    7] Training loss: 120.87851857, Validation loss: 120.73737940, Gradient norm: 57.77742352
INFO:root:[    8] Training loss: 120.74429038, Validation loss: 120.82026436, Gradient norm: 51.52950042
INFO:root:[    9] Training loss: 120.60327088, Validation loss: 120.69868995, Gradient norm: 52.83683863
INFO:root:[   10] Training loss: 120.53405640, Validation loss: 120.53177406, Gradient norm: 42.11590272
INFO:root:[   11] Training loss: 120.39329245, Validation loss: 120.37581608, Gradient norm: 46.85859753
INFO:root:[   12] Training loss: 120.20194035, Validation loss: 120.05401822, Gradient norm: 50.01815880
INFO:root:[   13] Training loss: 119.59736654, Validation loss: 119.20251333, Gradient norm: 34.47974794
INFO:root:[   14] Training loss: 118.51711273, Validation loss: 118.36491605, Gradient norm: 40.63344353
INFO:root:[   15] Training loss: 117.54644012, Validation loss: 117.12103771, Gradient norm: 44.37507537
INFO:root:[   16] Training loss: 116.63026928, Validation loss: 116.26837605, Gradient norm: 40.13333284
INFO:root:[   17] Training loss: 115.85337006, Validation loss: 115.61670132, Gradient norm: 41.25972800
INFO:root:[   18] Training loss: 115.26318785, Validation loss: 115.19048441, Gradient norm: 40.17586679
INFO:root:[   19] Training loss: 114.75419812, Validation loss: 114.48536630, Gradient norm: 41.77942677
INFO:root:[   20] Training loss: 114.27215745, Validation loss: 114.10897511, Gradient norm: 52.78996989
INFO:root:[   21] Training loss: 113.83560322, Validation loss: 113.54756717, Gradient norm: 57.76377546
INFO:root:[   22] Training loss: 113.30457914, Validation loss: 113.17598225, Gradient norm: 52.62292350
INFO:root:[   23] Training loss: 112.92561226, Validation loss: 112.62140919, Gradient norm: 47.39854789
INFO:root:[   24] Training loss: 112.45531416, Validation loss: 112.68265849, Gradient norm: 57.96134697
INFO:root:[   25] Training loss: 112.09129894, Validation loss: 112.44923375, Gradient norm: 65.66071676
INFO:root:[   26] Training loss: 111.65441678, Validation loss: 111.63391140, Gradient norm: 67.48092483
INFO:root:[   27] Training loss: 111.38521103, Validation loss: 111.34044595, Gradient norm: 63.09087777
INFO:root:[   28] Training loss: 111.06086717, Validation loss: 111.03125158, Gradient norm: 69.79656216
INFO:root:[   29] Training loss: 110.80335668, Validation loss: 110.79753613, Gradient norm: 88.24508186
INFO:root:[   30] Training loss: 110.49427397, Validation loss: 110.61787625, Gradient norm: 67.70124046
INFO:root:[   31] Training loss: 110.17368904, Validation loss: 110.29217819, Gradient norm: 70.00177965
INFO:root:[   32] Training loss: 110.12454325, Validation loss: 109.99320168, Gradient norm: 90.30408257
INFO:root:[   33] Training loss: 109.78949893, Validation loss: 109.83558023, Gradient norm: 80.17263180
INFO:root:[   34] Training loss: 109.59414200, Validation loss: 110.22833647, Gradient norm: 80.00703913
INFO:root:[   35] Training loss: 109.44844757, Validation loss: 109.59109576, Gradient norm: 83.55053551
INFO:root:[   36] Training loss: 109.25904049, Validation loss: 109.31389960, Gradient norm: 94.50572992
INFO:root:[   37] Training loss: 108.99350475, Validation loss: 109.36685891, Gradient norm: 80.51107286
INFO:root:[   38] Training loss: 108.89361896, Validation loss: 109.14621892, Gradient norm: 84.55330585
INFO:root:[   39] Training loss: 108.72947436, Validation loss: 108.74701033, Gradient norm: 92.15977478
INFO:root:[   40] Training loss: 108.56429426, Validation loss: 108.75296047, Gradient norm: 106.73460372
INFO:root:[   41] Training loss: 108.38054812, Validation loss: 108.44499417, Gradient norm: 95.38307011
INFO:root:[   42] Training loss: 108.21903202, Validation loss: 108.48848146, Gradient norm: 93.75117724
INFO:root:[   43] Training loss: 108.06309118, Validation loss: 108.27508966, Gradient norm: 97.93728586
INFO:root:[   44] Training loss: 107.98618978, Validation loss: 108.16986531, Gradient norm: 105.60221702
INFO:root:[   45] Training loss: 107.83084194, Validation loss: 108.18866098, Gradient norm: 111.41874999
INFO:root:[   46] Training loss: 107.67711990, Validation loss: 108.16650575, Gradient norm: 117.60064932
INFO:root:[   47] Training loss: 107.59282110, Validation loss: 107.66425376, Gradient norm: 125.26898236
INFO:root:[   48] Training loss: 107.43982372, Validation loss: 107.70500815, Gradient norm: 114.37127486
INFO:root:[   49] Training loss: 107.34546006, Validation loss: 108.87859713, Gradient norm: 130.29382189
INFO:root:[   50] Training loss: 107.34103225, Validation loss: 107.67606696, Gradient norm: 146.68129750
INFO:root:[   51] Training loss: 107.14073782, Validation loss: 107.37771501, Gradient norm: 124.43291447
INFO:root:[   52] Training loss: 106.97018635, Validation loss: 107.25007919, Gradient norm: 140.99868593
INFO:root:[   53] Training loss: 106.93836820, Validation loss: 107.76299759, Gradient norm: 182.36828386
INFO:root:[   54] Training loss: 106.81550099, Validation loss: 107.11022265, Gradient norm: 156.83424957
INFO:root:[   55] Training loss: 106.69578593, Validation loss: 107.59904427, Gradient norm: 156.27764629
INFO:root:[   56] Training loss: 106.63347045, Validation loss: 106.74634578, Gradient norm: 185.63162535
INFO:root:[   57] Training loss: 106.60028549, Validation loss: 106.80888419, Gradient norm: 183.87654198
INFO:root:[   58] Training loss: 106.41205550, Validation loss: 106.63737672, Gradient norm: 171.26241209
INFO:root:[   59] Training loss: 106.38873824, Validation loss: 106.66219593, Gradient norm: 218.74813144
INFO:root:[   60] Training loss: 106.30825691, Validation loss: 106.56308799, Gradient norm: 215.22803046
INFO:root:[   61] Training loss: 106.16178854, Validation loss: 106.44011583, Gradient norm: 186.27958819
INFO:root:[   62] Training loss: 106.16916521, Validation loss: 106.37079857, Gradient norm: 255.98042822
INFO:root:[   63] Training loss: 106.05807191, Validation loss: 106.36718461, Gradient norm: 184.62133560
INFO:root:[   64] Training loss: 105.98661757, Validation loss: 106.44155936, Gradient norm: 227.05123232
INFO:root:[   65] Training loss: 105.89620580, Validation loss: 106.26573418, Gradient norm: 251.90405541
INFO:root:[   66] Training loss: 105.85420544, Validation loss: 106.24925705, Gradient norm: 257.59018916
INFO:root:[   67] Training loss: 105.81767671, Validation loss: 106.02741715, Gradient norm: 276.72210272
INFO:root:[   68] Training loss: 105.75074903, Validation loss: 106.87352121, Gradient norm: 249.89279279
INFO:root:[   69] Training loss: 105.69321361, Validation loss: 105.90331005, Gradient norm: 298.36385703
INFO:root:[   70] Training loss: 105.62173381, Validation loss: 106.16252057, Gradient norm: 260.61547984
INFO:root:[   71] Training loss: 105.63007935, Validation loss: 105.70935558, Gradient norm: 318.16893646
INFO:root:[   72] Training loss: 105.49151692, Validation loss: 106.12996279, Gradient norm: 278.18943123
INFO:root:[   73] Training loss: 105.57781888, Validation loss: 106.00476285, Gradient norm: 335.32043803
INFO:root:[   74] Training loss: 105.44319133, Validation loss: 105.77458112, Gradient norm: 338.13057239
INFO:root:[   75] Training loss: 105.43230188, Validation loss: 105.68771257, Gradient norm: 369.78696174
INFO:root:[   76] Training loss: 105.33595877, Validation loss: 105.82698112, Gradient norm: 316.06937919
INFO:root:[   77] Training loss: 105.34288133, Validation loss: 106.00032175, Gradient norm: 334.33944374
INFO:root:[   78] Training loss: 105.37046713, Validation loss: 105.82055585, Gradient norm: 354.26864218
INFO:root:[   79] Training loss: 105.25764506, Validation loss: 105.53347910, Gradient norm: 365.27235029
INFO:root:[   80] Training loss: 105.15781362, Validation loss: 105.69932451, Gradient norm: 333.51170308
INFO:root:[   81] Training loss: 105.21494219, Validation loss: 105.38535335, Gradient norm: 366.14239608
INFO:root:[   82] Training loss: 105.13789152, Validation loss: 105.34598489, Gradient norm: 359.65822029
INFO:root:[   83] Training loss: 105.12113669, Validation loss: 105.32429373, Gradient norm: 395.01980226
INFO:root:[   84] Training loss: 105.02434958, Validation loss: 105.20633329, Gradient norm: 348.02156711
INFO:root:[   85] Training loss: 105.04602051, Validation loss: 105.59486863, Gradient norm: 389.54695030
INFO:root:[   86] Training loss: 105.07307164, Validation loss: 105.46999359, Gradient norm: 416.43528418
INFO:root:[   87] Training loss: 105.09148616, Validation loss: 105.04552539, Gradient norm: 401.46434777
INFO:root:[   88] Training loss: 105.00964268, Validation loss: 105.70967655, Gradient norm: 412.45021988
INFO:root:[   89] Training loss: 104.99083879, Validation loss: 105.41894163, Gradient norm: 436.94826288
INFO:root:[   90] Training loss: 104.93913418, Validation loss: 105.18410150, Gradient norm: 366.46413794
INFO:root:[   91] Training loss: 105.03300402, Validation loss: 105.31681376, Gradient norm: 440.72193359
INFO:root:[   92] Training loss: 104.97337429, Validation loss: 105.43132019, Gradient norm: 426.60601943
INFO:root:[   93] Training loss: 104.97314750, Validation loss: 105.27499811, Gradient norm: 434.52940423
INFO:root:[   94] Training loss: 104.87116984, Validation loss: 105.07722684, Gradient norm: 444.06075273
INFO:root:[   95] Training loss: 104.89030403, Validation loss: 105.24316853, Gradient norm: 376.08653962
INFO:root:[   96] Training loss: 104.93067696, Validation loss: 105.12926746, Gradient norm: 451.97849375
INFO:root:EP 96: Early stopping
INFO:root:Training the model took 1699.202s.
INFO:root:Emptying the cuda cache took 0.051s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 148.63463
INFO:root:EnergyScoreTrain: 104.73463
INFO:root:CoverageTrain: 0.67543
INFO:root:IntervalWidthTrain: 7.94869
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 149.29175
INFO:root:EnergyScoreValidation: 105.18392
INFO:root:CoverageValidation: 0.67477
INFO:root:IntervalWidthValidation: 7.95775
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 149.56321
INFO:root:EnergyScoreTest: 105.38731
INFO:root:CoverageTest: 0.67316
INFO:root:IntervalWidthTest: 7.93409
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.37553818, Validation loss: 121.31231611, Gradient norm: 36.51106766
INFO:root:[    2] Training loss: 121.25337071, Validation loss: 120.96154733, Gradient norm: 6.18254697
INFO:root:[    3] Training loss: 120.76771680, Validation loss: 120.46081438, Gradient norm: 6.62655066
INFO:root:[    4] Training loss: 120.26110577, Validation loss: 119.45336230, Gradient norm: 6.83149462
INFO:root:[    5] Training loss: 118.11932846, Validation loss: 116.64196277, Gradient norm: 11.93345263
INFO:root:[    6] Training loss: 115.98460692, Validation loss: 114.79296244, Gradient norm: 14.87384857
INFO:root:[    7] Training loss: 114.43448835, Validation loss: 113.57373336, Gradient norm: 16.96256357
INFO:root:[    8] Training loss: 113.35232942, Validation loss: 112.20387400, Gradient norm: 18.03984494
INFO:root:[    9] Training loss: 112.48006095, Validation loss: 111.62030766, Gradient norm: 17.78411830
INFO:root:[   10] Training loss: 111.80408863, Validation loss: 110.92304177, Gradient norm: 20.38705719
INFO:root:[   11] Training loss: 111.27416378, Validation loss: 110.76838395, Gradient norm: 22.52141006
INFO:root:[   12] Training loss: 110.82890374, Validation loss: 110.16657652, Gradient norm: 23.37863555
INFO:root:[   13] Training loss: 110.40648172, Validation loss: 109.72541441, Gradient norm: 27.61529230
INFO:root:[   14] Training loss: 109.93969740, Validation loss: 109.28901067, Gradient norm: 27.88924857
INFO:root:[   15] Training loss: 109.62060554, Validation loss: 109.21746247, Gradient norm: 26.13160642
INFO:root:[   16] Training loss: 109.31310927, Validation loss: 108.63988732, Gradient norm: 32.98102975
INFO:root:[   17] Training loss: 109.01498757, Validation loss: 108.50309832, Gradient norm: 32.95698736
INFO:root:[   18] Training loss: 108.79137245, Validation loss: 108.18965649, Gradient norm: 37.36672207
INFO:root:[   19] Training loss: 108.50101768, Validation loss: 108.00727160, Gradient norm: 32.99624356
INFO:root:[   20] Training loss: 108.33076767, Validation loss: 107.73653070, Gradient norm: 40.26457654
INFO:root:[   21] Training loss: 108.09456000, Validation loss: 107.58855175, Gradient norm: 36.77882023
INFO:root:[   22] Training loss: 107.95268459, Validation loss: 107.34416620, Gradient norm: 42.74955771
INFO:root:[   23] Training loss: 107.74096815, Validation loss: 107.28705702, Gradient norm: 42.15369713
INFO:root:[   24] Training loss: 107.56994197, Validation loss: 107.10718799, Gradient norm: 46.78485232
INFO:root:[   25] Training loss: 107.36654670, Validation loss: 107.01945838, Gradient norm: 45.23116900
INFO:root:[   26] Training loss: 107.28274388, Validation loss: 106.78953052, Gradient norm: 57.17143317
INFO:root:[   27] Training loss: 107.07119170, Validation loss: 106.67232014, Gradient norm: 50.67963611
INFO:root:[   28] Training loss: 106.94009123, Validation loss: 106.49444554, Gradient norm: 58.59359655
INFO:root:[   29] Training loss: 106.80453701, Validation loss: 106.51170928, Gradient norm: 64.69663355
INFO:root:[   30] Training loss: 106.62703104, Validation loss: 106.38312083, Gradient norm: 64.87623759
INFO:root:[   31] Training loss: 106.57310958, Validation loss: 105.95464088, Gradient norm: 66.51436783
INFO:root:[   32] Training loss: 106.39125237, Validation loss: 105.87685763, Gradient norm: 71.36761532
INFO:root:[   33] Training loss: 106.23656956, Validation loss: 106.00286208, Gradient norm: 75.12045239
INFO:root:[   34] Training loss: 106.17379869, Validation loss: 106.11599889, Gradient norm: 78.10316639
INFO:root:[   35] Training loss: 106.06228928, Validation loss: 105.82626895, Gradient norm: 80.98445157
INFO:root:[   36] Training loss: 105.93317373, Validation loss: 105.72683374, Gradient norm: 84.63518677
INFO:root:[   37] Training loss: 105.89218754, Validation loss: 105.59775964, Gradient norm: 92.98102873
INFO:root:[   38] Training loss: 105.71771369, Validation loss: 105.39802551, Gradient norm: 92.71781032
INFO:root:[   39] Training loss: 105.58299255, Validation loss: 105.39175099, Gradient norm: 88.14233976
INFO:root:[   40] Training loss: 105.56260290, Validation loss: 105.46793313, Gradient norm: 100.53083022
INFO:root:[   41] Training loss: 105.45625866, Validation loss: 105.62652956, Gradient norm: 97.47668437
INFO:root:[   42] Training loss: 105.35600159, Validation loss: 105.33718872, Gradient norm: 115.89022302
INFO:root:[   43] Training loss: 105.24771476, Validation loss: 105.25160059, Gradient norm: 91.28479638
INFO:root:[   44] Training loss: 105.21851457, Validation loss: 104.95533752, Gradient norm: 122.20270229
INFO:root:[   45] Training loss: 105.10200217, Validation loss: 104.88057788, Gradient norm: 114.04670774
INFO:root:[   46] Training loss: 104.99915064, Validation loss: 105.22577904, Gradient norm: 119.50429236
INFO:root:[   47] Training loss: 104.97592413, Validation loss: 104.85317967, Gradient norm: 125.62889210
INFO:root:[   48] Training loss: 104.83638122, Validation loss: 105.17819793, Gradient norm: 127.44202985
INFO:root:[   49] Training loss: 104.84498630, Validation loss: 105.03767921, Gradient norm: 145.68678785
INFO:root:[   50] Training loss: 104.68746874, Validation loss: 104.97939800, Gradient norm: 122.97456273
INFO:root:[   51] Training loss: 104.63706065, Validation loss: 104.68262429, Gradient norm: 148.84700692
INFO:root:[   52] Training loss: 104.61654461, Validation loss: 104.62505051, Gradient norm: 153.00971372
INFO:root:[   53] Training loss: 104.49573672, Validation loss: 104.94799936, Gradient norm: 155.47276502
INFO:root:[   54] Training loss: 104.38846554, Validation loss: 104.69544720, Gradient norm: 137.82649330
INFO:root:[   55] Training loss: 104.39314898, Validation loss: 104.47066840, Gradient norm: 165.07769699
INFO:root:[   56] Training loss: 104.31131312, Validation loss: 104.60701094, Gradient norm: 175.26971038
INFO:root:[   57] Training loss: 104.16856641, Validation loss: 104.64013724, Gradient norm: 146.49548583
INFO:root:[   58] Training loss: 104.16001750, Validation loss: 104.51762522, Gradient norm: 163.16632825
INFO:root:[   59] Training loss: 104.13835556, Validation loss: 104.48896395, Gradient norm: 178.97448261
INFO:root:[   60] Training loss: 104.06353557, Validation loss: 104.63177332, Gradient norm: 163.20393771
INFO:root:[   61] Training loss: 103.94314400, Validation loss: 104.31467333, Gradient norm: 144.12669000
INFO:root:[   62] Training loss: 103.90693530, Validation loss: 104.48949827, Gradient norm: 176.91870825
INFO:root:[   63] Training loss: 103.83699758, Validation loss: 104.38368146, Gradient norm: 188.26699630
INFO:root:[   64] Training loss: 103.86003363, Validation loss: 104.46911568, Gradient norm: 170.55018501
INFO:root:[   65] Training loss: 103.71753915, Validation loss: 104.08622216, Gradient norm: 178.12432244
INFO:root:[   66] Training loss: 103.67860163, Validation loss: 104.40584354, Gradient norm: 185.01053726
INFO:root:[   67] Training loss: 103.67001593, Validation loss: 104.28867051, Gradient norm: 175.08179739
INFO:root:[   68] Training loss: 103.59485086, Validation loss: 104.34639214, Gradient norm: 204.02168649
INFO:root:[   69] Training loss: 103.48181665, Validation loss: 104.06041060, Gradient norm: 179.41166228
INFO:root:[   70] Training loss: 103.48414335, Validation loss: 104.14358836, Gradient norm: 208.09223612
INFO:root:[   71] Training loss: 103.39270438, Validation loss: 103.98172628, Gradient norm: 204.15687768
INFO:root:[   72] Training loss: 103.37181051, Validation loss: 104.18074956, Gradient norm: 233.78167021
INFO:root:[   73] Training loss: 103.35304625, Validation loss: 104.27065277, Gradient norm: 229.24275659
INFO:root:[   74] Training loss: 103.19068132, Validation loss: 104.04728251, Gradient norm: 188.23806873
INFO:root:[   75] Training loss: 103.18049716, Validation loss: 104.17483310, Gradient norm: 205.28638589
INFO:root:[   76] Training loss: 103.11688624, Validation loss: 104.34948099, Gradient norm: 206.36109393
INFO:root:[   77] Training loss: 103.10193897, Validation loss: 104.00174476, Gradient norm: 231.90352257
INFO:root:[   78] Training loss: 103.00921253, Validation loss: 103.92923263, Gradient norm: 214.09119044
INFO:root:[   79] Training loss: 102.96977909, Validation loss: 104.32303804, Gradient norm: 241.05377399
INFO:root:[   80] Training loss: 102.94198332, Validation loss: 104.46265911, Gradient norm: 230.73025454
INFO:root:[   81] Training loss: 102.85343602, Validation loss: 103.64478723, Gradient norm: 232.83532844
INFO:root:[   82] Training loss: 102.83413399, Validation loss: 103.98265628, Gradient norm: 236.65992953
INFO:root:[   83] Training loss: 102.84402925, Validation loss: 103.84280948, Gradient norm: 240.48269782
INFO:root:[   84] Training loss: 102.70707190, Validation loss: 104.08772041, Gradient norm: 235.62940962
INFO:root:[   85] Training loss: 102.62917341, Validation loss: 103.82537947, Gradient norm: 192.28048093
INFO:root:[   86] Training loss: 102.71852179, Validation loss: 104.05186094, Gradient norm: 263.31849192
INFO:root:[   87] Training loss: 102.58677788, Validation loss: 104.01677441, Gradient norm: 254.52023488
INFO:root:[   88] Training loss: 102.55622972, Validation loss: 104.11232705, Gradient norm: 234.50340610
INFO:root:[   89] Training loss: 102.56201921, Validation loss: 104.33973483, Gradient norm: 294.24431777
INFO:root:[   90] Training loss: 102.43932451, Validation loss: 103.88426919, Gradient norm: 243.41088250
INFO:root:EP 90: Early stopping
INFO:root:Training the model took 1250.874s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 143.26076
INFO:root:EnergyScoreTrain: 100.83837
INFO:root:CoverageTrain: 0.34338
INFO:root:IntervalWidthTrain: 4.859
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.35377
INFO:root:EnergyScoreValidation: 103.75302
INFO:root:CoverageValidation: 0.34309
INFO:root:IntervalWidthValidation: 4.86482
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.57289
INFO:root:EnergyScoreTest: 103.90343
INFO:root:CoverageTest: 0.34478
INFO:root:IntervalWidthTest: 4.88358
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.99824666, Validation loss: 121.47056290, Gradient norm: 18.43084605
INFO:root:[    2] Training loss: 121.34094063, Validation loss: 121.31117485, Gradient norm: 7.18437130
INFO:root:[    3] Training loss: 121.32403605, Validation loss: 121.29323657, Gradient norm: 6.56947624
INFO:root:[    4] Training loss: 121.30607166, Validation loss: 121.28692890, Gradient norm: 6.60648919
INFO:root:[    5] Training loss: 121.30020675, Validation loss: 121.44686363, Gradient norm: 6.57423032
INFO:root:[    6] Training loss: 121.28595133, Validation loss: 121.29580899, Gradient norm: 6.97111495
INFO:root:[    7] Training loss: 121.22640222, Validation loss: 121.12860634, Gradient norm: 5.44382494
INFO:root:[    8] Training loss: 120.87715358, Validation loss: 120.47393115, Gradient norm: 7.58060197
INFO:root:[    9] Training loss: 119.49042396, Validation loss: 117.70034001, Gradient norm: 10.56753420
INFO:root:[   10] Training loss: 117.05306426, Validation loss: 115.37489056, Gradient norm: 18.89050752
INFO:root:[   11] Training loss: 115.04006458, Validation loss: 113.06340658, Gradient norm: 28.26264357
INFO:root:[   12] Training loss: 113.67095157, Validation loss: 112.29892283, Gradient norm: 29.25580542
INFO:root:[   13] Training loss: 112.81329872, Validation loss: 111.05939273, Gradient norm: 35.78085824
INFO:root:[   14] Training loss: 112.19529677, Validation loss: 110.31217220, Gradient norm: 43.33666390
INFO:root:[   15] Training loss: 111.65667455, Validation loss: 109.90096099, Gradient norm: 43.70760102
INFO:root:[   16] Training loss: 111.25720282, Validation loss: 109.73047796, Gradient norm: 39.91450932
INFO:root:[   17] Training loss: 110.91647899, Validation loss: 109.58916868, Gradient norm: 59.27537647
INFO:root:[   18] Training loss: 110.63473781, Validation loss: 109.12585607, Gradient norm: 61.34033213
INFO:root:[   19] Training loss: 110.31711106, Validation loss: 108.70299635, Gradient norm: 58.46006559
INFO:root:[   20] Training loss: 110.09332512, Validation loss: 109.22553253, Gradient norm: 62.57868300
INFO:root:[   21] Training loss: 109.83962662, Validation loss: 108.47323582, Gradient norm: 65.11051498
INFO:root:[   22] Training loss: 109.70112833, Validation loss: 107.89862429, Gradient norm: 70.91664729
INFO:root:[   23] Training loss: 109.50789615, Validation loss: 107.89709788, Gradient norm: 75.73850810
INFO:root:[   24] Training loss: 109.29363257, Validation loss: 107.65365469, Gradient norm: 70.03103491
INFO:root:[   25] Training loss: 109.11485797, Validation loss: 108.96975234, Gradient norm: 73.26631352
INFO:root:[   26] Training loss: 109.05024341, Validation loss: 107.86585051, Gradient norm: 86.53009249
INFO:root:[   27] Training loss: 108.90609978, Validation loss: 107.34310466, Gradient norm: 85.36700342
INFO:root:[   28] Training loss: 108.70057975, Validation loss: 107.53339386, Gradient norm: 85.21546791
INFO:root:[   29] Training loss: 108.60328485, Validation loss: 108.85484814, Gradient norm: 90.80229469
INFO:root:[   30] Training loss: 108.52515027, Validation loss: 108.07174525, Gradient norm: 105.08070358
INFO:root:[   31] Training loss: 108.40521017, Validation loss: 107.56812207, Gradient norm: 104.83069304
INFO:root:[   32] Training loss: 108.28998215, Validation loss: 110.46927301, Gradient norm: 116.31695792
INFO:root:[   33] Training loss: 108.23124621, Validation loss: 107.58171950, Gradient norm: 117.98490750
INFO:root:[   34] Training loss: 108.17786434, Validation loss: 108.59536191, Gradient norm: 115.83962654
INFO:root:[   35] Training loss: 108.00088866, Validation loss: 109.47742673, Gradient norm: 126.47867405
INFO:root:[   36] Training loss: 108.01135348, Validation loss: 106.93599543, Gradient norm: 149.84667795
INFO:root:[   37] Training loss: 107.87018221, Validation loss: 109.79753218, Gradient norm: 136.46837262
INFO:root:[   38] Training loss: 107.82909920, Validation loss: 107.98147609, Gradient norm: 152.93300587
INFO:root:[   39] Training loss: 107.73398583, Validation loss: 108.55815861, Gradient norm: 147.12264011
INFO:root:[   40] Training loss: 107.70027701, Validation loss: 106.80729728, Gradient norm: 169.83398998
INFO:root:[   41] Training loss: 107.54080180, Validation loss: 108.90602822, Gradient norm: 154.88923585
INFO:root:[   42] Training loss: 107.51224241, Validation loss: 107.86849055, Gradient norm: 168.72393788
INFO:root:[   43] Training loss: 107.52013816, Validation loss: 111.76139332, Gradient norm: 172.38845378
INFO:root:[   44] Training loss: 107.42671305, Validation loss: 110.42235513, Gradient norm: 183.50554792
INFO:root:[   45] Training loss: 107.39249528, Validation loss: 112.17516353, Gradient norm: 176.38842029
INFO:root:[   46] Training loss: 107.26348512, Validation loss: 109.22144370, Gradient norm: 191.57642437
INFO:root:[   47] Training loss: 107.27361743, Validation loss: 111.12574452, Gradient norm: 184.27867358
INFO:root:[   48] Training loss: 107.16843036, Validation loss: 113.37359645, Gradient norm: 188.17562949
INFO:root:[   49] Training loss: 107.20791916, Validation loss: 110.48089284, Gradient norm: 199.28857806
INFO:root:[   50] Training loss: 107.05161528, Validation loss: 113.34666759, Gradient norm: 182.34263818
INFO:root:[   51] Training loss: 107.11014759, Validation loss: 114.92101209, Gradient norm: 202.96204857
INFO:root:[   52] Training loss: 107.10797139, Validation loss: 112.72175467, Gradient norm: 211.09269001
INFO:root:[   53] Training loss: 107.02305373, Validation loss: 108.99780142, Gradient norm: 259.78437367
INFO:root:[   54] Training loss: 106.98386579, Validation loss: 114.35745160, Gradient norm: 203.33978842
INFO:root:[   55] Training loss: 106.88650884, Validation loss: 117.38936405, Gradient norm: 199.31883810
INFO:root:[   56] Training loss: 106.85192925, Validation loss: 117.06529894, Gradient norm: 229.04707954
INFO:root:[   57] Training loss: 106.90551049, Validation loss: 112.75970433, Gradient norm: 266.05369368
INFO:root:[   58] Training loss: 106.87219927, Validation loss: 115.43600306, Gradient norm: 231.11063765
INFO:root:[   59] Training loss: 106.76937940, Validation loss: 120.94143203, Gradient norm: 223.92556489
INFO:root:[   60] Training loss: 106.81895933, Validation loss: 115.01137622, Gradient norm: 247.55921269
INFO:root:[   61] Training loss: 106.75110856, Validation loss: 112.09074507, Gradient norm: 263.62489242
INFO:root:[   62] Training loss: 106.74820432, Validation loss: 115.51571129, Gradient norm: 247.48584642
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 863.096s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 149.23962
INFO:root:EnergyScoreTrain: 106.17074
INFO:root:CoverageTrain: 0.23958
INFO:root:IntervalWidthTrain: 3.34514
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 150.0538
INFO:root:EnergyScoreValidation: 106.82613
INFO:root:CoverageValidation: 0.2392
INFO:root:IntervalWidthValidation: 3.33711
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 150.31389
INFO:root:EnergyScoreTest: 107.01387
INFO:root:CoverageTest: 0.23886
INFO:root:IntervalWidthTest: 3.33769
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.63708408, Validation loss: 121.56134165, Gradient norm: 22.06643111
INFO:root:[    2] Training loss: 121.39609318, Validation loss: 121.81761169, Gradient norm: 7.09986148
INFO:root:[    3] Training loss: 121.35079389, Validation loss: 121.61906196, Gradient norm: 7.36843153
INFO:root:[    4] Training loss: 121.32608377, Validation loss: 121.54323578, Gradient norm: 5.84118450
INFO:root:[    5] Training loss: 121.31058901, Validation loss: 121.31139269, Gradient norm: 4.16471569
INFO:root:[    6] Training loss: 121.29183541, Validation loss: 121.39880739, Gradient norm: 4.92492245
INFO:root:[    7] Training loss: 121.12697898, Validation loss: 121.00004578, Gradient norm: 5.87288697
INFO:root:[    8] Training loss: 120.59820125, Validation loss: 119.84180082, Gradient norm: 7.04672300
INFO:root:[    9] Training loss: 119.44274639, Validation loss: 118.24033592, Gradient norm: 10.23563638
INFO:root:[   10] Training loss: 118.29279705, Validation loss: 117.10643689, Gradient norm: 12.82560700
INFO:root:[   11] Training loss: 117.22176530, Validation loss: 115.47075706, Gradient norm: 19.76353510
INFO:root:[   12] Training loss: 116.13034091, Validation loss: 114.82734128, Gradient norm: 22.41908988
INFO:root:[   13] Training loss: 115.35053199, Validation loss: 113.84242196, Gradient norm: 28.27447288
INFO:root:[   14] Training loss: 114.81878730, Validation loss: 113.87110427, Gradient norm: 32.00654640
INFO:root:[   15] Training loss: 114.31990537, Validation loss: 112.59231120, Gradient norm: 34.63320285
INFO:root:[   16] Training loss: 113.94678470, Validation loss: 112.59550555, Gradient norm: 39.25345676
INFO:root:[   17] Training loss: 113.54758933, Validation loss: 112.52662974, Gradient norm: 42.09738603
INFO:root:[   18] Training loss: 113.26223222, Validation loss: 112.91804320, Gradient norm: 45.33711295
INFO:root:[   19] Training loss: 113.02423670, Validation loss: 112.21150970, Gradient norm: 59.32390354
INFO:root:[   20] Training loss: 112.74713884, Validation loss: 111.84603224, Gradient norm: 58.63742106
INFO:root:[   21] Training loss: 112.53542281, Validation loss: 114.22753827, Gradient norm: 57.79700536
INFO:root:[   22] Training loss: 112.43011643, Validation loss: 111.69568634, Gradient norm: 73.16453172
INFO:root:[   23] Training loss: 112.25275468, Validation loss: 112.26982669, Gradient norm: 78.87567274
INFO:root:[   24] Training loss: 112.07627666, Validation loss: 111.51786304, Gradient norm: 77.20783862
INFO:root:[   25] Training loss: 111.92929219, Validation loss: 116.16241797, Gradient norm: 90.54638017
INFO:root:[   26] Training loss: 111.75187312, Validation loss: 120.22109512, Gradient norm: 82.32707482
INFO:root:[   27] Training loss: 111.65846934, Validation loss: 118.00148563, Gradient norm: 101.93629923
INFO:root:[   28] Training loss: 111.49010535, Validation loss: 115.04273119, Gradient norm: 98.24829805
INFO:root:[   29] Training loss: 111.44057998, Validation loss: 115.14954955, Gradient norm: 120.62169305
INFO:root:[   30] Training loss: 111.25391908, Validation loss: 122.00303624, Gradient norm: 108.88321828
INFO:root:[   31] Training loss: 111.25395520, Validation loss: 118.70501814, Gradient norm: 114.04705253
INFO:root:[   32] Training loss: 111.12672620, Validation loss: 120.53200110, Gradient norm: 114.70712043
INFO:root:[   33] Training loss: 111.00174416, Validation loss: 115.84478102, Gradient norm: 127.56734249
INFO:root:[   34] Training loss: 111.02094897, Validation loss: 118.99190206, Gradient norm: 131.52471706
INFO:root:[   35] Training loss: 110.89635332, Validation loss: 117.21198378, Gradient norm: 135.21204978
INFO:root:[   36] Training loss: 110.76565977, Validation loss: 123.03322943, Gradient norm: 140.47527095
INFO:root:[   37] Training loss: 110.69055264, Validation loss: 123.92005657, Gradient norm: 156.02376018
INFO:root:[   38] Training loss: 110.67186602, Validation loss: 115.86394632, Gradient norm: 157.92469298
INFO:root:[   39] Training loss: 110.63599213, Validation loss: 119.36427649, Gradient norm: 172.84605515
INFO:root:[   40] Training loss: 110.62138414, Validation loss: 122.72270755, Gradient norm: 162.96869878
INFO:root:[   41] Training loss: 110.57047886, Validation loss: 122.24979138, Gradient norm: 182.78417406
INFO:root:[   42] Training loss: 110.45339189, Validation loss: 121.30457885, Gradient norm: 176.99254125
INFO:root:[   43] Training loss: 110.42382785, Validation loss: 126.97928172, Gradient norm: 193.22276609
INFO:root:[   44] Training loss: 110.27502057, Validation loss: 122.44445696, Gradient norm: 185.59805175
INFO:root:[   45] Training loss: 110.24606931, Validation loss: 125.41303543, Gradient norm: 198.45291098
INFO:root:[   46] Training loss: 110.12155921, Validation loss: 118.30187094, Gradient norm: 206.94070869
INFO:root:[   47] Training loss: 110.11522459, Validation loss: 120.69435725, Gradient norm: 219.18616816
INFO:root:[   48] Training loss: 110.01700038, Validation loss: 119.84657709, Gradient norm: 217.04777304
INFO:root:[   49] Training loss: 109.94694580, Validation loss: 120.61755240, Gradient norm: 210.72013385
INFO:root:[   50] Training loss: 109.99806578, Validation loss: 118.07002969, Gradient norm: 248.91126614
INFO:root:[   51] Training loss: 109.91296745, Validation loss: 127.69144519, Gradient norm: 243.88736034
INFO:root:[   52] Training loss: 110.00683817, Validation loss: 115.40752385, Gradient norm: 278.06433879
INFO:root:[   53] Training loss: 109.75803328, Validation loss: 112.39854694, Gradient norm: 248.05071673
INFO:root:[   54] Training loss: 109.83981073, Validation loss: 112.79569718, Gradient norm: 281.85545746
INFO:root:[   55] Training loss: 109.78731692, Validation loss: 112.01601489, Gradient norm: 282.96953764
INFO:root:[   56] Training loss: 109.72946748, Validation loss: 117.40146821, Gradient norm: 297.36872404
INFO:root:[   57] Training loss: 109.66335499, Validation loss: 113.33259004, Gradient norm: 305.53380870
INFO:root:[   58] Training loss: 109.66052023, Validation loss: 115.27760762, Gradient norm: 297.89492912
INFO:root:[   59] Training loss: 109.64364604, Validation loss: 112.46336917, Gradient norm: 335.93107360
INFO:root:[   60] Training loss: 109.59893191, Validation loss: 110.13464803, Gradient norm: 311.28220877
INFO:root:[   61] Training loss: 109.55223327, Validation loss: 110.53060334, Gradient norm: 359.69010626
INFO:root:[   62] Training loss: 109.60315157, Validation loss: 113.97149527, Gradient norm: 368.33464045
INFO:root:[   63] Training loss: 109.58262837, Validation loss: 108.77171273, Gradient norm: 351.35872925
INFO:root:[   64] Training loss: 109.51545013, Validation loss: 109.74045984, Gradient norm: 344.56463151
INFO:root:[   65] Training loss: 109.44369149, Validation loss: 109.42766782, Gradient norm: 353.03361837
INFO:root:[   66] Training loss: 109.44670139, Validation loss: 108.18650818, Gradient norm: 385.91074390
INFO:root:[   67] Training loss: 109.39163957, Validation loss: 107.64011252, Gradient norm: 375.87350090
INFO:root:[   68] Training loss: 109.43750695, Validation loss: 106.68341354, Gradient norm: 365.32421907
INFO:root:[   69] Training loss: 109.31974826, Validation loss: 106.65150715, Gradient norm: 418.19298951
INFO:root:[   70] Training loss: 109.40939615, Validation loss: 106.95719541, Gradient norm: 417.11618347
INFO:root:[   71] Training loss: 109.63762955, Validation loss: 107.30095462, Gradient norm: 422.59808574
INFO:root:[   72] Training loss: 109.86525247, Validation loss: 111.22705131, Gradient norm: 459.32223146
INFO:root:[   73] Training loss: 109.86898669, Validation loss: 118.24178551, Gradient norm: 436.33157945
INFO:root:[   74] Training loss: 110.22554084, Validation loss: 127.87957501, Gradient norm: 525.71466653
INFO:root:[   75] Training loss: 109.92626332, Validation loss: 139.65595166, Gradient norm: 507.50541898
INFO:root:[   76] Training loss: 109.74307481, Validation loss: 146.92553237, Gradient norm: 520.54890191
INFO:root:[   77] Training loss: 109.68922120, Validation loss: 145.33738603, Gradient norm: 511.27526991
INFO:root:[   78] Training loss: 109.85255594, Validation loss: 150.40522240, Gradient norm: 532.43139161
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 1305.553s.
INFO:root:Emptying the cuda cache took 0.079s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 150.39642
INFO:root:EnergyScoreTrain: 106.30764
INFO:root:CoverageTrain: 0.21353
INFO:root:IntervalWidthTrain: 3.51589
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 150.94347
INFO:root:EnergyScoreValidation: 106.72794
INFO:root:CoverageValidation: 0.21326
INFO:root:IntervalWidthValidation: 3.50935
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 151.33099
INFO:root:EnergyScoreTest: 107.00194
INFO:root:CoverageTest: 0.21447
INFO:root:IntervalWidthTest: 3.52968
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.45616940, Validation loss: 121.72160392, Gradient norm: 19.56060151
INFO:root:[    2] Training loss: 121.41559196, Validation loss: 121.80086938, Gradient norm: 6.29473694
INFO:root:[    3] Training loss: 121.33315635, Validation loss: 121.68762944, Gradient norm: 5.34064465
