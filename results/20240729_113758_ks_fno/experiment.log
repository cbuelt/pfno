INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 2097152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 134.33524350, Validation loss: 126.12748192, Gradient norm: 659.03112075
INFO:root:[    2] Training loss: 122.87054443, Validation loss: 124.78261566, Gradient norm: 341.57774298
INFO:root:[    3] Training loss: 122.21167431, Validation loss: 124.00013522, Gradient norm: 156.29841124
INFO:root:[    4] Training loss: 122.18436776, Validation loss: 123.58064349, Gradient norm: 136.58629661
INFO:root:[    5] Training loss: 122.05135210, Validation loss: 123.44093507, Gradient norm: 122.98718733
INFO:root:[    6] Training loss: 121.87099869, Validation loss: 122.53275746, Gradient norm: 77.93565887
INFO:root:[    7] Training loss: 121.78888135, Validation loss: 122.78647508, Gradient norm: 64.33320170
INFO:root:[    8] Training loss: 121.77347673, Validation loss: 122.56304300, Gradient norm: 50.45189758
INFO:root:[    9] Training loss: 121.64826567, Validation loss: 122.21384351, Gradient norm: 52.14428058
INFO:root:[   10] Training loss: 121.27342825, Validation loss: 122.31837253, Gradient norm: 45.17798093
INFO:root:[   11] Training loss: 120.92767759, Validation loss: 122.28947975, Gradient norm: 45.51962195
INFO:root:[   12] Training loss: 120.78686834, Validation loss: 121.96803442, Gradient norm: 38.81510705
INFO:root:[   13] Training loss: 120.74393024, Validation loss: 122.11862156, Gradient norm: 41.24753445
INFO:root:[   14] Training loss: 120.70926781, Validation loss: 122.46898783, Gradient norm: 34.70627182
INFO:root:[   15] Training loss: 120.71659662, Validation loss: 121.74370680, Gradient norm: 46.18656701
INFO:root:[   16] Training loss: 120.54340849, Validation loss: 121.66924260, Gradient norm: 37.37870072
INFO:root:[   17] Training loss: 120.61387195, Validation loss: 121.57477438, Gradient norm: 44.90542766
INFO:root:[   18] Training loss: 120.55550594, Validation loss: 121.22562487, Gradient norm: 33.99471654
INFO:root:[   19] Training loss: 120.52899913, Validation loss: 121.65868720, Gradient norm: 33.94406055
INFO:root:[   20] Training loss: 120.59196303, Validation loss: 121.42898165, Gradient norm: 33.75184536
INFO:root:[   21] Training loss: 120.47532836, Validation loss: 121.51634506, Gradient norm: 27.68707618
INFO:root:[   22] Training loss: 120.57707775, Validation loss: 121.37071517, Gradient norm: 37.71475083
INFO:root:[   23] Training loss: 120.44340704, Validation loss: 121.10603333, Gradient norm: 26.40895845
INFO:root:[   24] Training loss: 120.52631263, Validation loss: 121.37051918, Gradient norm: 31.33360203
INFO:root:[   25] Training loss: 120.57917968, Validation loss: 121.23022803, Gradient norm: 34.37936684
INFO:root:[   26] Training loss: 120.45571899, Validation loss: 120.97471303, Gradient norm: 33.70404678
INFO:root:[   27] Training loss: 120.43752572, Validation loss: 121.51046437, Gradient norm: 35.65221448
INFO:root:[   28] Training loss: 120.36980013, Validation loss: 121.28037841, Gradient norm: 30.82817772
INFO:root:[   29] Training loss: 120.36217026, Validation loss: 120.87092090, Gradient norm: 24.98766781
INFO:root:[   30] Training loss: 120.38359684, Validation loss: 121.13575324, Gradient norm: 28.61884822
INFO:root:[   31] Training loss: 120.63629333, Validation loss: 122.39820046, Gradient norm: 39.06563642
INFO:root:[   32] Training loss: 120.49864096, Validation loss: 121.26374159, Gradient norm: 29.31221109
INFO:root:[   33] Training loss: 120.34412688, Validation loss: 121.24297464, Gradient norm: 29.42609717
INFO:root:[   34] Training loss: 120.09102219, Validation loss: 120.58520508, Gradient norm: 25.26130470
INFO:root:[   35] Training loss: 119.88142260, Validation loss: 119.64071261, Gradient norm: 27.87906306
INFO:root:[   36] Training loss: 118.53741104, Validation loss: 118.03846820, Gradient norm: 24.86963727
INFO:root:[   37] Training loss: 117.29089747, Validation loss: 116.73962718, Gradient norm: 28.30648735
INFO:root:[   38] Training loss: 116.03227349, Validation loss: 115.75202889, Gradient norm: 28.83283734
INFO:root:[   39] Training loss: 115.07841458, Validation loss: 115.18025734, Gradient norm: 31.14063017
INFO:root:[   40] Training loss: 114.17997728, Validation loss: 113.87273565, Gradient norm: 39.30537718
INFO:root:[   41] Training loss: 113.60476151, Validation loss: 113.62335600, Gradient norm: 37.52871262
INFO:root:[   42] Training loss: 112.99865655, Validation loss: 112.57379387, Gradient norm: 44.93683448
INFO:root:[   43] Training loss: 112.43358052, Validation loss: 112.16171344, Gradient norm: 43.09682534
INFO:root:[   44] Training loss: 112.04042553, Validation loss: 112.15419480, Gradient norm: 45.87518051
INFO:root:[   45] Training loss: 111.60603339, Validation loss: 111.40719236, Gradient norm: 45.47590937
INFO:root:[   46] Training loss: 111.25312542, Validation loss: 110.90989606, Gradient norm: 54.34717139
INFO:root:[   47] Training loss: 110.96046691, Validation loss: 110.57987739, Gradient norm: 49.37214605
INFO:root:[   48] Training loss: 110.54109320, Validation loss: 110.53091667, Gradient norm: 62.70246145
INFO:root:[   49] Training loss: 110.27849093, Validation loss: 110.14315428, Gradient norm: 46.05380098
INFO:root:[   50] Training loss: 109.88540197, Validation loss: 109.85217101, Gradient norm: 57.26879360
INFO:root:[   51] Training loss: 109.65141364, Validation loss: 109.63348520, Gradient norm: 44.56078987
INFO:root:[   52] Training loss: 109.32883440, Validation loss: 109.59773754, Gradient norm: 59.09373322
INFO:root:[   53] Training loss: 109.08246802, Validation loss: 109.12315132, Gradient norm: 53.24443485
INFO:root:[   54] Training loss: 108.75200248, Validation loss: 108.87729829, Gradient norm: 55.53319300
INFO:root:[   55] Training loss: 108.58528475, Validation loss: 108.76223045, Gradient norm: 57.28602406
INFO:root:[   56] Training loss: 108.25969142, Validation loss: 108.51958334, Gradient norm: 57.85528186
INFO:root:[   57] Training loss: 108.15338628, Validation loss: 108.08739208, Gradient norm: 53.33569794
INFO:root:[   58] Training loss: 107.85972886, Validation loss: 107.95469744, Gradient norm: 59.23875386
INFO:root:[   59] Training loss: 107.52226926, Validation loss: 108.12332679, Gradient norm: 61.54545895
INFO:root:[   60] Training loss: 107.35569723, Validation loss: 107.70457143, Gradient norm: 56.02310275
INFO:root:[   61] Training loss: 107.23084354, Validation loss: 107.02365770, Gradient norm: 69.07026486
INFO:root:[   62] Training loss: 106.96815626, Validation loss: 106.63423315, Gradient norm: 54.71901888
INFO:root:[   63] Training loss: 106.78959723, Validation loss: 106.97407926, Gradient norm: 67.79670244
INFO:root:[   64] Training loss: 106.81854147, Validation loss: 106.73489643, Gradient norm: 65.33038826
INFO:root:[   65] Training loss: 106.45590500, Validation loss: 106.61597653, Gradient norm: 63.65005305
INFO:root:[   66] Training loss: 106.36052947, Validation loss: 105.95705993, Gradient norm: 56.54807914
INFO:root:[   67] Training loss: 106.20800457, Validation loss: 107.10826795, Gradient norm: 71.20324343
INFO:root:[   68] Training loss: 106.03340257, Validation loss: 106.03966549, Gradient norm: 59.61418845
INFO:root:[   69] Training loss: 105.75705591, Validation loss: 106.17097841, Gradient norm: 67.57401131
INFO:root:[   70] Training loss: 105.71344230, Validation loss: 106.12931298, Gradient norm: 60.40539155
INFO:root:[   71] Training loss: 105.62360848, Validation loss: 105.73742281, Gradient norm: 64.99315389
INFO:root:[   72] Training loss: 105.53534368, Validation loss: 105.59687963, Gradient norm: 54.77231467
INFO:root:[   73] Training loss: 105.31836673, Validation loss: 105.53282613, Gradient norm: 70.60504847
INFO:root:[   74] Training loss: 105.39457601, Validation loss: 105.34793038, Gradient norm: 72.83357499
INFO:root:[   75] Training loss: 105.14241305, Validation loss: 105.25548448, Gradient norm: 73.32444715
INFO:root:[   76] Training loss: 105.03625475, Validation loss: 105.44957944, Gradient norm: 56.31264310
INFO:root:[   77] Training loss: 104.96544053, Validation loss: 104.98844462, Gradient norm: 75.78122657
INFO:root:[   78] Training loss: 104.73117639, Validation loss: 104.91337980, Gradient norm: 60.92894916
INFO:root:[   79] Training loss: 104.81254598, Validation loss: 104.89526893, Gradient norm: 79.79065878
INFO:root:[   80] Training loss: 104.63748561, Validation loss: 104.90492064, Gradient norm: 64.57024099
INFO:root:[   81] Training loss: 104.62288916, Validation loss: 104.82215145, Gradient norm: 60.22473147
INFO:root:[   82] Training loss: 104.43533312, Validation loss: 104.41318538, Gradient norm: 69.26310433
INFO:root:[   83] Training loss: 104.41103836, Validation loss: 104.44966178, Gradient norm: 59.13424592
INFO:root:[   84] Training loss: 104.24926042, Validation loss: 104.34121257, Gradient norm: 67.83003773
INFO:root:[   85] Training loss: 104.26048502, Validation loss: 104.60626615, Gradient norm: 65.74897807
INFO:root:[   86] Training loss: 104.23936868, Validation loss: 104.67335642, Gradient norm: 72.99852098
INFO:root:[   87] Training loss: 104.03766430, Validation loss: 104.28699993, Gradient norm: 74.09992071
INFO:root:[   88] Training loss: 103.90518540, Validation loss: 104.27465557, Gradient norm: 60.98105841
INFO:root:[   89] Training loss: 103.76504442, Validation loss: 104.61818721, Gradient norm: 69.12922332
INFO:root:[   90] Training loss: 103.81821455, Validation loss: 104.64104409, Gradient norm: 73.67960151
INFO:root:[   91] Training loss: 103.78842832, Validation loss: 104.11403735, Gradient norm: 71.64216144
INFO:root:[   92] Training loss: 103.64633489, Validation loss: 103.74957407, Gradient norm: 66.08468233
INFO:root:[   93] Training loss: 103.64923609, Validation loss: 103.98102806, Gradient norm: 70.35196130
INFO:root:[   94] Training loss: 103.38196260, Validation loss: 103.63707049, Gradient norm: 67.97277631
INFO:root:[   95] Training loss: 103.42149144, Validation loss: 103.89453125, Gradient norm: 69.73697405
INFO:root:[   96] Training loss: 103.41587978, Validation loss: 103.53020451, Gradient norm: 77.80349845
INFO:root:[   97] Training loss: 103.42186372, Validation loss: 103.56310272, Gradient norm: 77.15347843
INFO:root:[   98] Training loss: 103.16997386, Validation loss: 103.42160587, Gradient norm: 56.17898068
INFO:root:[   99] Training loss: 103.18107808, Validation loss: 103.27663869, Gradient norm: 74.01286484
INFO:root:[  100] Training loss: 103.07916118, Validation loss: 103.29557642, Gradient norm: 69.64471805
INFO:root:[  101] Training loss: 103.20195041, Validation loss: 103.47393720, Gradient norm: 73.89593052
INFO:root:[  102] Training loss: 103.07960038, Validation loss: 103.24796453, Gradient norm: 86.25737888
INFO:root:[  103] Training loss: 102.98255016, Validation loss: 103.63929670, Gradient norm: 67.69580503
INFO:root:[  104] Training loss: 102.88437531, Validation loss: 103.45530017, Gradient norm: 72.57647891
INFO:root:[  105] Training loss: 102.90592680, Validation loss: 103.23076025, Gradient norm: 79.56764633
INFO:root:[  106] Training loss: 102.74822404, Validation loss: 103.34969593, Gradient norm: 69.18961624
INFO:root:[  107] Training loss: 102.67051933, Validation loss: 103.24246926, Gradient norm: 67.79997843
INFO:root:[  108] Training loss: 102.67544144, Validation loss: 102.78810619, Gradient norm: 69.02643074
INFO:root:[  109] Training loss: 102.70998031, Validation loss: 103.29246337, Gradient norm: 73.06323934
INFO:root:[  110] Training loss: 102.63767020, Validation loss: 103.05884499, Gradient norm: 94.18827900
INFO:root:[  111] Training loss: 102.45704185, Validation loss: 102.99586329, Gradient norm: 64.19644490
INFO:root:[  112] Training loss: 102.43776946, Validation loss: 102.65727550, Gradient norm: 64.99831027
INFO:root:[  113] Training loss: 102.36638729, Validation loss: 102.76779464, Gradient norm: 84.94955851
INFO:root:[  114] Training loss: 102.23108241, Validation loss: 102.58512010, Gradient norm: 66.84733965
INFO:root:[  115] Training loss: 102.29356465, Validation loss: 102.72524788, Gradient norm: 75.33648438
INFO:root:[  116] Training loss: 102.23489002, Validation loss: 102.66742285, Gradient norm: 71.61743825
INFO:root:[  117] Training loss: 102.27677762, Validation loss: 102.68903430, Gradient norm: 80.89045160
INFO:root:[  118] Training loss: 102.28400009, Validation loss: 102.90658306, Gradient norm: 87.98314076
INFO:root:[  119] Training loss: 102.08206366, Validation loss: 102.21208507, Gradient norm: 71.09566541
INFO:root:[  120] Training loss: 102.17597968, Validation loss: 102.55709786, Gradient norm: 77.79300590
INFO:root:[  121] Training loss: 102.00941373, Validation loss: 102.62120319, Gradient norm: 70.77062967
INFO:root:[  122] Training loss: 101.93557381, Validation loss: 102.81545494, Gradient norm: 73.75386855
INFO:root:[  123] Training loss: 101.93806876, Validation loss: 102.43057277, Gradient norm: 94.44159789
INFO:root:[  124] Training loss: 101.82038940, Validation loss: 102.47428447, Gradient norm: 79.62259869
INFO:root:[  125] Training loss: 101.85816652, Validation loss: 102.30965766, Gradient norm: 78.82758920
INFO:root:[  126] Training loss: 101.82112918, Validation loss: 102.07538157, Gradient norm: 82.19078617
INFO:root:[  127] Training loss: 101.74403996, Validation loss: 102.48842463, Gradient norm: 70.24777556
INFO:root:[  128] Training loss: 101.65787270, Validation loss: 102.47122929, Gradient norm: 79.96919522
INFO:root:[  129] Training loss: 101.72894780, Validation loss: 102.14787634, Gradient norm: 87.40988202
INFO:root:[  130] Training loss: 101.58402023, Validation loss: 102.02071144, Gradient norm: 76.42674545
INFO:root:[  131] Training loss: 101.62283021, Validation loss: 101.98066606, Gradient norm: 87.25909447
INFO:root:[  132] Training loss: 101.54483106, Validation loss: 102.05847326, Gradient norm: 82.94478491
INFO:root:[  133] Training loss: 101.46143888, Validation loss: 102.57573726, Gradient norm: 73.16144513
INFO:root:[  134] Training loss: 101.28397842, Validation loss: 102.21046185, Gradient norm: 73.33777318
INFO:root:[  135] Training loss: 101.42872046, Validation loss: 102.11456615, Gradient norm: 95.82878155
INFO:root:[  136] Training loss: 101.41558176, Validation loss: 101.71168334, Gradient norm: 88.27694433
INFO:root:[  137] Training loss: 101.28449749, Validation loss: 102.55404374, Gradient norm: 76.20169944
INFO:root:[  138] Training loss: 101.32444662, Validation loss: 101.85937026, Gradient norm: 79.32627272
INFO:root:[  139] Training loss: 101.17858563, Validation loss: 101.47130032, Gradient norm: 91.77847786
INFO:root:[  140] Training loss: 101.10249511, Validation loss: 101.72828806, Gradient norm: 84.10561019
INFO:root:[  141] Training loss: 100.97783587, Validation loss: 101.60462110, Gradient norm: 70.37695000
INFO:root:[  142] Training loss: 101.11044784, Validation loss: 101.64602924, Gradient norm: 88.10094868
INFO:root:[  143] Training loss: 101.08788792, Validation loss: 101.67893903, Gradient norm: 93.53422042
INFO:root:[  144] Training loss: 100.97253472, Validation loss: 101.39957375, Gradient norm: 77.74317003
INFO:root:[  145] Training loss: 100.93279422, Validation loss: 101.33568652, Gradient norm: 92.15186009
INFO:root:[  146] Training loss: 100.88185558, Validation loss: 101.62402870, Gradient norm: 76.00817627
INFO:root:[  147] Training loss: 100.84848630, Validation loss: 101.58274710, Gradient norm: 94.04230186
INFO:root:[  148] Training loss: 100.79123897, Validation loss: 101.59180792, Gradient norm: 85.63671890
INFO:root:[  149] Training loss: 100.67593256, Validation loss: 101.17700011, Gradient norm: 89.83784315
INFO:root:[  150] Training loss: 100.64129416, Validation loss: 101.33115624, Gradient norm: 86.97602011
INFO:root:[  151] Training loss: 100.62079269, Validation loss: 101.30030191, Gradient norm: 93.60221370
INFO:root:[  152] Training loss: 100.70128483, Validation loss: 101.85643952, Gradient norm: 84.22383707
INFO:root:[  153] Training loss: 100.60934820, Validation loss: 101.30312663, Gradient norm: 85.38669056
INFO:root:[  154] Training loss: 100.77619968, Validation loss: 101.44327045, Gradient norm: 108.33066118
INFO:root:[  155] Training loss: 100.58678200, Validation loss: 101.18107921, Gradient norm: 88.02162585
INFO:root:[  156] Training loss: 100.52850727, Validation loss: 101.10536273, Gradient norm: 92.48038664
INFO:root:[  157] Training loss: 100.39689413, Validation loss: 100.81814602, Gradient norm: 90.78881569
INFO:root:[  158] Training loss: 100.39121577, Validation loss: 101.28672133, Gradient norm: 90.21002792
INFO:root:[  159] Training loss: 100.44069125, Validation loss: 101.08696089, Gradient norm: 96.53446873
INFO:root:[  160] Training loss: 100.37636094, Validation loss: 100.81871085, Gradient norm: 91.98412868
INFO:root:[  161] Training loss: 100.39687253, Validation loss: 101.12012534, Gradient norm: 91.88047097
INFO:root:[  162] Training loss: 100.19814057, Validation loss: 101.25286129, Gradient norm: 89.91327073
INFO:root:[  163] Training loss: 100.20969209, Validation loss: 100.92595120, Gradient norm: 84.81102536
INFO:root:[  164] Training loss: 100.22714281, Validation loss: 101.22944483, Gradient norm: 94.23194001
INFO:root:[  165] Training loss: 100.16615930, Validation loss: 100.90618739, Gradient norm: 93.25399276
INFO:root:[  166] Training loss: 100.24355782, Validation loss: 100.61716277, Gradient norm: 94.31198355
INFO:root:[  167] Training loss: 100.19370331, Validation loss: 100.62291191, Gradient norm: 101.69954719
INFO:root:[  168] Training loss: 100.14605652, Validation loss: 100.73307906, Gradient norm: 95.35305130
INFO:root:[  169] Training loss: 99.95347332, Validation loss: 100.81023512, Gradient norm: 96.24405987
INFO:root:[  170] Training loss: 99.96581734, Validation loss: 100.42779252, Gradient norm: 97.59136139
INFO:root:[  171] Training loss: 100.05501529, Validation loss: 100.96921750, Gradient norm: 92.59974050
INFO:root:[  172] Training loss: 100.04016633, Validation loss: 100.78259882, Gradient norm: 92.18306874
INFO:root:[  173] Training loss: 99.96359118, Validation loss: 100.55263703, Gradient norm: 91.37782683
INFO:root:[  174] Training loss: 99.81071472, Validation loss: 100.58101943, Gradient norm: 96.97637313
INFO:root:[  175] Training loss: 99.75684417, Validation loss: 100.39317611, Gradient norm: 88.60198899
INFO:root:[  176] Training loss: 99.88217271, Validation loss: 101.02312154, Gradient norm: 95.27035888
INFO:root:[  177] Training loss: 99.88785364, Validation loss: 100.37525019, Gradient norm: 103.75311072
INFO:root:[  178] Training loss: 99.70115061, Validation loss: 100.18513989, Gradient norm: 97.89606266
INFO:root:[  179] Training loss: 99.68760708, Validation loss: 100.54014456, Gradient norm: 87.38922882
INFO:root:[  180] Training loss: 99.77335277, Validation loss: 100.53955762, Gradient norm: 96.14082433
INFO:root:[  181] Training loss: 99.68659568, Validation loss: 100.11403709, Gradient norm: 92.60524533
INFO:root:[  182] Training loss: 99.60222389, Validation loss: 101.19816142, Gradient norm: 96.79329547
INFO:root:[  183] Training loss: 99.71539874, Validation loss: 100.31943775, Gradient norm: 103.56928068
INFO:root:[  184] Training loss: 99.60451764, Validation loss: 100.27621934, Gradient norm: 96.01003537
INFO:root:[  185] Training loss: 99.52595547, Validation loss: 100.28767106, Gradient norm: 85.18156497
INFO:root:[  186] Training loss: 99.50600440, Validation loss: 100.75061272, Gradient norm: 100.98645975
INFO:root:[  187] Training loss: 99.46039406, Validation loss: 100.13737304, Gradient norm: 91.77589028
INFO:root:[  188] Training loss: 99.50917229, Validation loss: 99.92700985, Gradient norm: 89.06444588
INFO:root:[  189] Training loss: 99.44572975, Validation loss: 99.81661014, Gradient norm: 92.08464047
INFO:root:[  190] Training loss: 99.44085187, Validation loss: 100.05115851, Gradient norm: 108.08881042
INFO:root:[  191] Training loss: 99.25807332, Validation loss: 99.93686071, Gradient norm: 98.29255569
INFO:root:[  192] Training loss: 99.27765689, Validation loss: 100.08264450, Gradient norm: 92.63372679
INFO:root:[  193] Training loss: 99.21186099, Validation loss: 100.26951625, Gradient norm: 102.55104507
INFO:root:[  194] Training loss: 99.30252163, Validation loss: 99.81157369, Gradient norm: 94.79384444
INFO:root:[  195] Training loss: 99.28927389, Validation loss: 100.56485748, Gradient norm: 86.94541041
INFO:root:[  196] Training loss: 99.11894253, Validation loss: 100.61333597, Gradient norm: 97.34461368
INFO:root:[  197] Training loss: 99.09800747, Validation loss: 100.05145764, Gradient norm: 98.55298857
INFO:root:[  198] Training loss: 99.05350211, Validation loss: 100.06784005, Gradient norm: 88.10393171
INFO:root:[  199] Training loss: 99.21815106, Validation loss: 100.11141021, Gradient norm: 95.74864487
INFO:root:[  200] Training loss: 98.98462589, Validation loss: 100.23358628, Gradient norm: 100.31148427
INFO:root:[  201] Training loss: 99.07082002, Validation loss: 99.77647005, Gradient norm: 83.10156693
INFO:root:[  202] Training loss: 99.08117669, Validation loss: 99.62880391, Gradient norm: 99.61911599
INFO:root:[  203] Training loss: 99.03783937, Validation loss: 99.90855066, Gradient norm: 97.00260542
INFO:root:[  204] Training loss: 99.02297771, Validation loss: 100.31553729, Gradient norm: 106.35361736
INFO:root:[  205] Training loss: 98.85042464, Validation loss: 99.98897631, Gradient norm: 88.72737021
INFO:root:[  206] Training loss: 98.95943248, Validation loss: 99.93607041, Gradient norm: 98.22277135
INFO:root:[  207] Training loss: 98.78019262, Validation loss: 99.84350744, Gradient norm: 89.12735282
INFO:root:[  208] Training loss: 98.95445170, Validation loss: 99.59441113, Gradient norm: 115.31200340
INFO:root:[  209] Training loss: 98.77562693, Validation loss: 100.30045108, Gradient norm: 95.90541980
INFO:root:[  210] Training loss: 98.84100673, Validation loss: 99.87213187, Gradient norm: 102.47540677
INFO:root:[  211] Training loss: 98.91132476, Validation loss: 100.06849933, Gradient norm: 93.03384919
INFO:root:[  212] Training loss: 98.75608772, Validation loss: 99.78189008, Gradient norm: 90.21471768
INFO:root:[  213] Training loss: 98.75454557, Validation loss: 100.01701618, Gradient norm: 101.54614858
INFO:root:[  214] Training loss: 98.66660403, Validation loss: 99.68968359, Gradient norm: 102.40332416
INFO:root:[  215] Training loss: 98.70567302, Validation loss: 99.48297172, Gradient norm: 96.02030309
INFO:root:[  216] Training loss: 98.66831599, Validation loss: 99.51285790, Gradient norm: 103.77386189
INFO:root:[  217] Training loss: 98.63452337, Validation loss: 99.60314889, Gradient norm: 107.91405967
INFO:root:[  218] Training loss: 98.54237501, Validation loss: 99.73542444, Gradient norm: 93.11564227
INFO:root:[  219] Training loss: 98.56616265, Validation loss: 99.58978219, Gradient norm: 99.66029241
INFO:root:[  220] Training loss: 98.58326883, Validation loss: 100.00200942, Gradient norm: 97.45044540
INFO:root:[  221] Training loss: 98.48007533, Validation loss: 99.35563002, Gradient norm: 110.87566582
INFO:root:[  222] Training loss: 98.52717232, Validation loss: 99.62552669, Gradient norm: 92.05338022
INFO:root:[  223] Training loss: 98.49149761, Validation loss: 99.85621301, Gradient norm: 109.92403340
INFO:root:[  224] Training loss: 98.35345304, Validation loss: 99.59974460, Gradient norm: 93.28543894
INFO:root:[  225] Training loss: 98.34102793, Validation loss: 99.25582544, Gradient norm: 94.75921471
INFO:root:[  226] Training loss: 98.27221551, Validation loss: 99.15636655, Gradient norm: 114.21059595
INFO:root:[  227] Training loss: 98.25946774, Validation loss: 99.32627527, Gradient norm: 91.70136902
INFO:root:[  228] Training loss: 98.38978658, Validation loss: 99.54660850, Gradient norm: 99.31062302
INFO:root:[  229] Training loss: 98.25545860, Validation loss: 99.52457244, Gradient norm: 85.39085287
INFO:root:[  230] Training loss: 98.38175958, Validation loss: 99.54196825, Gradient norm: 115.04796549
INFO:root:[  231] Training loss: 98.15294148, Validation loss: 99.40215617, Gradient norm: 91.87891211
INFO:root:[  232] Training loss: 98.21771443, Validation loss: 99.16099706, Gradient norm: 100.08383798
INFO:root:[  233] Training loss: 98.27487324, Validation loss: 99.42212993, Gradient norm: 99.34915318
INFO:root:[  234] Training loss: 98.16512663, Validation loss: 99.28941556, Gradient norm: 103.81221662
INFO:root:[  235] Training loss: 98.05020250, Validation loss: 99.42460580, Gradient norm: 97.16098406
INFO:root:[  236] Training loss: 98.21055758, Validation loss: 99.10550821, Gradient norm: 89.15273950
INFO:root:[  237] Training loss: 98.11316310, Validation loss: 99.39340736, Gradient norm: 114.95339612
INFO:root:[  238] Training loss: 98.10819528, Validation loss: 99.27293054, Gradient norm: 87.87160205
INFO:root:[  239] Training loss: 97.96069538, Validation loss: 99.12669241, Gradient norm: 97.83980612
INFO:root:[  240] Training loss: 98.18252503, Validation loss: 99.17272423, Gradient norm: 104.07097461
INFO:root:[  241] Training loss: 97.98707668, Validation loss: 99.34418356, Gradient norm: 104.25520752
INFO:root:[  242] Training loss: 97.87895311, Validation loss: 99.25247008, Gradient norm: 88.84450247
INFO:root:[  243] Training loss: 97.90712367, Validation loss: 99.00106417, Gradient norm: 99.75609955
INFO:root:[  244] Training loss: 97.98006608, Validation loss: 99.22045109, Gradient norm: 99.88157410
INFO:root:[  245] Training loss: 97.90537647, Validation loss: 99.28028186, Gradient norm: 104.09429795
INFO:root:[  246] Training loss: 98.00254538, Validation loss: 99.15115830, Gradient norm: 115.37138798
INFO:root:[  247] Training loss: 97.84185960, Validation loss: 99.77533222, Gradient norm: 91.55228988
INFO:root:[  248] Training loss: 97.75337847, Validation loss: 99.27940842, Gradient norm: 95.82280818
INFO:root:[  249] Training loss: 97.78899539, Validation loss: 99.21773108, Gradient norm: 104.51500530
INFO:root:[  250] Training loss: 97.77653841, Validation loss: 98.65172340, Gradient norm: 94.12289032
INFO:root:[  251] Training loss: 97.78452747, Validation loss: 98.92754390, Gradient norm: 113.57780813
INFO:root:[  252] Training loss: 97.61259373, Validation loss: 99.38518182, Gradient norm: 89.70027339
INFO:root:[  253] Training loss: 97.73267189, Validation loss: 99.01061170, Gradient norm: 107.69809799
INFO:root:[  254] Training loss: 97.76694110, Validation loss: 99.06208670, Gradient norm: 103.72434641
INFO:root:[  255] Training loss: 97.56638417, Validation loss: 98.77351327, Gradient norm: 96.56313005
INFO:root:[  256] Training loss: 97.65491661, Validation loss: 98.97515685, Gradient norm: 99.53702304
INFO:root:[  257] Training loss: 97.51027659, Validation loss: 98.89084967, Gradient norm: 92.43615829
INFO:root:[  258] Training loss: 97.49529800, Validation loss: 99.06806551, Gradient norm: 102.07244186
INFO:root:[  259] Training loss: 97.54734168, Validation loss: 98.72748118, Gradient norm: 102.79709654
INFO:root:EP 259: Early stopping
INFO:root:Training the model took 4673.427s.
INFO:root:Emptying the cuda cache took 0.046s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 138.11785
INFO:root:EnergyScoreTrain: 97.49234
INFO:root:CoverageTrain: 0.73918
INFO:root:IntervalWidthTrain: 7.58124
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 140.03946
INFO:root:EnergyScoreValidation: 98.84889
INFO:root:CoverageValidation: 0.73621
INFO:root:IntervalWidthValidation: 7.57604
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 140.22749
INFO:root:EnergyScoreTest: 98.98222
INFO:root:CoverageTest: 0.73585
INFO:root:IntervalWidthTest: 7.58122
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 125.97852805, Validation loss: 123.24358131, Gradient norm: 634.70498092
INFO:root:[    2] Training loss: 122.49500653, Validation loss: 121.98837280, Gradient norm: 483.47110753
INFO:root:[    3] Training loss: 121.87587016, Validation loss: 121.85939157, Gradient norm: 373.81701533
INFO:root:[    4] Training loss: 121.81945477, Validation loss: 121.92926736, Gradient norm: 419.60609360
INFO:root:[    5] Training loss: 121.75237315, Validation loss: 121.66151455, Gradient norm: 382.43289860
INFO:root:[    6] Training loss: 121.52564800, Validation loss: 121.68324964, Gradient norm: 259.91710984
INFO:root:[    7] Training loss: 121.46497527, Validation loss: 122.43072326, Gradient norm: 260.26639064
INFO:root:[    8] Training loss: 121.42333930, Validation loss: 121.01734109, Gradient norm: 340.64251309
INFO:root:[    9] Training loss: 121.14531593, Validation loss: 121.09885143, Gradient norm: 284.59907727
INFO:root:[   10] Training loss: 121.01014676, Validation loss: 120.83997003, Gradient norm: 295.97178046
INFO:root:[   11] Training loss: 120.73499764, Validation loss: 120.66568835, Gradient norm: 206.69986780
INFO:root:[   12] Training loss: 120.75831645, Validation loss: 120.55632519, Gradient norm: 231.33275548
INFO:root:[   13] Training loss: 120.58057748, Validation loss: 120.97385143, Gradient norm: 173.47014377
INFO:root:[   14] Training loss: 120.52805821, Validation loss: 120.55722861, Gradient norm: 159.82060470
INFO:root:[   15] Training loss: 120.49502840, Validation loss: 120.57163160, Gradient norm: 135.64734478
INFO:root:[   16] Training loss: 120.48954071, Validation loss: 120.41337217, Gradient norm: 119.62044587
INFO:root:[   17] Training loss: 120.50472476, Validation loss: 120.38545595, Gradient norm: 122.45538218
INFO:root:[   18] Training loss: 120.46846717, Validation loss: 121.06418425, Gradient norm: 98.61609144
INFO:root:[   19] Training loss: 120.46459482, Validation loss: 120.38408477, Gradient norm: 99.66181284
INFO:root:[   20] Training loss: 120.42382151, Validation loss: 120.49157478, Gradient norm: 97.57196015
INFO:root:[   21] Training loss: 120.40326340, Validation loss: 120.43292789, Gradient norm: 78.28682736
INFO:root:[   22] Training loss: 120.35994970, Validation loss: 120.32244689, Gradient norm: 78.20960030
INFO:root:[   23] Training loss: 120.22291214, Validation loss: 120.45568690, Gradient norm: 67.92276859
INFO:root:[   24] Training loss: 120.27640398, Validation loss: 120.31793292, Gradient norm: 63.57976346
INFO:root:[   25] Training loss: 120.35334217, Validation loss: 120.23070868, Gradient norm: 66.86535955
INFO:root:[   26] Training loss: 120.17686530, Validation loss: 120.17391415, Gradient norm: 55.71019516
INFO:root:[   27] Training loss: 120.20830023, Validation loss: 120.29450542, Gradient norm: 69.99857905
INFO:root:[   28] Training loss: 120.09743371, Validation loss: 120.26460003, Gradient norm: 56.05433644
INFO:root:[   29] Training loss: 119.87696352, Validation loss: 119.48179995, Gradient norm: 62.22111876
INFO:root:[   30] Training loss: 118.90901866, Validation loss: 118.43191213, Gradient norm: 66.91288732
INFO:root:[   31] Training loss: 117.59356109, Validation loss: 116.71980654, Gradient norm: 86.51692008
INFO:root:[   32] Training loss: 116.16060058, Validation loss: 115.81676825, Gradient norm: 123.72819763
INFO:root:[   33] Training loss: 115.08780258, Validation loss: 115.28037262, Gradient norm: 107.56232207
INFO:root:[   34] Training loss: 114.13546456, Validation loss: 113.62568691, Gradient norm: 108.54938137
INFO:root:[   35] Training loss: 113.24519024, Validation loss: 113.02443695, Gradient norm: 99.45803137
INFO:root:[   36] Training loss: 112.45224782, Validation loss: 112.24653810, Gradient norm: 77.47299349
INFO:root:[   37] Training loss: 111.81194778, Validation loss: 111.48090678, Gradient norm: 84.35590155
INFO:root:[   38] Training loss: 111.18245488, Validation loss: 111.07899396, Gradient norm: 87.27688105
INFO:root:[   39] Training loss: 110.65040892, Validation loss: 110.39340762, Gradient norm: 82.29425853
INFO:root:[   40] Training loss: 110.13571174, Validation loss: 109.84217414, Gradient norm: 63.06537136
INFO:root:[   41] Training loss: 109.84356271, Validation loss: 109.55704288, Gradient norm: 97.37559216
INFO:root:[   42] Training loss: 109.36629878, Validation loss: 108.99147034, Gradient norm: 79.28138127
INFO:root:[   43] Training loss: 108.97989270, Validation loss: 108.98201699, Gradient norm: 75.93465202
INFO:root:[   44] Training loss: 108.58872513, Validation loss: 108.73984659, Gradient norm: 66.40143474
INFO:root:[   45] Training loss: 108.34009930, Validation loss: 108.76715509, Gradient norm: 69.38048435
INFO:root:[   46] Training loss: 107.94140260, Validation loss: 107.95957973, Gradient norm: 78.25816229
INFO:root:[   47] Training loss: 107.52828831, Validation loss: 107.60320782, Gradient norm: 61.16905471
INFO:root:[   48] Training loss: 107.40468375, Validation loss: 107.36049757, Gradient norm: 72.95940164
INFO:root:[   49] Training loss: 107.17816263, Validation loss: 107.32248477, Gradient norm: 84.24779288
INFO:root:[   50] Training loss: 106.73881139, Validation loss: 106.63790788, Gradient norm: 77.97763758
INFO:root:[   51] Training loss: 106.55092992, Validation loss: 106.43828004, Gradient norm: 66.26970710
INFO:root:[   52] Training loss: 106.17237692, Validation loss: 106.29771976, Gradient norm: 72.71103697
INFO:root:[   53] Training loss: 106.15140851, Validation loss: 105.83023939, Gradient norm: 65.91960970
INFO:root:[   54] Training loss: 105.86586620, Validation loss: 105.61513756, Gradient norm: 72.49313963
INFO:root:[   55] Training loss: 105.61513074, Validation loss: 105.80269570, Gradient norm: 73.32550775
INFO:root:[   56] Training loss: 105.46113222, Validation loss: 105.37331785, Gradient norm: 74.66483815
INFO:root:[   57] Training loss: 105.26114337, Validation loss: 105.30077362, Gradient norm: 72.17932416
INFO:root:[   58] Training loss: 105.23645931, Validation loss: 105.14142661, Gradient norm: 74.98134468
INFO:root:[   59] Training loss: 104.99658622, Validation loss: 105.11468900, Gradient norm: 65.60033323
INFO:root:[   60] Training loss: 104.83669261, Validation loss: 104.88751615, Gradient norm: 68.49085216
INFO:root:[   61] Training loss: 104.64516571, Validation loss: 105.07489277, Gradient norm: 71.61440509
INFO:root:[   62] Training loss: 104.65266959, Validation loss: 104.51947627, Gradient norm: 75.94740218
INFO:root:[   63] Training loss: 104.52767404, Validation loss: 104.89307114, Gradient norm: 73.23591834
INFO:root:[   64] Training loss: 104.30670423, Validation loss: 104.24672909, Gradient norm: 67.11456797
INFO:root:[   65] Training loss: 104.09783969, Validation loss: 104.57060978, Gradient norm: 78.35431207
INFO:root:[   66] Training loss: 104.21212431, Validation loss: 104.59830133, Gradient norm: 64.20044887
INFO:root:[   67] Training loss: 104.01495922, Validation loss: 104.16922734, Gradient norm: 75.55440704
INFO:root:[   68] Training loss: 103.96011535, Validation loss: 104.20294295, Gradient norm: 72.77023268
INFO:root:[   69] Training loss: 103.86831706, Validation loss: 104.02274059, Gradient norm: 77.83612493
INFO:root:[   70] Training loss: 103.60202776, Validation loss: 104.03195796, Gradient norm: 62.24061901
INFO:root:[   71] Training loss: 103.62717444, Validation loss: 103.58907055, Gradient norm: 72.78996094
INFO:root:[   72] Training loss: 103.57076837, Validation loss: 103.88826515, Gradient norm: 78.26043169
INFO:root:[   73] Training loss: 103.30902552, Validation loss: 103.82722763, Gradient norm: 63.46597520
INFO:root:[   74] Training loss: 103.39998012, Validation loss: 103.95926087, Gradient norm: 76.60656651
INFO:root:[   75] Training loss: 103.37166096, Validation loss: 103.69460875, Gradient norm: 81.67064836
INFO:root:[   76] Training loss: 103.21936845, Validation loss: 103.28610782, Gradient norm: 67.88310549
INFO:root:[   77] Training loss: 103.16173959, Validation loss: 103.31530314, Gradient norm: 74.76092145
INFO:root:[   78] Training loss: 102.98432889, Validation loss: 103.07703610, Gradient norm: 66.55315526
INFO:root:[   79] Training loss: 103.02464747, Validation loss: 103.19265089, Gradient norm: 84.62889274
INFO:root:[   80] Training loss: 102.92978337, Validation loss: 103.02862312, Gradient norm: 68.82428227
INFO:root:[   81] Training loss: 102.90544743, Validation loss: 103.18328989, Gradient norm: 83.55439324
INFO:root:[   82] Training loss: 102.62439714, Validation loss: 103.24637393, Gradient norm: 74.67326415
INFO:root:[   83] Training loss: 102.54870801, Validation loss: 103.14542652, Gradient norm: 75.66875732
INFO:root:[   84] Training loss: 102.65314200, Validation loss: 102.62730329, Gradient norm: 77.84683379
INFO:root:[   85] Training loss: 102.52916171, Validation loss: 103.41077265, Gradient norm: 78.96628448
INFO:root:[   86] Training loss: 102.45538526, Validation loss: 102.82784219, Gradient norm: 77.83058492
INFO:root:[   87] Training loss: 102.32423549, Validation loss: 102.89083441, Gradient norm: 75.39966131
INFO:root:[   88] Training loss: 102.26705595, Validation loss: 102.84551292, Gradient norm: 74.79437169
INFO:root:[   89] Training loss: 102.13640844, Validation loss: 102.56783873, Gradient norm: 78.93089000
INFO:root:[   90] Training loss: 102.11276900, Validation loss: 102.63503581, Gradient norm: 71.90168406
INFO:root:[   91] Training loss: 102.13640081, Validation loss: 102.75630925, Gradient norm: 82.16445407
INFO:root:[   92] Training loss: 102.03124912, Validation loss: 102.58297861, Gradient norm: 76.29198903
INFO:root:[   93] Training loss: 101.98558348, Validation loss: 102.43878805, Gradient norm: 79.24452611
INFO:root:[   94] Training loss: 101.88327999, Validation loss: 102.43626956, Gradient norm: 77.27350357
INFO:root:[   95] Training loss: 101.94492131, Validation loss: 102.48679089, Gradient norm: 77.61526508
INFO:root:[   96] Training loss: 101.91434560, Validation loss: 102.09150064, Gradient norm: 80.40337635
INFO:root:[   97] Training loss: 101.78075760, Validation loss: 102.18502808, Gradient norm: 89.50514964
INFO:root:[   98] Training loss: 101.70197202, Validation loss: 102.07223432, Gradient norm: 73.77451702
INFO:root:[   99] Training loss: 101.60575462, Validation loss: 102.31591586, Gradient norm: 76.91700477
INFO:root:[  100] Training loss: 101.52786457, Validation loss: 102.07186732, Gradient norm: 69.39250312
INFO:root:[  101] Training loss: 101.63313347, Validation loss: 101.90164763, Gradient norm: 96.14283091
INFO:root:[  102] Training loss: 101.60297272, Validation loss: 101.93740818, Gradient norm: 66.96414459
INFO:root:[  103] Training loss: 101.50697077, Validation loss: 102.15365469, Gradient norm: 100.55747413
INFO:root:[  104] Training loss: 101.42871715, Validation loss: 101.85912481, Gradient norm: 59.68776384
INFO:root:[  105] Training loss: 101.42293441, Validation loss: 101.64098411, Gradient norm: 103.70051059
INFO:root:[  106] Training loss: 101.22890142, Validation loss: 101.69659082, Gradient norm: 79.93404541
INFO:root:[  107] Training loss: 101.32702934, Validation loss: 102.03833376, Gradient norm: 76.96900448
INFO:root:[  108] Training loss: 101.22940523, Validation loss: 101.88866898, Gradient norm: 88.65248963
INFO:root:[  109] Training loss: 101.06723360, Validation loss: 101.57784613, Gradient norm: 77.75500765
INFO:root:[  110] Training loss: 101.10596561, Validation loss: 101.66222960, Gradient norm: 83.14672130
INFO:root:[  111] Training loss: 101.06951344, Validation loss: 101.80770006, Gradient norm: 99.70288195
INFO:root:[  112] Training loss: 100.99366902, Validation loss: 101.47563882, Gradient norm: 84.84641050
INFO:root:[  113] Training loss: 100.89892349, Validation loss: 101.56389039, Gradient norm: 84.56743772
INFO:root:[  114] Training loss: 101.06983941, Validation loss: 101.70905725, Gradient norm: 86.92138422
INFO:root:[  115] Training loss: 100.89002241, Validation loss: 101.47397219, Gradient norm: 96.79544384
INFO:root:[  116] Training loss: 100.75274321, Validation loss: 101.71592055, Gradient norm: 85.54344239
INFO:root:[  117] Training loss: 100.82972846, Validation loss: 101.59828607, Gradient norm: 89.61043824
INFO:root:[  118] Training loss: 100.70214580, Validation loss: 101.34996191, Gradient norm: 75.42024975
INFO:root:[  119] Training loss: 100.66432521, Validation loss: 101.65163132, Gradient norm: 94.41914876
INFO:root:[  120] Training loss: 100.72859381, Validation loss: 101.15599560, Gradient norm: 95.03944502
INFO:root:[  121] Training loss: 100.63657278, Validation loss: 101.20321050, Gradient norm: 99.17052439
INFO:root:[  122] Training loss: 100.60375146, Validation loss: 101.34137094, Gradient norm: 105.31716366
INFO:root:[  123] Training loss: 100.61643111, Validation loss: 101.15953143, Gradient norm: 98.95146388
INFO:root:[  124] Training loss: 100.53558984, Validation loss: 101.21918356, Gradient norm: 80.88923947
INFO:root:[  125] Training loss: 100.37117139, Validation loss: 101.54649958, Gradient norm: 96.95799582
INFO:root:[  126] Training loss: 100.42641982, Validation loss: 101.25780513, Gradient norm: 100.53295210
INFO:root:[  127] Training loss: 100.37204783, Validation loss: 101.07541157, Gradient norm: 103.13502929
INFO:root:[  128] Training loss: 100.40560697, Validation loss: 100.85460689, Gradient norm: 94.78253581
INFO:root:[  129] Training loss: 100.33778179, Validation loss: 101.11858184, Gradient norm: 96.09929327
INFO:root:[  130] Training loss: 100.36523647, Validation loss: 100.76243486, Gradient norm: 122.04234585
INFO:root:[  131] Training loss: 100.21483220, Validation loss: 100.63083517, Gradient norm: 82.64222489
INFO:root:[  132] Training loss: 100.24882737, Validation loss: 101.04369644, Gradient norm: 109.55788461
INFO:root:[  133] Training loss: 100.20753756, Validation loss: 101.33959251, Gradient norm: 97.24582838
INFO:root:[  134] Training loss: 100.17074173, Validation loss: 100.59857651, Gradient norm: 116.82610815
INFO:root:[  135] Training loss: 100.05559101, Validation loss: 101.37751323, Gradient norm: 97.76397686
INFO:root:[  136] Training loss: 100.08888542, Validation loss: 100.82468520, Gradient norm: 96.20757008
INFO:root:[  137] Training loss: 100.01010766, Validation loss: 100.61914089, Gradient norm: 97.01474038
INFO:root:[  138] Training loss: 99.91717090, Validation loss: 100.74466916, Gradient norm: 108.28730787
INFO:root:[  139] Training loss: 99.97947551, Validation loss: 100.81424687, Gradient norm: 111.64611433
INFO:root:[  140] Training loss: 99.79519687, Validation loss: 100.67657076, Gradient norm: 104.12834684
INFO:root:[  141] Training loss: 99.86418753, Validation loss: 100.57775984, Gradient norm: 103.75661839
INFO:root:[  142] Training loss: 99.96297219, Validation loss: 100.70670661, Gradient norm: 126.53786986
INFO:root:[  143] Training loss: 99.69240503, Validation loss: 100.63513236, Gradient norm: 106.79645939
INFO:root:[  144] Training loss: 99.80308256, Validation loss: 100.82096968, Gradient norm: 129.92077051
INFO:root:[  145] Training loss: 99.72420691, Validation loss: 100.72697817, Gradient norm: 97.33282724
INFO:root:[  146] Training loss: 99.78145667, Validation loss: 100.72304956, Gradient norm: 132.47186990
INFO:root:[  147] Training loss: 99.65463372, Validation loss: 100.71685133, Gradient norm: 92.16811817
INFO:root:[  148] Training loss: 99.66171109, Validation loss: 100.41688353, Gradient norm: 123.04795153
INFO:root:[  149] Training loss: 99.67390260, Validation loss: 100.36146835, Gradient norm: 121.35582876
INFO:root:[  150] Training loss: 99.58669409, Validation loss: 100.40525055, Gradient norm: 114.25585244
INFO:root:[  151] Training loss: 99.58776133, Validation loss: 100.54662981, Gradient norm: 115.16387338
INFO:root:[  152] Training loss: 99.58499395, Validation loss: 100.41167476, Gradient norm: 131.03213128
INFO:root:[  153] Training loss: 99.49760903, Validation loss: 100.56736598, Gradient norm: 111.49002598
INFO:root:[  154] Training loss: 99.61052407, Validation loss: 100.44248436, Gradient norm: 131.69273335
INFO:root:[  155] Training loss: 99.59177432, Validation loss: 100.31075103, Gradient norm: 131.47956773
INFO:root:[  156] Training loss: 99.56253666, Validation loss: 100.46871343, Gradient norm: 125.28743210
INFO:root:[  157] Training loss: 99.42508988, Validation loss: 100.23656858, Gradient norm: 113.50438759
INFO:root:[  158] Training loss: 99.38026030, Validation loss: 100.54330339, Gradient norm: 132.32517361
INFO:root:[  159] Training loss: 99.37267877, Validation loss: 100.35320256, Gradient norm: 133.04639555
INFO:root:[  160] Training loss: 99.43364229, Validation loss: 100.26957597, Gradient norm: 122.85013110
INFO:root:[  161] Training loss: 99.35327020, Validation loss: 100.16363131, Gradient norm: 134.76028844
INFO:root:[  162] Training loss: 99.24714337, Validation loss: 100.55611130, Gradient norm: 128.63022400
INFO:root:[  163] Training loss: 99.30132098, Validation loss: 100.31837411, Gradient norm: 125.21942636
INFO:root:[  164] Training loss: 99.26418500, Validation loss: 100.69091429, Gradient norm: 136.78490144
INFO:root:[  165] Training loss: 99.22465616, Validation loss: 100.02334332, Gradient norm: 121.44380978
INFO:root:[  166] Training loss: 99.14520048, Validation loss: 100.70580108, Gradient norm: 150.89268264
INFO:root:[  167] Training loss: 99.12536898, Validation loss: 100.00882774, Gradient norm: 139.70087168
INFO:root:[  168] Training loss: 99.17826918, Validation loss: 100.12899438, Gradient norm: 145.78998127
INFO:root:[  169] Training loss: 99.17705138, Validation loss: 100.25658206, Gradient norm: 142.75257244
INFO:root:[  170] Training loss: 99.05817812, Validation loss: 100.66278997, Gradient norm: 127.20230582
INFO:root:[  171] Training loss: 99.14326821, Validation loss: 100.12946214, Gradient norm: 147.81523213
INFO:root:[  172] Training loss: 99.11709217, Validation loss: 100.10874386, Gradient norm: 162.29501260
INFO:root:[  173] Training loss: 99.01807809, Validation loss: 100.13086174, Gradient norm: 130.27101042
INFO:root:[  174] Training loss: 98.97040072, Validation loss: 100.07503062, Gradient norm: 136.25882890
INFO:root:[  175] Training loss: 99.06521174, Validation loss: 100.66644603, Gradient norm: 160.80329960
INFO:root:[  176] Training loss: 98.95303574, Validation loss: 100.01027732, Gradient norm: 145.44357292
INFO:root:EP 176: Early stopping
INFO:root:Training the model took 3124.523s.
INFO:root:Emptying the cuda cache took 0.045s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 140.47527
INFO:root:EnergyScoreTrain: 99.09023
INFO:root:CoverageTrain: 0.76186
INFO:root:IntervalWidthTrain: 7.97667
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 141.9959
INFO:root:EnergyScoreValidation: 100.15108
INFO:root:CoverageValidation: 0.75906
INFO:root:IntervalWidthValidation: 7.97418
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 142.27374
INFO:root:EnergyScoreTest: 100.34405
INFO:root:CoverageTest: 0.7586
INFO:root:IntervalWidthTest: 7.97576
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.82502828, Validation loss: 123.09220571, Gradient norm: 287.61161327
INFO:root:[    2] Training loss: 122.06922784, Validation loss: 122.02922742, Gradient norm: 234.91773083
INFO:root:[    3] Training loss: 121.83171480, Validation loss: 121.62568638, Gradient norm: 231.95427791
INFO:root:[    4] Training loss: 121.60699166, Validation loss: 121.88338365, Gradient norm: 215.49856511
INFO:root:[    5] Training loss: 121.33503629, Validation loss: 121.93415569, Gradient norm: 175.90648534
INFO:root:[    6] Training loss: 121.18097768, Validation loss: 121.14665906, Gradient norm: 185.01591708
INFO:root:[    7] Training loss: 120.91709204, Validation loss: 120.76078954, Gradient norm: 182.28073938
INFO:root:[    8] Training loss: 120.78574452, Validation loss: 121.47536232, Gradient norm: 192.72502939
INFO:root:[    9] Training loss: 120.61192808, Validation loss: 120.94598441, Gradient norm: 146.48721493
INFO:root:[   10] Training loss: 120.52745549, Validation loss: 120.56619973, Gradient norm: 167.32471433
INFO:root:[   11] Training loss: 120.43618349, Validation loss: 120.52786176, Gradient norm: 124.71371758
INFO:root:[   12] Training loss: 120.46682442, Validation loss: 120.68507122, Gradient norm: 162.74958759
INFO:root:[   13] Training loss: 120.39952310, Validation loss: 120.32668278, Gradient norm: 156.29527027
INFO:root:[   14] Training loss: 120.30915860, Validation loss: 120.80845616, Gradient norm: 124.07751011
INFO:root:[   15] Training loss: 120.16933421, Validation loss: 120.10768838, Gradient norm: 123.85693234
INFO:root:[   16] Training loss: 119.77999810, Validation loss: 119.30713312, Gradient norm: 142.89127834
INFO:root:[   17] Training loss: 118.59610316, Validation loss: 118.15231271, Gradient norm: 112.96726241
INFO:root:[   18] Training loss: 117.29786986, Validation loss: 116.67344823, Gradient norm: 143.99625976
INFO:root:[   19] Training loss: 115.91407195, Validation loss: 115.23847462, Gradient norm: 145.17777932
INFO:root:[   20] Training loss: 114.62141209, Validation loss: 114.15068949, Gradient norm: 84.96629198
INFO:root:[   21] Training loss: 113.64958697, Validation loss: 113.33960198, Gradient norm: 127.22151636
INFO:root:[   22] Training loss: 112.84588947, Validation loss: 112.65626921, Gradient norm: 106.28071310
INFO:root:[   23] Training loss: 112.17664641, Validation loss: 112.35051201, Gradient norm: 96.58774842
INFO:root:[   24] Training loss: 111.64314344, Validation loss: 111.53158569, Gradient norm: 93.62908576
INFO:root:[   25] Training loss: 111.14808202, Validation loss: 111.14751987, Gradient norm: 94.41777270
INFO:root:[   26] Training loss: 110.83770124, Validation loss: 110.80035690, Gradient norm: 119.87823525
INFO:root:[   27] Training loss: 110.42405343, Validation loss: 110.60714985, Gradient norm: 82.70124496
INFO:root:[   28] Training loss: 110.10482160, Validation loss: 110.33953910, Gradient norm: 99.38665268
INFO:root:[   29] Training loss: 109.86229827, Validation loss: 110.54636094, Gradient norm: 97.06948336
INFO:root:[   30] Training loss: 109.63583941, Validation loss: 109.69543667, Gradient norm: 104.28494020
INFO:root:[   31] Training loss: 109.35333623, Validation loss: 109.46659299, Gradient norm: 104.70334328
INFO:root:[   32] Training loss: 109.15017943, Validation loss: 109.22235607, Gradient norm: 105.64677102
INFO:root:[   33] Training loss: 108.85537706, Validation loss: 109.00556183, Gradient norm: 105.69816227
INFO:root:[   34] Training loss: 108.71351212, Validation loss: 108.98280019, Gradient norm: 127.64656667
INFO:root:[   35] Training loss: 108.40812292, Validation loss: 108.98787716, Gradient norm: 85.61448510
INFO:root:[   36] Training loss: 108.22121234, Validation loss: 108.59617957, Gradient norm: 75.72817080
INFO:root:[   37] Training loss: 108.15671830, Validation loss: 108.55671008, Gradient norm: 133.87310019
INFO:root:[   38] Training loss: 107.87831892, Validation loss: 108.39301695, Gradient norm: 102.46732837
INFO:root:[   39] Training loss: 107.71888760, Validation loss: 108.08918157, Gradient norm: 99.35487548
INFO:root:[   40] Training loss: 107.58984355, Validation loss: 107.62405606, Gradient norm: 114.52151847
INFO:root:[   41] Training loss: 107.29671640, Validation loss: 107.72044951, Gradient norm: 95.32484348
INFO:root:[   42] Training loss: 107.23137874, Validation loss: 107.80278989, Gradient norm: 126.03345888
INFO:root:[   43] Training loss: 107.04720860, Validation loss: 108.11310393, Gradient norm: 104.75719071
INFO:root:[   44] Training loss: 106.92795569, Validation loss: 107.50690618, Gradient norm: 115.96505445
INFO:root:[   45] Training loss: 106.77925441, Validation loss: 107.31477303, Gradient norm: 105.15259447
INFO:root:[   46] Training loss: 106.65785798, Validation loss: 106.98589272, Gradient norm: 117.39096096
INFO:root:[   47] Training loss: 106.43590087, Validation loss: 106.97403691, Gradient norm: 114.78272486
INFO:root:[   48] Training loss: 106.41145453, Validation loss: 106.98460046, Gradient norm: 125.51131641
INFO:root:[   49] Training loss: 106.25125129, Validation loss: 106.80749091, Gradient norm: 122.37204301
INFO:root:[   50] Training loss: 106.10511462, Validation loss: 106.55904283, Gradient norm: 125.13933084
INFO:root:[   51] Training loss: 106.01067886, Validation loss: 106.67147380, Gradient norm: 125.32506031
INFO:root:[   52] Training loss: 105.94832820, Validation loss: 106.45539803, Gradient norm: 117.56684233
INFO:root:[   53] Training loss: 105.73557606, Validation loss: 106.62703205, Gradient norm: 115.28425958
INFO:root:[   54] Training loss: 105.71043004, Validation loss: 106.18572393, Gradient norm: 130.83353963
INFO:root:[   55] Training loss: 105.58890236, Validation loss: 106.51103395, Gradient norm: 130.48645148
INFO:root:[   56] Training loss: 105.48792139, Validation loss: 105.96843562, Gradient norm: 136.10001511
INFO:root:[   57] Training loss: 105.39510474, Validation loss: 106.01998849, Gradient norm: 141.80927096
INFO:root:[   58] Training loss: 105.40948459, Validation loss: 105.94459008, Gradient norm: 150.35232599
INFO:root:[   59] Training loss: 105.16989716, Validation loss: 106.01609197, Gradient norm: 120.98626658
INFO:root:[   60] Training loss: 105.12444751, Validation loss: 106.25858780, Gradient norm: 144.22603708
INFO:root:[   61] Training loss: 104.96912823, Validation loss: 105.85264272, Gradient norm: 130.62003526
INFO:root:[   62] Training loss: 104.92666045, Validation loss: 105.59246695, Gradient norm: 138.03712794
INFO:root:[   63] Training loss: 104.86414972, Validation loss: 105.64627575, Gradient norm: 148.51887629
INFO:root:[   64] Training loss: 104.77729440, Validation loss: 105.70975915, Gradient norm: 148.25626538
INFO:root:[   65] Training loss: 104.72877050, Validation loss: 105.84294234, Gradient norm: 145.51229145
INFO:root:[   66] Training loss: 104.69340137, Validation loss: 105.35430198, Gradient norm: 157.76700707
INFO:root:[   67] Training loss: 104.54791550, Validation loss: 105.47893629, Gradient norm: 164.11393355
INFO:root:[   68] Training loss: 104.56822373, Validation loss: 105.43079481, Gradient norm: 172.78015290
INFO:root:[   69] Training loss: 104.34573047, Validation loss: 105.49934203, Gradient norm: 133.40500293
INFO:root:[   70] Training loss: 104.31786589, Validation loss: 105.24764173, Gradient norm: 172.44609234
INFO:root:[   71] Training loss: 104.25848922, Validation loss: 105.27373768, Gradient norm: 170.90030567
INFO:root:[   72] Training loss: 104.06587145, Validation loss: 105.10175429, Gradient norm: 147.11460917
INFO:root:[   73] Training loss: 104.17857806, Validation loss: 104.94200476, Gradient norm: 176.57726929
INFO:root:[   74] Training loss: 104.02168868, Validation loss: 105.28794177, Gradient norm: 164.54922591
INFO:root:[   75] Training loss: 104.02546152, Validation loss: 105.40577908, Gradient norm: 180.32038783
INFO:root:[   76] Training loss: 103.91854278, Validation loss: 104.82914313, Gradient norm: 144.97011135
INFO:root:[   77] Training loss: 103.81123190, Validation loss: 104.79281879, Gradient norm: 172.80337720
INFO:root:[   78] Training loss: 103.80443499, Validation loss: 105.02378898, Gradient norm: 176.03053062
INFO:root:[   79] Training loss: 103.78862465, Validation loss: 106.24815632, Gradient norm: 180.88738928
INFO:root:[   80] Training loss: 103.65558273, Validation loss: 105.67927157, Gradient norm: 175.82900642
INFO:root:[   81] Training loss: 103.62498670, Validation loss: 104.70377376, Gradient norm: 204.73420223
INFO:root:[   82] Training loss: 103.46843179, Validation loss: 104.77838450, Gradient norm: 180.19135943
INFO:root:[   83] Training loss: 103.48480832, Validation loss: 104.72060052, Gradient norm: 173.72827605
INFO:root:[   84] Training loss: 103.46312295, Validation loss: 104.70162464, Gradient norm: 205.85322565
INFO:root:[   85] Training loss: 103.29324044, Validation loss: 104.73739466, Gradient norm: 148.79519049
INFO:root:[   86] Training loss: 103.35634451, Validation loss: 104.60527249, Gradient norm: 200.81213748
INFO:root:[   87] Training loss: 103.20825020, Validation loss: 104.96035977, Gradient norm: 192.69330475
INFO:root:[   88] Training loss: 103.22316972, Validation loss: 104.63215506, Gradient norm: 179.59429582
INFO:root:[   89] Training loss: 103.16159996, Validation loss: 104.79383008, Gradient norm: 222.29649795
INFO:root:[   90] Training loss: 103.16937769, Validation loss: 104.50198917, Gradient norm: 223.04164073
INFO:root:[   91] Training loss: 103.00573886, Validation loss: 104.68268664, Gradient norm: 172.37982753
INFO:root:[   92] Training loss: 102.95249379, Validation loss: 104.39429342, Gradient norm: 196.05288302
INFO:root:[   93] Training loss: 102.89963329, Validation loss: 104.16725553, Gradient norm: 183.36205716
INFO:root:[   94] Training loss: 102.89586052, Validation loss: 104.45400791, Gradient norm: 196.19213526
INFO:root:[   95] Training loss: 102.85815781, Validation loss: 104.36034893, Gradient norm: 247.60264908
INFO:root:[   96] Training loss: 102.76650940, Validation loss: 104.13981260, Gradient norm: 202.63498244
INFO:root:[   97] Training loss: 102.72112754, Validation loss: 104.89945352, Gradient norm: 199.39847711
INFO:root:[   98] Training loss: 102.70484269, Validation loss: 104.33522849, Gradient norm: 237.59410299
INFO:root:[   99] Training loss: 102.65210008, Validation loss: 104.34306520, Gradient norm: 224.79173713
INFO:root:[  100] Training loss: 102.51205735, Validation loss: 104.14257523, Gradient norm: 200.27589155
INFO:root:[  101] Training loss: 102.49649149, Validation loss: 104.19504547, Gradient norm: 202.19667195
INFO:root:[  102] Training loss: 102.52139329, Validation loss: 104.11408523, Gradient norm: 260.34405317
INFO:root:[  103] Training loss: 102.43791685, Validation loss: 104.48748516, Gradient norm: 213.04437975
INFO:root:[  104] Training loss: 102.38879962, Validation loss: 103.93686992, Gradient norm: 252.54167376
INFO:root:[  105] Training loss: 102.29360786, Validation loss: 103.88505686, Gradient norm: 222.15277883
INFO:root:[  106] Training loss: 102.35876870, Validation loss: 104.16527478, Gradient norm: 254.56080047
INFO:root:[  107] Training loss: 102.24418681, Validation loss: 104.26870280, Gradient norm: 240.78949038
INFO:root:[  108] Training loss: 102.18890799, Validation loss: 104.09432983, Gradient norm: 220.13256766
INFO:root:[  109] Training loss: 102.18833464, Validation loss: 104.34306651, Gradient norm: 260.05878959
INFO:root:[  110] Training loss: 102.19297122, Validation loss: 104.13533362, Gradient norm: 271.87643723
INFO:root:[  111] Training loss: 102.13121573, Validation loss: 103.93319018, Gradient norm: 256.50052724
INFO:root:[  112] Training loss: 101.94965741, Validation loss: 104.76378237, Gradient norm: 226.89771967
INFO:root:[  113] Training loss: 102.00214528, Validation loss: 103.97218533, Gradient norm: 274.76129922
INFO:root:[  114] Training loss: 101.95790134, Validation loss: 104.04160546, Gradient norm: 257.75360507
INFO:root:EP 114: Early stopping
INFO:root:Training the model took 2009.226s.
INFO:root:Emptying the cuda cache took 0.044s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 144.74785
INFO:root:EnergyScoreTrain: 101.96299
INFO:root:CoverageTrain: 0.79831
INFO:root:IntervalWidthTrain: 8.31863
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.54526
INFO:root:EnergyScoreValidation: 103.93178
INFO:root:CoverageValidation: 0.79323
INFO:root:IntervalWidthValidation: 8.31347
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.58796
INFO:root:EnergyScoreTest: 103.96192
INFO:root:CoverageTest: 0.79212
INFO:root:IntervalWidthTest: 8.27823
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.19061948, Validation loss: 122.21224449, Gradient norm: 193.33589265
INFO:root:[    2] Training loss: 121.88814930, Validation loss: 122.38588662, Gradient norm: 200.31515436
INFO:root:[    3] Training loss: 121.45677820, Validation loss: 121.87170542, Gradient norm: 163.90562613
INFO:root:[    4] Training loss: 121.45497638, Validation loss: 121.30700079, Gradient norm: 148.63000884
INFO:root:[    5] Training loss: 121.05948551, Validation loss: 121.09848996, Gradient norm: 127.85157506
INFO:root:[    6] Training loss: 120.76624629, Validation loss: 120.80661063, Gradient norm: 109.76476254
INFO:root:[    7] Training loss: 120.63218176, Validation loss: 120.71108509, Gradient norm: 103.83929013
INFO:root:[    8] Training loss: 120.52242137, Validation loss: 120.64530235, Gradient norm: 109.55971280
INFO:root:[    9] Training loss: 120.40409615, Validation loss: 120.61606887, Gradient norm: 104.09573883
INFO:root:[   10] Training loss: 120.27063852, Validation loss: 120.29595895, Gradient norm: 85.11562927
INFO:root:[   11] Training loss: 119.73344509, Validation loss: 119.17900875, Gradient norm: 76.49979500
INFO:root:[   12] Training loss: 118.48282974, Validation loss: 118.04469641, Gradient norm: 65.93919177
INFO:root:[   13] Training loss: 117.14653852, Validation loss: 116.48382621, Gradient norm: 67.90745747
INFO:root:[   14] Training loss: 115.76812697, Validation loss: 115.24030909, Gradient norm: 67.49656114
INFO:root:[   15] Training loss: 114.62390461, Validation loss: 114.21473141, Gradient norm: 82.09168221
INFO:root:[   16] Training loss: 113.67996769, Validation loss: 113.33424614, Gradient norm: 56.09402007
INFO:root:[   17] Training loss: 112.89169494, Validation loss: 112.81107094, Gradient norm: 58.15795246
INFO:root:[   18] Training loss: 112.30265619, Validation loss: 112.21113928, Gradient norm: 78.07088883
INFO:root:[   19] Training loss: 111.70851777, Validation loss: 111.84118705, Gradient norm: 63.23622507
INFO:root:[   20] Training loss: 111.33390882, Validation loss: 111.31398168, Gradient norm: 72.33338578
INFO:root:[   21] Training loss: 110.92746451, Validation loss: 111.07168211, Gradient norm: 71.42756858
INFO:root:[   22] Training loss: 110.55660970, Validation loss: 110.64777743, Gradient norm: 67.83295899
INFO:root:[   23] Training loss: 110.33767403, Validation loss: 110.58432849, Gradient norm: 73.05890259
INFO:root:[   24] Training loss: 110.03097041, Validation loss: 110.31290725, Gradient norm: 72.23788559
INFO:root:[   25] Training loss: 109.76432976, Validation loss: 109.75294652, Gradient norm: 66.93033920
INFO:root:[   26] Training loss: 109.46023242, Validation loss: 109.93534851, Gradient norm: 69.52364148
INFO:root:[   27] Training loss: 109.32095924, Validation loss: 109.64175468, Gradient norm: 76.17125551
INFO:root:[   28] Training loss: 109.10939593, Validation loss: 109.35644900, Gradient norm: 66.96426158
INFO:root:[   29] Training loss: 108.92282192, Validation loss: 108.97087860, Gradient norm: 94.43945213
INFO:root:[   30] Training loss: 108.68628213, Validation loss: 108.88249627, Gradient norm: 68.98845125
INFO:root:[   31] Training loss: 108.55947302, Validation loss: 108.77527671, Gradient norm: 82.87045347
INFO:root:[   32] Training loss: 108.36128039, Validation loss: 108.93811325, Gradient norm: 79.45094823
INFO:root:[   33] Training loss: 108.19844825, Validation loss: 108.55681505, Gradient norm: 89.44262795
INFO:root:[   34] Training loss: 107.99635767, Validation loss: 108.33740103, Gradient norm: 81.39673145
INFO:root:[   35] Training loss: 107.88537652, Validation loss: 108.80735095, Gradient norm: 96.35498957
INFO:root:[   36] Training loss: 107.67715231, Validation loss: 107.96835275, Gradient norm: 77.68319023
INFO:root:[   37] Training loss: 107.60295753, Validation loss: 107.91460919, Gradient norm: 89.35912772
INFO:root:[   38] Training loss: 107.36172046, Validation loss: 107.80680137, Gradient norm: 82.15513394
INFO:root:[   39] Training loss: 107.26986181, Validation loss: 107.78675264, Gradient norm: 102.38087247
INFO:root:[   40] Training loss: 107.06762337, Validation loss: 107.47529234, Gradient norm: 71.05326468
INFO:root:[   41] Training loss: 107.02403050, Validation loss: 107.65176471, Gradient norm: 100.77721331
INFO:root:[   42] Training loss: 106.89366049, Validation loss: 107.35257090, Gradient norm: 103.24891675
INFO:root:[   43] Training loss: 106.70699081, Validation loss: 107.28668950, Gradient norm: 93.59805931
INFO:root:[   44] Training loss: 106.58375934, Validation loss: 107.28905882, Gradient norm: 117.66456310
INFO:root:[   45] Training loss: 106.49431306, Validation loss: 107.36064779, Gradient norm: 111.01617675
INFO:root:[   46] Training loss: 106.40063510, Validation loss: 106.93377422, Gradient norm: 107.60040657
INFO:root:[   47] Training loss: 106.31664850, Validation loss: 107.08107179, Gradient norm: 130.38974810
INFO:root:[   48] Training loss: 106.16394576, Validation loss: 106.75051617, Gradient norm: 103.20689961
INFO:root:[   49] Training loss: 106.06177723, Validation loss: 106.58524665, Gradient norm: 125.48992763
INFO:root:[   50] Training loss: 105.89613437, Validation loss: 106.40088048, Gradient norm: 116.25831650
INFO:root:[   51] Training loss: 105.79515582, Validation loss: 106.73746070, Gradient norm: 124.86359818
INFO:root:[   52] Training loss: 105.71251233, Validation loss: 107.01560237, Gradient norm: 140.58296095
INFO:root:[   53] Training loss: 105.64631214, Validation loss: 106.16314645, Gradient norm: 127.68310853
INFO:root:[   54] Training loss: 105.48129340, Validation loss: 105.93694016, Gradient norm: 136.36958269
INFO:root:[   55] Training loss: 105.45507542, Validation loss: 106.10102029, Gradient norm: 137.59120462
INFO:root:[   56] Training loss: 105.34043864, Validation loss: 106.00140881, Gradient norm: 141.71083846
INFO:root:[   57] Training loss: 105.25536164, Validation loss: 106.22798104, Gradient norm: 142.90254924
INFO:root:[   58] Training loss: 105.12124620, Validation loss: 105.77594968, Gradient norm: 169.82949922
INFO:root:[   59] Training loss: 104.97288419, Validation loss: 105.83620242, Gradient norm: 101.73166024
INFO:root:[   60] Training loss: 104.96868768, Validation loss: 106.18512489, Gradient norm: 159.38064731
INFO:root:[   61] Training loss: 104.84433632, Validation loss: 105.69999484, Gradient norm: 145.14298617
INFO:root:[   62] Training loss: 104.87334307, Validation loss: 105.55243262, Gradient norm: 176.76401914
INFO:root:[   63] Training loss: 104.67249345, Validation loss: 105.58108073, Gradient norm: 152.45666063
INFO:root:[   64] Training loss: 104.67682364, Validation loss: 105.51051646, Gradient norm: 190.99800941
INFO:root:[   65] Training loss: 104.59927456, Validation loss: 105.60671997, Gradient norm: 160.58826930
INFO:root:[   66] Training loss: 104.57565254, Validation loss: 106.02046835, Gradient norm: 170.77414218
INFO:root:[   67] Training loss: 104.44957652, Validation loss: 105.29439387, Gradient norm: 165.95587323
INFO:root:[   68] Training loss: 104.39412115, Validation loss: 105.16390070, Gradient norm: 200.26957199
INFO:root:[   69] Training loss: 104.38136001, Validation loss: 105.30257600, Gradient norm: 200.28478879
INFO:root:[   70] Training loss: 104.17888560, Validation loss: 105.58178422, Gradient norm: 185.04536196
INFO:root:[   71] Training loss: 104.20835741, Validation loss: 105.51070983, Gradient norm: 186.25795099
INFO:root:[   72] Training loss: 104.19127837, Validation loss: 105.24710977, Gradient norm: 206.17471715
INFO:root:[   73] Training loss: 104.03763331, Validation loss: 105.42093527, Gradient norm: 189.68195499
INFO:root:[   74] Training loss: 103.97818277, Validation loss: 104.87801861, Gradient norm: 207.10844015
INFO:root:[   75] Training loss: 103.96395557, Validation loss: 104.78383189, Gradient norm: 237.32527525
INFO:root:[   76] Training loss: 103.83260832, Validation loss: 104.69757896, Gradient norm: 199.18251811
INFO:root:[   77] Training loss: 103.89575330, Validation loss: 104.93994746, Gradient norm: 206.43214392
INFO:root:[   78] Training loss: 103.78817411, Validation loss: 104.81110408, Gradient norm: 234.65384019
INFO:root:[   79] Training loss: 103.76453629, Validation loss: 105.02958074, Gradient norm: 249.99082757
INFO:root:[   80] Training loss: 103.65461792, Validation loss: 105.07223616, Gradient norm: 253.01630449
INFO:root:[   81] Training loss: 103.58328153, Validation loss: 104.60779230, Gradient norm: 225.89433544
INFO:root:[   82] Training loss: 103.50385318, Validation loss: 104.85437380, Gradient norm: 234.67270791
INFO:root:[   83] Training loss: 103.50771507, Validation loss: 104.60480341, Gradient norm: 242.69625228
INFO:root:[   84] Training loss: 103.36749814, Validation loss: 104.54546672, Gradient norm: 211.06804571
INFO:root:[   85] Training loss: 103.41543869, Validation loss: 104.39727810, Gradient norm: 253.82140999
INFO:root:[   86] Training loss: 103.52191101, Validation loss: 104.88918778, Gradient norm: 287.69286457
INFO:root:[   87] Training loss: 103.25177009, Validation loss: 104.29132633, Gradient norm: 232.96017829
INFO:root:[   88] Training loss: 103.27914186, Validation loss: 104.51913742, Gradient norm: 259.73039590
INFO:root:[   89] Training loss: 103.25238827, Validation loss: 104.42362871, Gradient norm: 286.95010625
INFO:root:[   90] Training loss: 103.19538535, Validation loss: 104.44456061, Gradient norm: 287.87433084
INFO:root:[   91] Training loss: 103.05206387, Validation loss: 104.33290916, Gradient norm: 250.47419890
INFO:root:[   92] Training loss: 103.12325462, Validation loss: 104.26225149, Gradient norm: 282.96032288
INFO:root:[   93] Training loss: 103.05141186, Validation loss: 104.29686816, Gradient norm: 257.84065822
INFO:root:[   94] Training loss: 103.01195702, Validation loss: 104.20304502, Gradient norm: 285.10143377
INFO:root:[   95] Training loss: 102.87931905, Validation loss: 104.53058493, Gradient norm: 263.12738441
INFO:root:[   96] Training loss: 102.93889044, Validation loss: 104.27927083, Gradient norm: 283.93039992
INFO:root:[   97] Training loss: 102.82044402, Validation loss: 104.11710621, Gradient norm: 317.72814523
INFO:root:[   98] Training loss: 102.84045113, Validation loss: 104.11806014, Gradient norm: 312.87880195
INFO:root:[   99] Training loss: 102.82213971, Validation loss: 104.04184013, Gradient norm: 321.65680619
INFO:root:[  100] Training loss: 102.73589156, Validation loss: 104.43609540, Gradient norm: 295.70482601
INFO:root:[  101] Training loss: 102.70582554, Validation loss: 104.00671334, Gradient norm: 283.35434183
INFO:root:[  102] Training loss: 102.73106053, Validation loss: 104.72767139, Gradient norm: 361.49307841
INFO:root:[  103] Training loss: 102.57491485, Validation loss: 103.97127191, Gradient norm: 274.39072462
INFO:root:[  104] Training loss: 102.55957956, Validation loss: 104.54517022, Gradient norm: 300.48561058
INFO:root:[  105] Training loss: 102.57999933, Validation loss: 104.01879699, Gradient norm: 334.74973385
INFO:root:[  106] Training loss: 102.49101696, Validation loss: 103.99892583, Gradient norm: 290.55023217
INFO:root:[  107] Training loss: 102.52758944, Validation loss: 103.99713871, Gradient norm: 332.72997120
INFO:root:[  108] Training loss: 102.46610746, Validation loss: 104.06325031, Gradient norm: 308.79438689
INFO:root:[  109] Training loss: 102.50183504, Validation loss: 104.05225320, Gradient norm: 333.95008981
INFO:root:[  110] Training loss: 102.27088631, Validation loss: 104.36845793, Gradient norm: 299.66203920
INFO:root:[  111] Training loss: 102.33258826, Validation loss: 103.91590855, Gradient norm: 357.93981804
INFO:root:[  112] Training loss: 102.28368459, Validation loss: 103.92779936, Gradient norm: 307.63040898
INFO:root:[  113] Training loss: 102.30155985, Validation loss: 104.45138024, Gradient norm: 358.09516391
INFO:root:[  114] Training loss: 102.24186085, Validation loss: 104.08202651, Gradient norm: 338.28247506
INFO:root:[  115] Training loss: 102.27945925, Validation loss: 104.23023776, Gradient norm: 337.32320952
INFO:root:[  116] Training loss: 102.17289133, Validation loss: 103.86564689, Gradient norm: 364.51966999
INFO:root:[  117] Training loss: 102.10250949, Validation loss: 103.95944898, Gradient norm: 309.36809235
INFO:root:[  118] Training loss: 102.14081979, Validation loss: 103.82615241, Gradient norm: 391.29563215
INFO:root:[  119] Training loss: 101.98553966, Validation loss: 103.71055340, Gradient norm: 343.70664657
INFO:root:[  120] Training loss: 102.08911437, Validation loss: 104.08057982, Gradient norm: 308.78956125
INFO:root:[  121] Training loss: 102.04617910, Validation loss: 104.75445162, Gradient norm: 344.66834407
INFO:root:[  122] Training loss: 102.00262748, Validation loss: 104.53284060, Gradient norm: 351.67641385
INFO:root:[  123] Training loss: 101.95051629, Validation loss: 103.99950514, Gradient norm: 354.76714925
INFO:root:[  124] Training loss: 101.94842016, Validation loss: 103.95979230, Gradient norm: 368.51083461
INFO:root:[  125] Training loss: 101.93324935, Validation loss: 103.59753102, Gradient norm: 383.40917866
INFO:root:[  126] Training loss: 101.84742460, Validation loss: 103.57575200, Gradient norm: 334.82161009
INFO:root:[  127] Training loss: 101.81917883, Validation loss: 104.22309612, Gradient norm: 372.82558992
INFO:root:[  128] Training loss: 101.85923544, Validation loss: 103.47608343, Gradient norm: 384.73236445
INFO:root:[  129] Training loss: 101.70988505, Validation loss: 103.56309878, Gradient norm: 302.94392661
INFO:root:[  130] Training loss: 101.71606148, Validation loss: 104.86845845, Gradient norm: 356.21997289
INFO:root:[  131] Training loss: 101.71775838, Validation loss: 104.12894545, Gradient norm: 407.35244417
INFO:root:[  132] Training loss: 101.65583801, Validation loss: 103.77634351, Gradient norm: 377.62175132
INFO:root:[  133] Training loss: 101.57075872, Validation loss: 103.76653079, Gradient norm: 334.35830362
INFO:root:[  134] Training loss: 101.61886725, Validation loss: 103.76646607, Gradient norm: 359.04310915
INFO:root:[  135] Training loss: 101.59246745, Validation loss: 103.55141896, Gradient norm: 343.64507405
INFO:root:[  136] Training loss: 101.62187357, Validation loss: 104.02903774, Gradient norm: 408.64889793
INFO:root:[  137] Training loss: 101.54599134, Validation loss: 103.75718058, Gradient norm: 344.00937033
INFO:root:[  138] Training loss: 101.45307024, Validation loss: 103.45655770, Gradient norm: 367.81274939
INFO:root:[  139] Training loss: 101.47042259, Validation loss: 103.42258743, Gradient norm: 359.40564468
INFO:root:[  140] Training loss: 101.44024044, Validation loss: 103.47186779, Gradient norm: 357.57373635
INFO:root:[  141] Training loss: 101.34656417, Validation loss: 103.35391604, Gradient norm: 341.98056635
INFO:root:[  142] Training loss: 101.38902958, Validation loss: 103.30617602, Gradient norm: 357.58269299
INFO:root:[  143] Training loss: 101.43242652, Validation loss: 103.50343691, Gradient norm: 405.08032608
INFO:root:[  144] Training loss: 101.28699169, Validation loss: 103.58619927, Gradient norm: 348.33955172
INFO:root:[  145] Training loss: 101.33936749, Validation loss: 103.48654964, Gradient norm: 387.15706599
INFO:root:[  146] Training loss: 101.30391578, Validation loss: 104.05314452, Gradient norm: 337.29092259
INFO:root:[  147] Training loss: 101.27254351, Validation loss: 103.50908082, Gradient norm: 375.01692174
INFO:root:[  148] Training loss: 101.27146317, Validation loss: 103.89378410, Gradient norm: 390.93203019
INFO:root:[  149] Training loss: 101.19469378, Validation loss: 103.66111440, Gradient norm: 383.13641337
INFO:root:[  150] Training loss: 101.17785050, Validation loss: 103.44210500, Gradient norm: 321.46540098
INFO:root:[  151] Training loss: 101.11195637, Validation loss: 103.34107734, Gradient norm: 335.81557660
INFO:root:[  152] Training loss: 101.13777829, Validation loss: 103.25998425, Gradient norm: 392.44532670
INFO:root:[  153] Training loss: 101.20174948, Validation loss: 103.54285720, Gradient norm: 347.77011007
INFO:root:[  154] Training loss: 101.08493062, Validation loss: 103.58688276, Gradient norm: 342.39736919
INFO:root:[  155] Training loss: 101.09061580, Validation loss: 103.75697037, Gradient norm: 367.39113241
INFO:root:[  156] Training loss: 101.05487709, Validation loss: 103.86450616, Gradient norm: 347.51873407
INFO:root:[  157] Training loss: 101.03267508, Validation loss: 103.50132094, Gradient norm: 376.24829925
INFO:root:[  158] Training loss: 101.02313178, Validation loss: 103.43633428, Gradient norm: 341.91645278
INFO:root:[  159] Training loss: 100.90479420, Validation loss: 103.40583512, Gradient norm: 315.41268344
INFO:root:[  160] Training loss: 100.90130291, Validation loss: 103.65836624, Gradient norm: 359.78567213
INFO:root:[  161] Training loss: 100.97700663, Validation loss: 103.52845501, Gradient norm: 395.01975148
INFO:root:EP 161: Early stopping
INFO:root:Training the model took 2825.908s.
INFO:root:Emptying the cuda cache took 0.043s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 143.05807
INFO:root:EnergyScoreTrain: 100.7633
INFO:root:CoverageTrain: 0.79477
INFO:root:IntervalWidthTrain: 8.32709
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 146.66674
INFO:root:EnergyScoreValidation: 103.29779
INFO:root:CoverageValidation: 0.78767
INFO:root:IntervalWidthValidation: 8.31661
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 146.91571
INFO:root:EnergyScoreTest: 103.48087
INFO:root:CoverageTest: 0.78662
INFO:root:IntervalWidthTest: 8.29631
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 125.05896746, Validation loss: 122.14676587, Gradient norm: 160.17666876
INFO:root:[    2] Training loss: 121.81401055, Validation loss: 121.68384026, Gradient norm: 160.06212223
INFO:root:[    3] Training loss: 121.47651288, Validation loss: 121.60433355, Gradient norm: 139.39604600
INFO:root:[    4] Training loss: 121.45294588, Validation loss: 121.48581564, Gradient norm: 137.74877763
INFO:root:[    5] Training loss: 121.24053428, Validation loss: 121.07801266, Gradient norm: 136.35426836
INFO:root:[    6] Training loss: 120.92868299, Validation loss: 121.06871322, Gradient norm: 92.11280810
INFO:root:[    7] Training loss: 120.83403879, Validation loss: 120.77462900, Gradient norm: 117.35097813
INFO:root:[    8] Training loss: 120.62797641, Validation loss: 121.01507700, Gradient norm: 83.15886911
INFO:root:[    9] Training loss: 120.57384930, Validation loss: 120.44400761, Gradient norm: 104.40671629
INFO:root:[   10] Training loss: 120.45804481, Validation loss: 120.40102571, Gradient norm: 95.31471213
INFO:root:[   11] Training loss: 120.38152496, Validation loss: 120.66326904, Gradient norm: 81.80469187
INFO:root:[   12] Training loss: 120.27389715, Validation loss: 120.40321613, Gradient norm: 77.64522162
INFO:root:[   13] Training loss: 120.11647459, Validation loss: 120.02339593, Gradient norm: 65.35793344
INFO:root:[   14] Training loss: 119.40900374, Validation loss: 118.68056488, Gradient norm: 64.15603130
INFO:root:[   15] Training loss: 118.19546462, Validation loss: 117.82357999, Gradient norm: 54.98555319
INFO:root:[   16] Training loss: 117.24150977, Validation loss: 116.96040107, Gradient norm: 57.18987066
INFO:root:[   17] Training loss: 116.46781489, Validation loss: 116.10999719, Gradient norm: 54.18972748
INFO:root:[   18] Training loss: 115.69961210, Validation loss: 115.45388241, Gradient norm: 69.37200336
INFO:root:[   19] Training loss: 115.04963056, Validation loss: 115.16729105, Gradient norm: 55.62363764
INFO:root:[   20] Training loss: 114.44377251, Validation loss: 114.17335694, Gradient norm: 61.02396769
INFO:root:[   21] Training loss: 113.84686813, Validation loss: 114.10680968, Gradient norm: 62.89378461
INFO:root:[   22] Training loss: 113.36260034, Validation loss: 113.25433429, Gradient norm: 67.18554557
INFO:root:[   23] Training loss: 112.87183137, Validation loss: 113.10235806, Gradient norm: 58.00725680
INFO:root:[   24] Training loss: 112.48382676, Validation loss: 112.48860326, Gradient norm: 78.58390696
INFO:root:[   25] Training loss: 112.01809740, Validation loss: 112.02855761, Gradient norm: 65.11032141
INFO:root:[   26] Training loss: 111.69891931, Validation loss: 111.71365225, Gradient norm: 72.67473444
INFO:root:[   27] Training loss: 111.37262381, Validation loss: 111.48476962, Gradient norm: 75.52950191
INFO:root:[   28] Training loss: 111.04261638, Validation loss: 111.24350633, Gradient norm: 71.31521600
INFO:root:[   29] Training loss: 110.70216052, Validation loss: 110.96253073, Gradient norm: 75.17757158
INFO:root:[   30] Training loss: 110.50672845, Validation loss: 110.64985131, Gradient norm: 78.40429911
INFO:root:[   31] Training loss: 110.14996946, Validation loss: 110.46173622, Gradient norm: 69.41904202
INFO:root:[   32] Training loss: 109.98283211, Validation loss: 110.21317160, Gradient norm: 73.21760392
INFO:root:[   33] Training loss: 109.72132097, Validation loss: 109.79107929, Gradient norm: 78.06092657
INFO:root:[   34] Training loss: 109.52540217, Validation loss: 109.69942764, Gradient norm: 78.23064354
INFO:root:[   35] Training loss: 109.28799290, Validation loss: 109.59378604, Gradient norm: 78.15236058
INFO:root:[   36] Training loss: 109.07938169, Validation loss: 109.55726781, Gradient norm: 75.10600063
INFO:root:[   37] Training loss: 108.89422722, Validation loss: 109.18673233, Gradient norm: 90.68851225
INFO:root:[   38] Training loss: 108.73346784, Validation loss: 109.04213583, Gradient norm: 79.29673528
INFO:root:[   39] Training loss: 108.57805613, Validation loss: 108.91403672, Gradient norm: 101.90850405
INFO:root:[   40] Training loss: 108.34982793, Validation loss: 108.67003553, Gradient norm: 81.31371768
INFO:root:[   41] Training loss: 108.17680859, Validation loss: 108.69388817, Gradient norm: 98.98536474
INFO:root:[   42] Training loss: 107.97643705, Validation loss: 108.37998910, Gradient norm: 81.96955338
INFO:root:[   43] Training loss: 107.81160783, Validation loss: 108.16300807, Gradient norm: 88.34274571
INFO:root:[   44] Training loss: 107.65435150, Validation loss: 108.22690240, Gradient norm: 93.67981578
INFO:root:[   45] Training loss: 107.55111255, Validation loss: 107.96249547, Gradient norm: 94.88759562
INFO:root:[   46] Training loss: 107.40448585, Validation loss: 108.00306518, Gradient norm: 89.53289078
INFO:root:[   47] Training loss: 107.24166141, Validation loss: 107.67837761, Gradient norm: 101.57273390
INFO:root:[   48] Training loss: 107.13666886, Validation loss: 107.26956203, Gradient norm: 98.15938749
INFO:root:[   49] Training loss: 106.99919662, Validation loss: 107.61314603, Gradient norm: 99.42319965
INFO:root:[   50] Training loss: 106.89961087, Validation loss: 107.37193193, Gradient norm: 92.10636002
INFO:root:[   51] Training loss: 106.64830517, Validation loss: 107.28718698, Gradient norm: 89.14169454
INFO:root:[   52] Training loss: 106.56180147, Validation loss: 107.33466155, Gradient norm: 104.08731089
INFO:root:[   53] Training loss: 106.51556221, Validation loss: 106.97765534, Gradient norm: 115.54153661
INFO:root:[   54] Training loss: 106.39518859, Validation loss: 107.61081459, Gradient norm: 100.16557989
INFO:root:[   55] Training loss: 106.27386751, Validation loss: 106.94437514, Gradient norm: 111.31686614
INFO:root:[   56] Training loss: 106.18899394, Validation loss: 106.67015286, Gradient norm: 119.48534014
INFO:root:[   57] Training loss: 106.03342877, Validation loss: 106.47132874, Gradient norm: 107.88959236
INFO:root:[   58] Training loss: 105.97700804, Validation loss: 106.27290713, Gradient norm: 103.78669363
INFO:root:[   59] Training loss: 105.80669795, Validation loss: 106.69990198, Gradient norm: 119.16386315
INFO:root:[   60] Training loss: 105.79412828, Validation loss: 106.38521734, Gradient norm: 123.30926332
INFO:root:[   61] Training loss: 105.66985841, Validation loss: 106.39540100, Gradient norm: 117.93837420
INFO:root:[   62] Training loss: 105.53875712, Validation loss: 106.51487995, Gradient norm: 113.23432739
INFO:root:[   63] Training loss: 105.47129531, Validation loss: 106.22040268, Gradient norm: 121.20404504
INFO:root:[   64] Training loss: 105.34703921, Validation loss: 106.19024737, Gradient norm: 134.17286720
INFO:root:[   65] Training loss: 105.31996965, Validation loss: 105.95796283, Gradient norm: 138.21266744
INFO:root:[   66] Training loss: 105.21459623, Validation loss: 106.16209885, Gradient norm: 120.85491712
INFO:root:[   67] Training loss: 105.07506440, Validation loss: 105.72758773, Gradient norm: 159.53194414
INFO:root:[   68] Training loss: 105.07079146, Validation loss: 105.77270034, Gradient norm: 130.64150458
INFO:root:[   69] Training loss: 105.01949850, Validation loss: 105.72675639, Gradient norm: 145.88483127
INFO:root:[   70] Training loss: 104.93471203, Validation loss: 105.63548963, Gradient norm: 154.17944218
INFO:root:[   71] Training loss: 104.84649118, Validation loss: 105.55869714, Gradient norm: 153.91633557
INFO:root:[   72] Training loss: 104.69081433, Validation loss: 105.32087366, Gradient norm: 147.63147014
INFO:root:[   73] Training loss: 104.61965085, Validation loss: 105.51417015, Gradient norm: 144.23088434
INFO:root:[   74] Training loss: 104.68539753, Validation loss: 105.37808201, Gradient norm: 165.79217336
INFO:root:[   75] Training loss: 104.56828484, Validation loss: 105.25369920, Gradient norm: 159.33550672
INFO:root:[   76] Training loss: 104.54392972, Validation loss: 105.32124960, Gradient norm: 181.72802522
INFO:root:[   77] Training loss: 104.41026664, Validation loss: 105.61288820, Gradient norm: 155.51551901
INFO:root:[   78] Training loss: 104.36002768, Validation loss: 105.20001642, Gradient norm: 177.55147305
INFO:root:[   79] Training loss: 104.35312247, Validation loss: 105.12894913, Gradient norm: 185.87399827
INFO:root:[   80] Training loss: 104.26596225, Validation loss: 105.07436029, Gradient norm: 177.82717365
INFO:root:[   81] Training loss: 104.16752530, Validation loss: 105.12335784, Gradient norm: 183.48703137
INFO:root:[   82] Training loss: 104.18151511, Validation loss: 104.83773251, Gradient norm: 223.00328568
INFO:root:[   83] Training loss: 104.12485025, Validation loss: 105.00520219, Gradient norm: 199.23709729
INFO:root:[   84] Training loss: 104.07952314, Validation loss: 105.18943760, Gradient norm: 205.30053643
INFO:root:[   85] Training loss: 103.92109160, Validation loss: 105.09206285, Gradient norm: 180.61541642
INFO:root:[   86] Training loss: 103.97413946, Validation loss: 104.75617218, Gradient norm: 217.81480226
INFO:root:[   87] Training loss: 103.92076273, Validation loss: 104.75414802, Gradient norm: 214.36349091
INFO:root:[   88] Training loss: 103.77997252, Validation loss: 104.52990091, Gradient norm: 200.98336451
INFO:root:[   89] Training loss: 103.76691653, Validation loss: 104.68580522, Gradient norm: 219.89231879
INFO:root:[   90] Training loss: 103.69626428, Validation loss: 104.75722451, Gradient norm: 201.09446969
INFO:root:[   91] Training loss: 103.69178475, Validation loss: 104.48578118, Gradient norm: 211.73391329
INFO:root:[   92] Training loss: 103.73216869, Validation loss: 105.22684110, Gradient norm: 261.63467895
INFO:root:[   93] Training loss: 103.56041974, Validation loss: 104.69004111, Gradient norm: 229.23651532
INFO:root:[   94] Training loss: 103.51597514, Validation loss: 104.64294118, Gradient norm: 220.76939618
INFO:root:[   95] Training loss: 103.50619757, Validation loss: 104.61303369, Gradient norm: 237.92170274
INFO:root:[   96] Training loss: 103.49368117, Validation loss: 104.55140739, Gradient norm: 253.69305545
INFO:root:[   97] Training loss: 103.38496966, Validation loss: 104.37485557, Gradient norm: 232.66192581
INFO:root:[   98] Training loss: 103.41775216, Validation loss: 104.74673173, Gradient norm: 255.55543407
INFO:root:[   99] Training loss: 103.38398986, Validation loss: 104.66218567, Gradient norm: 274.05162266
INFO:root:[  100] Training loss: 103.30989291, Validation loss: 104.47298221, Gradient norm: 249.92659803
INFO:root:[  101] Training loss: 103.21274391, Validation loss: 104.51312414, Gradient norm: 236.72244924
INFO:root:[  102] Training loss: 103.28611202, Validation loss: 104.22612736, Gradient norm: 256.15871731
INFO:root:[  103] Training loss: 103.17647816, Validation loss: 104.34469815, Gradient norm: 266.27361333
INFO:root:[  104] Training loss: 103.17017918, Validation loss: 104.92757337, Gradient norm: 274.77939874
INFO:root:[  105] Training loss: 103.15718916, Validation loss: 104.15758646, Gradient norm: 266.18045247
INFO:root:[  106] Training loss: 103.10454391, Validation loss: 104.11460482, Gradient norm: 262.43459701
INFO:root:[  107] Training loss: 103.07762132, Validation loss: 104.33103890, Gradient norm: 297.53725719
INFO:root:[  108] Training loss: 103.08229146, Validation loss: 104.36057887, Gradient norm: 278.09585829
INFO:root:[  109] Training loss: 103.00509961, Validation loss: 104.36611912, Gradient norm: 295.54163117
INFO:root:[  110] Training loss: 103.06009823, Validation loss: 104.15384806, Gradient norm: 305.27533006
INFO:root:[  111] Training loss: 102.83457609, Validation loss: 104.21150997, Gradient norm: 241.57359309
INFO:root:[  112] Training loss: 102.82952996, Validation loss: 104.04758874, Gradient norm: 284.77367892
INFO:root:[  113] Training loss: 102.86910491, Validation loss: 103.81634443, Gradient norm: 269.62467070
INFO:root:[  114] Training loss: 102.89287473, Validation loss: 103.99297885, Gradient norm: 316.40945553
INFO:root:[  115] Training loss: 102.76699883, Validation loss: 103.95953027, Gradient norm: 255.91380609
INFO:root:[  116] Training loss: 102.73493262, Validation loss: 103.90372204, Gradient norm: 311.98339142
INFO:root:[  117] Training loss: 102.75783255, Validation loss: 103.78991042, Gradient norm: 301.92799220
INFO:root:[  118] Training loss: 102.70042541, Validation loss: 103.90122749, Gradient norm: 300.32088392
INFO:root:[  119] Training loss: 102.76989030, Validation loss: 104.39865744, Gradient norm: 349.76090036
INFO:root:[  120] Training loss: 102.61780818, Validation loss: 103.91992582, Gradient norm: 320.76401794
INFO:root:[  121] Training loss: 102.63158032, Validation loss: 104.43185846, Gradient norm: 315.37771939
INFO:root:[  122] Training loss: 102.58591873, Validation loss: 104.47303404, Gradient norm: 311.48305830
INFO:root:[  123] Training loss: 102.63213281, Validation loss: 104.68643267, Gradient norm: 360.86694981
INFO:root:[  124] Training loss: 102.59311392, Validation loss: 103.77711487, Gradient norm: 341.53587935
INFO:root:[  125] Training loss: 102.62632812, Validation loss: 103.94233230, Gradient norm: 322.33738970
INFO:root:[  126] Training loss: 102.51356655, Validation loss: 103.63585031, Gradient norm: 326.76694239
INFO:root:[  127] Training loss: 102.54195782, Validation loss: 103.69650347, Gradient norm: 317.98044710
INFO:root:[  128] Training loss: 102.56908451, Validation loss: 104.07496354, Gradient norm: 376.85727566
INFO:root:[  129] Training loss: 102.53028370, Validation loss: 103.89819678, Gradient norm: 366.38763808
INFO:root:[  130] Training loss: 102.39148462, Validation loss: 104.12176356, Gradient norm: 327.71550314
INFO:root:[  131] Training loss: 102.40613907, Validation loss: 103.69929057, Gradient norm: 328.60805554
INFO:root:[  132] Training loss: 102.34964388, Validation loss: 104.03091510, Gradient norm: 365.46055912
INFO:root:[  133] Training loss: 102.40403255, Validation loss: 103.55741146, Gradient norm: 356.85556263
INFO:root:[  134] Training loss: 102.29470744, Validation loss: 103.72809022, Gradient norm: 338.32567468
INFO:root:[  135] Training loss: 102.29619146, Validation loss: 104.23003756, Gradient norm: 372.95570893
INFO:root:[  136] Training loss: 102.37709349, Validation loss: 104.40128668, Gradient norm: 354.18286332
INFO:root:[  137] Training loss: 102.35018462, Validation loss: 104.37446173, Gradient norm: 378.65079607
INFO:root:[  138] Training loss: 102.26172631, Validation loss: 103.54567561, Gradient norm: 356.19767471
INFO:root:[  139] Training loss: 102.25820005, Validation loss: 103.72056132, Gradient norm: 393.33090937
INFO:root:[  140] Training loss: 102.24124010, Validation loss: 103.64602687, Gradient norm: 379.67887878
INFO:root:[  141] Training loss: 102.27280622, Validation loss: 103.73504902, Gradient norm: 375.87714075
INFO:root:[  142] Training loss: 102.21790516, Validation loss: 103.87560430, Gradient norm: 366.89329042
INFO:root:[  143] Training loss: 102.19291505, Validation loss: 103.76183266, Gradient norm: 382.50712505
INFO:root:[  144] Training loss: 102.27983093, Validation loss: 103.82744993, Gradient norm: 432.96359204
INFO:root:[  145] Training loss: 102.15378348, Validation loss: 105.55044029, Gradient norm: 356.52072677
INFO:root:[  146] Training loss: 102.17928193, Validation loss: 103.48722892, Gradient norm: 389.17378101
INFO:root:[  147] Training loss: 102.19174289, Validation loss: 103.79004406, Gradient norm: 410.12248164
INFO:root:[  148] Training loss: 102.32360273, Validation loss: 103.86606914, Gradient norm: 425.53810384
INFO:root:[  149] Training loss: 102.14925823, Validation loss: 103.53195006, Gradient norm: 393.80303123
INFO:root:[  150] Training loss: 102.21699173, Validation loss: 103.72421949, Gradient norm: 407.69314631
INFO:root:[  151] Training loss: 102.25387661, Validation loss: 103.52698885, Gradient norm: 430.22803789
INFO:root:[  152] Training loss: 102.32583719, Validation loss: 103.44470478, Gradient norm: 399.49440432
INFO:root:[  153] Training loss: 102.24868768, Validation loss: 103.52828559, Gradient norm: 407.90177110
INFO:root:[  154] Training loss: 102.20909051, Validation loss: 103.84367344, Gradient norm: 423.18917793
INFO:root:[  155] Training loss: 102.22908965, Validation loss: 103.69091455, Gradient norm: 475.28685560
INFO:root:[  156] Training loss: 102.22960663, Validation loss: 104.22833015, Gradient norm: 443.63859783
INFO:root:[  157] Training loss: 102.18007950, Validation loss: 103.43856759, Gradient norm: 462.61214131
INFO:root:[  158] Training loss: 102.16006814, Validation loss: 104.90759698, Gradient norm: 433.94449061
INFO:root:[  159] Training loss: 102.13004539, Validation loss: 103.80595635, Gradient norm: 434.70773851
INFO:root:[  160] Training loss: 102.26311682, Validation loss: 103.88688265, Gradient norm: 456.86120159
INFO:root:[  161] Training loss: 102.18063483, Validation loss: 103.46011642, Gradient norm: 492.40146569
INFO:root:[  162] Training loss: 102.17687745, Validation loss: 104.01350666, Gradient norm: 466.10914430
INFO:root:[  163] Training loss: 102.15520059, Validation loss: 103.47860376, Gradient norm: 467.17001704
INFO:root:[  164] Training loss: 102.11685174, Validation loss: 104.00645263, Gradient norm: 465.06036748
INFO:root:[  165] Training loss: 102.11952277, Validation loss: 104.49216645, Gradient norm: 495.67918839
INFO:root:[  166] Training loss: 102.17613706, Validation loss: 103.96059681, Gradient norm: 492.11210598
INFO:root:[  167] Training loss: 102.15764591, Validation loss: 103.35652766, Gradient norm: 504.39730714
INFO:root:[  168] Training loss: 102.06260506, Validation loss: 103.84458502, Gradient norm: 491.60170253
INFO:root:[  169] Training loss: 102.15904952, Validation loss: 103.45682184, Gradient norm: 476.33287705
INFO:root:[  170] Training loss: 102.29075427, Validation loss: 103.84475524, Gradient norm: 538.42977120
INFO:root:[  171] Training loss: 102.14722227, Validation loss: 103.71569324, Gradient norm: 476.54889733
INFO:root:[  172] Training loss: 102.12903608, Validation loss: 103.57133247, Gradient norm: 519.23505610
INFO:root:[  173] Training loss: 102.18549718, Validation loss: 103.66598037, Gradient norm: 541.02742159
INFO:root:[  174] Training loss: 102.09872821, Validation loss: 103.60748922, Gradient norm: 473.38383188
INFO:root:[  175] Training loss: 102.11570017, Validation loss: 103.61589471, Gradient norm: 537.93977439
INFO:root:[  176] Training loss: 102.08168158, Validation loss: 103.79123925, Gradient norm: 471.22339897
INFO:root:EP 176: Early stopping
INFO:root:Training the model took 3256.352s.
INFO:root:Emptying the cuda cache took 0.045s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 144.32776
INFO:root:EnergyScoreTrain: 101.69227
INFO:root:CoverageTrain: 0.76099
INFO:root:IntervalWidthTrain: 8.37259
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 146.85176
INFO:root:EnergyScoreValidation: 103.44192
INFO:root:CoverageValidation: 0.7569
INFO:root:IntervalWidthValidation: 8.3775
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 146.99907
INFO:root:EnergyScoreTest: 103.55295
INFO:root:CoverageTest: 0.75545
INFO:root:IntervalWidthTest: 8.35156
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 157286400
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.24573625, Validation loss: 121.76460003, Gradient norm: 115.14042929
INFO:root:[    2] Training loss: 121.46409965, Validation loss: 121.60283687, Gradient norm: 90.46783878
INFO:root:[    3] Training loss: 121.45343429, Validation loss: 121.52217418, Gradient norm: 88.81666247
INFO:root:[    4] Training loss: 121.37990050, Validation loss: 121.35391840, Gradient norm: 77.21625304
INFO:root:[    5] Training loss: 121.27100278, Validation loss: 121.26053304, Gradient norm: 71.68176824
INFO:root:[    6] Training loss: 121.04631083, Validation loss: 121.43030285, Gradient norm: 61.22633033
INFO:root:[    7] Training loss: 120.77101142, Validation loss: 120.79355963, Gradient norm: 52.84866944
INFO:root:[    8] Training loss: 120.65546053, Validation loss: 120.55855850, Gradient norm: 47.02047850
INFO:root:[    9] Training loss: 120.48893886, Validation loss: 120.49118568, Gradient norm: 60.73840231
INFO:root:[   10] Training loss: 120.25554704, Validation loss: 120.07764908, Gradient norm: 43.02952884
INFO:root:[   11] Training loss: 119.82352252, Validation loss: 119.52585733, Gradient norm: 44.83125319
INFO:root:[   12] Training loss: 119.10771767, Validation loss: 118.74599088, Gradient norm: 48.72971838
INFO:root:[   13] Training loss: 118.38206306, Validation loss: 118.29994360, Gradient norm: 39.17635388
INFO:root:[   14] Training loss: 117.80423473, Validation loss: 117.79773528, Gradient norm: 44.02107222
INFO:root:[   15] Training loss: 117.23025519, Validation loss: 116.93263113, Gradient norm: 49.97947840
INFO:root:[   16] Training loss: 116.66231922, Validation loss: 116.77595652, Gradient norm: 45.18572899
INFO:root:[   17] Training loss: 116.13062928, Validation loss: 116.12959026, Gradient norm: 39.77799745
INFO:root:[   18] Training loss: 115.71113492, Validation loss: 115.72914492, Gradient norm: 44.71763645
INFO:root:[   19] Training loss: 115.34676516, Validation loss: 115.38137239, Gradient norm: 43.90819091
INFO:root:[   20] Training loss: 115.00131428, Validation loss: 114.98739756, Gradient norm: 48.69967958
INFO:root:[   21] Training loss: 114.69618522, Validation loss: 114.49362788, Gradient norm: 55.23187545
INFO:root:[   22] Training loss: 114.45374804, Validation loss: 114.33171818, Gradient norm: 46.18104237
INFO:root:[   23] Training loss: 114.15658893, Validation loss: 114.36273404, Gradient norm: 63.45500583
INFO:root:[   24] Training loss: 113.88054779, Validation loss: 113.84357768, Gradient norm: 53.35474382
INFO:root:[   25] Training loss: 113.65439775, Validation loss: 113.51449480, Gradient norm: 56.35135749
INFO:root:[   26] Training loss: 113.35707707, Validation loss: 113.16934862, Gradient norm: 57.69534775
INFO:root:[   27] Training loss: 113.16281493, Validation loss: 113.10253406, Gradient norm: 64.55842145
INFO:root:[   28] Training loss: 112.95633988, Validation loss: 113.03515283, Gradient norm: 70.31714073
INFO:root:[   29] Training loss: 112.65082185, Validation loss: 112.73894606, Gradient norm: 53.58427370
INFO:root:[   30] Training loss: 112.47818432, Validation loss: 112.30046134, Gradient norm: 63.60964612
INFO:root:[   31] Training loss: 112.16181669, Validation loss: 112.08792904, Gradient norm: 59.60489690
INFO:root:[   32] Training loss: 112.02453634, Validation loss: 112.02750607, Gradient norm: 67.61910623
INFO:root:[   33] Training loss: 111.85383154, Validation loss: 112.05089753, Gradient norm: 66.21469894
INFO:root:[   34] Training loss: 111.62580055, Validation loss: 111.94749398, Gradient norm: 80.69465864
INFO:root:[   35] Training loss: 111.40535918, Validation loss: 111.53940661, Gradient norm: 72.27896542
INFO:root:[   36] Training loss: 111.19876625, Validation loss: 111.57889794, Gradient norm: 65.69147818
INFO:root:[   37] Training loss: 111.19558236, Validation loss: 111.52490050, Gradient norm: 74.89420077
INFO:root:[   38] Training loss: 110.98046510, Validation loss: 111.15400354, Gradient norm: 74.36924336
INFO:root:[   39] Training loss: 110.86202800, Validation loss: 110.99922233, Gradient norm: 88.66347659
INFO:root:[   40] Training loss: 110.60660992, Validation loss: 110.92459448, Gradient norm: 79.29881893
INFO:root:[   41] Training loss: 110.52591091, Validation loss: 110.63294562, Gradient norm: 82.18152718
INFO:root:[   42] Training loss: 110.37403431, Validation loss: 110.47921358, Gradient norm: 91.12717541
INFO:root:[   43] Training loss: 110.22144007, Validation loss: 110.25544476, Gradient norm: 85.17103228
INFO:root:[   44] Training loss: 110.13942361, Validation loss: 110.21855006, Gradient norm: 94.93760506
INFO:root:[   45] Training loss: 109.95061844, Validation loss: 110.22053844, Gradient norm: 98.27471206
INFO:root:[   46] Training loss: 109.92056268, Validation loss: 110.08301123, Gradient norm: 123.80152666
INFO:root:[   47] Training loss: 109.66177247, Validation loss: 109.77230730, Gradient norm: 82.98157277
INFO:root:[   48] Training loss: 109.70814494, Validation loss: 109.86117738, Gradient norm: 121.44051665
INFO:root:[   49] Training loss: 109.53463475, Validation loss: 109.73128878, Gradient norm: 118.58658935
INFO:root:[   50] Training loss: 109.40909057, Validation loss: 109.66433874, Gradient norm: 101.70220008
INFO:root:[   51] Training loss: 109.34984818, Validation loss: 109.28823116, Gradient norm: 117.52181776
INFO:root:[   52] Training loss: 109.22875072, Validation loss: 109.16659520, Gradient norm: 125.01749950
INFO:root:[   53] Training loss: 109.19312550, Validation loss: 109.26632006, Gradient norm: 140.27729087
INFO:root:[   54] Training loss: 108.99235737, Validation loss: 109.31914126, Gradient norm: 146.63750690
INFO:root:[   55] Training loss: 108.83085605, Validation loss: 109.05670824, Gradient norm: 148.22518622
INFO:root:[   56] Training loss: 108.76497529, Validation loss: 108.95658006, Gradient norm: 131.23707599
INFO:root:[   57] Training loss: 108.59620214, Validation loss: 108.92873251, Gradient norm: 134.74743757
INFO:root:[   58] Training loss: 108.66972432, Validation loss: 108.83556103, Gradient norm: 177.07252354
INFO:root:[   59] Training loss: 108.51675496, Validation loss: 108.77759973, Gradient norm: 164.32914031
INFO:root:[   60] Training loss: 108.41848633, Validation loss: 108.58537845, Gradient norm: 162.28515197
INFO:root:[   61] Training loss: 108.33812808, Validation loss: 108.64503821, Gradient norm: 188.39142405
INFO:root:[   62] Training loss: 108.18846380, Validation loss: 108.68437169, Gradient norm: 151.41035228
INFO:root:[   63] Training loss: 108.14182417, Validation loss: 108.17077032, Gradient norm: 178.71107969
INFO:root:[   64] Training loss: 108.23801841, Validation loss: 108.58795087, Gradient norm: 203.70421295
INFO:root:[   65] Training loss: 107.99951800, Validation loss: 108.25349821, Gradient norm: 175.47468035
INFO:root:[   66] Training loss: 107.90703137, Validation loss: 108.03820459, Gradient norm: 195.15206589
INFO:root:[   67] Training loss: 107.87050899, Validation loss: 107.94918928, Gradient norm: 208.52360554
INFO:root:[   68] Training loss: 107.76546100, Validation loss: 107.99164634, Gradient norm: 203.58579229
INFO:root:[   69] Training loss: 107.68827962, Validation loss: 107.79221712, Gradient norm: 229.51389307
INFO:root:[   70] Training loss: 107.59731522, Validation loss: 107.56174101, Gradient norm: 214.69111094
INFO:root:[   71] Training loss: 107.49328782, Validation loss: 107.80745776, Gradient norm: 206.02921798
INFO:root:[   72] Training loss: 107.49203984, Validation loss: 107.98972899, Gradient norm: 217.19480445
INFO:root:[   73] Training loss: 107.44586580, Validation loss: 107.62187432, Gradient norm: 234.75273084
INFO:root:[   74] Training loss: 107.30892978, Validation loss: 107.63311583, Gradient norm: 235.73673836
INFO:root:[   75] Training loss: 107.20660400, Validation loss: 107.58702877, Gradient norm: 236.95412324
INFO:root:[   76] Training loss: 107.38100568, Validation loss: 107.83927654, Gradient norm: 221.85287702
INFO:root:[   77] Training loss: 107.15375728, Validation loss: 107.37972523, Gradient norm: 238.08421321
INFO:root:[   78] Training loss: 107.16825090, Validation loss: 107.10901984, Gradient norm: 247.61994087
INFO:root:[   79] Training loss: 107.19749768, Validation loss: 107.20634303, Gradient norm: 265.49232520
INFO:root:[   80] Training loss: 107.13337410, Validation loss: 107.12995437, Gradient norm: 274.84672238
INFO:root:[   81] Training loss: 106.91549595, Validation loss: 107.04236708, Gradient norm: 244.51794883
INFO:root:[   82] Training loss: 107.01643081, Validation loss: 106.96229395, Gradient norm: 300.82661727
INFO:root:[   83] Training loss: 106.94072129, Validation loss: 106.88371671, Gradient norm: 270.68313300
INFO:root:[   84] Training loss: 106.89115980, Validation loss: 107.52762130, Gradient norm: 280.14187802
INFO:root:[   85] Training loss: 106.77789212, Validation loss: 106.96848534, Gradient norm: 260.00155302
INFO:root:[   86] Training loss: 106.73892860, Validation loss: 106.94073486, Gradient norm: 277.61342485
INFO:root:[   87] Training loss: 106.78942831, Validation loss: 106.76510199, Gradient norm: 277.43663065
INFO:root:[   88] Training loss: 106.60551351, Validation loss: 106.76518460, Gradient norm: 261.65041448
INFO:root:[   89] Training loss: 106.65824971, Validation loss: 106.73488591, Gradient norm: 290.60155793
INFO:root:[   90] Training loss: 106.49477771, Validation loss: 106.99252977, Gradient norm: 286.17058811
INFO:root:[   91] Training loss: 106.57292304, Validation loss: 107.19354090, Gradient norm: 307.12999575
INFO:root:[   92] Training loss: 106.39027729, Validation loss: 106.52843054, Gradient norm: 283.19075291
INFO:root:[   93] Training loss: 106.40108166, Validation loss: 106.55833540, Gradient norm: 321.41163645
INFO:root:[   94] Training loss: 106.35562350, Validation loss: 106.48599585, Gradient norm: 304.50741371
INFO:root:[   95] Training loss: 106.27769355, Validation loss: 106.82072475, Gradient norm: 264.76845820
INFO:root:[   96] Training loss: 106.31319920, Validation loss: 106.60701568, Gradient norm: 325.26452947
INFO:root:[   97] Training loss: 106.30477784, Validation loss: 106.43387814, Gradient norm: 308.36069180
INFO:root:[   98] Training loss: 106.29957837, Validation loss: 106.26190475, Gradient norm: 318.59101303
INFO:root:[   99] Training loss: 106.18741797, Validation loss: 106.26085268, Gradient norm: 294.78487655
INFO:root:[  100] Training loss: 106.24887132, Validation loss: 106.30426052, Gradient norm: 339.34515019
INFO:root:[  101] Training loss: 106.11118560, Validation loss: 106.40442763, Gradient norm: 310.47926786
INFO:root:[  102] Training loss: 106.19684959, Validation loss: 106.18268664, Gradient norm: 338.23394701
INFO:root:[  103] Training loss: 106.10429065, Validation loss: 106.37742825, Gradient norm: 354.82028079
INFO:root:[  104] Training loss: 105.99379737, Validation loss: 107.15553757, Gradient norm: 318.63448607
INFO:root:[  105] Training loss: 105.96129547, Validation loss: 106.32894950, Gradient norm: 318.14564965
INFO:root:[  106] Training loss: 105.97028040, Validation loss: 106.83619453, Gradient norm: 354.14678256
INFO:root:[  107] Training loss: 105.95946780, Validation loss: 106.57342135, Gradient norm: 327.31975106
INFO:root:[  108] Training loss: 105.94153325, Validation loss: 106.28200584, Gradient norm: 356.88726474
INFO:root:[  109] Training loss: 105.86165666, Validation loss: 105.88822332, Gradient norm: 341.25835297
INFO:root:[  110] Training loss: 105.84542232, Validation loss: 106.03398658, Gradient norm: 359.58949240
INFO:root:[  111] Training loss: 105.94913267, Validation loss: 106.77153383, Gradient norm: 364.57142151
INFO:root:[  112] Training loss: 105.76654620, Validation loss: 106.29475061, Gradient norm: 390.35680167
INFO:root:[  113] Training loss: 105.79608087, Validation loss: 106.11299870, Gradient norm: 387.21145761
INFO:root:[  114] Training loss: 105.72350271, Validation loss: 106.07791006, Gradient norm: 359.82261373
INFO:root:[  115] Training loss: 105.69083587, Validation loss: 105.84686306, Gradient norm: 397.83439068
INFO:root:[  116] Training loss: 105.71009759, Validation loss: 106.34503937, Gradient norm: 385.57948950
INFO:root:[  117] Training loss: 105.83145223, Validation loss: 106.31175048, Gradient norm: 442.92562925
INFO:root:[  118] Training loss: 105.79975540, Validation loss: 106.05153788, Gradient norm: 404.12278366
INFO:root:[  119] Training loss: 105.72930631, Validation loss: 106.00577335, Gradient norm: 410.66268327
INFO:root:[  120] Training loss: 105.68237230, Validation loss: 106.04639198, Gradient norm: 430.59610683
INFO:root:[  121] Training loss: 105.74757912, Validation loss: 106.86622935, Gradient norm: 440.29282958
INFO:root:[  122] Training loss: 105.66169016, Validation loss: 106.09950888, Gradient norm: 401.36818023
INFO:root:[  123] Training loss: 105.73086784, Validation loss: 106.16327667, Gradient norm: 459.31382863
INFO:root:[  124] Training loss: 105.55828473, Validation loss: 105.73849487, Gradient norm: 422.26511448
INFO:root:[  125] Training loss: 105.74119689, Validation loss: 105.83313962, Gradient norm: 448.87946058
INFO:root:[  126] Training loss: 105.61111970, Validation loss: 105.87061389, Gradient norm: 446.11949102
INFO:root:[  127] Training loss: 105.79184250, Validation loss: 106.62289008, Gradient norm: 493.97683459
INFO:root:[  128] Training loss: 105.56737052, Validation loss: 105.91937598, Gradient norm: 434.39207226
INFO:root:[  129] Training loss: 105.58027588, Validation loss: 106.17045093, Gradient norm: 485.60777030
INFO:root:[  130] Training loss: 105.59428514, Validation loss: 105.76143962, Gradient norm: 503.50156788
INFO:root:[  131] Training loss: 105.74952914, Validation loss: 106.10682178, Gradient norm: 474.43954772
INFO:root:[  132] Training loss: 105.59112184, Validation loss: 105.95738667, Gradient norm: 504.57977122
INFO:root:[  133] Training loss: 105.48562122, Validation loss: 106.50818029, Gradient norm: 509.45776307
INFO:root:EP 133: Early stopping
INFO:root:Training the model took 2493.267s.
INFO:root:Emptying the cuda cache took 0.047s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 149.15353
INFO:root:EnergyScoreTrain: 105.31881
INFO:root:CoverageTrain: 0.63009
INFO:root:IntervalWidthTrain: 8.11452
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 149.71774
INFO:root:EnergyScoreValidation: 105.69105
INFO:root:CoverageValidation: 0.62847
INFO:root:IntervalWidthValidation: 8.08868
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 149.96485
INFO:root:EnergyScoreTest: 105.85403
INFO:root:CoverageTest: 0.62805
INFO:root:IntervalWidthTest: 8.08914
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 163577856
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 121.62510310, Validation loss: 121.31433842, Gradient norm: 13.69209809
INFO:root:[    2] Training loss: 121.28673884, Validation loss: 121.42639976, Gradient norm: 5.63192453
INFO:root:[    3] Training loss: 120.84981530, Validation loss: 120.53225734, Gradient norm: 8.82317012
INFO:root:[    4] Training loss: 120.40035687, Validation loss: 120.00813504, Gradient norm: 9.70272257
INFO:root:[    5] Training loss: 118.45648794, Validation loss: 116.79323762, Gradient norm: 15.91989705
INFO:root:[    6] Training loss: 115.83685755, Validation loss: 114.62659560, Gradient norm: 23.48385762
INFO:root:[    7] Training loss: 113.98485329, Validation loss: 112.82872641, Gradient norm: 23.17986828
INFO:root:[    8] Training loss: 112.68342813, Validation loss: 111.68286449, Gradient norm: 24.34558894
INFO:root:[    9] Training loss: 111.65979355, Validation loss: 110.64907284, Gradient norm: 25.76919385
INFO:root:[   10] Training loss: 110.86882316, Validation loss: 110.22511870, Gradient norm: 25.42507881
INFO:root:[   11] Training loss: 110.31133945, Validation loss: 109.57073343, Gradient norm: 27.46906906
INFO:root:[   12] Training loss: 109.80864500, Validation loss: 109.20449592, Gradient norm: 26.01957148
INFO:root:[   13] Training loss: 109.37657037, Validation loss: 108.84560999, Gradient norm: 28.43238943
INFO:root:[   14] Training loss: 109.01413990, Validation loss: 108.44177799, Gradient norm: 30.09453577
INFO:root:[   15] Training loss: 108.65964481, Validation loss: 108.13885787, Gradient norm: 27.40970684
INFO:root:[   16] Training loss: 108.37866927, Validation loss: 107.83756282, Gradient norm: 31.73646871
INFO:root:[   17] Training loss: 108.13746407, Validation loss: 107.31394485, Gradient norm: 32.94673040
INFO:root:[   18] Training loss: 107.83142900, Validation loss: 107.40976347, Gradient norm: 32.57967890
INFO:root:[   19] Training loss: 107.64703079, Validation loss: 107.12902490, Gradient norm: 37.35822276
INFO:root:[   20] Training loss: 107.37012603, Validation loss: 106.87948529, Gradient norm: 35.87375762
INFO:root:[   21] Training loss: 107.16397027, Validation loss: 106.85220837, Gradient norm: 38.54338082
INFO:root:[   22] Training loss: 106.99054961, Validation loss: 106.59346534, Gradient norm: 39.33427983
INFO:root:[   23] Training loss: 106.79672937, Validation loss: 106.22343945, Gradient norm: 39.98425035
INFO:root:[   24] Training loss: 106.63459933, Validation loss: 106.08154060, Gradient norm: 39.87207294
INFO:root:[   25] Training loss: 106.42684984, Validation loss: 106.11561716, Gradient norm: 45.49732585
INFO:root:[   26] Training loss: 106.29790753, Validation loss: 106.00986717, Gradient norm: 45.48638849
INFO:root:[   27] Training loss: 106.16691238, Validation loss: 105.76253431, Gradient norm: 50.77747665
INFO:root:[   28] Training loss: 105.98870181, Validation loss: 105.58014311, Gradient norm: 51.07418013
INFO:root:[   29] Training loss: 105.84238076, Validation loss: 105.46433574, Gradient norm: 43.48497924
INFO:root:[   30] Training loss: 105.72066261, Validation loss: 105.43916768, Gradient norm: 53.49137588
INFO:root:[   31] Training loss: 105.59401885, Validation loss: 105.42328381, Gradient norm: 51.01864774
INFO:root:[   32] Training loss: 105.50822631, Validation loss: 105.19084957, Gradient norm: 62.28429269
INFO:root:[   33] Training loss: 105.41223374, Validation loss: 105.39267520, Gradient norm: 64.18947179
INFO:root:[   34] Training loss: 105.28196581, Validation loss: 105.13847377, Gradient norm: 61.65956466
INFO:root:[   35] Training loss: 105.15927597, Validation loss: 104.87762530, Gradient norm: 63.91147376
INFO:root:[   36] Training loss: 105.06020997, Validation loss: 104.98715052, Gradient norm: 71.29726931
INFO:root:[   37] Training loss: 104.95024251, Validation loss: 104.88358017, Gradient norm: 75.11553336
INFO:root:[   38] Training loss: 104.87263138, Validation loss: 104.59831343, Gradient norm: 72.84671572
INFO:root:[   39] Training loss: 104.78631065, Validation loss: 104.69040969, Gradient norm: 77.65036267
INFO:root:[   40] Training loss: 104.70021861, Validation loss: 104.58872433, Gradient norm: 78.49676732
INFO:root:[   41] Training loss: 104.63634835, Validation loss: 104.78281639, Gradient norm: 75.72796875
INFO:root:[   42] Training loss: 104.52128331, Validation loss: 104.46904439, Gradient norm: 89.39790581
INFO:root:[   43] Training loss: 104.41938586, Validation loss: 104.31652674, Gradient norm: 79.50879131
INFO:root:[   44] Training loss: 104.36667964, Validation loss: 104.34220807, Gradient norm: 88.00096919
INFO:root:[   45] Training loss: 104.32923558, Validation loss: 104.40785138, Gradient norm: 95.06627239
INFO:root:[   46] Training loss: 104.22444787, Validation loss: 104.37108638, Gradient norm: 94.44283280
INFO:root:[   47] Training loss: 104.10586825, Validation loss: 104.16890480, Gradient norm: 92.75660627
INFO:root:[   48] Training loss: 104.06154660, Validation loss: 104.03691575, Gradient norm: 93.55825585
INFO:root:[   49] Training loss: 104.02311734, Validation loss: 104.16359842, Gradient norm: 101.35839007
INFO:root:[   50] Training loss: 103.92277817, Validation loss: 104.01917819, Gradient norm: 110.31760380
INFO:root:[   51] Training loss: 103.88509862, Validation loss: 104.06724917, Gradient norm: 102.93351780
INFO:root:[   52] Training loss: 103.82661897, Validation loss: 104.03161358, Gradient norm: 125.95561607
INFO:root:[   53] Training loss: 103.76683159, Validation loss: 103.95411840, Gradient norm: 120.83039277
INFO:root:[   54] Training loss: 103.66746926, Validation loss: 104.12502447, Gradient norm: 102.46192964
INFO:root:[   55] Training loss: 103.68831196, Validation loss: 103.81338711, Gradient norm: 136.42141397
INFO:root:[   56] Training loss: 103.55498133, Validation loss: 104.40724051, Gradient norm: 101.86209478
INFO:root:[   57] Training loss: 103.51182131, Validation loss: 103.89145766, Gradient norm: 131.12185043
INFO:root:[   58] Training loss: 103.42366730, Validation loss: 103.86519149, Gradient norm: 125.92926712
INFO:root:[   59] Training loss: 103.41680645, Validation loss: 103.81758354, Gradient norm: 146.72402782
INFO:root:[   60] Training loss: 103.30813194, Validation loss: 103.61989278, Gradient norm: 125.60797651
INFO:root:[   61] Training loss: 103.24359387, Validation loss: 103.69873336, Gradient norm: 147.15548587
INFO:root:[   62] Training loss: 103.25183423, Validation loss: 103.69569871, Gradient norm: 152.18464301
INFO:root:[   63] Training loss: 103.11545239, Validation loss: 103.65611504, Gradient norm: 140.99091047
INFO:root:[   64] Training loss: 103.09840481, Validation loss: 103.70607021, Gradient norm: 153.72344912
INFO:root:[   65] Training loss: 103.03198911, Validation loss: 103.48956299, Gradient norm: 162.40088317
INFO:root:[   66] Training loss: 102.96910467, Validation loss: 103.59239854, Gradient norm: 163.92614150
INFO:root:[   67] Training loss: 102.90134531, Validation loss: 103.53437411, Gradient norm: 151.54089751
INFO:root:[   68] Training loss: 102.87675598, Validation loss: 103.49242743, Gradient norm: 171.99304721
INFO:root:[   69] Training loss: 102.80364349, Validation loss: 103.79016955, Gradient norm: 166.51091722
INFO:root:[   70] Training loss: 102.75162324, Validation loss: 103.25967486, Gradient norm: 163.49351366
INFO:root:[   71] Training loss: 102.75209829, Validation loss: 103.64258733, Gradient norm: 176.95978007
INFO:root:[   72] Training loss: 102.70220927, Validation loss: 103.36752714, Gradient norm: 186.46533593
INFO:root:[   73] Training loss: 102.60399135, Validation loss: 103.38258678, Gradient norm: 168.35090744
INFO:root:[   74] Training loss: 102.53611728, Validation loss: 103.51671890, Gradient norm: 185.90737998
INFO:root:[   75] Training loss: 102.57294545, Validation loss: 103.46297639, Gradient norm: 188.51759990
INFO:root:[   76] Training loss: 102.51071018, Validation loss: 103.44715329, Gradient norm: 194.13011307
INFO:root:[   77] Training loss: 102.43158020, Validation loss: 103.69714908, Gradient norm: 196.73579372
INFO:root:[   78] Training loss: 102.40664342, Validation loss: 103.57206542, Gradient norm: 213.43759827
INFO:root:[   79] Training loss: 102.33488032, Validation loss: 103.31949668, Gradient norm: 196.79197545
INFO:root:EP 79: Early stopping
INFO:root:Training the model took 1109.497s.
INFO:root:Emptying the cuda cache took 0.024s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 143.64208
INFO:root:EnergyScoreTrain: 101.11206
INFO:root:CoverageTrain: 0.35778
INFO:root:IntervalWidthTrain: 4.92988
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 146.78637
INFO:root:EnergyScoreValidation: 103.35674
INFO:root:CoverageValidation: 0.35663
INFO:root:IntervalWidthValidation: 4.92563
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.08306
INFO:root:EnergyScoreTest: 103.56721
INFO:root:CoverageTest: 0.35824
INFO:root:IntervalWidthTest: 4.94861
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.24101987, Validation loss: 121.51244933, Gradient norm: 20.19453520
INFO:root:[    2] Training loss: 121.39058003, Validation loss: 121.28323627, Gradient norm: 9.16088774
INFO:root:[    3] Training loss: 121.00641463, Validation loss: 120.73676011, Gradient norm: 7.54406950
INFO:root:[    4] Training loss: 120.48274413, Validation loss: 119.82516690, Gradient norm: 7.07223613
INFO:root:[    5] Training loss: 118.87123587, Validation loss: 116.94578657, Gradient norm: 8.69974864
INFO:root:[    6] Training loss: 116.62251505, Validation loss: 114.79670110, Gradient norm: 14.56926773
INFO:root:[    7] Training loss: 114.95903852, Validation loss: 113.08095603, Gradient norm: 14.94954892
INFO:root:[    8] Training loss: 113.88934205, Validation loss: 112.19288846, Gradient norm: 18.38163955
INFO:root:[    9] Training loss: 113.06971390, Validation loss: 111.28429123, Gradient norm: 19.34084276
INFO:root:[   10] Training loss: 112.45142189, Validation loss: 111.21553829, Gradient norm: 27.15328018
INFO:root:[   11] Training loss: 111.85928419, Validation loss: 110.24180998, Gradient norm: 25.15100101
INFO:root:[   12] Training loss: 111.48477456, Validation loss: 109.83652812, Gradient norm: 29.89755335
INFO:root:[   13] Training loss: 111.10155872, Validation loss: 109.46795260, Gradient norm: 30.15018665
INFO:root:[   14] Training loss: 110.77678599, Validation loss: 109.06005438, Gradient norm: 32.64304968
INFO:root:[   15] Training loss: 110.50225384, Validation loss: 109.22227873, Gradient norm: 35.98090653
INFO:root:[   16] Training loss: 110.27425844, Validation loss: 108.86995066, Gradient norm: 42.67284239
INFO:root:[   17] Training loss: 110.05302031, Validation loss: 108.72341051, Gradient norm: 39.58003785
INFO:root:[   18] Training loss: 109.80468244, Validation loss: 108.22777452, Gradient norm: 45.44693167
INFO:root:[   19] Training loss: 109.66518233, Validation loss: 108.25528428, Gradient norm: 50.28322385
INFO:root:[   20] Training loss: 109.51301689, Validation loss: 107.79773923, Gradient norm: 52.64597724
INFO:root:[   21] Training loss: 109.34124601, Validation loss: 107.99888848, Gradient norm: 62.52600329
INFO:root:[   22] Training loss: 109.17528176, Validation loss: 107.83865251, Gradient norm: 58.43808645
INFO:root:[   23] Training loss: 109.05594027, Validation loss: 107.67793169, Gradient norm: 71.30125119
INFO:root:[   24] Training loss: 108.89026662, Validation loss: 107.84250404, Gradient norm: 62.63567826
INFO:root:[   25] Training loss: 108.75926276, Validation loss: 107.82588196, Gradient norm: 66.30163117
INFO:root:[   26] Training loss: 108.65459543, Validation loss: 107.18352167, Gradient norm: 89.82642200
INFO:root:[   27] Training loss: 108.54771565, Validation loss: 106.89814443, Gradient norm: 83.52431827
INFO:root:[   28] Training loss: 108.38062590, Validation loss: 106.73350972, Gradient norm: 87.35877292
INFO:root:[   29] Training loss: 108.29312276, Validation loss: 107.19279690, Gradient norm: 92.34100515
INFO:root:[   30] Training loss: 108.17915108, Validation loss: 106.25351557, Gradient norm: 106.23080294
INFO:root:[   31] Training loss: 108.17758570, Validation loss: 107.20699810, Gradient norm: 110.16948455
INFO:root:[   32] Training loss: 108.12081369, Validation loss: 106.51716456, Gradient norm: 118.07691100
INFO:root:[   33] Training loss: 107.97694627, Validation loss: 106.54799599, Gradient norm: 128.44208857
INFO:root:[   34] Training loss: 107.89205703, Validation loss: 107.27661264, Gradient norm: 118.01583909
INFO:root:[   35] Training loss: 107.84682262, Validation loss: 106.34598147, Gradient norm: 132.85966078
INFO:root:[   36] Training loss: 107.82673145, Validation loss: 107.03509995, Gradient norm: 149.15470084
INFO:root:[   37] Training loss: 107.70475830, Validation loss: 105.80120981, Gradient norm: 140.34913069
INFO:root:[   38] Training loss: 107.57844429, Validation loss: 106.02362139, Gradient norm: 140.37628539
INFO:root:[   39] Training loss: 107.59247954, Validation loss: 105.80892997, Gradient norm: 159.11775505
INFO:root:[   40] Training loss: 107.52242239, Validation loss: 106.22306982, Gradient norm: 157.06230889
INFO:root:[   41] Training loss: 107.51481986, Validation loss: 105.73643967, Gradient norm: 177.56488452
INFO:root:[   42] Training loss: 107.35997948, Validation loss: 105.35854129, Gradient norm: 174.11247928
INFO:root:[   43] Training loss: 107.32733006, Validation loss: 105.22982762, Gradient norm: 175.59526908
INFO:root:[   44] Training loss: 107.30645813, Validation loss: 105.56888738, Gradient norm: 203.13797781
INFO:root:[   45] Training loss: 107.30035090, Validation loss: 106.55728228, Gradient norm: 213.17209880
INFO:root:[   46] Training loss: 107.23320109, Validation loss: 105.86411496, Gradient norm: 208.35952224
INFO:root:[   47] Training loss: 107.13282931, Validation loss: 106.17750181, Gradient norm: 205.64080648
INFO:root:[   48] Training loss: 107.12844707, Validation loss: 105.36793781, Gradient norm: 232.06472789
INFO:root:[   49] Training loss: 107.10582207, Validation loss: 105.51973540, Gradient norm: 227.88683113
INFO:root:[   50] Training loss: 106.95515435, Validation loss: 105.61711015, Gradient norm: 220.36922416
INFO:root:[   51] Training loss: 106.92494816, Validation loss: 105.54942243, Gradient norm: 220.99112845
INFO:root:[   52] Training loss: 106.90329864, Validation loss: 104.86594943, Gradient norm: 255.27922194
INFO:root:[   53] Training loss: 106.84490798, Validation loss: 105.12779052, Gradient norm: 252.41464230
INFO:root:[   54] Training loss: 106.81994116, Validation loss: 106.52569896, Gradient norm: 267.50726596
INFO:root:[   55] Training loss: 106.72395986, Validation loss: 105.32295411, Gradient norm: 252.29881830
INFO:root:[   56] Training loss: 106.69279439, Validation loss: 104.98384436, Gradient norm: 266.66802773
INFO:root:[   57] Training loss: 106.66142948, Validation loss: 104.60765418, Gradient norm: 262.68844374
INFO:root:[   58] Training loss: 106.65925969, Validation loss: 104.87715912, Gradient norm: 286.86173857
INFO:root:[   59] Training loss: 106.63707396, Validation loss: 104.66774723, Gradient norm: 296.53998996
INFO:root:[   60] Training loss: 106.64660719, Validation loss: 104.93244671, Gradient norm: 295.96902574
INFO:root:[   61] Training loss: 106.57527579, Validation loss: 104.79448016, Gradient norm: 293.22400763
INFO:root:[   62] Training loss: 106.55681799, Validation loss: 105.15121460, Gradient norm: 325.01151639
INFO:root:[   63] Training loss: 106.50585377, Validation loss: 104.76514251, Gradient norm: 326.93904281
INFO:root:[   64] Training loss: 106.50485243, Validation loss: 104.68174323, Gradient norm: 341.08505852
INFO:root:[   65] Training loss: 106.42746613, Validation loss: 104.13853297, Gradient norm: 307.86753179
INFO:root:[   66] Training loss: 106.43600572, Validation loss: 105.34397757, Gradient norm: 339.20848219
INFO:root:[   67] Training loss: 106.42372395, Validation loss: 105.01224097, Gradient norm: 362.41663837
INFO:root:[   68] Training loss: 106.39585411, Validation loss: 104.64086467, Gradient norm: 319.37303281
INFO:root:[   69] Training loss: 106.35825895, Validation loss: 104.44401024, Gradient norm: 382.80424534
INFO:root:[   70] Training loss: 106.34385789, Validation loss: 104.40887819, Gradient norm: 377.73368323
INFO:root:[   71] Training loss: 106.28068873, Validation loss: 104.73812577, Gradient norm: 402.70461543
INFO:root:[   72] Training loss: 106.30141787, Validation loss: 104.35604253, Gradient norm: 426.10349328
INFO:root:[   73] Training loss: 106.19095922, Validation loss: 104.73388856, Gradient norm: 410.76112772
INFO:root:[   74] Training loss: 106.28037289, Validation loss: 104.42022916, Gradient norm: 435.29921459
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 965.028s.
INFO:root:Emptying the cuda cache took 0.022s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 146.9254
INFO:root:EnergyScoreTrain: 103.45185
INFO:root:CoverageTrain: 0.17166
INFO:root:IntervalWidthTrain: 3.3837
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 148.15233
INFO:root:EnergyScoreValidation: 104.33821
INFO:root:CoverageValidation: 0.1714
INFO:root:IntervalWidthValidation: 3.37981
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 148.30862
INFO:root:EnergyScoreTest: 104.44202
INFO:root:CoverageTest: 0.17182
INFO:root:IntervalWidthTest: 3.3872
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 123.01555201, Validation loss: 121.74307461, Gradient norm: 25.66635739
INFO:root:[    2] Training loss: 121.49646813, Validation loss: 121.67415698, Gradient norm: 4.86589690
INFO:root:[    3] Training loss: 121.36840253, Validation loss: 121.55229424, Gradient norm: 4.23799436
INFO:root:[    4] Training loss: 121.32793467, Validation loss: 121.51106420, Gradient norm: 5.06280154
INFO:root:[    5] Training loss: 121.30132672, Validation loss: 121.36031447, Gradient norm: 4.60393592
INFO:root:[    6] Training loss: 121.10735827, Validation loss: 120.84707037, Gradient norm: 4.49562441
INFO:root:[    7] Training loss: 120.76349991, Validation loss: 120.48371072, Gradient norm: 5.61485869
INFO:root:[    8] Training loss: 119.91254634, Validation loss: 118.71660561, Gradient norm: 7.11815321
INFO:root:[    9] Training loss: 118.31033980, Validation loss: 116.61599363, Gradient norm: 15.61285848
INFO:root:[   10] Training loss: 117.03521776, Validation loss: 115.27287135, Gradient norm: 19.09846584
INFO:root:[   11] Training loss: 115.91945459, Validation loss: 114.06897946, Gradient norm: 23.45329854
INFO:root:[   12] Training loss: 115.15770755, Validation loss: 113.45260831, Gradient norm: 30.82558264
INFO:root:[   13] Training loss: 114.58734144, Validation loss: 112.67060642, Gradient norm: 37.59073322
INFO:root:[   14] Training loss: 114.07061052, Validation loss: 112.65072158, Gradient norm: 38.86425079
INFO:root:[   15] Training loss: 113.67098898, Validation loss: 111.89748514, Gradient norm: 44.39234726
INFO:root:[   16] Training loss: 113.37927280, Validation loss: 112.97376409, Gradient norm: 52.39319904
INFO:root:[   17] Training loss: 113.03058678, Validation loss: 111.50274816, Gradient norm: 56.26950119
INFO:root:[   18] Training loss: 112.77792932, Validation loss: 111.45901542, Gradient norm: 60.16450344
INFO:root:[   19] Training loss: 112.56550180, Validation loss: 111.88712679, Gradient norm: 76.43175642
INFO:root:[   20] Training loss: 112.29352394, Validation loss: 111.44596100, Gradient norm: 71.48612595
INFO:root:[   21] Training loss: 112.12509493, Validation loss: 112.75129384, Gradient norm: 68.38913699
INFO:root:[   22] Training loss: 111.98041379, Validation loss: 112.80959978, Gradient norm: 91.05480146
INFO:root:[   23] Training loss: 111.84759960, Validation loss: 113.96810150, Gradient norm: 89.15356460
INFO:root:[   24] Training loss: 111.60778545, Validation loss: 111.45125159, Gradient norm: 90.53900549
INFO:root:[   25] Training loss: 111.55617078, Validation loss: 114.74748177, Gradient norm: 108.45055220
INFO:root:[   26] Training loss: 111.43521976, Validation loss: 112.28549194, Gradient norm: 106.30284298
INFO:root:[   27] Training loss: 111.37945847, Validation loss: 117.05701789, Gradient norm: 109.19873916
INFO:root:[   28] Training loss: 111.24325744, Validation loss: 114.55889998, Gradient norm: 123.42564167
INFO:root:[   29] Training loss: 111.08415816, Validation loss: 114.01788462, Gradient norm: 117.05572188
INFO:root:[   30] Training loss: 111.07912283, Validation loss: 116.44791649, Gradient norm: 122.15076538
INFO:root:[   31] Training loss: 110.90266952, Validation loss: 114.69698913, Gradient norm: 122.08035753
INFO:root:[   32] Training loss: 110.87832642, Validation loss: 116.48563122, Gradient norm: 134.48700886
INFO:root:[   33] Training loss: 110.78690912, Validation loss: 118.44811065, Gradient norm: 149.44993417
INFO:root:[   34] Training loss: 110.72249995, Validation loss: 112.88953058, Gradient norm: 151.57826796
INFO:root:[   35] Training loss: 110.64219004, Validation loss: 115.72810495, Gradient norm: 146.05465218
INFO:root:[   36] Training loss: 110.54318264, Validation loss: 118.79258281, Gradient norm: 156.76568573
INFO:root:[   37] Training loss: 110.45595962, Validation loss: 118.30919305, Gradient norm: 151.53535921
INFO:root:[   38] Training loss: 110.43754429, Validation loss: 119.67105234, Gradient norm: 166.28952355
INFO:root:[   39] Training loss: 110.42585970, Validation loss: 118.18472606, Gradient norm: 201.29482680
INFO:root:[   40] Training loss: 110.33918647, Validation loss: 121.50786012, Gradient norm: 188.94589840
INFO:root:[   41] Training loss: 110.29050601, Validation loss: 124.67703800, Gradient norm: 197.03110823
INFO:root:[   42] Training loss: 110.27536949, Validation loss: 119.07158714, Gradient norm: 202.80400037
INFO:root:[   43] Training loss: 110.28540066, Validation loss: 121.31567646, Gradient norm: 225.82050040
INFO:root:[   44] Training loss: 110.15250127, Validation loss: 122.78687655, Gradient norm: 199.51329633
INFO:root:[   45] Training loss: 110.10449766, Validation loss: 125.59052513, Gradient norm: 182.93849367
INFO:root:[   46] Training loss: 110.03489415, Validation loss: 128.87150100, Gradient norm: 220.46434198
INFO:root:[   47] Training loss: 110.00211219, Validation loss: 125.00329932, Gradient norm: 193.05445809
INFO:root:[   48] Training loss: 110.05621426, Validation loss: 122.89685979, Gradient norm: 251.68079970
INFO:root:[   49] Training loss: 110.05976530, Validation loss: 119.13376275, Gradient norm: 245.89812725
INFO:root:[   50] Training loss: 109.88723822, Validation loss: 116.80814151, Gradient norm: 216.08215727
INFO:root:[   51] Training loss: 109.94164870, Validation loss: 120.18842500, Gradient norm: 247.15718488
INFO:root:[   52] Training loss: 109.89641044, Validation loss: 117.61185324, Gradient norm: 231.62340912
INFO:root:[   53] Training loss: 109.73810976, Validation loss: 114.97375278, Gradient norm: 224.35207959
INFO:root:[   54] Training loss: 109.86196602, Validation loss: 121.65237900, Gradient norm: 264.73092103
INFO:root:[   55] Training loss: 109.74858080, Validation loss: 118.95937584, Gradient norm: 286.88509486
INFO:root:[   56] Training loss: 109.73117828, Validation loss: 122.06144425, Gradient norm: 294.92885913
INFO:root:[   57] Training loss: 109.73867744, Validation loss: 114.32200175, Gradient norm: 308.28496997
INFO:root:[   58] Training loss: 109.62807525, Validation loss: 114.20636644, Gradient norm: 254.61863455
INFO:root:[   59] Training loss: 109.67671575, Validation loss: 120.38418658, Gradient norm: 276.83681561
INFO:root:[   60] Training loss: 109.66251832, Validation loss: 112.74411932, Gradient norm: 314.08573825
INFO:root:[   61] Training loss: 110.03022334, Validation loss: 110.59703038, Gradient norm: 294.54019805
INFO:root:[   62] Training loss: 109.86446104, Validation loss: 111.68031416, Gradient norm: 263.75582661
INFO:root:[   63] Training loss: 109.69053623, Validation loss: 110.97295485, Gradient norm: 306.39734083
INFO:root:[   64] Training loss: 109.67021294, Validation loss: 109.22382381, Gradient norm: 272.77036557
INFO:root:[   65] Training loss: 109.80965741, Validation loss: 113.02565371, Gradient norm: 373.81035906
INFO:root:[   66] Training loss: 109.63532048, Validation loss: 109.89629548, Gradient norm: 330.36554001
INFO:root:[   67] Training loss: 109.48489069, Validation loss: 108.37754453, Gradient norm: 352.58223737
INFO:root:[   68] Training loss: 109.50279371, Validation loss: 109.36913036, Gradient norm: 362.54638107
INFO:root:[   69] Training loss: 109.44728385, Validation loss: 110.11729878, Gradient norm: 356.16336336
INFO:root:[   70] Training loss: 109.44993706, Validation loss: 108.41593118, Gradient norm: 336.57026092
INFO:root:[   71] Training loss: 109.44925980, Validation loss: 107.94120789, Gradient norm: 362.31649291
INFO:root:[   72] Training loss: 109.30438793, Validation loss: 108.48932016, Gradient norm: 303.88864410
INFO:root:[   73] Training loss: 109.41162150, Validation loss: 108.34318253, Gradient norm: 371.39494693
INFO:root:[   74] Training loss: 109.20301373, Validation loss: 107.43860258, Gradient norm: 354.00124402
INFO:root:[   75] Training loss: 109.28511682, Validation loss: 107.60397313, Gradient norm: 379.67170544
INFO:root:[   76] Training loss: 109.34964759, Validation loss: 107.72334132, Gradient norm: 351.84436121
INFO:root:[   77] Training loss: 109.43166412, Validation loss: 107.31390249, Gradient norm: 354.31818392
INFO:root:[   78] Training loss: 109.50551403, Validation loss: 107.53681235, Gradient norm: 433.23095752
INFO:root:[   79] Training loss: 109.29980374, Validation loss: 106.59373764, Gradient norm: 395.42823215
INFO:root:[   80] Training loss: 109.36628190, Validation loss: 106.44095112, Gradient norm: 407.20879103
INFO:root:[   81] Training loss: 109.17309955, Validation loss: 106.90227403, Gradient norm: 442.27225370
INFO:root:[   82] Training loss: 109.05746710, Validation loss: 106.21646565, Gradient norm: 362.19763242
INFO:root:[   83] Training loss: 109.15183933, Validation loss: 106.46876736, Gradient norm: 447.16080576
INFO:root:[   84] Training loss: 109.09985419, Validation loss: 106.28905645, Gradient norm: 438.63870472
INFO:root:[   85] Training loss: 108.98726445, Validation loss: 106.31273520, Gradient norm: 409.07959464
INFO:root:[   86] Training loss: 108.93455303, Validation loss: 107.06175416, Gradient norm: 391.34403226
INFO:root:[   87] Training loss: 109.00779292, Validation loss: 106.20327654, Gradient norm: 439.21653615
INFO:root:[   88] Training loss: 108.99896558, Validation loss: 106.09627165, Gradient norm: 482.68224530
INFO:root:[   89] Training loss: 108.98033331, Validation loss: 106.42804850, Gradient norm: 455.02567658
INFO:root:[   90] Training loss: 108.92315370, Validation loss: 106.82088128, Gradient norm: 474.43701684
INFO:root:[   91] Training loss: 108.88098077, Validation loss: 106.88758087, Gradient norm: 469.04961584
INFO:root:[   92] Training loss: 108.76341416, Validation loss: 105.92739789, Gradient norm: 474.73677834
INFO:root:[   93] Training loss: 108.78266488, Validation loss: 106.42015207, Gradient norm: 448.39053269
INFO:root:[   94] Training loss: 109.01917658, Validation loss: 106.25481152, Gradient norm: 511.17365230
INFO:root:[   95] Training loss: 109.06291327, Validation loss: 106.28250938, Gradient norm: 432.37495956
INFO:root:[   96] Training loss: 108.83166632, Validation loss: 106.15683694, Gradient norm: 482.83996295
INFO:root:[   97] Training loss: 108.94748749, Validation loss: 105.89156000, Gradient norm: 514.24505829
INFO:root:[   98] Training loss: 108.76039178, Validation loss: 106.61522490, Gradient norm: 457.27090440
INFO:root:[   99] Training loss: 108.80624295, Validation loss: 106.37188037, Gradient norm: 500.15873550
INFO:root:[  100] Training loss: 108.76849973, Validation loss: 106.19915798, Gradient norm: 535.83361252
INFO:root:[  101] Training loss: 108.65612449, Validation loss: 106.15868062, Gradient norm: 507.87109921
INFO:root:[  102] Training loss: 108.82205342, Validation loss: 106.86387924, Gradient norm: 551.21549981
INFO:root:[  103] Training loss: 108.74837413, Validation loss: 106.64980553, Gradient norm: 619.46668582
INFO:root:[  104] Training loss: 108.74876323, Validation loss: 106.23684824, Gradient norm: 611.62705438
INFO:root:[  105] Training loss: 108.68884689, Validation loss: 105.87066072, Gradient norm: 485.73212563
INFO:root:[  106] Training loss: 108.84562433, Validation loss: 106.20452933, Gradient norm: 588.31700780
INFO:root:[  107] Training loss: 108.71571323, Validation loss: 105.85401417, Gradient norm: 563.66539175
INFO:root:[  108] Training loss: 108.66255823, Validation loss: 105.91009206, Gradient norm: 545.58570777
INFO:root:[  109] Training loss: 108.70016493, Validation loss: 105.67819214, Gradient norm: 537.58492278
INFO:root:[  110] Training loss: 108.85253386, Validation loss: 108.01621194, Gradient norm: 578.71030662
INFO:root:[  111] Training loss: 109.67102436, Validation loss: 109.99359736, Gradient norm: 519.83899255
INFO:root:[  112] Training loss: 109.78915115, Validation loss: 110.77722221, Gradient norm: 522.77507263
INFO:root:[  113] Training loss: 109.63088409, Validation loss: 112.49820052, Gradient norm: 524.57761058
INFO:root:[  114] Training loss: 110.32971968, Validation loss: 125.46182067, Gradient norm: 527.95031445
INFO:root:[  115] Training loss: 109.71954947, Validation loss: 137.04696655, Gradient norm: 518.05272040
INFO:root:[  116] Training loss: 109.48290030, Validation loss: 128.09474340, Gradient norm: 560.33790490
INFO:root:[  117] Training loss: 109.18243888, Validation loss: 127.24779537, Gradient norm: 544.70980177
INFO:root:[  118] Training loss: 109.12347750, Validation loss: 124.98621079, Gradient norm: 541.80617493
INFO:root:EP 118: Early stopping
INFO:root:Training the model took 1464.422s.
INFO:root:Emptying the cuda cache took 0.023s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 149.38761
INFO:root:EnergyScoreTrain: 105.20207
INFO:root:CoverageTrain: 0.25122
INFO:root:IntervalWidthTrain: 4.10867
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 150.22253
INFO:root:EnergyScoreValidation: 105.80878
INFO:root:CoverageValidation: 0.25048
INFO:root:IntervalWidthValidation: 4.09681
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 150.39046
INFO:root:EnergyScoreTest: 105.92325
INFO:root:CoverageTest: 0.25231
INFO:root:IntervalWidthTest: 4.12166
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.23678623, Validation loss: 122.29338389, Gradient norm: 16.97966383
INFO:root:[    2] Training loss: 121.35768701, Validation loss: 122.54946742, Gradient norm: 6.93476394
INFO:root:[    3] Training loss: 121.32786823, Validation loss: 122.03399185, Gradient norm: 6.91300916
INFO:root:[    4] Training loss: 121.30970190, Validation loss: 121.80305428, Gradient norm: 6.31443391
INFO:root:[    5] Training loss: 121.31154268, Validation loss: 122.14102699, Gradient norm: 5.20489059
INFO:root:[    6] Training loss: 121.29939155, Validation loss: 121.61609202, Gradient norm: 6.27425555
INFO:root:[    7] Training loss: 121.32903128, Validation loss: 121.74851595, Gradient norm: 6.28931579
INFO:root:[    8] Training loss: 121.31989410, Validation loss: 121.54664586, Gradient norm: 6.80991921
INFO:root:[    9] Training loss: 121.29902433, Validation loss: 121.52910667, Gradient norm: 6.28759181
INFO:root:[   10] Training loss: 121.29446452, Validation loss: 121.69152201, Gradient norm: 5.13367341
INFO:root:[   11] Training loss: 121.30098313, Validation loss: 121.55300061, Gradient norm: 5.86349363
INFO:root:[   12] Training loss: 121.30878455, Validation loss: 121.56603425, Gradient norm: 5.94601934
INFO:root:[   13] Training loss: 121.29796850, Validation loss: 121.55412687, Gradient norm: 4.29753877
INFO:root:[   14] Training loss: 121.29225456, Validation loss: 121.77949129, Gradient norm: 4.40741065
INFO:root:[   15] Training loss: 121.29226604, Validation loss: 121.50594698, Gradient norm: 5.15419226
INFO:root:[   16] Training loss: 121.27608895, Validation loss: 121.41794481, Gradient norm: 4.07972167
INFO:root:[   17] Training loss: 121.27676898, Validation loss: 121.53555403, Gradient norm: 4.42992927
INFO:root:[   18] Training loss: 121.29268930, Validation loss: 121.32291623, Gradient norm: 5.48177846
INFO:root:[   19] Training loss: 121.27004539, Validation loss: 121.43886040, Gradient norm: 3.37123600
INFO:root:[   20] Training loss: 121.27635537, Validation loss: 121.31702686, Gradient norm: 3.34597743
INFO:root:[   21] Training loss: 121.27293558, Validation loss: 121.31762511, Gradient norm: 3.06506710
INFO:root:[   22] Training loss: 121.28311184, Validation loss: 121.37897465, Gradient norm: 4.01947962
INFO:root:[   23] Training loss: 121.29912122, Validation loss: 121.32144691, Gradient norm: 3.31280402
INFO:root:[   24] Training loss: 121.26625061, Validation loss: 121.34539111, Gradient norm: 2.24327564
INFO:root:[   25] Training loss: 121.27500227, Validation loss: 121.23239557, Gradient norm: 2.67417878
INFO:root:[   26] Training loss: 121.28158239, Validation loss: 121.31262391, Gradient norm: 2.53166610
INFO:root:[   27] Training loss: 121.28975070, Validation loss: 121.43548952, Gradient norm: 2.67967911
INFO:root:[   28] Training loss: 121.26334260, Validation loss: 121.44282979, Gradient norm: 2.23350837
INFO:root:[   29] Training loss: 121.25985400, Validation loss: 121.45620517, Gradient norm: 1.86595418
INFO:root:[   30] Training loss: 121.26448046, Validation loss: 121.51007817, Gradient norm: 1.53638960
INFO:root:[   31] Training loss: 121.25800182, Validation loss: 121.41373680, Gradient norm: 1.72144624
INFO:root:[   32] Training loss: 121.12091152, Validation loss: 120.84535296, Gradient norm: 2.83307099
INFO:root:[   33] Training loss: 120.51811205, Validation loss: 119.68820217, Gradient norm: 5.65372009
INFO:root:[   34] Training loss: 119.22744008, Validation loss: 117.71992098, Gradient norm: 8.45467075
INFO:root:[   35] Training loss: 117.87611220, Validation loss: 116.05175360, Gradient norm: 11.22503588
INFO:root:[   36] Training loss: 116.60253555, Validation loss: 114.38807336, Gradient norm: 13.92162710
INFO:root:[   37] Training loss: 115.72939685, Validation loss: 113.49844203, Gradient norm: 19.57123589
INFO:root:[   38] Training loss: 115.07701064, Validation loss: 112.88935773, Gradient norm: 23.39480515
INFO:root:[   39] Training loss: 114.62956764, Validation loss: 112.96841641, Gradient norm: 33.35843699
INFO:root:[   40] Training loss: 114.33631701, Validation loss: 113.85760419, Gradient norm: 37.89012151
INFO:root:[   41] Training loss: 114.10151301, Validation loss: 113.04497423, Gradient norm: 50.84287260
INFO:root:[   42] Training loss: 113.88765122, Validation loss: 113.08714163, Gradient norm: 57.83293408
INFO:root:[   43] Training loss: 113.65271908, Validation loss: 113.28466639, Gradient norm: 60.02292256
INFO:root:[   44] Training loss: 113.58971311, Validation loss: 114.69478423, Gradient norm: 69.80065262
INFO:root:[   45] Training loss: 113.44470708, Validation loss: 118.68131151, Gradient norm: 75.67102800
INFO:root:[   46] Training loss: 113.39155923, Validation loss: 118.70525886, Gradient norm: 96.49990457
INFO:root:[   47] Training loss: 113.29115316, Validation loss: 120.76339038, Gradient norm: 92.60245755
INFO:root:[   48] Training loss: 113.24617207, Validation loss: 118.53622279, Gradient norm: 104.37464932
INFO:root:[   49] Training loss: 113.16024483, Validation loss: 119.24362498, Gradient norm: 117.72829466
INFO:root:[   50] Training loss: 113.09961505, Validation loss: 117.38703340, Gradient norm: 123.96867399
INFO:root:[   51] Training loss: 112.99002339, Validation loss: 122.61701045, Gradient norm: 121.78745701
INFO:root:[   52] Training loss: 113.05997703, Validation loss: 121.01136859, Gradient norm: 163.22234604
INFO:root:[   53] Training loss: 112.90403403, Validation loss: 122.44209053, Gradient norm: 152.34119176
INFO:root:[   54] Training loss: 113.12408332, Validation loss: 123.25958989, Gradient norm: 170.01710890
INFO:root:[   55] Training loss: 112.94709677, Validation loss: 125.95096115, Gradient norm: 176.47253757
INFO:root:[   56] Training loss: 112.86091438, Validation loss: 125.33160558, Gradient norm: 180.13087037
INFO:root:[   57] Training loss: 112.87436757, Validation loss: 120.81120826, Gradient norm: 194.18359641
INFO:root:[   58] Training loss: 112.97077003, Validation loss: 127.00311858, Gradient norm: 213.14261026
INFO:root:[   59] Training loss: 112.94863338, Validation loss: 130.08686276, Gradient norm: 204.62426303
INFO:root:[   60] Training loss: 112.89884449, Validation loss: 119.64960059, Gradient norm: 212.21433157
INFO:root:[   61] Training loss: 113.00444429, Validation loss: 124.00663100, Gradient norm: 234.77630451
INFO:root:[   62] Training loss: 112.94349421, Validation loss: 116.39484458, Gradient norm: 258.78440452
INFO:root:[   63] Training loss: 112.86547359, Validation loss: 117.34893957, Gradient norm: 256.23161451
INFO:root:[   64] Training loss: 112.78779015, Validation loss: 113.14125114, Gradient norm: 248.32017333
INFO:root:[   65] Training loss: 112.82304983, Validation loss: 113.65868483, Gradient norm: 290.33476809
INFO:root:[   66] Training loss: 112.69182803, Validation loss: 112.57275443, Gradient norm: 278.82726998
INFO:root:[   67] Training loss: 112.73355778, Validation loss: 113.77500889, Gradient norm: 306.83794453
INFO:root:[   68] Training loss: 112.60430699, Validation loss: 111.65664883, Gradient norm: 316.55914865
INFO:root:[   69] Training loss: 112.58933521, Validation loss: 111.56113276, Gradient norm: 323.92104607
INFO:root:[   70] Training loss: 112.38480262, Validation loss: 112.54848849, Gradient norm: 346.78947723
INFO:root:[   71] Training loss: 112.52023667, Validation loss: 109.70850372, Gradient norm: 365.03131494
INFO:root:[   72] Training loss: 112.46314577, Validation loss: 109.89516107, Gradient norm: 355.50396130
INFO:root:[   73] Training loss: 112.30346659, Validation loss: 110.50629767, Gradient norm: 342.79673925
INFO:root:[   74] Training loss: 112.39494668, Validation loss: 110.42092580, Gradient norm: 331.73708032
INFO:root:[   75] Training loss: 112.34017377, Validation loss: 110.07964535, Gradient norm: 386.17615032
INFO:root:[   76] Training loss: 112.25184274, Validation loss: 110.26513382, Gradient norm: 391.44225814
INFO:root:[   77] Training loss: 112.26472298, Validation loss: 109.75303571, Gradient norm: 462.17920970
INFO:root:[   78] Training loss: 112.25881890, Validation loss: 109.78113030, Gradient norm: 457.01241497
INFO:root:[   79] Training loss: 112.16762522, Validation loss: 109.90180732, Gradient norm: 525.04868464
INFO:root:[   80] Training loss: 112.14177319, Validation loss: 109.63672112, Gradient norm: 492.92657454
INFO:root:[   81] Training loss: 112.09283684, Validation loss: 109.41951910, Gradient norm: 465.14083948
INFO:root:[   82] Training loss: 112.17572703, Validation loss: 110.13798023, Gradient norm: 481.60958620
INFO:root:[   83] Training loss: 112.23250661, Validation loss: 109.66237088, Gradient norm: 539.36768114
INFO:root:[   84] Training loss: 112.15601646, Validation loss: 109.74721159, Gradient norm: 580.43820873
INFO:root:[   85] Training loss: 112.38364822, Validation loss: 112.09958833, Gradient norm: 627.35763304
INFO:root:[   86] Training loss: 112.43345088, Validation loss: 113.41616321, Gradient norm: 528.05991513
INFO:root:[   87] Training loss: 112.68844530, Validation loss: 110.16586882, Gradient norm: 583.44584671
INFO:root:[   88] Training loss: 112.50930536, Validation loss: 109.68611434, Gradient norm: 702.14765860
INFO:root:[   89] Training loss: 112.42395627, Validation loss: 109.67497069, Gradient norm: 684.46282336
INFO:root:[   90] Training loss: 112.10508350, Validation loss: 109.55620970, Gradient norm: 610.81301990
INFO:root:EP 90: Early stopping
INFO:root:Training the model took 1183.126s.
INFO:root:Emptying the cuda cache took 0.022s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 154.45437
INFO:root:EnergyScoreTrain: 109.13798
INFO:root:CoverageTrain: 0.30649
INFO:root:IntervalWidthTrain: 4.3937
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 154.7803
INFO:root:EnergyScoreValidation: 109.3892
INFO:root:CoverageValidation: 0.3058
INFO:root:IntervalWidthValidation: 4.38372
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 154.91939
INFO:root:EnergyScoreTest: 109.49135
INFO:root:CoverageTest: 0.30567
INFO:root:IntervalWidthTest: 4.3841
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 122.67022496, Validation loss: 122.79115585, Gradient norm: 19.97526818
INFO:root:[    2] Training loss: 121.42183996, Validation loss: 122.77370769, Gradient norm: 9.30718750
INFO:root:[    3] Training loss: 121.35935272, Validation loss: 122.17440954, Gradient norm: 6.62601329
INFO:root:[    4] Training loss: 121.33226668, Validation loss: 121.84631216, Gradient norm: 5.43963902
INFO:root:[    5] Training loss: 121.31165888, Validation loss: 121.91043065, Gradient norm: 3.48118589
INFO:root:[    6] Training loss: 121.30042307, Validation loss: 121.89075970, Gradient norm: 4.63031933
INFO:root:[    7] Training loss: 121.30369399, Validation loss: 121.45653271, Gradient norm: 5.06783977
INFO:root:[    8] Training loss: 121.28698933, Validation loss: 121.51426565, Gradient norm: 4.49533632
INFO:root:[    9] Training loss: 121.26825633, Validation loss: 121.42503857, Gradient norm: 4.00893042
INFO:root:[   10] Training loss: 121.28377209, Validation loss: 121.34053460, Gradient norm: 3.49530932
INFO:root:[   11] Training loss: 121.27143975, Validation loss: 121.31695767, Gradient norm: 3.52214749
INFO:root:[   12] Training loss: 121.28983766, Validation loss: 121.27681969, Gradient norm: 3.39450589
INFO:root:[   13] Training loss: 121.26035376, Validation loss: 121.32262631, Gradient norm: 2.41291744
INFO:root:[   14] Training loss: 121.28028228, Validation loss: 121.25253375, Gradient norm: 2.79659883
INFO:root:[   15] Training loss: 121.27288987, Validation loss: 121.30032138, Gradient norm: 2.95313918
INFO:root:[   16] Training loss: 121.27170961, Validation loss: 121.26079901, Gradient norm: 2.24367435
INFO:root:[   17] Training loss: 121.29127158, Validation loss: 121.34267820, Gradient norm: 1.79872424
INFO:root:[   18] Training loss: 121.26783219, Validation loss: 121.42756784, Gradient norm: 2.18263751
INFO:root:[   19] Training loss: 121.25334944, Validation loss: 121.51069378, Gradient norm: 1.64068193
INFO:root:[   20] Training loss: 121.25558607, Validation loss: 121.40965403, Gradient norm: 1.52385332
INFO:root:[   21] Training loss: 121.28178298, Validation loss: 121.38631308, Gradient norm: 1.46857426
INFO:root:[   22] Training loss: 121.26689810, Validation loss: 121.33723029, Gradient norm: 1.48317533
INFO:root:[   23] Training loss: 121.26159317, Validation loss: 121.45147442, Gradient norm: 1.42184503
INFO:root:[   24] Training loss: 121.28211232, Validation loss: 121.53970258, Gradient norm: 1.62385998
INFO:root:[   25] Training loss: 121.27820115, Validation loss: 121.40942804, Gradient norm: 1.36889579
INFO:root:[   26] Training loss: 121.27239585, Validation loss: 121.62227762, Gradient norm: 2.30669611
INFO:root:[   27] Training loss: 121.28552212, Validation loss: 121.56567856, Gradient norm: 1.25862212
INFO:root:[   28] Training loss: 121.24592185, Validation loss: 121.61854527, Gradient norm: 1.38078802
INFO:root:[   29] Training loss: 121.27464369, Validation loss: 121.48621710, Gradient norm: 1.04125976
INFO:root:[   30] Training loss: 121.24504339, Validation loss: 121.57968824, Gradient norm: 1.13987676
INFO:root:[   31] Training loss: 121.26058393, Validation loss: 121.38153918, Gradient norm: 1.01617530
INFO:root:[   32] Training loss: 121.27259995, Validation loss: 121.37843007, Gradient norm: 0.89689498
INFO:root:[   33] Training loss: 121.25379053, Validation loss: 121.43308705, Gradient norm: 0.80682560
INFO:root:[   34] Training loss: 121.27004829, Validation loss: 121.44595547, Gradient norm: 0.92488082
INFO:root:[   35] Training loss: 121.25270492, Validation loss: 121.42443742, Gradient norm: 0.75703918
INFO:root:[   36] Training loss: 121.29302580, Validation loss: 121.42885853, Gradient norm: 0.63135676
INFO:root:[   37] Training loss: 121.26922661, Validation loss: 121.55816072, Gradient norm: 0.63483070
INFO:root:[   38] Training loss: 121.25208735, Validation loss: 121.50732948, Gradient norm: 0.63678449
INFO:root:[   39] Training loss: 121.28068624, Validation loss: 121.48250448, Gradient norm: 0.62549268
INFO:root:[   40] Training loss: 121.26439390, Validation loss: 121.48557071, Gradient norm: 0.56832511
INFO:root:[   41] Training loss: 121.25939644, Validation loss: 121.47015039, Gradient norm: 0.59214047
INFO:root:[   42] Training loss: 121.24877248, Validation loss: 121.49559416, Gradient norm: 0.43734165
INFO:root:[   43] Training loss: 121.29394052, Validation loss: 121.49382440, Gradient norm: 0.58616972
INFO:root:[   44] Training loss: 121.26130899, Validation loss: 121.42302625, Gradient norm: 0.41597301
INFO:root:[   45] Training loss: 121.28440020, Validation loss: 121.45074173, Gradient norm: 0.60137235
INFO:root:[   46] Training loss: 121.24594656, Validation loss: 121.26571787, Gradient norm: 0.66700118
INFO:root:[   47] Training loss: 120.55193113, Validation loss: 119.77891146, Gradient norm: 3.26113787
INFO:root:[   48] Training loss: 118.95623671, Validation loss: 117.64930146, Gradient norm: 5.65902915
INFO:root:[   49] Training loss: 117.63498100, Validation loss: 116.12476744, Gradient norm: 7.18972955
INFO:root:[   50] Training loss: 116.69218094, Validation loss: 115.20135761, Gradient norm: 6.78024805
INFO:root:[   51] Training loss: 116.01754916, Validation loss: 114.22201065, Gradient norm: 7.25557496
INFO:root:[   52] Training loss: 115.38175107, Validation loss: 113.37091696, Gradient norm: 7.54288200
INFO:root:[   53] Training loss: 114.84525542, Validation loss: 112.44452667, Gradient norm: 7.86370131
INFO:root:[   54] Training loss: 114.46504677, Validation loss: 112.15424189, Gradient norm: 8.15243612
INFO:root:[   55] Training loss: 114.04670013, Validation loss: 111.65434344, Gradient norm: 8.11515517
INFO:root:[   56] Training loss: 113.74039790, Validation loss: 111.21559643, Gradient norm: 9.29187641
INFO:root:[   57] Training loss: 113.52811574, Validation loss: 110.73524002, Gradient norm: 9.37801046
INFO:root:[   58] Training loss: 113.27751470, Validation loss: 110.46730962, Gradient norm: 9.94109284
INFO:root:[   59] Training loss: 113.07565092, Validation loss: 110.13143132, Gradient norm: 10.76335812
INFO:root:[   60] Training loss: 112.85947432, Validation loss: 110.12831879, Gradient norm: 10.99619779
INFO:root:[   61] Training loss: 112.72617927, Validation loss: 110.31315402, Gradient norm: 13.31896703
INFO:root:[   62] Training loss: 112.60621002, Validation loss: 110.60091453, Gradient norm: 16.52846165
INFO:root:[   63] Training loss: 112.49559791, Validation loss: 110.78480609, Gradient norm: 17.18465715
INFO:root:[   64] Training loss: 112.40370523, Validation loss: 110.48005860, Gradient norm: 21.71773191
INFO:root:[   65] Training loss: 112.39221245, Validation loss: 112.15972769, Gradient norm: 23.01286899
INFO:root:[   66] Training loss: 112.34541193, Validation loss: 112.14664828, Gradient norm: 31.24110169
INFO:root:[   67] Training loss: 112.23570157, Validation loss: 113.85137045, Gradient norm: 31.58163263
INFO:root:[   68] Training loss: 112.25467938, Validation loss: 114.75309911, Gradient norm: 39.75927948
INFO:root:[   69] Training loss: 112.21329451, Validation loss: 115.86359274, Gradient norm: 42.25870661
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 861.466s.
INFO:root:Emptying the cuda cache took 0.022s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 155.91008
INFO:root:EnergyScoreTrain: 109.81025
INFO:root:CoverageTrain: 0.73591
INFO:root:IntervalWidthTrain: 7.69869
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 156.25926
INFO:root:EnergyScoreValidation: 110.06226
INFO:root:CoverageValidation: 0.73535
INFO:root:IntervalWidthValidation: 7.70412
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 156.52457
INFO:root:EnergyScoreTest: 110.2471
INFO:root:CoverageTest: 0.73613
INFO:root:IntervalWidthTest: 7.7208
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 124.85351211, Validation loss: 124.22624443, Gradient norm: 34.59209059
INFO:root:[    2] Training loss: 121.58913023, Validation loss: 123.44917218, Gradient norm: 6.80303866
INFO:root:[    3] Training loss: 121.42521114, Validation loss: 123.81655936, Gradient norm: 5.61699573
INFO:root:[    4] Training loss: 121.35614311, Validation loss: 123.48998918, Gradient norm: 5.19660089
INFO:root:[    5] Training loss: 121.35580336, Validation loss: 124.38562564, Gradient norm: 5.21093080
INFO:root:[    6] Training loss: 121.36317262, Validation loss: 123.91982164, Gradient norm: 9.22065179
INFO:root:[    7] Training loss: 121.30682650, Validation loss: 123.93983486, Gradient norm: 11.52890380
INFO:root:[    8] Training loss: 121.35176937, Validation loss: 124.79735039, Gradient norm: 17.28421908
INFO:root:[    9] Training loss: 121.32352258, Validation loss: 123.35146227, Gradient norm: 14.72476967
INFO:root:[   10] Training loss: 121.33018872, Validation loss: 125.08091157, Gradient norm: 17.37650960
INFO:root:[   11] Training loss: 121.31505760, Validation loss: 124.77342777, Gradient norm: 16.55507006
INFO:root:[   12] Training loss: 121.30214381, Validation loss: 123.60019789, Gradient norm: 13.61054666
INFO:root:[   13] Training loss: 121.32465484, Validation loss: 122.93228255, Gradient norm: 14.29447012
INFO:root:[   14] Training loss: 121.35070274, Validation loss: 124.23478199, Gradient norm: 14.51054678
INFO:root:[   15] Training loss: 121.30142158, Validation loss: 123.69885464, Gradient norm: 11.76811560
INFO:root:[   16] Training loss: 121.33801519, Validation loss: 123.92532796, Gradient norm: 11.07003110
INFO:root:[   17] Training loss: 121.31747916, Validation loss: 123.54675767, Gradient norm: 11.69791436
INFO:root:[   18] Training loss: 121.29536485, Validation loss: 122.66227459, Gradient norm: 9.75579760
INFO:root:[   19] Training loss: 121.30847998, Validation loss: 122.46012405, Gradient norm: 9.15034640
INFO:root:[   20] Training loss: 121.30264579, Validation loss: 123.08450186, Gradient norm: 9.82975497
INFO:root:[   21] Training loss: 121.27660653, Validation loss: 123.24572201, Gradient norm: 9.04099457
INFO:root:[   22] Training loss: 121.28827877, Validation loss: 122.71891995, Gradient norm: 10.61086671
INFO:root:[   23] Training loss: 121.29350017, Validation loss: 122.93108999, Gradient norm: 15.28131288
INFO:root:[   24] Training loss: 121.33414567, Validation loss: 123.17487967, Gradient norm: 12.04393602
INFO:root:[   25] Training loss: 121.28710303, Validation loss: 123.32554021, Gradient norm: 9.63912535
INFO:root:[   26] Training loss: 121.29160889, Validation loss: 122.97437392, Gradient norm: 9.35501426
INFO:root:[   27] Training loss: 121.29022007, Validation loss: 122.73397564, Gradient norm: 10.94602054
INFO:root:[   28] Training loss: 121.26731211, Validation loss: 123.25391283, Gradient norm: 9.01733961
INFO:root:[   29] Training loss: 121.30775918, Validation loss: 122.71523785, Gradient norm: 9.40188800
INFO:root:[   30] Training loss: 121.26915370, Validation loss: 122.37475533, Gradient norm: 11.14350594
INFO:root:[   31] Training loss: 121.30053245, Validation loss: 122.59549003, Gradient norm: 8.33385830
INFO:root:[   32] Training loss: 121.29798464, Validation loss: 122.61306184, Gradient norm: 11.47487069
INFO:root:[   33] Training loss: 121.26807917, Validation loss: 122.64642650, Gradient norm: 8.65310709
INFO:root:[   34] Training loss: 121.29330073, Validation loss: 122.40371862, Gradient norm: 9.40368537
INFO:root:[   35] Training loss: 121.29060350, Validation loss: 122.12466720, Gradient norm: 6.37530711
INFO:root:[   36] Training loss: 121.30324838, Validation loss: 121.83549500, Gradient norm: 9.25755813
INFO:root:[   37] Training loss: 121.27315636, Validation loss: 121.90915391, Gradient norm: 8.75433672
INFO:root:[   38] Training loss: 121.27585710, Validation loss: 121.78136234, Gradient norm: 6.68846786
INFO:root:[   39] Training loss: 121.28463597, Validation loss: 121.60086823, Gradient norm: 8.21488753
INFO:root:[   40] Training loss: 121.31243107, Validation loss: 122.13483245, Gradient norm: 8.44267376
INFO:root:[   41] Training loss: 121.29659264, Validation loss: 121.90452234, Gradient norm: 7.30384531
INFO:root:[   42] Training loss: 121.28966023, Validation loss: 122.01040807, Gradient norm: 8.92476144
INFO:root:[   43] Training loss: 121.27677053, Validation loss: 121.82446999, Gradient norm: 7.07516590
INFO:root:[   44] Training loss: 121.28305250, Validation loss: 122.25454922, Gradient norm: 6.94805609
INFO:root:[   45] Training loss: 121.29351699, Validation loss: 122.08435348, Gradient norm: 7.63516330
INFO:root:[   46] Training loss: 121.25077185, Validation loss: 122.11033341, Gradient norm: 6.36307373
INFO:root:[   47] Training loss: 121.29350841, Validation loss: 122.02632220, Gradient norm: 7.33093798
INFO:root:[   48] Training loss: 121.28177319, Validation loss: 121.88751484, Gradient norm: 7.15604281
INFO:root:[   49] Training loss: 121.29348552, Validation loss: 122.08286522, Gradient norm: 7.74876233
INFO:root:[   50] Training loss: 121.27032228, Validation loss: 121.89515318, Gradient norm: 6.12389120
INFO:root:[   51] Training loss: 121.28364779, Validation loss: 121.95258016, Gradient norm: 5.59901951
INFO:root:[   52] Training loss: 121.30251042, Validation loss: 122.03156202, Gradient norm: 8.88649568
INFO:root:[   53] Training loss: 121.30664258, Validation loss: 121.86458614, Gradient norm: 6.92032936
INFO:root:[   54] Training loss: 121.28036107, Validation loss: 122.00348479, Gradient norm: 6.20138373
INFO:root:[   55] Training loss: 121.29562459, Validation loss: 121.90048612, Gradient norm: 6.39348792
INFO:root:[   56] Training loss: 121.27337322, Validation loss: 122.08851650, Gradient norm: 5.78027254
INFO:root:[   57] Training loss: 121.30256106, Validation loss: 121.80777109, Gradient norm: 8.30229217
INFO:root:[   58] Training loss: 121.30650762, Validation loss: 121.61346567, Gradient norm: 7.34455063
INFO:root:[   59] Training loss: 121.27084587, Validation loss: 121.78323654, Gradient norm: 6.04540283
INFO:root:[   60] Training loss: 121.29653215, Validation loss: 121.96323789, Gradient norm: 8.72310013
INFO:root:[   61] Training loss: 121.27245932, Validation loss: 121.95186668, Gradient norm: 5.83709116
INFO:root:[   62] Training loss: 121.27129526, Validation loss: 121.85159091, Gradient norm: 6.60963301
INFO:root:[   63] Training loss: 121.30451979, Validation loss: 121.76237277, Gradient norm: 6.35172660
INFO:root:[   64] Training loss: 121.27859308, Validation loss: 121.92592463, Gradient norm: 6.97441507
INFO:root:[   65] Training loss: 121.30379675, Validation loss: 121.92001290, Gradient norm: 7.22247844
INFO:root:[   66] Training loss: 121.29271219, Validation loss: 121.96706653, Gradient norm: 5.72579167
INFO:root:[   67] Training loss: 121.28800336, Validation loss: 121.83990557, Gradient norm: 7.57590834
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 956.279s.
INFO:root:Emptying the cuda cache took 0.024s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 172.13851
INFO:root:EnergyScoreTrain: 121.62842
INFO:root:CoverageTrain: 0.88783
INFO:root:IntervalWidthTrain: 9.01617
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 172.16326
INFO:root:EnergyScoreValidation: 121.65535
INFO:root:CoverageValidation: 0.88749
INFO:root:IntervalWidthValidation: 9.00926
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 172.21115
INFO:root:EnergyScoreTest: 121.68181
INFO:root:CoverageTest: 0.8878
INFO:root:IntervalWidthTest: 9.01627
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.32097551, Validation loss: 170.48908470, Gradient norm: 3.78954116
INFO:root:[    2] Training loss: 169.68057291, Validation loss: 167.93572472, Gradient norm: 8.71722135
INFO:root:[    3] Training loss: 166.08797907, Validation loss: 164.02279032, Gradient norm: 18.35351297
INFO:root:[    4] Training loss: 162.99011892, Validation loss: 161.05827332, Gradient norm: 20.76069477
INFO:root:[    5] Training loss: 160.43478934, Validation loss: 158.66260818, Gradient norm: 24.36803647
INFO:root:[    6] Training loss: 158.59734648, Validation loss: 157.15084313, Gradient norm: 27.09893029
INFO:root:[    7] Training loss: 157.27472977, Validation loss: 155.89909258, Gradient norm: 25.11772634
INFO:root:[    8] Training loss: 156.26294742, Validation loss: 155.17710245, Gradient norm: 27.46298791
INFO:root:[    9] Training loss: 155.45059285, Validation loss: 154.27990091, Gradient norm: 25.86353989
INFO:root:[   10] Training loss: 154.77112438, Validation loss: 153.70710860, Gradient norm: 29.76674673
INFO:root:[   11] Training loss: 154.17064782, Validation loss: 153.06521238, Gradient norm: 32.25748456
INFO:root:[   12] Training loss: 153.67089115, Validation loss: 152.64419187, Gradient norm: 30.58008894
INFO:root:[   13] Training loss: 153.24570121, Validation loss: 152.51820952, Gradient norm: 35.84136302
INFO:root:[   14] Training loss: 152.78330818, Validation loss: 151.61594891, Gradient norm: 31.22410965
INFO:root:[   15] Training loss: 152.41328822, Validation loss: 151.39935355, Gradient norm: 35.15985783
INFO:root:[   16] Training loss: 152.03387208, Validation loss: 151.10407389, Gradient norm: 36.02462595
INFO:root:[   17] Training loss: 151.70544812, Validation loss: 151.05899942, Gradient norm: 41.49617858
INFO:root:[   18] Training loss: 151.45580731, Validation loss: 150.64876425, Gradient norm: 38.11575383
INFO:root:[   19] Training loss: 151.11838322, Validation loss: 150.24652678, Gradient norm: 39.49578802
INFO:root:[   20] Training loss: 150.84384344, Validation loss: 150.09247826, Gradient norm: 38.13170273
INFO:root:[   21] Training loss: 150.56782451, Validation loss: 149.76222808, Gradient norm: 50.84895138
INFO:root:[   22] Training loss: 150.36688286, Validation loss: 149.51885092, Gradient norm: 45.55184342
INFO:root:[   23] Training loss: 150.10916475, Validation loss: 149.60277058, Gradient norm: 47.83167456
INFO:root:[   24] Training loss: 149.90184197, Validation loss: 149.07876324, Gradient norm: 51.45639458
INFO:root:[   25] Training loss: 149.71456747, Validation loss: 148.93815139, Gradient norm: 60.88220537
INFO:root:[   26] Training loss: 149.48606157, Validation loss: 149.08285522, Gradient norm: 54.36647529
INFO:root:[   27] Training loss: 149.28568518, Validation loss: 148.82084919, Gradient norm: 60.92236034
INFO:root:[   28] Training loss: 149.09053823, Validation loss: 148.51091845, Gradient norm: 57.76399056
INFO:root:[   29] Training loss: 148.97506916, Validation loss: 148.34655656, Gradient norm: 62.85983652
INFO:root:[   30] Training loss: 148.82669594, Validation loss: 148.47667563, Gradient norm: 75.00412073
INFO:root:[   31] Training loss: 148.61592696, Validation loss: 148.18523328, Gradient norm: 69.71249694
INFO:root:[   32] Training loss: 148.47233568, Validation loss: 148.13408740, Gradient norm: 80.03168456
INFO:root:[   33] Training loss: 148.35012871, Validation loss: 148.05427762, Gradient norm: 79.55644108
INFO:root:[   34] Training loss: 148.17881775, Validation loss: 147.89481222, Gradient norm: 91.74035950
INFO:root:[   35] Training loss: 148.03262302, Validation loss: 147.76662787, Gradient norm: 82.59153147
INFO:root:[   36] Training loss: 147.89798784, Validation loss: 147.93313651, Gradient norm: 73.16449472
INFO:root:[   37] Training loss: 147.84924087, Validation loss: 148.23145215, Gradient norm: 113.08822229
INFO:root:[   38] Training loss: 147.69251015, Validation loss: 147.49678040, Gradient norm: 76.73333208
INFO:root:[   39] Training loss: 147.53125216, Validation loss: 147.39659066, Gradient norm: 103.73225325
INFO:root:[   40] Training loss: 147.45164071, Validation loss: 147.22054001, Gradient norm: 110.04221965
INFO:root:[   41] Training loss: 147.30451142, Validation loss: 147.03624068, Gradient norm: 119.43034169
INFO:root:[   42] Training loss: 147.19075444, Validation loss: 147.32436502, Gradient norm: 117.53040046
INFO:root:[   43] Training loss: 147.06266812, Validation loss: 147.21459803, Gradient norm: 112.57681304
INFO:root:[   44] Training loss: 147.00425788, Validation loss: 147.70694864, Gradient norm: 117.48811433
INFO:root:[   45] Training loss: 146.87080410, Validation loss: 147.05756668, Gradient norm: 115.21170876
INFO:root:[   46] Training loss: 146.83264511, Validation loss: 147.01068220, Gradient norm: 134.28356244
INFO:root:[   47] Training loss: 146.66878111, Validation loss: 146.86680077, Gradient norm: 135.10792213
INFO:root:[   48] Training loss: 146.58740883, Validation loss: 147.01307573, Gradient norm: 159.83440769
INFO:root:[   49] Training loss: 146.49191568, Validation loss: 146.70778840, Gradient norm: 151.77365620
INFO:root:[   50] Training loss: 146.31626487, Validation loss: 146.71659746, Gradient norm: 133.41299618
INFO:root:[   51] Training loss: 146.29108706, Validation loss: 146.67730713, Gradient norm: 159.86533985
INFO:root:[   52] Training loss: 146.15931850, Validation loss: 146.74823893, Gradient norm: 153.53331364
INFO:root:[   53] Training loss: 146.10813620, Validation loss: 146.73395459, Gradient norm: 151.48829294
INFO:root:[   54] Training loss: 145.96271697, Validation loss: 146.71439283, Gradient norm: 153.75229275
INFO:root:[   55] Training loss: 145.90976816, Validation loss: 146.33227118, Gradient norm: 158.98316904
INFO:root:[   56] Training loss: 145.86139996, Validation loss: 146.45884073, Gradient norm: 177.27648074
INFO:root:[   57] Training loss: 145.68688884, Validation loss: 146.67223174, Gradient norm: 165.68896667
INFO:root:[   58] Training loss: 145.69636927, Validation loss: 146.64375568, Gradient norm: 168.03183037
INFO:root:[   59] Training loss: 145.60339680, Validation loss: 146.24217908, Gradient norm: 201.15020239
INFO:root:[   60] Training loss: 145.47289350, Validation loss: 146.26752340, Gradient norm: 158.68240334
INFO:root:[   61] Training loss: 145.48266777, Validation loss: 146.80128426, Gradient norm: 221.37308017
INFO:root:[   62] Training loss: 145.27200452, Validation loss: 146.30848115, Gradient norm: 174.74305673
INFO:root:[   63] Training loss: 145.22958401, Validation loss: 146.22241158, Gradient norm: 217.30783861
INFO:root:[   64] Training loss: 145.16920620, Validation loss: 146.32474334, Gradient norm: 200.76210343
INFO:root:[   65] Training loss: 145.12719078, Validation loss: 146.29180277, Gradient norm: 233.53390983
INFO:root:[   66] Training loss: 145.01031386, Validation loss: 146.54697076, Gradient norm: 196.29546791
INFO:root:[   67] Training loss: 144.95472906, Validation loss: 146.11424255, Gradient norm: 208.21854645
INFO:root:[   68] Training loss: 144.91042969, Validation loss: 146.12578556, Gradient norm: 222.66204872
INFO:root:[   69] Training loss: 144.84916619, Validation loss: 146.10396550, Gradient norm: 222.58092041
INFO:root:[   70] Training loss: 144.69381430, Validation loss: 146.24425638, Gradient norm: 236.10532116
INFO:root:[   71] Training loss: 144.69464381, Validation loss: 146.91696535, Gradient norm: 242.20233966
INFO:root:[   72] Training loss: 144.62685320, Validation loss: 146.14098753, Gradient norm: 229.89590178
INFO:root:[   73] Training loss: 144.55025381, Validation loss: 146.16345531, Gradient norm: 272.54661426
INFO:root:[   74] Training loss: 144.45523301, Validation loss: 146.70935111, Gradient norm: 232.36202982
INFO:root:[   75] Training loss: 144.40774631, Validation loss: 145.84703380, Gradient norm: 244.29429439
INFO:root:[   76] Training loss: 144.29226874, Validation loss: 146.08295520, Gradient norm: 227.62304950
INFO:root:[   77] Training loss: 144.26403809, Validation loss: 146.13924645, Gradient norm: 256.32750512
INFO:root:[   78] Training loss: 144.21428269, Validation loss: 146.29047104, Gradient norm: 271.12406526
INFO:root:[   79] Training loss: 144.13894991, Validation loss: 146.11780522, Gradient norm: 230.45288559
INFO:root:[   80] Training loss: 144.06511614, Validation loss: 146.50413618, Gradient norm: 237.71419397
INFO:root:[   81] Training loss: 143.97076916, Validation loss: 145.99181971, Gradient norm: 267.86667530
INFO:root:[   82] Training loss: 143.94266328, Validation loss: 146.11586472, Gradient norm: 253.78310765
INFO:root:[   83] Training loss: 143.91398783, Validation loss: 146.58068427, Gradient norm: 280.21471814
INFO:root:[   84] Training loss: 143.83206501, Validation loss: 146.29824566, Gradient norm: 265.82855419
INFO:root:EP 84: Early stopping
INFO:root:Training the model took 1067.691s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 141.63522
INFO:root:EnergyScoreTrain: 140.95371
INFO:root:CoverageTrain: 0.00752
INFO:root:IntervalWidthTrain: 0.02571
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 145.8836
INFO:root:EnergyScoreValidation: 145.23643
INFO:root:CoverageValidation: 0.00687
INFO:root:IntervalWidthValidation: 0.02458
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 146.37183
INFO:root:EnergyScoreTest: 145.6961
INFO:root:CoverageTest: 0.00722
INFO:root:IntervalWidthTest: 0.02547
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 172.22067571, Validation loss: 171.49103625, Gradient norm: 18.68089401
INFO:root:[    2] Training loss: 171.11040382, Validation loss: 170.50799561, Gradient norm: 2.28946542
INFO:root:[    3] Training loss: 170.34141662, Validation loss: 169.41689011, Gradient norm: 3.53837273
INFO:root:[    4] Training loss: 168.05268090, Validation loss: 165.73849172, Gradient norm: 9.36342063
INFO:root:[    5] Training loss: 165.54875521, Validation loss: 163.31196331, Gradient norm: 11.50729325
INFO:root:[    6] Training loss: 163.69152994, Validation loss: 161.75923683, Gradient norm: 12.09383659
INFO:root:[    7] Training loss: 162.35512387, Validation loss: 160.34859703, Gradient norm: 13.87943391
INFO:root:[    8] Training loss: 161.30734334, Validation loss: 159.04544383, Gradient norm: 17.27585485
INFO:root:[    9] Training loss: 160.38019366, Validation loss: 158.33687776, Gradient norm: 19.10084039
INFO:root:[   10] Training loss: 159.55000373, Validation loss: 157.35815640, Gradient norm: 26.09600806
INFO:root:[   11] Training loss: 158.80629089, Validation loss: 156.52343803, Gradient norm: 32.02489501
INFO:root:[   12] Training loss: 158.17074274, Validation loss: 155.81623893, Gradient norm: 39.42365069
INFO:root:[   13] Training loss: 157.56310671, Validation loss: 155.41015099, Gradient norm: 43.96460390
INFO:root:[   14] Training loss: 157.09779763, Validation loss: 154.73522107, Gradient norm: 49.53357504
INFO:root:[   15] Training loss: 156.69684108, Validation loss: 154.33326248, Gradient norm: 65.89835776
INFO:root:[   16] Training loss: 156.31589758, Validation loss: 153.95797993, Gradient norm: 65.01366448
INFO:root:[   17] Training loss: 155.94984733, Validation loss: 153.57159845, Gradient norm: 92.77456874
INFO:root:[   18] Training loss: 155.62369004, Validation loss: 153.24397225, Gradient norm: 89.17460742
INFO:root:[   19] Training loss: 155.34414160, Validation loss: 153.12514680, Gradient norm: 112.46132216
INFO:root:[   20] Training loss: 155.02850261, Validation loss: 152.54603208, Gradient norm: 114.50037853
INFO:root:[   21] Training loss: 154.82098713, Validation loss: 152.38982681, Gradient norm: 136.95430417
INFO:root:[   22] Training loss: 154.54501532, Validation loss: 152.36930637, Gradient norm: 146.74038283
INFO:root:[   23] Training loss: 154.32508202, Validation loss: 151.67708246, Gradient norm: 159.53714414
INFO:root:[   24] Training loss: 154.14459512, Validation loss: 151.57350159, Gradient norm: 173.34160670
INFO:root:[   25] Training loss: 153.84767205, Validation loss: 151.41224249, Gradient norm: 168.94072114
INFO:root:[   26] Training loss: 153.63540568, Validation loss: 151.05304060, Gradient norm: 190.59800659
INFO:root:[   27] Training loss: 153.52324697, Validation loss: 150.93544217, Gradient norm: 192.84790387
INFO:root:[   28] Training loss: 153.24962866, Validation loss: 150.95008324, Gradient norm: 202.97834744
INFO:root:[   29] Training loss: 153.10842896, Validation loss: 150.63499609, Gradient norm: 219.43333886
INFO:root:[   30] Training loss: 152.99634343, Validation loss: 150.38064891, Gradient norm: 257.33937136
INFO:root:[   31] Training loss: 152.82381446, Validation loss: 150.75316857, Gradient norm: 246.75535857
INFO:root:[   32] Training loss: 152.63495224, Validation loss: 150.18965254, Gradient norm: 231.35102421
INFO:root:[   33] Training loss: 152.53013476, Validation loss: 150.05989811, Gradient norm: 229.42852305
INFO:root:[   34] Training loss: 152.42686597, Validation loss: 149.60671313, Gradient norm: 272.26048012
INFO:root:[   35] Training loss: 152.18638543, Validation loss: 150.37251492, Gradient norm: 264.05298222
INFO:root:[   36] Training loss: 152.18414806, Validation loss: 149.63229160, Gradient norm: 330.21590052
INFO:root:[   37] Training loss: 151.99512191, Validation loss: 149.55778346, Gradient norm: 306.86527921
INFO:root:[   38] Training loss: 151.98911414, Validation loss: 149.26035809, Gradient norm: 351.79511892
INFO:root:[   39] Training loss: 151.71644781, Validation loss: 149.28984543, Gradient norm: 313.01525497
INFO:root:[   40] Training loss: 151.71839918, Validation loss: 148.85346459, Gradient norm: 386.57380537
INFO:root:[   41] Training loss: 151.66307892, Validation loss: 149.50249244, Gradient norm: 387.26888374
INFO:root:[   42] Training loss: 151.45143033, Validation loss: 149.09886696, Gradient norm: 351.72490954
INFO:root:[   43] Training loss: 151.46813317, Validation loss: 148.79328392, Gradient norm: 395.45666109
INFO:root:[   44] Training loss: 151.30785876, Validation loss: 148.50521851, Gradient norm: 392.30784899
INFO:root:[   45] Training loss: 151.25470605, Validation loss: 148.53593708, Gradient norm: 408.39901350
INFO:root:[   46] Training loss: 151.20314769, Validation loss: 149.53319155, Gradient norm: 437.66598074
INFO:root:[   47] Training loss: 151.13622885, Validation loss: 148.60710460, Gradient norm: 473.24992370
INFO:root:[   48] Training loss: 151.04422915, Validation loss: 148.13671507, Gradient norm: 478.47204293
INFO:root:[   49] Training loss: 150.98399204, Validation loss: 148.11829297, Gradient norm: 471.35933549
INFO:root:[   50] Training loss: 150.96724559, Validation loss: 148.04459407, Gradient norm: 496.79509577
INFO:root:[   51] Training loss: 150.82315577, Validation loss: 148.10685835, Gradient norm: 464.95844342
INFO:root:[   52] Training loss: 150.85305205, Validation loss: 148.19242017, Gradient norm: 515.62096034
INFO:root:[   53] Training loss: 150.78140556, Validation loss: 148.89707105, Gradient norm: 518.40308310
INFO:root:[   54] Training loss: 150.68648374, Validation loss: 148.25186315, Gradient norm: 536.51000558
INFO:root:[   55] Training loss: 150.69505405, Validation loss: 147.80095594, Gradient norm: 562.49829624
INFO:root:[   56] Training loss: 150.48293743, Validation loss: 147.86130866, Gradient norm: 545.82598194
INFO:root:[   57] Training loss: 150.61401421, Validation loss: 149.25490596, Gradient norm: 540.71493913
INFO:root:[   58] Training loss: 150.45697683, Validation loss: 147.49314196, Gradient norm: 566.62011804
INFO:root:[   59] Training loss: 150.43753916, Validation loss: 149.38600527, Gradient norm: 619.70210599
INFO:root:[   60] Training loss: 150.35628746, Validation loss: 147.98827809, Gradient norm: 595.97706761
INFO:root:[   61] Training loss: 150.31430081, Validation loss: 147.83837259, Gradient norm: 624.02289903
INFO:root:[   62] Training loss: 150.41815226, Validation loss: 147.82922100, Gradient norm: 607.77071480
INFO:root:[   63] Training loss: 150.17413330, Validation loss: 147.59313965, Gradient norm: 632.44711208
INFO:root:[   64] Training loss: 150.22550964, Validation loss: 147.97541336, Gradient norm: 700.22850393
INFO:root:[   65] Training loss: 150.08129140, Validation loss: 147.70007693, Gradient norm: 597.42461917
INFO:root:[   66] Training loss: 150.20060919, Validation loss: 147.38015379, Gradient norm: 648.13750029
INFO:root:[   67] Training loss: 150.08002978, Validation loss: 147.55379355, Gradient norm: 711.24663897
INFO:root:[   68] Training loss: 150.19109500, Validation loss: 147.11778312, Gradient norm: 613.45731752
INFO:root:[   69] Training loss: 149.90250755, Validation loss: 147.27549112, Gradient norm: 694.67631094
INFO:root:[   70] Training loss: 150.06814400, Validation loss: 148.17975485, Gradient norm: 724.14862420
INFO:root:[   71] Training loss: 149.89216951, Validation loss: 147.78888992, Gradient norm: 670.96106803
INFO:root:[   72] Training loss: 149.85283384, Validation loss: 146.96998807, Gradient norm: 732.58149178
INFO:root:[   73] Training loss: 149.78439466, Validation loss: 147.48313062, Gradient norm: 739.24477223
INFO:root:[   74] Training loss: 149.80098488, Validation loss: 147.75525481, Gradient norm: 740.08923756
INFO:root:[   75] Training loss: 149.69831146, Validation loss: 147.34575890, Gradient norm: 681.37486781
INFO:root:[   76] Training loss: 149.79856859, Validation loss: 148.58130567, Gradient norm: 757.83303565
INFO:root:[   77] Training loss: 149.73131636, Validation loss: 147.05402979, Gradient norm: 736.19949543
INFO:root:[   78] Training loss: 149.60165567, Validation loss: 147.35878570, Gradient norm: 705.22805093
INFO:root:[   79] Training loss: 149.50807906, Validation loss: 146.50822396, Gradient norm: 778.92211833
INFO:root:[   80] Training loss: 149.83983686, Validation loss: 146.60753342, Gradient norm: 752.45730548
INFO:root:[   81] Training loss: 149.63979265, Validation loss: 146.33190234, Gradient norm: 698.38631675
INFO:root:[   82] Training loss: 149.43658380, Validation loss: 146.45546801, Gradient norm: 752.44735223
INFO:root:[   83] Training loss: 149.34212649, Validation loss: 146.54392427, Gradient norm: 711.32310652
INFO:root:[   84] Training loss: 149.54550549, Validation loss: 146.72719706, Gradient norm: 803.95824423
INFO:root:[   85] Training loss: 149.40434751, Validation loss: 146.98682746, Gradient norm: 770.55363374
INFO:root:[   86] Training loss: 149.31047666, Validation loss: 148.33036962, Gradient norm: 757.32970092
INFO:root:[   87] Training loss: 149.41432136, Validation loss: 148.38109668, Gradient norm: 793.15560317
INFO:root:[   88] Training loss: 149.16668242, Validation loss: 146.61685496, Gradient norm: 781.54621503
INFO:root:[   89] Training loss: 149.21972805, Validation loss: 146.95761056, Gradient norm: 815.78394070
INFO:root:[   90] Training loss: 149.18928811, Validation loss: 146.06023644, Gradient norm: 761.59179754
INFO:root:[   91] Training loss: 149.29768898, Validation loss: 147.41918630, Gradient norm: 791.51381056
INFO:root:[   92] Training loss: 149.17160075, Validation loss: 146.89636651, Gradient norm: 773.26688191
INFO:root:[   93] Training loss: 149.16414595, Validation loss: 146.39346156, Gradient norm: 775.23364127
INFO:root:[   94] Training loss: 149.12373541, Validation loss: 146.13870976, Gradient norm: 793.93536386
INFO:root:[   95] Training loss: 149.12403370, Validation loss: 148.25649077, Gradient norm: 815.54374889
INFO:root:[   96] Training loss: 149.14965388, Validation loss: 146.91580989, Gradient norm: 804.84306865
INFO:root:[   97] Training loss: 149.04704069, Validation loss: 145.72486351, Gradient norm: 760.52686970
INFO:root:[   98] Training loss: 149.05119837, Validation loss: 145.89826229, Gradient norm: 811.30834591
INFO:root:[   99] Training loss: 149.08864492, Validation loss: 146.77918217, Gradient norm: 840.56165520
INFO:root:[  100] Training loss: 148.89726149, Validation loss: 145.82958458, Gradient norm: 832.30207943
INFO:root:[  101] Training loss: 149.21827914, Validation loss: 146.40375492, Gradient norm: 856.88596705
INFO:root:[  102] Training loss: 148.86474272, Validation loss: 146.64663223, Gradient norm: 795.15960344
INFO:root:[  103] Training loss: 149.03029855, Validation loss: 148.82811658, Gradient norm: 885.50455592
INFO:root:[  104] Training loss: 148.77587013, Validation loss: 146.17759389, Gradient norm: 794.82892742
INFO:root:[  105] Training loss: 149.07918042, Validation loss: 147.34976512, Gradient norm: 882.77038703
INFO:root:[  106] Training loss: 149.09821556, Validation loss: 147.57521425, Gradient norm: 857.97550616
INFO:root:[  107] Training loss: 149.14201909, Validation loss: 145.58032332, Gradient norm: 844.94282487
INFO:root:[  108] Training loss: 148.76152079, Validation loss: 146.63377854, Gradient norm: 848.65319665
INFO:root:[  109] Training loss: 148.78986122, Validation loss: 146.27037048, Gradient norm: 872.23442938
INFO:root:[  110] Training loss: 148.90343158, Validation loss: 149.06358390, Gradient norm: 921.45962334
INFO:root:[  111] Training loss: 148.92699912, Validation loss: 146.10493101, Gradient norm: 889.23005522
INFO:root:[  112] Training loss: 148.83594331, Validation loss: 146.48141059, Gradient norm: 884.10773393
INFO:root:[  113] Training loss: 148.76475471, Validation loss: 146.72648726, Gradient norm: 880.28930525
INFO:root:[  114] Training loss: 148.80847992, Validation loss: 145.62987282, Gradient norm: 855.13362493
INFO:root:[  115] Training loss: 148.81122447, Validation loss: 145.73307800, Gradient norm: 906.75749435
INFO:root:[  116] Training loss: 148.70302562, Validation loss: 145.88336603, Gradient norm: 944.47072657
INFO:root:EP 116: Early stopping
INFO:root:Training the model took 1532.01s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 143.60896
INFO:root:EnergyScoreTrain: 142.78855
INFO:root:CoverageTrain: 0.00912
INFO:root:IntervalWidthTrain: 0.02757
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 145.65549
INFO:root:EnergyScoreValidation: 144.83827
INFO:root:CoverageValidation: 0.00888
INFO:root:IntervalWidthValidation: 0.02748
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 145.96915
INFO:root:EnergyScoreTest: 145.1527
INFO:root:CoverageTest: 0.0088
INFO:root:IntervalWidthTest: 0.02737
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.55358387, Validation loss: 171.44923453, Gradient norm: 2.71455296
INFO:root:[    2] Training loss: 170.98528945, Validation loss: 170.11378426, Gradient norm: 3.06916845
INFO:root:[    3] Training loss: 169.39039315, Validation loss: 166.84279922, Gradient norm: 6.29266005
INFO:root:[    4] Training loss: 166.18073171, Validation loss: 163.26126467, Gradient norm: 12.81529584
INFO:root:[    5] Training loss: 163.62298665, Validation loss: 160.78013295, Gradient norm: 15.66688544
INFO:root:[    6] Training loss: 162.21282108, Validation loss: 159.45311185, Gradient norm: 20.47937367
INFO:root:[    7] Training loss: 161.15444204, Validation loss: 158.33898400, Gradient norm: 26.40451646
INFO:root:[    8] Training loss: 160.33872405, Validation loss: 157.50382417, Gradient norm: 32.22256469
INFO:root:[    9] Training loss: 159.66770719, Validation loss: 156.84178793, Gradient norm: 49.44127493
INFO:root:[   10] Training loss: 159.01391764, Validation loss: 156.11917167, Gradient norm: 50.60322592
INFO:root:[   11] Training loss: 158.49673921, Validation loss: 155.65834782, Gradient norm: 66.78369520
INFO:root:[   12] Training loss: 158.02202613, Validation loss: 154.82431767, Gradient norm: 79.18590097
INFO:root:[   13] Training loss: 157.64638837, Validation loss: 154.68559265, Gradient norm: 90.40886396
INFO:root:[   14] Training loss: 157.37572121, Validation loss: 154.26381552, Gradient norm: 103.17821553
INFO:root:[   15] Training loss: 156.98443766, Validation loss: 154.10850157, Gradient norm: 118.78912148
INFO:root:[   16] Training loss: 156.72144662, Validation loss: 153.74885454, Gradient norm: 143.96233469
INFO:root:[   17] Training loss: 156.38183175, Validation loss: 153.19716986, Gradient norm: 153.05314860
INFO:root:[   18] Training loss: 156.17587159, Validation loss: 152.76069325, Gradient norm: 168.94561407
INFO:root:[   19] Training loss: 155.91584805, Validation loss: 152.96756245, Gradient norm: 192.12020857
INFO:root:[   20] Training loss: 155.70328826, Validation loss: 152.61593996, Gradient norm: 210.89259959
INFO:root:[   21] Training loss: 155.53079575, Validation loss: 152.08394965, Gradient norm: 219.46152747
INFO:root:[   22] Training loss: 155.31225734, Validation loss: 152.06135664, Gradient norm: 229.45242630
INFO:root:[   23] Training loss: 155.15412808, Validation loss: 152.11175432, Gradient norm: 275.19902990
INFO:root:[   24] Training loss: 154.96473964, Validation loss: 151.45294400, Gradient norm: 285.50013989
INFO:root:[   25] Training loss: 154.81629755, Validation loss: 152.17741973, Gradient norm: 286.50184428
INFO:root:[   26] Training loss: 154.71822688, Validation loss: 152.42233697, Gradient norm: 303.15185065
INFO:root:[   27] Training loss: 154.58654542, Validation loss: 151.12334416, Gradient norm: 326.62633351
INFO:root:[   28] Training loss: 154.49582017, Validation loss: 150.81004860, Gradient norm: 377.05949349
INFO:root:[   29] Training loss: 154.33003707, Validation loss: 150.71487374, Gradient norm: 347.34806146
INFO:root:[   30] Training loss: 154.24772793, Validation loss: 153.04594264, Gradient norm: 369.89658287
INFO:root:[   31] Training loss: 154.27449157, Validation loss: 150.31365809, Gradient norm: 416.66737620
INFO:root:[   32] Training loss: 154.08204678, Validation loss: 150.82950829, Gradient norm: 418.58097326
INFO:root:[   33] Training loss: 153.96805350, Validation loss: 150.94897198, Gradient norm: 443.41581437
INFO:root:[   34] Training loss: 153.98600094, Validation loss: 149.87725672, Gradient norm: 443.57882817
INFO:root:[   35] Training loss: 153.79931222, Validation loss: 150.17223595, Gradient norm: 456.86756860
INFO:root:[   36] Training loss: 153.86724354, Validation loss: 150.37060179, Gradient norm: 543.48891601
INFO:root:[   37] Training loss: 153.77679092, Validation loss: 150.93635559, Gradient norm: 461.64897147
INFO:root:[   38] Training loss: 153.69459817, Validation loss: 150.47945325, Gradient norm: 517.17151714
INFO:root:[   39] Training loss: 153.61979743, Validation loss: 150.34750051, Gradient norm: 522.65124773
INFO:root:[   40] Training loss: 153.48677725, Validation loss: 149.65797003, Gradient norm: 527.42013826
INFO:root:[   41] Training loss: 153.49769700, Validation loss: 149.61239992, Gradient norm: 560.53937251
INFO:root:[   42] Training loss: 153.69831727, Validation loss: 149.62034449, Gradient norm: 584.88471759
INFO:root:[   43] Training loss: 153.42520223, Validation loss: 150.86230890, Gradient norm: 558.36929439
INFO:root:[   44] Training loss: 153.44766019, Validation loss: 150.36970520, Gradient norm: 566.85735940
INFO:root:[   45] Training loss: 153.39440337, Validation loss: 149.51673679, Gradient norm: 571.61940511
INFO:root:[   46] Training loss: 153.38137898, Validation loss: 149.01825004, Gradient norm: 579.13937095
INFO:root:[   47] Training loss: 153.18338863, Validation loss: 149.48083180, Gradient norm: 621.08283547
INFO:root:[   48] Training loss: 153.26797566, Validation loss: 149.41135380, Gradient norm: 633.66390517
INFO:root:[   49] Training loss: 153.18700132, Validation loss: 148.94095059, Gradient norm: 646.91861517
INFO:root:[   50] Training loss: 153.25039511, Validation loss: 150.17883880, Gradient norm: 633.11012255
INFO:root:[   51] Training loss: 153.12285243, Validation loss: 150.39501953, Gradient norm: 723.16976917
INFO:root:[   52] Training loss: 153.19867159, Validation loss: 149.05381564, Gradient norm: 670.04829606
INFO:root:[   53] Training loss: 153.07043281, Validation loss: 149.10157144, Gradient norm: 661.96664577
INFO:root:[   54] Training loss: 152.94719527, Validation loss: 149.17459054, Gradient norm: 715.54683247
INFO:root:[   55] Training loss: 152.94391004, Validation loss: 151.04552539, Gradient norm: 681.17583765
INFO:root:[   56] Training loss: 153.16286219, Validation loss: 149.14356363, Gradient norm: 694.25931099
INFO:root:[   57] Training loss: 153.12983028, Validation loss: 149.13329815, Gradient norm: 741.35513730
INFO:root:[   58] Training loss: 153.18284175, Validation loss: 148.73795029, Gradient norm: 672.23774487
INFO:root:[   59] Training loss: 153.07388535, Validation loss: 148.60987696, Gradient norm: 640.69736096
INFO:root:[   60] Training loss: 152.86661104, Validation loss: 149.33447844, Gradient norm: 770.23079499
INFO:root:[   61] Training loss: 152.95596340, Validation loss: 148.64607923, Gradient norm: 705.14225658
INFO:root:[   62] Training loss: 152.90344886, Validation loss: 149.02501915, Gradient norm: 779.86589535
INFO:root:[   63] Training loss: 152.97806556, Validation loss: 149.97316505, Gradient norm: 759.19311765
INFO:root:[   64] Training loss: 152.68537700, Validation loss: 152.98307537, Gradient norm: 751.88505902
INFO:root:[   65] Training loss: 152.99728907, Validation loss: 149.09842129, Gradient norm: 786.86848218
INFO:root:[   66] Training loss: 152.70917761, Validation loss: 148.77476081, Gradient norm: 764.04404210
INFO:root:[   67] Training loss: 153.01126936, Validation loss: 148.97304246, Gradient norm: 778.43393814
INFO:root:[   68] Training loss: 152.63052922, Validation loss: 148.43589151, Gradient norm: 816.62308650
INFO:root:[   69] Training loss: 152.88442561, Validation loss: 148.53674106, Gradient norm: 778.91096056
INFO:root:[   70] Training loss: 152.71175135, Validation loss: 148.65933912, Gradient norm: 832.07033702
INFO:root:[   71] Training loss: 153.12376525, Validation loss: 149.35419806, Gradient norm: 799.01622735
INFO:root:[   72] Training loss: 152.59240061, Validation loss: 148.49637683, Gradient norm: 801.25052075
INFO:root:[   73] Training loss: 153.20055423, Validation loss: 149.32805818, Gradient norm: 788.03756279
INFO:root:[   74] Training loss: 153.02714106, Validation loss: 148.60378765, Gradient norm: 791.39227935
INFO:root:[   75] Training loss: 152.70842952, Validation loss: 148.09572470, Gradient norm: 796.80383391
INFO:root:[   76] Training loss: 152.71355418, Validation loss: 149.57008888, Gradient norm: 878.24321944
INFO:root:[   77] Training loss: 152.71445661, Validation loss: 149.50361002, Gradient norm: 821.57934534
INFO:root:[   78] Training loss: 153.13140532, Validation loss: 148.56807472, Gradient norm: 813.46211259
INFO:root:[   79] Training loss: 152.55718143, Validation loss: 148.40775746, Gradient norm: 866.22082659
INFO:root:[   80] Training loss: 152.56883780, Validation loss: 148.44060701, Gradient norm: 906.91063900
INFO:root:[   81] Training loss: 152.78820045, Validation loss: 148.40076210, Gradient norm: 906.72852708
INFO:root:[   82] Training loss: 152.53716946, Validation loss: 149.80613603, Gradient norm: 930.74982000
INFO:root:[   83] Training loss: 152.61471558, Validation loss: 148.35272532, Gradient norm: 913.34392239
INFO:root:[   84] Training loss: 152.68083556, Validation loss: 149.90228482, Gradient norm: 909.08160777
INFO:root:EP 84: Early stopping
INFO:root:Training the model took 1034.058s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 147.27679
INFO:root:EnergyScoreTrain: 146.54541
INFO:root:CoverageTrain: 0.00692
INFO:root:IntervalWidthTrain: 0.02375
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 148.17058
INFO:root:EnergyScoreValidation: 147.40712
INFO:root:CoverageValidation: 0.00707
INFO:root:IntervalWidthValidation: 0.02489
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 148.41198
INFO:root:EnergyScoreTest: 147.83472
INFO:root:CoverageTest: 0.00532
INFO:root:IntervalWidthTest: 0.01854
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.62067218, Validation loss: 171.54396005, Gradient norm: 3.32352262
INFO:root:[    2] Training loss: 171.30571173, Validation loss: 170.43194475, Gradient norm: 1.86742416
INFO:root:[    3] Training loss: 169.85805194, Validation loss: 167.74429953, Gradient norm: 5.22060693
INFO:root:[    4] Training loss: 167.33819688, Validation loss: 164.44346934, Gradient norm: 12.79762643
INFO:root:[    5] Training loss: 165.00238091, Validation loss: 161.92532349, Gradient norm: 19.18201830
INFO:root:[    6] Training loss: 163.55395859, Validation loss: 160.84099289, Gradient norm: 30.40387365
INFO:root:[    7] Training loss: 162.61127182, Validation loss: 159.98398932, Gradient norm: 46.52964692
INFO:root:[    8] Training loss: 161.89496552, Validation loss: 158.99589328, Gradient norm: 76.01785332
INFO:root:[    9] Training loss: 161.31988269, Validation loss: 158.25748523, Gradient norm: 95.98317837
INFO:root:[   10] Training loss: 160.85175195, Validation loss: 157.73182994, Gradient norm: 129.95598113
INFO:root:[   11] Training loss: 160.41398134, Validation loss: 156.89270967, Gradient norm: 151.81986139
INFO:root:[   12] Training loss: 160.08055803, Validation loss: 157.05794131, Gradient norm: 166.62262457
INFO:root:[   13] Training loss: 159.76163922, Validation loss: 156.66059297, Gradient norm: 221.27636262
INFO:root:[   14] Training loss: 159.41239956, Validation loss: 156.37716570, Gradient norm: 218.03149100
INFO:root:[   15] Training loss: 159.34175474, Validation loss: 155.52225415, Gradient norm: 280.92960387
INFO:root:[   16] Training loss: 158.92161803, Validation loss: 155.32039090, Gradient norm: 262.64203889
INFO:root:[   17] Training loss: 158.86704517, Validation loss: 154.92559183, Gradient norm: 295.35739982
INFO:root:[   18] Training loss: 158.70292177, Validation loss: 156.35683731, Gradient norm: 339.74459147
INFO:root:[   19] Training loss: 158.34667847, Validation loss: 154.66285863, Gradient norm: 316.64468319
INFO:root:[   20] Training loss: 158.24197307, Validation loss: 154.71775081, Gradient norm: 407.10134274
INFO:root:[   21] Training loss: 158.14436948, Validation loss: 154.19382556, Gradient norm: 412.50491788
INFO:root:[   22] Training loss: 157.92888891, Validation loss: 154.39603819, Gradient norm: 422.48275610
INFO:root:[   23] Training loss: 158.14445428, Validation loss: 155.20148757, Gradient norm: 372.46355854
INFO:root:[   24] Training loss: 157.83003437, Validation loss: 153.44311050, Gradient norm: 419.60399164
INFO:root:[   25] Training loss: 157.64547014, Validation loss: 153.56343763, Gradient norm: 476.65887175
INFO:root:[   26] Training loss: 157.57225496, Validation loss: 153.66744469, Gradient norm: 455.72876738
INFO:root:[   27] Training loss: 157.51096715, Validation loss: 152.95955211, Gradient norm: 520.66591225
INFO:root:[   28] Training loss: 157.28555703, Validation loss: 153.74660255, Gradient norm: 533.98992578
INFO:root:[   29] Training loss: 157.27070240, Validation loss: 153.22320557, Gradient norm: 586.30967835
INFO:root:[   30] Training loss: 157.63811486, Validation loss: 153.10148515, Gradient norm: 537.43525153
INFO:root:[   31] Training loss: 157.12014352, Validation loss: 152.68458662, Gradient norm: 566.71381259
INFO:root:[   32] Training loss: 157.47546994, Validation loss: 154.09829975, Gradient norm: 559.14114232
INFO:root:[   33] Training loss: 157.17017628, Validation loss: 155.63373697, Gradient norm: 618.74999879
INFO:root:[   34] Training loss: 156.91518692, Validation loss: 153.34790934, Gradient norm: 559.87629994
INFO:root:[   35] Training loss: 157.02505075, Validation loss: 154.43173744, Gradient norm: 675.84009164
INFO:root:[   36] Training loss: 156.93952496, Validation loss: 152.44708989, Gradient norm: 584.01538528
INFO:root:[   37] Training loss: 156.70266089, Validation loss: 152.47921648, Gradient norm: 664.05339650
INFO:root:[   38] Training loss: 156.71306374, Validation loss: 153.07319378, Gradient norm: 664.45759768
INFO:root:[   39] Training loss: 156.70743284, Validation loss: 152.25178633, Gradient norm: 708.24415096
INFO:root:[   40] Training loss: 157.17478524, Validation loss: 152.28173512, Gradient norm: 643.95521459
INFO:root:[   41] Training loss: 156.61967981, Validation loss: 151.84882749, Gradient norm: 691.76275745
INFO:root:[   42] Training loss: 156.71291507, Validation loss: 151.81010174, Gradient norm: 732.13834653
INFO:root:[   43] Training loss: 156.46830344, Validation loss: 152.11723749, Gradient norm: 714.37554506
INFO:root:[   44] Training loss: 156.46339322, Validation loss: 153.70604311, Gradient norm: 773.88011289
INFO:root:[   45] Training loss: 156.41458778, Validation loss: 154.56897446, Gradient norm: 769.38484920
INFO:root:[   46] Training loss: 156.73697629, Validation loss: 153.25566048, Gradient norm: 786.38420768
INFO:root:[   47] Training loss: 156.44672428, Validation loss: 151.70089827, Gradient norm: 739.09433623
INFO:root:[   48] Training loss: 156.29714156, Validation loss: 151.40380175, Gradient norm: 729.91700723
INFO:root:[   49] Training loss: 156.92256759, Validation loss: 152.70607784, Gradient norm: 699.20034907
INFO:root:[   50] Training loss: 156.32992149, Validation loss: 151.81932752, Gradient norm: 727.76094105
INFO:root:[   51] Training loss: 156.34047625, Validation loss: 152.21863635, Gradient norm: 766.44936360
INFO:root:[   52] Training loss: 156.04540611, Validation loss: 151.46620599, Gradient norm: 797.87200995
INFO:root:[   53] Training loss: 156.46410525, Validation loss: 151.69456482, Gradient norm: 772.42782891
INFO:root:[   54] Training loss: 156.30530420, Validation loss: 153.96075124, Gradient norm: 792.85154811
INFO:root:[   55] Training loss: 156.08663535, Validation loss: 151.87851163, Gradient norm: 809.79630099
INFO:root:[   56] Training loss: 156.24136974, Validation loss: 152.19022080, Gradient norm: 901.82014562
INFO:root:[   57] Training loss: 156.15744127, Validation loss: 151.61794886, Gradient norm: 805.20444862
INFO:root:[   58] Training loss: 156.07440591, Validation loss: 151.49679776, Gradient norm: 867.44395868
INFO:root:[   59] Training loss: 156.29297739, Validation loss: 151.81481355, Gradient norm: 866.90816901
INFO:root:[   60] Training loss: 156.19021863, Validation loss: 151.05570142, Gradient norm: 862.30329034
INFO:root:[   61] Training loss: 156.19203159, Validation loss: 153.67270003, Gradient norm: 939.56958244
INFO:root:[   62] Training loss: 156.05247579, Validation loss: 151.56352181, Gradient norm: 880.30435639
INFO:root:[   63] Training loss: 156.31661866, Validation loss: 151.92697617, Gradient norm: 866.11764620
INFO:root:[   64] Training loss: 156.16908912, Validation loss: 152.13010170, Gradient norm: 838.96572190
INFO:root:[   65] Training loss: 156.73059757, Validation loss: 151.95224052, Gradient norm: 853.46019447
INFO:root:[   66] Training loss: 156.49502604, Validation loss: 152.22119825, Gradient norm: 963.00593300
INFO:root:[   67] Training loss: 156.26547619, Validation loss: 151.41448659, Gradient norm: 947.89031003
INFO:root:[   68] Training loss: 156.34744074, Validation loss: 152.05601817, Gradient norm: 910.90140001
INFO:root:[   69] Training loss: 156.54958944, Validation loss: 151.70380796, Gradient norm: 985.33920536
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 930.284s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 150.66407
INFO:root:EnergyScoreTrain: 149.82421
INFO:root:CoverageTrain: 0.00656
INFO:root:IntervalWidthTrain: 0.02629
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 151.15391
INFO:root:EnergyScoreValidation: 150.33672
INFO:root:CoverageValidation: 0.00628
INFO:root:IntervalWidthValidation: 0.02558
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 151.33737
INFO:root:EnergyScoreTest: 150.40481
INFO:root:CoverageTest: 0.0072
INFO:root:IntervalWidthTest: 0.02933
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.71875756, Validation loss: 171.46467538, Gradient norm: 5.21180259
INFO:root:[    2] Training loss: 171.47170426, Validation loss: 171.45960683, Gradient norm: 0.45982505
INFO:root:[    3] Training loss: 171.20361247, Validation loss: 170.38933326, Gradient norm: 2.86269793
INFO:root:[    4] Training loss: 170.18569973, Validation loss: 168.64136637, Gradient norm: 4.59026330
INFO:root:[    5] Training loss: 168.19347024, Validation loss: 165.74139878, Gradient norm: 9.74957459
INFO:root:[    6] Training loss: 166.14826830, Validation loss: 163.14408085, Gradient norm: 19.34423958
INFO:root:[    7] Training loss: 164.81763357, Validation loss: 161.93325227, Gradient norm: 29.17332444
INFO:root:[    8] Training loss: 164.00888710, Validation loss: 161.19556243, Gradient norm: 38.69698338
INFO:root:[    9] Training loss: 163.43963366, Validation loss: 160.48033879, Gradient norm: 58.16976321
INFO:root:[   10] Training loss: 162.96185937, Validation loss: 160.08377391, Gradient norm: 71.96728760
INFO:root:[   11] Training loss: 162.64890391, Validation loss: 160.14829754, Gradient norm: 99.76899935
INFO:root:[   12] Training loss: 162.24905247, Validation loss: 159.10800908, Gradient norm: 115.35406231
INFO:root:[   13] Training loss: 162.00526631, Validation loss: 158.80767612, Gradient norm: 153.14076938
INFO:root:[   14] Training loss: 161.73168999, Validation loss: 158.21803599, Gradient norm: 174.08880356
INFO:root:[   15] Training loss: 161.59141959, Validation loss: 158.33648050, Gradient norm: 214.63770132
INFO:root:[   16] Training loss: 161.34370814, Validation loss: 157.60386184, Gradient norm: 239.59363309
INFO:root:[   17] Training loss: 161.20477754, Validation loss: 157.64415452, Gradient norm: 258.16118737
INFO:root:[   18] Training loss: 160.99268321, Validation loss: 157.49580225, Gradient norm: 304.57192741
INFO:root:[   19] Training loss: 160.90492343, Validation loss: 157.50984613, Gradient norm: 315.68608509
INFO:root:[   20] Training loss: 160.78000094, Validation loss: 156.71449069, Gradient norm: 361.65308115
INFO:root:[   21] Training loss: 160.70767901, Validation loss: 156.74608954, Gradient norm: 383.23951471
INFO:root:[   22] Training loss: 160.61789818, Validation loss: 156.38266833, Gradient norm: 381.04846336
INFO:root:[   23] Training loss: 160.41508497, Validation loss: 156.00675912, Gradient norm: 430.40892581
INFO:root:[   24] Training loss: 160.36739505, Validation loss: 156.23786190, Gradient norm: 454.19551342
INFO:root:[   25] Training loss: 160.27031877, Validation loss: 156.02105081, Gradient norm: 431.30391804
INFO:root:[   26] Training loss: 160.19367414, Validation loss: 156.36115975, Gradient norm: 482.00165844
INFO:root:[   27] Training loss: 160.10072178, Validation loss: 155.63241104, Gradient norm: 533.35217772
INFO:root:[   28] Training loss: 160.42064511, Validation loss: 155.93233622, Gradient norm: 465.21902268
INFO:root:[   29] Training loss: 160.01504274, Validation loss: 155.81814312, Gradient norm: 551.81784018
INFO:root:[   30] Training loss: 159.84941479, Validation loss: 155.17521404, Gradient norm: 554.68544507
INFO:root:[   31] Training loss: 160.00249380, Validation loss: 155.38617943, Gradient norm: 576.55563191
INFO:root:[   32] Training loss: 159.92309098, Validation loss: 155.80991706, Gradient norm: 574.33909042
INFO:root:[   33] Training loss: 159.64731065, Validation loss: 155.56332818, Gradient norm: 598.28368813
INFO:root:[   34] Training loss: 159.66061847, Validation loss: 155.66359158, Gradient norm: 617.13613733
INFO:root:[   35] Training loss: 159.75518367, Validation loss: 155.31908022, Gradient norm: 599.66910730
INFO:root:[   36] Training loss: 159.86092451, Validation loss: 155.37677739, Gradient norm: 601.37934132
INFO:root:[   37] Training loss: 159.69166754, Validation loss: 155.10773810, Gradient norm: 603.19644423
INFO:root:[   38] Training loss: 159.70058529, Validation loss: 154.71604919, Gradient norm: 634.36858525
INFO:root:[   39] Training loss: 159.41548400, Validation loss: 155.14851274, Gradient norm: 623.56342219
INFO:root:[   40] Training loss: 159.51689337, Validation loss: 155.01515671, Gradient norm: 668.97545322
INFO:root:[   41] Training loss: 159.53100667, Validation loss: 154.53993962, Gradient norm: 701.80033852
INFO:root:[   42] Training loss: 159.25126418, Validation loss: 154.77906747, Gradient norm: 750.06798571
INFO:root:[   43] Training loss: 159.46626984, Validation loss: 155.08265686, Gradient norm: 739.41381027
INFO:root:[   44] Training loss: 159.41713087, Validation loss: 155.82672330, Gradient norm: 772.98213750
INFO:root:[   45] Training loss: 159.39138389, Validation loss: 155.57678275, Gradient norm: 801.06954801
INFO:root:[   46] Training loss: 159.31065679, Validation loss: 154.61426202, Gradient norm: 777.61037337
INFO:root:[   47] Training loss: 159.41386184, Validation loss: 155.44827218, Gradient norm: 804.94589553
INFO:root:[   48] Training loss: 159.50641963, Validation loss: 155.92050960, Gradient norm: 850.42967705
INFO:root:[   49] Training loss: 159.41264856, Validation loss: 154.77512333, Gradient norm: 785.07360807
INFO:root:[   50] Training loss: 159.45989045, Validation loss: 155.29868027, Gradient norm: 853.87339931
INFO:root:[   51] Training loss: 159.48384000, Validation loss: 155.23932464, Gradient norm: 879.09304591
INFO:root:[   52] Training loss: 159.37605961, Validation loss: 161.26486837, Gradient norm: 921.18653827
INFO:root:[   53] Training loss: 159.59581183, Validation loss: 154.52163065, Gradient norm: 871.97774733
INFO:root:[   54] Training loss: 159.36823428, Validation loss: 155.10641953, Gradient norm: 893.53559601
INFO:root:[   55] Training loss: 159.34757091, Validation loss: 154.79932114, Gradient norm: 952.65973144
INFO:root:[   56] Training loss: 159.56684335, Validation loss: 156.25677438, Gradient norm: 1029.30240678
INFO:root:[   57] Training loss: 159.52019899, Validation loss: 155.12134105, Gradient norm: 989.38964655
INFO:root:[   58] Training loss: 159.63816347, Validation loss: 154.40049323, Gradient norm: 1003.04172316
INFO:root:[   59] Training loss: 161.11101498, Validation loss: 158.00961146, Gradient norm: 1194.93095340
INFO:root:[   60] Training loss: 160.72435727, Validation loss: 156.28009822, Gradient norm: 1044.57215145
INFO:root:[   61] Training loss: 160.18397603, Validation loss: 155.26760654, Gradient norm: 1079.86090011
INFO:root:[   62] Training loss: 160.19980493, Validation loss: 156.06193648, Gradient norm: 1174.99465504
INFO:root:[   63] Training loss: 160.12331681, Validation loss: 156.59959780, Gradient norm: 1229.44571763
INFO:root:[   64] Training loss: 161.26228589, Validation loss: 158.45544276, Gradient norm: 1328.37658767
INFO:root:[   65] Training loss: 160.93612873, Validation loss: 155.51278950, Gradient norm: 1252.52460502
INFO:root:[   66] Training loss: 160.64992841, Validation loss: 155.64205670, Gradient norm: 1263.76956655
INFO:root:[   67] Training loss: 161.11703316, Validation loss: 157.46525889, Gradient norm: 1376.19504726
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 755.602s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 154.21837
INFO:root:EnergyScoreTrain: 153.36813
INFO:root:CoverageTrain: 0.00527
INFO:root:IntervalWidthTrain: 0.02169
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 154.49223
INFO:root:EnergyScoreValidation: 153.74555
INFO:root:CoverageValidation: 0.0046
INFO:root:IntervalWidthValidation: 0.01897
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 154.72675
INFO:root:EnergyScoreTest: 154.01398
INFO:root:CoverageTest: 0.00433
INFO:root:IntervalWidthTest: 0.01795
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 297795584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 173.14021490, Validation loss: 171.42964067, Gradient norm: 20.69591114
INFO:root:[    2] Training loss: 171.51573978, Validation loss: 171.57689904, Gradient norm: 0.81009365
INFO:root:[    3] Training loss: 171.48196560, Validation loss: 171.54274460, Gradient norm: 0.48584320
INFO:root:[    4] Training loss: 171.47206804, Validation loss: 171.49574122, Gradient norm: 0.38813826
INFO:root:[    5] Training loss: 171.45991597, Validation loss: 171.42730239, Gradient norm: 0.32973315
INFO:root:[    6] Training loss: 171.46098058, Validation loss: 171.47425158, Gradient norm: 0.32558412
INFO:root:[    7] Training loss: 171.12951512, Validation loss: 170.40007124, Gradient norm: 2.12064593
INFO:root:[    8] Training loss: 170.33490638, Validation loss: 169.03017557, Gradient norm: 4.00032251
INFO:root:[    9] Training loss: 169.49144157, Validation loss: 168.19372506, Gradient norm: 18.71426119
INFO:root:[   10] Training loss: 168.90055753, Validation loss: 167.16001155, Gradient norm: 48.07561565
INFO:root:[   11] Training loss: 168.47520393, Validation loss: 167.02775416, Gradient norm: 94.73096539
INFO:root:[   12] Training loss: 168.10738109, Validation loss: 166.01393022, Gradient norm: 133.36840879
INFO:root:[   13] Training loss: 167.80898562, Validation loss: 165.29833563, Gradient norm: 159.78061819
INFO:root:[   14] Training loss: 167.55830518, Validation loss: 164.66691432, Gradient norm: 209.86389487
INFO:root:[   15] Training loss: 167.19242886, Validation loss: 164.30763034, Gradient norm: 252.77782125
INFO:root:[   16] Training loss: 167.11251183, Validation loss: 163.94559189, Gradient norm: 276.72328339
INFO:root:[   17] Training loss: 166.80417329, Validation loss: 163.35315994, Gradient norm: 312.13986275
INFO:root:[   18] Training loss: 166.73920096, Validation loss: 163.52293080, Gradient norm: 347.70606968
INFO:root:[   19] Training loss: 166.52603946, Validation loss: 163.30450492, Gradient norm: 385.52907429
INFO:root:[   20] Training loss: 166.58520508, Validation loss: 165.90421111, Gradient norm: 425.97196048
INFO:root:[   21] Training loss: 166.43145374, Validation loss: 163.00961409, Gradient norm: 458.45870435
INFO:root:[   22] Training loss: 166.63973270, Validation loss: 164.80585453, Gradient norm: 462.38045018
INFO:root:[   23] Training loss: 167.10021338, Validation loss: 166.03586815, Gradient norm: 414.03880060
INFO:root:[   24] Training loss: 168.09944085, Validation loss: 164.92348927, Gradient norm: 425.90166289
INFO:root:[   25] Training loss: 167.93383438, Validation loss: 164.16742890, Gradient norm: 498.93197474
INFO:root:[   26] Training loss: 167.37221628, Validation loss: 163.93386315, Gradient norm: 584.25938021
INFO:root:[   27] Training loss: 167.03979101, Validation loss: 164.63561117, Gradient norm: 583.54769787
INFO:root:[   28] Training loss: 167.27021803, Validation loss: 165.71235288, Gradient norm: 629.29510711
INFO:root:[   29] Training loss: 167.05020871, Validation loss: 163.58872828, Gradient norm: 559.56668565
INFO:root:[   30] Training loss: 167.96943327, Validation loss: 164.95327180, Gradient norm: 739.86237789
INFO:root:[   31] Training loss: 168.74059937, Validation loss: 167.13851876, Gradient norm: 782.71863336
INFO:root:[   32] Training loss: 168.62480596, Validation loss: 166.26182293, Gradient norm: 741.17617672
INFO:root:[   33] Training loss: 167.79252003, Validation loss: 164.65743282, Gradient norm: 637.35467183
INFO:root:[   34] Training loss: 167.47743185, Validation loss: 164.61176379, Gradient norm: 616.20132315
INFO:root:[   35] Training loss: 167.52962582, Validation loss: 165.19450273, Gradient norm: 643.53884958
INFO:root:[   36] Training loss: 169.65463973, Validation loss: 168.66976245, Gradient norm: 733.61533253
INFO:root:[   37] Training loss: 169.96930877, Validation loss: 169.05588189, Gradient norm: 400.93493659
INFO:root:[   38] Training loss: 170.02912754, Validation loss: 168.52998142, Gradient norm: 553.79210749
INFO:root:[   39] Training loss: 169.91510347, Validation loss: 168.96454541, Gradient norm: 486.03166936
INFO:root:[   40] Training loss: 169.79518168, Validation loss: 169.21677793, Gradient norm: 417.49895008
INFO:root:[   41] Training loss: 169.78649605, Validation loss: 168.79931272, Gradient norm: 465.77427256
INFO:root:[   42] Training loss: 169.76859898, Validation loss: 168.48192097, Gradient norm: 468.45329107
INFO:root:[   43] Training loss: 169.66897380, Validation loss: 168.25824974, Gradient norm: 427.20523399
INFO:root:[   44] Training loss: 169.69677221, Validation loss: 168.45499815, Gradient norm: 522.27647146
INFO:root:[   45] Training loss: 169.74767040, Validation loss: 168.97737648, Gradient norm: 607.17130328
INFO:root:[   46] Training loss: 169.79621901, Validation loss: 168.53427598, Gradient norm: 647.52090932
INFO:root:[   47] Training loss: 169.82237298, Validation loss: 168.45396792, Gradient norm: 602.73884015
INFO:root:[   48] Training loss: 170.42242108, Validation loss: 169.84234935, Gradient norm: 562.31702192
INFO:root:[   49] Training loss: 171.38911843, Validation loss: 171.52702752, Gradient norm: 60.90659870
INFO:root:[   50] Training loss: 171.45985210, Validation loss: 171.47633625, Gradient norm: 0.08782369
INFO:root:[   51] Training loss: 171.45751670, Validation loss: 171.44366613, Gradient norm: 0.07920778
INFO:root:[   52] Training loss: 171.45331594, Validation loss: 171.45070359, Gradient norm: 0.11445394
INFO:root:[   53] Training loss: 171.45092517, Validation loss: 171.45903752, Gradient norm: 0.09636039
INFO:root:[   54] Training loss: 171.45454501, Validation loss: 171.62098852, Gradient norm: 0.08336612
INFO:root:[   55] Training loss: 171.45491892, Validation loss: 171.46311845, Gradient norm: 0.08387515
INFO:root:[   56] Training loss: 171.46436168, Validation loss: 171.51161720, Gradient norm: 0.10277751
INFO:root:[   57] Training loss: 171.45303547, Validation loss: 171.38021009, Gradient norm: 0.09055179
INFO:root:[   58] Training loss: 171.45338264, Validation loss: 171.51606540, Gradient norm: 0.28337341
INFO:root:[   59] Training loss: 171.44951245, Validation loss: 171.46871475, Gradient norm: 0.08941394
INFO:root:[   60] Training loss: 171.45341154, Validation loss: 171.50903215, Gradient norm: 0.08105401
INFO:root:[   61] Training loss: 171.45596408, Validation loss: 171.46923723, Gradient norm: 0.08043356
INFO:root:[   62] Training loss: 171.45608750, Validation loss: 171.43417306, Gradient norm: 0.19060102
INFO:root:[   63] Training loss: 171.46101649, Validation loss: 171.44687521, Gradient norm: 0.07745886
INFO:root:[   64] Training loss: 171.45517197, Validation loss: 171.53668844, Gradient norm: 0.09853586
INFO:root:[   65] Training loss: 171.45853134, Validation loss: 171.54210268, Gradient norm: 0.09039176
INFO:root:[   66] Training loss: 171.44874438, Validation loss: 171.53539355, Gradient norm: 0.08636269
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 759.87s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 162.98187
INFO:root:EnergyScoreTrain: 162.32577
INFO:root:CoverageTrain: 0.0033
INFO:root:IntervalWidthTrain: 0.01893
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 163.04492
INFO:root:EnergyScoreValidation: 162.39913
INFO:root:CoverageValidation: 0.00325
INFO:root:IntervalWidthValidation: 0.01862
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 163.15265
INFO:root:EnergyScoreTest: 162.43478
INFO:root:CoverageTest: 0.00368
INFO:root:IntervalWidthTest: 0.02115
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 295698432
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.43445378, Validation loss: 170.98512952, Gradient norm: 2.47848999
INFO:root:[    2] Training loss: 170.34952484, Validation loss: 169.73946565, Gradient norm: 5.46813322
INFO:root:[    3] Training loss: 167.21502348, Validation loss: 164.91049721, Gradient norm: 14.64293502
INFO:root:[    4] Training loss: 163.09752608, Validation loss: 161.90226693, Gradient norm: 22.26781765
INFO:root:[    5] Training loss: 160.58301186, Validation loss: 159.93306495, Gradient norm: 22.67215180
INFO:root:[    6] Training loss: 158.84232243, Validation loss: 158.26233278, Gradient norm: 25.46989064
INFO:root:[    7] Training loss: 157.59330074, Validation loss: 157.28264539, Gradient norm: 29.74102525
INFO:root:[    8] Training loss: 156.64261512, Validation loss: 156.62399397, Gradient norm: 25.66686916
INFO:root:[    9] Training loss: 155.96551230, Validation loss: 156.10100582, Gradient norm: 41.68641102
INFO:root:[   10] Training loss: 155.28701040, Validation loss: 155.56207644, Gradient norm: 28.42868800
INFO:root:[   11] Training loss: 154.76067413, Validation loss: 154.94142151, Gradient norm: 39.11220791
INFO:root:[   12] Training loss: 154.26936421, Validation loss: 154.69616226, Gradient norm: 34.64377149
INFO:root:[   13] Training loss: 153.86297850, Validation loss: 154.03673685, Gradient norm: 39.95075576
INFO:root:[   14] Training loss: 153.42775990, Validation loss: 153.88369856, Gradient norm: 45.20260521
INFO:root:[   15] Training loss: 153.01468179, Validation loss: 153.29255834, Gradient norm: 37.13851576
INFO:root:[   16] Training loss: 152.68249120, Validation loss: 153.22489929, Gradient norm: 54.34966581
INFO:root:[   17] Training loss: 152.38467731, Validation loss: 152.79158336, Gradient norm: 57.23000434
INFO:root:[   18] Training loss: 152.01095000, Validation loss: 152.61251778, Gradient norm: 48.42269189
INFO:root:[   19] Training loss: 151.72318018, Validation loss: 152.24864723, Gradient norm: 58.01231501
INFO:root:[   20] Training loss: 151.48105277, Validation loss: 152.23411244, Gradient norm: 67.96794404
INFO:root:[   21] Training loss: 151.20539518, Validation loss: 151.76507305, Gradient norm: 63.96639593
INFO:root:[   22] Training loss: 150.99156756, Validation loss: 151.85256169, Gradient norm: 74.73598532
INFO:root:[   23] Training loss: 150.70957137, Validation loss: 151.36897173, Gradient norm: 62.00148124
INFO:root:[   24] Training loss: 150.52606539, Validation loss: 151.23002940, Gradient norm: 86.31231167
INFO:root:[   25] Training loss: 150.35174034, Validation loss: 151.18750842, Gradient norm: 82.38378208
INFO:root:[   26] Training loss: 150.16060213, Validation loss: 150.99726342, Gradient norm: 81.84069699
INFO:root:[   27] Training loss: 149.92734791, Validation loss: 150.83539923, Gradient norm: 100.07756569
INFO:root:[   28] Training loss: 149.77069078, Validation loss: 150.89743410, Gradient norm: 90.51495474
INFO:root:[   29] Training loss: 149.64233777, Validation loss: 150.45734221, Gradient norm: 116.65778561
INFO:root:[   30] Training loss: 149.41626125, Validation loss: 150.46962659, Gradient norm: 97.20286782
INFO:root:[   31] Training loss: 149.28413283, Validation loss: 150.19510414, Gradient norm: 119.89380981
INFO:root:[   32] Training loss: 149.11923623, Validation loss: 150.26428644, Gradient norm: 119.02309399
INFO:root:[   33] Training loss: 149.00932893, Validation loss: 150.33245060, Gradient norm: 132.23069927
INFO:root:[   34] Training loss: 148.85503745, Validation loss: 150.09632663, Gradient norm: 127.43800660
INFO:root:[   35] Training loss: 148.76593112, Validation loss: 149.81027906, Gradient norm: 153.36522548
INFO:root:[   36] Training loss: 148.58876443, Validation loss: 150.78829640, Gradient norm: 120.82391462
INFO:root:[   37] Training loss: 148.59595429, Validation loss: 149.64797658, Gradient norm: 145.22671343
INFO:root:[   38] Training loss: 148.34001956, Validation loss: 149.65359971, Gradient norm: 139.25899797
INFO:root:[   39] Training loss: 148.25429825, Validation loss: 149.40023804, Gradient norm: 159.61054709
INFO:root:[   40] Training loss: 148.11726055, Validation loss: 149.49404749, Gradient norm: 170.46019923
INFO:root:[   41] Training loss: 148.06868481, Validation loss: 149.68904903, Gradient norm: 177.04678210
INFO:root:[   42] Training loss: 147.95338629, Validation loss: 149.69103109, Gradient norm: 174.25173135
INFO:root:[   43] Training loss: 147.84370017, Validation loss: 149.53938714, Gradient norm: 186.25069512
INFO:root:[   44] Training loss: 147.72617617, Validation loss: 149.34372580, Gradient norm: 191.08607994
INFO:root:[   45] Training loss: 147.69877665, Validation loss: 149.06543442, Gradient norm: 194.67065134
INFO:root:[   46] Training loss: 147.56394931, Validation loss: 149.79999411, Gradient norm: 194.08784024
INFO:root:[   47] Training loss: 147.44315264, Validation loss: 149.15746807, Gradient norm: 195.24930660
INFO:root:[   48] Training loss: 147.31872761, Validation loss: 148.95473559, Gradient norm: 204.17624151
INFO:root:[   49] Training loss: 147.27269387, Validation loss: 148.95669029, Gradient norm: 207.78760839
INFO:root:[   50] Training loss: 147.16446368, Validation loss: 148.86494604, Gradient norm: 184.38777043
INFO:root:[   51] Training loss: 147.16529036, Validation loss: 149.81883135, Gradient norm: 249.53377157
INFO:root:[   52] Training loss: 147.06420628, Validation loss: 148.66915315, Gradient norm: 203.66442448
INFO:root:[   53] Training loss: 146.92335321, Validation loss: 150.24695613, Gradient norm: 220.07768604
INFO:root:[   54] Training loss: 146.93462514, Validation loss: 148.64342052, Gradient norm: 228.68039820
INFO:root:[   55] Training loss: 146.73358370, Validation loss: 148.65843622, Gradient norm: 219.30644021
INFO:root:[   56] Training loss: 146.75675046, Validation loss: 148.75115230, Gradient norm: 270.43247232
INFO:root:[   57] Training loss: 146.60540380, Validation loss: 148.54450252, Gradient norm: 230.13035089
INFO:root:[   58] Training loss: 146.56872640, Validation loss: 148.41649812, Gradient norm: 262.06582220
INFO:root:[   59] Training loss: 146.50689198, Validation loss: 148.67601434, Gradient norm: 257.92745938
INFO:root:[   60] Training loss: 146.38324528, Validation loss: 148.36979728, Gradient norm: 242.56891350
INFO:root:[   61] Training loss: 146.33751726, Validation loss: 148.53045812, Gradient norm: 265.14022890
INFO:root:[   62] Training loss: 146.25257603, Validation loss: 148.88641621, Gradient norm: 237.24515185
INFO:root:[   63] Training loss: 146.18432077, Validation loss: 148.50114441, Gradient norm: 261.31800756
INFO:root:[   64] Training loss: 146.15444838, Validation loss: 148.37274065, Gradient norm: 279.14235052
INFO:root:[   65] Training loss: 146.03889695, Validation loss: 149.41373312, Gradient norm: 289.72861912
INFO:root:[   66] Training loss: 146.05916089, Validation loss: 148.51408544, Gradient norm: 289.99488883
INFO:root:[   67] Training loss: 145.88488540, Validation loss: 148.19088272, Gradient norm: 254.37304927
INFO:root:[   68] Training loss: 145.94768571, Validation loss: 148.41787614, Gradient norm: 310.48061271
INFO:root:[   69] Training loss: 145.84751500, Validation loss: 148.61137601, Gradient norm: 274.28793575
INFO:root:[   70] Training loss: 145.67303534, Validation loss: 148.31552334, Gradient norm: 251.53219372
INFO:root:[   71] Training loss: 145.70157563, Validation loss: 148.26181872, Gradient norm: 294.43742179
INFO:root:[   72] Training loss: 145.57414894, Validation loss: 147.92735343, Gradient norm: 272.99836829
INFO:root:[   73] Training loss: 145.50959035, Validation loss: 148.59202891, Gradient norm: 295.17894935
INFO:root:[   74] Training loss: 145.50401117, Validation loss: 148.07584302, Gradient norm: 323.78883021
INFO:root:[   75] Training loss: 145.36524113, Validation loss: 148.22159971, Gradient norm: 293.14400527
INFO:root:[   76] Training loss: 145.35103938, Validation loss: 148.29393900, Gradient norm: 346.32869482
INFO:root:[   77] Training loss: 145.27120661, Validation loss: 148.18106237, Gradient norm: 274.69719334
INFO:root:[   78] Training loss: 145.34156894, Validation loss: 148.04623834, Gradient norm: 343.75044612
INFO:root:[   79] Training loss: 145.19496965, Validation loss: 148.40968533, Gradient norm: 300.07016300
INFO:root:[   80] Training loss: 145.16693088, Validation loss: 149.50471444, Gradient norm: 330.60717283
INFO:root:[   81] Training loss: 145.23349634, Validation loss: 148.17355399, Gradient norm: 316.80702379
INFO:root:EP 81: Early stopping
INFO:root:Training the model took 935.519s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 143.29161
INFO:root:EnergyScoreTrain: 128.97694
INFO:root:CoverageTrain: 0.29527
INFO:root:IntervalWidthTrain: 1.20725
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 146.2124
INFO:root:EnergyScoreValidation: 131.86283
INFO:root:CoverageValidation: 0.28756
INFO:root:IntervalWidthValidation: 1.20758
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 146.47256
INFO:root:EnergyScoreTest: 132.15329
INFO:root:CoverageTest: 0.28496
INFO:root:IntervalWidthTest: 1.20315
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.56834952, Validation loss: 171.19657109, Gradient norm: 5.82987587
INFO:root:[    2] Training loss: 170.33621324, Validation loss: 169.22435103, Gradient norm: 6.19041547
INFO:root:[    3] Training loss: 167.59003104, Validation loss: 166.02828348, Gradient norm: 10.81713374
INFO:root:[    4] Training loss: 164.29664207, Validation loss: 163.03990963, Gradient norm: 12.85366192
INFO:root:[    5] Training loss: 161.99536119, Validation loss: 161.23234505, Gradient norm: 17.21846031
INFO:root:[    6] Training loss: 160.46483646, Validation loss: 159.83690933, Gradient norm: 23.97376194
INFO:root:[    7] Training loss: 159.20475499, Validation loss: 158.69595390, Gradient norm: 29.81004043
INFO:root:[    8] Training loss: 158.21423272, Validation loss: 158.02205790, Gradient norm: 40.97061238
INFO:root:[    9] Training loss: 157.51655417, Validation loss: 157.35102002, Gradient norm: 47.88214466
INFO:root:[   10] Training loss: 156.95567646, Validation loss: 156.95193113, Gradient norm: 60.06540240
INFO:root:[   11] Training loss: 156.42196723, Validation loss: 156.47659617, Gradient norm: 67.39779920
INFO:root:[   12] Training loss: 156.09860743, Validation loss: 156.08592066, Gradient norm: 99.30676315
INFO:root:[   13] Training loss: 155.69097590, Validation loss: 155.78350672, Gradient norm: 88.27796761
INFO:root:[   14] Training loss: 155.34653480, Validation loss: 155.89397878, Gradient norm: 117.03094554
INFO:root:[   15] Training loss: 155.08468385, Validation loss: 155.19103741, Gradient norm: 151.88718792
INFO:root:[   16] Training loss: 154.78702971, Validation loss: 154.75827447, Gradient norm: 161.61802602
INFO:root:[   17] Training loss: 154.55825995, Validation loss: 154.65020594, Gradient norm: 181.11145240
INFO:root:[   18] Training loss: 154.34712935, Validation loss: 154.93192739, Gradient norm: 204.70938838
INFO:root:[   19] Training loss: 154.11984104, Validation loss: 154.12536095, Gradient norm: 227.63306150
INFO:root:[   20] Training loss: 153.92116486, Validation loss: 154.00651708, Gradient norm: 248.70788075
INFO:root:[   21] Training loss: 153.66475414, Validation loss: 153.84092291, Gradient norm: 246.71186013
INFO:root:[   22] Training loss: 153.62074064, Validation loss: 153.75232933, Gradient norm: 283.90796781
INFO:root:[   23] Training loss: 153.37316746, Validation loss: 153.55511317, Gradient norm: 300.95447377
INFO:root:[   24] Training loss: 153.21801569, Validation loss: 153.53329994, Gradient norm: 327.22931564
INFO:root:[   25] Training loss: 153.12596833, Validation loss: 153.55916359, Gradient norm: 353.43050440
INFO:root:[   26] Training loss: 152.98544420, Validation loss: 153.50458816, Gradient norm: 370.15118435
INFO:root:[   27] Training loss: 152.87437844, Validation loss: 153.09755365, Gradient norm: 347.32275777
INFO:root:[   28] Training loss: 152.64033535, Validation loss: 152.81113144, Gradient norm: 405.53674317
INFO:root:[   29] Training loss: 152.56020348, Validation loss: 153.01822478, Gradient norm: 430.70134666
INFO:root:[   30] Training loss: 152.48540207, Validation loss: 153.02193846, Gradient norm: 460.28959856
INFO:root:[   31] Training loss: 152.47302449, Validation loss: 152.58842731, Gradient norm: 469.80924047
INFO:root:[   32] Training loss: 152.24305279, Validation loss: 152.78760660, Gradient norm: 440.64701820
INFO:root:[   33] Training loss: 152.19773851, Validation loss: 152.63325027, Gradient norm: 504.57604916
INFO:root:[   34] Training loss: 152.14579948, Validation loss: 153.65586695, Gradient norm: 529.23091838
INFO:root:[   35] Training loss: 152.12142121, Validation loss: 152.34373106, Gradient norm: 533.36719024
INFO:root:[   36] Training loss: 152.04141235, Validation loss: 153.70711333, Gradient norm: 546.71753995
INFO:root:[   37] Training loss: 151.85242698, Validation loss: 152.21709205, Gradient norm: 553.66863853
INFO:root:[   38] Training loss: 151.80627090, Validation loss: 152.28886045, Gradient norm: 575.17177284
INFO:root:[   39] Training loss: 151.74684521, Validation loss: 151.99464417, Gradient norm: 622.34666810
INFO:root:[   40] Training loss: 151.77545139, Validation loss: 152.44477476, Gradient norm: 660.07595007
INFO:root:[   41] Training loss: 151.62692936, Validation loss: 152.49971377, Gradient norm: 653.59962662
INFO:root:[   42] Training loss: 151.76685475, Validation loss: 151.92533454, Gradient norm: 665.94446322
INFO:root:[   43] Training loss: 151.51654296, Validation loss: 151.74229273, Gradient norm: 696.12461171
INFO:root:[   44] Training loss: 151.59924114, Validation loss: 153.28120685, Gradient norm: 767.13870552
INFO:root:[   45] Training loss: 151.49832261, Validation loss: 152.10197449, Gradient norm: 719.92310819
INFO:root:[   46] Training loss: 151.73341734, Validation loss: 153.08112467, Gradient norm: 757.01537893
INFO:root:[   47] Training loss: 151.42331662, Validation loss: 152.30640175, Gradient norm: 718.30126032
INFO:root:[   48] Training loss: 151.43195741, Validation loss: 151.80143580, Gradient norm: 750.83200380
INFO:root:[   49] Training loss: 151.60713344, Validation loss: 152.87838693, Gradient norm: 803.23152294
INFO:root:[   50] Training loss: 151.47655723, Validation loss: 152.43900483, Gradient norm: 701.95130804
INFO:root:[   51] Training loss: 151.32161760, Validation loss: 151.91485490, Gradient norm: 808.65432617
INFO:root:[   52] Training loss: 151.35009509, Validation loss: 152.16071767, Gradient norm: 770.63565248
INFO:root:[   53] Training loss: 151.55072427, Validation loss: 151.46372986, Gradient norm: 757.87066834
INFO:root:[   54] Training loss: 151.16736731, Validation loss: 151.49502932, Gradient norm: 821.45455276
INFO:root:[   55] Training loss: 151.19433553, Validation loss: 151.43957151, Gradient norm: 854.69732502
INFO:root:[   56] Training loss: 151.79243618, Validation loss: 156.15760908, Gradient norm: 724.78091906
INFO:root:[   57] Training loss: 151.51548956, Validation loss: 152.11368429, Gradient norm: 808.90892449
INFO:root:[   58] Training loss: 151.00466649, Validation loss: 151.50194681, Gradient norm: 898.54770878
INFO:root:[   59] Training loss: 151.11074519, Validation loss: 151.31586377, Gradient norm: 910.11670615
INFO:root:[   60] Training loss: 151.28997074, Validation loss: 151.48666645, Gradient norm: 843.42320874
INFO:root:[   61] Training loss: 151.22643638, Validation loss: 151.42174030, Gradient norm: 947.97938877
INFO:root:[   62] Training loss: 151.01297119, Validation loss: 151.42532927, Gradient norm: 920.71175856
INFO:root:[   63] Training loss: 150.96331017, Validation loss: 153.25061561, Gradient norm: 1004.48578154
INFO:root:[   64] Training loss: 151.31656951, Validation loss: 151.51588019, Gradient norm: 927.64805866
INFO:root:[   65] Training loss: 151.31363334, Validation loss: 150.87540936, Gradient norm: 936.71585504
INFO:root:[   66] Training loss: 151.88649162, Validation loss: 151.05140160, Gradient norm: 804.98243935
INFO:root:[   67] Training loss: 151.56616522, Validation loss: 151.80435812, Gradient norm: 874.11672288
INFO:root:[   68] Training loss: 151.24105727, Validation loss: 152.57108228, Gradient norm: 840.14018627
INFO:root:[   69] Training loss: 150.95231183, Validation loss: 152.22127822, Gradient norm: 968.41899540
INFO:root:[   70] Training loss: 150.80054886, Validation loss: 151.40293253, Gradient norm: 945.89990472
INFO:root:[   71] Training loss: 151.06978249, Validation loss: 150.99176657, Gradient norm: 875.94918364
INFO:root:[   72] Training loss: 151.07714641, Validation loss: 157.43048990, Gradient norm: 1005.69425345
INFO:root:[   73] Training loss: 150.90198686, Validation loss: 151.70444357, Gradient norm: 1025.54554772
INFO:root:[   74] Training loss: 151.05895253, Validation loss: 151.28890412, Gradient norm: 1003.05069065
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 861.36s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 146.68275
INFO:root:EnergyScoreTrain: 127.80128
INFO:root:CoverageTrain: 0.32545
INFO:root:IntervalWidthTrain: 1.4775
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 147.68823
INFO:root:EnergyScoreValidation: 128.79578
INFO:root:CoverageValidation: 0.32195
INFO:root:IntervalWidthValidation: 1.4777
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 147.90255
INFO:root:EnergyScoreTest: 129.0464
INFO:root:CoverageTest: 0.31897
INFO:root:IntervalWidthTest: 1.4715
INFO:root:###21 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 172.12113399, Validation loss: 171.43324490, Gradient norm: 13.57610736
INFO:root:[    2] Training loss: 171.20861857, Validation loss: 170.78434806, Gradient norm: 1.86855465
INFO:root:[    3] Training loss: 169.93191258, Validation loss: 168.62180986, Gradient norm: 5.29672674
INFO:root:[    4] Training loss: 167.21983945, Validation loss: 165.86814196, Gradient norm: 9.54457660
INFO:root:[    5] Training loss: 164.62385762, Validation loss: 163.62237233, Gradient norm: 11.22651764
INFO:root:[    6] Training loss: 162.64842940, Validation loss: 162.29948583, Gradient norm: 12.44526317
INFO:root:[    7] Training loss: 161.38107907, Validation loss: 161.01132676, Gradient norm: 14.47800631
INFO:root:[    8] Training loss: 160.42650368, Validation loss: 160.22337447, Gradient norm: 15.67545142
INFO:root:[    9] Training loss: 159.67087832, Validation loss: 159.69379820, Gradient norm: 23.20272893
INFO:root:[   10] Training loss: 159.11487113, Validation loss: 159.14363519, Gradient norm: 24.78417038
INFO:root:[   11] Training loss: 158.64774734, Validation loss: 158.84742105, Gradient norm: 38.26856079
INFO:root:[   12] Training loss: 158.27264634, Validation loss: 158.62450172, Gradient norm: 55.66855746
INFO:root:[   13] Training loss: 157.93429673, Validation loss: 157.96326157, Gradient norm: 74.17742940
INFO:root:[   14] Training loss: 157.61089210, Validation loss: 157.87657850, Gradient norm: 86.01691622
INFO:root:[   15] Training loss: 157.38983640, Validation loss: 157.48916468, Gradient norm: 97.66782510
INFO:root:[   16] Training loss: 157.10776837, Validation loss: 157.34610827, Gradient norm: 121.93510915
INFO:root:[   17] Training loss: 156.90988267, Validation loss: 158.27854341, Gradient norm: 148.62072775
INFO:root:[   18] Training loss: 156.72421035, Validation loss: 156.97294722, Gradient norm: 167.54977867
INFO:root:[   19] Training loss: 156.51423078, Validation loss: 156.70279141, Gradient norm: 190.91526931
INFO:root:[   20] Training loss: 156.33000818, Validation loss: 156.37133000, Gradient norm: 216.00125976
INFO:root:[   21] Training loss: 156.22056539, Validation loss: 156.77934686, Gradient norm: 250.68494639
INFO:root:[   22] Training loss: 156.07399730, Validation loss: 156.40184126, Gradient norm: 258.82058322
INFO:root:[   23] Training loss: 155.90471831, Validation loss: 156.17775332, Gradient norm: 296.51268913
INFO:root:[   24] Training loss: 155.71244285, Validation loss: 156.31811576, Gradient norm: 300.22080687
INFO:root:[   25] Training loss: 155.71860389, Validation loss: 155.91938729, Gradient norm: 342.46627554
INFO:root:[   26] Training loss: 155.49846035, Validation loss: 156.47625575, Gradient norm: 327.93266913
INFO:root:[   27] Training loss: 155.49022479, Validation loss: 155.56076734, Gradient norm: 382.76534461
INFO:root:[   28] Training loss: 155.39824751, Validation loss: 155.43412833, Gradient norm: 389.92677800
INFO:root:[   29] Training loss: 155.26586198, Validation loss: 155.60223810, Gradient norm: 422.67037391
INFO:root:[   30] Training loss: 155.26850297, Validation loss: 155.78671843, Gradient norm: 418.64473458
INFO:root:[   31] Training loss: 155.07954029, Validation loss: 155.63845141, Gradient norm: 452.25868100
INFO:root:[   32] Training loss: 155.05236857, Validation loss: 155.15199753, Gradient norm: 481.56097252
INFO:root:[   33] Training loss: 155.05039829, Validation loss: 157.42532401, Gradient norm: 549.09707574
INFO:root:[   34] Training loss: 154.91244129, Validation loss: 154.87359409, Gradient norm: 470.59941461
INFO:root:[   35] Training loss: 155.11155431, Validation loss: 155.92444006, Gradient norm: 525.37607755
INFO:root:[   36] Training loss: 154.82043065, Validation loss: 155.47629310, Gradient norm: 568.75451744
INFO:root:[   37] Training loss: 154.77661767, Validation loss: 154.78820643, Gradient norm: 589.51784692
INFO:root:[   38] Training loss: 154.84085704, Validation loss: 155.19284584, Gradient norm: 611.13296941
INFO:root:[   39] Training loss: 154.76169539, Validation loss: 155.14384987, Gradient norm: 648.50161297
INFO:root:[   40] Training loss: 155.09867859, Validation loss: 155.20807306, Gradient norm: 593.76339558
INFO:root:[   41] Training loss: 155.18212459, Validation loss: 154.67877618, Gradient norm: 565.63001155
INFO:root:[   42] Training loss: 154.61253600, Validation loss: 154.53696468, Gradient norm: 693.77291563
INFO:root:[   43] Training loss: 154.79667893, Validation loss: 154.52420623, Gradient norm: 690.16664677
INFO:root:[   44] Training loss: 154.68284256, Validation loss: 154.36070672, Gradient norm: 630.65685489
INFO:root:[   45] Training loss: 155.24977476, Validation loss: 155.91718634, Gradient norm: 714.20600072
INFO:root:[   46] Training loss: 154.64581947, Validation loss: 154.86596154, Gradient norm: 663.94511473
INFO:root:[   47] Training loss: 154.27331300, Validation loss: 155.72159392, Gradient norm: 752.10171681
INFO:root:[   48] Training loss: 154.77912066, Validation loss: 154.37221606, Gradient norm: 772.67901512
INFO:root:[   49] Training loss: 154.80945634, Validation loss: 164.81216694, Gradient norm: 860.17489682
INFO:root:[   50] Training loss: 155.82235839, Validation loss: 155.13833250, Gradient norm: 623.19626884
INFO:root:[   51] Training loss: 154.62508966, Validation loss: 154.39789450, Gradient norm: 782.15293523
INFO:root:[   52] Training loss: 155.07849729, Validation loss: 154.71213295, Gradient norm: 812.75171536
INFO:root:[   53] Training loss: 154.90627174, Validation loss: 154.57110438, Gradient norm: 779.36513447
INFO:root:[   54] Training loss: 154.94397864, Validation loss: 157.20664978, Gradient norm: 876.43941877
INFO:root:[   55] Training loss: 154.98269707, Validation loss: 154.23874270, Gradient norm: 718.88979630
INFO:root:[   56] Training loss: 154.84662108, Validation loss: 155.29495081, Gradient norm: 840.87592159
INFO:root:[   57] Training loss: 154.54788505, Validation loss: 154.13344706, Gradient norm: 845.49392839
INFO:root:[   58] Training loss: 154.92636162, Validation loss: 154.67819687, Gradient norm: 927.33341997
INFO:root:[   59] Training loss: 154.36943108, Validation loss: 155.17031650, Gradient norm: 983.95759422
INFO:root:[   60] Training loss: 154.81638627, Validation loss: 154.50532742, Gradient norm: 906.68716331
INFO:root:[   61] Training loss: 154.66849795, Validation loss: 154.73538208, Gradient norm: 985.38198294
INFO:root:[   62] Training loss: 154.60442683, Validation loss: 154.26840947, Gradient norm: 1038.12140032
INFO:root:[   63] Training loss: 154.56209855, Validation loss: 154.62713307, Gradient norm: 1053.30038599
INFO:root:[   64] Training loss: 154.88350995, Validation loss: 154.98947196, Gradient norm: 1102.08745136
INFO:root:[   65] Training loss: 156.22609326, Validation loss: 155.67189605, Gradient norm: 851.25118618
INFO:root:[   66] Training loss: 155.38023147, Validation loss: 155.34521590, Gradient norm: 1060.17978925
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 865.203s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 150.1369
INFO:root:EnergyScoreTrain: 130.60606
INFO:root:CoverageTrain: 0.30078
INFO:root:IntervalWidthTrain: 1.47318
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 150.81154
INFO:root:EnergyScoreValidation: 131.29154
INFO:root:CoverageValidation: 0.29774
INFO:root:IntervalWidthValidation: 1.47115
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 151.01088
INFO:root:EnergyScoreTest: 131.51438
INFO:root:CoverageTest: 0.29595
INFO:root:IntervalWidthTest: 1.46721
INFO:root:###22 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.60335993, Validation loss: 171.51378763, Gradient norm: 3.54813099
INFO:root:[    2] Training loss: 171.19552221, Validation loss: 170.59040938, Gradient norm: 2.17149561
INFO:root:[    3] Training loss: 169.78153046, Validation loss: 168.61803831, Gradient norm: 5.53721222
INFO:root:[    4] Training loss: 167.84404694, Validation loss: 167.04727857, Gradient norm: 8.97233158
INFO:root:[    5] Training loss: 166.28780858, Validation loss: 165.66635448, Gradient norm: 12.54539121
INFO:root:[    6] Training loss: 164.66836399, Validation loss: 163.91354370, Gradient norm: 19.19186902
INFO:root:[    7] Training loss: 163.41029290, Validation loss: 163.20083776, Gradient norm: 22.45251994
INFO:root:[    8] Training loss: 162.56450093, Validation loss: 162.42573916, Gradient norm: 37.82244814
INFO:root:[    9] Training loss: 161.92065187, Validation loss: 161.82442764, Gradient norm: 50.51249446
INFO:root:[   10] Training loss: 161.40008410, Validation loss: 161.31679824, Gradient norm: 72.09191652
INFO:root:[   11] Training loss: 161.00653306, Validation loss: 161.07786244, Gradient norm: 99.87303569
INFO:root:[   12] Training loss: 160.61892201, Validation loss: 160.71229764, Gradient norm: 129.05334434
INFO:root:[   13] Training loss: 160.29661479, Validation loss: 160.32340214, Gradient norm: 161.09317893
INFO:root:[   14] Training loss: 160.05016482, Validation loss: 159.88304191, Gradient norm: 193.91710906
INFO:root:[   15] Training loss: 159.92425726, Validation loss: 159.71804494, Gradient norm: 214.31690577
INFO:root:[   16] Training loss: 159.52946445, Validation loss: 159.59361478, Gradient norm: 236.17536757
INFO:root:[   17] Training loss: 159.45102280, Validation loss: 159.44715145, Gradient norm: 308.17719578
INFO:root:[   18] Training loss: 159.27670828, Validation loss: 159.20129131, Gradient norm: 340.04631170
INFO:root:[   19] Training loss: 159.03830753, Validation loss: 159.30933775, Gradient norm: 365.84514186
INFO:root:[   20] Training loss: 159.00057659, Validation loss: 158.85679837, Gradient norm: 419.96157096
INFO:root:[   21] Training loss: 158.80628373, Validation loss: 159.17942968, Gradient norm: 445.09965442
INFO:root:[   22] Training loss: 158.81398064, Validation loss: 158.44746136, Gradient norm: 459.34432458
INFO:root:[   23] Training loss: 158.71132410, Validation loss: 158.27915271, Gradient norm: 504.46765449
INFO:root:[   24] Training loss: 158.55158321, Validation loss: 159.01903455, Gradient norm: 529.10827339
INFO:root:[   25] Training loss: 158.57199569, Validation loss: 158.02354273, Gradient norm: 540.06960609
INFO:root:[   26] Training loss: 158.50127215, Validation loss: 158.17384181, Gradient norm: 561.47593216
INFO:root:[   27] Training loss: 158.22079846, Validation loss: 158.67844785, Gradient norm: 530.65817748
INFO:root:[   28] Training loss: 158.14048321, Validation loss: 157.77443774, Gradient norm: 646.69464684
INFO:root:[   29] Training loss: 158.15862159, Validation loss: 158.72832568, Gradient norm: 650.25666826
INFO:root:[   30] Training loss: 158.11865113, Validation loss: 157.84017576, Gradient norm: 641.33675754
INFO:root:[   31] Training loss: 158.11287588, Validation loss: 158.16667649, Gradient norm: 600.01590096
INFO:root:[   32] Training loss: 158.17248319, Validation loss: 157.76701197, Gradient norm: 555.33806237
INFO:root:[   33] Training loss: 157.60826381, Validation loss: 157.49155558, Gradient norm: 632.31251239
INFO:root:[   34] Training loss: 158.05770644, Validation loss: 157.35687203, Gradient norm: 647.53670494
INFO:root:[   35] Training loss: 157.46929932, Validation loss: 157.86131865, Gradient norm: 620.61046985
INFO:root:[   36] Training loss: 157.43020886, Validation loss: 157.16329325, Gradient norm: 720.05737499
INFO:root:[   37] Training loss: 157.36782405, Validation loss: 157.08006865, Gradient norm: 720.14767852
INFO:root:[   38] Training loss: 157.28684133, Validation loss: 156.88880079, Gradient norm: 740.81612662
INFO:root:[   39] Training loss: 157.50534409, Validation loss: 157.76285369, Gradient norm: 681.22045595
INFO:root:[   40] Training loss: 157.14381449, Validation loss: 157.59766677, Gradient norm: 727.56339549
INFO:root:[   41] Training loss: 157.01762363, Validation loss: 156.77574526, Gradient norm: 739.82926308
INFO:root:[   42] Training loss: 157.10363932, Validation loss: 158.54723805, Gradient norm: 783.31890619
INFO:root:[   43] Training loss: 157.20244092, Validation loss: 159.55615024, Gradient norm: 712.46833610
INFO:root:[   44] Training loss: 157.05247282, Validation loss: 156.55183621, Gradient norm: 773.34560885
INFO:root:[   45] Training loss: 157.00150576, Validation loss: 158.66559469, Gradient norm: 807.39522511
INFO:root:[   46] Training loss: 156.89524031, Validation loss: 156.65309617, Gradient norm: 772.27574017
INFO:root:[   47] Training loss: 156.76066940, Validation loss: 157.19434646, Gradient norm: 810.10407515
INFO:root:[   48] Training loss: 156.70910807, Validation loss: 156.63751852, Gradient norm: 786.38729669
INFO:root:[   49] Training loss: 157.08065040, Validation loss: 158.27753001, Gradient norm: 812.09431715
INFO:root:[   50] Training loss: 156.97288702, Validation loss: 156.76236541, Gradient norm: 829.90103380
INFO:root:[   51] Training loss: 156.72134426, Validation loss: 156.48475226, Gradient norm: 804.24670106
INFO:root:[   52] Training loss: 157.07906510, Validation loss: 157.29961474, Gradient norm: 883.59074664
INFO:root:[   53] Training loss: 157.09927530, Validation loss: 156.67225279, Gradient norm: 817.89602137
INFO:root:[   54] Training loss: 156.84404167, Validation loss: 157.27359167, Gradient norm: 863.63075838
INFO:root:[   55] Training loss: 157.00284165, Validation loss: 157.14362572, Gradient norm: 923.82460048
INFO:root:[   56] Training loss: 158.18263717, Validation loss: 157.77926531, Gradient norm: 885.44349158
INFO:root:[   57] Training loss: 157.30078341, Validation loss: 157.29445727, Gradient norm: 888.79933573
INFO:root:[   58] Training loss: 156.89580462, Validation loss: 156.58023282, Gradient norm: 924.13637523
INFO:root:[   59] Training loss: 156.97734853, Validation loss: 156.98657016, Gradient norm: 973.74362885
INFO:root:[   60] Training loss: 157.04787155, Validation loss: 156.53279061, Gradient norm: 917.18325505
INFO:root:[   61] Training loss: 157.26917544, Validation loss: 156.85293369, Gradient norm: 1006.15028097
INFO:root:[   62] Training loss: 157.48909078, Validation loss: 158.98566253, Gradient norm: 1096.27580816
INFO:root:[   63] Training loss: 158.69634416, Validation loss: 158.34786303, Gradient norm: 1136.82779045
INFO:root:[   64] Training loss: 158.30635651, Validation loss: 157.74266052, Gradient norm: 1028.47721313
INFO:root:[   65] Training loss: 157.79656010, Validation loss: 157.22675929, Gradient norm: 1094.39978468
INFO:root:[   66] Training loss: 157.56006521, Validation loss: 156.78433280, Gradient norm: 1100.16767753
INFO:root:[   67] Training loss: 157.76719666, Validation loss: 157.36086457, Gradient norm: 1024.70311678
INFO:root:[   68] Training loss: 157.79758676, Validation loss: 157.51902455, Gradient norm: 1198.27623051
INFO:root:[   69] Training loss: 157.83292490, Validation loss: 157.55018826, Gradient norm: 1198.13006787
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 1057.941s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 152.74745
INFO:root:EnergyScoreTrain: 133.27265
INFO:root:CoverageTrain: 0.28064
INFO:root:IntervalWidthTrain: 1.41956
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 153.03286
INFO:root:EnergyScoreValidation: 133.55133
INFO:root:CoverageValidation: 0.27944
INFO:root:IntervalWidthValidation: 1.42079
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 153.19958
INFO:root:EnergyScoreTest: 133.71707
INFO:root:CoverageTest: 0.27849
INFO:root:IntervalWidthTest: 1.4183
INFO:root:###23 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 161480704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 171.70604483, Validation loss: 171.43541218, Gradient norm: 4.79279059
INFO:root:[    2] Training loss: 171.45705367, Validation loss: 171.42883511, Gradient norm: 0.36529858
INFO:root:[    3] Training loss: 171.00454631, Validation loss: 170.44103530, Gradient norm: 3.45214417
INFO:root:[    4] Training loss: 169.29182151, Validation loss: 167.81223113, Gradient norm: 5.14830157
INFO:root:[    5] Training loss: 166.48787331, Validation loss: 165.38095514, Gradient norm: 9.73351099
INFO:root:[    6] Training loss: 164.61821956, Validation loss: 164.06891027, Gradient norm: 18.09893825
INFO:root:[    7] Training loss: 163.58920828, Validation loss: 163.12066440, Gradient norm: 31.21010507
INFO:root:[    8] Training loss: 162.89088697, Validation loss: 162.54124819, Gradient norm: 53.66524319
INFO:root:[    9] Training loss: 162.36196400, Validation loss: 163.46911358, Gradient norm: 73.55447263
INFO:root:[   10] Training loss: 161.93741911, Validation loss: 162.13226897, Gradient norm: 97.96698932
INFO:root:[   11] Training loss: 161.52663077, Validation loss: 161.31561227, Gradient norm: 137.40187068
INFO:root:[   12] Training loss: 161.20407375, Validation loss: 160.88540649, Gradient norm: 162.64687751
INFO:root:[   13] Training loss: 160.94719149, Validation loss: 161.08382916, Gradient norm: 199.43326805
INFO:root:[   14] Training loss: 160.65209515, Validation loss: 160.36406208, Gradient norm: 220.28960617
INFO:root:[   15] Training loss: 160.68679121, Validation loss: 160.26823004, Gradient norm: 240.51104303
INFO:root:[   16] Training loss: 160.16551884, Validation loss: 159.91351950, Gradient norm: 235.14308260
INFO:root:[   17] Training loss: 160.07834159, Validation loss: 159.72924121, Gradient norm: 290.71451900
INFO:root:[   18] Training loss: 159.81610026, Validation loss: 159.62272328, Gradient norm: 324.19020672
INFO:root:[   19] Training loss: 159.88577176, Validation loss: 159.77482289, Gradient norm: 324.38269985
INFO:root:[   20] Training loss: 159.52512272, Validation loss: 159.18264402, Gradient norm: 353.07918070
INFO:root:[   21] Training loss: 159.40076993, Validation loss: 159.67922447, Gradient norm: 391.58561005
INFO:root:[   22] Training loss: 159.27391025, Validation loss: 158.90937700, Gradient norm: 415.64147795
INFO:root:[   23] Training loss: 159.15271658, Validation loss: 159.17903979, Gradient norm: 438.85788157
INFO:root:[   24] Training loss: 159.17782768, Validation loss: 159.08224066, Gradient norm: 470.46605220
INFO:root:[   25] Training loss: 158.95991300, Validation loss: 158.67598067, Gradient norm: 456.34246529
INFO:root:[   26] Training loss: 158.96381385, Validation loss: 158.75433718, Gradient norm: 454.57419258
INFO:root:[   27] Training loss: 158.88046305, Validation loss: 158.71775029, Gradient norm: 494.81839392
INFO:root:[   28] Training loss: 158.70658969, Validation loss: 158.77244357, Gradient norm: 517.16260362
INFO:root:[   29] Training loss: 158.68733269, Validation loss: 158.75067823, Gradient norm: 553.94843906
INFO:root:[   30] Training loss: 158.62345670, Validation loss: 158.37481269, Gradient norm: 523.41758660
INFO:root:[   31] Training loss: 158.55788955, Validation loss: 158.61088404, Gradient norm: 578.73779915
INFO:root:[   32] Training loss: 158.31858137, Validation loss: 157.98160211, Gradient norm: 557.11858366
INFO:root:[   33] Training loss: 158.50142528, Validation loss: 158.17926552, Gradient norm: 625.58639462
INFO:root:[   34] Training loss: 158.22850685, Validation loss: 158.03733405, Gradient norm: 637.37803712
INFO:root:[   35] Training loss: 158.69145230, Validation loss: 158.18848182, Gradient norm: 591.12434419
INFO:root:[   36] Training loss: 158.21087336, Validation loss: 158.46472484, Gradient norm: 594.47296005
INFO:root:[   37] Training loss: 158.31829942, Validation loss: 159.05665588, Gradient norm: 642.15351433
INFO:root:[   38] Training loss: 158.13337599, Validation loss: 159.37360882, Gradient norm: 611.47373189
INFO:root:[   39] Training loss: 158.34059278, Validation loss: 158.33858358, Gradient norm: 605.60278043
INFO:root:[   40] Training loss: 158.01568293, Validation loss: 161.22653198, Gradient norm: 647.22109165
INFO:root:[   41] Training loss: 158.21561196, Validation loss: 158.00817661, Gradient norm: 639.15612748
INFO:root:[   42] Training loss: 157.83140780, Validation loss: 157.72122929, Gradient norm: 705.67055983
INFO:root:[   43] Training loss: 158.18057859, Validation loss: 157.81567699, Gradient norm: 661.20498720
INFO:root:[   44] Training loss: 158.15276668, Validation loss: 158.45568164, Gradient norm: 671.90285211
INFO:root:[   45] Training loss: 157.96163859, Validation loss: 157.50189051, Gradient norm: 690.28938884
INFO:root:[   46] Training loss: 157.98048617, Validation loss: 158.23400142, Gradient norm: 784.51690615
INFO:root:[   47] Training loss: 157.86916318, Validation loss: 157.62650325, Gradient norm: 713.29480132
INFO:root:[   48] Training loss: 157.75385683, Validation loss: 157.55890892, Gradient norm: 748.41351033
INFO:root:[   49] Training loss: 157.76384661, Validation loss: 158.46637594, Gradient norm: 794.28095661
