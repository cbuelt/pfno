INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05552131, Validation loss: 0.02844344, Gradient norm: 0.68065297
INFO:root:[    2] Training loss: 0.02398607, Validation loss: 0.02131741, Gradient norm: 0.77829180
INFO:root:[    3] Training loss: 0.01899091, Validation loss: 0.01700915, Gradient norm: 0.70990652
INFO:root:[    4] Training loss: 0.01664783, Validation loss: 0.01745548, Gradient norm: 0.52502523
INFO:root:[    5] Training loss: 0.01566173, Validation loss: 0.01708988, Gradient norm: 0.52178071
INFO:root:[    6] Training loss: 0.01494325, Validation loss: 0.01350342, Gradient norm: 0.53034833
INFO:root:[    7] Training loss: 0.01434035, Validation loss: 0.01451685, Gradient norm: 0.56646004
INFO:root:[    8] Training loss: 0.01354427, Validation loss: 0.01418480, Gradient norm: 0.52821183
INFO:root:[    9] Training loss: 0.01263278, Validation loss: 0.01215627, Gradient norm: 0.48813566
INFO:root:[   10] Training loss: 0.01180097, Validation loss: 0.01124884, Gradient norm: 0.40700968
INFO:root:[   11] Training loss: 0.01162131, Validation loss: 0.01120299, Gradient norm: 0.41514507
INFO:root:[   12] Training loss: 0.01134457, Validation loss: 0.01080842, Gradient norm: 0.46308676
INFO:root:[   13] Training loss: 0.01101765, Validation loss: 0.01088533, Gradient norm: 0.42939077
INFO:root:[   14] Training loss: 0.01074961, Validation loss: 0.01182915, Gradient norm: 0.43149547
INFO:root:[   15] Training loss: 0.01045136, Validation loss: 0.01050672, Gradient norm: 0.43528204
INFO:root:[   16] Training loss: 0.01068956, Validation loss: 0.01032478, Gradient norm: 0.47209492
INFO:root:[   17] Training loss: 0.00997390, Validation loss: 0.00977224, Gradient norm: 0.39442518
INFO:root:[   18] Training loss: 0.00926575, Validation loss: 0.00978064, Gradient norm: 0.28555451
INFO:root:[   19] Training loss: 0.00950785, Validation loss: 0.00908651, Gradient norm: 0.37460287
INFO:root:[   20] Training loss: 0.00919545, Validation loss: 0.00891235, Gradient norm: 0.33816874
INFO:root:[   21] Training loss: 0.00988493, Validation loss: 0.00984940, Gradient norm: 0.49119584
INFO:root:[   22] Training loss: 0.00942372, Validation loss: 0.00873385, Gradient norm: 0.40683868
INFO:root:[   23] Training loss: 0.00878911, Validation loss: 0.00929821, Gradient norm: 0.31289820
INFO:root:[   24] Training loss: 0.00909977, Validation loss: 0.00979403, Gradient norm: 0.40963319
INFO:root:[   25] Training loss: 0.00884682, Validation loss: 0.00938738, Gradient norm: 0.35689720
INFO:root:[   26] Training loss: 0.00863682, Validation loss: 0.00897935, Gradient norm: 0.33161588
INFO:root:[   27] Training loss: 0.00915969, Validation loss: 0.01045813, Gradient norm: 0.46792453
INFO:root:[   28] Training loss: 0.00850904, Validation loss: 0.00927269, Gradient norm: 0.32848511
INFO:root:[   29] Training loss: 0.00849970, Validation loss: 0.00827682, Gradient norm: 0.35429232
INFO:root:[   30] Training loss: 0.00824348, Validation loss: 0.00890683, Gradient norm: 0.30728997
INFO:root:[   31] Training loss: 0.00833589, Validation loss: 0.00892979, Gradient norm: 0.35542604
INFO:root:[   32] Training loss: 0.00846188, Validation loss: 0.01280171, Gradient norm: 0.37553353
INFO:root:[   33] Training loss: 0.00809587, Validation loss: 0.00905285, Gradient norm: 0.29935165
INFO:root:[   34] Training loss: 0.00815583, Validation loss: 0.00908830, Gradient norm: 0.35549364
INFO:root:[   35] Training loss: 0.00826933, Validation loss: 0.00835757, Gradient norm: 0.38837957
INFO:root:[   36] Training loss: 0.00806789, Validation loss: 0.01075094, Gradient norm: 0.36585136
INFO:root:[   37] Training loss: 0.00781235, Validation loss: 0.01009230, Gradient norm: 0.30358867
INFO:root:[   38] Training loss: 0.00786212, Validation loss: 0.00914793, Gradient norm: 0.34118048
INFO:root:[   39] Training loss: 0.00750169, Validation loss: 0.00871400, Gradient norm: 0.29110016
INFO:root:[   40] Training loss: 0.00761931, Validation loss: 0.00824722, Gradient norm: 0.32100736
INFO:root:[   41] Training loss: 0.00759431, Validation loss: 0.00813897, Gradient norm: 0.32465746
INFO:root:[   42] Training loss: 0.00800288, Validation loss: 0.00965559, Gradient norm: 0.41515114
INFO:root:[   43] Training loss: 0.00736440, Validation loss: 0.00864580, Gradient norm: 0.29383228
INFO:root:[   44] Training loss: 0.00740183, Validation loss: 0.00798440, Gradient norm: 0.30276562
INFO:root:[   45] Training loss: 0.00756908, Validation loss: 0.00794868, Gradient norm: 0.34077984
INFO:root:[   46] Training loss: 0.00748215, Validation loss: 0.00806016, Gradient norm: 0.34335175
INFO:root:[   47] Training loss: 0.00736048, Validation loss: 0.00815268, Gradient norm: 0.31607682
INFO:root:[   48] Training loss: 0.00754325, Validation loss: 0.00812612, Gradient norm: 0.35203392
INFO:root:[   49] Training loss: 0.00725624, Validation loss: 0.00845984, Gradient norm: 0.32897089
INFO:root:[   50] Training loss: 0.00732881, Validation loss: 0.00796276, Gradient norm: 0.33356597
INFO:root:[   51] Training loss: 0.00702632, Validation loss: 0.00777757, Gradient norm: 0.25406888
INFO:root:[   52] Training loss: 0.00763081, Validation loss: 0.00780459, Gradient norm: 0.41254406
INFO:root:[   53] Training loss: 0.00728860, Validation loss: 0.00840408, Gradient norm: 0.34018926
INFO:root:[   54] Training loss: 0.00709621, Validation loss: 0.00836659, Gradient norm: 0.35048149
INFO:root:[   55] Training loss: 0.00697312, Validation loss: 0.00793631, Gradient norm: 0.31640783
INFO:root:[   56] Training loss: 0.00691516, Validation loss: 0.00794623, Gradient norm: 0.29873668
INFO:root:[   57] Training loss: 0.00680142, Validation loss: 0.00784004, Gradient norm: 0.29654333
INFO:root:[   58] Training loss: 0.00680978, Validation loss: 0.00811218, Gradient norm: 0.30171396
INFO:root:[   59] Training loss: 0.00694132, Validation loss: 0.00786326, Gradient norm: 0.32063775
INFO:root:[   60] Training loss: 0.00689573, Validation loss: 0.00916596, Gradient norm: 0.34604417
INFO:root:[   61] Training loss: 0.00668047, Validation loss: 0.00831428, Gradient norm: 0.31082609
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 825.471s.
INFO:root:Emptying the cuda cache took 0.027s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.27766
INFO:root:EnergyScoreTrain: 0.2067
INFO:root:CoverageTrain: 0.96775
INFO:root:IntervalWidthTrain: 0.03973
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.33046
INFO:root:EnergyScoreValidation: 0.24749
INFO:root:CoverageValidation: 0.94706
INFO:root:IntervalWidthValidation: 0.04001
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.76661
INFO:root:EnergyScoreTest: 0.59854
INFO:root:CoverageTest: 0.58118
INFO:root:IntervalWidthTest: 0.0428
INFO:root:###2 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05113220, Validation loss: 0.02914011, Gradient norm: 0.38352628
INFO:root:[    2] Training loss: 0.02468113, Validation loss: 0.02108854, Gradient norm: 0.41383974
INFO:root:[    3] Training loss: 0.01912489, Validation loss: 0.01864036, Gradient norm: 0.26920480
INFO:root:[    4] Training loss: 0.01714704, Validation loss: 0.01641020, Gradient norm: 0.30333635
INFO:root:[    5] Training loss: 0.01603480, Validation loss: 0.01491907, Gradient norm: 0.32510240
INFO:root:[    6] Training loss: 0.01486721, Validation loss: 0.01447304, Gradient norm: 0.31157873
INFO:root:[    7] Training loss: 0.01365765, Validation loss: 0.01314481, Gradient norm: 0.26596933
INFO:root:[    8] Training loss: 0.01304222, Validation loss: 0.01282681, Gradient norm: 0.28617687
INFO:root:[    9] Training loss: 0.01260482, Validation loss: 0.01222475, Gradient norm: 0.30452980
INFO:root:[   10] Training loss: 0.01193361, Validation loss: 0.01182898, Gradient norm: 0.26057998
INFO:root:[   11] Training loss: 0.01133117, Validation loss: 0.01120408, Gradient norm: 0.21317119
INFO:root:[   12] Training loss: 0.01113889, Validation loss: 0.01099961, Gradient norm: 0.25647358
INFO:root:[   13] Training loss: 0.01096457, Validation loss: 0.01282304, Gradient norm: 0.27548010
INFO:root:[   14] Training loss: 0.01078483, Validation loss: 0.01058970, Gradient norm: 0.28602129
INFO:root:[   15] Training loss: 0.01039879, Validation loss: 0.01026360, Gradient norm: 0.24461869
INFO:root:[   16] Training loss: 0.01019787, Validation loss: 0.01112357, Gradient norm: 0.26058233
INFO:root:[   17] Training loss: 0.00981971, Validation loss: 0.01034213, Gradient norm: 0.23044084
INFO:root:[   18] Training loss: 0.00986276, Validation loss: 0.00975614, Gradient norm: 0.24500691
INFO:root:[   19] Training loss: 0.00998928, Validation loss: 0.00958778, Gradient norm: 0.28106882
INFO:root:[   20] Training loss: 0.00952850, Validation loss: 0.00973497, Gradient norm: 0.24176836
INFO:root:[   21] Training loss: 0.00974287, Validation loss: 0.01037200, Gradient norm: 0.29764757
INFO:root:[   22] Training loss: 0.00927191, Validation loss: 0.00946604, Gradient norm: 0.23870507
INFO:root:[   23] Training loss: 0.00917195, Validation loss: 0.00972710, Gradient norm: 0.25292799
INFO:root:[   24] Training loss: 0.00913920, Validation loss: 0.00901563, Gradient norm: 0.23942310
INFO:root:[   25] Training loss: 0.00915582, Validation loss: 0.00964962, Gradient norm: 0.26889385
INFO:root:[   26] Training loss: 0.00916237, Validation loss: 0.00938783, Gradient norm: 0.26661818
INFO:root:[   27] Training loss: 0.00877996, Validation loss: 0.00924945, Gradient norm: 0.23075367
INFO:root:[   28] Training loss: 0.00855065, Validation loss: 0.00929638, Gradient norm: 0.21488700
INFO:root:[   29] Training loss: 0.00867455, Validation loss: 0.00928071, Gradient norm: 0.25651742
INFO:root:[   30] Training loss: 0.00849577, Validation loss: 0.00876313, Gradient norm: 0.23698636
INFO:root:[   31] Training loss: 0.00841781, Validation loss: 0.00891231, Gradient norm: 0.23325582
INFO:root:[   32] Training loss: 0.00853986, Validation loss: 0.01091721, Gradient norm: 0.25714959
INFO:root:[   33] Training loss: 0.00851053, Validation loss: 0.00885031, Gradient norm: 0.26889497
INFO:root:[   34] Training loss: 0.00828309, Validation loss: 0.00847347, Gradient norm: 0.23654217
INFO:root:[   35] Training loss: 0.00842738, Validation loss: 0.00886197, Gradient norm: 0.26046085
INFO:root:[   36] Training loss: 0.00808102, Validation loss: 0.00864087, Gradient norm: 0.21870293
INFO:root:[   37] Training loss: 0.00829700, Validation loss: 0.00855908, Gradient norm: 0.25748532
INFO:root:[   38] Training loss: 0.00815668, Validation loss: 0.00868663, Gradient norm: 0.25562135
INFO:root:[   39] Training loss: 0.00817576, Validation loss: 0.00823957, Gradient norm: 0.26576496
INFO:root:[   40] Training loss: 0.00796642, Validation loss: 0.00839611, Gradient norm: 0.25161376
INFO:root:[   41] Training loss: 0.00791754, Validation loss: 0.00833246, Gradient norm: 0.22959599
INFO:root:[   42] Training loss: 0.00764073, Validation loss: 0.00832686, Gradient norm: 0.21211681
INFO:root:[   43] Training loss: 0.00781032, Validation loss: 0.00839057, Gradient norm: 0.22782929
INFO:root:[   44] Training loss: 0.00794981, Validation loss: 0.00921896, Gradient norm: 0.26056323
INFO:root:[   45] Training loss: 0.00774369, Validation loss: 0.00830082, Gradient norm: 0.22975932
INFO:root:[   46] Training loss: 0.00752988, Validation loss: 0.00856017, Gradient norm: 0.20596671
INFO:root:[   47] Training loss: 0.00751130, Validation loss: 0.00816035, Gradient norm: 0.21052222
INFO:root:[   48] Training loss: 0.00742882, Validation loss: 0.00806048, Gradient norm: 0.21641715
INFO:root:[   49] Training loss: 0.00726353, Validation loss: 0.00802145, Gradient norm: 0.20269485
INFO:root:[   50] Training loss: 0.00760331, Validation loss: 0.00843903, Gradient norm: 0.25215338
INFO:root:[   51] Training loss: 0.00719887, Validation loss: 0.00810633, Gradient norm: 0.18486190
INFO:root:[   52] Training loss: 0.00767312, Validation loss: 0.00803654, Gradient norm: 0.26774170
INFO:root:[   53] Training loss: 0.00723660, Validation loss: 0.00831807, Gradient norm: 0.20051297
INFO:root:[   54] Training loss: 0.00732714, Validation loss: 0.00851466, Gradient norm: 0.21181362
INFO:root:[   55] Training loss: 0.00735767, Validation loss: 0.00804896, Gradient norm: 0.24038643
INFO:root:[   56] Training loss: 0.00702968, Validation loss: 0.00832184, Gradient norm: 0.19047629
INFO:root:[   57] Training loss: 0.00702194, Validation loss: 0.00809472, Gradient norm: 0.21201710
INFO:root:[   58] Training loss: 0.00736779, Validation loss: 0.00797221, Gradient norm: 0.26334093
INFO:root:[   59] Training loss: 0.00732862, Validation loss: 0.00822602, Gradient norm: 0.26011533
INFO:root:[   60] Training loss: 0.00678442, Validation loss: 0.00783206, Gradient norm: 0.16939124
INFO:root:[   61] Training loss: 0.00719551, Validation loss: 0.00829683, Gradient norm: 0.25877829
INFO:root:[   62] Training loss: 0.00693080, Validation loss: 0.00807776, Gradient norm: 0.20703152
INFO:root:[   63] Training loss: 0.00687297, Validation loss: 0.00966611, Gradient norm: 0.21593453
INFO:root:[   64] Training loss: 0.00674577, Validation loss: 0.00826181, Gradient norm: 0.21007932
INFO:root:[   65] Training loss: 0.00686074, Validation loss: 0.00773182, Gradient norm: 0.22045085
INFO:root:[   66] Training loss: 0.00669150, Validation loss: 0.00813427, Gradient norm: 0.20449368
INFO:root:[   67] Training loss: 0.00676111, Validation loss: 0.00855643, Gradient norm: 0.21430550
INFO:root:[   68] Training loss: 0.00675413, Validation loss: 0.00936741, Gradient norm: 0.20880300
INFO:root:[   69] Training loss: 0.00670168, Validation loss: 0.00797701, Gradient norm: 0.24169481
INFO:root:[   70] Training loss: 0.00655536, Validation loss: 0.00773321, Gradient norm: 0.20342120
INFO:root:[   71] Training loss: 0.00637042, Validation loss: 0.00819500, Gradient norm: 0.19379369
INFO:root:[   72] Training loss: 0.00659085, Validation loss: 0.00821807, Gradient norm: 0.22263737
INFO:root:[   73] Training loss: 0.00669859, Validation loss: 0.00852434, Gradient norm: 0.24822271
INFO:root:[   74] Training loss: 0.00648426, Validation loss: 0.00839449, Gradient norm: 0.22177014
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 975.039s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.26584
INFO:root:EnergyScoreTrain: 0.20355
INFO:root:CoverageTrain: 0.99037
INFO:root:IntervalWidthTrain: 0.0498
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.33851
INFO:root:EnergyScoreValidation: 0.25201
INFO:root:CoverageValidation: 0.97561
INFO:root:IntervalWidthValidation: 0.05019
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.51964
INFO:root:EnergyScoreTest: 0.36965
INFO:root:CoverageTest: 0.89009
INFO:root:IntervalWidthTest: 0.05387
INFO:root:###3 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05049862, Validation loss: 0.02728934, Gradient norm: 0.35328373
INFO:root:[    2] Training loss: 0.02373957, Validation loss: 0.02060126, Gradient norm: 0.25603652
INFO:root:[    3] Training loss: 0.01907600, Validation loss: 0.01805367, Gradient norm: 0.25972593
INFO:root:[    4] Training loss: 0.01715041, Validation loss: 0.01628581, Gradient norm: 0.27865880
INFO:root:[    5] Training loss: 0.01604548, Validation loss: 0.01519781, Gradient norm: 0.29868687
INFO:root:[    6] Training loss: 0.01461337, Validation loss: 0.01474737, Gradient norm: 0.21297260
INFO:root:[    7] Training loss: 0.01406516, Validation loss: 0.01439292, Gradient norm: 0.25530883
INFO:root:[    8] Training loss: 0.01343895, Validation loss: 0.01319207, Gradient norm: 0.27592704
INFO:root:[    9] Training loss: 0.01280931, Validation loss: 0.01247877, Gradient norm: 0.26382147
INFO:root:[   10] Training loss: 0.01234676, Validation loss: 0.01237514, Gradient norm: 0.23721439
INFO:root:[   11] Training loss: 0.01215570, Validation loss: 0.01330304, Gradient norm: 0.24732422
INFO:root:[   12] Training loss: 0.01195418, Validation loss: 0.01383530, Gradient norm: 0.28716083
INFO:root:[   13] Training loss: 0.01147315, Validation loss: 0.01144002, Gradient norm: 0.24888155
INFO:root:[   14] Training loss: 0.01105396, Validation loss: 0.01095921, Gradient norm: 0.23225624
INFO:root:[   15] Training loss: 0.01088585, Validation loss: 0.01077843, Gradient norm: 0.21942809
INFO:root:[   16] Training loss: 0.01078455, Validation loss: 0.01059860, Gradient norm: 0.23939313
INFO:root:[   17] Training loss: 0.01071708, Validation loss: 0.01069138, Gradient norm: 0.26419376
INFO:root:[   18] Training loss: 0.01021004, Validation loss: 0.01077154, Gradient norm: 0.20616241
INFO:root:[   19] Training loss: 0.01018991, Validation loss: 0.01021751, Gradient norm: 0.21267468
INFO:root:[   20] Training loss: 0.01038067, Validation loss: 0.01272059, Gradient norm: 0.27562735
INFO:root:[   21] Training loss: 0.01007721, Validation loss: 0.00994683, Gradient norm: 0.26135954
INFO:root:[   22] Training loss: 0.00992573, Validation loss: 0.00993284, Gradient norm: 0.25183852
INFO:root:[   23] Training loss: 0.00937562, Validation loss: 0.00948771, Gradient norm: 0.18742150
INFO:root:[   24] Training loss: 0.00969972, Validation loss: 0.01001401, Gradient norm: 0.24888252
INFO:root:[   25] Training loss: 0.00955640, Validation loss: 0.01180546, Gradient norm: 0.25505808
INFO:root:[   26] Training loss: 0.00972065, Validation loss: 0.00937003, Gradient norm: 0.28291328
INFO:root:[   27] Training loss: 0.00920114, Validation loss: 0.00919107, Gradient norm: 0.21710683
INFO:root:[   28] Training loss: 0.00912784, Validation loss: 0.00939724, Gradient norm: 0.21238300
INFO:root:[   29] Training loss: 0.00906146, Validation loss: 0.00978948, Gradient norm: 0.22610760
INFO:root:[   30] Training loss: 0.00881391, Validation loss: 0.00918900, Gradient norm: 0.19487623
INFO:root:[   31] Training loss: 0.00895469, Validation loss: 0.00911380, Gradient norm: 0.22905342
INFO:root:[   32] Training loss: 0.00872049, Validation loss: 0.00937219, Gradient norm: 0.21392713
INFO:root:[   33] Training loss: 0.00898700, Validation loss: 0.00908763, Gradient norm: 0.26590084
INFO:root:[   34] Training loss: 0.00886346, Validation loss: 0.00891443, Gradient norm: 0.24129156
INFO:root:[   35] Training loss: 0.00879377, Validation loss: 0.00897441, Gradient norm: 0.23297722
INFO:root:[   36] Training loss: 0.00858231, Validation loss: 0.00881440, Gradient norm: 0.22152117
INFO:root:[   37] Training loss: 0.00843479, Validation loss: 0.00893201, Gradient norm: 0.20889269
INFO:root:[   38] Training loss: 0.00835660, Validation loss: 0.00889454, Gradient norm: 0.21009805
INFO:root:[   39] Training loss: 0.00837728, Validation loss: 0.00850292, Gradient norm: 0.23446026
INFO:root:[   40] Training loss: 0.00823714, Validation loss: 0.00883894, Gradient norm: 0.21157506
INFO:root:[   41] Training loss: 0.00820741, Validation loss: 0.00943054, Gradient norm: 0.21723517
INFO:root:[   42] Training loss: 0.00824138, Validation loss: 0.00876524, Gradient norm: 0.23202536
INFO:root:[   43] Training loss: 0.00789596, Validation loss: 0.00861852, Gradient norm: 0.17785550
INFO:root:[   44] Training loss: 0.00826688, Validation loss: 0.00878670, Gradient norm: 0.25026684
INFO:root:[   45] Training loss: 0.00837103, Validation loss: 0.00903611, Gradient norm: 0.26910425
INFO:root:[   46] Training loss: 0.00815411, Validation loss: 0.00882347, Gradient norm: 0.22819605
INFO:root:[   47] Training loss: 0.00786959, Validation loss: 0.00856697, Gradient norm: 0.22757024
INFO:root:[   48] Training loss: 0.00778627, Validation loss: 0.00871826, Gradient norm: 0.18947210
INFO:root:[   49] Training loss: 0.00809200, Validation loss: 0.00856362, Gradient norm: 0.25357628
INFO:root:[   50] Training loss: 0.00803657, Validation loss: 0.00832767, Gradient norm: 0.24403091
INFO:root:[   51] Training loss: 0.00790332, Validation loss: 0.00851595, Gradient norm: 0.23437065
INFO:root:[   52] Training loss: 0.00773285, Validation loss: 0.00832071, Gradient norm: 0.22932725
INFO:root:[   53] Training loss: 0.00771462, Validation loss: 0.00865446, Gradient norm: 0.21793869
INFO:root:[   54] Training loss: 0.00790181, Validation loss: 0.00829277, Gradient norm: 0.27183005
INFO:root:[   55] Training loss: 0.00762629, Validation loss: 0.00899926, Gradient norm: 0.22114945
INFO:root:[   56] Training loss: 0.00760406, Validation loss: 0.00781768, Gradient norm: 0.20692335
INFO:root:[   57] Training loss: 0.00777718, Validation loss: 0.00920411, Gradient norm: 0.26002771
INFO:root:[   58] Training loss: 0.00759964, Validation loss: 0.00841771, Gradient norm: 0.22945843
INFO:root:[   59] Training loss: 0.00744652, Validation loss: 0.00839567, Gradient norm: 0.21376764
INFO:root:[   60] Training loss: 0.00777808, Validation loss: 0.00858730, Gradient norm: 0.25054943
INFO:root:[   61] Training loss: 0.00749350, Validation loss: 0.00821775, Gradient norm: 0.23635017
INFO:root:[   62] Training loss: 0.00744254, Validation loss: 0.00847450, Gradient norm: 0.22597636
INFO:root:[   63] Training loss: 0.00732028, Validation loss: 0.00800862, Gradient norm: 0.21664971
INFO:root:[   64] Training loss: 0.00727746, Validation loss: 0.00991824, Gradient norm: 0.22415662
INFO:root:[   65] Training loss: 0.00721939, Validation loss: 0.00829016, Gradient norm: 0.22936182
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 866.837s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.29197
INFO:root:EnergyScoreTrain: 0.231
INFO:root:CoverageTrain: 0.99279
INFO:root:IntervalWidthTrain: 0.0585
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.3351
INFO:root:EnergyScoreValidation: 0.25793
INFO:root:CoverageValidation: 0.98641
INFO:root:IntervalWidthValidation: 0.05907
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.46806
INFO:root:EnergyScoreTest: 0.33388
INFO:root:CoverageTest: 0.95483
INFO:root:IntervalWidthTest: 0.06111
INFO:root:###4 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04841789, Validation loss: 0.02798898, Gradient norm: 0.30703812
INFO:root:[    2] Training loss: 0.02528218, Validation loss: 0.02228668, Gradient norm: 0.25152969
INFO:root:[    3] Training loss: 0.02076214, Validation loss: 0.01976407, Gradient norm: 0.23926315
INFO:root:[    4] Training loss: 0.01841690, Validation loss: 0.01832939, Gradient norm: 0.21951269
INFO:root:[    5] Training loss: 0.01722868, Validation loss: 0.01643832, Gradient norm: 0.26899495
INFO:root:[    6] Training loss: 0.01599088, Validation loss: 0.01639198, Gradient norm: 0.20794996
INFO:root:[    7] Training loss: 0.01495978, Validation loss: 0.01449357, Gradient norm: 0.17863247
INFO:root:[    8] Training loss: 0.01427789, Validation loss: 0.01382994, Gradient norm: 0.20495192
INFO:root:[    9] Training loss: 0.01370387, Validation loss: 0.01331510, Gradient norm: 0.20890952
INFO:root:[   10] Training loss: 0.01325354, Validation loss: 0.01280883, Gradient norm: 0.22858416
INFO:root:[   11] Training loss: 0.01277245, Validation loss: 0.01246841, Gradient norm: 0.19954825
INFO:root:[   12] Training loss: 0.01258833, Validation loss: 0.01348475, Gradient norm: 0.24446424
INFO:root:[   13] Training loss: 0.01195279, Validation loss: 0.01266181, Gradient norm: 0.18388405
INFO:root:[   14] Training loss: 0.01176567, Validation loss: 0.01187877, Gradient norm: 0.20187018
INFO:root:[   15] Training loss: 0.01127479, Validation loss: 0.01200972, Gradient norm: 0.19374895
INFO:root:[   16] Training loss: 0.01119589, Validation loss: 0.01104485, Gradient norm: 0.22045176
INFO:root:[   17] Training loss: 0.01113297, Validation loss: 0.01203263, Gradient norm: 0.22585539
INFO:root:[   18] Training loss: 0.01100585, Validation loss: 0.01208615, Gradient norm: 0.24647490
INFO:root:[   19] Training loss: 0.01057751, Validation loss: 0.01114802, Gradient norm: 0.18972405
INFO:root:[   20] Training loss: 0.01039860, Validation loss: 0.01028741, Gradient norm: 0.20459111
INFO:root:[   21] Training loss: 0.01017798, Validation loss: 0.01032219, Gradient norm: 0.17756493
INFO:root:[   22] Training loss: 0.01037865, Validation loss: 0.01098208, Gradient norm: 0.25307187
INFO:root:[   23] Training loss: 0.00982133, Validation loss: 0.00998338, Gradient norm: 0.17763892
INFO:root:[   24] Training loss: 0.00983059, Validation loss: 0.01002274, Gradient norm: 0.19712450
INFO:root:[   25] Training loss: 0.00980825, Validation loss: 0.00978876, Gradient norm: 0.21226977
INFO:root:[   26] Training loss: 0.00985621, Validation loss: 0.01020371, Gradient norm: 0.23692342
INFO:root:[   27] Training loss: 0.00963162, Validation loss: 0.00976840, Gradient norm: 0.20263754
INFO:root:[   28] Training loss: 0.00950127, Validation loss: 0.01002708, Gradient norm: 0.21361601
INFO:root:[   29] Training loss: 0.00936004, Validation loss: 0.00987014, Gradient norm: 0.21294741
INFO:root:[   30] Training loss: 0.00948091, Validation loss: 0.01016160, Gradient norm: 0.23400700
INFO:root:[   31] Training loss: 0.00919966, Validation loss: 0.00926354, Gradient norm: 0.21451395
INFO:root:[   32] Training loss: 0.00905145, Validation loss: 0.01114180, Gradient norm: 0.24050509
INFO:root:[   33] Training loss: 0.00920906, Validation loss: 0.00930460, Gradient norm: 0.23999715
INFO:root:[   34] Training loss: 0.00893379, Validation loss: 0.00959881, Gradient norm: 0.22448721
INFO:root:[   35] Training loss: 0.00877850, Validation loss: 0.00923737, Gradient norm: 0.21533974
INFO:root:[   36] Training loss: 0.00876763, Validation loss: 0.00931058, Gradient norm: 0.20065303
INFO:root:[   37] Training loss: 0.00890775, Validation loss: 0.00903928, Gradient norm: 0.24515010
INFO:root:[   38] Training loss: 0.00875043, Validation loss: 0.01109752, Gradient norm: 0.24039453
INFO:root:[   39] Training loss: 0.00897694, Validation loss: 0.00963182, Gradient norm: 0.27355596
INFO:root:[   40] Training loss: 0.00868841, Validation loss: 0.00981984, Gradient norm: 0.25640776
INFO:root:[   41] Training loss: 0.00856786, Validation loss: 0.00969106, Gradient norm: 0.22503711
INFO:root:[   42] Training loss: 0.00833755, Validation loss: 0.00968178, Gradient norm: 0.21097349
INFO:root:[   43] Training loss: 0.00833727, Validation loss: 0.00895009, Gradient norm: 0.23695795
INFO:root:[   44] Training loss: 0.00850937, Validation loss: 0.00907341, Gradient norm: 0.24112035
INFO:root:[   45] Training loss: 0.00836323, Validation loss: 0.00893744, Gradient norm: 0.23558493
INFO:root:[   46] Training loss: 0.00831973, Validation loss: 0.00854946, Gradient norm: 0.26173466
INFO:root:[   47] Training loss: 0.00842141, Validation loss: 0.00856480, Gradient norm: 0.25749899
INFO:root:[   48] Training loss: 0.00820955, Validation loss: 0.00855466, Gradient norm: 0.24456536
INFO:root:[   49] Training loss: 0.00806164, Validation loss: 0.00860268, Gradient norm: 0.21902217
INFO:root:[   50] Training loss: 0.00793980, Validation loss: 0.00879058, Gradient norm: 0.22936847
INFO:root:[   51] Training loss: 0.00799704, Validation loss: 0.00854931, Gradient norm: 0.24335101
INFO:root:[   52] Training loss: 0.00802438, Validation loss: 0.00821999, Gradient norm: 0.22644748
INFO:root:[   53] Training loss: 0.00790116, Validation loss: 0.00874786, Gradient norm: 0.22745356
INFO:root:[   54] Training loss: 0.00820203, Validation loss: 0.00862916, Gradient norm: 0.30494305
INFO:root:[   55] Training loss: 0.00786822, Validation loss: 0.00848412, Gradient norm: 0.24278599
INFO:root:[   56] Training loss: 0.00783422, Validation loss: 0.00841113, Gradient norm: 0.24495684
INFO:root:[   57] Training loss: 0.00783490, Validation loss: 0.00926252, Gradient norm: 0.26811010
INFO:root:[   58] Training loss: 0.00779395, Validation loss: 0.00884480, Gradient norm: 0.25367939
INFO:root:[   59] Training loss: 0.00770163, Validation loss: 0.00821608, Gradient norm: 0.23001825
INFO:root:[   60] Training loss: 0.00781099, Validation loss: 0.01039279, Gradient norm: 0.27146376
INFO:root:[   61] Training loss: 0.00767559, Validation loss: 0.00865587, Gradient norm: 0.22998854
INFO:root:[   62] Training loss: 0.00760852, Validation loss: 0.00843233, Gradient norm: 0.23020500
INFO:root:[   63] Training loss: 0.00737689, Validation loss: 0.00869148, Gradient norm: 0.20680705
INFO:root:[   64] Training loss: 0.00783361, Validation loss: 0.00803037, Gradient norm: 0.28956686
INFO:root:[   65] Training loss: 0.00760100, Validation loss: 0.00949391, Gradient norm: 0.25014109
INFO:root:[   66] Training loss: 0.00759777, Validation loss: 0.00992675, Gradient norm: 0.25854085
INFO:root:[   67] Training loss: 0.00740959, Validation loss: 0.00829566, Gradient norm: 0.23193810
INFO:root:[   68] Training loss: 0.00726545, Validation loss: 0.00875455, Gradient norm: 0.20358202
INFO:root:[   69] Training loss: 0.00765289, Validation loss: 0.00832592, Gradient norm: 0.29606113
INFO:root:[   70] Training loss: 0.00753433, Validation loss: 0.00899378, Gradient norm: 0.28621836
INFO:root:[   71] Training loss: 0.00708662, Validation loss: 0.00786478, Gradient norm: 0.20784474
INFO:root:[   72] Training loss: 0.00740754, Validation loss: 0.00909600, Gradient norm: 0.24845739
INFO:root:[   73] Training loss: 0.00725985, Validation loss: 0.00826298, Gradient norm: 0.25436007
INFO:root:[   74] Training loss: 0.00714850, Validation loss: 0.00796410, Gradient norm: 0.22592309
INFO:root:[   75] Training loss: 0.00712024, Validation loss: 0.00815992, Gradient norm: 0.22276917
INFO:root:[   76] Training loss: 0.00722829, Validation loss: 0.00819025, Gradient norm: 0.24326353
INFO:root:[   77] Training loss: 0.00721488, Validation loss: 0.00820475, Gradient norm: 0.26681979
INFO:root:[   78] Training loss: 0.00714960, Validation loss: 0.00971577, Gradient norm: 0.24020773
INFO:root:[   79] Training loss: 0.00734178, Validation loss: 0.00838505, Gradient norm: 0.28936738
INFO:root:[   80] Training loss: 0.00690771, Validation loss: 0.00802022, Gradient norm: 0.22001710
INFO:root:EP 80: Early stopping
INFO:root:Training the model took 1056.297s.
INFO:root:Emptying the cuda cache took 0.028s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.27853
INFO:root:EnergyScoreTrain: 0.22311
INFO:root:CoverageTrain: 0.9952
INFO:root:IntervalWidthTrain: 0.05953
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.33254
INFO:root:EnergyScoreValidation: 0.25524
INFO:root:CoverageValidation: 0.98965
INFO:root:IntervalWidthValidation: 0.06013
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.50109
INFO:root:EnergyScoreTest: 0.35622
INFO:root:CoverageTest: 0.94789
INFO:root:IntervalWidthTest: 0.06321
INFO:root:###5 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05255139, Validation loss: 0.02876930, Gradient norm: 0.29332366
INFO:root:[    2] Training loss: 0.02565148, Validation loss: 0.02386262, Gradient norm: 0.20581373
INFO:root:[    3] Training loss: 0.02141614, Validation loss: 0.01965772, Gradient norm: 0.22604382
INFO:root:[    4] Training loss: 0.01910597, Validation loss: 0.01822463, Gradient norm: 0.22268719
INFO:root:[    5] Training loss: 0.01709534, Validation loss: 0.01668482, Gradient norm: 0.20503410
INFO:root:[    6] Training loss: 0.01635211, Validation loss: 0.01628607, Gradient norm: 0.23665023
INFO:root:[    7] Training loss: 0.01536278, Validation loss: 0.01613355, Gradient norm: 0.20555037
INFO:root:[    8] Training loss: 0.01457443, Validation loss: 0.01471746, Gradient norm: 0.21646658
INFO:root:[    9] Training loss: 0.01423362, Validation loss: 0.01364082, Gradient norm: 0.25336713
INFO:root:[   10] Training loss: 0.01342110, Validation loss: 0.01343527, Gradient norm: 0.20657839
INFO:root:[   11] Training loss: 0.01279592, Validation loss: 0.01292813, Gradient norm: 0.19344648
INFO:root:[   12] Training loss: 0.01259984, Validation loss: 0.01256339, Gradient norm: 0.21383380
INFO:root:[   13] Training loss: 0.01226669, Validation loss: 0.01344456, Gradient norm: 0.22639643
INFO:root:[   14] Training loss: 0.01232394, Validation loss: 0.01163777, Gradient norm: 0.27587284
INFO:root:[   15] Training loss: 0.01179751, Validation loss: 0.01292260, Gradient norm: 0.20363320
INFO:root:[   16] Training loss: 0.01162643, Validation loss: 0.01113288, Gradient norm: 0.20598960
INFO:root:[   17] Training loss: 0.01133209, Validation loss: 0.01173400, Gradient norm: 0.23171129
INFO:root:[   18] Training loss: 0.01109566, Validation loss: 0.01131809, Gradient norm: 0.20158021
INFO:root:[   19] Training loss: 0.01095950, Validation loss: 0.01143513, Gradient norm: 0.19857771
INFO:root:[   20] Training loss: 0.01092249, Validation loss: 0.01076792, Gradient norm: 0.23827896
INFO:root:[   21] Training loss: 0.01062874, Validation loss: 0.01160435, Gradient norm: 0.22297819
INFO:root:[   22] Training loss: 0.01052072, Validation loss: 0.01061766, Gradient norm: 0.22605645
INFO:root:[   23] Training loss: 0.01062230, Validation loss: 0.01062400, Gradient norm: 0.25707693
INFO:root:[   24] Training loss: 0.01037102, Validation loss: 0.01140578, Gradient norm: 0.23226435
INFO:root:[   25] Training loss: 0.01044302, Validation loss: 0.01112416, Gradient norm: 0.25658755
INFO:root:[   26] Training loss: 0.00976896, Validation loss: 0.01087950, Gradient norm: 0.19829969
INFO:root:[   27] Training loss: 0.01023698, Validation loss: 0.01007536, Gradient norm: 0.24367674
INFO:root:[   28] Training loss: 0.00984135, Validation loss: 0.01017519, Gradient norm: 0.20914746
INFO:root:[   29] Training loss: 0.00989782, Validation loss: 0.00973072, Gradient norm: 0.23918867
INFO:root:[   30] Training loss: 0.00964947, Validation loss: 0.00982207, Gradient norm: 0.20484211
INFO:root:[   31] Training loss: 0.01005795, Validation loss: 0.00983314, Gradient norm: 0.28864240
INFO:root:[   32] Training loss: 0.00952694, Validation loss: 0.00990049, Gradient norm: 0.22344980
INFO:root:[   33] Training loss: 0.00929141, Validation loss: 0.00976998, Gradient norm: 0.21698398
INFO:root:[   34] Training loss: 0.00938548, Validation loss: 0.00951243, Gradient norm: 0.21842721
INFO:root:[   35] Training loss: 0.00929937, Validation loss: 0.00979455, Gradient norm: 0.22924366
INFO:root:[   36] Training loss: 0.00919118, Validation loss: 0.00917073, Gradient norm: 0.21689991
INFO:root:[   37] Training loss: 0.00907361, Validation loss: 0.00916251, Gradient norm: 0.22941400
INFO:root:[   38] Training loss: 0.00916415, Validation loss: 0.00917408, Gradient norm: 0.23860138
INFO:root:[   39] Training loss: 0.00910736, Validation loss: 0.00966972, Gradient norm: 0.22932632
INFO:root:[   40] Training loss: 0.00902813, Validation loss: 0.00939320, Gradient norm: 0.25690789
INFO:root:[   41] Training loss: 0.00895392, Validation loss: 0.00959625, Gradient norm: 0.24235251
INFO:root:[   42] Training loss: 0.00891036, Validation loss: 0.00910024, Gradient norm: 0.25493257
INFO:root:[   43] Training loss: 0.00887306, Validation loss: 0.00929224, Gradient norm: 0.23273405
INFO:root:[   44] Training loss: 0.00901991, Validation loss: 0.00914707, Gradient norm: 0.27284116
INFO:root:[   45] Training loss: 0.00870353, Validation loss: 0.00970798, Gradient norm: 0.24672561
INFO:root:[   46] Training loss: 0.00863919, Validation loss: 0.00919254, Gradient norm: 0.22952115
INFO:root:[   47] Training loss: 0.00883051, Validation loss: 0.00891993, Gradient norm: 0.27183808
INFO:root:[   48] Training loss: 0.00876713, Validation loss: 0.00920046, Gradient norm: 0.26984244
INFO:root:[   49] Training loss: 0.00843859, Validation loss: 0.00951704, Gradient norm: 0.22694302
INFO:root:[   50] Training loss: 0.00855664, Validation loss: 0.01056011, Gradient norm: 0.25902362
INFO:root:[   51] Training loss: 0.00835994, Validation loss: 0.00890598, Gradient norm: 0.24607695
INFO:root:[   52] Training loss: 0.00820197, Validation loss: 0.00868040, Gradient norm: 0.20295166
INFO:root:[   53] Training loss: 0.00839531, Validation loss: 0.00876682, Gradient norm: 0.25613237
INFO:root:[   54] Training loss: 0.00819622, Validation loss: 0.00870919, Gradient norm: 0.22485745
INFO:root:[   55] Training loss: 0.00848647, Validation loss: 0.00860753, Gradient norm: 0.26872393
INFO:root:[   56] Training loss: 0.00804099, Validation loss: 0.00881144, Gradient norm: 0.21173960
INFO:root:[   57] Training loss: 0.00834178, Validation loss: 0.00894759, Gradient norm: 0.24340164
INFO:root:[   58] Training loss: 0.00824427, Validation loss: 0.00889138, Gradient norm: 0.24743785
INFO:root:[   59] Training loss: 0.00807285, Validation loss: 0.00844479, Gradient norm: 0.23699211
INFO:root:[   60] Training loss: 0.00816108, Validation loss: 0.01001602, Gradient norm: 0.26398615
INFO:root:[   61] Training loss: 0.00805002, Validation loss: 0.00904371, Gradient norm: 0.21687593
INFO:root:[   62] Training loss: 0.00823660, Validation loss: 0.00877583, Gradient norm: 0.27817957
INFO:root:[   63] Training loss: 0.00791119, Validation loss: 0.00832554, Gradient norm: 0.23739949
INFO:root:[   64] Training loss: 0.00794593, Validation loss: 0.01003783, Gradient norm: 0.24601729
INFO:root:[   65] Training loss: 0.00786291, Validation loss: 0.00876399, Gradient norm: 0.23930639
INFO:root:[   66] Training loss: 0.00793396, Validation loss: 0.00846868, Gradient norm: 0.23662795
INFO:root:[   67] Training loss: 0.00768445, Validation loss: 0.00838295, Gradient norm: 0.21207905
INFO:root:[   68] Training loss: 0.00770095, Validation loss: 0.00933812, Gradient norm: 0.22216017
INFO:root:[   69] Training loss: 0.00803445, Validation loss: 0.00900481, Gradient norm: 0.29478651
INFO:root:[   70] Training loss: 0.00773911, Validation loss: 0.00968094, Gradient norm: 0.26213482
INFO:root:[   71] Training loss: 0.00774061, Validation loss: 0.00854796, Gradient norm: 0.23691940
INFO:root:[   72] Training loss: 0.00778053, Validation loss: 0.00870333, Gradient norm: 0.26592644
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 969.395s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.29752
INFO:root:EnergyScoreTrain: 0.24463
INFO:root:CoverageTrain: 0.99532
INFO:root:IntervalWidthTrain: 0.06687
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.33912
INFO:root:EnergyScoreValidation: 0.26901
INFO:root:CoverageValidation: 0.99123
INFO:root:IntervalWidthValidation: 0.06759
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.46889
INFO:root:EnergyScoreTest: 0.33618
INFO:root:CoverageTest: 0.97569
INFO:root:IntervalWidthTest: 0.07364
INFO:root:###6 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04408700, Validation loss: 0.02944634, Gradient norm: 0.24604057
INFO:root:[    2] Training loss: 0.02777663, Validation loss: 0.02472734, Gradient norm: 0.23074916
INFO:root:[    3] Training loss: 0.02368386, Validation loss: 0.02239334, Gradient norm: 0.22076608
INFO:root:[    4] Training loss: 0.02089453, Validation loss: 0.02003259, Gradient norm: 0.21113369
INFO:root:[    5] Training loss: 0.01909840, Validation loss: 0.01899458, Gradient norm: 0.19308969
INFO:root:[    6] Training loss: 0.01821703, Validation loss: 0.01783593, Gradient norm: 0.23714163
INFO:root:[    7] Training loss: 0.01710581, Validation loss: 0.01634327, Gradient norm: 0.21108529
INFO:root:[    8] Training loss: 0.01614356, Validation loss: 0.01571894, Gradient norm: 0.17882286
INFO:root:[    9] Training loss: 0.01523358, Validation loss: 0.01488335, Gradient norm: 0.16009035
INFO:root:[   10] Training loss: 0.01463470, Validation loss: 0.01489137, Gradient norm: 0.17403028
INFO:root:[   11] Training loss: 0.01458132, Validation loss: 0.01377252, Gradient norm: 0.23420011
INFO:root:[   12] Training loss: 0.01404355, Validation loss: 0.01349994, Gradient norm: 0.23723218
INFO:root:[   13] Training loss: 0.01336795, Validation loss: 0.01473545, Gradient norm: 0.17461044
INFO:root:[   14] Training loss: 0.01326958, Validation loss: 0.01321005, Gradient norm: 0.19571025
INFO:root:[   15] Training loss: 0.01271221, Validation loss: 0.01309709, Gradient norm: 0.19302197
INFO:root:[   16] Training loss: 0.01252165, Validation loss: 0.01239021, Gradient norm: 0.20102958
INFO:root:[   17] Training loss: 0.01234781, Validation loss: 0.01231433, Gradient norm: 0.19569876
INFO:root:[   18] Training loss: 0.01217261, Validation loss: 0.01412676, Gradient norm: 0.20850537
INFO:root:[   19] Training loss: 0.01162839, Validation loss: 0.01161527, Gradient norm: 0.19264568
INFO:root:[   20] Training loss: 0.01157922, Validation loss: 0.01154455, Gradient norm: 0.18429754
INFO:root:[   21] Training loss: 0.01129659, Validation loss: 0.01157528, Gradient norm: 0.20494268
INFO:root:[   22] Training loss: 0.01130168, Validation loss: 0.01147374, Gradient norm: 0.21637578
INFO:root:[   23] Training loss: 0.01115246, Validation loss: 0.01140101, Gradient norm: 0.21390088
INFO:root:[   24] Training loss: 0.01103151, Validation loss: 0.01103606, Gradient norm: 0.21400703
INFO:root:[   25] Training loss: 0.01090100, Validation loss: 0.01118865, Gradient norm: 0.21992712
INFO:root:[   26] Training loss: 0.01070551, Validation loss: 0.01106719, Gradient norm: 0.21210374
INFO:root:[   27] Training loss: 0.01089179, Validation loss: 0.01084523, Gradient norm: 0.25811736
INFO:root:[   28] Training loss: 0.01051699, Validation loss: 0.01129730, Gradient norm: 0.22305376
INFO:root:[   29] Training loss: 0.01050148, Validation loss: 0.01041966, Gradient norm: 0.23126753
INFO:root:[   30] Training loss: 0.01065049, Validation loss: 0.01034475, Gradient norm: 0.27146501
INFO:root:[   31] Training loss: 0.01028069, Validation loss: 0.01059529, Gradient norm: 0.22392571
INFO:root:[   32] Training loss: 0.01016396, Validation loss: 0.01102215, Gradient norm: 0.22453799
INFO:root:[   33] Training loss: 0.01003293, Validation loss: 0.01120613, Gradient norm: 0.23137047
INFO:root:[   34] Training loss: 0.01022035, Validation loss: 0.01143174, Gradient norm: 0.28036198
INFO:root:[   35] Training loss: 0.00993874, Validation loss: 0.00998423, Gradient norm: 0.23742394
INFO:root:[   36] Training loss: 0.00997101, Validation loss: 0.01036070, Gradient norm: 0.23429209
INFO:root:[   37] Training loss: 0.00972985, Validation loss: 0.00976961, Gradient norm: 0.22314294
INFO:root:[   38] Training loss: 0.00973273, Validation loss: 0.00992834, Gradient norm: 0.24622573
INFO:root:[   39] Training loss: 0.00967107, Validation loss: 0.00976391, Gradient norm: 0.22997279
INFO:root:[   40] Training loss: 0.00997384, Validation loss: 0.01075057, Gradient norm: 0.29774337
INFO:root:[   41] Training loss: 0.00951644, Validation loss: 0.01043990, Gradient norm: 0.21885786
INFO:root:[   42] Training loss: 0.00950397, Validation loss: 0.00997507, Gradient norm: 0.24255957
INFO:root:[   43] Training loss: 0.00954414, Validation loss: 0.00943003, Gradient norm: 0.26716947
INFO:root:[   44] Training loss: 0.00955576, Validation loss: 0.00959514, Gradient norm: 0.25879206
INFO:root:[   45] Training loss: 0.00924657, Validation loss: 0.00938719, Gradient norm: 0.24551718
INFO:root:[   46] Training loss: 0.00921668, Validation loss: 0.00933153, Gradient norm: 0.22815328
INFO:root:[   47] Training loss: 0.00942054, Validation loss: 0.00960000, Gradient norm: 0.27782389
INFO:root:[   48] Training loss: 0.00937072, Validation loss: 0.00936936, Gradient norm: 0.26847933
INFO:root:[   49] Training loss: 0.00920221, Validation loss: 0.00973627, Gradient norm: 0.24152827
INFO:root:[   50] Training loss: 0.00911976, Validation loss: 0.00926825, Gradient norm: 0.24833218
INFO:root:[   51] Training loss: 0.00917254, Validation loss: 0.01019760, Gradient norm: 0.25917517
INFO:root:[   52] Training loss: 0.00908079, Validation loss: 0.01040464, Gradient norm: 0.25569111
INFO:root:[   53] Training loss: 0.00875939, Validation loss: 0.00897591, Gradient norm: 0.22634251
INFO:root:[   54] Training loss: 0.00896857, Validation loss: 0.00901192, Gradient norm: 0.26803352
INFO:root:[   55] Training loss: 0.00885444, Validation loss: 0.00989932, Gradient norm: 0.22835112
INFO:root:[   56] Training loss: 0.00905099, Validation loss: 0.00939806, Gradient norm: 0.27132503
INFO:root:[   57] Training loss: 0.00883020, Validation loss: 0.00937074, Gradient norm: 0.25618797
INFO:root:[   58] Training loss: 0.00861897, Validation loss: 0.00928368, Gradient norm: 0.21376258
INFO:root:[   59] Training loss: 0.00883130, Validation loss: 0.00936468, Gradient norm: 0.26891061
INFO:root:[   60] Training loss: 0.00847455, Validation loss: 0.00925596, Gradient norm: 0.22456355
INFO:root:[   61] Training loss: 0.00892507, Validation loss: 0.00901626, Gradient norm: 0.28778620
INFO:root:[   62] Training loss: 0.00862604, Validation loss: 0.00891149, Gradient norm: 0.22412709
INFO:root:[   63] Training loss: 0.00854071, Validation loss: 0.00889916, Gradient norm: 0.22562463
INFO:root:[   64] Training loss: 0.00897852, Validation loss: 0.00903372, Gradient norm: 0.29527197
INFO:root:[   65] Training loss: 0.00847535, Validation loss: 0.00955001, Gradient norm: 0.22711895
INFO:root:[   66] Training loss: 0.00831853, Validation loss: 0.00885061, Gradient norm: 0.21140464
INFO:root:[   67] Training loss: 0.00847204, Validation loss: 0.00884593, Gradient norm: 0.22446576
INFO:root:[   68] Training loss: 0.00827038, Validation loss: 0.00971154, Gradient norm: 0.21920867
INFO:root:[   69] Training loss: 0.00845822, Validation loss: 0.00870538, Gradient norm: 0.25053616
INFO:root:[   70] Training loss: 0.00834544, Validation loss: 0.00886573, Gradient norm: 0.23077678
INFO:root:[   71] Training loss: 0.00830893, Validation loss: 0.00887412, Gradient norm: 0.22250805
INFO:root:[   72] Training loss: 0.00835008, Validation loss: 0.00901929, Gradient norm: 0.25187256
INFO:root:[   73] Training loss: 0.00839896, Validation loss: 0.00880011, Gradient norm: 0.24258064
INFO:root:[   74] Training loss: 0.00830033, Validation loss: 0.00863639, Gradient norm: 0.23198682
INFO:root:[   75] Training loss: 0.00807847, Validation loss: 0.00890761, Gradient norm: 0.22309617
INFO:root:[   76] Training loss: 0.00843134, Validation loss: 0.00880780, Gradient norm: 0.27319128
INFO:root:[   77] Training loss: 0.00809596, Validation loss: 0.00892628, Gradient norm: 0.21792743
INFO:root:[   78] Training loss: 0.00791112, Validation loss: 0.00836652, Gradient norm: 0.19827025
INFO:root:[   79] Training loss: 0.00815006, Validation loss: 0.00870375, Gradient norm: 0.23117880
INFO:root:[   80] Training loss: 0.00802218, Validation loss: 0.00856053, Gradient norm: 0.22828068
INFO:root:[   81] Training loss: 0.00824465, Validation loss: 0.00851528, Gradient norm: 0.26022209
INFO:root:[   82] Training loss: 0.00798905, Validation loss: 0.00858528, Gradient norm: 0.23942575
INFO:root:[   83] Training loss: 0.00791138, Validation loss: 0.00881189, Gradient norm: 0.22565811
INFO:root:[   84] Training loss: 0.00792324, Validation loss: 0.00915876, Gradient norm: 0.21329757
INFO:root:[   85] Training loss: 0.00802357, Validation loss: 0.00870942, Gradient norm: 0.23299592
INFO:root:[   86] Training loss: 0.00774651, Validation loss: 0.01096677, Gradient norm: 0.19901401
INFO:root:[   87] Training loss: 0.00785405, Validation loss: 0.00856678, Gradient norm: 0.22328274
INFO:root:EP 87: Early stopping
INFO:root:Training the model took 1180.102s.
INFO:root:Emptying the cuda cache took 0.03s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.29637
INFO:root:EnergyScoreTrain: 0.24829
INFO:root:CoverageTrain: 0.99681
INFO:root:IntervalWidthTrain: 0.06883
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.33575
INFO:root:EnergyScoreValidation: 0.27049
INFO:root:CoverageValidation: 0.99377
INFO:root:IntervalWidthValidation: 0.06938
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.60605
INFO:root:EnergyScoreTest: 0.43618
INFO:root:CoverageTest: 0.9346
INFO:root:IntervalWidthTest: 0.07673
INFO:root:###7 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06198762, Validation loss: 0.03703591, Gradient norm: 0.26742106
INFO:root:[    2] Training loss: 0.03311170, Validation loss: 0.03023324, Gradient norm: 0.17215349
INFO:root:[    3] Training loss: 0.02872547, Validation loss: 0.02725963, Gradient norm: 0.18853653
INFO:root:[    4] Training loss: 0.02564858, Validation loss: 0.02485566, Gradient norm: 0.18306032
INFO:root:[    5] Training loss: 0.02349746, Validation loss: 0.02246023, Gradient norm: 0.19460616
INFO:root:[    6] Training loss: 0.02167158, Validation loss: 0.02125460, Gradient norm: 0.20123979
INFO:root:[    7] Training loss: 0.02032275, Validation loss: 0.01960729, Gradient norm: 0.17301200
INFO:root:[    8] Training loss: 0.01923177, Validation loss: 0.01866491, Gradient norm: 0.18758799
INFO:root:[    9] Training loss: 0.01825787, Validation loss: 0.01803688, Gradient norm: 0.18345798
INFO:root:[   10] Training loss: 0.01773803, Validation loss: 0.01756065, Gradient norm: 0.19813973
INFO:root:[   11] Training loss: 0.01691033, Validation loss: 0.01676962, Gradient norm: 0.17806642
INFO:root:[   12] Training loss: 0.01622900, Validation loss: 0.01625976, Gradient norm: 0.18191695
INFO:root:[   13] Training loss: 0.01590723, Validation loss: 0.01529859, Gradient norm: 0.21029442
INFO:root:[   14] Training loss: 0.01532782, Validation loss: 0.01526184, Gradient norm: 0.19151275
INFO:root:[   15] Training loss: 0.01494634, Validation loss: 0.01510266, Gradient norm: 0.20474410
INFO:root:[   16] Training loss: 0.01467473, Validation loss: 0.01510019, Gradient norm: 0.22346348
INFO:root:[   17] Training loss: 0.01441424, Validation loss: 0.01476824, Gradient norm: 0.22402193
INFO:root:[   18] Training loss: 0.01410043, Validation loss: 0.01426600, Gradient norm: 0.22101073
INFO:root:[   19] Training loss: 0.01401891, Validation loss: 0.01403297, Gradient norm: 0.26130823
INFO:root:[   20] Training loss: 0.01359187, Validation loss: 0.01471718, Gradient norm: 0.22711939
INFO:root:[   21] Training loss: 0.01327730, Validation loss: 0.01311517, Gradient norm: 0.20973997
INFO:root:[   22] Training loss: 0.01323106, Validation loss: 0.01292418, Gradient norm: 0.23831771
INFO:root:[   23] Training loss: 0.01295141, Validation loss: 0.01395503, Gradient norm: 0.24496880
INFO:root:[   24] Training loss: 0.01298937, Validation loss: 0.01495359, Gradient norm: 0.25722150
INFO:root:[   25] Training loss: 0.01275675, Validation loss: 0.01349279, Gradient norm: 0.24674883
INFO:root:[   26] Training loss: 0.01270071, Validation loss: 0.01457439, Gradient norm: 0.30107557
INFO:root:[   27] Training loss: 0.01222034, Validation loss: 0.01258379, Gradient norm: 0.23125221
INFO:root:[   28] Training loss: 0.01247456, Validation loss: 0.01281465, Gradient norm: 0.26490007
INFO:root:[   29] Training loss: 0.01226737, Validation loss: 0.01241687, Gradient norm: 0.27576027
INFO:root:[   30] Training loss: 0.01208821, Validation loss: 0.01182699, Gradient norm: 0.25178716
INFO:root:[   31] Training loss: 0.01179467, Validation loss: 0.01156396, Gradient norm: 0.23722980
INFO:root:[   32] Training loss: 0.01156327, Validation loss: 0.01180748, Gradient norm: 0.23827560
INFO:root:[   33] Training loss: 0.01164119, Validation loss: 0.01263850, Gradient norm: 0.25738887
INFO:root:[   34] Training loss: 0.01139459, Validation loss: 0.01147800, Gradient norm: 0.23576223
INFO:root:[   35] Training loss: 0.01145903, Validation loss: 0.01146361, Gradient norm: 0.26307627
INFO:root:[   36] Training loss: 0.01134371, Validation loss: 0.01143196, Gradient norm: 0.27144107
INFO:root:[   37] Training loss: 0.01128601, Validation loss: 0.01146188, Gradient norm: 0.26240558
INFO:root:[   38] Training loss: 0.01119295, Validation loss: 0.01153072, Gradient norm: 0.25617602
INFO:root:[   39] Training loss: 0.01132244, Validation loss: 0.01144899, Gradient norm: 0.27866621
INFO:root:[   40] Training loss: 0.01107748, Validation loss: 0.01097405, Gradient norm: 0.27513738
INFO:root:[   41] Training loss: 0.01096221, Validation loss: 0.01172154, Gradient norm: 0.25867841
INFO:root:[   42] Training loss: 0.01098053, Validation loss: 0.01082469, Gradient norm: 0.27034672
INFO:root:[   43] Training loss: 0.01108755, Validation loss: 0.01255373, Gradient norm: 0.30627439
INFO:root:[   44] Training loss: 0.01067425, Validation loss: 0.01109482, Gradient norm: 0.26121286
INFO:root:[   45] Training loss: 0.01057362, Validation loss: 0.01098652, Gradient norm: 0.25291570
INFO:root:[   46] Training loss: 0.01055423, Validation loss: 0.01075806, Gradient norm: 0.22663145
INFO:root:[   47] Training loss: 0.01054920, Validation loss: 0.01089023, Gradient norm: 0.27276343
INFO:root:[   48] Training loss: 0.01043909, Validation loss: 0.01055490, Gradient norm: 0.25730390
INFO:root:[   49] Training loss: 0.01042066, Validation loss: 0.01054678, Gradient norm: 0.28207809
INFO:root:[   50] Training loss: 0.01039734, Validation loss: 0.01050682, Gradient norm: 0.26745909
INFO:root:[   51] Training loss: 0.01040198, Validation loss: 0.01042389, Gradient norm: 0.28109600
INFO:root:[   52] Training loss: 0.01042203, Validation loss: 0.01031779, Gradient norm: 0.25896383
INFO:root:[   53] Training loss: 0.01029228, Validation loss: 0.01025094, Gradient norm: 0.25268908
INFO:root:[   54] Training loss: 0.01018568, Validation loss: 0.01080622, Gradient norm: 0.24667228
INFO:root:[   55] Training loss: 0.01007094, Validation loss: 0.01083217, Gradient norm: 0.24165240
INFO:root:[   56] Training loss: 0.01022473, Validation loss: 0.00978170, Gradient norm: 0.28469462
INFO:root:[   57] Training loss: 0.01002282, Validation loss: 0.01002956, Gradient norm: 0.25784316
INFO:root:[   58] Training loss: 0.00989306, Validation loss: 0.01106868, Gradient norm: 0.23740696
INFO:root:[   59] Training loss: 0.00998413, Validation loss: 0.01050305, Gradient norm: 0.28799731
INFO:root:[   60] Training loss: 0.00971036, Validation loss: 0.01004247, Gradient norm: 0.24036663
INFO:root:[   61] Training loss: 0.00977137, Validation loss: 0.00990965, Gradient norm: 0.27568912
INFO:root:[   62] Training loss: 0.00962186, Validation loss: 0.01023779, Gradient norm: 0.21837407
INFO:root:[   63] Training loss: 0.00990165, Validation loss: 0.00995505, Gradient norm: 0.28323677
INFO:root:[   64] Training loss: 0.00977251, Validation loss: 0.01009427, Gradient norm: 0.26115799
INFO:root:[   65] Training loss: 0.00967576, Validation loss: 0.01005471, Gradient norm: 0.24019173
INFO:root:[   66] Training loss: 0.00962298, Validation loss: 0.00974521, Gradient norm: 0.24702956
INFO:root:[   67] Training loss: 0.00944343, Validation loss: 0.01015025, Gradient norm: 0.22060875
INFO:root:[   68] Training loss: 0.00956377, Validation loss: 0.00960010, Gradient norm: 0.25397686
INFO:root:[   69] Training loss: 0.00958527, Validation loss: 0.00969141, Gradient norm: 0.24639968
INFO:root:[   70] Training loss: 0.00940422, Validation loss: 0.00934569, Gradient norm: 0.24234991
INFO:root:[   71] Training loss: 0.00931970, Validation loss: 0.00975205, Gradient norm: 0.22211131
INFO:root:[   72] Training loss: 0.00936267, Validation loss: 0.00969083, Gradient norm: 0.24089059
INFO:root:[   73] Training loss: 0.00918968, Validation loss: 0.00938461, Gradient norm: 0.21945685
INFO:root:[   74] Training loss: 0.00942195, Validation loss: 0.00931009, Gradient norm: 0.25944873
INFO:root:[   75] Training loss: 0.00930039, Validation loss: 0.01345269, Gradient norm: 0.24975270
INFO:root:[   76] Training loss: 0.00912754, Validation loss: 0.01093439, Gradient norm: 0.21870885
INFO:root:[   77] Training loss: 0.00917757, Validation loss: 0.00914934, Gradient norm: 0.21887599
INFO:root:[   78] Training loss: 0.00904129, Validation loss: 0.00940612, Gradient norm: 0.22097391
INFO:root:[   79] Training loss: 0.00908340, Validation loss: 0.00926597, Gradient norm: 0.22729578
INFO:root:[   80] Training loss: 0.00903407, Validation loss: 0.00943413, Gradient norm: 0.23764285
INFO:root:[   81] Training loss: 0.00931697, Validation loss: 0.01101994, Gradient norm: 0.27968608
INFO:root:[   82] Training loss: 0.00903039, Validation loss: 0.00922971, Gradient norm: 0.23333886
INFO:root:[   83] Training loss: 0.00894292, Validation loss: 0.00952172, Gradient norm: 0.21967137
INFO:root:[   84] Training loss: 0.00882830, Validation loss: 0.00933340, Gradient norm: 0.21910584
INFO:root:[   85] Training loss: 0.00895786, Validation loss: 0.00933636, Gradient norm: 0.22988890
INFO:root:[   86] Training loss: 0.00888981, Validation loss: 0.00922630, Gradient norm: 0.24682527
INFO:root:EP 86: Early stopping
INFO:root:Training the model took 1137.898s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.33869
INFO:root:EnergyScoreTrain: 0.28483
INFO:root:CoverageTrain: 0.99726
INFO:root:IntervalWidthTrain: 0.08013
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.35885
INFO:root:EnergyScoreValidation: 0.29623
INFO:root:CoverageValidation: 0.99592
INFO:root:IntervalWidthValidation: 0.08104
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.76933
INFO:root:EnergyScoreTest: 0.54245
INFO:root:CoverageTest: 0.99035
INFO:root:IntervalWidthTest: 0.12126
INFO:root:###8 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06529970, Validation loss: 0.03573212, Gradient norm: 0.36984482
INFO:root:[    2] Training loss: 0.03198717, Validation loss: 0.02623793, Gradient norm: 0.35036915
INFO:root:[    3] Training loss: 0.02625897, Validation loss: 0.02313286, Gradient norm: 0.41889401
INFO:root:[    4] Training loss: 0.02299703, Validation loss: 0.01942413, Gradient norm: 0.32710623
INFO:root:[    5] Training loss: 0.02139977, Validation loss: 0.01811646, Gradient norm: 0.38497807
INFO:root:[    6] Training loss: 0.02033373, Validation loss: 0.01690779, Gradient norm: 0.43895192
INFO:root:[    7] Training loss: 0.01900919, Validation loss: 0.01938201, Gradient norm: 0.42149550
INFO:root:[    8] Training loss: 0.01769490, Validation loss: 0.01463959, Gradient norm: 0.32064439
INFO:root:[    9] Training loss: 0.01756393, Validation loss: 0.01467501, Gradient norm: 0.45059073
INFO:root:[   10] Training loss: 0.01641711, Validation loss: 0.01357450, Gradient norm: 0.33470099
INFO:root:[   11] Training loss: 0.01583057, Validation loss: 0.01441541, Gradient norm: 0.32699018
INFO:root:[   12] Training loss: 0.01570391, Validation loss: 0.01320575, Gradient norm: 0.38300585
INFO:root:[   13] Training loss: 0.01557196, Validation loss: 0.01292276, Gradient norm: 0.42432799
INFO:root:[   14] Training loss: 0.01514396, Validation loss: 0.01326102, Gradient norm: 0.39781791
INFO:root:[   15] Training loss: 0.01466175, Validation loss: 0.01287943, Gradient norm: 0.36224386
INFO:root:[   16] Training loss: 0.01485886, Validation loss: 0.01410987, Gradient norm: 0.45111588
INFO:root:[   17] Training loss: 0.01444334, Validation loss: 0.01269248, Gradient norm: 0.40917229
INFO:root:[   18] Training loss: 0.01403638, Validation loss: 0.01204785, Gradient norm: 0.35761436
INFO:root:[   19] Training loss: 0.01432810, Validation loss: 0.01244744, Gradient norm: 0.46298547
INFO:root:[   20] Training loss: 0.01354179, Validation loss: 0.01232896, Gradient norm: 0.34114390
INFO:root:[   21] Training loss: 0.01305315, Validation loss: 0.01167941, Gradient norm: 0.28016729
INFO:root:[   22] Training loss: 0.01357340, Validation loss: 0.01173738, Gradient norm: 0.44244980
INFO:root:[   23] Training loss: 0.01341198, Validation loss: 0.01156076, Gradient norm: 0.39833616
INFO:root:[   24] Training loss: 0.01292928, Validation loss: 0.01137292, Gradient norm: 0.33076483
INFO:root:[   25] Training loss: 0.01295632, Validation loss: 0.01126907, Gradient norm: 0.37613278
INFO:root:[   26] Training loss: 0.01301241, Validation loss: 0.01105453, Gradient norm: 0.41629111
INFO:root:[   27] Training loss: 0.01280820, Validation loss: 0.01367098, Gradient norm: 0.40150499
INFO:root:[   28] Training loss: 0.01268022, Validation loss: 0.01231431, Gradient norm: 0.38411847
INFO:root:[   29] Training loss: 0.01255622, Validation loss: 0.01098790, Gradient norm: 0.39878843
INFO:root:[   30] Training loss: 0.01220659, Validation loss: 0.01161685, Gradient norm: 0.35107723
INFO:root:[   31] Training loss: 0.01233557, Validation loss: 0.01214130, Gradient norm: 0.39499776
INFO:root:[   32] Training loss: 0.01207150, Validation loss: 0.01178221, Gradient norm: 0.36448331
INFO:root:[   33] Training loss: 0.01213751, Validation loss: 0.01078349, Gradient norm: 0.37057443
INFO:root:[   34] Training loss: 0.01177753, Validation loss: 0.01155085, Gradient norm: 0.35129681
INFO:root:[   35] Training loss: 0.01188625, Validation loss: 0.01095598, Gradient norm: 0.35911874
INFO:root:[   36] Training loss: 0.01171012, Validation loss: 0.01234662, Gradient norm: 0.38720962
INFO:root:[   37] Training loss: 0.01160819, Validation loss: 0.01165839, Gradient norm: 0.36244390
INFO:root:[   38] Training loss: 0.01194639, Validation loss: 0.01057709, Gradient norm: 0.41662817
INFO:root:[   39] Training loss: 0.01114870, Validation loss: 0.01172017, Gradient norm: 0.30682762
INFO:root:[   40] Training loss: 0.01158165, Validation loss: 0.01116154, Gradient norm: 0.39843318
INFO:root:[   41] Training loss: 0.01131777, Validation loss: 0.01058250, Gradient norm: 0.36991195
INFO:root:[   42] Training loss: 0.01123858, Validation loss: 0.01108519, Gradient norm: 0.35836131
INFO:root:[   43] Training loss: 0.01153634, Validation loss: 0.01066864, Gradient norm: 0.44178385
INFO:root:[   44] Training loss: 0.01108030, Validation loss: 0.01044596, Gradient norm: 0.36655838
INFO:root:[   45] Training loss: 0.01105643, Validation loss: 0.01024586, Gradient norm: 0.34743382
INFO:root:[   46] Training loss: 0.01072124, Validation loss: 0.01112792, Gradient norm: 0.30421972
INFO:root:[   47] Training loss: 0.01113449, Validation loss: 0.01127928, Gradient norm: 0.39265599
INFO:root:[   48] Training loss: 0.01122430, Validation loss: 0.01394728, Gradient norm: 0.44332430
INFO:root:[   49] Training loss: 0.01081666, Validation loss: 0.01212744, Gradient norm: 0.37233403
INFO:root:[   50] Training loss: 0.01076220, Validation loss: 0.01014197, Gradient norm: 0.36766911
INFO:root:[   51] Training loss: 0.01054419, Validation loss: 0.01147269, Gradient norm: 0.36359477
INFO:root:[   52] Training loss: 0.01046109, Validation loss: 0.01089104, Gradient norm: 0.34060289
INFO:root:[   53] Training loss: 0.01068522, Validation loss: 0.01046200, Gradient norm: 0.40498604
INFO:root:[   54] Training loss: 0.01040223, Validation loss: 0.01152990, Gradient norm: 0.35740380
INFO:root:[   55] Training loss: 0.01022430, Validation loss: 0.01010649, Gradient norm: 0.31095447
INFO:root:[   56] Training loss: 0.01079935, Validation loss: 0.01243528, Gradient norm: 0.44841310
INFO:root:[   57] Training loss: 0.01042171, Validation loss: 0.01038531, Gradient norm: 0.37841348
INFO:root:[   58] Training loss: 0.01007997, Validation loss: 0.01055463, Gradient norm: 0.32359911
INFO:root:[   59] Training loss: 0.01009431, Validation loss: 0.01135486, Gradient norm: 0.33200762
INFO:root:[   60] Training loss: 0.01033508, Validation loss: 0.00989859, Gradient norm: 0.41469828
INFO:root:[   61] Training loss: 0.01032401, Validation loss: 0.01046553, Gradient norm: 0.40859200
INFO:root:[   62] Training loss: 0.01024691, Validation loss: 0.01111532, Gradient norm: 0.42374277
INFO:root:[   63] Training loss: 0.01028060, Validation loss: 0.01017004, Gradient norm: 0.42475697
INFO:root:[   64] Training loss: 0.01000275, Validation loss: 0.01034673, Gradient norm: 0.35368390
INFO:root:[   65] Training loss: 0.00998010, Validation loss: 0.01073969, Gradient norm: 0.37313693
INFO:root:[   66] Training loss: 0.00985685, Validation loss: 0.01007089, Gradient norm: 0.34160560
INFO:root:[   67] Training loss: 0.00971091, Validation loss: 0.01132865, Gradient norm: 0.34108881
INFO:root:[   68] Training loss: 0.00993522, Validation loss: 0.01030643, Gradient norm: 0.38956846
INFO:root:[   69] Training loss: 0.00975259, Validation loss: 0.01002478, Gradient norm: 0.36746291
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 365.657s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.44684
INFO:root:EnergyScoreTrain: 0.24221
INFO:root:CoverageTrain: 0.69287
INFO:root:IntervalWidthTrain: 0.03292
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.5023
INFO:root:EnergyScoreValidation: 0.2801
INFO:root:CoverageValidation: 0.66545
INFO:root:IntervalWidthValidation: 0.03335
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.60249
INFO:root:EnergyScoreTest: 0.34893
INFO:root:CoverageTest: 0.60272
INFO:root:IntervalWidthTest: 0.03457
INFO:root:###9 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07969719, Validation loss: 0.04063276, Gradient norm: 0.39799955
INFO:root:[    2] Training loss: 0.03768806, Validation loss: 0.02897703, Gradient norm: 0.34412859
INFO:root:[    3] Training loss: 0.03171182, Validation loss: 0.02596050, Gradient norm: 0.31815886
INFO:root:[    4] Training loss: 0.02879509, Validation loss: 0.02183995, Gradient norm: 0.33514772
INFO:root:[    5] Training loss: 0.02679460, Validation loss: 0.02034417, Gradient norm: 0.31825731
INFO:root:[    6] Training loss: 0.02573021, Validation loss: 0.02005689, Gradient norm: 0.39999425
INFO:root:[    7] Training loss: 0.02393892, Validation loss: 0.01772053, Gradient norm: 0.30681254
INFO:root:[    8] Training loss: 0.02317486, Validation loss: 0.01882551, Gradient norm: 0.40666045
INFO:root:[    9] Training loss: 0.02168996, Validation loss: 0.01728429, Gradient norm: 0.29051448
INFO:root:[   10] Training loss: 0.02122555, Validation loss: 0.01641828, Gradient norm: 0.35542078
INFO:root:[   11] Training loss: 0.02018578, Validation loss: 0.01471571, Gradient norm: 0.29612094
INFO:root:[   12] Training loss: 0.01967747, Validation loss: 0.01492172, Gradient norm: 0.31664406
INFO:root:[   13] Training loss: 0.01899258, Validation loss: 0.01406026, Gradient norm: 0.29247064
INFO:root:[   14] Training loss: 0.01858178, Validation loss: 0.01438593, Gradient norm: 0.31940398
INFO:root:[   15] Training loss: 0.01863533, Validation loss: 0.01881428, Gradient norm: 0.39104524
INFO:root:[   16] Training loss: 0.01781570, Validation loss: 0.01408753, Gradient norm: 0.30461049
INFO:root:[   17] Training loss: 0.01757955, Validation loss: 0.01411186, Gradient norm: 0.32915469
INFO:root:[   18] Training loss: 0.01729888, Validation loss: 0.01571357, Gradient norm: 0.36024738
INFO:root:[   19] Training loss: 0.01730684, Validation loss: 0.01440063, Gradient norm: 0.40954967
INFO:root:[   20] Training loss: 0.01700153, Validation loss: 0.01282844, Gradient norm: 0.36516411
INFO:root:[   21] Training loss: 0.01648585, Validation loss: 0.01338626, Gradient norm: 0.33551503
INFO:root:[   22] Training loss: 0.01627805, Validation loss: 0.01518266, Gradient norm: 0.32980876
INFO:root:[   23] Training loss: 0.01622972, Validation loss: 0.01463186, Gradient norm: 0.35806163
INFO:root:[   24] Training loss: 0.01603938, Validation loss: 0.01245589, Gradient norm: 0.37773920
INFO:root:[   25] Training loss: 0.01571106, Validation loss: 0.01411246, Gradient norm: 0.37173599
INFO:root:[   26] Training loss: 0.01586996, Validation loss: 0.01204310, Gradient norm: 0.41402620
INFO:root:[   27] Training loss: 0.01526281, Validation loss: 0.01196473, Gradient norm: 0.34014883
INFO:root:[   28] Training loss: 0.01505695, Validation loss: 0.01436746, Gradient norm: 0.29325464
INFO:root:[   29] Training loss: 0.01520261, Validation loss: 0.01239870, Gradient norm: 0.39051213
INFO:root:[   30] Training loss: 0.01510505, Validation loss: 0.01496128, Gradient norm: 0.39694381
INFO:root:[   31] Training loss: 0.01483127, Validation loss: 0.01228333, Gradient norm: 0.35598686
INFO:root:[   32] Training loss: 0.01478251, Validation loss: 0.01154038, Gradient norm: 0.38644058
INFO:root:[   33] Training loss: 0.01476730, Validation loss: 0.01436275, Gradient norm: 0.38710714
INFO:root:[   34] Training loss: 0.01451004, Validation loss: 0.01233956, Gradient norm: 0.35656093
INFO:root:[   35] Training loss: 0.01419565, Validation loss: 0.01196990, Gradient norm: 0.32887728
INFO:root:[   36] Training loss: 0.01422090, Validation loss: 0.01171544, Gradient norm: 0.34231334
INFO:root:[   37] Training loss: 0.01434596, Validation loss: 0.01336957, Gradient norm: 0.38178708
INFO:root:[   38] Training loss: 0.01406100, Validation loss: 0.01276208, Gradient norm: 0.37475912
INFO:root:[   39] Training loss: 0.01419349, Validation loss: 0.01303040, Gradient norm: 0.43429575
INFO:root:[   40] Training loss: 0.01390013, Validation loss: 0.01182651, Gradient norm: 0.36189626
INFO:root:[   41] Training loss: 0.01404009, Validation loss: 0.01167013, Gradient norm: 0.39232178
INFO:root:[   42] Training loss: 0.01367145, Validation loss: 0.01146349, Gradient norm: 0.34073734
INFO:root:[   43] Training loss: 0.01390357, Validation loss: 0.01180920, Gradient norm: 0.42378531
INFO:root:[   44] Training loss: 0.01357808, Validation loss: 0.01311453, Gradient norm: 0.35466514
INFO:root:[   45] Training loss: 0.01357752, Validation loss: 0.01116420, Gradient norm: 0.40903378
INFO:root:[   46] Training loss: 0.01339540, Validation loss: 0.01286556, Gradient norm: 0.36913084
INFO:root:[   47] Training loss: 0.01318143, Validation loss: 0.01200422, Gradient norm: 0.36106701
INFO:root:[   48] Training loss: 0.01320671, Validation loss: 0.01087992, Gradient norm: 0.39211860
INFO:root:[   49] Training loss: 0.01356224, Validation loss: 0.01100982, Gradient norm: 0.44656714
INFO:root:[   50] Training loss: 0.01315438, Validation loss: 0.01056896, Gradient norm: 0.39789535
INFO:root:[   51] Training loss: 0.01293509, Validation loss: 0.01330576, Gradient norm: 0.36844583
INFO:root:[   52] Training loss: 0.01283423, Validation loss: 0.01075195, Gradient norm: 0.33529231
INFO:root:[   53] Training loss: 0.01293560, Validation loss: 0.01070683, Gradient norm: 0.39513222
INFO:root:[   54] Training loss: 0.01293409, Validation loss: 0.01368158, Gradient norm: 0.40512935
INFO:root:[   55] Training loss: 0.01295998, Validation loss: 0.01102887, Gradient norm: 0.42769272
INFO:root:[   56] Training loss: 0.01241286, Validation loss: 0.01100509, Gradient norm: 0.31518040
INFO:root:[   57] Training loss: 0.01286322, Validation loss: 0.01158060, Gradient norm: 0.40945337
INFO:root:[   58] Training loss: 0.01254050, Validation loss: 0.01078988, Gradient norm: 0.39809332
INFO:root:[   59] Training loss: 0.01256029, Validation loss: 0.01259972, Gradient norm: 0.41886243
INFO:root:[   60] Training loss: 0.01222574, Validation loss: 0.01059637, Gradient norm: 0.32667920
INFO:root:[   61] Training loss: 0.01256034, Validation loss: 0.01068059, Gradient norm: 0.41813510
INFO:root:[   62] Training loss: 0.01217024, Validation loss: 0.01069420, Gradient norm: 0.35238772
INFO:root:[   63] Training loss: 0.01255466, Validation loss: 0.01045626, Gradient norm: 0.46500026
INFO:root:[   64] Training loss: 0.01236021, Validation loss: 0.01075718, Gradient norm: 0.41316465
INFO:root:[   65] Training loss: 0.01224345, Validation loss: 0.01081695, Gradient norm: 0.40789312
INFO:root:[   66] Training loss: 0.01192972, Validation loss: 0.01042846, Gradient norm: 0.30980832
INFO:root:[   67] Training loss: 0.01202660, Validation loss: 0.01109398, Gradient norm: 0.36674709
INFO:root:[   68] Training loss: 0.01221272, Validation loss: 0.01060368, Gradient norm: 0.41843761
INFO:root:[   69] Training loss: 0.01199743, Validation loss: 0.01041814, Gradient norm: 0.40229410
INFO:root:[   70] Training loss: 0.01161307, Validation loss: 0.01081185, Gradient norm: 0.31437703
INFO:root:[   71] Training loss: 0.01173103, Validation loss: 0.01113461, Gradient norm: 0.38331014
INFO:root:[   72] Training loss: 0.01178483, Validation loss: 0.01126954, Gradient norm: 0.39702737
INFO:root:[   73] Training loss: 0.01175562, Validation loss: 0.01036473, Gradient norm: 0.36514594
INFO:root:[   74] Training loss: 0.01160359, Validation loss: 0.01077389, Gradient norm: 0.37000022
INFO:root:[   75] Training loss: 0.01170723, Validation loss: 0.01084771, Gradient norm: 0.37170249
INFO:root:[   76] Training loss: 0.01133111, Validation loss: 0.01070682, Gradient norm: 0.31388195
INFO:root:[   77] Training loss: 0.01155917, Validation loss: 0.01063010, Gradient norm: 0.39586620
INFO:root:[   78] Training loss: 0.01137516, Validation loss: 0.01215289, Gradient norm: 0.37680621
INFO:root:[   79] Training loss: 0.01147462, Validation loss: 0.01073991, Gradient norm: 0.39191267
INFO:root:[   80] Training loss: 0.01136891, Validation loss: 0.01082813, Gradient norm: 0.37472660
INFO:root:[   81] Training loss: 0.01138622, Validation loss: 0.01082998, Gradient norm: 0.39289819
INFO:root:[   82] Training loss: 0.01113339, Validation loss: 0.01035721, Gradient norm: 0.35253544
INFO:root:[   83] Training loss: 0.01139978, Validation loss: 0.01030551, Gradient norm: 0.41430661
INFO:root:[   84] Training loss: 0.01109274, Validation loss: 0.01399025, Gradient norm: 0.38830971
INFO:root:[   85] Training loss: 0.01102460, Validation loss: 0.01058200, Gradient norm: 0.34497973
INFO:root:[   86] Training loss: 0.01115601, Validation loss: 0.01380610, Gradient norm: 0.40674703
INFO:root:[   87] Training loss: 0.01093757, Validation loss: 0.01123888, Gradient norm: 0.33312011
INFO:root:[   88] Training loss: 0.01100170, Validation loss: 0.01039804, Gradient norm: 0.36185206
INFO:root:[   89] Training loss: 0.01097212, Validation loss: 0.00993498, Gradient norm: 0.36939573
INFO:root:[   90] Training loss: 0.01106035, Validation loss: 0.01072731, Gradient norm: 0.42791501
INFO:root:[   91] Training loss: 0.01092611, Validation loss: 0.01027363, Gradient norm: 0.38523855
INFO:root:[   92] Training loss: 0.01112218, Validation loss: 0.01076455, Gradient norm: 0.42877212
INFO:root:[   93] Training loss: 0.01060768, Validation loss: 0.00980869, Gradient norm: 0.32599539
INFO:root:[   94] Training loss: 0.01078793, Validation loss: 0.01022660, Gradient norm: 0.40424035
INFO:root:[   95] Training loss: 0.01075567, Validation loss: 0.01060893, Gradient norm: 0.37152716
INFO:root:[   96] Training loss: 0.01086961, Validation loss: 0.00995923, Gradient norm: 0.42800339
INFO:root:[   97] Training loss: 0.01044006, Validation loss: 0.01140923, Gradient norm: 0.32589439
INFO:root:[   98] Training loss: 0.01041910, Validation loss: 0.00981127, Gradient norm: 0.34137946
INFO:root:[   99] Training loss: 0.01045104, Validation loss: 0.00978818, Gradient norm: 0.35710823
INFO:root:[  100] Training loss: 0.01052219, Validation loss: 0.01165126, Gradient norm: 0.37471125
INFO:root:[  101] Training loss: 0.01061368, Validation loss: 0.00983767, Gradient norm: 0.39285880
INFO:root:[  102] Training loss: 0.01037267, Validation loss: 0.01054749, Gradient norm: 0.36953418
INFO:root:[  103] Training loss: 0.01053829, Validation loss: 0.01027443, Gradient norm: 0.39458645
INFO:root:[  104] Training loss: 0.01042171, Validation loss: 0.01037497, Gradient norm: 0.36153369
INFO:root:[  105] Training loss: 0.01012975, Validation loss: 0.00990953, Gradient norm: 0.32621026
INFO:root:[  106] Training loss: 0.01026439, Validation loss: 0.01142248, Gradient norm: 0.36738637
INFO:root:[  107] Training loss: 0.01041899, Validation loss: 0.01071478, Gradient norm: 0.37739049
INFO:root:[  108] Training loss: 0.01031984, Validation loss: 0.01034746, Gradient norm: 0.39487638
INFO:root:EP 108: Early stopping
INFO:root:Training the model took 566.635s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.46109
INFO:root:EnergyScoreTrain: 0.23637
INFO:root:CoverageTrain: 0.70046
INFO:root:IntervalWidthTrain: 0.03635
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.54331
INFO:root:EnergyScoreValidation: 0.31798
INFO:root:CoverageValidation: 0.62377
INFO:root:IntervalWidthValidation: 0.03314
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.59815
INFO:root:EnergyScoreTest: 0.34555
INFO:root:CoverageTest: 0.58117
INFO:root:IntervalWidthTest: 0.03339
INFO:root:###10 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 299892736
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07084201, Validation loss: 0.03942602, Gradient norm: 0.33392406
INFO:root:[    2] Training loss: 0.04178213, Validation loss: 0.03066974, Gradient norm: 0.24907593
INFO:root:[    3] Training loss: 0.03591279, Validation loss: 0.02546335, Gradient norm: 0.30257896
INFO:root:[    4] Training loss: 0.03213001, Validation loss: 0.02264405, Gradient norm: 0.27812302
INFO:root:[    5] Training loss: 0.02942217, Validation loss: 0.02155611, Gradient norm: 0.29013879
INFO:root:[    6] Training loss: 0.02756901, Validation loss: 0.02009394, Gradient norm: 0.28292234
INFO:root:[    7] Training loss: 0.02605868, Validation loss: 0.01863849, Gradient norm: 0.27679562
INFO:root:[    8] Training loss: 0.02484643, Validation loss: 0.01791360, Gradient norm: 0.29368558
INFO:root:[    9] Training loss: 0.02375236, Validation loss: 0.01833277, Gradient norm: 0.29167988
INFO:root:[   10] Training loss: 0.02278263, Validation loss: 0.01622235, Gradient norm: 0.27480778
INFO:root:[   11] Training loss: 0.02208192, Validation loss: 0.01597174, Gradient norm: 0.32010608
INFO:root:[   12] Training loss: 0.02127848, Validation loss: 0.01572207, Gradient norm: 0.27241229
INFO:root:[   13] Training loss: 0.02082302, Validation loss: 0.01566071, Gradient norm: 0.30696594
INFO:root:[   14] Training loss: 0.02037730, Validation loss: 0.01668464, Gradient norm: 0.30533485
INFO:root:[   15] Training loss: 0.02003968, Validation loss: 0.01451865, Gradient norm: 0.34383903
INFO:root:[   16] Training loss: 0.01982384, Validation loss: 0.01514702, Gradient norm: 0.31783809
INFO:root:[   17] Training loss: 0.01883909, Validation loss: 0.01554167, Gradient norm: 0.28229246
INFO:root:[   18] Training loss: 0.01906429, Validation loss: 0.01492516, Gradient norm: 0.34647699
INFO:root:[   19] Training loss: 0.01837050, Validation loss: 0.01342840, Gradient norm: 0.30206929
INFO:root:[   20] Training loss: 0.01829861, Validation loss: 0.01801960, Gradient norm: 0.34538585
INFO:root:[   21] Training loss: 0.01781177, Validation loss: 0.01318343, Gradient norm: 0.29361851
INFO:root:[   22] Training loss: 0.01781516, Validation loss: 0.01364522, Gradient norm: 0.34564583
INFO:root:[   23] Training loss: 0.01772608, Validation loss: 0.01559264, Gradient norm: 0.37143748
INFO:root:[   24] Training loss: 0.01721320, Validation loss: 0.01403431, Gradient norm: 0.32885562
INFO:root:[   25] Training loss: 0.01687175, Validation loss: 0.01257646, Gradient norm: 0.31380345
INFO:root:[   26] Training loss: 0.01703772, Validation loss: 0.01275140, Gradient norm: 0.37000327
INFO:root:[   27] Training loss: 0.01668488, Validation loss: 0.01346633, Gradient norm: 0.36152458
INFO:root:[   28] Training loss: 0.01656210, Validation loss: 0.01288830, Gradient norm: 0.33063776
INFO:root:[   29] Training loss: 0.01665055, Validation loss: 0.01310269, Gradient norm: 0.40087899
INFO:root:[   30] Training loss: 0.01638099, Validation loss: 0.01298465, Gradient norm: 0.34875459
INFO:root:[   31] Training loss: 0.01596851, Validation loss: 0.01332332, Gradient norm: 0.32677523
INFO:root:[   32] Training loss: 0.01605302, Validation loss: 0.01438848, Gradient norm: 0.35389742
INFO:root:[   33] Training loss: 0.01591512, Validation loss: 0.01250471, Gradient norm: 0.36526185
INFO:root:[   34] Training loss: 0.01582859, Validation loss: 0.01228086, Gradient norm: 0.37391863
INFO:root:[   35] Training loss: 0.01605012, Validation loss: 0.01293004, Gradient norm: 0.45599149
INFO:root:[   36] Training loss: 0.01572323, Validation loss: 0.01223530, Gradient norm: 0.39246459
INFO:root:[   37] Training loss: 0.01527078, Validation loss: 0.01192364, Gradient norm: 0.35615661
INFO:root:[   38] Training loss: 0.01521658, Validation loss: 0.01258950, Gradient norm: 0.35477693
INFO:root:[   39] Training loss: 0.01490820, Validation loss: 0.01208312, Gradient norm: 0.33585181
INFO:root:[   40] Training loss: 0.01519292, Validation loss: 0.01278983, Gradient norm: 0.41050655
INFO:root:[   41] Training loss: 0.01504260, Validation loss: 0.01333022, Gradient norm: 0.39303065
INFO:root:[   42] Training loss: 0.01483443, Validation loss: 0.01279801, Gradient norm: 0.34296595
INFO:root:[   43] Training loss: 0.01497652, Validation loss: 0.01180529, Gradient norm: 0.39258036
INFO:root:[   44] Training loss: 0.01475116, Validation loss: 0.01139928, Gradient norm: 0.41056744
INFO:root:[   45] Training loss: 0.01468548, Validation loss: 0.01262265, Gradient norm: 0.40829892
INFO:root:[   46] Training loss: 0.01468966, Validation loss: 0.01356500, Gradient norm: 0.43602767
INFO:root:[   47] Training loss: 0.01443030, Validation loss: 0.01243080, Gradient norm: 0.37066395
INFO:root:[   48] Training loss: 0.01464542, Validation loss: 0.01143105, Gradient norm: 0.41536446
INFO:root:[   49] Training loss: 0.01409940, Validation loss: 0.01244397, Gradient norm: 0.33222553
INFO:root:[   50] Training loss: 0.01436355, Validation loss: 0.01281639, Gradient norm: 0.43338421
INFO:root:[   51] Training loss: 0.01442412, Validation loss: 0.01084722, Gradient norm: 0.41913619
INFO:root:[   52] Training loss: 0.01417720, Validation loss: 0.01100462, Gradient norm: 0.42775491
INFO:root:[   53] Training loss: 0.01376584, Validation loss: 0.01335886, Gradient norm: 0.32327472
INFO:root:[   54] Training loss: 0.01386868, Validation loss: 0.01130778, Gradient norm: 0.38613737
INFO:root:[   55] Training loss: 0.01395543, Validation loss: 0.01106815, Gradient norm: 0.41061181
INFO:root:[   56] Training loss: 0.01403886, Validation loss: 0.01131188, Gradient norm: 0.46123489
INFO:root:[   57] Training loss: 0.01363666, Validation loss: 0.01200188, Gradient norm: 0.35158227
INFO:root:[   58] Training loss: 0.01354025, Validation loss: 0.01085105, Gradient norm: 0.36905214
INFO:root:[   59] Training loss: 0.01387422, Validation loss: 0.01120761, Gradient norm: 0.42646736
INFO:root:[   60] Training loss: 0.01378019, Validation loss: 0.01159399, Gradient norm: 0.40709472
INFO:root:[   61] Training loss: 0.01344932, Validation loss: 0.01108060, Gradient norm: 0.38026148
INFO:root:[   62] Training loss: 0.01350517, Validation loss: 0.01096499, Gradient norm: 0.37390006
INFO:root:[   63] Training loss: 0.01332234, Validation loss: 0.01123628, Gradient norm: 0.36379196
INFO:root:[   64] Training loss: 0.01335059, Validation loss: 0.01173121, Gradient norm: 0.39102071
INFO:root:[   65] Training loss: 0.01309818, Validation loss: 0.01126827, Gradient norm: 0.35985561
INFO:root:[   66] Training loss: 0.01309046, Validation loss: 0.01079683, Gradient norm: 0.37512793
INFO:root:[   67] Training loss: 0.01308158, Validation loss: 0.01145229, Gradient norm: 0.40268901
INFO:root:[   68] Training loss: 0.01302844, Validation loss: 0.01099355, Gradient norm: 0.37647948
INFO:root:[   69] Training loss: 0.01307892, Validation loss: 0.01375340, Gradient norm: 0.38153726
INFO:root:[   70] Training loss: 0.01301037, Validation loss: 0.01145346, Gradient norm: 0.39920138
INFO:root:[   71] Training loss: 0.01326761, Validation loss: 0.01103612, Gradient norm: 0.43867116
INFO:root:[   72] Training loss: 0.01270776, Validation loss: 0.01072713, Gradient norm: 0.37437277
INFO:root:[   73] Training loss: 0.01262998, Validation loss: 0.01426313, Gradient norm: 0.34807504
INFO:root:[   74] Training loss: 0.01260504, Validation loss: 0.01348318, Gradient norm: 0.35040666
INFO:root:[   75] Training loss: 0.01261356, Validation loss: 0.01122703, Gradient norm: 0.37139202
INFO:root:[   76] Training loss: 0.01254851, Validation loss: 0.01094488, Gradient norm: 0.35073132
INFO:root:[   77] Training loss: 0.01247158, Validation loss: 0.01041886, Gradient norm: 0.36686146
INFO:root:[   78] Training loss: 0.01219911, Validation loss: 0.01352472, Gradient norm: 0.33338280
INFO:root:[   79] Training loss: 0.01283579, Validation loss: 0.01070390, Gradient norm: 0.43870056
INFO:root:[   80] Training loss: 0.01249532, Validation loss: 0.01146171, Gradient norm: 0.42283731
INFO:root:[   81] Training loss: 0.01235311, Validation loss: 0.01071327, Gradient norm: 0.36566132
INFO:root:[   82] Training loss: 0.01253961, Validation loss: 0.01081504, Gradient norm: 0.40079221
INFO:root:[   83] Training loss: 0.01204380, Validation loss: 0.01094018, Gradient norm: 0.32127062
INFO:root:[   84] Training loss: 0.01218793, Validation loss: 0.01204137, Gradient norm: 0.37973712
INFO:root:[   85] Training loss: 0.01220661, Validation loss: 0.01090033, Gradient norm: 0.38176862
INFO:root:[   86] Training loss: 0.01212898, Validation loss: 0.01091276, Gradient norm: 0.37830313
INFO:root:EP 86: Early stopping
INFO:root:Training the model took 452.962s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.53902
INFO:root:EnergyScoreTrain: 0.28688
INFO:root:CoverageTrain: 0.68993
INFO:root:IntervalWidthTrain: 0.04007
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.54341
INFO:root:EnergyScoreValidation: 0.29185
INFO:root:CoverageValidation: 0.69319
INFO:root:IntervalWidthValidation: 0.04039
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.70723
INFO:root:EnergyScoreTest: 0.41422
INFO:root:CoverageTest: 0.54421
INFO:root:IntervalWidthTest: 0.03955
INFO:root:###11 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 306184192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07650854, Validation loss: 0.03749213, Gradient norm: 0.33080327
INFO:root:[    2] Training loss: 0.04462434, Validation loss: 0.03148090, Gradient norm: 0.25242386
INFO:root:[    3] Training loss: 0.03858271, Validation loss: 0.02738613, Gradient norm: 0.30004572
INFO:root:[    4] Training loss: 0.03455896, Validation loss: 0.02461405, Gradient norm: 0.28093321
INFO:root:[    5] Training loss: 0.03172437, Validation loss: 0.02268069, Gradient norm: 0.30226548
INFO:root:[    6] Training loss: 0.02961292, Validation loss: 0.02283638, Gradient norm: 0.25642466
INFO:root:[    7] Training loss: 0.02815571, Validation loss: 0.02094369, Gradient norm: 0.27715161
INFO:root:[    8] Training loss: 0.02658121, Validation loss: 0.01963721, Gradient norm: 0.28188118
INFO:root:[    9] Training loss: 0.02567797, Validation loss: 0.01903234, Gradient norm: 0.28939322
INFO:root:[   10] Training loss: 0.02468605, Validation loss: 0.01939845, Gradient norm: 0.29684239
INFO:root:[   11] Training loss: 0.02390507, Validation loss: 0.01739048, Gradient norm: 0.28220454
INFO:root:[   12] Training loss: 0.02332895, Validation loss: 0.01748258, Gradient norm: 0.34451277
INFO:root:[   13] Training loss: 0.02254359, Validation loss: 0.01600041, Gradient norm: 0.31228835
INFO:root:[   14] Training loss: 0.02216998, Validation loss: 0.01573876, Gradient norm: 0.33381973
INFO:root:[   15] Training loss: 0.02152067, Validation loss: 0.01576988, Gradient norm: 0.29905734
INFO:root:[   16] Training loss: 0.02119487, Validation loss: 0.01552599, Gradient norm: 0.31911454
INFO:root:[   17] Training loss: 0.02092743, Validation loss: 0.01473742, Gradient norm: 0.35148364
INFO:root:[   18] Training loss: 0.02030830, Validation loss: 0.01543747, Gradient norm: 0.28533133
INFO:root:[   19] Training loss: 0.02017282, Validation loss: 0.01401533, Gradient norm: 0.33343831
INFO:root:[   20] Training loss: 0.01985905, Validation loss: 0.01436107, Gradient norm: 0.34583335
INFO:root:[   21] Training loss: 0.01974656, Validation loss: 0.01408958, Gradient norm: 0.36580188
INFO:root:[   22] Training loss: 0.01918525, Validation loss: 0.01450067, Gradient norm: 0.33024250
INFO:root:[   23] Training loss: 0.01883944, Validation loss: 0.01350200, Gradient norm: 0.30058950
INFO:root:[   24] Training loss: 0.01899249, Validation loss: 0.01386057, Gradient norm: 0.38291497
INFO:root:[   25] Training loss: 0.01827044, Validation loss: 0.01452771, Gradient norm: 0.27494432
INFO:root:[   26] Training loss: 0.01841427, Validation loss: 0.01379168, Gradient norm: 0.39493903
INFO:root:[   27] Training loss: 0.01789430, Validation loss: 0.01433287, Gradient norm: 0.29718407
INFO:root:[   28] Training loss: 0.01796994, Validation loss: 0.01351714, Gradient norm: 0.35257242
INFO:root:[   29] Training loss: 0.01792321, Validation loss: 0.01367691, Gradient norm: 0.37429521
INFO:root:[   30] Training loss: 0.01783024, Validation loss: 0.01309902, Gradient norm: 0.39385672
INFO:root:[   31] Training loss: 0.01738010, Validation loss: 0.01340527, Gradient norm: 0.38716130
INFO:root:[   32] Training loss: 0.01742108, Validation loss: 0.01312763, Gradient norm: 0.36694760
INFO:root:[   33] Training loss: 0.01714999, Validation loss: 0.01324043, Gradient norm: 0.36515177
INFO:root:[   34] Training loss: 0.01710071, Validation loss: 0.01347578, Gradient norm: 0.39295870
INFO:root:[   35] Training loss: 0.01713739, Validation loss: 0.01317171, Gradient norm: 0.41027775
INFO:root:[   36] Training loss: 0.01696893, Validation loss: 0.01376236, Gradient norm: 0.41537202
INFO:root:[   37] Training loss: 0.01666803, Validation loss: 0.01264619, Gradient norm: 0.38162519
INFO:root:[   38] Training loss: 0.01695068, Validation loss: 0.01272690, Gradient norm: 0.43476536
INFO:root:[   39] Training loss: 0.01629737, Validation loss: 0.01273246, Gradient norm: 0.31282015
INFO:root:[   40] Training loss: 0.01656828, Validation loss: 0.01256400, Gradient norm: 0.41289884
INFO:root:[   41] Training loss: 0.01636635, Validation loss: 0.01326387, Gradient norm: 0.41067562
INFO:root:[   42] Training loss: 0.01597625, Validation loss: 0.01233125, Gradient norm: 0.34081687
INFO:root:[   43] Training loss: 0.01588072, Validation loss: 0.01293644, Gradient norm: 0.34999470
INFO:root:[   44] Training loss: 0.01580435, Validation loss: 0.01409014, Gradient norm: 0.37695486
INFO:root:[   45] Training loss: 0.01608467, Validation loss: 0.01232005, Gradient norm: 0.41809027
INFO:root:[   46] Training loss: 0.01591296, Validation loss: 0.01272291, Gradient norm: 0.40101223
INFO:root:[   47] Training loss: 0.01543228, Validation loss: 0.01247731, Gradient norm: 0.33885109
INFO:root:[   48] Training loss: 0.01584460, Validation loss: 0.01200726, Gradient norm: 0.43673269
INFO:root:[   49] Training loss: 0.01540127, Validation loss: 0.01302137, Gradient norm: 0.35251264
INFO:root:[   50] Training loss: 0.01527784, Validation loss: 0.01224212, Gradient norm: 0.38422270
INFO:root:[   51] Training loss: 0.01544294, Validation loss: 0.01595291, Gradient norm: 0.42441328
INFO:root:[   52] Training loss: 0.01536882, Validation loss: 0.01790403, Gradient norm: 0.40893474
INFO:root:[   53] Training loss: 0.01511807, Validation loss: 0.01326195, Gradient norm: 0.37015984
INFO:root:[   54] Training loss: 0.01492450, Validation loss: 0.01238379, Gradient norm: 0.35961914
INFO:root:[   55] Training loss: 0.01503900, Validation loss: 0.01210645, Gradient norm: 0.35872229
INFO:root:[   56] Training loss: 0.01501400, Validation loss: 0.01357020, Gradient norm: 0.40202175
INFO:root:[   57] Training loss: 0.01494051, Validation loss: 0.01261121, Gradient norm: 0.39599000
INFO:root:[   58] Training loss: 0.01489109, Validation loss: 0.01519926, Gradient norm: 0.37146260
INFO:root:[   59] Training loss: 0.01479193, Validation loss: 0.01291051, Gradient norm: 0.39485931
INFO:root:[   60] Training loss: 0.01458944, Validation loss: 0.01181699, Gradient norm: 0.36418953
INFO:root:[   61] Training loss: 0.01452109, Validation loss: 0.01211509, Gradient norm: 0.34926533
INFO:root:[   62] Training loss: 0.01449787, Validation loss: 0.01218367, Gradient norm: 0.38427834
INFO:root:[   63] Training loss: 0.01440594, Validation loss: 0.01189262, Gradient norm: 0.40509273
INFO:root:[   64] Training loss: 0.01431403, Validation loss: 0.01164908, Gradient norm: 0.35575827
INFO:root:[   65] Training loss: 0.01406953, Validation loss: 0.01274687, Gradient norm: 0.34007142
INFO:root:[   66] Training loss: 0.01407542, Validation loss: 0.01269296, Gradient norm: 0.35056229
INFO:root:[   67] Training loss: 0.01432421, Validation loss: 0.01328500, Gradient norm: 0.41249151
INFO:root:[   68] Training loss: 0.01421305, Validation loss: 0.01211112, Gradient norm: 0.38153958
INFO:root:[   69] Training loss: 0.01405943, Validation loss: 0.01160701, Gradient norm: 0.37525891
INFO:root:[   70] Training loss: 0.01391132, Validation loss: 0.01254994, Gradient norm: 0.34381498
INFO:root:[   71] Training loss: 0.01410763, Validation loss: 0.01228334, Gradient norm: 0.39344935
INFO:root:[   72] Training loss: 0.01394361, Validation loss: 0.01215063, Gradient norm: 0.39102913
INFO:root:[   73] Training loss: 0.01397362, Validation loss: 0.01246107, Gradient norm: 0.40228855
INFO:root:[   74] Training loss: 0.01364747, Validation loss: 0.01190463, Gradient norm: 0.31589145
INFO:root:[   75] Training loss: 0.01375153, Validation loss: 0.01274477, Gradient norm: 0.37291438
INFO:root:[   76] Training loss: 0.01357927, Validation loss: 0.01251663, Gradient norm: 0.35108767
INFO:root:[   77] Training loss: 0.01368903, Validation loss: 0.01218147, Gradient norm: 0.35639152
INFO:root:[   78] Training loss: 0.01357802, Validation loss: 0.01232294, Gradient norm: 0.35908755
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 410.907s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.59245
INFO:root:EnergyScoreTrain: 0.30223
INFO:root:CoverageTrain: 0.67498
INFO:root:IntervalWidthTrain: 0.04587
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.62494
INFO:root:EnergyScoreValidation: 0.33241
INFO:root:CoverageValidation: 0.65993
INFO:root:IntervalWidthValidation: 0.04444
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.79956
INFO:root:EnergyScoreTest: 0.47648
INFO:root:CoverageTest: 0.49819
INFO:root:IntervalWidthTest: 0.04051
INFO:root:###12 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07122213, Validation loss: 0.04173006, Gradient norm: 0.28016418
INFO:root:[    2] Training loss: 0.04584895, Validation loss: 0.03594917, Gradient norm: 0.23779623
INFO:root:[    3] Training loss: 0.04029020, Validation loss: 0.03093246, Gradient norm: 0.28110208
INFO:root:[    4] Training loss: 0.03589960, Validation loss: 0.03254381, Gradient norm: 0.28936515
INFO:root:[    5] Training loss: 0.03270673, Validation loss: 0.02495661, Gradient norm: 0.25691729
INFO:root:[    6] Training loss: 0.03046755, Validation loss: 0.02346355, Gradient norm: 0.28179325
INFO:root:[    7] Training loss: 0.02890641, Validation loss: 0.02347582, Gradient norm: 0.26584339
INFO:root:[    8] Training loss: 0.02803239, Validation loss: 0.02508785, Gradient norm: 0.32387678
INFO:root:[    9] Training loss: 0.02659175, Validation loss: 0.02038201, Gradient norm: 0.27686511
INFO:root:[   10] Training loss: 0.02542545, Validation loss: 0.02019228, Gradient norm: 0.28619564
INFO:root:[   11] Training loss: 0.02453683, Validation loss: 0.02119955, Gradient norm: 0.26392551
INFO:root:[   12] Training loss: 0.02388250, Validation loss: 0.01894328, Gradient norm: 0.28530340
INFO:root:[   13] Training loss: 0.02303294, Validation loss: 0.01712774, Gradient norm: 0.25442568
INFO:root:[   14] Training loss: 0.02285809, Validation loss: 0.01622669, Gradient norm: 0.33812720
INFO:root:[   15] Training loss: 0.02182196, Validation loss: 0.01596908, Gradient norm: 0.26449957
INFO:root:[   16] Training loss: 0.02187768, Validation loss: 0.01788249, Gradient norm: 0.33561452
INFO:root:[   17] Training loss: 0.02107772, Validation loss: 0.01544229, Gradient norm: 0.31042209
INFO:root:[   18] Training loss: 0.02094284, Validation loss: 0.01685895, Gradient norm: 0.36109850
INFO:root:[   19] Training loss: 0.02033752, Validation loss: 0.01487192, Gradient norm: 0.31531061
INFO:root:[   20] Training loss: 0.02050954, Validation loss: 0.01637595, Gradient norm: 0.38510740
INFO:root:[   21] Training loss: 0.01964427, Validation loss: 0.01595456, Gradient norm: 0.31272901
INFO:root:[   22] Training loss: 0.01969371, Validation loss: 0.01486634, Gradient norm: 0.34909311
INFO:root:[   23] Training loss: 0.01912033, Validation loss: 0.01441636, Gradient norm: 0.30288847
INFO:root:[   24] Training loss: 0.01938332, Validation loss: 0.01503962, Gradient norm: 0.40270143
INFO:root:[   25] Training loss: 0.01877979, Validation loss: 0.01626489, Gradient norm: 0.32748548
INFO:root:[   26] Training loss: 0.01878107, Validation loss: 0.01789072, Gradient norm: 0.37480675
INFO:root:[   27] Training loss: 0.01859559, Validation loss: 0.02313301, Gradient norm: 0.33013995
INFO:root:[   28] Training loss: 0.01868780, Validation loss: 0.02004053, Gradient norm: 0.39229577
INFO:root:[   29] Training loss: 0.01830799, Validation loss: 0.01396612, Gradient norm: 0.38397128
INFO:root:[   30] Training loss: 0.01787261, Validation loss: 0.01961996, Gradient norm: 0.32045149
INFO:root:[   31] Training loss: 0.01813627, Validation loss: 0.01494501, Gradient norm: 0.42059032
INFO:root:[   32] Training loss: 0.01780401, Validation loss: 0.01896394, Gradient norm: 0.38787614
INFO:root:[   33] Training loss: 0.01778173, Validation loss: 0.01345891, Gradient norm: 0.39783805
INFO:root:[   34] Training loss: 0.01756248, Validation loss: 0.01495725, Gradient norm: 0.38026027
INFO:root:[   35] Training loss: 0.01735808, Validation loss: 0.01373984, Gradient norm: 0.37340822
INFO:root:[   36] Training loss: 0.01708550, Validation loss: 0.01328865, Gradient norm: 0.36915180
INFO:root:[   37] Training loss: 0.01691618, Validation loss: 0.01759807, Gradient norm: 0.34427131
INFO:root:[   38] Training loss: 0.01697936, Validation loss: 0.01606229, Gradient norm: 0.36813468
INFO:root:[   39] Training loss: 0.01670765, Validation loss: 0.01856019, Gradient norm: 0.36131972
INFO:root:[   40] Training loss: 0.01647151, Validation loss: 0.01960757, Gradient norm: 0.32624115
INFO:root:[   41] Training loss: 0.01653526, Validation loss: 0.01546917, Gradient norm: 0.34157646
INFO:root:[   42] Training loss: 0.01634174, Validation loss: 0.01368105, Gradient norm: 0.34365218
INFO:root:[   43] Training loss: 0.01669746, Validation loss: 0.01407841, Gradient norm: 0.41750472
INFO:root:[   44] Training loss: 0.01631884, Validation loss: 0.01934030, Gradient norm: 0.40972573
INFO:root:[   45] Training loss: 0.01625284, Validation loss: 0.01673085, Gradient norm: 0.37514658
INFO:root:[   46] Training loss: 0.01584055, Validation loss: 0.01725570, Gradient norm: 0.36700867
INFO:root:[   47] Training loss: 0.01619692, Validation loss: 0.01558111, Gradient norm: 0.39565602
INFO:root:[   48] Training loss: 0.01584656, Validation loss: 0.01508318, Gradient norm: 0.37561655
INFO:root:[   49] Training loss: 0.01577667, Validation loss: 0.01417257, Gradient norm: 0.33681819
INFO:root:[   50] Training loss: 0.01561330, Validation loss: 0.01242082, Gradient norm: 0.36129109
INFO:root:[   51] Training loss: 0.01562177, Validation loss: 0.01204578, Gradient norm: 0.35779142
INFO:root:[   52] Training loss: 0.01589513, Validation loss: 0.01361715, Gradient norm: 0.42768976
INFO:root:[   53] Training loss: 0.01549511, Validation loss: 0.01300660, Gradient norm: 0.36075372
INFO:root:[   54] Training loss: 0.01551270, Validation loss: 0.01472040, Gradient norm: 0.38963964
INFO:root:[   55] Training loss: 0.01517537, Validation loss: 0.01376490, Gradient norm: 0.34166374
INFO:root:[   56] Training loss: 0.01507902, Validation loss: 0.01244577, Gradient norm: 0.34682264
INFO:root:[   57] Training loss: 0.01525701, Validation loss: 0.01353871, Gradient norm: 0.39453861
INFO:root:[   58] Training loss: 0.01541794, Validation loss: 0.01140082, Gradient norm: 0.40709628
INFO:root:[   59] Training loss: 0.01501065, Validation loss: 0.01613465, Gradient norm: 0.36033710
INFO:root:[   60] Training loss: 0.01508511, Validation loss: 0.01413443, Gradient norm: 0.38596319
INFO:root:[   61] Training loss: 0.01493563, Validation loss: 0.01823011, Gradient norm: 0.37622903
INFO:root:[   62] Training loss: 0.01499313, Validation loss: 0.01255386, Gradient norm: 0.37554772
INFO:root:[   63] Training loss: 0.01445991, Validation loss: 0.01597691, Gradient norm: 0.30159375
INFO:root:[   64] Training loss: 0.01491787, Validation loss: 0.01391103, Gradient norm: 0.38414070
INFO:root:[   65] Training loss: 0.01459335, Validation loss: 0.01365703, Gradient norm: 0.35741854
INFO:root:[   66] Training loss: 0.01471985, Validation loss: 0.01333221, Gradient norm: 0.38225239
INFO:root:[   67] Training loss: 0.01445302, Validation loss: 0.01265310, Gradient norm: 0.33602512
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 356.436s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.62434
INFO:root:EnergyScoreTrain: 0.32684
INFO:root:CoverageTrain: 0.69128
INFO:root:IntervalWidthTrain: 0.04904
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.61912
INFO:root:EnergyScoreValidation: 0.33374
INFO:root:CoverageValidation: 0.6859
INFO:root:IntervalWidthValidation: 0.05184
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.82401
INFO:root:EnergyScoreTest: 0.45884
INFO:root:CoverageTest: 0.56337
INFO:root:IntervalWidthTest: 0.05192
INFO:root:###13 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07552368, Validation loss: 0.03952848, Gradient norm: 0.24363751
INFO:root:[    2] Training loss: 0.04976095, Validation loss: 0.03671197, Gradient norm: 0.23898924
INFO:root:[    3] Training loss: 0.04411017, Validation loss: 0.03734044, Gradient norm: 0.25410682
INFO:root:[    4] Training loss: 0.03979923, Validation loss: 0.03058470, Gradient norm: 0.28173891
INFO:root:[    5] Training loss: 0.03657532, Validation loss: 0.02856444, Gradient norm: 0.26722322
INFO:root:[    6] Training loss: 0.03442389, Validation loss: 0.02534632, Gradient norm: 0.26764463
INFO:root:[    7] Training loss: 0.03235019, Validation loss: 0.02466114, Gradient norm: 0.24945310
INFO:root:[    8] Training loss: 0.03084607, Validation loss: 0.02291456, Gradient norm: 0.25558174
INFO:root:[    9] Training loss: 0.02973853, Validation loss: 0.02324403, Gradient norm: 0.27486541
INFO:root:[   10] Training loss: 0.02877543, Validation loss: 0.02130895, Gradient norm: 0.32061899
INFO:root:[   11] Training loss: 0.02791095, Validation loss: 0.02392512, Gradient norm: 0.29824517
INFO:root:[   12] Training loss: 0.02702875, Validation loss: 0.02077019, Gradient norm: 0.31578973
INFO:root:[   13] Training loss: 0.02632315, Validation loss: 0.02164524, Gradient norm: 0.32252649
INFO:root:[   14] Training loss: 0.02575217, Validation loss: 0.02157895, Gradient norm: 0.33395876
INFO:root:[   15] Training loss: 0.02506115, Validation loss: 0.01960416, Gradient norm: 0.34383138
INFO:root:[   16] Training loss: 0.02440305, Validation loss: 0.01898551, Gradient norm: 0.30277119
INFO:root:[   17] Training loss: 0.02397550, Validation loss: 0.02464081, Gradient norm: 0.32852526
INFO:root:[   18] Training loss: 0.02385052, Validation loss: 0.02162442, Gradient norm: 0.37401982
INFO:root:[   19] Training loss: 0.02329094, Validation loss: 0.02155005, Gradient norm: 0.36570604
INFO:root:[   20] Training loss: 0.02301677, Validation loss: 0.01808527, Gradient norm: 0.35626822
INFO:root:[   21] Training loss: 0.02242158, Validation loss: 0.01724464, Gradient norm: 0.30462550
INFO:root:[   22] Training loss: 0.02218406, Validation loss: 0.01810017, Gradient norm: 0.34662850
INFO:root:[   23] Training loss: 0.02195061, Validation loss: 0.01774350, Gradient norm: 0.35339284
INFO:root:[   24] Training loss: 0.02141273, Validation loss: 0.01647638, Gradient norm: 0.33000069
INFO:root:[   25] Training loss: 0.02124636, Validation loss: 0.01595194, Gradient norm: 0.33486062
INFO:root:[   26] Training loss: 0.02109382, Validation loss: 0.01691085, Gradient norm: 0.36261823
INFO:root:[   27] Training loss: 0.02082987, Validation loss: 0.01610349, Gradient norm: 0.36241628
INFO:root:[   28] Training loss: 0.02083853, Validation loss: 0.01630196, Gradient norm: 0.39036219
INFO:root:[   29] Training loss: 0.02031919, Validation loss: 0.01622941, Gradient norm: 0.35862331
INFO:root:[   30] Training loss: 0.02029307, Validation loss: 0.01729002, Gradient norm: 0.36197428
INFO:root:[   31] Training loss: 0.01999824, Validation loss: 0.01621441, Gradient norm: 0.37101219
INFO:root:[   32] Training loss: 0.02022652, Validation loss: 0.01685750, Gradient norm: 0.38886763
INFO:root:[   33] Training loss: 0.01977563, Validation loss: 0.01850912, Gradient norm: 0.38556905
INFO:root:[   34] Training loss: 0.01933673, Validation loss: 0.01496518, Gradient norm: 0.34005569
INFO:root:[   35] Training loss: 0.01913976, Validation loss: 0.01955230, Gradient norm: 0.31118874
INFO:root:[   36] Training loss: 0.01911958, Validation loss: 0.01502566, Gradient norm: 0.35208167
INFO:root:[   37] Training loss: 0.01935275, Validation loss: 0.01533812, Gradient norm: 0.38963187
INFO:root:[   38] Training loss: 0.01890009, Validation loss: 0.01605399, Gradient norm: 0.35604298
INFO:root:[   39] Training loss: 0.01864962, Validation loss: 0.01658049, Gradient norm: 0.33687300
INFO:root:[   40] Training loss: 0.01888009, Validation loss: 0.01581490, Gradient norm: 0.35936922
INFO:root:[   41] Training loss: 0.01840447, Validation loss: 0.01725068, Gradient norm: 0.36481803
INFO:root:[   42] Training loss: 0.01828117, Validation loss: 0.01639294, Gradient norm: 0.35297766
INFO:root:[   43] Training loss: 0.01811358, Validation loss: 0.01659321, Gradient norm: 0.32540811
INFO:root:[   44] Training loss: 0.01827922, Validation loss: 0.01558510, Gradient norm: 0.37656738
INFO:root:[   45] Training loss: 0.01811180, Validation loss: 0.01502398, Gradient norm: 0.38465082
INFO:root:[   46] Training loss: 0.01776998, Validation loss: 0.01498278, Gradient norm: 0.33669585
INFO:root:[   47] Training loss: 0.01788776, Validation loss: 0.01589338, Gradient norm: 0.35797457
INFO:root:[   48] Training loss: 0.01790070, Validation loss: 0.01506996, Gradient norm: 0.40450232
INFO:root:[   49] Training loss: 0.01735387, Validation loss: 0.01528374, Gradient norm: 0.29020844
INFO:root:[   50] Training loss: 0.01751429, Validation loss: 0.01633256, Gradient norm: 0.35903385
INFO:root:[   51] Training loss: 0.01772687, Validation loss: 0.01566778, Gradient norm: 0.40603680
INFO:root:[   52] Training loss: 0.01743621, Validation loss: 0.01520166, Gradient norm: 0.36683121
INFO:root:[   53] Training loss: 0.01737365, Validation loss: 0.01574951, Gradient norm: 0.36157217
INFO:root:[   54] Training loss: 0.01704355, Validation loss: 0.01582972, Gradient norm: 0.33256327
INFO:root:[   55] Training loss: 0.01690163, Validation loss: 0.01468022, Gradient norm: 0.32174767
INFO:root:[   56] Training loss: 0.01697330, Validation loss: 0.01627564, Gradient norm: 0.33209333
INFO:root:[   57] Training loss: 0.01670811, Validation loss: 0.01490488, Gradient norm: 0.29104600
INFO:root:[   58] Training loss: 0.01682057, Validation loss: 0.01518396, Gradient norm: 0.32622418
INFO:root:[   59] Training loss: 0.01692060, Validation loss: 0.01632217, Gradient norm: 0.37282481
INFO:root:[   60] Training loss: 0.01669966, Validation loss: 0.01442964, Gradient norm: 0.34067700
INFO:root:[   61] Training loss: 0.01669821, Validation loss: 0.01483598, Gradient norm: 0.34299340
INFO:root:[   62] Training loss: 0.01633250, Validation loss: 0.01883427, Gradient norm: 0.29981295
INFO:root:[   63] Training loss: 0.01654283, Validation loss: 0.01553380, Gradient norm: 0.34607556
INFO:root:[   64] Training loss: 0.01634608, Validation loss: 0.01610502, Gradient norm: 0.33134781
INFO:root:[   65] Training loss: 0.01635291, Validation loss: 0.01512458, Gradient norm: 0.32377943
INFO:root:[   66] Training loss: 0.01632127, Validation loss: 0.01631437, Gradient norm: 0.34665707
INFO:root:[   67] Training loss: 0.01618892, Validation loss: 0.01657152, Gradient norm: 0.33212762
INFO:root:[   68] Training loss: 0.01601105, Validation loss: 0.01590999, Gradient norm: 0.31990816
INFO:root:[   69] Training loss: 0.01615416, Validation loss: 0.01444522, Gradient norm: 0.33173470
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 368.015s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.73551
INFO:root:EnergyScoreTrain: 0.40785
INFO:root:CoverageTrain: 0.64513
INFO:root:IntervalWidthTrain: 0.05076
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.80944
INFO:root:EnergyScoreValidation: 0.42339
INFO:root:CoverageValidation: 0.64247
INFO:root:IntervalWidthValidation: 0.05538
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86463
INFO:root:EnergyScoreTest: 0.50089
INFO:root:CoverageTest: 0.55354
INFO:root:IntervalWidthTest: 0.04897
INFO:root:###14 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08730237, Validation loss: 0.05253950, Gradient norm: 0.19474430
INFO:root:[    2] Training loss: 0.05685985, Validation loss: 0.04323143, Gradient norm: 0.24323910
INFO:root:[    3] Training loss: 0.05002696, Validation loss: 0.04307615, Gradient norm: 0.24058568
INFO:root:[    4] Training loss: 0.04648451, Validation loss: 0.04131652, Gradient norm: 0.27348771
INFO:root:[    5] Training loss: 0.04358130, Validation loss: 0.03986034, Gradient norm: 0.23807329
INFO:root:[    6] Training loss: 0.04109334, Validation loss: 0.03549946, Gradient norm: 0.22366485
INFO:root:[    7] Training loss: 0.03909791, Validation loss: 0.03617567, Gradient norm: 0.28841141
INFO:root:[    8] Training loss: 0.03742199, Validation loss: 0.03275127, Gradient norm: 0.27775457
INFO:root:[    9] Training loss: 0.03585321, Validation loss: 0.03304574, Gradient norm: 0.27154501
INFO:root:[   10] Training loss: 0.03461034, Validation loss: 0.03184241, Gradient norm: 0.30209056
INFO:root:[   11] Training loss: 0.03349800, Validation loss: 0.03027304, Gradient norm: 0.29268621
INFO:root:[   12] Training loss: 0.03245197, Validation loss: 0.02921707, Gradient norm: 0.30237554
INFO:root:[   13] Training loss: 0.03152927, Validation loss: 0.02993577, Gradient norm: 0.27204782
INFO:root:[   14] Training loss: 0.03080525, Validation loss: 0.02974899, Gradient norm: 0.32227293
INFO:root:[   15] Training loss: 0.03020549, Validation loss: 0.03130152, Gradient norm: 0.28278802
INFO:root:[   16] Training loss: 0.02968076, Validation loss: 0.03223290, Gradient norm: 0.34373157
INFO:root:[   17] Training loss: 0.02889061, Validation loss: 0.03283944, Gradient norm: 0.33502964
INFO:root:[   18] Training loss: 0.02822278, Validation loss: 0.02945132, Gradient norm: 0.30641800
INFO:root:[   19] Training loss: 0.02791817, Validation loss: 0.02798347, Gradient norm: 0.33229506
INFO:root:[   20] Training loss: 0.02731934, Validation loss: 0.03131255, Gradient norm: 0.29977922
INFO:root:[   21] Training loss: 0.02697306, Validation loss: 0.02946468, Gradient norm: 0.32236465
INFO:root:[   22] Training loss: 0.02682098, Validation loss: 0.02825303, Gradient norm: 0.33401064
INFO:root:[   23] Training loss: 0.02624035, Validation loss: 0.02939826, Gradient norm: 0.33446943
INFO:root:[   24] Training loss: 0.02614799, Validation loss: 0.02989341, Gradient norm: 0.30850803
INFO:root:[   25] Training loss: 0.02551124, Validation loss: 0.03394231, Gradient norm: 0.26404096
INFO:root:[   26] Training loss: 0.02512153, Validation loss: 0.03019184, Gradient norm: 0.29277210
INFO:root:[   27] Training loss: 0.02519833, Validation loss: 0.03244213, Gradient norm: 0.29876579
INFO:root:[   28] Training loss: 0.02475148, Validation loss: 0.03126145, Gradient norm: 0.29021070
INFO:root:[   29] Training loss: 0.02451165, Validation loss: 0.03327510, Gradient norm: 0.30353256
INFO:root:[   30] Training loss: 0.02422469, Validation loss: 0.03031399, Gradient norm: 0.28186964
INFO:root:[   31] Training loss: 0.02422396, Validation loss: 0.03039682, Gradient norm: 0.31759343
INFO:root:[   32] Training loss: 0.02395755, Validation loss: 0.02957929, Gradient norm: 0.30499553
INFO:root:[   33] Training loss: 0.02353863, Validation loss: 0.03209477, Gradient norm: 0.30374983
INFO:root:[   34] Training loss: 0.02361971, Validation loss: 0.03347955, Gradient norm: 0.30525467
INFO:root:[   35] Training loss: 0.02338145, Validation loss: 0.03521389, Gradient norm: 0.30756358
INFO:root:[   36] Training loss: 0.02325456, Validation loss: 0.03462727, Gradient norm: 0.30036245
INFO:root:[   37] Training loss: 0.02301288, Validation loss: 0.03146700, Gradient norm: 0.29611254
INFO:root:[   38] Training loss: 0.02290415, Validation loss: 0.03234673, Gradient norm: 0.29127801
INFO:root:[   39] Training loss: 0.02263107, Validation loss: 0.03676540, Gradient norm: 0.27767876
INFO:root:[   40] Training loss: 0.02245756, Validation loss: 0.03379969, Gradient norm: 0.26547375
INFO:root:[   41] Training loss: 0.02236479, Validation loss: 0.03081815, Gradient norm: 0.28890300
INFO:root:[   42] Training loss: 0.02215551, Validation loss: 0.03193762, Gradient norm: 0.29069416
INFO:root:[   43] Training loss: 0.02213922, Validation loss: 0.03136353, Gradient norm: 0.30044738
INFO:root:[   44] Training loss: 0.02194231, Validation loss: 0.03127215, Gradient norm: 0.29511038
INFO:root:[   45] Training loss: 0.02155617, Validation loss: 0.03519783, Gradient norm: 0.26347370
INFO:root:[   46] Training loss: 0.02166870, Validation loss: 0.03457970, Gradient norm: 0.28757801
INFO:root:[   47] Training loss: 0.02155706, Validation loss: 0.03233542, Gradient norm: 0.28715439
INFO:root:[   48] Training loss: 0.02150547, Validation loss: 0.03042974, Gradient norm: 0.31164054
INFO:root:[   49] Training loss: 0.02119989, Validation loss: 0.03025052, Gradient norm: 0.27018970
INFO:root:[   50] Training loss: 0.02085990, Validation loss: 0.03191783, Gradient norm: 0.25354361
INFO:root:[   51] Training loss: 0.02088953, Validation loss: 0.03220980, Gradient norm: 0.25458584
INFO:root:[   52] Training loss: 0.02088445, Validation loss: 0.03604403, Gradient norm: 0.25994308
INFO:root:[   53] Training loss: 0.02077008, Validation loss: 0.03070430, Gradient norm: 0.26798049
INFO:root:[   54] Training loss: 0.02070548, Validation loss: 0.03164724, Gradient norm: 0.26545863
INFO:root:[   55] Training loss: 0.02063163, Validation loss: 0.03031177, Gradient norm: 0.26662730
INFO:root:[   56] Training loss: 0.02052606, Validation loss: 0.03252001, Gradient norm: 0.28870927
INFO:root:[   57] Training loss: 0.02025286, Validation loss: 0.03295744, Gradient norm: 0.26527508
INFO:root:[   58] Training loss: 0.02008086, Validation loss: 0.03002269, Gradient norm: 0.24606255
INFO:root:[   59] Training loss: 0.02029934, Validation loss: 0.03215991, Gradient norm: 0.31701202
INFO:root:[   60] Training loss: 0.02016393, Validation loss: 0.02999615, Gradient norm: 0.26426336
INFO:root:[   61] Training loss: 0.02035322, Validation loss: 0.02969647, Gradient norm: 0.29422216
INFO:root:[   62] Training loss: 0.01977493, Validation loss: 0.03128824, Gradient norm: 0.25468642
INFO:root:[   63] Training loss: 0.01985875, Validation loss: 0.03130574, Gradient norm: 0.24501831
INFO:root:[   64] Training loss: 0.01966948, Validation loss: 0.03048335, Gradient norm: 0.24552796
INFO:root:[   65] Training loss: 0.01965332, Validation loss: 0.03098699, Gradient norm: 0.26195166
INFO:root:[   66] Training loss: 0.01998488, Validation loss: 0.03405396, Gradient norm: 0.29726279
INFO:root:[   67] Training loss: 0.01946349, Validation loss: 0.03127121, Gradient norm: 0.25765109
INFO:root:[   68] Training loss: 0.01936860, Validation loss: 0.03062300, Gradient norm: 0.24247367
INFO:root:[   69] Training loss: 0.01965508, Validation loss: 0.03191611, Gradient norm: 0.26828192
INFO:root:[   70] Training loss: 0.01927542, Validation loss: 0.03131702, Gradient norm: 0.25553237
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 370.484s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.04101
INFO:root:EnergyScoreTrain: 0.63867
INFO:root:CoverageTrain: 0.50085
INFO:root:IntervalWidthTrain: 0.05165
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.11753
INFO:root:EnergyScoreValidation: 0.7492
INFO:root:CoverageValidation: 0.392
INFO:root:IntervalWidthValidation: 0.03976
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.01857
INFO:root:EnergyScoreTest: 0.64038
INFO:root:CoverageTest: 0.51042
INFO:root:IntervalWidthTest: 0.0493
INFO:root:###15 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 304087040
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06831752, Validation loss: 0.03869366, Gradient norm: 0.38501302
INFO:root:[    2] Training loss: 0.03351315, Validation loss: 0.02911371, Gradient norm: 0.38790734
INFO:root:[    3] Training loss: 0.02769923, Validation loss: 0.02503148, Gradient norm: 0.44040567
INFO:root:[    4] Training loss: 0.02447803, Validation loss: 0.02286696, Gradient norm: 0.40775441
INFO:root:[    5] Training loss: 0.02261684, Validation loss: 0.02189905, Gradient norm: 0.41520810
INFO:root:[    6] Training loss: 0.02164580, Validation loss: 0.02051547, Gradient norm: 0.46026444
INFO:root:[    7] Training loss: 0.01982729, Validation loss: 0.01910107, Gradient norm: 0.37433372
INFO:root:[    8] Training loss: 0.01922644, Validation loss: 0.01866273, Gradient norm: 0.46435749
INFO:root:[    9] Training loss: 0.01841312, Validation loss: 0.01768287, Gradient norm: 0.43925996
INFO:root:[   10] Training loss: 0.01791443, Validation loss: 0.01730166, Gradient norm: 0.43866183
INFO:root:[   11] Training loss: 0.01670320, Validation loss: 0.01717284, Gradient norm: 0.34261164
INFO:root:[   12] Training loss: 0.01668194, Validation loss: 0.01748925, Gradient norm: 0.42536762
INFO:root:[   13] Training loss: 0.01610973, Validation loss: 0.01595787, Gradient norm: 0.40169545
INFO:root:[   14] Training loss: 0.01577699, Validation loss: 0.01574557, Gradient norm: 0.38564912
INFO:root:[   15] Training loss: 0.01527904, Validation loss: 0.01541690, Gradient norm: 0.36838782
INFO:root:[   16] Training loss: 0.01548311, Validation loss: 0.01530344, Gradient norm: 0.46546324
INFO:root:[   17] Training loss: 0.01530518, Validation loss: 0.01569970, Gradient norm: 0.42638881
INFO:root:[   18] Training loss: 0.01522178, Validation loss: 0.01461020, Gradient norm: 0.45223189
INFO:root:[   19] Training loss: 0.01439453, Validation loss: 0.01535572, Gradient norm: 0.37541655
INFO:root:[   20] Training loss: 0.01450960, Validation loss: 0.01605442, Gradient norm: 0.42293590
INFO:root:[   21] Training loss: 0.01415441, Validation loss: 0.01459966, Gradient norm: 0.37427444
INFO:root:[   22] Training loss: 0.01376380, Validation loss: 0.01409800, Gradient norm: 0.35125741
INFO:root:[   23] Training loss: 0.01407313, Validation loss: 0.01491561, Gradient norm: 0.42790170
INFO:root:[   24] Training loss: 0.01345296, Validation loss: 0.01431427, Gradient norm: 0.31854356
INFO:root:[   25] Training loss: 0.01387836, Validation loss: 0.01358632, Gradient norm: 0.45454667
INFO:root:[   26] Training loss: 0.01400762, Validation loss: 0.01373958, Gradient norm: 0.47782274
INFO:root:[   27] Training loss: 0.01317501, Validation loss: 0.01552685, Gradient norm: 0.34499617
INFO:root:[   28] Training loss: 0.01331344, Validation loss: 0.01394025, Gradient norm: 0.41629913
INFO:root:[   29] Training loss: 0.01298014, Validation loss: 0.01591198, Gradient norm: 0.37189171
INFO:root:[   30] Training loss: 0.01278222, Validation loss: 0.01388835, Gradient norm: 0.34761458
INFO:root:[   31] Training loss: 0.01298152, Validation loss: 0.01432477, Gradient norm: 0.41206852
INFO:root:[   32] Training loss: 0.01255906, Validation loss: 0.01388675, Gradient norm: 0.35492996
INFO:root:[   33] Training loss: 0.01287772, Validation loss: 0.01296520, Gradient norm: 0.41065251
INFO:root:[   34] Training loss: 0.01262983, Validation loss: 0.01478199, Gradient norm: 0.40446439
INFO:root:[   35] Training loss: 0.01231238, Validation loss: 0.01318897, Gradient norm: 0.37020694
INFO:root:[   36] Training loss: 0.01232931, Validation loss: 0.01602730, Gradient norm: 0.36568817
INFO:root:[   37] Training loss: 0.01261719, Validation loss: 0.01384312, Gradient norm: 0.45094991
INFO:root:[   38] Training loss: 0.01198713, Validation loss: 0.01286442, Gradient norm: 0.31744416
INFO:root:[   39] Training loss: 0.01223055, Validation loss: 0.01279867, Gradient norm: 0.41318797
INFO:root:[   40] Training loss: 0.01211981, Validation loss: 0.01337240, Gradient norm: 0.38974110
INFO:root:[   41] Training loss: 0.01183778, Validation loss: 0.01313641, Gradient norm: 0.37152291
INFO:root:[   42] Training loss: 0.01175448, Validation loss: 0.01234050, Gradient norm: 0.36927661
INFO:root:[   43] Training loss: 0.01211512, Validation loss: 0.01253227, Gradient norm: 0.43930818
INFO:root:[   44] Training loss: 0.01191765, Validation loss: 0.01318851, Gradient norm: 0.42585441
INFO:root:[   45] Training loss: 0.01170153, Validation loss: 0.01391121, Gradient norm: 0.40911188
INFO:root:[   46] Training loss: 0.01163540, Validation loss: 0.01924076, Gradient norm: 0.38537476
INFO:root:[   47] Training loss: 0.01147723, Validation loss: 0.01232263, Gradient norm: 0.34332392
INFO:root:[   48] Training loss: 0.01125683, Validation loss: 0.01217816, Gradient norm: 0.34376371
INFO:root:[   49] Training loss: 0.01141687, Validation loss: 0.01230046, Gradient norm: 0.37559781
INFO:root:[   50] Training loss: 0.01105368, Validation loss: 0.01454734, Gradient norm: 0.33057716
INFO:root:[   51] Training loss: 0.01128438, Validation loss: 0.01255124, Gradient norm: 0.39476978
INFO:root:[   52] Training loss: 0.01143191, Validation loss: 0.01241663, Gradient norm: 0.41967482
INFO:root:[   53] Training loss: 0.01102343, Validation loss: 0.01202700, Gradient norm: 0.38950045
INFO:root:[   54] Training loss: 0.01107849, Validation loss: 0.01200289, Gradient norm: 0.39536477
INFO:root:[   55] Training loss: 0.01083749, Validation loss: 0.01222373, Gradient norm: 0.35648826
INFO:root:[   56] Training loss: 0.01076797, Validation loss: 0.01201707, Gradient norm: 0.34742501
INFO:root:[   57] Training loss: 0.01081749, Validation loss: 0.01239342, Gradient norm: 0.37911808
INFO:root:[   58] Training loss: 0.01072448, Validation loss: 0.01189263, Gradient norm: 0.36862981
INFO:root:[   59] Training loss: 0.01073778, Validation loss: 0.01199300, Gradient norm: 0.37357685
INFO:root:[   60] Training loss: 0.01028701, Validation loss: 0.01208313, Gradient norm: 0.30435938
INFO:root:[   61] Training loss: 0.01066699, Validation loss: 0.01268296, Gradient norm: 0.38249888
INFO:root:[   62] Training loss: 0.01051223, Validation loss: 0.01192410, Gradient norm: 0.38512180
INFO:root:[   63] Training loss: 0.01029289, Validation loss: 0.01246473, Gradient norm: 0.35130355
INFO:root:[   64] Training loss: 0.01068985, Validation loss: 0.01205014, Gradient norm: 0.42986277
INFO:root:[   65] Training loss: 0.01025538, Validation loss: 0.01173931, Gradient norm: 0.32974361
INFO:root:[   66] Training loss: 0.00999101, Validation loss: 0.01199374, Gradient norm: 0.30256067
INFO:root:[   67] Training loss: 0.01012232, Validation loss: 0.01178088, Gradient norm: 0.36080628
INFO:root:[   68] Training loss: 0.01023384, Validation loss: 0.01200821, Gradient norm: 0.38975950
INFO:root:[   69] Training loss: 0.01008723, Validation loss: 0.01200893, Gradient norm: 0.36016016
INFO:root:[   70] Training loss: 0.01007855, Validation loss: 0.01234123, Gradient norm: 0.36531759
INFO:root:[   71] Training loss: 0.01027906, Validation loss: 0.01349454, Gradient norm: 0.40507810
INFO:root:[   72] Training loss: 0.01028954, Validation loss: 0.01217571, Gradient norm: 0.42688467
INFO:root:[   73] Training loss: 0.00987932, Validation loss: 0.01238189, Gradient norm: 0.35666727
INFO:root:[   74] Training loss: 0.01001680, Validation loss: 0.01232497, Gradient norm: 0.39353964
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 394.763s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.25434
INFO:root:EnergyScoreTrain: 0.19381
INFO:root:CoverageTrain: 0.85357
INFO:root:IntervalWidthTrain: 0.0195
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.32823
INFO:root:EnergyScoreValidation: 0.26069
INFO:root:CoverageValidation: 0.79206
INFO:root:IntervalWidthValidation: 0.01961
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.5364
INFO:root:EnergyScoreTest: 0.44346
INFO:root:CoverageTest: 0.44626
INFO:root:IntervalWidthTest: 0.02086
INFO:root:###16 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06710001, Validation loss: 0.04061781, Gradient norm: 0.32317539
INFO:root:[    2] Training loss: 0.03677090, Validation loss: 0.03366675, Gradient norm: 0.28255725
INFO:root:[    3] Training loss: 0.03084610, Validation loss: 0.03012097, Gradient norm: 0.28360808
INFO:root:[    4] Training loss: 0.02759187, Validation loss: 0.02734564, Gradient norm: 0.32801811
INFO:root:[    5] Training loss: 0.02567091, Validation loss: 0.02603247, Gradient norm: 0.32735208
INFO:root:[    6] Training loss: 0.02440489, Validation loss: 0.02431762, Gradient norm: 0.33578097
INFO:root:[    7] Training loss: 0.02301012, Validation loss: 0.02241833, Gradient norm: 0.34246731
INFO:root:[    8] Training loss: 0.02172738, Validation loss: 0.02187077, Gradient norm: 0.29470795
INFO:root:[    9] Training loss: 0.02149146, Validation loss: 0.02036833, Gradient norm: 0.36305517
INFO:root:[   10] Training loss: 0.01999081, Validation loss: 0.02075796, Gradient norm: 0.27471067
INFO:root:[   11] Training loss: 0.01957814, Validation loss: 0.01921154, Gradient norm: 0.32323736
INFO:root:[   12] Training loss: 0.01902995, Validation loss: 0.02125177, Gradient norm: 0.32756080
INFO:root:[   13] Training loss: 0.01888572, Validation loss: 0.01925786, Gradient norm: 0.37711782
INFO:root:[   14] Training loss: 0.01795039, Validation loss: 0.01803432, Gradient norm: 0.29901077
INFO:root:[   15] Training loss: 0.01753154, Validation loss: 0.01807122, Gradient norm: 0.29531667
INFO:root:[   16] Training loss: 0.01763423, Validation loss: 0.01756656, Gradient norm: 0.36802958
INFO:root:[   17] Training loss: 0.01706113, Validation loss: 0.01718996, Gradient norm: 0.32193967
INFO:root:[   18] Training loss: 0.01738970, Validation loss: 0.01738555, Gradient norm: 0.39469675
INFO:root:[   19] Training loss: 0.01662079, Validation loss: 0.01676646, Gradient norm: 0.32887808
INFO:root:[   20] Training loss: 0.01648282, Validation loss: 0.01658596, Gradient norm: 0.34989527
INFO:root:[   21] Training loss: 0.01623692, Validation loss: 0.01672927, Gradient norm: 0.34507230
INFO:root:[   22] Training loss: 0.01649737, Validation loss: 0.01621833, Gradient norm: 0.41160051
INFO:root:[   23] Training loss: 0.01559458, Validation loss: 0.01608253, Gradient norm: 0.27069167
INFO:root:[   24] Training loss: 0.01543550, Validation loss: 0.01619628, Gradient norm: 0.31270421
INFO:root:[   25] Training loss: 0.01568514, Validation loss: 0.01626323, Gradient norm: 0.37669259
INFO:root:[   26] Training loss: 0.01535235, Validation loss: 0.01607960, Gradient norm: 0.34254637
INFO:root:[   27] Training loss: 0.01507025, Validation loss: 0.01549454, Gradient norm: 0.35105956
INFO:root:[   28] Training loss: 0.01548675, Validation loss: 0.01753461, Gradient norm: 0.43179769
INFO:root:[   29] Training loss: 0.01487274, Validation loss: 0.01633846, Gradient norm: 0.37259770
INFO:root:[   30] Training loss: 0.01483626, Validation loss: 0.01518230, Gradient norm: 0.36035101
INFO:root:[   31] Training loss: 0.01454246, Validation loss: 0.01487313, Gradient norm: 0.32929386
INFO:root:[   32] Training loss: 0.01455451, Validation loss: 0.01511612, Gradient norm: 0.36447750
INFO:root:[   33] Training loss: 0.01441849, Validation loss: 0.01651125, Gradient norm: 0.35457332
INFO:root:[   34] Training loss: 0.01424246, Validation loss: 0.01605477, Gradient norm: 0.34973630
INFO:root:[   35] Training loss: 0.01429309, Validation loss: 0.01478822, Gradient norm: 0.39503149
INFO:root:[   36] Training loss: 0.01440508, Validation loss: 0.01458365, Gradient norm: 0.44405713
INFO:root:[   37] Training loss: 0.01377912, Validation loss: 0.01436384, Gradient norm: 0.29915724
INFO:root:[   38] Training loss: 0.01382741, Validation loss: 0.01425061, Gradient norm: 0.35122953
INFO:root:[   39] Training loss: 0.01375307, Validation loss: 0.01454724, Gradient norm: 0.34993300
INFO:root:[   40] Training loss: 0.01365629, Validation loss: 0.01415038, Gradient norm: 0.32461892
INFO:root:[   41] Training loss: 0.01388062, Validation loss: 0.01427087, Gradient norm: 0.42181401
INFO:root:[   42] Training loss: 0.01381104, Validation loss: 0.01493469, Gradient norm: 0.38968346
INFO:root:[   43] Training loss: 0.01349494, Validation loss: 0.01420722, Gradient norm: 0.37314433
INFO:root:[   44] Training loss: 0.01323675, Validation loss: 0.01399658, Gradient norm: 0.35803573
INFO:root:[   45] Training loss: 0.01311741, Validation loss: 0.01378803, Gradient norm: 0.31966194
INFO:root:[   46] Training loss: 0.01307453, Validation loss: 0.01399451, Gradient norm: 0.32841652
INFO:root:[   47] Training loss: 0.01317072, Validation loss: 0.01383618, Gradient norm: 0.38584973
INFO:root:[   48] Training loss: 0.01291998, Validation loss: 0.01511439, Gradient norm: 0.33932937
INFO:root:[   49] Training loss: 0.01306461, Validation loss: 0.01428353, Gradient norm: 0.37071228
INFO:root:[   50] Training loss: 0.01305872, Validation loss: 0.01387450, Gradient norm: 0.40828245
INFO:root:[   51] Training loss: 0.01286633, Validation loss: 0.01369526, Gradient norm: 0.36356596
INFO:root:[   52] Training loss: 0.01271383, Validation loss: 0.01388123, Gradient norm: 0.36300682
INFO:root:[   53] Training loss: 0.01278398, Validation loss: 0.01340021, Gradient norm: 0.39354433
INFO:root:[   54] Training loss: 0.01247766, Validation loss: 0.01364658, Gradient norm: 0.33533924
INFO:root:[   55] Training loss: 0.01244114, Validation loss: 0.01331051, Gradient norm: 0.33189849
INFO:root:[   56] Training loss: 0.01275336, Validation loss: 0.01370966, Gradient norm: 0.40965182
INFO:root:[   57] Training loss: 0.01228648, Validation loss: 0.01385485, Gradient norm: 0.34957033
INFO:root:[   58] Training loss: 0.01251373, Validation loss: 0.01328206, Gradient norm: 0.38256863
INFO:root:[   59] Training loss: 0.01217788, Validation loss: 0.01337914, Gradient norm: 0.36245546
INFO:root:[   60] Training loss: 0.01215205, Validation loss: 0.01286400, Gradient norm: 0.32877349
INFO:root:[   61] Training loss: 0.01223108, Validation loss: 0.01452540, Gradient norm: 0.37439792
INFO:root:[   62] Training loss: 0.01224803, Validation loss: 0.01328873, Gradient norm: 0.37480341
INFO:root:[   63] Training loss: 0.01200109, Validation loss: 0.01297325, Gradient norm: 0.35539056
INFO:root:[   64] Training loss: 0.01249381, Validation loss: 0.01484219, Gradient norm: 0.44021993
INFO:root:[   65] Training loss: 0.01217301, Validation loss: 0.01402058, Gradient norm: 0.41800423
INFO:root:[   66] Training loss: 0.01196765, Validation loss: 0.01332721, Gradient norm: 0.37829386
INFO:root:[   67] Training loss: 0.01174071, Validation loss: 0.01302096, Gradient norm: 0.31939671
INFO:root:[   68] Training loss: 0.01185835, Validation loss: 0.01297184, Gradient norm: 0.36489042
INFO:root:[   69] Training loss: 0.01172009, Validation loss: 0.01312039, Gradient norm: 0.34221505
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 369.044s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.2774
INFO:root:EnergyScoreTrain: 0.20662
INFO:root:CoverageTrain: 0.91813
INFO:root:IntervalWidthTrain: 0.02727
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.32733
INFO:root:EnergyScoreValidation: 0.24932
INFO:root:CoverageValidation: 0.88363
INFO:root:IntervalWidthValidation: 0.02748
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.50704
INFO:root:EnergyScoreTest: 0.39178
INFO:root:CoverageTest: 0.65655
INFO:root:IntervalWidthTest: 0.02983
INFO:root:###17 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06550095, Validation loss: 0.04377302, Gradient norm: 0.27506894
INFO:root:[    2] Training loss: 0.04027172, Validation loss: 0.03664853, Gradient norm: 0.26702138
INFO:root:[    3] Training loss: 0.03470845, Validation loss: 0.03344140, Gradient norm: 0.30033619
INFO:root:[    4] Training loss: 0.03117147, Validation loss: 0.02980266, Gradient norm: 0.26631227
INFO:root:[    5] Training loss: 0.02899162, Validation loss: 0.02886613, Gradient norm: 0.29314020
INFO:root:[    6] Training loss: 0.02714591, Validation loss: 0.02667256, Gradient norm: 0.28421906
INFO:root:[    7] Training loss: 0.02607016, Validation loss: 0.02529843, Gradient norm: 0.32007117
INFO:root:[    8] Training loss: 0.02469684, Validation loss: 0.02469907, Gradient norm: 0.27326665
INFO:root:[    9] Training loss: 0.02356836, Validation loss: 0.02321376, Gradient norm: 0.30942650
INFO:root:[   10] Training loss: 0.02275887, Validation loss: 0.02221295, Gradient norm: 0.30446828
INFO:root:[   11] Training loss: 0.02197854, Validation loss: 0.02171194, Gradient norm: 0.33780198
INFO:root:[   12] Training loss: 0.02140485, Validation loss: 0.02099866, Gradient norm: 0.31508528
INFO:root:[   13] Training loss: 0.02102016, Validation loss: 0.02048977, Gradient norm: 0.33094861
INFO:root:[   14] Training loss: 0.02017028, Validation loss: 0.02023668, Gradient norm: 0.29533757
INFO:root:[   15] Training loss: 0.01966097, Validation loss: 0.02003445, Gradient norm: 0.32252352
INFO:root:[   16] Training loss: 0.01931945, Validation loss: 0.02046146, Gradient norm: 0.32379315
INFO:root:[   17] Training loss: 0.01888352, Validation loss: 0.01869418, Gradient norm: 0.30251334
INFO:root:[   18] Training loss: 0.01857852, Validation loss: 0.01853042, Gradient norm: 0.31526293
INFO:root:[   19] Training loss: 0.01836880, Validation loss: 0.01835909, Gradient norm: 0.32483117
INFO:root:[   20] Training loss: 0.01812573, Validation loss: 0.01938608, Gradient norm: 0.34717530
INFO:root:[   21] Training loss: 0.01807430, Validation loss: 0.02134376, Gradient norm: 0.37789658
INFO:root:[   22] Training loss: 0.01764076, Validation loss: 0.01787555, Gradient norm: 0.34839415
INFO:root:[   23] Training loss: 0.01729363, Validation loss: 0.01707327, Gradient norm: 0.31816177
INFO:root:[   24] Training loss: 0.01704901, Validation loss: 0.01849726, Gradient norm: 0.35135319
INFO:root:[   25] Training loss: 0.01702260, Validation loss: 0.01889448, Gradient norm: 0.36203664
INFO:root:[   26] Training loss: 0.01695568, Validation loss: 0.01902084, Gradient norm: 0.40724704
INFO:root:[   27] Training loss: 0.01638254, Validation loss: 0.01661440, Gradient norm: 0.32114848
INFO:root:[   28] Training loss: 0.01639243, Validation loss: 0.01664875, Gradient norm: 0.37926135
INFO:root:[   29] Training loss: 0.01624449, Validation loss: 0.01661156, Gradient norm: 0.38811526
INFO:root:[   30] Training loss: 0.01598097, Validation loss: 0.01656480, Gradient norm: 0.37237769
INFO:root:[   31] Training loss: 0.01576351, Validation loss: 0.01769819, Gradient norm: 0.34772318
INFO:root:[   32] Training loss: 0.01582167, Validation loss: 0.01607652, Gradient norm: 0.40153509
INFO:root:[   33] Training loss: 0.01576329, Validation loss: 0.01624421, Gradient norm: 0.41264067
INFO:root:[   34] Training loss: 0.01533578, Validation loss: 0.01732869, Gradient norm: 0.35183605
INFO:root:[   35] Training loss: 0.01560389, Validation loss: 0.01533053, Gradient norm: 0.41822940
INFO:root:[   36] Training loss: 0.01523484, Validation loss: 0.01597963, Gradient norm: 0.37232118
INFO:root:[   37] Training loss: 0.01498287, Validation loss: 0.01608657, Gradient norm: 0.35812238
INFO:root:[   38] Training loss: 0.01538330, Validation loss: 0.01710584, Gradient norm: 0.44477584
INFO:root:[   39] Training loss: 0.01508336, Validation loss: 0.01536521, Gradient norm: 0.41758286
INFO:root:[   40] Training loss: 0.01501794, Validation loss: 0.01624417, Gradient norm: 0.43633292
INFO:root:[   41] Training loss: 0.01493701, Validation loss: 0.01599430, Gradient norm: 0.43625740
INFO:root:[   42] Training loss: 0.01445880, Validation loss: 0.01533938, Gradient norm: 0.33474444
INFO:root:[   43] Training loss: 0.01435617, Validation loss: 0.01483331, Gradient norm: 0.37800876
INFO:root:[   44] Training loss: 0.01432435, Validation loss: 0.01461217, Gradient norm: 0.37598843
INFO:root:[   45] Training loss: 0.01421779, Validation loss: 0.01533333, Gradient norm: 0.34819800
INFO:root:[   46] Training loss: 0.01477335, Validation loss: 0.01450470, Gradient norm: 0.51284471
INFO:root:[   47] Training loss: 0.01459524, Validation loss: 0.01479040, Gradient norm: 0.48955119
INFO:root:[   48] Training loss: 0.01401510, Validation loss: 0.01654179, Gradient norm: 0.39292273
INFO:root:[   49] Training loss: 0.01391070, Validation loss: 0.01432708, Gradient norm: 0.38067015
INFO:root:[   50] Training loss: 0.01362257, Validation loss: 0.01429197, Gradient norm: 0.35328089
INFO:root:[   51] Training loss: 0.01374691, Validation loss: 0.01556087, Gradient norm: 0.38240539
INFO:root:[   52] Training loss: 0.01368072, Validation loss: 0.01426818, Gradient norm: 0.39994524
INFO:root:[   53] Training loss: 0.01367564, Validation loss: 0.01494975, Gradient norm: 0.40015308
INFO:root:[   54] Training loss: 0.01332836, Validation loss: 0.01377465, Gradient norm: 0.37802631
INFO:root:[   55] Training loss: 0.01375021, Validation loss: 0.01420685, Gradient norm: 0.46146860
INFO:root:[   56] Training loss: 0.01336290, Validation loss: 0.01414506, Gradient norm: 0.38965657
INFO:root:[   57] Training loss: 0.01356118, Validation loss: 0.01440876, Gradient norm: 0.41882421
INFO:root:[   58] Training loss: 0.01326030, Validation loss: 0.01710233, Gradient norm: 0.40183658
INFO:root:[   59] Training loss: 0.01340375, Validation loss: 0.01365113, Gradient norm: 0.41113370
INFO:root:[   60] Training loss: 0.01319639, Validation loss: 0.01355608, Gradient norm: 0.39248564
INFO:root:[   61] Training loss: 0.01309263, Validation loss: 0.01364403, Gradient norm: 0.36993443
INFO:root:[   62] Training loss: 0.01311040, Validation loss: 0.01425918, Gradient norm: 0.41686688
INFO:root:[   63] Training loss: 0.01290970, Validation loss: 0.01422623, Gradient norm: 0.37337336
INFO:root:[   64] Training loss: 0.01299871, Validation loss: 0.01372422, Gradient norm: 0.41796611
INFO:root:[   65] Training loss: 0.01289621, Validation loss: 0.01383827, Gradient norm: 0.38576969
INFO:root:[   66] Training loss: 0.01262574, Validation loss: 0.01343023, Gradient norm: 0.36891417
INFO:root:[   67] Training loss: 0.01284286, Validation loss: 0.01363414, Gradient norm: 0.42129128
INFO:root:[   68] Training loss: 0.01264249, Validation loss: 0.01355138, Gradient norm: 0.37035212
INFO:root:[   69] Training loss: 0.01276661, Validation loss: 0.01344658, Gradient norm: 0.41064365
INFO:root:[   70] Training loss: 0.01256640, Validation loss: 0.01387498, Gradient norm: 0.38397525
INFO:root:[   71] Training loss: 0.01280481, Validation loss: 0.01370921, Gradient norm: 0.43611948
INFO:root:[   72] Training loss: 0.01260041, Validation loss: 0.01343813, Gradient norm: 0.39963712
INFO:root:[   73] Training loss: 0.01258310, Validation loss: 0.01493068, Gradient norm: 0.41824188
INFO:root:[   74] Training loss: 0.01252246, Validation loss: 0.01418943, Gradient norm: 0.40512111
INFO:root:[   75] Training loss: 0.01240953, Validation loss: 0.01341030, Gradient norm: 0.39182005
INFO:root:[   76] Training loss: 0.01222327, Validation loss: 0.01345684, Gradient norm: 0.36732918
INFO:root:[   77] Training loss: 0.01243396, Validation loss: 0.01353879, Gradient norm: 0.41325925
INFO:root:[   78] Training loss: 0.01223333, Validation loss: 0.01308698, Gradient norm: 0.39093225
INFO:root:[   79] Training loss: 0.01219830, Validation loss: 0.01350474, Gradient norm: 0.40488389
INFO:root:[   80] Training loss: 0.01196863, Validation loss: 0.01307585, Gradient norm: 0.34729066
INFO:root:[   81] Training loss: 0.01196177, Validation loss: 0.01300156, Gradient norm: 0.36502537
INFO:root:[   82] Training loss: 0.01202171, Validation loss: 0.01453632, Gradient norm: 0.38383487
INFO:root:[   83] Training loss: 0.01213630, Validation loss: 0.01386242, Gradient norm: 0.40730258
INFO:root:[   84] Training loss: 0.01206278, Validation loss: 0.01308925, Gradient norm: 0.39938632
INFO:root:[   85] Training loss: 0.01194633, Validation loss: 0.01355328, Gradient norm: 0.37257349
INFO:root:[   86] Training loss: 0.01178515, Validation loss: 0.01373964, Gradient norm: 0.32719431
INFO:root:[   87] Training loss: 0.01164561, Validation loss: 0.01302508, Gradient norm: 0.32576877
INFO:root:[   88] Training loss: 0.01174980, Validation loss: 0.01529832, Gradient norm: 0.38677500
INFO:root:[   89] Training loss: 0.01164583, Validation loss: 0.01312824, Gradient norm: 0.34651464
INFO:root:[   90] Training loss: 0.01181901, Validation loss: 0.01328288, Gradient norm: 0.37628888
INFO:root:EP 90: Early stopping
INFO:root:Training the model took 451.778s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.26829
INFO:root:EnergyScoreTrain: 0.19946
INFO:root:CoverageTrain: 0.93938
INFO:root:IntervalWidthTrain: 0.02869
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.31975
INFO:root:EnergyScoreValidation: 0.24305
INFO:root:CoverageValidation: 0.90976
INFO:root:IntervalWidthValidation: 0.0289
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.50697
INFO:root:EnergyScoreTest: 0.38668
INFO:root:CoverageTest: 0.76041
INFO:root:IntervalWidthTest: 0.03351
INFO:root:###18 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07010355, Validation loss: 0.04674795, Gradient norm: 0.30368130
INFO:root:[    2] Training loss: 0.04355522, Validation loss: 0.04012939, Gradient norm: 0.27899879
INFO:root:[    3] Training loss: 0.03784965, Validation loss: 0.03569912, Gradient norm: 0.25849459
INFO:root:[    4] Training loss: 0.03376228, Validation loss: 0.03357803, Gradient norm: 0.27099853
INFO:root:[    5] Training loss: 0.03125184, Validation loss: 0.03043680, Gradient norm: 0.32433973
INFO:root:[    6] Training loss: 0.02955393, Validation loss: 0.02868387, Gradient norm: 0.30978065
INFO:root:[    7] Training loss: 0.02763458, Validation loss: 0.02867802, Gradient norm: 0.28058772
INFO:root:[    8] Training loss: 0.02637003, Validation loss: 0.02681750, Gradient norm: 0.26809507
INFO:root:[    9] Training loss: 0.02516717, Validation loss: 0.02473120, Gradient norm: 0.28672840
INFO:root:[   10] Training loss: 0.02440698, Validation loss: 0.02372693, Gradient norm: 0.32274099
INFO:root:[   11] Training loss: 0.02368211, Validation loss: 0.02328715, Gradient norm: 0.31126769
INFO:root:[   12] Training loss: 0.02307372, Validation loss: 0.02292877, Gradient norm: 0.33565446
INFO:root:[   13] Training loss: 0.02224817, Validation loss: 0.02218025, Gradient norm: 0.31116769
INFO:root:[   14] Training loss: 0.02181419, Validation loss: 0.02134676, Gradient norm: 0.33818163
INFO:root:[   15] Training loss: 0.02109929, Validation loss: 0.02111878, Gradient norm: 0.28315286
INFO:root:[   16] Training loss: 0.02088346, Validation loss: 0.02057543, Gradient norm: 0.32040442
INFO:root:[   17] Training loss: 0.02030675, Validation loss: 0.02032666, Gradient norm: 0.30940741
INFO:root:[   18] Training loss: 0.01998690, Validation loss: 0.02050495, Gradient norm: 0.34055391
INFO:root:[   19] Training loss: 0.01962215, Validation loss: 0.02078875, Gradient norm: 0.31051510
INFO:root:[   20] Training loss: 0.01975447, Validation loss: 0.02280149, Gradient norm: 0.37633971
INFO:root:[   21] Training loss: 0.01932875, Validation loss: 0.01941947, Gradient norm: 0.37235394
INFO:root:[   22] Training loss: 0.01873189, Validation loss: 0.01892690, Gradient norm: 0.30533994
INFO:root:[   23] Training loss: 0.01866741, Validation loss: 0.01871255, Gradient norm: 0.36327489
INFO:root:[   24] Training loss: 0.01817625, Validation loss: 0.01842401, Gradient norm: 0.29000852
INFO:root:[   25] Training loss: 0.01833607, Validation loss: 0.01796138, Gradient norm: 0.34753833
INFO:root:[   26] Training loss: 0.01784339, Validation loss: 0.01897744, Gradient norm: 0.32358199
INFO:root:[   27] Training loss: 0.01789757, Validation loss: 0.01839488, Gradient norm: 0.33637132
INFO:root:[   28] Training loss: 0.01764112, Validation loss: 0.01762273, Gradient norm: 0.35521044
INFO:root:[   29] Training loss: 0.01731305, Validation loss: 0.01820069, Gradient norm: 0.32505267
INFO:root:[   30] Training loss: 0.01737360, Validation loss: 0.01736723, Gradient norm: 0.38171926
INFO:root:[   31] Training loss: 0.01696394, Validation loss: 0.01826870, Gradient norm: 0.34552679
INFO:root:[   32] Training loss: 0.01721797, Validation loss: 0.01688426, Gradient norm: 0.42126184
INFO:root:[   33] Training loss: 0.01662920, Validation loss: 0.01672899, Gradient norm: 0.35665014
INFO:root:[   34] Training loss: 0.01723220, Validation loss: 0.01703402, Gradient norm: 0.44126064
INFO:root:[   35] Training loss: 0.01655750, Validation loss: 0.01703001, Gradient norm: 0.35743454
INFO:root:[   36] Training loss: 0.01648274, Validation loss: 0.01670292, Gradient norm: 0.34841264
INFO:root:[   37] Training loss: 0.01632175, Validation loss: 0.01639784, Gradient norm: 0.36220777
INFO:root:[   38] Training loss: 0.01656814, Validation loss: 0.01674991, Gradient norm: 0.40513959
INFO:root:[   39] Training loss: 0.01603548, Validation loss: 0.01602514, Gradient norm: 0.35499672
INFO:root:[   40] Training loss: 0.01615106, Validation loss: 0.01697393, Gradient norm: 0.38711256
INFO:root:[   41] Training loss: 0.01627485, Validation loss: 0.01669579, Gradient norm: 0.43703383
INFO:root:[   42] Training loss: 0.01585334, Validation loss: 0.01631678, Gradient norm: 0.37126461
INFO:root:[   43] Training loss: 0.01585390, Validation loss: 0.01675534, Gradient norm: 0.38904941
INFO:root:[   44] Training loss: 0.01547453, Validation loss: 0.01574595, Gradient norm: 0.33279626
INFO:root:[   45] Training loss: 0.01532891, Validation loss: 0.01626917, Gradient norm: 0.35683072
INFO:root:[   46] Training loss: 0.01544486, Validation loss: 0.01587908, Gradient norm: 0.36556349
INFO:root:[   47] Training loss: 0.01555691, Validation loss: 0.01600548, Gradient norm: 0.39761632
INFO:root:[   48] Training loss: 0.01526242, Validation loss: 0.01600347, Gradient norm: 0.34936666
INFO:root:[   49] Training loss: 0.01487544, Validation loss: 0.01590915, Gradient norm: 0.31500477
INFO:root:[   50] Training loss: 0.01496349, Validation loss: 0.01604590, Gradient norm: 0.33405108
INFO:root:[   51] Training loss: 0.01502809, Validation loss: 0.01681794, Gradient norm: 0.36431973
INFO:root:[   52] Training loss: 0.01514229, Validation loss: 0.01709652, Gradient norm: 0.40710941
INFO:root:[   53] Training loss: 0.01474668, Validation loss: 0.01695536, Gradient norm: 0.33556589
INFO:root:[   54] Training loss: 0.01487723, Validation loss: 0.01537141, Gradient norm: 0.39376233
INFO:root:[   55] Training loss: 0.01447674, Validation loss: 0.01514345, Gradient norm: 0.33412880
INFO:root:[   56] Training loss: 0.01473182, Validation loss: 0.01494361, Gradient norm: 0.36828325
INFO:root:[   57] Training loss: 0.01467748, Validation loss: 0.01548974, Gradient norm: 0.37455949
INFO:root:[   58] Training loss: 0.01458109, Validation loss: 0.01498152, Gradient norm: 0.39694010
INFO:root:[   59] Training loss: 0.01460807, Validation loss: 0.01499143, Gradient norm: 0.37893440
INFO:root:[   60] Training loss: 0.01432090, Validation loss: 0.01538877, Gradient norm: 0.35435753
INFO:root:[   61] Training loss: 0.01452036, Validation loss: 0.01528311, Gradient norm: 0.40420314
INFO:root:[   62] Training loss: 0.01430386, Validation loss: 0.01471357, Gradient norm: 0.36958042
INFO:root:[   63] Training loss: 0.01398069, Validation loss: 0.01477087, Gradient norm: 0.34786744
INFO:root:[   64] Training loss: 0.01423421, Validation loss: 0.01462817, Gradient norm: 0.38326158
INFO:root:[   65] Training loss: 0.01416135, Validation loss: 0.01471488, Gradient norm: 0.36920821
INFO:root:[   66] Training loss: 0.01414259, Validation loss: 0.01518719, Gradient norm: 0.39321306
INFO:root:[   67] Training loss: 0.01402325, Validation loss: 0.01437192, Gradient norm: 0.36578376
INFO:root:[   68] Training loss: 0.01384460, Validation loss: 0.01468464, Gradient norm: 0.31600575
INFO:root:[   69] Training loss: 0.01382511, Validation loss: 0.01506165, Gradient norm: 0.32492604
INFO:root:[   70] Training loss: 0.01400488, Validation loss: 0.01444764, Gradient norm: 0.36677559
INFO:root:[   71] Training loss: 0.01383408, Validation loss: 0.01420622, Gradient norm: 0.35516322
INFO:root:[   72] Training loss: 0.01348692, Validation loss: 0.01594300, Gradient norm: 0.31654249
INFO:root:[   73] Training loss: 0.01391642, Validation loss: 0.01483722, Gradient norm: 0.39711503
INFO:root:[   74] Training loss: 0.01330225, Validation loss: 0.01418433, Gradient norm: 0.28515509
INFO:root:[   75] Training loss: 0.01390022, Validation loss: 0.01532815, Gradient norm: 0.38621944
INFO:root:[   76] Training loss: 0.01341611, Validation loss: 0.01434234, Gradient norm: 0.34233881
INFO:root:[   77] Training loss: 0.01322525, Validation loss: 0.01447676, Gradient norm: 0.29858541
INFO:root:[   78] Training loss: 0.01317662, Validation loss: 0.01588342, Gradient norm: 0.30751976
INFO:root:[   79] Training loss: 0.01344012, Validation loss: 0.01552938, Gradient norm: 0.37834685
INFO:root:[   80] Training loss: 0.01356761, Validation loss: 0.01428106, Gradient norm: 0.36830298
INFO:root:[   81] Training loss: 0.01348244, Validation loss: 0.01439277, Gradient norm: 0.39199875
INFO:root:[   82] Training loss: 0.01312604, Validation loss: 0.01386065, Gradient norm: 0.31294130
INFO:root:[   83] Training loss: 0.01340051, Validation loss: 0.01406269, Gradient norm: 0.36703993
INFO:root:[   84] Training loss: 0.01297056, Validation loss: 0.01386983, Gradient norm: 0.31116274
INFO:root:[   85] Training loss: 0.01303173, Validation loss: 0.01399265, Gradient norm: 0.32578037
INFO:root:[   86] Training loss: 0.01290372, Validation loss: 0.01448887, Gradient norm: 0.30640133
INFO:root:[   87] Training loss: 0.01310129, Validation loss: 0.01641316, Gradient norm: 0.34317434
INFO:root:[   88] Training loss: 0.01285361, Validation loss: 0.01375717, Gradient norm: 0.32657859
INFO:root:[   89] Training loss: 0.01289748, Validation loss: 0.01381992, Gradient norm: 0.34277214
INFO:root:[   90] Training loss: 0.01287549, Validation loss: 0.01368566, Gradient norm: 0.33830509
INFO:root:[   91] Training loss: 0.01271473, Validation loss: 0.01357015, Gradient norm: 0.31730572
INFO:root:[   92] Training loss: 0.01264793, Validation loss: 0.01423594, Gradient norm: 0.33966220
INFO:root:[   93] Training loss: 0.01238779, Validation loss: 0.01368661, Gradient norm: 0.27819022
INFO:root:[   94] Training loss: 0.01245097, Validation loss: 0.01338161, Gradient norm: 0.28188843
INFO:root:[   95] Training loss: 0.01258604, Validation loss: 0.01361477, Gradient norm: 0.32096548
INFO:root:[   96] Training loss: 0.01273883, Validation loss: 0.01373175, Gradient norm: 0.35262603
INFO:root:[   97] Training loss: 0.01253916, Validation loss: 0.01337980, Gradient norm: 0.30586115
INFO:root:[   98] Training loss: 0.01238697, Validation loss: 0.01354511, Gradient norm: 0.29506285
INFO:root:[   99] Training loss: 0.01246630, Validation loss: 0.01483770, Gradient norm: 0.31637996
INFO:root:[  100] Training loss: 0.01234798, Validation loss: 0.01353162, Gradient norm: 0.30769243
INFO:root:[  101] Training loss: 0.01243539, Validation loss: 0.01337516, Gradient norm: 0.32526952
INFO:root:[  102] Training loss: 0.01218645, Validation loss: 0.01350448, Gradient norm: 0.29078887
INFO:root:[  103] Training loss: 0.01226914, Validation loss: 0.01361839, Gradient norm: 0.29896801
INFO:root:[  104] Training loss: 0.01211953, Validation loss: 0.01381869, Gradient norm: 0.29583752
INFO:root:[  105] Training loss: 0.01226414, Validation loss: 0.01729696, Gradient norm: 0.32668339
INFO:root:[  106] Training loss: 0.01222557, Validation loss: 0.01361315, Gradient norm: 0.32891853
INFO:root:[  107] Training loss: 0.01207943, Validation loss: 0.01324649, Gradient norm: 0.29720161
INFO:root:[  108] Training loss: 0.01206574, Validation loss: 0.01326032, Gradient norm: 0.31371913
INFO:root:[  109] Training loss: 0.01191451, Validation loss: 0.01349006, Gradient norm: 0.28117747
INFO:root:[  110] Training loss: 0.01184426, Validation loss: 0.01403402, Gradient norm: 0.26014398
INFO:root:[  111] Training loss: 0.01207409, Validation loss: 0.01349541, Gradient norm: 0.34355174
INFO:root:[  112] Training loss: 0.01184220, Validation loss: 0.01386259, Gradient norm: 0.30587534
INFO:root:[  113] Training loss: 0.01201394, Validation loss: 0.01325773, Gradient norm: 0.33466947
INFO:root:[  114] Training loss: 0.01169686, Validation loss: 0.01379810, Gradient norm: 0.28006853
INFO:root:[  115] Training loss: 0.01191009, Validation loss: 0.01357553, Gradient norm: 0.30694145
INFO:root:[  116] Training loss: 0.01167974, Validation loss: 0.01319389, Gradient norm: 0.29231213
INFO:root:[  117] Training loss: 0.01165941, Validation loss: 0.01307712, Gradient norm: 0.28394059
INFO:root:[  118] Training loss: 0.01155032, Validation loss: 0.01367530, Gradient norm: 0.26000712
INFO:root:[  119] Training loss: 0.01158093, Validation loss: 0.01368815, Gradient norm: 0.27306352
INFO:root:[  120] Training loss: 0.01160048, Validation loss: 0.01297927, Gradient norm: 0.27502544
INFO:root:[  121] Training loss: 0.01168102, Validation loss: 0.01292837, Gradient norm: 0.29360866
INFO:root:[  122] Training loss: 0.01163901, Validation loss: 0.01429260, Gradient norm: 0.29276935
INFO:root:[  123] Training loss: 0.01163359, Validation loss: 0.01281460, Gradient norm: 0.32180082
INFO:root:[  124] Training loss: 0.01142688, Validation loss: 0.01315529, Gradient norm: 0.28602863
INFO:root:[  125] Training loss: 0.01131424, Validation loss: 0.01304360, Gradient norm: 0.27650854
INFO:root:[  126] Training loss: 0.01154298, Validation loss: 0.01328986, Gradient norm: 0.32512069
INFO:root:[  127] Training loss: 0.01128136, Validation loss: 0.01302923, Gradient norm: 0.28294172
INFO:root:[  128] Training loss: 0.01117106, Validation loss: 0.01310481, Gradient norm: 0.25967774
INFO:root:[  129] Training loss: 0.01139476, Validation loss: 0.01364493, Gradient norm: 0.32085648
INFO:root:[  130] Training loss: 0.01117346, Validation loss: 0.01279197, Gradient norm: 0.26508483
INFO:root:[  131] Training loss: 0.01100442, Validation loss: 0.01330886, Gradient norm: 0.25766997
INFO:root:[  132] Training loss: 0.01132949, Validation loss: 0.01290253, Gradient norm: 0.30432451
INFO:root:[  133] Training loss: 0.01138130, Validation loss: 0.01329917, Gradient norm: 0.30476787
INFO:root:[  134] Training loss: 0.01123335, Validation loss: 0.01302270, Gradient norm: 0.29386352
INFO:root:[  135] Training loss: 0.01101824, Validation loss: 0.01308902, Gradient norm: 0.26771673
INFO:root:[  136] Training loss: 0.01116090, Validation loss: 0.01266820, Gradient norm: 0.31436095
INFO:root:[  137] Training loss: 0.01099012, Validation loss: 0.01288265, Gradient norm: 0.26131796
INFO:root:[  138] Training loss: 0.01125504, Validation loss: 0.01295953, Gradient norm: 0.31636004
INFO:root:[  139] Training loss: 0.01103706, Validation loss: 0.01334614, Gradient norm: 0.30605982
INFO:root:[  140] Training loss: 0.01098031, Validation loss: 0.01268862, Gradient norm: 0.28871851
INFO:root:[  141] Training loss: 0.01093558, Validation loss: 0.01294284, Gradient norm: 0.25868270
INFO:root:[  142] Training loss: 0.01097856, Validation loss: 0.01455450, Gradient norm: 0.29352051
INFO:root:[  143] Training loss: 0.01093031, Validation loss: 0.01289475, Gradient norm: 0.28520370
INFO:root:[  144] Training loss: 0.01107856, Validation loss: 0.01297156, Gradient norm: 0.32764749
INFO:root:[  145] Training loss: 0.01092695, Validation loss: 0.01366482, Gradient norm: 0.31472624
INFO:root:[  146] Training loss: 0.01083377, Validation loss: 0.01263542, Gradient norm: 0.27539817
INFO:root:[  147] Training loss: 0.01075997, Validation loss: 0.01254398, Gradient norm: 0.26857306
INFO:root:[  148] Training loss: 0.01081043, Validation loss: 0.01290642, Gradient norm: 0.29087227
INFO:root:[  149] Training loss: 0.01063494, Validation loss: 0.01291104, Gradient norm: 0.26506593
INFO:root:[  150] Training loss: 0.01078499, Validation loss: 0.01285855, Gradient norm: 0.28750449
INFO:root:[  151] Training loss: 0.01042510, Validation loss: 0.01261364, Gradient norm: 0.23917532
INFO:root:[  152] Training loss: 0.01058048, Validation loss: 0.01258581, Gradient norm: 0.26871256
INFO:root:[  153] Training loss: 0.01065581, Validation loss: 0.01300068, Gradient norm: 0.27605225
INFO:root:[  154] Training loss: 0.01059355, Validation loss: 0.01276226, Gradient norm: 0.27668451
INFO:root:[  155] Training loss: 0.01073251, Validation loss: 0.01295786, Gradient norm: 0.29897706
INFO:root:[  156] Training loss: 0.01060224, Validation loss: 0.01249570, Gradient norm: 0.28899158
INFO:root:[  157] Training loss: 0.01067098, Validation loss: 0.01266077, Gradient norm: 0.31287652
INFO:root:[  158] Training loss: 0.01037092, Validation loss: 0.01224239, Gradient norm: 0.25528049
INFO:root:[  159] Training loss: 0.01046150, Validation loss: 0.01291643, Gradient norm: 0.26541048
INFO:root:[  160] Training loss: 0.01038448, Validation loss: 0.01263458, Gradient norm: 0.24421537
INFO:root:[  161] Training loss: 0.01046116, Validation loss: 0.01275491, Gradient norm: 0.28294060
INFO:root:[  162] Training loss: 0.01048743, Validation loss: 0.01250798, Gradient norm: 0.29595585
INFO:root:[  163] Training loss: 0.01027327, Validation loss: 0.01280302, Gradient norm: 0.26349198
INFO:root:[  164] Training loss: 0.01029482, Validation loss: 0.01266469, Gradient norm: 0.29088972
INFO:root:[  165] Training loss: 0.01043860, Validation loss: 0.01250075, Gradient norm: 0.30047996
INFO:root:[  166] Training loss: 0.01019296, Validation loss: 0.01301724, Gradient norm: 0.25270876
INFO:root:[  167] Training loss: 0.01023528, Validation loss: 0.01253964, Gradient norm: 0.26222218
INFO:root:EP 167: Early stopping
INFO:root:Training the model took 831.559s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.21379
INFO:root:EnergyScoreTrain: 0.16065
INFO:root:CoverageTrain: 0.96141
INFO:root:IntervalWidthTrain: 0.02684
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.30149
INFO:root:EnergyScoreValidation: 0.23357
INFO:root:CoverageValidation: 0.91538
INFO:root:IntervalWidthValidation: 0.02699
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.74403
INFO:root:EnergyScoreTest: 0.56583
INFO:root:CoverageTest: 0.70503
INFO:root:IntervalWidthTest: 0.04852
INFO:root:###19 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 165675008
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07559524, Validation loss: 0.05157328, Gradient norm: 0.25894031
INFO:root:[    2] Training loss: 0.04667003, Validation loss: 0.04428696, Gradient norm: 0.25834312
INFO:root:[    3] Training loss: 0.04091830, Validation loss: 0.03805462, Gradient norm: 0.25378193
INFO:root:[    4] Training loss: 0.03679385, Validation loss: 0.03551722, Gradient norm: 0.27793694
INFO:root:[    5] Training loss: 0.03322989, Validation loss: 0.03643420, Gradient norm: 0.25278148
INFO:root:[    6] Training loss: 0.03147030, Validation loss: 0.03041130, Gradient norm: 0.29762099
INFO:root:[    7] Training loss: 0.02955776, Validation loss: 0.03023564, Gradient norm: 0.26497795
INFO:root:[    8] Training loss: 0.02850035, Validation loss: 0.02886025, Gradient norm: 0.28443017
INFO:root:[    9] Training loss: 0.02734865, Validation loss: 0.02675472, Gradient norm: 0.26333139
INFO:root:[   10] Training loss: 0.02602650, Validation loss: 0.02633129, Gradient norm: 0.26053360
INFO:root:[   11] Training loss: 0.02502194, Validation loss: 0.02496836, Gradient norm: 0.23333242
INFO:root:[   12] Training loss: 0.02443961, Validation loss: 0.02463430, Gradient norm: 0.26376536
INFO:root:[   13] Training loss: 0.02383672, Validation loss: 0.02375189, Gradient norm: 0.29657660
INFO:root:[   14] Training loss: 0.02321482, Validation loss: 0.02282387, Gradient norm: 0.28845054
INFO:root:[   15] Training loss: 0.02275710, Validation loss: 0.02292914, Gradient norm: 0.30286672
INFO:root:[   16] Training loss: 0.02256683, Validation loss: 0.02207378, Gradient norm: 0.35880097
INFO:root:[   17] Training loss: 0.02161376, Validation loss: 0.02211981, Gradient norm: 0.28575562
INFO:root:[   18] Training loss: 0.02082899, Validation loss: 0.02081076, Gradient norm: 0.24956721
INFO:root:[   19] Training loss: 0.02113446, Validation loss: 0.02278236, Gradient norm: 0.33370884
INFO:root:[   20] Training loss: 0.02052586, Validation loss: 0.02439144, Gradient norm: 0.29437032
INFO:root:[   21] Training loss: 0.02034705, Validation loss: 0.02192588, Gradient norm: 0.35442892
INFO:root:[   22] Training loss: 0.02006256, Validation loss: 0.02175682, Gradient norm: 0.34533903
INFO:root:[   23] Training loss: 0.01990802, Validation loss: 0.02093817, Gradient norm: 0.37057220
INFO:root:[   24] Training loss: 0.01948660, Validation loss: 0.01952004, Gradient norm: 0.35702290
INFO:root:[   25] Training loss: 0.01914075, Validation loss: 0.01979650, Gradient norm: 0.29428925
INFO:root:[   26] Training loss: 0.01927917, Validation loss: 0.01933977, Gradient norm: 0.38307324
INFO:root:[   27] Training loss: 0.01904533, Validation loss: 0.01973119, Gradient norm: 0.38243421
INFO:root:[   28] Training loss: 0.01884439, Validation loss: 0.01832258, Gradient norm: 0.38561588
INFO:root:[   29] Training loss: 0.01845494, Validation loss: 0.01889247, Gradient norm: 0.33115512
INFO:root:[   30] Training loss: 0.01819601, Validation loss: 0.01841281, Gradient norm: 0.33302269
INFO:root:[   31] Training loss: 0.01802199, Validation loss: 0.01822005, Gradient norm: 0.31960502
INFO:root:[   32] Training loss: 0.01778534, Validation loss: 0.01789007, Gradient norm: 0.29202841
INFO:root:[   33] Training loss: 0.01792362, Validation loss: 0.01879662, Gradient norm: 0.40904184
INFO:root:[   34] Training loss: 0.01757761, Validation loss: 0.01766081, Gradient norm: 0.31707757
INFO:root:[   35] Training loss: 0.01760496, Validation loss: 0.01796731, Gradient norm: 0.37011623
INFO:root:[   36] Training loss: 0.01727386, Validation loss: 0.01915984, Gradient norm: 0.33088954
INFO:root:[   37] Training loss: 0.01730656, Validation loss: 0.01716943, Gradient norm: 0.35797017
INFO:root:[   38] Training loss: 0.01735689, Validation loss: 0.01724056, Gradient norm: 0.39302523
INFO:root:[   39] Training loss: 0.01679192, Validation loss: 0.01717669, Gradient norm: 0.30522632
INFO:root:[   40] Training loss: 0.01695263, Validation loss: 0.01689034, Gradient norm: 0.38250031
INFO:root:[   41] Training loss: 0.01678685, Validation loss: 0.01714282, Gradient norm: 0.36464378
INFO:root:[   42] Training loss: 0.01673438, Validation loss: 0.01683830, Gradient norm: 0.37548447
INFO:root:[   43] Training loss: 0.01636132, Validation loss: 0.01786755, Gradient norm: 0.31654079
INFO:root:[   44] Training loss: 0.01650308, Validation loss: 0.01740706, Gradient norm: 0.37527623
INFO:root:[   45] Training loss: 0.01642140, Validation loss: 0.01694764, Gradient norm: 0.36551656
INFO:root:[   46] Training loss: 0.01639981, Validation loss: 0.01670828, Gradient norm: 0.37638916
INFO:root:[   47] Training loss: 0.01610035, Validation loss: 0.01712405, Gradient norm: 0.34031280
INFO:root:[   48] Training loss: 0.01606275, Validation loss: 0.01648317, Gradient norm: 0.33321140
INFO:root:[   49] Training loss: 0.01621033, Validation loss: 0.01610496, Gradient norm: 0.38453284
INFO:root:[   50] Training loss: 0.01619285, Validation loss: 0.01614864, Gradient norm: 0.37800768
INFO:root:[   51] Training loss: 0.01571348, Validation loss: 0.01664779, Gradient norm: 0.31529004
INFO:root:[   52] Training loss: 0.01570765, Validation loss: 0.01629579, Gradient norm: 0.33628353
INFO:root:[   53] Training loss: 0.01566964, Validation loss: 0.01658088, Gradient norm: 0.33476372
INFO:root:[   54] Training loss: 0.01560119, Validation loss: 0.01650502, Gradient norm: 0.35513338
INFO:root:[   55] Training loss: 0.01596629, Validation loss: 0.01806303, Gradient norm: 0.39724057
INFO:root:[   56] Training loss: 0.01541227, Validation loss: 0.01591872, Gradient norm: 0.32025917
INFO:root:[   57] Training loss: 0.01518026, Validation loss: 0.01661991, Gradient norm: 0.32324238
INFO:root:[   58] Training loss: 0.01513157, Validation loss: 0.01662634, Gradient norm: 0.31739209
INFO:root:[   59] Training loss: 0.01521109, Validation loss: 0.01590245, Gradient norm: 0.33182060
INFO:root:[   60] Training loss: 0.01503805, Validation loss: 0.01590592, Gradient norm: 0.32712461
INFO:root:[   61] Training loss: 0.01497478, Validation loss: 0.01596827, Gradient norm: 0.29711717
INFO:root:[   62] Training loss: 0.01501363, Validation loss: 0.01560741, Gradient norm: 0.30880141
INFO:root:[   63] Training loss: 0.01506425, Validation loss: 0.01556435, Gradient norm: 0.34666085
INFO:root:[   64] Training loss: 0.01501683, Validation loss: 0.01757223, Gradient norm: 0.33063745
INFO:root:[   65] Training loss: 0.01504236, Validation loss: 0.01506097, Gradient norm: 0.35775370
INFO:root:[   66] Training loss: 0.01474007, Validation loss: 0.01551873, Gradient norm: 0.31565955
INFO:root:[   67] Training loss: 0.01465637, Validation loss: 0.01515167, Gradient norm: 0.29605974
INFO:root:[   68] Training loss: 0.01463318, Validation loss: 0.01540619, Gradient norm: 0.35054193
INFO:root:[   69] Training loss: 0.01451109, Validation loss: 0.01478456, Gradient norm: 0.33126291
INFO:root:[   70] Training loss: 0.01448744, Validation loss: 0.01543387, Gradient norm: 0.30406277
INFO:root:[   71] Training loss: 0.01442361, Validation loss: 0.01502312, Gradient norm: 0.32732557
INFO:root:[   72] Training loss: 0.01426789, Validation loss: 0.01496686, Gradient norm: 0.29254137
INFO:root:[   73] Training loss: 0.01411547, Validation loss: 0.01500399, Gradient norm: 0.29292702
INFO:root:[   74] Training loss: 0.01424560, Validation loss: 0.01540475, Gradient norm: 0.31389597
INFO:root:[   75] Training loss: 0.01428169, Validation loss: 0.01570600, Gradient norm: 0.32732513
INFO:root:[   76] Training loss: 0.01417664, Validation loss: 0.01489407, Gradient norm: 0.32110243
INFO:root:[   77] Training loss: 0.01408172, Validation loss: 0.01514518, Gradient norm: 0.32492590
INFO:root:[   78] Training loss: 0.01407921, Validation loss: 0.01449049, Gradient norm: 0.30255685
INFO:root:[   79] Training loss: 0.01414529, Validation loss: 0.01520989, Gradient norm: 0.32478217
INFO:root:[   80] Training loss: 0.01404215, Validation loss: 0.01464265, Gradient norm: 0.33826775
INFO:root:[   81] Training loss: 0.01380464, Validation loss: 0.01462258, Gradient norm: 0.28399541
INFO:root:[   82] Training loss: 0.01382842, Validation loss: 0.01434391, Gradient norm: 0.30166363
INFO:root:[   83] Training loss: 0.01365156, Validation loss: 0.01463622, Gradient norm: 0.27796778
INFO:root:[   84] Training loss: 0.01382019, Validation loss: 0.01442717, Gradient norm: 0.32476773
INFO:root:[   85] Training loss: 0.01366889, Validation loss: 0.01562136, Gradient norm: 0.30690959
INFO:root:[   86] Training loss: 0.01354172, Validation loss: 0.01453572, Gradient norm: 0.30219198
INFO:root:[   87] Training loss: 0.01371535, Validation loss: 0.01470534, Gradient norm: 0.33453051
INFO:root:[   88] Training loss: 0.01349416, Validation loss: 0.01439388, Gradient norm: 0.28635579
INFO:root:[   89] Training loss: 0.01358649, Validation loss: 0.01429632, Gradient norm: 0.29960275
INFO:root:[   90] Training loss: 0.01339461, Validation loss: 0.01463254, Gradient norm: 0.29270015
INFO:root:[   91] Training loss: 0.01358574, Validation loss: 0.01440019, Gradient norm: 0.32587226
INFO:root:[   92] Training loss: 0.01343883, Validation loss: 0.01455023, Gradient norm: 0.30517112
INFO:root:[   93] Training loss: 0.01336093, Validation loss: 0.01442685, Gradient norm: 0.31705997
INFO:root:[   94] Training loss: 0.01328746, Validation loss: 0.01440072, Gradient norm: 0.29924994
INFO:root:[   95] Training loss: 0.01325004, Validation loss: 0.01407641, Gradient norm: 0.30440082
INFO:root:[   96] Training loss: 0.01305211, Validation loss: 0.01598806, Gradient norm: 0.27715632
INFO:root:[   97] Training loss: 0.01328923, Validation loss: 0.01427547, Gradient norm: 0.32384886
INFO:root:[   98] Training loss: 0.01325359, Validation loss: 0.01547082, Gradient norm: 0.31436848
INFO:root:[   99] Training loss: 0.01312723, Validation loss: 0.01543300, Gradient norm: 0.29103738
INFO:root:[  100] Training loss: 0.01302309, Validation loss: 0.01552864, Gradient norm: 0.30337229
INFO:root:[  101] Training loss: 0.01300133, Validation loss: 0.01396458, Gradient norm: 0.30125925
INFO:root:[  102] Training loss: 0.01303893, Validation loss: 0.01491518, Gradient norm: 0.31970660
INFO:root:[  103] Training loss: 0.01298498, Validation loss: 0.01382740, Gradient norm: 0.28415304
INFO:root:[  104] Training loss: 0.01290886, Validation loss: 0.01364955, Gradient norm: 0.29609155
INFO:root:[  105] Training loss: 0.01268856, Validation loss: 0.01397679, Gradient norm: 0.25920741
INFO:root:[  106] Training loss: 0.01281776, Validation loss: 0.01447543, Gradient norm: 0.29372486
INFO:root:[  107] Training loss: 0.01294172, Validation loss: 0.01417388, Gradient norm: 0.30954278
INFO:root:[  108] Training loss: 0.01259722, Validation loss: 0.01373751, Gradient norm: 0.28307749
INFO:root:[  109] Training loss: 0.01248910, Validation loss: 0.01383734, Gradient norm: 0.27827287
INFO:root:[  110] Training loss: 0.01268252, Validation loss: 0.01433405, Gradient norm: 0.29546249
INFO:root:[  111] Training loss: 0.01256688, Validation loss: 0.01434536, Gradient norm: 0.29952257
INFO:root:[  112] Training loss: 0.01277630, Validation loss: 0.01402182, Gradient norm: 0.31797429
INFO:root:[  113] Training loss: 0.01255028, Validation loss: 0.01375401, Gradient norm: 0.29976039
INFO:root:EP 113: Early stopping
INFO:root:Training the model took 564.524s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.26571
INFO:root:EnergyScoreTrain: 0.19783
INFO:root:CoverageTrain: 0.95691
INFO:root:IntervalWidthTrain: 0.03297
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.31981
INFO:root:EnergyScoreValidation: 0.24155
INFO:root:CoverageValidation: 0.92971
INFO:root:IntervalWidthValidation: 0.03324
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.61022
INFO:root:EnergyScoreTest: 0.4396
INFO:root:CoverageTest: 0.84483
INFO:root:IntervalWidthTest: 0.05344
INFO:root:###20 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.08230621, Validation loss: 0.05991347, Gradient norm: 0.24309497
INFO:root:[    2] Training loss: 0.05210175, Validation loss: 0.04842550, Gradient norm: 0.23348526
INFO:root:[    3] Training loss: 0.04562778, Validation loss: 0.04346828, Gradient norm: 0.23698736
INFO:root:[    4] Training loss: 0.04131929, Validation loss: 0.03971527, Gradient norm: 0.25301219
INFO:root:[    5] Training loss: 0.03768302, Validation loss: 0.03608127, Gradient norm: 0.25249404
INFO:root:[    6] Training loss: 0.03507053, Validation loss: 0.03477415, Gradient norm: 0.27898635
INFO:root:[    7] Training loss: 0.03333526, Validation loss: 0.03225219, Gradient norm: 0.28035654
INFO:root:[    8] Training loss: 0.03136826, Validation loss: 0.03178617, Gradient norm: 0.21686915
INFO:root:[    9] Training loss: 0.03071626, Validation loss: 0.02962134, Gradient norm: 0.29540245
INFO:root:[   10] Training loss: 0.02940516, Validation loss: 0.02999090, Gradient norm: 0.27569675
INFO:root:[   11] Training loss: 0.02869551, Validation loss: 0.02963190, Gradient norm: 0.31534602
INFO:root:[   12] Training loss: 0.02775746, Validation loss: 0.02840077, Gradient norm: 0.27927813
INFO:root:[   13] Training loss: 0.02702809, Validation loss: 0.02743198, Gradient norm: 0.31586342
INFO:root:[   14] Training loss: 0.02613012, Validation loss: 0.02548206, Gradient norm: 0.31171035
INFO:root:[   15] Training loss: 0.02587813, Validation loss: 0.02621845, Gradient norm: 0.35558139
INFO:root:[   16] Training loss: 0.02495108, Validation loss: 0.02474114, Gradient norm: 0.28882761
INFO:root:[   17] Training loss: 0.02452479, Validation loss: 0.02423830, Gradient norm: 0.31413519
INFO:root:[   18] Training loss: 0.02403075, Validation loss: 0.02399081, Gradient norm: 0.32607160
INFO:root:[   19] Training loss: 0.02340641, Validation loss: 0.02431951, Gradient norm: 0.30832906
INFO:root:[   20] Training loss: 0.02322519, Validation loss: 0.02391729, Gradient norm: 0.33539713
INFO:root:[   21] Training loss: 0.02269859, Validation loss: 0.02309778, Gradient norm: 0.29586411
INFO:root:[   22] Training loss: 0.02247010, Validation loss: 0.02263181, Gradient norm: 0.32313507
INFO:root:[   23] Training loss: 0.02247576, Validation loss: 0.02346099, Gradient norm: 0.32537471
INFO:root:[   24] Training loss: 0.02211382, Validation loss: 0.02137118, Gradient norm: 0.35863061
INFO:root:[   25] Training loss: 0.02167266, Validation loss: 0.02341001, Gradient norm: 0.34763492
INFO:root:[   26] Training loss: 0.02139650, Validation loss: 0.02160089, Gradient norm: 0.30253307
INFO:root:[   27] Training loss: 0.02102445, Validation loss: 0.02070076, Gradient norm: 0.32925615
INFO:root:[   28] Training loss: 0.02105700, Validation loss: 0.02221898, Gradient norm: 0.36671768
INFO:root:[   29] Training loss: 0.02072106, Validation loss: 0.02059155, Gradient norm: 0.34009585
INFO:root:[   30] Training loss: 0.02039838, Validation loss: 0.02044621, Gradient norm: 0.34891559
INFO:root:[   31] Training loss: 0.02078996, Validation loss: 0.02019121, Gradient norm: 0.41691185
INFO:root:[   32] Training loss: 0.02010847, Validation loss: 0.02057539, Gradient norm: 0.33879274
INFO:root:[   33] Training loss: 0.02023804, Validation loss: 0.02038668, Gradient norm: 0.38474004
INFO:root:[   34] Training loss: 0.01940977, Validation loss: 0.02065511, Gradient norm: 0.31568783
INFO:root:[   35] Training loss: 0.01948529, Validation loss: 0.02017120, Gradient norm: 0.33347532
INFO:root:[   36] Training loss: 0.01932661, Validation loss: 0.01933997, Gradient norm: 0.32576578
INFO:root:[   37] Training loss: 0.01940468, Validation loss: 0.01937848, Gradient norm: 0.36776531
INFO:root:[   38] Training loss: 0.01921098, Validation loss: 0.01978241, Gradient norm: 0.35107843
INFO:root:[   39] Training loss: 0.01886193, Validation loss: 0.01909160, Gradient norm: 0.30857527
INFO:root:[   40] Training loss: 0.01889630, Validation loss: 0.01956372, Gradient norm: 0.34239134
INFO:root:[   41] Training loss: 0.01889523, Validation loss: 0.02233892, Gradient norm: 0.37122821
INFO:root:[   42] Training loss: 0.01844852, Validation loss: 0.01909206, Gradient norm: 0.34592411
INFO:root:[   43] Training loss: 0.01870034, Validation loss: 0.01849283, Gradient norm: 0.40107125
INFO:root:[   44] Training loss: 0.01811352, Validation loss: 0.01804365, Gradient norm: 0.34060708
INFO:root:[   45] Training loss: 0.01815819, Validation loss: 0.01858057, Gradient norm: 0.30232457
INFO:root:[   46] Training loss: 0.01798592, Validation loss: 0.01811388, Gradient norm: 0.30881529
INFO:root:[   47] Training loss: 0.01786254, Validation loss: 0.01805714, Gradient norm: 0.33812843
INFO:root:[   48] Training loss: 0.01799743, Validation loss: 0.01786230, Gradient norm: 0.36231055
INFO:root:[   49] Training loss: 0.01782743, Validation loss: 0.01782497, Gradient norm: 0.34692492
INFO:root:[   50] Training loss: 0.01780186, Validation loss: 0.01794128, Gradient norm: 0.36765421
INFO:root:[   51] Training loss: 0.01747445, Validation loss: 0.01832238, Gradient norm: 0.30863741
INFO:root:[   52] Training loss: 0.01742269, Validation loss: 0.01780785, Gradient norm: 0.34649423
INFO:root:[   53] Training loss: 0.01741366, Validation loss: 0.01798724, Gradient norm: 0.34744021
INFO:root:[   54] Training loss: 0.01723807, Validation loss: 0.01719349, Gradient norm: 0.32392822
INFO:root:[   55] Training loss: 0.01704665, Validation loss: 0.01725412, Gradient norm: 0.28678469
INFO:root:[   56] Training loss: 0.01711496, Validation loss: 0.01739487, Gradient norm: 0.34287848
INFO:root:[   57] Training loss: 0.01691555, Validation loss: 0.01743086, Gradient norm: 0.31973672
INFO:root:[   58] Training loss: 0.01700009, Validation loss: 0.01674982, Gradient norm: 0.31040733
INFO:root:[   59] Training loss: 0.01670691, Validation loss: 0.01726221, Gradient norm: 0.32063572
INFO:root:[   60] Training loss: 0.01675481, Validation loss: 0.01700547, Gradient norm: 0.32173539
INFO:root:[   61] Training loss: 0.01675105, Validation loss: 0.01706747, Gradient norm: 0.33670635
INFO:root:[   62] Training loss: 0.01669819, Validation loss: 0.01717392, Gradient norm: 0.32456344
INFO:root:[   63] Training loss: 0.01667248, Validation loss: 0.01774072, Gradient norm: 0.34556995
INFO:root:[   64] Training loss: 0.01646028, Validation loss: 0.01644114, Gradient norm: 0.31126427
INFO:root:[   65] Training loss: 0.01627346, Validation loss: 0.01660599, Gradient norm: 0.27569109
INFO:root:[   66] Training loss: 0.01632391, Validation loss: 0.01727146, Gradient norm: 0.33267171
INFO:root:[   67] Training loss: 0.01606504, Validation loss: 0.01646225, Gradient norm: 0.26968371
INFO:root:[   68] Training loss: 0.01618676, Validation loss: 0.01630633, Gradient norm: 0.29493228
INFO:root:[   69] Training loss: 0.01634687, Validation loss: 0.01647705, Gradient norm: 0.35142626
INFO:root:[   70] Training loss: 0.01598778, Validation loss: 0.01630931, Gradient norm: 0.29987909
INFO:root:[   71] Training loss: 0.01601650, Validation loss: 0.01717765, Gradient norm: 0.32204229
INFO:root:[   72] Training loss: 0.01604233, Validation loss: 0.01659434, Gradient norm: 0.31331937
INFO:root:[   73] Training loss: 0.01600337, Validation loss: 0.01655259, Gradient norm: 0.32626441
INFO:root:[   74] Training loss: 0.01592650, Validation loss: 0.01672685, Gradient norm: 0.31912201
INFO:root:[   75] Training loss: 0.01582092, Validation loss: 0.01626854, Gradient norm: 0.30839286
INFO:root:[   76] Training loss: 0.01596749, Validation loss: 0.01626675, Gradient norm: 0.35761251
INFO:root:[   77] Training loss: 0.01588531, Validation loss: 0.01607330, Gradient norm: 0.34146698
INFO:root:[   78] Training loss: 0.01538543, Validation loss: 0.01599968, Gradient norm: 0.28077967
INFO:root:[   79] Training loss: 0.01571752, Validation loss: 0.01600564, Gradient norm: 0.31975162
INFO:root:[   80] Training loss: 0.01546798, Validation loss: 0.01559588, Gradient norm: 0.29799745
INFO:root:[   81] Training loss: 0.01530017, Validation loss: 0.01632821, Gradient norm: 0.26403100
INFO:root:[   82] Training loss: 0.01545360, Validation loss: 0.01562377, Gradient norm: 0.31348197
INFO:root:[   83] Training loss: 0.01542376, Validation loss: 0.01586512, Gradient norm: 0.25945039
INFO:root:[   84] Training loss: 0.01540914, Validation loss: 0.01560633, Gradient norm: 0.31884630
INFO:root:[   85] Training loss: 0.01521007, Validation loss: 0.01573700, Gradient norm: 0.27274857
INFO:root:[   86] Training loss: 0.01509013, Validation loss: 0.01547485, Gradient norm: 0.29762257
INFO:root:[   87] Training loss: 0.01499998, Validation loss: 0.01558798, Gradient norm: 0.28447224
INFO:root:[   88] Training loss: 0.01511617, Validation loss: 0.01553662, Gradient norm: 0.28925683
INFO:root:[   89] Training loss: 0.01524635, Validation loss: 0.01552759, Gradient norm: 0.31179454
INFO:root:[   90] Training loss: 0.01512800, Validation loss: 0.01571705, Gradient norm: 0.33494178
INFO:root:[   91] Training loss: 0.01503740, Validation loss: 0.01564154, Gradient norm: 0.30240607
INFO:root:[   92] Training loss: 0.01501600, Validation loss: 0.01521518, Gradient norm: 0.29588861
INFO:root:[   93] Training loss: 0.01464756, Validation loss: 0.01599689, Gradient norm: 0.25964252
INFO:root:[   94] Training loss: 0.01487360, Validation loss: 0.01564025, Gradient norm: 0.29584391
INFO:root:[   95] Training loss: 0.01466971, Validation loss: 0.01590700, Gradient norm: 0.25337972
INFO:root:[   96] Training loss: 0.01472625, Validation loss: 0.01504170, Gradient norm: 0.27952720
INFO:root:[   97] Training loss: 0.01461211, Validation loss: 0.01495648, Gradient norm: 0.27469857
INFO:root:[   98] Training loss: 0.01466019, Validation loss: 0.01559420, Gradient norm: 0.28873704
INFO:root:[   99] Training loss: 0.01461093, Validation loss: 0.01528310, Gradient norm: 0.27397196
INFO:root:[  100] Training loss: 0.01469817, Validation loss: 0.01542288, Gradient norm: 0.30842733
INFO:root:[  101] Training loss: 0.01442688, Validation loss: 0.01499823, Gradient norm: 0.26398977
INFO:root:[  102] Training loss: 0.01455016, Validation loss: 0.01633226, Gradient norm: 0.29027464
INFO:root:[  103] Training loss: 0.01466296, Validation loss: 0.01527490, Gradient norm: 0.28690485
INFO:root:[  104] Training loss: 0.01435363, Validation loss: 0.01549483, Gradient norm: 0.27264006
INFO:root:[  105] Training loss: 0.01445359, Validation loss: 0.01527386, Gradient norm: 0.29465537
INFO:root:[  106] Training loss: 0.01444780, Validation loss: 0.01492917, Gradient norm: 0.28923981
INFO:root:[  107] Training loss: 0.01418383, Validation loss: 0.01478649, Gradient norm: 0.25376059
INFO:root:[  108] Training loss: 0.01446058, Validation loss: 0.01487144, Gradient norm: 0.30281279
INFO:root:[  109] Training loss: 0.01439726, Validation loss: 0.01537224, Gradient norm: 0.30835217
INFO:root:[  110] Training loss: 0.01406334, Validation loss: 0.01469955, Gradient norm: 0.25986678
INFO:root:[  111] Training loss: 0.01419438, Validation loss: 0.01440491, Gradient norm: 0.29620401
INFO:root:[  112] Training loss: 0.01406945, Validation loss: 0.01616029, Gradient norm: 0.26467003
INFO:root:[  113] Training loss: 0.01394892, Validation loss: 0.01481474, Gradient norm: 0.25300963
INFO:root:[  114] Training loss: 0.01401495, Validation loss: 0.01456276, Gradient norm: 0.26549206
INFO:root:[  115] Training loss: 0.01382409, Validation loss: 0.01477021, Gradient norm: 0.24182429
INFO:root:[  116] Training loss: 0.01399829, Validation loss: 0.01448075, Gradient norm: 0.27711508
INFO:root:[  117] Training loss: 0.01411388, Validation loss: 0.01444593, Gradient norm: 0.28562595
INFO:root:[  118] Training loss: 0.01390218, Validation loss: 0.01438421, Gradient norm: 0.25991126
INFO:root:[  119] Training loss: 0.01394133, Validation loss: 0.01519457, Gradient norm: 0.27843829
INFO:root:[  120] Training loss: 0.01404715, Validation loss: 0.01456286, Gradient norm: 0.27500643
INFO:root:[  121] Training loss: 0.01388125, Validation loss: 0.01453143, Gradient norm: 0.27737134
INFO:root:[  122] Training loss: 0.01385143, Validation loss: 0.01484828, Gradient norm: 0.26982580
INFO:root:[  123] Training loss: 0.01373656, Validation loss: 0.01439020, Gradient norm: 0.26231674
INFO:root:[  124] Training loss: 0.01357961, Validation loss: 0.01478071, Gradient norm: 0.24527634
INFO:root:[  125] Training loss: 0.01359221, Validation loss: 0.01621197, Gradient norm: 0.25801072
INFO:root:[  126] Training loss: 0.01364957, Validation loss: 0.01497224, Gradient norm: 0.26885895
INFO:root:[  127] Training loss: 0.01357259, Validation loss: 0.01471665, Gradient norm: 0.23974484
INFO:root:EP 127: Early stopping
INFO:root:Training the model took 633.875s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.28667
INFO:root:EnergyScoreTrain: 0.21433
INFO:root:CoverageTrain: 0.95729
INFO:root:IntervalWidthTrain: 0.03636
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.31955
INFO:root:EnergyScoreValidation: 0.24092
INFO:root:CoverageValidation: 0.9411
INFO:root:IntervalWidthValidation: 0.03663
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.9016
INFO:root:EnergyScoreTest: 0.63883
INFO:root:CoverageTest: 0.80813
INFO:root:IntervalWidthTest: 0.07582
INFO:root:###21 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.09851494, Validation loss: 0.07244022, Gradient norm: 0.27705533
INFO:root:[    2] Training loss: 0.06358229, Validation loss: 0.05836133, Gradient norm: 0.23782309
INFO:root:[    3] Training loss: 0.05540684, Validation loss: 0.05240159, Gradient norm: 0.28170625
INFO:root:[    4] Training loss: 0.05034341, Validation loss: 0.04916962, Gradient norm: 0.24960290
INFO:root:[    5] Training loss: 0.04701713, Validation loss: 0.04612813, Gradient norm: 0.29025749
INFO:root:[    6] Training loss: 0.04425172, Validation loss: 0.04368692, Gradient norm: 0.24955107
INFO:root:[    7] Training loss: 0.04156059, Validation loss: 0.04104889, Gradient norm: 0.27719893
INFO:root:[    8] Training loss: 0.03970953, Validation loss: 0.04090250, Gradient norm: 0.26623509
INFO:root:[    9] Training loss: 0.03803150, Validation loss: 0.03697742, Gradient norm: 0.34929006
INFO:root:[   10] Training loss: 0.03624348, Validation loss: 0.03539857, Gradient norm: 0.31615736
INFO:root:[   11] Training loss: 0.03489072, Validation loss: 0.03510152, Gradient norm: 0.29421929
INFO:root:[   12] Training loss: 0.03366539, Validation loss: 0.03334810, Gradient norm: 0.29384697
INFO:root:[   13] Training loss: 0.03268904, Validation loss: 0.03249792, Gradient norm: 0.33081120
INFO:root:[   14] Training loss: 0.03180582, Validation loss: 0.03209227, Gradient norm: 0.33314810
INFO:root:[   15] Training loss: 0.03093073, Validation loss: 0.03362811, Gradient norm: 0.32164392
INFO:root:[   16] Training loss: 0.03019513, Validation loss: 0.03004374, Gradient norm: 0.34340817
INFO:root:[   17] Training loss: 0.03003719, Validation loss: 0.03393157, Gradient norm: 0.36502251
INFO:root:[   18] Training loss: 0.02879669, Validation loss: 0.02897941, Gradient norm: 0.33118744
INFO:root:[   19] Training loss: 0.02835619, Validation loss: 0.02869693, Gradient norm: 0.33132873
INFO:root:[   20] Training loss: 0.02794880, Validation loss: 0.02791515, Gradient norm: 0.33519614
INFO:root:[   21] Training loss: 0.02779662, Validation loss: 0.02733387, Gradient norm: 0.38780969
INFO:root:[   22] Training loss: 0.02717330, Validation loss: 0.02695585, Gradient norm: 0.35923920
INFO:root:[   23] Training loss: 0.02663771, Validation loss: 0.02663218, Gradient norm: 0.35294109
INFO:root:[   24] Training loss: 0.02633390, Validation loss: 0.02822302, Gradient norm: 0.35598636
INFO:root:[   25] Training loss: 0.02582525, Validation loss: 0.02602913, Gradient norm: 0.33704533
INFO:root:[   26] Training loss: 0.02590935, Validation loss: 0.02580423, Gradient norm: 0.36019780
INFO:root:[   27] Training loss: 0.02523565, Validation loss: 0.02517953, Gradient norm: 0.31143098
INFO:root:[   28] Training loss: 0.02525943, Validation loss: 0.02495899, Gradient norm: 0.37403125
INFO:root:[   29] Training loss: 0.02462895, Validation loss: 0.02513077, Gradient norm: 0.31530145
INFO:root:[   30] Training loss: 0.02468505, Validation loss: 0.02466344, Gradient norm: 0.32403328
INFO:root:[   31] Training loss: 0.02434656, Validation loss: 0.02646799, Gradient norm: 0.31685118
INFO:root:[   32] Training loss: 0.02427089, Validation loss: 0.02421054, Gradient norm: 0.35620094
INFO:root:[   33] Training loss: 0.02357117, Validation loss: 0.02373102, Gradient norm: 0.29233995
INFO:root:[   34] Training loss: 0.02377137, Validation loss: 0.02335579, Gradient norm: 0.34035443
INFO:root:[   35] Training loss: 0.02341566, Validation loss: 0.02413015, Gradient norm: 0.30314778
INFO:root:[   36] Training loss: 0.02328991, Validation loss: 0.02288988, Gradient norm: 0.33304719
INFO:root:[   37] Training loss: 0.02288585, Validation loss: 0.02331762, Gradient norm: 0.29639468
INFO:root:[   38] Training loss: 0.02289875, Validation loss: 0.02270445, Gradient norm: 0.32756059
INFO:root:[   39] Training loss: 0.02259915, Validation loss: 0.02304583, Gradient norm: 0.30543147
INFO:root:[   40] Training loss: 0.02242864, Validation loss: 0.02238680, Gradient norm: 0.29531866
INFO:root:[   41] Training loss: 0.02226357, Validation loss: 0.02279577, Gradient norm: 0.30168772
INFO:root:[   42] Training loss: 0.02198610, Validation loss: 0.02269828, Gradient norm: 0.27406767
INFO:root:[   43] Training loss: 0.02187853, Validation loss: 0.02167926, Gradient norm: 0.30585202
INFO:root:[   44] Training loss: 0.02207320, Validation loss: 0.02277160, Gradient norm: 0.35729113
INFO:root:[   45] Training loss: 0.02164671, Validation loss: 0.02238059, Gradient norm: 0.28924715
INFO:root:[   46] Training loss: 0.02158198, Validation loss: 0.02160844, Gradient norm: 0.28754160
INFO:root:[   47] Training loss: 0.02129450, Validation loss: 0.02151117, Gradient norm: 0.26684170
INFO:root:[   48] Training loss: 0.02131703, Validation loss: 0.02319455, Gradient norm: 0.27782783
INFO:root:[   49] Training loss: 0.02112392, Validation loss: 0.02153260, Gradient norm: 0.29709955
INFO:root:[   50] Training loss: 0.02115104, Validation loss: 0.02251632, Gradient norm: 0.30769479
INFO:root:[   51] Training loss: 0.02087842, Validation loss: 0.02191910, Gradient norm: 0.29982666
INFO:root:[   52] Training loss: 0.02093551, Validation loss: 0.02067843, Gradient norm: 0.28669774
INFO:root:[   53] Training loss: 0.02048823, Validation loss: 0.02126173, Gradient norm: 0.25970168
INFO:root:[   54] Training loss: 0.02047969, Validation loss: 0.02131019, Gradient norm: 0.26305507
INFO:root:[   55] Training loss: 0.02053592, Validation loss: 0.02064942, Gradient norm: 0.29330516
INFO:root:[   56] Training loss: 0.02033799, Validation loss: 0.02094701, Gradient norm: 0.29163608
INFO:root:[   57] Training loss: 0.02015231, Validation loss: 0.02139515, Gradient norm: 0.26515928
INFO:root:[   58] Training loss: 0.02021657, Validation loss: 0.02053442, Gradient norm: 0.26889045
INFO:root:[   59] Training loss: 0.02020784, Validation loss: 0.02013463, Gradient norm: 0.30601493
INFO:root:[   60] Training loss: 0.01992674, Validation loss: 0.01994639, Gradient norm: 0.26982175
INFO:root:[   61] Training loss: 0.01981482, Validation loss: 0.02015317, Gradient norm: 0.25949046
INFO:root:[   62] Training loss: 0.01989029, Validation loss: 0.02031655, Gradient norm: 0.27758291
INFO:root:[   63] Training loss: 0.01971728, Validation loss: 0.02045363, Gradient norm: 0.27136970
INFO:root:[   64] Training loss: 0.01963456, Validation loss: 0.02088198, Gradient norm: 0.28079435
INFO:root:[   65] Training loss: 0.01961081, Validation loss: 0.01932407, Gradient norm: 0.27405933
INFO:root:[   66] Training loss: 0.01957923, Validation loss: 0.02025510, Gradient norm: 0.28728962
INFO:root:[   67] Training loss: 0.01947759, Validation loss: 0.01984838, Gradient norm: 0.27779781
INFO:root:[   68] Training loss: 0.01928100, Validation loss: 0.01982294, Gradient norm: 0.27271633
INFO:root:[   69] Training loss: 0.01918106, Validation loss: 0.02053149, Gradient norm: 0.26986082
INFO:root:[   70] Training loss: 0.01909416, Validation loss: 0.01981296, Gradient norm: 0.25358032
INFO:root:[   71] Training loss: 0.01903880, Validation loss: 0.01933292, Gradient norm: 0.25644923
INFO:root:[   72] Training loss: 0.01906249, Validation loss: 0.01929602, Gradient norm: 0.26690270
INFO:root:[   73] Training loss: 0.01911110, Validation loss: 0.01891807, Gradient norm: 0.28627577
INFO:root:[   74] Training loss: 0.01895426, Validation loss: 0.01907707, Gradient norm: 0.25369351
INFO:root:[   75] Training loss: 0.01879549, Validation loss: 0.01968313, Gradient norm: 0.26594303
INFO:root:[   76] Training loss: 0.01891412, Validation loss: 0.01947492, Gradient norm: 0.28626772
INFO:root:[   77] Training loss: 0.01882394, Validation loss: 0.01969375, Gradient norm: 0.27403815
INFO:root:[   78] Training loss: 0.01864808, Validation loss: 0.01986370, Gradient norm: 0.25776200
INFO:root:[   79] Training loss: 0.01845887, Validation loss: 0.01881002, Gradient norm: 0.24672052
INFO:root:[   80] Training loss: 0.01845212, Validation loss: 0.01861073, Gradient norm: 0.23256162
INFO:root:[   81] Training loss: 0.01852229, Validation loss: 0.01914737, Gradient norm: 0.27527752
INFO:root:[   82] Training loss: 0.01837681, Validation loss: 0.01880260, Gradient norm: 0.26216404
INFO:root:[   83] Training loss: 0.01831093, Validation loss: 0.01913996, Gradient norm: 0.26501388
INFO:root:[   84] Training loss: 0.01815307, Validation loss: 0.01992783, Gradient norm: 0.24809998
INFO:root:[   85] Training loss: 0.01823330, Validation loss: 0.01846323, Gradient norm: 0.25319007
INFO:root:[   86] Training loss: 0.01823830, Validation loss: 0.01848100, Gradient norm: 0.26087167
INFO:root:[   87] Training loss: 0.01814337, Validation loss: 0.01909971, Gradient norm: 0.25942485
INFO:root:[   88] Training loss: 0.01787371, Validation loss: 0.01900042, Gradient norm: 0.24428281
INFO:root:[   89] Training loss: 0.01811995, Validation loss: 0.01814973, Gradient norm: 0.24933427
INFO:root:[   90] Training loss: 0.01791105, Validation loss: 0.01786814, Gradient norm: 0.25172490
INFO:root:[   91] Training loss: 0.01779661, Validation loss: 0.01819659, Gradient norm: 0.24648533
INFO:root:[   92] Training loss: 0.01771280, Validation loss: 0.01858389, Gradient norm: 0.23150674
INFO:root:[   93] Training loss: 0.01797276, Validation loss: 0.01846625, Gradient norm: 0.27079665
INFO:root:[   94] Training loss: 0.01779161, Validation loss: 0.01832590, Gradient norm: 0.26494991
INFO:root:[   95] Training loss: 0.01777983, Validation loss: 0.01832768, Gradient norm: 0.25313623
INFO:root:[   96] Training loss: 0.01767579, Validation loss: 0.01887181, Gradient norm: 0.24923423
INFO:root:[   97] Training loss: 0.01774700, Validation loss: 0.01885402, Gradient norm: 0.27711265
INFO:root:[   98] Training loss: 0.01771597, Validation loss: 0.01883845, Gradient norm: 0.25268131
INFO:root:[   99] Training loss: 0.01740977, Validation loss: 0.01766289, Gradient norm: 0.23421453
INFO:root:[  100] Training loss: 0.01754096, Validation loss: 0.01802202, Gradient norm: 0.25532551
INFO:root:[  101] Training loss: 0.01742885, Validation loss: 0.01874932, Gradient norm: 0.25967132
INFO:root:[  102] Training loss: 0.01737868, Validation loss: 0.01814175, Gradient norm: 0.24752821
INFO:root:[  103] Training loss: 0.01724509, Validation loss: 0.01785988, Gradient norm: 0.23370232
INFO:root:[  104] Training loss: 0.01728138, Validation loss: 0.01752678, Gradient norm: 0.24985054
INFO:root:[  105] Training loss: 0.01731993, Validation loss: 0.01737833, Gradient norm: 0.24844992
INFO:root:[  106] Training loss: 0.01732857, Validation loss: 0.01768801, Gradient norm: 0.26211203
INFO:root:[  107] Training loss: 0.01719257, Validation loss: 0.01796504, Gradient norm: 0.25748870
INFO:root:[  108] Training loss: 0.01717777, Validation loss: 0.01758103, Gradient norm: 0.24151576
INFO:root:[  109] Training loss: 0.01725594, Validation loss: 0.01745147, Gradient norm: 0.25499519
INFO:root:[  110] Training loss: 0.01690124, Validation loss: 0.01783946, Gradient norm: 0.23123050
INFO:root:[  111] Training loss: 0.01683125, Validation loss: 0.01779707, Gradient norm: 0.22043874
INFO:root:[  112] Training loss: 0.01695165, Validation loss: 0.01772609, Gradient norm: 0.24031378
INFO:root:[  113] Training loss: 0.01694754, Validation loss: 0.01737162, Gradient norm: 0.24698237
INFO:root:[  114] Training loss: 0.01691357, Validation loss: 0.01694710, Gradient norm: 0.23301282
INFO:root:[  115] Training loss: 0.01685628, Validation loss: 0.01786438, Gradient norm: 0.23294134
INFO:root:[  116] Training loss: 0.01707121, Validation loss: 0.01773451, Gradient norm: 0.26867374
INFO:root:[  117] Training loss: 0.01675596, Validation loss: 0.01725382, Gradient norm: 0.24539233
INFO:root:[  118] Training loss: 0.01666134, Validation loss: 0.01788755, Gradient norm: 0.24468075
INFO:root:[  119] Training loss: 0.01702683, Validation loss: 0.01830790, Gradient norm: 0.28190792
INFO:root:[  120] Training loss: 0.01673114, Validation loss: 0.01718057, Gradient norm: 0.24606717
INFO:root:[  121] Training loss: 0.01680140, Validation loss: 0.01715506, Gradient norm: 0.25445494
INFO:root:[  122] Training loss: 0.01669200, Validation loss: 0.01852476, Gradient norm: 0.22561922
INFO:root:[  123] Training loss: 0.01664690, Validation loss: 0.01716720, Gradient norm: 0.23678055
INFO:root:EP 123: Early stopping
INFO:root:Training the model took 614.236s.
INFO:root:Emptying the cuda cache took 0.013s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.33326
INFO:root:EnergyScoreTrain: 0.25087
INFO:root:CoverageTrain: 0.96836
INFO:root:IntervalWidthTrain: 0.04617
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.35334
INFO:root:EnergyScoreValidation: 0.26669
INFO:root:CoverageValidation: 0.96035
INFO:root:IntervalWidthValidation: 0.04666
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.72227
INFO:root:EnergyScoreTest: 0.54378
INFO:root:CoverageTest: 0.99613
INFO:root:IntervalWidthTest: 0.14761
