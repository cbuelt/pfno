INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05539429, Validation loss: 0.03274814, Gradient norm: 0.65202214
INFO:root:[    2] Training loss: 0.02602926, Validation loss: 0.02229806, Gradient norm: 0.70088477
INFO:root:[    3] Training loss: 0.01835343, Validation loss: 0.02015940, Gradient norm: 0.54071608
INFO:root:[    4] Training loss: 0.01815922, Validation loss: 0.02310579, Gradient norm: 0.65248483
INFO:root:[    5] Training loss: 0.01551197, Validation loss: 0.01929232, Gradient norm: 0.58807494
INFO:root:[    6] Training loss: 0.01477308, Validation loss: 0.01897726, Gradient norm: 0.55041050
INFO:root:[    7] Training loss: 0.01353057, Validation loss: 0.01951210, Gradient norm: 0.53536562
INFO:root:[    8] Training loss: 0.01284114, Validation loss: 0.01976635, Gradient norm: 0.49292679
INFO:root:[    9] Training loss: 0.01253057, Validation loss: 0.01786497, Gradient norm: 0.51715032
INFO:root:[   10] Training loss: 0.01165592, Validation loss: 0.01660363, Gradient norm: 0.45423426
INFO:root:[   11] Training loss: 0.01083151, Validation loss: 0.01965055, Gradient norm: 0.34493090
INFO:root:[   12] Training loss: 0.01174351, Validation loss: 0.01866561, Gradient norm: 0.55465888
INFO:root:[   13] Training loss: 0.01000343, Validation loss: 0.01691922, Gradient norm: 0.38422781
INFO:root:[   14] Training loss: 0.00988963, Validation loss: 0.01668084, Gradient norm: 0.43326909
INFO:root:[   15] Training loss: 0.00989515, Validation loss: 0.01787802, Gradient norm: 0.40930944
INFO:root:[   16] Training loss: 0.00978869, Validation loss: 0.01673424, Gradient norm: 0.38722250
INFO:root:[   17] Training loss: 0.00924690, Validation loss: 0.01544455, Gradient norm: 0.37429988
INFO:root:[   18] Training loss: 0.00975979, Validation loss: 0.01629774, Gradient norm: 0.44675525
INFO:root:[   19] Training loss: 0.00951562, Validation loss: 0.01669684, Gradient norm: 0.44866826
INFO:root:[   20] Training loss: 0.00877371, Validation loss: 0.01526812, Gradient norm: 0.43477307
INFO:root:[   21] Training loss: 0.00814228, Validation loss: 0.01603194, Gradient norm: 0.32878249
INFO:root:[   22] Training loss: 0.00901537, Validation loss: 0.01669208, Gradient norm: 0.42006486
INFO:root:[   23] Training loss: 0.00862839, Validation loss: 0.01645735, Gradient norm: 0.37604727
INFO:root:[   24] Training loss: 0.00784848, Validation loss: 0.01725065, Gradient norm: 0.35545773
INFO:root:[   25] Training loss: 0.00806901, Validation loss: 0.01747492, Gradient norm: 0.40177697
INFO:root:[   26] Training loss: 0.00799516, Validation loss: 0.01821710, Gradient norm: 0.37814319
INFO:root:[   27] Training loss: 0.00813708, Validation loss: 0.01761399, Gradient norm: 0.40081712
INFO:root:[   28] Training loss: 0.00785623, Validation loss: 0.01450557, Gradient norm: 0.41237320
INFO:root:[   29] Training loss: 0.00739913, Validation loss: 0.01570595, Gradient norm: 0.35935454
INFO:root:[   30] Training loss: 0.00668040, Validation loss: 0.01653565, Gradient norm: 0.32928777
INFO:root:[   31] Training loss: 0.00802233, Validation loss: 0.01488030, Gradient norm: 0.41473305
INFO:root:[   32] Training loss: 0.00734594, Validation loss: 0.01589620, Gradient norm: 0.36676207
INFO:root:[   33] Training loss: 0.00693503, Validation loss: 0.01707500, Gradient norm: 0.38018808
INFO:root:[   34] Training loss: 0.00687653, Validation loss: 0.01574033, Gradient norm: 0.36156296
INFO:root:[   35] Training loss: 0.00598635, Validation loss: 0.01639736, Gradient norm: 0.29154395
INFO:root:[   36] Training loss: 0.00712832, Validation loss: 0.01818553, Gradient norm: 0.37233728
INFO:root:[   37] Training loss: 0.00670082, Validation loss: 0.01511700, Gradient norm: 0.34794487
INFO:root:[   38] Training loss: 0.00692127, Validation loss: 0.01737714, Gradient norm: 0.35705299
INFO:root:[   39] Training loss: 0.00679855, Validation loss: 0.01619852, Gradient norm: 0.34497019
INFO:root:[   40] Training loss: 0.00659168, Validation loss: 0.01614351, Gradient norm: 0.38429721
INFO:root:[   41] Training loss: 0.00631319, Validation loss: 0.01539271, Gradient norm: 0.36811525
INFO:root:[   42] Training loss: 0.00597276, Validation loss: 0.01788773, Gradient norm: 0.33823121
INFO:root:[   43] Training loss: 0.00576330, Validation loss: 0.01495248, Gradient norm: 0.28516473
INFO:root:[   44] Training loss: 0.00593040, Validation loss: 0.01545539, Gradient norm: 0.36144321
INFO:root:[   45] Training loss: 0.00647674, Validation loss: 0.01370361, Gradient norm: 0.36171608
INFO:root:[   46] Training loss: 0.00574232, Validation loss: 0.01581896, Gradient norm: 0.26646414
INFO:root:[   47] Training loss: 0.00607032, Validation loss: 0.01513572, Gradient norm: 0.38955778
INFO:root:[   48] Training loss: 0.00561004, Validation loss: 0.01634388, Gradient norm: 0.31361586
INFO:root:[   49] Training loss: 0.00518510, Validation loss: 0.01506876, Gradient norm: 0.26441184
INFO:root:[   50] Training loss: 0.00518119, Validation loss: 0.01623898, Gradient norm: 0.26204164
INFO:root:[   51] Training loss: 0.00550053, Validation loss: 0.01344145, Gradient norm: 0.30491816
INFO:root:[   52] Training loss: 0.00583522, Validation loss: 0.01634840, Gradient norm: 0.35695123
INFO:root:[   53] Training loss: 0.00551567, Validation loss: 0.01291945, Gradient norm: 0.30937856
INFO:root:[   54] Training loss: 0.00534366, Validation loss: 0.01604083, Gradient norm: 0.28480476
INFO:root:[   55] Training loss: 0.00585931, Validation loss: 0.01364809, Gradient norm: 0.37016022
INFO:root:[   56] Training loss: 0.00539734, Validation loss: 0.01406508, Gradient norm: 0.32758663
INFO:root:[   57] Training loss: 0.00509168, Validation loss: 0.01455452, Gradient norm: 0.26212588
INFO:root:[   58] Training loss: 0.00590115, Validation loss: 0.01330663, Gradient norm: 0.37676080
INFO:root:[   59] Training loss: 0.00499801, Validation loss: 0.01450796, Gradient norm: 0.23203136
INFO:root:[   60] Training loss: 0.00481915, Validation loss: 0.01591862, Gradient norm: 0.28707927
INFO:root:[   61] Training loss: 0.00528917, Validation loss: 0.01391482, Gradient norm: 0.35026793
INFO:root:[   62] Training loss: 0.00526881, Validation loss: 0.01419820, Gradient norm: 0.35721504
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1800.984s.
INFO:root:Emptying the cuda cache took 0.057s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00934
INFO:root:EnergyScoreTrain: 0.00679
INFO:root:CoverageTrain: 0.97964
INFO:root:IntervalWidthTrain: 0.04089
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01701
INFO:root:EnergyScoreValidation: 0.01299
INFO:root:CoverageValidation: 0.71819
INFO:root:IntervalWidthValidation: 0.04096
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01709
INFO:root:EnergyScoreTest: 0.01304
INFO:root:CoverageTest: 0.70891
INFO:root:IntervalWidthTest: 0.04105
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05077364, Validation loss: 0.04263808, Gradient norm: 0.74567215
INFO:root:[    2] Training loss: 0.02534976, Validation loss: 0.03660208, Gradient norm: 0.67956680
INFO:root:[    3] Training loss: 0.01979393, Validation loss: 0.03075727, Gradient norm: 0.59766913
INFO:root:[    4] Training loss: 0.01818997, Validation loss: 0.02062488, Gradient norm: 0.59055798
INFO:root:[    5] Training loss: 0.01558591, Validation loss: 0.02066759, Gradient norm: 0.50004930
INFO:root:[    6] Training loss: 0.01414527, Validation loss: 0.01778524, Gradient norm: 0.41722055
INFO:root:[    7] Training loss: 0.01385656, Validation loss: 0.01852744, Gradient norm: 0.43979933
INFO:root:[    8] Training loss: 0.01340764, Validation loss: 0.01923123, Gradient norm: 0.37058044
INFO:root:[    9] Training loss: 0.01330283, Validation loss: 0.02540658, Gradient norm: 0.46064994
INFO:root:[   10] Training loss: 0.01334733, Validation loss: 0.01538180, Gradient norm: 0.44870627
INFO:root:[   11] Training loss: 0.01151738, Validation loss: 0.01748744, Gradient norm: 0.37724071
INFO:root:[   12] Training loss: 0.01057494, Validation loss: 0.01500133, Gradient norm: 0.28746441
INFO:root:[   13] Training loss: 0.01184385, Validation loss: 0.01502953, Gradient norm: 0.44119920
INFO:root:[   14] Training loss: 0.01058660, Validation loss: 0.01446540, Gradient norm: 0.35658983
INFO:root:[   15] Training loss: 0.01081897, Validation loss: 0.01416510, Gradient norm: 0.37522900
INFO:root:[   16] Training loss: 0.01079722, Validation loss: 0.01505297, Gradient norm: 0.36692538
INFO:root:[   17] Training loss: 0.01030984, Validation loss: 0.01943841, Gradient norm: 0.37927799
INFO:root:[   18] Training loss: 0.00951222, Validation loss: 0.01379718, Gradient norm: 0.25542995
INFO:root:[   19] Training loss: 0.00992411, Validation loss: 0.01590920, Gradient norm: 0.34180320
INFO:root:[   20] Training loss: 0.01061718, Validation loss: 0.01227695, Gradient norm: 0.37794569
INFO:root:[   21] Training loss: 0.00979005, Validation loss: 0.01271987, Gradient norm: 0.33041044
INFO:root:[   22] Training loss: 0.00967726, Validation loss: 0.01452126, Gradient norm: 0.35208130
INFO:root:[   23] Training loss: 0.00816314, Validation loss: 0.01803220, Gradient norm: 0.21169992
INFO:root:[   24] Training loss: 0.00930034, Validation loss: 0.01275382, Gradient norm: 0.36988395
INFO:root:[   25] Training loss: 0.00876104, Validation loss: 0.01241234, Gradient norm: 0.29126302
INFO:root:[   26] Training loss: 0.00884995, Validation loss: 0.01136432, Gradient norm: 0.29330860
INFO:root:[   27] Training loss: 0.00854142, Validation loss: 0.01173168, Gradient norm: 0.32522760
INFO:root:[   28] Training loss: 0.00914545, Validation loss: 0.01465127, Gradient norm: 0.34390929
INFO:root:[   29] Training loss: 0.00790413, Validation loss: 0.01173407, Gradient norm: 0.22912531
INFO:root:[   30] Training loss: 0.00865791, Validation loss: 0.01267765, Gradient norm: 0.34187130
INFO:root:[   31] Training loss: 0.00820621, Validation loss: 0.01180795, Gradient norm: 0.27769218
INFO:root:[   32] Training loss: 0.00813886, Validation loss: 0.01221296, Gradient norm: 0.30995479
INFO:root:[   33] Training loss: 0.00819264, Validation loss: 0.01458279, Gradient norm: 0.30719290
INFO:root:[   34] Training loss: 0.00783804, Validation loss: 0.01128809, Gradient norm: 0.24866493
INFO:root:[   35] Training loss: 0.00798845, Validation loss: 0.01608997, Gradient norm: 0.29888100
INFO:root:[   36] Training loss: 0.00789753, Validation loss: 0.01368048, Gradient norm: 0.28265015
INFO:root:[   37] Training loss: 0.00726747, Validation loss: 0.01144959, Gradient norm: 0.26741015
INFO:root:[   38] Training loss: 0.00708281, Validation loss: 0.01285754, Gradient norm: 0.25539380
INFO:root:[   39] Training loss: 0.00699892, Validation loss: 0.01679287, Gradient norm: 0.24741640
INFO:root:[   40] Training loss: 0.00705228, Validation loss: 0.01173960, Gradient norm: 0.28939670
INFO:root:[   41] Training loss: 0.00654962, Validation loss: 0.01271280, Gradient norm: 0.19078929
INFO:root:[   42] Training loss: 0.00702966, Validation loss: 0.01162755, Gradient norm: 0.24348278
INFO:root:[   43] Training loss: 0.00748518, Validation loss: 0.01105525, Gradient norm: 0.29316027
INFO:root:[   44] Training loss: 0.00687500, Validation loss: 0.01269352, Gradient norm: 0.25275588
INFO:root:[   45] Training loss: 0.00686746, Validation loss: 0.01425474, Gradient norm: 0.28778786
INFO:root:[   46] Training loss: 0.00665982, Validation loss: 0.01100897, Gradient norm: 0.22126013
INFO:root:[   47] Training loss: 0.00721519, Validation loss: 0.01191802, Gradient norm: 0.31189565
INFO:root:[   48] Training loss: 0.00676089, Validation loss: 0.01161456, Gradient norm: 0.27930134
INFO:root:[   49] Training loss: 0.00655097, Validation loss: 0.01334751, Gradient norm: 0.25593023
INFO:root:[   50] Training loss: 0.00640195, Validation loss: 0.01297624, Gradient norm: 0.27701703
INFO:root:[   51] Training loss: 0.00608962, Validation loss: 0.01333861, Gradient norm: 0.20107287
INFO:root:[   52] Training loss: 0.00616895, Validation loss: 0.01062709, Gradient norm: 0.25833879
INFO:root:[   53] Training loss: 0.00648849, Validation loss: 0.01211442, Gradient norm: 0.24882819
INFO:root:[   54] Training loss: 0.00671166, Validation loss: 0.01078589, Gradient norm: 0.24668837
INFO:root:[   55] Training loss: 0.00597485, Validation loss: 0.01438385, Gradient norm: 0.17852187
INFO:root:[   56] Training loss: 0.00611064, Validation loss: 0.01409437, Gradient norm: 0.27162106
INFO:root:[   57] Training loss: 0.00584258, Validation loss: 0.01389640, Gradient norm: 0.24844666
INFO:root:[   58] Training loss: 0.00643746, Validation loss: 0.01111358, Gradient norm: 0.24989935
INFO:root:[   59] Training loss: 0.00598403, Validation loss: 0.01214337, Gradient norm: 0.24057264
INFO:root:[   60] Training loss: 0.00640763, Validation loss: 0.01030601, Gradient norm: 0.27837728
INFO:root:[   61] Training loss: 0.00626272, Validation loss: 0.01407128, Gradient norm: 0.28744847
INFO:root:[   62] Training loss: 0.00577230, Validation loss: 0.01140451, Gradient norm: 0.22315937
INFO:root:[   63] Training loss: 0.00593473, Validation loss: 0.01010284, Gradient norm: 0.23205756
INFO:root:[   64] Training loss: 0.00598903, Validation loss: 0.01071030, Gradient norm: 0.22613711
INFO:root:[   65] Training loss: 0.00598440, Validation loss: 0.01115026, Gradient norm: 0.25770066
INFO:root:[   66] Training loss: 0.00564278, Validation loss: 0.01102231, Gradient norm: 0.23580123
INFO:root:[   67] Training loss: 0.00570710, Validation loss: 0.01112334, Gradient norm: 0.22258195
INFO:root:[   68] Training loss: 0.00575318, Validation loss: 0.01009834, Gradient norm: 0.25490538
INFO:root:[   69] Training loss: 0.00566985, Validation loss: 0.01147795, Gradient norm: 0.25597586
INFO:root:[   70] Training loss: 0.00566568, Validation loss: 0.01080697, Gradient norm: 0.23488174
INFO:root:[   71] Training loss: 0.00563381, Validation loss: 0.01050278, Gradient norm: 0.20789476
INFO:root:[   72] Training loss: 0.00562888, Validation loss: 0.01095229, Gradient norm: 0.23284662
INFO:root:[   73] Training loss: 0.00587916, Validation loss: 0.01212802, Gradient norm: 0.22752770
INFO:root:[   74] Training loss: 0.00538981, Validation loss: 0.01075481, Gradient norm: 0.22617473
INFO:root:[   75] Training loss: 0.00562594, Validation loss: 0.01006396, Gradient norm: 0.25272721
INFO:root:[   76] Training loss: 0.00615341, Validation loss: 0.01113158, Gradient norm: 0.28866339
INFO:root:[   77] Training loss: 0.00536544, Validation loss: 0.01029284, Gradient norm: 0.24323810
INFO:root:[   78] Training loss: 0.00523284, Validation loss: 0.01405720, Gradient norm: 0.25366960
INFO:root:[   79] Training loss: 0.00572041, Validation loss: 0.01448443, Gradient norm: 0.26182290
INFO:root:[   80] Training loss: 0.00596594, Validation loss: 0.01014656, Gradient norm: 0.27692473
INFO:root:[   81] Training loss: 0.00573946, Validation loss: 0.01125237, Gradient norm: 0.26561449
INFO:root:[   82] Training loss: 0.00528196, Validation loss: 0.01070067, Gradient norm: 0.22917108
INFO:root:[   83] Training loss: 0.00547475, Validation loss: 0.01391368, Gradient norm: 0.25236706
INFO:root:[   84] Training loss: 0.00525447, Validation loss: 0.01321855, Gradient norm: 0.24856163
INFO:root:EP 84: Early stopping
INFO:root:Training the model took 2240.583s.
INFO:root:Emptying the cuda cache took 0.057s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0099
INFO:root:EnergyScoreTrain: 0.00707
INFO:root:CoverageTrain: 0.98746
INFO:root:IntervalWidthTrain: 0.04479
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01395
INFO:root:EnergyScoreValidation: 0.01008
INFO:root:CoverageValidation: 0.91202
INFO:root:IntervalWidthValidation: 0.04644
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01375
INFO:root:EnergyScoreTest: 0.00993
INFO:root:CoverageTest: 0.90954
INFO:root:IntervalWidthTest: 0.04655
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04425635, Validation loss: 0.02489378, Gradient norm: 0.58106216
INFO:root:[    2] Training loss: 0.02170396, Validation loss: 0.01937051, Gradient norm: 0.47927697
INFO:root:[    3] Training loss: 0.01860186, Validation loss: 0.03074108, Gradient norm: 0.45033820
INFO:root:[    4] Training loss: 0.01708510, Validation loss: 0.01952338, Gradient norm: 0.44792581
INFO:root:[    5] Training loss: 0.01475617, Validation loss: 0.01702759, Gradient norm: 0.34431617
INFO:root:[    6] Training loss: 0.01253623, Validation loss: 0.01867764, Gradient norm: 0.26017232
INFO:root:[    7] Training loss: 0.01387840, Validation loss: 0.01743070, Gradient norm: 0.35906576
INFO:root:[    8] Training loss: 0.01261538, Validation loss: 0.01842153, Gradient norm: 0.32680409
INFO:root:[    9] Training loss: 0.01156115, Validation loss: 0.01735037, Gradient norm: 0.24675792
INFO:root:[   10] Training loss: 0.01299568, Validation loss: 0.01706760, Gradient norm: 0.37250170
INFO:root:[   11] Training loss: 0.01102757, Validation loss: 0.01630524, Gradient norm: 0.27963638
INFO:root:[   12] Training loss: 0.01185118, Validation loss: 0.02102153, Gradient norm: 0.29798491
INFO:root:[   13] Training loss: 0.01045874, Validation loss: 0.01487153, Gradient norm: 0.22872058
INFO:root:[   14] Training loss: 0.01072293, Validation loss: 0.01317004, Gradient norm: 0.25088256
INFO:root:[   15] Training loss: 0.01025933, Validation loss: 0.01770192, Gradient norm: 0.25580458
INFO:root:[   16] Training loss: 0.01023510, Validation loss: 0.01627684, Gradient norm: 0.25378801
INFO:root:[   17] Training loss: 0.01008245, Validation loss: 0.01391382, Gradient norm: 0.25133528
INFO:root:[   18] Training loss: 0.01066829, Validation loss: 0.01430229, Gradient norm: 0.29678369
INFO:root:[   19] Training loss: 0.00933179, Validation loss: 0.01598490, Gradient norm: 0.22602631
INFO:root:[   20] Training loss: 0.01036812, Validation loss: 0.01707402, Gradient norm: 0.29255634
INFO:root:[   21] Training loss: 0.00914503, Validation loss: 0.01445733, Gradient norm: 0.21782569
INFO:root:[   22] Training loss: 0.00994940, Validation loss: 0.01583316, Gradient norm: 0.33443443
INFO:root:[   23] Training loss: 0.00888595, Validation loss: 0.01302303, Gradient norm: 0.18215712
INFO:root:[   24] Training loss: 0.00828598, Validation loss: 0.01621139, Gradient norm: 0.19863732
INFO:root:[   25] Training loss: 0.00896571, Validation loss: 0.01835136, Gradient norm: 0.25913165
INFO:root:[   26] Training loss: 0.00945359, Validation loss: 0.01170244, Gradient norm: 0.25437225
INFO:root:[   27] Training loss: 0.00867882, Validation loss: 0.01156867, Gradient norm: 0.27006693
INFO:root:[   28] Training loss: 0.00880347, Validation loss: 0.01656965, Gradient norm: 0.26877380
INFO:root:[   29] Training loss: 0.00847537, Validation loss: 0.01201597, Gradient norm: 0.21593854
INFO:root:[   30] Training loss: 0.00864122, Validation loss: 0.01283976, Gradient norm: 0.22945319
INFO:root:[   31] Training loss: 0.00821792, Validation loss: 0.01131997, Gradient norm: 0.24011771
INFO:root:[   32] Training loss: 0.00845397, Validation loss: 0.01191638, Gradient norm: 0.25181015
INFO:root:[   33] Training loss: 0.00760508, Validation loss: 0.01329424, Gradient norm: 0.18860402
INFO:root:[   34] Training loss: 0.00903815, Validation loss: 0.01225734, Gradient norm: 0.26553411
INFO:root:[   35] Training loss: 0.00747345, Validation loss: 0.01292247, Gradient norm: 0.17313853
INFO:root:[   36] Training loss: 0.00795188, Validation loss: 0.01247271, Gradient norm: 0.24576916
INFO:root:[   37] Training loss: 0.00772852, Validation loss: 0.01113897, Gradient norm: 0.26440904
INFO:root:[   38] Training loss: 0.00815171, Validation loss: 0.01792334, Gradient norm: 0.24986382
INFO:root:[   39] Training loss: 0.00714085, Validation loss: 0.01243482, Gradient norm: 0.18839090
INFO:root:[   40] Training loss: 0.00729518, Validation loss: 0.01358215, Gradient norm: 0.21772620
INFO:root:[   41] Training loss: 0.00706298, Validation loss: 0.01355574, Gradient norm: 0.22344099
INFO:root:[   42] Training loss: 0.00787190, Validation loss: 0.01277180, Gradient norm: 0.28429446
INFO:root:[   43] Training loss: 0.00716577, Validation loss: 0.01171196, Gradient norm: 0.22518768
INFO:root:[   44] Training loss: 0.00771058, Validation loss: 0.01250127, Gradient norm: 0.25362869
INFO:root:[   45] Training loss: 0.00672229, Validation loss: 0.01197763, Gradient norm: 0.22070923
INFO:root:[   46] Training loss: 0.00676350, Validation loss: 0.01149226, Gradient norm: 0.19024863
INFO:root:[   47] Training loss: 0.00714643, Validation loss: 0.01229675, Gradient norm: 0.22893605
INFO:root:[   48] Training loss: 0.00691930, Validation loss: 0.01382706, Gradient norm: 0.22290964
INFO:root:[   49] Training loss: 0.00692097, Validation loss: 0.01226130, Gradient norm: 0.22974661
INFO:root:[   50] Training loss: 0.00656846, Validation loss: 0.01487657, Gradient norm: 0.22344586
INFO:root:[   51] Training loss: 0.00720631, Validation loss: 0.01118853, Gradient norm: 0.25762023
INFO:root:[   52] Training loss: 0.00698777, Validation loss: 0.01226050, Gradient norm: 0.24134506
INFO:root:[   53] Training loss: 0.00725928, Validation loss: 0.01250817, Gradient norm: 0.28843681
INFO:root:[   54] Training loss: 0.00655538, Validation loss: 0.01505453, Gradient norm: 0.24814597
INFO:root:[   55] Training loss: 0.00705781, Validation loss: 0.01212779, Gradient norm: 0.26585874
INFO:root:[   56] Training loss: 0.00671437, Validation loss: 0.01042953, Gradient norm: 0.23809470
INFO:root:[   57] Training loss: 0.00671993, Validation loss: 0.01324411, Gradient norm: 0.21407258
INFO:root:[   58] Training loss: 0.00694978, Validation loss: 0.01150637, Gradient norm: 0.25082788
INFO:root:[   59] Training loss: 0.00640235, Validation loss: 0.01479800, Gradient norm: 0.21579012
INFO:root:[   60] Training loss: 0.00634442, Validation loss: 0.01200012, Gradient norm: 0.24413175
INFO:root:[   61] Training loss: 0.00622949, Validation loss: 0.01227156, Gradient norm: 0.20691077
INFO:root:[   62] Training loss: 0.00651089, Validation loss: 0.01435355, Gradient norm: 0.23356828
INFO:root:[   63] Training loss: 0.00629359, Validation loss: 0.01453678, Gradient norm: 0.27421519
INFO:root:[   64] Training loss: 0.00652435, Validation loss: 0.01329229, Gradient norm: 0.24160807
INFO:root:[   65] Training loss: 0.00669007, Validation loss: 0.01227259, Gradient norm: 0.24983369
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1731.222s.
INFO:root:Emptying the cuda cache took 0.051s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00766
INFO:root:EnergyScoreTrain: 0.00604
INFO:root:CoverageTrain: 0.99661
INFO:root:IntervalWidthTrain: 0.05193
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01446
INFO:root:EnergyScoreValidation: 0.01039
INFO:root:CoverageValidation: 0.94931
INFO:root:IntervalWidthValidation: 0.05355
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01463
INFO:root:EnergyScoreTest: 0.01055
INFO:root:CoverageTest: 0.94242
INFO:root:IntervalWidthTest: 0.0536
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04817111, Validation loss: 0.02972597, Gradient norm: 0.60126615
INFO:root:[    2] Training loss: 0.02379423, Validation loss: 0.02373818, Gradient norm: 0.49740113
INFO:root:[    3] Training loss: 0.01811075, Validation loss: 0.02162455, Gradient norm: 0.35675814
INFO:root:[    4] Training loss: 0.01850028, Validation loss: 0.01942365, Gradient norm: 0.45572335
INFO:root:[    5] Training loss: 0.01541730, Validation loss: 0.02071798, Gradient norm: 0.32335830
INFO:root:[    6] Training loss: 0.01512205, Validation loss: 0.02069879, Gradient norm: 0.38030244
INFO:root:[    7] Training loss: 0.01349264, Validation loss: 0.01813877, Gradient norm: 0.31523907
INFO:root:[    8] Training loss: 0.01251250, Validation loss: 0.01728168, Gradient norm: 0.27803838
INFO:root:[    9] Training loss: 0.01308573, Validation loss: 0.01712368, Gradient norm: 0.32621408
INFO:root:[   10] Training loss: 0.01303324, Validation loss: 0.01504022, Gradient norm: 0.34140839
INFO:root:[   11] Training loss: 0.01140453, Validation loss: 0.01782549, Gradient norm: 0.24811361
INFO:root:[   12] Training loss: 0.01169344, Validation loss: 0.01484956, Gradient norm: 0.31341000
INFO:root:[   13] Training loss: 0.01201648, Validation loss: 0.01450501, Gradient norm: 0.35425882
INFO:root:[   14] Training loss: 0.01133186, Validation loss: 0.01597365, Gradient norm: 0.27373453
INFO:root:[   15] Training loss: 0.01125104, Validation loss: 0.01688279, Gradient norm: 0.30015970
INFO:root:[   16] Training loss: 0.01156002, Validation loss: 0.01443908, Gradient norm: 0.35144565
INFO:root:[   17] Training loss: 0.01090842, Validation loss: 0.01531374, Gradient norm: 0.28663254
INFO:root:[   18] Training loss: 0.01074727, Validation loss: 0.02247920, Gradient norm: 0.28226890
INFO:root:[   19] Training loss: 0.01029904, Validation loss: 0.01279941, Gradient norm: 0.27504154
INFO:root:[   20] Training loss: 0.00974504, Validation loss: 0.01887269, Gradient norm: 0.25204669
INFO:root:[   21] Training loss: 0.00985972, Validation loss: 0.01856741, Gradient norm: 0.29567003
INFO:root:[   22] Training loss: 0.00973380, Validation loss: 0.01521304, Gradient norm: 0.24363447
INFO:root:[   23] Training loss: 0.01000085, Validation loss: 0.01301946, Gradient norm: 0.23440409
INFO:root:[   24] Training loss: 0.00925494, Validation loss: 0.02261843, Gradient norm: 0.25768161
INFO:root:[   25] Training loss: 0.00968726, Validation loss: 0.01406585, Gradient norm: 0.24768594
INFO:root:[   26] Training loss: 0.01109613, Validation loss: 0.01319298, Gradient norm: 0.37891133
INFO:root:[   27] Training loss: 0.00907098, Validation loss: 0.01426196, Gradient norm: 0.23048675
INFO:root:[   28] Training loss: 0.00942524, Validation loss: 0.01275662, Gradient norm: 0.22529189
INFO:root:[   29] Training loss: 0.00915287, Validation loss: 0.01267195, Gradient norm: 0.21995104
INFO:root:[   30] Training loss: 0.00921227, Validation loss: 0.01271216, Gradient norm: 0.26324792
INFO:root:[   31] Training loss: 0.00948825, Validation loss: 0.01320283, Gradient norm: 0.32127932
INFO:root:[   32] Training loss: 0.00874179, Validation loss: 0.01212347, Gradient norm: 0.23047725
INFO:root:[   33] Training loss: 0.00819647, Validation loss: 0.01227337, Gradient norm: 0.21802462
INFO:root:[   34] Training loss: 0.00796914, Validation loss: 0.01388411, Gradient norm: 0.21189812
INFO:root:[   35] Training loss: 0.00914345, Validation loss: 0.01189598, Gradient norm: 0.29606720
INFO:root:[   36] Training loss: 0.00953143, Validation loss: 0.01439446, Gradient norm: 0.32730345
INFO:root:[   37] Training loss: 0.00742284, Validation loss: 0.01167450, Gradient norm: 0.15164374
INFO:root:[   38] Training loss: 0.00792027, Validation loss: 0.01260109, Gradient norm: 0.19438265
INFO:root:[   39] Training loss: 0.00927441, Validation loss: 0.01956395, Gradient norm: 0.33701020
INFO:root:[   40] Training loss: 0.00857783, Validation loss: 0.01337357, Gradient norm: 0.25769270
INFO:root:[   41] Training loss: 0.00792410, Validation loss: 0.01364827, Gradient norm: 0.24729899
INFO:root:[   42] Training loss: 0.00773432, Validation loss: 0.01367911, Gradient norm: 0.21063465
INFO:root:[   43] Training loss: 0.00801955, Validation loss: 0.01298849, Gradient norm: 0.23526815
INFO:root:[   44] Training loss: 0.00778635, Validation loss: 0.01210112, Gradient norm: 0.23984323
INFO:root:[   45] Training loss: 0.00853844, Validation loss: 0.01463967, Gradient norm: 0.30618917
INFO:root:[   46] Training loss: 0.00757134, Validation loss: 0.01269120, Gradient norm: 0.22819981
INFO:root:[   47] Training loss: 0.00775012, Validation loss: 0.01469598, Gradient norm: 0.25062726
INFO:root:[   48] Training loss: 0.00766449, Validation loss: 0.01472536, Gradient norm: 0.27433164
INFO:root:[   49] Training loss: 0.00676563, Validation loss: 0.01302406, Gradient norm: 0.19473348
INFO:root:[   50] Training loss: 0.00815236, Validation loss: 0.01314805, Gradient norm: 0.27048211
INFO:root:[   51] Training loss: 0.00837950, Validation loss: 0.01275041, Gradient norm: 0.26656436
INFO:root:[   52] Training loss: 0.00676214, Validation loss: 0.01289766, Gradient norm: 0.19050307
INFO:root:[   53] Training loss: 0.00745519, Validation loss: 0.01413743, Gradient norm: 0.25842254
INFO:root:[   54] Training loss: 0.00755599, Validation loss: 0.01348009, Gradient norm: 0.22958029
INFO:root:[   55] Training loss: 0.00689815, Validation loss: 0.01320003, Gradient norm: 0.22987703
INFO:root:[   56] Training loss: 0.00722616, Validation loss: 0.01259541, Gradient norm: 0.22288948
INFO:root:[   57] Training loss: 0.00702746, Validation loss: 0.01701477, Gradient norm: 0.25202507
INFO:root:[   58] Training loss: 0.00764121, Validation loss: 0.01322448, Gradient norm: 0.30126909
INFO:root:[   59] Training loss: 0.00726203, Validation loss: 0.01418950, Gradient norm: 0.26398024
INFO:root:[   60] Training loss: 0.00747800, Validation loss: 0.01332398, Gradient norm: 0.25449466
INFO:root:[   61] Training loss: 0.00735678, Validation loss: 0.01491438, Gradient norm: 0.22685644
INFO:root:[   62] Training loss: 0.00705236, Validation loss: 0.01455092, Gradient norm: 0.25679471
INFO:root:[   63] Training loss: 0.00729504, Validation loss: 0.01399168, Gradient norm: 0.28548623
INFO:root:[   64] Training loss: 0.00681927, Validation loss: 0.01572852, Gradient norm: 0.22881044
INFO:root:[   65] Training loss: 0.00702248, Validation loss: 0.01341790, Gradient norm: 0.24739306
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1725.465s.
INFO:root:Emptying the cuda cache took 0.051s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00867
INFO:root:EnergyScoreTrain: 0.00708
INFO:root:CoverageTrain: 0.9971
INFO:root:IntervalWidthTrain: 0.06298
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01631
INFO:root:EnergyScoreValidation: 0.0117
INFO:root:CoverageValidation: 0.95623
INFO:root:IntervalWidthValidation: 0.06566
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01658
INFO:root:EnergyScoreTest: 0.01194
INFO:root:CoverageTest: 0.95203
INFO:root:IntervalWidthTest: 0.06547
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 159383552
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04542773, Validation loss: 0.02795230, Gradient norm: 0.47796114
INFO:root:[    2] Training loss: 0.02382369, Validation loss: 0.01920798, Gradient norm: 0.39738384
INFO:root:[    3] Training loss: 0.02029611, Validation loss: 0.02090899, Gradient norm: 0.39884347
INFO:root:[    4] Training loss: 0.01612409, Validation loss: 0.01792975, Gradient norm: 0.29119948
INFO:root:[    5] Training loss: 0.01492667, Validation loss: 0.01848361, Gradient norm: 0.28267683
INFO:root:[    6] Training loss: 0.01647596, Validation loss: 0.01648973, Gradient norm: 0.39482263
INFO:root:[    7] Training loss: 0.01378402, Validation loss: 0.01653757, Gradient norm: 0.30205689
INFO:root:[    8] Training loss: 0.01379106, Validation loss: 0.02025854, Gradient norm: 0.27804150
INFO:root:[    9] Training loss: 0.01246974, Validation loss: 0.01376123, Gradient norm: 0.23838503
INFO:root:[   10] Training loss: 0.01314693, Validation loss: 0.01415580, Gradient norm: 0.34199293
INFO:root:[   11] Training loss: 0.01210847, Validation loss: 0.01297697, Gradient norm: 0.25440382
INFO:root:[   12] Training loss: 0.01161352, Validation loss: 0.01282523, Gradient norm: 0.24840696
INFO:root:[   13] Training loss: 0.01135856, Validation loss: 0.01284817, Gradient norm: 0.26221984
INFO:root:[   14] Training loss: 0.01130895, Validation loss: 0.01269153, Gradient norm: 0.23857150
INFO:root:[   15] Training loss: 0.01111786, Validation loss: 0.01186961, Gradient norm: 0.22056969
INFO:root:[   16] Training loss: 0.01066929, Validation loss: 0.01378291, Gradient norm: 0.22977424
INFO:root:[   17] Training loss: 0.01086323, Validation loss: 0.01893461, Gradient norm: 0.25282441
INFO:root:[   18] Training loss: 0.01138569, Validation loss: 0.01682713, Gradient norm: 0.27091297
INFO:root:[   19] Training loss: 0.00984915, Validation loss: 0.01142238, Gradient norm: 0.18599796
INFO:root:[   20] Training loss: 0.01028877, Validation loss: 0.01277755, Gradient norm: 0.25203400
INFO:root:[   21] Training loss: 0.00991743, Validation loss: 0.01291087, Gradient norm: 0.24572024
INFO:root:[   22] Training loss: 0.01009677, Validation loss: 0.01785951, Gradient norm: 0.25376724
INFO:root:[   23] Training loss: 0.01016164, Validation loss: 0.01150445, Gradient norm: 0.25641728
INFO:root:[   24] Training loss: 0.00954095, Validation loss: 0.01089029, Gradient norm: 0.22342857
INFO:root:[   25] Training loss: 0.01053491, Validation loss: 0.01102215, Gradient norm: 0.28822587
INFO:root:[   26] Training loss: 0.00932945, Validation loss: 0.01062926, Gradient norm: 0.20006053
INFO:root:[   27] Training loss: 0.00958341, Validation loss: 0.01273981, Gradient norm: 0.24509900
INFO:root:[   28] Training loss: 0.00884444, Validation loss: 0.01132650, Gradient norm: 0.19269005
INFO:root:[   29] Training loss: 0.00996623, Validation loss: 0.01203143, Gradient norm: 0.25175425
INFO:root:[   30] Training loss: 0.00886139, Validation loss: 0.01128624, Gradient norm: 0.20139507
INFO:root:[   31] Training loss: 0.00955361, Validation loss: 0.01423990, Gradient norm: 0.28471347
INFO:root:[   32] Training loss: 0.00875337, Validation loss: 0.01058240, Gradient norm: 0.20971236
INFO:root:[   33] Training loss: 0.00884270, Validation loss: 0.01102483, Gradient norm: 0.25799044
INFO:root:[   34] Training loss: 0.00810751, Validation loss: 0.01068455, Gradient norm: 0.20551161
INFO:root:[   35] Training loss: 0.00853152, Validation loss: 0.01061498, Gradient norm: 0.22392340
INFO:root:[   36] Training loss: 0.00876009, Validation loss: 0.01090590, Gradient norm: 0.23987458
INFO:root:[   37] Training loss: 0.00853102, Validation loss: 0.01227320, Gradient norm: 0.21767732
INFO:root:[   38] Training loss: 0.00776961, Validation loss: 0.01068914, Gradient norm: 0.19584449
INFO:root:[   39] Training loss: 0.00832284, Validation loss: 0.01151580, Gradient norm: 0.23436757
INFO:root:[   40] Training loss: 0.00804017, Validation loss: 0.01332905, Gradient norm: 0.22664714
INFO:root:[   41] Training loss: 0.00834890, Validation loss: 0.01108825, Gradient norm: 0.24541162
INFO:root:[   42] Training loss: 0.00826181, Validation loss: 0.01273644, Gradient norm: 0.26286307
INFO:root:[   43] Training loss: 0.00792735, Validation loss: 0.02010020, Gradient norm: 0.22070089
INFO:root:[   44] Training loss: 0.00845506, Validation loss: 0.01178788, Gradient norm: 0.24228741
INFO:root:[   45] Training loss: 0.00787267, Validation loss: 0.01041011, Gradient norm: 0.22567293
INFO:root:[   46] Training loss: 0.00828404, Validation loss: 0.01081549, Gradient norm: 0.24702588
INFO:root:[   47] Training loss: 0.00733627, Validation loss: 0.01672878, Gradient norm: 0.18851902
INFO:root:[   48] Training loss: 0.00833167, Validation loss: 0.01142790, Gradient norm: 0.24831382
INFO:root:[   49] Training loss: 0.00782313, Validation loss: 0.01178717, Gradient norm: 0.24021373
INFO:root:[   50] Training loss: 0.00719691, Validation loss: 0.01246683, Gradient norm: 0.20518203
INFO:root:[   51] Training loss: 0.00746518, Validation loss: 0.01278923, Gradient norm: 0.22191602
INFO:root:[   52] Training loss: 0.00787541, Validation loss: 0.01112938, Gradient norm: 0.24486824
INFO:root:[   53] Training loss: 0.00814165, Validation loss: 0.01072469, Gradient norm: 0.24927112
INFO:root:[   54] Training loss: 0.00710309, Validation loss: 0.01379774, Gradient norm: 0.19242663
INFO:root:[   55] Training loss: 0.00768402, Validation loss: 0.01275690, Gradient norm: 0.26077406
INFO:root:[   56] Training loss: 0.00742102, Validation loss: 0.01370122, Gradient norm: 0.19497484
INFO:root:[   57] Training loss: 0.00756190, Validation loss: 0.01336203, Gradient norm: 0.26021302
INFO:root:[   58] Training loss: 0.00802813, Validation loss: 0.01225622, Gradient norm: 0.29278112
INFO:root:[   59] Training loss: 0.00726765, Validation loss: 0.01421595, Gradient norm: 0.22529749
INFO:root:[   60] Training loss: 0.00742927, Validation loss: 0.01187560, Gradient norm: 0.23886755
INFO:root:[   61] Training loss: 0.00720453, Validation loss: 0.01287685, Gradient norm: 0.18535056
INFO:root:[   62] Training loss: 0.00688772, Validation loss: 0.01130785, Gradient norm: 0.22648250
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1643.126s.
INFO:root:Emptying the cuda cache took 0.052s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01025
INFO:root:EnergyScoreTrain: 0.00789
INFO:root:CoverageTrain: 0.99726
INFO:root:IntervalWidthTrain: 0.06681
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01449
INFO:root:EnergyScoreValidation: 0.01056
INFO:root:CoverageValidation: 0.98337
INFO:root:IntervalWidthValidation: 0.07182
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01462
INFO:root:EnergyScoreTest: 0.01067
INFO:root:CoverageTest: 0.98152
INFO:root:IntervalWidthTest: 0.07195
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 169869312
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04752042, Validation loss: 0.02669152, Gradient norm: 0.47044955
INFO:root:[    2] Training loss: 0.02517755, Validation loss: 0.02465571, Gradient norm: 0.38906087
INFO:root:[    3] Training loss: 0.01901069, Validation loss: 0.01901408, Gradient norm: 0.29554890
INFO:root:[    4] Training loss: 0.01887274, Validation loss: 0.02112120, Gradient norm: 0.33291585
INFO:root:[    5] Training loss: 0.01635145, Validation loss: 0.02016867, Gradient norm: 0.31456441
INFO:root:[    6] Training loss: 0.01672535, Validation loss: 0.01877435, Gradient norm: 0.35277441
INFO:root:[    7] Training loss: 0.01336792, Validation loss: 0.01571887, Gradient norm: 0.16458738
INFO:root:[    8] Training loss: 0.01542957, Validation loss: 0.01424863, Gradient norm: 0.34740076
INFO:root:[    9] Training loss: 0.01241232, Validation loss: 0.01708914, Gradient norm: 0.22225612
INFO:root:[   10] Training loss: 0.01450688, Validation loss: 0.02244754, Gradient norm: 0.32190319
INFO:root:[   11] Training loss: 0.01283471, Validation loss: 0.02035187, Gradient norm: 0.29447172
INFO:root:[   12] Training loss: 0.01283181, Validation loss: 0.01989017, Gradient norm: 0.29091324
INFO:root:[   13] Training loss: 0.01207904, Validation loss: 0.01778406, Gradient norm: 0.26356051
INFO:root:[   14] Training loss: 0.01176337, Validation loss: 0.01302230, Gradient norm: 0.23868782
INFO:root:[   15] Training loss: 0.01214228, Validation loss: 0.02141626, Gradient norm: 0.27586688
INFO:root:[   16] Training loss: 0.01108205, Validation loss: 0.01281015, Gradient norm: 0.21881558
INFO:root:[   17] Training loss: 0.01178415, Validation loss: 0.01324216, Gradient norm: 0.25900720
INFO:root:[   18] Training loss: 0.01140633, Validation loss: 0.01288221, Gradient norm: 0.23379373
INFO:root:[   19] Training loss: 0.01150303, Validation loss: 0.01359086, Gradient norm: 0.28312153
INFO:root:[   20] Training loss: 0.01016582, Validation loss: 0.01720673, Gradient norm: 0.21305515
INFO:root:[   21] Training loss: 0.01084521, Validation loss: 0.01401665, Gradient norm: 0.23156784
INFO:root:[   22] Training loss: 0.01045862, Validation loss: 0.01522509, Gradient norm: 0.25016441
INFO:root:[   23] Training loss: 0.00995462, Validation loss: 0.01289790, Gradient norm: 0.21989731
INFO:root:[   24] Training loss: 0.01059870, Validation loss: 0.01378268, Gradient norm: 0.24350392
INFO:root:[   25] Training loss: 0.01035134, Validation loss: 0.01318914, Gradient norm: 0.24825525
INFO:root:[   26] Training loss: 0.01028845, Validation loss: 0.02462729, Gradient norm: 0.26981746
INFO:root:[   27] Training loss: 0.00991581, Validation loss: 0.01155844, Gradient norm: 0.20084197
INFO:root:[   28] Training loss: 0.01007984, Validation loss: 0.01274927, Gradient norm: 0.20754224
INFO:root:[   29] Training loss: 0.00917148, Validation loss: 0.01382227, Gradient norm: 0.19957187
INFO:root:[   30] Training loss: 0.01085625, Validation loss: 0.01745250, Gradient norm: 0.26827120
INFO:root:[   31] Training loss: 0.00904482, Validation loss: 0.01341611, Gradient norm: 0.18890246
INFO:root:[   32] Training loss: 0.01001625, Validation loss: 0.01391618, Gradient norm: 0.30445270
INFO:root:[   33] Training loss: 0.00947383, Validation loss: 0.01221295, Gradient norm: 0.24994264
INFO:root:[   34] Training loss: 0.00896162, Validation loss: 0.01605772, Gradient norm: 0.21123870
INFO:root:[   35] Training loss: 0.00965473, Validation loss: 0.01340156, Gradient norm: 0.23587620
INFO:root:[   36] Training loss: 0.00974865, Validation loss: 0.01615443, Gradient norm: 0.29194354
INFO:root:[   37] Training loss: 0.00926768, Validation loss: 0.01242291, Gradient norm: 0.22468720
INFO:root:[   38] Training loss: 0.00926806, Validation loss: 0.01246678, Gradient norm: 0.25896146
INFO:root:[   39] Training loss: 0.00930214, Validation loss: 0.01515172, Gradient norm: 0.28204054
INFO:root:[   40] Training loss: 0.00874634, Validation loss: 0.01650976, Gradient norm: 0.20282089
INFO:root:[   41] Training loss: 0.00829024, Validation loss: 0.01419728, Gradient norm: 0.20485924
INFO:root:[   42] Training loss: 0.00929268, Validation loss: 0.01218262, Gradient norm: 0.28201876
INFO:root:[   43] Training loss: 0.00880777, Validation loss: 0.01227634, Gradient norm: 0.23332979
INFO:root:[   44] Training loss: 0.00897239, Validation loss: 0.01235892, Gradient norm: 0.26987815
INFO:root:[   45] Training loss: 0.00838586, Validation loss: 0.01259871, Gradient norm: 0.17024301
INFO:root:[   46] Training loss: 0.00832652, Validation loss: 0.01234822, Gradient norm: 0.21532203
INFO:root:[   47] Training loss: 0.00886825, Validation loss: 0.01190641, Gradient norm: 0.23574752
INFO:root:[   48] Training loss: 0.00809012, Validation loss: 0.01225721, Gradient norm: 0.19276722
INFO:root:[   49] Training loss: 0.00847171, Validation loss: 0.01362196, Gradient norm: 0.22420403
INFO:root:[   50] Training loss: 0.00812014, Validation loss: 0.01272589, Gradient norm: 0.22200286
INFO:root:[   51] Training loss: 0.00861845, Validation loss: 0.01205614, Gradient norm: 0.22369730
INFO:root:[   52] Training loss: 0.00849294, Validation loss: 0.01200918, Gradient norm: 0.25454654
INFO:root:[   53] Training loss: 0.00876294, Validation loss: 0.01331129, Gradient norm: 0.23662926
INFO:root:[   54] Training loss: 0.00815172, Validation loss: 0.01444515, Gradient norm: 0.21342125
INFO:root:[   55] Training loss: 0.00789435, Validation loss: 0.01299647, Gradient norm: 0.23525954
INFO:root:[   56] Training loss: 0.00823102, Validation loss: 0.01206754, Gradient norm: 0.22149011
INFO:root:[   57] Training loss: 0.00841752, Validation loss: 0.01707328, Gradient norm: 0.23419656
INFO:root:[   58] Training loss: 0.00786565, Validation loss: 0.01602861, Gradient norm: 0.23226026
INFO:root:[   59] Training loss: 0.00825648, Validation loss: 0.01240108, Gradient norm: 0.28096595
INFO:root:[   60] Training loss: 0.00826677, Validation loss: 0.01321313, Gradient norm: 0.24797905
INFO:root:[   61] Training loss: 0.00760468, Validation loss: 0.01196736, Gradient norm: 0.21596058
INFO:root:[   62] Training loss: 0.00758188, Validation loss: 0.01496984, Gradient norm: 0.22232902
INFO:root:[   63] Training loss: 0.00777807, Validation loss: 0.01144770, Gradient norm: 0.25329137
INFO:root:[   64] Training loss: 0.00863703, Validation loss: 0.01827143, Gradient norm: 0.26518681
INFO:root:[   65] Training loss: 0.00762321, Validation loss: 0.01331309, Gradient norm: 0.24986159
INFO:root:[   66] Training loss: 0.00844447, Validation loss: 0.01349874, Gradient norm: 0.28240842
INFO:root:[   67] Training loss: 0.00767486, Validation loss: 0.01327390, Gradient norm: 0.21760690
INFO:root:[   68] Training loss: 0.00805865, Validation loss: 0.01804761, Gradient norm: 0.23591310
INFO:root:[   69] Training loss: 0.00715123, Validation loss: 0.01519242, Gradient norm: 0.21270658
INFO:root:[   70] Training loss: 0.00737317, Validation loss: 0.01318567, Gradient norm: 0.26439848
INFO:root:[   71] Training loss: 0.00784885, Validation loss: 0.01608165, Gradient norm: 0.27240345
INFO:root:[   72] Training loss: 0.00739372, Validation loss: 0.01341185, Gradient norm: 0.18874637
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 1939.138s.
INFO:root:Emptying the cuda cache took 0.055s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00939
INFO:root:EnergyScoreTrain: 0.00739
INFO:root:CoverageTrain: 0.99825
INFO:root:IntervalWidthTrain: 0.06551
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01543
INFO:root:EnergyScoreValidation: 0.01147
INFO:root:CoverageValidation: 0.98787
INFO:root:IntervalWidthValidation: 0.08013
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01563
INFO:root:EnergyScoreTest: 0.01159
INFO:root:CoverageTest: 0.98668
INFO:root:IntervalWidthTest: 0.08045
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05591513, Validation loss: 0.02696494, Gradient norm: 0.70922486
INFO:root:[    2] Training loss: 0.02599711, Validation loss: 0.01987812, Gradient norm: 0.63487094
INFO:root:[    3] Training loss: 0.02056477, Validation loss: 0.02644882, Gradient norm: 0.58532471
INFO:root:[    4] Training loss: 0.01860776, Validation loss: 0.01644404, Gradient norm: 0.52275105
INFO:root:[    5] Training loss: 0.01644256, Validation loss: 0.01487459, Gradient norm: 0.51517781
INFO:root:[    6] Training loss: 0.01402206, Validation loss: 0.01324309, Gradient norm: 0.40344097
INFO:root:[    7] Training loss: 0.01434638, Validation loss: 0.02127905, Gradient norm: 0.47912684
INFO:root:[    8] Training loss: 0.01344712, Validation loss: 0.01284767, Gradient norm: 0.43189268
INFO:root:[    9] Training loss: 0.01298861, Validation loss: 0.01313543, Gradient norm: 0.41435017
INFO:root:[   10] Training loss: 0.01240027, Validation loss: 0.01400958, Gradient norm: 0.41896569
INFO:root:[   11] Training loss: 0.01248406, Validation loss: 0.01372340, Gradient norm: 0.44987381
INFO:root:[   12] Training loss: 0.01202932, Validation loss: 0.01285165, Gradient norm: 0.41785564
INFO:root:[   13] Training loss: 0.01209182, Validation loss: 0.01726639, Gradient norm: 0.43640327
INFO:root:[   14] Training loss: 0.01128635, Validation loss: 0.01397351, Gradient norm: 0.40165985
INFO:root:[   15] Training loss: 0.01081025, Validation loss: 0.01170309, Gradient norm: 0.38005794
INFO:root:[   16] Training loss: 0.01098204, Validation loss: 0.01309806, Gradient norm: 0.36172125
INFO:root:[   17] Training loss: 0.00994488, Validation loss: 0.01127363, Gradient norm: 0.31986311
INFO:root:[   18] Training loss: 0.01086463, Validation loss: 0.01684031, Gradient norm: 0.42044808
INFO:root:[   19] Training loss: 0.01034065, Validation loss: 0.01163208, Gradient norm: 0.35928750
INFO:root:[   20] Training loss: 0.01099753, Validation loss: 0.01461580, Gradient norm: 0.40564289
INFO:root:[   21] Training loss: 0.00974467, Validation loss: 0.01124420, Gradient norm: 0.34452792
INFO:root:[   22] Training loss: 0.00940684, Validation loss: 0.01130503, Gradient norm: 0.29742072
INFO:root:[   23] Training loss: 0.00935775, Validation loss: 0.01140559, Gradient norm: 0.31106234
INFO:root:[   24] Training loss: 0.00978734, Validation loss: 0.01134365, Gradient norm: 0.37530301
INFO:root:[   25] Training loss: 0.00925128, Validation loss: 0.01170448, Gradient norm: 0.32191907
INFO:root:[   26] Training loss: 0.00916008, Validation loss: 0.01268049, Gradient norm: 0.33166053
INFO:root:[   27] Training loss: 0.00843976, Validation loss: 0.01440143, Gradient norm: 0.29651093
INFO:root:[   28] Training loss: 0.00885572, Validation loss: 0.01139955, Gradient norm: 0.31115759
INFO:root:[   29] Training loss: 0.00920065, Validation loss: 0.01190863, Gradient norm: 0.34756772
INFO:root:[   30] Training loss: 0.00885476, Validation loss: 0.01615627, Gradient norm: 0.34779063
INFO:root:[   31] Training loss: 0.00856733, Validation loss: 0.01465435, Gradient norm: 0.31466557
INFO:root:[   32] Training loss: 0.00827061, Validation loss: 0.01066655, Gradient norm: 0.31191014
INFO:root:[   33] Training loss: 0.00825947, Validation loss: 0.01275314, Gradient norm: 0.33595117
INFO:root:[   34] Training loss: 0.00833793, Validation loss: 0.01167322, Gradient norm: 0.33631300
INFO:root:[   35] Training loss: 0.00769677, Validation loss: 0.01373911, Gradient norm: 0.29654809
INFO:root:[   36] Training loss: 0.00780669, Validation loss: 0.01174958, Gradient norm: 0.29343132
INFO:root:[   37] Training loss: 0.00799048, Validation loss: 0.01187344, Gradient norm: 0.28822261
INFO:root:[   38] Training loss: 0.00780991, Validation loss: 0.01069802, Gradient norm: 0.31924215
INFO:root:[   39] Training loss: 0.00799556, Validation loss: 0.01072197, Gradient norm: 0.34704838
INFO:root:[   40] Training loss: 0.00756737, Validation loss: 0.01328313, Gradient norm: 0.30372927
INFO:root:[   41] Training loss: 0.00750112, Validation loss: 0.01029964, Gradient norm: 0.33842190
INFO:root:[   42] Training loss: 0.00756363, Validation loss: 0.01697333, Gradient norm: 0.30760221
INFO:root:[   43] Training loss: 0.00757056, Validation loss: 0.01501255, Gradient norm: 0.33166495
INFO:root:[   44] Training loss: 0.00726132, Validation loss: 0.01414466, Gradient norm: 0.28532862
INFO:root:[   45] Training loss: 0.00718087, Validation loss: 0.01141709, Gradient norm: 0.32095311
INFO:root:[   46] Training loss: 0.00755221, Validation loss: 0.01079377, Gradient norm: 0.33409968
INFO:root:[   47] Training loss: 0.00722272, Validation loss: 0.01433651, Gradient norm: 0.30477009
INFO:root:[   48] Training loss: 0.00753146, Validation loss: 0.01266875, Gradient norm: 0.30371806
INFO:root:[   49] Training loss: 0.00728383, Validation loss: 0.01182142, Gradient norm: 0.30998650
INFO:root:[   50] Training loss: 0.00675165, Validation loss: 0.01228626, Gradient norm: 0.30474403
INFO:root:[   51] Training loss: 0.00663722, Validation loss: 0.01073155, Gradient norm: 0.28331484
INFO:root:[   52] Training loss: 0.00682989, Validation loss: 0.01034466, Gradient norm: 0.30007113
INFO:root:[   53] Training loss: 0.00689530, Validation loss: 0.01294151, Gradient norm: 0.31536737
INFO:root:[   54] Training loss: 0.00686172, Validation loss: 0.01027618, Gradient norm: 0.31077758
INFO:root:[   55] Training loss: 0.00643364, Validation loss: 0.01055170, Gradient norm: 0.29426951
INFO:root:[   56] Training loss: 0.00674099, Validation loss: 0.01070528, Gradient norm: 0.31204251
INFO:root:[   57] Training loss: 0.00706211, Validation loss: 0.01397673, Gradient norm: 0.32075200
INFO:root:[   58] Training loss: 0.00661454, Validation loss: 0.01293807, Gradient norm: 0.32757821
INFO:root:[   59] Training loss: 0.00700042, Validation loss: 0.01365009, Gradient norm: 0.33234902
INFO:root:[   60] Training loss: 0.00643160, Validation loss: 0.01464187, Gradient norm: 0.28948022
INFO:root:[   61] Training loss: 0.00678218, Validation loss: 0.01259568, Gradient norm: 0.27070594
INFO:root:[   62] Training loss: 0.00649791, Validation loss: 0.01095659, Gradient norm: 0.29789427
INFO:root:[   63] Training loss: 0.00649072, Validation loss: 0.01098615, Gradient norm: 0.28732179
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 1195.433s.
INFO:root:Emptying the cuda cache took 0.033s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00801
INFO:root:EnergyScoreTrain: 0.0058
INFO:root:CoverageTrain: 0.76977
INFO:root:IntervalWidthTrain: 0.02546
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01373
INFO:root:EnergyScoreValidation: 0.01041
INFO:root:CoverageValidation: 0.62802
INFO:root:IntervalWidthValidation: 0.025
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01356
INFO:root:EnergyScoreTest: 0.01026
INFO:root:CoverageTest: 0.62688
INFO:root:IntervalWidthTest: 0.02504
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05806160, Validation loss: 0.03976265, Gradient norm: 0.65401533
INFO:root:[    2] Training loss: 0.02775416, Validation loss: 0.03428658, Gradient norm: 0.52282674
INFO:root:[    3] Training loss: 0.02120969, Validation loss: 0.02609439, Gradient norm: 0.43507899
INFO:root:[    4] Training loss: 0.01913564, Validation loss: 0.01568262, Gradient norm: 0.42133704
INFO:root:[    5] Training loss: 0.01742051, Validation loss: 0.02128879, Gradient norm: 0.39356944
INFO:root:[    6] Training loss: 0.01663191, Validation loss: 0.01511113, Gradient norm: 0.39234134
INFO:root:[    7] Training loss: 0.01575092, Validation loss: 0.01631465, Gradient norm: 0.38428181
INFO:root:[    8] Training loss: 0.01464243, Validation loss: 0.01686694, Gradient norm: 0.35390218
INFO:root:[    9] Training loss: 0.01348831, Validation loss: 0.01260941, Gradient norm: 0.32298449
INFO:root:[   10] Training loss: 0.01380004, Validation loss: 0.01657184, Gradient norm: 0.36173851
INFO:root:[   11] Training loss: 0.01420853, Validation loss: 0.01416505, Gradient norm: 0.38894328
INFO:root:[   12] Training loss: 0.01348833, Validation loss: 0.02077767, Gradient norm: 0.29674314
INFO:root:[   13] Training loss: 0.01361353, Validation loss: 0.01349415, Gradient norm: 0.31963718
INFO:root:[   14] Training loss: 0.01251764, Validation loss: 0.01317651, Gradient norm: 0.35504608
INFO:root:[   15] Training loss: 0.01233122, Validation loss: 0.01268114, Gradient norm: 0.32233004
INFO:root:[   16] Training loss: 0.01110892, Validation loss: 0.01454628, Gradient norm: 0.23665369
INFO:root:[   17] Training loss: 0.01237712, Validation loss: 0.01544189, Gradient norm: 0.35351897
INFO:root:[   18] Training loss: 0.01182712, Validation loss: 0.01204862, Gradient norm: 0.32032099
INFO:root:[   19] Training loss: 0.01204350, Validation loss: 0.01501787, Gradient norm: 0.31407593
INFO:root:[   20] Training loss: 0.01051880, Validation loss: 0.01210899, Gradient norm: 0.21568935
INFO:root:[   21] Training loss: 0.01167939, Validation loss: 0.01188782, Gradient norm: 0.31967242
INFO:root:[   22] Training loss: 0.01129130, Validation loss: 0.01653067, Gradient norm: 0.29533957
INFO:root:[   23] Training loss: 0.01056128, Validation loss: 0.01188113, Gradient norm: 0.28622978
INFO:root:[   24] Training loss: 0.01079984, Validation loss: 0.01467013, Gradient norm: 0.26764818
INFO:root:[   25] Training loss: 0.01106284, Validation loss: 0.01313466, Gradient norm: 0.31342194
INFO:root:[   26] Training loss: 0.01054882, Validation loss: 0.01545587, Gradient norm: 0.26462547
INFO:root:[   27] Training loss: 0.01043621, Validation loss: 0.01333797, Gradient norm: 0.32024665
INFO:root:[   28] Training loss: 0.00997589, Validation loss: 0.01520739, Gradient norm: 0.25881010
INFO:root:[   29] Training loss: 0.01019270, Validation loss: 0.01236015, Gradient norm: 0.27524685
INFO:root:[   30] Training loss: 0.01016458, Validation loss: 0.01343426, Gradient norm: 0.29383387
INFO:root:[   31] Training loss: 0.01012764, Validation loss: 0.01291669, Gradient norm: 0.29829585
INFO:root:[   32] Training loss: 0.00990978, Validation loss: 0.01714680, Gradient norm: 0.31560330
INFO:root:[   33] Training loss: 0.01055096, Validation loss: 0.01746811, Gradient norm: 0.25763189
INFO:root:[   34] Training loss: 0.01010923, Validation loss: 0.01303350, Gradient norm: 0.31076789
INFO:root:[   35] Training loss: 0.00949145, Validation loss: 0.01251965, Gradient norm: 0.29655884
INFO:root:[   36] Training loss: 0.00906680, Validation loss: 0.01223595, Gradient norm: 0.23193213
INFO:root:[   37] Training loss: 0.00923346, Validation loss: 0.01363853, Gradient norm: 0.27496288
INFO:root:[   38] Training loss: 0.00910193, Validation loss: 0.01296070, Gradient norm: 0.25736858
INFO:root:[   39] Training loss: 0.00930080, Validation loss: 0.01572117, Gradient norm: 0.31661319
INFO:root:[   40] Training loss: 0.00938574, Validation loss: 0.01443515, Gradient norm: 0.28303559
INFO:root:[   41] Training loss: 0.00929716, Validation loss: 0.01622346, Gradient norm: 0.26749848
INFO:root:[   42] Training loss: 0.00880619, Validation loss: 0.01210090, Gradient norm: 0.24846318
INFO:root:[   43] Training loss: 0.00904480, Validation loss: 0.01298924, Gradient norm: 0.31916251
INFO:root:[   44] Training loss: 0.00872913, Validation loss: 0.01766544, Gradient norm: 0.26481622
INFO:root:[   45] Training loss: 0.00873210, Validation loss: 0.01240661, Gradient norm: 0.29475120
INFO:root:[   46] Training loss: 0.00901268, Validation loss: 0.01327977, Gradient norm: 0.29429470
INFO:root:[   47] Training loss: 0.00890488, Validation loss: 0.01383737, Gradient norm: 0.27674651
INFO:root:[   48] Training loss: 0.00843749, Validation loss: 0.01186384, Gradient norm: 0.25551420
INFO:root:[   49] Training loss: 0.00852557, Validation loss: 0.01291779, Gradient norm: 0.29204525
INFO:root:[   50] Training loss: 0.00817952, Validation loss: 0.01190825, Gradient norm: 0.27636598
INFO:root:[   51] Training loss: 0.00873145, Validation loss: 0.01289461, Gradient norm: 0.31341619
INFO:root:[   52] Training loss: 0.00883520, Validation loss: 0.01291739, Gradient norm: 0.30509698
INFO:root:[   53] Training loss: 0.00876434, Validation loss: 0.01536088, Gradient norm: 0.27555330
INFO:root:[   54] Training loss: 0.00846902, Validation loss: 0.01411388, Gradient norm: 0.27894518
INFO:root:[   55] Training loss: 0.00781251, Validation loss: 0.01370646, Gradient norm: 0.24705090
INFO:root:[   56] Training loss: 0.00811632, Validation loss: 0.01253845, Gradient norm: 0.29965219
INFO:root:[   57] Training loss: 0.00825606, Validation loss: 0.01393855, Gradient norm: 0.26989081
INFO:root:[   58] Training loss: 0.00767729, Validation loss: 0.01257013, Gradient norm: 0.24266262
INFO:root:[   59] Training loss: 0.00762113, Validation loss: 0.01388661, Gradient norm: 0.27074380
INFO:root:[   60] Training loss: 0.00799103, Validation loss: 0.01250370, Gradient norm: 0.30630230
INFO:root:[   61] Training loss: 0.00774826, Validation loss: 0.01249990, Gradient norm: 0.28157878
INFO:root:[   62] Training loss: 0.00770628, Validation loss: 0.01244543, Gradient norm: 0.25008666
INFO:root:[   63] Training loss: 0.00827728, Validation loss: 0.01336974, Gradient norm: 0.26249714
INFO:root:[   64] Training loss: 0.00819884, Validation loss: 0.01357728, Gradient norm: 0.25527845
INFO:root:[   65] Training loss: 0.00744075, Validation loss: 0.01329821, Gradient norm: 0.28033203
INFO:root:[   66] Training loss: 0.00785656, Validation loss: 0.01208672, Gradient norm: 0.28003409
INFO:root:[   67] Training loss: 0.00703428, Validation loss: 0.01659645, Gradient norm: 0.24340470
INFO:root:[   68] Training loss: 0.00776132, Validation loss: 0.01492902, Gradient norm: 0.31551275
INFO:root:[   69] Training loss: 0.00823944, Validation loss: 0.01496082, Gradient norm: 0.28277831
INFO:root:[   70] Training loss: 0.00761831, Validation loss: 0.01313368, Gradient norm: 0.22901739
INFO:root:[   71] Training loss: 0.00708230, Validation loss: 0.01311015, Gradient norm: 0.26763841
INFO:root:[   72] Training loss: 0.00723339, Validation loss: 0.01275302, Gradient norm: 0.29215131
INFO:root:[   73] Training loss: 0.00744138, Validation loss: 0.01438872, Gradient norm: 0.25729545
INFO:root:[   74] Training loss: 0.00749497, Validation loss: 0.01706320, Gradient norm: 0.29168641
INFO:root:[   75] Training loss: 0.00718209, Validation loss: 0.01401756, Gradient norm: 0.28228680
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 1425.486s.
INFO:root:Emptying the cuda cache took 0.029s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0111
INFO:root:EnergyScoreTrain: 0.00804
INFO:root:CoverageTrain: 0.81931
INFO:root:IntervalWidthTrain: 0.03659
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01585
INFO:root:EnergyScoreValidation: 0.01171
INFO:root:CoverageValidation: 0.68638
INFO:root:IntervalWidthValidation: 0.03714
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01584
INFO:root:EnergyScoreTest: 0.01166
INFO:root:CoverageTest: 0.68058
INFO:root:IntervalWidthTest: 0.03721
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06081951, Validation loss: 0.02465384, Gradient norm: 0.51242017
INFO:root:[    2] Training loss: 0.02921674, Validation loss: 0.02796172, Gradient norm: 0.47848975
INFO:root:[    3] Training loss: 0.02348649, Validation loss: 0.01874716, Gradient norm: 0.43661603
INFO:root:[    4] Training loss: 0.02026521, Validation loss: 0.01588437, Gradient norm: 0.38022635
INFO:root:[    5] Training loss: 0.01862654, Validation loss: 0.01794814, Gradient norm: 0.36064475
INFO:root:[    6] Training loss: 0.01866122, Validation loss: 0.01434699, Gradient norm: 0.40004113
INFO:root:[    7] Training loss: 0.01645037, Validation loss: 0.01511571, Gradient norm: 0.31088620
INFO:root:[    8] Training loss: 0.01536418, Validation loss: 0.01330154, Gradient norm: 0.30203290
INFO:root:[    9] Training loss: 0.01469319, Validation loss: 0.01936001, Gradient norm: 0.29764480
INFO:root:[   10] Training loss: 0.01451556, Validation loss: 0.01166452, Gradient norm: 0.29005612
INFO:root:[   11] Training loss: 0.01350707, Validation loss: 0.01635211, Gradient norm: 0.26406410
INFO:root:[   12] Training loss: 0.01330372, Validation loss: 0.01263104, Gradient norm: 0.28506761
INFO:root:[   13] Training loss: 0.01451223, Validation loss: 0.01395879, Gradient norm: 0.31057406
INFO:root:[   14] Training loss: 0.01422990, Validation loss: 0.01267841, Gradient norm: 0.34508028
INFO:root:[   15] Training loss: 0.01457770, Validation loss: 0.01274901, Gradient norm: 0.33262026
INFO:root:[   16] Training loss: 0.01259880, Validation loss: 0.01148266, Gradient norm: 0.26343482
INFO:root:[   17] Training loss: 0.01376506, Validation loss: 0.01277654, Gradient norm: 0.32680038
INFO:root:[   18] Training loss: 0.01164241, Validation loss: 0.01131958, Gradient norm: 0.23641307
INFO:root:[   19] Training loss: 0.01248826, Validation loss: 0.01895210, Gradient norm: 0.28540310
INFO:root:[   20] Training loss: 0.01263514, Validation loss: 0.01326138, Gradient norm: 0.31271273
INFO:root:[   21] Training loss: 0.01310645, Validation loss: 0.01340023, Gradient norm: 0.28014919
INFO:root:[   22] Training loss: 0.01171108, Validation loss: 0.01156442, Gradient norm: 0.23006904
INFO:root:[   23] Training loss: 0.01271605, Validation loss: 0.01244758, Gradient norm: 0.31106637
INFO:root:[   24] Training loss: 0.01200802, Validation loss: 0.01164673, Gradient norm: 0.30954871
INFO:root:[   25] Training loss: 0.01182234, Validation loss: 0.01240125, Gradient norm: 0.26316253
INFO:root:[   26] Training loss: 0.01141321, Validation loss: 0.01265727, Gradient norm: 0.29406517
INFO:root:[   27] Training loss: 0.01128627, Validation loss: 0.01076679, Gradient norm: 0.29064794
INFO:root:[   28] Training loss: 0.01206135, Validation loss: 0.01072843, Gradient norm: 0.28507227
INFO:root:[   29] Training loss: 0.01105715, Validation loss: 0.01310492, Gradient norm: 0.25728056
INFO:root:[   30] Training loss: 0.01140213, Validation loss: 0.01085120, Gradient norm: 0.27362562
INFO:root:[   31] Training loss: 0.01092804, Validation loss: 0.01140787, Gradient norm: 0.22823815
INFO:root:[   32] Training loss: 0.01128679, Validation loss: 0.01545302, Gradient norm: 0.28425361
INFO:root:[   33] Training loss: 0.01134651, Validation loss: 0.01318226, Gradient norm: 0.30543518
INFO:root:[   34] Training loss: 0.01119055, Validation loss: 0.01014272, Gradient norm: 0.28140413
INFO:root:[   35] Training loss: 0.01063688, Validation loss: 0.01449707, Gradient norm: 0.25231222
INFO:root:[   36] Training loss: 0.01090677, Validation loss: 0.01354305, Gradient norm: 0.26162225
INFO:root:[   37] Training loss: 0.00966710, Validation loss: 0.01064224, Gradient norm: 0.19959513
INFO:root:[   38] Training loss: 0.01145637, Validation loss: 0.01260320, Gradient norm: 0.31893601
INFO:root:[   39] Training loss: 0.01027760, Validation loss: 0.01124592, Gradient norm: 0.21892398
INFO:root:[   40] Training loss: 0.01057095, Validation loss: 0.01454542, Gradient norm: 0.27876926
INFO:root:[   41] Training loss: 0.01051968, Validation loss: 0.01372435, Gradient norm: 0.30509859
INFO:root:[   42] Training loss: 0.01025907, Validation loss: 0.01144732, Gradient norm: 0.27899425
INFO:root:[   43] Training loss: 0.01017953, Validation loss: 0.01377138, Gradient norm: 0.26940645
INFO:root:[   44] Training loss: 0.01007778, Validation loss: 0.01043652, Gradient norm: 0.22678305
INFO:root:[   45] Training loss: 0.00938823, Validation loss: 0.01384359, Gradient norm: 0.23094693
INFO:root:[   46] Training loss: 0.01021696, Validation loss: 0.01229044, Gradient norm: 0.29184220
INFO:root:[   47] Training loss: 0.00953853, Validation loss: 0.01133243, Gradient norm: 0.25862633
INFO:root:[   48] Training loss: 0.01059771, Validation loss: 0.01049215, Gradient norm: 0.29031474
INFO:root:[   49] Training loss: 0.00936260, Validation loss: 0.01194837, Gradient norm: 0.28114656
INFO:root:[   50] Training loss: 0.01040239, Validation loss: 0.01206849, Gradient norm: 0.31194968
INFO:root:[   51] Training loss: 0.00952793, Validation loss: 0.01318023, Gradient norm: 0.27321809
INFO:root:[   52] Training loss: 0.00938163, Validation loss: 0.01204257, Gradient norm: 0.27840190
INFO:root:[   53] Training loss: 0.00997500, Validation loss: 0.01441151, Gradient norm: 0.25773392
INFO:root:[   54] Training loss: 0.00924568, Validation loss: 0.01176320, Gradient norm: 0.24424867
INFO:root:[   55] Training loss: 0.00923571, Validation loss: 0.01181540, Gradient norm: 0.27236523
INFO:root:[   56] Training loss: 0.00960804, Validation loss: 0.01050893, Gradient norm: 0.29734877
INFO:root:[   57] Training loss: 0.00898418, Validation loss: 0.01028917, Gradient norm: 0.24333712
INFO:root:[   58] Training loss: 0.00904076, Validation loss: 0.01184248, Gradient norm: 0.22089191
INFO:root:[   59] Training loss: 0.00959152, Validation loss: 0.01259665, Gradient norm: 0.26505216
INFO:root:[   60] Training loss: 0.00927981, Validation loss: 0.01092740, Gradient norm: 0.29875481
INFO:root:[   61] Training loss: 0.00844927, Validation loss: 0.01137488, Gradient norm: 0.26008458
INFO:root:[   62] Training loss: 0.00848484, Validation loss: 0.01446620, Gradient norm: 0.22951474
INFO:root:[   63] Training loss: 0.00923202, Validation loss: 0.01094289, Gradient norm: 0.27610075
INFO:root:[   64] Training loss: 0.00879760, Validation loss: 0.01057359, Gradient norm: 0.26595911
INFO:root:[   65] Training loss: 0.00895070, Validation loss: 0.01284670, Gradient norm: 0.27032351
INFO:root:[   66] Training loss: 0.00904765, Validation loss: 0.01051940, Gradient norm: 0.25335576
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 1247.208s.
INFO:root:Emptying the cuda cache took 0.028s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00898
INFO:root:EnergyScoreTrain: 0.00677
INFO:root:CoverageTrain: 0.91312
INFO:root:IntervalWidthTrain: 0.03743
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.014
INFO:root:EnergyScoreValidation: 0.0102
INFO:root:CoverageValidation: 0.79612
INFO:root:IntervalWidthValidation: 0.03722
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01437
INFO:root:EnergyScoreTest: 0.01052
INFO:root:CoverageTest: 0.78591
INFO:root:IntervalWidthTest: 0.03729
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05954711, Validation loss: 0.03618174, Gradient norm: 0.51346704
INFO:root:[    2] Training loss: 0.03152376, Validation loss: 0.02338882, Gradient norm: 0.45708884
INFO:root:[    3] Training loss: 0.02677289, Validation loss: 0.01825024, Gradient norm: 0.44304565
INFO:root:[    4] Training loss: 0.02255111, Validation loss: 0.02014977, Gradient norm: 0.38381981
INFO:root:[    5] Training loss: 0.02118284, Validation loss: 0.01450606, Gradient norm: 0.39439904
INFO:root:[    6] Training loss: 0.02093568, Validation loss: 0.01537983, Gradient norm: 0.44040042
INFO:root:[    7] Training loss: 0.01921131, Validation loss: 0.02117196, Gradient norm: 0.36109837
INFO:root:[    8] Training loss: 0.01766032, Validation loss: 0.01365818, Gradient norm: 0.34469247
INFO:root:[    9] Training loss: 0.01733300, Validation loss: 0.01766911, Gradient norm: 0.36924950
INFO:root:[   10] Training loss: 0.01736694, Validation loss: 0.01366420, Gradient norm: 0.37680117
INFO:root:[   11] Training loss: 0.01584038, Validation loss: 0.01861406, Gradient norm: 0.31626229
INFO:root:[   12] Training loss: 0.01657035, Validation loss: 0.01265263, Gradient norm: 0.38927100
INFO:root:[   13] Training loss: 0.01458779, Validation loss: 0.01234988, Gradient norm: 0.25664904
INFO:root:[   14] Training loss: 0.01503293, Validation loss: 0.01396087, Gradient norm: 0.33789637
INFO:root:[   15] Training loss: 0.01488029, Validation loss: 0.01654686, Gradient norm: 0.31812937
INFO:root:[   16] Training loss: 0.01452126, Validation loss: 0.01106725, Gradient norm: 0.29183696
INFO:root:[   17] Training loss: 0.01376947, Validation loss: 0.01260474, Gradient norm: 0.30009726
INFO:root:[   18] Training loss: 0.01387083, Validation loss: 0.01387422, Gradient norm: 0.27490472
INFO:root:[   19] Training loss: 0.01325246, Validation loss: 0.01708832, Gradient norm: 0.31188340
INFO:root:[   20] Training loss: 0.01482263, Validation loss: 0.01533404, Gradient norm: 0.33309176
INFO:root:[   21] Training loss: 0.01431367, Validation loss: 0.01387258, Gradient norm: 0.33753490
INFO:root:[   22] Training loss: 0.01307722, Validation loss: 0.01401340, Gradient norm: 0.28540264
INFO:root:[   23] Training loss: 0.01288051, Validation loss: 0.01470318, Gradient norm: 0.28425712
INFO:root:[   24] Training loss: 0.01280825, Validation loss: 0.01391128, Gradient norm: 0.30543213
INFO:root:[   25] Training loss: 0.01242042, Validation loss: 0.01336379, Gradient norm: 0.26142981
INFO:root:[   26] Training loss: 0.01400259, Validation loss: 0.01325937, Gradient norm: 0.34863295
INFO:root:[   27] Training loss: 0.01234457, Validation loss: 0.01187745, Gradient norm: 0.27332059
INFO:root:[   28] Training loss: 0.01235356, Validation loss: 0.01330474, Gradient norm: 0.30802527
INFO:root:[   29] Training loss: 0.01123769, Validation loss: 0.01431219, Gradient norm: 0.27264888
INFO:root:[   30] Training loss: 0.01260488, Validation loss: 0.01398973, Gradient norm: 0.28298841
INFO:root:[   31] Training loss: 0.01264163, Validation loss: 0.01457620, Gradient norm: 0.31707987
INFO:root:[   32] Training loss: 0.01152664, Validation loss: 0.01227163, Gradient norm: 0.27136851
INFO:root:[   33] Training loss: 0.01237358, Validation loss: 0.01153938, Gradient norm: 0.27252920
INFO:root:[   34] Training loss: 0.01101985, Validation loss: 0.01198746, Gradient norm: 0.26435445
INFO:root:[   35] Training loss: 0.01191144, Validation loss: 0.01395407, Gradient norm: 0.30005792
INFO:root:[   36] Training loss: 0.01172953, Validation loss: 0.01337895, Gradient norm: 0.29346269
INFO:root:[   37] Training loss: 0.01064961, Validation loss: 0.01187638, Gradient norm: 0.22068736
INFO:root:[   38] Training loss: 0.01120918, Validation loss: 0.01560444, Gradient norm: 0.27683334
INFO:root:[   39] Training loss: 0.01190093, Validation loss: 0.01490128, Gradient norm: 0.32190510
INFO:root:[   40] Training loss: 0.01086715, Validation loss: 0.01135790, Gradient norm: 0.26411799
INFO:root:[   41] Training loss: 0.01036524, Validation loss: 0.01205926, Gradient norm: 0.21433387
INFO:root:[   42] Training loss: 0.01125814, Validation loss: 0.01410342, Gradient norm: 0.27837804
INFO:root:[   43] Training loss: 0.01093936, Validation loss: 0.01256109, Gradient norm: 0.26539994
INFO:root:[   44] Training loss: 0.01113817, Validation loss: 0.01498656, Gradient norm: 0.28604215
INFO:root:[   45] Training loss: 0.01012190, Validation loss: 0.01245157, Gradient norm: 0.27649256
INFO:root:[   46] Training loss: 0.01094367, Validation loss: 0.01325733, Gradient norm: 0.28155967
INFO:root:[   47] Training loss: 0.01071828, Validation loss: 0.01480557, Gradient norm: 0.27165004
INFO:root:[   48] Training loss: 0.01068177, Validation loss: 0.01421754, Gradient norm: 0.29035529
INFO:root:[   49] Training loss: 0.01046678, Validation loss: 0.01224799, Gradient norm: 0.25764898
INFO:root:[   50] Training loss: 0.01014200, Validation loss: 0.01356785, Gradient norm: 0.25162167
INFO:root:[   51] Training loss: 0.01024402, Validation loss: 0.01384626, Gradient norm: 0.27731706
INFO:root:[   52] Training loss: 0.00979734, Validation loss: 0.01333343, Gradient norm: 0.26060717
INFO:root:[   53] Training loss: 0.00996471, Validation loss: 0.01350611, Gradient norm: 0.23223915
INFO:root:[   54] Training loss: 0.00969168, Validation loss: 0.01380993, Gradient norm: 0.27432441
INFO:root:[   55] Training loss: 0.00985322, Validation loss: 0.01430646, Gradient norm: 0.25579351
INFO:root:[   56] Training loss: 0.01091746, Validation loss: 0.01292592, Gradient norm: 0.27298062
INFO:root:[   57] Training loss: 0.01006896, Validation loss: 0.01312743, Gradient norm: 0.29247406
INFO:root:[   58] Training loss: 0.00992931, Validation loss: 0.01767949, Gradient norm: 0.25846000
INFO:root:[   59] Training loss: 0.00979648, Validation loss: 0.01421424, Gradient norm: 0.20895251
INFO:root:[   60] Training loss: 0.00991073, Validation loss: 0.01360803, Gradient norm: 0.27082969
INFO:root:[   61] Training loss: 0.00941162, Validation loss: 0.01562839, Gradient norm: 0.25504927
INFO:root:[   62] Training loss: 0.00967480, Validation loss: 0.01416787, Gradient norm: 0.22212037
INFO:root:[   63] Training loss: 0.00911401, Validation loss: 0.01526995, Gradient norm: 0.26802465
INFO:root:[   64] Training loss: 0.00944946, Validation loss: 0.01417828, Gradient norm: 0.24918013
INFO:root:[   65] Training loss: 0.00911224, Validation loss: 0.01413394, Gradient norm: 0.23143928
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1287.365s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01285
INFO:root:EnergyScoreTrain: 0.00957
INFO:root:CoverageTrain: 0.90277
INFO:root:IntervalWidthTrain: 0.04885
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01538
INFO:root:EnergyScoreValidation: 0.01118
INFO:root:CoverageValidation: 0.84314
INFO:root:IntervalWidthValidation: 0.04834
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01553
INFO:root:EnergyScoreTest: 0.01129
INFO:root:CoverageTest: 0.84147
INFO:root:IntervalWidthTest: 0.04846
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 167772160
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06522470, Validation loss: 0.02753327, Gradient norm: 0.43246908
INFO:root:[    2] Training loss: 0.03317495, Validation loss: 0.02280740, Gradient norm: 0.41672291
INFO:root:[    3] Training loss: 0.02739627, Validation loss: 0.03010365, Gradient norm: 0.43460949
INFO:root:[    4] Training loss: 0.02343345, Validation loss: 0.01906013, Gradient norm: 0.37628669
INFO:root:[    5] Training loss: 0.02231266, Validation loss: 0.01821644, Gradient norm: 0.36852164
INFO:root:[    6] Training loss: 0.02058070, Validation loss: 0.01834172, Gradient norm: 0.36414435
INFO:root:[    7] Training loss: 0.01936252, Validation loss: 0.01676334, Gradient norm: 0.32763454
INFO:root:[    8] Training loss: 0.01803380, Validation loss: 0.01530832, Gradient norm: 0.30990542
INFO:root:[    9] Training loss: 0.01820164, Validation loss: 0.01533004, Gradient norm: 0.37556460
INFO:root:[   10] Training loss: 0.01778150, Validation loss: 0.01823265, Gradient norm: 0.31751870
INFO:root:[   11] Training loss: 0.01719082, Validation loss: 0.02046984, Gradient norm: 0.32185936
INFO:root:[   12] Training loss: 0.01578375, Validation loss: 0.01507801, Gradient norm: 0.26043049
INFO:root:[   13] Training loss: 0.01586697, Validation loss: 0.01526477, Gradient norm: 0.33289635
INFO:root:[   14] Training loss: 0.01599830, Validation loss: 0.01463566, Gradient norm: 0.33583945
INFO:root:[   15] Training loss: 0.01482510, Validation loss: 0.01411385, Gradient norm: 0.29673113
INFO:root:[   16] Training loss: 0.01425751, Validation loss: 0.01553944, Gradient norm: 0.27718161
INFO:root:[   17] Training loss: 0.01454497, Validation loss: 0.01405471, Gradient norm: 0.29481721
INFO:root:[   18] Training loss: 0.01316326, Validation loss: 0.01641638, Gradient norm: 0.24994121
INFO:root:[   19] Training loss: 0.01445957, Validation loss: 0.01575872, Gradient norm: 0.30495364
INFO:root:[   20] Training loss: 0.01395925, Validation loss: 0.01464974, Gradient norm: 0.28039593
INFO:root:[   21] Training loss: 0.01342450, Validation loss: 0.01255787, Gradient norm: 0.27092453
INFO:root:[   22] Training loss: 0.01381766, Validation loss: 0.01902017, Gradient norm: 0.25258571
INFO:root:[   23] Training loss: 0.01405069, Validation loss: 0.01318124, Gradient norm: 0.32234255
INFO:root:[   24] Training loss: 0.01357698, Validation loss: 0.02121425, Gradient norm: 0.31410184
INFO:root:[   25] Training loss: 0.01293855, Validation loss: 0.01387159, Gradient norm: 0.25331722
INFO:root:[   26] Training loss: 0.01343809, Validation loss: 0.01628812, Gradient norm: 0.26098974
INFO:root:[   27] Training loss: 0.01269856, Validation loss: 0.01320415, Gradient norm: 0.29182347
INFO:root:[   28] Training loss: 0.01285769, Validation loss: 0.01348019, Gradient norm: 0.28772970
INFO:root:[   29] Training loss: 0.01261636, Validation loss: 0.01227860, Gradient norm: 0.26650911
INFO:root:[   30] Training loss: 0.01230048, Validation loss: 0.01538803, Gradient norm: 0.26461939
INFO:root:[   31] Training loss: 0.01244169, Validation loss: 0.01383184, Gradient norm: 0.27624217
INFO:root:[   32] Training loss: 0.01229353, Validation loss: 0.01488499, Gradient norm: 0.24666827
INFO:root:[   33] Training loss: 0.01205050, Validation loss: 0.01392459, Gradient norm: 0.29076381
INFO:root:[   34] Training loss: 0.01239234, Validation loss: 0.01183710, Gradient norm: 0.29674783
INFO:root:[   35] Training loss: 0.01223253, Validation loss: 0.01300088, Gradient norm: 0.27494458
INFO:root:[   36] Training loss: 0.01139276, Validation loss: 0.01851143, Gradient norm: 0.22951501
