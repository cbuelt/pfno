INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 75497472
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.07727763, Validation loss: 0.22763180, Gradient norm: 12.06598370
INFO:root:[    2] Training loss: 0.17289003, Validation loss: 0.15293612, Gradient norm: 4.14841245
INFO:root:[    3] Training loss: 0.14599712, Validation loss: 0.13868114, Gradient norm: 1.44767102
INFO:root:[    4] Training loss: 0.13446560, Validation loss: 0.13006646, Gradient norm: 1.46565225
INFO:root:[    5] Training loss: 0.12806214, Validation loss: 0.12493977, Gradient norm: 1.80949598
INFO:root:[    6] Training loss: 0.12525330, Validation loss: 0.12909602, Gradient norm: 2.23263940
INFO:root:[    7] Training loss: 0.12394844, Validation loss: 0.12580803, Gradient norm: 3.47306625
INFO:root:[    8] Training loss: 0.11650761, Validation loss: 0.11710941, Gradient norm: 1.87507015
INFO:root:[    9] Training loss: 0.11666443, Validation loss: 0.12914342, Gradient norm: 2.22966881
INFO:root:[   10] Training loss: 0.11935736, Validation loss: 0.12433595, Gradient norm: 3.96686032
INFO:root:[   11] Training loss: 0.11210085, Validation loss: 0.10833765, Gradient norm: 2.78910206
INFO:root:[   12] Training loss: 0.10900375, Validation loss: 0.11700157, Gradient norm: 2.55625479
INFO:root:[   13] Training loss: 0.11147911, Validation loss: 0.11245042, Gradient norm: 3.47472753
INFO:root:[   14] Training loss: 0.10373800, Validation loss: 0.10382117, Gradient norm: 1.87869253
INFO:root:[   15] Training loss: 0.10290318, Validation loss: 0.10152194, Gradient norm: 1.46104257
INFO:root:[   16] Training loss: 0.10239077, Validation loss: 0.09851654, Gradient norm: 2.52944521
INFO:root:[   17] Training loss: 0.09827194, Validation loss: 0.09796312, Gradient norm: 1.57690455
INFO:root:[   18] Training loss: 0.09779560, Validation loss: 0.09671079, Gradient norm: 1.64841124
INFO:root:[   19] Training loss: 0.09651394, Validation loss: 0.09413395, Gradient norm: 1.91951970
INFO:root:[   20] Training loss: 0.09665027, Validation loss: 0.09666807, Gradient norm: 2.85587393
INFO:root:[   21] Training loss: 0.09642965, Validation loss: 0.09376062, Gradient norm: 3.04375437
INFO:root:[   22] Training loss: 0.09171673, Validation loss: 0.09176964, Gradient norm: 1.93343955
INFO:root:[   23] Training loss: 0.09152302, Validation loss: 0.10006466, Gradient norm: 2.00001839
INFO:root:[   24] Training loss: 0.09146632, Validation loss: 0.09012546, Gradient norm: 2.50563912
INFO:root:[   25] Training loss: 0.09036596, Validation loss: 0.08811647, Gradient norm: 1.85044348
INFO:root:[   26] Training loss: 0.09128213, Validation loss: 0.08814187, Gradient norm: 2.85090127
INFO:root:[   27] Training loss: 0.08911806, Validation loss: 0.08898213, Gradient norm: 1.98321713
INFO:root:[   28] Training loss: 0.08734206, Validation loss: 0.09199286, Gradient norm: 1.79695531
INFO:root:[   29] Training loss: 0.08837439, Validation loss: 0.08655585, Gradient norm: 2.64319631
INFO:root:[   30] Training loss: 0.08605516, Validation loss: 0.09056102, Gradient norm: 2.21019375
INFO:root:[   31] Training loss: 0.08890155, Validation loss: 0.08644519, Gradient norm: 3.58413064
INFO:root:[   32] Training loss: 0.08556398, Validation loss: 0.08335265, Gradient norm: 2.60290172
INFO:root:[   33] Training loss: 0.08347152, Validation loss: 0.08434481, Gradient norm: 2.65937300
INFO:root:[   34] Training loss: 0.08141383, Validation loss: 0.07944251, Gradient norm: 1.99514723
INFO:root:[   35] Training loss: 0.08142276, Validation loss: 0.08394313, Gradient norm: 1.91073512
INFO:root:[   36] Training loss: 0.08350860, Validation loss: 0.07851190, Gradient norm: 2.72582939
INFO:root:[   37] Training loss: 0.08195702, Validation loss: 0.07865254, Gradient norm: 2.57435694
INFO:root:[   38] Training loss: 0.07888706, Validation loss: 0.07866123, Gradient norm: 1.57924876
INFO:root:[   39] Training loss: 0.08033153, Validation loss: 0.07878995, Gradient norm: 2.72385135
INFO:root:[   40] Training loss: 0.08115827, Validation loss: 0.07787925, Gradient norm: 2.92049367
INFO:root:[   41] Training loss: 0.07790980, Validation loss: 0.07833187, Gradient norm: 2.37311763
INFO:root:[   42] Training loss: 0.07938039, Validation loss: 0.07868355, Gradient norm: 2.54795848
INFO:root:[   43] Training loss: 0.07919686, Validation loss: 0.07430755, Gradient norm: 2.92771218
INFO:root:[   44] Training loss: 0.07804359, Validation loss: 0.08364335, Gradient norm: 2.60715217
INFO:root:[   45] Training loss: 0.07546833, Validation loss: 0.07468669, Gradient norm: 1.77721400
INFO:root:[   46] Training loss: 0.07436049, Validation loss: 0.07499011, Gradient norm: 1.27365876
INFO:root:[   47] Training loss: 0.07453189, Validation loss: 0.07715473, Gradient norm: 1.91955437
INFO:root:[   48] Training loss: 0.07257531, Validation loss: 0.07220848, Gradient norm: 1.55950562
INFO:root:[   49] Training loss: 0.07220000, Validation loss: 0.08708883, Gradient norm: 1.40715286
INFO:root:[   50] Training loss: 0.07546193, Validation loss: 0.07340409, Gradient norm: 3.07182690
INFO:root:[   51] Training loss: 0.07122428, Validation loss: 0.07352831, Gradient norm: 1.55595330
INFO:root:[   52] Training loss: 0.07175617, Validation loss: 0.07107533, Gradient norm: 1.92949327
INFO:root:[   53] Training loss: 0.07148940, Validation loss: 0.07802129, Gradient norm: 2.18055843
INFO:root:[   54] Training loss: 0.07181106, Validation loss: 0.07008389, Gradient norm: 2.73829974
INFO:root:[   55] Training loss: 0.07233701, Validation loss: 0.07646478, Gradient norm: 3.04959033
INFO:root:[   56] Training loss: 0.07138514, Validation loss: 0.06995469, Gradient norm: 2.72074899
INFO:root:[   57] Training loss: 0.06918313, Validation loss: 0.06904944, Gradient norm: 2.02164136
INFO:root:[   58] Training loss: 0.06900066, Validation loss: 0.06627378, Gradient norm: 1.95796858
INFO:root:[   59] Training loss: 0.06906045, Validation loss: 0.06749947, Gradient norm: 2.35618698
INFO:root:[   60] Training loss: 0.06890977, Validation loss: 0.06971403, Gradient norm: 2.14821997
INFO:root:[   61] Training loss: 0.06907011, Validation loss: 0.06548468, Gradient norm: 2.65144415
INFO:root:[   62] Training loss: 0.06784408, Validation loss: 0.06764061, Gradient norm: 2.19794943
INFO:root:[   63] Training loss: 0.06571216, Validation loss: 0.06721584, Gradient norm: 1.28444622
INFO:root:[   64] Training loss: 0.06795074, Validation loss: 0.06595161, Gradient norm: 2.33667294
INFO:root:[   65] Training loss: 0.06763572, Validation loss: 0.06969255, Gradient norm: 2.47926073
INFO:root:[   66] Training loss: 0.06688410, Validation loss: 0.06407288, Gradient norm: 2.52773257
INFO:root:[   67] Training loss: 0.06585322, Validation loss: 0.06513117, Gradient norm: 2.01186145
INFO:root:[   68] Training loss: 0.06623400, Validation loss: 0.07207435, Gradient norm: 2.45537106
INFO:root:[   69] Training loss: 0.06738251, Validation loss: 0.06877114, Gradient norm: 2.71901486
INFO:root:[   70] Training loss: 0.06419787, Validation loss: 0.06307803, Gradient norm: 1.67133545
INFO:root:[   71] Training loss: 0.06408220, Validation loss: 0.06483437, Gradient norm: 1.64601495
INFO:root:[   72] Training loss: 0.06372115, Validation loss: 0.06416805, Gradient norm: 1.67214422
INFO:root:[   73] Training loss: 0.06344337, Validation loss: 0.06376693, Gradient norm: 1.83581250
INFO:root:[   74] Training loss: 0.06434982, Validation loss: 0.06596983, Gradient norm: 2.22011199
INFO:root:[   75] Training loss: 0.06658956, Validation loss: 0.06161021, Gradient norm: 3.24583185
INFO:root:[   76] Training loss: 0.06557356, Validation loss: 0.06326655, Gradient norm: 2.74483134
INFO:root:[   77] Training loss: 0.06637190, Validation loss: 0.06466554, Gradient norm: 3.17095451
INFO:root:[   78] Training loss: 0.06283332, Validation loss: 0.06113052, Gradient norm: 1.84574767
INFO:root:[   79] Training loss: 0.06098274, Validation loss: 0.06684365, Gradient norm: 0.99901744
INFO:root:[   80] Training loss: 0.06300805, Validation loss: 0.06126262, Gradient norm: 1.98218631
INFO:root:[   81] Training loss: 0.06198220, Validation loss: 0.06328368, Gradient norm: 1.77420254
INFO:root:[   82] Training loss: 0.06306557, Validation loss: 0.06668402, Gradient norm: 2.52555169
INFO:root:[   83] Training loss: 0.06604573, Validation loss: 0.06173740, Gradient norm: 3.47396598
INFO:root:[   84] Training loss: 0.06295113, Validation loss: 0.06335333, Gradient norm: 2.61991260
INFO:root:[   85] Training loss: 0.06201224, Validation loss: 0.06155033, Gradient norm: 2.35795105
INFO:root:[   86] Training loss: 0.06091270, Validation loss: 0.07167583, Gradient norm: 1.88664329
INFO:root:[   87] Training loss: 0.06230078, Validation loss: 0.06399218, Gradient norm: 2.49255919
INFO:root:[   88] Training loss: 0.06173494, Validation loss: 0.05895788, Gradient norm: 2.34562172
INFO:root:[   89] Training loss: 0.06054537, Validation loss: 0.05964986, Gradient norm: 1.98457153
INFO:root:[   90] Training loss: 0.05991988, Validation loss: 0.05969040, Gradient norm: 1.46940971
INFO:root:[   91] Training loss: 0.05862983, Validation loss: 0.05846136, Gradient norm: 0.96991443
INFO:root:[   92] Training loss: 0.05993120, Validation loss: 0.05901080, Gradient norm: 1.48084127
INFO:root:[   93] Training loss: 0.05839484, Validation loss: 0.05962471, Gradient norm: 1.22169187
INFO:root:[   94] Training loss: 0.05933975, Validation loss: 0.06080365, Gradient norm: 1.61112377
INFO:root:[   95] Training loss: 0.06037378, Validation loss: 0.07572036, Gradient norm: 2.38029844
INFO:root:[   96] Training loss: 0.06430048, Validation loss: 0.05734207, Gradient norm: 3.29747859
INFO:root:[   97] Training loss: 0.05915377, Validation loss: 0.06469447, Gradient norm: 1.80402073
INFO:root:[   98] Training loss: 0.06148313, Validation loss: 0.05982132, Gradient norm: 2.77333429
INFO:root:[   99] Training loss: 0.05956080, Validation loss: 0.06190824, Gradient norm: 2.26406603
INFO:root:[  100] Training loss: 0.06089714, Validation loss: 0.05711209, Gradient norm: 2.78840037
INFO:root:[  101] Training loss: 0.05823140, Validation loss: 0.05824445, Gradient norm: 1.77014701
INFO:root:[  102] Training loss: 0.05840317, Validation loss: 0.05854590, Gradient norm: 1.78493270
INFO:root:[  103] Training loss: 0.05934467, Validation loss: 0.06126419, Gradient norm: 2.36121746
INFO:root:[  104] Training loss: 0.05880814, Validation loss: 0.05599434, Gradient norm: 2.28154088
INFO:root:[  105] Training loss: 0.05788833, Validation loss: 0.05577939, Gradient norm: 1.87963366
INFO:root:[  106] Training loss: 0.05751305, Validation loss: 0.05854507, Gradient norm: 1.66380176
INFO:root:[  107] Training loss: 0.05840515, Validation loss: 0.05670435, Gradient norm: 2.07946864
INFO:root:[  108] Training loss: 0.05728210, Validation loss: 0.06025663, Gradient norm: 1.80024856
INFO:root:[  109] Training loss: 0.05643475, Validation loss: 0.06162365, Gradient norm: 1.31651410
INFO:root:[  110] Training loss: 0.05853035, Validation loss: 0.05759967, Gradient norm: 2.42261385
INFO:root:[  111] Training loss: 0.05633357, Validation loss: 0.06035342, Gradient norm: 1.74979765
INFO:root:[  112] Training loss: 0.05615817, Validation loss: 0.05642859, Gradient norm: 1.52957201
INFO:root:[  113] Training loss: 0.05651166, Validation loss: 0.05943768, Gradient norm: 1.96205527
INFO:root:[  114] Training loss: 0.05855291, Validation loss: 0.05996772, Gradient norm: 2.77835598
INFO:root:[  115] Training loss: 0.05719428, Validation loss: 0.05539890, Gradient norm: 2.17581571
INFO:root:[  116] Training loss: 0.05579418, Validation loss: 0.05536695, Gradient norm: 1.43739628
INFO:root:[  117] Training loss: 0.05609560, Validation loss: 0.05506290, Gradient norm: 1.86191099
INFO:root:[  118] Training loss: 0.05673184, Validation loss: 0.05621616, Gradient norm: 2.30830413
INFO:root:[  119] Training loss: 0.05679618, Validation loss: 0.05382930, Gradient norm: 2.12460791
INFO:root:[  120] Training loss: 0.05665913, Validation loss: 0.05548301, Gradient norm: 2.35272005
INFO:root:[  121] Training loss: 0.05582262, Validation loss: 0.05368092, Gradient norm: 2.03701231
INFO:root:[  122] Training loss: 0.05678062, Validation loss: 0.05756599, Gradient norm: 2.44581874
INFO:root:[  123] Training loss: 0.05537522, Validation loss: 0.05614722, Gradient norm: 1.83316394
INFO:root:[  124] Training loss: 0.05557584, Validation loss: 0.05696561, Gradient norm: 1.92767506
INFO:root:[  125] Training loss: 0.05676980, Validation loss: 0.05370445, Gradient norm: 2.53816970
INFO:root:[  126] Training loss: 0.05629188, Validation loss: 0.05668771, Gradient norm: 2.43454858
INFO:root:[  127] Training loss: 0.05474297, Validation loss: 0.05640992, Gradient norm: 1.81837397
INFO:root:[  128] Training loss: 0.05419626, Validation loss: 0.05363712, Gradient norm: 1.58172849
INFO:root:[  129] Training loss: 0.05433459, Validation loss: 0.05759769, Gradient norm: 1.65543224
INFO:root:[  130] Training loss: 0.05708860, Validation loss: 0.05216243, Gradient norm: 2.78959011
INFO:root:[  131] Training loss: 0.05490710, Validation loss: 0.05407248, Gradient norm: 2.12600189
INFO:root:[  132] Training loss: 0.05417840, Validation loss: 0.05732969, Gradient norm: 1.48068234
INFO:root:[  133] Training loss: 0.05490943, Validation loss: 0.05467347, Gradient norm: 2.06679308
INFO:root:[  134] Training loss: 0.05483839, Validation loss: 0.05503069, Gradient norm: 2.25851428
INFO:root:[  135] Training loss: 0.05383035, Validation loss: 0.05287115, Gradient norm: 1.92101459
INFO:root:[  136] Training loss: 0.05251554, Validation loss: 0.05716920, Gradient norm: 1.13760629
INFO:root:[  137] Training loss: 0.05447701, Validation loss: 0.05843295, Gradient norm: 2.30439732
INFO:root:[  138] Training loss: 0.05570849, Validation loss: 0.05214623, Gradient norm: 2.90104937
INFO:root:[  139] Training loss: 0.05328254, Validation loss: 0.05348629, Gradient norm: 1.86436861
INFO:root:[  140] Training loss: 0.05295124, Validation loss: 0.05384555, Gradient norm: 1.53956663
INFO:root:[  141] Training loss: 0.05342771, Validation loss: 0.05196938, Gradient norm: 1.99921677
INFO:root:[  142] Training loss: 0.05204636, Validation loss: 0.05198674, Gradient norm: 1.29468041
INFO:root:[  143] Training loss: 0.05354292, Validation loss: 0.05165013, Gradient norm: 2.00211742
INFO:root:[  144] Training loss: 0.05246816, Validation loss: 0.05157674, Gradient norm: 1.63710320
INFO:root:[  145] Training loss: 0.05211745, Validation loss: 0.05493757, Gradient norm: 1.72593561
INFO:root:[  146] Training loss: 0.05210962, Validation loss: 0.05430567, Gradient norm: 1.86165891
INFO:root:[  147] Training loss: 0.05214558, Validation loss: 0.05061213, Gradient norm: 1.80836086
INFO:root:[  148] Training loss: 0.05179537, Validation loss: 0.05190819, Gradient norm: 1.75869269
INFO:root:[  149] Training loss: 0.05133218, Validation loss: 0.05198386, Gradient norm: 1.27466365
INFO:root:[  150] Training loss: 0.05146149, Validation loss: 0.05710846, Gradient norm: 1.58490190
INFO:root:[  151] Training loss: 0.05315161, Validation loss: 0.05407162, Gradient norm: 2.43719453
INFO:root:[  152] Training loss: 0.05299236, Validation loss: 0.05262641, Gradient norm: 2.25460711
INFO:root:[  153] Training loss: 0.05248016, Validation loss: 0.05128554, Gradient norm: 2.23396301
INFO:root:[  154] Training loss: 0.05283185, Validation loss: 0.05108276, Gradient norm: 2.43245572
INFO:root:[  155] Training loss: 0.05161428, Validation loss: 0.05049791, Gradient norm: 1.85735262
INFO:root:[  156] Training loss: 0.05088969, Validation loss: 0.04990466, Gradient norm: 1.38784480
INFO:root:[  157] Training loss: 0.05090441, Validation loss: 0.05189502, Gradient norm: 1.66288621
INFO:root:[  158] Training loss: 0.05146648, Validation loss: 0.05211801, Gradient norm: 2.08474947
INFO:root:[  159] Training loss: 0.05052853, Validation loss: 0.05219665, Gradient norm: 1.48461681
INFO:root:[  160] Training loss: 0.05184451, Validation loss: 0.05305254, Gradient norm: 2.39255803
INFO:root:[  161] Training loss: 0.05248045, Validation loss: 0.04967670, Gradient norm: 2.53229431
INFO:root:[  162] Training loss: 0.05092496, Validation loss: 0.04923875, Gradient norm: 1.97507200
INFO:root:[  163] Training loss: 0.05187929, Validation loss: 0.04978041, Gradient norm: 2.38076578
INFO:root:[  164] Training loss: 0.05095447, Validation loss: 0.04886340, Gradient norm: 2.10764944
INFO:root:[  165] Training loss: 0.05115955, Validation loss: 0.05243029, Gradient norm: 2.29792837
INFO:root:[  166] Training loss: 0.05029756, Validation loss: 0.05346027, Gradient norm: 1.74084252
INFO:root:[  167] Training loss: 0.05008975, Validation loss: 0.05197706, Gradient norm: 1.88196238
INFO:root:[  168] Training loss: 0.05114673, Validation loss: 0.05252192, Gradient norm: 2.33776216
INFO:root:[  169] Training loss: 0.05015936, Validation loss: 0.05019144, Gradient norm: 2.01544237
INFO:root:[  170] Training loss: 0.05027294, Validation loss: 0.05410509, Gradient norm: 2.25702852
INFO:root:[  171] Training loss: 0.05194966, Validation loss: 0.05058205, Gradient norm: 2.74622216
INFO:root:[  172] Training loss: 0.05067891, Validation loss: 0.04882913, Gradient norm: 2.31347575
INFO:root:[  173] Training loss: 0.04963502, Validation loss: 0.04813474, Gradient norm: 1.95843074
INFO:root:[  174] Training loss: 0.05036581, Validation loss: 0.04902925, Gradient norm: 2.27242951
INFO:root:[  175] Training loss: 0.04937083, Validation loss: 0.04800649, Gradient norm: 1.85896872
INFO:root:[  176] Training loss: 0.04818019, Validation loss: 0.04828961, Gradient norm: 1.06468958
INFO:root:[  177] Training loss: 0.04883779, Validation loss: 0.04870735, Gradient norm: 1.66738688
INFO:root:[  178] Training loss: 0.04979806, Validation loss: 0.04785000, Gradient norm: 2.15040024
INFO:root:[  179] Training loss: 0.04864904, Validation loss: 0.04761320, Gradient norm: 1.60144432
INFO:root:[  180] Training loss: 0.04940726, Validation loss: 0.05373096, Gradient norm: 1.98343362
INFO:root:[  181] Training loss: 0.04847027, Validation loss: 0.04783039, Gradient norm: 1.53335423
INFO:root:[  182] Training loss: 0.04767444, Validation loss: 0.04741769, Gradient norm: 1.07266026
INFO:root:[  183] Training loss: 0.04833393, Validation loss: 0.04831743, Gradient norm: 1.48469149
INFO:root:[  184] Training loss: 0.04819427, Validation loss: 0.04824923, Gradient norm: 1.42844790
INFO:root:[  185] Training loss: 0.04813889, Validation loss: 0.04995025, Gradient norm: 1.67126132
INFO:root:[  186] Training loss: 0.04844354, Validation loss: 0.04656331, Gradient norm: 1.88220204
INFO:root:[  187] Training loss: 0.04864561, Validation loss: 0.04750771, Gradient norm: 1.90680028
INFO:root:[  188] Training loss: 0.04909301, Validation loss: 0.04735625, Gradient norm: 2.08454071
INFO:root:[  189] Training loss: 0.04756435, Validation loss: 0.04632252, Gradient norm: 1.57161019
INFO:root:[  190] Training loss: 0.04877121, Validation loss: 0.04746825, Gradient norm: 2.00933122
INFO:root:[  191] Training loss: 0.04800437, Validation loss: 0.04654624, Gradient norm: 1.83830009
INFO:root:[  192] Training loss: 0.04735382, Validation loss: 0.04823431, Gradient norm: 1.66836572
INFO:root:[  193] Training loss: 0.04789717, Validation loss: 0.04800296, Gradient norm: 1.98304168
INFO:root:[  194] Training loss: 0.04822100, Validation loss: 0.04633688, Gradient norm: 2.04884182
INFO:root:[  195] Training loss: 0.04822408, Validation loss: 0.05081929, Gradient norm: 2.09350553
INFO:root:[  196] Training loss: 0.04744735, Validation loss: 0.04760449, Gradient norm: 1.72252946
INFO:root:[  197] Training loss: 0.04659773, Validation loss: 0.04613967, Gradient norm: 1.33751449
INFO:root:[  198] Training loss: 0.04729291, Validation loss: 0.04588701, Gradient norm: 1.76845237
INFO:root:[  199] Training loss: 0.04712870, Validation loss: 0.04596299, Gradient norm: 1.96164947
INFO:root:[  200] Training loss: 0.04854295, Validation loss: 0.05111814, Gradient norm: 2.50288796
INFO:root:[  201] Training loss: 0.04745116, Validation loss: 0.05078892, Gradient norm: 1.91171316
INFO:root:[  202] Training loss: 0.04768525, Validation loss: 0.04641606, Gradient norm: 2.11547093
INFO:root:[  203] Training loss: 0.04691547, Validation loss: 0.04548912, Gradient norm: 1.76495419
INFO:root:[  204] Training loss: 0.04584881, Validation loss: 0.04764146, Gradient norm: 0.89180015
INFO:root:[  205] Training loss: 0.04614953, Validation loss: 0.04512322, Gradient norm: 1.28054566
INFO:root:[  206] Training loss: 0.04602305, Validation loss: 0.04837265, Gradient norm: 1.16253113
INFO:root:[  207] Training loss: 0.04606816, Validation loss: 0.05267786, Gradient norm: 1.59202148
INFO:root:[  208] Training loss: 0.04812952, Validation loss: 0.04573591, Gradient norm: 2.62062605
INFO:root:[  209] Training loss: 0.04657066, Validation loss: 0.04471947, Gradient norm: 1.93361109
INFO:root:[  210] Training loss: 0.04696534, Validation loss: 0.04594198, Gradient norm: 2.17453937
INFO:root:[  211] Training loss: 0.04605097, Validation loss: 0.04915962, Gradient norm: 1.59385800
INFO:root:[  212] Training loss: 0.04624721, Validation loss: 0.04792475, Gradient norm: 1.63483304
INFO:root:[  213] Training loss: 0.04669355, Validation loss: 0.04514436, Gradient norm: 2.13967410
INFO:root:[  214] Training loss: 0.04588661, Validation loss: 0.05151429, Gradient norm: 1.67601362
INFO:root:[  215] Training loss: 0.04694984, Validation loss: 0.04860530, Gradient norm: 2.24053365
INFO:root:[  216] Training loss: 0.04572911, Validation loss: 0.04429974, Gradient norm: 1.76650559
INFO:root:[  217] Training loss: 0.04639074, Validation loss: 0.04635259, Gradient norm: 2.10019627
INFO:root:[  218] Training loss: 0.04612478, Validation loss: 0.04982954, Gradient norm: 1.93835454
INFO:root:[  219] Training loss: 0.04668668, Validation loss: 0.04419162, Gradient norm: 2.24744926
INFO:root:[  220] Training loss: 0.04525515, Validation loss: 0.04404771, Gradient norm: 1.48445992
INFO:root:[  221] Training loss: 0.04575274, Validation loss: 0.04429642, Gradient norm: 1.77942282
INFO:root:[  222] Training loss: 0.04607302, Validation loss: 0.04406279, Gradient norm: 2.03563806
INFO:root:[  223] Training loss: 0.04510509, Validation loss: 0.04633333, Gradient norm: 1.48620911
INFO:root:[  224] Training loss: 0.04535968, Validation loss: 0.04736073, Gradient norm: 1.59228400
INFO:root:[  225] Training loss: 0.04510261, Validation loss: 0.04473266, Gradient norm: 1.56355569
INFO:root:[  226] Training loss: 0.04443876, Validation loss: 0.04407362, Gradient norm: 1.18168779
INFO:root:[  227] Training loss: 0.04553762, Validation loss: 0.04478089, Gradient norm: 1.67921159
INFO:root:[  228] Training loss: 0.04686430, Validation loss: 0.04811707, Gradient norm: 2.55503417
INFO:root:[  229] Training loss: 0.04606655, Validation loss: 0.04653607, Gradient norm: 2.28074992
INFO:root:[  230] Training loss: 0.04506834, Validation loss: 0.04381900, Gradient norm: 1.94991703
INFO:root:[  231] Training loss: 0.04425859, Validation loss: 0.04376768, Gradient norm: 1.36022990
INFO:root:[  232] Training loss: 0.04455206, Validation loss: 0.04506237, Gradient norm: 1.33057102
INFO:root:[  233] Training loss: 0.04468508, Validation loss: 0.04441770, Gradient norm: 1.66826575
INFO:root:[  234] Training loss: 0.04368766, Validation loss: 0.04485359, Gradient norm: 1.13883367
INFO:root:[  235] Training loss: 0.04411826, Validation loss: 0.04505039, Gradient norm: 1.49283131
INFO:root:[  236] Training loss: 0.04447237, Validation loss: 0.04549575, Gradient norm: 1.67185111
INFO:root:[  237] Training loss: 0.04542352, Validation loss: 0.04309087, Gradient norm: 2.21290717
INFO:root:[  238] Training loss: 0.04517750, Validation loss: 0.04916172, Gradient norm: 2.07370799
INFO:root:[  239] Training loss: 0.04482641, Validation loss: 0.04353718, Gradient norm: 1.96448691
INFO:root:[  240] Training loss: 0.04481282, Validation loss: 0.04310129, Gradient norm: 2.07109623
INFO:root:[  241] Training loss: 0.04333476, Validation loss: 0.04531694, Gradient norm: 1.21630793
INFO:root:[  242] Training loss: 0.04515102, Validation loss: 0.04566723, Gradient norm: 2.26228363
INFO:root:[  243] Training loss: 0.04378026, Validation loss: 0.04596930, Gradient norm: 1.66538331
INFO:root:[  244] Training loss: 0.04428741, Validation loss: 0.04272165, Gradient norm: 1.96849772
INFO:root:[  245] Training loss: 0.04392932, Validation loss: 0.04487133, Gradient norm: 1.83633040
INFO:root:[  246] Training loss: 0.04313734, Validation loss: 0.04526585, Gradient norm: 1.37142476
INFO:root:[  247] Training loss: 0.04479011, Validation loss: 0.04286475, Gradient norm: 2.24092851
INFO:root:[  248] Training loss: 0.04366090, Validation loss: 0.04434354, Gradient norm: 1.74898094
INFO:root:[  249] Training loss: 0.04320162, Validation loss: 0.04230776, Gradient norm: 1.38411133
INFO:root:[  250] Training loss: 0.04353714, Validation loss: 0.04281364, Gradient norm: 1.60111923
INFO:root:[  251] Training loss: 0.04298610, Validation loss: 0.04318262, Gradient norm: 0.92862056
INFO:root:[  252] Training loss: 0.04359316, Validation loss: 0.04516232, Gradient norm: 1.63249703
INFO:root:[  253] Training loss: 0.04395183, Validation loss: 0.04379913, Gradient norm: 2.05104980
INFO:root:[  254] Training loss: 0.04272003, Validation loss: 0.04564393, Gradient norm: 1.26578807
INFO:root:[  255] Training loss: 0.04325744, Validation loss: 0.04317995, Gradient norm: 1.65825459
INFO:root:[  256] Training loss: 0.04284140, Validation loss: 0.04350119, Gradient norm: 1.40508858
INFO:root:[  257] Training loss: 0.04317249, Validation loss: 0.04242955, Gradient norm: 1.69981297
INFO:root:[  258] Training loss: 0.04283513, Validation loss: 0.04256013, Gradient norm: 1.47208278
INFO:root:[  259] Training loss: 0.04329599, Validation loss: 0.04153450, Gradient norm: 1.85021356
INFO:root:[  260] Training loss: 0.04277153, Validation loss: 0.04143192, Gradient norm: 1.53779720
INFO:root:[  261] Training loss: 0.04256222, Validation loss: 0.04187803, Gradient norm: 1.53306978
INFO:root:[  262] Training loss: 0.04340911, Validation loss: 0.04220931, Gradient norm: 1.90046603
INFO:root:[  263] Training loss: 0.04280024, Validation loss: 0.04656748, Gradient norm: 1.67848329
INFO:root:[  264] Training loss: 0.04445901, Validation loss: 0.04478968, Gradient norm: 2.43721741
INFO:root:[  265] Training loss: 0.04224549, Validation loss: 0.04453245, Gradient norm: 1.33846046
INFO:root:[  266] Training loss: 0.04248912, Validation loss: 0.04163690, Gradient norm: 1.57349410
INFO:root:[  267] Training loss: 0.04270930, Validation loss: 0.04386251, Gradient norm: 1.64360948
INFO:root:[  268] Training loss: 0.04257517, Validation loss: 0.04529699, Gradient norm: 1.69916779
INFO:root:[  269] Training loss: 0.04307387, Validation loss: 0.04110114, Gradient norm: 1.90458979
INFO:root:[  270] Training loss: 0.04138609, Validation loss: 0.04126369, Gradient norm: 0.61471348
INFO:root:[  271] Training loss: 0.04240809, Validation loss: 0.04553788, Gradient norm: 1.58367663
INFO:root:[  272] Training loss: 0.04289995, Validation loss: 0.04098894, Gradient norm: 2.08168305
INFO:root:[  273] Training loss: 0.04282289, Validation loss: 0.04238811, Gradient norm: 1.93104159
INFO:root:[  274] Training loss: 0.04195991, Validation loss: 0.04230110, Gradient norm: 1.49327586
INFO:root:[  275] Training loss: 0.04240843, Validation loss: 0.04202337, Gradient norm: 1.73336557
INFO:root:[  276] Training loss: 0.04202112, Validation loss: 0.04321788, Gradient norm: 1.57825619
INFO:root:[  277] Training loss: 0.04225549, Validation loss: 0.04063863, Gradient norm: 1.79834932
INFO:root:[  278] Training loss: 0.04201337, Validation loss: 0.04467981, Gradient norm: 1.76534281
INFO:root:[  279] Training loss: 0.04235686, Validation loss: 0.04092879, Gradient norm: 1.88068387
INFO:root:[  280] Training loss: 0.04148499, Validation loss: 0.04182702, Gradient norm: 1.29834521
INFO:root:[  281] Training loss: 0.04085054, Validation loss: 0.04076995, Gradient norm: 0.84812443
INFO:root:[  282] Training loss: 0.04132071, Validation loss: 0.04261129, Gradient norm: 1.12660979
INFO:root:[  283] Training loss: 0.04223867, Validation loss: 0.04085733, Gradient norm: 1.88970948
INFO:root:[  284] Training loss: 0.04185569, Validation loss: 0.04164705, Gradient norm: 1.78918676
INFO:root:[  285] Training loss: 0.04156890, Validation loss: 0.04132060, Gradient norm: 1.71864347
INFO:root:[  286] Training loss: 0.04216870, Validation loss: 0.04256183, Gradient norm: 1.90383147
INFO:root:EP 286: Early stopping
INFO:root:Training the model took 13244.007s.
INFO:root:Emptying the cuda cache took 0.107s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.86697
INFO:root:EnergyScoreTrain: 1.27876
INFO:root:CoverageTrain: 0.92978
INFO:root:IntervalWidthTrain: 0.07799
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.79149
INFO:root:EnergyScoreValidation: 1.22728
INFO:root:CoverageValidation: 0.93014
INFO:root:IntervalWidthValidation: 0.07825
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.48345
INFO:root:EnergyScoreTest: 1.01487
INFO:root:CoverageTest: 0.92726
INFO:root:IntervalWidthTest: 0.07714
INFO:root:###2 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1910505472
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.25341366, Validation loss: 0.34548666, Gradient norm: 8.27667540
INFO:root:[    2] Training loss: 0.26027486, Validation loss: 0.22070144, Gradient norm: 2.39129505
INFO:root:[    3] Training loss: 0.21258522, Validation loss: 0.20662043, Gradient norm: 1.33590365
INFO:root:[    4] Training loss: 0.19746040, Validation loss: 0.19060118, Gradient norm: 1.94855975
INFO:root:[    5] Training loss: 0.18822043, Validation loss: 0.18237979, Gradient norm: 1.74348769
INFO:root:[    6] Training loss: 0.18513586, Validation loss: 0.18037788, Gradient norm: 1.79971816
INFO:root:[    7] Training loss: 0.18054761, Validation loss: 0.17563629, Gradient norm: 2.85354788
INFO:root:[    8] Training loss: 0.17468905, Validation loss: 0.17762545, Gradient norm: 2.28255292
INFO:root:[    9] Training loss: 0.16878679, Validation loss: 0.16544567, Gradient norm: 2.13999823
INFO:root:[   10] Training loss: 0.16354235, Validation loss: 0.15806711, Gradient norm: 1.47741617
INFO:root:[   11] Training loss: 0.15985274, Validation loss: 0.15649381, Gradient norm: 1.36469291
INFO:root:[   12] Training loss: 0.15784986, Validation loss: 0.15432217, Gradient norm: 2.48136215
INFO:root:[   13] Training loss: 0.15791016, Validation loss: 0.15122806, Gradient norm: 2.63061768
INFO:root:[   14] Training loss: 0.15186922, Validation loss: 0.15967770, Gradient norm: 2.42722558
INFO:root:[   15] Training loss: 0.15084221, Validation loss: 0.14392189, Gradient norm: 2.31070694
INFO:root:[   16] Training loss: 0.14436798, Validation loss: 0.14291559, Gradient norm: 1.56019512
INFO:root:[   17] Training loss: 0.14771392, Validation loss: 0.14197497, Gradient norm: 3.00909485
INFO:root:[   18] Training loss: 0.14231982, Validation loss: 0.13845492, Gradient norm: 1.99931447
INFO:root:[   19] Training loss: 0.13823742, Validation loss: 0.13841644, Gradient norm: 1.38076598
INFO:root:[   20] Training loss: 0.13826532, Validation loss: 0.15387293, Gradient norm: 2.03279375
INFO:root:[   21] Training loss: 0.13970806, Validation loss: 0.13293546, Gradient norm: 2.45415979
INFO:root:[   22] Training loss: 0.13486423, Validation loss: 0.13405430, Gradient norm: 2.01918736
INFO:root:[   23] Training loss: 0.13515305, Validation loss: 0.14143878, Gradient norm: 2.60354025
INFO:root:[   24] Training loss: 0.13394798, Validation loss: 0.13047272, Gradient norm: 2.41871072
INFO:root:[   25] Training loss: 0.13448482, Validation loss: 0.12858528, Gradient norm: 2.91249346
INFO:root:[   26] Training loss: 0.12798916, Validation loss: 0.12612253, Gradient norm: 1.30828113
INFO:root:[   27] Training loss: 0.12963014, Validation loss: 0.12641128, Gradient norm: 2.61656739
INFO:root:[   28] Training loss: 0.12822921, Validation loss: 0.12786924, Gradient norm: 2.39431988
INFO:root:[   29] Training loss: 0.12479984, Validation loss: 0.12432027, Gradient norm: 1.60454866
INFO:root:[   30] Training loss: 0.12199549, Validation loss: 0.12021794, Gradient norm: 1.14265546
INFO:root:[   31] Training loss: 0.12103343, Validation loss: 0.11881575, Gradient norm: 1.76957966
INFO:root:[   32] Training loss: 0.11976234, Validation loss: 0.12374268, Gradient norm: 1.64139010
INFO:root:[   33] Training loss: 0.11810016, Validation loss: 0.11820256, Gradient norm: 1.46608926
INFO:root:[   34] Training loss: 0.11841639, Validation loss: 0.12236196, Gradient norm: 1.85660644
INFO:root:[   35] Training loss: 0.11661317, Validation loss: 0.12050125, Gradient norm: 1.96619148
INFO:root:[   36] Training loss: 0.12022461, Validation loss: 0.11955953, Gradient norm: 3.00723266
INFO:root:[   37] Training loss: 0.11466356, Validation loss: 0.11189092, Gradient norm: 2.08708490
INFO:root:[   38] Training loss: 0.11317646, Validation loss: 0.11105338, Gradient norm: 1.50917328
INFO:root:[   39] Training loss: 0.11415853, Validation loss: 0.11153677, Gradient norm: 2.40049059
INFO:root:[   40] Training loss: 0.11189199, Validation loss: 0.10957727, Gradient norm: 1.67655363
INFO:root:[   41] Training loss: 0.10924339, Validation loss: 0.11135887, Gradient norm: 1.18727858
INFO:root:[   42] Training loss: 0.10937930, Validation loss: 0.10853523, Gradient norm: 2.05668925
INFO:root:[   43] Training loss: 0.10836071, Validation loss: 0.10697307, Gradient norm: 2.22350861
INFO:root:[   44] Training loss: 0.10782066, Validation loss: 0.10816191, Gradient norm: 2.28421989
INFO:root:[   45] Training loss: 0.10635300, Validation loss: 0.10651931, Gradient norm: 2.00805947
INFO:root:[   46] Training loss: 0.10544406, Validation loss: 0.10467095, Gradient norm: 1.94400081
INFO:root:[   47] Training loss: 0.10354600, Validation loss: 0.10834982, Gradient norm: 1.29925797
INFO:root:[   48] Training loss: 0.11064566, Validation loss: 0.10753674, Gradient norm: 3.72459719
INFO:root:[   49] Training loss: 0.10327632, Validation loss: 0.10322773, Gradient norm: 1.63676378
INFO:root:[   50] Training loss: 0.10240141, Validation loss: 0.10004804, Gradient norm: 1.77855874
INFO:root:[   51] Training loss: 0.10275038, Validation loss: 0.10171203, Gradient norm: 1.72026799
INFO:root:[   52] Training loss: 0.10075853, Validation loss: 0.09776493, Gradient norm: 1.37903634
INFO:root:[   53] Training loss: 0.09954170, Validation loss: 0.10125088, Gradient norm: 1.69467570
INFO:root:[   54] Training loss: 0.10301991, Validation loss: 0.11041202, Gradient norm: 2.99360925
INFO:root:[   55] Training loss: 0.10086693, Validation loss: 0.10153376, Gradient norm: 2.42568278
INFO:root:[   56] Training loss: 0.10042569, Validation loss: 0.09938254, Gradient norm: 2.70237640
INFO:root:[   57] Training loss: 0.09740830, Validation loss: 0.09651833, Gradient norm: 2.30062832
INFO:root:[   58] Training loss: 0.09699778, Validation loss: 0.09971534, Gradient norm: 1.67353963
INFO:root:[   59] Training loss: 0.09936453, Validation loss: 0.09377789, Gradient norm: 2.65402967
INFO:root:[   60] Training loss: 0.09823153, Validation loss: 0.10352293, Gradient norm: 2.86281609
INFO:root:[   61] Training loss: 0.10111029, Validation loss: 0.09432685, Gradient norm: 3.44422764
INFO:root:[   62] Training loss: 0.09524248, Validation loss: 0.09820702, Gradient norm: 1.74405864
INFO:root:[   63] Training loss: 0.09562896, Validation loss: 0.09357421, Gradient norm: 2.03708640
INFO:root:[   64] Training loss: 0.09285624, Validation loss: 0.09415041, Gradient norm: 1.03065596
INFO:root:[   65] Training loss: 0.09251581, Validation loss: 0.09585007, Gradient norm: 1.73827535
INFO:root:[   66] Training loss: 0.09309244, Validation loss: 0.09301447, Gradient norm: 1.79079762
INFO:root:[   67] Training loss: 0.09335192, Validation loss: 0.09598502, Gradient norm: 2.15922760
INFO:root:[   68] Training loss: 0.09225673, Validation loss: 0.09033746, Gradient norm: 2.07518177
INFO:root:[   69] Training loss: 0.09217699, Validation loss: 0.09062308, Gradient norm: 1.60649260
INFO:root:[   70] Training loss: 0.09050324, Validation loss: 0.08902292, Gradient norm: 1.39829144
INFO:root:[   71] Training loss: 0.09216813, Validation loss: 0.09914511, Gradient norm: 2.12803028
INFO:root:[   72] Training loss: 0.09304200, Validation loss: 0.08891994, Gradient norm: 2.66813018
INFO:root:[   73] Training loss: 0.08961494, Validation loss: 0.09128704, Gradient norm: 1.69960586
INFO:root:[   74] Training loss: 0.09011036, Validation loss: 0.08961025, Gradient norm: 2.04466526
INFO:root:[   75] Training loss: 0.08963660, Validation loss: 0.08937730, Gradient norm: 1.88767204
INFO:root:[   76] Training loss: 0.08954581, Validation loss: 0.08921296, Gradient norm: 1.89099749
INFO:root:[   77] Training loss: 0.08965992, Validation loss: 0.08495710, Gradient norm: 2.41178869
INFO:root:[   78] Training loss: 0.08859300, Validation loss: 0.08922693, Gradient norm: 2.09851215
INFO:root:[   79] Training loss: 0.08924535, Validation loss: 0.09758183, Gradient norm: 2.49507477
INFO:root:[   80] Training loss: 0.08796771, Validation loss: 0.08604557, Gradient norm: 1.62388378
INFO:root:[   81] Training loss: 0.08836913, Validation loss: 0.09064853, Gradient norm: 2.35786216
INFO:root:[   82] Training loss: 0.08937133, Validation loss: 0.08632425, Gradient norm: 2.77615820
INFO:root:[   83] Training loss: 0.08520916, Validation loss: 0.08496996, Gradient norm: 0.98146301
INFO:root:[   84] Training loss: 0.08816113, Validation loss: 0.09186802, Gradient norm: 2.56750680
INFO:root:[   85] Training loss: 0.08761344, Validation loss: 0.08769199, Gradient norm: 2.54552062
INFO:root:[   86] Training loss: 0.08632608, Validation loss: 0.08450614, Gradient norm: 1.87567422
INFO:root:[   87] Training loss: 0.08583265, Validation loss: 0.08466673, Gradient norm: 1.91375167
INFO:root:[   88] Training loss: 0.08515514, Validation loss: 0.08542197, Gradient norm: 1.93898274
INFO:root:[   89] Training loss: 0.08517805, Validation loss: 0.08536953, Gradient norm: 1.88776476
INFO:root:[   90] Training loss: 0.08664345, Validation loss: 0.08417628, Gradient norm: 2.93957529
INFO:root:[   91] Training loss: 0.08589366, Validation loss: 0.08456559, Gradient norm: 2.20262498
INFO:root:[   92] Training loss: 0.08313035, Validation loss: 0.08358896, Gradient norm: 1.42927631
INFO:root:[   93] Training loss: 0.08424788, Validation loss: 0.08862508, Gradient norm: 2.15880350
INFO:root:[   94] Training loss: 0.08382638, Validation loss: 0.08778819, Gradient norm: 1.87873500
INFO:root:[   95] Training loss: 0.08245667, Validation loss: 0.08269077, Gradient norm: 1.49324239
INFO:root:[   96] Training loss: 0.08290041, Validation loss: 0.08084152, Gradient norm: 1.49017824
INFO:root:[   97] Training loss: 0.08181997, Validation loss: 0.08707273, Gradient norm: 1.62613653
INFO:root:[   98] Training loss: 0.08320022, Validation loss: 0.08252790, Gradient norm: 2.50897901
INFO:root:[   99] Training loss: 0.08359956, Validation loss: 0.07972167, Gradient norm: 2.58454812
INFO:root:[  100] Training loss: 0.08378845, Validation loss: 0.08852436, Gradient norm: 2.62629281
INFO:root:[  101] Training loss: 0.08275809, Validation loss: 0.08298383, Gradient norm: 2.30359057
INFO:root:[  102] Training loss: 0.08236738, Validation loss: 0.08108710, Gradient norm: 2.39250122
INFO:root:[  103] Training loss: 0.08306409, Validation loss: 0.07944995, Gradient norm: 2.76738586
INFO:root:[  104] Training loss: 0.08185107, Validation loss: 0.08024937, Gradient norm: 2.25052171
INFO:root:[  105] Training loss: 0.08158561, Validation loss: 0.08212626, Gradient norm: 2.17921209
INFO:root:[  106] Training loss: 0.08045175, Validation loss: 0.07903342, Gradient norm: 1.97021490
INFO:root:[  107] Training loss: 0.08015459, Validation loss: 0.07879646, Gradient norm: 1.81392191
INFO:root:[  108] Training loss: 0.07938524, Validation loss: 0.07908629, Gradient norm: 1.75463215
INFO:root:[  109] Training loss: 0.07971879, Validation loss: 0.07772075, Gradient norm: 1.78529425
INFO:root:[  110] Training loss: 0.07986539, Validation loss: 0.08363763, Gradient norm: 2.13002459
INFO:root:[  111] Training loss: 0.07904555, Validation loss: 0.07756818, Gradient norm: 1.69439988
INFO:root:[  112] Training loss: 0.08013785, Validation loss: 0.07762937, Gradient norm: 2.42634318
INFO:root:[  113] Training loss: 0.07831653, Validation loss: 0.08470859, Gradient norm: 1.65769695
INFO:root:[  114] Training loss: 0.07893035, Validation loss: 0.07901015, Gradient norm: 2.23510455
INFO:root:[  115] Training loss: 0.07856316, Validation loss: 0.07665229, Gradient norm: 2.24449745
INFO:root:[  116] Training loss: 0.07836007, Validation loss: 0.07771070, Gradient norm: 2.43496421
INFO:root:[  117] Training loss: 0.07786982, Validation loss: 0.07778183, Gradient norm: 1.88126295
INFO:root:[  118] Training loss: 0.07662576, Validation loss: 0.07579717, Gradient norm: 1.48059378
INFO:root:[  119] Training loss: 0.07667360, Validation loss: 0.08092959, Gradient norm: 1.70329613
INFO:root:[  120] Training loss: 0.07691677, Validation loss: 0.07791655, Gradient norm: 2.12036769
INFO:root:[  121] Training loss: 0.07700873, Validation loss: 0.07565053, Gradient norm: 1.90320608
INFO:root:[  122] Training loss: 0.07856334, Validation loss: 0.07484591, Gradient norm: 2.87934315
INFO:root:[  123] Training loss: 0.07757362, Validation loss: 0.07630656, Gradient norm: 2.50190571
INFO:root:[  124] Training loss: 0.07635647, Validation loss: 0.07421392, Gradient norm: 1.98941003
INFO:root:[  125] Training loss: 0.07629055, Validation loss: 0.07942632, Gradient norm: 2.10149783
INFO:root:[  126] Training loss: 0.07562017, Validation loss: 0.07501850, Gradient norm: 1.89413074
INFO:root:[  127] Training loss: 0.07552909, Validation loss: 0.07396769, Gradient norm: 2.00685144
INFO:root:[  128] Training loss: 0.07438255, Validation loss: 0.07428959, Gradient norm: 1.16066739
INFO:root:[  129] Training loss: 0.07531734, Validation loss: 0.07566882, Gradient norm: 2.02828049
INFO:root:[  130] Training loss: 0.07512837, Validation loss: 0.07368367, Gradient norm: 2.06671517
INFO:root:[  131] Training loss: 0.07523723, Validation loss: 0.07684299, Gradient norm: 2.31639494
INFO:root:[  132] Training loss: 0.07562171, Validation loss: 0.07677488, Gradient norm: 2.46286817
INFO:root:[  133] Training loss: 0.07494349, Validation loss: 0.07456251, Gradient norm: 2.36759645
INFO:root:[  134] Training loss: 0.07450521, Validation loss: 0.07313233, Gradient norm: 2.34104565
INFO:root:[  135] Training loss: 0.07327677, Validation loss: 0.07276363, Gradient norm: 1.45413898
INFO:root:[  136] Training loss: 0.07348696, Validation loss: 0.07364250, Gradient norm: 1.85468200
INFO:root:[  137] Training loss: 0.07433847, Validation loss: 0.07417893, Gradient norm: 2.31925318
INFO:root:[  138] Training loss: 0.07439757, Validation loss: 0.07164862, Gradient norm: 2.60279365
INFO:root:[  139] Training loss: 0.07317126, Validation loss: 0.07181333, Gradient norm: 1.81746395
INFO:root:[  140] Training loss: 0.07309377, Validation loss: 0.07340714, Gradient norm: 2.28212832
INFO:root:[  141] Training loss: 0.07279770, Validation loss: 0.07295440, Gradient norm: 2.08112092
INFO:root:[  142] Training loss: 0.07217554, Validation loss: 0.07109670, Gradient norm: 1.68508164
INFO:root:[  143] Training loss: 0.07134318, Validation loss: 0.07441826, Gradient norm: 1.13976117
INFO:root:[  144] Training loss: 0.07209102, Validation loss: 0.07068856, Gradient norm: 1.81631347
INFO:root:[  145] Training loss: 0.07269479, Validation loss: 0.07112108, Gradient norm: 2.38112897
INFO:root:[  146] Training loss: 0.07307593, Validation loss: 0.07365105, Gradient norm: 2.63582427
INFO:root:[  147] Training loss: 0.07264535, Validation loss: 0.07210568, Gradient norm: 2.53020595
INFO:root:[  148] Training loss: 0.07299608, Validation loss: 0.07152501, Gradient norm: 2.69086341
INFO:root:[  149] Training loss: 0.07134186, Validation loss: 0.07021321, Gradient norm: 1.92830926
INFO:root:[  150] Training loss: 0.07056808, Validation loss: 0.06991911, Gradient norm: 1.75037694
INFO:root:[  151] Training loss: 0.06971110, Validation loss: 0.07005419, Gradient norm: 1.12926626
INFO:root:[  152] Training loss: 0.07054920, Validation loss: 0.06902647, Gradient norm: 1.76767760
INFO:root:[  153] Training loss: 0.07115061, Validation loss: 0.07099880, Gradient norm: 2.13405003
INFO:root:[  154] Training loss: 0.07074711, Validation loss: 0.07435760, Gradient norm: 2.16578583
INFO:root:[  155] Training loss: 0.07091751, Validation loss: 0.07157985, Gradient norm: 2.39014770
INFO:root:[  156] Training loss: 0.07097655, Validation loss: 0.07020380, Gradient norm: 2.32630513
INFO:root:[  157] Training loss: 0.06946887, Validation loss: 0.06819186, Gradient norm: 1.46327578
INFO:root:[  158] Training loss: 0.06849383, Validation loss: 0.06994042, Gradient norm: 0.98110357
INFO:root:[  159] Training loss: 0.06907354, Validation loss: 0.06794784, Gradient norm: 1.80293486
INFO:root:[  160] Training loss: 0.06917802, Validation loss: 0.06788169, Gradient norm: 1.74423830
INFO:root:[  161] Training loss: 0.06968804, Validation loss: 0.06818687, Gradient norm: 2.25409628
INFO:root:[  162] Training loss: 0.06917640, Validation loss: 0.06953508, Gradient norm: 1.83162423
INFO:root:[  163] Training loss: 0.06830990, Validation loss: 0.07170953, Gradient norm: 1.59196619
INFO:root:[  164] Training loss: 0.06910882, Validation loss: 0.06821094, Gradient norm: 2.24349992
INFO:root:[  165] Training loss: 0.06864110, Validation loss: 0.06790924, Gradient norm: 2.20374823
INFO:root:[  166] Training loss: 0.06723979, Validation loss: 0.06729190, Gradient norm: 1.12482104
INFO:root:[  167] Training loss: 0.06794518, Validation loss: 0.06812030, Gradient norm: 1.95202625
INFO:root:[  168] Training loss: 0.06892356, Validation loss: 0.06734293, Gradient norm: 2.50932617
INFO:root:[  169] Training loss: 0.06742468, Validation loss: 0.06699389, Gradient norm: 1.45422864
INFO:root:[  170] Training loss: 0.06747425, Validation loss: 0.06689802, Gradient norm: 1.68422897
INFO:root:[  171] Training loss: 0.06675762, Validation loss: 0.06638145, Gradient norm: 1.36000608
INFO:root:[  172] Training loss: 0.06735020, Validation loss: 0.06700813, Gradient norm: 1.83155131
INFO:root:[  173] Training loss: 0.06694048, Validation loss: 0.06678823, Gradient norm: 1.78813919
INFO:root:[  174] Training loss: 0.06661521, Validation loss: 0.06618463, Gradient norm: 1.68237426
INFO:root:[  175] Training loss: 0.06604607, Validation loss: 0.06623695, Gradient norm: 1.27047770
INFO:root:[  176] Training loss: 0.06616822, Validation loss: 0.06652396, Gradient norm: 1.44669267
INFO:root:[  177] Training loss: 0.06606900, Validation loss: 0.06707932, Gradient norm: 1.53849664
INFO:root:[  178] Training loss: 0.06573707, Validation loss: 0.06517816, Gradient norm: 1.25407819
INFO:root:[  179] Training loss: 0.06495793, Validation loss: 0.06495839, Gradient norm: 0.86280527
INFO:root:[  180] Training loss: 0.06592321, Validation loss: 0.06729923, Gradient norm: 1.60163248
INFO:root:[  181] Training loss: 0.06611576, Validation loss: 0.06760433, Gradient norm: 2.15278001
INFO:root:[  182] Training loss: 0.06655519, Validation loss: 0.06539633, Gradient norm: 2.32758370
INFO:root:[  183] Training loss: 0.06563461, Validation loss: 0.06388097, Gradient norm: 1.94732164
INFO:root:[  184] Training loss: 0.06515696, Validation loss: 0.06406160, Gradient norm: 1.77611741
INFO:root:[  185] Training loss: 0.06459490, Validation loss: 0.06508001, Gradient norm: 1.38162688
INFO:root:[  186] Training loss: 0.06447048, Validation loss: 0.06485542, Gradient norm: 1.38585012
INFO:root:[  187] Training loss: 0.06443784, Validation loss: 0.06538604, Gradient norm: 1.54887004
INFO:root:[  188] Training loss: 0.06521891, Validation loss: 0.06408579, Gradient norm: 2.08257631
INFO:root:[  189] Training loss: 0.06582140, Validation loss: 0.06596361, Gradient norm: 2.48680954
INFO:root:[  190] Training loss: 0.06426993, Validation loss: 0.06718503, Gradient norm: 1.63047845
INFO:root:[  191] Training loss: 0.06399551, Validation loss: 0.06397554, Gradient norm: 1.57859144
INFO:root:[  192] Training loss: 0.06356262, Validation loss: 0.06315062, Gradient norm: 1.43999850
INFO:root:[  193] Training loss: 0.06329774, Validation loss: 0.06480163, Gradient norm: 1.16830385
INFO:root:[  194] Training loss: 0.06398343, Validation loss: 0.06460453, Gradient norm: 1.92620947
INFO:root:[  195] Training loss: 0.06394641, Validation loss: 0.06215501, Gradient norm: 1.92725409
INFO:root:[  196] Training loss: 0.06366412, Validation loss: 0.06241452, Gradient norm: 1.77434159
INFO:root:[  197] Training loss: 0.06297061, Validation loss: 0.06293560, Gradient norm: 1.21170307
INFO:root:[  198] Training loss: 0.06319311, Validation loss: 0.06294701, Gradient norm: 1.59051945
INFO:root:[  199] Training loss: 0.06315922, Validation loss: 0.06208152, Gradient norm: 1.68847761
INFO:root:[  200] Training loss: 0.06349232, Validation loss: 0.06330284, Gradient norm: 1.98184513
INFO:root:[  201] Training loss: 0.06297685, Validation loss: 0.06243556, Gradient norm: 1.84079268
INFO:root:[  202] Training loss: 0.06227291, Validation loss: 0.06288132, Gradient norm: 1.31219461
INFO:root:[  203] Training loss: 0.06264131, Validation loss: 0.06360548, Gradient norm: 1.75510469
INFO:root:[  204] Training loss: 0.06262464, Validation loss: 0.06208999, Gradient norm: 1.73853095
INFO:root:[  205] Training loss: 0.06204470, Validation loss: 0.06174631, Gradient norm: 1.46140985
INFO:root:[  206] Training loss: 0.06184696, Validation loss: 0.06393462, Gradient norm: 1.37694218
INFO:root:[  207] Training loss: 0.06216979, Validation loss: 0.06311936, Gradient norm: 1.79195636
INFO:root:[  208] Training loss: 0.06277785, Validation loss: 0.06172072, Gradient norm: 2.22982598
INFO:root:[  209] Training loss: 0.06325872, Validation loss: 0.06284721, Gradient norm: 2.38478070
INFO:root:[  210] Training loss: 0.06220086, Validation loss: 0.06178191, Gradient norm: 1.89449093
INFO:root:[  211] Training loss: 0.06154320, Validation loss: 0.06339102, Gradient norm: 1.48695346
INFO:root:[  212] Training loss: 0.06242235, Validation loss: 0.06393247, Gradient norm: 2.19908761
INFO:root:[  213] Training loss: 0.06223862, Validation loss: 0.06244988, Gradient norm: 2.09240986
INFO:root:[  214] Training loss: 0.06121393, Validation loss: 0.06183189, Gradient norm: 1.62120509
INFO:root:[  215] Training loss: 0.06129901, Validation loss: 0.06073064, Gradient norm: 1.67106824
INFO:root:[  216] Training loss: 0.06115652, Validation loss: 0.06325867, Gradient norm: 1.67444852
INFO:root:[  217] Training loss: 0.06087844, Validation loss: 0.06130094, Gradient norm: 1.52080942
INFO:root:[  218] Training loss: 0.06068561, Validation loss: 0.06083000, Gradient norm: 1.33177902
INFO:root:[  219] Training loss: 0.06092528, Validation loss: 0.06001403, Gradient norm: 1.77880066
INFO:root:[  220] Training loss: 0.06158928, Validation loss: 0.06057831, Gradient norm: 2.11554290
INFO:root:[  221] Training loss: 0.06066084, Validation loss: 0.06026656, Gradient norm: 1.63673281
INFO:root:[  222] Training loss: 0.06042012, Validation loss: 0.05951764, Gradient norm: 1.53847694
INFO:root:[  223] Training loss: 0.06020248, Validation loss: 0.06068725, Gradient norm: 1.46882536
INFO:root:[  224] Training loss: 0.06052181, Validation loss: 0.06157292, Gradient norm: 1.78142925
INFO:root:[  225] Training loss: 0.05970935, Validation loss: 0.05980618, Gradient norm: 1.30952365
INFO:root:[  226] Training loss: 0.06018567, Validation loss: 0.05916996, Gradient norm: 1.63945559
INFO:root:[  227] Training loss: 0.06026940, Validation loss: 0.06092622, Gradient norm: 1.86461953
INFO:root:[  228] Training loss: 0.06018474, Validation loss: 0.06121578, Gradient norm: 1.76139320
INFO:root:[  229] Training loss: 0.06032501, Validation loss: 0.05990344, Gradient norm: 1.92722972
INFO:root:[  230] Training loss: 0.05978023, Validation loss: 0.05955759, Gradient norm: 1.67721053
INFO:root:[  231] Training loss: 0.05963895, Validation loss: 0.05882127, Gradient norm: 1.55955125
INFO:root:[  232] Training loss: 0.05980396, Validation loss: 0.05853511, Gradient norm: 1.91092921
INFO:root:[  233] Training loss: 0.05961732, Validation loss: 0.05981776, Gradient norm: 1.72280716
INFO:root:[  234] Training loss: 0.05952984, Validation loss: 0.05963548, Gradient norm: 1.76500044
INFO:root:[  235] Training loss: 0.05965841, Validation loss: 0.05913585, Gradient norm: 1.94716459
INFO:root:[  236] Training loss: 0.05923594, Validation loss: 0.06045762, Gradient norm: 1.74234694
INFO:root:[  237] Training loss: 0.05898269, Validation loss: 0.05970939, Gradient norm: 1.51685986
INFO:root:[  238] Training loss: 0.05946907, Validation loss: 0.05958859, Gradient norm: 2.04796759
INFO:root:[  239] Training loss: 0.05903531, Validation loss: 0.05902477, Gradient norm: 1.64762045
INFO:root:[  240] Training loss: 0.05964753, Validation loss: 0.05951476, Gradient norm: 2.12025957
INFO:root:[  241] Training loss: 0.05912239, Validation loss: 0.06025071, Gradient norm: 1.99586729
INFO:root:EP 241: Early stopping
INFO:root:Training the model took 11156.231s.
INFO:root:Emptying the cuda cache took 0.112s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 2.42044
INFO:root:EnergyScoreTrain: 1.83073
INFO:root:CoverageTrain: 0.98091
INFO:root:IntervalWidthTrain: 0.1478
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 2.33169
INFO:root:EnergyScoreValidation: 1.76227
INFO:root:CoverageValidation: 0.98106
INFO:root:IntervalWidthValidation: 0.14835
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.91946
INFO:root:EnergyScoreTest: 1.45304
INFO:root:CoverageTest: 0.98091
INFO:root:IntervalWidthTest: 0.14698
INFO:root:###3 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 369098752
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.93301103, Validation loss: 0.33455139, Gradient norm: 6.62399970
INFO:root:[    2] Training loss: 0.29868646, Validation loss: 0.27783951, Gradient norm: 1.57819990
INFO:root:[    3] Training loss: 0.26197515, Validation loss: 0.24772862, Gradient norm: 1.25324547
INFO:root:[    4] Training loss: 0.23753123, Validation loss: 0.23056417, Gradient norm: 1.33073248
INFO:root:[    5] Training loss: 0.22630368, Validation loss: 0.21229838, Gradient norm: 1.52082189
INFO:root:[    6] Training loss: 0.21120722, Validation loss: 0.20634450, Gradient norm: 1.36044836
INFO:root:[    7] Training loss: 0.20595499, Validation loss: 0.20782368, Gradient norm: 2.09127707
INFO:root:[    8] Training loss: 0.20153552, Validation loss: 0.19134341, Gradient norm: 2.37121869
INFO:root:[    9] Training loss: 0.19305594, Validation loss: 0.18887379, Gradient norm: 1.79955756
INFO:root:[   10] Training loss: 0.18607247, Validation loss: 0.18408977, Gradient norm: 1.86189665
INFO:root:[   11] Training loss: 0.18134256, Validation loss: 0.18049417, Gradient norm: 1.78238248
INFO:root:[   12] Training loss: 0.17655964, Validation loss: 0.17923247, Gradient norm: 2.10141277
INFO:root:[   13] Training loss: 0.17418431, Validation loss: 0.17620283, Gradient norm: 2.06629147
INFO:root:[   14] Training loss: 0.16893389, Validation loss: 0.16194108, Gradient norm: 1.87348478
INFO:root:[   15] Training loss: 0.16673886, Validation loss: 0.16189804, Gradient norm: 2.09855239
INFO:root:[   16] Training loss: 0.16206583, Validation loss: 0.15632416, Gradient norm: 1.67840926
INFO:root:[   17] Training loss: 0.15708902, Validation loss: 0.17047335, Gradient norm: 1.68464221
INFO:root:[   18] Training loss: 0.16012277, Validation loss: 0.15056324, Gradient norm: 2.80393932
INFO:root:[   19] Training loss: 0.15617106, Validation loss: 0.14859347, Gradient norm: 3.07960991
INFO:root:[   20] Training loss: 0.15062574, Validation loss: 0.15423846, Gradient norm: 2.43361637
INFO:root:[   21] Training loss: 0.15092648, Validation loss: 0.15184177, Gradient norm: 2.27954577
INFO:root:[   22] Training loss: 0.14721624, Validation loss: 0.15009332, Gradient norm: 2.57773326
INFO:root:[   23] Training loss: 0.14635818, Validation loss: 0.14699165, Gradient norm: 2.59055427
INFO:root:[   24] Training loss: 0.14240080, Validation loss: 0.13577611, Gradient norm: 2.56431188
INFO:root:[   25] Training loss: 0.13755866, Validation loss: 0.14376563, Gradient norm: 1.34226886
INFO:root:[   26] Training loss: 0.13975990, Validation loss: 0.13261915, Gradient norm: 2.57150030
INFO:root:[   27] Training loss: 0.13663071, Validation loss: 0.13840521, Gradient norm: 2.24728967
INFO:root:[   28] Training loss: 0.13208753, Validation loss: 0.12956848, Gradient norm: 1.01735852
INFO:root:[   29] Training loss: 0.13187270, Validation loss: 0.13692452, Gradient norm: 1.71464160
INFO:root:[   30] Training loss: 0.12976827, Validation loss: 0.13820427, Gradient norm: 1.72758397
INFO:root:[   31] Training loss: 0.12984869, Validation loss: 0.12453553, Gradient norm: 2.42755695
INFO:root:[   32] Training loss: 0.12471861, Validation loss: 0.12632407, Gradient norm: 1.25975984
INFO:root:[   33] Training loss: 0.12574757, Validation loss: 0.12749356, Gradient norm: 2.30899802
INFO:root:[   34] Training loss: 0.12364435, Validation loss: 0.12994445, Gradient norm: 2.48067372
INFO:root:[   35] Training loss: 0.12275495, Validation loss: 0.12138779, Gradient norm: 1.79498445
INFO:root:[   36] Training loss: 0.12241386, Validation loss: 0.12826702, Gradient norm: 2.36844270
INFO:root:[   37] Training loss: 0.12123979, Validation loss: 0.12164943, Gradient norm: 3.04769326
INFO:root:[   38] Training loss: 0.12003560, Validation loss: 0.11841100, Gradient norm: 2.51897852
INFO:root:[   39] Training loss: 0.11806569, Validation loss: 0.11371080, Gradient norm: 2.15220895
INFO:root:[   40] Training loss: 0.11648461, Validation loss: 0.11869662, Gradient norm: 1.71472811
INFO:root:[   41] Training loss: 0.11550691, Validation loss: 0.11382417, Gradient norm: 1.88229006
INFO:root:[   42] Training loss: 0.11405328, Validation loss: 0.11144810, Gradient norm: 1.60917192
INFO:root:[   43] Training loss: 0.11280633, Validation loss: 0.11395316, Gradient norm: 1.40337750
INFO:root:[   44] Training loss: 0.11168896, Validation loss: 0.11164969, Gradient norm: 1.82689498
INFO:root:[   45] Training loss: 0.11028911, Validation loss: 0.11195764, Gradient norm: 1.52447769
INFO:root:[   46] Training loss: 0.11009527, Validation loss: 0.12044101, Gradient norm: 1.55057661
INFO:root:[   47] Training loss: 0.10931986, Validation loss: 0.10813916, Gradient norm: 1.64602292
INFO:root:[   48] Training loss: 0.10872144, Validation loss: 0.10805166, Gradient norm: 2.14792815
INFO:root:[   49] Training loss: 0.10912690, Validation loss: 0.10971727, Gradient norm: 2.55840693
INFO:root:[   50] Training loss: 0.10866884, Validation loss: 0.11296385, Gradient norm: 2.64746224
INFO:root:[   51] Training loss: 0.10643837, Validation loss: 0.11097832, Gradient norm: 2.40920656
INFO:root:[   52] Training loss: 0.10813195, Validation loss: 0.10566325, Gradient norm: 2.90290580
INFO:root:[   53] Training loss: 0.10306464, Validation loss: 0.10341332, Gradient norm: 1.33686622
INFO:root:[   54] Training loss: 0.10269327, Validation loss: 0.10380689, Gradient norm: 1.05960873
INFO:root:[   55] Training loss: 0.10255875, Validation loss: 0.10201530, Gradient norm: 1.17224571
INFO:root:[   56] Training loss: 0.10214189, Validation loss: 0.10099274, Gradient norm: 1.36703931
INFO:root:[   57] Training loss: 0.10307226, Validation loss: 0.10103802, Gradient norm: 2.02002285
INFO:root:[   58] Training loss: 0.09990180, Validation loss: 0.10286892, Gradient norm: 0.90682058
INFO:root:[   59] Training loss: 0.10198028, Validation loss: 0.10076600, Gradient norm: 1.95972748
INFO:root:[   60] Training loss: 0.09881335, Validation loss: 0.10027194, Gradient norm: 1.19003309
INFO:root:[   61] Training loss: 0.09926858, Validation loss: 0.10172972, Gradient norm: 1.70401617
INFO:root:[   62] Training loss: 0.09887301, Validation loss: 0.09670132, Gradient norm: 1.88223667
INFO:root:[   63] Training loss: 0.09832763, Validation loss: 0.09992339, Gradient norm: 1.87672043
INFO:root:[   64] Training loss: 0.09770312, Validation loss: 0.09577260, Gradient norm: 1.81226522
INFO:root:[   65] Training loss: 0.09606651, Validation loss: 0.09530221, Gradient norm: 1.00848119
INFO:root:[   66] Training loss: 0.09551474, Validation loss: 0.09391387, Gradient norm: 1.23235572
INFO:root:[   67] Training loss: 0.09633727, Validation loss: 0.09509866, Gradient norm: 1.90035615
INFO:root:[   68] Training loss: 0.09519376, Validation loss: 0.10218562, Gradient norm: 1.85748019
INFO:root:[   69] Training loss: 0.09729232, Validation loss: 0.09371268, Gradient norm: 2.35217727
INFO:root:[   70] Training loss: 0.09352716, Validation loss: 0.09434396, Gradient norm: 1.45844679
INFO:root:[   71] Training loss: 0.09476414, Validation loss: 0.09854412, Gradient norm: 1.83973511
INFO:root:[   72] Training loss: 0.09390603, Validation loss: 0.09140810, Gradient norm: 2.05206803
INFO:root:[   73] Training loss: 0.09170287, Validation loss: 0.09104707, Gradient norm: 0.95378257
INFO:root:[   74] Training loss: 0.09160898, Validation loss: 0.09299981, Gradient norm: 1.23625660
INFO:root:[   75] Training loss: 0.09236836, Validation loss: 0.09000581, Gradient norm: 1.93087439
INFO:root:[   76] Training loss: 0.09176493, Validation loss: 0.09025688, Gradient norm: 1.51102795
INFO:root:[   77] Training loss: 0.09075487, Validation loss: 0.09068468, Gradient norm: 1.46155001
INFO:root:[   78] Training loss: 0.09069003, Validation loss: 0.09075162, Gradient norm: 1.40620493
INFO:root:[   79] Training loss: 0.09081792, Validation loss: 0.08828391, Gradient norm: 1.66822650
INFO:root:[   80] Training loss: 0.08912827, Validation loss: 0.08937652, Gradient norm: 1.09825297
INFO:root:[   81] Training loss: 0.08884458, Validation loss: 0.08853112, Gradient norm: 1.26655956
INFO:root:[   82] Training loss: 0.08873853, Validation loss: 0.09049893, Gradient norm: 1.36095287
INFO:root:[   83] Training loss: 0.08769142, Validation loss: 0.08697878, Gradient norm: 1.11031289
INFO:root:[   84] Training loss: 0.08746717, Validation loss: 0.08749574, Gradient norm: 1.27309551
INFO:root:[   85] Training loss: 0.08686133, Validation loss: 0.08822721, Gradient norm: 1.07907525
INFO:root:[   86] Training loss: 0.08713229, Validation loss: 0.08595084, Gradient norm: 1.47102513
INFO:root:[   87] Training loss: 0.08624598, Validation loss: 0.08818818, Gradient norm: 1.06356549
INFO:root:[   88] Training loss: 0.08636144, Validation loss: 0.08771415, Gradient norm: 1.51040796
INFO:root:[   89] Training loss: 0.08580228, Validation loss: 0.08950149, Gradient norm: 1.44883262
INFO:root:[   90] Training loss: 0.08544921, Validation loss: 0.08543241, Gradient norm: 1.16329274
INFO:root:[   91] Training loss: 0.08535405, Validation loss: 0.08355956, Gradient norm: 1.45979300
INFO:root:[   92] Training loss: 0.08556380, Validation loss: 0.08346115, Gradient norm: 1.83672725
INFO:root:[   93] Training loss: 0.08490708, Validation loss: 0.08805232, Gradient norm: 1.66541131
INFO:root:[   94] Training loss: 0.08478848, Validation loss: 0.08423228, Gradient norm: 1.71024499
INFO:root:[   95] Training loss: 0.08465317, Validation loss: 0.08322698, Gradient norm: 2.00356439
INFO:root:[   96] Training loss: 0.08459522, Validation loss: 0.08358565, Gradient norm: 1.93025544
INFO:root:[   97] Training loss: 0.08311516, Validation loss: 0.08375201, Gradient norm: 1.48793347
INFO:root:[   98] Training loss: 0.08307928, Validation loss: 0.08367066, Gradient norm: 1.60354635
INFO:root:[   99] Training loss: 0.08279878, Validation loss: 0.08468005, Gradient norm: 1.43910677
INFO:root:[  100] Training loss: 0.08180550, Validation loss: 0.08120057, Gradient norm: 1.36956354
INFO:root:[  101] Training loss: 0.08186954, Validation loss: 0.08061506, Gradient norm: 1.49078156
INFO:root:[  102] Training loss: 0.08097874, Validation loss: 0.08288721, Gradient norm: 0.95999949
INFO:root:[  103] Training loss: 0.08117654, Validation loss: 0.08008990, Gradient norm: 1.41827434
INFO:root:[  104] Training loss: 0.07973958, Validation loss: 0.07969453, Gradient norm: 0.44996724
INFO:root:[  105] Training loss: 0.08023045, Validation loss: 0.08144387, Gradient norm: 1.21993818
INFO:root:[  106] Training loss: 0.08064937, Validation loss: 0.07968149, Gradient norm: 1.61946870
INFO:root:[  107] Training loss: 0.07942277, Validation loss: 0.08195782, Gradient norm: 1.01216918
INFO:root:[  108] Training loss: 0.08063636, Validation loss: 0.07864929, Gradient norm: 1.83550577
INFO:root:[  109] Training loss: 0.07982845, Validation loss: 0.08456891, Gradient norm: 1.45177684
INFO:root:[  110] Training loss: 0.07994639, Validation loss: 0.07877817, Gradient norm: 1.81114851
INFO:root:[  111] Training loss: 0.07966674, Validation loss: 0.07815367, Gradient norm: 1.73105346
INFO:root:[  112] Training loss: 0.07894690, Validation loss: 0.07801686, Gradient norm: 1.46133969
INFO:root:[  113] Training loss: 0.07774122, Validation loss: 0.07738792, Gradient norm: 0.74607820
INFO:root:[  114] Training loss: 0.07738495, Validation loss: 0.07731526, Gradient norm: 0.79730653
INFO:root:[  115] Training loss: 0.07773747, Validation loss: 0.07795467, Gradient norm: 1.18479513
INFO:root:[  116] Training loss: 0.07758053, Validation loss: 0.07684334, Gradient norm: 1.35545596
INFO:root:[  117] Training loss: 0.07753535, Validation loss: 0.07678166, Gradient norm: 1.51634941
INFO:root:[  118] Training loss: 0.07752369, Validation loss: 0.07722969, Gradient norm: 1.46166501
INFO:root:[  119] Training loss: 0.07637867, Validation loss: 0.07617489, Gradient norm: 0.86302866
INFO:root:[  120] Training loss: 0.07604316, Validation loss: 0.07536105, Gradient norm: 0.87021101
INFO:root:[  121] Training loss: 0.07602321, Validation loss: 0.07546668, Gradient norm: 1.01928326
INFO:root:[  122] Training loss: 0.07642944, Validation loss: 0.07608971, Gradient norm: 1.40919256
INFO:root:[  123] Training loss: 0.07557351, Validation loss: 0.07462491, Gradient norm: 1.13765705
INFO:root:[  124] Training loss: 0.07528310, Validation loss: 0.07514608, Gradient norm: 0.89716868
INFO:root:[  125] Training loss: 0.07548099, Validation loss: 0.07805950, Gradient norm: 1.32170263
INFO:root:[  126] Training loss: 0.07586968, Validation loss: 0.07541728, Gradient norm: 1.66078364
INFO:root:[  127] Training loss: 0.07589025, Validation loss: 0.07483608, Gradient norm: 1.69761119
INFO:root:[  128] Training loss: 0.07497955, Validation loss: 0.07420719, Gradient norm: 1.40217755
INFO:root:[  129] Training loss: 0.07390699, Validation loss: 0.07614623, Gradient norm: 0.52234146
INFO:root:[  130] Training loss: 0.07461616, Validation loss: 0.07342715, Gradient norm: 1.30438627
INFO:root:[  131] Training loss: 0.07351137, Validation loss: 0.07378379, Gradient norm: 0.78394806
INFO:root:[  132] Training loss: 0.07380635, Validation loss: 0.07316972, Gradient norm: 1.08676613
INFO:root:[  133] Training loss: 0.07346239, Validation loss: 0.07325045, Gradient norm: 0.95755275
INFO:root:[  134] Training loss: 0.07364441, Validation loss: 0.07354543, Gradient norm: 1.31738928
INFO:root:[  135] Training loss: 0.07301141, Validation loss: 0.07409525, Gradient norm: 0.99648550
INFO:root:[  136] Training loss: 0.07304929, Validation loss: 0.07218271, Gradient norm: 1.23790524
INFO:root:[  137] Training loss: 0.07258883, Validation loss: 0.07280951, Gradient norm: 0.85617799
INFO:root:[  138] Training loss: 0.07292648, Validation loss: 0.07219726, Gradient norm: 1.31873527
INFO:root:[  139] Training loss: 0.07297449, Validation loss: 0.07223642, Gradient norm: 1.33929225
INFO:root:[  140] Training loss: 0.07269571, Validation loss: 0.07202940, Gradient norm: 1.35746345
INFO:root:[  141] Training loss: 0.07192400, Validation loss: 0.07422436, Gradient norm: 0.83959902
INFO:root:[  142] Training loss: 0.07203908, Validation loss: 0.07223475, Gradient norm: 1.18382340
INFO:root:[  143] Training loss: 0.07160774, Validation loss: 0.07119332, Gradient norm: 1.04520620
INFO:root:[  144] Training loss: 0.07147062, Validation loss: 0.07080375, Gradient norm: 0.93268954
INFO:root:[  145] Training loss: 0.07084618, Validation loss: 0.07107813, Gradient norm: 0.55168758
INFO:root:[  146] Training loss: 0.07129203, Validation loss: 0.07145732, Gradient norm: 1.17529683
INFO:root:[  147] Training loss: 0.07096260, Validation loss: 0.07112582, Gradient norm: 1.02517102
INFO:root:[  148] Training loss: 0.07085693, Validation loss: 0.07031153, Gradient norm: 1.05762288
INFO:root:[  149] Training loss: 0.07047175, Validation loss: 0.07026072, Gradient norm: 0.87327464
INFO:root:[  150] Training loss: 0.07039294, Validation loss: 0.07048794, Gradient norm: 0.97147448
INFO:root:[  151] Training loss: 0.07043901, Validation loss: 0.07170923, Gradient norm: 1.07818167
INFO:root:[  152] Training loss: 0.07083268, Validation loss: 0.07120737, Gradient norm: 1.54936726
INFO:root:[  153] Training loss: 0.06990954, Validation loss: 0.07091501, Gradient norm: 0.88457446
INFO:root:[  154] Training loss: 0.07014598, Validation loss: 0.06936005, Gradient norm: 1.29142495
INFO:root:[  155] Training loss: 0.06975586, Validation loss: 0.07064613, Gradient norm: 1.10374617
INFO:root:[  156] Training loss: 0.06995202, Validation loss: 0.06966145, Gradient norm: 1.33116925
INFO:root:[  157] Training loss: 0.06964315, Validation loss: 0.06890168, Gradient norm: 1.12476648
INFO:root:[  158] Training loss: 0.06929924, Validation loss: 0.06899898, Gradient norm: 1.01388673
INFO:root:[  159] Training loss: 0.06918086, Validation loss: 0.06854300, Gradient norm: 1.00081123
INFO:root:[  160] Training loss: 0.06924877, Validation loss: 0.06955593, Gradient norm: 1.22827380
INFO:root:[  161] Training loss: 0.06906097, Validation loss: 0.06908424, Gradient norm: 1.26697184
INFO:root:[  162] Training loss: 0.06851440, Validation loss: 0.06880657, Gradient norm: 0.93448446
INFO:root:[  163] Training loss: 0.06849614, Validation loss: 0.06802968, Gradient norm: 1.06618991
INFO:root:[  164] Training loss: 0.06836079, Validation loss: 0.06902353, Gradient norm: 1.06117033
INFO:root:[  165] Training loss: 0.06829799, Validation loss: 0.06775030, Gradient norm: 1.03165675
INFO:root:[  166] Training loss: 0.06815717, Validation loss: 0.06836778, Gradient norm: 1.10755337
INFO:root:[  167] Training loss: 0.06861223, Validation loss: 0.06846970, Gradient norm: 1.37101072
INFO:root:[  168] Training loss: 0.06797295, Validation loss: 0.06792434, Gradient norm: 1.17176316
INFO:root:[  169] Training loss: 0.06843173, Validation loss: 0.06803727, Gradient norm: 1.49496258
INFO:root:[  170] Training loss: 0.06747804, Validation loss: 0.06783914, Gradient norm: 0.84803533
INFO:root:[  171] Training loss: 0.06745097, Validation loss: 0.06847123, Gradient norm: 1.07135757
INFO:root:[  172] Training loss: 0.06741614, Validation loss: 0.06732327, Gradient norm: 1.21808149
INFO:root:[  173] Training loss: 0.06730761, Validation loss: 0.06652647, Gradient norm: 1.16981419
INFO:root:[  174] Training loss: 0.06710927, Validation loss: 0.06679898, Gradient norm: 1.08823759
INFO:root:[  175] Training loss: 0.06661032, Validation loss: 0.06698192, Gradient norm: 0.81414631
INFO:root:[  176] Training loss: 0.06651028, Validation loss: 0.06682681, Gradient norm: 0.89449636
INFO:root:[  177] Training loss: 0.06683317, Validation loss: 0.06775385, Gradient norm: 1.22517080
INFO:root:[  178] Training loss: 0.06663349, Validation loss: 0.06683631, Gradient norm: 1.16734598
INFO:root:[  179] Training loss: 0.06640727, Validation loss: 0.06622468, Gradient norm: 1.01875668
INFO:root:[  180] Training loss: 0.06632007, Validation loss: 0.06636505, Gradient norm: 1.00351842
INFO:root:[  181] Training loss: 0.06581171, Validation loss: 0.06578234, Gradient norm: 0.68465924
INFO:root:[  182] Training loss: 0.06572853, Validation loss: 0.06567686, Gradient norm: 0.92593840
INFO:root:[  183] Training loss: 0.06612798, Validation loss: 0.06541953, Gradient norm: 1.29564317
INFO:root:[  184] Training loss: 0.06622894, Validation loss: 0.06539066, Gradient norm: 1.39088810
INFO:root:[  185] Training loss: 0.06545949, Validation loss: 0.06535764, Gradient norm: 0.88973727
INFO:root:[  186] Training loss: 0.06564498, Validation loss: 0.06523162, Gradient norm: 1.09439491
INFO:root:[  187] Training loss: 0.06534834, Validation loss: 0.06500700, Gradient norm: 1.02964584
INFO:root:[  188] Training loss: 0.06554341, Validation loss: 0.06586250, Gradient norm: 1.24582642
INFO:root:[  189] Training loss: 0.06541108, Validation loss: 0.06492235, Gradient norm: 1.24441697
INFO:root:[  190] Training loss: 0.06481006, Validation loss: 0.06478510, Gradient norm: 0.88254661
INFO:root:[  191] Training loss: 0.06459262, Validation loss: 0.06549546, Gradient norm: 0.86123731
INFO:root:[  192] Training loss: 0.06469817, Validation loss: 0.06430483, Gradient norm: 0.98075961
INFO:root:[  193] Training loss: 0.06485800, Validation loss: 0.06423478, Gradient norm: 1.19099557
INFO:root:[  194] Training loss: 0.06463120, Validation loss: 0.06442133, Gradient norm: 1.08771606
INFO:root:[  195] Training loss: 0.06442364, Validation loss: 0.06454525, Gradient norm: 1.10410465
INFO:root:[  196] Training loss: 0.06413096, Validation loss: 0.06376982, Gradient norm: 0.97398450
INFO:root:[  197] Training loss: 0.06420742, Validation loss: 0.06343441, Gradient norm: 1.20524705
INFO:root:[  198] Training loss: 0.06415169, Validation loss: 0.06365852, Gradient norm: 1.17327781
INFO:root:[  199] Training loss: 0.06391571, Validation loss: 0.06378177, Gradient norm: 1.06898700
INFO:root:[  200] Training loss: 0.06377362, Validation loss: 0.06323745, Gradient norm: 1.05403749
INFO:root:[  201] Training loss: 0.06345435, Validation loss: 0.06356397, Gradient norm: 0.85493757
INFO:root:[  202] Training loss: 0.06337505, Validation loss: 0.06357786, Gradient norm: 0.81726424
INFO:root:[  203] Training loss: 0.06330158, Validation loss: 0.06286912, Gradient norm: 0.82516070
INFO:root:[  204] Training loss: 0.06335363, Validation loss: 0.06326376, Gradient norm: 1.06128015
INFO:root:[  205] Training loss: 0.06312825, Validation loss: 0.06313064, Gradient norm: 0.81089601
INFO:root:[  206] Training loss: 0.06314262, Validation loss: 0.06308879, Gradient norm: 0.95685437
INFO:root:[  207] Training loss: 0.06316534, Validation loss: 0.06350987, Gradient norm: 1.06633680
INFO:root:[  208] Training loss: 0.06293966, Validation loss: 0.06390314, Gradient norm: 1.09959823
INFO:root:[  209] Training loss: 0.06270754, Validation loss: 0.06229962, Gradient norm: 1.02254518
INFO:root:[  210] Training loss: 0.06263220, Validation loss: 0.06264451, Gradient norm: 1.00045847
INFO:root:[  211] Training loss: 0.06248106, Validation loss: 0.06368437, Gradient norm: 0.97258057
INFO:root:[  212] Training loss: 0.06276581, Validation loss: 0.06352047, Gradient norm: 1.28170444
INFO:root:[  213] Training loss: 0.06223370, Validation loss: 0.06216420, Gradient norm: 0.83819769
INFO:root:[  214] Training loss: 0.06212443, Validation loss: 0.06233485, Gradient norm: 0.83233773
INFO:root:[  215] Training loss: 0.06215095, Validation loss: 0.06203479, Gradient norm: 1.04530009
INFO:root:[  216] Training loss: 0.06198767, Validation loss: 0.06247386, Gradient norm: 0.96294875
INFO:root:[  217] Training loss: 0.06172630, Validation loss: 0.06142155, Gradient norm: 0.91516877
INFO:root:[  218] Training loss: 0.06146777, Validation loss: 0.06264875, Gradient norm: 0.69287884
INFO:root:[  219] Training loss: 0.06178387, Validation loss: 0.06207626, Gradient norm: 1.05256279
INFO:root:[  220] Training loss: 0.06141513, Validation loss: 0.06154181, Gradient norm: 0.79651412
INFO:root:[  221] Training loss: 0.06137845, Validation loss: 0.06094055, Gradient norm: 0.92054136
INFO:root:[  222] Training loss: 0.06125966, Validation loss: 0.06164223, Gradient norm: 0.82690473
INFO:root:[  223] Training loss: 0.06134239, Validation loss: 0.06210250, Gradient norm: 1.06177427
INFO:root:[  224] Training loss: 0.06102424, Validation loss: 0.06102510, Gradient norm: 1.00939839
INFO:root:[  225] Training loss: 0.06097769, Validation loss: 0.06065065, Gradient norm: 1.01678843
INFO:root:[  226] Training loss: 0.06108314, Validation loss: 0.06086766, Gradient norm: 1.15146293
INFO:root:[  227] Training loss: 0.06093740, Validation loss: 0.06051116, Gradient norm: 1.17158027
INFO:root:[  228] Training loss: 0.06066735, Validation loss: 0.06141128, Gradient norm: 0.95523055
INFO:root:[  229] Training loss: 0.06070526, Validation loss: 0.06107189, Gradient norm: 1.18369590
INFO:root:[  230] Training loss: 0.06047658, Validation loss: 0.06008550, Gradient norm: 0.86731035
INFO:root:[  231] Training loss: 0.06065263, Validation loss: 0.06003986, Gradient norm: 1.05371979
INFO:root:[  232] Training loss: 0.06051765, Validation loss: 0.05997209, Gradient norm: 1.24185459
INFO:root:[  233] Training loss: 0.06020548, Validation loss: 0.05975612, Gradient norm: 1.04874617
INFO:root:[  234] Training loss: 0.06021573, Validation loss: 0.05983995, Gradient norm: 1.00535868
INFO:root:[  235] Training loss: 0.06025577, Validation loss: 0.05978954, Gradient norm: 1.18498015
INFO:root:[  236] Training loss: 0.05987327, Validation loss: 0.06030548, Gradient norm: 0.98119754
INFO:root:[  237] Training loss: 0.05996313, Validation loss: 0.06045290, Gradient norm: 1.06235577
INFO:root:[  238] Training loss: 0.05976303, Validation loss: 0.06049731, Gradient norm: 1.01377058
INFO:root:[  239] Training loss: 0.05963932, Validation loss: 0.05976877, Gradient norm: 0.97804235
INFO:root:[  240] Training loss: 0.05956206, Validation loss: 0.05917449, Gradient norm: 1.02972726
INFO:root:[  241] Training loss: 0.05965815, Validation loss: 0.05927804, Gradient norm: 1.16784382
INFO:root:[  242] Training loss: 0.05953792, Validation loss: 0.05974864, Gradient norm: 1.11857400
INFO:root:[  243] Training loss: 0.05909142, Validation loss: 0.05885928, Gradient norm: 0.65386098
INFO:root:[  244] Training loss: 0.05892336, Validation loss: 0.05978481, Gradient norm: 0.71827646
INFO:root:[  245] Training loss: 0.05907186, Validation loss: 0.05868424, Gradient norm: 0.86139195
INFO:root:[  246] Training loss: 0.05934374, Validation loss: 0.05881118, Gradient norm: 1.17065955
INFO:root:[  247] Training loss: 0.05894912, Validation loss: 0.05861091, Gradient norm: 1.11420271
INFO:root:[  248] Training loss: 0.05871012, Validation loss: 0.05871556, Gradient norm: 0.79908231
INFO:root:[  249] Training loss: 0.05871072, Validation loss: 0.05885739, Gradient norm: 1.01570074
INFO:root:[  250] Training loss: 0.05865390, Validation loss: 0.05876273, Gradient norm: 1.01522499
INFO:root:[  251] Training loss: 0.05845207, Validation loss: 0.05824751, Gradient norm: 0.97843573
INFO:root:[  252] Training loss: 0.05811434, Validation loss: 0.05811360, Gradient norm: 0.33177290
INFO:root:[  253] Training loss: 0.05801716, Validation loss: 0.05786040, Gradient norm: 0.57761741
INFO:root:[  254] Training loss: 0.05802702, Validation loss: 0.05849138, Gradient norm: 0.66146324
INFO:root:[  255] Training loss: 0.05811198, Validation loss: 0.05791043, Gradient norm: 0.77836444
INFO:root:[  256] Training loss: 0.05792741, Validation loss: 0.05847837, Gradient norm: 0.94134486
INFO:root:[  257] Training loss: 0.05804605, Validation loss: 0.05815178, Gradient norm: 1.12192336
INFO:root:[  258] Training loss: 0.05780744, Validation loss: 0.05788932, Gradient norm: 0.92681534
INFO:root:[  259] Training loss: 0.05789192, Validation loss: 0.05831129, Gradient norm: 1.18996441
INFO:root:[  260] Training loss: 0.05777200, Validation loss: 0.05759140, Gradient norm: 1.11569507
INFO:root:[  261] Training loss: 0.05746381, Validation loss: 0.05747124, Gradient norm: 0.92907651
INFO:root:[  262] Training loss: 0.05750524, Validation loss: 0.05835545, Gradient norm: 1.02381294
INFO:root:[  263] Training loss: 0.05744557, Validation loss: 0.05815672, Gradient norm: 1.07298839
INFO:root:[  264] Training loss: 0.05728549, Validation loss: 0.05730506, Gradient norm: 1.04103360
INFO:root:[  265] Training loss: 0.05713110, Validation loss: 0.05695699, Gradient norm: 0.88154894
INFO:root:[  266] Training loss: 0.05717826, Validation loss: 0.05723270, Gradient norm: 1.02297317
INFO:root:[  267] Training loss: 0.05722108, Validation loss: 0.05800467, Gradient norm: 1.10651638
INFO:root:[  268] Training loss: 0.05723443, Validation loss: 0.05707352, Gradient norm: 1.20228011
INFO:root:[  269] Training loss: 0.05681153, Validation loss: 0.05736141, Gradient norm: 1.03021126
INFO:root:[  270] Training loss: 0.05680086, Validation loss: 0.05710379, Gradient norm: 1.08107682
INFO:root:[  271] Training loss: 0.05671833, Validation loss: 0.05664129, Gradient norm: 0.99262888
INFO:root:[  272] Training loss: 0.05678371, Validation loss: 0.05665805, Gradient norm: 1.00292491
INFO:root:[  273] Training loss: 0.05660613, Validation loss: 0.05621674, Gradient norm: 1.05261706
INFO:root:[  274] Training loss: 0.05636455, Validation loss: 0.05644537, Gradient norm: 0.81758628
INFO:root:[  275] Training loss: 0.05602062, Validation loss: 0.05588644, Gradient norm: 0.38362085
INFO:root:[  276] Training loss: 0.05612674, Validation loss: 0.05665807, Gradient norm: 0.68911471
INFO:root:[  277] Training loss: 0.05621939, Validation loss: 0.05644412, Gradient norm: 1.01732836
INFO:root:[  278] Training loss: 0.05598641, Validation loss: 0.05583970, Gradient norm: 0.86993290
INFO:root:[  279] Training loss: 0.05589930, Validation loss: 0.05618084, Gradient norm: 0.96043690
INFO:root:[  280] Training loss: 0.05577240, Validation loss: 0.05567972, Gradient norm: 0.91417797
INFO:root:[  281] Training loss: 0.05596798, Validation loss: 0.05651356, Gradient norm: 0.75687034
INFO:root:[  282] Training loss: 0.05589152, Validation loss: 0.05640987, Gradient norm: 1.16420935
INFO:root:[  283] Training loss: 0.05559712, Validation loss: 0.05526784, Gradient norm: 1.00708778
INFO:root:[  284] Training loss: 0.05553404, Validation loss: 0.05590797, Gradient norm: 0.73856768
INFO:root:[  285] Training loss: 0.05566643, Validation loss: 0.05590439, Gradient norm: 1.14738647
INFO:root:[  286] Training loss: 0.05536591, Validation loss: 0.05509392, Gradient norm: 0.96924675
INFO:root:[  287] Training loss: 0.05531107, Validation loss: 0.05559558, Gradient norm: 0.93564106
INFO:root:[  288] Training loss: 0.05536283, Validation loss: 0.05604471, Gradient norm: 1.24891658
INFO:root:[  289] Training loss: 0.05519595, Validation loss: 0.05547534, Gradient norm: 1.11465951
INFO:root:[  290] Training loss: 0.05496765, Validation loss: 0.05549136, Gradient norm: 0.95387620
INFO:root:[  291] Training loss: 0.05489146, Validation loss: 0.05478135, Gradient norm: 0.93021255
INFO:root:[  292] Training loss: 0.05491419, Validation loss: 0.05593599, Gradient norm: 0.73062393
INFO:root:[  293] Training loss: 0.05503171, Validation loss: 0.05474290, Gradient norm: 1.15434313
INFO:root:[  294] Training loss: 0.05485218, Validation loss: 0.05477003, Gradient norm: 1.12396692
INFO:root:[  295] Training loss: 0.05478427, Validation loss: 0.05486620, Gradient norm: 0.85886657
INFO:root:[  296] Training loss: 0.05478950, Validation loss: 0.05448918, Gradient norm: 1.19820812
INFO:root:[  297] Training loss: 0.05453054, Validation loss: 0.05429375, Gradient norm: 1.17033497
INFO:root:[  298] Training loss: 0.05452396, Validation loss: 0.05495706, Gradient norm: 0.75913109
INFO:root:[  299] Training loss: 0.05472908, Validation loss: 0.05466919, Gradient norm: 1.33894166
INFO:root:[  300] Training loss: 0.05424779, Validation loss: 0.05442837, Gradient norm: 1.06144326
INFO:root:[  301] Training loss: 0.05415608, Validation loss: 0.05447458, Gradient norm: 0.71770346
INFO:root:[  302] Training loss: 0.05431153, Validation loss: 0.05401302, Gradient norm: 1.21693799
INFO:root:[  303] Training loss: 0.05387599, Validation loss: 0.05404757, Gradient norm: 0.80621791
INFO:root:[  304] Training loss: 0.05407809, Validation loss: 0.05408588, Gradient norm: 0.91494216
INFO:root:[  305] Training loss: 0.05400982, Validation loss: 0.05355602, Gradient norm: 1.20005699
INFO:root:[  306] Training loss: 0.05368311, Validation loss: 0.05370138, Gradient norm: 0.89242047
INFO:root:[  307] Training loss: 0.05353059, Validation loss: 0.05345195, Gradient norm: 0.83666857
INFO:root:[  308] Training loss: 0.05362642, Validation loss: 0.05348984, Gradient norm: 1.06995852
INFO:root:[  309] Training loss: 0.05360530, Validation loss: 0.05334261, Gradient norm: 1.14332213
INFO:root:[  310] Training loss: 0.05339155, Validation loss: 0.05388146, Gradient norm: 0.94245445
INFO:root:[  311] Training loss: 0.05334019, Validation loss: 0.05376799, Gradient norm: 0.97136973
INFO:root:[  312] Training loss: 0.05357716, Validation loss: 0.05395264, Gradient norm: 1.21054478
INFO:root:[  313] Training loss: 0.05367711, Validation loss: 0.05350100, Gradient norm: 1.38606893
INFO:root:[  314] Training loss: 0.05325904, Validation loss: 0.05382531, Gradient norm: 1.20878454
INFO:root:[  315] Training loss: 0.05309905, Validation loss: 0.05326178, Gradient norm: 1.12317912
INFO:root:[  316] Training loss: 0.05287914, Validation loss: 0.05291229, Gradient norm: 0.93224705
INFO:root:[  317] Training loss: 0.05270956, Validation loss: 0.05292549, Gradient norm: 0.67918473
INFO:root:[  318] Training loss: 0.05265260, Validation loss: 0.05267540, Gradient norm: 0.39784588
INFO:root:[  319] Training loss: 0.05258046, Validation loss: 0.05286173, Gradient norm: 0.70101647
INFO:root:[  320] Training loss: 0.05254960, Validation loss: 0.05318551, Gradient norm: 0.91867207
INFO:root:[  321] Training loss: 0.05266006, Validation loss: 0.05244690, Gradient norm: 1.06981117
INFO:root:[  322] Training loss: 0.05255462, Validation loss: 0.05258620, Gradient norm: 1.02455133
INFO:root:[  323] Training loss: 0.05278590, Validation loss: 0.05314126, Gradient norm: 1.15303755
INFO:root:[  324] Training loss: 0.05295347, Validation loss: 0.05314219, Gradient norm: 1.32471389
INFO:root:[  325] Training loss: 0.05257044, Validation loss: 0.05188463, Gradient norm: 1.43475627
INFO:root:[  326] Training loss: 0.05217386, Validation loss: 0.05219967, Gradient norm: 0.93095380
INFO:root:[  327] Training loss: 0.05214379, Validation loss: 0.05220413, Gradient norm: 0.64524240
INFO:root:[  328] Training loss: 0.05205202, Validation loss: 0.05210505, Gradient norm: 0.94698477
INFO:root:[  329] Training loss: 0.05196944, Validation loss: 0.05181951, Gradient norm: 0.70154063
INFO:root:[  330] Training loss: 0.05188827, Validation loss: 0.05192805, Gradient norm: 0.96705284
INFO:root:[  331] Training loss: 0.05171754, Validation loss: 0.05269581, Gradient norm: 0.77053130
INFO:root:[  332] Training loss: 0.05214603, Validation loss: 0.05241839, Gradient norm: 1.22781706
INFO:root:[  333] Training loss: 0.05178978, Validation loss: 0.05177190, Gradient norm: 1.24771158
INFO:root:[  334] Training loss: 0.05156280, Validation loss: 0.05188971, Gradient norm: 0.85978047
INFO:root:[  335] Training loss: 0.05161967, Validation loss: 0.05208185, Gradient norm: 1.11804474
INFO:root:[  336] Training loss: 0.05147795, Validation loss: 0.05140252, Gradient norm: 1.00261124
INFO:root:[  337] Training loss: 0.05149168, Validation loss: 0.05161737, Gradient norm: 0.94333599
INFO:root:[  338] Training loss: 0.05125531, Validation loss: 0.05147090, Gradient norm: 1.01856655
INFO:root:[  339] Training loss: 0.05122482, Validation loss: 0.05113140, Gradient norm: 1.19655673
INFO:root:[  340] Training loss: 0.05116984, Validation loss: 0.05099332, Gradient norm: 1.11406063
INFO:root:[  341] Training loss: 0.05100678, Validation loss: 0.05165408, Gradient norm: 0.78725642
INFO:root:[  342] Training loss: 0.05122918, Validation loss: 0.05079674, Gradient norm: 1.22135362
INFO:root:[  343] Training loss: 0.05083742, Validation loss: 0.05077147, Gradient norm: 0.96785573
INFO:root:[  344] Training loss: 0.05074434, Validation loss: 0.05066882, Gradient norm: 0.80666428
INFO:root:[  345] Training loss: 0.05075034, Validation loss: 0.05111533, Gradient norm: 0.89896940
INFO:root:[  346] Training loss: 0.05089664, Validation loss: 0.05087289, Gradient norm: 1.21421253
INFO:root:[  347] Training loss: 0.05076333, Validation loss: 0.05052581, Gradient norm: 1.05726232
INFO:root:[  348] Training loss: 0.05058556, Validation loss: 0.05082421, Gradient norm: 0.85120060
INFO:root:[  349] Training loss: 0.05084005, Validation loss: 0.05054442, Gradient norm: 0.64654583
INFO:root:[  350] Training loss: 0.05082722, Validation loss: 0.05103415, Gradient norm: 1.08530298
INFO:root:[  351] Training loss: 0.05033525, Validation loss: 0.05046203, Gradient norm: 1.08970931
INFO:root:[  352] Training loss: 0.05016463, Validation loss: 0.05010458, Gradient norm: 1.01874534
INFO:root:[  353] Training loss: 0.05009405, Validation loss: 0.05053492, Gradient norm: 1.08492883
INFO:root:[  354] Training loss: 0.05011286, Validation loss: 0.05039529, Gradient norm: 1.19600028
INFO:root:[  355] Training loss: 0.05005406, Validation loss: 0.04993359, Gradient norm: 1.11338061
INFO:root:[  356] Training loss: 0.05004065, Validation loss: 0.05000036, Gradient norm: 1.08776833
INFO:root:[  357] Training loss: 0.04994513, Validation loss: 0.05002538, Gradient norm: 1.08097984
INFO:root:[  358] Training loss: 0.04965492, Validation loss: 0.04954245, Gradient norm: 0.90113563
INFO:root:[  359] Training loss: 0.04968610, Validation loss: 0.04959441, Gradient norm: 0.76032562
INFO:root:[  360] Training loss: 0.04984785, Validation loss: 0.04984010, Gradient norm: 0.83464184
INFO:root:[  361] Training loss: 0.04962007, Validation loss: 0.04942852, Gradient norm: 1.17556081
INFO:root:[  362] Training loss: 0.04945307, Validation loss: 0.04978355, Gradient norm: 1.02574490
INFO:root:[  363] Training loss: 0.04930960, Validation loss: 0.04946574, Gradient norm: 0.90394877
INFO:root:[  364] Training loss: 0.04941459, Validation loss: 0.04976640, Gradient norm: 1.18482481
INFO:root:[  365] Training loss: 0.04927960, Validation loss: 0.04917258, Gradient norm: 1.16781928
INFO:root:[  366] Training loss: 0.04930547, Validation loss: 0.04978647, Gradient norm: 1.26874127
INFO:root:[  367] Training loss: 0.04916116, Validation loss: 0.04890276, Gradient norm: 1.03889659
INFO:root:[  368] Training loss: 0.04902962, Validation loss: 0.04899713, Gradient norm: 0.92217677
INFO:root:[  369] Training loss: 0.04904312, Validation loss: 0.04958670, Gradient norm: 1.16532510
INFO:root:[  370] Training loss: 0.04901685, Validation loss: 0.04911989, Gradient norm: 1.29555147
INFO:root:[  371] Training loss: 0.04871481, Validation loss: 0.04886719, Gradient norm: 0.86687200
INFO:root:[  372] Training loss: 0.04864295, Validation loss: 0.04901480, Gradient norm: 0.80713126
INFO:root:[  373] Training loss: 0.04883401, Validation loss: 0.04875037, Gradient norm: 1.18536944
INFO:root:[  374] Training loss: 0.04929106, Validation loss: 0.04943855, Gradient norm: 1.32193199
INFO:root:[  375] Training loss: 0.04872709, Validation loss: 0.04868366, Gradient norm: 1.25235335
INFO:root:[  376] Training loss: 0.04865965, Validation loss: 0.04839936, Gradient norm: 1.06960563
INFO:root:[  377] Training loss: 0.04873665, Validation loss: 0.04887687, Gradient norm: 1.11557269
INFO:root:[  378] Training loss: 0.04839130, Validation loss: 0.04840791, Gradient norm: 1.05667333
INFO:root:[  379] Training loss: 0.04842076, Validation loss: 0.04877570, Gradient norm: 0.99105483
INFO:root:[  380] Training loss: 0.04836440, Validation loss: 0.04842121, Gradient norm: 1.23894214
INFO:root:[  381] Training loss: 0.04805011, Validation loss: 0.04833662, Gradient norm: 0.91926386
INFO:root:[  382] Training loss: 0.04813637, Validation loss: 0.04783105, Gradient norm: 0.95291624
INFO:root:[  383] Training loss: 0.04790531, Validation loss: 0.04778653, Gradient norm: 0.84179903
INFO:root:[  384] Training loss: 0.04790451, Validation loss: 0.04836716, Gradient norm: 0.98426832
INFO:root:[  385] Training loss: 0.04779099, Validation loss: 0.04750629, Gradient norm: 1.08274984
INFO:root:[  386] Training loss: 0.04765986, Validation loss: 0.04810031, Gradient norm: 0.97001462
INFO:root:[  387] Training loss: 0.04766831, Validation loss: 0.04755520, Gradient norm: 1.14264327
INFO:root:[  388] Training loss: 0.04762036, Validation loss: 0.04782583, Gradient norm: 1.25481099
INFO:root:[  389] Training loss: 0.04753920, Validation loss: 0.04731624, Gradient norm: 1.24609337
INFO:root:[  390] Training loss: 0.04742530, Validation loss: 0.04778578, Gradient norm: 0.96809826
INFO:root:[  391] Training loss: 0.04767790, Validation loss: 0.04784124, Gradient norm: 1.33343831
INFO:root:[  392] Training loss: 0.04753827, Validation loss: 0.04751957, Gradient norm: 1.32850269
INFO:root:[  393] Training loss: 0.04718574, Validation loss: 0.04707128, Gradient norm: 1.16281102
INFO:root:[  394] Training loss: 0.04712632, Validation loss: 0.04722498, Gradient norm: 0.92039906
INFO:root:[  395] Training loss: 0.04736228, Validation loss: 0.04748146, Gradient norm: 1.27111412
INFO:root:[  396] Training loss: 0.04711015, Validation loss: 0.04711532, Gradient norm: 1.26549930
INFO:root:[  397] Training loss: 0.04707683, Validation loss: 0.04714635, Gradient norm: 0.95853958
INFO:root:[  398] Training loss: 0.04727412, Validation loss: 0.04747130, Gradient norm: 1.07405129
INFO:root:[  399] Training loss: 0.04699412, Validation loss: 0.04695010, Gradient norm: 1.36726018
INFO:root:[  400] Training loss: 0.04678322, Validation loss: 0.04663318, Gradient norm: 1.20638665
INFO:root:[  401] Training loss: 0.04702512, Validation loss: 0.04692116, Gradient norm: 1.27924002
INFO:root:[  402] Training loss: 0.04668888, Validation loss: 0.04656809, Gradient norm: 1.15047686
INFO:root:[  403] Training loss: 0.04662479, Validation loss: 0.04709853, Gradient norm: 1.26471073
INFO:root:[  404] Training loss: 0.04655231, Validation loss: 0.04647561, Gradient norm: 1.35890987
INFO:root:[  405] Training loss: 0.04638215, Validation loss: 0.04655152, Gradient norm: 1.05516059
INFO:root:[  406] Training loss: 0.04643726, Validation loss: 0.04650134, Gradient norm: 1.12878346
INFO:root:[  407] Training loss: 0.04633708, Validation loss: 0.04612037, Gradient norm: 1.24357856
INFO:root:[  408] Training loss: 0.04609930, Validation loss: 0.04634184, Gradient norm: 0.98346977
INFO:root:[  409] Training loss: 0.04606853, Validation loss: 0.04607930, Gradient norm: 1.06898384
INFO:root:[  410] Training loss: 0.04602363, Validation loss: 0.04588446, Gradient norm: 1.09722493
INFO:root:[  411] Training loss: 0.04592102, Validation loss: 0.04603834, Gradient norm: 1.14170150
INFO:root:[  412] Training loss: 0.04595192, Validation loss: 0.04627575, Gradient norm: 1.01691402
INFO:root:[  413] Training loss: 0.04609241, Validation loss: 0.04605647, Gradient norm: 1.36101231
INFO:root:[  414] Training loss: 0.04599317, Validation loss: 0.04638330, Gradient norm: 1.38303450
INFO:root:[  415] Training loss: 0.04582487, Validation loss: 0.04571437, Gradient norm: 1.46206425
INFO:root:[  416] Training loss: 0.04555863, Validation loss: 0.04565640, Gradient norm: 0.99089766
INFO:root:[  417] Training loss: 0.04563396, Validation loss: 0.04604893, Gradient norm: 0.95203819
INFO:root:[  418] Training loss: 0.04549554, Validation loss: 0.04562950, Gradient norm: 1.02366936
INFO:root:[  419] Training loss: 0.04547717, Validation loss: 0.04537947, Gradient norm: 1.11877244
INFO:root:[  420] Training loss: 0.04536929, Validation loss: 0.04541965, Gradient norm: 0.95829604
INFO:root:[  421] Training loss: 0.04557445, Validation loss: 0.04596526, Gradient norm: 0.95043169
INFO:root:[  422] Training loss: 0.04557158, Validation loss: 0.04618877, Gradient norm: 1.00180930
INFO:root:[  423] Training loss: 0.04558046, Validation loss: 0.04535896, Gradient norm: 1.13523417
INFO:root:[  424] Training loss: 0.04545972, Validation loss: 0.04548840, Gradient norm: 1.34017510
INFO:root:[  425] Training loss: 0.04520715, Validation loss: 0.04503525, Gradient norm: 1.04036303
INFO:root:[  426] Training loss: 0.04499841, Validation loss: 0.04526218, Gradient norm: 1.16472268
INFO:root:[  427] Training loss: 0.04491252, Validation loss: 0.04495301, Gradient norm: 1.16172449
INFO:root:[  428] Training loss: 0.04478700, Validation loss: 0.04507537, Gradient norm: 1.13645137
INFO:root:[  429] Training loss: 0.04466113, Validation loss: 0.04460230, Gradient norm: 1.13357113
INFO:root:[  430] Training loss: 0.04461444, Validation loss: 0.04456826, Gradient norm: 1.17898149
INFO:root:[  431] Training loss: 0.04460737, Validation loss: 0.04455440, Gradient norm: 1.20302890
INFO:root:[  432] Training loss: 0.04460013, Validation loss: 0.04480470, Gradient norm: 1.26123146
INFO:root:[  433] Training loss: 0.04438097, Validation loss: 0.04461212, Gradient norm: 1.06978188
INFO:root:[  434] Training loss: 0.04448552, Validation loss: 0.04479583, Gradient norm: 1.32230400
INFO:root:[  435] Training loss: 0.04434427, Validation loss: 0.04427442, Gradient norm: 1.21924054
INFO:root:[  436] Training loss: 0.04438881, Validation loss: 0.04454532, Gradient norm: 1.21141607
INFO:root:[  437] Training loss: 0.04436828, Validation loss: 0.04417940, Gradient norm: 1.47392121
INFO:root:[  438] Training loss: 0.04431546, Validation loss: 0.04425557, Gradient norm: 1.35445880
INFO:root:[  439] Training loss: 0.04398156, Validation loss: 0.04411084, Gradient norm: 0.97962836
INFO:root:[  440] Training loss: 0.04405159, Validation loss: 0.04440879, Gradient norm: 1.21937311
INFO:root:[  441] Training loss: 0.04403474, Validation loss: 0.04398970, Gradient norm: 1.55290323
INFO:root:[  442] Training loss: 0.04387697, Validation loss: 0.04373263, Gradient norm: 1.25512523
INFO:root:[  443] Training loss: 0.04392080, Validation loss: 0.04440048, Gradient norm: 1.49166825
INFO:root:[  444] Training loss: 0.04388664, Validation loss: 0.04376979, Gradient norm: 1.48802126
INFO:root:[  445] Training loss: 0.04373147, Validation loss: 0.04368403, Gradient norm: 1.41640680
INFO:root:[  446] Training loss: 0.04361424, Validation loss: 0.04371777, Gradient norm: 1.27456874
INFO:root:[  447] Training loss: 0.04357771, Validation loss: 0.04357228, Gradient norm: 1.33153001
INFO:root:[  448] Training loss: 0.04345415, Validation loss: 0.04330096, Gradient norm: 1.32172076
INFO:root:[  449] Training loss: 0.04329405, Validation loss: 0.04321015, Gradient norm: 1.08448079
INFO:root:[  450] Training loss: 0.04328729, Validation loss: 0.04329957, Gradient norm: 1.26329303
INFO:root:[  451] Training loss: 0.04333350, Validation loss: 0.04354060, Gradient norm: 1.13733456
INFO:root:[  452] Training loss: 0.04333128, Validation loss: 0.04333538, Gradient norm: 1.25174812
INFO:root:[  453] Training loss: 0.04305977, Validation loss: 0.04329877, Gradient norm: 1.38668082
INFO:root:[  454] Training loss: 0.04293960, Validation loss: 0.04318696, Gradient norm: 1.13569623
INFO:root:[  455] Training loss: 0.04303444, Validation loss: 0.04283763, Gradient norm: 1.61746927
INFO:root:[  456] Training loss: 0.04285663, Validation loss: 0.04275488, Gradient norm: 1.50066649
INFO:root:[  457] Training loss: 0.04288061, Validation loss: 0.04349599, Gradient norm: 1.51983199
INFO:root:[  458] Training loss: 0.04304627, Validation loss: 0.04313891, Gradient norm: 1.60123626
INFO:root:[  459] Training loss: 0.04353860, Validation loss: 0.04278175, Gradient norm: 1.19272660
INFO:root:[  460] Training loss: 0.04351188, Validation loss: 0.04413642, Gradient norm: 1.28416108
INFO:root:[  461] Training loss: 0.04317490, Validation loss: 0.04275410, Gradient norm: 1.06044693
INFO:root:[  462] Training loss: 0.04256729, Validation loss: 0.04228553, Gradient norm: 1.05217565
INFO:root:[  463] Training loss: 0.04236407, Validation loss: 0.04238308, Gradient norm: 1.08302970
INFO:root:[  464] Training loss: 0.04232542, Validation loss: 0.04235341, Gradient norm: 1.13330401
INFO:root:[  465] Training loss: 0.04217419, Validation loss: 0.04228292, Gradient norm: 1.09611006
INFO:root:[  466] Training loss: 0.04215412, Validation loss: 0.04218456, Gradient norm: 1.15673691
INFO:root:[  467] Training loss: 0.04207996, Validation loss: 0.04233738, Gradient norm: 1.08649601
INFO:root:[  468] Training loss: 0.04204141, Validation loss: 0.04231266, Gradient norm: 1.21393417
INFO:root:[  469] Training loss: 0.04196005, Validation loss: 0.04214419, Gradient norm: 1.20747526
INFO:root:[  470] Training loss: 0.04192598, Validation loss: 0.04207047, Gradient norm: 1.44191759
INFO:root:[  471] Training loss: 0.04201626, Validation loss: 0.04175854, Gradient norm: 1.84650467
INFO:root:[  472] Training loss: 0.04194544, Validation loss: 0.04224473, Gradient norm: 1.81582557
INFO:root:[  473] Training loss: 0.04183200, Validation loss: 0.04202458, Gradient norm: 1.68891263
INFO:root:[  474] Training loss: 0.04168298, Validation loss: 0.04157519, Gradient norm: 1.48227107
INFO:root:[  475] Training loss: 0.04166770, Validation loss: 0.04154912, Gradient norm: 1.31265943
INFO:root:[  476] Training loss: 0.04162315, Validation loss: 0.04167991, Gradient norm: 1.20792284
INFO:root:[  477] Training loss: 0.04152462, Validation loss: 0.04149153, Gradient norm: 1.08481670
INFO:root:[  478] Training loss: 0.04152733, Validation loss: 0.04162644, Gradient norm: 1.50957254
INFO:root:[  479] Training loss: 0.04139764, Validation loss: 0.04159246, Gradient norm: 1.77557298
INFO:root:[  480] Training loss: 0.04139196, Validation loss: 0.04137836, Gradient norm: 1.75145798
INFO:root:[  481] Training loss: 0.04138645, Validation loss: 0.04161476, Gradient norm: 1.74640834
INFO:root:[  482] Training loss: 0.04158875, Validation loss: 0.04130492, Gradient norm: 1.42873978
INFO:root:[  483] Training loss: 0.04113203, Validation loss: 0.04114357, Gradient norm: 1.29165861
INFO:root:[  484] Training loss: 0.04098544, Validation loss: 0.04112608, Gradient norm: 1.25823022
INFO:root:[  485] Training loss: 0.04088747, Validation loss: 0.04126776, Gradient norm: 1.18676856
INFO:root:[  486] Training loss: 0.04098662, Validation loss: 0.04125533, Gradient norm: 1.33922744
INFO:root:[  487] Training loss: 0.04115164, Validation loss: 0.04107757, Gradient norm: 1.85627631
INFO:root:[  488] Training loss: 0.04116456, Validation loss: 0.04099175, Gradient norm: 1.91752907
INFO:root:[  489] Training loss: 0.04106725, Validation loss: 0.04084618, Gradient norm: 1.55537228
INFO:root:[  490] Training loss: 0.04091471, Validation loss: 0.04048258, Gradient norm: 1.19912089
INFO:root:[  491] Training loss: 0.04076145, Validation loss: 0.04092474, Gradient norm: 1.47634754
INFO:root:[  492] Training loss: 0.04053138, Validation loss: 0.04067837, Gradient norm: 1.37908117
INFO:root:[  493] Training loss: 0.04042379, Validation loss: 0.04081492, Gradient norm: 1.25444965
INFO:root:[  494] Training loss: 0.04037132, Validation loss: 0.04060515, Gradient norm: 1.41722977
INFO:root:[  495] Training loss: 0.04031413, Validation loss: 0.04060310, Gradient norm: 1.42997011
INFO:root:[  496] Training loss: 0.04027938, Validation loss: 0.04041481, Gradient norm: 1.33141638
INFO:root:[  497] Training loss: 0.04031991, Validation loss: 0.04102793, Gradient norm: 1.56284915
INFO:root:[  498] Training loss: 0.04037487, Validation loss: 0.04025933, Gradient norm: 1.65081735
INFO:root:[  499] Training loss: 0.04024715, Validation loss: 0.04026039, Gradient norm: 1.34013021
INFO:root:[  500] Training loss: 0.04027959, Validation loss: 0.04054791, Gradient norm: 1.94073238
INFO:root:[  501] Training loss: 0.04007033, Validation loss: 0.04005984, Gradient norm: 1.75240232
INFO:root:[  502] Training loss: 0.03989670, Validation loss: 0.04003976, Gradient norm: 1.35527790
INFO:root:[  503] Training loss: 0.03988897, Validation loss: 0.04010708, Gradient norm: 1.40308276
INFO:root:[  504] Training loss: 0.04011478, Validation loss: 0.04004864, Gradient norm: 1.95710590
INFO:root:[  505] Training loss: 0.03992311, Validation loss: 0.03971084, Gradient norm: 1.90247927
INFO:root:[  506] Training loss: 0.03980060, Validation loss: 0.03998511, Gradient norm: 1.63551549
INFO:root:[  507] Training loss: 0.04011916, Validation loss: 0.04018655, Gradient norm: 1.23237058
INFO:root:[  508] Training loss: 0.04023572, Validation loss: 0.03969505, Gradient norm: 1.39615636
INFO:root:[  509] Training loss: 0.03977957, Validation loss: 0.03986818, Gradient norm: 1.25491768
INFO:root:[  510] Training loss: 0.03969472, Validation loss: 0.04002527, Gradient norm: 1.14907004
INFO:root:[  511] Training loss: 0.03963867, Validation loss: 0.03964882, Gradient norm: 1.29082978
INFO:root:[  512] Training loss: 0.03933302, Validation loss: 0.03923440, Gradient norm: 1.31070272
INFO:root:[  513] Training loss: 0.03931012, Validation loss: 0.03916982, Gradient norm: 1.43328595
INFO:root:[  514] Training loss: 0.03948991, Validation loss: 0.03915021, Gradient norm: 1.68368097
INFO:root:[  515] Training loss: 0.03921208, Validation loss: 0.03925717, Gradient norm: 1.34382008
INFO:root:[  516] Training loss: 0.03921739, Validation loss: 0.03909590, Gradient norm: 1.47134250
INFO:root:[  517] Training loss: 0.03906300, Validation loss: 0.03916195, Gradient norm: 1.34635660
INFO:root:[  518] Training loss: 0.03898687, Validation loss: 0.03902471, Gradient norm: 1.43351890
INFO:root:[  519] Training loss: 0.03904810, Validation loss: 0.03906718, Gradient norm: 1.53384691
INFO:root:[  520] Training loss: 0.03897683, Validation loss: 0.03889600, Gradient norm: 1.52676129
INFO:root:[  521] Training loss: 0.03876846, Validation loss: 0.03888749, Gradient norm: 1.39259451
INFO:root:[  522] Training loss: 0.03877634, Validation loss: 0.03937730, Gradient norm: 1.47543922
INFO:root:[  523] Training loss: 0.03886409, Validation loss: 0.03903844, Gradient norm: 1.76579659
INFO:root:[  524] Training loss: 0.03879963, Validation loss: 0.03864889, Gradient norm: 1.43328481
INFO:root:[  525] Training loss: 0.03868517, Validation loss: 0.03882125, Gradient norm: 1.48562886
INFO:root:[  526] Training loss: 0.03868961, Validation loss: 0.03857728, Gradient norm: 1.62893171
INFO:root:[  527] Training loss: 0.03866514, Validation loss: 0.03846016, Gradient norm: 1.37631870
INFO:root:[  528] Training loss: 0.03870628, Validation loss: 0.03918011, Gradient norm: 1.40092724
INFO:root:[  529] Training loss: 0.03860035, Validation loss: 0.03850327, Gradient norm: 1.35646521
INFO:root:[  530] Training loss: 0.03863709, Validation loss: 0.03868722, Gradient norm: 1.48725426
INFO:root:[  531] Training loss: 0.03837573, Validation loss: 0.03844215, Gradient norm: 1.48062560
INFO:root:[  532] Training loss: 0.03823958, Validation loss: 0.03819644, Gradient norm: 1.55941613
INFO:root:[  533] Training loss: 0.03829951, Validation loss: 0.03827985, Gradient norm: 1.25291147
INFO:root:[  534] Training loss: 0.03820568, Validation loss: 0.03808011, Gradient norm: 1.54319267
INFO:root:[  535] Training loss: 0.03810459, Validation loss: 0.03819127, Gradient norm: 1.57353834
INFO:root:[  536] Training loss: 0.03799530, Validation loss: 0.03826982, Gradient norm: 1.43821600
INFO:root:[  537] Training loss: 0.03803877, Validation loss: 0.03823133, Gradient norm: 1.65725017
INFO:root:[  538] Training loss: 0.03789044, Validation loss: 0.03775580, Gradient norm: 1.55402799
INFO:root:[  539] Training loss: 0.03780572, Validation loss: 0.03799203, Gradient norm: 1.49203343
INFO:root:[  540] Training loss: 0.03775792, Validation loss: 0.03790170, Gradient norm: 1.36450209
INFO:root:[  541] Training loss: 0.03777368, Validation loss: 0.03760171, Gradient norm: 1.41844870
INFO:root:[  542] Training loss: 0.03771056, Validation loss: 0.03800486, Gradient norm: 1.45098032
INFO:root:[  543] Training loss: 0.03778813, Validation loss: 0.03786742, Gradient norm: 1.70662102
INFO:root:[  544] Training loss: 0.03777420, Validation loss: 0.03778998, Gradient norm: 1.55231783
INFO:root:[  545] Training loss: 0.03824567, Validation loss: 0.03891151, Gradient norm: 1.39541573
INFO:root:[  546] Training loss: 0.03784625, Validation loss: 0.03761066, Gradient norm: 1.84799569
INFO:root:[  547] Training loss: 0.03754390, Validation loss: 0.03745087, Gradient norm: 1.74649314
INFO:root:[  548] Training loss: 0.03750773, Validation loss: 0.03763370, Gradient norm: 1.72271307
INFO:root:[  549] Training loss: 0.03740948, Validation loss: 0.03755848, Gradient norm: 1.47408258
INFO:root:[  550] Training loss: 0.03741541, Validation loss: 0.03749459, Gradient norm: 1.52769194
INFO:root:[  551] Training loss: 0.03737237, Validation loss: 0.03726774, Gradient norm: 1.53072972
INFO:root:[  552] Training loss: 0.03720598, Validation loss: 0.03772098, Gradient norm: 1.58373724
INFO:root:[  553] Training loss: 0.03715523, Validation loss: 0.03692913, Gradient norm: 1.68422278
INFO:root:[  554] Training loss: 0.03716709, Validation loss: 0.03724627, Gradient norm: 1.63157833
INFO:root:[  555] Training loss: 0.03709690, Validation loss: 0.03706611, Gradient norm: 1.69054781
INFO:root:[  556] Training loss: 0.03700829, Validation loss: 0.03731498, Gradient norm: 1.71491401
INFO:root:[  557] Training loss: 0.03689838, Validation loss: 0.03687332, Gradient norm: 1.79838504
INFO:root:[  558] Training loss: 0.03695786, Validation loss: 0.03706305, Gradient norm: 1.68363908
INFO:root:[  559] Training loss: 0.03688481, Validation loss: 0.03681678, Gradient norm: 1.62132671
INFO:root:[  560] Training loss: 0.03686218, Validation loss: 0.03687523, Gradient norm: 1.54905821
INFO:root:[  561] Training loss: 0.03677555, Validation loss: 0.03690168, Gradient norm: 1.87976261
INFO:root:[  562] Training loss: 0.03660035, Validation loss: 0.03684548, Gradient norm: 1.46599882
INFO:root:[  563] Training loss: 0.03661605, Validation loss: 0.03676030, Gradient norm: 1.65742011
INFO:root:[  564] Training loss: 0.03662410, Validation loss: 0.03655440, Gradient norm: 1.73582471
INFO:root:[  565] Training loss: 0.03664027, Validation loss: 0.03654675, Gradient norm: 1.68608225
INFO:root:[  566] Training loss: 0.03657586, Validation loss: 0.03671927, Gradient norm: 1.66931582
INFO:root:[  567] Training loss: 0.03655955, Validation loss: 0.03678068, Gradient norm: 1.61159996
INFO:root:[  568] Training loss: 0.03635978, Validation loss: 0.03649312, Gradient norm: 1.71245736
INFO:root:[  569] Training loss: 0.03635789, Validation loss: 0.03647056, Gradient norm: 1.76732660
INFO:root:[  570] Training loss: 0.03627546, Validation loss: 0.03613115, Gradient norm: 1.39049628
INFO:root:[  571] Training loss: 0.03631411, Validation loss: 0.03610950, Gradient norm: 1.61449759
INFO:root:[  572] Training loss: 0.03613573, Validation loss: 0.03613095, Gradient norm: 1.54118254
INFO:root:[  573] Training loss: 0.03622947, Validation loss: 0.03654777, Gradient norm: 1.91187978
INFO:root:[  574] Training loss: 0.03606726, Validation loss: 0.03640311, Gradient norm: 1.80597977
INFO:root:[  575] Training loss: 0.03609924, Validation loss: 0.03603961, Gradient norm: 1.94495019
INFO:root:[  576] Training loss: 0.03592534, Validation loss: 0.03595135, Gradient norm: 1.47378221
INFO:root:[  577] Training loss: 0.03607536, Validation loss: 0.03615277, Gradient norm: 1.39055692
INFO:root:[  578] Training loss: 0.03615144, Validation loss: 0.03578424, Gradient norm: 1.67571766
INFO:root:[  579] Training loss: 0.03595432, Validation loss: 0.03632030, Gradient norm: 1.42932498
INFO:root:[  580] Training loss: 0.03596380, Validation loss: 0.03589251, Gradient norm: 1.86646548
INFO:root:[  581] Training loss: 0.03577945, Validation loss: 0.03587402, Gradient norm: 1.66175880
INFO:root:[  582] Training loss: 0.03568346, Validation loss: 0.03567643, Gradient norm: 1.61847035
INFO:root:[  583] Training loss: 0.03570677, Validation loss: 0.03590545, Gradient norm: 1.92870299
INFO:root:[  584] Training loss: 0.03567289, Validation loss: 0.03578728, Gradient norm: 1.80190723
INFO:root:[  585] Training loss: 0.03564033, Validation loss: 0.03584258, Gradient norm: 1.66891555
INFO:root:[  586] Training loss: 0.03571539, Validation loss: 0.03605304, Gradient norm: 2.23929302
INFO:root:[  587] Training loss: 0.03559798, Validation loss: 0.03588296, Gradient norm: 2.26215053
INFO:root:[  588] Training loss: 0.03551004, Validation loss: 0.03534164, Gradient norm: 2.28108892
INFO:root:[  589] Training loss: 0.03544495, Validation loss: 0.03574710, Gradient norm: 1.67162258
INFO:root:[  590] Training loss: 0.03573353, Validation loss: 0.03607744, Gradient norm: 1.98527042
INFO:root:[  591] Training loss: 0.03546813, Validation loss: 0.03530209, Gradient norm: 1.45501112
INFO:root:[  592] Training loss: 0.03543998, Validation loss: 0.03569863, Gradient norm: 1.72138712
INFO:root:[  593] Training loss: 0.03524151, Validation loss: 0.03536129, Gradient norm: 1.86782276
INFO:root:[  594] Training loss: 0.03518970, Validation loss: 0.03519606, Gradient norm: 2.08671527
INFO:root:[  595] Training loss: 0.03520242, Validation loss: 0.03518379, Gradient norm: 1.37403837
INFO:root:[  596] Training loss: 0.03518382, Validation loss: 0.03580639, Gradient norm: 1.73925781
INFO:root:[  597] Training loss: 0.03517185, Validation loss: 0.03503975, Gradient norm: 2.08558786
INFO:root:[  598] Training loss: 0.03502448, Validation loss: 0.03519997, Gradient norm: 1.83512225
INFO:root:[  599] Training loss: 0.03492461, Validation loss: 0.03517147, Gradient norm: 1.81349330
INFO:root:[  600] Training loss: 0.03492178, Validation loss: 0.03487788, Gradient norm: 1.89807344
INFO:root:[  601] Training loss: 0.03493142, Validation loss: 0.03494494, Gradient norm: 1.99864107
INFO:root:[  602] Training loss: 0.03494389, Validation loss: 0.03488401, Gradient norm: 2.37659734
INFO:root:[  603] Training loss: 0.03478097, Validation loss: 0.03471093, Gradient norm: 2.13198875
INFO:root:[  604] Training loss: 0.03479480, Validation loss: 0.03508007, Gradient norm: 2.22839060
INFO:root:[  605] Training loss: 0.03480403, Validation loss: 0.03510187, Gradient norm: 2.00311067
INFO:root:[  606] Training loss: 0.03473511, Validation loss: 0.03486921, Gradient norm: 2.29199383
INFO:root:[  607] Training loss: 0.03471098, Validation loss: 0.03469408, Gradient norm: 1.75617441
INFO:root:[  608] Training loss: 0.03482267, Validation loss: 0.03472390, Gradient norm: 2.12314171
INFO:root:[  609] Training loss: 0.03451780, Validation loss: 0.03475161, Gradient norm: 1.72612280
INFO:root:[  610] Training loss: 0.03458853, Validation loss: 0.03489469, Gradient norm: 1.72656648
INFO:root:[  611] Training loss: 0.03453156, Validation loss: 0.03465288, Gradient norm: 2.19915276
INFO:root:[  612] Training loss: 0.03453090, Validation loss: 0.03449040, Gradient norm: 2.01615510
INFO:root:[  613] Training loss: 0.03447471, Validation loss: 0.03446717, Gradient norm: 1.77874057
INFO:root:[  614] Training loss: 0.03448202, Validation loss: 0.03441785, Gradient norm: 2.21558431
INFO:root:[  615] Training loss: 0.03430150, Validation loss: 0.03412332, Gradient norm: 2.02759095
INFO:root:[  616] Training loss: 0.03424102, Validation loss: 0.03446760, Gradient norm: 1.81329975
INFO:root:[  617] Training loss: 0.03421440, Validation loss: 0.03451485, Gradient norm: 1.90423150
INFO:root:[  618] Training loss: 0.03417934, Validation loss: 0.03409501, Gradient norm: 1.90013178
INFO:root:[  619] Training loss: 0.03413175, Validation loss: 0.03432720, Gradient norm: 1.50815075
INFO:root:[  620] Training loss: 0.03407511, Validation loss: 0.03400584, Gradient norm: 1.80100005
INFO:root:[  621] Training loss: 0.03409317, Validation loss: 0.03413510, Gradient norm: 2.13026665
INFO:root:[  622] Training loss: 0.03408701, Validation loss: 0.03384647, Gradient norm: 2.57762568
INFO:root:[  623] Training loss: 0.03398154, Validation loss: 0.03416714, Gradient norm: 1.99603878
INFO:root:[  624] Training loss: 0.03408137, Validation loss: 0.03407885, Gradient norm: 2.23445126
INFO:root:[  625] Training loss: 0.03388329, Validation loss: 0.03424566, Gradient norm: 1.91095064
INFO:root:[  626] Training loss: 0.03383565, Validation loss: 0.03422339, Gradient norm: 2.04502411
INFO:root:[  627] Training loss: 0.03395652, Validation loss: 0.03376235, Gradient norm: 2.50544740
INFO:root:[  628] Training loss: 0.03392521, Validation loss: 0.03396384, Gradient norm: 2.21247823
INFO:root:[  629] Training loss: 0.03378802, Validation loss: 0.03393496, Gradient norm: 1.90220139
INFO:root:[  630] Training loss: 0.03378837, Validation loss: 0.03378494, Gradient norm: 2.09916847
INFO:root:[  631] Training loss: 0.03371180, Validation loss: 0.03403110, Gradient norm: 2.14677249
INFO:root:[  632] Training loss: 0.03363773, Validation loss: 0.03349787, Gradient norm: 2.21359781
INFO:root:[  633] Training loss: 0.03355606, Validation loss: 0.03366858, Gradient norm: 1.74620854
INFO:root:[  634] Training loss: 0.03349090, Validation loss: 0.03359477, Gradient norm: 1.92358516
INFO:root:[  635] Training loss: 0.03356432, Validation loss: 0.03384266, Gradient norm: 2.39839078
INFO:root:[  636] Training loss: 0.03348040, Validation loss: 0.03332682, Gradient norm: 2.54616164
INFO:root:[  637] Training loss: 0.03347526, Validation loss: 0.03361514, Gradient norm: 2.05942582
INFO:root:[  638] Training loss: 0.03334986, Validation loss: 0.03335443, Gradient norm: 2.12598203
INFO:root:[  639] Training loss: 0.03329503, Validation loss: 0.03333473, Gradient norm: 2.16492089
INFO:root:[  640] Training loss: 0.03325769, Validation loss: 0.03379428, Gradient norm: 2.00334057
INFO:root:[  641] Training loss: 0.03337939, Validation loss: 0.03317356, Gradient norm: 2.58919087
INFO:root:[  642] Training loss: 0.03338925, Validation loss: 0.03330653, Gradient norm: 2.32359458
INFO:root:[  643] Training loss: 0.03349413, Validation loss: 0.03377430, Gradient norm: 1.88899999
INFO:root:[  644] Training loss: 0.03357511, Validation loss: 0.03458979, Gradient norm: 1.74451197
INFO:root:[  645] Training loss: 0.03376688, Validation loss: 0.03312502, Gradient norm: 2.13726404
INFO:root:[  646] Training loss: 0.03330317, Validation loss: 0.03304209, Gradient norm: 1.95400213
INFO:root:[  647] Training loss: 0.03304235, Validation loss: 0.03331973, Gradient norm: 2.00871714
INFO:root:[  648] Training loss: 0.03305047, Validation loss: 0.03302507, Gradient norm: 2.02168843
INFO:root:[  649] Training loss: 0.03306637, Validation loss: 0.03325576, Gradient norm: 2.04002283
INFO:root:[  650] Training loss: 0.03303813, Validation loss: 0.03297023, Gradient norm: 2.53143300
INFO:root:[  651] Training loss: 0.03291731, Validation loss: 0.03281133, Gradient norm: 2.22890418
INFO:root:[  652] Training loss: 0.03286927, Validation loss: 0.03272019, Gradient norm: 1.85543031
INFO:root:[  653] Training loss: 0.03283211, Validation loss: 0.03285376, Gradient norm: 2.37528572
INFO:root:[  654] Training loss: 0.03292779, Validation loss: 0.03323748, Gradient norm: 2.76788868
INFO:root:[  655] Training loss: 0.03289405, Validation loss: 0.03305454, Gradient norm: 2.56985237
INFO:root:[  656] Training loss: 0.03271035, Validation loss: 0.03266677, Gradient norm: 2.52089346
INFO:root:[  657] Training loss: 0.03265243, Validation loss: 0.03256667, Gradient norm: 1.88104885
INFO:root:[  658] Training loss: 0.03263672, Validation loss: 0.03308245, Gradient norm: 1.51794708
INFO:root:[  659] Training loss: 0.03280559, Validation loss: 0.03307378, Gradient norm: 2.47086863
INFO:root:[  660] Training loss: 0.03272823, Validation loss: 0.03283606, Gradient norm: 2.75070755
INFO:root:[  661] Training loss: 0.03262533, Validation loss: 0.03263270, Gradient norm: 2.07592043
INFO:root:[  662] Training loss: 0.03248554, Validation loss: 0.03253041, Gradient norm: 1.69236511
INFO:root:[  663] Training loss: 0.03241638, Validation loss: 0.03246751, Gradient norm: 1.93722019
INFO:root:[  664] Training loss: 0.03249838, Validation loss: 0.03277697, Gradient norm: 2.42796113
INFO:root:[  665] Training loss: 0.03246673, Validation loss: 0.03281635, Gradient norm: 2.12952460
INFO:root:[  666] Training loss: 0.03253246, Validation loss: 0.03285128, Gradient norm: 2.74877638
INFO:root:[  667] Training loss: 0.03237446, Validation loss: 0.03249100, Gradient norm: 2.43251810
INFO:root:[  668] Training loss: 0.03232867, Validation loss: 0.03226765, Gradient norm: 2.35788316
INFO:root:[  669] Training loss: 0.03237476, Validation loss: 0.03261042, Gradient norm: 2.45411452
INFO:root:[  670] Training loss: 0.03217632, Validation loss: 0.03260832, Gradient norm: 1.97749977
INFO:root:[  671] Training loss: 0.03235115, Validation loss: 0.03281696, Gradient norm: 2.88138276
INFO:root:[  672] Training loss: 0.03238545, Validation loss: 0.03232487, Gradient norm: 2.46170669
INFO:root:[  673] Training loss: 0.03218208, Validation loss: 0.03222444, Gradient norm: 2.25793829
INFO:root:[  674] Training loss: 0.03207423, Validation loss: 0.03222680, Gradient norm: 2.35337045
INFO:root:[  675] Training loss: 0.03213757, Validation loss: 0.03192879, Gradient norm: 2.67164608
INFO:root:[  676] Training loss: 0.03207101, Validation loss: 0.03257816, Gradient norm: 2.68559531
INFO:root:[  677] Training loss: 0.03208037, Validation loss: 0.03216946, Gradient norm: 3.12318798
INFO:root:[  678] Training loss: 0.03194250, Validation loss: 0.03221196, Gradient norm: 2.52445009
INFO:root:[  679] Training loss: 0.03200586, Validation loss: 0.03209132, Gradient norm: 2.50357449
INFO:root:[  680] Training loss: 0.03189530, Validation loss: 0.03201649, Gradient norm: 2.18842480
INFO:root:[  681] Training loss: 0.03191926, Validation loss: 0.03194202, Gradient norm: 2.55081848
INFO:root:[  682] Training loss: 0.03188290, Validation loss: 0.03194864, Gradient norm: 2.51006731
INFO:root:[  683] Training loss: 0.03181503, Validation loss: 0.03200992, Gradient norm: 2.14034034
INFO:root:[  684] Training loss: 0.03191463, Validation loss: 0.03196339, Gradient norm: 2.87533339
INFO:root:EP 684: Early stopping
INFO:root:Training the model took 31582.004s.
INFO:root:Emptying the cuda cache took 0.109s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.36125
INFO:root:EnergyScoreTrain: 0.99942
INFO:root:CoverageTrain: 0.98417
INFO:root:IntervalWidthTrain: 0.07329
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.30874
INFO:root:EnergyScoreValidation: 0.96161
INFO:root:CoverageValidation: 0.98428
INFO:root:IntervalWidthValidation: 0.07364
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.07701
INFO:root:EnergyScoreTest: 0.79054
INFO:root:CoverageTest: 0.98421
INFO:root:IntervalWidthTest: 0.07268
INFO:root:###4 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 2329935872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.48244761, Validation loss: 0.32613887, Gradient norm: 6.24149491
INFO:root:[    2] Training loss: 0.30277480, Validation loss: 0.29967235, Gradient norm: 1.55639518
INFO:root:[    3] Training loss: 0.28671418, Validation loss: 0.27888240, Gradient norm: 1.59988956
INFO:root:[    4] Training loss: 0.26683270, Validation loss: 0.25084020, Gradient norm: 1.75731434
INFO:root:[    5] Training loss: 0.25644205, Validation loss: 0.24729336, Gradient norm: 1.65890647
INFO:root:[    6] Training loss: 0.24665652, Validation loss: 0.24361756, Gradient norm: 1.66120309
INFO:root:[    7] Training loss: 0.23423149, Validation loss: 0.22802899, Gradient norm: 1.06144295
INFO:root:[    8] Training loss: 0.22825177, Validation loss: 0.22997416, Gradient norm: 1.23708643
INFO:root:[    9] Training loss: 0.22671113, Validation loss: 0.22473178, Gradient norm: 1.61614864
INFO:root:[   10] Training loss: 0.22419977, Validation loss: 0.21199924, Gradient norm: 1.96948420
INFO:root:[   11] Training loss: 0.21707273, Validation loss: 0.21898114, Gradient norm: 1.90399151
INFO:root:[   12] Training loss: 0.20901377, Validation loss: 0.20657704, Gradient norm: 1.85703107
INFO:root:[   13] Training loss: 0.20360344, Validation loss: 0.20634221, Gradient norm: 1.21572250
INFO:root:[   14] Training loss: 0.20072277, Validation loss: 0.20045593, Gradient norm: 1.54812612
INFO:root:[   15] Training loss: 0.19865741, Validation loss: 0.19395133, Gradient norm: 1.64593822
INFO:root:[   16] Training loss: 0.18825829, Validation loss: 0.19202384, Gradient norm: 1.27671318
INFO:root:[   17] Training loss: 0.18646548, Validation loss: 0.18271471, Gradient norm: 1.25486689
INFO:root:[   18] Training loss: 0.18267854, Validation loss: 0.17725267, Gradient norm: 1.53528079
INFO:root:[   19] Training loss: 0.17987589, Validation loss: 0.17692018, Gradient norm: 1.35059386
INFO:root:[   20] Training loss: 0.17991250, Validation loss: 0.17402610, Gradient norm: 1.94767351
INFO:root:[   21] Training loss: 0.17620498, Validation loss: 0.17137908, Gradient norm: 2.08202450
INFO:root:[   22] Training loss: 0.16926848, Validation loss: 0.16896588, Gradient norm: 0.88457386
INFO:root:[   23] Training loss: 0.16774391, Validation loss: 0.16386677, Gradient norm: 1.56973772
INFO:root:[   24] Training loss: 0.16588827, Validation loss: 0.17110990, Gradient norm: 1.29719994
INFO:root:[   25] Training loss: 0.16383721, Validation loss: 0.16612379, Gradient norm: 1.75348147
INFO:root:[   26] Training loss: 0.16054427, Validation loss: 0.16024601, Gradient norm: 1.19928883
INFO:root:[   27] Training loss: 0.15917580, Validation loss: 0.16047491, Gradient norm: 1.66098025
INFO:root:[   28] Training loss: 0.15652466, Validation loss: 0.16314299, Gradient norm: 1.14871364
INFO:root:[   29] Training loss: 0.15821786, Validation loss: 0.15367078, Gradient norm: 2.31013610
INFO:root:[   30] Training loss: 0.15299813, Validation loss: 0.15786713, Gradient norm: 1.50858629
INFO:root:[   31] Training loss: 0.15314652, Validation loss: 0.15123349, Gradient norm: 1.48495131
INFO:root:[   32] Training loss: 0.14998468, Validation loss: 0.14772960, Gradient norm: 1.09210343
INFO:root:[   33] Training loss: 0.14831922, Validation loss: 0.14781297, Gradient norm: 1.01341339
INFO:root:[   34] Training loss: 0.14537822, Validation loss: 0.14544162, Gradient norm: 0.89568284
INFO:root:[   35] Training loss: 0.14345690, Validation loss: 0.15412364, Gradient norm: 1.16501117
INFO:root:[   36] Training loss: 0.14349477, Validation loss: 0.14123178, Gradient norm: 1.64473810
INFO:root:[   37] Training loss: 0.14164979, Validation loss: 0.13908198, Gradient norm: 1.41523451
INFO:root:[   38] Training loss: 0.14042781, Validation loss: 0.13845910, Gradient norm: 1.26938346
INFO:root:[   39] Training loss: 0.13756568, Validation loss: 0.13856181, Gradient norm: 1.02777557
INFO:root:[   40] Training loss: 0.13879415, Validation loss: 0.13957690, Gradient norm: 1.45827996
INFO:root:[   41] Training loss: 0.13747674, Validation loss: 0.13536583, Gradient norm: 1.51656907
INFO:root:[   42] Training loss: 0.13480286, Validation loss: 0.13369997, Gradient norm: 1.22204392
INFO:root:[   43] Training loss: 0.13382394, Validation loss: 0.13507773, Gradient norm: 0.85940256
INFO:root:[   44] Training loss: 0.13374093, Validation loss: 0.13125477, Gradient norm: 1.34152029
INFO:root:[   45] Training loss: 0.13096322, Validation loss: 0.13025932, Gradient norm: 0.91785390
INFO:root:[   46] Training loss: 0.13162748, Validation loss: 0.12886561, Gradient norm: 1.54032327
INFO:root:[   47] Training loss: 0.12891307, Validation loss: 0.12884720, Gradient norm: 1.32140198
INFO:root:[   48] Training loss: 0.12871455, Validation loss: 0.12846412, Gradient norm: 1.18434396
INFO:root:[   49] Training loss: 0.12733089, Validation loss: 0.12891141, Gradient norm: 0.92293421
INFO:root:[   50] Training loss: 0.12693682, Validation loss: 0.13321586, Gradient norm: 1.20730689
INFO:root:[   51] Training loss: 0.12712966, Validation loss: 0.12389562, Gradient norm: 1.80360496
INFO:root:[   52] Training loss: 0.12600123, Validation loss: 0.12421258, Gradient norm: 1.64154958
INFO:root:[   53] Training loss: 0.12321810, Validation loss: 0.13204439, Gradient norm: 0.94950656
INFO:root:[   54] Training loss: 0.12625331, Validation loss: 0.12226899, Gradient norm: 2.26749477
INFO:root:[   55] Training loss: 0.12122417, Validation loss: 0.12323249, Gradient norm: 0.91668928
INFO:root:[   56] Training loss: 0.12143103, Validation loss: 0.12045871, Gradient norm: 1.00773839
INFO:root:[   57] Training loss: 0.12045689, Validation loss: 0.11954306, Gradient norm: 1.09773763
INFO:root:[   58] Training loss: 0.12055233, Validation loss: 0.11907967, Gradient norm: 1.42214000
INFO:root:[   59] Training loss: 0.11820640, Validation loss: 0.11929903, Gradient norm: 0.90135662
INFO:root:[   60] Training loss: 0.11947718, Validation loss: 0.12166413, Gradient norm: 1.54693464
INFO:root:[   61] Training loss: 0.11871776, Validation loss: 0.11904148, Gradient norm: 1.38179365
INFO:root:[   62] Training loss: 0.11762562, Validation loss: 0.11945954, Gradient norm: 1.41549440
INFO:root:[   63] Training loss: 0.11784266, Validation loss: 0.11675273, Gradient norm: 1.20015030
INFO:root:[   64] Training loss: 0.11576596, Validation loss: 0.11772359, Gradient norm: 1.46041522
INFO:root:[   65] Training loss: 0.11673630, Validation loss: 0.11418925, Gradient norm: 1.81754645
INFO:root:[   66] Training loss: 0.11485155, Validation loss: 0.11513465, Gradient norm: 0.75592542
INFO:root:[   67] Training loss: 0.11388504, Validation loss: 0.11314385, Gradient norm: 1.17486082
INFO:root:[   68] Training loss: 0.11445331, Validation loss: 0.11254778, Gradient norm: 0.95984653
INFO:root:[   69] Training loss: 0.11467035, Validation loss: 0.11414621, Gradient norm: 1.59125067
INFO:root:[   70] Training loss: 0.11373170, Validation loss: 0.11265510, Gradient norm: 1.61345961
INFO:root:[   71] Training loss: 0.11249875, Validation loss: 0.11225243, Gradient norm: 1.63914488
INFO:root:[   72] Training loss: 0.11142663, Validation loss: 0.11146842, Gradient norm: 1.28254089
INFO:root:[   73] Training loss: 0.11150997, Validation loss: 0.11146282, Gradient norm: 1.64629640
INFO:root:[   74] Training loss: 0.11026871, Validation loss: 0.11004996, Gradient norm: 1.51024106
INFO:root:[   75] Training loss: 0.11082100, Validation loss: 0.11034647, Gradient norm: 1.32878474
INFO:root:[   76] Training loss: 0.11017175, Validation loss: 0.11085377, Gradient norm: 1.15153763
INFO:root:[   77] Training loss: 0.10979283, Validation loss: 0.10901970, Gradient norm: 1.50374906
INFO:root:[   78] Training loss: 0.10914203, Validation loss: 0.11078217, Gradient norm: 1.41253822
INFO:root:[   79] Training loss: 0.10865804, Validation loss: 0.10828225, Gradient norm: 1.46198187
INFO:root:[   80] Training loss: 0.10821108, Validation loss: 0.11165748, Gradient norm: 1.38144535
INFO:root:[   81] Training loss: 0.10885661, Validation loss: 0.10836605, Gradient norm: 1.48072064
INFO:root:[   82] Training loss: 0.10828342, Validation loss: 0.10759437, Gradient norm: 1.41454349
INFO:root:[   83] Training loss: 0.10756114, Validation loss: 0.10815883, Gradient norm: 1.50224284
INFO:root:[   84] Training loss: 0.10648068, Validation loss: 0.10646557, Gradient norm: 1.40179772
INFO:root:[   85] Training loss: 0.10581021, Validation loss: 0.10652402, Gradient norm: 1.23627500
INFO:root:[   86] Training loss: 0.10562578, Validation loss: 0.10538588, Gradient norm: 0.98606779
INFO:root:[   87] Training loss: 0.10494750, Validation loss: 0.10540356, Gradient norm: 1.01360883
INFO:root:[   88] Training loss: 0.10532452, Validation loss: 0.10622461, Gradient norm: 1.34138202
INFO:root:[   89] Training loss: 0.10437791, Validation loss: 0.10475558, Gradient norm: 1.40371916
INFO:root:[   90] Training loss: 0.10476859, Validation loss: 0.10420860, Gradient norm: 1.74789855
INFO:root:[   91] Training loss: 0.10430615, Validation loss: 0.10455138, Gradient norm: 1.26985108
INFO:root:[   92] Training loss: 0.10485847, Validation loss: 0.10465028, Gradient norm: 2.23174881
INFO:root:[   93] Training loss: 0.10369433, Validation loss: 0.10455407, Gradient norm: 1.59128794
INFO:root:[   94] Training loss: 0.10272259, Validation loss: 0.10258625, Gradient norm: 0.90733888
INFO:root:[   95] Training loss: 0.10277731, Validation loss: 0.10335991, Gradient norm: 1.21544002
INFO:root:[   96] Training loss: 0.10280885, Validation loss: 0.10220428, Gradient norm: 1.73501521
INFO:root:[   97] Training loss: 0.10205410, Validation loss: 0.10137098, Gradient norm: 1.24769889
INFO:root:[   98] Training loss: 0.10131104, Validation loss: 0.10195019, Gradient norm: 0.98074477
INFO:root:[   99] Training loss: 0.10132166, Validation loss: 0.10196398, Gradient norm: 0.73207276
INFO:root:[  100] Training loss: 0.10145071, Validation loss: 0.10047664, Gradient norm: 1.49282177
INFO:root:[  101] Training loss: 0.10112516, Validation loss: 0.10006682, Gradient norm: 1.79480461
INFO:root:[  102] Training loss: 0.10045205, Validation loss: 0.09973467, Gradient norm: 1.69859273
INFO:root:[  103] Training loss: 0.10024239, Validation loss: 0.10115006, Gradient norm: 1.70792142
INFO:root:[  104] Training loss: 0.09953131, Validation loss: 0.09957524, Gradient norm: 1.26635393
INFO:root:[  105] Training loss: 0.09947292, Validation loss: 0.09951647, Gradient norm: 1.27787250
INFO:root:[  106] Training loss: 0.09964695, Validation loss: 0.09817097, Gradient norm: 1.66436889
INFO:root:[  107] Training loss: 0.09963789, Validation loss: 0.09922270, Gradient norm: 1.19980620
INFO:root:[  108] Training loss: 0.09941754, Validation loss: 0.09931207, Gradient norm: 2.15653371
INFO:root:[  109] Training loss: 0.09905935, Validation loss: 0.09852250, Gradient norm: 2.07588287
INFO:root:[  110] Training loss: 0.09822191, Validation loss: 0.09696698, Gradient norm: 1.29992000
INFO:root:[  111] Training loss: 0.09730259, Validation loss: 0.09715844, Gradient norm: 0.90520008
INFO:root:[  112] Training loss: 0.09713353, Validation loss: 0.09693353, Gradient norm: 1.98653977
INFO:root:[  113] Training loss: 0.09734806, Validation loss: 0.09720880, Gradient norm: 1.54632519
INFO:root:[  114] Training loss: 0.09699954, Validation loss: 0.09767759, Gradient norm: 1.10973569
INFO:root:[  115] Training loss: 0.09645974, Validation loss: 0.09660823, Gradient norm: 1.61235887
INFO:root:[  116] Training loss: 0.09650965, Validation loss: 0.09592049, Gradient norm: 1.85013742
INFO:root:[  117] Training loss: 0.09599483, Validation loss: 0.09595699, Gradient norm: 1.33672864
INFO:root:[  118] Training loss: 0.09578166, Validation loss: 0.09568383, Gradient norm: 1.37025132
INFO:root:[  119] Training loss: 0.09595150, Validation loss: 0.09588600, Gradient norm: 1.61987089
INFO:root:[  120] Training loss: 0.09564747, Validation loss: 0.09534618, Gradient norm: 1.50940711
INFO:root:[  121] Training loss: 0.09506979, Validation loss: 0.09575280, Gradient norm: 1.09933902
INFO:root:[  122] Training loss: 0.09496833, Validation loss: 0.09505877, Gradient norm: 1.13532742
INFO:root:[  123] Training loss: 0.09518974, Validation loss: 0.09604217, Gradient norm: 1.36988949
INFO:root:[  124] Training loss: 0.09474148, Validation loss: 0.09612896, Gradient norm: 2.12311708
INFO:root:[  125] Training loss: 0.09527638, Validation loss: 0.09586213, Gradient norm: 2.32916560
INFO:root:[  126] Training loss: 0.09440678, Validation loss: 0.09435888, Gradient norm: 2.55108272
INFO:root:[  127] Training loss: 0.09389427, Validation loss: 0.09329533, Gradient norm: 1.73772392
INFO:root:[  128] Training loss: 0.09402644, Validation loss: 0.09622318, Gradient norm: 1.60238093
INFO:root:[  129] Training loss: 0.09440431, Validation loss: 0.09336260, Gradient norm: 3.59435599
INFO:root:[  130] Training loss: 0.09334774, Validation loss: 0.09430919, Gradient norm: 2.06187581
INFO:root:[  131] Training loss: 0.09311143, Validation loss: 0.09383513, Gradient norm: 1.86878464
INFO:root:[  132] Training loss: 0.09302584, Validation loss: 0.09247420, Gradient norm: 2.29567567
INFO:root:[  133] Training loss: 0.09265376, Validation loss: 0.09279951, Gradient norm: 2.24992833
INFO:root:[  134] Training loss: 0.09233480, Validation loss: 0.09242452, Gradient norm: 2.01219091
INFO:root:[  135] Training loss: 0.09220804, Validation loss: 0.09248569, Gradient norm: 1.73173377
INFO:root:[  136] Training loss: 0.09231990, Validation loss: 0.09309198, Gradient norm: 2.37819100
INFO:root:[  137] Training loss: 0.09222468, Validation loss: 0.09122272, Gradient norm: 2.24413579
INFO:root:[  138] Training loss: 0.09236766, Validation loss: 0.09157510, Gradient norm: 2.65097617
INFO:root:[  139] Training loss: 0.09188558, Validation loss: 0.09212531, Gradient norm: 2.07026188
INFO:root:[  140] Training loss: 0.09219615, Validation loss: 0.09252393, Gradient norm: 3.37976798
INFO:root:[  141] Training loss: 0.09116992, Validation loss: 0.09214365, Gradient norm: 1.50795655
INFO:root:[  142] Training loss: 0.09207411, Validation loss: 0.09155230, Gradient norm: 2.36713588
INFO:root:[  143] Training loss: 0.09126499, Validation loss: 0.09157758, Gradient norm: 2.30984840
INFO:root:[  144] Training loss: 0.09094042, Validation loss: 0.09171500, Gradient norm: 1.75239501
INFO:root:[  145] Training loss: 0.09121293, Validation loss: 0.09152783, Gradient norm: 1.60454550
INFO:root:[  146] Training loss: 0.09144451, Validation loss: 0.09219510, Gradient norm: 2.54104319
INFO:root:[  147] Training loss: 0.09129837, Validation loss: 0.09121548, Gradient norm: 3.53143716
INFO:root:[  148] Training loss: 0.09105579, Validation loss: 0.09031058, Gradient norm: 2.94822856
INFO:root:[  149] Training loss: 0.09096706, Validation loss: 0.09133959, Gradient norm: 3.87834359
INFO:root:[  150] Training loss: 0.09092537, Validation loss: 0.09377440, Gradient norm: 3.05551768
INFO:root:[  151] Training loss: 0.09142905, Validation loss: 0.09117810, Gradient norm: 4.75622233
INFO:root:[  152] Training loss: 0.09060351, Validation loss: 0.08997273, Gradient norm: 3.79755124
INFO:root:[  153] Training loss: 0.09029918, Validation loss: 0.08985165, Gradient norm: 3.04376718
INFO:root:[  154] Training loss: 0.09036177, Validation loss: 0.08958205, Gradient norm: 1.84536092
INFO:root:[  155] Training loss: 0.09097959, Validation loss: 0.09015901, Gradient norm: 4.34428179
INFO:root:[  156] Training loss: 0.09045044, Validation loss: 0.09020525, Gradient norm: 3.90363258
INFO:root:[  157] Training loss: 0.09001010, Validation loss: 0.09069477, Gradient norm: 3.66895490
INFO:root:[  158] Training loss: 0.08977354, Validation loss: 0.08977945, Gradient norm: 3.33813848
INFO:root:[  159] Training loss: 0.08992457, Validation loss: 0.08989397, Gradient norm: 4.69083071
INFO:root:[  160] Training loss: 0.08938685, Validation loss: 0.08982478, Gradient norm: 2.47299022
INFO:root:[  161] Training loss: 0.08967544, Validation loss: 0.08952895, Gradient norm: 2.82430760
INFO:root:[  162] Training loss: 0.09036858, Validation loss: 0.09016304, Gradient norm: 4.12699943
INFO:root:[  163] Training loss: 0.08995787, Validation loss: 0.08989936, Gradient norm: 3.52085503
INFO:root:[  164] Training loss: 0.09062188, Validation loss: 0.09197402, Gradient norm: 5.21866238
INFO:root:[  165] Training loss: 0.08996890, Validation loss: 0.08928089, Gradient norm: 5.35214531
INFO:root:[  166] Training loss: 0.09103074, Validation loss: 0.09004039, Gradient norm: 6.84985977
INFO:root:[  167] Training loss: 0.09073724, Validation loss: 0.09002536, Gradient norm: 7.39002078
INFO:root:[  168] Training loss: 0.08912907, Validation loss: 0.08977488, Gradient norm: 4.16014413
INFO:root:[  169] Training loss: 0.08929612, Validation loss: 0.09061327, Gradient norm: 3.51333201
INFO:root:[  170] Training loss: 0.08897183, Validation loss: 0.08993892, Gradient norm: 3.81377105
INFO:root:[  171] Training loss: 0.08963261, Validation loss: 0.09000189, Gradient norm: 4.68932290
INFO:root:[  172] Training loss: 0.08953088, Validation loss: 0.09105830, Gradient norm: 6.33574155
INFO:root:[  173] Training loss: 0.08885971, Validation loss: 0.08834253, Gradient norm: 4.59334193
INFO:root:[  174] Training loss: 0.08828046, Validation loss: 0.08912183, Gradient norm: 2.09128630
INFO:root:[  175] Training loss: 0.08828539, Validation loss: 0.08850331, Gradient norm: 3.30741834
INFO:root:[  176] Training loss: 0.08893617, Validation loss: 0.08891054, Gradient norm: 3.63785060
INFO:root:[  177] Training loss: 0.08881762, Validation loss: 0.08847296, Gradient norm: 2.97479738
INFO:root:[  178] Training loss: 0.08851481, Validation loss: 0.08837222, Gradient norm: 3.95096184
INFO:root:[  179] Training loss: 0.08882505, Validation loss: 0.08938672, Gradient norm: 3.18793430
INFO:root:[  180] Training loss: 0.08954227, Validation loss: 0.08916006, Gradient norm: 7.63785972
INFO:root:[  181] Training loss: 0.08869171, Validation loss: 0.08866443, Gradient norm: 4.12928642
INFO:root:[  182] Training loss: 0.08906081, Validation loss: 0.08880505, Gradient norm: 6.21478838
INFO:root:EP 182: Early stopping
INFO:root:Training the model took 8440.272s.
INFO:root:Emptying the cuda cache took 0.109s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 3.53467
INFO:root:EnergyScoreTrain: 2.7598
INFO:root:CoverageTrain: 0.98831
INFO:root:IntervalWidthTrain: 0.23811
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 3.39571
INFO:root:EnergyScoreValidation: 2.65085
INFO:root:CoverageValidation: 0.98834
INFO:root:IntervalWidthValidation: 0.23827
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.80701
INFO:root:EnergyScoreTest: 2.19092
INFO:root:CoverageTest: 0.98828
INFO:root:IntervalWidthTest: 0.23663
INFO:root:###5 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1652555776
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.29081120, Validation loss: 0.53933764, Gradient norm: 8.95818713
INFO:root:[    2] Training loss: 0.47783136, Validation loss: 0.44631027, Gradient norm: 2.10521652
INFO:root:[    3] Training loss: 0.43182671, Validation loss: 0.39109435, Gradient norm: 1.78991222
INFO:root:[    4] Training loss: 0.39775876, Validation loss: 0.39402132, Gradient norm: 1.68234022
INFO:root:[    5] Training loss: 0.37446788, Validation loss: 0.35980079, Gradient norm: 2.01998930
INFO:root:[    6] Training loss: 0.36054947, Validation loss: 0.35400672, Gradient norm: 2.04539424
INFO:root:[    7] Training loss: 0.34293255, Validation loss: 0.34419941, Gradient norm: 1.80502798
INFO:root:[    8] Training loss: 0.32149237, Validation loss: 0.31176729, Gradient norm: 1.93716039
INFO:root:[    9] Training loss: 0.30804230, Validation loss: 0.31644610, Gradient norm: 1.65529553
INFO:root:[   10] Training loss: 0.30271483, Validation loss: 0.30459591, Gradient norm: 2.58855655
INFO:root:[   11] Training loss: 0.28258408, Validation loss: 0.28405227, Gradient norm: 1.60222818
INFO:root:[   12] Training loss: 0.27477955, Validation loss: 0.27221124, Gradient norm: 1.35301565
INFO:root:[   13] Training loss: 0.27066856, Validation loss: 0.26657720, Gradient norm: 1.88027777
INFO:root:[   14] Training loss: 0.26412985, Validation loss: 0.26085152, Gradient norm: 1.85369816
INFO:root:[   15] Training loss: 0.25704727, Validation loss: 0.25448509, Gradient norm: 2.49034273
INFO:root:[   16] Training loss: 0.25732370, Validation loss: 0.25159660, Gradient norm: 2.56974860
INFO:root:[   17] Training loss: 0.24539222, Validation loss: 0.24493806, Gradient norm: 1.32289582
INFO:root:[   18] Training loss: 0.23982844, Validation loss: 0.24129849, Gradient norm: 1.13006554
INFO:root:[   19] Training loss: 0.23911181, Validation loss: 0.24223889, Gradient norm: 2.64273543
INFO:root:[   20] Training loss: 0.24004397, Validation loss: 0.23146977, Gradient norm: 3.08023910
INFO:root:[   21] Training loss: 0.22506021, Validation loss: 0.22351912, Gradient norm: 1.28872628
INFO:root:[   22] Training loss: 0.22658956, Validation loss: 0.22270070, Gradient norm: 2.21770454
INFO:root:[   23] Training loss: 0.22171403, Validation loss: 0.22021557, Gradient norm: 1.90178369
INFO:root:[   24] Training loss: 0.21811550, Validation loss: 0.21892683, Gradient norm: 1.78860278
INFO:root:[   25] Training loss: 0.21409996, Validation loss: 0.20935777, Gradient norm: 1.82892435
INFO:root:[   26] Training loss: 0.20944332, Validation loss: 0.21079846, Gradient norm: 1.65941437
INFO:root:[   27] Training loss: 0.20675335, Validation loss: 0.20663432, Gradient norm: 1.67356627
INFO:root:[   28] Training loss: 0.20555263, Validation loss: 0.19866527, Gradient norm: 1.37525451
INFO:root:[   29] Training loss: 0.20554586, Validation loss: 0.21007435, Gradient norm: 1.31242743
INFO:root:[   30] Training loss: 0.20265618, Validation loss: 0.19881297, Gradient norm: 1.99184374
INFO:root:[   31] Training loss: 0.19836525, Validation loss: 0.19554956, Gradient norm: 1.17391820
INFO:root:[   32] Training loss: 0.19830897, Validation loss: 0.20197697, Gradient norm: 1.98433014
INFO:root:[   33] Training loss: 0.19500854, Validation loss: 0.19858885, Gradient norm: 1.66605557
INFO:root:[   34] Training loss: 0.19185841, Validation loss: 0.18785190, Gradient norm: 1.74753315
INFO:root:[   35] Training loss: 0.18898656, Validation loss: 0.18645841, Gradient norm: 1.64858644
INFO:root:[   36] Training loss: 0.18815514, Validation loss: 0.18721341, Gradient norm: 1.61420951
INFO:root:[   37] Training loss: 0.18464012, Validation loss: 0.18643641, Gradient norm: 1.48074520
INFO:root:[   38] Training loss: 0.18288018, Validation loss: 0.18173869, Gradient norm: 1.91056197
INFO:root:[   39] Training loss: 0.18078794, Validation loss: 0.17885044, Gradient norm: 1.72374478
INFO:root:[   40] Training loss: 0.17962736, Validation loss: 0.17774858, Gradient norm: 1.72425524
INFO:root:[   41] Training loss: 0.17674927, Validation loss: 0.17297515, Gradient norm: 1.23512887
INFO:root:[   42] Training loss: 0.17527182, Validation loss: 0.17307943, Gradient norm: 1.43800897
INFO:root:[   43] Training loss: 0.17385929, Validation loss: 0.17290751, Gradient norm: 1.47644085
INFO:root:[   44] Training loss: 0.17178899, Validation loss: 0.17052243, Gradient norm: 1.43356661
INFO:root:[   45] Training loss: 0.16896221, Validation loss: 0.17027691, Gradient norm: 0.89411993
INFO:root:[   46] Training loss: 0.16581150, Validation loss: 0.16685328, Gradient norm: 1.07546922
INFO:root:[   47] Training loss: 0.16611562, Validation loss: 0.16395561, Gradient norm: 1.26650593
INFO:root:[   48] Training loss: 0.16469049, Validation loss: 0.16636193, Gradient norm: 1.80538764
INFO:root:[   49] Training loss: 0.16214701, Validation loss: 0.15839958, Gradient norm: 1.63903714
INFO:root:[   50] Training loss: 0.16213730, Validation loss: 0.16422686, Gradient norm: 1.71681482
INFO:root:[   51] Training loss: 0.15946584, Validation loss: 0.15738507, Gradient norm: 1.97449887
INFO:root:[   52] Training loss: 0.15759831, Validation loss: 0.15843020, Gradient norm: 1.24722688
INFO:root:[   53] Training loss: 0.15692480, Validation loss: 0.15588815, Gradient norm: 1.76513641
INFO:root:[   54] Training loss: 0.15414366, Validation loss: 0.15423452, Gradient norm: 1.05950408
INFO:root:[   55] Training loss: 0.15248405, Validation loss: 0.15505668, Gradient norm: 1.10554662
INFO:root:[   56] Training loss: 0.15165288, Validation loss: 0.15048595, Gradient norm: 1.31866186
INFO:root:[   57] Training loss: 0.15065598, Validation loss: 0.15802279, Gradient norm: 1.49339255
INFO:root:[   58] Training loss: 0.15075315, Validation loss: 0.14974220, Gradient norm: 1.67232823
INFO:root:[   59] Training loss: 0.14820928, Validation loss: 0.14773216, Gradient norm: 1.07622824
INFO:root:[   60] Training loss: 0.14723008, Validation loss: 0.14545689, Gradient norm: 1.79901277
INFO:root:[   61] Training loss: 0.14505822, Validation loss: 0.14566228, Gradient norm: 1.20727771
INFO:root:[   62] Training loss: 0.14526574, Validation loss: 0.14279192, Gradient norm: 1.55859219
INFO:root:[   63] Training loss: 0.14313697, Validation loss: 0.13929165, Gradient norm: 1.43816011
INFO:root:[   64] Training loss: 0.14172466, Validation loss: 0.14076972, Gradient norm: 1.19417452
INFO:root:[   65] Training loss: 0.13995817, Validation loss: 0.13850803, Gradient norm: 1.22021360
INFO:root:[   66] Training loss: 0.13999283, Validation loss: 0.13800472, Gradient norm: 1.65887186
INFO:root:[   67] Training loss: 0.14000351, Validation loss: 0.14329826, Gradient norm: 2.23128461
INFO:root:[   68] Training loss: 0.13709934, Validation loss: 0.13582824, Gradient norm: 1.65475674
INFO:root:[   69] Training loss: 0.13550388, Validation loss: 0.13437361, Gradient norm: 1.30930237
INFO:root:[   70] Training loss: 0.13476540, Validation loss: 0.13364558, Gradient norm: 1.34495005
INFO:root:[   71] Training loss: 0.13405995, Validation loss: 0.13338732, Gradient norm: 1.38745614
INFO:root:[   72] Training loss: 0.13308297, Validation loss: 0.13014872, Gradient norm: 1.82289021
INFO:root:[   73] Training loss: 0.13163929, Validation loss: 0.13082823, Gradient norm: 0.83379049
INFO:root:[   74] Training loss: 0.13003091, Validation loss: 0.13120353, Gradient norm: 0.96175019
INFO:root:[   75] Training loss: 0.12920424, Validation loss: 0.12742990, Gradient norm: 1.02434136
INFO:root:[   76] Training loss: 0.12811578, Validation loss: 0.12811205, Gradient norm: 1.39174797
INFO:root:[   77] Training loss: 0.12693401, Validation loss: 0.12935663, Gradient norm: 1.10048563
INFO:root:[   78] Training loss: 0.12647745, Validation loss: 0.12545334, Gradient norm: 1.10458806
INFO:root:[   79] Training loss: 0.12629354, Validation loss: 0.12679538, Gradient norm: 1.45877233
INFO:root:[   80] Training loss: 0.12484923, Validation loss: 0.12394038, Gradient norm: 1.57327647
INFO:root:[   81] Training loss: 0.12426274, Validation loss: 0.12437485, Gradient norm: 1.36478095
INFO:root:[   82] Training loss: 0.12277441, Validation loss: 0.12218739, Gradient norm: 1.17581946
INFO:root:[   83] Training loss: 0.12154912, Validation loss: 0.12074328, Gradient norm: 0.72180938
INFO:root:[   84] Training loss: 0.12109391, Validation loss: 0.12087277, Gradient norm: 1.42585886
INFO:root:[   85] Training loss: 0.12021123, Validation loss: 0.12024900, Gradient norm: 1.14258627
INFO:root:[   86] Training loss: 0.12061693, Validation loss: 0.11918385, Gradient norm: 2.04475995
INFO:root:[   87] Training loss: 0.11898244, Validation loss: 0.11833935, Gradient norm: 1.10005847
INFO:root:[   88] Training loss: 0.11837723, Validation loss: 0.11829131, Gradient norm: 1.20252568
INFO:root:[   89] Training loss: 0.11774487, Validation loss: 0.11939832, Gradient norm: 1.18182138
INFO:root:[   90] Training loss: 0.11704134, Validation loss: 0.11645860, Gradient norm: 1.35532925
INFO:root:[   91] Training loss: 0.11655685, Validation loss: 0.11506875, Gradient norm: 1.34139997
INFO:root:[   92] Training loss: 0.11533463, Validation loss: 0.11563441, Gradient norm: 1.24739999
INFO:root:[   93] Training loss: 0.11481665, Validation loss: 0.11462623, Gradient norm: 1.41466926
INFO:root:[   94] Training loss: 0.11534783, Validation loss: 0.11484840, Gradient norm: 2.26412488
INFO:root:[   95] Training loss: 0.11344524, Validation loss: 0.11304856, Gradient norm: 1.13691403
INFO:root:[   96] Training loss: 0.11251935, Validation loss: 0.11161623, Gradient norm: 1.06548882
INFO:root:[   97] Training loss: 0.11234173, Validation loss: 0.11156682, Gradient norm: 1.48325909
INFO:root:[   98] Training loss: 0.11189078, Validation loss: 0.11119082, Gradient norm: 1.48755637
INFO:root:[   99] Training loss: 0.11087961, Validation loss: 0.11113904, Gradient norm: 1.45472431
INFO:root:[  100] Training loss: 0.11017996, Validation loss: 0.11028398, Gradient norm: 1.31722656
INFO:root:[  101] Training loss: 0.11019847, Validation loss: 0.10949575, Gradient norm: 1.64177077
INFO:root:[  102] Training loss: 0.10909652, Validation loss: 0.10819589, Gradient norm: 1.20415885
INFO:root:[  103] Training loss: 0.10828484, Validation loss: 0.10785363, Gradient norm: 1.31176415
INFO:root:[  104] Training loss: 0.10818542, Validation loss: 0.10776008, Gradient norm: 1.39279589
INFO:root:[  105] Training loss: 0.10720220, Validation loss: 0.10802452, Gradient norm: 1.11416432
INFO:root:[  106] Training loss: 0.10661282, Validation loss: 0.10754091, Gradient norm: 0.94592231
INFO:root:[  107] Training loss: 0.10653555, Validation loss: 0.10716895, Gradient norm: 1.39736452
INFO:root:[  108] Training loss: 0.10561912, Validation loss: 0.10590383, Gradient norm: 1.32003918
INFO:root:[  109] Training loss: 0.10494335, Validation loss: 0.10420644, Gradient norm: 1.52585307
INFO:root:[  110] Training loss: 0.10477888, Validation loss: 0.10390227, Gradient norm: 1.39227552
INFO:root:[  111] Training loss: 0.10369639, Validation loss: 0.10369294, Gradient norm: 1.26808014
INFO:root:[  112] Training loss: 0.10362436, Validation loss: 0.10501182, Gradient norm: 1.55413547
INFO:root:[  113] Training loss: 0.10373527, Validation loss: 0.10268326, Gradient norm: 1.99751284
INFO:root:[  114] Training loss: 0.10304089, Validation loss: 0.10511457, Gradient norm: 2.10420273
INFO:root:[  115] Training loss: 0.10231397, Validation loss: 0.10108659, Gradient norm: 1.66343343
INFO:root:[  116] Training loss: 0.10105957, Validation loss: 0.10088690, Gradient norm: 0.86507858
INFO:root:[  117] Training loss: 0.10079949, Validation loss: 0.10079484, Gradient norm: 1.15336989
INFO:root:[  118] Training loss: 0.10030846, Validation loss: 0.09972526, Gradient norm: 1.33843914
INFO:root:[  119] Training loss: 0.09969730, Validation loss: 0.09953460, Gradient norm: 1.05608539
INFO:root:[  120] Training loss: 0.09903725, Validation loss: 0.09902806, Gradient norm: 1.07752975
INFO:root:[  121] Training loss: 0.09916893, Validation loss: 0.09877421, Gradient norm: 1.39795679
INFO:root:[  122] Training loss: 0.09862877, Validation loss: 0.09917154, Gradient norm: 1.21704826
INFO:root:[  123] Training loss: 0.09772455, Validation loss: 0.09721893, Gradient norm: 0.81689896
INFO:root:[  124] Training loss: 0.09781299, Validation loss: 0.09810209, Gradient norm: 1.60869529
INFO:root:[  125] Training loss: 0.09676743, Validation loss: 0.09695986, Gradient norm: 1.04284060
INFO:root:[  126] Training loss: 0.09672175, Validation loss: 0.09679197, Gradient norm: 1.10721917
INFO:root:[  127] Training loss: 0.09619997, Validation loss: 0.09532260, Gradient norm: 1.33401751
INFO:root:[  128] Training loss: 0.09547699, Validation loss: 0.09589363, Gradient norm: 0.93239902
INFO:root:[  129] Training loss: 0.09551421, Validation loss: 0.09467733, Gradient norm: 1.64181820
INFO:root:[  130] Training loss: 0.09445952, Validation loss: 0.09437285, Gradient norm: 0.96813467
INFO:root:[  131] Training loss: 0.09427455, Validation loss: 0.09376112, Gradient norm: 1.12896691
INFO:root:[  132] Training loss: 0.09397635, Validation loss: 0.09366593, Gradient norm: 1.24461842
INFO:root:[  133] Training loss: 0.09350949, Validation loss: 0.09339303, Gradient norm: 0.89608672
INFO:root:[  134] Training loss: 0.09329065, Validation loss: 0.09339348, Gradient norm: 1.22644353
INFO:root:[  135] Training loss: 0.09312867, Validation loss: 0.09235389, Gradient norm: 1.74252095
INFO:root:[  136] Training loss: 0.09240686, Validation loss: 0.09319288, Gradient norm: 1.55816859
INFO:root:[  137] Training loss: 0.09219081, Validation loss: 0.09342258, Gradient norm: 1.66353416
INFO:root:[  138] Training loss: 0.09173237, Validation loss: 0.09103691, Gradient norm: 1.66507779
INFO:root:[  139] Training loss: 0.09122929, Validation loss: 0.09146771, Gradient norm: 1.37299509
INFO:root:[  140] Training loss: 0.09103067, Validation loss: 0.09105908, Gradient norm: 1.72404156
INFO:root:[  141] Training loss: 0.09054651, Validation loss: 0.08987116, Gradient norm: 1.30869683
INFO:root:[  142] Training loss: 0.09015696, Validation loss: 0.09038125, Gradient norm: 1.68355819
INFO:root:[  143] Training loss: 0.08986147, Validation loss: 0.08989595, Gradient norm: 0.96563244
INFO:root:[  144] Training loss: 0.08963753, Validation loss: 0.08949457, Gradient norm: 1.53300021
INFO:root:[  145] Training loss: 0.08933143, Validation loss: 0.08974636, Gradient norm: 1.10441206
INFO:root:[  146] Training loss: 0.08895484, Validation loss: 0.08884152, Gradient norm: 1.41316222
INFO:root:[  147] Training loss: 0.08858970, Validation loss: 0.08879441, Gradient norm: 1.46176845
INFO:root:[  148] Training loss: 0.08813668, Validation loss: 0.08784109, Gradient norm: 0.94285260
INFO:root:[  149] Training loss: 0.08784397, Validation loss: 0.08760280, Gradient norm: 0.95936734
INFO:root:[  150] Training loss: 0.08757709, Validation loss: 0.08812459, Gradient norm: 1.44276010
INFO:root:[  151] Training loss: 0.08739384, Validation loss: 0.08740712, Gradient norm: 1.47101712
INFO:root:[  152] Training loss: 0.08734261, Validation loss: 0.08686384, Gradient norm: 1.73267144
INFO:root:[  153] Training loss: 0.08655520, Validation loss: 0.08718797, Gradient norm: 0.94227777
INFO:root:[  154] Training loss: 0.08658933, Validation loss: 0.08683664, Gradient norm: 1.47232763
INFO:root:[  155] Training loss: 0.08609567, Validation loss: 0.08659474, Gradient norm: 0.75074844
INFO:root:[  156] Training loss: 0.08582116, Validation loss: 0.08677212, Gradient norm: 1.58851764
INFO:root:[  157] Training loss: 0.08552833, Validation loss: 0.08522405, Gradient norm: 1.56167569
INFO:root:[  158] Training loss: 0.08512567, Validation loss: 0.08522410, Gradient norm: 1.47963361
INFO:root:[  159] Training loss: 0.08502837, Validation loss: 0.08446598, Gradient norm: 2.15139242
INFO:root:[  160] Training loss: 0.08452848, Validation loss: 0.08493699, Gradient norm: 1.06251279
INFO:root:[  161] Training loss: 0.08464455, Validation loss: 0.08541819, Gradient norm: 1.72609546
INFO:root:[  162] Training loss: 0.08422713, Validation loss: 0.08416854, Gradient norm: 1.83916713
INFO:root:[  163] Training loss: 0.08414741, Validation loss: 0.08430875, Gradient norm: 1.88222864
INFO:root:[  164] Training loss: 0.08347865, Validation loss: 0.08366609, Gradient norm: 1.32573993
INFO:root:[  165] Training loss: 0.08355984, Validation loss: 0.08443730, Gradient norm: 1.46883763
INFO:root:[  166] Training loss: 0.08373198, Validation loss: 0.08363024, Gradient norm: 2.49825561
INFO:root:[  167] Training loss: 0.08311678, Validation loss: 0.08292085, Gradient norm: 2.18517016
INFO:root:[  168] Training loss: 0.08269823, Validation loss: 0.08244911, Gradient norm: 1.17780035
INFO:root:[  169] Training loss: 0.08260971, Validation loss: 0.08234662, Gradient norm: 1.43047597
INFO:root:[  170] Training loss: 0.08274222, Validation loss: 0.08270908, Gradient norm: 2.64805966
INFO:root:[  171] Training loss: 0.08204103, Validation loss: 0.08227363, Gradient norm: 1.57162368
INFO:root:[  172] Training loss: 0.08199567, Validation loss: 0.08176782, Gradient norm: 1.10805257
INFO:root:[  173] Training loss: 0.08174926, Validation loss: 0.08195788, Gradient norm: 1.56494344
INFO:root:[  174] Training loss: 0.08150931, Validation loss: 0.08187043, Gradient norm: 1.06711344
INFO:root:[  175] Training loss: 0.08152026, Validation loss: 0.08120240, Gradient norm: 2.11119107
INFO:root:[  176] Training loss: 0.08132720, Validation loss: 0.08090344, Gradient norm: 2.39408346
INFO:root:[  177] Training loss: 0.08089186, Validation loss: 0.08152493, Gradient norm: 2.02376511
INFO:root:[  178] Training loss: 0.08105888, Validation loss: 0.08014530, Gradient norm: 2.35498721
INFO:root:[  179] Training loss: 0.08085368, Validation loss: 0.08113104, Gradient norm: 2.78155081
INFO:root:[  180] Training loss: 0.08037955, Validation loss: 0.08079870, Gradient norm: 1.87215178
INFO:root:[  181] Training loss: 0.08019214, Validation loss: 0.08010049, Gradient norm: 2.12397672
INFO:root:[  182] Training loss: 0.08010227, Validation loss: 0.08054638, Gradient norm: 2.27993319
INFO:root:[  183] Training loss: 0.08003550, Validation loss: 0.07929346, Gradient norm: 2.97800158
INFO:root:[  184] Training loss: 0.07982329, Validation loss: 0.07966054, Gradient norm: 2.73928606
INFO:root:[  185] Training loss: 0.07927316, Validation loss: 0.08009937, Gradient norm: 1.86946774
INFO:root:[  186] Training loss: 0.07918245, Validation loss: 0.07897518, Gradient norm: 2.03243436
INFO:root:[  187] Training loss: 0.07888526, Validation loss: 0.07950248, Gradient norm: 1.49331624
INFO:root:[  188] Training loss: 0.07883924, Validation loss: 0.07898171, Gradient norm: 1.34969766
INFO:root:[  189] Training loss: 0.07871923, Validation loss: 0.07837764, Gradient norm: 1.75094176
INFO:root:[  190] Training loss: 0.07854275, Validation loss: 0.07798197, Gradient norm: 2.09604740
INFO:root:[  191] Training loss: 0.07851987, Validation loss: 0.07799808, Gradient norm: 2.95548007
INFO:root:[  192] Training loss: 0.07807732, Validation loss: 0.07779040, Gradient norm: 2.26864462
INFO:root:[  193] Training loss: 0.07780069, Validation loss: 0.07811215, Gradient norm: 1.75987430
INFO:root:[  194] Training loss: 0.07779326, Validation loss: 0.07882293, Gradient norm: 1.44154779
INFO:root:[  195] Training loss: 0.07765742, Validation loss: 0.07759979, Gradient norm: 2.22630012
INFO:root:[  196] Training loss: 0.07741985, Validation loss: 0.07744691, Gradient norm: 1.96699069
INFO:root:[  197] Training loss: 0.07727327, Validation loss: 0.07752406, Gradient norm: 1.95748533
INFO:root:[  198] Training loss: 0.07695466, Validation loss: 0.07726230, Gradient norm: 1.87576012
INFO:root:[  199] Training loss: 0.07677763, Validation loss: 0.07630958, Gradient norm: 1.83422253
INFO:root:[  200] Training loss: 0.07672312, Validation loss: 0.07665081, Gradient norm: 2.59631364
INFO:root:[  201] Training loss: 0.07668379, Validation loss: 0.07667166, Gradient norm: 2.88810226
INFO:root:[  202] Training loss: 0.07610540, Validation loss: 0.07629910, Gradient norm: 1.94393891
INFO:root:[  203] Training loss: 0.07593401, Validation loss: 0.07608321, Gradient norm: 1.08027875
INFO:root:[  204] Training loss: 0.07603083, Validation loss: 0.07598369, Gradient norm: 1.22126449
INFO:root:[  205] Training loss: 0.07604120, Validation loss: 0.07643092, Gradient norm: 2.23533250
INFO:root:[  206] Training loss: 0.07580680, Validation loss: 0.07610002, Gradient norm: 2.12305958
INFO:root:[  207] Training loss: 0.07571289, Validation loss: 0.07595889, Gradient norm: 2.22742091
INFO:root:[  208] Training loss: 0.07556530, Validation loss: 0.07502336, Gradient norm: 2.06515009
INFO:root:[  209] Training loss: 0.07518292, Validation loss: 0.07543718, Gradient norm: 1.71087284
INFO:root:[  210] Training loss: 0.07519561, Validation loss: 0.07523863, Gradient norm: 2.67281406
INFO:root:[  211] Training loss: 0.07496564, Validation loss: 0.07466527, Gradient norm: 2.05970898
INFO:root:[  212] Training loss: 0.07464047, Validation loss: 0.07510896, Gradient norm: 2.06845484
INFO:root:[  213] Training loss: 0.07444255, Validation loss: 0.07519389, Gradient norm: 1.41446376
INFO:root:[  214] Training loss: 0.07449715, Validation loss: 0.07458253, Gradient norm: 2.94847776
INFO:root:[  215] Training loss: 0.07421520, Validation loss: 0.07380366, Gradient norm: 1.99980098
INFO:root:[  216] Training loss: 0.07401117, Validation loss: 0.07442232, Gradient norm: 1.33498246
INFO:root:[  217] Training loss: 0.07405988, Validation loss: 0.07354975, Gradient norm: 2.56399979
INFO:root:[  218] Training loss: 0.07387771, Validation loss: 0.07477945, Gradient norm: 1.93704928
INFO:root:[  219] Training loss: 0.07362951, Validation loss: 0.07431705, Gradient norm: 2.80841035
INFO:root:[  220] Training loss: 0.07346218, Validation loss: 0.07380199, Gradient norm: 2.41522260
INFO:root:[  221] Training loss: 0.07326393, Validation loss: 0.07372769, Gradient norm: 2.05198667
INFO:root:[  222] Training loss: 0.07298230, Validation loss: 0.07303689, Gradient norm: 2.30163267
INFO:root:[  223] Training loss: 0.07286592, Validation loss: 0.07280976, Gradient norm: 2.45638148
INFO:root:[  224] Training loss: 0.07300728, Validation loss: 0.07242567, Gradient norm: 2.15658419
INFO:root:[  225] Training loss: 0.07272090, Validation loss: 0.07319583, Gradient norm: 2.80056375
INFO:root:[  226] Training loss: 0.07240165, Validation loss: 0.07212139, Gradient norm: 2.97929129
INFO:root:[  227] Training loss: 0.07213966, Validation loss: 0.07207486, Gradient norm: 2.26336089
INFO:root:[  228] Training loss: 0.07218856, Validation loss: 0.07304927, Gradient norm: 2.59115010
INFO:root:[  229] Training loss: 0.07223152, Validation loss: 0.07207970, Gradient norm: 3.19544348
INFO:root:[  230] Training loss: 0.07224142, Validation loss: 0.07207832, Gradient norm: 3.21954222
INFO:root:[  231] Training loss: 0.07196181, Validation loss: 0.07211422, Gradient norm: 3.32465884
INFO:root:[  232] Training loss: 0.07151361, Validation loss: 0.07131204, Gradient norm: 2.43576012
INFO:root:[  233] Training loss: 0.07128957, Validation loss: 0.07174081, Gradient norm: 1.94737963
INFO:root:[  234] Training loss: 0.07137094, Validation loss: 0.07159481, Gradient norm: 2.21489025
INFO:root:[  235] Training loss: 0.07116391, Validation loss: 0.07154604, Gradient norm: 3.15994647
INFO:root:[  236] Training loss: 0.07109911, Validation loss: 0.07085271, Gradient norm: 2.95423068
INFO:root:[  237] Training loss: 0.07058730, Validation loss: 0.07067592, Gradient norm: 1.42762912
INFO:root:[  238] Training loss: 0.07059804, Validation loss: 0.07085375, Gradient norm: 2.74995344
INFO:root:[  239] Training loss: 0.07058616, Validation loss: 0.07073347, Gradient norm: 3.20159738
INFO:root:[  240] Training loss: 0.07032391, Validation loss: 0.07061166, Gradient norm: 1.67390641
INFO:root:[  241] Training loss: 0.07040675, Validation loss: 0.07129696, Gradient norm: 2.41768313
INFO:root:[  242] Training loss: 0.07022192, Validation loss: 0.07027933, Gradient norm: 3.38154139
INFO:root:[  243] Training loss: 0.06984549, Validation loss: 0.07027832, Gradient norm: 2.24619345
INFO:root:[  244] Training loss: 0.07005846, Validation loss: 0.07043059, Gradient norm: 3.07172162
INFO:root:[  245] Training loss: 0.06974835, Validation loss: 0.07004142, Gradient norm: 3.05015077
INFO:root:[  246] Training loss: 0.06967232, Validation loss: 0.07092437, Gradient norm: 2.73191844
INFO:root:[  247] Training loss: 0.06967960, Validation loss: 0.07008417, Gradient norm: 4.17038239
INFO:root:[  248] Training loss: 0.06941632, Validation loss: 0.07069861, Gradient norm: 3.28788676
INFO:root:[  249] Training loss: 0.06962814, Validation loss: 0.06941251, Gradient norm: 3.55891932
INFO:root:[  250] Training loss: 0.06933070, Validation loss: 0.06876874, Gradient norm: 4.07560514
INFO:root:[  251] Training loss: 0.06898941, Validation loss: 0.06884911, Gradient norm: 3.43870966
INFO:root:[  252] Training loss: 0.06864169, Validation loss: 0.06884423, Gradient norm: 1.64715524
INFO:root:[  253] Training loss: 0.06882992, Validation loss: 0.06937663, Gradient norm: 3.86092006
INFO:root:[  254] Training loss: 0.06863162, Validation loss: 0.06840089, Gradient norm: 3.31302108
INFO:root:[  255] Training loss: 0.06841969, Validation loss: 0.06824780, Gradient norm: 3.26352882
INFO:root:[  256] Training loss: 0.06827952, Validation loss: 0.06864251, Gradient norm: 3.28601023
INFO:root:[  257] Training loss: 0.06818349, Validation loss: 0.06801917, Gradient norm: 4.26757110
INFO:root:[  258] Training loss: 0.06806197, Validation loss: 0.06912632, Gradient norm: 2.17829061
INFO:root:[  259] Training loss: 0.06815207, Validation loss: 0.06837590, Gradient norm: 4.33580001
INFO:root:[  260] Training loss: 0.06804992, Validation loss: 0.06806111, Gradient norm: 3.78378252
INFO:root:[  261] Training loss: 0.06783892, Validation loss: 0.06744373, Gradient norm: 4.63103958
INFO:root:[  262] Training loss: 0.06741860, Validation loss: 0.06761111, Gradient norm: 2.50593531
INFO:root:[  263] Training loss: 0.06741866, Validation loss: 0.06723577, Gradient norm: 4.11708035
INFO:root:[  264] Training loss: 0.06709703, Validation loss: 0.06789963, Gradient norm: 3.55336392
INFO:root:[  265] Training loss: 0.06713673, Validation loss: 0.06794683, Gradient norm: 3.08173955
INFO:root:[  266] Training loss: 0.06696841, Validation loss: 0.06750667, Gradient norm: 3.94119893
INFO:root:[  267] Training loss: 0.06674223, Validation loss: 0.06772316, Gradient norm: 3.23556878
INFO:root:[  268] Training loss: 0.06694071, Validation loss: 0.06690308, Gradient norm: 3.91702537
INFO:root:[  269] Training loss: 0.06669881, Validation loss: 0.06654864, Gradient norm: 2.77616682
INFO:root:[  270] Training loss: 0.06635230, Validation loss: 0.06638867, Gradient norm: 2.14145543
INFO:root:[  271] Training loss: 0.06627033, Validation loss: 0.06622237, Gradient norm: 3.92411592
INFO:root:[  272] Training loss: 0.06616231, Validation loss: 0.06605153, Gradient norm: 2.92413109
INFO:root:[  273] Training loss: 0.06613934, Validation loss: 0.06627088, Gradient norm: 3.77075985
INFO:root:[  274] Training loss: 0.06577998, Validation loss: 0.06570368, Gradient norm: 2.47976324
INFO:root:[  275] Training loss: 0.06593995, Validation loss: 0.06541213, Gradient norm: 3.97619257
INFO:root:[  276] Training loss: 0.06582118, Validation loss: 0.06574548, Gradient norm: 5.00468282
INFO:root:[  277] Training loss: 0.06550308, Validation loss: 0.06549191, Gradient norm: 5.51031205
INFO:root:[  278] Training loss: 0.06544205, Validation loss: 0.06610877, Gradient norm: 4.87919779
INFO:root:[  279] Training loss: 0.06528288, Validation loss: 0.06555196, Gradient norm: 3.71324732
INFO:root:[  280] Training loss: 0.06516892, Validation loss: 0.06539429, Gradient norm: 3.48073354
INFO:root:[  281] Training loss: 0.06519137, Validation loss: 0.06507630, Gradient norm: 4.94534774
INFO:root:[  282] Training loss: 0.06528527, Validation loss: 0.06706382, Gradient norm: 5.57351748
INFO:root:[  283] Training loss: 0.06496327, Validation loss: 0.06477990, Gradient norm: 4.66951386
INFO:root:[  284] Training loss: 0.06493216, Validation loss: 0.06448591, Gradient norm: 3.94883979
INFO:root:[  285] Training loss: 0.06464251, Validation loss: 0.06517568, Gradient norm: 2.29568695
INFO:root:[  286] Training loss: 0.06476178, Validation loss: 0.06479017, Gradient norm: 4.75343515
INFO:root:[  287] Training loss: 0.06450869, Validation loss: 0.06425217, Gradient norm: 4.13507274
INFO:root:[  288] Training loss: 0.06427569, Validation loss: 0.06395297, Gradient norm: 5.72197394
INFO:root:[  289] Training loss: 0.06384568, Validation loss: 0.06374017, Gradient norm: 2.85315000
INFO:root:[  290] Training loss: 0.06410910, Validation loss: 0.06484681, Gradient norm: 5.34556319
INFO:root:[  291] Training loss: 0.06385923, Validation loss: 0.06424279, Gradient norm: 5.66850364
INFO:root:[  292] Training loss: 0.06361589, Validation loss: 0.06422519, Gradient norm: 4.80421350
INFO:root:[  293] Training loss: 0.06366115, Validation loss: 0.06308180, Gradient norm: 5.90073964
INFO:root:[  294] Training loss: 0.06330155, Validation loss: 0.06357199, Gradient norm: 2.64738404
INFO:root:[  295] Training loss: 0.06359135, Validation loss: 0.06338801, Gradient norm: 6.09615137
INFO:root:[  296] Training loss: 0.06310998, Validation loss: 0.06300668, Gradient norm: 4.73238982
INFO:root:[  297] Training loss: 0.06311102, Validation loss: 0.06338071, Gradient norm: 3.75810132
INFO:root:[  298] Training loss: 0.06312407, Validation loss: 0.06330509, Gradient norm: 4.43909234
INFO:root:[  299] Training loss: 0.06324587, Validation loss: 0.06324560, Gradient norm: 3.40169471
INFO:root:[  300] Training loss: 0.06271669, Validation loss: 0.06239008, Gradient norm: 4.64709663
INFO:root:[  301] Training loss: 0.06267938, Validation loss: 0.06408796, Gradient norm: 2.47356924
INFO:root:[  302] Training loss: 0.06309042, Validation loss: 0.06298046, Gradient norm: 10.28341870
INFO:root:[  303] Training loss: 0.06227285, Validation loss: 0.06323925, Gradient norm: 5.35409450
INFO:root:[  304] Training loss: 0.06230968, Validation loss: 0.06241812, Gradient norm: 5.06163304
INFO:root:[  305] Training loss: 0.06205865, Validation loss: 0.06204404, Gradient norm: 4.79153963
INFO:root:[  306] Training loss: 0.06193887, Validation loss: 0.06230253, Gradient norm: 3.71725621
INFO:root:[  307] Training loss: 0.06165014, Validation loss: 0.06197898, Gradient norm: 3.15999388
INFO:root:[  308] Training loss: 0.06177925, Validation loss: 0.06172867, Gradient norm: 2.92764912
INFO:root:[  309] Training loss: 0.06178350, Validation loss: 0.06308178, Gradient norm: 7.07511312
INFO:root:[  310] Training loss: 0.06165668, Validation loss: 0.06136803, Gradient norm: 5.18695330
INFO:root:[  311] Training loss: 0.06148845, Validation loss: 0.06122459, Gradient norm: 5.21591282
INFO:root:[  312] Training loss: 0.06104304, Validation loss: 0.06148259, Gradient norm: 4.82305181
INFO:root:[  313] Training loss: 0.06105165, Validation loss: 0.06072663, Gradient norm: 3.69742987
INFO:root:[  314] Training loss: 0.06093581, Validation loss: 0.06157690, Gradient norm: 4.31957312
INFO:root:[  315] Training loss: 0.06126469, Validation loss: 0.06249495, Gradient norm: 7.01372077
INFO:root:[  316] Training loss: 0.06106696, Validation loss: 0.06109162, Gradient norm: 6.81298591
INFO:root:[  317] Training loss: 0.06054760, Validation loss: 0.06081685, Gradient norm: 3.48676126
INFO:root:[  318] Training loss: 0.06080370, Validation loss: 0.06078079, Gradient norm: 4.87263137
INFO:root:[  319] Training loss: 0.06055333, Validation loss: 0.06025301, Gradient norm: 5.45648237
INFO:root:[  320] Training loss: 0.06021670, Validation loss: 0.06069179, Gradient norm: 6.28370294
INFO:root:[  321] Training loss: 0.06036541, Validation loss: 0.06041948, Gradient norm: 5.31479355
INFO:root:[  322] Training loss: 0.06020061, Validation loss: 0.05984578, Gradient norm: 5.21239885
INFO:root:[  323] Training loss: 0.05977598, Validation loss: 0.06030108, Gradient norm: 4.72946777
INFO:root:[  324] Training loss: 0.05996447, Validation loss: 0.06043182, Gradient norm: 4.76633457
INFO:root:[  325] Training loss: 0.05996368, Validation loss: 0.05954962, Gradient norm: 7.39638182
INFO:root:[  326] Training loss: 0.05986931, Validation loss: 0.06007917, Gradient norm: 6.84846003
INFO:root:[  327] Training loss: 0.05953991, Validation loss: 0.05952257, Gradient norm: 5.15941733
INFO:root:[  328] Training loss: 0.05966919, Validation loss: 0.06077293, Gradient norm: 6.28016546
INFO:root:[  329] Training loss: 0.05919736, Validation loss: 0.05903249, Gradient norm: 4.76455223
INFO:root:[  330] Training loss: 0.05906904, Validation loss: 0.05910918, Gradient norm: 5.81590850
INFO:root:[  331] Training loss: 0.05908174, Validation loss: 0.05888263, Gradient norm: 6.72857702
INFO:root:[  332] Training loss: 0.05911299, Validation loss: 0.05971594, Gradient norm: 7.77381139
INFO:root:[  333] Training loss: 0.05860833, Validation loss: 0.05881311, Gradient norm: 4.42493326
INFO:root:[  334] Training loss: 0.05863810, Validation loss: 0.05976521, Gradient norm: 5.53191360
INFO:root:[  335] Training loss: 0.05902669, Validation loss: 0.05906456, Gradient norm: 10.51189763
INFO:root:[  336] Training loss: 0.05846654, Validation loss: 0.05866286, Gradient norm: 6.57527786
INFO:root:[  337] Training loss: 0.05844892, Validation loss: 0.05927848, Gradient norm: 6.02205612
INFO:root:[  338] Training loss: 0.05867639, Validation loss: 0.05892478, Gradient norm: 4.28958584
INFO:root:[  339] Training loss: 0.05830033, Validation loss: 0.05811667, Gradient norm: 5.33013863
INFO:root:[  340] Training loss: 0.05805307, Validation loss: 0.05855571, Gradient norm: 4.57633391
INFO:root:[  341] Training loss: 0.05835900, Validation loss: 0.05830600, Gradient norm: 7.87499869
INFO:root:[  342] Training loss: 0.05774146, Validation loss: 0.05762157, Gradient norm: 6.60233548
INFO:root:[  343] Training loss: 0.05761824, Validation loss: 0.05742427, Gradient norm: 6.25493928
INFO:root:[  344] Training loss: 0.05795853, Validation loss: 0.05748242, Gradient norm: 7.05350698
INFO:root:[  345] Training loss: 0.05786071, Validation loss: 0.05746038, Gradient norm: 9.94975300
INFO:root:[  346] Training loss: 0.05761594, Validation loss: 0.05894867, Gradient norm: 6.32235954
INFO:root:[  347] Training loss: 0.05730979, Validation loss: 0.05738585, Gradient norm: 7.89656936
INFO:root:[  348] Training loss: 0.05722059, Validation loss: 0.05860217, Gradient norm: 8.30551818
INFO:root:[  349] Training loss: 0.05730591, Validation loss: 0.05695150, Gradient norm: 10.54933601
INFO:root:[  350] Training loss: 0.05670325, Validation loss: 0.05697289, Gradient norm: 4.70884871
INFO:root:[  351] Training loss: 0.05683435, Validation loss: 0.05702118, Gradient norm: 5.85688827
INFO:root:[  352] Training loss: 0.05684972, Validation loss: 0.05665842, Gradient norm: 7.22119786
INFO:root:[  353] Training loss: 0.05668107, Validation loss: 0.05667949, Gradient norm: 5.31615425
INFO:root:[  354] Training loss: 0.05666683, Validation loss: 0.05654320, Gradient norm: 7.44110820
INFO:root:[  355] Training loss: 0.05661296, Validation loss: 0.05645917, Gradient norm: 7.21794507
INFO:root:[  356] Training loss: 0.05649230, Validation loss: 0.05609577, Gradient norm: 5.71073804
INFO:root:[  357] Training loss: 0.05626725, Validation loss: 0.05577781, Gradient norm: 3.01874534
INFO:root:[  358] Training loss: 0.05639438, Validation loss: 0.05636546, Gradient norm: 8.23004075
INFO:root:[  359] Training loss: 0.05606107, Validation loss: 0.05562522, Gradient norm: 7.87947098
INFO:root:[  360] Training loss: 0.05596344, Validation loss: 0.05579166, Gradient norm: 7.27775338
INFO:root:[  361] Training loss: 0.05600861, Validation loss: 0.05595899, Gradient norm: 7.87521505
INFO:root:[  362] Training loss: 0.05595443, Validation loss: 0.05609665, Gradient norm: 9.39060047
INFO:root:[  363] Training loss: 0.05567268, Validation loss: 0.05584469, Gradient norm: 6.30388029
INFO:root:[  364] Training loss: 0.05566380, Validation loss: 0.05631999, Gradient norm: 6.60355915
INFO:root:[  365] Training loss: 0.05567000, Validation loss: 0.05610837, Gradient norm: 5.38813956
INFO:root:[  366] Training loss: 0.05584260, Validation loss: 0.05693619, Gradient norm: 8.39849376
INFO:root:[  367] Training loss: 0.05614061, Validation loss: 0.05486117, Gradient norm: 16.32203090
INFO:root:[  368] Training loss: 0.05502990, Validation loss: 0.05503460, Gradient norm: 5.87361773
INFO:root:[  369] Training loss: 0.05508224, Validation loss: 0.05501433, Gradient norm: 9.66149433
INFO:root:[  370] Training loss: 0.05490537, Validation loss: 0.05513136, Gradient norm: 6.35886729
INFO:root:[  371] Training loss: 0.05520280, Validation loss: 0.05563005, Gradient norm: 12.12852081
INFO:root:[  372] Training loss: 0.05482823, Validation loss: 0.05437345, Gradient norm: 10.17882620
INFO:root:[  373] Training loss: 0.05517233, Validation loss: 0.05454510, Gradient norm: 15.14546508
INFO:root:[  374] Training loss: 0.05453212, Validation loss: 0.05432694, Gradient norm: 8.73323070
INFO:root:[  375] Training loss: 0.05448978, Validation loss: 0.05503856, Gradient norm: 9.39636015
INFO:root:[  376] Training loss: 0.05447547, Validation loss: 0.05412867, Gradient norm: 7.88402300
INFO:root:[  377] Training loss: 0.05418406, Validation loss: 0.05520503, Gradient norm: 5.34024849
INFO:root:[  378] Training loss: 0.05436803, Validation loss: 0.05471158, Gradient norm: 7.88510761
INFO:root:[  379] Training loss: 0.05447336, Validation loss: 0.05459242, Gradient norm: 7.92256837
INFO:root:[  380] Training loss: 0.05411321, Validation loss: 0.05395977, Gradient norm: 9.04760467
INFO:root:[  381] Training loss: 0.05416527, Validation loss: 0.05430282, Gradient norm: 12.53725242
INFO:root:[  382] Training loss: 0.05403096, Validation loss: 0.05447881, Gradient norm: 11.56022775
INFO:root:[  383] Training loss: 0.05384034, Validation loss: 0.05383895, Gradient norm: 11.35926061
INFO:root:[  384] Training loss: 0.05359225, Validation loss: 0.05436801, Gradient norm: 7.07632791
INFO:root:[  385] Training loss: 0.05353358, Validation loss: 0.05410304, Gradient norm: 8.52161861
INFO:root:[  386] Training loss: 0.05346582, Validation loss: 0.05403546, Gradient norm: 11.85297812
INFO:root:[  387] Training loss: 0.05376777, Validation loss: 0.05300380, Gradient norm: 14.85311966
INFO:root:[  388] Training loss: 0.05304861, Validation loss: 0.05379032, Gradient norm: 7.91547454
INFO:root:[  389] Training loss: 0.05308142, Validation loss: 0.05279650, Gradient norm: 9.50814140
INFO:root:[  390] Training loss: 0.05335832, Validation loss: 0.05293577, Gradient norm: 12.32381069
INFO:root:[  391] Training loss: 0.05311560, Validation loss: 0.05345963, Gradient norm: 11.00404726
INFO:root:[  392] Training loss: 0.05296144, Validation loss: 0.05371540, Gradient norm: 12.33688817
INFO:root:[  393] Training loss: 0.05292519, Validation loss: 0.05279297, Gradient norm: 10.81822780
INFO:root:[  394] Training loss: 0.05260913, Validation loss: 0.05284596, Gradient norm: 7.06449368
INFO:root:[  395] Training loss: 0.05263830, Validation loss: 0.05348275, Gradient norm: 10.36289538
INFO:root:[  396] Training loss: 0.05291285, Validation loss: 0.05491332, Gradient norm: 12.31443810
INFO:root:[  397] Training loss: 0.05269542, Validation loss: 0.05257603, Gradient norm: 16.17734552
INFO:root:[  398] Training loss: 0.05236716, Validation loss: 0.05289097, Gradient norm: 9.42052409
INFO:root:[  399] Training loss: 0.05250774, Validation loss: 0.05263442, Gradient norm: 12.90924786
INFO:root:[  400] Training loss: 0.05228671, Validation loss: 0.05224943, Gradient norm: 10.29793551
INFO:root:[  401] Training loss: 0.05245234, Validation loss: 0.05384348, Gradient norm: 15.39709679
INFO:root:[  402] Training loss: 0.05212476, Validation loss: 0.05215661, Gradient norm: 11.74937463
INFO:root:[  403] Training loss: 0.05193682, Validation loss: 0.05177404, Gradient norm: 8.24220411
INFO:root:[  404] Training loss: 0.05171421, Validation loss: 0.05229990, Gradient norm: 8.65117116
INFO:root:[  405] Training loss: 0.05207114, Validation loss: 0.05186737, Gradient norm: 14.99818192
INFO:root:[  406] Training loss: 0.05180617, Validation loss: 0.05207340, Gradient norm: 12.58854320
INFO:root:[  407] Training loss: 0.05176112, Validation loss: 0.05216759, Gradient norm: 13.18488714
INFO:root:[  408] Training loss: 0.05161806, Validation loss: 0.05129464, Gradient norm: 11.92216754
INFO:root:[  409] Training loss: 0.05144656, Validation loss: 0.05168903, Gradient norm: 12.32572065
INFO:root:[  410] Training loss: 0.05137325, Validation loss: 0.05182356, Gradient norm: 8.19627890
INFO:root:[  411] Training loss: 0.05148614, Validation loss: 0.05373594, Gradient norm: 11.94618939
INFO:root:[  412] Training loss: 0.05242096, Validation loss: 0.05199059, Gradient norm: 27.49023686
INFO:root:[  413] Training loss: 0.05135565, Validation loss: 0.05312496, Gradient norm: 15.90127108
INFO:root:[  414] Training loss: 0.05117692, Validation loss: 0.05272270, Gradient norm: 12.84675080
INFO:root:[  415] Training loss: 0.05093822, Validation loss: 0.05067745, Gradient norm: 10.70815568
INFO:root:[  416] Training loss: 0.05103847, Validation loss: 0.05334955, Gradient norm: 14.98433352
INFO:root:[  417] Training loss: 0.05103784, Validation loss: 0.05056589, Gradient norm: 18.10392042
INFO:root:[  418] Training loss: 0.05097322, Validation loss: 0.05085177, Gradient norm: 18.72159409
INFO:root:[  419] Training loss: 0.05088347, Validation loss: 0.05126335, Gradient norm: 18.95747194
INFO:root:[  420] Training loss: 0.05048059, Validation loss: 0.05115510, Gradient norm: 11.47529913
INFO:root:[  421] Training loss: 0.05037759, Validation loss: 0.05059455, Gradient norm: 13.46878135
INFO:root:[  422] Training loss: 0.05031079, Validation loss: 0.05019321, Gradient norm: 9.02755502
INFO:root:[  423] Training loss: 0.05034143, Validation loss: 0.05015450, Gradient norm: 10.71587350
INFO:root:[  424] Training loss: 0.05028416, Validation loss: 0.05173451, Gradient norm: 11.87434354
INFO:root:[  425] Training loss: 0.05043034, Validation loss: 0.05029823, Gradient norm: 18.63847135
INFO:root:[  426] Training loss: 0.05050371, Validation loss: 0.05027024, Gradient norm: 16.80493078
INFO:root:[  427] Training loss: 0.05025670, Validation loss: 0.05002158, Gradient norm: 17.18704395
INFO:root:[  428] Training loss: 0.05045330, Validation loss: 0.04984397, Gradient norm: 22.14372899
INFO:root:[  429] Training loss: 0.05033185, Validation loss: 0.05010827, Gradient norm: 20.86476939
INFO:root:[  430] Training loss: 0.04974777, Validation loss: 0.04968077, Gradient norm: 11.78959390
INFO:root:[  431] Training loss: 0.04974678, Validation loss: 0.04988481, Gradient norm: 10.83407498
INFO:root:[  432] Training loss: 0.04968333, Validation loss: 0.05036609, Gradient norm: 13.26656108
INFO:root:[  433] Training loss: 0.04998917, Validation loss: 0.04957400, Gradient norm: 20.32673300
INFO:root:[  434] Training loss: 0.04965824, Validation loss: 0.05090923, Gradient norm: 14.74815179
INFO:root:[  435] Training loss: 0.04960979, Validation loss: 0.05197073, Gradient norm: 18.48058171
INFO:root:[  436] Training loss: 0.04990702, Validation loss: 0.05168497, Gradient norm: 23.31542047
INFO:root:[  437] Training loss: 0.04954483, Validation loss: 0.04928241, Gradient norm: 18.32623112
INFO:root:[  438] Training loss: 0.04944683, Validation loss: 0.04943409, Gradient norm: 13.31412811
INFO:root:[  439] Training loss: 0.04967353, Validation loss: 0.05192915, Gradient norm: 24.92699641
INFO:root:[  440] Training loss: 0.04932567, Validation loss: 0.04907362, Gradient norm: 18.72066053
INFO:root:[  441] Training loss: 0.04888520, Validation loss: 0.04923290, Gradient norm: 11.37187128
INFO:root:[  442] Training loss: 0.04926186, Validation loss: 0.04915921, Gradient norm: 20.56787252
INFO:root:[  443] Training loss: 0.04902218, Validation loss: 0.04900496, Gradient norm: 6.69720543
INFO:root:[  444] Training loss: 0.04921997, Validation loss: 0.04954482, Gradient norm: 19.50570760
INFO:root:[  445] Training loss: 0.04917308, Validation loss: 0.04961493, Gradient norm: 17.07722443
INFO:root:[  446] Training loss: 0.04917102, Validation loss: 0.04890076, Gradient norm: 17.99195373
INFO:root:[  447] Training loss: 0.04905767, Validation loss: 0.04869270, Gradient norm: 21.13059089
INFO:root:[  448] Training loss: 0.04884407, Validation loss: 0.04884345, Gradient norm: 18.22154370
INFO:root:[  449] Training loss: 0.04863591, Validation loss: 0.04835644, Gradient norm: 11.84566825
INFO:root:[  450] Training loss: 0.04870483, Validation loss: 0.04908842, Gradient norm: 20.78771471
INFO:root:[  451] Training loss: 0.04907634, Validation loss: 0.05085707, Gradient norm: 23.90263739
INFO:root:[  452] Training loss: 0.04887948, Validation loss: 0.04810301, Gradient norm: 27.08659638
INFO:root:[  453] Training loss: 0.04821335, Validation loss: 0.04863079, Gradient norm: 10.78690128
INFO:root:[  454] Training loss: 0.04839516, Validation loss: 0.04833253, Gradient norm: 16.44716476
INFO:root:[  455] Training loss: 0.04876137, Validation loss: 0.04834474, Gradient norm: 26.16111489
INFO:root:[  456] Training loss: 0.04872177, Validation loss: 0.05016413, Gradient norm: 24.43345697
INFO:root:[  457] Training loss: 0.04837057, Validation loss: 0.04791720, Gradient norm: 20.43735264
INFO:root:[  458] Training loss: 0.04823096, Validation loss: 0.04792847, Gradient norm: 15.67742936
INFO:root:[  459] Training loss: 0.04801840, Validation loss: 0.04802579, Gradient norm: 17.81212280
INFO:root:[  460] Training loss: 0.04892880, Validation loss: 0.05047689, Gradient norm: 35.38529542
INFO:root:[  461] Training loss: 0.04832931, Validation loss: 0.04821666, Gradient norm: 27.02577924
INFO:root:[  462] Training loss: 0.04808484, Validation loss: 0.04771590, Gradient norm: 23.27359019
INFO:root:[  463] Training loss: 0.04784284, Validation loss: 0.04783902, Gradient norm: 19.28774745
INFO:root:[  464] Training loss: 0.04752547, Validation loss: 0.04853337, Gradient norm: 12.11402048
INFO:root:[  465] Training loss: 0.04797652, Validation loss: 0.04850787, Gradient norm: 24.21628290
INFO:root:[  466] Training loss: 0.04783865, Validation loss: 0.04728244, Gradient norm: 23.23660182
INFO:root:[  467] Training loss: 0.04807835, Validation loss: 0.04947136, Gradient norm: 24.82384165
INFO:root:[  468] Training loss: 0.04820927, Validation loss: 0.04736067, Gradient norm: 32.28514790
INFO:root:[  469] Training loss: 0.04762845, Validation loss: 0.04717370, Gradient norm: 19.95047074
INFO:root:[  470] Training loss: 0.04757298, Validation loss: 0.04870769, Gradient norm: 24.59113543
INFO:root:[  471] Training loss: 0.04762714, Validation loss: 0.04751347, Gradient norm: 21.61802839
INFO:root:[  472] Training loss: 0.04730857, Validation loss: 0.04718729, Gradient norm: 21.83396314
INFO:root:[  473] Training loss: 0.04721144, Validation loss: 0.04735299, Gradient norm: 13.31079789
INFO:root:[  474] Training loss: 0.04761874, Validation loss: 0.05017612, Gradient norm: 26.52382317
INFO:root:[  475] Training loss: 0.04813704, Validation loss: 0.04692299, Gradient norm: 36.07507759
INFO:root:[  476] Training loss: 0.04718796, Validation loss: 0.04681895, Gradient norm: 20.32573581
INFO:root:[  477] Training loss: 0.04687889, Validation loss: 0.04740488, Gradient norm: 16.43861576
INFO:root:[  478] Training loss: 0.04741724, Validation loss: 0.04967667, Gradient norm: 28.10045650
INFO:root:[  479] Training loss: 0.04745362, Validation loss: 0.04672193, Gradient norm: 26.75215765
INFO:root:[  480] Training loss: 0.04703685, Validation loss: 0.04828857, Gradient norm: 25.80442126
INFO:root:[  481] Training loss: 0.04694512, Validation loss: 0.04687639, Gradient norm: 21.46597140
INFO:root:[  482] Training loss: 0.04693319, Validation loss: 0.04902601, Gradient norm: 19.51648303
INFO:root:[  483] Training loss: 0.04784904, Validation loss: 0.04657770, Gradient norm: 40.97432535
INFO:root:[  484] Training loss: 0.04674275, Validation loss: 0.04905121, Gradient norm: 19.72900714
INFO:root:[  485] Training loss: 0.04720957, Validation loss: 0.04670595, Gradient norm: 31.69238707
INFO:root:[  486] Training loss: 0.04652409, Validation loss: 0.04639682, Gradient norm: 22.60920473
INFO:root:[  487] Training loss: 0.04644883, Validation loss: 0.04650208, Gradient norm: 16.69593069
INFO:root:[  488] Training loss: 0.04700034, Validation loss: 0.04703817, Gradient norm: 26.52536547
INFO:root:[  489] Training loss: 0.04644429, Validation loss: 0.04687062, Gradient norm: 18.32151173
INFO:root:[  490] Training loss: 0.04695602, Validation loss: 0.04608939, Gradient norm: 32.35294488
INFO:root:[  491] Training loss: 0.04660082, Validation loss: 0.04611535, Gradient norm: 27.66293771
INFO:root:[  492] Training loss: 0.04669445, Validation loss: 0.04640509, Gradient norm: 31.45022527
INFO:root:[  493] Training loss: 0.04675631, Validation loss: 0.04616654, Gradient norm: 30.75749498
INFO:root:[  494] Training loss: 0.04606871, Validation loss: 0.04662669, Gradient norm: 16.07727794
INFO:root:[  495] Training loss: 0.04726050, Validation loss: 0.04824062, Gradient norm: 40.26488647
INFO:root:[  496] Training loss: 0.04623209, Validation loss: 0.04684821, Gradient norm: 24.56930783
INFO:root:[  497] Training loss: 0.04654932, Validation loss: 0.04592345, Gradient norm: 32.72343630
INFO:root:[  498] Training loss: 0.04603773, Validation loss: 0.04665868, Gradient norm: 21.25190677
INFO:root:[  499] Training loss: 0.04688657, Validation loss: 0.04582842, Gradient norm: 37.00697706
INFO:root:[  500] Training loss: 0.04584741, Validation loss: 0.04581842, Gradient norm: 13.89247297
INFO:root:[  501] Training loss: 0.04594107, Validation loss: 0.04619939, Gradient norm: 23.78002967
INFO:root:[  502] Training loss: 0.04639437, Validation loss: 0.04559474, Gradient norm: 36.56409971
INFO:root:[  503] Training loss: 0.04584582, Validation loss: 0.04610248, Gradient norm: 19.51686046
INFO:root:[  504] Training loss: 0.04595544, Validation loss: 0.04826383, Gradient norm: 28.90285512
INFO:root:[  505] Training loss: 0.04631714, Validation loss: 0.04789636, Gradient norm: 36.73743076
INFO:root:[  506] Training loss: 0.04609816, Validation loss: 0.04548683, Gradient norm: 36.73903797
INFO:root:[  507] Training loss: 0.04559724, Validation loss: 0.04616496, Gradient norm: 14.59619046
INFO:root:[  508] Training loss: 0.04543374, Validation loss: 0.04554524, Gradient norm: 20.41663008
INFO:root:[  509] Training loss: 0.04693107, Validation loss: 0.04804067, Gradient norm: 50.86323691
INFO:root:[  510] Training loss: 0.04555620, Validation loss: 0.04606288, Gradient norm: 31.05480788
INFO:root:[  511] Training loss: 0.04689977, Validation loss: 0.04565655, Gradient norm: 51.63748155
INFO:root:[  512] Training loss: 0.04542696, Validation loss: 0.04526959, Gradient norm: 25.36536344
INFO:root:[  513] Training loss: 0.04552501, Validation loss: 0.04542697, Gradient norm: 25.26443280
INFO:root:[  514] Training loss: 0.04542894, Validation loss: 0.04621927, Gradient norm: 28.14231709
INFO:root:[  515] Training loss: 0.04513864, Validation loss: 0.04511087, Gradient norm: 26.59807714
INFO:root:[  516] Training loss: 0.04522521, Validation loss: 0.04505108, Gradient norm: 26.79284054
INFO:root:[  517] Training loss: 0.04520926, Validation loss: 0.04496457, Gradient norm: 19.13971243
INFO:root:[  518] Training loss: 0.04566832, Validation loss: 0.04525929, Gradient norm: 35.36960462
INFO:root:[  519] Training loss: 0.04486419, Validation loss: 0.04528533, Gradient norm: 15.74546763
INFO:root:[  520] Training loss: 0.04553837, Validation loss: 0.04750266, Gradient norm: 34.08171220
INFO:root:[  521] Training loss: 0.04544077, Validation loss: 0.04475642, Gradient norm: 32.61733984
INFO:root:[  522] Training loss: 0.04533762, Validation loss: 0.04545843, Gradient norm: 28.33075685
INFO:root:[  523] Training loss: 0.04673851, Validation loss: 0.04524610, Gradient norm: 58.88093714
INFO:root:[  524] Training loss: 0.04470143, Validation loss: 0.04472605, Gradient norm: 20.40407232
INFO:root:[  525] Training loss: 0.04475095, Validation loss: 0.04590026, Gradient norm: 21.33255727
INFO:root:[  526] Training loss: 0.04523741, Validation loss: 0.04471294, Gradient norm: 38.60529303
INFO:root:[  527] Training loss: 0.04460134, Validation loss: 0.04553171, Gradient norm: 25.91633803
INFO:root:[  528] Training loss: 0.04568398, Validation loss: 0.04531338, Gradient norm: 46.51147651
INFO:root:[  529] Training loss: 0.04455080, Validation loss: 0.04476834, Gradient norm: 20.25092117
INFO:root:[  530] Training loss: 0.04476677, Validation loss: 0.04762492, Gradient norm: 32.73615264
INFO:root:[  531] Training loss: 0.04504343, Validation loss: 0.04660985, Gradient norm: 40.10065440
INFO:root:[  532] Training loss: 0.04538456, Validation loss: 0.04454613, Gradient norm: 43.31989969
INFO:root:[  533] Training loss: 0.04443442, Validation loss: 0.04420625, Gradient norm: 23.63292387
INFO:root:[  534] Training loss: 0.04440232, Validation loss: 0.04562985, Gradient norm: 28.41901569
INFO:root:[  535] Training loss: 0.04473169, Validation loss: 0.04409190, Gradient norm: 35.65721496
INFO:root:[  536] Training loss: 0.04466184, Validation loss: 0.05305574, Gradient norm: 24.28458423
INFO:root:[  537] Training loss: 0.04688567, Validation loss: 0.04447661, Gradient norm: 70.38063598
INFO:root:[  538] Training loss: 0.04411469, Validation loss: 0.04436603, Gradient norm: 21.72034402
INFO:root:[  539] Training loss: 0.04409484, Validation loss: 0.04604817, Gradient norm: 19.31669035
INFO:root:[  540] Training loss: 0.04479457, Validation loss: 0.04387665, Gradient norm: 37.67469325
INFO:root:[  541] Training loss: 0.04427479, Validation loss: 0.04397762, Gradient norm: 31.77483890
INFO:root:[  542] Training loss: 0.04426788, Validation loss: 0.04623393, Gradient norm: 33.61261333
INFO:root:[  543] Training loss: 0.04488543, Validation loss: 0.04705895, Gradient norm: 45.09622101
INFO:root:[  544] Training loss: 0.04453007, Validation loss: 0.04498506, Gradient norm: 39.10781667
INFO:root:[  545] Training loss: 0.04397813, Validation loss: 0.04552410, Gradient norm: 27.89837556
INFO:root:[  546] Training loss: 0.04408934, Validation loss: 0.04449757, Gradient norm: 31.55295619
INFO:root:[  547] Training loss: 0.04453710, Validation loss: 0.04429778, Gradient norm: 41.90381636
INFO:root:[  548] Training loss: 0.04400231, Validation loss: 0.04375028, Gradient norm: 29.30180131
INFO:root:[  549] Training loss: 0.04433251, Validation loss: 0.04491140, Gradient norm: 38.40468549
INFO:root:[  550] Training loss: 0.04462449, Validation loss: 0.04329419, Gradient norm: 44.27918721
INFO:root:[  551] Training loss: 0.04367943, Validation loss: 0.04362670, Gradient norm: 21.16558368
INFO:root:[  552] Training loss: 0.04423932, Validation loss: 0.04495803, Gradient norm: 42.50133464
INFO:root:[  553] Training loss: 0.04469138, Validation loss: 0.04334372, Gradient norm: 53.65991306
INFO:root:[  554] Training loss: 0.04401652, Validation loss: 0.04352400, Gradient norm: 35.44035004
INFO:root:[  555] Training loss: 0.04373173, Validation loss: 0.04466067, Gradient norm: 31.16658774
INFO:root:[  556] Training loss: 0.04385363, Validation loss: 0.04469729, Gradient norm: 34.65813174
INFO:root:[  557] Training loss: 0.04455992, Validation loss: 0.04454860, Gradient norm: 51.68470460
INFO:root:[  558] Training loss: 0.04370702, Validation loss: 0.04487687, Gradient norm: 35.94518765
INFO:root:[  559] Training loss: 0.04355553, Validation loss: 0.04401001, Gradient norm: 35.39852517
INFO:root:EP 559: Early stopping
INFO:root:Training the model took 25831.161s.
INFO:root:Emptying the cuda cache took 0.11s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.8649
INFO:root:EnergyScoreTrain: 1.3625
INFO:root:CoverageTrain: 0.98436
INFO:root:IntervalWidthTrain: 0.09719
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.79113
INFO:root:EnergyScoreValidation: 1.30843
INFO:root:CoverageValidation: 0.98437
INFO:root:IntervalWidthValidation: 0.0973
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.47742
INFO:root:EnergyScoreTest: 1.07917
INFO:root:CoverageTest: 0.98419
INFO:root:IntervalWidthTest: 0.09658
INFO:root:###6 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 2134900736
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.47843687, Validation loss: 0.72729064, Gradient norm: 9.58040574
INFO:root:[    2] Training loss: 0.60441390, Validation loss: 0.56085438, Gradient norm: 2.26450826
INFO:root:[    3] Training loss: 0.53492875, Validation loss: 0.50520844, Gradient norm: 1.82356112
INFO:root:[    4] Training loss: 0.49726150, Validation loss: 0.48161052, Gradient norm: 2.25107940
INFO:root:[    5] Training loss: 0.46309392, Validation loss: 0.44880343, Gradient norm: 1.61088421
INFO:root:[    6] Training loss: 0.43341734, Validation loss: 0.43265733, Gradient norm: 1.33825252
INFO:root:[    7] Training loss: 0.40432623, Validation loss: 0.40748129, Gradient norm: 1.51768952
INFO:root:[    8] Training loss: 0.39733470, Validation loss: 0.38354484, Gradient norm: 1.78345103
INFO:root:[    9] Training loss: 0.39693073, Validation loss: 0.38022824, Gradient norm: 2.40006533
INFO:root:[   10] Training loss: 0.37951297, Validation loss: 0.37142169, Gradient norm: 1.65416612
INFO:root:[   11] Training loss: 0.36616539, Validation loss: 0.35664356, Gradient norm: 1.79987553
INFO:root:[   12] Training loss: 0.36056190, Validation loss: 0.36885446, Gradient norm: 2.31141920
INFO:root:[   13] Training loss: 0.35637735, Validation loss: 0.34718646, Gradient norm: 2.13555668
INFO:root:[   14] Training loss: 0.35224324, Validation loss: 0.35136236, Gradient norm: 2.02802329
INFO:root:[   15] Training loss: 0.34182295, Validation loss: 0.33471278, Gradient norm: 2.10968201
INFO:root:[   16] Training loss: 0.32655589, Validation loss: 0.33214196, Gradient norm: 2.15653177
INFO:root:[   17] Training loss: 0.32766686, Validation loss: 0.32810716, Gradient norm: 2.15871235
INFO:root:[   18] Training loss: 0.31454047, Validation loss: 0.30241394, Gradient norm: 1.56928964
INFO:root:[   19] Training loss: 0.30328113, Validation loss: 0.30487080, Gradient norm: 1.83976145
INFO:root:[   20] Training loss: 0.29696380, Validation loss: 0.28961840, Gradient norm: 2.06552857
INFO:root:[   21] Training loss: 0.29511778, Validation loss: 0.29107951, Gradient norm: 2.15525234
INFO:root:[   22] Training loss: 0.28854278, Validation loss: 0.29278507, Gradient norm: 2.19513010
INFO:root:[   23] Training loss: 0.28355089, Validation loss: 0.27412449, Gradient norm: 2.35543680
INFO:root:[   24] Training loss: 0.27777387, Validation loss: 0.28277279, Gradient norm: 2.48119409
INFO:root:[   25] Training loss: 0.27437970, Validation loss: 0.27032977, Gradient norm: 2.23202302
INFO:root:[   26] Training loss: 0.27273340, Validation loss: 0.27121152, Gradient norm: 2.08356251
INFO:root:[   27] Training loss: 0.26549295, Validation loss: 0.26649750, Gradient norm: 1.76812278
INFO:root:[   28] Training loss: 0.26422767, Validation loss: 0.26179300, Gradient norm: 2.08307601
INFO:root:[   29] Training loss: 0.26116618, Validation loss: 0.25676583, Gradient norm: 3.13415046
INFO:root:[   30] Training loss: 0.25914547, Validation loss: 0.25707836, Gradient norm: 2.60950105
INFO:root:[   31] Training loss: 0.25460291, Validation loss: 0.25033898, Gradient norm: 2.66589612
INFO:root:[   32] Training loss: 0.25019577, Validation loss: 0.24693522, Gradient norm: 1.68048192
INFO:root:[   33] Training loss: 0.24756136, Validation loss: 0.24677516, Gradient norm: 2.46996010
INFO:root:[   34] Training loss: 0.24594819, Validation loss: 0.25177972, Gradient norm: 2.30059051
INFO:root:[   35] Training loss: 0.24634163, Validation loss: 0.24538724, Gradient norm: 4.25131487
INFO:root:[   36] Training loss: 0.24857783, Validation loss: 0.24027741, Gradient norm: 5.52372368
INFO:root:[   37] Training loss: 0.24021022, Validation loss: 0.23729755, Gradient norm: 2.91585216
INFO:root:[   38] Training loss: 0.23911015, Validation loss: 0.23834890, Gradient norm: 4.37028541
INFO:root:[   39] Training loss: 0.23695675, Validation loss: 0.23410823, Gradient norm: 3.07870838
INFO:root:[   40] Training loss: 0.23592172, Validation loss: 0.23393485, Gradient norm: 4.99589999
INFO:root:[   41] Training loss: 0.23455847, Validation loss: 0.23110825, Gradient norm: 3.14575466
INFO:root:[   42] Training loss: 0.23230790, Validation loss: 0.22955018, Gradient norm: 3.50974666
INFO:root:[   43] Training loss: 0.22985545, Validation loss: 0.23063830, Gradient norm: 2.27742895
INFO:root:[   44] Training loss: 0.22833934, Validation loss: 0.22988104, Gradient norm: 3.35574241
INFO:root:[   45] Training loss: 0.22664883, Validation loss: 0.23008330, Gradient norm: 3.02742128
INFO:root:[   46] Training loss: 0.22494440, Validation loss: 0.22203688, Gradient norm: 3.41783383
INFO:root:[   47] Training loss: 0.22597203, Validation loss: 0.22454819, Gradient norm: 3.82311240
INFO:root:[   48] Training loss: 0.22622707, Validation loss: 0.22036772, Gradient norm: 5.46186021
INFO:root:[   49] Training loss: 0.22300559, Validation loss: 0.21811514, Gradient norm: 3.78975619
INFO:root:[   50] Training loss: 0.22375560, Validation loss: 0.22382548, Gradient norm: 3.89371900
INFO:root:[   51] Training loss: 0.22138308, Validation loss: 0.21934738, Gradient norm: 4.23657589
INFO:root:[   52] Training loss: 0.21798867, Validation loss: 0.21574452, Gradient norm: 2.50533326
INFO:root:[   53] Training loss: 0.21815797, Validation loss: 0.21583356, Gradient norm: 3.53673271
INFO:root:[   54] Training loss: 0.21477755, Validation loss: 0.21477252, Gradient norm: 2.17119221
INFO:root:[   55] Training loss: 0.21492391, Validation loss: 0.21499247, Gradient norm: 2.94198624
INFO:root:[   56] Training loss: 0.21250032, Validation loss: 0.21233597, Gradient norm: 4.04487699
INFO:root:[   57] Training loss: 0.21210093, Validation loss: 0.20906676, Gradient norm: 3.73933762
INFO:root:[   58] Training loss: 0.21089927, Validation loss: 0.21013730, Gradient norm: 3.37733035
INFO:root:[   59] Training loss: 0.20952221, Validation loss: 0.20624853, Gradient norm: 3.23318496
INFO:root:[   60] Training loss: 0.20822219, Validation loss: 0.20865133, Gradient norm: 3.43045448
INFO:root:[   61] Training loss: 0.20767896, Validation loss: 0.20455058, Gradient norm: 3.89596978
INFO:root:[   62] Training loss: 0.20578244, Validation loss: 0.20389333, Gradient norm: 4.23742519
INFO:root:[   63] Training loss: 0.20410134, Validation loss: 0.20620831, Gradient norm: 4.87204216
INFO:root:[   64] Training loss: 0.20555369, Validation loss: 0.20129817, Gradient norm: 4.52422289
INFO:root:[   65] Training loss: 0.20285080, Validation loss: 0.20190078, Gradient norm: 6.31380942
INFO:root:[   66] Training loss: 0.20018142, Validation loss: 0.20009144, Gradient norm: 4.22003619
INFO:root:[   67] Training loss: 0.19918756, Validation loss: 0.19949628, Gradient norm: 4.87250274
INFO:root:[   68] Training loss: 0.19713887, Validation loss: 0.20172208, Gradient norm: 3.24830319
INFO:root:[   69] Training loss: 0.19669205, Validation loss: 0.19745158, Gradient norm: 5.33312970
INFO:root:[   70] Training loss: 0.19574481, Validation loss: 0.19657579, Gradient norm: 4.60300870
INFO:root:[   71] Training loss: 0.19551261, Validation loss: 0.19532668, Gradient norm: 6.71990666
INFO:root:[   72] Training loss: 0.19381831, Validation loss: 0.19223834, Gradient norm: 4.25575430
INFO:root:[   73] Training loss: 0.19144789, Validation loss: 0.19126531, Gradient norm: 2.37719755
INFO:root:[   74] Training loss: 0.19097530, Validation loss: 0.19275614, Gradient norm: 5.20495066
INFO:root:[   75] Training loss: 0.18946143, Validation loss: 0.18806514, Gradient norm: 4.71042599
INFO:root:[   76] Training loss: 0.18941156, Validation loss: 0.18952466, Gradient norm: 6.63026017
INFO:root:[   77] Training loss: 0.18816981, Validation loss: 0.18541905, Gradient norm: 4.77414947
INFO:root:[   78] Training loss: 0.18697759, Validation loss: 0.19034238, Gradient norm: 6.15392006
INFO:root:[   79] Training loss: 0.18578287, Validation loss: 0.18492679, Gradient norm: 5.78159457
INFO:root:[   80] Training loss: 0.18379635, Validation loss: 0.18393154, Gradient norm: 4.06209721
INFO:root:[   81] Training loss: 0.18553413, Validation loss: 0.18537222, Gradient norm: 8.23382401
INFO:root:[   82] Training loss: 0.18307679, Validation loss: 0.18329370, Gradient norm: 6.25099998
INFO:root:[   83] Training loss: 0.18211189, Validation loss: 0.18245353, Gradient norm: 5.69637767
INFO:root:[   84] Training loss: 0.18131951, Validation loss: 0.18056693, Gradient norm: 6.64080389
INFO:root:[   85] Training loss: 0.18056874, Validation loss: 0.17966557, Gradient norm: 8.63493398
INFO:root:[   86] Training loss: 0.17993188, Validation loss: 0.17908903, Gradient norm: 5.23164819
INFO:root:[   87] Training loss: 0.18098172, Validation loss: 0.18297011, Gradient norm: 11.28637065
INFO:root:[   88] Training loss: 0.17933844, Validation loss: 0.18276819, Gradient norm: 10.59144331
INFO:root:[   89] Training loss: 0.17836969, Validation loss: 0.17697289, Gradient norm: 9.95561752
INFO:root:[   90] Training loss: 0.17639545, Validation loss: 0.17768535, Gradient norm: 7.35375622
INFO:root:[   91] Training loss: 0.17606888, Validation loss: 0.17649522, Gradient norm: 7.51352185
INFO:root:[   92] Training loss: 0.17567647, Validation loss: 0.17564143, Gradient norm: 11.15819126
INFO:root:[   93] Training loss: 0.17595570, Validation loss: 0.17602128, Gradient norm: 7.99714975
INFO:root:[   94] Training loss: 0.17428746, Validation loss: 0.17159216, Gradient norm: 9.59075095
INFO:root:[   95] Training loss: 0.17329011, Validation loss: 0.17194313, Gradient norm: 9.16137614
INFO:root:[   96] Training loss: 0.17328565, Validation loss: 0.17107252, Gradient norm: 10.50718454
INFO:root:[   97] Training loss: 0.17187710, Validation loss: 0.17540845, Gradient norm: 11.92593147
INFO:root:[   98] Training loss: 0.17216986, Validation loss: 0.17049549, Gradient norm: 16.84293859
INFO:root:[   99] Training loss: 0.17144480, Validation loss: 0.17125337, Gradient norm: 10.75758771
INFO:root:[  100] Training loss: 0.17359175, Validation loss: 0.16830104, Gradient norm: 19.62644712
INFO:root:[  101] Training loss: 0.16898756, Validation loss: 0.16981348, Gradient norm: 9.32885306
INFO:root:[  102] Training loss: 0.16955908, Validation loss: 0.16726800, Gradient norm: 11.16260894
INFO:root:[  103] Training loss: 0.16811024, Validation loss: 0.16619029, Gradient norm: 13.18330060
INFO:root:[  104] Training loss: 0.16832994, Validation loss: 0.16861712, Gradient norm: 12.80922833
INFO:root:[  105] Training loss: 0.16727405, Validation loss: 0.17125231, Gradient norm: 11.91233280
INFO:root:[  106] Training loss: 0.16724440, Validation loss: 0.16686132, Gradient norm: 15.13209677
INFO:root:[  107] Training loss: 0.16685825, Validation loss: 0.16804854, Gradient norm: 18.04496738
INFO:root:[  108] Training loss: 0.16574986, Validation loss: 0.17051585, Gradient norm: 15.06404716
INFO:root:[  109] Training loss: 0.16501031, Validation loss: 0.16504159, Gradient norm: 16.94147339
INFO:root:[  110] Training loss: 0.16435417, Validation loss: 0.16342587, Gradient norm: 12.99783398
INFO:root:[  111] Training loss: 0.16275877, Validation loss: 0.16487118, Gradient norm: 12.91399333
INFO:root:[  112] Training loss: 0.16593382, Validation loss: 0.16218338, Gradient norm: 28.00809167
INFO:root:[  113] Training loss: 0.16213791, Validation loss: 0.16704980, Gradient norm: 13.03013246
INFO:root:[  114] Training loss: 0.16293227, Validation loss: 0.16594594, Gradient norm: 19.79790877
INFO:root:[  115] Training loss: 0.16306470, Validation loss: 0.16383790, Gradient norm: 24.71274581
INFO:root:[  116] Training loss: 0.16307006, Validation loss: 0.15993579, Gradient norm: 31.14158766
INFO:root:[  117] Training loss: 0.15948971, Validation loss: 0.16096447, Gradient norm: 12.75244245
INFO:root:[  118] Training loss: 0.16056993, Validation loss: 0.16379162, Gradient norm: 21.87361312
INFO:root:[  119] Training loss: 0.15947135, Validation loss: 0.16046085, Gradient norm: 19.73093728
INFO:root:[  120] Training loss: 0.15949964, Validation loss: 0.15844112, Gradient norm: 22.01642650
INFO:root:[  121] Training loss: 0.15830866, Validation loss: 0.15736784, Gradient norm: 13.24580804
INFO:root:[  122] Training loss: 0.15865105, Validation loss: 0.15735997, Gradient norm: 25.87624936
INFO:root:[  123] Training loss: 0.15811115, Validation loss: 0.15701300, Gradient norm: 23.80731190
INFO:root:[  124] Training loss: 0.15632324, Validation loss: 0.15597305, Gradient norm: 17.43565101
INFO:root:[  125] Training loss: 0.15720525, Validation loss: 0.15527703, Gradient norm: 27.75694829
INFO:root:[  126] Training loss: 0.15644703, Validation loss: 0.15497502, Gradient norm: 23.94979732
INFO:root:[  127] Training loss: 0.15714440, Validation loss: 0.15443521, Gradient norm: 22.91036611
INFO:root:[  128] Training loss: 0.15491217, Validation loss: 0.15447876, Gradient norm: 20.59211935
INFO:root:[  129] Training loss: 0.15482314, Validation loss: 0.15276967, Gradient norm: 27.49540181
INFO:root:[  130] Training loss: 0.15535990, Validation loss: 0.15321830, Gradient norm: 37.16873659
INFO:root:[  131] Training loss: 0.15541646, Validation loss: 0.15472827, Gradient norm: 36.82876114
INFO:root:[  132] Training loss: 0.15494865, Validation loss: 0.15267753, Gradient norm: 31.68925443
INFO:root:[  133] Training loss: 0.15240743, Validation loss: 0.15606130, Gradient norm: 17.36835279
INFO:root:[  134] Training loss: 0.15507162, Validation loss: 0.15122081, Gradient norm: 48.47026281
INFO:root:[  135] Training loss: 0.15383313, Validation loss: 0.15303119, Gradient norm: 40.04868569
INFO:root:[  136] Training loss: 0.15086944, Validation loss: 0.15433805, Gradient norm: 24.18473565
INFO:root:[  137] Training loss: 0.15159857, Validation loss: 0.15018276, Gradient norm: 34.63673485
INFO:root:[  138] Training loss: 0.15207933, Validation loss: 0.15011803, Gradient norm: 34.32123474
INFO:root:[  139] Training loss: 0.15152697, Validation loss: 0.15334276, Gradient norm: 40.84006753
INFO:root:[  140] Training loss: 0.15044094, Validation loss: 0.14932673, Gradient norm: 36.12174723
INFO:root:[  141] Training loss: 0.14956472, Validation loss: 0.14805186, Gradient norm: 26.34125702
INFO:root:[  142] Training loss: 0.14926755, Validation loss: 0.14981932, Gradient norm: 33.71722742
INFO:root:[  143] Training loss: 0.14939041, Validation loss: 0.15048888, Gradient norm: 37.27192640
INFO:root:[  144] Training loss: 0.14790618, Validation loss: 0.14712734, Gradient norm: 35.23212554
INFO:root:[  145] Training loss: 0.15068503, Validation loss: 0.14781082, Gradient norm: 61.44723917
INFO:root:[  146] Training loss: 0.14756129, Validation loss: 0.14582614, Gradient norm: 40.41443951
INFO:root:[  147] Training loss: 0.14643775, Validation loss: 0.14695499, Gradient norm: 34.32975174
INFO:root:[  148] Training loss: 0.14676372, Validation loss: 0.14617039, Gradient norm: 45.64482556
INFO:root:[  149] Training loss: 0.14609440, Validation loss: 0.14523204, Gradient norm: 44.65185611
INFO:root:[  150] Training loss: 0.14633833, Validation loss: 0.15346965, Gradient norm: 45.79251273
INFO:root:[  151] Training loss: 0.14567141, Validation loss: 0.14398685, Gradient norm: 38.33220621
INFO:root:[  152] Training loss: 0.14421155, Validation loss: 0.14445343, Gradient norm: 34.18165484
INFO:root:[  153] Training loss: 0.14701534, Validation loss: 0.14235735, Gradient norm: 69.82655470
INFO:root:[  154] Training loss: 0.14342588, Validation loss: 0.16183359, Gradient norm: 32.75349848
INFO:root:[  155] Training loss: 0.14825364, Validation loss: 0.14305994, Gradient norm: 87.72292262
INFO:root:[  156] Training loss: 0.14216990, Validation loss: 0.14186018, Gradient norm: 38.75136648
INFO:root:[  157] Training loss: 0.14506649, Validation loss: 0.14402665, Gradient norm: 69.28187293
INFO:root:[  158] Training loss: 0.14256958, Validation loss: 0.14088361, Gradient norm: 51.35749740
INFO:root:[  159] Training loss: 0.14274504, Validation loss: 0.14108076, Gradient norm: 48.12957487
INFO:root:[  160] Training loss: 0.14193007, Validation loss: 0.14197877, Gradient norm: 46.76102350
INFO:root:[  161] Training loss: 0.14095410, Validation loss: 0.14089011, Gradient norm: 51.13879736
INFO:root:[  162] Training loss: 0.14380911, Validation loss: 0.14912454, Gradient norm: 78.01864885
INFO:root:[  163] Training loss: 0.14334351, Validation loss: 0.14256921, Gradient norm: 82.28463716
INFO:root:[  164] Training loss: 0.14225753, Validation loss: 0.14232622, Gradient norm: 71.04128347
INFO:root:[  165] Training loss: 0.13985604, Validation loss: 0.13896007, Gradient norm: 52.78321719
INFO:root:[  166] Training loss: 0.14204969, Validation loss: 0.16754286, Gradient norm: 78.75220346
INFO:root:[  167] Training loss: 0.14193175, Validation loss: 0.14124453, Gradient norm: 78.89559452
INFO:root:[  168] Training loss: 0.13894750, Validation loss: 0.13702612, Gradient norm: 44.42322989
INFO:root:[  169] Training loss: 0.13971937, Validation loss: 0.14702951, Gradient norm: 64.72974714
INFO:root:[  170] Training loss: 0.14076159, Validation loss: 0.13762460, Gradient norm: 84.01820939
INFO:root:[  171] Training loss: 0.13755674, Validation loss: 0.13772603, Gradient norm: 39.28254839
INFO:root:[  172] Training loss: 0.13779863, Validation loss: 0.13803005, Gradient norm: 53.84505465
INFO:root:[  173] Training loss: 0.13706942, Validation loss: 0.13760870, Gradient norm: 54.74414337
INFO:root:[  174] Training loss: 0.13830264, Validation loss: 0.13540228, Gradient norm: 79.38074851
INFO:root:[  175] Training loss: 0.13813622, Validation loss: 0.14262056, Gradient norm: 81.46129656
INFO:root:[  176] Training loss: 0.14036284, Validation loss: 0.14572638, Gradient norm: 105.72014252
INFO:root:[  177] Training loss: 0.13754956, Validation loss: 0.13633944, Gradient norm: 75.67138291
INFO:root:[  178] Training loss: 0.13697740, Validation loss: 0.13342119, Gradient norm: 75.38784540
INFO:root:[  179] Training loss: 0.13799869, Validation loss: 0.13657033, Gradient norm: 92.45639118
INFO:root:[  180] Training loss: 0.13638841, Validation loss: 0.14193764, Gradient norm: 79.29143573
INFO:root:[  181] Training loss: 0.13772028, Validation loss: 0.13661871, Gradient norm: 108.47113364
INFO:root:[  182] Training loss: 0.13522439, Validation loss: 0.13386132, Gradient norm: 78.98305933
INFO:root:[  183] Training loss: 0.13469148, Validation loss: 0.13784572, Gradient norm: 69.42761618
INFO:root:[  184] Training loss: 0.13372734, Validation loss: 0.13234174, Gradient norm: 60.90278555
INFO:root:[  185] Training loss: 0.13543024, Validation loss: 0.13249348, Gradient norm: 93.84757793
INFO:root:[  186] Training loss: 0.13361173, Validation loss: 0.13239585, Gradient norm: 60.67081664
INFO:root:[  187] Training loss: 0.13235810, Validation loss: 0.13242579, Gradient norm: 49.44996146
INFO:root:[  188] Training loss: 0.13310487, Validation loss: 0.13266781, Gradient norm: 67.52390013
INFO:root:[  189] Training loss: 0.13376109, Validation loss: 0.14161743, Gradient norm: 91.89226459
INFO:root:[  190] Training loss: 0.13647948, Validation loss: 0.13032798, Gradient norm: 127.33511510
INFO:root:[  191] Training loss: 0.13186863, Validation loss: 0.13148768, Gradient norm: 59.01310452
INFO:root:[  192] Training loss: 0.13267934, Validation loss: 0.13084265, Gradient norm: 90.84847580
INFO:root:[  193] Training loss: 0.13151107, Validation loss: 0.13125246, Gradient norm: 59.93381640
INFO:root:[  194] Training loss: 0.13375205, Validation loss: 0.14777691, Gradient norm: 105.23473987
INFO:root:[  195] Training loss: 0.13163958, Validation loss: 0.13431491, Gradient norm: 81.48773117
INFO:root:[  196] Training loss: 0.13176731, Validation loss: 0.13000679, Gradient norm: 93.52451520
INFO:root:[  197] Training loss: 0.13175490, Validation loss: 0.12887906, Gradient norm: 87.88030998
INFO:root:[  198] Training loss: 0.13145099, Validation loss: 0.13087547, Gradient norm: 88.13638967
INFO:root:[  199] Training loss: 0.13059569, Validation loss: 0.12945274, Gradient norm: 83.25062205
INFO:root:[  200] Training loss: 0.13188287, Validation loss: 0.14854465, Gradient norm: 108.03056235
INFO:root:[  201] Training loss: 0.13354451, Validation loss: 0.12761804, Gradient norm: 139.20749454
INFO:root:[  202] Training loss: 0.12857985, Validation loss: 0.12760544, Gradient norm: 60.49957413
INFO:root:[  203] Training loss: 0.12870660, Validation loss: 0.13600517, Gradient norm: 70.15655374
INFO:root:[  204] Training loss: 0.13064009, Validation loss: 0.12719984, Gradient norm: 117.41533202
INFO:root:[  205] Training loss: 0.13141195, Validation loss: 0.12915430, Gradient norm: 133.52395740
INFO:root:[  206] Training loss: 0.12945260, Validation loss: 0.12661674, Gradient norm: 99.30287763
INFO:root:[  207] Training loss: 0.12897372, Validation loss: 0.12751815, Gradient norm: 87.77035414
INFO:root:[  208] Training loss: 0.12879998, Validation loss: 0.12941284, Gradient norm: 107.90610899
INFO:root:[  209] Training loss: 0.12942879, Validation loss: 0.12655436, Gradient norm: 113.83553614
INFO:root:[  210] Training loss: 0.12965207, Validation loss: 0.12580149, Gradient norm: 135.33547274
INFO:root:[  211] Training loss: 0.12961628, Validation loss: 0.12598850, Gradient norm: 128.41639668
INFO:root:[  212] Training loss: 0.12739340, Validation loss: 0.13141067, Gradient norm: 93.97603715
INFO:root:[  213] Training loss: 0.12746192, Validation loss: 0.13055700, Gradient norm: 108.99072100
INFO:root:[  214] Training loss: 0.12622532, Validation loss: 0.12717834, Gradient norm: 97.40209128
INFO:root:[  215] Training loss: 0.12794142, Validation loss: 0.12555617, Gradient norm: 123.27265295
INFO:root:[  216] Training loss: 0.12688691, Validation loss: 0.12610350, Gradient norm: 103.40631229
INFO:root:[  217] Training loss: 0.12527696, Validation loss: 0.12573471, Gradient norm: 67.80841475
INFO:root:[  218] Training loss: 0.12811539, Validation loss: 0.13185064, Gradient norm: 141.70889836
INFO:root:[  219] Training loss: 0.12600548, Validation loss: 0.12624350, Gradient norm: 91.15104903
INFO:root:[  220] Training loss: 0.12518089, Validation loss: 0.12533621, Gradient norm: 93.84849179
INFO:root:[  221] Training loss: 0.12671682, Validation loss: 0.12531722, Gradient norm: 135.10005471
INFO:root:[  222] Training loss: 0.12554130, Validation loss: 0.13495250, Gradient norm: 94.57884091
INFO:root:[  223] Training loss: 0.12725168, Validation loss: 0.12371141, Gradient norm: 145.10175853
INFO:root:[  224] Training loss: 0.12473454, Validation loss: 0.12302981, Gradient norm: 103.66973717
INFO:root:[  225] Training loss: 0.12388240, Validation loss: 0.12783717, Gradient norm: 88.11068673
INFO:root:[  226] Training loss: 0.12566568, Validation loss: 0.12247453, Gradient norm: 125.69507959
INFO:root:[  227] Training loss: 0.12731236, Validation loss: 0.13553461, Gradient norm: 157.20635284
INFO:root:[  228] Training loss: 0.12556766, Validation loss: 0.12597876, Gradient norm: 128.94301851
INFO:root:[  229] Training loss: 0.12413495, Validation loss: 0.12163207, Gradient norm: 105.19792865
INFO:root:[  230] Training loss: 0.12196878, Validation loss: 0.12143306, Gradient norm: 71.87876852
INFO:root:[  231] Training loss: 0.12199424, Validation loss: 0.12417308, Gradient norm: 66.25173952
INFO:root:[  232] Training loss: 0.12529035, Validation loss: 0.12047458, Gradient norm: 152.02051036
INFO:root:[  233] Training loss: 0.12209726, Validation loss: 0.13057690, Gradient norm: 95.53296646
INFO:root:[  234] Training loss: 0.12606886, Validation loss: 0.11973870, Gradient norm: 157.25961948
INFO:root:[  235] Training loss: 0.12202422, Validation loss: 0.12067997, Gradient norm: 99.80347493
INFO:root:[  236] Training loss: 0.12209067, Validation loss: 0.11977661, Gradient norm: 109.29987662
INFO:root:[  237] Training loss: 0.12254581, Validation loss: 0.12080091, Gradient norm: 131.23573862
INFO:root:[  238] Training loss: 0.12205250, Validation loss: 0.12000222, Gradient norm: 131.48788315
INFO:root:[  239] Training loss: 0.12151641, Validation loss: 0.12055320, Gradient norm: 99.51300903
INFO:root:[  240] Training loss: 0.12281081, Validation loss: 0.13068210, Gradient norm: 152.83966503
INFO:root:[  241] Training loss: 0.12332761, Validation loss: 0.12850720, Gradient norm: 150.10301108
INFO:root:[  242] Training loss: 0.12305882, Validation loss: 0.11904864, Gradient norm: 144.02140377
INFO:root:[  243] Training loss: 0.12043136, Validation loss: 0.11842575, Gradient norm: 110.53202501
INFO:root:[  244] Training loss: 0.12225097, Validation loss: 0.11879940, Gradient norm: 122.29280300
INFO:root:[  245] Training loss: 0.11926127, Validation loss: 0.12102465, Gradient norm: 85.89417295
INFO:root:[  246] Training loss: 0.11883629, Validation loss: 0.11930549, Gradient norm: 85.48552718
INFO:root:[  247] Training loss: 0.12559024, Validation loss: 0.12139499, Gradient norm: 200.16864763
INFO:root:[  248] Training loss: 0.11841999, Validation loss: 0.11885093, Gradient norm: 87.87245521
INFO:root:[  249] Training loss: 0.11915693, Validation loss: 0.12287035, Gradient norm: 110.45492099
INFO:root:[  250] Training loss: 0.12123979, Validation loss: 0.11867635, Gradient norm: 162.95117510
INFO:root:[  251] Training loss: 0.12157356, Validation loss: 0.14202682, Gradient norm: 153.63577140
INFO:root:[  252] Training loss: 0.12420619, Validation loss: 0.11606152, Gradient norm: 188.04380538
INFO:root:[  253] Training loss: 0.11863694, Validation loss: 0.11718035, Gradient norm: 114.38605397
INFO:root:[  254] Training loss: 0.11674060, Validation loss: 0.11627066, Gradient norm: 82.54189678
INFO:root:[  255] Training loss: 0.11679750, Validation loss: 0.11671913, Gradient norm: 85.36585137
INFO:root:[  256] Training loss: 0.11758575, Validation loss: 0.12147736, Gradient norm: 121.95506973
INFO:root:[  257] Training loss: 0.11793896, Validation loss: 0.11809419, Gradient norm: 120.87721656
INFO:root:[  258] Training loss: 0.11992643, Validation loss: 0.12516556, Gradient norm: 171.29514537
INFO:root:[  259] Training loss: 0.11719226, Validation loss: 0.11750318, Gradient norm: 116.97104977
INFO:root:[  260] Training loss: 0.11686629, Validation loss: 0.11516657, Gradient norm: 117.45544794
INFO:root:[  261] Training loss: 0.11810108, Validation loss: 0.11525863, Gradient norm: 149.95596646
INFO:root:[  262] Training loss: 0.11653846, Validation loss: 0.12274188, Gradient norm: 115.34757441
INFO:root:[  263] Training loss: 0.11605926, Validation loss: 0.11439301, Gradient norm: 120.69720175
INFO:root:[  264] Training loss: 0.11618987, Validation loss: 0.12106246, Gradient norm: 105.69912257
INFO:root:[  265] Training loss: 0.11548503, Validation loss: 0.11470944, Gradient norm: 107.60739460
INFO:root:[  266] Training loss: 0.11701765, Validation loss: 0.11881566, Gradient norm: 146.81604401
INFO:root:[  267] Training loss: 0.11836351, Validation loss: 0.12529970, Gradient norm: 184.54107406
INFO:root:[  268] Training loss: 0.12350595, Validation loss: 0.11870450, Gradient norm: 244.33895704
INFO:root:[  269] Training loss: 0.11561908, Validation loss: 0.11840059, Gradient norm: 126.29160553
INFO:root:[  270] Training loss: 0.11614882, Validation loss: 0.12168688, Gradient norm: 145.72412533
INFO:root:[  271] Training loss: 0.11542142, Validation loss: 0.11308015, Gradient norm: 127.44515077
INFO:root:[  272] Training loss: 0.11412335, Validation loss: 0.11551259, Gradient norm: 111.12764230
INFO:root:[  273] Training loss: 0.11418451, Validation loss: 0.11329099, Gradient norm: 124.57005109
INFO:root:[  274] Training loss: 0.11482370, Validation loss: 0.11323524, Gradient norm: 133.12531330
INFO:root:[  275] Training loss: 0.11513171, Validation loss: 0.11536499, Gradient norm: 143.25277132
INFO:root:[  276] Training loss: 0.11482338, Validation loss: 0.11248704, Gradient norm: 156.53249337
INFO:root:[  277] Training loss: 0.11242674, Validation loss: 0.12333872, Gradient norm: 78.47934848
INFO:root:[  278] Training loss: 0.11908705, Validation loss: 0.11248949, Gradient norm: 226.64774044
INFO:root:[  279] Training loss: 0.11247835, Validation loss: 0.11099204, Gradient norm: 92.02333934
INFO:root:[  280] Training loss: 0.11377499, Validation loss: 0.11595941, Gradient norm: 128.54663185
INFO:root:[  281] Training loss: 0.11536100, Validation loss: 0.11269314, Gradient norm: 175.49865294
INFO:root:[  282] Training loss: 0.11284485, Validation loss: 0.11453375, Gradient norm: 121.96705663
INFO:root:[  283] Training loss: 0.11495499, Validation loss: 0.11071339, Gradient norm: 175.50970562
INFO:root:[  284] Training loss: 0.11072830, Validation loss: 0.11153435, Gradient norm: 60.28420319
INFO:root:[  285] Training loss: 0.11234896, Validation loss: 0.11015283, Gradient norm: 136.62049043
INFO:root:[  286] Training loss: 0.11455866, Validation loss: 0.11418833, Gradient norm: 185.78276978
INFO:root:[  287] Training loss: 0.11196624, Validation loss: 0.10957454, Gradient norm: 133.10447384
INFO:root:[  288] Training loss: 0.11197389, Validation loss: 0.10945736, Gradient norm: 138.92668024
INFO:root:[  289] Training loss: 0.11303666, Validation loss: 0.10976089, Gradient norm: 133.05029831
INFO:root:[  290] Training loss: 0.11324340, Validation loss: 0.11671974, Gradient norm: 162.78389952
INFO:root:[  291] Training loss: 0.11307849, Validation loss: 0.11041963, Gradient norm: 174.37554433
INFO:root:[  292] Training loss: 0.11516609, Validation loss: 0.11323956, Gradient norm: 214.30474756
INFO:root:[  293] Training loss: 0.11116585, Validation loss: 0.10914769, Gradient norm: 130.64592932
INFO:root:[  294] Training loss: 0.11164894, Validation loss: 0.10860084, Gradient norm: 153.68777549
INFO:root:[  295] Training loss: 0.10998749, Validation loss: 0.11032476, Gradient norm: 103.21895768
INFO:root:[  296] Training loss: 0.10942159, Validation loss: 0.11554687, Gradient norm: 96.10534326
INFO:root:[  297] Training loss: 0.11081104, Validation loss: 0.10903997, Gradient norm: 136.35060580
INFO:root:[  298] Training loss: 0.11035041, Validation loss: 0.10848002, Gradient norm: 146.08972479
INFO:root:[  299] Training loss: 0.11033563, Validation loss: 0.10815907, Gradient norm: 128.11580067
INFO:root:[  300] Training loss: 0.11436830, Validation loss: 0.11565269, Gradient norm: 199.05028559
INFO:root:[  301] Training loss: 0.11199085, Validation loss: 0.11195850, Gradient norm: 172.64424849
INFO:root:[  302] Training loss: 0.10890709, Validation loss: 0.10672886, Gradient norm: 108.70771985
INFO:root:[  303] Training loss: 0.10752191, Validation loss: 0.10760075, Gradient norm: 75.60851879
INFO:root:[  304] Training loss: 0.10780866, Validation loss: 0.11091385, Gradient norm: 94.35555304
INFO:root:[  305] Training loss: 0.10981938, Validation loss: 0.11355992, Gradient norm: 169.35657592
INFO:root:[  306] Training loss: 0.10895641, Validation loss: 0.10763514, Gradient norm: 142.06176558
INFO:root:[  307] Training loss: 0.10948660, Validation loss: 0.11801487, Gradient norm: 161.80867693
INFO:root:[  308] Training loss: 0.10936482, Validation loss: 0.10748722, Gradient norm: 161.55591483
INFO:root:[  309] Training loss: 0.10748218, Validation loss: 0.10652917, Gradient norm: 117.27163486
INFO:root:[  310] Training loss: 0.11100522, Validation loss: 0.11295967, Gradient norm: 188.96087646
INFO:root:[  311] Training loss: 0.10840365, Validation loss: 0.10597092, Gradient norm: 151.82395536
INFO:root:[  312] Training loss: 0.10674994, Validation loss: 0.10541316, Gradient norm: 101.30882443
INFO:root:[  313] Training loss: 0.11169764, Validation loss: 0.10664776, Gradient norm: 203.12092877
INFO:root:[  314] Training loss: 0.10773798, Validation loss: 0.11060803, Gradient norm: 141.75034311
INFO:root:[  315] Training loss: 0.10768659, Validation loss: 0.10650540, Gradient norm: 157.77377165
INFO:root:[  316] Training loss: 0.10642156, Validation loss: 0.10490959, Gradient norm: 109.71545430
INFO:root:[  317] Training loss: 0.10666602, Validation loss: 0.11956930, Gradient norm: 129.11967709
INFO:root:[  318] Training loss: 0.10850648, Validation loss: 0.10647211, Gradient norm: 188.63668434
INFO:root:[  319] Training loss: 0.10531737, Validation loss: 0.10722195, Gradient norm: 97.45943265
INFO:root:[  320] Training loss: 0.10661618, Validation loss: 0.10467863, Gradient norm: 153.65459665
INFO:root:[  321] Training loss: 0.10544404, Validation loss: 0.10771233, Gradient norm: 113.80945208
INFO:root:[  322] Training loss: 0.10803912, Validation loss: 0.10420945, Gradient norm: 201.92997713
INFO:root:[  323] Training loss: 0.10655200, Validation loss: 0.10631899, Gradient norm: 159.90711029
INFO:root:[  324] Training loss: 0.10622535, Validation loss: 0.10447777, Gradient norm: 156.37427672
INFO:root:[  325] Training loss: 0.10658955, Validation loss: 0.10340590, Gradient norm: 158.11182325
INFO:root:[  326] Training loss: 0.10447591, Validation loss: 0.11386664, Gradient norm: 108.06194513
INFO:root:[  327] Training loss: 0.10802913, Validation loss: 0.10853446, Gradient norm: 198.13269340
INFO:root:[  328] Training loss: 0.10484776, Validation loss: 0.10250267, Gradient norm: 137.60020882
INFO:root:[  329] Training loss: 0.10346647, Validation loss: 0.10845461, Gradient norm: 100.62000559
INFO:root:[  330] Training loss: 0.10686868, Validation loss: 0.10232012, Gradient norm: 193.18819655
INFO:root:[  331] Training loss: 0.10583151, Validation loss: 0.13072273, Gradient norm: 165.35427843
INFO:root:[  332] Training loss: 0.10797915, Validation loss: 0.10683343, Gradient norm: 213.55298839
INFO:root:[  333] Training loss: 0.10465670, Validation loss: 0.10331110, Gradient norm: 153.31862058
INFO:root:[  334] Training loss: 0.10253035, Validation loss: 0.10495254, Gradient norm: 82.03678064
INFO:root:[  335] Training loss: 0.10349006, Validation loss: 0.10303542, Gradient norm: 121.25909395
INFO:root:[  336] Training loss: 0.10232411, Validation loss: 0.10239015, Gradient norm: 94.20073123
INFO:root:[  337] Training loss: 0.10517816, Validation loss: 0.10367836, Gradient norm: 181.76407437
INFO:root:[  338] Training loss: 0.10450761, Validation loss: 0.10094690, Gradient norm: 158.89106476
INFO:root:[  339] Training loss: 0.10331756, Validation loss: 0.10119863, Gradient norm: 152.08583613
INFO:root:[  340] Training loss: 0.10230657, Validation loss: 0.10399112, Gradient norm: 117.11093462
INFO:root:[  341] Training loss: 0.10736743, Validation loss: 0.11187439, Gradient norm: 228.53271841
INFO:root:[  342] Training loss: 0.10397299, Validation loss: 0.10125257, Gradient norm: 167.02113441
INFO:root:[  343] Training loss: 0.10108987, Validation loss: 0.10298648, Gradient norm: 82.47277825
INFO:root:[  344] Training loss: 0.10351263, Validation loss: 0.11221857, Gradient norm: 165.18236897
INFO:root:[  345] Training loss: 0.10314631, Validation loss: 0.09999277, Gradient norm: 156.01548337
INFO:root:[  346] Training loss: 0.10436636, Validation loss: 0.10045663, Gradient norm: 183.96348791
INFO:root:[  347] Training loss: 0.10267961, Validation loss: 0.10000351, Gradient norm: 144.94977841
INFO:root:[  348] Training loss: 0.10132542, Validation loss: 0.10766032, Gradient norm: 118.56290576
INFO:root:[  349] Training loss: 0.10081975, Validation loss: 0.10110816, Gradient norm: 97.15109524
INFO:root:[  350] Training loss: 0.10347945, Validation loss: 0.10187631, Gradient norm: 180.25933305
INFO:root:[  351] Training loss: 0.10178993, Validation loss: 0.10775720, Gradient norm: 159.05606306
INFO:root:[  352] Training loss: 0.10458512, Validation loss: 0.10501245, Gradient norm: 224.50224106
INFO:root:[  353] Training loss: 0.10185434, Validation loss: 0.09891577, Gradient norm: 138.30857586
INFO:root:[  354] Training loss: 0.10189837, Validation loss: 0.10615763, Gradient norm: 165.27843057
INFO:root:[  355] Training loss: 0.10054435, Validation loss: 0.09998726, Gradient norm: 138.57533331
INFO:root:[  356] Training loss: 0.10049253, Validation loss: 0.10250895, Gradient norm: 128.12414357
INFO:root:[  357] Training loss: 0.10175424, Validation loss: 0.10075684, Gradient norm: 174.00190778
INFO:root:[  358] Training loss: 0.10045936, Validation loss: 0.10049865, Gradient norm: 125.25499668
INFO:root:[  359] Training loss: 0.10316138, Validation loss: 0.10197920, Gradient norm: 220.58365207
INFO:root:[  360] Training loss: 0.10020951, Validation loss: 0.09863948, Gradient norm: 120.30358894
INFO:root:[  361] Training loss: 0.10091432, Validation loss: 0.10090007, Gradient norm: 169.66786199
INFO:root:[  362] Training loss: 0.10079010, Validation loss: 0.09808114, Gradient norm: 166.88217217
INFO:root:[  363] Training loss: 0.10061048, Validation loss: 0.09805189, Gradient norm: 173.35197530
INFO:root:[  364] Training loss: 0.09861409, Validation loss: 0.09726003, Gradient norm: 115.10742078
INFO:root:[  365] Training loss: 0.09770560, Validation loss: 0.09846958, Gradient norm: 89.50507437
INFO:root:[  366] Training loss: 0.10396297, Validation loss: 0.09946613, Gradient norm: 247.86003104
INFO:root:[  367] Training loss: 0.09764660, Validation loss: 0.10077148, Gradient norm: 97.93329083
INFO:root:[  368] Training loss: 0.09831446, Validation loss: 0.10018081, Gradient norm: 109.97193514
INFO:root:[  369] Training loss: 0.09952196, Validation loss: 0.09721413, Gradient norm: 178.89957751
INFO:root:[  370] Training loss: 0.09963300, Validation loss: 0.09805645, Gradient norm: 165.72330663
INFO:root:[  371] Training loss: 0.10029122, Validation loss: 0.09681178, Gradient norm: 199.50749032
INFO:root:[  372] Training loss: 0.09766018, Validation loss: 0.10844526, Gradient norm: 114.76010323
INFO:root:[  373] Training loss: 0.09852880, Validation loss: 0.09586885, Gradient norm: 143.08730042
INFO:root:[  374] Training loss: 0.09861413, Validation loss: 0.09764616, Gradient norm: 162.28694878
INFO:root:[  375] Training loss: 0.09952950, Validation loss: 0.09929912, Gradient norm: 186.29088478
INFO:root:[  376] Training loss: 0.09997997, Validation loss: 0.09543588, Gradient norm: 207.44405071
INFO:root:[  377] Training loss: 0.09829390, Validation loss: 0.09566445, Gradient norm: 160.11524784
INFO:root:[  378] Training loss: 0.09676765, Validation loss: 0.09761411, Gradient norm: 118.72746481
INFO:root:[  379] Training loss: 0.09820513, Validation loss: 0.09541711, Gradient norm: 172.52588180
INFO:root:[  380] Training loss: 0.10049993, Validation loss: 0.09961064, Gradient norm: 216.18436596
INFO:root:[  381] Training loss: 0.09886448, Validation loss: 0.10450375, Gradient norm: 199.23383602
INFO:root:[  382] Training loss: 0.09598209, Validation loss: 0.09630600, Gradient norm: 104.79439244
INFO:root:[  383] Training loss: 0.09590338, Validation loss: 0.10704957, Gradient norm: 116.04274518
INFO:root:[  384] Training loss: 0.09661686, Validation loss: 0.09430421, Gradient norm: 140.72629328
INFO:root:[  385] Training loss: 0.09469510, Validation loss: 0.10503864, Gradient norm: 68.54834096
INFO:root:[  386] Training loss: 0.09823331, Validation loss: 0.09812674, Gradient norm: 193.50660272
INFO:root:[  387] Training loss: 0.09501056, Validation loss: 0.09368304, Gradient norm: 112.29134828
INFO:root:[  388] Training loss: 0.09862921, Validation loss: 0.09428797, Gradient norm: 211.38614692
INFO:root:[  389] Training loss: 0.09738605, Validation loss: 0.10349573, Gradient norm: 182.39552819
INFO:root:[  390] Training loss: 0.09865711, Validation loss: 0.09638888, Gradient norm: 222.47271676
INFO:root:[  391] Training loss: 0.09512732, Validation loss: 0.09599107, Gradient norm: 126.14110317
INFO:root:[  392] Training loss: 0.09484443, Validation loss: 0.09599226, Gradient norm: 112.17707800
INFO:root:[  393] Training loss: 0.09606024, Validation loss: 0.09537590, Gradient norm: 166.10119178
INFO:root:[  394] Training loss: 0.09716624, Validation loss: 0.09513008, Gradient norm: 171.43414624
INFO:root:[  395] Training loss: 0.09382388, Validation loss: 0.09282003, Gradient norm: 90.94644270
INFO:root:[  396] Training loss: 0.09439156, Validation loss: 0.09509702, Gradient norm: 116.98823583
INFO:root:[  397] Training loss: 0.09608790, Validation loss: 0.10117179, Gradient norm: 152.69116225
INFO:root:[  398] Training loss: 0.09442751, Validation loss: 0.09837906, Gradient norm: 138.32769830
INFO:root:[  399] Training loss: 0.09453131, Validation loss: 0.10045705, Gradient norm: 133.70738379
INFO:root:[  400] Training loss: 0.09583400, Validation loss: 0.09214382, Gradient norm: 193.93237738
INFO:root:[  401] Training loss: 0.09511862, Validation loss: 0.10132054, Gradient norm: 154.98900630
INFO:root:[  402] Training loss: 0.09751242, Validation loss: 0.09601737, Gradient norm: 211.57359448
INFO:root:[  403] Training loss: 0.09422669, Validation loss: 0.09908930, Gradient norm: 147.33029762
INFO:root:[  404] Training loss: 0.09390230, Validation loss: 0.09171149, Gradient norm: 141.22611030
INFO:root:[  405] Training loss: 0.09265716, Validation loss: 0.09140096, Gradient norm: 111.82739184
INFO:root:[  406] Training loss: 0.09373296, Validation loss: 0.09446770, Gradient norm: 138.47033545
INFO:root:[  407] Training loss: 0.09825193, Validation loss: 0.09345159, Gradient norm: 259.81946997
INFO:root:[  408] Training loss: 0.09406785, Validation loss: 0.09184159, Gradient norm: 169.62090156
INFO:root:[  409] Training loss: 0.09288151, Validation loss: 0.09406365, Gradient norm: 136.15264542
INFO:root:[  410] Training loss: 0.09392781, Validation loss: 0.09752768, Gradient norm: 171.49095901
INFO:root:[  411] Training loss: 0.09499390, Validation loss: 0.09067709, Gradient norm: 192.31993071
INFO:root:[  412] Training loss: 0.09152743, Validation loss: 0.09086408, Gradient norm: 103.38571967
INFO:root:[  413] Training loss: 0.09489084, Validation loss: 0.09177034, Gradient norm: 207.11869481
INFO:root:[  414] Training loss: 0.09291585, Validation loss: 0.09455846, Gradient norm: 143.70977441
INFO:root:[  415] Training loss: 0.09238976, Validation loss: 0.09400878, Gradient norm: 148.40834737
INFO:root:[  416] Training loss: 0.09342438, Validation loss: 0.09951981, Gradient norm: 183.44632716
INFO:root:[  417] Training loss: 0.09259463, Validation loss: 0.09487052, Gradient norm: 146.92522307
INFO:root:[  418] Training loss: 0.09228301, Validation loss: 0.09121508, Gradient norm: 152.06208596
INFO:root:[  419] Training loss: 0.09228015, Validation loss: 0.09400153, Gradient norm: 165.57919994
INFO:root:[  420] Training loss: 0.09074665, Validation loss: 0.08979880, Gradient norm: 108.55810446
INFO:root:[  421] Training loss: 0.09236070, Validation loss: 0.10209620, Gradient norm: 171.83403696
INFO:root:[  422] Training loss: 0.09173370, Validation loss: 0.08932508, Gradient norm: 111.01123646
INFO:root:[  423] Training loss: 0.09145044, Validation loss: 0.08930685, Gradient norm: 150.91900411
INFO:root:[  424] Training loss: 0.09279155, Validation loss: 0.08888485, Gradient norm: 188.43740519
INFO:root:[  425] Training loss: 0.08994322, Validation loss: 0.09276623, Gradient norm: 109.00224579
INFO:root:[  426] Training loss: 0.09419246, Validation loss: 0.09441045, Gradient norm: 224.61927894
INFO:root:[  427] Training loss: 0.09152909, Validation loss: 0.10347954, Gradient norm: 159.14667340
INFO:root:[  428] Training loss: 0.09195357, Validation loss: 0.08920352, Gradient norm: 183.48058998
INFO:root:[  429] Training loss: 0.08965215, Validation loss: 0.09132458, Gradient norm: 101.82293847
INFO:root:[  430] Training loss: 0.09001965, Validation loss: 0.09401157, Gradient norm: 128.40330843
INFO:root:[  431] Training loss: 0.09265318, Validation loss: 0.08885335, Gradient norm: 223.41752439
INFO:root:[  432] Training loss: 0.08865277, Validation loss: 0.08790002, Gradient norm: 91.90986404
INFO:root:[  433] Training loss: 0.09078980, Validation loss: 0.09005629, Gradient norm: 155.07010843
INFO:root:[  434] Training loss: 0.09061356, Validation loss: 0.08792888, Gradient norm: 173.00861620
INFO:root:[  435] Training loss: 0.08963628, Validation loss: 0.09087405, Gradient norm: 137.57305673
INFO:root:[  436] Training loss: 0.08875620, Validation loss: 0.08785048, Gradient norm: 110.81815603
INFO:root:[  437] Training loss: 0.09513507, Validation loss: 0.08808546, Gradient norm: 266.79064109
INFO:root:[  438] Training loss: 0.08841098, Validation loss: 0.09939189, Gradient norm: 103.17275342
INFO:root:[  439] Training loss: 0.09125109, Validation loss: 0.09411688, Gradient norm: 202.48822488
INFO:root:[  440] Training loss: 0.08937145, Validation loss: 0.08965911, Gradient norm: 137.99541734
INFO:root:[  441] Training loss: 0.08763195, Validation loss: 0.08696039, Gradient norm: 94.21638022
INFO:root:[  442] Training loss: 0.08820896, Validation loss: 0.08939671, Gradient norm: 112.60478925
INFO:root:[  443] Training loss: 0.08982653, Validation loss: 0.09099833, Gradient norm: 157.75235576
INFO:root:[  444] Training loss: 0.08958633, Validation loss: 0.08999569, Gradient norm: 176.65042279
INFO:root:[  445] Training loss: 0.08943157, Validation loss: 0.09788373, Gradient norm: 176.34150063
INFO:root:[  446] Training loss: 0.09030271, Validation loss: 0.08640332, Gradient norm: 178.24182173
INFO:root:[  447] Training loss: 0.09026519, Validation loss: 0.08621195, Gradient norm: 180.36609638
INFO:root:[  448] Training loss: 0.08895088, Validation loss: 0.09446348, Gradient norm: 162.43946329
INFO:root:[  449] Training loss: 0.08879792, Validation loss: 0.09086059, Gradient norm: 162.46207302
INFO:root:[  450] Training loss: 0.08789313, Validation loss: 0.08972550, Gradient norm: 127.92970771
INFO:root:[  451] Training loss: 0.08695296, Validation loss: 0.08582528, Gradient norm: 122.16332708
INFO:root:[  452] Training loss: 0.08955065, Validation loss: 0.08547409, Gradient norm: 190.22596893
INFO:root:[  453] Training loss: 0.08725866, Validation loss: 0.09647471, Gradient norm: 128.38027456
INFO:root:[  454] Training loss: 0.08954197, Validation loss: 0.08554586, Gradient norm: 191.92991538
INFO:root:[  455] Training loss: 0.08623196, Validation loss: 0.08904577, Gradient norm: 113.95964462
INFO:root:[  456] Training loss: 0.08949164, Validation loss: 0.08517274, Gradient norm: 204.02454672
INFO:root:[  457] Training loss: 0.08586329, Validation loss: 0.08947220, Gradient norm: 103.42999035
INFO:root:[  458] Training loss: 0.08747628, Validation loss: 0.08551416, Gradient norm: 173.33587116
INFO:root:[  459] Training loss: 0.08870890, Validation loss: 0.08843513, Gradient norm: 187.70083843
INFO:root:[  460] Training loss: 0.08926569, Validation loss: 0.08677614, Gradient norm: 196.27608228
INFO:root:[  461] Training loss: 0.08536544, Validation loss: 0.08460344, Gradient norm: 88.42591579
INFO:root:[  462] Training loss: 0.08747777, Validation loss: 0.08539150, Gradient norm: 161.08638240
INFO:root:[  463] Training loss: 0.08642132, Validation loss: 0.08625430, Gradient norm: 147.15931427
INFO:root:[  464] Training loss: 0.08781065, Validation loss: 0.08423140, Gradient norm: 177.30888020
INFO:root:[  465] Training loss: 0.08557672, Validation loss: 0.08853623, Gradient norm: 114.83516822
INFO:root:[  466] Training loss: 0.08723264, Validation loss: 0.08438328, Gradient norm: 168.66310577
INFO:root:[  467] Training loss: 0.08415542, Validation loss: 0.08553229, Gradient norm: 76.07825540
INFO:root:[  468] Training loss: 0.08880154, Validation loss: 0.08792115, Gradient norm: 213.71715329
INFO:root:[  469] Training loss: 0.08606093, Validation loss: 0.08359794, Gradient norm: 146.01733892
INFO:root:[  470] Training loss: 0.08541870, Validation loss: 0.08643658, Gradient norm: 130.08221312
INFO:root:[  471] Training loss: 0.08788358, Validation loss: 0.08415411, Gradient norm: 208.14256648
INFO:root:[  472] Training loss: 0.08408027, Validation loss: 0.08311301, Gradient norm: 95.73659139
INFO:root:[  473] Training loss: 0.08520020, Validation loss: 0.08349506, Gradient norm: 154.93942952
INFO:root:[  474] Training loss: 0.08561898, Validation loss: 0.09797785, Gradient norm: 149.50273786
INFO:root:[  475] Training loss: 0.08810812, Validation loss: 0.08362082, Gradient norm: 212.75663569
INFO:root:[  476] Training loss: 0.08484566, Validation loss: 0.08467692, Gradient norm: 137.52475478
INFO:root:[  477] Training loss: 0.08347990, Validation loss: 0.08349765, Gradient norm: 102.36034334
INFO:root:[  478] Training loss: 0.08745475, Validation loss: 0.08292562, Gradient norm: 209.20287769
INFO:root:[  479] Training loss: 0.08508244, Validation loss: 0.08241081, Gradient norm: 149.87415604
INFO:root:[  480] Training loss: 0.08372163, Validation loss: 0.08518050, Gradient norm: 114.69158550
INFO:root:[  481] Training loss: 0.08426482, Validation loss: 0.08221426, Gradient norm: 142.47460461
INFO:root:[  482] Training loss: 0.08533192, Validation loss: 0.08475179, Gradient norm: 174.95641686
INFO:root:[  483] Training loss: 0.08654841, Validation loss: 0.08748937, Gradient norm: 204.39553777
INFO:root:[  484] Training loss: 0.08422729, Validation loss: 0.08196070, Gradient norm: 146.58469753
INFO:root:[  485] Training loss: 0.08451393, Validation loss: 0.08190457, Gradient norm: 158.09361513
INFO:root:[  486] Training loss: 0.08481760, Validation loss: 0.08147860, Gradient norm: 156.55344038
INFO:root:[  487] Training loss: 0.08406052, Validation loss: 0.08253736, Gradient norm: 149.37598021
INFO:root:[  488] Training loss: 0.08476006, Validation loss: 0.08633764, Gradient norm: 147.32358578
INFO:root:[  489] Training loss: 0.08421825, Validation loss: 0.08275532, Gradient norm: 165.72267050
INFO:root:[  490] Training loss: 0.08207441, Validation loss: 0.08391543, Gradient norm: 101.05720227
INFO:root:[  491] Training loss: 0.08298461, Validation loss: 0.08200286, Gradient norm: 137.01181441
INFO:root:[  492] Training loss: 0.08220620, Validation loss: 0.09537346, Gradient norm: 111.47590586
INFO:root:[  493] Training loss: 0.08394057, Validation loss: 0.08347694, Gradient norm: 159.17560339
INFO:root:[  494] Training loss: 0.08496383, Validation loss: 0.08871675, Gradient norm: 209.28823671
INFO:root:[  495] Training loss: 0.08355944, Validation loss: 0.08728040, Gradient norm: 169.74270893
INFO:root:[  496] Training loss: 0.08294988, Validation loss: 0.08062282, Gradient norm: 151.19425899
INFO:root:[  497] Training loss: 0.08284124, Validation loss: 0.08475133, Gradient norm: 147.99538500
INFO:root:[  498] Training loss: 0.08180231, Validation loss: 0.08310274, Gradient norm: 122.76171027
INFO:root:[  499] Training loss: 0.08309400, Validation loss: 0.08638591, Gradient norm: 166.23446355
INFO:root:[  500] Training loss: 0.08159851, Validation loss: 0.08113128, Gradient norm: 122.24308173
INFO:root:[  501] Training loss: 0.08214306, Validation loss: 0.08046065, Gradient norm: 138.67112945
INFO:root:[  502] Training loss: 0.08395705, Validation loss: 0.08189260, Gradient norm: 208.29540707
INFO:root:[  503] Training loss: 0.08682812, Validation loss: 0.08102121, Gradient norm: 245.42067748
INFO:root:[  504] Training loss: 0.08063393, Validation loss: 0.08417080, Gradient norm: 98.26903134
INFO:root:[  505] Training loss: 0.08124893, Validation loss: 0.08525729, Gradient norm: 110.90844211
INFO:root:[  506] Training loss: 0.08090558, Validation loss: 0.07954293, Gradient norm: 122.61085660
INFO:root:[  507] Training loss: 0.08133780, Validation loss: 0.08044443, Gradient norm: 142.16850311
INFO:root:[  508] Training loss: 0.08262935, Validation loss: 0.07943325, Gradient norm: 160.78659135
INFO:root:[  509] Training loss: 0.08183082, Validation loss: 0.08102519, Gradient norm: 160.43166082
INFO:root:[  510] Training loss: 0.08154254, Validation loss: 0.07991594, Gradient norm: 145.85653784
INFO:root:[  511] Training loss: 0.08230702, Validation loss: 0.08232792, Gradient norm: 167.22835039
INFO:root:[  512] Training loss: 0.08201635, Validation loss: 0.08439929, Gradient norm: 173.14621086
INFO:root:[  513] Training loss: 0.08029743, Validation loss: 0.07884320, Gradient norm: 125.74045378
INFO:root:[  514] Training loss: 0.08000496, Validation loss: 0.09298026, Gradient norm: 113.02543936
INFO:root:[  515] Training loss: 0.08359902, Validation loss: 0.08006692, Gradient norm: 218.57233174
INFO:root:[  516] Training loss: 0.08118600, Validation loss: 0.08148361, Gradient norm: 160.51453100
INFO:root:[  517] Training loss: 0.08063359, Validation loss: 0.07882041, Gradient norm: 142.45293479
INFO:root:[  518] Training loss: 0.07946303, Validation loss: 0.07865828, Gradient norm: 104.47124554
INFO:root:[  519] Training loss: 0.07967817, Validation loss: 0.08290587, Gradient norm: 114.95093403
INFO:root:[  520] Training loss: 0.08118090, Validation loss: 0.07981776, Gradient norm: 165.37741518
INFO:root:[  521] Training loss: 0.08025320, Validation loss: 0.07975577, Gradient norm: 146.39460219
INFO:root:[  522] Training loss: 0.07914870, Validation loss: 0.07793310, Gradient norm: 118.94522235
INFO:root:[  523] Training loss: 0.08313395, Validation loss: 0.07943029, Gradient norm: 212.81250554
INFO:root:[  524] Training loss: 0.07820582, Validation loss: 0.07815947, Gradient norm: 84.22215111
INFO:root:[  525] Training loss: 0.08002343, Validation loss: 0.08078156, Gradient norm: 151.25502844
INFO:root:[  526] Training loss: 0.08014044, Validation loss: 0.07973014, Gradient norm: 145.10714045
INFO:root:[  527] Training loss: 0.08149637, Validation loss: 0.07814002, Gradient norm: 197.20776661
INFO:root:[  528] Training loss: 0.07782326, Validation loss: 0.08325669, Gradient norm: 79.21281654
INFO:root:[  529] Training loss: 0.07942865, Validation loss: 0.08462848, Gradient norm: 136.55593383
INFO:root:[  530] Training loss: 0.08097188, Validation loss: 0.07776680, Gradient norm: 192.56965740
INFO:root:[  531] Training loss: 0.07759171, Validation loss: 0.07705886, Gradient norm: 98.18823594
INFO:root:[  532] Training loss: 0.07929800, Validation loss: 0.07969903, Gradient norm: 152.81840203
INFO:root:[  533] Training loss: 0.07702846, Validation loss: 0.08153715, Gradient norm: 74.61721047
INFO:root:[  534] Training loss: 0.08298825, Validation loss: 0.07807646, Gradient norm: 255.17689006
INFO:root:[  535] Training loss: 0.07708334, Validation loss: 0.07742420, Gradient norm: 92.05887018
INFO:root:[  536] Training loss: 0.07728957, Validation loss: 0.07736763, Gradient norm: 101.54559695
INFO:root:[  537] Training loss: 0.07893006, Validation loss: 0.09019288, Gradient norm: 175.38294615
INFO:root:[  538] Training loss: 0.07992312, Validation loss: 0.07682302, Gradient norm: 191.37686851
INFO:root:[  539] Training loss: 0.07793136, Validation loss: 0.07669217, Gradient norm: 132.12394899
INFO:root:[  540] Training loss: 0.07898458, Validation loss: 0.09638897, Gradient norm: 165.62467130
INFO:root:[  541] Training loss: 0.07919385, Validation loss: 0.07576932, Gradient norm: 158.93938445
INFO:root:[  542] Training loss: 0.07783319, Validation loss: 0.07704640, Gradient norm: 132.76994194
INFO:root:[  543] Training loss: 0.07721703, Validation loss: 0.08193036, Gradient norm: 119.35401455
INFO:root:[  544] Training loss: 0.07756302, Validation loss: 0.07567221, Gradient norm: 128.75567577
INFO:root:[  545] Training loss: 0.07762163, Validation loss: 0.08628356, Gradient norm: 146.66647471
INFO:root:[  546] Training loss: 0.07980583, Validation loss: 0.07758204, Gradient norm: 203.52902163
INFO:root:[  547] Training loss: 0.07820581, Validation loss: 0.08027257, Gradient norm: 157.85858602
INFO:root:[  548] Training loss: 0.07767738, Validation loss: 0.07554083, Gradient norm: 143.93416141
INFO:root:[  549] Training loss: 0.07565273, Validation loss: 0.07775279, Gradient norm: 82.78358857
INFO:root:[  550] Training loss: 0.07708236, Validation loss: 0.07819971, Gradient norm: 122.47535004
INFO:root:[  551] Training loss: 0.07813980, Validation loss: 0.07662133, Gradient norm: 169.99888404
INFO:root:[  552] Training loss: 0.07516384, Validation loss: 0.07483708, Gradient norm: 75.10707259
INFO:root:[  553] Training loss: 0.07757406, Validation loss: 0.07475651, Gradient norm: 156.15557313
INFO:root:[  554] Training loss: 0.07877555, Validation loss: 0.07673661, Gradient norm: 203.55985050
INFO:root:[  555] Training loss: 0.07539787, Validation loss: 0.07459230, Gradient norm: 113.94278369
INFO:root:[  556] Training loss: 0.07588122, Validation loss: 0.08491362, Gradient norm: 132.68044902
INFO:root:[  557] Training loss: 0.07736160, Validation loss: 0.07451445, Gradient norm: 166.78419631
INFO:root:[  558] Training loss: 0.07633620, Validation loss: 0.07511251, Gradient norm: 131.54203909
INFO:root:[  559] Training loss: 0.07630823, Validation loss: 0.07600770, Gradient norm: 132.64905237
INFO:root:[  560] Training loss: 0.07679427, Validation loss: 0.07534384, Gradient norm: 158.47204560
INFO:root:[  561] Training loss: 0.07635400, Validation loss: 0.07423561, Gradient norm: 160.13516168
INFO:root:[  562] Training loss: 0.07512011, Validation loss: 0.08377180, Gradient norm: 116.41750170
INFO:root:[  563] Training loss: 0.07607019, Validation loss: 0.07393402, Gradient norm: 130.23192897
INFO:root:[  564] Training loss: 0.07742209, Validation loss: 0.07580577, Gradient norm: 176.92092729
INFO:root:[  565] Training loss: 0.07627047, Validation loss: 0.07425960, Gradient norm: 159.93318406
INFO:root:[  566] Training loss: 0.07583198, Validation loss: 0.07515558, Gradient norm: 139.23231428
INFO:root:[  567] Training loss: 0.07461853, Validation loss: 0.07321923, Gradient norm: 113.02007223
INFO:root:[  568] Training loss: 0.07555471, Validation loss: 0.07690083, Gradient norm: 146.19417285
INFO:root:[  569] Training loss: 0.07759875, Validation loss: 0.08569973, Gradient norm: 188.76959106
INFO:root:[  570] Training loss: 0.07774187, Validation loss: 0.07628895, Gradient norm: 198.59940777
INFO:root:[  571] Training loss: 0.07424381, Validation loss: 0.07340680, Gradient norm: 103.17942792
INFO:root:[  572] Training loss: 0.07320717, Validation loss: 0.07359618, Gradient norm: 73.53305071
INFO:root:[  573] Training loss: 0.07514806, Validation loss: 0.07403671, Gradient norm: 125.44968775
INFO:root:[  574] Training loss: 0.07429428, Validation loss: 0.07384208, Gradient norm: 126.74785797
INFO:root:[  575] Training loss: 0.07441582, Validation loss: 0.07372110, Gradient norm: 136.39007689
INFO:root:[  576] Training loss: 0.07440008, Validation loss: 0.07303471, Gradient norm: 123.07565900
INFO:root:[  577] Training loss: 0.07420352, Validation loss: 0.07235680, Gradient norm: 119.27969999
INFO:root:[  578] Training loss: 0.07355490, Validation loss: 0.08135138, Gradient norm: 104.40053364
INFO:root:[  579] Training loss: 0.07463439, Validation loss: 0.07248001, Gradient norm: 152.93781685
INFO:root:[  580] Training loss: 0.07490151, Validation loss: 0.07511722, Gradient norm: 163.12610127
INFO:root:[  581] Training loss: 0.07426769, Validation loss: 0.07241289, Gradient norm: 147.19850755
INFO:root:[  582] Training loss: 0.07395675, Validation loss: 0.07189131, Gradient norm: 145.09540339
INFO:root:[  583] Training loss: 0.07363197, Validation loss: 0.07256819, Gradient norm: 127.06586866
INFO:root:[  584] Training loss: 0.07324270, Validation loss: 0.07328092, Gradient norm: 129.35233516
INFO:root:[  585] Training loss: 0.07551076, Validation loss: 0.07224894, Gradient norm: 184.19649232
INFO:root:[  586] Training loss: 0.07359547, Validation loss: 0.07281387, Gradient norm: 144.91653459
INFO:root:[  587] Training loss: 0.07249475, Validation loss: 0.07104343, Gradient norm: 114.06164228
INFO:root:[  588] Training loss: 0.07248600, Validation loss: 0.07609118, Gradient norm: 112.37029465
INFO:root:[  589] Training loss: 0.07465807, Validation loss: 0.07234658, Gradient norm: 164.40431376
INFO:root:[  590] Training loss: 0.07397694, Validation loss: 0.07097133, Gradient norm: 161.36357063
INFO:root:[  591] Training loss: 0.07323808, Validation loss: 0.07957723, Gradient norm: 134.58574535
INFO:root:[  592] Training loss: 0.07133771, Validation loss: 0.07071880, Gradient norm: 62.87294437
INFO:root:[  593] Training loss: 0.07239871, Validation loss: 0.07077403, Gradient norm: 115.93917748
INFO:root:[  594] Training loss: 0.07347870, Validation loss: 0.07600638, Gradient norm: 163.26860179
INFO:root:[  595] Training loss: 0.07384970, Validation loss: 0.07501120, Gradient norm: 164.78013206
INFO:root:[  596] Training loss: 0.07147592, Validation loss: 0.07110395, Gradient norm: 107.22995311
INFO:root:[  597] Training loss: 0.07149695, Validation loss: 0.07015514, Gradient norm: 110.61265391
INFO:root:[  598] Training loss: 0.07167374, Validation loss: 0.07042969, Gradient norm: 122.70170518
INFO:root:[  599] Training loss: 0.07308523, Validation loss: 0.07260686, Gradient norm: 148.63116801
INFO:root:[  600] Training loss: 0.07318040, Validation loss: 0.07048029, Gradient norm: 174.41994115
INFO:root:[  601] Training loss: 0.07129762, Validation loss: 0.07099307, Gradient norm: 122.93566776
INFO:root:[  602] Training loss: 0.07007469, Validation loss: 0.07204419, Gradient norm: 76.07277587
INFO:root:[  603] Training loss: 0.07112442, Validation loss: 0.06999838, Gradient norm: 112.66778101
INFO:root:[  604] Training loss: 0.07217923, Validation loss: 0.07173277, Gradient norm: 170.10233942
INFO:root:[  605] Training loss: 0.07216478, Validation loss: 0.06959482, Gradient norm: 157.58353456
INFO:root:[  606] Training loss: 0.07089805, Validation loss: 0.07243174, Gradient norm: 130.45048879
INFO:root:[  607] Training loss: 0.07177426, Validation loss: 0.06905970, Gradient norm: 159.69726933
INFO:root:[  608] Training loss: 0.07089221, Validation loss: 0.07226216, Gradient norm: 128.92374290
INFO:root:[  609] Training loss: 0.07187876, Validation loss: 0.06959603, Gradient norm: 161.48786259
INFO:root:[  610] Training loss: 0.07127835, Validation loss: 0.07100849, Gradient norm: 141.70235202
INFO:root:[  611] Training loss: 0.07011499, Validation loss: 0.07036597, Gradient norm: 109.76016175
INFO:root:[  612] Training loss: 0.07030569, Validation loss: 0.07057127, Gradient norm: 126.43920806
INFO:root:[  613] Training loss: 0.07234013, Validation loss: 0.06879340, Gradient norm: 173.01587675
INFO:root:[  614] Training loss: 0.06984996, Validation loss: 0.06884007, Gradient norm: 113.89277771
INFO:root:[  615] Training loss: 0.06900953, Validation loss: 0.06910520, Gradient norm: 80.81721136
INFO:root:[  616] Training loss: 0.07012516, Validation loss: 0.07072173, Gradient norm: 123.87504518
INFO:root:[  617] Training loss: 0.07142293, Validation loss: 0.06838629, Gradient norm: 169.45232242
INFO:root:[  618] Training loss: 0.07103324, Validation loss: 0.06835254, Gradient norm: 156.67514855
INFO:root:[  619] Training loss: 0.06924589, Validation loss: 0.06797754, Gradient norm: 113.96980575
INFO:root:[  620] Training loss: 0.07007923, Validation loss: 0.06808722, Gradient norm: 127.26012802
INFO:root:[  621] Training loss: 0.06902926, Validation loss: 0.07584515, Gradient norm: 109.76322673
INFO:root:[  622] Training loss: 0.07063857, Validation loss: 0.07456995, Gradient norm: 162.39995459
INFO:root:[  623] Training loss: 0.06885433, Validation loss: 0.06741588, Gradient norm: 103.25136031
INFO:root:[  624] Training loss: 0.07037064, Validation loss: 0.07358086, Gradient norm: 136.68066741
INFO:root:[  625] Training loss: 0.06797879, Validation loss: 0.06794893, Gradient norm: 78.47198408
INFO:root:[  626] Training loss: 0.07016357, Validation loss: 0.06736827, Gradient norm: 150.28177283
INFO:root:[  627] Training loss: 0.06907240, Validation loss: 0.06726043, Gradient norm: 122.02247108
INFO:root:[  628] Training loss: 0.07095964, Validation loss: 0.06849663, Gradient norm: 183.51575836
INFO:root:[  629] Training loss: 0.06794729, Validation loss: 0.06730258, Gradient norm: 96.18991930
INFO:root:[  630] Training loss: 0.06775196, Validation loss: 0.06742694, Gradient norm: 101.42756137
INFO:root:[  631] Training loss: 0.06910111, Validation loss: 0.06774674, Gradient norm: 151.85210278
INFO:root:[  632] Training loss: 0.06871837, Validation loss: 0.06894051, Gradient norm: 142.37384133
INFO:root:[  633] Training loss: 0.06773349, Validation loss: 0.06659992, Gradient norm: 111.08732581
INFO:root:[  634] Training loss: 0.06842982, Validation loss: 0.06762003, Gradient norm: 146.43324011
INFO:root:[  635] Training loss: 0.06866089, Validation loss: 0.06861215, Gradient norm: 149.64907996
INFO:root:[  636] Training loss: 0.06676087, Validation loss: 0.06896166, Gradient norm: 81.23008863
INFO:root:[  637] Training loss: 0.06960317, Validation loss: 0.06867263, Gradient norm: 166.82828584
INFO:root:[  638] Training loss: 0.06805058, Validation loss: 0.06794635, Gradient norm: 118.59776598
INFO:root:[  639] Training loss: 0.06839869, Validation loss: 0.06602492, Gradient norm: 150.23119152
INFO:root:[  640] Training loss: 0.06684876, Validation loss: 0.06601093, Gradient norm: 101.85263241
INFO:root:[  641] Training loss: 0.06814234, Validation loss: 0.07194011, Gradient norm: 149.82231307
INFO:root:[  642] Training loss: 0.06723009, Validation loss: 0.06579848, Gradient norm: 121.00344438
INFO:root:[  643] Training loss: 0.06604495, Validation loss: 0.06785371, Gradient norm: 87.36542917
INFO:root:[  644] Training loss: 0.06848542, Validation loss: 0.06571238, Gradient norm: 174.57201092
INFO:root:[  645] Training loss: 0.06695889, Validation loss: 0.06590772, Gradient norm: 118.92662872
INFO:root:[  646] Training loss: 0.06664239, Validation loss: 0.06614305, Gradient norm: 106.81312263
INFO:root:[  647] Training loss: 0.06792356, Validation loss: 0.06978661, Gradient norm: 160.02008579
INFO:root:[  648] Training loss: 0.06676625, Validation loss: 0.06514183, Gradient norm: 122.61725642
INFO:root:[  649] Training loss: 0.06607621, Validation loss: 0.06628727, Gradient norm: 111.64196756
INFO:root:[  650] Training loss: 0.06823102, Validation loss: 0.06883227, Gradient norm: 158.25930036
INFO:root:[  651] Training loss: 0.06571831, Validation loss: 0.06465671, Gradient norm: 95.73723533
INFO:root:[  652] Training loss: 0.06616367, Validation loss: 0.08073430, Gradient norm: 127.05912023
INFO:root:[  653] Training loss: 0.06739713, Validation loss: 0.06579729, Gradient norm: 152.67083515
INFO:root:[  654] Training loss: 0.06644372, Validation loss: 0.07194043, Gradient norm: 127.08251835
INFO:root:[  655] Training loss: 0.06629269, Validation loss: 0.06391217, Gradient norm: 122.82558482
INFO:root:[  656] Training loss: 0.06606737, Validation loss: 0.06467461, Gradient norm: 97.39629332
INFO:root:[  657] Training loss: 0.06556745, Validation loss: 0.06507486, Gradient norm: 119.59978854
INFO:root:[  658] Training loss: 0.06508647, Validation loss: 0.06425915, Gradient norm: 107.11300992
INFO:root:[  659] Training loss: 0.06761598, Validation loss: 0.07181254, Gradient norm: 183.50007885
INFO:root:[  660] Training loss: 0.06637801, Validation loss: 0.06842717, Gradient norm: 150.84043550
INFO:root:[  661] Training loss: 0.06459516, Validation loss: 0.06693831, Gradient norm: 93.31107545
INFO:root:[  662] Training loss: 0.06413939, Validation loss: 0.06578477, Gradient norm: 86.35950888
INFO:root:[  663] Training loss: 0.06496412, Validation loss: 0.06905920, Gradient norm: 130.63000405
INFO:root:[  664] Training loss: 0.06565970, Validation loss: 0.06799295, Gradient norm: 141.91788612
INFO:root:EP 664: Early stopping
INFO:root:Training the model took 30647.593s.
INFO:root:Emptying the cuda cache took 0.11s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 2.8307
INFO:root:EnergyScoreTrain: 1.99655
INFO:root:CoverageTrain: 0.97093
INFO:root:IntervalWidthTrain: 0.13691
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 2.72346
INFO:root:EnergyScoreValidation: 1.92138
INFO:root:CoverageValidation: 0.97094
INFO:root:IntervalWidthValidation: 0.13749
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.24137
INFO:root:EnergyScoreTest: 1.57939
INFO:root:CoverageTest: 0.97074
INFO:root:IntervalWidthTest: 0.13549
INFO:root:###7 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1608515584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.43929865, Validation loss: 1.46463084, Gradient norm: 12.14852340
INFO:root:[    2] Training loss: 1.33531733, Validation loss: 1.20266263, Gradient norm: 3.81916809
INFO:root:[    3] Training loss: 1.11418634, Validation loss: 1.03700687, Gradient norm: 2.98669384
INFO:root:[    4] Training loss: 0.98114491, Validation loss: 0.90430676, Gradient norm: 2.42051254
INFO:root:[    5] Training loss: 0.88418997, Validation loss: 0.83508353, Gradient norm: 2.00508668
INFO:root:[    6] Training loss: 0.80912557, Validation loss: 0.78039307, Gradient norm: 1.90006757
INFO:root:[    7] Training loss: 0.75769825, Validation loss: 0.73624556, Gradient norm: 1.88893127
INFO:root:[    8] Training loss: 0.71075277, Validation loss: 0.69292258, Gradient norm: 1.64228915
INFO:root:[    9] Training loss: 0.67922363, Validation loss: 0.64207734, Gradient norm: 1.83076113
INFO:root:[   10] Training loss: 0.64985430, Validation loss: 0.62721645, Gradient norm: 1.55881604
INFO:root:[   11] Training loss: 0.63289282, Validation loss: 0.62051764, Gradient norm: 1.67885549
INFO:root:[   12] Training loss: 0.61498955, Validation loss: 0.60991645, Gradient norm: 1.58241040
INFO:root:[   13] Training loss: 0.61817286, Validation loss: 0.62608669, Gradient norm: 2.44179694
INFO:root:[   14] Training loss: 0.60767914, Validation loss: 0.59765755, Gradient norm: 1.86745707
INFO:root:[   15] Training loss: 0.60112794, Validation loss: 0.58325584, Gradient norm: 2.12750067
INFO:root:[   16] Training loss: 0.59271182, Validation loss: 0.58618460, Gradient norm: 2.05096119
INFO:root:[   17] Training loss: 0.59050391, Validation loss: 0.61585067, Gradient norm: 2.34504046
INFO:root:[   18] Training loss: 0.59843823, Validation loss: 0.60787243, Gradient norm: 3.52353150
INFO:root:[   19] Training loss: 0.60864364, Validation loss: 0.62506441, Gradient norm: 3.67877031
INFO:root:[   20] Training loss: 0.61944090, Validation loss: 0.63337131, Gradient norm: 5.04047609
INFO:root:[   21] Training loss: 0.63734230, Validation loss: 0.64791141, Gradient norm: 8.36909612
INFO:root:[   22] Training loss: 0.65465891, Validation loss: 0.63307018, Gradient norm: 15.75719753
INFO:root:[   23] Training loss: 0.65124830, Validation loss: 0.61322989, Gradient norm: 37.40171440
INFO:root:[   24] Training loss: 0.60480207, Validation loss: 0.60484360, Gradient norm: 65.11980308
INFO:root:[   25] Training loss: 0.59854712, Validation loss: 0.57592522, Gradient norm: 131.27920593
INFO:root:[   26] Training loss: 0.58060008, Validation loss: 0.57869299, Gradient norm: 99.43354833
INFO:root:[   27] Training loss: 0.58083334, Validation loss: 0.57540827, Gradient norm: 183.32131358
INFO:root:[   28] Training loss: 0.57689132, Validation loss: 0.56181709, Gradient norm: 231.52311080
INFO:root:[   29] Training loss: 0.57681339, Validation loss: 0.56273228, Gradient norm: 324.18902490
INFO:root:[   30] Training loss: 0.56033016, Validation loss: 0.57758909, Gradient norm: 343.37439129
INFO:root:[   31] Training loss: 0.55865454, Validation loss: 0.57400334, Gradient norm: 337.54128093
INFO:root:[   32] Training loss: 0.55096341, Validation loss: 0.53928787, Gradient norm: 262.75407654
INFO:root:[   33] Training loss: 0.55812707, Validation loss: 0.55872113, Gradient norm: 404.15738471
INFO:root:[   34] Training loss: 0.54702667, Validation loss: 0.53872987, Gradient norm: 401.24922280
INFO:root:[   35] Training loss: 0.54842986, Validation loss: 0.58422548, Gradient norm: 533.16724531
INFO:root:[   36] Training loss: 0.56232776, Validation loss: 0.58979194, Gradient norm: 678.64783588
INFO:root:[   37] Training loss: 0.55691043, Validation loss: 0.60981990, Gradient norm: 614.70521846
INFO:root:[   38] Training loss: 0.55719515, Validation loss: 0.53321523, Gradient norm: 608.21881892
INFO:root:[   39] Training loss: 0.53554361, Validation loss: 0.52110920, Gradient norm: 407.09835550
INFO:root:[   40] Training loss: 0.53389964, Validation loss: 0.53325460, Gradient norm: 404.22211160
INFO:root:[   41] Training loss: 0.53393687, Validation loss: 0.53537019, Gradient norm: 424.33053227
INFO:root:[   42] Training loss: 0.54497281, Validation loss: 0.52497485, Gradient norm: 744.29504989
INFO:root:[   43] Training loss: 0.51785832, Validation loss: 0.51763145, Gradient norm: 278.32762234
INFO:root:[   44] Training loss: 0.51742566, Validation loss: 0.68017423, Gradient norm: 370.11524965
INFO:root:[   45] Training loss: 0.54027342, Validation loss: 0.68325055, Gradient norm: 763.35380869
INFO:root:[   46] Training loss: 0.55066882, Validation loss: 0.50945689, Gradient norm: 939.33540890
INFO:root:[   47] Training loss: 0.51661219, Validation loss: 0.51445505, Gradient norm: 323.80868464
INFO:root:[   48] Training loss: 0.51166858, Validation loss: 0.51359570, Gradient norm: 404.69628119
INFO:root:[   49] Training loss: 0.51565426, Validation loss: 0.53355287, Gradient norm: 519.83055174
INFO:root:[   50] Training loss: 0.52874044, Validation loss: 0.51487692, Gradient norm: 713.10181928
INFO:root:[   51] Training loss: 0.50691367, Validation loss: 0.50552440, Gradient norm: 447.94398426
INFO:root:[   52] Training loss: 0.52250644, Validation loss: 0.52429058, Gradient norm: 692.77326305
INFO:root:[   53] Training loss: 0.51753115, Validation loss: 0.49944258, Gradient norm: 692.49323928
INFO:root:[   54] Training loss: 0.49893044, Validation loss: 0.51019930, Gradient norm: 375.21983184
INFO:root:[   55] Training loss: 0.49816236, Validation loss: 0.49868634, Gradient norm: 357.82031487
INFO:root:[   56] Training loss: 0.51176958, Validation loss: 0.49691523, Gradient norm: 709.37416716
INFO:root:[   57] Training loss: 0.50291198, Validation loss: 0.48840084, Gradient norm: 771.81156047
INFO:root:[   58] Training loss: 0.50379227, Validation loss: 0.50256299, Gradient norm: 675.00155166
INFO:root:[   59] Training loss: 0.48939413, Validation loss: 0.50048394, Gradient norm: 392.43600311
INFO:root:[   60] Training loss: 0.50516480, Validation loss: 0.48471208, Gradient norm: 846.51013554
INFO:root:[   61] Training loss: 0.48864964, Validation loss: 0.48545219, Gradient norm: 480.41115253
INFO:root:[   62] Training loss: 0.49608851, Validation loss: 0.48035716, Gradient norm: 767.79375953
INFO:root:[   63] Training loss: 0.49509965, Validation loss: 0.48377907, Gradient norm: 710.88049157
INFO:root:[   64] Training loss: 0.48692071, Validation loss: 0.47957767, Gradient norm: 626.91269847
INFO:root:[   65] Training loss: 0.49269078, Validation loss: 0.50271400, Gradient norm: 813.56557803
INFO:root:[   66] Training loss: 0.50254813, Validation loss: 0.47173385, Gradient norm: 1052.02613845
INFO:root:[   67] Training loss: 0.48898108, Validation loss: 0.48140809, Gradient norm: 683.27675560
INFO:root:[   68] Training loss: 0.47908551, Validation loss: 0.52386039, Gradient norm: 507.03697748
INFO:root:[   69] Training loss: 0.47934716, Validation loss: 0.47686715, Gradient norm: 732.97690535
INFO:root:[   70] Training loss: 0.47200883, Validation loss: 0.51252739, Gradient norm: 518.05224848
INFO:root:[   71] Training loss: 0.48778027, Validation loss: 0.46784745, Gradient norm: 807.22632810
INFO:root:[   72] Training loss: 0.47805331, Validation loss: 0.48033741, Gradient norm: 742.09625376
INFO:root:[   73] Training loss: 0.47919044, Validation loss: 0.53123508, Gradient norm: 725.56543800
INFO:root:[   74] Training loss: 0.49349249, Validation loss: 0.51118699, Gradient norm: 1016.80705826
INFO:root:[   75] Training loss: 0.47922089, Validation loss: 0.46125171, Gradient norm: 731.11697822
INFO:root:[   76] Training loss: 0.46835045, Validation loss: 0.47933474, Gradient norm: 541.42128405
INFO:root:[   77] Training loss: 0.46392148, Validation loss: 0.46662494, Gradient norm: 520.29172228
INFO:root:[   78] Training loss: 0.46690613, Validation loss: 0.55975215, Gradient norm: 687.28647308
INFO:root:[   79] Training loss: 0.47579974, Validation loss: 0.46022432, Gradient norm: 919.47479505
INFO:root:[   80] Training loss: 0.46127325, Validation loss: 0.45389625, Gradient norm: 561.51671929
INFO:root:[   81] Training loss: 0.45436189, Validation loss: 0.45670247, Gradient norm: 412.05018209
INFO:root:[   82] Training loss: 0.46152628, Validation loss: 0.45148787, Gradient norm: 553.12280707
INFO:root:[   83] Training loss: 0.46062123, Validation loss: 0.46317650, Gradient norm: 833.80722822
INFO:root:[   84] Training loss: 0.46206661, Validation loss: 0.44740998, Gradient norm: 947.63364897
INFO:root:[   85] Training loss: 0.45613230, Validation loss: 0.43941475, Gradient norm: 772.61121091
INFO:root:[   86] Training loss: 0.45372357, Validation loss: 0.46389488, Gradient norm: 786.96633965
INFO:root:[   87] Training loss: 0.44869479, Validation loss: 0.46554780, Gradient norm: 653.66677731
INFO:root:[   88] Training loss: 0.46701507, Validation loss: 0.47169433, Gradient norm: 1220.26408062
INFO:root:[   89] Training loss: 0.44899933, Validation loss: 0.42668094, Gradient norm: 831.34119170
INFO:root:[   90] Training loss: 0.44107407, Validation loss: 0.42370743, Gradient norm: 784.79047274
INFO:root:[   91] Training loss: 0.42445741, Validation loss: 0.42464627, Gradient norm: 468.89020580
INFO:root:[   92] Training loss: 0.44352213, Validation loss: 0.41696815, Gradient norm: 1055.63617015
INFO:root:[   93] Training loss: 0.42621271, Validation loss: 0.42104418, Gradient norm: 846.21882495
INFO:root:[   94] Training loss: 0.41889324, Validation loss: 0.45870948, Gradient norm: 683.48304007
INFO:root:[   95] Training loss: 0.43692539, Validation loss: 0.44930058, Gradient norm: 1411.48015622
INFO:root:[   96] Training loss: 0.43345508, Validation loss: 0.40779706, Gradient norm: 1190.74526381
INFO:root:[   97] Training loss: 0.41191592, Validation loss: 0.39908343, Gradient norm: 673.40011668
INFO:root:[   98] Training loss: 0.40573120, Validation loss: 0.46740680, Gradient norm: 601.35737945
INFO:root:[   99] Training loss: 0.41531985, Validation loss: 0.39909677, Gradient norm: 780.86359413
INFO:root:[  100] Training loss: 0.40849250, Validation loss: 0.42294543, Gradient norm: 782.45667282
INFO:root:[  101] Training loss: 0.40948845, Validation loss: 0.39607787, Gradient norm: 777.41364908
INFO:root:[  102] Training loss: 0.40575607, Validation loss: 0.39733729, Gradient norm: 854.52087953
INFO:root:[  103] Training loss: 0.40515874, Validation loss: 0.39157180, Gradient norm: 860.27553333
INFO:root:[  104] Training loss: 0.40916172, Validation loss: 0.43661814, Gradient norm: 845.05488756
INFO:root:[  105] Training loss: 0.39987886, Validation loss: 0.39510928, Gradient norm: 666.60912587
INFO:root:[  106] Training loss: 0.40412902, Validation loss: 0.44480303, Gradient norm: 840.43880179
INFO:root:[  107] Training loss: 0.41868319, Validation loss: 0.38783332, Gradient norm: 1306.58737039
INFO:root:[  108] Training loss: 0.40442524, Validation loss: 0.41251776, Gradient norm: 921.21892556
INFO:root:[  109] Training loss: 0.38850347, Validation loss: 0.39644567, Gradient norm: 446.38833322
INFO:root:[  110] Training loss: 0.39339087, Validation loss: 0.40859767, Gradient norm: 635.65913318
INFO:root:[  111] Training loss: 0.39809901, Validation loss: 0.38361930, Gradient norm: 855.72157356
INFO:root:[  112] Training loss: 0.38686956, Validation loss: 0.40132679, Gradient norm: 582.57215301
INFO:root:[  113] Training loss: 0.38754973, Validation loss: 0.37930517, Gradient norm: 708.25550520
INFO:root:[  114] Training loss: 0.38006533, Validation loss: 0.37725283, Gradient norm: 446.93423619
INFO:root:[  115] Training loss: 0.38661536, Validation loss: 0.37447772, Gradient norm: 739.39553603
INFO:root:[  116] Training loss: 0.39248239, Validation loss: 0.38442941, Gradient norm: 929.48753793
INFO:root:[  117] Training loss: 0.39767407, Validation loss: 0.37852168, Gradient norm: 1081.39378284
INFO:root:[  118] Training loss: 0.37836319, Validation loss: 0.37958532, Gradient norm: 478.95355294
INFO:root:[  119] Training loss: 0.38968320, Validation loss: 0.37801267, Gradient norm: 925.43423803
INFO:root:[  120] Training loss: 0.39454334, Validation loss: 0.38508660, Gradient norm: 1033.07622262
INFO:root:[  121] Training loss: 0.38266209, Validation loss: 0.36963969, Gradient norm: 619.62385041
INFO:root:[  122] Training loss: 0.37736299, Validation loss: 0.36665007, Gradient norm: 543.07029434
INFO:root:[  123] Training loss: 0.39154469, Validation loss: 0.38336840, Gradient norm: 1020.29473632
INFO:root:[  124] Training loss: 0.37954018, Validation loss: 0.36785678, Gradient norm: 697.71012115
INFO:root:[  125] Training loss: 0.36788110, Validation loss: 0.37209854, Gradient norm: 410.26525800
INFO:root:[  126] Training loss: 0.37080774, Validation loss: 0.36062317, Gradient norm: 508.16046427
INFO:root:[  127] Training loss: 0.37120707, Validation loss: 0.36395388, Gradient norm: 654.22613295
INFO:root:[  128] Training loss: 0.37990040, Validation loss: 0.42895621, Gradient norm: 924.50373994
INFO:root:[  129] Training loss: 0.37224398, Validation loss: 0.37752101, Gradient norm: 721.65994541
INFO:root:[  130] Training loss: 0.38151812, Validation loss: 0.38309019, Gradient norm: 1029.85770679
INFO:root:[  131] Training loss: 0.36738729, Validation loss: 0.37793797, Gradient norm: 584.79318137
INFO:root:[  132] Training loss: 0.37413790, Validation loss: 0.39664574, Gradient norm: 830.25915926
INFO:root:[  133] Training loss: 0.36265313, Validation loss: 0.38158809, Gradient norm: 503.33955040
INFO:root:[  134] Training loss: 0.36209426, Validation loss: 0.36721587, Gradient norm: 592.27725937
INFO:root:[  135] Training loss: 0.36041107, Validation loss: 0.35422092, Gradient norm: 490.39297427
INFO:root:[  136] Training loss: 0.36875894, Validation loss: 0.35063028, Gradient norm: 891.08000978
INFO:root:[  137] Training loss: 0.36282119, Validation loss: 0.35323475, Gradient norm: 765.14879157
INFO:root:[  138] Training loss: 0.36106845, Validation loss: 0.40733909, Gradient norm: 686.88815479
INFO:root:[  139] Training loss: 0.36331232, Validation loss: 0.35718154, Gradient norm: 747.47396780
INFO:root:[  140] Training loss: 0.36525877, Validation loss: 0.34784501, Gradient norm: 942.53962623
INFO:root:[  141] Training loss: 0.35006782, Validation loss: 0.34494833, Gradient norm: 391.80086281
INFO:root:[  142] Training loss: 0.34963147, Validation loss: 0.36236527, Gradient norm: 487.16007373
INFO:root:[  143] Training loss: 0.35575667, Validation loss: 0.34778759, Gradient norm: 748.52957511
INFO:root:[  144] Training loss: 0.35194088, Validation loss: 0.34480923, Gradient norm: 721.94175408
INFO:root:[  145] Training loss: 0.38272782, Validation loss: 0.34240717, Gradient norm: 1351.92897962
INFO:root:[  146] Training loss: 0.34142147, Validation loss: 0.34126650, Gradient norm: 228.53048298
INFO:root:[  147] Training loss: 0.34809580, Validation loss: 0.36668691, Gradient norm: 539.99587782
INFO:root:[  148] Training loss: 0.35755563, Validation loss: 0.34195711, Gradient norm: 981.37952415
INFO:root:[  149] Training loss: 0.34370242, Validation loss: 0.34928804, Gradient norm: 498.95384878
INFO:root:[  150] Training loss: 0.35477962, Validation loss: 0.34754706, Gradient norm: 894.70088168
INFO:root:[  151] Training loss: 0.34988797, Validation loss: 0.33638616, Gradient norm: 831.36089968
INFO:root:[  152] Training loss: 0.34115637, Validation loss: 0.33835630, Gradient norm: 501.84626458
INFO:root:[  153] Training loss: 0.33949190, Validation loss: 0.34626964, Gradient norm: 502.47107573
INFO:root:[  154] Training loss: 0.34655283, Validation loss: 0.33499985, Gradient norm: 866.12603031
INFO:root:[  155] Training loss: 0.34061067, Validation loss: 0.33616604, Gradient norm: 580.80686933
INFO:root:[  156] Training loss: 0.33685874, Validation loss: 0.36206646, Gradient norm: 534.40408845
INFO:root:[  157] Training loss: 0.33653908, Validation loss: 0.33183054, Gradient norm: 600.50052897
INFO:root:[  158] Training loss: 0.33672405, Validation loss: 0.34387826, Gradient norm: 657.40649565
INFO:root:[  159] Training loss: 0.34963486, Validation loss: 0.36368342, Gradient norm: 1013.32471721
INFO:root:[  160] Training loss: 0.34031612, Validation loss: 0.32769381, Gradient norm: 798.64315896
INFO:root:[  161] Training loss: 0.33241987, Validation loss: 0.32394598, Gradient norm: 540.19571491
INFO:root:[  162] Training loss: 0.33687946, Validation loss: 0.34672680, Gradient norm: 809.72720718
INFO:root:[  163] Training loss: 0.33214378, Validation loss: 0.32982320, Gradient norm: 556.54424169
INFO:root:[  164] Training loss: 0.33848021, Validation loss: 0.33520144, Gradient norm: 877.79022581
INFO:root:[  165] Training loss: 0.33033795, Validation loss: 0.32083219, Gradient norm: 666.97478957
INFO:root:[  166] Training loss: 0.32350157, Validation loss: 0.33649582, Gradient norm: 385.03217768
INFO:root:[  167] Training loss: 0.33219248, Validation loss: 0.33085817, Gradient norm: 705.77010890
INFO:root:[  168] Training loss: 0.32803986, Validation loss: 0.31805598, Gradient norm: 707.52151091
INFO:root:[  169] Training loss: 0.32067746, Validation loss: 0.32681943, Gradient norm: 510.97016820
INFO:root:[  170] Training loss: 0.32417033, Validation loss: 0.31671890, Gradient norm: 605.44584399
INFO:root:[  171] Training loss: 0.32353597, Validation loss: 0.31918140, Gradient norm: 606.62007000
INFO:root:[  172] Training loss: 0.32045884, Validation loss: 0.32711825, Gradient norm: 551.59294702
INFO:root:[  173] Training loss: 0.32087778, Validation loss: 0.34871818, Gradient norm: 671.02564928
INFO:root:[  174] Training loss: 0.31855586, Validation loss: 0.31353847, Gradient norm: 556.09912755
INFO:root:[  175] Training loss: 0.31688834, Validation loss: 0.31149299, Gradient norm: 645.43156494
INFO:root:[  176] Training loss: 0.32140912, Validation loss: 0.32570673, Gradient norm: 761.29723219
INFO:root:[  177] Training loss: 0.32229399, Validation loss: 0.31242803, Gradient norm: 924.19001932
INFO:root:[  178] Training loss: 0.31020452, Validation loss: 0.30344422, Gradient norm: 366.22393120
INFO:root:[  179] Training loss: 0.31061432, Validation loss: 0.32362417, Gradient norm: 456.78193242
INFO:root:[  180] Training loss: 0.30958039, Validation loss: 0.32022359, Gradient norm: 544.30671062
INFO:root:[  181] Training loss: 0.31170600, Validation loss: 0.30528316, Gradient norm: 665.24107138
INFO:root:[  182] Training loss: 0.32332844, Validation loss: 0.52168806, Gradient norm: 867.58795142
INFO:root:[  183] Training loss: 0.34328288, Validation loss: 0.34183315, Gradient norm: 1103.76736272
INFO:root:[  184] Training loss: 0.31121568, Validation loss: 0.29923573, Gradient norm: 495.46925784
INFO:root:[  185] Training loss: 0.30674714, Validation loss: 0.31348569, Gradient norm: 479.09105912
INFO:root:[  186] Training loss: 0.30593078, Validation loss: 0.29934705, Gradient norm: 540.62186483
INFO:root:[  187] Training loss: 0.30749216, Validation loss: 0.30253246, Gradient norm: 589.81006335
INFO:root:[  188] Training loss: 0.30099102, Validation loss: 0.29965998, Gradient norm: 348.70735215
INFO:root:[  189] Training loss: 0.30005536, Validation loss: 0.30752518, Gradient norm: 370.34131877
INFO:root:[  190] Training loss: 0.30471421, Validation loss: 0.31610220, Gradient norm: 664.72480019
INFO:root:[  191] Training loss: 0.29951323, Validation loss: 0.29503281, Gradient norm: 549.43135555
INFO:root:[  192] Training loss: 0.30943724, Validation loss: 0.29428685, Gradient norm: 825.73656559
INFO:root:[  193] Training loss: 0.29651598, Validation loss: 0.29205567, Gradient norm: 483.86158650
INFO:root:[  194] Training loss: 0.29990593, Validation loss: 0.28875042, Gradient norm: 665.61998295
INFO:root:[  195] Training loss: 0.29944761, Validation loss: 0.32445121, Gradient norm: 661.35607792
INFO:root:[  196] Training loss: 0.29969231, Validation loss: 0.28921689, Gradient norm: 684.97904416
INFO:root:[  197] Training loss: 0.29585170, Validation loss: 0.29093125, Gradient norm: 559.08015049
INFO:root:[  198] Training loss: 0.29188936, Validation loss: 0.30533303, Gradient norm: 509.37327040
INFO:root:[  199] Training loss: 0.29170673, Validation loss: 0.30365404, Gradient norm: 551.07018693
INFO:root:[  200] Training loss: 0.30483032, Validation loss: 0.31809827, Gradient norm: 983.96811371
INFO:root:[  201] Training loss: 0.29239679, Validation loss: 0.28841424, Gradient norm: 597.55515872
INFO:root:[  202] Training loss: 0.29008459, Validation loss: 0.34262989, Gradient norm: 587.76673049
INFO:root:[  203] Training loss: 0.29389124, Validation loss: 0.29505399, Gradient norm: 708.23909764
INFO:root:[  204] Training loss: 0.28413201, Validation loss: 0.28424786, Gradient norm: 406.68592886
INFO:root:[  205] Training loss: 0.28350831, Validation loss: 0.29030496, Gradient norm: 472.16942607
INFO:root:[  206] Training loss: 0.28055526, Validation loss: 0.29543099, Gradient norm: 322.40174013
INFO:root:[  207] Training loss: 0.28299847, Validation loss: 0.27455179, Gradient norm: 532.32861494
INFO:root:[  208] Training loss: 0.28744925, Validation loss: 0.28143374, Gradient norm: 674.73237422
INFO:root:[  209] Training loss: 0.28225524, Validation loss: 0.27411552, Gradient norm: 638.42490799
INFO:root:[  210] Training loss: 0.27838987, Validation loss: 0.29688591, Gradient norm: 476.85270784
INFO:root:[  211] Training loss: 0.28223810, Validation loss: 0.27546942, Gradient norm: 638.86819374
INFO:root:[  212] Training loss: 0.27805576, Validation loss: 0.27170556, Gradient norm: 523.45324129
INFO:root:[  213] Training loss: 0.27359284, Validation loss: 0.27218024, Gradient norm: 407.69932246
INFO:root:[  214] Training loss: 0.28731742, Validation loss: 0.28227667, Gradient norm: 910.18139990
INFO:root:[  215] Training loss: 0.27471181, Validation loss: 0.27686773, Gradient norm: 487.76392627
INFO:root:[  216] Training loss: 0.27335833, Validation loss: 0.27892254, Gradient norm: 516.48414650
INFO:root:[  217] Training loss: 0.27280587, Validation loss: 0.26940578, Gradient norm: 582.61082237
INFO:root:[  218] Training loss: 0.27973515, Validation loss: 0.28115647, Gradient norm: 817.02399835
INFO:root:[  219] Training loss: 0.27336266, Validation loss: 0.28174525, Gradient norm: 661.73557514
INFO:root:[  220] Training loss: 0.26947290, Validation loss: 0.28221612, Gradient norm: 542.50840769
INFO:root:[  221] Training loss: 0.29025670, Validation loss: 0.27645450, Gradient norm: 1103.26027014
INFO:root:[  222] Training loss: 0.26638233, Validation loss: 0.27129685, Gradient norm: 429.62559587
INFO:root:[  223] Training loss: 0.26349133, Validation loss: 0.26222262, Gradient norm: 303.98218215
INFO:root:[  224] Training loss: 0.26475844, Validation loss: 0.27861601, Gradient norm: 466.19169753
INFO:root:[  225] Training loss: 0.26889611, Validation loss: 0.27704499, Gradient norm: 660.07961766
INFO:root:[  226] Training loss: 0.26824321, Validation loss: 0.31455924, Gradient norm: 589.02135096
INFO:root:[  227] Training loss: 0.26892700, Validation loss: 0.25813256, Gradient norm: 702.84872503
INFO:root:[  228] Training loss: 0.26642178, Validation loss: 0.26088573, Gradient norm: 666.49285751
INFO:root:[  229] Training loss: 0.25889410, Validation loss: 0.26073286, Gradient norm: 373.31425567
INFO:root:[  230] Training loss: 0.25859395, Validation loss: 0.26371749, Gradient norm: 411.89316762
INFO:root:[  231] Training loss: 0.25991450, Validation loss: 0.25405525, Gradient norm: 579.98280894
INFO:root:[  232] Training loss: 0.25581295, Validation loss: 0.25419489, Gradient norm: 390.56071729
INFO:root:[  233] Training loss: 0.28200369, Validation loss: 0.25199073, Gradient norm: 1080.24567851
INFO:root:[  234] Training loss: 0.25805694, Validation loss: 0.25403193, Gradient norm: 444.80107537
INFO:root:[  235] Training loss: 0.25306846, Validation loss: 0.25035906, Gradient norm: 383.96997075
INFO:root:[  236] Training loss: 0.25096270, Validation loss: 0.24823729, Gradient norm: 329.87199855
INFO:root:[  237] Training loss: 0.25033092, Validation loss: 0.25322095, Gradient norm: 336.34149126
INFO:root:[  238] Training loss: 0.25553368, Validation loss: 0.24645420, Gradient norm: 678.81442490
INFO:root:[  239] Training loss: 0.26346008, Validation loss: 0.28453467, Gradient norm: 822.84622991
INFO:root:[  240] Training loss: 0.25007924, Validation loss: 0.25529359, Gradient norm: 380.69160937
INFO:root:[  241] Training loss: 0.25754761, Validation loss: 0.24444959, Gradient norm: 715.85958384
INFO:root:[  242] Training loss: 0.24839125, Validation loss: 0.24924407, Gradient norm: 396.87445021
INFO:root:[  243] Training loss: 0.24668115, Validation loss: 0.26098631, Gradient norm: 400.88733411
INFO:root:[  244] Training loss: 0.24851967, Validation loss: 0.25632303, Gradient norm: 527.50401662
INFO:root:[  245] Training loss: 0.25454114, Validation loss: 0.24347357, Gradient norm: 814.94452671
INFO:root:[  246] Training loss: 0.24606242, Validation loss: 0.25698471, Gradient norm: 563.56234518
INFO:root:[  247] Training loss: 0.24684124, Validation loss: 0.24360489, Gradient norm: 548.92645452
INFO:root:[  248] Training loss: 0.24422314, Validation loss: 0.24204460, Gradient norm: 533.02429800
INFO:root:[  249] Training loss: 0.24151380, Validation loss: 0.24779396, Gradient norm: 469.65097784
INFO:root:[  250] Training loss: 0.24241588, Validation loss: 0.23965791, Gradient norm: 521.00489186
INFO:root:[  251] Training loss: 0.23677733, Validation loss: 0.24249770, Gradient norm: 267.86413686
INFO:root:[  252] Training loss: 0.24754507, Validation loss: 0.23590662, Gradient norm: 828.69642710
INFO:root:[  253] Training loss: 0.23744247, Validation loss: 0.24924513, Gradient norm: 455.49066489
INFO:root:[  254] Training loss: 0.23370480, Validation loss: 0.23195185, Gradient norm: 265.76719650
INFO:root:[  255] Training loss: 0.26900433, Validation loss: 0.26770729, Gradient norm: 1101.39909920
INFO:root:[  256] Training loss: 0.23807413, Validation loss: 0.23393873, Gradient norm: 487.41167205
INFO:root:[  257] Training loss: 0.23280305, Validation loss: 0.23034879, Gradient norm: 276.23929109
INFO:root:[  258] Training loss: 0.23050310, Validation loss: 0.22976398, Gradient norm: 233.76594643
INFO:root:[  259] Training loss: 0.23325787, Validation loss: 0.22771909, Gradient norm: 436.80111279
INFO:root:[  260] Training loss: 0.23349798, Validation loss: 0.23666873, Gradient norm: 498.72175933
INFO:root:[  261] Training loss: 0.23728674, Validation loss: 0.22931313, Gradient norm: 703.45357472
INFO:root:[  262] Training loss: 0.22930218, Validation loss: 0.23549420, Gradient norm: 366.89930872
INFO:root:[  263] Training loss: 0.23162803, Validation loss: 0.22905559, Gradient norm: 566.92559596
INFO:root:[  264] Training loss: 0.22639986, Validation loss: 0.22441724, Gradient norm: 394.89289530
INFO:root:[  265] Training loss: 0.22917698, Validation loss: 0.22514653, Gradient norm: 571.20205446
INFO:root:[  266] Training loss: 0.23102110, Validation loss: 0.22442429, Gradient norm: 652.07622171
INFO:root:[  267] Training loss: 0.22769068, Validation loss: 0.24487162, Gradient norm: 545.67327589
INFO:root:[  268] Training loss: 0.22820391, Validation loss: 0.22289812, Gradient norm: 544.69657593
INFO:root:[  269] Training loss: 0.22312725, Validation loss: 0.22030313, Gradient norm: 412.84574964
INFO:root:[  270] Training loss: 0.22378844, Validation loss: 0.21756345, Gradient norm: 459.27065650
INFO:root:[  271] Training loss: 0.22117429, Validation loss: 0.21840096, Gradient norm: 365.97383306
INFO:root:[  272] Training loss: 0.22518013, Validation loss: 0.21727338, Gradient norm: 660.38024880
INFO:root:[  273] Training loss: 0.21815510, Validation loss: 0.23135868, Gradient norm: 374.60931697
INFO:root:[  274] Training loss: 0.22854021, Validation loss: 0.28798155, Gradient norm: 754.59804755
INFO:root:[  275] Training loss: 0.23265698, Validation loss: 0.22412076, Gradient norm: 784.72221828
INFO:root:[  276] Training loss: 0.21933754, Validation loss: 0.22069071, Gradient norm: 506.95751218
INFO:root:[  277] Training loss: 0.21546126, Validation loss: 0.24144971, Gradient norm: 367.75332460
INFO:root:[  278] Training loss: 0.21992375, Validation loss: 0.21839149, Gradient norm: 511.50968396
INFO:root:[  279] Training loss: 0.21625039, Validation loss: 0.21268553, Gradient norm: 476.73505035
INFO:root:[  280] Training loss: 0.21328891, Validation loss: 0.24686049, Gradient norm: 385.51281243
INFO:root:[  281] Training loss: 0.21957738, Validation loss: 0.20886349, Gradient norm: 581.52190500
INFO:root:[  282] Training loss: 0.21294539, Validation loss: 0.21067339, Gradient norm: 415.62432391
INFO:root:[  283] Training loss: 0.21194930, Validation loss: 0.20774019, Gradient norm: 440.62724938
INFO:root:[  284] Training loss: 0.20924070, Validation loss: 0.21746434, Gradient norm: 334.47042050
INFO:root:[  285] Training loss: 0.21147347, Validation loss: 0.20903632, Gradient norm: 476.93957223
INFO:root:[  286] Training loss: 0.20705161, Validation loss: 0.20781605, Gradient norm: 319.01675026
INFO:root:[  287] Training loss: 0.20855125, Validation loss: 0.20915369, Gradient norm: 433.91139051
INFO:root:[  288] Training loss: 0.20981953, Validation loss: 0.20420833, Gradient norm: 531.54205525
INFO:root:[  289] Training loss: 0.20360710, Validation loss: 0.20267188, Gradient norm: 317.81823049
INFO:root:[  290] Training loss: 0.22657030, Validation loss: 0.20571347, Gradient norm: 1001.79433673
INFO:root:[  291] Training loss: 0.20474858, Validation loss: 0.21237062, Gradient norm: 385.36590996
INFO:root:[  292] Training loss: 0.20622536, Validation loss: 0.19937978, Gradient norm: 511.84314664
INFO:root:[  293] Training loss: 0.20478570, Validation loss: 0.20077116, Gradient norm: 493.61026351
INFO:root:[  294] Training loss: 0.20071070, Validation loss: 0.20009269, Gradient norm: 262.65464433
INFO:root:[  295] Training loss: 0.20245587, Validation loss: 0.19836000, Gradient norm: 438.80558566
INFO:root:[  296] Training loss: 0.20203532, Validation loss: 0.22342558, Gradient norm: 462.36952894
INFO:root:[  297] Training loss: 0.21006211, Validation loss: 0.19742891, Gradient norm: 745.28820399
INFO:root:[  298] Training loss: 0.20072789, Validation loss: 0.21419114, Gradient norm: 439.28488194
INFO:root:[  299] Training loss: 0.19973157, Validation loss: 0.19440175, Gradient norm: 428.00744018
INFO:root:[  300] Training loss: 0.19630414, Validation loss: 0.20334751, Gradient norm: 376.05090273
INFO:root:[  301] Training loss: 0.19540420, Validation loss: 0.19558849, Gradient norm: 403.39621107
INFO:root:[  302] Training loss: 0.20042128, Validation loss: 0.22066722, Gradient norm: 590.75464052
INFO:root:[  303] Training loss: 0.21242844, Validation loss: 0.19296783, Gradient norm: 806.97764773
INFO:root:[  304] Training loss: 0.19263737, Validation loss: 0.19076666, Gradient norm: 259.52071875
INFO:root:[  305] Training loss: 0.19201490, Validation loss: 0.19774156, Gradient norm: 317.27026789
INFO:root:[  306] Training loss: 0.19224735, Validation loss: 0.19394836, Gradient norm: 373.20434292
INFO:root:[  307] Training loss: 0.19151057, Validation loss: 0.18917436, Gradient norm: 374.46411310
INFO:root:[  308] Training loss: 0.18787885, Validation loss: 0.18723305, Gradient norm: 258.55781076
INFO:root:[  309] Training loss: 0.19035294, Validation loss: 0.18726973, Gradient norm: 399.89395849
INFO:root:[  310] Training loss: 0.19305663, Validation loss: 0.19608428, Gradient norm: 644.54339149
INFO:root:[  311] Training loss: 0.19948686, Validation loss: 0.18530126, Gradient norm: 769.26333544
INFO:root:[  312] Training loss: 0.18583329, Validation loss: 0.18360469, Gradient norm: 247.32138783
INFO:root:[  313] Training loss: 0.18643013, Validation loss: 0.18653776, Gradient norm: 335.62211688
INFO:root:[  314] Training loss: 0.19082286, Validation loss: 0.19047980, Gradient norm: 622.98739385
INFO:root:[  315] Training loss: 0.18989507, Validation loss: 0.18837722, Gradient norm: 566.99750778
INFO:root:[  316] Training loss: 0.18138337, Validation loss: 0.18300156, Gradient norm: 164.90215623
INFO:root:[  317] Training loss: 0.20201485, Validation loss: 0.18041761, Gradient norm: 824.65611345
INFO:root:[  318] Training loss: 0.18245433, Validation loss: 0.18180259, Gradient norm: 309.52239412
INFO:root:[  319] Training loss: 0.18583634, Validation loss: 0.18041724, Gradient norm: 424.56338363
INFO:root:[  320] Training loss: 0.18639100, Validation loss: 0.17777649, Gradient norm: 486.24449162
INFO:root:[  321] Training loss: 0.17858979, Validation loss: 0.18911428, Gradient norm: 166.04568936
INFO:root:[  322] Training loss: 0.18193741, Validation loss: 0.18684450, Gradient norm: 473.87739231
INFO:root:[  323] Training loss: 0.17787208, Validation loss: 0.17558663, Gradient norm: 265.37993907
INFO:root:[  324] Training loss: 0.17751228, Validation loss: 0.19704107, Gradient norm: 355.71348587
INFO:root:[  325] Training loss: 0.17931315, Validation loss: 0.17825610, Gradient norm: 457.95207117
INFO:root:[  326] Training loss: 0.18482867, Validation loss: 0.23657723, Gradient norm: 568.21860797
INFO:root:[  327] Training loss: 0.18120315, Validation loss: 0.17337168, Gradient norm: 489.70217481
INFO:root:[  328] Training loss: 0.18006818, Validation loss: 0.17314893, Gradient norm: 517.81174540
INFO:root:[  329] Training loss: 0.17350563, Validation loss: 0.17175262, Gradient norm: 255.02595622
INFO:root:[  330] Training loss: 0.17594309, Validation loss: 0.17323434, Gradient norm: 429.14879976
INFO:root:[  331] Training loss: 0.17171897, Validation loss: 0.18112153, Gradient norm: 285.16266565
INFO:root:[  332] Training loss: 0.17244518, Validation loss: 0.16843303, Gradient norm: 317.03436664
INFO:root:[  333] Training loss: 0.17198017, Validation loss: 0.16801873, Gradient norm: 338.34862774
INFO:root:[  334] Training loss: 0.17077830, Validation loss: 0.16833270, Gradient norm: 325.01006967
INFO:root:[  335] Training loss: 0.19239995, Validation loss: 0.17014290, Gradient norm: 867.60549564
INFO:root:[  336] Training loss: 0.17039057, Validation loss: 0.16782250, Gradient norm: 288.76588691
INFO:root:[  337] Training loss: 0.16766584, Validation loss: 0.17012191, Gradient norm: 276.54315869
INFO:root:[  338] Training loss: 0.16635446, Validation loss: 0.16377044, Gradient norm: 250.11527415
INFO:root:[  339] Training loss: 0.16755181, Validation loss: 0.17384405, Gradient norm: 323.48987064
INFO:root:[  340] Training loss: 0.17099600, Validation loss: 0.16420829, Gradient norm: 517.60192738
INFO:root:[  341] Training loss: 0.16367056, Validation loss: 0.16380332, Gradient norm: 266.08014264
INFO:root:[  342] Training loss: 0.16660973, Validation loss: 0.17162184, Gradient norm: 420.93571682
INFO:root:[  343] Training loss: 0.16782257, Validation loss: 0.16655681, Gradient norm: 521.84777875
INFO:root:[  344] Training loss: 0.16283610, Validation loss: 0.15925566, Gradient norm: 331.82266515
INFO:root:[  345] Training loss: 0.16651009, Validation loss: 0.15965825, Gradient norm: 527.66140156
INFO:root:[  346] Training loss: 0.16337317, Validation loss: 0.15907789, Gradient norm: 359.86456822
INFO:root:[  347] Training loss: 0.17004516, Validation loss: 0.16272189, Gradient norm: 613.45718706
INFO:root:[  348] Training loss: 0.15933545, Validation loss: 0.16712511, Gradient norm: 241.04649155
INFO:root:[  349] Training loss: 0.15885812, Validation loss: 0.16024601, Gradient norm: 282.52048265
INFO:root:[  350] Training loss: 0.16156938, Validation loss: 0.19422501, Gradient norm: 429.03683995
INFO:root:[  351] Training loss: 0.16832641, Validation loss: 0.15760245, Gradient norm: 632.30405837
INFO:root:[  352] Training loss: 0.15674261, Validation loss: 0.15886814, Gradient norm: 248.03740758
INFO:root:[  353] Training loss: 0.15666119, Validation loss: 0.15366677, Gradient norm: 319.87941210
INFO:root:[  354] Training loss: 0.15463786, Validation loss: 0.16447837, Gradient norm: 223.69681196
INFO:root:[  355] Training loss: 0.16709739, Validation loss: 0.15846901, Gradient norm: 744.00195680
INFO:root:[  356] Training loss: 0.15763304, Validation loss: 0.15358961, Gradient norm: 422.76914769
INFO:root:[  357] Training loss: 0.15566128, Validation loss: 0.15139531, Gradient norm: 400.56620355
INFO:root:[  358] Training loss: 0.15284290, Validation loss: 0.15101105, Gradient norm: 278.80311088
INFO:root:[  359] Training loss: 0.15093544, Validation loss: 0.15399433, Gradient norm: 233.47077194
INFO:root:[  360] Training loss: 0.15328095, Validation loss: 0.15158397, Gradient norm: 368.83743271
INFO:root:[  361] Training loss: 0.15531619, Validation loss: 0.15002526, Gradient norm: 457.31505323
INFO:root:[  362] Training loss: 0.15942883, Validation loss: 0.15266279, Gradient norm: 553.00334956
INFO:root:[  363] Training loss: 0.14944175, Validation loss: 0.15317962, Gradient norm: 261.02415354
INFO:root:[  364] Training loss: 0.15013573, Validation loss: 0.14793978, Gradient norm: 313.03666933
INFO:root:[  365] Training loss: 0.15197914, Validation loss: 0.14686169, Gradient norm: 436.79223879
INFO:root:[  366] Training loss: 0.14746120, Validation loss: 0.14582072, Gradient norm: 230.42747029
INFO:root:[  367] Training loss: 0.14601539, Validation loss: 0.17995096, Gradient norm: 224.40969513
INFO:root:[  368] Training loss: 0.16552355, Validation loss: 0.14970561, Gradient norm: 707.39401454
INFO:root:[  369] Training loss: 0.14550001, Validation loss: 0.14411667, Gradient norm: 233.91150204
INFO:root:[  370] Training loss: 0.14396329, Validation loss: 0.14530404, Gradient norm: 161.64371811
INFO:root:[  371] Training loss: 0.14418806, Validation loss: 0.14350112, Gradient norm: 211.76348421
INFO:root:[  372] Training loss: 0.14453731, Validation loss: 0.14277637, Gradient norm: 306.69722312
INFO:root:[  373] Training loss: 0.15588915, Validation loss: 0.14154468, Gradient norm: 596.12608339
INFO:root:[  374] Training loss: 0.14224040, Validation loss: 0.15076105, Gradient norm: 155.93890823
INFO:root:[  375] Training loss: 0.14155627, Validation loss: 0.13988352, Gradient norm: 206.16408718
INFO:root:[  376] Training loss: 0.14128990, Validation loss: 0.13976643, Gradient norm: 232.79016846
INFO:root:[  377] Training loss: 0.15778697, Validation loss: 0.14062630, Gradient norm: 667.86350506
INFO:root:[  378] Training loss: 0.14028535, Validation loss: 0.13907036, Gradient norm: 190.53557057
INFO:root:[  379] Training loss: 0.13814597, Validation loss: 0.13840005, Gradient norm: 149.25653498
INFO:root:[  380] Training loss: 0.13872540, Validation loss: 0.15213167, Gradient norm: 239.90783219
INFO:root:[  381] Training loss: 0.14689112, Validation loss: 0.13881350, Gradient norm: 562.43166028
INFO:root:[  382] Training loss: 0.13959904, Validation loss: 0.15254265, Gradient norm: 294.63591225
INFO:root:[  383] Training loss: 0.13809984, Validation loss: 0.13654512, Gradient norm: 254.16892753
INFO:root:[  384] Training loss: 0.13586503, Validation loss: 0.13655040, Gradient norm: 175.26047637
INFO:root:[  385] Training loss: 0.14464898, Validation loss: 0.20304171, Gradient norm: 495.12721067
INFO:root:[  386] Training loss: 0.14432802, Validation loss: 0.13538657, Gradient norm: 413.85342800
INFO:root:[  387] Training loss: 0.13422736, Validation loss: 0.14303071, Gradient norm: 145.73119127
INFO:root:[  388] Training loss: 0.13529792, Validation loss: 0.13251220, Gradient norm: 285.25836366
INFO:root:[  389] Training loss: 0.13354228, Validation loss: 0.14458939, Gradient norm: 219.08146904
INFO:root:[  390] Training loss: 0.13377330, Validation loss: 0.13208053, Gradient norm: 251.25454900
INFO:root:[  391] Training loss: 0.13198247, Validation loss: 0.13070678, Gradient norm: 227.54774836
INFO:root:[  392] Training loss: 0.13488662, Validation loss: 0.13838296, Gradient norm: 348.72839302
INFO:root:[  393] Training loss: 0.13438233, Validation loss: 0.13270469, Gradient norm: 343.09212936
INFO:root:[  394] Training loss: 0.13721272, Validation loss: 0.15997790, Gradient norm: 493.11473029
INFO:root:[  395] Training loss: 0.13346843, Validation loss: 0.13525486, Gradient norm: 321.96196680
INFO:root:[  396] Training loss: 0.13033013, Validation loss: 0.15882940, Gradient norm: 240.51288428
INFO:root:[  397] Training loss: 0.13469307, Validation loss: 0.12771629, Gradient norm: 375.58491674
INFO:root:[  398] Training loss: 0.12835316, Validation loss: 0.14845177, Gradient norm: 178.36213894
INFO:root:[  399] Training loss: 0.13217530, Validation loss: 0.12715873, Gradient norm: 345.18018066
INFO:root:[  400] Training loss: 0.13027904, Validation loss: 0.13824604, Gradient norm: 339.14704022
INFO:root:[  401] Training loss: 0.13499747, Validation loss: 0.13704411, Gradient norm: 500.16221839
INFO:root:[  402] Training loss: 0.13082367, Validation loss: 0.12674969, Gradient norm: 354.40285995
INFO:root:[  403] Training loss: 0.12636157, Validation loss: 0.12816143, Gradient norm: 176.99981016
INFO:root:[  404] Training loss: 0.12569285, Validation loss: 0.12515414, Gradient norm: 181.15080696
INFO:root:[  405] Training loss: 0.12588370, Validation loss: 0.12638122, Gradient norm: 248.65056122
INFO:root:[  406] Training loss: 0.12597279, Validation loss: 0.13579034, Gradient norm: 273.22314735
INFO:root:[  407] Training loss: 0.12717831, Validation loss: 0.12382017, Gradient norm: 365.41792337
INFO:root:[  408] Training loss: 0.12567467, Validation loss: 0.12353662, Gradient norm: 264.97694502
INFO:root:[  409] Training loss: 0.12369279, Validation loss: 0.12637448, Gradient norm: 253.21064709
INFO:root:[  410] Training loss: 0.12466906, Validation loss: 0.16206403, Gradient norm: 277.13956671
INFO:root:[  411] Training loss: 0.13843512, Validation loss: 0.12213358, Gradient norm: 495.14963110
INFO:root:[  412] Training loss: 0.12250809, Validation loss: 0.12049969, Gradient norm: 195.68729215
INFO:root:[  413] Training loss: 0.12147671, Validation loss: 0.13848960, Gradient norm: 164.36080053
INFO:root:[  414] Training loss: 0.12387299, Validation loss: 0.11949839, Gradient norm: 314.99190294
INFO:root:[  415] Training loss: 0.12006638, Validation loss: 0.12185975, Gradient norm: 187.32108737
INFO:root:[  416] Training loss: 0.12005061, Validation loss: 0.11948143, Gradient norm: 182.56982653
INFO:root:[  417] Training loss: 0.12921283, Validation loss: 0.12675789, Gradient norm: 457.05451013
INFO:root:[  418] Training loss: 0.11951977, Validation loss: 0.11822606, Gradient norm: 163.68032835
INFO:root:[  419] Training loss: 0.11919344, Validation loss: 0.12110368, Gradient norm: 182.57106760
INFO:root:[  420] Training loss: 0.11864552, Validation loss: 0.12392788, Gradient norm: 197.61826455
INFO:root:[  421] Training loss: 0.11849992, Validation loss: 0.11625603, Gradient norm: 215.37310857
INFO:root:[  422] Training loss: 0.11718147, Validation loss: 0.11997484, Gradient norm: 163.65943551
INFO:root:[  423] Training loss: 0.11836100, Validation loss: 0.11814910, Gradient norm: 281.19385797
INFO:root:[  424] Training loss: 0.12891856, Validation loss: 0.11558707, Gradient norm: 540.78388797
INFO:root:[  425] Training loss: 0.11654511, Validation loss: 0.11506802, Gradient norm: 172.64200673
INFO:root:[  426] Training loss: 0.11613000, Validation loss: 0.12044963, Gradient norm: 188.99072342
INFO:root:[  427] Training loss: 0.11542003, Validation loss: 0.11652184, Gradient norm: 199.02505175
INFO:root:[  428] Training loss: 0.11358256, Validation loss: 0.11308864, Gradient norm: 129.20987212
INFO:root:[  429] Training loss: 0.11407189, Validation loss: 0.13681777, Gradient norm: 176.21141422
INFO:root:[  430] Training loss: 0.12319329, Validation loss: 0.11276389, Gradient norm: 466.23818127
INFO:root:[  431] Training loss: 0.11338667, Validation loss: 0.11297778, Gradient norm: 167.40893675
INFO:root:[  432] Training loss: 0.11298892, Validation loss: 0.13767174, Gradient norm: 175.03598810
INFO:root:[  433] Training loss: 0.11381259, Validation loss: 0.11128669, Gradient norm: 212.51379182
INFO:root:[  434] Training loss: 0.11234843, Validation loss: 0.11385019, Gradient norm: 175.52310277
INFO:root:[  435] Training loss: 0.11691224, Validation loss: 0.11103604, Gradient norm: 392.61096964
INFO:root:[  436] Training loss: 0.11070723, Validation loss: 0.11682351, Gradient norm: 154.19983671
INFO:root:[  437] Training loss: 0.11085576, Validation loss: 0.10991157, Gradient norm: 207.53853721
INFO:root:[  438] Training loss: 0.11043255, Validation loss: 0.10980591, Gradient norm: 193.01519792
INFO:root:[  439] Training loss: 0.10957370, Validation loss: 0.10820146, Gradient norm: 211.60100762
INFO:root:[  440] Training loss: 0.12238264, Validation loss: 0.12153813, Gradient norm: 473.64194541
INFO:root:[  441] Training loss: 0.11060225, Validation loss: 0.10831990, Gradient norm: 223.70379364
INFO:root:[  442] Training loss: 0.10693187, Validation loss: 0.10796488, Gradient norm: 74.18638287
INFO:root:[  443] Training loss: 0.10779571, Validation loss: 0.10576462, Gradient norm: 185.57679680
INFO:root:[  444] Training loss: 0.10811260, Validation loss: 0.10901063, Gradient norm: 226.18718143
INFO:root:[  445] Training loss: 0.10612145, Validation loss: 0.10706298, Gradient norm: 154.21464307
INFO:root:[  446] Training loss: 0.10583815, Validation loss: 0.10531163, Gradient norm: 155.66066800
INFO:root:[  447] Training loss: 0.10613418, Validation loss: 0.10748418, Gradient norm: 214.94406568
INFO:root:[  448] Training loss: 0.10606826, Validation loss: 0.10657193, Gradient norm: 238.25043442
INFO:root:[  449] Training loss: 0.10788105, Validation loss: 0.10425597, Gradient norm: 334.74861515
INFO:root:[  450] Training loss: 0.10495275, Validation loss: 0.12216993, Gradient norm: 233.24194320
INFO:root:[  451] Training loss: 0.10530356, Validation loss: 0.11218374, Gradient norm: 267.60973247
INFO:root:[  452] Training loss: 0.10536897, Validation loss: 0.10203432, Gradient norm: 262.19264822
INFO:root:[  453] Training loss: 0.10766838, Validation loss: 0.15157491, Gradient norm: 308.57103422
INFO:root:[  454] Training loss: 0.11711506, Validation loss: 0.10341889, Gradient norm: 424.00581996
INFO:root:[  455] Training loss: 0.10183081, Validation loss: 0.10088552, Gradient norm: 96.10277518
INFO:root:[  456] Training loss: 0.10065614, Validation loss: 0.10107532, Gradient norm: 77.86092466
INFO:root:[  457] Training loss: 0.10192886, Validation loss: 0.10037868, Gradient norm: 193.24505555
INFO:root:[  458] Training loss: 0.10096739, Validation loss: 0.09938072, Gradient norm: 181.24023228
INFO:root:[  459] Training loss: 0.10373471, Validation loss: 0.10011227, Gradient norm: 294.09027065
INFO:root:[  460] Training loss: 0.10076617, Validation loss: 0.09944548, Gradient norm: 200.61382866
INFO:root:[  461] Training loss: 0.10258448, Validation loss: 0.09892339, Gradient norm: 296.98434870
INFO:root:[  462] Training loss: 0.10265380, Validation loss: 0.09804936, Gradient norm: 347.33059348
INFO:root:[  463] Training loss: 0.09888829, Validation loss: 0.09914445, Gradient norm: 175.43016050
INFO:root:[  464] Training loss: 0.10062510, Validation loss: 0.10586667, Gradient norm: 299.06186693
INFO:root:[  465] Training loss: 0.10184669, Validation loss: 0.11448896, Gradient norm: 330.83538857
INFO:root:[  466] Training loss: 0.10335348, Validation loss: 0.09955025, Gradient norm: 377.49140957
INFO:root:[  467] Training loss: 0.10001359, Validation loss: 0.09766597, Gradient norm: 269.51392017
INFO:root:[  468] Training loss: 0.09688405, Validation loss: 0.09811225, Gradient norm: 154.74039579
INFO:root:[  469] Training loss: 0.09949298, Validation loss: 0.09631894, Gradient norm: 329.03679301
INFO:root:[  470] Training loss: 0.10270428, Validation loss: 0.09825272, Gradient norm: 354.17753180
INFO:root:[  471] Training loss: 0.09645212, Validation loss: 0.09917334, Gradient norm: 187.07079318
INFO:root:[  472] Training loss: 0.09951539, Validation loss: 0.09543941, Gradient norm: 297.79599654
INFO:root:[  473] Training loss: 0.09754879, Validation loss: 0.10346390, Gradient norm: 290.37246807
INFO:root:[  474] Training loss: 0.09939971, Validation loss: 0.10046098, Gradient norm: 375.12707883
INFO:root:[  475] Training loss: 0.09599030, Validation loss: 0.10362428, Gradient norm: 228.57984863
INFO:root:[  476] Training loss: 0.11355175, Validation loss: 0.11054651, Gradient norm: 561.34136435
INFO:root:[  477] Training loss: 0.09758571, Validation loss: 0.09427667, Gradient norm: 185.79022258
INFO:root:[  478] Training loss: 0.09389235, Validation loss: 0.09469584, Gradient norm: 117.50707100
INFO:root:[  479] Training loss: 0.09278424, Validation loss: 0.09317952, Gradient norm: 112.04017408
INFO:root:[  480] Training loss: 0.09439700, Validation loss: 0.09583988, Gradient norm: 235.87954442
INFO:root:[  481] Training loss: 0.09498540, Validation loss: 0.09243601, Gradient norm: 255.51063649
INFO:root:[  482] Training loss: 0.09223731, Validation loss: 0.10892051, Gradient norm: 197.15367433
INFO:root:[  483] Training loss: 0.09469102, Validation loss: 0.11518229, Gradient norm: 279.15668347
INFO:root:[  484] Training loss: 0.10974148, Validation loss: 0.09500525, Gradient norm: 532.54831070
INFO:root:[  485] Training loss: 0.09171939, Validation loss: 0.09204874, Gradient norm: 101.71246945
INFO:root:[  486] Training loss: 0.09100858, Validation loss: 0.09278184, Gradient norm: 171.45188386
INFO:root:[  487] Training loss: 0.09094107, Validation loss: 0.09069442, Gradient norm: 205.43561802
INFO:root:[  488] Training loss: 0.09122820, Validation loss: 0.09299932, Gradient norm: 230.83047567
INFO:root:[  489] Training loss: 0.09189547, Validation loss: 0.09900078, Gradient norm: 281.07201575
INFO:root:[  490] Training loss: 0.11077380, Validation loss: 0.09309808, Gradient norm: 555.38157585
INFO:root:[  491] Training loss: 0.09056110, Validation loss: 0.08963711, Gradient norm: 100.47190446
INFO:root:[  492] Training loss: 0.08821022, Validation loss: 0.08857081, Gradient norm: 68.40814047
INFO:root:[  493] Training loss: 0.09039183, Validation loss: 0.08916071, Gradient norm: 251.74236649
INFO:root:[  494] Training loss: 0.08999061, Validation loss: 0.09802260, Gradient norm: 258.49083017
INFO:root:[  495] Training loss: 0.09059626, Validation loss: 0.08798101, Gradient norm: 241.02669200
INFO:root:[  496] Training loss: 0.09230443, Validation loss: 0.10462491, Gradient norm: 364.75326722
INFO:root:[  497] Training loss: 0.09800325, Validation loss: 0.09111407, Gradient norm: 422.00066819
INFO:root:[  498] Training loss: 0.08869492, Validation loss: 0.08722236, Gradient norm: 157.04547360
INFO:root:[  499] Training loss: 0.08905890, Validation loss: 0.08963216, Gradient norm: 244.55604253
INFO:root:[  500] Training loss: 0.09630034, Validation loss: 0.08929483, Gradient norm: 382.30973728
INFO:root:[  501] Training loss: 0.08720221, Validation loss: 0.09030071, Gradient norm: 143.26070605
INFO:root:[  502] Training loss: 0.08740603, Validation loss: 0.08876017, Gradient norm: 194.08997986
INFO:root:[  503] Training loss: 0.09577198, Validation loss: 0.08970333, Gradient norm: 415.45212546
INFO:root:[  504] Training loss: 0.08635409, Validation loss: 0.08508075, Gradient norm: 106.73559378
INFO:root:[  505] Training loss: 0.09280269, Validation loss: 0.12375268, Gradient norm: 273.61002052
INFO:root:[  506] Training loss: 0.09571490, Validation loss: 0.08699955, Gradient norm: 270.01146484
INFO:root:[  507] Training loss: 0.08508884, Validation loss: 0.08556959, Gradient norm: 66.53524677
INFO:root:[  508] Training loss: 0.08644797, Validation loss: 0.08904125, Gradient norm: 190.35564388
INFO:root:[  509] Training loss: 0.09372402, Validation loss: 0.09879569, Gradient norm: 422.35647246
INFO:root:[  510] Training loss: 0.08877622, Validation loss: 0.08515770, Gradient norm: 211.87873197
INFO:root:[  511] Training loss: 0.08538052, Validation loss: 0.08640422, Gradient norm: 137.79345635
INFO:root:[  512] Training loss: 0.08750385, Validation loss: 0.08835001, Gradient norm: 250.57033268
INFO:root:[  513] Training loss: 0.08541140, Validation loss: 0.08619788, Gradient norm: 217.33731907
INFO:root:EP 513: Early stopping
INFO:root:Training the model took 23693.823s.
INFO:root:Emptying the cuda cache took 0.109s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 3.89565
INFO:root:EnergyScoreTrain: 2.65528
INFO:root:CoverageTrain: 0.96097
INFO:root:IntervalWidthTrain: 0.1312
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 3.76137
INFO:root:EnergyScoreValidation: 2.5626
INFO:root:CoverageValidation: 0.96111
INFO:root:IntervalWidthValidation: 0.13211
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 3.07052
INFO:root:EnergyScoreTest: 2.09015
INFO:root:CoverageTest: 0.96061
INFO:root:IntervalWidthTest: 0.12864
INFO:root:###8 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 2327838720
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.28879951, Validation loss: 0.31924100, Gradient norm: 8.41815320
INFO:root:[    2] Training loss: 0.37827463, Validation loss: 0.23901583, Gradient norm: 2.55599725
INFO:root:[    3] Training loss: 0.34025896, Validation loss: 0.22040041, Gradient norm: 2.16399623
INFO:root:[    4] Training loss: 0.33119352, Validation loss: 0.23635730, Gradient norm: 4.23380322
INFO:root:[    5] Training loss: 0.31033055, Validation loss: 0.18763214, Gradient norm: 3.11691717
INFO:root:[    6] Training loss: 0.28763432, Validation loss: 0.18520926, Gradient norm: 2.57420374
INFO:root:[    7] Training loss: 0.27891574, Validation loss: 0.16218565, Gradient norm: 3.89569219
INFO:root:[    8] Training loss: 0.26778864, Validation loss: 0.16318598, Gradient norm: 2.74844742
INFO:root:[    9] Training loss: 0.25391180, Validation loss: 0.14128715, Gradient norm: 2.86329807
INFO:root:[   10] Training loss: 0.24569857, Validation loss: 0.16505801, Gradient norm: 2.80336329
INFO:root:[   11] Training loss: 0.23922674, Validation loss: 0.14665697, Gradient norm: 3.30767729
INFO:root:[   12] Training loss: 0.23746102, Validation loss: 0.15105062, Gradient norm: 4.07245905
INFO:root:[   13] Training loss: 0.22967462, Validation loss: 0.12651386, Gradient norm: 3.93295316
INFO:root:[   14] Training loss: 0.22524763, Validation loss: 0.11951584, Gradient norm: 2.64124908
INFO:root:[   15] Training loss: 0.21666191, Validation loss: 0.12681865, Gradient norm: 1.64048521
INFO:root:[   16] Training loss: 0.21885399, Validation loss: 0.11908324, Gradient norm: 3.06332986
INFO:root:[   17] Training loss: 0.21017265, Validation loss: 0.11559829, Gradient norm: 3.33781939
INFO:root:[   18] Training loss: 0.20533000, Validation loss: 0.11030096, Gradient norm: 1.95465645
INFO:root:[   19] Training loss: 0.20314437, Validation loss: 0.11490076, Gradient norm: 2.28115939
INFO:root:[   20] Training loss: 0.20050131, Validation loss: 0.12429699, Gradient norm: 2.33308819
INFO:root:[   21] Training loss: 0.19585120, Validation loss: 0.14499081, Gradient norm: 2.73380702
INFO:root:[   22] Training loss: 0.19789170, Validation loss: 0.11366745, Gradient norm: 4.66968638
INFO:root:[   23] Training loss: 0.19852302, Validation loss: 0.10598017, Gradient norm: 4.65378194
INFO:root:[   24] Training loss: 0.19279954, Validation loss: 0.10988399, Gradient norm: 3.58990857
INFO:root:[   25] Training loss: 0.18767267, Validation loss: 0.11677332, Gradient norm: 2.90784123
INFO:root:[   26] Training loss: 0.18875558, Validation loss: 0.09855425, Gradient norm: 3.47157830
INFO:root:[   27] Training loss: 0.18823418, Validation loss: 0.09903699, Gradient norm: 4.14554789
INFO:root:[   28] Training loss: 0.18330446, Validation loss: 0.09989729, Gradient norm: 3.24144584
INFO:root:[   29] Training loss: 0.17902951, Validation loss: 0.09624952, Gradient norm: 2.00349391
INFO:root:[   30] Training loss: 0.18104411, Validation loss: 0.10128545, Gradient norm: 4.28481877
INFO:root:[   31] Training loss: 0.18188513, Validation loss: 0.15053782, Gradient norm: 4.44349114
INFO:root:[   32] Training loss: 0.18075927, Validation loss: 0.09486658, Gradient norm: 4.39079936
INFO:root:[   33] Training loss: 0.17583210, Validation loss: 0.09569303, Gradient norm: 3.47185919
INFO:root:[   34] Training loss: 0.17522307, Validation loss: 0.09063999, Gradient norm: 3.93395126
INFO:root:[   35] Training loss: 0.17135361, Validation loss: 0.09083199, Gradient norm: 2.68498853
INFO:root:[   36] Training loss: 0.17262307, Validation loss: 0.09005735, Gradient norm: 3.71105745
INFO:root:[   37] Training loss: 0.17457319, Validation loss: 0.10308398, Gradient norm: 4.86869321
INFO:root:[   38] Training loss: 0.16899862, Validation loss: 0.09511456, Gradient norm: 3.13600149
INFO:root:[   39] Training loss: 0.16691276, Validation loss: 0.08894736, Gradient norm: 2.47756266
INFO:root:[   40] Training loss: 0.16428427, Validation loss: 0.10403866, Gradient norm: 2.26582524
INFO:root:[   41] Training loss: 0.16216662, Validation loss: 0.09318520, Gradient norm: 2.41634844
INFO:root:[   42] Training loss: 0.16544070, Validation loss: 0.09132656, Gradient norm: 3.52723876
INFO:root:[   43] Training loss: 0.15816455, Validation loss: 0.08561558, Gradient norm: 1.66572428
INFO:root:[   44] Training loss: 0.16150045, Validation loss: 0.10362760, Gradient norm: 2.77498968
INFO:root:[   45] Training loss: 0.16339536, Validation loss: 0.10258012, Gradient norm: 5.02193368
INFO:root:[   46] Training loss: 0.16055255, Validation loss: 0.09464323, Gradient norm: 3.45679243
INFO:root:[   47] Training loss: 0.15868994, Validation loss: 0.09769217, Gradient norm: 3.33234018
INFO:root:[   48] Training loss: 0.15885072, Validation loss: 0.08453598, Gradient norm: 3.13634632
INFO:root:[   49] Training loss: 0.15570886, Validation loss: 0.12235255, Gradient norm: 2.73356000
INFO:root:[   50] Training loss: 0.15818081, Validation loss: 0.08536215, Gradient norm: 3.61728750
INFO:root:[   51] Training loss: 0.15594037, Validation loss: 0.10347588, Gradient norm: 3.70682204
INFO:root:[   52] Training loss: 0.15340954, Validation loss: 0.11097652, Gradient norm: 2.80448478
INFO:root:[   53] Training loss: 0.15523367, Validation loss: 0.08905401, Gradient norm: 3.19400422
INFO:root:[   54] Training loss: 0.15111066, Validation loss: 0.09342487, Gradient norm: 2.62328719
INFO:root:[   55] Training loss: 0.15724346, Validation loss: 0.09789519, Gradient norm: 5.34352846
INFO:root:[   56] Training loss: 0.15501939, Validation loss: 0.08112412, Gradient norm: 4.91858501
INFO:root:[   57] Training loss: 0.14953264, Validation loss: 0.08122683, Gradient norm: 3.07429084
INFO:root:[   58] Training loss: 0.14940608, Validation loss: 0.08416240, Gradient norm: 3.37319538
INFO:root:[   59] Training loss: 0.14964238, Validation loss: 0.10353344, Gradient norm: 3.64147648
INFO:root:[   60] Training loss: 0.14691516, Validation loss: 0.08762796, Gradient norm: 2.34256963
INFO:root:[   61] Training loss: 0.14931971, Validation loss: 0.10840163, Gradient norm: 3.92099281
INFO:root:[   62] Training loss: 0.14727436, Validation loss: 0.08590908, Gradient norm: 3.62958883
INFO:root:[   63] Training loss: 0.14742352, Validation loss: 0.07886365, Gradient norm: 3.43374628
INFO:root:[   64] Training loss: 0.14640991, Validation loss: 0.08197478, Gradient norm: 2.93133948
INFO:root:[   65] Training loss: 0.14297124, Validation loss: 0.08342193, Gradient norm: 2.48678491
INFO:root:[   66] Training loss: 0.14306488, Validation loss: 0.07870407, Gradient norm: 2.49802944
INFO:root:[   67] Training loss: 0.14165220, Validation loss: 0.07819731, Gradient norm: 3.21988398
INFO:root:[   68] Training loss: 0.14094199, Validation loss: 0.09150056, Gradient norm: 1.42016287
INFO:root:[   69] Training loss: 0.14119019, Validation loss: 0.07923418, Gradient norm: 2.85464550
INFO:root:[   70] Training loss: 0.14459291, Validation loss: 0.10080537, Gradient norm: 4.04936428
INFO:root:[   71] Training loss: 0.14100853, Validation loss: 0.08828181, Gradient norm: 3.46049870
INFO:root:[   72] Training loss: 0.14040623, Validation loss: 0.08645507, Gradient norm: 3.02030415
INFO:root:[   73] Training loss: 0.14197176, Validation loss: 0.07853786, Gradient norm: 3.93887162
INFO:root:[   74] Training loss: 0.13934156, Validation loss: 0.10025529, Gradient norm: 3.26851262
INFO:root:[   75] Training loss: 0.14053728, Validation loss: 0.09605599, Gradient norm: 4.31978113
INFO:root:[   76] Training loss: 0.13791996, Validation loss: 0.07557894, Gradient norm: 3.00049147
INFO:root:[   77] Training loss: 0.13798817, Validation loss: 0.07732224, Gradient norm: 3.37913924
INFO:root:[   78] Training loss: 0.13837607, Validation loss: 0.07554733, Gradient norm: 4.27593831
INFO:root:[   79] Training loss: 0.13440130, Validation loss: 0.07720587, Gradient norm: 2.18690428
INFO:root:[   80] Training loss: 0.13742348, Validation loss: 0.07544174, Gradient norm: 3.37487776
INFO:root:[   81] Training loss: 0.13489924, Validation loss: 0.07492844, Gradient norm: 2.41444906
INFO:root:[   82] Training loss: 0.13665779, Validation loss: 0.09295060, Gradient norm: 3.63057088
INFO:root:[   83] Training loss: 0.13530356, Validation loss: 0.07743731, Gradient norm: 3.54600872
INFO:root:[   84] Training loss: 0.13258844, Validation loss: 0.08162162, Gradient norm: 2.48060043
INFO:root:[   85] Training loss: 0.13487409, Validation loss: 0.07754470, Gradient norm: 3.81422615
INFO:root:[   86] Training loss: 0.13245571, Validation loss: 0.07805505, Gradient norm: 2.70544541
INFO:root:[   87] Training loss: 0.13223061, Validation loss: 0.08366357, Gradient norm: 2.70574846
INFO:root:[   88] Training loss: 0.13132934, Validation loss: 0.09622563, Gradient norm: 3.04000800
INFO:root:[   89] Training loss: 0.13287752, Validation loss: 0.08019598, Gradient norm: 3.93301197
INFO:root:[   90] Training loss: 0.13183733, Validation loss: 0.07266985, Gradient norm: 3.29477017
INFO:root:[   91] Training loss: 0.12967552, Validation loss: 0.08061226, Gradient norm: 2.51097169
INFO:root:[   92] Training loss: 0.13037125, Validation loss: 0.07746665, Gradient norm: 2.79992921
INFO:root:[   93] Training loss: 0.12770415, Validation loss: 0.07140456, Gradient norm: 2.17624500
INFO:root:[   94] Training loss: 0.12954459, Validation loss: 0.08396014, Gradient norm: 2.35181876
INFO:root:[   95] Training loss: 0.13036542, Validation loss: 0.07116087, Gradient norm: 3.86845272
INFO:root:[   96] Training loss: 0.12862827, Validation loss: 0.07284674, Gradient norm: 3.52135606
INFO:root:[   97] Training loss: 0.12906189, Validation loss: 0.07587119, Gradient norm: 4.18629753
INFO:root:[   98] Training loss: 0.12933732, Validation loss: 0.07436178, Gradient norm: 3.85260374
INFO:root:[   99] Training loss: 0.12627439, Validation loss: 0.07284576, Gradient norm: 2.75054712
INFO:root:[  100] Training loss: 0.12741250, Validation loss: 0.07174016, Gradient norm: 2.76985976
INFO:root:[  101] Training loss: 0.12762329, Validation loss: 0.08567918, Gradient norm: 3.42380417
INFO:root:[  102] Training loss: 0.12523481, Validation loss: 0.07322969, Gradient norm: 2.94594898
INFO:root:[  103] Training loss: 0.13089133, Validation loss: 0.10544242, Gradient norm: 5.16049014
INFO:root:[  104] Training loss: 0.12940862, Validation loss: 0.06918792, Gradient norm: 4.51319843
INFO:root:[  105] Training loss: 0.12340216, Validation loss: 0.07482392, Gradient norm: 2.26858667
INFO:root:[  106] Training loss: 0.12439118, Validation loss: 0.07497098, Gradient norm: 2.84469835
INFO:root:[  107] Training loss: 0.12323828, Validation loss: 0.06908969, Gradient norm: 2.64923652
INFO:root:[  108] Training loss: 0.12224514, Validation loss: 0.07358191, Gradient norm: 2.09817176
INFO:root:[  109] Training loss: 0.12239815, Validation loss: 0.06808739, Gradient norm: 2.17766544
INFO:root:[  110] Training loss: 0.12186924, Validation loss: 0.07040557, Gradient norm: 2.43381528
INFO:root:[  111] Training loss: 0.12205074, Validation loss: 0.06832343, Gradient norm: 3.23455007
INFO:root:[  112] Training loss: 0.12233352, Validation loss: 0.07875005, Gradient norm: 2.67011063
INFO:root:[  113] Training loss: 0.12091286, Validation loss: 0.06736206, Gradient norm: 2.72283390
INFO:root:[  114] Training loss: 0.12161890, Validation loss: 0.06910260, Gradient norm: 2.50769413
INFO:root:[  115] Training loss: 0.12071574, Validation loss: 0.07228060, Gradient norm: 2.59331584
INFO:root:[  116] Training loss: 0.12076493, Validation loss: 0.07213693, Gradient norm: 3.44372661
INFO:root:[  117] Training loss: 0.12118402, Validation loss: 0.06656810, Gradient norm: 3.69052804
INFO:root:[  118] Training loss: 0.12262501, Validation loss: 0.07087784, Gradient norm: 4.33363481
INFO:root:[  119] Training loss: 0.12006942, Validation loss: 0.06687824, Gradient norm: 2.96463678
INFO:root:[  120] Training loss: 0.12017890, Validation loss: 0.07009176, Gradient norm: 3.78656911
INFO:root:[  121] Training loss: 0.11966788, Validation loss: 0.07863391, Gradient norm: 3.68812261
INFO:root:[  122] Training loss: 0.11944354, Validation loss: 0.07510052, Gradient norm: 3.24933282
INFO:root:[  123] Training loss: 0.11720402, Validation loss: 0.06570789, Gradient norm: 1.91327088
INFO:root:[  124] Training loss: 0.11800391, Validation loss: 0.06990139, Gradient norm: 2.26012196
INFO:root:[  125] Training loss: 0.11753913, Validation loss: 0.07121535, Gradient norm: 3.02942182
INFO:root:[  126] Training loss: 0.11634975, Validation loss: 0.06461746, Gradient norm: 2.04131525
INFO:root:[  127] Training loss: 0.11708505, Validation loss: 0.06584046, Gradient norm: 3.31459867
INFO:root:[  128] Training loss: 0.11822126, Validation loss: 0.06770941, Gradient norm: 4.13277855
INFO:root:[  129] Training loss: 0.11613179, Validation loss: 0.06481952, Gradient norm: 2.95038356
INFO:root:[  130] Training loss: 0.11534750, Validation loss: 0.06911558, Gradient norm: 2.58324353
INFO:root:[  131] Training loss: 0.11450581, Validation loss: 0.06427662, Gradient norm: 2.65103899
INFO:root:[  132] Training loss: 0.11546763, Validation loss: 0.06291309, Gradient norm: 2.93920759
INFO:root:[  133] Training loss: 0.11521396, Validation loss: 0.06315806, Gradient norm: 2.61680279
INFO:root:[  134] Training loss: 0.11605321, Validation loss: 0.06664673, Gradient norm: 3.79757872
INFO:root:[  135] Training loss: 0.11437763, Validation loss: 0.06490606, Gradient norm: 2.70275896
INFO:root:[  136] Training loss: 0.11357717, Validation loss: 0.06643934, Gradient norm: 3.00045123
INFO:root:[  137] Training loss: 0.11331770, Validation loss: 0.06435961, Gradient norm: 2.60007996
INFO:root:[  138] Training loss: 0.11281228, Validation loss: 0.06239057, Gradient norm: 2.59317912
INFO:root:[  139] Training loss: 0.11317777, Validation loss: 0.07244403, Gradient norm: 3.02175189
INFO:root:[  140] Training loss: 0.11424518, Validation loss: 0.06412171, Gradient norm: 3.68981289
INFO:root:[  141] Training loss: 0.11240920, Validation loss: 0.07103152, Gradient norm: 2.68420483
INFO:root:[  142] Training loss: 0.11231975, Validation loss: 0.06123368, Gradient norm: 3.33339868
INFO:root:[  143] Training loss: 0.11127093, Validation loss: 0.06283040, Gradient norm: 2.91755431
INFO:root:[  144] Training loss: 0.11063540, Validation loss: 0.06282082, Gradient norm: 2.67919944
INFO:root:[  145] Training loss: 0.11059098, Validation loss: 0.06317505, Gradient norm: 2.59547804
INFO:root:[  146] Training loss: 0.11006164, Validation loss: 0.06050369, Gradient norm: 2.35053577
INFO:root:[  147] Training loss: 0.10862185, Validation loss: 0.06006173, Gradient norm: 1.25227719
INFO:root:[  148] Training loss: 0.11056402, Validation loss: 0.06365485, Gradient norm: 2.62040015
INFO:root:[  149] Training loss: 0.11236593, Validation loss: 0.06856966, Gradient norm: 4.26723718
INFO:root:[  150] Training loss: 0.10923623, Validation loss: 0.05969389, Gradient norm: 2.56589321
INFO:root:[  151] Training loss: 0.10908948, Validation loss: 0.06086977, Gradient norm: 2.17361934
INFO:root:[  152] Training loss: 0.10981453, Validation loss: 0.06262965, Gradient norm: 3.03154777
INFO:root:[  153] Training loss: 0.10821696, Validation loss: 0.05944960, Gradient norm: 2.39978953
INFO:root:[  154] Training loss: 0.10913424, Validation loss: 0.06392910, Gradient norm: 2.72766844
INFO:root:[  155] Training loss: 0.10770657, Validation loss: 0.08311654, Gradient norm: 2.41954893
INFO:root:[  156] Training loss: 0.10965086, Validation loss: 0.06875383, Gradient norm: 3.45775769
INFO:root:[  157] Training loss: 0.10786044, Validation loss: 0.05860103, Gradient norm: 2.69273619
INFO:root:[  158] Training loss: 0.10822234, Validation loss: 0.05875792, Gradient norm: 2.95309060
INFO:root:[  159] Training loss: 0.10694584, Validation loss: 0.05852597, Gradient norm: 2.11312162
INFO:root:[  160] Training loss: 0.10839989, Validation loss: 0.06237073, Gradient norm: 3.10226101
INFO:root:[  161] Training loss: 0.10778038, Validation loss: 0.06088300, Gradient norm: 3.13139317
INFO:root:[  162] Training loss: 0.10586376, Validation loss: 0.06331203, Gradient norm: 2.21076987
INFO:root:[  163] Training loss: 0.10712390, Validation loss: 0.06250116, Gradient norm: 3.30572062
INFO:root:[  164] Training loss: 0.10706913, Validation loss: 0.06898442, Gradient norm: 3.27801413
INFO:root:[  165] Training loss: 0.10679762, Validation loss: 0.05828327, Gradient norm: 3.04794846
INFO:root:[  166] Training loss: 0.10583151, Validation loss: 0.06539776, Gradient norm: 2.99754252
INFO:root:[  167] Training loss: 0.10538619, Validation loss: 0.05710512, Gradient norm: 3.24026153
INFO:root:[  168] Training loss: 0.10573725, Validation loss: 0.05799981, Gradient norm: 3.03360694
INFO:root:[  169] Training loss: 0.10550182, Validation loss: 0.05965202, Gradient norm: 2.27309794
INFO:root:[  170] Training loss: 0.10467667, Validation loss: 0.05646518, Gradient norm: 2.65152241
INFO:root:[  171] Training loss: 0.10353520, Validation loss: 0.05849683, Gradient norm: 2.01694802
INFO:root:[  172] Training loss: 0.10419829, Validation loss: 0.06066719, Gradient norm: 2.59851195
INFO:root:[  173] Training loss: 0.10459838, Validation loss: 0.05888409, Gradient norm: 2.75757744
INFO:root:[  174] Training loss: 0.10349244, Validation loss: 0.06042899, Gradient norm: 1.68740518
INFO:root:[  175] Training loss: 0.10391525, Validation loss: 0.06635014, Gradient norm: 2.85251203
INFO:root:[  176] Training loss: 0.10476681, Validation loss: 0.06189849, Gradient norm: 3.69174219
INFO:root:[  177] Training loss: 0.10370131, Validation loss: 0.06031775, Gradient norm: 2.83585787
INFO:root:[  178] Training loss: 0.10316728, Validation loss: 0.05547000, Gradient norm: 2.39096079
INFO:root:[  179] Training loss: 0.10246568, Validation loss: 0.05877380, Gradient norm: 2.02112630
INFO:root:[  180] Training loss: 0.10158050, Validation loss: 0.05907135, Gradient norm: 1.67012320
INFO:root:[  181] Training loss: 0.10154433, Validation loss: 0.05499390, Gradient norm: 1.72299997
INFO:root:[  182] Training loss: 0.10239121, Validation loss: 0.05838342, Gradient norm: 2.78220919
INFO:root:[  183] Training loss: 0.10217964, Validation loss: 0.05490449, Gradient norm: 2.61623257
INFO:root:[  184] Training loss: 0.10141556, Validation loss: 0.05733707, Gradient norm: 2.19260823
INFO:root:[  185] Training loss: 0.10104669, Validation loss: 0.06173989, Gradient norm: 2.33617829
INFO:root:[  186] Training loss: 0.10209715, Validation loss: 0.05656551, Gradient norm: 3.29108731
INFO:root:[  187] Training loss: 0.10112849, Validation loss: 0.05492735, Gradient norm: 2.57022476
INFO:root:[  188] Training loss: 0.10038647, Validation loss: 0.05399756, Gradient norm: 2.28898809
INFO:root:[  189] Training loss: 0.10143950, Validation loss: 0.05926034, Gradient norm: 2.87058100
INFO:root:[  190] Training loss: 0.10257870, Validation loss: 0.05862462, Gradient norm: 3.54416536
INFO:root:[  191] Training loss: 0.10002084, Validation loss: 0.05430095, Gradient norm: 2.40685700
INFO:root:[  192] Training loss: 0.10028121, Validation loss: 0.05583987, Gradient norm: 2.53933873
INFO:root:[  193] Training loss: 0.10005298, Validation loss: 0.05394475, Gradient norm: 2.67043482
INFO:root:[  194] Training loss: 0.10086274, Validation loss: 0.05430069, Gradient norm: 3.33010253
INFO:root:[  195] Training loss: 0.09980701, Validation loss: 0.05764218, Gradient norm: 2.53530374
INFO:root:[  196] Training loss: 0.09991800, Validation loss: 0.05809745, Gradient norm: 2.32770851
INFO:root:[  197] Training loss: 0.09963497, Validation loss: 0.05462662, Gradient norm: 2.31283797
INFO:root:[  198] Training loss: 0.09949995, Validation loss: 0.05876719, Gradient norm: 2.60800427
INFO:root:[  199] Training loss: 0.09973495, Validation loss: 0.05672163, Gradient norm: 2.93876807
INFO:root:[  200] Training loss: 0.09926196, Validation loss: 0.05961116, Gradient norm: 2.76128599
INFO:root:[  201] Training loss: 0.09922717, Validation loss: 0.05847316, Gradient norm: 2.87495963
INFO:root:[  202] Training loss: 0.09866239, Validation loss: 0.05322250, Gradient norm: 2.44594752
INFO:root:[  203] Training loss: 0.09872943, Validation loss: 0.05374773, Gradient norm: 2.50302603
INFO:root:[  204] Training loss: 0.09904294, Validation loss: 0.05320337, Gradient norm: 3.10780143
INFO:root:[  205] Training loss: 0.09784326, Validation loss: 0.05628550, Gradient norm: 2.05300896
INFO:root:[  206] Training loss: 0.09882249, Validation loss: 0.05466788, Gradient norm: 3.12716642
INFO:root:[  207] Training loss: 0.09812537, Validation loss: 0.05656094, Gradient norm: 3.05879486
INFO:root:[  208] Training loss: 0.09810645, Validation loss: 0.05257911, Gradient norm: 2.70272979
INFO:root:[  209] Training loss: 0.09673226, Validation loss: 0.05292832, Gradient norm: 1.95919286
INFO:root:[  210] Training loss: 0.09711592, Validation loss: 0.05773397, Gradient norm: 2.14946689
INFO:root:[  211] Training loss: 0.09747672, Validation loss: 0.05430672, Gradient norm: 2.63203485
INFO:root:[  212] Training loss: 0.09780571, Validation loss: 0.05304015, Gradient norm: 3.17182113
INFO:root:[  213] Training loss: 0.09676554, Validation loss: 0.05185260, Gradient norm: 1.80441300
INFO:root:[  214] Training loss: 0.09635115, Validation loss: 0.05182759, Gradient norm: 2.05393888
INFO:root:[  215] Training loss: 0.09621976, Validation loss: 0.05223460, Gradient norm: 2.27284043
INFO:root:[  216] Training loss: 0.09641117, Validation loss: 0.05136199, Gradient norm: 2.90126817
INFO:root:[  217] Training loss: 0.09689068, Validation loss: 0.05294056, Gradient norm: 2.83587966
INFO:root:[  218] Training loss: 0.09596375, Validation loss: 0.05189444, Gradient norm: 2.37824239
INFO:root:[  219] Training loss: 0.09487197, Validation loss: 0.05166828, Gradient norm: 1.47468062
INFO:root:[  220] Training loss: 0.09612031, Validation loss: 0.05458869, Gradient norm: 2.62134139
INFO:root:[  221] Training loss: 0.09547639, Validation loss: 0.05355313, Gradient norm: 2.34592882
INFO:root:[  222] Training loss: 0.09577022, Validation loss: 0.05607343, Gradient norm: 2.49951256
INFO:root:[  223] Training loss: 0.09475619, Validation loss: 0.05263582, Gradient norm: 2.14310059
INFO:root:[  224] Training loss: 0.09614192, Validation loss: 0.05285047, Gradient norm: 3.08788804
INFO:root:[  225] Training loss: 0.09549529, Validation loss: 0.05369953, Gradient norm: 2.70757757
INFO:root:EP 225: Early stopping
INFO:root:Training the model took 3595.207s.
INFO:root:Emptying the cuda cache took 0.046s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.72563
INFO:root:EnergyScoreTrain: 1.2826
INFO:root:CoverageTrain: 0.51857
INFO:root:IntervalWidthTrain: 0.01623
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.57539
INFO:root:EnergyScoreValidation: 1.15383
INFO:root:CoverageValidation: 0.6428
INFO:root:IntervalWidthValidation: 0.01792
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.38318
INFO:root:EnergyScoreTest: 0.96761
INFO:root:CoverageTest: 0.58402
INFO:root:IntervalWidthTest: 0.02007
INFO:root:###9 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 924844032
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.31668693, Validation loss: 0.65778118, Gradient norm: 7.73361117
INFO:root:[    2] Training loss: 0.74687003, Validation loss: 0.26413440, Gradient norm: 2.90900219
INFO:root:[    3] Training loss: 0.53804065, Validation loss: 0.21346729, Gradient norm: 2.12211225
INFO:root:[    4] Training loss: 0.46966593, Validation loss: 0.20465850, Gradient norm: 2.55511040
INFO:root:[    5] Training loss: 0.43492838, Validation loss: 0.18044205, Gradient norm: 3.17090225
INFO:root:[    6] Training loss: 0.41696399, Validation loss: 0.19732217, Gradient norm: 3.15000296
INFO:root:[    7] Training loss: 0.40409964, Validation loss: 0.19414167, Gradient norm: 3.79187375
INFO:root:[    8] Training loss: 0.38345663, Validation loss: 0.17219400, Gradient norm: 2.35083321
INFO:root:[    9] Training loss: 0.37017861, Validation loss: 0.19619953, Gradient norm: 2.17972842
INFO:root:[   10] Training loss: 0.36689341, Validation loss: 0.16106819, Gradient norm: 3.87741048
INFO:root:[   11] Training loss: 0.35383720, Validation loss: 0.17156531, Gradient norm: 3.06463540
INFO:root:[   12] Training loss: 0.34933273, Validation loss: 0.15236582, Gradient norm: 3.16579503
INFO:root:[   13] Training loss: 0.33980706, Validation loss: 0.15014195, Gradient norm: 4.10828404
INFO:root:[   14] Training loss: 0.33107494, Validation loss: 0.17072057, Gradient norm: 4.10791864
INFO:root:[   15] Training loss: 0.32698086, Validation loss: 0.14456817, Gradient norm: 4.04034696
INFO:root:[   16] Training loss: 0.31644040, Validation loss: 0.14317299, Gradient norm: 1.86197304
INFO:root:[   17] Training loss: 0.31383063, Validation loss: 0.14570014, Gradient norm: 2.91068390
INFO:root:[   18] Training loss: 0.30472628, Validation loss: 0.14001953, Gradient norm: 2.62553249
INFO:root:[   19] Training loss: 0.30509117, Validation loss: 0.14188713, Gradient norm: 2.53572256
INFO:root:[   20] Training loss: 0.30066344, Validation loss: 0.13258888, Gradient norm: 2.62935367
INFO:root:[   21] Training loss: 0.29110789, Validation loss: 0.14118456, Gradient norm: 2.48170961
INFO:root:[   22] Training loss: 0.29012186, Validation loss: 0.12995739, Gradient norm: 3.60011022
INFO:root:[   23] Training loss: 0.28146983, Validation loss: 0.14947883, Gradient norm: 2.21530171
INFO:root:[   24] Training loss: 0.27910202, Validation loss: 0.14420634, Gradient norm: 3.42091039
INFO:root:[   25] Training loss: 0.27192071, Validation loss: 0.13338211, Gradient norm: 1.98506700
INFO:root:[   26] Training loss: 0.26787042, Validation loss: 0.13863059, Gradient norm: 2.93533435
INFO:root:[   27] Training loss: 0.26330874, Validation loss: 0.12182646, Gradient norm: 2.06664347
INFO:root:[   28] Training loss: 0.26351516, Validation loss: 0.12118650, Gradient norm: 3.08801326
INFO:root:[   29] Training loss: 0.25594203, Validation loss: 0.14685103, Gradient norm: 3.35537943
INFO:root:[   30] Training loss: 0.25238242, Validation loss: 0.12232026, Gradient norm: 2.37864818
INFO:root:[   31] Training loss: 0.24847840, Validation loss: 0.11573206, Gradient norm: 3.12169030
INFO:root:[   32] Training loss: 0.24403681, Validation loss: 0.12480090, Gradient norm: 2.54174097
INFO:root:[   33] Training loss: 0.24152876, Validation loss: 0.11943970, Gradient norm: 2.29452844
INFO:root:[   34] Training loss: 0.23649349, Validation loss: 0.11394537, Gradient norm: 1.71500601
INFO:root:[   35] Training loss: 0.23651609, Validation loss: 0.12883178, Gradient norm: 2.92284346
INFO:root:[   36] Training loss: 0.23324923, Validation loss: 0.11296548, Gradient norm: 1.85277253
INFO:root:[   37] Training loss: 0.22832417, Validation loss: 0.12027950, Gradient norm: 1.56151813
INFO:root:[   38] Training loss: 0.22507830, Validation loss: 0.13788530, Gradient norm: 1.66099903
INFO:root:[   39] Training loss: 0.22681125, Validation loss: 0.12891293, Gradient norm: 3.21720691
INFO:root:[   40] Training loss: 0.21983358, Validation loss: 0.12409050, Gradient norm: 1.58819982
INFO:root:[   41] Training loss: 0.21870496, Validation loss: 0.13131384, Gradient norm: 1.44009527
INFO:root:[   42] Training loss: 0.21543037, Validation loss: 0.12097952, Gradient norm: 1.37214902
INFO:root:[   43] Training loss: 0.21479240, Validation loss: 0.11850397, Gradient norm: 1.90204777
INFO:root:[   44] Training loss: 0.21475724, Validation loss: 0.11187694, Gradient norm: 2.30882867
INFO:root:[   45] Training loss: 0.21008594, Validation loss: 0.14132898, Gradient norm: 1.38198165
INFO:root:[   46] Training loss: 0.20756083, Validation loss: 0.11038944, Gradient norm: 1.73549422
INFO:root:[   47] Training loss: 0.20683005, Validation loss: 0.13320487, Gradient norm: 2.64374084
INFO:root:[   48] Training loss: 0.20203953, Validation loss: 0.13036968, Gradient norm: 1.22225149
INFO:root:[   49] Training loss: 0.20086551, Validation loss: 0.12507571, Gradient norm: 1.03788148
INFO:root:[   50] Training loss: 0.19819445, Validation loss: 0.14672256, Gradient norm: 1.37392871
INFO:root:[   51] Training loss: 0.19827450, Validation loss: 0.10998860, Gradient norm: 2.10685762
INFO:root:[   52] Training loss: 0.19647541, Validation loss: 0.13602785, Gradient norm: 1.50525748
INFO:root:[   53] Training loss: 0.19445934, Validation loss: 0.13747876, Gradient norm: 1.18104305
INFO:root:[   54] Training loss: 0.19226707, Validation loss: 0.13283940, Gradient norm: 0.97406688
INFO:root:[   55] Training loss: 0.19139222, Validation loss: 0.12640040, Gradient norm: 1.42833068
INFO:root:[   56] Training loss: 0.19004518, Validation loss: 0.14247154, Gradient norm: 1.75788047
INFO:root:[   57] Training loss: 0.18837561, Validation loss: 0.14235302, Gradient norm: 1.54426104
INFO:root:[   58] Training loss: 0.18687317, Validation loss: 0.11336143, Gradient norm: 1.82426663
INFO:root:[   59] Training loss: 0.18620773, Validation loss: 0.11796642, Gradient norm: 2.31928643
INFO:root:[   60] Training loss: 0.18494082, Validation loss: 0.13048920, Gradient norm: 2.06988543
INFO:root:[   61] Training loss: 0.18371785, Validation loss: 0.13104996, Gradient norm: 1.20809479
INFO:root:[   62] Training loss: 0.18181988, Validation loss: 0.13765533, Gradient norm: 1.53560309
INFO:root:[   63] Training loss: 0.18021127, Validation loss: 0.13310623, Gradient norm: 0.77735983
INFO:root:[   64] Training loss: 0.17921442, Validation loss: 0.12456726, Gradient norm: 1.11468267
INFO:root:[   65] Training loss: 0.17838289, Validation loss: 0.12676608, Gradient norm: 1.19343661
INFO:root:[   66] Training loss: 0.17686739, Validation loss: 0.14263757, Gradient norm: 1.04305177
INFO:root:[   67] Training loss: 0.17648925, Validation loss: 0.11683621, Gradient norm: 1.24545595
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 1083.041s.
INFO:root:Emptying the cuda cache took 0.042s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 3.37366
INFO:root:EnergyScoreTrain: 2.67242
INFO:root:CoverageTrain: 0.3863
INFO:root:IntervalWidthTrain: 0.02265
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 3.49751
INFO:root:EnergyScoreValidation: 2.83805
INFO:root:CoverageValidation: 0.18887
INFO:root:IntervalWidthValidation: 0.02085
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.87582
INFO:root:EnergyScoreTest: 2.24453
INFO:root:CoverageTest: 0.28269
INFO:root:IntervalWidthTest: 0.02442
INFO:root:###10 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 924844032
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.72179059, Validation loss: 0.50757563, Gradient norm: 6.19233537
INFO:root:[    2] Training loss: 0.65944635, Validation loss: 0.23728900, Gradient norm: 3.35590326
INFO:root:[    3] Training loss: 0.53949813, Validation loss: 0.21212286, Gradient norm: 1.84958138
INFO:root:[    4] Training loss: 0.50925272, Validation loss: 0.21954881, Gradient norm: 2.51120791
INFO:root:[    5] Training loss: 0.47966192, Validation loss: 0.20522698, Gradient norm: 2.83358664
INFO:root:[    6] Training loss: 0.46279978, Validation loss: 0.19530189, Gradient norm: 2.01725927
INFO:root:[    7] Training loss: 0.44763047, Validation loss: 0.19418571, Gradient norm: 2.13508933
INFO:root:[    8] Training loss: 0.42794470, Validation loss: 0.18765076, Gradient norm: 1.60622523
INFO:root:[    9] Training loss: 0.41444356, Validation loss: 0.18318074, Gradient norm: 2.10048140
INFO:root:[   10] Training loss: 0.40820804, Validation loss: 0.18269903, Gradient norm: 2.28546347
INFO:root:[   11] Training loss: 0.39468827, Validation loss: 0.18019703, Gradient norm: 1.59120024
INFO:root:[   12] Training loss: 0.38035102, Validation loss: 0.19195587, Gradient norm: 1.66673768
INFO:root:[   13] Training loss: 0.37638791, Validation loss: 0.18019203, Gradient norm: 1.93671679
INFO:root:[   14] Training loss: 0.36887728, Validation loss: 0.16565829, Gradient norm: 2.43430667
INFO:root:[   15] Training loss: 0.35939218, Validation loss: 0.17810476, Gradient norm: 1.72052812
INFO:root:[   16] Training loss: 0.34765903, Validation loss: 0.15938422, Gradient norm: 1.70297649
INFO:root:[   17] Training loss: 0.34292929, Validation loss: 0.15497805, Gradient norm: 1.79799948
INFO:root:[   18] Training loss: 0.33814817, Validation loss: 0.16531392, Gradient norm: 1.71750012
INFO:root:[   19] Training loss: 0.33193486, Validation loss: 0.15946439, Gradient norm: 1.41843848
INFO:root:[   20] Training loss: 0.32454540, Validation loss: 0.17261784, Gradient norm: 1.18617908
INFO:root:[   21] Training loss: 0.31904560, Validation loss: 0.17200293, Gradient norm: 2.18893435
INFO:root:[   22] Training loss: 0.31278681, Validation loss: 0.18318811, Gradient norm: 1.45991265
INFO:root:[   23] Training loss: 0.30748909, Validation loss: 0.17178443, Gradient norm: 1.46917866
INFO:root:[   24] Training loss: 0.30289553, Validation loss: 0.16738330, Gradient norm: 1.19503181
INFO:root:[   25] Training loss: 0.29916856, Validation loss: 0.20213568, Gradient norm: 1.55825522
INFO:root:[   26] Training loss: 0.29644840, Validation loss: 0.16600757, Gradient norm: 2.17136831
INFO:root:[   27] Training loss: 0.29333969, Validation loss: 0.16955113, Gradient norm: 2.03644112
INFO:root:[   28] Training loss: 0.28920793, Validation loss: 0.16496316, Gradient norm: 2.48427792
INFO:root:[   29] Training loss: 0.28427677, Validation loss: 0.16835688, Gradient norm: 1.61562332
INFO:root:[   30] Training loss: 0.28029106, Validation loss: 0.19183244, Gradient norm: 1.25529096
INFO:root:[   31] Training loss: 0.27513476, Validation loss: 0.17114860, Gradient norm: 1.11718024
INFO:root:[   32] Training loss: 0.27267653, Validation loss: 0.15937118, Gradient norm: 1.61073486
INFO:root:[   33] Training loss: 0.27206463, Validation loss: 0.19014140, Gradient norm: 2.48513980
INFO:root:[   34] Training loss: 0.26756514, Validation loss: 0.19989874, Gradient norm: 1.73795046
INFO:root:[   35] Training loss: 0.26570565, Validation loss: 0.17553590, Gradient norm: 2.16875499
INFO:root:[   36] Training loss: 0.26281389, Validation loss: 0.15276526, Gradient norm: 1.98138854
INFO:root:[   37] Training loss: 0.25958123, Validation loss: 0.18350425, Gradient norm: 1.43158397
INFO:root:[   38] Training loss: 0.25659464, Validation loss: 0.18407516, Gradient norm: 0.99510830
INFO:root:[   39] Training loss: 0.25237396, Validation loss: 0.18528982, Gradient norm: 1.05542823
INFO:root:[   40] Training loss: 0.25090497, Validation loss: 0.21769997, Gradient norm: 1.22073298
INFO:root:[   41] Training loss: 0.24951184, Validation loss: 0.20807219, Gradient norm: 1.51394517
INFO:root:[   42] Training loss: 0.24486719, Validation loss: 0.19685864, Gradient norm: 0.86365309
INFO:root:[   43] Training loss: 0.24458623, Validation loss: 0.17404449, Gradient norm: 1.22178795
INFO:root:[   44] Training loss: 0.24179077, Validation loss: 0.20399071, Gradient norm: 1.54382536
INFO:root:[   45] Training loss: 0.23998901, Validation loss: 0.22590607, Gradient norm: 1.58653671
INFO:root:[   46] Training loss: 0.23886834, Validation loss: 0.21070529, Gradient norm: 1.78758031
INFO:root:[   47] Training loss: 0.23602117, Validation loss: 0.21092082, Gradient norm: 0.98359273
INFO:root:[   48] Training loss: 0.23310866, Validation loss: 0.21203363, Gradient norm: 0.88506564
INFO:root:[   49] Training loss: 0.23097695, Validation loss: 0.18299141, Gradient norm: 1.06809556
INFO:root:[   50] Training loss: 0.23023636, Validation loss: 0.20588283, Gradient norm: 1.59452179
INFO:root:[   51] Training loss: 0.22807520, Validation loss: 0.19621708, Gradient norm: 1.40908084
INFO:root:[   52] Training loss: 0.22588415, Validation loss: 0.20303057, Gradient norm: 1.16445271
INFO:root:[   53] Training loss: 0.22431653, Validation loss: 0.19170855, Gradient norm: 0.99686860
INFO:root:[   54] Training loss: 0.22325513, Validation loss: 0.23182808, Gradient norm: 0.97378533
INFO:root:[   55] Training loss: 0.22238387, Validation loss: 0.23710106, Gradient norm: 1.17678818
INFO:root:[   56] Training loss: 0.21949085, Validation loss: 0.20673980, Gradient norm: 0.99885192
INFO:root:[   57] Training loss: 0.21786746, Validation loss: 0.20627671, Gradient norm: 1.00510390
INFO:root:[   58] Training loss: 0.21618185, Validation loss: 0.20059561, Gradient norm: 0.89069729
INFO:root:[   59] Training loss: 0.21622070, Validation loss: 0.20629233, Gradient norm: 1.47217117
INFO:root:[   60] Training loss: 0.21362394, Validation loss: 0.21583354, Gradient norm: 0.61322520
INFO:root:[   61] Training loss: 0.21230034, Validation loss: 0.24380716, Gradient norm: 0.71046586
INFO:root:[   62] Training loss: 0.21219659, Validation loss: 0.22738456, Gradient norm: 1.52399944
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1001.521s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 5.62006
INFO:root:EnergyScoreTrain: 3.49458
INFO:root:CoverageTrain: 0.54123
INFO:root:IntervalWidthTrain: 0.09996
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 5.24232
INFO:root:EnergyScoreValidation: 2.75164
INFO:root:CoverageValidation: 0.6965
INFO:root:IntervalWidthValidation: 0.12898
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 3.88139
INFO:root:EnergyScoreTest: 2.24525
INFO:root:CoverageTest: 0.73827
INFO:root:IntervalWidthTest: 0.08678
INFO:root:###11 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1117782016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.60034269, Validation loss: 0.41673506, Gradient norm: 5.88124132
INFO:root:[    2] Training loss: 0.81436793, Validation loss: 0.34421037, Gradient norm: 2.12093307
INFO:root:[    3] Training loss: 0.70800295, Validation loss: 0.29584717, Gradient norm: 2.55799287
INFO:root:[    4] Training loss: 0.64772870, Validation loss: 0.28684418, Gradient norm: 2.25594800
INFO:root:[    5] Training loss: 0.60952520, Validation loss: 0.30674168, Gradient norm: 1.82408349
INFO:root:[    6] Training loss: 0.56636489, Validation loss: 0.31891235, Gradient norm: 2.11185465
INFO:root:[    7] Training loss: 0.54569413, Validation loss: 0.25855204, Gradient norm: 2.57679612
INFO:root:[    8] Training loss: 0.52019186, Validation loss: 0.32751261, Gradient norm: 2.34455537
INFO:root:[    9] Training loss: 0.50651269, Validation loss: 0.19834622, Gradient norm: 2.95596812
INFO:root:[   10] Training loss: 0.48045386, Validation loss: 0.19959920, Gradient norm: 2.14490809
INFO:root:[   11] Training loss: 0.46764139, Validation loss: 0.17482340, Gradient norm: 2.50146475
INFO:root:[   12] Training loss: 0.45104999, Validation loss: 0.19186819, Gradient norm: 2.26648222
INFO:root:[   13] Training loss: 0.43128878, Validation loss: 0.17794609, Gradient norm: 1.39708098
INFO:root:[   14] Training loss: 0.41951050, Validation loss: 0.16943216, Gradient norm: 1.95408266
INFO:root:[   15] Training loss: 0.40233241, Validation loss: 0.16226785, Gradient norm: 1.32399288
INFO:root:[   16] Training loss: 0.39011096, Validation loss: 0.15805922, Gradient norm: 1.31287182
INFO:root:[   17] Training loss: 0.38026958, Validation loss: 0.18643004, Gradient norm: 1.68300613
INFO:root:[   18] Training loss: 0.37574554, Validation loss: 0.15298993, Gradient norm: 2.44332396
INFO:root:[   19] Training loss: 0.36524959, Validation loss: 0.15602495, Gradient norm: 2.16539727
INFO:root:[   20] Training loss: 0.35251586, Validation loss: 0.14888915, Gradient norm: 1.38717385
INFO:root:[   21] Training loss: 0.34529088, Validation loss: 0.14876540, Gradient norm: 1.66419101
INFO:root:[   22] Training loss: 0.33948865, Validation loss: 0.15014769, Gradient norm: 1.32730712
INFO:root:[   23] Training loss: 0.33467198, Validation loss: 0.15650302, Gradient norm: 1.37713283
INFO:root:[   24] Training loss: 0.32790559, Validation loss: 0.15056776, Gradient norm: 1.23394500
INFO:root:[   25] Training loss: 0.32195781, Validation loss: 0.16081428, Gradient norm: 1.33830479
INFO:root:[   26] Training loss: 0.31349912, Validation loss: 0.16027995, Gradient norm: 1.11939995
INFO:root:[   27] Training loss: 0.31096938, Validation loss: 0.18092931, Gradient norm: 1.05332310
INFO:root:[   28] Training loss: 0.30553355, Validation loss: 0.17020814, Gradient norm: 1.38789428
INFO:root:[   29] Training loss: 0.30011756, Validation loss: 0.17270511, Gradient norm: 1.22778257
INFO:root:[   30] Training loss: 0.29653958, Validation loss: 0.17858137, Gradient norm: 1.20769726
INFO:root:[   31] Training loss: 0.29278715, Validation loss: 0.18113027, Gradient norm: 1.44179426
INFO:root:[   32] Training loss: 0.28894126, Validation loss: 0.16303220, Gradient norm: 1.75556785
INFO:root:[   33] Training loss: 0.28463657, Validation loss: 0.16580535, Gradient norm: 0.99209319
INFO:root:[   34] Training loss: 0.28064110, Validation loss: 0.15699628, Gradient norm: 0.98611916
INFO:root:[   35] Training loss: 0.27698156, Validation loss: 0.15156486, Gradient norm: 0.79255656
INFO:root:[   36] Training loss: 0.27475742, Validation loss: 0.16545116, Gradient norm: 1.07737062
INFO:root:[   37] Training loss: 0.27201956, Validation loss: 0.17795219, Gradient norm: 1.35798474
INFO:root:[   38] Training loss: 0.26832133, Validation loss: 0.19824736, Gradient norm: 0.70114985
INFO:root:[   39] Training loss: 0.26659838, Validation loss: 0.21441702, Gradient norm: 1.52367124
INFO:root:[   40] Training loss: 0.26370212, Validation loss: 0.19049081, Gradient norm: 1.04194600
INFO:root:[   41] Training loss: 0.26047888, Validation loss: 0.19910863, Gradient norm: 0.63177028
INFO:root:[   42] Training loss: 0.26001647, Validation loss: 0.20304870, Gradient norm: 1.42274852
INFO:root:[   43] Training loss: 0.25630121, Validation loss: 0.19004044, Gradient norm: 1.00972947
INFO:root:[   44] Training loss: 0.25414579, Validation loss: 0.20566077, Gradient norm: 1.02419541
INFO:root:[   45] Training loss: 0.25186156, Validation loss: 0.20942523, Gradient norm: 0.65611228
INFO:root:[   46] Training loss: 0.25054610, Validation loss: 0.21069728, Gradient norm: 1.13372947
INFO:root:[   47] Training loss: 0.24879857, Validation loss: 0.20491247, Gradient norm: 0.98832603
INFO:root:[   48] Training loss: 0.24634920, Validation loss: 0.19954894, Gradient norm: 0.81454119
INFO:root:[   49] Training loss: 0.24434777, Validation loss: 0.21618986, Gradient norm: 0.85105184
INFO:root:[   50] Training loss: 0.24208557, Validation loss: 0.23277208, Gradient norm: 0.69567621
INFO:root:[   51] Training loss: 0.24109216, Validation loss: 0.22747398, Gradient norm: 1.06981583
INFO:root:[   52] Training loss: 0.23905691, Validation loss: 0.21540499, Gradient norm: 0.89398570
INFO:root:[   53] Training loss: 0.23739393, Validation loss: 0.20405756, Gradient norm: 0.80496596
INFO:root:[   54] Training loss: 0.23609702, Validation loss: 0.22203085, Gradient norm: 0.88656120
INFO:root:[   55] Training loss: 0.23439243, Validation loss: 0.21663546, Gradient norm: 0.83056463
INFO:root:[   56] Training loss: 0.23404862, Validation loss: 0.22551203, Gradient norm: 0.88980311
INFO:root:[   57] Training loss: 0.23177698, Validation loss: 0.21804784, Gradient norm: 0.77817993
INFO:root:[   58] Training loss: 0.23049917, Validation loss: 0.24302900, Gradient norm: 0.90509595
INFO:root:[   59] Training loss: 0.22937932, Validation loss: 0.24474059, Gradient norm: 1.03868981
INFO:root:[   60] Training loss: 0.22769072, Validation loss: 0.23512127, Gradient norm: 0.55253070
INFO:root:[   61] Training loss: 0.22610208, Validation loss: 0.25399431, Gradient norm: 0.63158350
INFO:root:[   62] Training loss: 0.22552415, Validation loss: 0.24765355, Gradient norm: 0.68703130
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1001.291s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 5.16237
INFO:root:EnergyScoreTrain: 3.34716
INFO:root:CoverageTrain: 0.64246
INFO:root:IntervalWidthTrain: 0.08237
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 5.13815
INFO:root:EnergyScoreValidation: 3.66929
INFO:root:CoverageValidation: 0.42344
INFO:root:IntervalWidthValidation: 0.05579
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 4.11659
INFO:root:EnergyScoreTest: 2.33502
INFO:root:CoverageTest: 0.76417
INFO:root:IntervalWidthTest: 0.1045
INFO:root:###12 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1117782016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.14676493, Validation loss: 0.52086918, Gradient norm: 7.21426081
INFO:root:[    2] Training loss: 1.30903722, Validation loss: 0.63641886, Gradient norm: 3.17824753
INFO:root:[    3] Training loss: 1.18423258, Validation loss: 0.58141815, Gradient norm: 2.96089701
INFO:root:[    4] Training loss: 1.05865345, Validation loss: 0.42791241, Gradient norm: 2.92111831
INFO:root:[    5] Training loss: 0.96395418, Validation loss: 0.51180009, Gradient norm: 2.83631458
INFO:root:[    6] Training loss: 0.87955437, Validation loss: 0.33662727, Gradient norm: 3.09755778
INFO:root:[    7] Training loss: 0.82669343, Validation loss: 0.35097197, Gradient norm: 2.86012036
INFO:root:[    8] Training loss: 0.78581395, Validation loss: 0.38211679, Gradient norm: 2.91177792
INFO:root:[    9] Training loss: 0.73345642, Validation loss: 0.29580605, Gradient norm: 2.53725010
INFO:root:[   10] Training loss: 0.71808654, Validation loss: 0.23138013, Gradient norm: 4.01312028
INFO:root:[   11] Training loss: 0.69844299, Validation loss: 0.28130857, Gradient norm: 2.98862315
INFO:root:[   12] Training loss: 0.68198906, Validation loss: 0.22932337, Gradient norm: 3.01258159
INFO:root:[   13] Training loss: 0.65948533, Validation loss: 0.31090987, Gradient norm: 2.86776778
INFO:root:[   14] Training loss: 0.63939109, Validation loss: 0.21499328, Gradient norm: 2.55501397
INFO:root:[   15] Training loss: 0.62481129, Validation loss: 0.31629908, Gradient norm: 2.01038740
INFO:root:[   16] Training loss: 0.61620115, Validation loss: 0.21814417, Gradient norm: 2.56534369
INFO:root:[   17] Training loss: 0.59770792, Validation loss: 0.24059360, Gradient norm: 1.44637711
INFO:root:[   18] Training loss: 0.59139545, Validation loss: 0.18065505, Gradient norm: 2.95135574
INFO:root:[   19] Training loss: 0.57826521, Validation loss: 0.21494427, Gradient norm: 2.60046219
INFO:root:[   20] Training loss: 0.56529754, Validation loss: 0.19632170, Gradient norm: 2.26229363
INFO:root:[   21] Training loss: 0.54660066, Validation loss: 0.22776163, Gradient norm: 1.86839462
INFO:root:[   22] Training loss: 0.53900638, Validation loss: 0.18812182, Gradient norm: 2.93388791
INFO:root:[   23] Training loss: 0.52740958, Validation loss: 0.20632214, Gradient norm: 2.27042355
INFO:root:[   24] Training loss: 0.51605629, Validation loss: 0.16872238, Gradient norm: 2.32294369
INFO:root:[   25] Training loss: 0.51148272, Validation loss: 0.16449787, Gradient norm: 2.67044902
INFO:root:[   26] Training loss: 0.50613156, Validation loss: 0.17237887, Gradient norm: 2.54809428
INFO:root:[   27] Training loss: 0.49635761, Validation loss: 0.18495475, Gradient norm: 2.50107231
INFO:root:[   28] Training loss: 0.48897651, Validation loss: 0.16549227, Gradient norm: 2.38028415
INFO:root:[   29] Training loss: 0.48419209, Validation loss: 0.16513741, Gradient norm: 1.74609851
INFO:root:[   30] Training loss: 0.48580420, Validation loss: 0.16545102, Gradient norm: 3.32016263
INFO:root:[   31] Training loss: 0.47108397, Validation loss: 0.16923253, Gradient norm: 2.32823695
INFO:root:[   32] Training loss: 0.46359779, Validation loss: 0.17943364, Gradient norm: 1.18039604
INFO:root:[   33] Training loss: 0.45648888, Validation loss: 0.18971750, Gradient norm: 1.65820539
INFO:root:[   34] Training loss: 0.45120895, Validation loss: 0.16272669, Gradient norm: 1.51723784
INFO:root:[   35] Training loss: 0.44564370, Validation loss: 0.17918646, Gradient norm: 1.60813847
INFO:root:[   36] Training loss: 0.44498050, Validation loss: 0.21573495, Gradient norm: 2.27638065
INFO:root:[   37] Training loss: 0.43694897, Validation loss: 0.23044462, Gradient norm: 1.54441059
INFO:root:[   38] Training loss: 0.43413655, Validation loss: 0.21484285, Gradient norm: 1.55495655
INFO:root:[   39] Training loss: 0.42812383, Validation loss: 0.17593127, Gradient norm: 1.57331543
INFO:root:[   40] Training loss: 0.42522126, Validation loss: 0.21377401, Gradient norm: 1.74321858
INFO:root:[   41] Training loss: 0.42264806, Validation loss: 0.18931090, Gradient norm: 2.48294192
INFO:root:[   42] Training loss: 0.41785750, Validation loss: 0.25197946, Gradient norm: 2.03497201
INFO:root:[   43] Training loss: 0.41146840, Validation loss: 0.22519607, Gradient norm: 1.07633821
INFO:root:[   44] Training loss: 0.40561948, Validation loss: 0.22826197, Gradient norm: 1.18382166
INFO:root:[   45] Training loss: 0.40112571, Validation loss: 0.23009255, Gradient norm: 1.37116783
INFO:root:[   46] Training loss: 0.39879207, Validation loss: 0.28119654, Gradient norm: 1.05490184
INFO:root:[   47] Training loss: 0.39393945, Validation loss: 0.27312924, Gradient norm: 1.17219993
INFO:root:[   48] Training loss: 0.39150192, Validation loss: 0.28758760, Gradient norm: 1.17351070
INFO:root:[   49] Training loss: 0.38885353, Validation loss: 0.30865351, Gradient norm: 1.49413866
INFO:root:[   50] Training loss: 0.38412054, Validation loss: 0.26110154, Gradient norm: 1.21776794
INFO:root:[   51] Training loss: 0.38064861, Validation loss: 0.30355755, Gradient norm: 0.86668816
INFO:root:[   52] Training loss: 0.37635119, Validation loss: 0.27147057, Gradient norm: 1.04038137
INFO:root:[   53] Training loss: 0.37330185, Validation loss: 0.28683865, Gradient norm: 1.20827361
INFO:root:[   54] Training loss: 0.37041086, Validation loss: 0.29458424, Gradient norm: 1.28235752
INFO:root:[   55] Training loss: 0.36686320, Validation loss: 0.32303743, Gradient norm: 1.23857804
INFO:root:[   56] Training loss: 0.36452050, Validation loss: 0.34750084, Gradient norm: 1.40699980
INFO:root:[   57] Training loss: 0.36368494, Validation loss: 0.29484969, Gradient norm: 1.54108502
INFO:root:[   58] Training loss: 0.36134260, Validation loss: 0.32444840, Gradient norm: 1.54658131
INFO:root:[   59] Training loss: 0.35625920, Validation loss: 0.37716660, Gradient norm: 0.86532220
INFO:root:[   60] Training loss: 0.35367478, Validation loss: 0.36159722, Gradient norm: 1.15357633
INFO:root:[   61] Training loss: 0.35170831, Validation loss: 0.33514196, Gradient norm: 1.02917996
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 985.265s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 10.07207
INFO:root:EnergyScoreTrain: 5.61977
INFO:root:CoverageTrain: 0.74996
INFO:root:IntervalWidthTrain: 0.30002
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 9.08422
INFO:root:EnergyScoreValidation: 4.88121
INFO:root:CoverageValidation: 0.74939
INFO:root:IntervalWidthValidation: 0.2967
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 6.7658
INFO:root:EnergyScoreTest: 3.35838
INFO:root:CoverageTest: 0.94438
INFO:root:IntervalWidthTest: 0.34148
INFO:root:###13 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1117782016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.59654358, Validation loss: 0.61273307, Gradient norm: 5.86668797
INFO:root:[    2] Training loss: 1.58363694, Validation loss: 0.68754281, Gradient norm: 3.88866411
INFO:root:[    3] Training loss: 1.33783884, Validation loss: 0.56172852, Gradient norm: 3.05544534
INFO:root:[    4] Training loss: 1.20789521, Validation loss: 0.64061821, Gradient norm: 3.46828449
INFO:root:[    5] Training loss: 1.13055149, Validation loss: 0.55436430, Gradient norm: 3.21213516
INFO:root:[    6] Training loss: 1.06895618, Validation loss: 0.53932980, Gradient norm: 3.47444658
INFO:root:[    7] Training loss: 1.01052760, Validation loss: 0.53364370, Gradient norm: 3.22061038
INFO:root:[    8] Training loss: 0.97815049, Validation loss: 0.44672736, Gradient norm: 3.42787404
INFO:root:[    9] Training loss: 0.93708669, Validation loss: 0.39288244, Gradient norm: 2.67828560
INFO:root:[   10] Training loss: 0.91726223, Validation loss: 0.38750434, Gradient norm: 3.13929804
INFO:root:[   11] Training loss: 0.88960357, Validation loss: 0.39683383, Gradient norm: 2.26308386
INFO:root:[   12] Training loss: 0.87425146, Validation loss: 0.26494997, Gradient norm: 3.23758344
INFO:root:[   13] Training loss: 0.85341451, Validation loss: 0.21697607, Gradient norm: 2.91994442
INFO:root:[   14] Training loss: 0.83727927, Validation loss: 0.28070483, Gradient norm: 2.77580839
INFO:root:[   15] Training loss: 0.81684686, Validation loss: 0.23150962, Gradient norm: 3.46809229
INFO:root:[   16] Training loss: 0.80139825, Validation loss: 0.27161610, Gradient norm: 2.50963535
INFO:root:[   17] Training loss: 0.77672118, Validation loss: 0.20749408, Gradient norm: 2.21950624
INFO:root:[   18] Training loss: 0.76909799, Validation loss: 0.20942238, Gradient norm: 3.49109460
INFO:root:[   19] Training loss: 0.74379122, Validation loss: 0.21787002, Gradient norm: 1.92201252
INFO:root:[   20] Training loss: 0.73429261, Validation loss: 0.21147434, Gradient norm: 2.49141045
INFO:root:[   21] Training loss: 0.71930885, Validation loss: 0.20681345, Gradient norm: 2.50704025
INFO:root:[   22] Training loss: 0.70725050, Validation loss: 0.19182717, Gradient norm: 2.28981540
INFO:root:[   23] Training loss: 0.69592124, Validation loss: 0.19215686, Gradient norm: 1.93404305
INFO:root:[   24] Training loss: 0.68596057, Validation loss: 0.19136350, Gradient norm: 2.47032068
INFO:root:[   25] Training loss: 0.67017107, Validation loss: 0.20267941, Gradient norm: 2.92818987
INFO:root:[   26] Training loss: 0.66139044, Validation loss: 0.21976565, Gradient norm: 2.78249766
INFO:root:[   27] Training loss: 0.64943799, Validation loss: 0.22395163, Gradient norm: 2.52924003
INFO:root:[   28] Training loss: 0.64401368, Validation loss: 0.23491133, Gradient norm: 2.62276929
INFO:root:[   29] Training loss: 0.64022707, Validation loss: 0.18536426, Gradient norm: 2.54329381
INFO:root:[   30] Training loss: 0.63130013, Validation loss: 0.24561481, Gradient norm: 2.86108895
INFO:root:[   31] Training loss: 0.61868648, Validation loss: 0.23644487, Gradient norm: 3.17443788
INFO:root:[   32] Training loss: 0.61144364, Validation loss: 0.25392068, Gradient norm: 2.02722617
INFO:root:[   33] Training loss: 0.60759420, Validation loss: 0.23613854, Gradient norm: 3.48809363
INFO:root:[   34] Training loss: 0.59888216, Validation loss: 0.23722226, Gradient norm: 1.98637596
INFO:root:[   35] Training loss: 0.59367968, Validation loss: 0.23866847, Gradient norm: 2.54867400
INFO:root:[   36] Training loss: 0.58819169, Validation loss: 0.23294783, Gradient norm: 2.87807723
INFO:root:[   37] Training loss: 0.58472186, Validation loss: 0.22395232, Gradient norm: 4.12175626
INFO:root:[   38] Training loss: 0.57846753, Validation loss: 0.24479935, Gradient norm: 3.46450661
INFO:root:[   39] Training loss: 0.57138950, Validation loss: 0.28046925, Gradient norm: 2.31394772
INFO:root:[   40] Training loss: 0.56666227, Validation loss: 0.25006127, Gradient norm: 2.16146009
INFO:root:[   41] Training loss: 0.56373933, Validation loss: 0.30753127, Gradient norm: 2.45692778
INFO:root:[   42] Training loss: 0.56377003, Validation loss: 0.23141926, Gradient norm: 3.58972256
INFO:root:[   43] Training loss: 0.55668102, Validation loss: 0.27586147, Gradient norm: 3.68526989
INFO:root:[   44] Training loss: 0.54918310, Validation loss: 0.29340354, Gradient norm: 2.47571612
INFO:root:[   45] Training loss: 0.54519133, Validation loss: 0.25459218, Gradient norm: 2.97647783
INFO:root:[   46] Training loss: 0.54234461, Validation loss: 0.33378421, Gradient norm: 3.43352545
INFO:root:[   47] Training loss: 0.53746759, Validation loss: 0.39658523, Gradient norm: 3.17414914
INFO:root:[   48] Training loss: 0.53441857, Validation loss: 0.40143791, Gradient norm: 3.23149695
INFO:root:[   49] Training loss: 0.52844564, Validation loss: 0.37272266, Gradient norm: 3.18823581
INFO:root:[   50] Training loss: 0.52625651, Validation loss: 0.39106517, Gradient norm: 3.44771298
INFO:root:[   51] Training loss: 0.52154563, Validation loss: 0.36406419, Gradient norm: 3.57796129
INFO:root:[   52] Training loss: 0.52038681, Validation loss: 0.37372705, Gradient norm: 4.60996862
INFO:root:[   53] Training loss: 0.51741564, Validation loss: 0.37496463, Gradient norm: 4.72036130
INFO:root:[   54] Training loss: 0.51046881, Validation loss: 0.40536071, Gradient norm: 3.96486045
INFO:root:[   55] Training loss: 0.50841196, Validation loss: 0.44940585, Gradient norm: 1.86674254
INFO:root:[   56] Training loss: 0.50534391, Validation loss: 0.41731374, Gradient norm: 3.19184507
INFO:root:[   57] Training loss: 0.50351942, Validation loss: 0.39541831, Gradient norm: 3.21276709
INFO:root:[   58] Training loss: 0.49953756, Validation loss: 0.47399337, Gradient norm: 2.72856301
INFO:root:[   59] Training loss: 0.49703348, Validation loss: 0.43374382, Gradient norm: 3.71062505
INFO:root:[   60] Training loss: 0.49410472, Validation loss: 0.42364332, Gradient norm: 3.14497953
INFO:root:[   61] Training loss: 0.49070738, Validation loss: 0.41608612, Gradient norm: 4.37867822
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 971.577s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 57.96738
INFO:root:EnergyScoreTrain: 25.75936
INFO:root:CoverageTrain: 0.70149
INFO:root:IntervalWidthTrain: 1.92621
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 90.03992
INFO:root:EnergyScoreValidation: 47.14478
INFO:root:CoverageValidation: 0.53424
INFO:root:IntervalWidthValidation: 2.02538
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 18.01903
INFO:root:EnergyScoreTest: 8.83623
INFO:root:CoverageTest: 0.9995
INFO:root:IntervalWidthTest: 2.12568
INFO:root:###14 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1117782016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 3.89082591, Validation loss: 2.21174177, Gradient norm: 11.81905812
INFO:root:[    2] Training loss: 2.93752724, Validation loss: 2.30136979, Gradient norm: 8.07956062
INFO:root:[    3] Training loss: 2.47838650, Validation loss: 2.26912185, Gradient norm: 6.26094551
INFO:root:[    4] Training loss: 2.28508751, Validation loss: 2.15768051, Gradient norm: 5.50289911
INFO:root:[    5] Training loss: 2.10796861, Validation loss: 1.97430410, Gradient norm: 4.84645813
INFO:root:[    6] Training loss: 1.97905811, Validation loss: 2.02850179, Gradient norm: 4.54061367
INFO:root:[    7] Training loss: 1.88040809, Validation loss: 1.92074740, Gradient norm: 4.44205915
INFO:root:[    8] Training loss: 1.81141478, Validation loss: 1.72347518, Gradient norm: 4.27574021
INFO:root:[    9] Training loss: 1.76245348, Validation loss: 1.54012583, Gradient norm: 4.34577196
INFO:root:[   10] Training loss: 1.74436171, Validation loss: 1.47834136, Gradient norm: 4.53747789
INFO:root:[   11] Training loss: 1.68961639, Validation loss: 1.48450973, Gradient norm: 5.14169871
INFO:root:[   12] Training loss: 1.68183014, Validation loss: 1.40524441, Gradient norm: 5.56589683
INFO:root:[   13] Training loss: 1.70489081, Validation loss: 1.39867383, Gradient norm: 6.41967859
INFO:root:[   14] Training loss: 1.72121213, Validation loss: 1.43466310, Gradient norm: 7.66455580
INFO:root:[   15] Training loss: 1.78173772, Validation loss: 1.41770935, Gradient norm: 12.55590605
INFO:root:[   16] Training loss: 1.82548853, Validation loss: 1.50704674, Gradient norm: 22.16355509
INFO:root:[   17] Training loss: 1.79124414, Validation loss: 0.93128203, Gradient norm: 62.11694673
INFO:root:[   18] Training loss: 1.67088379, Validation loss: 0.86446359, Gradient norm: 92.84655083
INFO:root:[   19] Training loss: 1.59852808, Validation loss: 0.94440367, Gradient norm: 122.30996518
INFO:root:[   20] Training loss: 1.56904778, Validation loss: 0.87858329, Gradient norm: 181.66603199
INFO:root:[   21] Training loss: 1.52172626, Validation loss: 0.85013204, Gradient norm: 150.62873363
INFO:root:[   22] Training loss: 1.51421994, Validation loss: 1.08019280, Gradient norm: 356.68392343
INFO:root:[   23] Training loss: 1.49482037, Validation loss: 0.76591877, Gradient norm: 355.79271577
INFO:root:[   24] Training loss: 1.47216246, Validation loss: 0.74376286, Gradient norm: 295.64687323
INFO:root:[   25] Training loss: 1.50210617, Validation loss: 0.62905627, Gradient norm: 624.24789967
INFO:root:[   26] Training loss: 1.47070644, Validation loss: 0.96853495, Gradient norm: 559.61412731
INFO:root:[   27] Training loss: 1.43870789, Validation loss: 1.46554490, Gradient norm: 419.07231021
INFO:root:[   28] Training loss: 1.44502052, Validation loss: 1.03737084, Gradient norm: 720.89302168
INFO:root:[   29] Training loss: 1.41752040, Validation loss: 0.81613176, Gradient norm: 446.70580866
INFO:root:[   30] Training loss: 1.40797615, Validation loss: 0.48897864, Gradient norm: 475.36861374
INFO:root:[   31] Training loss: 1.41292779, Validation loss: 0.63378197, Gradient norm: 634.90779695
INFO:root:[   32] Training loss: 1.40525475, Validation loss: 0.60929583, Gradient norm: 631.63616930
INFO:root:[   33] Training loss: 1.40656106, Validation loss: 0.59156284, Gradient norm: 931.98815018
INFO:root:[   34] Training loss: 1.38507897, Validation loss: 1.17541198, Gradient norm: 699.43516007
INFO:root:[   35] Training loss: 1.39851367, Validation loss: 1.15237776, Gradient norm: 838.78686475
INFO:root:[   36] Training loss: 1.36923019, Validation loss: 0.81048386, Gradient norm: 533.75788173
INFO:root:[   37] Training loss: 1.36585404, Validation loss: 1.06504957, Gradient norm: 512.41059234
INFO:root:[   38] Training loss: 1.36928769, Validation loss: 0.86693913, Gradient norm: 741.58364596
INFO:root:[   39] Training loss: 1.37417904, Validation loss: 0.74298241, Gradient norm: 1182.62281185
INFO:root:[   40] Training loss: 1.33352718, Validation loss: 0.81973021, Gradient norm: 572.77828112
INFO:root:[   41] Training loss: 1.33760076, Validation loss: 0.69345292, Gradient norm: 751.45999495
INFO:root:[   42] Training loss: 1.33463775, Validation loss: 0.55611747, Gradient norm: 755.28330061
INFO:root:[   43] Training loss: 1.32227567, Validation loss: 0.87964533, Gradient norm: 620.17770618
INFO:root:[   44] Training loss: 1.31907303, Validation loss: 1.05442582, Gradient norm: 804.49706675
INFO:root:[   45] Training loss: 1.30621550, Validation loss: 1.03381419, Gradient norm: 739.77485169
INFO:root:[   46] Training loss: 1.31602852, Validation loss: 1.02703889, Gradient norm: 834.72916519
INFO:root:[   47] Training loss: 1.29535971, Validation loss: 1.10073662, Gradient norm: 722.23183352
INFO:root:[   48] Training loss: 1.31380199, Validation loss: 0.77841601, Gradient norm: 1205.72903253
INFO:root:[   49] Training loss: 1.30926406, Validation loss: 0.95062322, Gradient norm: 1294.20991350
INFO:root:[   50] Training loss: 1.27205575, Validation loss: 0.63070062, Gradient norm: 839.57091271
INFO:root:[   51] Training loss: 1.27720592, Validation loss: 1.22064888, Gradient norm: 914.74707493
INFO:root:[   52] Training loss: 1.30935647, Validation loss: 1.06183531, Gradient norm: 1492.54927333
INFO:root:[   53] Training loss: 1.25467628, Validation loss: 0.81456767, Gradient norm: 761.85557038
INFO:root:[   54] Training loss: 1.24398672, Validation loss: 0.90294883, Gradient norm: 666.14731943
INFO:root:[   55] Training loss: 1.25847460, Validation loss: 0.85161893, Gradient norm: 1002.18800507
INFO:root:[   56] Training loss: 1.23657288, Validation loss: 0.94450479, Gradient norm: 795.80735520
INFO:root:[   57] Training loss: 1.22862330, Validation loss: 0.83074277, Gradient norm: 867.37975971
INFO:root:[   58] Training loss: 1.22028824, Validation loss: 0.47056691, Gradient norm: 607.78316949
INFO:root:[   59] Training loss: 1.25041849, Validation loss: 0.51642233, Gradient norm: 1272.58930976
INFO:root:[   60] Training loss: 1.26746536, Validation loss: 0.41758601, Gradient norm: 1797.61376394
INFO:root:[   61] Training loss: 1.22188529, Validation loss: 0.47299176, Gradient norm: 1071.58007069
INFO:root:[   62] Training loss: 1.24865826, Validation loss: 0.96058908, Gradient norm: 1761.83123292
INFO:root:[   63] Training loss: 1.22294067, Validation loss: 0.90230991, Gradient norm: 1157.53367321
INFO:root:[   64] Training loss: 1.20696065, Validation loss: 0.43717375, Gradient norm: 1111.91486146
INFO:root:[   65] Training loss: 1.18226336, Validation loss: 0.82615111, Gradient norm: 806.27705336
INFO:root:[   66] Training loss: 1.18699648, Validation loss: 0.88204207, Gradient norm: 1006.51106508
INFO:root:[   67] Training loss: 1.17102858, Validation loss: 0.72728647, Gradient norm: 543.42052373
INFO:root:[   68] Training loss: 1.17777238, Validation loss: 0.73552623, Gradient norm: 1210.84153564
INFO:root:[   69] Training loss: 1.16723941, Validation loss: 0.88653805, Gradient norm: 999.72385234
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 1097.734s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 425.81901
INFO:root:EnergyScoreTrain: 187.07189
INFO:root:CoverageTrain: 0.82227
INFO:root:IntervalWidthTrain: 13.14269
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 242.89067
INFO:root:EnergyScoreValidation: 113.2586
INFO:root:CoverageValidation: 0.64482
INFO:root:IntervalWidthValidation: 7.73565
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 278.86113
INFO:root:EnergyScoreTest: 89.18208
INFO:root:CoverageTest: 0.68012
INFO:root:IntervalWidthTest: 17.61297
INFO:root:###15 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1117782016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.28458744, Validation loss: 0.40694828, Gradient norm: 7.55719886
INFO:root:[    2] Training loss: 0.33265993, Validation loss: 0.30077962, Gradient norm: 3.05078318
INFO:root:[    3] Training loss: 0.29096133, Validation loss: 0.27266056, Gradient norm: 3.30812032
INFO:root:[    4] Training loss: 0.27017804, Validation loss: 0.27489124, Gradient norm: 2.20772477
INFO:root:[    5] Training loss: 0.26033201, Validation loss: 0.25171278, Gradient norm: 2.29513632
INFO:root:[    6] Training loss: 0.24762418, Validation loss: 0.24589320, Gradient norm: 2.20537247
INFO:root:[    7] Training loss: 0.23697683, Validation loss: 0.23219665, Gradient norm: 2.91253659
INFO:root:[    8] Training loss: 0.23002350, Validation loss: 0.22884723, Gradient norm: 2.39334837
INFO:root:[    9] Training loss: 0.22584417, Validation loss: 0.22197601, Gradient norm: 3.03935726
INFO:root:[   10] Training loss: 0.21859401, Validation loss: 0.21839681, Gradient norm: 2.29268881
INFO:root:[   11] Training loss: 0.21283431, Validation loss: 0.21260484, Gradient norm: 2.60209270
INFO:root:[   12] Training loss: 0.20782650, Validation loss: 0.20643408, Gradient norm: 3.30774715
INFO:root:[   13] Training loss: 0.20313009, Validation loss: 0.21499631, Gradient norm: 3.31624442
INFO:root:[   14] Training loss: 0.20066239, Validation loss: 0.19527114, Gradient norm: 2.81162751
INFO:root:[   15] Training loss: 0.19708994, Validation loss: 0.19391433, Gradient norm: 2.29661424
INFO:root:[   16] Training loss: 0.19372581, Validation loss: 0.19902845, Gradient norm: 3.28052569
INFO:root:[   17] Training loss: 0.19008648, Validation loss: 0.18696743, Gradient norm: 2.89937038
INFO:root:[   18] Training loss: 0.18754792, Validation loss: 0.18888677, Gradient norm: 3.80615307
INFO:root:[   19] Training loss: 0.18281573, Validation loss: 0.19135475, Gradient norm: 2.78949515
INFO:root:[   20] Training loss: 0.18070189, Validation loss: 0.18810733, Gradient norm: 2.99856015
INFO:root:[   21] Training loss: 0.18146306, Validation loss: 0.18018041, Gradient norm: 3.97065848
INFO:root:[   22] Training loss: 0.17615679, Validation loss: 0.17067640, Gradient norm: 3.59277693
INFO:root:[   23] Training loss: 0.17041862, Validation loss: 0.17366298, Gradient norm: 2.70920651
INFO:root:[   24] Training loss: 0.16937236, Validation loss: 0.17245444, Gradient norm: 2.73694786
INFO:root:[   25] Training loss: 0.16892980, Validation loss: 0.17244017, Gradient norm: 3.30128008
INFO:root:[   26] Training loss: 0.16628016, Validation loss: 0.16652167, Gradient norm: 3.16304375
INFO:root:[   27] Training loss: 0.16282947, Validation loss: 0.15960675, Gradient norm: 2.88826515
INFO:root:[   28] Training loss: 0.15990909, Validation loss: 0.16524941, Gradient norm: 1.65496220
INFO:root:[   29] Training loss: 0.16072349, Validation loss: 0.16141583, Gradient norm: 3.20280438
INFO:root:[   30] Training loss: 0.15993631, Validation loss: 0.15588971, Gradient norm: 3.19070929
INFO:root:[   31] Training loss: 0.15459150, Validation loss: 0.15436321, Gradient norm: 1.92256306
INFO:root:[   32] Training loss: 0.15514710, Validation loss: 0.15086375, Gradient norm: 3.27888432
INFO:root:[   33] Training loss: 0.15570064, Validation loss: 0.15174911, Gradient norm: 3.55087168
INFO:root:[   34] Training loss: 0.14981926, Validation loss: 0.15532711, Gradient norm: 1.60032322
INFO:root:[   35] Training loss: 0.14803864, Validation loss: 0.14700160, Gradient norm: 2.64252228
INFO:root:[   36] Training loss: 0.14570033, Validation loss: 0.14692342, Gradient norm: 1.84081358
INFO:root:[   37] Training loss: 0.14846285, Validation loss: 0.14576556, Gradient norm: 3.33337047
INFO:root:[   38] Training loss: 0.14396314, Validation loss: 0.14271518, Gradient norm: 2.20812045
INFO:root:[   39] Training loss: 0.14312867, Validation loss: 0.14953007, Gradient norm: 2.38671998
INFO:root:[   40] Training loss: 0.14518685, Validation loss: 0.14467894, Gradient norm: 4.04281338
INFO:root:[   41] Training loss: 0.14031763, Validation loss: 0.13991232, Gradient norm: 2.51905267
INFO:root:[   42] Training loss: 0.13801088, Validation loss: 0.13704169, Gradient norm: 2.25444104
INFO:root:[   43] Training loss: 0.13915109, Validation loss: 0.13778388, Gradient norm: 2.93214027
INFO:root:[   44] Training loss: 0.13715780, Validation loss: 0.13415083, Gradient norm: 2.35523888
INFO:root:[   45] Training loss: 0.13690054, Validation loss: 0.13819510, Gradient norm: 2.63505660
INFO:root:[   46] Training loss: 0.13437684, Validation loss: 0.13500964, Gradient norm: 2.24088704
INFO:root:[   47] Training loss: 0.13425409, Validation loss: 0.13058478, Gradient norm: 2.63431054
INFO:root:[   48] Training loss: 0.13234715, Validation loss: 0.13264114, Gradient norm: 2.10846381
INFO:root:[   49] Training loss: 0.13311168, Validation loss: 0.13648974, Gradient norm: 2.99941550
INFO:root:[   50] Training loss: 0.13243451, Validation loss: 0.13442426, Gradient norm: 3.31048864
INFO:root:[   51] Training loss: 0.13253206, Validation loss: 0.13264285, Gradient norm: 3.49580606
INFO:root:[   52] Training loss: 0.12950774, Validation loss: 0.13244757, Gradient norm: 2.21175309
INFO:root:[   53] Training loss: 0.12856688, Validation loss: 0.12956795, Gradient norm: 2.44702491
INFO:root:[   54] Training loss: 0.13025653, Validation loss: 0.12754112, Gradient norm: 3.56538192
INFO:root:[   55] Training loss: 0.12583877, Validation loss: 0.12450810, Gradient norm: 2.27394011
INFO:root:[   56] Training loss: 0.12697417, Validation loss: 0.12467509, Gradient norm: 2.57236118
INFO:root:[   57] Training loss: 0.12627219, Validation loss: 0.12448060, Gradient norm: 2.48710374
INFO:root:[   58] Training loss: 0.12271225, Validation loss: 0.12382142, Gradient norm: 1.62672781
INFO:root:[   59] Training loss: 0.12213673, Validation loss: 0.12248209, Gradient norm: 1.56277407
INFO:root:[   60] Training loss: 0.12220260, Validation loss: 0.12089565, Gradient norm: 1.68234808
INFO:root:[   61] Training loss: 0.12066069, Validation loss: 0.12017869, Gradient norm: 1.68051592
INFO:root:[   62] Training loss: 0.12014820, Validation loss: 0.12080102, Gradient norm: 1.92590489
INFO:root:[   63] Training loss: 0.12019990, Validation loss: 0.12162448, Gradient norm: 2.26341438
INFO:root:[   64] Training loss: 0.11924755, Validation loss: 0.11925669, Gradient norm: 2.33578083
INFO:root:[   65] Training loss: 0.11729119, Validation loss: 0.11662872, Gradient norm: 1.47755688
INFO:root:[   66] Training loss: 0.11981966, Validation loss: 0.12321237, Gradient norm: 2.92577659
INFO:root:[   67] Training loss: 0.11838883, Validation loss: 0.11613910, Gradient norm: 2.56496150
INFO:root:[   68] Training loss: 0.11913101, Validation loss: 0.11495726, Gradient norm: 2.96855008
INFO:root:[   69] Training loss: 0.11577916, Validation loss: 0.11584163, Gradient norm: 1.86344373
INFO:root:[   70] Training loss: 0.11655167, Validation loss: 0.12099561, Gradient norm: 2.95745415
INFO:root:[   71] Training loss: 0.11651355, Validation loss: 0.11755350, Gradient norm: 2.74071274
INFO:root:[   72] Training loss: 0.11452771, Validation loss: 0.11317862, Gradient norm: 2.61854675
INFO:root:[   73] Training loss: 0.11483004, Validation loss: 0.11780909, Gradient norm: 2.44945863
INFO:root:[   74] Training loss: 0.11473592, Validation loss: 0.11380229, Gradient norm: 2.58978291
INFO:root:[   75] Training loss: 0.11357321, Validation loss: 0.11224782, Gradient norm: 1.88917372
INFO:root:[   76] Training loss: 0.11231578, Validation loss: 0.11310903, Gradient norm: 1.97888576
INFO:root:[   77] Training loss: 0.11399768, Validation loss: 0.11315317, Gradient norm: 2.54008359
INFO:root:[   78] Training loss: 0.11262122, Validation loss: 0.11091813, Gradient norm: 2.73824359
INFO:root:[   79] Training loss: 0.10981905, Validation loss: 0.10949047, Gradient norm: 1.30440422
INFO:root:[   80] Training loss: 0.10950307, Validation loss: 0.11406466, Gradient norm: 1.49179380
INFO:root:[   81] Training loss: 0.11106017, Validation loss: 0.10876203, Gradient norm: 2.89586711
INFO:root:[   82] Training loss: 0.10905568, Validation loss: 0.11238957, Gradient norm: 1.55550982
INFO:root:[   83] Training loss: 0.10888439, Validation loss: 0.10570752, Gradient norm: 2.27330719
INFO:root:[   84] Training loss: 0.10995383, Validation loss: 0.10667456, Gradient norm: 2.65842080
INFO:root:[   85] Training loss: 0.10841394, Validation loss: 0.10663996, Gradient norm: 2.44801951
INFO:root:[   86] Training loss: 0.10981783, Validation loss: 0.10574603, Gradient norm: 3.10991974
INFO:root:[   87] Training loss: 0.10652450, Validation loss: 0.10867536, Gradient norm: 1.89132675
INFO:root:[   88] Training loss: 0.10852298, Validation loss: 0.10729515, Gradient norm: 2.94591831
INFO:root:[   89] Training loss: 0.10590760, Validation loss: 0.10410498, Gradient norm: 1.84490212
INFO:root:[   90] Training loss: 0.10503460, Validation loss: 0.10516595, Gradient norm: 1.29309003
INFO:root:[   91] Training loss: 0.10699909, Validation loss: 0.10944066, Gradient norm: 2.44488638
INFO:root:[   92] Training loss: 0.10551701, Validation loss: 0.10424676, Gradient norm: 2.31780457
INFO:root:[   93] Training loss: 0.10372642, Validation loss: 0.10287583, Gradient norm: 1.42218436
INFO:root:[   94] Training loss: 0.10408622, Validation loss: 0.10457506, Gradient norm: 1.67892787
INFO:root:[   95] Training loss: 0.10263765, Validation loss: 0.10338897, Gradient norm: 1.23137992
INFO:root:[   96] Training loss: 0.10388208, Validation loss: 0.10339165, Gradient norm: 2.71936259
INFO:root:[   97] Training loss: 0.10267481, Validation loss: 0.10529065, Gradient norm: 2.04307360
INFO:root:[   98] Training loss: 0.10390563, Validation loss: 0.10432999, Gradient norm: 2.82641902
INFO:root:[   99] Training loss: 0.10197374, Validation loss: 0.10400771, Gradient norm: 2.32839440
INFO:root:[  100] Training loss: 0.10274574, Validation loss: 0.10346029, Gradient norm: 2.54901037
INFO:root:[  101] Training loss: 0.10161173, Validation loss: 0.10750878, Gradient norm: 2.43720424
INFO:root:[  102] Training loss: 0.10216602, Validation loss: 0.10045437, Gradient norm: 2.59437253
INFO:root:[  103] Training loss: 0.10108346, Validation loss: 0.09990053, Gradient norm: 1.63034046
INFO:root:[  104] Training loss: 0.09957656, Validation loss: 0.10315397, Gradient norm: 1.44218172
INFO:root:[  105] Training loss: 0.09968708, Validation loss: 0.09937240, Gradient norm: 1.81693047
INFO:root:[  106] Training loss: 0.09933701, Validation loss: 0.09821751, Gradient norm: 2.35292658
INFO:root:[  107] Training loss: 0.09979146, Validation loss: 0.09879786, Gradient norm: 2.45386968
INFO:root:[  108] Training loss: 0.09986798, Validation loss: 0.09981368, Gradient norm: 2.26865222
INFO:root:[  109] Training loss: 0.09836949, Validation loss: 0.09911758, Gradient norm: 2.00413584
INFO:root:[  110] Training loss: 0.09780905, Validation loss: 0.09797987, Gradient norm: 1.78381306
INFO:root:[  111] Training loss: 0.09780891, Validation loss: 0.10059225, Gradient norm: 1.95856660
INFO:root:[  112] Training loss: 0.09721471, Validation loss: 0.09637838, Gradient norm: 1.74318768
INFO:root:[  113] Training loss: 0.09620875, Validation loss: 0.09847658, Gradient norm: 1.57002638
INFO:root:[  114] Training loss: 0.09758213, Validation loss: 0.09984764, Gradient norm: 2.32542188
INFO:root:[  115] Training loss: 0.09687557, Validation loss: 0.09786516, Gradient norm: 2.34651072
INFO:root:[  116] Training loss: 0.09735996, Validation loss: 0.09538016, Gradient norm: 2.31489671
INFO:root:[  117] Training loss: 0.09510769, Validation loss: 0.09814810, Gradient norm: 1.70784380
INFO:root:[  118] Training loss: 0.09565080, Validation loss: 0.09653670, Gradient norm: 2.06349744
INFO:root:[  119] Training loss: 0.09549042, Validation loss: 0.09279821, Gradient norm: 2.18870929
INFO:root:[  120] Training loss: 0.09573431, Validation loss: 0.09697049, Gradient norm: 2.55815259
INFO:root:[  121] Training loss: 0.09455598, Validation loss: 0.09472787, Gradient norm: 2.10754678
INFO:root:[  122] Training loss: 0.09357536, Validation loss: 0.09432823, Gradient norm: 1.64765170
INFO:root:[  123] Training loss: 0.09363419, Validation loss: 0.09351491, Gradient norm: 2.08984546
INFO:root:[  124] Training loss: 0.09385470, Validation loss: 0.09517319, Gradient norm: 1.76237331
INFO:root:[  125] Training loss: 0.09550270, Validation loss: 0.09312721, Gradient norm: 2.82508206
INFO:root:[  126] Training loss: 0.09270027, Validation loss: 0.09354232, Gradient norm: 1.91726126
INFO:root:[  127] Training loss: 0.09252829, Validation loss: 0.09265529, Gradient norm: 1.88612441
INFO:root:[  128] Training loss: 0.09300715, Validation loss: 0.09197906, Gradient norm: 2.35777858
INFO:root:[  129] Training loss: 0.09333575, Validation loss: 0.09401581, Gradient norm: 2.35428515
INFO:root:[  130] Training loss: 0.09133845, Validation loss: 0.09214522, Gradient norm: 1.72963915
INFO:root:[  131] Training loss: 0.09079143, Validation loss: 0.09194657, Gradient norm: 1.41656819
INFO:root:[  132] Training loss: 0.09100343, Validation loss: 0.09202674, Gradient norm: 1.57298082
INFO:root:[  133] Training loss: 0.09077345, Validation loss: 0.09270908, Gradient norm: 1.58481821
INFO:root:[  134] Training loss: 0.09125746, Validation loss: 0.09113124, Gradient norm: 1.96120030
INFO:root:[  135] Training loss: 0.09099291, Validation loss: 0.09046361, Gradient norm: 2.05075292
INFO:root:[  136] Training loss: 0.09011061, Validation loss: 0.08893169, Gradient norm: 1.84501433
INFO:root:[  137] Training loss: 0.09040824, Validation loss: 0.09161699, Gradient norm: 1.78086482
INFO:root:[  138] Training loss: 0.08944444, Validation loss: 0.09059471, Gradient norm: 1.80305721
INFO:root:[  139] Training loss: 0.08947493, Validation loss: 0.09126026, Gradient norm: 1.77191754
INFO:root:[  140] Training loss: 0.09041447, Validation loss: 0.09030305, Gradient norm: 2.03112760
INFO:root:[  141] Training loss: 0.08951229, Validation loss: 0.08981186, Gradient norm: 1.96310597
INFO:root:[  142] Training loss: 0.08857097, Validation loss: 0.08836709, Gradient norm: 1.86164493
INFO:root:[  143] Training loss: 0.08941256, Validation loss: 0.09042844, Gradient norm: 2.27393139
INFO:root:[  144] Training loss: 0.08843900, Validation loss: 0.08805854, Gradient norm: 1.99857793
INFO:root:[  145] Training loss: 0.08803025, Validation loss: 0.08830831, Gradient norm: 1.72028068
INFO:root:[  146] Training loss: 0.08761844, Validation loss: 0.08758202, Gradient norm: 1.60058243
INFO:root:[  147] Training loss: 0.08781857, Validation loss: 0.08772749, Gradient norm: 2.17343409
INFO:root:[  148] Training loss: 0.08830421, Validation loss: 0.08968033, Gradient norm: 2.20970700
INFO:root:[  149] Training loss: 0.08714932, Validation loss: 0.08746688, Gradient norm: 2.12065513
INFO:root:[  150] Training loss: 0.08713844, Validation loss: 0.08704520, Gradient norm: 1.72924723
INFO:root:[  151] Training loss: 0.08595955, Validation loss: 0.08779305, Gradient norm: 1.41427199
INFO:root:[  152] Training loss: 0.08690570, Validation loss: 0.08639902, Gradient norm: 1.87615445
INFO:root:[  153] Training loss: 0.08648269, Validation loss: 0.08822930, Gradient norm: 1.93295017
INFO:root:[  154] Training loss: 0.08598670, Validation loss: 0.08539145, Gradient norm: 1.72326110
INFO:root:[  155] Training loss: 0.08614878, Validation loss: 0.08725191, Gradient norm: 1.76940727
INFO:root:[  156] Training loss: 0.08572788, Validation loss: 0.08510436, Gradient norm: 1.80613456
INFO:root:[  157] Training loss: 0.08565622, Validation loss: 0.08396114, Gradient norm: 1.77822972
INFO:root:[  158] Training loss: 0.08470322, Validation loss: 0.08654608, Gradient norm: 1.31067998
INFO:root:[  159] Training loss: 0.08540821, Validation loss: 0.08362786, Gradient norm: 1.70064876
INFO:root:[  160] Training loss: 0.08472197, Validation loss: 0.08574022, Gradient norm: 1.72256100
INFO:root:[  161] Training loss: 0.08472970, Validation loss: 0.08491135, Gradient norm: 1.60498506
INFO:root:[  162] Training loss: 0.08511697, Validation loss: 0.08349390, Gradient norm: 2.06124026
INFO:root:[  163] Training loss: 0.08439019, Validation loss: 0.08399847, Gradient norm: 1.59041172
INFO:root:[  164] Training loss: 0.08412105, Validation loss: 0.08317467, Gradient norm: 1.52107884
INFO:root:[  165] Training loss: 0.08403327, Validation loss: 0.08704517, Gradient norm: 1.65642914
INFO:root:[  166] Training loss: 0.08446090, Validation loss: 0.08322106, Gradient norm: 2.03770125
INFO:root:[  167] Training loss: 0.08417229, Validation loss: 0.08562763, Gradient norm: 2.09086818
INFO:root:[  168] Training loss: 0.08407902, Validation loss: 0.08429762, Gradient norm: 2.08266631
INFO:root:[  169] Training loss: 0.08404399, Validation loss: 0.08505382, Gradient norm: 1.98398780
INFO:root:[  170] Training loss: 0.08324208, Validation loss: 0.08304044, Gradient norm: 1.45626578
INFO:root:[  171] Training loss: 0.08315127, Validation loss: 0.08225538, Gradient norm: 1.58757203
INFO:root:[  172] Training loss: 0.08308681, Validation loss: 0.08451199, Gradient norm: 1.74260524
INFO:root:[  173] Training loss: 0.08303316, Validation loss: 0.08183233, Gradient norm: 1.73472917
INFO:root:[  174] Training loss: 0.08250627, Validation loss: 0.08222228, Gradient norm: 1.57457523
INFO:root:[  175] Training loss: 0.08212913, Validation loss: 0.08285946, Gradient norm: 1.53271815
INFO:root:[  176] Training loss: 0.08216804, Validation loss: 0.08183310, Gradient norm: 1.34246092
INFO:root:[  177] Training loss: 0.08218277, Validation loss: 0.08312050, Gradient norm: 1.56631993
INFO:root:[  178] Training loss: 0.08208918, Validation loss: 0.08106386, Gradient norm: 1.77243324
INFO:root:[  179] Training loss: 0.08201045, Validation loss: 0.08300138, Gradient norm: 1.78763349
INFO:root:[  180] Training loss: 0.08128972, Validation loss: 0.08188635, Gradient norm: 1.43552416
INFO:root:[  181] Training loss: 0.08182741, Validation loss: 0.08008068, Gradient norm: 1.75959523
INFO:root:[  182] Training loss: 0.08135003, Validation loss: 0.08125532, Gradient norm: 1.82685886
INFO:root:[  183] Training loss: 0.08105200, Validation loss: 0.07982456, Gradient norm: 1.32117579
INFO:root:[  184] Training loss: 0.08060003, Validation loss: 0.08182243, Gradient norm: 1.57513115
INFO:root:[  185] Training loss: 0.08088509, Validation loss: 0.08125250, Gradient norm: 1.82593755
INFO:root:[  186] Training loss: 0.08071448, Validation loss: 0.07974328, Gradient norm: 1.75964401
INFO:root:[  187] Training loss: 0.08104582, Validation loss: 0.08314985, Gradient norm: 1.71524281
INFO:root:[  188] Training loss: 0.08029907, Validation loss: 0.07984205, Gradient norm: 1.79169704
INFO:root:[  189] Training loss: 0.08050139, Validation loss: 0.08072938, Gradient norm: 1.63026316
INFO:root:[  190] Training loss: 0.08040469, Validation loss: 0.07995488, Gradient norm: 1.85452132
INFO:root:[  191] Training loss: 0.07938956, Validation loss: 0.07840900, Gradient norm: 1.36419937
INFO:root:[  192] Training loss: 0.07952992, Validation loss: 0.08026788, Gradient norm: 1.22075567
INFO:root:[  193] Training loss: 0.08041099, Validation loss: 0.07965605, Gradient norm: 1.88536486
INFO:root:[  194] Training loss: 0.07961870, Validation loss: 0.07933347, Gradient norm: 1.87595441
INFO:root:[  195] Training loss: 0.07923054, Validation loss: 0.07906431, Gradient norm: 1.33215132
INFO:root:[  196] Training loss: 0.07926282, Validation loss: 0.07792401, Gradient norm: 1.55046291
INFO:root:[  197] Training loss: 0.07907267, Validation loss: 0.08107579, Gradient norm: 1.49654667
INFO:root:[  198] Training loss: 0.07942317, Validation loss: 0.08016121, Gradient norm: 1.92851839
INFO:root:[  199] Training loss: 0.07919387, Validation loss: 0.07888533, Gradient norm: 1.55436153
INFO:root:[  200] Training loss: 0.07876735, Validation loss: 0.07794451, Gradient norm: 1.57955869
INFO:root:[  201] Training loss: 0.07876305, Validation loss: 0.07862495, Gradient norm: 1.65523418
INFO:root:[  202] Training loss: 0.07828701, Validation loss: 0.07808705, Gradient norm: 1.40039916
INFO:root:[  203] Training loss: 0.07828889, Validation loss: 0.07717384, Gradient norm: 1.77824330
INFO:root:[  204] Training loss: 0.07795447, Validation loss: 0.07927400, Gradient norm: 1.41356494
INFO:root:[  205] Training loss: 0.07810300, Validation loss: 0.07892470, Gradient norm: 1.57644227
INFO:root:[  206] Training loss: 0.07841812, Validation loss: 0.07722619, Gradient norm: 1.58504091
INFO:root:[  207] Training loss: 0.07780157, Validation loss: 0.07806739, Gradient norm: 1.67949501
INFO:root:[  208] Training loss: 0.07859602, Validation loss: 0.07748455, Gradient norm: 1.95705112
INFO:root:[  209] Training loss: 0.07811590, Validation loss: 0.07670640, Gradient norm: 2.00569970
INFO:root:[  210] Training loss: 0.07748145, Validation loss: 0.07969707, Gradient norm: 1.40505459
INFO:root:[  211] Training loss: 0.07873219, Validation loss: 0.07777315, Gradient norm: 2.50246978
INFO:root:[  212] Training loss: 0.07765759, Validation loss: 0.07705144, Gradient norm: 1.54940328
INFO:root:[  213] Training loss: 0.07718440, Validation loss: 0.07662990, Gradient norm: 1.41799921
INFO:root:[  214] Training loss: 0.07734074, Validation loss: 0.07736162, Gradient norm: 1.64820477
INFO:root:[  215] Training loss: 0.07803353, Validation loss: 0.07695482, Gradient norm: 2.43084739
INFO:root:[  216] Training loss: 0.07665500, Validation loss: 0.07639861, Gradient norm: 1.59533359
INFO:root:[  217] Training loss: 0.07801571, Validation loss: 0.07850363, Gradient norm: 2.04957489
INFO:root:[  218] Training loss: 0.07674320, Validation loss: 0.07665920, Gradient norm: 1.68534911
INFO:root:[  219] Training loss: 0.07641070, Validation loss: 0.07810729, Gradient norm: 1.22462551
INFO:root:[  220] Training loss: 0.07668413, Validation loss: 0.07611925, Gradient norm: 1.88233722
INFO:root:[  221] Training loss: 0.07609252, Validation loss: 0.07594206, Gradient norm: 1.24019005
INFO:root:[  222] Training loss: 0.07607166, Validation loss: 0.07614092, Gradient norm: 1.10797041
INFO:root:[  223] Training loss: 0.07655072, Validation loss: 0.07817270, Gradient norm: 1.64722499
INFO:root:[  224] Training loss: 0.07633047, Validation loss: 0.07589377, Gradient norm: 1.77741712
INFO:root:[  225] Training loss: 0.07636394, Validation loss: 0.07546883, Gradient norm: 1.97047601
INFO:root:[  226] Training loss: 0.07561610, Validation loss: 0.07591680, Gradient norm: 1.31965214
INFO:root:[  227] Training loss: 0.07632042, Validation loss: 0.07551863, Gradient norm: 1.37216178
INFO:root:[  228] Training loss: 0.07587003, Validation loss: 0.07466844, Gradient norm: 1.61126475
INFO:root:[  229] Training loss: 0.07600093, Validation loss: 0.07519495, Gradient norm: 1.49886077
INFO:root:[  230] Training loss: 0.07587045, Validation loss: 0.07614240, Gradient norm: 1.89603499
INFO:root:[  231] Training loss: 0.07544292, Validation loss: 0.07601341, Gradient norm: 1.51744680
INFO:root:[  232] Training loss: 0.07563261, Validation loss: 0.07718841, Gradient norm: 1.09035304
INFO:root:[  233] Training loss: 0.07594933, Validation loss: 0.07504217, Gradient norm: 1.86035863
INFO:root:[  234] Training loss: 0.07503454, Validation loss: 0.07438520, Gradient norm: 1.26220783
INFO:root:[  235] Training loss: 0.07478256, Validation loss: 0.07488394, Gradient norm: 1.25996873
INFO:root:[  236] Training loss: 0.07523307, Validation loss: 0.07517273, Gradient norm: 1.72733328
INFO:root:[  237] Training loss: 0.07508168, Validation loss: 0.07659527, Gradient norm: 1.46686699
INFO:root:[  238] Training loss: 0.07478167, Validation loss: 0.07464209, Gradient norm: 1.60268976
INFO:root:[  239] Training loss: 0.07436891, Validation loss: 0.07606430, Gradient norm: 1.05655553
INFO:root:[  240] Training loss: 0.07485168, Validation loss: 0.07440455, Gradient norm: 1.77801759
INFO:root:[  241] Training loss: 0.07434954, Validation loss: 0.07489411, Gradient norm: 1.45963741
INFO:root:[  242] Training loss: 0.07478718, Validation loss: 0.07367833, Gradient norm: 1.54277470
INFO:root:[  243] Training loss: 0.07398736, Validation loss: 0.07329854, Gradient norm: 1.40553175
INFO:root:[  244] Training loss: 0.07400572, Validation loss: 0.07368899, Gradient norm: 1.26371858
INFO:root:[  245] Training loss: 0.07385908, Validation loss: 0.07313819, Gradient norm: 1.80589531
INFO:root:[  246] Training loss: 0.07353063, Validation loss: 0.07325974, Gradient norm: 1.30712393
INFO:root:[  247] Training loss: 0.07383407, Validation loss: 0.07513838, Gradient norm: 1.65219451
INFO:root:[  248] Training loss: 0.07366083, Validation loss: 0.07401167, Gradient norm: 1.53018128
INFO:root:[  249] Training loss: 0.07359279, Validation loss: 0.07343902, Gradient norm: 1.37704963
INFO:root:[  250] Training loss: 0.07375574, Validation loss: 0.07388216, Gradient norm: 1.31494520
INFO:root:[  251] Training loss: 0.07413436, Validation loss: 0.07341617, Gradient norm: 2.03780835
INFO:root:[  252] Training loss: 0.07331290, Validation loss: 0.07340901, Gradient norm: 1.56494443
INFO:root:[  253] Training loss: 0.07311956, Validation loss: 0.07274311, Gradient norm: 0.98035372
INFO:root:[  254] Training loss: 0.07361767, Validation loss: 0.07218498, Gradient norm: 1.45850887
INFO:root:[  255] Training loss: 0.07323821, Validation loss: 0.07306372, Gradient norm: 1.62766106
INFO:root:[  256] Training loss: 0.07389737, Validation loss: 0.07479378, Gradient norm: 1.11089094
INFO:root:[  257] Training loss: 0.07392884, Validation loss: 0.07258822, Gradient norm: 1.96697266
INFO:root:[  258] Training loss: 0.07273396, Validation loss: 0.07322971, Gradient norm: 1.15049241
INFO:root:[  259] Training loss: 0.07270981, Validation loss: 0.07219163, Gradient norm: 1.62910143
INFO:root:[  260] Training loss: 0.07243559, Validation loss: 0.07326934, Gradient norm: 1.05855409
INFO:root:[  261] Training loss: 0.07266668, Validation loss: 0.07206291, Gradient norm: 1.57111774
INFO:root:[  262] Training loss: 0.07289695, Validation loss: 0.07384960, Gradient norm: 1.76422035
INFO:root:[  263] Training loss: 0.07230807, Validation loss: 0.07266167, Gradient norm: 1.47467892
INFO:root:[  264] Training loss: 0.07239510, Validation loss: 0.07223035, Gradient norm: 1.49348745
INFO:root:[  265] Training loss: 0.07256398, Validation loss: 0.07315842, Gradient norm: 0.90140951
INFO:root:[  266] Training loss: 0.07312225, Validation loss: 0.07204680, Gradient norm: 1.68168582
INFO:root:[  267] Training loss: 0.07212799, Validation loss: 0.07208988, Gradient norm: 1.55627612
INFO:root:[  268] Training loss: 0.07178824, Validation loss: 0.07186218, Gradient norm: 1.32159757
INFO:root:[  269] Training loss: 0.07180665, Validation loss: 0.07198959, Gradient norm: 1.17509814
INFO:root:[  270] Training loss: 0.07218583, Validation loss: 0.07154828, Gradient norm: 1.24316414
INFO:root:[  271] Training loss: 0.07211989, Validation loss: 0.07199693, Gradient norm: 1.57148907
INFO:root:[  272] Training loss: 0.07188287, Validation loss: 0.07152966, Gradient norm: 1.65715713
INFO:root:[  273] Training loss: 0.07150279, Validation loss: 0.07357196, Gradient norm: 1.47074022
INFO:root:[  274] Training loss: 0.07172195, Validation loss: 0.07130845, Gradient norm: 1.68798274
INFO:root:[  275] Training loss: 0.07189293, Validation loss: 0.07193348, Gradient norm: 1.75444701
INFO:root:[  276] Training loss: 0.07215539, Validation loss: 0.07180760, Gradient norm: 1.58539616
INFO:root:[  277] Training loss: 0.07264938, Validation loss: 0.07266086, Gradient norm: 1.88602718
INFO:root:[  278] Training loss: 0.07165743, Validation loss: 0.07076760, Gradient norm: 1.77424985
INFO:root:[  279] Training loss: 0.07097792, Validation loss: 0.07142945, Gradient norm: 1.24028948
INFO:root:[  280] Training loss: 0.07113885, Validation loss: 0.07142505, Gradient norm: 1.40915524
INFO:root:[  281] Training loss: 0.07117201, Validation loss: 0.07135899, Gradient norm: 1.67682565
INFO:root:[  282] Training loss: 0.07101374, Validation loss: 0.07145160, Gradient norm: 1.45711569
INFO:root:[  283] Training loss: 0.07120020, Validation loss: 0.07140056, Gradient norm: 1.39086268
INFO:root:[  284] Training loss: 0.07204653, Validation loss: 0.07189209, Gradient norm: 1.84939111
INFO:root:[  285] Training loss: 0.07160710, Validation loss: 0.07099950, Gradient norm: 1.83113160
INFO:root:[  286] Training loss: 0.07084010, Validation loss: 0.07085913, Gradient norm: 1.36339493
INFO:root:[  287] Training loss: 0.07025699, Validation loss: 0.07106106, Gradient norm: 1.10475032
INFO:root:[  288] Training loss: 0.07047825, Validation loss: 0.07016615, Gradient norm: 1.13638877
INFO:root:[  289] Training loss: 0.07008080, Validation loss: 0.07010743, Gradient norm: 1.19444745
INFO:root:[  290] Training loss: 0.07009237, Validation loss: 0.06996172, Gradient norm: 1.08264507
INFO:root:[  291] Training loss: 0.07028230, Validation loss: 0.07103457, Gradient norm: 1.08759041
INFO:root:[  292] Training loss: 0.07153586, Validation loss: 0.07375150, Gradient norm: 1.32624573
INFO:root:[  293] Training loss: 0.07155964, Validation loss: 0.07080396, Gradient norm: 1.77686035
INFO:root:[  294] Training loss: 0.06984908, Validation loss: 0.06954263, Gradient norm: 1.10491367
INFO:root:[  295] Training loss: 0.06982943, Validation loss: 0.06985682, Gradient norm: 0.91048144
INFO:root:[  296] Training loss: 0.07021280, Validation loss: 0.07005786, Gradient norm: 1.56740181
INFO:root:[  297] Training loss: 0.06952721, Validation loss: 0.07073806, Gradient norm: 0.97588880
INFO:root:[  298] Training loss: 0.07053247, Validation loss: 0.07106299, Gradient norm: 1.12010525
INFO:root:[  299] Training loss: 0.06968192, Validation loss: 0.06992281, Gradient norm: 1.49624393
INFO:root:[  300] Training loss: 0.06974148, Validation loss: 0.07054519, Gradient norm: 1.16447092
INFO:root:[  301] Training loss: 0.06948511, Validation loss: 0.07056119, Gradient norm: 1.36262493
INFO:root:[  302] Training loss: 0.06981553, Validation loss: 0.07050855, Gradient norm: 1.14216300
INFO:root:[  303] Training loss: 0.06971820, Validation loss: 0.06905440, Gradient norm: 1.25969643
INFO:root:[  304] Training loss: 0.06941665, Validation loss: 0.06889051, Gradient norm: 1.37615697
INFO:root:[  305] Training loss: 0.06921891, Validation loss: 0.06910990, Gradient norm: 1.33505920
INFO:root:[  306] Training loss: 0.06925676, Validation loss: 0.06982291, Gradient norm: 1.29698176
INFO:root:[  307] Training loss: 0.06920420, Validation loss: 0.06855469, Gradient norm: 1.70264102
INFO:root:[  308] Training loss: 0.06889720, Validation loss: 0.06967288, Gradient norm: 1.35293644
INFO:root:[  309] Training loss: 0.06931285, Validation loss: 0.06908523, Gradient norm: 1.12556869
INFO:root:[  310] Training loss: 0.07009683, Validation loss: 0.06961334, Gradient norm: 1.64231699
INFO:root:[  311] Training loss: 0.06939220, Validation loss: 0.06943401, Gradient norm: 1.61743273
INFO:root:[  312] Training loss: 0.06848775, Validation loss: 0.06844711, Gradient norm: 1.29188541
INFO:root:[  313] Training loss: 0.06864489, Validation loss: 0.06877061, Gradient norm: 1.06684149
INFO:root:[  314] Training loss: 0.06913061, Validation loss: 0.06929930, Gradient norm: 1.07364509
INFO:root:[  315] Training loss: 0.06912226, Validation loss: 0.06827848, Gradient norm: 1.70147975
INFO:root:[  316] Training loss: 0.06850599, Validation loss: 0.06855853, Gradient norm: 1.52602774
INFO:root:[  317] Training loss: 0.06847354, Validation loss: 0.06962597, Gradient norm: 0.94762325
INFO:root:[  318] Training loss: 0.06872193, Validation loss: 0.06855419, Gradient norm: 1.43649598
INFO:root:[  319] Training loss: 0.06836077, Validation loss: 0.06841533, Gradient norm: 1.58912854
INFO:root:[  320] Training loss: 0.06824033, Validation loss: 0.06795873, Gradient norm: 1.47240220
INFO:root:[  321] Training loss: 0.06806018, Validation loss: 0.06811936, Gradient norm: 1.40267537
INFO:root:[  322] Training loss: 0.06855039, Validation loss: 0.06859934, Gradient norm: 1.16896882
INFO:root:[  323] Training loss: 0.06896058, Validation loss: 0.06837164, Gradient norm: 1.49189888
INFO:root:[  324] Training loss: 0.06824252, Validation loss: 0.06761374, Gradient norm: 1.65904065
INFO:root:[  325] Training loss: 0.06757961, Validation loss: 0.06873620, Gradient norm: 1.23358502
INFO:root:[  326] Training loss: 0.06829117, Validation loss: 0.06952702, Gradient norm: 1.00580565
INFO:root:[  327] Training loss: 0.06879957, Validation loss: 0.06880936, Gradient norm: 1.41905535
INFO:root:[  328] Training loss: 0.06783599, Validation loss: 0.06767151, Gradient norm: 1.45243811
INFO:root:[  329] Training loss: 0.06810278, Validation loss: 0.06834248, Gradient norm: 1.16620812
INFO:root:[  330] Training loss: 0.06800151, Validation loss: 0.06811581, Gradient norm: 1.48910443
INFO:root:[  331] Training loss: 0.06775499, Validation loss: 0.06746938, Gradient norm: 1.54567569
INFO:root:[  332] Training loss: 0.06705584, Validation loss: 0.06746240, Gradient norm: 1.11457056
INFO:root:[  333] Training loss: 0.06741603, Validation loss: 0.06694893, Gradient norm: 1.24542409
INFO:root:[  334] Training loss: 0.06727833, Validation loss: 0.06773669, Gradient norm: 1.38157556
INFO:root:[  335] Training loss: 0.06750170, Validation loss: 0.06785729, Gradient norm: 1.44180404
INFO:root:[  336] Training loss: 0.06836881, Validation loss: 0.06805565, Gradient norm: 1.83773800
INFO:root:[  337] Training loss: 0.06768098, Validation loss: 0.06793592, Gradient norm: 1.72002602
INFO:root:[  338] Training loss: 0.06684976, Validation loss: 0.06692603, Gradient norm: 1.53106206
INFO:root:[  339] Training loss: 0.06683146, Validation loss: 0.06673727, Gradient norm: 1.27924583
INFO:root:[  340] Training loss: 0.06713438, Validation loss: 0.06718357, Gradient norm: 1.10848890
INFO:root:[  341] Training loss: 0.06709287, Validation loss: 0.06783234, Gradient norm: 1.41623608
INFO:root:[  342] Training loss: 0.06674414, Validation loss: 0.06742500, Gradient norm: 1.55461314
INFO:root:[  343] Training loss: 0.06674098, Validation loss: 0.06693647, Gradient norm: 1.32829042
INFO:root:[  344] Training loss: 0.06669073, Validation loss: 0.06663924, Gradient norm: 0.88980887
INFO:root:[  345] Training loss: 0.06720801, Validation loss: 0.06804119, Gradient norm: 0.95746498
INFO:root:[  346] Training loss: 0.06735705, Validation loss: 0.06714509, Gradient norm: 1.25582185
INFO:root:[  347] Training loss: 0.06646237, Validation loss: 0.06626740, Gradient norm: 1.44673884
INFO:root:[  348] Training loss: 0.06618559, Validation loss: 0.06666529, Gradient norm: 1.35284207
INFO:root:[  349] Training loss: 0.06628803, Validation loss: 0.06648936, Gradient norm: 1.35623648
INFO:root:[  350] Training loss: 0.06656972, Validation loss: 0.06673261, Gradient norm: 1.16101545
INFO:root:[  351] Training loss: 0.06725911, Validation loss: 0.06648705, Gradient norm: 1.68324519
INFO:root:[  352] Training loss: 0.06634708, Validation loss: 0.06556685, Gradient norm: 1.76959401
INFO:root:[  353] Training loss: 0.06589606, Validation loss: 0.06548631, Gradient norm: 1.16241830
INFO:root:[  354] Training loss: 0.06627817, Validation loss: 0.06720317, Gradient norm: 1.10945462
INFO:root:[  355] Training loss: 0.06683021, Validation loss: 0.06695181, Gradient norm: 1.75350833
INFO:root:[  356] Training loss: 0.06597121, Validation loss: 0.06557730, Gradient norm: 1.32246495
INFO:root:[  357] Training loss: 0.06607227, Validation loss: 0.06583908, Gradient norm: 1.46645347
INFO:root:[  358] Training loss: 0.06555525, Validation loss: 0.06641900, Gradient norm: 1.26148683
INFO:root:[  359] Training loss: 0.06564526, Validation loss: 0.06580340, Gradient norm: 1.16309606
INFO:root:[  360] Training loss: 0.06558531, Validation loss: 0.06558164, Gradient norm: 1.21752950
INFO:root:[  361] Training loss: 0.06562206, Validation loss: 0.06491235, Gradient norm: 1.55124664
INFO:root:[  362] Training loss: 0.06522603, Validation loss: 0.06499624, Gradient norm: 1.20337805
INFO:root:[  363] Training loss: 0.06548121, Validation loss: 0.06708745, Gradient norm: 0.98857194
INFO:root:[  364] Training loss: 0.06595718, Validation loss: 0.06606233, Gradient norm: 1.20347898
INFO:root:[  365] Training loss: 0.06547011, Validation loss: 0.06533790, Gradient norm: 1.25040856
INFO:root:[  366] Training loss: 0.06519614, Validation loss: 0.06596909, Gradient norm: 1.08001491
INFO:root:[  367] Training loss: 0.06522966, Validation loss: 0.06568633, Gradient norm: 1.61721032
INFO:root:[  368] Training loss: 0.06537494, Validation loss: 0.06484312, Gradient norm: 1.53911766
INFO:root:[  369] Training loss: 0.06534538, Validation loss: 0.06566492, Gradient norm: 1.23660751
INFO:root:[  370] Training loss: 0.06534911, Validation loss: 0.06532658, Gradient norm: 1.71835711
INFO:root:[  371] Training loss: 0.06521334, Validation loss: 0.06459155, Gradient norm: 1.65269223
INFO:root:[  372] Training loss: 0.06443783, Validation loss: 0.06442772, Gradient norm: 1.16451638
INFO:root:[  373] Training loss: 0.06442859, Validation loss: 0.06502033, Gradient norm: 1.57886751
INFO:root:[  374] Training loss: 0.06505114, Validation loss: 0.06639311, Gradient norm: 1.68010705
INFO:root:[  375] Training loss: 0.06482109, Validation loss: 0.06470599, Gradient norm: 1.38579005
INFO:root:[  376] Training loss: 0.06444358, Validation loss: 0.06434864, Gradient norm: 1.18648651
INFO:root:[  377] Training loss: 0.06458416, Validation loss: 0.06530030, Gradient norm: 1.46081021
INFO:root:[  378] Training loss: 0.06528508, Validation loss: 0.06566980, Gradient norm: 1.51143655
INFO:root:[  379] Training loss: 0.06489237, Validation loss: 0.06493626, Gradient norm: 1.28977038
INFO:root:[  380] Training loss: 0.06436676, Validation loss: 0.06402765, Gradient norm: 1.45300178
INFO:root:[  381] Training loss: 0.06426765, Validation loss: 0.06534031, Gradient norm: 0.90745643
INFO:root:[  382] Training loss: 0.06472648, Validation loss: 0.06491172, Gradient norm: 1.26954411
INFO:root:[  383] Training loss: 0.06453124, Validation loss: 0.06469431, Gradient norm: 1.10746742
INFO:root:[  384] Training loss: 0.06426171, Validation loss: 0.06427486, Gradient norm: 1.00634693
INFO:root:[  385] Training loss: 0.06395691, Validation loss: 0.06372665, Gradient norm: 1.26030042
INFO:root:[  386] Training loss: 0.06390934, Validation loss: 0.06423797, Gradient norm: 1.60900724
INFO:root:[  387] Training loss: 0.06369717, Validation loss: 0.06400369, Gradient norm: 1.53065954
INFO:root:[  388] Training loss: 0.06343705, Validation loss: 0.06362883, Gradient norm: 1.20614076
INFO:root:[  389] Training loss: 0.06361029, Validation loss: 0.06357180, Gradient norm: 1.33787092
INFO:root:[  390] Training loss: 0.06347342, Validation loss: 0.06412047, Gradient norm: 1.01493761
INFO:root:[  391] Training loss: 0.06356185, Validation loss: 0.06339277, Gradient norm: 1.22171413
INFO:root:[  392] Training loss: 0.06355425, Validation loss: 0.06463521, Gradient norm: 1.24806232
INFO:root:[  393] Training loss: 0.06458857, Validation loss: 0.06268747, Gradient norm: 1.32362878
INFO:root:[  394] Training loss: 0.06413478, Validation loss: 0.06447835, Gradient norm: 1.21404481
INFO:root:[  395] Training loss: 0.06350179, Validation loss: 0.06429647, Gradient norm: 1.12863860
INFO:root:[  396] Training loss: 0.06348097, Validation loss: 0.06341467, Gradient norm: 1.05867349
INFO:root:[  397] Training loss: 0.06347387, Validation loss: 0.06417309, Gradient norm: 1.20811259
INFO:root:[  398] Training loss: 0.06370117, Validation loss: 0.06368782, Gradient norm: 1.20278311
INFO:root:[  399] Training loss: 0.06360191, Validation loss: 0.06392396, Gradient norm: 1.14209812
INFO:root:[  400] Training loss: 0.06301792, Validation loss: 0.06314589, Gradient norm: 0.98009006
INFO:root:[  401] Training loss: 0.06306740, Validation loss: 0.06317033, Gradient norm: 1.44215722
INFO:root:[  402] Training loss: 0.06284844, Validation loss: 0.06336178, Gradient norm: 1.28577435
INFO:root:EP 402: Early stopping
INFO:root:Training the model took 6333.78s.
INFO:root:Emptying the cuda cache took 0.042s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.30917
INFO:root:EnergyScoreTrain: 0.87359
INFO:root:CoverageTrain: 0.8934
INFO:root:IntervalWidthTrain: 0.03993
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.25701
INFO:root:EnergyScoreValidation: 0.8385
INFO:root:CoverageValidation: 0.89323
INFO:root:IntervalWidthValidation: 0.04002
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.04008
INFO:root:EnergyScoreTest: 0.69433
INFO:root:CoverageTest: 0.89403
INFO:root:IntervalWidthTest: 0.03982
INFO:root:###16 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 731906048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.06099336, Validation loss: 0.58466076, Gradient norm: 8.57942182
INFO:root:[    2] Training loss: 0.55451829, Validation loss: 0.51372776, Gradient norm: 2.57557634
INFO:root:[    3] Training loss: 0.49901157, Validation loss: 0.48967791, Gradient norm: 2.35377048
INFO:root:[    4] Training loss: 0.46815793, Validation loss: 0.44709391, Gradient norm: 2.87192858
INFO:root:[    5] Training loss: 0.44763330, Validation loss: 0.43016438, Gradient norm: 2.38609356
INFO:root:[    6] Training loss: 0.43766852, Validation loss: 0.42133714, Gradient norm: 3.47909988
INFO:root:[    7] Training loss: 0.42297472, Validation loss: 0.41008622, Gradient norm: 3.46451754
INFO:root:[    8] Training loss: 0.39754300, Validation loss: 0.40376307, Gradient norm: 2.65525917
INFO:root:[    9] Training loss: 0.40537243, Validation loss: 0.39302124, Gradient norm: 3.98243038
INFO:root:[   10] Training loss: 0.38780701, Validation loss: 0.37146910, Gradient norm: 2.74304320
INFO:root:[   11] Training loss: 0.37522020, Validation loss: 0.36946484, Gradient norm: 2.63159759
INFO:root:[   12] Training loss: 0.36469823, Validation loss: 0.35936243, Gradient norm: 4.34395547
INFO:root:[   13] Training loss: 0.36486000, Validation loss: 0.35638459, Gradient norm: 3.14737626
INFO:root:[   14] Training loss: 0.35121220, Validation loss: 0.36043927, Gradient norm: 2.53279084
INFO:root:[   15] Training loss: 0.34402914, Validation loss: 0.34998658, Gradient norm: 3.44992636
INFO:root:[   16] Training loss: 0.33727846, Validation loss: 0.33832903, Gradient norm: 2.61821492
INFO:root:[   17] Training loss: 0.32999359, Validation loss: 0.33488376, Gradient norm: 2.65605596
INFO:root:[   18] Training loss: 0.32499496, Validation loss: 0.31750371, Gradient norm: 2.91692079
INFO:root:[   19] Training loss: 0.31589618, Validation loss: 0.31224629, Gradient norm: 3.35246898
INFO:root:[   20] Training loss: 0.31110234, Validation loss: 0.30347978, Gradient norm: 3.25962521
INFO:root:[   21] Training loss: 0.30419787, Validation loss: 0.31073092, Gradient norm: 2.71946613
INFO:root:[   22] Training loss: 0.30157944, Validation loss: 0.29493479, Gradient norm: 3.87079056
INFO:root:[   23] Training loss: 0.29993285, Validation loss: 0.29554892, Gradient norm: 3.23118824
INFO:root:[   24] Training loss: 0.29435843, Validation loss: 0.28552476, Gradient norm: 4.66859331
INFO:root:[   25] Training loss: 0.29074566, Validation loss: 0.27874630, Gradient norm: 3.53879801
INFO:root:[   26] Training loss: 0.28081411, Validation loss: 0.28735198, Gradient norm: 2.12236870
INFO:root:[   27] Training loss: 0.27945582, Validation loss: 0.27843678, Gradient norm: 3.48137848
INFO:root:[   28] Training loss: 0.27784997, Validation loss: 0.27566913, Gradient norm: 3.38740674
INFO:root:[   29] Training loss: 0.27104408, Validation loss: 0.27011761, Gradient norm: 3.44107827
INFO:root:[   30] Training loss: 0.26996572, Validation loss: 0.26254436, Gradient norm: 3.79797691
INFO:root:[   31] Training loss: 0.26712345, Validation loss: 0.26866388, Gradient norm: 3.04138383
INFO:root:[   32] Training loss: 0.25976959, Validation loss: 0.26423247, Gradient norm: 2.60417513
INFO:root:[   33] Training loss: 0.25783431, Validation loss: 0.25763256, Gradient norm: 2.26062096
INFO:root:[   34] Training loss: 0.25820246, Validation loss: 0.25248852, Gradient norm: 2.69166284
INFO:root:[   35] Training loss: 0.25293379, Validation loss: 0.25112919, Gradient norm: 2.09542053
INFO:root:[   36] Training loss: 0.25106116, Validation loss: 0.24847469, Gradient norm: 1.96222328
INFO:root:[   37] Training loss: 0.24758144, Validation loss: 0.24783543, Gradient norm: 2.88430928
INFO:root:[   38] Training loss: 0.24794610, Validation loss: 0.24157839, Gradient norm: 3.36428034
INFO:root:[   39] Training loss: 0.24337018, Validation loss: 0.23921092, Gradient norm: 2.78936570
INFO:root:[   40] Training loss: 0.24068694, Validation loss: 0.24284634, Gradient norm: 2.42489313
INFO:root:[   41] Training loss: 0.23855521, Validation loss: 0.24154781, Gradient norm: 2.76849426
INFO:root:[   42] Training loss: 0.23320505, Validation loss: 0.23600882, Gradient norm: 2.28875139
INFO:root:[   43] Training loss: 0.23225532, Validation loss: 0.23302846, Gradient norm: 2.09798320
INFO:root:[   44] Training loss: 0.23133175, Validation loss: 0.23094806, Gradient norm: 2.07623080
INFO:root:[   45] Training loss: 0.23038155, Validation loss: 0.22640138, Gradient norm: 2.98820857
INFO:root:[   46] Training loss: 0.22780426, Validation loss: 0.22624587, Gradient norm: 2.08625076
INFO:root:[   47] Training loss: 0.22397024, Validation loss: 0.22208101, Gradient norm: 2.31525542
INFO:root:[   48] Training loss: 0.22547256, Validation loss: 0.22557605, Gradient norm: 3.29185118
INFO:root:[   49] Training loss: 0.22364346, Validation loss: 0.21999399, Gradient norm: 2.38470078
INFO:root:[   50] Training loss: 0.21942685, Validation loss: 0.21808441, Gradient norm: 2.58757765
INFO:root:[   51] Training loss: 0.21889067, Validation loss: 0.21996800, Gradient norm: 2.04607720
INFO:root:[   52] Training loss: 0.21852957, Validation loss: 0.21654512, Gradient norm: 3.48172852
INFO:root:[   53] Training loss: 0.21585373, Validation loss: 0.21417056, Gradient norm: 2.17022673
INFO:root:[   54] Training loss: 0.21209749, Validation loss: 0.21061184, Gradient norm: 1.90370371
INFO:root:[   55] Training loss: 0.21120926, Validation loss: 0.21094596, Gradient norm: 1.74189905
INFO:root:[   56] Training loss: 0.21147009, Validation loss: 0.21122309, Gradient norm: 2.88484809
INFO:root:[   57] Training loss: 0.21272079, Validation loss: 0.20901680, Gradient norm: 4.01409675
INFO:root:[   58] Training loss: 0.20656564, Validation loss: 0.20618802, Gradient norm: 1.37329173
INFO:root:[   59] Training loss: 0.20696295, Validation loss: 0.20742662, Gradient norm: 1.27503020
INFO:root:[   60] Training loss: 0.20571166, Validation loss: 0.20199128, Gradient norm: 2.21108562
INFO:root:[   61] Training loss: 0.20412677, Validation loss: 0.20370051, Gradient norm: 1.74624354
INFO:root:[   62] Training loss: 0.20366794, Validation loss: 0.20258520, Gradient norm: 2.25896874
INFO:root:[   63] Training loss: 0.20221041, Validation loss: 0.20167893, Gradient norm: 2.19619691
INFO:root:[   64] Training loss: 0.20111561, Validation loss: 0.20059028, Gradient norm: 2.26100496
INFO:root:[   65] Training loss: 0.19947822, Validation loss: 0.19656882, Gradient norm: 2.25064903
INFO:root:[   66] Training loss: 0.19955027, Validation loss: 0.19586530, Gradient norm: 1.98633159
INFO:root:[   67] Training loss: 0.19749850, Validation loss: 0.19671984, Gradient norm: 1.90951996
INFO:root:[   68] Training loss: 0.19561684, Validation loss: 0.19441768, Gradient norm: 2.17575184
INFO:root:[   69] Training loss: 0.19468271, Validation loss: 0.19604336, Gradient norm: 1.81225714
INFO:root:[   70] Training loss: 0.19513540, Validation loss: 0.19704064, Gradient norm: 2.62815430
INFO:root:[   71] Training loss: 0.19461214, Validation loss: 0.19402871, Gradient norm: 2.78642720
INFO:root:[   72] Training loss: 0.19301934, Validation loss: 0.18979409, Gradient norm: 2.18209928
INFO:root:[   73] Training loss: 0.19420421, Validation loss: 0.19242506, Gradient norm: 3.01943795
INFO:root:[   74] Training loss: 0.19175867, Validation loss: 0.19064830, Gradient norm: 2.72498411
INFO:root:[   75] Training loss: 0.19150294, Validation loss: 0.18854001, Gradient norm: 2.95922589
INFO:root:[   76] Training loss: 0.19144716, Validation loss: 0.19101804, Gradient norm: 2.84579092
INFO:root:[   77] Training loss: 0.18780256, Validation loss: 0.18998734, Gradient norm: 2.29634782
INFO:root:[   78] Training loss: 0.18814026, Validation loss: 0.18890352, Gradient norm: 2.73871109
INFO:root:[   79] Training loss: 0.18681116, Validation loss: 0.18752251, Gradient norm: 1.57800579
INFO:root:[   80] Training loss: 0.18690402, Validation loss: 0.19062031, Gradient norm: 2.08353989
INFO:root:[   81] Training loss: 0.18576293, Validation loss: 0.18675580, Gradient norm: 2.31173437
INFO:root:[   82] Training loss: 0.18581207, Validation loss: 0.18360053, Gradient norm: 1.81468007
INFO:root:[   83] Training loss: 0.18361407, Validation loss: 0.18254365, Gradient norm: 1.83476303
INFO:root:[   84] Training loss: 0.18245049, Validation loss: 0.18356733, Gradient norm: 1.03948599
INFO:root:[   85] Training loss: 0.18170460, Validation loss: 0.17951726, Gradient norm: 0.98218882
INFO:root:[   86] Training loss: 0.18290436, Validation loss: 0.18801346, Gradient norm: 2.98274724
INFO:root:[   87] Training loss: 0.18305864, Validation loss: 0.18292910, Gradient norm: 3.18733240
INFO:root:[   88] Training loss: 0.18029695, Validation loss: 0.18188850, Gradient norm: 1.75942868
INFO:root:[   89] Training loss: 0.18032324, Validation loss: 0.17820703, Gradient norm: 2.71784097
INFO:root:[   90] Training loss: 0.17825620, Validation loss: 0.17785501, Gradient norm: 1.23015827
INFO:root:[   91] Training loss: 0.17821862, Validation loss: 0.17633968, Gradient norm: 1.64702053
INFO:root:[   92] Training loss: 0.17690716, Validation loss: 0.17574005, Gradient norm: 2.23412177
INFO:root:[   93] Training loss: 0.17605837, Validation loss: 0.17463469, Gradient norm: 2.07162043
INFO:root:[   94] Training loss: 0.17591989, Validation loss: 0.17876772, Gradient norm: 1.83883519
INFO:root:[   95] Training loss: 0.17697985, Validation loss: 0.17581308, Gradient norm: 2.62307682
INFO:root:[   96] Training loss: 0.17496477, Validation loss: 0.17472030, Gradient norm: 1.53930687
INFO:root:[   97] Training loss: 0.17400782, Validation loss: 0.17278113, Gradient norm: 2.09270499
INFO:root:[   98] Training loss: 0.17418129, Validation loss: 0.17286371, Gradient norm: 1.78924739
INFO:root:[   99] Training loss: 0.17219674, Validation loss: 0.17355150, Gradient norm: 1.09549840
INFO:root:[  100] Training loss: 0.17304044, Validation loss: 0.17434520, Gradient norm: 2.37108427
INFO:root:[  101] Training loss: 0.17236382, Validation loss: 0.17170012, Gradient norm: 2.98888949
INFO:root:[  102] Training loss: 0.17118638, Validation loss: 0.17048396, Gradient norm: 2.15141562
INFO:root:[  103] Training loss: 0.17089487, Validation loss: 0.17187715, Gradient norm: 2.39774667
INFO:root:[  104] Training loss: 0.17033477, Validation loss: 0.16881857, Gradient norm: 2.55362894
INFO:root:[  105] Training loss: 0.16984945, Validation loss: 0.17071666, Gradient norm: 2.10092592
INFO:root:[  106] Training loss: 0.16772777, Validation loss: 0.17015041, Gradient norm: 1.64557904
INFO:root:[  107] Training loss: 0.16753500, Validation loss: 0.16697830, Gradient norm: 1.38396485
INFO:root:[  108] Training loss: 0.16816730, Validation loss: 0.16715429, Gradient norm: 1.92568787
INFO:root:[  109] Training loss: 0.16690736, Validation loss: 0.16723376, Gradient norm: 1.58358180
INFO:root:[  110] Training loss: 0.16610073, Validation loss: 0.16768045, Gradient norm: 1.12758912
INFO:root:[  111] Training loss: 0.16460938, Validation loss: 0.16383138, Gradient norm: 1.25833045
INFO:root:[  112] Training loss: 0.16478486, Validation loss: 0.16375961, Gradient norm: 1.35289759
INFO:root:[  113] Training loss: 0.16279711, Validation loss: 0.16450926, Gradient norm: 1.32692449
INFO:root:[  114] Training loss: 0.16452751, Validation loss: 0.16367848, Gradient norm: 2.36599306
INFO:root:[  115] Training loss: 0.16283497, Validation loss: 0.16367057, Gradient norm: 2.34792923
INFO:root:[  116] Training loss: 0.16179281, Validation loss: 0.16293005, Gradient norm: 1.55495931
INFO:root:[  117] Training loss: 0.16327985, Validation loss: 0.16218333, Gradient norm: 2.17961740
INFO:root:[  118] Training loss: 0.16236506, Validation loss: 0.16101331, Gradient norm: 2.25760715
INFO:root:[  119] Training loss: 0.16089604, Validation loss: 0.15989689, Gradient norm: 2.18328102
INFO:root:[  120] Training loss: 0.16054074, Validation loss: 0.16066459, Gradient norm: 1.61973874
INFO:root:[  121] Training loss: 0.15946070, Validation loss: 0.16102465, Gradient norm: 1.25916344
INFO:root:[  122] Training loss: 0.15969176, Validation loss: 0.15994852, Gradient norm: 1.70536047
INFO:root:[  123] Training loss: 0.15843582, Validation loss: 0.15989429, Gradient norm: 1.37628937
INFO:root:[  124] Training loss: 0.15868364, Validation loss: 0.15755533, Gradient norm: 1.87803809
INFO:root:[  125] Training loss: 0.15786352, Validation loss: 0.16137511, Gradient norm: 2.20240191
INFO:root:[  126] Training loss: 0.15824594, Validation loss: 0.15714642, Gradient norm: 2.50927192
INFO:root:[  127] Training loss: 0.15706393, Validation loss: 0.15640441, Gradient norm: 1.87691181
INFO:root:[  128] Training loss: 0.15556336, Validation loss: 0.15760996, Gradient norm: 1.54929536
INFO:root:[  129] Training loss: 0.15600313, Validation loss: 0.15404361, Gradient norm: 2.38476368
INFO:root:[  130] Training loss: 0.15578020, Validation loss: 0.15550299, Gradient norm: 1.42963856
INFO:root:[  131] Training loss: 0.15511801, Validation loss: 0.15335486, Gradient norm: 1.94032624
INFO:root:[  132] Training loss: 0.15443920, Validation loss: 0.15409237, Gradient norm: 1.76664304
INFO:root:[  133] Training loss: 0.15308736, Validation loss: 0.15321908, Gradient norm: 0.84191409
INFO:root:[  134] Training loss: 0.15295427, Validation loss: 0.15318944, Gradient norm: 0.88573614
INFO:root:[  135] Training loss: 0.15356938, Validation loss: 0.15298771, Gradient norm: 1.23232327
INFO:root:[  136] Training loss: 0.15211580, Validation loss: 0.15193234, Gradient norm: 1.37545654
INFO:root:[  137] Training loss: 0.15147118, Validation loss: 0.15219379, Gradient norm: 1.36341569
INFO:root:[  138] Training loss: 0.15144260, Validation loss: 0.15015378, Gradient norm: 1.52785423
INFO:root:[  139] Training loss: 0.15109733, Validation loss: 0.15152735, Gradient norm: 1.99221710
INFO:root:[  140] Training loss: 0.15067298, Validation loss: 0.15035560, Gradient norm: 1.26199586
INFO:root:[  141] Training loss: 0.14979780, Validation loss: 0.15154060, Gradient norm: 1.42811354
INFO:root:[  142] Training loss: 0.15100936, Validation loss: 0.14971493, Gradient norm: 2.24678418
INFO:root:[  143] Training loss: 0.15011672, Validation loss: 0.14993133, Gradient norm: 2.10255458
INFO:root:[  144] Training loss: 0.14887273, Validation loss: 0.14886135, Gradient norm: 1.45147339
INFO:root:[  145] Training loss: 0.14883418, Validation loss: 0.14894407, Gradient norm: 1.70009430
INFO:root:[  146] Training loss: 0.14815550, Validation loss: 0.14817930, Gradient norm: 1.90675180
INFO:root:[  147] Training loss: 0.14834097, Validation loss: 0.14809874, Gradient norm: 1.62811710
INFO:root:[  148] Training loss: 0.14813729, Validation loss: 0.14767599, Gradient norm: 2.18328953
INFO:root:[  149] Training loss: 0.14746630, Validation loss: 0.14741895, Gradient norm: 2.18109146
INFO:root:[  150] Training loss: 0.14697578, Validation loss: 0.14607806, Gradient norm: 1.73989464
INFO:root:[  151] Training loss: 0.14644376, Validation loss: 0.14565097, Gradient norm: 1.58532568
INFO:root:[  152] Training loss: 0.14569843, Validation loss: 0.14661395, Gradient norm: 1.79616142
INFO:root:[  153] Training loss: 0.14578760, Validation loss: 0.14407782, Gradient norm: 1.37660948
INFO:root:[  154] Training loss: 0.14492085, Validation loss: 0.14324488, Gradient norm: 1.41149873
INFO:root:[  155] Training loss: 0.14402115, Validation loss: 0.14348644, Gradient norm: 1.13305909
INFO:root:[  156] Training loss: 0.14438116, Validation loss: 0.14521258, Gradient norm: 1.86199717
INFO:root:[  157] Training loss: 0.14415373, Validation loss: 0.14298688, Gradient norm: 2.06045804
INFO:root:[  158] Training loss: 0.14300283, Validation loss: 0.14337213, Gradient norm: 1.66820490
INFO:root:[  159] Training loss: 0.14286447, Validation loss: 0.14333665, Gradient norm: 1.38758921
INFO:root:[  160] Training loss: 0.14326777, Validation loss: 0.14366123, Gradient norm: 1.88422686
INFO:root:[  161] Training loss: 0.14177740, Validation loss: 0.14367610, Gradient norm: 1.61818877
INFO:root:[  162] Training loss: 0.14245723, Validation loss: 0.14069450, Gradient norm: 1.98979539
INFO:root:[  163] Training loss: 0.14181818, Validation loss: 0.14193898, Gradient norm: 1.87982032
INFO:root:[  164] Training loss: 0.14065476, Validation loss: 0.14140610, Gradient norm: 1.20721020
INFO:root:[  165] Training loss: 0.14084965, Validation loss: 0.14192282, Gradient norm: 1.21114303
INFO:root:[  166] Training loss: 0.14011099, Validation loss: 0.13971970, Gradient norm: 1.23017900
INFO:root:[  167] Training loss: 0.14018189, Validation loss: 0.14056248, Gradient norm: 1.01938686
INFO:root:[  168] Training loss: 0.13982502, Validation loss: 0.13980318, Gradient norm: 1.05839573
INFO:root:[  169] Training loss: 0.13912367, Validation loss: 0.13904667, Gradient norm: 1.15448614
INFO:root:[  170] Training loss: 0.13901547, Validation loss: 0.13816652, Gradient norm: 1.85059433
INFO:root:[  171] Training loss: 0.13930213, Validation loss: 0.13859998, Gradient norm: 2.59659377
INFO:root:[  172] Training loss: 0.13768613, Validation loss: 0.13833038, Gradient norm: 1.24236296
INFO:root:[  173] Training loss: 0.13814722, Validation loss: 0.13864121, Gradient norm: 1.81253801
INFO:root:[  174] Training loss: 0.13770194, Validation loss: 0.13698341, Gradient norm: 1.63414852
INFO:root:[  175] Training loss: 0.13631868, Validation loss: 0.13840295, Gradient norm: 0.87402697
INFO:root:[  176] Training loss: 0.13672873, Validation loss: 0.13709541, Gradient norm: 1.76094361
INFO:root:[  177] Training loss: 0.13558843, Validation loss: 0.13579549, Gradient norm: 1.51467584
INFO:root:[  178] Training loss: 0.13611127, Validation loss: 0.13619420, Gradient norm: 1.46876777
INFO:root:[  179] Training loss: 0.13577086, Validation loss: 0.13570759, Gradient norm: 1.18441459
INFO:root:[  180] Training loss: 0.13510130, Validation loss: 0.13489274, Gradient norm: 1.90289712
INFO:root:[  181] Training loss: 0.13530714, Validation loss: 0.13446638, Gradient norm: 1.74599440
INFO:root:[  182] Training loss: 0.13492902, Validation loss: 0.13554988, Gradient norm: 1.86442109
INFO:root:[  183] Training loss: 0.13433986, Validation loss: 0.13511312, Gradient norm: 1.50727257
INFO:root:[  184] Training loss: 0.13413669, Validation loss: 0.13405191, Gradient norm: 1.38999324
INFO:root:[  185] Training loss: 0.13312363, Validation loss: 0.13257136, Gradient norm: 1.27795175
INFO:root:[  186] Training loss: 0.13356968, Validation loss: 0.13321713, Gradient norm: 1.52597671
INFO:root:[  187] Training loss: 0.13323050, Validation loss: 0.13334725, Gradient norm: 2.24177791
INFO:root:[  188] Training loss: 0.13292907, Validation loss: 0.13230385, Gradient norm: 2.12511165
INFO:root:[  189] Training loss: 0.13194792, Validation loss: 0.13214125, Gradient norm: 1.46416725
INFO:root:[  190] Training loss: 0.13181595, Validation loss: 0.13260835, Gradient norm: 1.03101394
INFO:root:[  191] Training loss: 0.13207747, Validation loss: 0.13195012, Gradient norm: 1.85492621
INFO:root:[  192] Training loss: 0.13128220, Validation loss: 0.13139529, Gradient norm: 1.89160971
INFO:root:[  193] Training loss: 0.13082111, Validation loss: 0.13147192, Gradient norm: 1.64113561
INFO:root:[  194] Training loss: 0.13134753, Validation loss: 0.13086370, Gradient norm: 1.70136354
INFO:root:[  195] Training loss: 0.13069616, Validation loss: 0.13025669, Gradient norm: 2.64497817
INFO:root:[  196] Training loss: 0.12988973, Validation loss: 0.13010504, Gradient norm: 1.02956668
INFO:root:[  197] Training loss: 0.12974942, Validation loss: 0.12906741, Gradient norm: 1.20580798
INFO:root:[  198] Training loss: 0.12932615, Validation loss: 0.13024266, Gradient norm: 1.51650506
INFO:root:[  199] Training loss: 0.12887423, Validation loss: 0.12826855, Gradient norm: 1.78067995
INFO:root:[  200] Training loss: 0.12821270, Validation loss: 0.12887530, Gradient norm: 1.60597329
INFO:root:[  201] Training loss: 0.12792353, Validation loss: 0.12855039, Gradient norm: 1.26832002
INFO:root:[  202] Training loss: 0.12744573, Validation loss: 0.12683829, Gradient norm: 0.82318783
INFO:root:[  203] Training loss: 0.12754256, Validation loss: 0.12787347, Gradient norm: 1.77827655
INFO:root:[  204] Training loss: 0.12783704, Validation loss: 0.12730433, Gradient norm: 2.13815256
INFO:root:[  205] Training loss: 0.12707890, Validation loss: 0.12663850, Gradient norm: 1.79363340
INFO:root:[  206] Training loss: 0.12657638, Validation loss: 0.12757590, Gradient norm: 1.73091135
INFO:root:[  207] Training loss: 0.12624739, Validation loss: 0.12547536, Gradient norm: 1.05536288
INFO:root:[  208] Training loss: 0.12545560, Validation loss: 0.12587369, Gradient norm: 1.63257134
INFO:root:[  209] Training loss: 0.12584318, Validation loss: 0.12556634, Gradient norm: 2.27434372
INFO:root:[  210] Training loss: 0.12586240, Validation loss: 0.12605272, Gradient norm: 2.24082683
INFO:root:[  211] Training loss: 0.12493463, Validation loss: 0.12450556, Gradient norm: 1.59316361
INFO:root:[  212] Training loss: 0.12449009, Validation loss: 0.12539680, Gradient norm: 1.60281463
INFO:root:[  213] Training loss: 0.12465965, Validation loss: 0.12429225, Gradient norm: 2.17520349
INFO:root:[  214] Training loss: 0.12446337, Validation loss: 0.12523825, Gradient norm: 2.02713970
INFO:root:[  215] Training loss: 0.12376461, Validation loss: 0.12466999, Gradient norm: 2.23955695
INFO:root:[  216] Training loss: 0.12386944, Validation loss: 0.12376033, Gradient norm: 1.78023432
INFO:root:[  217] Training loss: 0.12317327, Validation loss: 0.12309662, Gradient norm: 1.92128601
INFO:root:[  218] Training loss: 0.12310082, Validation loss: 0.12269764, Gradient norm: 2.32137316
INFO:root:[  219] Training loss: 0.12323069, Validation loss: 0.12365948, Gradient norm: 2.09560062
INFO:root:[  220] Training loss: 0.12339175, Validation loss: 0.12154862, Gradient norm: 3.05294138
INFO:root:[  221] Training loss: 0.12264551, Validation loss: 0.12389422, Gradient norm: 2.06406774
INFO:root:[  222] Training loss: 0.12189447, Validation loss: 0.12144562, Gradient norm: 2.32302760
INFO:root:[  223] Training loss: 0.12152575, Validation loss: 0.12205430, Gradient norm: 1.47095225
INFO:root:[  224] Training loss: 0.12176215, Validation loss: 0.12089039, Gradient norm: 2.57084319
INFO:root:[  225] Training loss: 0.12101010, Validation loss: 0.11992417, Gradient norm: 1.62483909
INFO:root:[  226] Training loss: 0.12078897, Validation loss: 0.12303608, Gradient norm: 1.79429824
INFO:root:[  227] Training loss: 0.12108987, Validation loss: 0.12125234, Gradient norm: 2.99201757
INFO:root:[  228] Training loss: 0.12096886, Validation loss: 0.12087360, Gradient norm: 1.95909839
INFO:root:[  229] Training loss: 0.12025297, Validation loss: 0.12057464, Gradient norm: 1.39893772
INFO:root:[  230] Training loss: 0.11972500, Validation loss: 0.12017557, Gradient norm: 2.03869162
INFO:root:[  231] Training loss: 0.11978678, Validation loss: 0.11854962, Gradient norm: 1.88771756
INFO:root:[  232] Training loss: 0.11924653, Validation loss: 0.12002940, Gradient norm: 2.36920073
INFO:root:[  233] Training loss: 0.11902594, Validation loss: 0.11937228, Gradient norm: 1.71333291
INFO:root:[  234] Training loss: 0.11920085, Validation loss: 0.11894744, Gradient norm: 2.65499820
INFO:root:[  235] Training loss: 0.11887347, Validation loss: 0.11885273, Gradient norm: 2.50959924
INFO:root:[  236] Training loss: 0.11819347, Validation loss: 0.11916677, Gradient norm: 1.57435127
INFO:root:[  237] Training loss: 0.11821049, Validation loss: 0.11806324, Gradient norm: 2.44722777
INFO:root:[  238] Training loss: 0.11779929, Validation loss: 0.11794605, Gradient norm: 1.93461810
INFO:root:[  239] Training loss: 0.11752847, Validation loss: 0.11668155, Gradient norm: 1.74531866
INFO:root:[  240] Training loss: 0.11677220, Validation loss: 0.11750567, Gradient norm: 1.42305067
INFO:root:[  241] Training loss: 0.11711130, Validation loss: 0.11561488, Gradient norm: 2.05638510
INFO:root:[  242] Training loss: 0.11618742, Validation loss: 0.11781470, Gradient norm: 1.45526710
INFO:root:[  243] Training loss: 0.11637980, Validation loss: 0.11699912, Gradient norm: 2.99567414
INFO:root:[  244] Training loss: 0.11652653, Validation loss: 0.11603921, Gradient norm: 1.55575138
INFO:root:[  245] Training loss: 0.11598458, Validation loss: 0.11619906, Gradient norm: 2.23144987
INFO:root:[  246] Training loss: 0.11540190, Validation loss: 0.11584519, Gradient norm: 2.00711290
INFO:root:[  247] Training loss: 0.11565866, Validation loss: 0.11605077, Gradient norm: 1.80620137
INFO:root:[  248] Training loss: 0.11507973, Validation loss: 0.11597129, Gradient norm: 2.67830557
INFO:root:[  249] Training loss: 0.11498880, Validation loss: 0.11691094, Gradient norm: 2.21341564
INFO:root:[  250] Training loss: 0.11477055, Validation loss: 0.11409116, Gradient norm: 2.52972799
INFO:root:[  251] Training loss: 0.11447815, Validation loss: 0.11558349, Gradient norm: 2.44779994
INFO:root:[  252] Training loss: 0.11437919, Validation loss: 0.11440106, Gradient norm: 3.22706924
INFO:root:[  253] Training loss: 0.11354945, Validation loss: 0.11392445, Gradient norm: 1.99659327
INFO:root:[  254] Training loss: 0.11329499, Validation loss: 0.11396384, Gradient norm: 1.48767501
INFO:root:[  255] Training loss: 0.11357124, Validation loss: 0.11242958, Gradient norm: 2.70989319
INFO:root:[  256] Training loss: 0.11325692, Validation loss: 0.11285114, Gradient norm: 2.02790052
INFO:root:[  257] Training loss: 0.11284102, Validation loss: 0.11310120, Gradient norm: 1.84422653
INFO:root:[  258] Training loss: 0.11277152, Validation loss: 0.11245856, Gradient norm: 2.11209760
INFO:root:[  259] Training loss: 0.11244020, Validation loss: 0.11179021, Gradient norm: 2.52653685
INFO:root:[  260] Training loss: 0.11171246, Validation loss: 0.11245937, Gradient norm: 1.69049870
INFO:root:[  261] Training loss: 0.11194609, Validation loss: 0.11222692, Gradient norm: 2.31660193
INFO:root:[  262] Training loss: 0.11122465, Validation loss: 0.11300670, Gradient norm: 1.53383462
INFO:root:[  263] Training loss: 0.11160277, Validation loss: 0.11287666, Gradient norm: 2.57253806
INFO:root:[  264] Training loss: 0.11088684, Validation loss: 0.11168447, Gradient norm: 2.30608773
INFO:root:[  265] Training loss: 0.11083669, Validation loss: 0.11084854, Gradient norm: 2.69277328
INFO:root:[  266] Training loss: 0.11046261, Validation loss: 0.11008809, Gradient norm: 2.39048936
INFO:root:[  267] Training loss: 0.11013197, Validation loss: 0.10983021, Gradient norm: 1.43981102
INFO:root:[  268] Training loss: 0.10959693, Validation loss: 0.11027833, Gradient norm: 1.77474090
INFO:root:[  269] Training loss: 0.10992617, Validation loss: 0.10839549, Gradient norm: 2.13599176
INFO:root:[  270] Training loss: 0.10949768, Validation loss: 0.10943439, Gradient norm: 2.59066133
INFO:root:[  271] Training loss: 0.10912271, Validation loss: 0.10990027, Gradient norm: 1.78120668
INFO:root:[  272] Training loss: 0.10887802, Validation loss: 0.10863020, Gradient norm: 1.22674610
INFO:root:[  273] Training loss: 0.10858090, Validation loss: 0.10890731, Gradient norm: 1.72631137
INFO:root:[  274] Training loss: 0.10839764, Validation loss: 0.10893362, Gradient norm: 2.02167707
INFO:root:[  275] Training loss: 0.10796656, Validation loss: 0.10728549, Gradient norm: 2.73177383
INFO:root:[  276] Training loss: 0.10765498, Validation loss: 0.10801069, Gradient norm: 2.58634629
INFO:root:[  277] Training loss: 0.10794333, Validation loss: 0.10837629, Gradient norm: 2.54137048
INFO:root:[  278] Training loss: 0.10750614, Validation loss: 0.10715197, Gradient norm: 2.54804534
INFO:root:[  279] Training loss: 0.10712047, Validation loss: 0.10713888, Gradient norm: 2.72216458
INFO:root:[  280] Training loss: 0.10699083, Validation loss: 0.10736184, Gradient norm: 2.86498231
INFO:root:[  281] Training loss: 0.10657492, Validation loss: 0.10705679, Gradient norm: 2.06512409
INFO:root:[  282] Training loss: 0.10621725, Validation loss: 0.10641294, Gradient norm: 1.95666170
INFO:root:[  283] Training loss: 0.10661441, Validation loss: 0.10599190, Gradient norm: 3.25812193
INFO:root:[  284] Training loss: 0.10582026, Validation loss: 0.10627610, Gradient norm: 2.06656064
INFO:root:[  285] Training loss: 0.10654309, Validation loss: 0.10722585, Gradient norm: 2.35454247
INFO:root:[  286] Training loss: 0.10583803, Validation loss: 0.10534232, Gradient norm: 3.05558025
INFO:root:[  287] Training loss: 0.10510825, Validation loss: 0.10530211, Gradient norm: 1.39058315
INFO:root:[  288] Training loss: 0.10508398, Validation loss: 0.10455228, Gradient norm: 2.41538111
INFO:root:[  289] Training loss: 0.10447076, Validation loss: 0.10449864, Gradient norm: 2.06652350
INFO:root:[  290] Training loss: 0.10445327, Validation loss: 0.10507442, Gradient norm: 2.30348423
INFO:root:[  291] Training loss: 0.10492567, Validation loss: 0.10470705, Gradient norm: 1.74228913
INFO:root:[  292] Training loss: 0.10438159, Validation loss: 0.10513283, Gradient norm: 1.65280914
INFO:root:[  293] Training loss: 0.10366363, Validation loss: 0.10421262, Gradient norm: 2.53899211
INFO:root:[  294] Training loss: 0.10345912, Validation loss: 0.10347373, Gradient norm: 2.94032201
INFO:root:[  295] Training loss: 0.10335767, Validation loss: 0.10343977, Gradient norm: 2.72614082
INFO:root:[  296] Training loss: 0.10262618, Validation loss: 0.10385182, Gradient norm: 2.57728882
INFO:root:[  297] Training loss: 0.10317403, Validation loss: 0.10342418, Gradient norm: 2.79327049
INFO:root:[  298] Training loss: 0.10307316, Validation loss: 0.10279853, Gradient norm: 3.01755803
INFO:root:[  299] Training loss: 0.10263279, Validation loss: 0.10185971, Gradient norm: 2.98278334
INFO:root:[  300] Training loss: 0.10212593, Validation loss: 0.10221358, Gradient norm: 2.93553376
INFO:root:[  301] Training loss: 0.10236033, Validation loss: 0.10163254, Gradient norm: 3.42218470
INFO:root:[  302] Training loss: 0.10185103, Validation loss: 0.10161500, Gradient norm: 3.02450345
INFO:root:[  303] Training loss: 0.10216758, Validation loss: 0.10183171, Gradient norm: 3.09184221
INFO:root:[  304] Training loss: 0.10193107, Validation loss: 0.10177498, Gradient norm: 3.39537204
INFO:root:[  305] Training loss: 0.10145250, Validation loss: 0.10134617, Gradient norm: 4.57259875
INFO:root:[  306] Training loss: 0.10064598, Validation loss: 0.10176102, Gradient norm: 2.40918612
INFO:root:[  307] Training loss: 0.10059607, Validation loss: 0.10104348, Gradient norm: 2.61790498
INFO:root:[  308] Training loss: 0.10066118, Validation loss: 0.10205195, Gradient norm: 2.59500818
INFO:root:[  309] Training loss: 0.10042824, Validation loss: 0.09962709, Gradient norm: 3.59625415
INFO:root:[  310] Training loss: 0.10030530, Validation loss: 0.09993511, Gradient norm: 2.18559160
INFO:root:[  311] Training loss: 0.09977336, Validation loss: 0.10025770, Gradient norm: 3.68700544
INFO:root:[  312] Training loss: 0.09962166, Validation loss: 0.09876429, Gradient norm: 3.08146181
INFO:root:[  313] Training loss: 0.09976234, Validation loss: 0.10008001, Gradient norm: 3.51251112
INFO:root:[  314] Training loss: 0.09936154, Validation loss: 0.09905444, Gradient norm: 3.26529718
INFO:root:[  315] Training loss: 0.09918203, Validation loss: 0.09857158, Gradient norm: 2.84767019
INFO:root:[  316] Training loss: 0.09925009, Validation loss: 0.09818988, Gradient norm: 2.86225566
INFO:root:[  317] Training loss: 0.09809020, Validation loss: 0.09873890, Gradient norm: 2.43425414
INFO:root:[  318] Training loss: 0.09822232, Validation loss: 0.09945622, Gradient norm: 2.95329095
INFO:root:[  319] Training loss: 0.09854947, Validation loss: 0.09961296, Gradient norm: 4.39614308
INFO:root:[  320] Training loss: 0.09833135, Validation loss: 0.09820659, Gradient norm: 4.21458810
INFO:root:[  321] Training loss: 0.09820218, Validation loss: 0.09813882, Gradient norm: 1.96147968
INFO:root:[  322] Training loss: 0.09807007, Validation loss: 0.09818254, Gradient norm: 2.99103134
INFO:root:[  323] Training loss: 0.09790513, Validation loss: 0.09731313, Gradient norm: 3.31266691
INFO:root:[  324] Training loss: 0.09737176, Validation loss: 0.09839749, Gradient norm: 4.43012392
INFO:root:[  325] Training loss: 0.09731925, Validation loss: 0.09729321, Gradient norm: 3.47732143
INFO:root:[  326] Training loss: 0.09680876, Validation loss: 0.09647627, Gradient norm: 3.59618401
INFO:root:[  327] Training loss: 0.09620359, Validation loss: 0.09761661, Gradient norm: 2.49974628
INFO:root:[  328] Training loss: 0.09652543, Validation loss: 0.09687146, Gradient norm: 2.34846182
INFO:root:[  329] Training loss: 0.09680469, Validation loss: 0.09743360, Gradient norm: 2.56276086
INFO:root:[  330] Training loss: 0.09632357, Validation loss: 0.09627889, Gradient norm: 4.01424243
INFO:root:[  331] Training loss: 0.09600612, Validation loss: 0.09641661, Gradient norm: 3.92844158
INFO:root:[  332] Training loss: 0.09640422, Validation loss: 0.09667818, Gradient norm: 4.91897133
INFO:root:[  333] Training loss: 0.09535954, Validation loss: 0.09569775, Gradient norm: 3.82545842
INFO:root:[  334] Training loss: 0.09558958, Validation loss: 0.09592474, Gradient norm: 3.52206199
INFO:root:[  335] Training loss: 0.09470485, Validation loss: 0.09691118, Gradient norm: 2.77024099
INFO:root:[  336] Training loss: 0.09477677, Validation loss: 0.09402026, Gradient norm: 3.49998587
INFO:root:[  337] Training loss: 0.09469931, Validation loss: 0.09639273, Gradient norm: 3.05946401
INFO:root:[  338] Training loss: 0.09461425, Validation loss: 0.09419682, Gradient norm: 2.08924711
INFO:root:[  339] Training loss: 0.09410249, Validation loss: 0.09459558, Gradient norm: 3.33923949
INFO:root:[  340] Training loss: 0.09384597, Validation loss: 0.09456235, Gradient norm: 2.73215703
INFO:root:[  341] Training loss: 0.09376914, Validation loss: 0.09318566, Gradient norm: 5.05040163
INFO:root:[  342] Training loss: 0.09349784, Validation loss: 0.09476703, Gradient norm: 5.09734841
INFO:root:[  343] Training loss: 0.09345057, Validation loss: 0.09318147, Gradient norm: 2.95860791
INFO:root:[  344] Training loss: 0.09325610, Validation loss: 0.09386910, Gradient norm: 4.85726194
INFO:root:[  345] Training loss: 0.09284155, Validation loss: 0.09341632, Gradient norm: 3.89683681
INFO:root:[  346] Training loss: 0.09299601, Validation loss: 0.09212956, Gradient norm: 3.77527468
INFO:root:[  347] Training loss: 0.09311889, Validation loss: 0.09353346, Gradient norm: 4.89726895
INFO:root:[  348] Training loss: 0.09249100, Validation loss: 0.09245218, Gradient norm: 4.25712160
INFO:root:[  349] Training loss: 0.09281905, Validation loss: 0.09336881, Gradient norm: 5.22940912
INFO:root:[  350] Training loss: 0.09249725, Validation loss: 0.09260483, Gradient norm: 4.76420798
INFO:root:[  351] Training loss: 0.09248378, Validation loss: 0.09210144, Gradient norm: 5.92106554
INFO:root:[  352] Training loss: 0.09190120, Validation loss: 0.09230408, Gradient norm: 3.37814981
INFO:root:[  353] Training loss: 0.09186484, Validation loss: 0.09247810, Gradient norm: 2.99691501
INFO:root:[  354] Training loss: 0.09188655, Validation loss: 0.09166899, Gradient norm: 4.70345455
INFO:root:[  355] Training loss: 0.09128782, Validation loss: 0.09302355, Gradient norm: 2.90065124
INFO:root:[  356] Training loss: 0.09139883, Validation loss: 0.09176551, Gradient norm: 2.54991006
INFO:root:[  357] Training loss: 0.09120448, Validation loss: 0.09138831, Gradient norm: 4.52623217
INFO:root:[  358] Training loss: 0.09096214, Validation loss: 0.09174701, Gradient norm: 3.95127407
INFO:root:[  359] Training loss: 0.09062911, Validation loss: 0.09089452, Gradient norm: 5.06015872
INFO:root:[  360] Training loss: 0.09015285, Validation loss: 0.09103734, Gradient norm: 2.76130653
INFO:root:[  361] Training loss: 0.09028626, Validation loss: 0.09033656, Gradient norm: 5.14067688
INFO:root:[  362] Training loss: 0.09041240, Validation loss: 0.09057504, Gradient norm: 7.60765085
INFO:root:[  363] Training loss: 0.08989940, Validation loss: 0.09080356, Gradient norm: 5.11542732
INFO:root:[  364] Training loss: 0.09027144, Validation loss: 0.09043882, Gradient norm: 6.22099864
INFO:root:[  365] Training loss: 0.08962321, Validation loss: 0.08960689, Gradient norm: 5.50293063
INFO:root:[  366] Training loss: 0.08986012, Validation loss: 0.08956938, Gradient norm: 5.34243752
INFO:root:[  367] Training loss: 0.08976633, Validation loss: 0.08923121, Gradient norm: 6.44048884
INFO:root:[  368] Training loss: 0.08934039, Validation loss: 0.09053055, Gradient norm: 3.83745693
INFO:root:[  369] Training loss: 0.08940416, Validation loss: 0.08976783, Gradient norm: 6.35783472
INFO:root:[  370] Training loss: 0.08878372, Validation loss: 0.08833115, Gradient norm: 5.92835553
INFO:root:[  371] Training loss: 0.08867725, Validation loss: 0.08969497, Gradient norm: 5.16360363
INFO:root:[  372] Training loss: 0.08824004, Validation loss: 0.08851121, Gradient norm: 3.91124045
INFO:root:[  373] Training loss: 0.08815847, Validation loss: 0.08800780, Gradient norm: 3.68337029
INFO:root:[  374] Training loss: 0.08847811, Validation loss: 0.08886972, Gradient norm: 6.79056324
INFO:root:[  375] Training loss: 0.08781459, Validation loss: 0.08925951, Gradient norm: 3.96711129
INFO:root:[  376] Training loss: 0.08846012, Validation loss: 0.08789338, Gradient norm: 7.04293006
INFO:root:[  377] Training loss: 0.08768064, Validation loss: 0.08844389, Gradient norm: 5.46895842
INFO:root:[  378] Training loss: 0.08752238, Validation loss: 0.08835328, Gradient norm: 4.95623122
INFO:root:[  379] Training loss: 0.08756528, Validation loss: 0.08825877, Gradient norm: 4.18205934
INFO:root:[  380] Training loss: 0.08696182, Validation loss: 0.08740859, Gradient norm: 5.45913019
INFO:root:[  381] Training loss: 0.08761476, Validation loss: 0.08704773, Gradient norm: 4.61424888
INFO:root:[  382] Training loss: 0.08717297, Validation loss: 0.08767982, Gradient norm: 6.81645586
INFO:root:[  383] Training loss: 0.08748669, Validation loss: 0.08797245, Gradient norm: 3.98115866
INFO:root:[  384] Training loss: 0.08736263, Validation loss: 0.08663410, Gradient norm: 2.83739222
INFO:root:[  385] Training loss: 0.08694140, Validation loss: 0.08753109, Gradient norm: 5.82782026
INFO:root:[  386] Training loss: 0.08631299, Validation loss: 0.08627062, Gradient norm: 3.86388144
INFO:root:[  387] Training loss: 0.08690616, Validation loss: 0.08689299, Gradient norm: 3.22333186
INFO:root:[  388] Training loss: 0.08602491, Validation loss: 0.08568620, Gradient norm: 5.47582362
INFO:root:[  389] Training loss: 0.08635922, Validation loss: 0.08581094, Gradient norm: 4.34514502
INFO:root:[  390] Training loss: 0.08543398, Validation loss: 0.08678366, Gradient norm: 3.85696699
INFO:root:[  391] Training loss: 0.08600103, Validation loss: 0.08712817, Gradient norm: 4.85152287
INFO:root:[  392] Training loss: 0.08563059, Validation loss: 0.08520464, Gradient norm: 5.82244108
INFO:root:[  393] Training loss: 0.08493094, Validation loss: 0.08561767, Gradient norm: 6.36598045
INFO:root:[  394] Training loss: 0.08488705, Validation loss: 0.08524196, Gradient norm: 5.73135836
INFO:root:[  395] Training loss: 0.08515812, Validation loss: 0.08483485, Gradient norm: 5.10139986
INFO:root:[  396] Training loss: 0.08460821, Validation loss: 0.08454462, Gradient norm: 4.70663715
INFO:root:[  397] Training loss: 0.08501518, Validation loss: 0.08542589, Gradient norm: 4.07536503
INFO:root:[  398] Training loss: 0.08525247, Validation loss: 0.08465819, Gradient norm: 8.94626654
INFO:root:[  399] Training loss: 0.08461171, Validation loss: 0.08454642, Gradient norm: 3.76048757
INFO:root:[  400] Training loss: 0.08402884, Validation loss: 0.08619269, Gradient norm: 8.64029769
INFO:root:[  401] Training loss: 0.08436210, Validation loss: 0.08463005, Gradient norm: 9.19759308
INFO:root:[  402] Training loss: 0.08394281, Validation loss: 0.08457506, Gradient norm: 5.48977474
INFO:root:[  403] Training loss: 0.08402799, Validation loss: 0.08461626, Gradient norm: 3.79467543
INFO:root:[  404] Training loss: 0.08356068, Validation loss: 0.08307040, Gradient norm: 3.50967993
INFO:root:[  405] Training loss: 0.08375027, Validation loss: 0.08488258, Gradient norm: 3.34448730
INFO:root:[  406] Training loss: 0.08334731, Validation loss: 0.08459135, Gradient norm: 3.52139994
INFO:root:[  407] Training loss: 0.08352454, Validation loss: 0.08517756, Gradient norm: 5.98567338
INFO:root:[  408] Training loss: 0.08319413, Validation loss: 0.08467820, Gradient norm: 8.99879276
INFO:root:[  409] Training loss: 0.08283672, Validation loss: 0.08232190, Gradient norm: 4.52979159
INFO:root:[  410] Training loss: 0.08210768, Validation loss: 0.08244442, Gradient norm: 3.21697102
INFO:root:[  411] Training loss: 0.08247047, Validation loss: 0.08216216, Gradient norm: 6.34346232
INFO:root:[  412] Training loss: 0.08210275, Validation loss: 0.08214830, Gradient norm: 4.09333936
INFO:root:[  413] Training loss: 0.08256169, Validation loss: 0.08279415, Gradient norm: 9.98507000
INFO:root:[  414] Training loss: 0.08208870, Validation loss: 0.08224251, Gradient norm: 9.38133510
INFO:root:[  415] Training loss: 0.08180181, Validation loss: 0.08302776, Gradient norm: 7.04854008
INFO:root:[  416] Training loss: 0.08206433, Validation loss: 0.08125096, Gradient norm: 7.52947270
INFO:root:[  417] Training loss: 0.08172727, Validation loss: 0.08201837, Gradient norm: 5.95188955
INFO:root:[  418] Training loss: 0.08175124, Validation loss: 0.08177280, Gradient norm: 5.93297724
INFO:root:[  419] Training loss: 0.08171591, Validation loss: 0.08167243, Gradient norm: 8.08700177
INFO:root:[  420] Training loss: 0.08091383, Validation loss: 0.08158728, Gradient norm: 7.63502079
INFO:root:[  421] Training loss: 0.08059004, Validation loss: 0.08111578, Gradient norm: 6.62939172
INFO:root:[  422] Training loss: 0.08096528, Validation loss: 0.08098493, Gradient norm: 7.93163529
INFO:root:[  423] Training loss: 0.08108111, Validation loss: 0.08246368, Gradient norm: 7.66233451
INFO:root:[  424] Training loss: 0.08087284, Validation loss: 0.08071021, Gradient norm: 7.00401928
INFO:root:[  425] Training loss: 0.08062264, Validation loss: 0.08109408, Gradient norm: 6.18905938
INFO:root:[  426] Training loss: 0.08021682, Validation loss: 0.08141132, Gradient norm: 3.71861845
INFO:root:[  427] Training loss: 0.08009722, Validation loss: 0.07941545, Gradient norm: 6.15116452
INFO:root:[  428] Training loss: 0.07994541, Validation loss: 0.08085377, Gradient norm: 4.65963133
INFO:root:[  429] Training loss: 0.08019225, Validation loss: 0.08084047, Gradient norm: 6.21780105
INFO:root:[  430] Training loss: 0.07968698, Validation loss: 0.07948428, Gradient norm: 5.52046399
INFO:root:[  431] Training loss: 0.07956669, Validation loss: 0.07949501, Gradient norm: 7.81112730
INFO:root:[  432] Training loss: 0.07943671, Validation loss: 0.08069496, Gradient norm: 7.91768114
INFO:root:[  433] Training loss: 0.07959079, Validation loss: 0.07929042, Gradient norm: 9.26067704
INFO:root:[  434] Training loss: 0.07901888, Validation loss: 0.07950153, Gradient norm: 6.02920588
INFO:root:[  435] Training loss: 0.07947835, Validation loss: 0.07961650, Gradient norm: 8.83683717
INFO:root:[  436] Training loss: 0.07851866, Validation loss: 0.07912518, Gradient norm: 5.75546803
INFO:root:[  437] Training loss: 0.07865839, Validation loss: 0.07879750, Gradient norm: 9.08630592
INFO:root:[  438] Training loss: 0.07844863, Validation loss: 0.07930801, Gradient norm: 8.93517498
INFO:root:[  439] Training loss: 0.07895351, Validation loss: 0.07866269, Gradient norm: 8.57746557
INFO:root:[  440] Training loss: 0.07839860, Validation loss: 0.07837201, Gradient norm: 7.96649564
INFO:root:[  441] Training loss: 0.07800695, Validation loss: 0.07863602, Gradient norm: 6.91860308
INFO:root:[  442] Training loss: 0.07853241, Validation loss: 0.07862768, Gradient norm: 8.74702059
INFO:root:[  443] Training loss: 0.07854312, Validation loss: 0.07777904, Gradient norm: 8.46850528
INFO:root:[  444] Training loss: 0.07772276, Validation loss: 0.07929799, Gradient norm: 5.99788061
INFO:root:[  445] Training loss: 0.07788837, Validation loss: 0.07769101, Gradient norm: 11.00427261
INFO:root:[  446] Training loss: 0.07811471, Validation loss: 0.07793225, Gradient norm: 9.51422343
INFO:root:[  447] Training loss: 0.07761592, Validation loss: 0.07820917, Gradient norm: 9.11831227
INFO:root:[  448] Training loss: 0.07755551, Validation loss: 0.07793700, Gradient norm: 9.61866079
INFO:root:[  449] Training loss: 0.07721183, Validation loss: 0.07832034, Gradient norm: 8.89556854
INFO:root:[  450] Training loss: 0.07724103, Validation loss: 0.07880483, Gradient norm: 9.93130796
INFO:root:[  451] Training loss: 0.07709967, Validation loss: 0.07794043, Gradient norm: 9.69496687
INFO:root:[  452] Training loss: 0.07645333, Validation loss: 0.07761851, Gradient norm: 7.02305123
INFO:root:[  453] Training loss: 0.07663806, Validation loss: 0.07718606, Gradient norm: 5.66476190
INFO:root:[  454] Training loss: 0.07685036, Validation loss: 0.07643786, Gradient norm: 8.21307624
INFO:root:[  455] Training loss: 0.07632327, Validation loss: 0.07845522, Gradient norm: 5.57290181
INFO:root:[  456] Training loss: 0.07651243, Validation loss: 0.07621780, Gradient norm: 9.68313927
INFO:root:[  457] Training loss: 0.07603384, Validation loss: 0.07655870, Gradient norm: 4.98410519
INFO:root:[  458] Training loss: 0.07583883, Validation loss: 0.07684967, Gradient norm: 6.38992153
INFO:root:[  459] Training loss: 0.07673946, Validation loss: 0.07674306, Gradient norm: 6.61170276
INFO:root:[  460] Training loss: 0.07609155, Validation loss: 0.07740715, Gradient norm: 8.59660053
INFO:root:[  461] Training loss: 0.07553706, Validation loss: 0.07582192, Gradient norm: 10.92397415
INFO:root:[  462] Training loss: 0.07532625, Validation loss: 0.07623741, Gradient norm: 10.55010976
INFO:root:[  463] Training loss: 0.07552089, Validation loss: 0.07590451, Gradient norm: 10.39045943
INFO:root:[  464] Training loss: 0.07540394, Validation loss: 0.07666958, Gradient norm: 4.92695587
INFO:root:[  465] Training loss: 0.07508573, Validation loss: 0.07543418, Gradient norm: 11.63711572
INFO:root:[  466] Training loss: 0.07505907, Validation loss: 0.07461889, Gradient norm: 8.20409203
INFO:root:[  467] Training loss: 0.07543256, Validation loss: 0.07373923, Gradient norm: 13.97623927
INFO:root:[  468] Training loss: 0.07444961, Validation loss: 0.07486528, Gradient norm: 8.50059009
INFO:root:[  469] Training loss: 0.07529047, Validation loss: 0.07450738, Gradient norm: 10.92327663
INFO:root:[  470] Training loss: 0.07484024, Validation loss: 0.07492879, Gradient norm: 7.19983083
INFO:root:[  471] Training loss: 0.07438023, Validation loss: 0.07442745, Gradient norm: 6.97769311
INFO:root:[  472] Training loss: 0.07438126, Validation loss: 0.07427655, Gradient norm: 10.47617859
INFO:root:[  473] Training loss: 0.07439494, Validation loss: 0.07531946, Gradient norm: 11.33264269
INFO:root:[  474] Training loss: 0.07444920, Validation loss: 0.07537774, Gradient norm: 13.50265989
INFO:root:[  475] Training loss: 0.07456034, Validation loss: 0.07414348, Gradient norm: 12.68621343
INFO:root:[  476] Training loss: 0.07405005, Validation loss: 0.07329422, Gradient norm: 11.23729883
INFO:root:[  477] Training loss: 0.07414523, Validation loss: 0.07378116, Gradient norm: 10.92345095
INFO:root:[  478] Training loss: 0.07362614, Validation loss: 0.07532133, Gradient norm: 8.29694154
INFO:root:[  479] Training loss: 0.07395526, Validation loss: 0.07478533, Gradient norm: 8.03150535
INFO:root:[  480] Training loss: 0.07361727, Validation loss: 0.07545701, Gradient norm: 10.92782066
INFO:root:[  481] Training loss: 0.07372784, Validation loss: 0.07239390, Gradient norm: 13.72821261
INFO:root:[  482] Training loss: 0.07383595, Validation loss: 0.07492291, Gradient norm: 11.90297864
INFO:root:[  483] Training loss: 0.07336380, Validation loss: 0.07351505, Gradient norm: 12.16807225
INFO:root:[  484] Training loss: 0.07323290, Validation loss: 0.07581107, Gradient norm: 10.28631516
INFO:root:[  485] Training loss: 0.07305616, Validation loss: 0.07294461, Gradient norm: 12.45425773
INFO:root:[  486] Training loss: 0.07247366, Validation loss: 0.07304130, Gradient norm: 10.81624899
INFO:root:[  487] Training loss: 0.07296514, Validation loss: 0.07229700, Gradient norm: 8.69866159
INFO:root:[  488] Training loss: 0.07225531, Validation loss: 0.07220489, Gradient norm: 12.15361599
INFO:root:[  489] Training loss: 0.07277503, Validation loss: 0.07229149, Gradient norm: 15.54573743
INFO:root:[  490] Training loss: 0.07234630, Validation loss: 0.07388005, Gradient norm: 9.82414589
INFO:root:[  491] Training loss: 0.07221759, Validation loss: 0.07221391, Gradient norm: 12.51652825
INFO:root:[  492] Training loss: 0.07277626, Validation loss: 0.07288966, Gradient norm: 17.03351622
INFO:root:[  493] Training loss: 0.07156366, Validation loss: 0.07233580, Gradient norm: 11.54846531
INFO:root:[  494] Training loss: 0.07192764, Validation loss: 0.07145688, Gradient norm: 7.56032661
INFO:root:[  495] Training loss: 0.07176645, Validation loss: 0.07280509, Gradient norm: 11.28442435
INFO:root:[  496] Training loss: 0.07172005, Validation loss: 0.07276433, Gradient norm: 7.82258368
INFO:root:[  497] Training loss: 0.07178904, Validation loss: 0.07164368, Gradient norm: 10.37219126
INFO:root:[  498] Training loss: 0.07226265, Validation loss: 0.07238373, Gradient norm: 12.99508653
INFO:root:[  499] Training loss: 0.07196722, Validation loss: 0.07336262, Gradient norm: 20.27783108
INFO:root:[  500] Training loss: 0.07124188, Validation loss: 0.07260398, Gradient norm: 8.47855720
INFO:root:[  501] Training loss: 0.07131434, Validation loss: 0.07065604, Gradient norm: 15.26987177
INFO:root:[  502] Training loss: 0.07181337, Validation loss: 0.07210628, Gradient norm: 18.09208384
INFO:root:[  503] Training loss: 0.07104146, Validation loss: 0.07177141, Gradient norm: 11.76274022
INFO:root:[  504] Training loss: 0.07083900, Validation loss: 0.07089330, Gradient norm: 10.89482123
INFO:root:[  505] Training loss: 0.07138817, Validation loss: 0.07128410, Gradient norm: 19.05805590
INFO:root:[  506] Training loss: 0.07092786, Validation loss: 0.07104613, Gradient norm: 16.68831862
INFO:root:[  507] Training loss: 0.07069429, Validation loss: 0.07100180, Gradient norm: 11.09434370
INFO:root:[  508] Training loss: 0.07102495, Validation loss: 0.07066321, Gradient norm: 15.08878625
INFO:root:[  509] Training loss: 0.07034675, Validation loss: 0.07067480, Gradient norm: 12.76769900
INFO:root:[  510] Training loss: 0.07055037, Validation loss: 0.07073325, Gradient norm: 10.86967028
INFO:root:[  511] Training loss: 0.07077969, Validation loss: 0.07019043, Gradient norm: 13.48775266
INFO:root:[  512] Training loss: 0.07014025, Validation loss: 0.06961491, Gradient norm: 11.42571624
INFO:root:[  513] Training loss: 0.07060509, Validation loss: 0.07018803, Gradient norm: 12.49284366
INFO:root:[  514] Training loss: 0.07002627, Validation loss: 0.06979765, Gradient norm: 15.73396161
INFO:root:[  515] Training loss: 0.07045920, Validation loss: 0.07078799, Gradient norm: 10.01113997
INFO:root:[  516] Training loss: 0.07000459, Validation loss: 0.07120318, Gradient norm: 17.24753835
INFO:root:[  517] Training loss: 0.06990361, Validation loss: 0.06981036, Gradient norm: 17.03576649
INFO:root:[  518] Training loss: 0.07059038, Validation loss: 0.06957618, Gradient norm: 16.84920618
INFO:root:[  519] Training loss: 0.06990796, Validation loss: 0.06951013, Gradient norm: 18.99592077
INFO:root:[  520] Training loss: 0.07021887, Validation loss: 0.06968827, Gradient norm: 21.65551915
INFO:root:[  521] Training loss: 0.07008852, Validation loss: 0.06998798, Gradient norm: 17.77425856
INFO:root:[  522] Training loss: 0.06962905, Validation loss: 0.06869418, Gradient norm: 14.52765244
INFO:root:[  523] Training loss: 0.06934205, Validation loss: 0.07020766, Gradient norm: 11.90100662
INFO:root:[  524] Training loss: 0.06954241, Validation loss: 0.06937417, Gradient norm: 21.10232818
INFO:root:[  525] Training loss: 0.06926212, Validation loss: 0.06938279, Gradient norm: 16.75617691
INFO:root:[  526] Training loss: 0.06917324, Validation loss: 0.06973714, Gradient norm: 16.71273019
INFO:root:[  527] Training loss: 0.06879255, Validation loss: 0.06905354, Gradient norm: 12.69945122
INFO:root:[  528] Training loss: 0.06887649, Validation loss: 0.06921492, Gradient norm: 11.05565666
INFO:root:[  529] Training loss: 0.06882773, Validation loss: 0.06884431, Gradient norm: 20.80403817
INFO:root:[  530] Training loss: 0.06877222, Validation loss: 0.06909894, Gradient norm: 20.05871223
INFO:root:[  531] Training loss: 0.06885539, Validation loss: 0.06834942, Gradient norm: 21.32221591
INFO:root:[  532] Training loss: 0.06879397, Validation loss: 0.06866312, Gradient norm: 22.59997519
INFO:root:[  533] Training loss: 0.06810071, Validation loss: 0.06897860, Gradient norm: 12.64469767
INFO:root:[  534] Training loss: 0.06810020, Validation loss: 0.06886410, Gradient norm: 12.24336399
INFO:root:[  535] Training loss: 0.06857998, Validation loss: 0.06890639, Gradient norm: 11.89980218
INFO:root:[  536] Training loss: 0.06860353, Validation loss: 0.06789833, Gradient norm: 17.11274046
INFO:root:[  537] Training loss: 0.06844864, Validation loss: 0.06865706, Gradient norm: 18.64470520
INFO:root:[  538] Training loss: 0.06793390, Validation loss: 0.06839574, Gradient norm: 13.02782805
INFO:root:[  539] Training loss: 0.06855992, Validation loss: 0.06789974, Gradient norm: 26.77809862
INFO:root:[  540] Training loss: 0.06788671, Validation loss: 0.06744966, Gradient norm: 18.66909998
INFO:root:[  541] Training loss: 0.06793101, Validation loss: 0.06834427, Gradient norm: 15.53687996
INFO:root:[  542] Training loss: 0.06787516, Validation loss: 0.06777433, Gradient norm: 18.78474860
INFO:root:[  543] Training loss: 0.06749935, Validation loss: 0.06825973, Gradient norm: 18.45623093
INFO:root:[  544] Training loss: 0.06816698, Validation loss: 0.06708272, Gradient norm: 25.44093244
INFO:root:[  545] Training loss: 0.06733653, Validation loss: 0.06859345, Gradient norm: 20.32229878
INFO:root:[  546] Training loss: 0.06748483, Validation loss: 0.06739864, Gradient norm: 22.47619208
INFO:root:[  547] Training loss: 0.06740997, Validation loss: 0.06664512, Gradient norm: 14.32047785
INFO:root:[  548] Training loss: 0.06681707, Validation loss: 0.06833716, Gradient norm: 8.17321320
INFO:root:[  549] Training loss: 0.06713990, Validation loss: 0.06752431, Gradient norm: 11.97534843
INFO:root:[  550] Training loss: 0.06716386, Validation loss: 0.06786694, Gradient norm: 13.89983918
INFO:root:[  551] Training loss: 0.06727320, Validation loss: 0.06647971, Gradient norm: 20.63497075
INFO:root:[  552] Training loss: 0.06755399, Validation loss: 0.06838414, Gradient norm: 23.80729602
INFO:root:[  553] Training loss: 0.06723427, Validation loss: 0.06707934, Gradient norm: 25.53019898
INFO:root:[  554] Training loss: 0.06694488, Validation loss: 0.06643180, Gradient norm: 16.09256404
INFO:root:[  555] Training loss: 0.06712725, Validation loss: 0.06803012, Gradient norm: 14.23852775
INFO:root:[  556] Training loss: 0.06721747, Validation loss: 0.06813561, Gradient norm: 21.29862699
INFO:root:[  557] Training loss: 0.06682431, Validation loss: 0.06679665, Gradient norm: 24.46616517
INFO:root:[  558] Training loss: 0.06646256, Validation loss: 0.06675306, Gradient norm: 16.41510820
INFO:root:[  559] Training loss: 0.06634501, Validation loss: 0.06758966, Gradient norm: 19.90831167
INFO:root:[  560] Training loss: 0.06617063, Validation loss: 0.06595989, Gradient norm: 19.95471666
INFO:root:[  561] Training loss: 0.06650536, Validation loss: 0.06614871, Gradient norm: 27.47160658
INFO:root:[  562] Training loss: 0.06647286, Validation loss: 0.06666739, Gradient norm: 17.57638957
INFO:root:[  563] Training loss: 0.06595999, Validation loss: 0.07107068, Gradient norm: 13.62543063
INFO:root:[  564] Training loss: 0.06681885, Validation loss: 0.06724212, Gradient norm: 31.70024430
INFO:root:[  565] Training loss: 0.06665941, Validation loss: 0.06619231, Gradient norm: 29.16199865
INFO:root:[  566] Training loss: 0.06569070, Validation loss: 0.06517408, Gradient norm: 19.92210602
INFO:root:[  567] Training loss: 0.06581797, Validation loss: 0.06626720, Gradient norm: 16.18007461
INFO:root:[  568] Training loss: 0.06575640, Validation loss: 0.06587014, Gradient norm: 18.59025002
INFO:root:[  569] Training loss: 0.06627971, Validation loss: 0.06717438, Gradient norm: 20.93498239
INFO:root:[  570] Training loss: 0.06589864, Validation loss: 0.06511771, Gradient norm: 30.30146648
INFO:root:[  571] Training loss: 0.06571961, Validation loss: 0.06552094, Gradient norm: 22.04919355
INFO:root:[  572] Training loss: 0.06517384, Validation loss: 0.06590019, Gradient norm: 18.04987621
INFO:root:[  573] Training loss: 0.06565781, Validation loss: 0.06643732, Gradient norm: 26.39419981
INFO:root:[  574] Training loss: 0.06561049, Validation loss: 0.06510761, Gradient norm: 27.64804060
INFO:root:[  575] Training loss: 0.06571241, Validation loss: 0.06525820, Gradient norm: 24.98185085
INFO:root:[  576] Training loss: 0.06537408, Validation loss: 0.06463741, Gradient norm: 23.91740918
INFO:root:[  577] Training loss: 0.06535314, Validation loss: 0.06471611, Gradient norm: 21.75952092
INFO:root:[  578] Training loss: 0.06516037, Validation loss: 0.06624955, Gradient norm: 22.85542437
INFO:root:[  579] Training loss: 0.06539741, Validation loss: 0.06670673, Gradient norm: 27.42588945
INFO:root:[  580] Training loss: 0.06482440, Validation loss: 0.06604649, Gradient norm: 20.62838742
INFO:root:[  581] Training loss: 0.06466181, Validation loss: 0.06473099, Gradient norm: 19.19664989
INFO:root:[  582] Training loss: 0.06505659, Validation loss: 0.06466477, Gradient norm: 19.17929225
INFO:root:[  583] Training loss: 0.06533986, Validation loss: 0.06495311, Gradient norm: 30.86097896
INFO:root:[  584] Training loss: 0.06533427, Validation loss: 0.06531040, Gradient norm: 29.05730460
INFO:root:[  585] Training loss: 0.06455525, Validation loss: 0.06503357, Gradient norm: 19.41836159
INFO:root:[  586] Training loss: 0.06449236, Validation loss: 0.06436877, Gradient norm: 25.35900037
INFO:root:[  587] Training loss: 0.06501693, Validation loss: 0.06465585, Gradient norm: 26.05168348
INFO:root:[  588] Training loss: 0.06437912, Validation loss: 0.06533350, Gradient norm: 21.82819899
INFO:root:[  589] Training loss: 0.06477589, Validation loss: 0.06549754, Gradient norm: 25.73048874
INFO:root:[  590] Training loss: 0.06495283, Validation loss: 0.06420734, Gradient norm: 27.74118847
INFO:root:[  591] Training loss: 0.06426924, Validation loss: 0.06438302, Gradient norm: 21.41736014
INFO:root:[  592] Training loss: 0.06414212, Validation loss: 0.06468743, Gradient norm: 19.70508194
INFO:root:[  593] Training loss: 0.06415411, Validation loss: 0.06432139, Gradient norm: 29.99273808
INFO:root:[  594] Training loss: 0.06403885, Validation loss: 0.06376083, Gradient norm: 17.68876321
INFO:root:[  595] Training loss: 0.06449529, Validation loss: 0.06327982, Gradient norm: 33.10531558
INFO:root:[  596] Training loss: 0.06425098, Validation loss: 0.06433107, Gradient norm: 21.97708354
INFO:root:[  597] Training loss: 0.06431691, Validation loss: 0.06318233, Gradient norm: 29.01289241
INFO:root:[  598] Training loss: 0.06350185, Validation loss: 0.06333762, Gradient norm: 20.67549030
INFO:root:[  599] Training loss: 0.06427503, Validation loss: 0.06346016, Gradient norm: 22.28438926
INFO:root:[  600] Training loss: 0.06415455, Validation loss: 0.06421392, Gradient norm: 33.46572030
INFO:root:[  601] Training loss: 0.06403729, Validation loss: 0.06398932, Gradient norm: 28.66020761
INFO:root:[  602] Training loss: 0.06360635, Validation loss: 0.06439244, Gradient norm: 20.68360553
INFO:root:[  603] Training loss: 0.06362614, Validation loss: 0.06440659, Gradient norm: 25.53211713
INFO:root:[  604] Training loss: 0.06350590, Validation loss: 0.06332966, Gradient norm: 17.34856648
INFO:root:[  605] Training loss: 0.06388806, Validation loss: 0.06330890, Gradient norm: 32.50366202
INFO:root:[  606] Training loss: 0.06367461, Validation loss: 0.06328565, Gradient norm: 30.50641023
INFO:root:[  607] Training loss: 0.06355705, Validation loss: 0.06299957, Gradient norm: 30.26665259
INFO:root:[  608] Training loss: 0.06294978, Validation loss: 0.06394969, Gradient norm: 19.77933899
INFO:root:[  609] Training loss: 0.06340280, Validation loss: 0.06374983, Gradient norm: 29.78901080
INFO:root:[  610] Training loss: 0.06348851, Validation loss: 0.06312809, Gradient norm: 29.10642197
INFO:root:[  611] Training loss: 0.06321524, Validation loss: 0.06275739, Gradient norm: 24.85831999
INFO:root:[  612] Training loss: 0.06265854, Validation loss: 0.06295916, Gradient norm: 21.54524945
INFO:root:[  613] Training loss: 0.06261145, Validation loss: 0.06245949, Gradient norm: 17.99044394
INFO:root:[  614] Training loss: 0.06279315, Validation loss: 0.06215660, Gradient norm: 24.00245479
INFO:root:[  615] Training loss: 0.06275038, Validation loss: 0.06188392, Gradient norm: 24.80069313
INFO:root:[  616] Training loss: 0.06264375, Validation loss: 0.06273738, Gradient norm: 26.11369150
INFO:root:[  617] Training loss: 0.06252633, Validation loss: 0.06260828, Gradient norm: 28.67383424
INFO:root:[  618] Training loss: 0.06280831, Validation loss: 0.06268226, Gradient norm: 31.36410822
INFO:root:[  619] Training loss: 0.06337601, Validation loss: 0.06186424, Gradient norm: 39.34464200
INFO:root:[  620] Training loss: 0.06242318, Validation loss: 0.06284678, Gradient norm: 22.93318798
INFO:root:[  621] Training loss: 0.06204094, Validation loss: 0.06224419, Gradient norm: 21.33552156
INFO:root:[  622] Training loss: 0.06250689, Validation loss: 0.06181950, Gradient norm: 28.48102199
INFO:root:[  623] Training loss: 0.06185642, Validation loss: 0.06405276, Gradient norm: 23.83913290
INFO:root:[  624] Training loss: 0.06238224, Validation loss: 0.06199085, Gradient norm: 25.84578973
INFO:root:[  625] Training loss: 0.06210617, Validation loss: 0.06241689, Gradient norm: 26.40429324
INFO:root:[  626] Training loss: 0.06211533, Validation loss: 0.06163700, Gradient norm: 31.48590189
INFO:root:[  627] Training loss: 0.06171126, Validation loss: 0.06210004, Gradient norm: 20.60795790
INFO:root:[  628] Training loss: 0.06198229, Validation loss: 0.06183897, Gradient norm: 27.02154907
INFO:root:[  629] Training loss: 0.06146575, Validation loss: 0.06246738, Gradient norm: 22.34500288
INFO:root:[  630] Training loss: 0.06187350, Validation loss: 0.06140741, Gradient norm: 31.45297267
INFO:root:[  631] Training loss: 0.06137260, Validation loss: 0.06214904, Gradient norm: 23.32520906
INFO:root:[  632] Training loss: 0.06136600, Validation loss: 0.06134828, Gradient norm: 26.03757932
INFO:root:[  633] Training loss: 0.06216677, Validation loss: 0.06111489, Gradient norm: 40.50062790
INFO:root:[  634] Training loss: 0.06188433, Validation loss: 0.06133474, Gradient norm: 36.55927414
INFO:root:[  635] Training loss: 0.06169902, Validation loss: 0.06298111, Gradient norm: 33.11761603
INFO:root:[  636] Training loss: 0.06139081, Validation loss: 0.06135114, Gradient norm: 38.28684467
INFO:root:[  637] Training loss: 0.06088151, Validation loss: 0.06129783, Gradient norm: 19.10255110
INFO:root:[  638] Training loss: 0.06069381, Validation loss: 0.06395833, Gradient norm: 20.18935401
INFO:root:[  639] Training loss: 0.06147445, Validation loss: 0.06071719, Gradient norm: 38.16706612
INFO:root:[  640] Training loss: 0.06049813, Validation loss: 0.06147652, Gradient norm: 26.21299984
INFO:root:[  641] Training loss: 0.06074265, Validation loss: 0.06469968, Gradient norm: 23.06180833
INFO:root:[  642] Training loss: 0.06085011, Validation loss: 0.06090176, Gradient norm: 27.19207834
INFO:root:[  643] Training loss: 0.06133882, Validation loss: 0.06070863, Gradient norm: 37.95231826
INFO:root:[  644] Training loss: 0.06090998, Validation loss: 0.06065614, Gradient norm: 31.75745231
INFO:root:[  645] Training loss: 0.06075208, Validation loss: 0.06057910, Gradient norm: 23.06059453
INFO:root:[  646] Training loss: 0.06077969, Validation loss: 0.06051592, Gradient norm: 29.54276169
INFO:root:[  647] Training loss: 0.06054013, Validation loss: 0.06234288, Gradient norm: 26.08894255
INFO:root:[  648] Training loss: 0.06142735, Validation loss: 0.06036126, Gradient norm: 47.15292506
INFO:root:[  649] Training loss: 0.06053825, Validation loss: 0.06000950, Gradient norm: 32.77124063
INFO:root:[  650] Training loss: 0.06046813, Validation loss: 0.06108582, Gradient norm: 34.18345960
INFO:root:[  651] Training loss: 0.06050917, Validation loss: 0.06108086, Gradient norm: 32.10780486
INFO:root:[  652] Training loss: 0.06011247, Validation loss: 0.05972007, Gradient norm: 24.82982009
INFO:root:[  653] Training loss: 0.06024006, Validation loss: 0.06164672, Gradient norm: 25.10781578
INFO:root:[  654] Training loss: 0.06109946, Validation loss: 0.06009804, Gradient norm: 44.67491521
INFO:root:[  655] Training loss: 0.06010760, Validation loss: 0.06414720, Gradient norm: 26.50875648
INFO:root:[  656] Training loss: 0.06031832, Validation loss: 0.05990427, Gradient norm: 29.95358034
INFO:root:[  657] Training loss: 0.06035606, Validation loss: 0.05973386, Gradient norm: 37.16856660
INFO:root:[  658] Training loss: 0.05934691, Validation loss: 0.05944678, Gradient norm: 10.70551880
INFO:root:[  659] Training loss: 0.06053060, Validation loss: 0.06052212, Gradient norm: 36.81299447
INFO:root:[  660] Training loss: 0.06031456, Validation loss: 0.06014953, Gradient norm: 34.06130337
INFO:root:[  661] Training loss: 0.05959435, Validation loss: 0.06071651, Gradient norm: 20.67584965
INFO:root:[  662] Training loss: 0.06023234, Validation loss: 0.06127902, Gradient norm: 44.22014341
INFO:root:[  663] Training loss: 0.06022828, Validation loss: 0.05899475, Gradient norm: 41.03623117
INFO:root:[  664] Training loss: 0.05952951, Validation loss: 0.06164925, Gradient norm: 31.29531303
INFO:root:[  665] Training loss: 0.06009086, Validation loss: 0.05917030, Gradient norm: 40.05475666
INFO:root:[  666] Training loss: 0.06001041, Validation loss: 0.05989089, Gradient norm: 33.95127235
INFO:root:[  667] Training loss: 0.06011381, Validation loss: 0.05911209, Gradient norm: 42.82959723
INFO:root:[  668] Training loss: 0.05960598, Validation loss: 0.06092540, Gradient norm: 36.34901341
INFO:root:[  669] Training loss: 0.05974405, Validation loss: 0.05942523, Gradient norm: 38.43878095
INFO:root:[  670] Training loss: 0.05919212, Validation loss: 0.05954641, Gradient norm: 26.68806124
INFO:root:[  671] Training loss: 0.06046268, Validation loss: 0.06091706, Gradient norm: 48.66765229
INFO:root:[  672] Training loss: 0.05944027, Validation loss: 0.05992708, Gradient norm: 31.47017277
INFO:root:EP 672: Early stopping
INFO:root:Training the model took 10734.78s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.29988
INFO:root:EnergyScoreTrain: 0.86851
INFO:root:CoverageTrain: 0.93356
INFO:root:IntervalWidthTrain: 0.03399
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.25002
INFO:root:EnergyScoreValidation: 0.83518
INFO:root:CoverageValidation: 0.93283
INFO:root:IntervalWidthValidation: 0.03397
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.0244
INFO:root:EnergyScoreTest: 0.68477
INFO:root:CoverageTest: 0.93274
INFO:root:IntervalWidthTest: 0.03339
INFO:root:###17 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1409286144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.68417483, Validation loss: 1.00807166, Gradient norm: 6.84312280
INFO:root:[    2] Training loss: 0.81681263, Validation loss: 0.69308647, Gradient norm: 3.36455340
INFO:root:[    3] Training loss: 0.66184127, Validation loss: 0.62726799, Gradient norm: 2.33726660
INFO:root:[    4] Training loss: 0.59688675, Validation loss: 0.59747360, Gradient norm: 2.50838365
INFO:root:[    5] Training loss: 0.57201486, Validation loss: 0.55048220, Gradient norm: 3.16429338
INFO:root:[    6] Training loss: 0.54743065, Validation loss: 0.53355776, Gradient norm: 2.24347931
INFO:root:[    7] Training loss: 0.52077976, Validation loss: 0.51724292, Gradient norm: 2.81140304
INFO:root:[    8] Training loss: 0.50768080, Validation loss: 0.49379096, Gradient norm: 2.39553584
INFO:root:[    9] Training loss: 0.48740681, Validation loss: 0.48278551, Gradient norm: 2.10215802
INFO:root:[   10] Training loss: 0.47446178, Validation loss: 0.47522781, Gradient norm: 2.56044096
INFO:root:[   11] Training loss: 0.46301743, Validation loss: 0.46626426, Gradient norm: 2.92884865
INFO:root:[   12] Training loss: 0.44652610, Validation loss: 0.44151332, Gradient norm: 2.11977548
INFO:root:[   13] Training loss: 0.43683047, Validation loss: 0.43345503, Gradient norm: 2.29951508
INFO:root:[   14] Training loss: 0.42421252, Validation loss: 0.41458240, Gradient norm: 1.79729559
INFO:root:[   15] Training loss: 0.41745994, Validation loss: 0.40471616, Gradient norm: 2.39546863
INFO:root:[   16] Training loss: 0.40605248, Validation loss: 0.40642346, Gradient norm: 2.64252902
INFO:root:[   17] Training loss: 0.39055587, Validation loss: 0.39539689, Gradient norm: 1.86165728
INFO:root:[   18] Training loss: 0.38653616, Validation loss: 0.38161142, Gradient norm: 3.02170711
INFO:root:[   19] Training loss: 0.37765006, Validation loss: 0.38168329, Gradient norm: 3.08411196
INFO:root:[   20] Training loss: 0.36996740, Validation loss: 0.36318489, Gradient norm: 2.54981361
INFO:root:[   21] Training loss: 0.36192350, Validation loss: 0.36561601, Gradient norm: 1.73756690
INFO:root:[   22] Training loss: 0.35459978, Validation loss: 0.35246086, Gradient norm: 1.63256859
INFO:root:[   23] Training loss: 0.34880671, Validation loss: 0.34684236, Gradient norm: 1.64690917
INFO:root:[   24] Training loss: 0.34402183, Validation loss: 0.34759024, Gradient norm: 2.96466934
INFO:root:[   25] Training loss: 0.33825137, Validation loss: 0.33105676, Gradient norm: 2.99720759
INFO:root:[   26] Training loss: 0.33123156, Validation loss: 0.34046897, Gradient norm: 1.78585310
INFO:root:[   27] Training loss: 0.32738519, Validation loss: 0.32867374, Gradient norm: 2.53279067
INFO:root:[   28] Training loss: 0.32163881, Validation loss: 0.32325562, Gradient norm: 1.70658907
INFO:root:[   29] Training loss: 0.31757048, Validation loss: 0.31715054, Gradient norm: 1.53909218
INFO:root:[   30] Training loss: 0.31147630, Validation loss: 0.30867313, Gradient norm: 1.58968712
INFO:root:[   31] Training loss: 0.30931663, Validation loss: 0.30614692, Gradient norm: 2.44046972
INFO:root:[   32] Training loss: 0.30543517, Validation loss: 0.30316806, Gradient norm: 1.98288218
INFO:root:[   33] Training loss: 0.30111934, Validation loss: 0.30230575, Gradient norm: 2.46579036
INFO:root:[   34] Training loss: 0.30202356, Validation loss: 0.29796542, Gradient norm: 2.25973081
INFO:root:[   35] Training loss: 0.29810048, Validation loss: 0.29272051, Gradient norm: 2.21453235
INFO:root:[   36] Training loss: 0.29483402, Validation loss: 0.29156280, Gradient norm: 2.43915923
INFO:root:[   37] Training loss: 0.29059678, Validation loss: 0.29134274, Gradient norm: 1.54933005
INFO:root:[   38] Training loss: 0.28619927, Validation loss: 0.28695468, Gradient norm: 1.90841678
INFO:root:[   39] Training loss: 0.28479787, Validation loss: 0.28209894, Gradient norm: 1.29132554
INFO:root:[   40] Training loss: 0.28226801, Validation loss: 0.27790513, Gradient norm: 1.57512598
INFO:root:[   41] Training loss: 0.27726982, Validation loss: 0.27422040, Gradient norm: 0.99067436
INFO:root:[   42] Training loss: 0.27453669, Validation loss: 0.27591426, Gradient norm: 1.15508359
INFO:root:[   43] Training loss: 0.27355524, Validation loss: 0.27650809, Gradient norm: 1.56292968
INFO:root:[   44] Training loss: 0.27096514, Validation loss: 0.27392475, Gradient norm: 1.87729102
INFO:root:[   45] Training loss: 0.26825191, Validation loss: 0.26562397, Gradient norm: 1.27848688
INFO:root:[   46] Training loss: 0.26736624, Validation loss: 0.26565370, Gradient norm: 1.52406422
INFO:root:[   47] Training loss: 0.26578618, Validation loss: 0.26377271, Gradient norm: 1.97920689
INFO:root:[   48] Training loss: 0.26485087, Validation loss: 0.26206138, Gradient norm: 2.83469887
INFO:root:[   49] Training loss: 0.26099371, Validation loss: 0.26239648, Gradient norm: 1.56188507
INFO:root:[   50] Training loss: 0.25946540, Validation loss: 0.26006968, Gradient norm: 1.80462088
INFO:root:[   51] Training loss: 0.25922634, Validation loss: 0.25529229, Gradient norm: 2.19515004
INFO:root:[   52] Training loss: 0.25606022, Validation loss: 0.25252351, Gradient norm: 1.95202354
INFO:root:[   53] Training loss: 0.25368587, Validation loss: 0.25740465, Gradient norm: 2.02969868
INFO:root:[   54] Training loss: 0.25176095, Validation loss: 0.25112458, Gradient norm: 1.41832725
INFO:root:[   55] Training loss: 0.25229034, Validation loss: 0.24977601, Gradient norm: 1.67526216
INFO:root:[   56] Training loss: 0.24926094, Validation loss: 0.25106134, Gradient norm: 1.15691882
INFO:root:[   57] Training loss: 0.24899866, Validation loss: 0.24894901, Gradient norm: 1.79080932
INFO:root:[   58] Training loss: 0.24719197, Validation loss: 0.24587575, Gradient norm: 1.53719621
INFO:root:[   59] Training loss: 0.24466020, Validation loss: 0.24716417, Gradient norm: 1.48933287
INFO:root:[   60] Training loss: 0.24505915, Validation loss: 0.24392884, Gradient norm: 1.68124644
INFO:root:[   61] Training loss: 0.24153106, Validation loss: 0.23998212, Gradient norm: 1.17825883
INFO:root:[   62] Training loss: 0.24113397, Validation loss: 0.24207285, Gradient norm: 1.38218183
INFO:root:[   63] Training loss: 0.24013189, Validation loss: 0.23892828, Gradient norm: 1.55440397
INFO:root:[   64] Training loss: 0.23902410, Validation loss: 0.23715479, Gradient norm: 1.65109664
INFO:root:[   65] Training loss: 0.23656551, Validation loss: 0.23741296, Gradient norm: 1.14110788
INFO:root:[   66] Training loss: 0.23564877, Validation loss: 0.23536726, Gradient norm: 1.11637615
INFO:root:[   67] Training loss: 0.23506010, Validation loss: 0.23496385, Gradient norm: 1.30981507
INFO:root:[   68] Training loss: 0.23361532, Validation loss: 0.23371550, Gradient norm: 1.67672433
INFO:root:[   69] Training loss: 0.23296591, Validation loss: 0.23463689, Gradient norm: 1.88631713
INFO:root:[   70] Training loss: 0.23055946, Validation loss: 0.23214243, Gradient norm: 1.31199437
INFO:root:[   71] Training loss: 0.23009667, Validation loss: 0.23064862, Gradient norm: 1.38418956
INFO:root:[   72] Training loss: 0.22875969, Validation loss: 0.22713167, Gradient norm: 1.18463579
INFO:root:[   73] Training loss: 0.22853519, Validation loss: 0.22790267, Gradient norm: 1.22939039
INFO:root:[   74] Training loss: 0.22657105, Validation loss: 0.22539474, Gradient norm: 1.21048733
INFO:root:[   75] Training loss: 0.22640146, Validation loss: 0.22460971, Gradient norm: 1.33327020
INFO:root:[   76] Training loss: 0.22405135, Validation loss: 0.22283100, Gradient norm: 1.04118925
INFO:root:[   77] Training loss: 0.22394675, Validation loss: 0.22277155, Gradient norm: 1.21847708
INFO:root:[   78] Training loss: 0.22225488, Validation loss: 0.22240016, Gradient norm: 1.03822530
INFO:root:[   79] Training loss: 0.22210525, Validation loss: 0.22052809, Gradient norm: 1.20439800
INFO:root:[   80] Training loss: 0.22011428, Validation loss: 0.21908719, Gradient norm: 1.05832186
INFO:root:[   81] Training loss: 0.21941762, Validation loss: 0.21994393, Gradient norm: 0.99270661
INFO:root:[   82] Training loss: 0.21777221, Validation loss: 0.21816026, Gradient norm: 0.84993259
INFO:root:[   83] Training loss: 0.21729930, Validation loss: 0.21946842, Gradient norm: 1.10268078
INFO:root:[   84] Training loss: 0.21786248, Validation loss: 0.21749367, Gradient norm: 1.63280396
INFO:root:[   85] Training loss: 0.21710761, Validation loss: 0.21687443, Gradient norm: 1.92816035
INFO:root:[   86] Training loss: 0.21494122, Validation loss: 0.21501593, Gradient norm: 0.89438653
INFO:root:[   87] Training loss: 0.21437181, Validation loss: 0.21460387, Gradient norm: 1.56327250
INFO:root:[   88] Training loss: 0.21378090, Validation loss: 0.21307622, Gradient norm: 1.24575349
INFO:root:[   89] Training loss: 0.21269671, Validation loss: 0.21163549, Gradient norm: 1.12971898
INFO:root:[   90] Training loss: 0.21170340, Validation loss: 0.21321251, Gradient norm: 1.28539297
INFO:root:[   91] Training loss: 0.21130764, Validation loss: 0.20989370, Gradient norm: 1.48783625
INFO:root:[   92] Training loss: 0.20966756, Validation loss: 0.20881242, Gradient norm: 0.96689174
INFO:root:[   93] Training loss: 0.20970226, Validation loss: 0.21022370, Gradient norm: 1.65748038
INFO:root:[   94] Training loss: 0.20817511, Validation loss: 0.20689244, Gradient norm: 1.52888010
INFO:root:[   95] Training loss: 0.20798433, Validation loss: 0.20894330, Gradient norm: 1.51937463
INFO:root:[   96] Training loss: 0.20670825, Validation loss: 0.20610723, Gradient norm: 1.23287611
INFO:root:[   97] Training loss: 0.20624180, Validation loss: 0.20573682, Gradient norm: 1.24784690
INFO:root:[   98] Training loss: 0.20525151, Validation loss: 0.20482917, Gradient norm: 1.10789061
INFO:root:[   99] Training loss: 0.20536284, Validation loss: 0.20521027, Gradient norm: 1.69491561
INFO:root:[  100] Training loss: 0.20385509, Validation loss: 0.20378690, Gradient norm: 1.19737289
INFO:root:[  101] Training loss: 0.20399465, Validation loss: 0.20318323, Gradient norm: 1.70691390
INFO:root:[  102] Training loss: 0.20198614, Validation loss: 0.20165474, Gradient norm: 1.31048582
INFO:root:[  103] Training loss: 0.20211840, Validation loss: 0.20264103, Gradient norm: 1.08937113
INFO:root:[  104] Training loss: 0.20120834, Validation loss: 0.20198416, Gradient norm: 1.29745168
INFO:root:[  105] Training loss: 0.20019701, Validation loss: 0.19995334, Gradient norm: 0.96376188
INFO:root:[  106] Training loss: 0.19942537, Validation loss: 0.19983378, Gradient norm: 1.23109963
INFO:root:[  107] Training loss: 0.19927310, Validation loss: 0.19909615, Gradient norm: 1.47086290
INFO:root:[  108] Training loss: 0.19786894, Validation loss: 0.19888703, Gradient norm: 0.64462702
INFO:root:[  109] Training loss: 0.19730565, Validation loss: 0.19727695, Gradient norm: 1.56171329
INFO:root:[  110] Training loss: 0.19683217, Validation loss: 0.19860706, Gradient norm: 1.18170499
INFO:root:[  111] Training loss: 0.19679747, Validation loss: 0.19644869, Gradient norm: 1.54513009
INFO:root:[  112] Training loss: 0.19464148, Validation loss: 0.19550587, Gradient norm: 0.65636997
INFO:root:[  113] Training loss: 0.19503455, Validation loss: 0.19467049, Gradient norm: 1.59383470
INFO:root:[  114] Training loss: 0.19473825, Validation loss: 0.19580404, Gradient norm: 1.88230000
INFO:root:[  115] Training loss: 0.19335823, Validation loss: 0.19412598, Gradient norm: 1.88306763
INFO:root:[  116] Training loss: 0.19319768, Validation loss: 0.19261857, Gradient norm: 1.21159449
INFO:root:[  117] Training loss: 0.19263932, Validation loss: 0.19234614, Gradient norm: 1.55522184
INFO:root:[  118] Training loss: 0.19147853, Validation loss: 0.19164559, Gradient norm: 1.27782494
INFO:root:[  119] Training loss: 0.19056267, Validation loss: 0.19063502, Gradient norm: 1.07764310
INFO:root:[  120] Training loss: 0.18980565, Validation loss: 0.18894796, Gradient norm: 1.31270960
INFO:root:[  121] Training loss: 0.18900492, Validation loss: 0.18893629, Gradient norm: 0.83817514
INFO:root:[  122] Training loss: 0.18961054, Validation loss: 0.18886936, Gradient norm: 1.82756628
INFO:root:[  123] Training loss: 0.18817552, Validation loss: 0.18821401, Gradient norm: 1.06064975
INFO:root:[  124] Training loss: 0.18719392, Validation loss: 0.18791157, Gradient norm: 0.72870900
INFO:root:[  125] Training loss: 0.18755231, Validation loss: 0.18849072, Gradient norm: 1.45045502
INFO:root:[  126] Training loss: 0.18695665, Validation loss: 0.18771740, Gradient norm: 2.07512572
INFO:root:[  127] Training loss: 0.18572140, Validation loss: 0.18435451, Gradient norm: 1.38992881
INFO:root:[  128] Training loss: 0.18542123, Validation loss: 0.18544624, Gradient norm: 1.43093821
INFO:root:[  129] Training loss: 0.18435154, Validation loss: 0.18391748, Gradient norm: 1.68343869
INFO:root:[  130] Training loss: 0.18382165, Validation loss: 0.18380928, Gradient norm: 1.28467854
INFO:root:[  131] Training loss: 0.18315103, Validation loss: 0.18360654, Gradient norm: 1.14336030
INFO:root:[  132] Training loss: 0.18270686, Validation loss: 0.18241490, Gradient norm: 1.09546834
INFO:root:[  133] Training loss: 0.18292755, Validation loss: 0.18287854, Gradient norm: 1.60283615
INFO:root:[  134] Training loss: 0.18176872, Validation loss: 0.18148535, Gradient norm: 0.84452640
INFO:root:[  135] Training loss: 0.18071130, Validation loss: 0.18069328, Gradient norm: 0.71502909
INFO:root:[  136] Training loss: 0.18014118, Validation loss: 0.17913059, Gradient norm: 0.98935420
INFO:root:[  137] Training loss: 0.17952874, Validation loss: 0.17955116, Gradient norm: 0.66845798
INFO:root:[  138] Training loss: 0.17936775, Validation loss: 0.17863192, Gradient norm: 2.47160516
INFO:root:[  139] Training loss: 0.17828673, Validation loss: 0.17907057, Gradient norm: 1.83740480
INFO:root:[  140] Training loss: 0.17781850, Validation loss: 0.17785884, Gradient norm: 1.97082074
INFO:root:[  141] Training loss: 0.17697257, Validation loss: 0.17737148, Gradient norm: 1.28023421
INFO:root:[  142] Training loss: 0.17676998, Validation loss: 0.17593640, Gradient norm: 1.03601996
INFO:root:[  143] Training loss: 0.17631640, Validation loss: 0.17715312, Gradient norm: 1.69407875
INFO:root:[  144] Training loss: 0.17609216, Validation loss: 0.17524995, Gradient norm: 2.37336196
INFO:root:[  145] Training loss: 0.17502351, Validation loss: 0.17426643, Gradient norm: 1.87680464
INFO:root:[  146] Training loss: 0.17465029, Validation loss: 0.17405147, Gradient norm: 1.34200449
INFO:root:[  147] Training loss: 0.17394084, Validation loss: 0.17387335, Gradient norm: 1.66886372
INFO:root:[  148] Training loss: 0.17272615, Validation loss: 0.17275734, Gradient norm: 0.67695759
INFO:root:[  149] Training loss: 0.17189439, Validation loss: 0.17185648, Gradient norm: 1.14568510
INFO:root:[  150] Training loss: 0.17241002, Validation loss: 0.17198046, Gradient norm: 1.01730033
INFO:root:[  151] Training loss: 0.17173174, Validation loss: 0.17123349, Gradient norm: 1.98771363
INFO:root:[  152] Training loss: 0.17119380, Validation loss: 0.17059840, Gradient norm: 1.87894207
INFO:root:[  153] Training loss: 0.16990859, Validation loss: 0.16962785, Gradient norm: 1.22074909
INFO:root:[  154] Training loss: 0.16997661, Validation loss: 0.17031549, Gradient norm: 1.00034444
INFO:root:[  155] Training loss: 0.16913524, Validation loss: 0.16919416, Gradient norm: 1.09734217
INFO:root:[  156] Training loss: 0.16857835, Validation loss: 0.16889964, Gradient norm: 1.93228121
INFO:root:[  157] Training loss: 0.16869366, Validation loss: 0.16689477, Gradient norm: 2.41584836
INFO:root:[  158] Training loss: 0.16743302, Validation loss: 0.16600726, Gradient norm: 1.41991920
INFO:root:[  159] Training loss: 0.16685005, Validation loss: 0.16719875, Gradient norm: 1.46239363
INFO:root:[  160] Training loss: 0.16679744, Validation loss: 0.16684446, Gradient norm: 2.63437140
INFO:root:[  161] Training loss: 0.16621180, Validation loss: 0.16556310, Gradient norm: 2.39717429
INFO:root:[  162] Training loss: 0.16511356, Validation loss: 0.16557645, Gradient norm: 1.39625560
INFO:root:[  163] Training loss: 0.16456506, Validation loss: 0.16416981, Gradient norm: 1.55950440
INFO:root:[  164] Training loss: 0.16471214, Validation loss: 0.16426962, Gradient norm: 1.44803120
INFO:root:[  165] Training loss: 0.16373724, Validation loss: 0.16497985, Gradient norm: 1.75278375
INFO:root:[  166] Training loss: 0.16371945, Validation loss: 0.16265627, Gradient norm: 2.62356388
INFO:root:[  167] Training loss: 0.16264773, Validation loss: 0.16230819, Gradient norm: 1.72270168
INFO:root:[  168] Training loss: 0.16235744, Validation loss: 0.16190797, Gradient norm: 1.80783694
INFO:root:[  169] Training loss: 0.16176138, Validation loss: 0.16182791, Gradient norm: 1.74839021
INFO:root:[  170] Training loss: 0.16105083, Validation loss: 0.16115298, Gradient norm: 1.74404174
INFO:root:[  171] Training loss: 0.16081390, Validation loss: 0.16066101, Gradient norm: 1.76720618
INFO:root:[  172] Training loss: 0.15992387, Validation loss: 0.16048304, Gradient norm: 1.31387018
INFO:root:[  173] Training loss: 0.15990200, Validation loss: 0.16001532, Gradient norm: 2.24824047
INFO:root:[  174] Training loss: 0.15893671, Validation loss: 0.15977457, Gradient norm: 2.53503951
INFO:root:[  175] Training loss: 0.15880743, Validation loss: 0.15835761, Gradient norm: 1.80177088
INFO:root:[  176] Training loss: 0.15805890, Validation loss: 0.15822406, Gradient norm: 2.34754575
INFO:root:[  177] Training loss: 0.15759778, Validation loss: 0.15795763, Gradient norm: 2.31982802
INFO:root:[  178] Training loss: 0.15722193, Validation loss: 0.15749659, Gradient norm: 1.25602568
INFO:root:[  179] Training loss: 0.15657558, Validation loss: 0.15698755, Gradient norm: 1.68928688
INFO:root:[  180] Training loss: 0.15588932, Validation loss: 0.15624004, Gradient norm: 1.94808753
INFO:root:[  181] Training loss: 0.15561832, Validation loss: 0.15588889, Gradient norm: 1.53097454
INFO:root:[  182] Training loss: 0.15556700, Validation loss: 0.15438759, Gradient norm: 2.41825368
INFO:root:[  183] Training loss: 0.15539104, Validation loss: 0.15610968, Gradient norm: 1.69178117
INFO:root:[  184] Training loss: 0.15473036, Validation loss: 0.15488391, Gradient norm: 2.55156431
INFO:root:[  185] Training loss: 0.15353237, Validation loss: 0.15362478, Gradient norm: 1.68443661
INFO:root:[  186] Training loss: 0.15289274, Validation loss: 0.15337193, Gradient norm: 1.41511403
INFO:root:[  187] Training loss: 0.15247889, Validation loss: 0.15305366, Gradient norm: 1.46130932
INFO:root:[  188] Training loss: 0.15213773, Validation loss: 0.15242620, Gradient norm: 1.75038754
INFO:root:[  189] Training loss: 0.15188153, Validation loss: 0.15167856, Gradient norm: 1.88002747
INFO:root:[  190] Training loss: 0.15086304, Validation loss: 0.15185043, Gradient norm: 1.75885966
INFO:root:[  191] Training loss: 0.15023197, Validation loss: 0.15017973, Gradient norm: 1.28470190
INFO:root:[  192] Training loss: 0.14989195, Validation loss: 0.15021985, Gradient norm: 2.00919079
INFO:root:[  193] Training loss: 0.14951503, Validation loss: 0.14987074, Gradient norm: 2.43850717
INFO:root:[  194] Training loss: 0.14882458, Validation loss: 0.14868633, Gradient norm: 2.08310828
INFO:root:[  195] Training loss: 0.14884378, Validation loss: 0.14927657, Gradient norm: 2.54521299
INFO:root:[  196] Training loss: 0.14753316, Validation loss: 0.14899386, Gradient norm: 2.17267894
INFO:root:[  197] Training loss: 0.14727434, Validation loss: 0.14864524, Gradient norm: 2.16510926
INFO:root:[  198] Training loss: 0.14770748, Validation loss: 0.14679342, Gradient norm: 3.36457260
INFO:root:[  199] Training loss: 0.14662662, Validation loss: 0.14647065, Gradient norm: 1.97635101
INFO:root:[  200] Training loss: 0.14608036, Validation loss: 0.14634417, Gradient norm: 2.23971619
INFO:root:[  201] Training loss: 0.14607453, Validation loss: 0.14570692, Gradient norm: 2.69324335
INFO:root:[  202] Training loss: 0.14536010, Validation loss: 0.14510650, Gradient norm: 2.43519766
INFO:root:[  203] Training loss: 0.14489394, Validation loss: 0.14404178, Gradient norm: 2.24503885
INFO:root:[  204] Training loss: 0.14439797, Validation loss: 0.14385398, Gradient norm: 2.44846278
INFO:root:[  205] Training loss: 0.14399137, Validation loss: 0.14388887, Gradient norm: 2.45574939
INFO:root:[  206] Training loss: 0.14345314, Validation loss: 0.14444159, Gradient norm: 1.63928930
INFO:root:[  207] Training loss: 0.14323509, Validation loss: 0.14372348, Gradient norm: 3.11026403
INFO:root:[  208] Training loss: 0.14264816, Validation loss: 0.14214476, Gradient norm: 1.70117445
INFO:root:[  209] Training loss: 0.14178655, Validation loss: 0.14177392, Gradient norm: 2.66413664
INFO:root:[  210] Training loss: 0.14160752, Validation loss: 0.14116999, Gradient norm: 1.36008049
INFO:root:[  211] Training loss: 0.14070571, Validation loss: 0.14078276, Gradient norm: 1.68056194
INFO:root:[  212] Training loss: 0.14027060, Validation loss: 0.14054658, Gradient norm: 2.36513664
INFO:root:[  213] Training loss: 0.13993638, Validation loss: 0.13991325, Gradient norm: 2.70923905
INFO:root:[  214] Training loss: 0.13985601, Validation loss: 0.13973447, Gradient norm: 2.56939526
INFO:root:[  215] Training loss: 0.13902276, Validation loss: 0.13863344, Gradient norm: 1.87262561
INFO:root:[  216] Training loss: 0.13888232, Validation loss: 0.13928982, Gradient norm: 2.12589398
INFO:root:[  217] Training loss: 0.13858917, Validation loss: 0.13882662, Gradient norm: 2.24914868
INFO:root:[  218] Training loss: 0.13818340, Validation loss: 0.13774384, Gradient norm: 2.16758464
INFO:root:[  219] Training loss: 0.13766655, Validation loss: 0.13639344, Gradient norm: 3.56093688
INFO:root:[  220] Training loss: 0.13705854, Validation loss: 0.13685640, Gradient norm: 1.98984040
INFO:root:[  221] Training loss: 0.13618260, Validation loss: 0.13581182, Gradient norm: 2.78784326
INFO:root:[  222] Training loss: 0.13583531, Validation loss: 0.13552329, Gradient norm: 2.45637360
INFO:root:[  223] Training loss: 0.13565532, Validation loss: 0.13521947, Gradient norm: 2.43291060
INFO:root:[  224] Training loss: 0.13506729, Validation loss: 0.13553174, Gradient norm: 2.49711984
INFO:root:[  225] Training loss: 0.13492841, Validation loss: 0.13470752, Gradient norm: 3.42684814
INFO:root:[  226] Training loss: 0.13408019, Validation loss: 0.13447599, Gradient norm: 2.23830585
INFO:root:[  227] Training loss: 0.13393308, Validation loss: 0.13408766, Gradient norm: 3.20879637
INFO:root:[  228] Training loss: 0.13330813, Validation loss: 0.13414772, Gradient norm: 2.41716450
INFO:root:[  229] Training loss: 0.13266600, Validation loss: 0.13250894, Gradient norm: 2.57855437
INFO:root:[  230] Training loss: 0.13246661, Validation loss: 0.13202809, Gradient norm: 2.16030100
INFO:root:[  231] Training loss: 0.13199861, Validation loss: 0.13161397, Gradient norm: 2.36763669
INFO:root:[  232] Training loss: 0.13151343, Validation loss: 0.13186762, Gradient norm: 2.19076789
INFO:root:[  233] Training loss: 0.13073915, Validation loss: 0.13100486, Gradient norm: 2.46313888
INFO:root:[  234] Training loss: 0.13079288, Validation loss: 0.13118772, Gradient norm: 2.69917274
INFO:root:[  235] Training loss: 0.13024447, Validation loss: 0.13014652, Gradient norm: 3.11136067
INFO:root:[  236] Training loss: 0.12958597, Validation loss: 0.13013171, Gradient norm: 2.76777341
INFO:root:[  237] Training loss: 0.12917073, Validation loss: 0.12888650, Gradient norm: 2.73862919
INFO:root:[  238] Training loss: 0.12836709, Validation loss: 0.12906728, Gradient norm: 3.28437676
INFO:root:[  239] Training loss: 0.12862507, Validation loss: 0.12842307, Gradient norm: 3.42391040
INFO:root:[  240] Training loss: 0.12776625, Validation loss: 0.12759527, Gradient norm: 2.62651081
INFO:root:[  241] Training loss: 0.12702302, Validation loss: 0.12814478, Gradient norm: 2.62202535
INFO:root:[  242] Training loss: 0.12710045, Validation loss: 0.12701325, Gradient norm: 2.03029137
INFO:root:[  243] Training loss: 0.12651768, Validation loss: 0.12665477, Gradient norm: 3.66450027
INFO:root:[  244] Training loss: 0.12596343, Validation loss: 0.12566141, Gradient norm: 2.46964199
INFO:root:[  245] Training loss: 0.12626209, Validation loss: 0.12654602, Gradient norm: 2.57196919
INFO:root:[  246] Training loss: 0.12552757, Validation loss: 0.12564204, Gradient norm: 4.52956038
INFO:root:[  247] Training loss: 0.12472728, Validation loss: 0.12465621, Gradient norm: 3.75121716
INFO:root:[  248] Training loss: 0.12405028, Validation loss: 0.12421507, Gradient norm: 3.06683475
INFO:root:[  249] Training loss: 0.12361604, Validation loss: 0.12384678, Gradient norm: 2.99965140
INFO:root:[  250] Training loss: 0.12315534, Validation loss: 0.12420658, Gradient norm: 2.68725603
INFO:root:[  251] Training loss: 0.12315454, Validation loss: 0.12271535, Gradient norm: 3.69768565
INFO:root:[  252] Training loss: 0.12312470, Validation loss: 0.12347015, Gradient norm: 5.09776432
INFO:root:[  253] Training loss: 0.12244197, Validation loss: 0.12280868, Gradient norm: 3.69387367
INFO:root:[  254] Training loss: 0.12146506, Validation loss: 0.12183517, Gradient norm: 4.01252194
INFO:root:[  255] Training loss: 0.12143338, Validation loss: 0.12161292, Gradient norm: 3.82653491
INFO:root:[  256] Training loss: 0.12099410, Validation loss: 0.12091135, Gradient norm: 3.85237584
INFO:root:[  257] Training loss: 0.12024668, Validation loss: 0.12082258, Gradient norm: 4.62640084
INFO:root:[  258] Training loss: 0.12054297, Validation loss: 0.12067347, Gradient norm: 4.02360537
INFO:root:[  259] Training loss: 0.11973853, Validation loss: 0.12066311, Gradient norm: 3.35118688
INFO:root:[  260] Training loss: 0.12023932, Validation loss: 0.12057154, Gradient norm: 2.90045719
INFO:root:[  261] Training loss: 0.11899327, Validation loss: 0.11843258, Gradient norm: 4.18866286
INFO:root:[  262] Training loss: 0.11871566, Validation loss: 0.11858268, Gradient norm: 3.99522816
INFO:root:[  263] Training loss: 0.11778458, Validation loss: 0.11805541, Gradient norm: 4.98229326
INFO:root:[  264] Training loss: 0.11780203, Validation loss: 0.11818750, Gradient norm: 3.87695693
INFO:root:[  265] Training loss: 0.11708508, Validation loss: 0.11782772, Gradient norm: 4.34092874
INFO:root:[  266] Training loss: 0.11705519, Validation loss: 0.11676125, Gradient norm: 3.35132761
INFO:root:[  267] Training loss: 0.11670347, Validation loss: 0.11680886, Gradient norm: 3.93287154
INFO:root:[  268] Training loss: 0.11591374, Validation loss: 0.11584907, Gradient norm: 4.83554363
INFO:root:[  269] Training loss: 0.11615840, Validation loss: 0.11781592, Gradient norm: 4.62626656
INFO:root:[  270] Training loss: 0.11536863, Validation loss: 0.11564787, Gradient norm: 5.36869875
INFO:root:[  271] Training loss: 0.11486328, Validation loss: 0.11605562, Gradient norm: 5.74195503
INFO:root:[  272] Training loss: 0.11477366, Validation loss: 0.11467147, Gradient norm: 4.43689347
INFO:root:[  273] Training loss: 0.11488511, Validation loss: 0.11553428, Gradient norm: 6.27901633
INFO:root:[  274] Training loss: 0.11389736, Validation loss: 0.11417938, Gradient norm: 5.26780486
INFO:root:[  275] Training loss: 0.11362488, Validation loss: 0.11428537, Gradient norm: 3.11741108
INFO:root:[  276] Training loss: 0.11345615, Validation loss: 0.11279975, Gradient norm: 3.40013476
INFO:root:[  277] Training loss: 0.11289013, Validation loss: 0.11309579, Gradient norm: 3.79266815
INFO:root:[  278] Training loss: 0.11223193, Validation loss: 0.11280259, Gradient norm: 4.19099332
INFO:root:[  279] Training loss: 0.11199982, Validation loss: 0.11196068, Gradient norm: 2.51581565
INFO:root:[  280] Training loss: 0.11152286, Validation loss: 0.11194987, Gradient norm: 5.56350522
INFO:root:[  281] Training loss: 0.11117305, Validation loss: 0.11112163, Gradient norm: 4.63853523
INFO:root:[  282] Training loss: 0.11045643, Validation loss: 0.11042432, Gradient norm: 5.66539822
INFO:root:[  283] Training loss: 0.11032264, Validation loss: 0.11064869, Gradient norm: 6.07253003
INFO:root:[  284] Training loss: 0.11038236, Validation loss: 0.11053442, Gradient norm: 6.10356943
INFO:root:[  285] Training loss: 0.11004406, Validation loss: 0.11000723, Gradient norm: 6.04327415
INFO:root:[  286] Training loss: 0.10914886, Validation loss: 0.10857357, Gradient norm: 5.31331457
INFO:root:[  287] Training loss: 0.10881133, Validation loss: 0.11026212, Gradient norm: 5.21780812
INFO:root:[  288] Training loss: 0.10904766, Validation loss: 0.10822971, Gradient norm: 5.21986049
INFO:root:[  289] Training loss: 0.10839668, Validation loss: 0.10805230, Gradient norm: 5.04838717
INFO:root:[  290] Training loss: 0.10792311, Validation loss: 0.10820918, Gradient norm: 5.49318733
INFO:root:[  291] Training loss: 0.10770131, Validation loss: 0.10743559, Gradient norm: 7.46949751
INFO:root:[  292] Training loss: 0.10757311, Validation loss: 0.10728302, Gradient norm: 5.97299255
INFO:root:[  293] Training loss: 0.10743659, Validation loss: 0.10682669, Gradient norm: 7.60359257
INFO:root:[  294] Training loss: 0.10702900, Validation loss: 0.10700451, Gradient norm: 3.79152703
INFO:root:[  295] Training loss: 0.10635274, Validation loss: 0.10641090, Gradient norm: 4.77521996
INFO:root:[  296] Training loss: 0.10579217, Validation loss: 0.10609338, Gradient norm: 4.55131287
INFO:root:[  297] Training loss: 0.10564392, Validation loss: 0.10612079, Gradient norm: 4.52247160
INFO:root:[  298] Training loss: 0.10492401, Validation loss: 0.10516546, Gradient norm: 4.03281798
INFO:root:[  299] Training loss: 0.10487809, Validation loss: 0.10473639, Gradient norm: 6.72444819
INFO:root:[  300] Training loss: 0.10413870, Validation loss: 0.10443070, Gradient norm: 5.50675275
INFO:root:[  301] Training loss: 0.10428100, Validation loss: 0.10404203, Gradient norm: 5.72912708
INFO:root:[  302] Training loss: 0.10385038, Validation loss: 0.10331931, Gradient norm: 6.90986666
INFO:root:[  303] Training loss: 0.10284728, Validation loss: 0.10305231, Gradient norm: 6.93723373
INFO:root:[  304] Training loss: 0.10289893, Validation loss: 0.10399595, Gradient norm: 7.72926649
INFO:root:[  305] Training loss: 0.10267008, Validation loss: 0.10245432, Gradient norm: 6.79851121
INFO:root:[  306] Training loss: 0.10232720, Validation loss: 0.10189846, Gradient norm: 4.69191155
INFO:root:[  307] Training loss: 0.10226420, Validation loss: 0.10320881, Gradient norm: 7.47937793
INFO:root:[  308] Training loss: 0.10223301, Validation loss: 0.10089193, Gradient norm: 6.89273312
INFO:root:[  309] Training loss: 0.10111605, Validation loss: 0.10272997, Gradient norm: 6.82949009
INFO:root:[  310] Training loss: 0.10079862, Validation loss: 0.10096946, Gradient norm: 6.07121302
INFO:root:[  311] Training loss: 0.10008411, Validation loss: 0.10022978, Gradient norm: 3.48321406
INFO:root:[  312] Training loss: 0.10069479, Validation loss: 0.10086162, Gradient norm: 7.30432074
INFO:root:[  313] Training loss: 0.09990265, Validation loss: 0.10079400, Gradient norm: 5.65201877
INFO:root:[  314] Training loss: 0.09935686, Validation loss: 0.09872365, Gradient norm: 7.62740670
INFO:root:[  315] Training loss: 0.09873120, Validation loss: 0.09958353, Gradient norm: 7.03792255
INFO:root:[  316] Training loss: 0.09905366, Validation loss: 0.09875515, Gradient norm: 6.23090657
INFO:root:[  317] Training loss: 0.09835684, Validation loss: 0.09903037, Gradient norm: 6.66653902
INFO:root:[  318] Training loss: 0.09831066, Validation loss: 0.09769998, Gradient norm: 9.29869923
INFO:root:[  319] Training loss: 0.09776489, Validation loss: 0.09834684, Gradient norm: 4.82354173
INFO:root:[  320] Training loss: 0.09783097, Validation loss: 0.09826497, Gradient norm: 4.34345329
INFO:root:[  321] Training loss: 0.09729121, Validation loss: 0.09686633, Gradient norm: 5.07143592
INFO:root:[  322] Training loss: 0.09697736, Validation loss: 0.09667653, Gradient norm: 7.63702318
INFO:root:[  323] Training loss: 0.09647634, Validation loss: 0.09628138, Gradient norm: 7.33944364
INFO:root:[  324] Training loss: 0.09577996, Validation loss: 0.09622282, Gradient norm: 3.56502162
INFO:root:[  325] Training loss: 0.09595506, Validation loss: 0.09516358, Gradient norm: 6.82433191
INFO:root:[  326] Training loss: 0.09523300, Validation loss: 0.09450577, Gradient norm: 5.68592870
INFO:root:[  327] Training loss: 0.09473061, Validation loss: 0.09483674, Gradient norm: 3.99375622
INFO:root:[  328] Training loss: 0.09455266, Validation loss: 0.09502422, Gradient norm: 5.55327263
INFO:root:[  329] Training loss: 0.09446214, Validation loss: 0.09497807, Gradient norm: 6.78207524
INFO:root:[  330] Training loss: 0.09412194, Validation loss: 0.09598652, Gradient norm: 7.39694076
INFO:root:[  331] Training loss: 0.09470964, Validation loss: 0.09399102, Gradient norm: 10.91078487
INFO:root:[  332] Training loss: 0.09339421, Validation loss: 0.09377196, Gradient norm: 6.05914970
INFO:root:[  333] Training loss: 0.09301929, Validation loss: 0.09337023, Gradient norm: 5.55467754
INFO:root:[  334] Training loss: 0.09250552, Validation loss: 0.09374246, Gradient norm: 7.23400899
INFO:root:[  335] Training loss: 0.09278182, Validation loss: 0.09268173, Gradient norm: 6.34744765
INFO:root:[  336] Training loss: 0.09272263, Validation loss: 0.09238520, Gradient norm: 10.30723256
INFO:root:[  337] Training loss: 0.09220525, Validation loss: 0.09294329, Gradient norm: 8.17495948
INFO:root:[  338] Training loss: 0.09202691, Validation loss: 0.09159271, Gradient norm: 12.95881881
INFO:root:[  339] Training loss: 0.09148714, Validation loss: 0.09151283, Gradient norm: 8.08919392
INFO:root:[  340] Training loss: 0.09118554, Validation loss: 0.09116240, Gradient norm: 6.76215352
INFO:root:[  341] Training loss: 0.09068237, Validation loss: 0.09072573, Gradient norm: 7.79856127
INFO:root:[  342] Training loss: 0.09090829, Validation loss: 0.09035755, Gradient norm: 11.53652004
INFO:root:[  343] Training loss: 0.09003946, Validation loss: 0.09076768, Gradient norm: 7.96361644
INFO:root:[  344] Training loss: 0.08984667, Validation loss: 0.09051913, Gradient norm: 6.28402455
INFO:root:[  345] Training loss: 0.08977402, Validation loss: 0.08993763, Gradient norm: 6.45797481
INFO:root:[  346] Training loss: 0.08913640, Validation loss: 0.09080912, Gradient norm: 7.70575937
INFO:root:[  347] Training loss: 0.08967314, Validation loss: 0.08892528, Gradient norm: 12.89489035
INFO:root:[  348] Training loss: 0.08892097, Validation loss: 0.08939912, Gradient norm: 8.49004391
INFO:root:[  349] Training loss: 0.08906408, Validation loss: 0.08868245, Gradient norm: 9.90617106
INFO:root:[  350] Training loss: 0.08841557, Validation loss: 0.08807039, Gradient norm: 7.08077317
INFO:root:[  351] Training loss: 0.08822469, Validation loss: 0.08989763, Gradient norm: 6.41398640
INFO:root:[  352] Training loss: 0.08772844, Validation loss: 0.08781433, Gradient norm: 6.66210706
INFO:root:[  353] Training loss: 0.08790752, Validation loss: 0.08755570, Gradient norm: 9.36867143
INFO:root:[  354] Training loss: 0.08744217, Validation loss: 0.08770754, Gradient norm: 7.20250243
INFO:root:[  355] Training loss: 0.08724586, Validation loss: 0.08761119, Gradient norm: 8.27965190
INFO:root:[  356] Training loss: 0.08692473, Validation loss: 0.08773175, Gradient norm: 8.87782422
INFO:root:[  357] Training loss: 0.08709686, Validation loss: 0.08657883, Gradient norm: 10.75059436
INFO:root:[  358] Training loss: 0.08642968, Validation loss: 0.08675783, Gradient norm: 9.45502807
INFO:root:[  359] Training loss: 0.08640721, Validation loss: 0.08726762, Gradient norm: 7.91370065
INFO:root:[  360] Training loss: 0.08601184, Validation loss: 0.08626290, Gradient norm: 8.67948751
INFO:root:[  361] Training loss: 0.08552829, Validation loss: 0.08612871, Gradient norm: 9.20779543
INFO:root:[  362] Training loss: 0.08545766, Validation loss: 0.08552315, Gradient norm: 8.82464280
INFO:root:[  363] Training loss: 0.08539249, Validation loss: 0.08480487, Gradient norm: 9.86082868
INFO:root:[  364] Training loss: 0.08480820, Validation loss: 0.08472840, Gradient norm: 8.88083937
INFO:root:[  365] Training loss: 0.08492542, Validation loss: 0.08571160, Gradient norm: 7.34592539
INFO:root:[  366] Training loss: 0.08484196, Validation loss: 0.08470082, Gradient norm: 10.87459173
INFO:root:[  367] Training loss: 0.08465392, Validation loss: 0.08485309, Gradient norm: 9.85123066
INFO:root:[  368] Training loss: 0.08391402, Validation loss: 0.08498254, Gradient norm: 7.53455182
INFO:root:[  369] Training loss: 0.08414878, Validation loss: 0.08413616, Gradient norm: 9.51760725
INFO:root:[  370] Training loss: 0.08391051, Validation loss: 0.08446795, Gradient norm: 11.05474631
INFO:root:[  371] Training loss: 0.08323225, Validation loss: 0.08351952, Gradient norm: 9.14488467
INFO:root:[  372] Training loss: 0.08308549, Validation loss: 0.08295226, Gradient norm: 8.27290674
INFO:root:[  373] Training loss: 0.08303461, Validation loss: 0.08291470, Gradient norm: 8.84303865
INFO:root:[  374] Training loss: 0.08263532, Validation loss: 0.08440133, Gradient norm: 7.12152694
INFO:root:[  375] Training loss: 0.08338851, Validation loss: 0.08256365, Gradient norm: 8.57571183
INFO:root:[  376] Training loss: 0.08267756, Validation loss: 0.08261712, Gradient norm: 9.43309281
INFO:root:[  377] Training loss: 0.08215741, Validation loss: 0.08205078, Gradient norm: 12.93680024
INFO:root:[  378] Training loss: 0.08281090, Validation loss: 0.08167367, Gradient norm: 19.30649145
INFO:root:[  379] Training loss: 0.08165279, Validation loss: 0.08312485, Gradient norm: 10.42895612
INFO:root:[  380] Training loss: 0.08154304, Validation loss: 0.08117279, Gradient norm: 13.01797118
INFO:root:[  381] Training loss: 0.08129753, Validation loss: 0.08137432, Gradient norm: 7.84853730
INFO:root:[  382] Training loss: 0.08123565, Validation loss: 0.08062288, Gradient norm: 11.10846305
INFO:root:[  383] Training loss: 0.08093285, Validation loss: 0.08147797, Gradient norm: 11.09891287
INFO:root:[  384] Training loss: 0.08077589, Validation loss: 0.08136368, Gradient norm: 15.30759431
INFO:root:[  385] Training loss: 0.08050633, Validation loss: 0.08121753, Gradient norm: 9.35247324
INFO:root:[  386] Training loss: 0.07995744, Validation loss: 0.07985027, Gradient norm: 5.39561037
INFO:root:[  387] Training loss: 0.07992472, Validation loss: 0.08044331, Gradient norm: 7.73211740
INFO:root:[  388] Training loss: 0.07993482, Validation loss: 0.08013779, Gradient norm: 13.40001570
INFO:root:[  389] Training loss: 0.08003609, Validation loss: 0.07984384, Gradient norm: 15.45826251
INFO:root:[  390] Training loss: 0.07928415, Validation loss: 0.08039650, Gradient norm: 9.47717472
INFO:root:[  391] Training loss: 0.07930524, Validation loss: 0.08010563, Gradient norm: 9.71113825
INFO:root:[  392] Training loss: 0.07914623, Validation loss: 0.07886702, Gradient norm: 13.21984233
INFO:root:[  393] Training loss: 0.07863515, Validation loss: 0.07898000, Gradient norm: 12.15860760
INFO:root:[  394] Training loss: 0.07844752, Validation loss: 0.07910498, Gradient norm: 8.53890897
INFO:root:[  395] Training loss: 0.07825740, Validation loss: 0.07825000, Gradient norm: 9.77494902
INFO:root:[  396] Training loss: 0.07802511, Validation loss: 0.07802435, Gradient norm: 7.60723633
INFO:root:[  397] Training loss: 0.07815172, Validation loss: 0.07978125, Gradient norm: 9.46371952
INFO:root:[  398] Training loss: 0.07805603, Validation loss: 0.07894668, Gradient norm: 12.63748821
INFO:root:[  399] Training loss: 0.07835241, Validation loss: 0.07795599, Gradient norm: 21.47552387
INFO:root:[  400] Training loss: 0.07742444, Validation loss: 0.07732236, Gradient norm: 11.03237567
INFO:root:[  401] Training loss: 0.07738581, Validation loss: 0.07822615, Gradient norm: 15.01843008
INFO:root:[  402] Training loss: 0.07784128, Validation loss: 0.07802093, Gradient norm: 21.01872276
INFO:root:[  403] Training loss: 0.07706287, Validation loss: 0.07714192, Gradient norm: 12.35218922
INFO:root:[  404] Training loss: 0.07660329, Validation loss: 0.07780070, Gradient norm: 8.84953075
INFO:root:[  405] Training loss: 0.07712729, Validation loss: 0.07710545, Gradient norm: 14.78592375
INFO:root:[  406] Training loss: 0.07695394, Validation loss: 0.07677560, Gradient norm: 14.73970795
INFO:root:[  407] Training loss: 0.07646079, Validation loss: 0.07563252, Gradient norm: 7.67518550
INFO:root:[  408] Training loss: 0.07616563, Validation loss: 0.07648671, Gradient norm: 11.43752843
INFO:root:[  409] Training loss: 0.07589683, Validation loss: 0.07605314, Gradient norm: 13.11782309
INFO:root:[  410] Training loss: 0.07635994, Validation loss: 0.07583546, Gradient norm: 18.35525390
INFO:root:[  411] Training loss: 0.07567324, Validation loss: 0.07616100, Gradient norm: 10.99529052
INFO:root:[  412] Training loss: 0.07540522, Validation loss: 0.07623417, Gradient norm: 7.32164928
INFO:root:[  413] Training loss: 0.07571390, Validation loss: 0.07559294, Gradient norm: 14.45574601
INFO:root:[  414] Training loss: 0.07528161, Validation loss: 0.07718440, Gradient norm: 12.22526737
INFO:root:[  415] Training loss: 0.07570755, Validation loss: 0.07836452, Gradient norm: 19.91947382
INFO:root:[  416] Training loss: 0.07547363, Validation loss: 0.07510669, Gradient norm: 21.12733987
INFO:root:[  417] Training loss: 0.07469582, Validation loss: 0.07499852, Gradient norm: 11.95171447
INFO:root:[  418] Training loss: 0.07526788, Validation loss: 0.07510145, Gradient norm: 19.92752234
INFO:root:[  419] Training loss: 0.07466238, Validation loss: 0.07474719, Gradient norm: 12.93860181
INFO:root:[  420] Training loss: 0.07453749, Validation loss: 0.07448257, Gradient norm: 15.47962124
INFO:root:[  421] Training loss: 0.07458235, Validation loss: 0.07505015, Gradient norm: 18.01727800
INFO:root:[  422] Training loss: 0.07411046, Validation loss: 0.07448357, Gradient norm: 13.00750342
INFO:root:[  423] Training loss: 0.07364997, Validation loss: 0.07405498, Gradient norm: 9.70529175
INFO:root:[  424] Training loss: 0.07397829, Validation loss: 0.07389348, Gradient norm: 12.99384488
INFO:root:[  425] Training loss: 0.07393692, Validation loss: 0.07382153, Gradient norm: 16.71156102
INFO:root:[  426] Training loss: 0.07391169, Validation loss: 0.07431440, Gradient norm: 10.65535702
INFO:root:[  427] Training loss: 0.07390300, Validation loss: 0.07309540, Gradient norm: 19.61315192
INFO:root:[  428] Training loss: 0.07322991, Validation loss: 0.07462514, Gradient norm: 10.76276546
INFO:root:[  429] Training loss: 0.07305387, Validation loss: 0.07342863, Gradient norm: 12.30384344
INFO:root:[  430] Training loss: 0.07322113, Validation loss: 0.07438722, Gradient norm: 12.53185661
INFO:root:[  431] Training loss: 0.07326003, Validation loss: 0.07556767, Gradient norm: 16.69328901
INFO:root:[  432] Training loss: 0.07389197, Validation loss: 0.07326726, Gradient norm: 23.00095916
INFO:root:[  433] Training loss: 0.07338723, Validation loss: 0.07335860, Gradient norm: 22.12742725
INFO:root:[  434] Training loss: 0.07276915, Validation loss: 0.07265397, Gradient norm: 10.30839980
INFO:root:[  435] Training loss: 0.07294120, Validation loss: 0.07572418, Gradient norm: 18.26906058
INFO:root:[  436] Training loss: 0.07275592, Validation loss: 0.07292149, Gradient norm: 16.60631831
INFO:root:[  437] Training loss: 0.07334803, Validation loss: 0.07435518, Gradient norm: 27.49077038
INFO:root:[  438] Training loss: 0.07273252, Validation loss: 0.07261559, Gradient norm: 22.78899115
INFO:root:[  439] Training loss: 0.07242402, Validation loss: 0.07235129, Gradient norm: 16.94065291
INFO:root:[  440] Training loss: 0.07213189, Validation loss: 0.07216510, Gradient norm: 15.47465256
INFO:root:[  441] Training loss: 0.07207939, Validation loss: 0.07219749, Gradient norm: 14.45408822
INFO:root:[  442] Training loss: 0.07238591, Validation loss: 0.07388352, Gradient norm: 21.38502591
INFO:root:[  443] Training loss: 0.07181257, Validation loss: 0.07150617, Gradient norm: 12.32824701
INFO:root:[  444] Training loss: 0.07172688, Validation loss: 0.07140184, Gradient norm: 21.70541326
INFO:root:[  445] Training loss: 0.07173643, Validation loss: 0.07242310, Gradient norm: 11.48729147
INFO:root:[  446] Training loss: 0.07227304, Validation loss: 0.07259529, Gradient norm: 25.19912155
INFO:root:[  447] Training loss: 0.07218236, Validation loss: 0.07115873, Gradient norm: 28.41473057
INFO:root:[  448] Training loss: 0.07133026, Validation loss: 0.07106978, Gradient norm: 16.74789727
INFO:root:[  449] Training loss: 0.07109924, Validation loss: 0.07121071, Gradient norm: 14.92100807
INFO:root:[  450] Training loss: 0.07116209, Validation loss: 0.07095741, Gradient norm: 16.42686652
INFO:root:[  451] Training loss: 0.07092846, Validation loss: 0.07084559, Gradient norm: 15.60152326
INFO:root:[  452] Training loss: 0.07093499, Validation loss: 0.07103894, Gradient norm: 20.21132749
INFO:root:[  453] Training loss: 0.07068635, Validation loss: 0.07083198, Gradient norm: 18.00138516
INFO:root:[  454] Training loss: 0.07127874, Validation loss: 0.07035780, Gradient norm: 25.85114632
INFO:root:[  455] Training loss: 0.07095854, Validation loss: 0.07120258, Gradient norm: 19.50837909
INFO:root:[  456] Training loss: 0.07010974, Validation loss: 0.07003189, Gradient norm: 10.96748609
INFO:root:[  457] Training loss: 0.07091777, Validation loss: 0.07101919, Gradient norm: 23.80308624
INFO:root:[  458] Training loss: 0.07028015, Validation loss: 0.07097406, Gradient norm: 18.82185676
INFO:root:[  459] Training loss: 0.07084012, Validation loss: 0.07101152, Gradient norm: 28.60377301
INFO:root:[  460] Training loss: 0.07087637, Validation loss: 0.07046600, Gradient norm: 29.94417846
INFO:root:[  461] Training loss: 0.07038865, Validation loss: 0.06968320, Gradient norm: 17.41700045
INFO:root:[  462] Training loss: 0.07015720, Validation loss: 0.07045439, Gradient norm: 23.47594556
INFO:root:[  463] Training loss: 0.06999465, Validation loss: 0.06981761, Gradient norm: 25.55382127
INFO:root:[  464] Training loss: 0.06957118, Validation loss: 0.07007810, Gradient norm: 13.53129826
INFO:root:[  465] Training loss: 0.06966107, Validation loss: 0.07265579, Gradient norm: 21.56695928
INFO:root:[  466] Training loss: 0.06970375, Validation loss: 0.06940593, Gradient norm: 18.37155862
INFO:root:[  467] Training loss: 0.06941669, Validation loss: 0.07023977, Gradient norm: 17.99489718
INFO:root:[  468] Training loss: 0.06964068, Validation loss: 0.06952305, Gradient norm: 26.27806755
INFO:root:[  469] Training loss: 0.06999511, Validation loss: 0.07011763, Gradient norm: 32.49852525
INFO:root:[  470] Training loss: 0.06927310, Validation loss: 0.06886848, Gradient norm: 15.76813979
INFO:root:[  471] Training loss: 0.06910192, Validation loss: 0.06964590, Gradient norm: 21.93781618
INFO:root:[  472] Training loss: 0.06931609, Validation loss: 0.06915515, Gradient norm: 26.65578503
INFO:root:[  473] Training loss: 0.06900805, Validation loss: 0.06883995, Gradient norm: 26.94866798
INFO:root:[  474] Training loss: 0.06862618, Validation loss: 0.06967618, Gradient norm: 21.71544932
INFO:root:[  475] Training loss: 0.06888985, Validation loss: 0.06899215, Gradient norm: 28.23454100
INFO:root:[  476] Training loss: 0.06874331, Validation loss: 0.06826591, Gradient norm: 28.64265784
INFO:root:[  477] Training loss: 0.07016525, Validation loss: 0.06842537, Gradient norm: 47.65562115
INFO:root:[  478] Training loss: 0.06890043, Validation loss: 0.06848813, Gradient norm: 27.51973016
INFO:root:[  479] Training loss: 0.06791019, Validation loss: 0.06819184, Gradient norm: 19.25086538
INFO:root:[  480] Training loss: 0.06801902, Validation loss: 0.06831275, Gradient norm: 16.01914685
INFO:root:[  481] Training loss: 0.06901034, Validation loss: 0.06824781, Gradient norm: 41.38649610
INFO:root:[  482] Training loss: 0.06816384, Validation loss: 0.06801198, Gradient norm: 28.29372399
INFO:root:[  483] Training loss: 0.06827799, Validation loss: 0.06916011, Gradient norm: 29.96196489
INFO:root:[  484] Training loss: 0.06851394, Validation loss: 0.06725539, Gradient norm: 33.89472889
INFO:root:[  485] Training loss: 0.06745906, Validation loss: 0.06840834, Gradient norm: 20.36739847
INFO:root:[  486] Training loss: 0.06945681, Validation loss: 0.06870699, Gradient norm: 51.66484894
INFO:root:[  487] Training loss: 0.06791606, Validation loss: 0.06810593, Gradient norm: 34.58173515
INFO:root:[  488] Training loss: 0.06721738, Validation loss: 0.06757315, Gradient norm: 27.12429864
INFO:root:[  489] Training loss: 0.06706216, Validation loss: 0.06807310, Gradient norm: 18.93266603
INFO:root:[  490] Training loss: 0.06731482, Validation loss: 0.06680265, Gradient norm: 30.08260417
INFO:root:[  491] Training loss: 0.06764044, Validation loss: 0.06682691, Gradient norm: 30.73247212
INFO:root:[  492] Training loss: 0.06761121, Validation loss: 0.06951270, Gradient norm: 38.13590835
INFO:root:[  493] Training loss: 0.06676396, Validation loss: 0.06718054, Gradient norm: 15.09154072
INFO:root:[  494] Training loss: 0.06719583, Validation loss: 0.06669158, Gradient norm: 31.30249912
INFO:root:[  495] Training loss: 0.06705493, Validation loss: 0.06718673, Gradient norm: 26.82336282
INFO:root:[  496] Training loss: 0.06683087, Validation loss: 0.06670147, Gradient norm: 32.67124829
INFO:root:[  497] Training loss: 0.06725535, Validation loss: 0.06760275, Gradient norm: 37.09881361
INFO:root:[  498] Training loss: 0.06735585, Validation loss: 0.06740586, Gradient norm: 39.93433951
INFO:root:[  499] Training loss: 0.06746326, Validation loss: 0.06718857, Gradient norm: 41.98448040
INFO:root:[  500] Training loss: 0.06663654, Validation loss: 0.06719158, Gradient norm: 31.83995487
INFO:root:[  501] Training loss: 0.06785941, Validation loss: 0.06673004, Gradient norm: 54.86198537
INFO:root:[  502] Training loss: 0.06636793, Validation loss: 0.06772924, Gradient norm: 29.15872541
INFO:root:[  503] Training loss: 0.06712395, Validation loss: 0.06661617, Gradient norm: 43.07302485
INFO:root:[  504] Training loss: 0.06657146, Validation loss: 0.06622601, Gradient norm: 40.02563457
INFO:root:[  505] Training loss: 0.06627892, Validation loss: 0.06856798, Gradient norm: 37.06215031
INFO:root:[  506] Training loss: 0.06720885, Validation loss: 0.06663896, Gradient norm: 49.43763025
INFO:root:[  507] Training loss: 0.06565748, Validation loss: 0.06607801, Gradient norm: 22.82461160
INFO:root:[  508] Training loss: 0.06583073, Validation loss: 0.06584242, Gradient norm: 28.56896223
INFO:root:[  509] Training loss: 0.06669758, Validation loss: 0.06591177, Gradient norm: 49.13058369
INFO:root:[  510] Training loss: 0.06630989, Validation loss: 0.06551291, Gradient norm: 39.65563218
INFO:root:[  511] Training loss: 0.06626433, Validation loss: 0.06705594, Gradient norm: 42.11482962
INFO:root:[  512] Training loss: 0.06648778, Validation loss: 0.06884382, Gradient norm: 53.92420953
INFO:root:[  513] Training loss: 0.06744025, Validation loss: 0.06848916, Gradient norm: 64.04393727
INFO:root:[  514] Training loss: 0.06556104, Validation loss: 0.06586952, Gradient norm: 35.25482798
INFO:root:[  515] Training loss: 0.06550521, Validation loss: 0.06591290, Gradient norm: 34.76934611
INFO:root:[  516] Training loss: 0.06636579, Validation loss: 0.06782297, Gradient norm: 48.91933102
INFO:root:[  517] Training loss: 0.06596486, Validation loss: 0.06621859, Gradient norm: 44.27608339
INFO:root:[  518] Training loss: 0.06608212, Validation loss: 0.06538285, Gradient norm: 49.43471312
INFO:root:[  519] Training loss: 0.06605415, Validation loss: 0.06468478, Gradient norm: 46.82356506
INFO:root:[  520] Training loss: 0.06534072, Validation loss: 0.06541141, Gradient norm: 38.10260045
INFO:root:[  521] Training loss: 0.06541522, Validation loss: 0.06538215, Gradient norm: 42.45011702
INFO:root:[  522] Training loss: 0.06656275, Validation loss: 0.06460663, Gradient norm: 58.07848516
INFO:root:[  523] Training loss: 0.06617138, Validation loss: 0.06765262, Gradient norm: 54.61345645
INFO:root:[  524] Training loss: 0.06575161, Validation loss: 0.06471102, Gradient norm: 48.09794808
INFO:root:[  525] Training loss: 0.06502286, Validation loss: 0.06468014, Gradient norm: 45.21285759
INFO:root:[  526] Training loss: 0.06556572, Validation loss: 0.06643158, Gradient norm: 55.81037735
INFO:root:[  527] Training loss: 0.06543535, Validation loss: 0.06518110, Gradient norm: 58.32048742
INFO:root:[  528] Training loss: 0.06508994, Validation loss: 0.06418764, Gradient norm: 45.57603091
INFO:root:[  529] Training loss: 0.06643426, Validation loss: 0.06568123, Gradient norm: 63.04519990
INFO:root:[  530] Training loss: 0.06433492, Validation loss: 0.06433169, Gradient norm: 32.95105190
INFO:root:[  531] Training loss: 0.06588342, Validation loss: 0.06423354, Gradient norm: 63.35525043
INFO:root:[  532] Training loss: 0.06462324, Validation loss: 0.06410391, Gradient norm: 42.74033240
INFO:root:[  533] Training loss: 0.06522353, Validation loss: 0.06420936, Gradient norm: 52.68090814
INFO:root:[  534] Training loss: 0.06525885, Validation loss: 0.06583149, Gradient norm: 58.56167299
INFO:root:[  535] Training loss: 0.06494669, Validation loss: 0.06575485, Gradient norm: 59.00261221
INFO:root:[  536] Training loss: 0.06561811, Validation loss: 0.06627865, Gradient norm: 69.28039873
INFO:root:[  537] Training loss: 0.06549284, Validation loss: 0.06623934, Gradient norm: 67.34879859
INFO:root:[  538] Training loss: 0.06502860, Validation loss: 0.06349414, Gradient norm: 57.78434413
INFO:root:[  539] Training loss: 0.06549747, Validation loss: 0.06403026, Gradient norm: 70.36218402
INFO:root:[  540] Training loss: 0.06405459, Validation loss: 0.06472836, Gradient norm: 43.62180979
INFO:root:[  541] Training loss: 0.06466186, Validation loss: 0.06509950, Gradient norm: 62.64172802
INFO:root:[  542] Training loss: 0.06432552, Validation loss: 0.06391437, Gradient norm: 54.90586388
INFO:root:[  543] Training loss: 0.06463040, Validation loss: 0.06347400, Gradient norm: 60.26845831
INFO:root:[  544] Training loss: 0.06487066, Validation loss: 0.06437936, Gradient norm: 69.65163892
INFO:root:[  545] Training loss: 0.06415858, Validation loss: 0.06821303, Gradient norm: 60.70584456
INFO:root:[  546] Training loss: 0.06476498, Validation loss: 0.06459533, Gradient norm: 68.51918023
INFO:root:[  547] Training loss: 0.06405212, Validation loss: 0.06487483, Gradient norm: 58.32127689
INFO:root:[  548] Training loss: 0.06426680, Validation loss: 0.06332117, Gradient norm: 69.30445608
INFO:root:[  549] Training loss: 0.06504637, Validation loss: 0.06371807, Gradient norm: 83.02852253
INFO:root:[  550] Training loss: 0.06314031, Validation loss: 0.06334380, Gradient norm: 38.28317179
INFO:root:[  551] Training loss: 0.06591463, Validation loss: 0.06443034, Gradient norm: 90.96665566
INFO:root:[  552] Training loss: 0.06356019, Validation loss: 0.06315840, Gradient norm: 64.67752460
INFO:root:[  553] Training loss: 0.06359461, Validation loss: 0.06294949, Gradient norm: 51.39655233
INFO:root:[  554] Training loss: 0.06498088, Validation loss: 0.06236270, Gradient norm: 83.93696240
INFO:root:[  555] Training loss: 0.06435107, Validation loss: 0.06450836, Gradient norm: 68.50796153
INFO:root:[  556] Training loss: 0.06315258, Validation loss: 0.06263271, Gradient norm: 62.44646309
INFO:root:[  557] Training loss: 0.06396140, Validation loss: 0.07475424, Gradient norm: 69.75806069
INFO:root:[  558] Training loss: 0.06418639, Validation loss: 0.06364595, Gradient norm: 79.01320438
INFO:root:[  559] Training loss: 0.06319802, Validation loss: 0.06226107, Gradient norm: 57.67551372
INFO:root:[  560] Training loss: 0.06414390, Validation loss: 0.06462517, Gradient norm: 73.62235410
INFO:root:[  561] Training loss: 0.06376624, Validation loss: 0.06266772, Gradient norm: 78.66268533
INFO:root:[  562] Training loss: 0.06367087, Validation loss: 0.06790963, Gradient norm: 79.29502569
INFO:root:[  563] Training loss: 0.06376548, Validation loss: 0.06259540, Gradient norm: 78.82107563
INFO:root:[  564] Training loss: 0.06430312, Validation loss: 0.07041972, Gradient norm: 77.53089747
INFO:root:[  565] Training loss: 0.06388687, Validation loss: 0.06278985, Gradient norm: 79.80072553
INFO:root:[  566] Training loss: 0.06376365, Validation loss: 0.06843654, Gradient norm: 77.44140385
INFO:root:[  567] Training loss: 0.06553937, Validation loss: 0.06368689, Gradient norm: 90.14516104
INFO:root:[  568] Training loss: 0.06308016, Validation loss: 0.06438088, Gradient norm: 72.38455013
INFO:root:EP 568: Early stopping
INFO:root:Training the model took 8976.337s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.40395
INFO:root:EnergyScoreTrain: 0.94262
INFO:root:CoverageTrain: 0.88985
INFO:root:IntervalWidthTrain: 0.02507
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.35217
INFO:root:EnergyScoreValidation: 0.90766
INFO:root:CoverageValidation: 0.88977
INFO:root:IntervalWidthValidation: 0.02526
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.10988
INFO:root:EnergyScoreTest: 0.74557
INFO:root:CoverageTest: 0.88725
INFO:root:IntervalWidthTest: 0.0246
INFO:root:###18 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1602224128
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.06923622, Validation loss: 1.13638945, Gradient norm: 9.20995165
INFO:root:[    2] Training loss: 1.05250617, Validation loss: 1.00196651, Gradient norm: 3.25338355
INFO:root:[    3] Training loss: 0.93546918, Validation loss: 0.91385515, Gradient norm: 2.28559338
INFO:root:[    4] Training loss: 0.87774302, Validation loss: 0.82102903, Gradient norm: 2.75749837
INFO:root:[    5] Training loss: 0.81980707, Validation loss: 0.81479109, Gradient norm: 2.18220313
INFO:root:[    6] Training loss: 0.78956687, Validation loss: 0.77762871, Gradient norm: 3.55904466
INFO:root:[    7] Training loss: 0.75881842, Validation loss: 0.73766598, Gradient norm: 3.17731919
INFO:root:[    8] Training loss: 0.72696940, Validation loss: 0.72143054, Gradient norm: 2.57382444
INFO:root:[    9] Training loss: 0.71040467, Validation loss: 0.69669783, Gradient norm: 3.49614200
INFO:root:[   10] Training loss: 0.69073334, Validation loss: 0.68332154, Gradient norm: 2.67796627
INFO:root:[   11] Training loss: 0.66298207, Validation loss: 0.66778387, Gradient norm: 2.97398242
INFO:root:[   12] Training loss: 0.65536163, Validation loss: 0.66587068, Gradient norm: 3.40459892
INFO:root:[   13] Training loss: 0.64056809, Validation loss: 0.62658609, Gradient norm: 3.32604874
INFO:root:[   14] Training loss: 0.62436305, Validation loss: 0.61506265, Gradient norm: 2.77675921
INFO:root:[   15] Training loss: 0.61113416, Validation loss: 0.59573441, Gradient norm: 3.05457279
INFO:root:[   16] Training loss: 0.59899847, Validation loss: 0.59737140, Gradient norm: 2.90335069
INFO:root:[   17] Training loss: 0.58086680, Validation loss: 0.57644131, Gradient norm: 3.21831927
INFO:root:[   18] Training loss: 0.57468766, Validation loss: 0.56512221, Gradient norm: 2.55529964
INFO:root:[   19] Training loss: 0.56354800, Validation loss: 0.55596899, Gradient norm: 2.92598781
INFO:root:[   20] Training loss: 0.55054453, Validation loss: 0.54295794, Gradient norm: 3.15464406
INFO:root:[   21] Training loss: 0.54071367, Validation loss: 0.53869558, Gradient norm: 3.09002408
INFO:root:[   22] Training loss: 0.53849127, Validation loss: 0.53554383, Gradient norm: 2.86576407
INFO:root:[   23] Training loss: 0.53231926, Validation loss: 0.53233922, Gradient norm: 2.70015071
INFO:root:[   24] Training loss: 0.52444258, Validation loss: 0.51806495, Gradient norm: 3.80719696
INFO:root:[   25] Training loss: 0.51910931, Validation loss: 0.50257079, Gradient norm: 3.99823606
INFO:root:[   26] Training loss: 0.50879395, Validation loss: 0.49380797, Gradient norm: 3.31461446
INFO:root:[   27] Training loss: 0.50380230, Validation loss: 0.49006746, Gradient norm: 3.10721396
INFO:root:[   28] Training loss: 0.49585299, Validation loss: 0.48777888, Gradient norm: 2.42121243
INFO:root:[   29] Training loss: 0.48998499, Validation loss: 0.47877589, Gradient norm: 3.36172538
INFO:root:[   30] Training loss: 0.48603120, Validation loss: 0.47810030, Gradient norm: 2.69022027
INFO:root:[   31] Training loss: 0.47674349, Validation loss: 0.47209291, Gradient norm: 2.80946487
INFO:root:[   32] Training loss: 0.47328079, Validation loss: 0.46954974, Gradient norm: 2.50855963
INFO:root:[   33] Training loss: 0.46620639, Validation loss: 0.47657609, Gradient norm: 2.14126544
INFO:root:[   34] Training loss: 0.46355238, Validation loss: 0.46491432, Gradient norm: 3.62136486
INFO:root:[   35] Training loss: 0.45911927, Validation loss: 0.46175920, Gradient norm: 3.35629130
INFO:root:[   36] Training loss: 0.45364457, Validation loss: 0.44829310, Gradient norm: 3.03859381
INFO:root:[   37] Training loss: 0.44973869, Validation loss: 0.44612218, Gradient norm: 3.58637027
INFO:root:[   38] Training loss: 0.44477734, Validation loss: 0.44248910, Gradient norm: 2.40940731
INFO:root:[   39] Training loss: 0.44239570, Validation loss: 0.44224809, Gradient norm: 2.45033524
INFO:root:[   40] Training loss: 0.43968730, Validation loss: 0.44461111, Gradient norm: 3.79120976
INFO:root:[   41] Training loss: 0.43373840, Validation loss: 0.43025871, Gradient norm: 2.80633221
INFO:root:[   42] Training loss: 0.43243046, Validation loss: 0.43441877, Gradient norm: 3.86375171
INFO:root:[   43] Training loss: 0.42843544, Validation loss: 0.42844871, Gradient norm: 3.28805535
INFO:root:[   44] Training loss: 0.43020456, Validation loss: 0.43411328, Gradient norm: 3.71964488
INFO:root:[   45] Training loss: 0.42500304, Validation loss: 0.42377929, Gradient norm: 3.28529621
INFO:root:[   46] Training loss: 0.42013589, Validation loss: 0.42131474, Gradient norm: 3.19962466
INFO:root:[   47] Training loss: 0.41682036, Validation loss: 0.42843842, Gradient norm: 2.57818992
INFO:root:[   48] Training loss: 0.41581125, Validation loss: 0.41499273, Gradient norm: 3.63780368
INFO:root:[   49] Training loss: 0.41404869, Validation loss: 0.41100889, Gradient norm: 3.50082731
INFO:root:[   50] Training loss: 0.40973717, Validation loss: 0.40573824, Gradient norm: 2.59383160
INFO:root:[   51] Training loss: 0.40638580, Validation loss: 0.40567254, Gradient norm: 3.27461256
INFO:root:[   52] Training loss: 0.40450391, Validation loss: 0.41463420, Gradient norm: 4.31827686
INFO:root:[   53] Training loss: 0.40310904, Validation loss: 0.39988150, Gradient norm: 4.81093496
INFO:root:[   54] Training loss: 0.39875877, Validation loss: 0.40319799, Gradient norm: 4.02959462
INFO:root:[   55] Training loss: 0.39929461, Validation loss: 0.40019593, Gradient norm: 4.12580916
INFO:root:[   56] Training loss: 0.39684019, Validation loss: 0.38835912, Gradient norm: 4.25443968
INFO:root:[   57] Training loss: 0.39429858, Validation loss: 0.39689272, Gradient norm: 4.15312182
INFO:root:[   58] Training loss: 0.39358639, Validation loss: 0.39656863, Gradient norm: 4.05041195
INFO:root:[   59] Training loss: 0.39264675, Validation loss: 0.39400818, Gradient norm: 4.79999283
INFO:root:[   60] Training loss: 0.38916276, Validation loss: 0.38475318, Gradient norm: 4.59304703
INFO:root:[   61] Training loss: 0.38571394, Validation loss: 0.38593123, Gradient norm: 3.48623888
INFO:root:[   62] Training loss: 0.38540819, Validation loss: 0.38421538, Gradient norm: 3.95086921
INFO:root:[   63] Training loss: 0.38092280, Validation loss: 0.37684182, Gradient norm: 2.93083714
INFO:root:[   64] Training loss: 0.38171086, Validation loss: 0.37759876, Gradient norm: 4.12249711
INFO:root:[   65] Training loss: 0.37856409, Validation loss: 0.37882274, Gradient norm: 3.71249398
INFO:root:[   66] Training loss: 0.37976557, Validation loss: 0.37578700, Gradient norm: 3.56680799
INFO:root:[   67] Training loss: 0.38015764, Validation loss: 0.38384452, Gradient norm: 4.24484127
INFO:root:[   68] Training loss: 0.37562755, Validation loss: 0.37680953, Gradient norm: 4.84728176
INFO:root:[   69] Training loss: 0.37730474, Validation loss: 0.37619359, Gradient norm: 5.42700180
INFO:root:[   70] Training loss: 0.37242729, Validation loss: 0.37362027, Gradient norm: 3.71934769
INFO:root:[   71] Training loss: 0.37282865, Validation loss: 0.37384096, Gradient norm: 5.93774797
INFO:root:[   72] Training loss: 0.37057827, Validation loss: 0.37148451, Gradient norm: 5.40499116
INFO:root:[   73] Training loss: 0.36888361, Validation loss: 0.37128657, Gradient norm: 3.96526019
INFO:root:[   74] Training loss: 0.36843822, Validation loss: 0.37260509, Gradient norm: 5.70184836
INFO:root:[   75] Training loss: 0.36849343, Validation loss: 0.37395582, Gradient norm: 5.01829174
INFO:root:[   76] Training loss: 0.36653194, Validation loss: 0.37113805, Gradient norm: 6.08365198
INFO:root:[   77] Training loss: 0.36981196, Validation loss: 0.36809737, Gradient norm: 4.95924305
INFO:root:[   78] Training loss: 0.36785622, Validation loss: 0.36754192, Gradient norm: 3.09488600
INFO:root:[   79] Training loss: 0.36750414, Validation loss: 0.36823674, Gradient norm: 5.12958772
INFO:root:[   80] Training loss: 0.36780403, Validation loss: 0.36728858, Gradient norm: 6.48542996
INFO:root:[   81] Training loss: 0.37049474, Validation loss: 0.36797910, Gradient norm: 8.70759980
INFO:root:[   82] Training loss: 0.36996268, Validation loss: 0.36812092, Gradient norm: 5.97602369
INFO:root:[   83] Training loss: 0.36768308, Validation loss: 0.37014410, Gradient norm: 8.80796245
INFO:root:[   84] Training loss: 0.36557106, Validation loss: 0.36901616, Gradient norm: 5.80820208
INFO:root:[   85] Training loss: 0.36593710, Validation loss: 0.36501135, Gradient norm: 6.71591805
INFO:root:[   86] Training loss: 0.36827184, Validation loss: 0.36690026, Gradient norm: 5.95114635
INFO:root:[   87] Training loss: 0.36789600, Validation loss: 0.36939836, Gradient norm: 10.27764797
INFO:root:[   88] Training loss: 0.36809426, Validation loss: 0.37061858, Gradient norm: 8.91937380
INFO:root:[   89] Training loss: 0.36731983, Validation loss: 0.36538178, Gradient norm: 5.84762355
INFO:root:[   90] Training loss: 0.36770249, Validation loss: 0.36645755, Gradient norm: 10.86752949
INFO:root:[   91] Training loss: 0.36966834, Validation loss: 0.36855264, Gradient norm: 9.59668978
INFO:root:[   92] Training loss: 0.37152964, Validation loss: 0.36503483, Gradient norm: 18.79644391
INFO:root:[   93] Training loss: 0.36656721, Validation loss: 0.37038865, Gradient norm: 11.85781560
INFO:root:[   94] Training loss: 0.36871326, Validation loss: 0.37018158, Gradient norm: 15.23067063
INFO:root:[   95] Training loss: 0.36914662, Validation loss: 0.36432599, Gradient norm: 12.85411528
INFO:root:[   96] Training loss: 0.37029879, Validation loss: 0.36902787, Gradient norm: 14.69419756
INFO:root:[   97] Training loss: 0.36933109, Validation loss: 0.36658746, Gradient norm: 17.18473186
INFO:root:[   98] Training loss: 0.36786957, Validation loss: 0.36777081, Gradient norm: 12.68360612
INFO:root:[   99] Training loss: 0.36872123, Validation loss: 0.37790089, Gradient norm: 23.63604165
INFO:root:[  100] Training loss: 0.36809721, Validation loss: 0.37154537, Gradient norm: 22.97352855
INFO:root:[  101] Training loss: 0.36412540, Validation loss: 0.36220361, Gradient norm: 19.04796881
INFO:root:[  102] Training loss: 0.36747889, Validation loss: 0.36519616, Gradient norm: 20.00635314
INFO:root:[  103] Training loss: 0.36402717, Validation loss: 0.36820575, Gradient norm: 17.58661059
INFO:root:[  104] Training loss: 0.36674827, Validation loss: 0.36263980, Gradient norm: 32.90196879
INFO:root:[  105] Training loss: 0.36362312, Validation loss: 0.36544793, Gradient norm: 21.36044184
INFO:root:[  106] Training loss: 0.36757298, Validation loss: 0.36096230, Gradient norm: 31.87961072
INFO:root:[  107] Training loss: 0.36517736, Validation loss: 0.38536380, Gradient norm: 35.59003040
INFO:root:[  108] Training loss: 0.37335508, Validation loss: 0.35889577, Gradient norm: 73.56861995
INFO:root:[  109] Training loss: 0.36488555, Validation loss: 0.36119997, Gradient norm: 42.91084675
INFO:root:[  110] Training loss: 0.36455305, Validation loss: 0.36981567, Gradient norm: 45.28072269
INFO:root:[  111] Training loss: 0.36196299, Validation loss: 0.35643132, Gradient norm: 37.34221585
INFO:root:[  112] Training loss: 0.36311854, Validation loss: 0.36104140, Gradient norm: 48.58931871
INFO:root:[  113] Training loss: 0.36008875, Validation loss: 0.35884100, Gradient norm: 49.41735001
INFO:root:[  114] Training loss: 0.36093188, Validation loss: 0.35979865, Gradient norm: 51.65406472
INFO:root:[  115] Training loss: 0.36447544, Validation loss: 0.36789103, Gradient norm: 65.11169300
INFO:root:[  116] Training loss: 0.35936090, Validation loss: 0.35464776, Gradient norm: 58.86507793
INFO:root:[  117] Training loss: 0.36254220, Validation loss: 0.36631286, Gradient norm: 65.16060344
INFO:root:[  118] Training loss: 0.35948606, Validation loss: 0.35206911, Gradient norm: 72.37889697
INFO:root:[  119] Training loss: 0.35716744, Validation loss: 0.36222887, Gradient norm: 60.34501197
INFO:root:[  120] Training loss: 0.35696518, Validation loss: 0.34971077, Gradient norm: 53.61000606
INFO:root:[  121] Training loss: 0.35538788, Validation loss: 0.36346947, Gradient norm: 56.50405801
INFO:root:[  122] Training loss: 0.35381536, Validation loss: 0.36423810, Gradient norm: 67.05835329
INFO:root:[  123] Training loss: 0.35311715, Validation loss: 0.37197709, Gradient norm: 79.52135907
INFO:root:[  124] Training loss: 0.35364103, Validation loss: 0.35741589, Gradient norm: 79.87729387
INFO:root:[  125] Training loss: 0.35627049, Validation loss: 0.34900310, Gradient norm: 95.68969190
INFO:root:[  126] Training loss: 0.35986660, Validation loss: 0.35862688, Gradient norm: 134.53614391
INFO:root:[  127] Training loss: 0.35156614, Validation loss: 0.34574860, Gradient norm: 80.79111974
INFO:root:[  128] Training loss: 0.35383141, Validation loss: 0.36019802, Gradient norm: 123.36364142
INFO:root:[  129] Training loss: 0.35121835, Validation loss: 0.34600806, Gradient norm: 89.85803433
INFO:root:[  130] Training loss: 0.34534532, Validation loss: 0.34551076, Gradient norm: 61.60279789
INFO:root:[  131] Training loss: 0.34626523, Validation loss: 0.34723432, Gradient norm: 98.62856489
INFO:root:[  132] Training loss: 0.34561793, Validation loss: 0.34434640, Gradient norm: 70.33537119
INFO:root:[  133] Training loss: 0.34755539, Validation loss: 0.34276789, Gradient norm: 120.43639296
INFO:root:[  134] Training loss: 0.34714212, Validation loss: 0.33996057, Gradient norm: 123.90527366
INFO:root:[  135] Training loss: 0.34339766, Validation loss: 0.35527682, Gradient norm: 100.47480623
INFO:root:[  136] Training loss: 0.35674544, Validation loss: 0.34939789, Gradient norm: 229.58686493
INFO:root:[  137] Training loss: 0.34366628, Validation loss: 0.33852655, Gradient norm: 129.20066067
INFO:root:[  138] Training loss: 0.34730089, Validation loss: 0.34406375, Gradient norm: 168.03727512
INFO:root:[  139] Training loss: 0.34207555, Validation loss: 0.34253392, Gradient norm: 125.53546699
INFO:root:[  140] Training loss: 0.34749275, Validation loss: 0.34164538, Gradient norm: 183.74817503
INFO:root:[  141] Training loss: 0.34207000, Validation loss: 0.33627892, Gradient norm: 156.54435891
INFO:root:[  142] Training loss: 0.33895806, Validation loss: 0.33721814, Gradient norm: 118.38378350
INFO:root:[  143] Training loss: 0.34337275, Validation loss: 0.34727616, Gradient norm: 164.55865210
INFO:root:[  144] Training loss: 0.33476976, Validation loss: 0.33454874, Gradient norm: 99.37783014
INFO:root:[  145] Training loss: 0.33713744, Validation loss: 0.33210524, Gradient norm: 107.63984956
INFO:root:[  146] Training loss: 0.33552851, Validation loss: 0.34345325, Gradient norm: 122.09301864
INFO:root:[  147] Training loss: 0.34060390, Validation loss: 0.33174109, Gradient norm: 200.70696319
INFO:root:[  148] Training loss: 0.34091901, Validation loss: 0.35444736, Gradient norm: 224.39521917
INFO:root:[  149] Training loss: 0.33323228, Validation loss: 0.33128310, Gradient norm: 112.35206616
INFO:root:[  150] Training loss: 0.33396297, Validation loss: 0.34117319, Gradient norm: 166.87349347
INFO:root:[  151] Training loss: 0.34165974, Validation loss: 0.34982075, Gradient norm: 236.15083908
INFO:root:[  152] Training loss: 0.33722775, Validation loss: 0.34113963, Gradient norm: 217.99636317
INFO:root:[  153] Training loss: 0.34179094, Validation loss: 0.33399136, Gradient norm: 268.86174087
INFO:root:[  154] Training loss: 0.34576054, Validation loss: 0.32738961, Gradient norm: 302.36673299
INFO:root:[  155] Training loss: 0.32972746, Validation loss: 0.32709236, Gradient norm: 125.62147905
INFO:root:[  156] Training loss: 0.32649494, Validation loss: 0.32642074, Gradient norm: 128.70847792
INFO:root:[  157] Training loss: 0.32636132, Validation loss: 0.32817914, Gradient norm: 102.68011427
INFO:root:[  158] Training loss: 0.33503864, Validation loss: 0.32820264, Gradient norm: 229.82460039
INFO:root:[  159] Training loss: 0.33107001, Validation loss: 0.32589623, Gradient norm: 201.25481419
INFO:root:[  160] Training loss: 0.32923944, Validation loss: 0.32352619, Gradient norm: 224.90867270
INFO:root:[  161] Training loss: 0.32754972, Validation loss: 0.32193796, Gradient norm: 189.44058768
INFO:root:[  162] Training loss: 0.32533177, Validation loss: 0.33498354, Gradient norm: 154.22674740
INFO:root:[  163] Training loss: 0.32837617, Validation loss: 0.32025870, Gradient norm: 226.47427593
INFO:root:[  164] Training loss: 0.32904501, Validation loss: 0.32439346, Gradient norm: 230.46832300
INFO:root:[  165] Training loss: 0.33143077, Validation loss: 0.34012659, Gradient norm: 266.01010496
INFO:root:[  166] Training loss: 0.34815479, Validation loss: 0.32140134, Gradient norm: 406.00154118
INFO:root:[  167] Training loss: 0.32310030, Validation loss: 0.31991935, Gradient norm: 163.43804191
INFO:root:[  168] Training loss: 0.32055282, Validation loss: 0.31748457, Gradient norm: 128.60304116
INFO:root:[  169] Training loss: 0.31997490, Validation loss: 0.31724514, Gradient norm: 111.13008946
INFO:root:[  170] Training loss: 0.32694864, Validation loss: 0.31823428, Gradient norm: 288.21353507
INFO:root:[  171] Training loss: 0.31851174, Validation loss: 0.33292258, Gradient norm: 136.14829261
INFO:root:[  172] Training loss: 0.32097709, Validation loss: 0.31975614, Gradient norm: 211.55949831
INFO:root:[  173] Training loss: 0.32564904, Validation loss: 0.35408793, Gradient norm: 302.22838505
INFO:root:[  174] Training loss: 0.32429780, Validation loss: 0.31375056, Gradient norm: 266.26836625
INFO:root:[  175] Training loss: 0.31911334, Validation loss: 0.33221710, Gradient norm: 219.33570783
INFO:root:[  176] Training loss: 0.31841543, Validation loss: 0.31324555, Gradient norm: 204.80131093
INFO:root:[  177] Training loss: 0.32035155, Validation loss: 0.32515397, Gradient norm: 249.26710069
INFO:root:[  178] Training loss: 0.31930351, Validation loss: 0.31332639, Gradient norm: 249.82131872
INFO:root:[  179] Training loss: 0.31805961, Validation loss: 0.33418448, Gradient norm: 214.60229236
INFO:root:[  180] Training loss: 0.31389888, Validation loss: 0.32787188, Gradient norm: 181.98546373
INFO:root:[  181] Training loss: 0.31421372, Validation loss: 0.31088038, Gradient norm: 173.65175574
INFO:root:[  182] Training loss: 0.31706955, Validation loss: 0.31812158, Gradient norm: 238.34426387
INFO:root:[  183] Training loss: 0.31397193, Validation loss: 0.32088010, Gradient norm: 247.02913828
INFO:root:[  184] Training loss: 0.31752228, Validation loss: 0.34396199, Gradient norm: 293.03062695
INFO:root:[  185] Training loss: 0.32603447, Validation loss: 0.33330756, Gradient norm: 405.69976790
INFO:root:[  186] Training loss: 0.31802858, Validation loss: 0.30904721, Gradient norm: 317.63268127
INFO:root:[  187] Training loss: 0.31160109, Validation loss: 0.31393055, Gradient norm: 227.74385219
INFO:root:[  188] Training loss: 0.31231188, Validation loss: 0.31035012, Gradient norm: 218.20993928
INFO:root:[  189] Training loss: 0.31747793, Validation loss: 0.31386071, Gradient norm: 326.45883226
INFO:root:[  190] Training loss: 0.31061143, Validation loss: 0.31102190, Gradient norm: 214.84302825
INFO:root:[  191] Training loss: 0.31377613, Validation loss: 0.30391677, Gradient norm: 306.58339540
INFO:root:[  192] Training loss: 0.30779673, Validation loss: 0.30695129, Gradient norm: 209.78499965
INFO:root:[  193] Training loss: 0.31473945, Validation loss: 0.32136661, Gradient norm: 363.83733466
INFO:root:[  194] Training loss: 0.31389934, Validation loss: 0.31548548, Gradient norm: 331.05187478
INFO:root:[  195] Training loss: 0.31015966, Validation loss: 0.33446158, Gradient norm: 291.61347645
INFO:root:[  196] Training loss: 0.30916353, Validation loss: 0.30229751, Gradient norm: 290.45791148
INFO:root:[  197] Training loss: 0.30729697, Validation loss: 0.30531714, Gradient norm: 258.35677687
INFO:root:[  198] Training loss: 0.30457794, Validation loss: 0.31213941, Gradient norm: 248.82373629
INFO:root:[  199] Training loss: 0.30577718, Validation loss: 0.30344425, Gradient norm: 286.33186105
INFO:root:[  200] Training loss: 0.30791738, Validation loss: 0.30701613, Gradient norm: 328.16378737
INFO:root:[  201] Training loss: 0.31617200, Validation loss: 0.31788568, Gradient norm: 430.51983856
INFO:root:[  202] Training loss: 0.30859679, Validation loss: 0.30065438, Gradient norm: 358.42509319
INFO:root:[  203] Training loss: 0.30443561, Validation loss: 0.31016763, Gradient norm: 272.89713420
INFO:root:[  204] Training loss: 0.30074295, Validation loss: 0.29708370, Gradient norm: 202.89845252
INFO:root:[  205] Training loss: 0.30228107, Validation loss: 0.31020662, Gradient norm: 265.79994720
INFO:root:[  206] Training loss: 0.30330450, Validation loss: 0.30003256, Gradient norm: 269.78609017
INFO:root:[  207] Training loss: 0.31033391, Validation loss: 0.35024205, Gradient norm: 416.98209241
INFO:root:[  208] Training loss: 0.30521801, Validation loss: 0.29741926, Gradient norm: 330.00054117
INFO:root:[  209] Training loss: 0.30586323, Validation loss: 0.29571621, Gradient norm: 366.18139582
INFO:root:[  210] Training loss: 0.29846894, Validation loss: 0.29410336, Gradient norm: 278.83843411
INFO:root:[  211] Training loss: 0.29931120, Validation loss: 0.30048963, Gradient norm: 292.00401318
INFO:root:[  212] Training loss: 0.29859035, Validation loss: 0.29502510, Gradient norm: 276.41290060
INFO:root:[  213] Training loss: 0.30042637, Validation loss: 0.30288811, Gradient norm: 305.33671648
INFO:root:[  214] Training loss: 0.30561539, Validation loss: 0.29146406, Gradient norm: 416.63528838
INFO:root:[  215] Training loss: 0.29754449, Validation loss: 0.34368314, Gradient norm: 285.65681536
INFO:root:[  216] Training loss: 0.30304021, Validation loss: 0.29692599, Gradient norm: 366.72467705
INFO:root:[  217] Training loss: 0.29831190, Validation loss: 0.31158741, Gradient norm: 311.24374135
INFO:root:[  218] Training loss: 0.29699091, Validation loss: 0.29652502, Gradient norm: 311.49911114
INFO:root:[  219] Training loss: 0.29805082, Validation loss: 0.31703790, Gradient norm: 328.84737800
INFO:root:[  220] Training loss: 0.31180479, Validation loss: 0.29743114, Gradient norm: 551.74555284
INFO:root:[  221] Training loss: 0.29639789, Validation loss: 0.28940301, Gradient norm: 298.27174581
INFO:root:[  222] Training loss: 0.29247750, Validation loss: 0.29332437, Gradient norm: 218.12873068
INFO:root:[  223] Training loss: 0.29595016, Validation loss: 0.32292873, Gradient norm: 331.70036482
INFO:root:[  224] Training loss: 0.29519422, Validation loss: 0.28680044, Gradient norm: 350.70971234
INFO:root:[  225] Training loss: 0.29595447, Validation loss: 0.28740660, Gradient norm: 340.39767276
INFO:root:[  226] Training loss: 0.29420048, Validation loss: 0.31974082, Gradient norm: 322.74372461
INFO:root:[  227] Training loss: 0.29177825, Validation loss: 0.28642076, Gradient norm: 251.57874390
INFO:root:[  228] Training loss: 0.29411355, Validation loss: 0.29361977, Gradient norm: 350.23650825
INFO:root:[  229] Training loss: 0.29727984, Validation loss: 0.28960476, Gradient norm: 432.30042742
INFO:root:[  230] Training loss: 0.30143406, Validation loss: 0.28505256, Gradient norm: 504.39640939
INFO:root:[  231] Training loss: 0.29195715, Validation loss: 0.28440572, Gradient norm: 317.45987686
INFO:root:[  232] Training loss: 0.28885987, Validation loss: 0.28780830, Gradient norm: 267.45026383
INFO:root:[  233] Training loss: 0.29191893, Validation loss: 0.29439170, Gradient norm: 366.69657150
INFO:root:[  234] Training loss: 0.29349877, Validation loss: 0.28956258, Gradient norm: 426.58933006
INFO:root:[  235] Training loss: 0.29064876, Validation loss: 0.28542557, Gradient norm: 364.90804616
INFO:root:[  236] Training loss: 0.29375581, Validation loss: 0.28966996, Gradient norm: 401.92622558
INFO:root:[  237] Training loss: 0.28628530, Validation loss: 0.32235106, Gradient norm: 285.66700624
INFO:root:[  238] Training loss: 0.29249593, Validation loss: 0.28574096, Gradient norm: 446.96162878
INFO:root:[  239] Training loss: 0.28800554, Validation loss: 0.29590985, Gradient norm: 357.32755600
INFO:root:[  240] Training loss: 0.28683878, Validation loss: 0.28125991, Gradient norm: 331.42620752
INFO:root:[  241] Training loss: 0.28795136, Validation loss: 0.29202506, Gradient norm: 372.18245612
INFO:root:[  242] Training loss: 0.28808710, Validation loss: 0.28057540, Gradient norm: 382.15138721
INFO:root:[  243] Training loss: 0.28798483, Validation loss: 0.28220839, Gradient norm: 388.05058181
INFO:root:[  244] Training loss: 0.28581480, Validation loss: 0.29276053, Gradient norm: 366.45866406
INFO:root:[  245] Training loss: 0.28691956, Validation loss: 0.28564861, Gradient norm: 407.52018611
INFO:root:[  246] Training loss: 0.28494019, Validation loss: 0.30468292, Gradient norm: 370.13478938
INFO:root:[  247] Training loss: 0.28270303, Validation loss: 0.27779724, Gradient norm: 322.44777358
INFO:root:[  248] Training loss: 0.28095107, Validation loss: 0.28283574, Gradient norm: 293.80559539
INFO:root:[  249] Training loss: 0.28756415, Validation loss: 0.27879293, Gradient norm: 446.26047643
INFO:root:[  250] Training loss: 0.28415798, Validation loss: 0.28159598, Gradient norm: 381.45810476
INFO:root:[  251] Training loss: 0.28514829, Validation loss: 0.27502927, Gradient norm: 386.91185939
INFO:root:[  252] Training loss: 0.27955227, Validation loss: 0.28996064, Gradient norm: 288.85868605
INFO:root:[  253] Training loss: 0.28487925, Validation loss: 0.27419493, Gradient norm: 404.40619936
INFO:root:[  254] Training loss: 0.28007784, Validation loss: 0.28686640, Gradient norm: 310.66017838
INFO:root:[  255] Training loss: 0.27962232, Validation loss: 0.27630383, Gradient norm: 366.52772612
INFO:root:[  256] Training loss: 0.27885688, Validation loss: 0.28092948, Gradient norm: 359.00673016
INFO:root:[  257] Training loss: 0.28059347, Validation loss: 0.27649985, Gradient norm: 373.73394766
INFO:root:[  258] Training loss: 0.28424998, Validation loss: 0.28466159, Gradient norm: 500.00612966
INFO:root:[  259] Training loss: 0.27975801, Validation loss: 0.27133760, Gradient norm: 418.00495969
INFO:root:[  260] Training loss: 0.27974676, Validation loss: 0.28176403, Gradient norm: 401.45847211
INFO:root:[  261] Training loss: 0.27978931, Validation loss: 0.27124696, Gradient norm: 455.11968254
INFO:root:[  262] Training loss: 0.27856298, Validation loss: 0.26999054, Gradient norm: 402.17442686
INFO:root:[  263] Training loss: 0.27406941, Validation loss: 0.28852125, Gradient norm: 315.12732763
INFO:root:[  264] Training loss: 0.27959345, Validation loss: 0.28242425, Gradient norm: 432.11499120
INFO:root:[  265] Training loss: 0.27634127, Validation loss: 0.26809866, Gradient norm: 378.94823367
INFO:root:[  266] Training loss: 0.27769325, Validation loss: 0.26828491, Gradient norm: 415.73462060
INFO:root:[  267] Training loss: 0.27695562, Validation loss: 0.26944183, Gradient norm: 454.02858564
INFO:root:[  268] Training loss: 0.27548201, Validation loss: 0.29914419, Gradient norm: 429.13322303
INFO:root:[  269] Training loss: 0.27438815, Validation loss: 0.26830084, Gradient norm: 389.62202685
INFO:root:[  270] Training loss: 0.27341462, Validation loss: 0.26617477, Gradient norm: 401.28671083
INFO:root:[  271] Training loss: 0.27123669, Validation loss: 0.26817529, Gradient norm: 345.64424079
INFO:root:[  272] Training loss: 0.28299864, Validation loss: 0.26706647, Gradient norm: 601.18267127
INFO:root:[  273] Training loss: 0.27222377, Validation loss: 0.26353443, Gradient norm: 382.10579381
INFO:root:[  274] Training loss: 0.26738640, Validation loss: 0.27410427, Gradient norm: 276.96389020
INFO:root:[  275] Training loss: 0.26925939, Validation loss: 0.26808725, Gradient norm: 310.79225300
INFO:root:[  276] Training loss: 0.27420289, Validation loss: 0.26398979, Gradient norm: 453.87171592
INFO:root:[  277] Training loss: 0.27009380, Validation loss: 0.26988133, Gradient norm: 341.97489305
INFO:root:[  278] Training loss: 0.27395718, Validation loss: 0.28017125, Gradient norm: 489.07778803
INFO:root:[  279] Training loss: 0.26853822, Validation loss: 0.26443138, Gradient norm: 333.01910163
INFO:root:[  280] Training loss: 0.26855699, Validation loss: 0.26744818, Gradient norm: 365.37126708
INFO:root:[  281] Training loss: 0.27158608, Validation loss: 0.26186585, Gradient norm: 468.06293609
INFO:root:[  282] Training loss: 0.27587912, Validation loss: 0.28984002, Gradient norm: 531.40475804
INFO:root:[  283] Training loss: 0.27098651, Validation loss: 0.26025783, Gradient norm: 428.83643077
INFO:root:[  284] Training loss: 0.26775597, Validation loss: 0.26681412, Gradient norm: 384.09382190
INFO:root:[  285] Training loss: 0.26586923, Validation loss: 0.26010838, Gradient norm: 351.47922999
INFO:root:[  286] Training loss: 0.26371600, Validation loss: 0.25876075, Gradient norm: 316.40582275
INFO:root:[  287] Training loss: 0.26869307, Validation loss: 0.27098997, Gradient norm: 415.41188915
INFO:root:[  288] Training loss: 0.27344447, Validation loss: 0.26566244, Gradient norm: 561.88325780
INFO:root:[  289] Training loss: 0.26687225, Validation loss: 0.25831875, Gradient norm: 421.99749755
INFO:root:[  290] Training loss: 0.26164419, Validation loss: 0.26038920, Gradient norm: 263.81669521
INFO:root:[  291] Training loss: 0.26550406, Validation loss: 0.25715091, Gradient norm: 389.57614953
INFO:root:[  292] Training loss: 0.26232016, Validation loss: 0.27911293, Gradient norm: 332.06410075
INFO:root:[  293] Training loss: 0.26577302, Validation loss: 0.26562589, Gradient norm: 453.95672175
INFO:root:[  294] Training loss: 0.26873499, Validation loss: 0.26465994, Gradient norm: 531.31904229
INFO:root:[  295] Training loss: 0.26244993, Validation loss: 0.25567717, Gradient norm: 405.05857016
INFO:root:[  296] Training loss: 0.26972300, Validation loss: 0.25624070, Gradient norm: 554.23900640
INFO:root:[  297] Training loss: 0.25706346, Validation loss: 0.25483790, Gradient norm: 215.42363000
INFO:root:[  298] Training loss: 0.26151928, Validation loss: 0.25940388, Gradient norm: 392.75136412
INFO:root:[  299] Training loss: 0.25949057, Validation loss: 0.26539785, Gradient norm: 345.03982799
INFO:root:[  300] Training loss: 0.26552447, Validation loss: 0.25433981, Gradient norm: 485.46042224
INFO:root:[  301] Training loss: 0.25824325, Validation loss: 0.25792187, Gradient norm: 309.54905498
INFO:root:[  302] Training loss: 0.26330528, Validation loss: 0.26754680, Gradient norm: 455.66276714
INFO:root:[  303] Training loss: 0.25929110, Validation loss: 0.26678679, Gradient norm: 381.56466066
INFO:root:[  304] Training loss: 0.25898137, Validation loss: 0.26009346, Gradient norm: 418.99197534
INFO:root:[  305] Training loss: 0.25978750, Validation loss: 0.25107833, Gradient norm: 438.77036701
INFO:root:[  306] Training loss: 0.26804149, Validation loss: 0.26303973, Gradient norm: 583.79671546
INFO:root:[  307] Training loss: 0.25976399, Validation loss: 0.25481445, Gradient norm: 420.26851005
INFO:root:[  308] Training loss: 0.25844265, Validation loss: 0.25620308, Gradient norm: 402.43270221
INFO:root:[  309] Training loss: 0.25685757, Validation loss: 0.25300909, Gradient norm: 343.13068250
INFO:root:[  310] Training loss: 0.25980848, Validation loss: 0.25355271, Gradient norm: 448.80577884
INFO:root:[  311] Training loss: 0.25226448, Validation loss: 0.26324992, Gradient norm: 227.23884488
INFO:root:[  312] Training loss: 0.25173479, Validation loss: 0.25392648, Gradient norm: 212.16309437
INFO:root:[  313] Training loss: 0.25603619, Validation loss: 0.27478491, Gradient norm: 424.32149336
INFO:root:[  314] Training loss: 0.25867418, Validation loss: 0.24880910, Gradient norm: 441.05029383
INFO:root:[  315] Training loss: 0.25769435, Validation loss: 0.25635282, Gradient norm: 391.85858702
INFO:root:[  316] Training loss: 0.25828537, Validation loss: 0.25624836, Gradient norm: 464.43515603
INFO:root:[  317] Training loss: 0.25841888, Validation loss: 0.25077151, Gradient norm: 454.55008518
INFO:root:[  318] Training loss: 0.25261446, Validation loss: 0.25627458, Gradient norm: 343.35041598
INFO:root:[  319] Training loss: 0.25332801, Validation loss: 0.25809223, Gradient norm: 391.98371193
INFO:root:[  320] Training loss: 0.25171103, Validation loss: 0.26121219, Gradient norm: 347.70725944
INFO:root:[  321] Training loss: 0.25312672, Validation loss: 0.24725133, Gradient norm: 420.99962155
INFO:root:[  322] Training loss: 0.25632518, Validation loss: 0.25563643, Gradient norm: 482.24277144
INFO:root:[  323] Training loss: 0.25533645, Validation loss: 0.28916715, Gradient norm: 437.55453756
INFO:root:[  324] Training loss: 0.25460198, Validation loss: 0.24437606, Gradient norm: 428.34880263
INFO:root:[  325] Training loss: 0.25280322, Validation loss: 0.25734216, Gradient norm: 398.81037891
INFO:root:[  326] Training loss: 0.25029831, Validation loss: 0.25525132, Gradient norm: 377.03756349
INFO:root:[  327] Training loss: 0.25412932, Validation loss: 0.25115192, Gradient norm: 446.72629933
INFO:root:[  328] Training loss: 0.25058214, Validation loss: 0.26974560, Gradient norm: 397.11713776
INFO:root:[  329] Training loss: 0.24830952, Validation loss: 0.25155940, Gradient norm: 328.34391441
INFO:root:[  330] Training loss: 0.24646940, Validation loss: 0.24842021, Gradient norm: 301.06171361
INFO:root:[  331] Training loss: 0.24713487, Validation loss: 0.28261868, Gradient norm: 338.35157105
INFO:root:[  332] Training loss: 0.25289710, Validation loss: 0.24534540, Gradient norm: 475.25624285
INFO:root:[  333] Training loss: 0.25115751, Validation loss: 0.24100824, Gradient norm: 469.66420213
INFO:root:[  334] Training loss: 0.24895013, Validation loss: 0.24413122, Gradient norm: 429.65417304
INFO:root:[  335] Training loss: 0.25470044, Validation loss: 0.28184156, Gradient norm: 539.95916161
INFO:root:[  336] Training loss: 0.24874685, Validation loss: 0.24057548, Gradient norm: 361.17671995
INFO:root:[  337] Training loss: 0.24534368, Validation loss: 0.26256804, Gradient norm: 334.02176489
INFO:root:[  338] Training loss: 0.24741915, Validation loss: 0.24328168, Gradient norm: 387.05873231
INFO:root:[  339] Training loss: 0.24245358, Validation loss: 0.25057097, Gradient norm: 268.49089823
INFO:root:[  340] Training loss: 0.25096779, Validation loss: 0.25283446, Gradient norm: 511.54582509
INFO:root:[  341] Training loss: 0.24463899, Validation loss: 0.23780356, Gradient norm: 335.76707923
INFO:root:[  342] Training loss: 0.24406089, Validation loss: 0.23990375, Gradient norm: 325.93591540
INFO:root:[  343] Training loss: 0.24628853, Validation loss: 0.24209540, Gradient norm: 438.98005461
INFO:root:[  344] Training loss: 0.24507702, Validation loss: 0.25795180, Gradient norm: 391.93819293
INFO:root:[  345] Training loss: 0.24741817, Validation loss: 0.23854615, Gradient norm: 412.15202128
INFO:root:[  346] Training loss: 0.24604172, Validation loss: 0.24283036, Gradient norm: 440.97361991
INFO:root:[  347] Training loss: 0.24318170, Validation loss: 0.23882048, Gradient norm: 360.75596549
INFO:root:[  348] Training loss: 0.24311131, Validation loss: 0.23924710, Gradient norm: 375.95369571
INFO:root:[  349] Training loss: 0.24206333, Validation loss: 0.24385318, Gradient norm: 327.81023282
INFO:root:[  350] Training loss: 0.24126501, Validation loss: 0.23589596, Gradient norm: 351.47880175
INFO:root:[  351] Training loss: 0.24290539, Validation loss: 0.23433206, Gradient norm: 441.79961123
INFO:root:[  352] Training loss: 0.24483845, Validation loss: 0.23434068, Gradient norm: 434.18367855
INFO:root:[  353] Training loss: 0.24076715, Validation loss: 0.25013715, Gradient norm: 360.65503399
INFO:root:[  354] Training loss: 0.24389182, Validation loss: 0.26009100, Gradient norm: 442.77200877
INFO:root:[  355] Training loss: 0.24073380, Validation loss: 0.26634042, Gradient norm: 356.19366840
INFO:root:[  356] Training loss: 0.23999703, Validation loss: 0.24472268, Gradient norm: 387.49553778
INFO:root:[  357] Training loss: 0.23999137, Validation loss: 0.24947722, Gradient norm: 418.55178051
INFO:root:[  358] Training loss: 0.23887633, Validation loss: 0.26224735, Gradient norm: 362.58972650
INFO:root:[  359] Training loss: 0.24257322, Validation loss: 0.23234404, Gradient norm: 443.86421887
INFO:root:[  360] Training loss: 0.23557596, Validation loss: 0.23181701, Gradient norm: 301.99133658
INFO:root:[  361] Training loss: 0.25073841, Validation loss: 0.23634547, Gradient norm: 604.52063478
INFO:root:[  362] Training loss: 0.24199686, Validation loss: 0.24229098, Gradient norm: 465.87809530
INFO:root:[  363] Training loss: 0.23503930, Validation loss: 0.23020217, Gradient norm: 270.44514061
INFO:root:[  364] Training loss: 0.23937732, Validation loss: 0.25214698, Gradient norm: 429.26283356
INFO:root:[  365] Training loss: 0.23576239, Validation loss: 0.22986144, Gradient norm: 344.54473251
INFO:root:[  366] Training loss: 0.23657459, Validation loss: 0.23192271, Gradient norm: 387.42653678
INFO:root:[  367] Training loss: 0.24000556, Validation loss: 0.23034465, Gradient norm: 464.23128095
INFO:root:[  368] Training loss: 0.23011544, Validation loss: 0.22815154, Gradient norm: 157.17542138
INFO:root:[  369] Training loss: 0.23256149, Validation loss: 0.24785741, Gradient norm: 297.45204593
INFO:root:[  370] Training loss: 0.23827096, Validation loss: 0.23118828, Gradient norm: 408.82354049
INFO:root:[  371] Training loss: 0.23234774, Validation loss: 0.24722505, Gradient norm: 239.72150819
INFO:root:[  372] Training loss: 0.23598967, Validation loss: 0.26255871, Gradient norm: 375.48670099
INFO:root:[  373] Training loss: 0.23944165, Validation loss: 0.22887021, Gradient norm: 513.00725529
INFO:root:[  374] Training loss: 0.23114263, Validation loss: 0.23010319, Gradient norm: 257.88729596
INFO:root:[  375] Training loss: 0.23498594, Validation loss: 0.22633848, Gradient norm: 397.05122319
INFO:root:[  376] Training loss: 0.23622623, Validation loss: 0.22805845, Gradient norm: 460.56508530
INFO:root:[  377] Training loss: 0.23034837, Validation loss: 0.23153675, Gradient norm: 306.86640607
INFO:root:[  378] Training loss: 0.22919517, Validation loss: 0.24378929, Gradient norm: 299.48575924
INFO:root:[  379] Training loss: 0.23706487, Validation loss: 0.22890150, Gradient norm: 496.25293413
INFO:root:[  380] Training loss: 0.23313179, Validation loss: 0.22899123, Gradient norm: 411.97302403
INFO:root:[  381] Training loss: 0.23188990, Validation loss: 0.22577929, Gradient norm: 345.12674542
INFO:root:[  382] Training loss: 0.22928760, Validation loss: 0.22460047, Gradient norm: 295.40716791
INFO:root:[  383] Training loss: 0.23076993, Validation loss: 0.22914560, Gradient norm: 394.91567048
INFO:root:[  384] Training loss: 0.22990578, Validation loss: 0.23263446, Gradient norm: 381.09831170
INFO:root:[  385] Training loss: 0.23151955, Validation loss: 0.23662248, Gradient norm: 460.39903450
INFO:root:[  386] Training loss: 0.23086211, Validation loss: 0.22409977, Gradient norm: 396.92073360
INFO:root:[  387] Training loss: 0.23122469, Validation loss: 0.22432606, Gradient norm: 416.30295105
INFO:root:[  388] Training loss: 0.22721167, Validation loss: 0.22640762, Gradient norm: 308.55960568
INFO:root:[  389] Training loss: 0.22622580, Validation loss: 0.24201486, Gradient norm: 292.13372775
INFO:root:[  390] Training loss: 0.23239105, Validation loss: 0.23050431, Gradient norm: 476.73517599
INFO:root:[  391] Training loss: 0.22986202, Validation loss: 0.23270760, Gradient norm: 410.41968808
INFO:root:[  392] Training loss: 0.22867902, Validation loss: 0.22054339, Gradient norm: 363.38880754
INFO:root:[  393] Training loss: 0.22266014, Validation loss: 0.22062625, Gradient norm: 203.51325640
INFO:root:[  394] Training loss: 0.22540911, Validation loss: 0.22418414, Gradient norm: 370.16695095
INFO:root:[  395] Training loss: 0.22891016, Validation loss: 0.22528636, Gradient norm: 408.57169378
INFO:root:[  396] Training loss: 0.22733928, Validation loss: 0.22239337, Gradient norm: 403.34874393
INFO:root:[  397] Training loss: 0.22755665, Validation loss: 0.24300023, Gradient norm: 418.76168156
INFO:root:[  398] Training loss: 0.23024923, Validation loss: 0.23537001, Gradient norm: 440.61191695
INFO:root:[  399] Training loss: 0.22572801, Validation loss: 0.21874068, Gradient norm: 384.66346928
INFO:root:[  400] Training loss: 0.22345229, Validation loss: 0.22975252, Gradient norm: 311.57957558
INFO:root:[  401] Training loss: 0.22268277, Validation loss: 0.22422836, Gradient norm: 305.89104319
INFO:root:[  402] Training loss: 0.22363129, Validation loss: 0.22255771, Gradient norm: 339.09465523
INFO:root:[  403] Training loss: 0.22599223, Validation loss: 0.21952053, Gradient norm: 398.07739595
INFO:root:[  404] Training loss: 0.22449867, Validation loss: 0.22603814, Gradient norm: 429.72744188
INFO:root:[  405] Training loss: 0.22316361, Validation loss: 0.22261054, Gradient norm: 367.29646648
INFO:root:[  406] Training loss: 0.22481942, Validation loss: 0.21631628, Gradient norm: 381.20353462
INFO:root:[  407] Training loss: 0.22486471, Validation loss: 0.23234894, Gradient norm: 420.36054448
INFO:root:[  408] Training loss: 0.22183689, Validation loss: 0.21820663, Gradient norm: 346.49927579
INFO:root:[  409] Training loss: 0.22558489, Validation loss: 0.22435949, Gradient norm: 432.05416443
INFO:root:[  410] Training loss: 0.22023536, Validation loss: 0.21744467, Gradient norm: 325.27634264
INFO:root:[  411] Training loss: 0.22250958, Validation loss: 0.21502811, Gradient norm: 328.00035076
INFO:root:[  412] Training loss: 0.22014820, Validation loss: 0.21951261, Gradient norm: 370.60002300
INFO:root:[  413] Training loss: 0.22070215, Validation loss: 0.21643807, Gradient norm: 402.46865187
INFO:root:[  414] Training loss: 0.21956762, Validation loss: 0.21567454, Gradient norm: 334.18136892
INFO:root:[  415] Training loss: 0.22260425, Validation loss: 0.21521900, Gradient norm: 429.14990809
INFO:root:[  416] Training loss: 0.22164545, Validation loss: 0.23285899, Gradient norm: 414.88258865
INFO:root:[  417] Training loss: 0.22093571, Validation loss: 0.21702573, Gradient norm: 400.59468603
INFO:root:[  418] Training loss: 0.21728013, Validation loss: 0.21443835, Gradient norm: 323.84025036
INFO:root:[  419] Training loss: 0.21526392, Validation loss: 0.21449555, Gradient norm: 281.13829565
INFO:root:[  420] Training loss: 0.21957321, Validation loss: 0.23189831, Gradient norm: 391.73164477
INFO:root:[  421] Training loss: 0.21554440, Validation loss: 0.22237918, Gradient norm: 313.47322634
INFO:root:[  422] Training loss: 0.21346414, Validation loss: 0.22321719, Gradient norm: 180.61694458
INFO:root:[  423] Training loss: 0.21815456, Validation loss: 0.21182312, Gradient norm: 374.63544255
INFO:root:[  424] Training loss: 0.21866709, Validation loss: 0.21059243, Gradient norm: 385.01847749
INFO:root:[  425] Training loss: 0.22063548, Validation loss: 0.21756473, Gradient norm: 447.84825035
INFO:root:[  426] Training loss: 0.21644210, Validation loss: 0.21958901, Gradient norm: 369.27305843
INFO:root:[  427] Training loss: 0.21737361, Validation loss: 0.21023516, Gradient norm: 383.08574736
INFO:root:[  428] Training loss: 0.21162015, Validation loss: 0.21240648, Gradient norm: 229.95463243
INFO:root:[  429] Training loss: 0.21468452, Validation loss: 0.21050256, Gradient norm: 343.21846042
INFO:root:[  430] Training loss: 0.21383531, Validation loss: 0.20857364, Gradient norm: 334.03912289
INFO:root:[  431] Training loss: 0.21458796, Validation loss: 0.21758943, Gradient norm: 357.95637137
INFO:root:[  432] Training loss: 0.21370865, Validation loss: 0.22449699, Gradient norm: 333.09076997
INFO:root:[  433] Training loss: 0.21946500, Validation loss: 0.22645290, Gradient norm: 480.71277347
INFO:root:[  434] Training loss: 0.21816803, Validation loss: 0.20722539, Gradient norm: 460.03593993
INFO:root:[  435] Training loss: 0.20952950, Validation loss: 0.20663305, Gradient norm: 228.07216885
INFO:root:[  436] Training loss: 0.20888068, Validation loss: 0.20590859, Gradient norm: 203.46878448
INFO:root:[  437] Training loss: 0.21621678, Validation loss: 0.23229651, Gradient norm: 438.92371855
INFO:root:[  438] Training loss: 0.21106838, Validation loss: 0.21222716, Gradient norm: 323.06487454
INFO:root:[  439] Training loss: 0.21210567, Validation loss: 0.20604711, Gradient norm: 341.29476442
INFO:root:[  440] Training loss: 0.20878775, Validation loss: 0.20530944, Gradient norm: 283.55056546
INFO:root:[  441] Training loss: 0.21129836, Validation loss: 0.20594278, Gradient norm: 376.79021695
INFO:root:[  442] Training loss: 0.20991335, Validation loss: 0.20329945, Gradient norm: 366.87148049
INFO:root:[  443] Training loss: 0.21123912, Validation loss: 0.20398916, Gradient norm: 375.35949924
INFO:root:[  444] Training loss: 0.20877744, Validation loss: 0.20352043, Gradient norm: 316.50512355
INFO:root:[  445] Training loss: 0.20963631, Validation loss: 0.20268549, Gradient norm: 368.85971817
INFO:root:[  446] Training loss: 0.20859454, Validation loss: 0.20327154, Gradient norm: 352.10164361
INFO:root:[  447] Training loss: 0.20763825, Validation loss: 0.21976165, Gradient norm: 325.49046195
INFO:root:[  448] Training loss: 0.20852232, Validation loss: 0.21003087, Gradient norm: 368.98774714
INFO:root:[  449] Training loss: 0.21226750, Validation loss: 0.20451191, Gradient norm: 439.47764141
INFO:root:[  450] Training loss: 0.20597974, Validation loss: 0.20084999, Gradient norm: 309.83449378
INFO:root:[  451] Training loss: 0.20931601, Validation loss: 0.20122571, Gradient norm: 387.67879607
INFO:root:[  452] Training loss: 0.20831749, Validation loss: 0.20436480, Gradient norm: 360.11479084
INFO:root:[  453] Training loss: 0.20813152, Validation loss: 0.20431962, Gradient norm: 356.64944909
INFO:root:[  454] Training loss: 0.20244786, Validation loss: 0.19967167, Gradient norm: 226.07230185
INFO:root:[  455] Training loss: 0.20573519, Validation loss: 0.20670581, Gradient norm: 357.57255165
INFO:root:[  456] Training loss: 0.20637466, Validation loss: 0.21053199, Gradient norm: 365.29185765
INFO:root:[  457] Training loss: 0.20672551, Validation loss: 0.20061452, Gradient norm: 397.55482538
INFO:root:[  458] Training loss: 0.20597740, Validation loss: 0.21465107, Gradient norm: 364.26042653
INFO:root:[  459] Training loss: 0.20404856, Validation loss: 0.20025153, Gradient norm: 336.10217241
INFO:root:[  460] Training loss: 0.20334279, Validation loss: 0.19881386, Gradient norm: 329.41998050
INFO:root:[  461] Training loss: 0.20548274, Validation loss: 0.22172753, Gradient norm: 406.48368816
INFO:root:[  462] Training loss: 0.20297076, Validation loss: 0.19878638, Gradient norm: 314.62779269
INFO:root:[  463] Training loss: 0.20566090, Validation loss: 0.21906485, Gradient norm: 387.19896958
INFO:root:[  464] Training loss: 0.20605744, Validation loss: 0.19668199, Gradient norm: 396.29400567
INFO:root:[  465] Training loss: 0.20083689, Validation loss: 0.19696652, Gradient norm: 294.02654364
INFO:root:[  466] Training loss: 0.19913633, Validation loss: 0.24933330, Gradient norm: 242.66579763
INFO:root:[  467] Training loss: 0.20381796, Validation loss: 0.19883716, Gradient norm: 363.92251314
INFO:root:[  468] Training loss: 0.20307920, Validation loss: 0.19762189, Gradient norm: 395.77340480
INFO:root:[  469] Training loss: 0.20138087, Validation loss: 0.21312609, Gradient norm: 370.89698989
INFO:root:[  470] Training loss: 0.20085819, Validation loss: 0.19463184, Gradient norm: 322.72732510
INFO:root:[  471] Training loss: 0.19822380, Validation loss: 0.21566348, Gradient norm: 276.69135182
INFO:root:[  472] Training loss: 0.20519773, Validation loss: 0.20145307, Gradient norm: 449.76846242
INFO:root:[  473] Training loss: 0.19608590, Validation loss: 0.20542481, Gradient norm: 213.42573767
INFO:root:[  474] Training loss: 0.19889666, Validation loss: 0.19971637, Gradient norm: 325.80356651
INFO:root:[  475] Training loss: 0.19922275, Validation loss: 0.19907183, Gradient norm: 347.40276150
INFO:root:[  476] Training loss: 0.19987329, Validation loss: 0.19510207, Gradient norm: 370.54991673
INFO:root:[  477] Training loss: 0.19788603, Validation loss: 0.19221919, Gradient norm: 319.52595723
INFO:root:[  478] Training loss: 0.19850071, Validation loss: 0.19282208, Gradient norm: 321.38073084
INFO:root:[  479] Training loss: 0.19806496, Validation loss: 0.19446999, Gradient norm: 334.89323717
INFO:root:[  480] Training loss: 0.20083541, Validation loss: 0.19232230, Gradient norm: 474.49239649
INFO:root:[  481] Training loss: 0.19699041, Validation loss: 0.19189513, Gradient norm: 326.92788277
INFO:root:[  482] Training loss: 0.19807768, Validation loss: 0.19079699, Gradient norm: 351.46658181
INFO:root:[  483] Training loss: 0.19778562, Validation loss: 0.20275890, Gradient norm: 339.15055394
INFO:root:[  484] Training loss: 0.19279625, Validation loss: 0.19560508, Gradient norm: 219.85595699
INFO:root:[  485] Training loss: 0.20443646, Validation loss: 0.19424137, Gradient norm: 498.02403746
INFO:root:[  486] Training loss: 0.19170518, Validation loss: 0.19843783, Gradient norm: 189.41370011
INFO:root:[  487] Training loss: 0.19687222, Validation loss: 0.19196320, Gradient norm: 372.93710333
INFO:root:[  488] Training loss: 0.19604260, Validation loss: 0.19722612, Gradient norm: 355.58461899
INFO:root:[  489] Training loss: 0.19641330, Validation loss: 0.19863092, Gradient norm: 364.76055365
INFO:root:[  490] Training loss: 0.19312419, Validation loss: 0.20803775, Gradient norm: 259.10221360
INFO:root:[  491] Training loss: 0.19529629, Validation loss: 0.19279788, Gradient norm: 375.63591350
INFO:root:EP 491: Early stopping
INFO:root:Training the model took 7841.862s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 3.45847
INFO:root:EnergyScoreTrain: 2.34681
INFO:root:CoverageTrain: 0.94303
INFO:root:IntervalWidthTrain: 0.14514
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 3.32815
INFO:root:EnergyScoreValidation: 2.25711
INFO:root:CoverageValidation: 0.94251
INFO:root:IntervalWidthValidation: 0.1451
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.73822
INFO:root:EnergyScoreTest: 1.85957
INFO:root:CoverageTest: 0.94385
INFO:root:IntervalWidthTest: 0.14468
INFO:root:###19 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 369098752
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.51360215, Validation loss: 1.51773763, Gradient norm: 7.32257825
INFO:root:[    2] Training loss: 1.30049310, Validation loss: 1.17215902, Gradient norm: 3.27336830
INFO:root:[    3] Training loss: 1.11654407, Validation loss: 1.05516322, Gradient norm: 2.68696855
INFO:root:[    4] Training loss: 1.02114300, Validation loss: 1.00353302, Gradient norm: 2.97527574
INFO:root:[    5] Training loss: 0.97094326, Validation loss: 0.93904880, Gradient norm: 2.80260750
INFO:root:[    6] Training loss: 0.89213590, Validation loss: 0.89580870, Gradient norm: 3.09939381
INFO:root:[    7] Training loss: 0.87024637, Validation loss: 0.87901251, Gradient norm: 2.98340149
INFO:root:[    8] Training loss: 0.83786275, Validation loss: 0.81202300, Gradient norm: 2.64892987
INFO:root:[    9] Training loss: 0.82005528, Validation loss: 0.81702300, Gradient norm: 2.35080286
INFO:root:[   10] Training loss: 0.81539053, Validation loss: 0.78635387, Gradient norm: 2.78867078
INFO:root:[   11] Training loss: 0.78764035, Validation loss: 0.78615342, Gradient norm: 3.00558544
INFO:root:[   12] Training loss: 0.76104308, Validation loss: 0.76409844, Gradient norm: 3.09795800
INFO:root:[   13] Training loss: 0.75173267, Validation loss: 0.73057804, Gradient norm: 2.51093760
INFO:root:[   14] Training loss: 0.72505944, Validation loss: 0.71626167, Gradient norm: 2.62023524
INFO:root:[   15] Training loss: 0.70863332, Validation loss: 0.69987765, Gradient norm: 2.84289764
INFO:root:[   16] Training loss: 0.69323290, Validation loss: 0.69972119, Gradient norm: 2.75179200
INFO:root:[   17] Training loss: 0.68069308, Validation loss: 0.67111448, Gradient norm: 2.55752362
INFO:root:[   18] Training loss: 0.66888416, Validation loss: 0.66040968, Gradient norm: 2.52663387
INFO:root:[   19] Training loss: 0.66468072, Validation loss: 0.65080934, Gradient norm: 2.93935388
INFO:root:[   20] Training loss: 0.63580896, Validation loss: 0.63113376, Gradient norm: 2.19436696
INFO:root:[   21] Training loss: 0.62610591, Validation loss: 0.61703104, Gradient norm: 1.59051554
INFO:root:[   22] Training loss: 0.61298554, Validation loss: 0.62286875, Gradient norm: 2.29546731
INFO:root:[   23] Training loss: 0.60298604, Validation loss: 0.60513624, Gradient norm: 2.37023209
INFO:root:[   24] Training loss: 0.59779674, Validation loss: 0.58852899, Gradient norm: 3.10685436
INFO:root:[   25] Training loss: 0.58337817, Validation loss: 0.58057602, Gradient norm: 2.40286322
INFO:root:[   26] Training loss: 0.57433341, Validation loss: 0.57552943, Gradient norm: 2.01510661
INFO:root:[   27] Training loss: 0.56729859, Validation loss: 0.56104091, Gradient norm: 1.92713842
INFO:root:[   28] Training loss: 0.55764115, Validation loss: 0.57053714, Gradient norm: 1.94761624
INFO:root:[   29] Training loss: 0.55473357, Validation loss: 0.55584020, Gradient norm: 3.53731511
INFO:root:[   30] Training loss: 0.54547857, Validation loss: 0.54282928, Gradient norm: 2.36363754
INFO:root:[   31] Training loss: 0.53577626, Validation loss: 0.53032366, Gradient norm: 2.13896189
INFO:root:[   32] Training loss: 0.52402072, Validation loss: 0.51960711, Gradient norm: 2.00341827
INFO:root:[   33] Training loss: 0.52288055, Validation loss: 0.51896934, Gradient norm: 2.48802911
INFO:root:[   34] Training loss: 0.51912885, Validation loss: 0.51099013, Gradient norm: 3.16914440
INFO:root:[   35] Training loss: 0.51238028, Validation loss: 0.51240656, Gradient norm: 2.19241394
INFO:root:[   36] Training loss: 0.50230223, Validation loss: 0.50555363, Gradient norm: 1.85667179
INFO:root:[   37] Training loss: 0.49754444, Validation loss: 0.49128564, Gradient norm: 1.49762785
INFO:root:[   38] Training loss: 0.49611342, Validation loss: 0.49096014, Gradient norm: 1.69961177
INFO:root:[   39] Training loss: 0.48695310, Validation loss: 0.48975062, Gradient norm: 1.79714488
INFO:root:[   40] Training loss: 0.48296343, Validation loss: 0.47946804, Gradient norm: 1.75760443
INFO:root:[   41] Training loss: 0.47974573, Validation loss: 0.48063277, Gradient norm: 1.54587711
INFO:root:[   42] Training loss: 0.47231084, Validation loss: 0.47893569, Gradient norm: 2.41706155
INFO:root:[   43] Training loss: 0.47212319, Validation loss: 0.46995797, Gradient norm: 2.96045513
INFO:root:[   44] Training loss: 0.46612395, Validation loss: 0.45825594, Gradient norm: 2.50300563
INFO:root:[   45] Training loss: 0.46170224, Validation loss: 0.45876152, Gradient norm: 2.06278936
INFO:root:[   46] Training loss: 0.45671606, Validation loss: 0.45636109, Gradient norm: 2.02898205
INFO:root:[   47] Training loss: 0.44852257, Validation loss: 0.44585393, Gradient norm: 1.71611859
INFO:root:[   48] Training loss: 0.44640152, Validation loss: 0.44865770, Gradient norm: 1.92856284
INFO:root:[   49] Training loss: 0.44117637, Validation loss: 0.43994892, Gradient norm: 1.27562321
INFO:root:[   50] Training loss: 0.43940994, Validation loss: 0.43756579, Gradient norm: 1.92399357
INFO:root:[   51] Training loss: 0.43882775, Validation loss: 0.43295318, Gradient norm: 2.44680637
INFO:root:[   52] Training loss: 0.43290700, Validation loss: 0.42993775, Gradient norm: 2.40904247
INFO:root:[   53] Training loss: 0.42593804, Validation loss: 0.42905905, Gradient norm: 1.04549390
INFO:root:[   54] Training loss: 0.42369345, Validation loss: 0.42585015, Gradient norm: 1.35136684
INFO:root:[   55] Training loss: 0.42039318, Validation loss: 0.41768331, Gradient norm: 1.32517696
INFO:root:[   56] Training loss: 0.41759092, Validation loss: 0.40995005, Gradient norm: 1.42428219
INFO:root:[   57] Training loss: 0.41153965, Validation loss: 0.41520206, Gradient norm: 0.95821239
INFO:root:[   58] Training loss: 0.40642446, Validation loss: 0.40236622, Gradient norm: 1.22637723
INFO:root:[   59] Training loss: 0.40206173, Validation loss: 0.40158422, Gradient norm: 1.04198186
INFO:root:[   60] Training loss: 0.40042763, Validation loss: 0.39636936, Gradient norm: 1.73982875
INFO:root:[   61] Training loss: 0.39664834, Validation loss: 0.39752303, Gradient norm: 1.52705054
INFO:root:[   62] Training loss: 0.39106072, Validation loss: 0.38748557, Gradient norm: 1.22927303
INFO:root:[   63] Training loss: 0.38793017, Validation loss: 0.38770791, Gradient norm: 1.71386569
INFO:root:[   64] Training loss: 0.38714532, Validation loss: 0.38101477, Gradient norm: 2.42959584
INFO:root:[   65] Training loss: 0.38085214, Validation loss: 0.37897606, Gradient norm: 1.86898630
INFO:root:[   66] Training loss: 0.37813006, Validation loss: 0.37614319, Gradient norm: 1.52053085
INFO:root:[   67] Training loss: 0.37442418, Validation loss: 0.37114534, Gradient norm: 1.44748481
INFO:root:[   68] Training loss: 0.37257392, Validation loss: 0.36965392, Gradient norm: 1.77097881
INFO:root:[   69] Training loss: 0.36786703, Validation loss: 0.36619001, Gradient norm: 1.24550791
INFO:root:[   70] Training loss: 0.36319953, Validation loss: 0.36437146, Gradient norm: 1.15501773
INFO:root:[   71] Training loss: 0.36053833, Validation loss: 0.36306191, Gradient norm: 1.57891520
INFO:root:[   72] Training loss: 0.35910316, Validation loss: 0.35378057, Gradient norm: 1.58012738
INFO:root:[   73] Training loss: 0.35479782, Validation loss: 0.35307678, Gradient norm: 1.14279058
INFO:root:[   74] Training loss: 0.35212198, Validation loss: 0.35116698, Gradient norm: 1.78211802
INFO:root:[   75] Training loss: 0.34873935, Validation loss: 0.34686793, Gradient norm: 1.22340959
INFO:root:[   76] Training loss: 0.34582492, Validation loss: 0.34222044, Gradient norm: 1.41921619
INFO:root:[   77] Training loss: 0.34236425, Validation loss: 0.34315275, Gradient norm: 1.46869517
INFO:root:[   78] Training loss: 0.34014363, Validation loss: 0.34500823, Gradient norm: 1.63394365
INFO:root:[   79] Training loss: 0.33861334, Validation loss: 0.33676002, Gradient norm: 1.49066110
INFO:root:[   80] Training loss: 0.33708390, Validation loss: 0.33351778, Gradient norm: 1.89733819
INFO:root:[   81] Training loss: 0.33292449, Validation loss: 0.33356255, Gradient norm: 1.01504081
INFO:root:[   82] Training loss: 0.33092129, Validation loss: 0.32913397, Gradient norm: 1.43407749
INFO:root:[   83] Training loss: 0.32814521, Validation loss: 0.32816376, Gradient norm: 1.61940883
INFO:root:[   84] Training loss: 0.32761377, Validation loss: 0.32659035, Gradient norm: 2.15878624
INFO:root:[   85] Training loss: 0.32596295, Validation loss: 0.32451957, Gradient norm: 1.82145309
INFO:root:[   86] Training loss: 0.32209731, Validation loss: 0.31954851, Gradient norm: 1.44950886
INFO:root:[   87] Training loss: 0.32097579, Validation loss: 0.31902488, Gradient norm: 1.02529333
INFO:root:[   88] Training loss: 0.31643334, Validation loss: 0.31590710, Gradient norm: 1.11623731
INFO:root:[   89] Training loss: 0.31450712, Validation loss: 0.31519484, Gradient norm: 1.08503478
INFO:root:[   90] Training loss: 0.31427205, Validation loss: 0.31184762, Gradient norm: 1.65343330
INFO:root:[   91] Training loss: 0.31234649, Validation loss: 0.31193599, Gradient norm: 1.29316879
INFO:root:[   92] Training loss: 0.31004646, Validation loss: 0.31124000, Gradient norm: 1.02804049
INFO:root:[   93] Training loss: 0.30829588, Validation loss: 0.30995308, Gradient norm: 1.81749122
INFO:root:[   94] Training loss: 0.30615284, Validation loss: 0.30757944, Gradient norm: 1.38741820
INFO:root:[   95] Training loss: 0.30445422, Validation loss: 0.30513805, Gradient norm: 1.78634139
INFO:root:[   96] Training loss: 0.30180215, Validation loss: 0.30412777, Gradient norm: 1.49183546
INFO:root:[   97] Training loss: 0.30104632, Validation loss: 0.29852588, Gradient norm: 1.08081142
INFO:root:[   98] Training loss: 0.29868448, Validation loss: 0.29834415, Gradient norm: 1.24099155
INFO:root:[   99] Training loss: 0.29765951, Validation loss: 0.29685146, Gradient norm: 1.48657666
INFO:root:[  100] Training loss: 0.29638860, Validation loss: 0.29571386, Gradient norm: 1.50661002
INFO:root:[  101] Training loss: 0.29591510, Validation loss: 0.29483110, Gradient norm: 1.35327157
INFO:root:[  102] Training loss: 0.29474587, Validation loss: 0.29452818, Gradient norm: 1.69601413
INFO:root:[  103] Training loss: 0.29354055, Validation loss: 0.29138745, Gradient norm: 1.79252786
INFO:root:[  104] Training loss: 0.29065269, Validation loss: 0.29057193, Gradient norm: 1.05511319
INFO:root:[  105] Training loss: 0.28916439, Validation loss: 0.29057718, Gradient norm: 1.56992117
INFO:root:[  106] Training loss: 0.28859747, Validation loss: 0.28861678, Gradient norm: 1.71046589
INFO:root:[  107] Training loss: 0.28702882, Validation loss: 0.28688187, Gradient norm: 1.34233110
INFO:root:[  108] Training loss: 0.28545724, Validation loss: 0.28397755, Gradient norm: 1.26002260
INFO:root:[  109] Training loss: 0.28372806, Validation loss: 0.28335719, Gradient norm: 1.14271112
INFO:root:[  110] Training loss: 0.28367658, Validation loss: 0.28326431, Gradient norm: 1.74677909
INFO:root:[  111] Training loss: 0.28144662, Validation loss: 0.28179495, Gradient norm: 1.02210953
INFO:root:[  112] Training loss: 0.28136434, Validation loss: 0.27982484, Gradient norm: 1.68587793
INFO:root:[  113] Training loss: 0.27953191, Validation loss: 0.27964582, Gradient norm: 1.27080344
INFO:root:[  114] Training loss: 0.27819913, Validation loss: 0.27808706, Gradient norm: 1.45471685
INFO:root:[  115] Training loss: 0.27655360, Validation loss: 0.27987668, Gradient norm: 1.34685341
INFO:root:[  116] Training loss: 0.27662950, Validation loss: 0.27495684, Gradient norm: 2.22743684
INFO:root:[  117] Training loss: 0.27514401, Validation loss: 0.27536521, Gradient norm: 2.41568570
INFO:root:[  118] Training loss: 0.27320924, Validation loss: 0.27317347, Gradient norm: 1.27983603
INFO:root:[  119] Training loss: 0.27236548, Validation loss: 0.27182254, Gradient norm: 2.25993370
INFO:root:[  120] Training loss: 0.27157823, Validation loss: 0.27220756, Gradient norm: 1.31446453
INFO:root:[  121] Training loss: 0.27046347, Validation loss: 0.27013912, Gradient norm: 2.18151813
INFO:root:[  122] Training loss: 0.26864506, Validation loss: 0.26759353, Gradient norm: 1.91789911
INFO:root:[  123] Training loss: 0.26888815, Validation loss: 0.26842446, Gradient norm: 2.22184402
INFO:root:[  124] Training loss: 0.26768005, Validation loss: 0.26860038, Gradient norm: 1.76728660
INFO:root:[  125] Training loss: 0.26672108, Validation loss: 0.26594078, Gradient norm: 1.76385018
INFO:root:[  126] Training loss: 0.26510771, Validation loss: 0.26398992, Gradient norm: 1.82397241
INFO:root:[  127] Training loss: 0.26479576, Validation loss: 0.26468869, Gradient norm: 1.72190939
INFO:root:[  128] Training loss: 0.26327823, Validation loss: 0.26299997, Gradient norm: 1.64815841
INFO:root:[  129] Training loss: 0.26394606, Validation loss: 0.26357923, Gradient norm: 2.29552697
INFO:root:[  130] Training loss: 0.26285592, Validation loss: 0.26127367, Gradient norm: 2.60586289
INFO:root:[  131] Training loss: 0.26054000, Validation loss: 0.26013238, Gradient norm: 1.75547324
INFO:root:[  132] Training loss: 0.25908907, Validation loss: 0.26170070, Gradient norm: 1.80002527
INFO:root:[  133] Training loss: 0.25884104, Validation loss: 0.25813091, Gradient norm: 2.18467275
INFO:root:[  134] Training loss: 0.25772432, Validation loss: 0.25935386, Gradient norm: 1.30091174
INFO:root:[  135] Training loss: 0.25700997, Validation loss: 0.25683045, Gradient norm: 1.77230745
INFO:root:[  136] Training loss: 0.25580567, Validation loss: 0.25590294, Gradient norm: 2.85549657
INFO:root:[  137] Training loss: 0.25619267, Validation loss: 0.25535395, Gradient norm: 2.02050037
INFO:root:[  138] Training loss: 0.25506387, Validation loss: 0.25868977, Gradient norm: 2.79994742
INFO:root:[  139] Training loss: 0.25313619, Validation loss: 0.25256559, Gradient norm: 2.93855238
INFO:root:[  140] Training loss: 0.25203209, Validation loss: 0.25224334, Gradient norm: 2.52349228
INFO:root:[  141] Training loss: 0.25166567, Validation loss: 0.25152121, Gradient norm: 2.52636572
INFO:root:[  142] Training loss: 0.24998822, Validation loss: 0.24941617, Gradient norm: 2.66031387
INFO:root:[  143] Training loss: 0.24979841, Validation loss: 0.25004324, Gradient norm: 2.16205471
INFO:root:[  144] Training loss: 0.24833452, Validation loss: 0.24781462, Gradient norm: 1.61474415
INFO:root:[  145] Training loss: 0.24803428, Validation loss: 0.24683034, Gradient norm: 2.72745363
INFO:root:[  146] Training loss: 0.24619617, Validation loss: 0.24649446, Gradient norm: 1.72184782
INFO:root:[  147] Training loss: 0.24508748, Validation loss: 0.24638101, Gradient norm: 2.23044462
INFO:root:[  148] Training loss: 0.24474491, Validation loss: 0.24383013, Gradient norm: 2.77565635
INFO:root:[  149] Training loss: 0.24359863, Validation loss: 0.24437883, Gradient norm: 2.21547911
INFO:root:[  150] Training loss: 0.24384840, Validation loss: 0.24338002, Gradient norm: 3.17877788
INFO:root:[  151] Training loss: 0.24330773, Validation loss: 0.24256367, Gradient norm: 2.34259677
INFO:root:[  152] Training loss: 0.24190678, Validation loss: 0.24181583, Gradient norm: 1.67599794
INFO:root:[  153] Training loss: 0.24162430, Validation loss: 0.24114121, Gradient norm: 2.99462514
INFO:root:[  154] Training loss: 0.24056942, Validation loss: 0.24090161, Gradient norm: 1.92177164
INFO:root:[  155] Training loss: 0.23872686, Validation loss: 0.23837903, Gradient norm: 3.08993984
INFO:root:[  156] Training loss: 0.23798229, Validation loss: 0.23821599, Gradient norm: 3.26181637
INFO:root:[  157] Training loss: 0.23787337, Validation loss: 0.23640439, Gradient norm: 3.56764378
INFO:root:[  158] Training loss: 0.23587736, Validation loss: 0.23648624, Gradient norm: 2.07883977
INFO:root:[  159] Training loss: 0.23511475, Validation loss: 0.23383982, Gradient norm: 2.07804947
INFO:root:[  160] Training loss: 0.23367845, Validation loss: 0.23335779, Gradient norm: 2.83398094
INFO:root:[  161] Training loss: 0.23292818, Validation loss: 0.23210883, Gradient norm: 2.40652959
INFO:root:[  162] Training loss: 0.23213397, Validation loss: 0.23368513, Gradient norm: 3.12616271
INFO:root:[  163] Training loss: 0.23107266, Validation loss: 0.23099180, Gradient norm: 2.94902431
INFO:root:[  164] Training loss: 0.23017206, Validation loss: 0.22935259, Gradient norm: 2.58787663
INFO:root:[  165] Training loss: 0.22926449, Validation loss: 0.22827053, Gradient norm: 3.28800771
INFO:root:[  166] Training loss: 0.22751312, Validation loss: 0.22925881, Gradient norm: 3.08559194
INFO:root:[  167] Training loss: 0.22685013, Validation loss: 0.22635043, Gradient norm: 2.40846843
INFO:root:[  168] Training loss: 0.22626304, Validation loss: 0.22605784, Gradient norm: 2.18048518
INFO:root:[  169] Training loss: 0.22494235, Validation loss: 0.22471817, Gradient norm: 3.22791066
INFO:root:[  170] Training loss: 0.22354329, Validation loss: 0.22481810, Gradient norm: 3.23627160
INFO:root:[  171] Training loss: 0.22374583, Validation loss: 0.22601552, Gradient norm: 2.75976374
INFO:root:[  172] Training loss: 0.22269586, Validation loss: 0.22186762, Gradient norm: 3.21420532
INFO:root:[  173] Training loss: 0.22040849, Validation loss: 0.21998633, Gradient norm: 2.46085747
INFO:root:[  174] Training loss: 0.22021610, Validation loss: 0.22066187, Gradient norm: 3.90392947
INFO:root:[  175] Training loss: 0.21963500, Validation loss: 0.21947455, Gradient norm: 2.45987101
INFO:root:[  176] Training loss: 0.21871687, Validation loss: 0.21751343, Gradient norm: 2.79633362
INFO:root:[  177] Training loss: 0.21757626, Validation loss: 0.21671766, Gradient norm: 3.28766300
INFO:root:[  178] Training loss: 0.21610134, Validation loss: 0.21652066, Gradient norm: 2.33861204
INFO:root:[  179] Training loss: 0.21563096, Validation loss: 0.21537703, Gradient norm: 3.12655123
INFO:root:[  180] Training loss: 0.21410882, Validation loss: 0.21439690, Gradient norm: 2.47736131
INFO:root:[  181] Training loss: 0.21371507, Validation loss: 0.21396020, Gradient norm: 2.95028150
INFO:root:[  182] Training loss: 0.21308631, Validation loss: 0.21210127, Gradient norm: 3.88789395
INFO:root:[  183] Training loss: 0.21129472, Validation loss: 0.21120428, Gradient norm: 3.40407152
INFO:root:[  184] Training loss: 0.21173832, Validation loss: 0.21075851, Gradient norm: 5.86202732
INFO:root:[  185] Training loss: 0.20982860, Validation loss: 0.21006439, Gradient norm: 4.46446954
INFO:root:[  186] Training loss: 0.20947623, Validation loss: 0.20910604, Gradient norm: 4.12370242
INFO:root:[  187] Training loss: 0.20796974, Validation loss: 0.20844647, Gradient norm: 3.71057005
INFO:root:[  188] Training loss: 0.20655489, Validation loss: 0.20702258, Gradient norm: 2.14477500
INFO:root:[  189] Training loss: 0.20609518, Validation loss: 0.20734668, Gradient norm: 3.10497994
INFO:root:[  190] Training loss: 0.20488236, Validation loss: 0.20545389, Gradient norm: 3.82751246
INFO:root:[  191] Training loss: 0.20436231, Validation loss: 0.20471332, Gradient norm: 2.92541402
INFO:root:[  192] Training loss: 0.20353346, Validation loss: 0.20398731, Gradient norm: 2.32769322
INFO:root:[  193] Training loss: 0.20240852, Validation loss: 0.20358330, Gradient norm: 3.03292954
INFO:root:[  194] Training loss: 0.20196054, Validation loss: 0.20166603, Gradient norm: 3.79876591
INFO:root:[  195] Training loss: 0.20096602, Validation loss: 0.20060493, Gradient norm: 3.53757854
INFO:root:[  196] Training loss: 0.19962923, Validation loss: 0.19860065, Gradient norm: 3.38588709
INFO:root:[  197] Training loss: 0.19870905, Validation loss: 0.19810349, Gradient norm: 3.57840571
INFO:root:[  198] Training loss: 0.19778774, Validation loss: 0.19670135, Gradient norm: 3.99275300
INFO:root:[  199] Training loss: 0.19737465, Validation loss: 0.19586130, Gradient norm: 4.92462427
INFO:root:[  200] Training loss: 0.19555118, Validation loss: 0.19597398, Gradient norm: 4.00440726
INFO:root:[  201] Training loss: 0.19519137, Validation loss: 0.19619609, Gradient norm: 4.48041540
INFO:root:[  202] Training loss: 0.19413054, Validation loss: 0.19442133, Gradient norm: 3.03472800
INFO:root:[  203] Training loss: 0.19356512, Validation loss: 0.19370256, Gradient norm: 3.53174751
INFO:root:[  204] Training loss: 0.19246313, Validation loss: 0.19209565, Gradient norm: 2.20999418
INFO:root:[  205] Training loss: 0.19145409, Validation loss: 0.19094804, Gradient norm: 3.51319958
INFO:root:[  206] Training loss: 0.19108944, Validation loss: 0.18984477, Gradient norm: 5.48856169
INFO:root:[  207] Training loss: 0.18928620, Validation loss: 0.18951041, Gradient norm: 4.44295142
INFO:root:[  208] Training loss: 0.18932077, Validation loss: 0.18875683, Gradient norm: 3.45631672
INFO:root:[  209] Training loss: 0.18819042, Validation loss: 0.18839214, Gradient norm: 2.46471205
INFO:root:[  210] Training loss: 0.18760118, Validation loss: 0.18754217, Gradient norm: 4.63922036
INFO:root:[  211] Training loss: 0.18651907, Validation loss: 0.18821994, Gradient norm: 4.87155699
INFO:root:[  212] Training loss: 0.18565841, Validation loss: 0.18560108, Gradient norm: 3.83683749
INFO:root:[  213] Training loss: 0.18446696, Validation loss: 0.18590612, Gradient norm: 2.21192237
INFO:root:[  214] Training loss: 0.18461576, Validation loss: 0.18371419, Gradient norm: 5.94809918
INFO:root:[  215] Training loss: 0.18333560, Validation loss: 0.18321061, Gradient norm: 4.67518417
INFO:root:[  216] Training loss: 0.18214735, Validation loss: 0.18164134, Gradient norm: 4.13359736
INFO:root:[  217] Training loss: 0.18100686, Validation loss: 0.18248096, Gradient norm: 2.91123029
INFO:root:[  218] Training loss: 0.17988215, Validation loss: 0.17913408, Gradient norm: 3.30796307
INFO:root:[  219] Training loss: 0.17946068, Validation loss: 0.17986912, Gradient norm: 3.72709948
INFO:root:[  220] Training loss: 0.17836637, Validation loss: 0.17778946, Gradient norm: 3.18356640
INFO:root:[  221] Training loss: 0.17755853, Validation loss: 0.17759636, Gradient norm: 3.10228094
INFO:root:[  222] Training loss: 0.17668497, Validation loss: 0.17671111, Gradient norm: 4.75709260
INFO:root:[  223] Training loss: 0.17584224, Validation loss: 0.17650299, Gradient norm: 3.09595826
INFO:root:[  224] Training loss: 0.17532997, Validation loss: 0.17538870, Gradient norm: 6.17551032
INFO:root:[  225] Training loss: 0.17489202, Validation loss: 0.17465874, Gradient norm: 6.55146957
INFO:root:[  226] Training loss: 0.17339959, Validation loss: 0.17358654, Gradient norm: 4.43720554
INFO:root:[  227] Training loss: 0.17307520, Validation loss: 0.17271131, Gradient norm: 5.52867259
INFO:root:[  228] Training loss: 0.17187315, Validation loss: 0.17186090, Gradient norm: 2.81654981
INFO:root:[  229] Training loss: 0.17186829, Validation loss: 0.17115928, Gradient norm: 6.65974607
INFO:root:[  230] Training loss: 0.17077481, Validation loss: 0.17025535, Gradient norm: 5.05431334
INFO:root:[  231] Training loss: 0.16919819, Validation loss: 0.16896820, Gradient norm: 2.87890604
INFO:root:[  232] Training loss: 0.16889033, Validation loss: 0.16976372, Gradient norm: 5.82750350
INFO:root:[  233] Training loss: 0.16858189, Validation loss: 0.16748562, Gradient norm: 5.26152528
INFO:root:[  234] Training loss: 0.16743713, Validation loss: 0.16700482, Gradient norm: 3.62407906
INFO:root:[  235] Training loss: 0.16633697, Validation loss: 0.16658056, Gradient norm: 3.81866699
INFO:root:[  236] Training loss: 0.16592638, Validation loss: 0.16807093, Gradient norm: 3.85501719
INFO:root:[  237] Training loss: 0.16491169, Validation loss: 0.16494513, Gradient norm: 5.11057259
INFO:root:[  238] Training loss: 0.16418962, Validation loss: 0.16419754, Gradient norm: 5.22794414
INFO:root:[  239] Training loss: 0.16332435, Validation loss: 0.16267491, Gradient norm: 3.51502694
INFO:root:[  240] Training loss: 0.16267586, Validation loss: 0.16345184, Gradient norm: 2.56647446
INFO:root:[  241] Training loss: 0.16224760, Validation loss: 0.16239817, Gradient norm: 3.57776866
INFO:root:[  242] Training loss: 0.16112968, Validation loss: 0.16121957, Gradient norm: 4.89866288
INFO:root:[  243] Training loss: 0.16071535, Validation loss: 0.16083692, Gradient norm: 3.93081979
INFO:root:[  244] Training loss: 0.15979078, Validation loss: 0.16119955, Gradient norm: 4.02931689
INFO:root:[  245] Training loss: 0.15952034, Validation loss: 0.15862744, Gradient norm: 7.78114578
INFO:root:[  246] Training loss: 0.15782755, Validation loss: 0.15794266, Gradient norm: 3.67685244
INFO:root:[  247] Training loss: 0.15741068, Validation loss: 0.15811182, Gradient norm: 3.57504485
INFO:root:[  248] Training loss: 0.15675703, Validation loss: 0.15679924, Gradient norm: 5.24526714
INFO:root:[  249] Training loss: 0.15676523, Validation loss: 0.15576018, Gradient norm: 6.03725725
INFO:root:[  250] Training loss: 0.15546034, Validation loss: 0.15556033, Gradient norm: 4.66765473
INFO:root:[  251] Training loss: 0.15454709, Validation loss: 0.15518977, Gradient norm: 5.40315752
INFO:root:[  252] Training loss: 0.15408104, Validation loss: 0.15353556, Gradient norm: 3.51741150
INFO:root:[  253] Training loss: 0.15332252, Validation loss: 0.15327378, Gradient norm: 3.72293813
INFO:root:[  254] Training loss: 0.15231812, Validation loss: 0.15345081, Gradient norm: 2.34875768
INFO:root:[  255] Training loss: 0.15136516, Validation loss: 0.15175574, Gradient norm: 3.88859064
INFO:root:[  256] Training loss: 0.15079981, Validation loss: 0.15081052, Gradient norm: 5.65389442
INFO:root:[  257] Training loss: 0.15002625, Validation loss: 0.14957742, Gradient norm: 2.88481853
INFO:root:[  258] Training loss: 0.14951667, Validation loss: 0.14918429, Gradient norm: 5.19588657
INFO:root:[  259] Training loss: 0.14866875, Validation loss: 0.14784698, Gradient norm: 6.28124791
INFO:root:[  260] Training loss: 0.14806362, Validation loss: 0.14748141, Gradient norm: 4.07650658
INFO:root:[  261] Training loss: 0.14692286, Validation loss: 0.14760489, Gradient norm: 3.92866917
INFO:root:[  262] Training loss: 0.14631284, Validation loss: 0.14611331, Gradient norm: 5.01935836
INFO:root:[  263] Training loss: 0.14582297, Validation loss: 0.14610057, Gradient norm: 4.35618332
INFO:root:[  264] Training loss: 0.14505104, Validation loss: 0.14520825, Gradient norm: 4.81332859
INFO:root:[  265] Training loss: 0.14398137, Validation loss: 0.14533149, Gradient norm: 3.80932237
INFO:root:[  266] Training loss: 0.14397893, Validation loss: 0.14361173, Gradient norm: 5.10412621
INFO:root:[  267] Training loss: 0.14333593, Validation loss: 0.14303707, Gradient norm: 8.18555324
INFO:root:[  268] Training loss: 0.14219283, Validation loss: 0.14234102, Gradient norm: 5.52080408
INFO:root:[  269] Training loss: 0.14158869, Validation loss: 0.14204587, Gradient norm: 5.34669565
INFO:root:[  270] Training loss: 0.14098565, Validation loss: 0.14131307, Gradient norm: 3.42394713
INFO:root:[  271] Training loss: 0.14090953, Validation loss: 0.13974077, Gradient norm: 7.90632055
INFO:root:[  272] Training loss: 0.13987533, Validation loss: 0.13934941, Gradient norm: 4.47641212
INFO:root:[  273] Training loss: 0.13927626, Validation loss: 0.13920977, Gradient norm: 4.90597643
INFO:root:[  274] Training loss: 0.13896346, Validation loss: 0.14047485, Gradient norm: 5.50443533
INFO:root:[  275] Training loss: 0.13817403, Validation loss: 0.13767436, Gradient norm: 5.92589745
INFO:root:[  276] Training loss: 0.13754599, Validation loss: 0.13740400, Gradient norm: 4.77234534
INFO:root:[  277] Training loss: 0.13664322, Validation loss: 0.13739978, Gradient norm: 5.44192907
INFO:root:[  278] Training loss: 0.13590207, Validation loss: 0.13535418, Gradient norm: 6.28221459
INFO:root:[  279] Training loss: 0.13511332, Validation loss: 0.13482665, Gradient norm: 4.60917061
INFO:root:[  280] Training loss: 0.13441565, Validation loss: 0.13531553, Gradient norm: 3.29805000
INFO:root:[  281] Training loss: 0.13370908, Validation loss: 0.13384442, Gradient norm: 5.51094117
INFO:root:[  282] Training loss: 0.13287604, Validation loss: 0.13387202, Gradient norm: 3.90097724
INFO:root:[  283] Training loss: 0.13287667, Validation loss: 0.13293856, Gradient norm: 6.53413244
INFO:root:[  284] Training loss: 0.13220924, Validation loss: 0.13162240, Gradient norm: 5.71015996
INFO:root:[  285] Training loss: 0.13147656, Validation loss: 0.13170824, Gradient norm: 3.48520707
INFO:root:[  286] Training loss: 0.13107320, Validation loss: 0.13067890, Gradient norm: 6.99446429
INFO:root:[  287] Training loss: 0.13016695, Validation loss: 0.12991928, Gradient norm: 6.49631698
INFO:root:[  288] Training loss: 0.12961985, Validation loss: 0.12964741, Gradient norm: 7.08787337
INFO:root:[  289] Training loss: 0.12936582, Validation loss: 0.12981617, Gradient norm: 5.12888079
INFO:root:[  290] Training loss: 0.12856070, Validation loss: 0.12875961, Gradient norm: 6.65908114
INFO:root:[  291] Training loss: 0.12821822, Validation loss: 0.12760208, Gradient norm: 8.03313213
INFO:root:[  292] Training loss: 0.12751916, Validation loss: 0.12679027, Gradient norm: 6.97988024
INFO:root:[  293] Training loss: 0.12691362, Validation loss: 0.12630127, Gradient norm: 7.41137961
INFO:root:[  294] Training loss: 0.12597131, Validation loss: 0.12596759, Gradient norm: 4.24007847
INFO:root:[  295] Training loss: 0.12544531, Validation loss: 0.12557584, Gradient norm: 3.35141264
INFO:root:[  296] Training loss: 0.12537627, Validation loss: 0.12565112, Gradient norm: 5.82575040
INFO:root:[  297] Training loss: 0.12465896, Validation loss: 0.12559485, Gradient norm: 6.51997562
INFO:root:[  298] Training loss: 0.12449468, Validation loss: 0.12427965, Gradient norm: 10.55040206
INFO:root:[  299] Training loss: 0.12360445, Validation loss: 0.12461160, Gradient norm: 7.65750735
INFO:root:[  300] Training loss: 0.12293212, Validation loss: 0.12276233, Gradient norm: 4.80746762
INFO:root:[  301] Training loss: 0.12264845, Validation loss: 0.12180766, Gradient norm: 5.95137541
INFO:root:[  302] Training loss: 0.12178810, Validation loss: 0.12235305, Gradient norm: 3.96266261
INFO:root:[  303] Training loss: 0.12147317, Validation loss: 0.12186270, Gradient norm: 3.02593957
INFO:root:[  304] Training loss: 0.12127418, Validation loss: 0.12185313, Gradient norm: 4.61718048
INFO:root:[  305] Training loss: 0.12036943, Validation loss: 0.12014272, Gradient norm: 5.51553966
INFO:root:[  306] Training loss: 0.11935619, Validation loss: 0.11937506, Gradient norm: 5.42745665
INFO:root:[  307] Training loss: 0.11907000, Validation loss: 0.11915182, Gradient norm: 5.08095176
INFO:root:[  308] Training loss: 0.11873432, Validation loss: 0.11893217, Gradient norm: 4.20265125
INFO:root:[  309] Training loss: 0.11826241, Validation loss: 0.11841347, Gradient norm: 3.33057201
INFO:root:[  310] Training loss: 0.11789900, Validation loss: 0.11686807, Gradient norm: 4.79891737
INFO:root:[  311] Training loss: 0.11719389, Validation loss: 0.11858120, Gradient norm: 4.88830057
INFO:root:[  312] Training loss: 0.11678988, Validation loss: 0.11758162, Gradient norm: 6.68522579
INFO:root:[  313] Training loss: 0.11585576, Validation loss: 0.11658027, Gradient norm: 6.54301359
INFO:root:[  314] Training loss: 0.11561270, Validation loss: 0.11563373, Gradient norm: 6.59899158
INFO:root:[  315] Training loss: 0.11484122, Validation loss: 0.11540089, Gradient norm: 3.34092244
INFO:root:[  316] Training loss: 0.11444725, Validation loss: 0.11467677, Gradient norm: 4.90315525
INFO:root:[  317] Training loss: 0.11384622, Validation loss: 0.11471998, Gradient norm: 4.89531749
INFO:root:[  318] Training loss: 0.11357162, Validation loss: 0.11312970, Gradient norm: 5.68731107
INFO:root:[  319] Training loss: 0.11383056, Validation loss: 0.11383532, Gradient norm: 10.42081132
INFO:root:[  320] Training loss: 0.11237689, Validation loss: 0.11274582, Gradient norm: 7.01664602
INFO:root:[  321] Training loss: 0.11199848, Validation loss: 0.11286220, Gradient norm: 5.80043124
INFO:root:[  322] Training loss: 0.11177155, Validation loss: 0.11164474, Gradient norm: 4.27486911
INFO:root:[  323] Training loss: 0.11080558, Validation loss: 0.11133119, Gradient norm: 5.14092211
INFO:root:[  324] Training loss: 0.11055859, Validation loss: 0.11077457, Gradient norm: 5.79411666
INFO:root:[  325] Training loss: 0.11025524, Validation loss: 0.11122106, Gradient norm: 6.47289064
INFO:root:[  326] Training loss: 0.11030128, Validation loss: 0.11080513, Gradient norm: 10.18834896
INFO:root:[  327] Training loss: 0.10962834, Validation loss: 0.10908674, Gradient norm: 10.54104657
INFO:root:[  328] Training loss: 0.10891537, Validation loss: 0.10922910, Gradient norm: 5.42013403
INFO:root:[  329] Training loss: 0.10855764, Validation loss: 0.10844871, Gradient norm: 7.65299284
INFO:root:[  330] Training loss: 0.10812982, Validation loss: 0.10869591, Gradient norm: 6.52269208
INFO:root:[  331] Training loss: 0.10783804, Validation loss: 0.10818595, Gradient norm: 6.80834283
INFO:root:[  332] Training loss: 0.10745267, Validation loss: 0.10766311, Gradient norm: 5.03293590
INFO:root:[  333] Training loss: 0.10672844, Validation loss: 0.10618550, Gradient norm: 6.77565966
INFO:root:[  334] Training loss: 0.10630827, Validation loss: 0.10815359, Gradient norm: 8.41502291
INFO:root:[  335] Training loss: 0.10621509, Validation loss: 0.10578874, Gradient norm: 9.89503781
INFO:root:[  336] Training loss: 0.10509009, Validation loss: 0.10619592, Gradient norm: 6.33510625
INFO:root:[  337] Training loss: 0.10461488, Validation loss: 0.10488567, Gradient norm: 5.27651222
INFO:root:[  338] Training loss: 0.10457657, Validation loss: 0.10508316, Gradient norm: 5.77409347
INFO:root:[  339] Training loss: 0.10396053, Validation loss: 0.10435385, Gradient norm: 2.93764710
INFO:root:[  340] Training loss: 0.10345342, Validation loss: 0.10466925, Gradient norm: 5.39438751
INFO:root:[  341] Training loss: 0.10320415, Validation loss: 0.10326214, Gradient norm: 6.20358917
INFO:root:[  342] Training loss: 0.10263422, Validation loss: 0.10289150, Gradient norm: 9.01567753
INFO:root:[  343] Training loss: 0.10269277, Validation loss: 0.10244516, Gradient norm: 6.30806133
INFO:root:[  344] Training loss: 0.10258871, Validation loss: 0.10178902, Gradient norm: 9.15858699
INFO:root:[  345] Training loss: 0.10215805, Validation loss: 0.10209742, Gradient norm: 10.22273898
INFO:root:[  346] Training loss: 0.10139214, Validation loss: 0.10112927, Gradient norm: 7.59070403
INFO:root:[  347] Training loss: 0.10091241, Validation loss: 0.10154532, Gradient norm: 4.49037984
INFO:root:[  348] Training loss: 0.10099912, Validation loss: 0.10074573, Gradient norm: 5.80376535
INFO:root:[  349] Training loss: 0.10036016, Validation loss: 0.10071871, Gradient norm: 9.03311574
INFO:root:[  350] Training loss: 0.09989767, Validation loss: 0.10110766, Gradient norm: 10.71882298
INFO:root:[  351] Training loss: 0.09962010, Validation loss: 0.09972509, Gradient norm: 10.03246071
INFO:root:[  352] Training loss: 0.09898759, Validation loss: 0.10005290, Gradient norm: 8.83562572
INFO:root:[  353] Training loss: 0.09889639, Validation loss: 0.09862248, Gradient norm: 8.96150411
INFO:root:[  354] Training loss: 0.09825978, Validation loss: 0.09902040, Gradient norm: 6.32873800
INFO:root:[  355] Training loss: 0.09798335, Validation loss: 0.09807780, Gradient norm: 6.62336140
INFO:root:[  356] Training loss: 0.09781632, Validation loss: 0.09811826, Gradient norm: 9.97718628
INFO:root:[  357] Training loss: 0.09711753, Validation loss: 0.09817837, Gradient norm: 7.59620958
INFO:root:[  358] Training loss: 0.09678224, Validation loss: 0.09715706, Gradient norm: 7.15317336
INFO:root:[  359] Training loss: 0.09664590, Validation loss: 0.09664355, Gradient norm: 5.92875746
INFO:root:[  360] Training loss: 0.09640188, Validation loss: 0.09750489, Gradient norm: 7.98674474
INFO:root:[  361] Training loss: 0.09604284, Validation loss: 0.09592618, Gradient norm: 5.85029422
INFO:root:[  362] Training loss: 0.09602242, Validation loss: 0.09655286, Gradient norm: 7.91936657
INFO:root:[  363] Training loss: 0.09474203, Validation loss: 0.09575162, Gradient norm: 7.17521971
INFO:root:[  364] Training loss: 0.09482874, Validation loss: 0.09484624, Gradient norm: 8.38867395
INFO:root:[  365] Training loss: 0.09388857, Validation loss: 0.09385763, Gradient norm: 6.25954629
INFO:root:[  366] Training loss: 0.09345248, Validation loss: 0.09365831, Gradient norm: 4.85431783
INFO:root:[  367] Training loss: 0.09312549, Validation loss: 0.09290852, Gradient norm: 5.92447527
INFO:root:[  368] Training loss: 0.09265201, Validation loss: 0.09277810, Gradient norm: 6.62594407
INFO:root:[  369] Training loss: 0.09160986, Validation loss: 0.09201017, Gradient norm: 5.28905530
INFO:root:[  370] Training loss: 0.09139100, Validation loss: 0.09105178, Gradient norm: 6.40759495
INFO:root:[  371] Training loss: 0.09117017, Validation loss: 0.09142725, Gradient norm: 8.45903736
INFO:root:[  372] Training loss: 0.09040200, Validation loss: 0.09034420, Gradient norm: 3.86635152
INFO:root:[  373] Training loss: 0.09003847, Validation loss: 0.09003291, Gradient norm: 4.57931278
INFO:root:[  374] Training loss: 0.08969860, Validation loss: 0.09007930, Gradient norm: 5.67639620
INFO:root:[  375] Training loss: 0.08942949, Validation loss: 0.09039527, Gradient norm: 7.12784355
INFO:root:[  376] Training loss: 0.08876972, Validation loss: 0.08853606, Gradient norm: 7.74230011
INFO:root:[  377] Training loss: 0.08828568, Validation loss: 0.08831122, Gradient norm: 4.99359168
INFO:root:[  378] Training loss: 0.08807473, Validation loss: 0.08814131, Gradient norm: 8.11655533
INFO:root:[  379] Training loss: 0.08808238, Validation loss: 0.08846334, Gradient norm: 8.56865341
INFO:root:[  380] Training loss: 0.08717521, Validation loss: 0.08713017, Gradient norm: 4.41353440
INFO:root:[  381] Training loss: 0.08679947, Validation loss: 0.08647563, Gradient norm: 4.44974791
INFO:root:[  382] Training loss: 0.08632401, Validation loss: 0.08597994, Gradient norm: 7.21896967
INFO:root:[  383] Training loss: 0.08581645, Validation loss: 0.08600146, Gradient norm: 7.39163917
INFO:root:[  384] Training loss: 0.08597728, Validation loss: 0.08573652, Gradient norm: 8.88700079
INFO:root:[  385] Training loss: 0.08529075, Validation loss: 0.08488486, Gradient norm: 7.31708072
INFO:root:[  386] Training loss: 0.08482229, Validation loss: 0.08497548, Gradient norm: 5.41880863
INFO:root:[  387] Training loss: 0.08447680, Validation loss: 0.08468989, Gradient norm: 5.76197928
INFO:root:[  388] Training loss: 0.08447007, Validation loss: 0.08492495, Gradient norm: 5.45766172
INFO:root:[  389] Training loss: 0.08432538, Validation loss: 0.08476859, Gradient norm: 6.97724078
INFO:root:[  390] Training loss: 0.08371912, Validation loss: 0.08384947, Gradient norm: 3.37440975
INFO:root:[  391] Training loss: 0.08334371, Validation loss: 0.08391420, Gradient norm: 5.29463760
INFO:root:[  392] Training loss: 0.08337391, Validation loss: 0.08285091, Gradient norm: 5.79516479
INFO:root:[  393] Training loss: 0.08298082, Validation loss: 0.08311783, Gradient norm: 6.57998205
INFO:root:[  394] Training loss: 0.08315451, Validation loss: 0.08274131, Gradient norm: 10.10012376
INFO:root:[  395] Training loss: 0.08236015, Validation loss: 0.08334964, Gradient norm: 3.81046840
INFO:root:[  396] Training loss: 0.08262544, Validation loss: 0.08290972, Gradient norm: 6.58166497
INFO:root:[  397] Training loss: 0.08224801, Validation loss: 0.08197955, Gradient norm: 6.66169505
INFO:root:[  398] Training loss: 0.08173509, Validation loss: 0.08248428, Gradient norm: 4.45826336
INFO:root:[  399] Training loss: 0.08132709, Validation loss: 0.08240230, Gradient norm: 3.99791699
INFO:root:[  400] Training loss: 0.08136820, Validation loss: 0.08139084, Gradient norm: 6.56219328
INFO:root:[  401] Training loss: 0.08131247, Validation loss: 0.08068354, Gradient norm: 7.19000169
INFO:root:[  402] Training loss: 0.08057879, Validation loss: 0.08104679, Gradient norm: 4.52459901
INFO:root:[  403] Training loss: 0.08041180, Validation loss: 0.08088939, Gradient norm: 3.79236383
INFO:root:[  404] Training loss: 0.08036195, Validation loss: 0.08067244, Gradient norm: 4.33657025
INFO:root:[  405] Training loss: 0.08017282, Validation loss: 0.08091867, Gradient norm: 5.96901854
INFO:root:[  406] Training loss: 0.07996420, Validation loss: 0.08095214, Gradient norm: 3.62653346
INFO:root:[  407] Training loss: 0.07989601, Validation loss: 0.08061228, Gradient norm: 6.53099650
INFO:root:[  408] Training loss: 0.08006988, Validation loss: 0.07908990, Gradient norm: 11.02323903
INFO:root:[  409] Training loss: 0.07938019, Validation loss: 0.07970747, Gradient norm: 6.82726803
INFO:root:[  410] Training loss: 0.07954769, Validation loss: 0.08147617, Gradient norm: 9.61365209
INFO:root:[  411] Training loss: 0.07944607, Validation loss: 0.07963828, Gradient norm: 8.84033879
INFO:root:[  412] Training loss: 0.07932458, Validation loss: 0.08000010, Gradient norm: 9.24139957
INFO:root:[  413] Training loss: 0.07922400, Validation loss: 0.07980705, Gradient norm: 7.87775105
INFO:root:[  414] Training loss: 0.07874734, Validation loss: 0.07974763, Gradient norm: 7.20168759
INFO:root:[  415] Training loss: 0.07849066, Validation loss: 0.08014001, Gradient norm: 6.97802869
INFO:root:[  416] Training loss: 0.07878715, Validation loss: 0.07977663, Gradient norm: 8.52097956
INFO:root:[  417] Training loss: 0.07856134, Validation loss: 0.07878643, Gradient norm: 8.92662153
INFO:root:[  418] Training loss: 0.07838256, Validation loss: 0.07870428, Gradient norm: 6.83927177
INFO:root:[  419] Training loss: 0.07838526, Validation loss: 0.07915630, Gradient norm: 8.23748202
INFO:root:[  420] Training loss: 0.07818711, Validation loss: 0.07787822, Gradient norm: 7.22245753
INFO:root:[  421] Training loss: 0.07814262, Validation loss: 0.07859779, Gradient norm: 9.27376830
INFO:root:[  422] Training loss: 0.07801097, Validation loss: 0.07761351, Gradient norm: 8.08946100
INFO:root:[  423] Training loss: 0.07770729, Validation loss: 0.07754893, Gradient norm: 4.48318770
INFO:root:[  424] Training loss: 0.07789229, Validation loss: 0.07750972, Gradient norm: 8.20596210
INFO:root:[  425] Training loss: 0.07794688, Validation loss: 0.07891152, Gradient norm: 10.51558284
INFO:root:[  426] Training loss: 0.07767961, Validation loss: 0.07770147, Gradient norm: 10.34126197
INFO:root:[  427] Training loss: 0.07701056, Validation loss: 0.07737138, Gradient norm: 5.76722596
INFO:root:[  428] Training loss: 0.07732207, Validation loss: 0.07722576, Gradient norm: 8.27304263
INFO:root:[  429] Training loss: 0.07769563, Validation loss: 0.07764527, Gradient norm: 12.70534040
INFO:root:[  430] Training loss: 0.07670085, Validation loss: 0.07722501, Gradient norm: 5.84457241
INFO:root:[  431] Training loss: 0.07690398, Validation loss: 0.07795585, Gradient norm: 6.68870513
INFO:root:[  432] Training loss: 0.07667236, Validation loss: 0.07721795, Gradient norm: 8.24697806
INFO:root:[  433] Training loss: 0.07662334, Validation loss: 0.07715561, Gradient norm: 7.09649447
INFO:root:[  434] Training loss: 0.07660396, Validation loss: 0.07659419, Gradient norm: 9.94226160
INFO:root:[  435] Training loss: 0.07671308, Validation loss: 0.07764521, Gradient norm: 11.00465343
INFO:root:[  436] Training loss: 0.07670667, Validation loss: 0.07693025, Gradient norm: 11.06758711
INFO:root:[  437] Training loss: 0.07597718, Validation loss: 0.07593767, Gradient norm: 6.66126770
INFO:root:[  438] Training loss: 0.07604691, Validation loss: 0.07615621, Gradient norm: 8.57748402
INFO:root:[  439] Training loss: 0.07616171, Validation loss: 0.07763693, Gradient norm: 7.25352907
INFO:root:[  440] Training loss: 0.07572726, Validation loss: 0.07617140, Gradient norm: 8.45726813
INFO:root:[  441] Training loss: 0.07594434, Validation loss: 0.07595833, Gradient norm: 7.65816669
INFO:root:[  442] Training loss: 0.07573487, Validation loss: 0.07643667, Gradient norm: 5.04876966
INFO:root:[  443] Training loss: 0.07614230, Validation loss: 0.07582101, Gradient norm: 12.38761347
INFO:root:[  444] Training loss: 0.07570040, Validation loss: 0.07957328, Gradient norm: 9.80228297
INFO:root:[  445] Training loss: 0.07551218, Validation loss: 0.07614142, Gradient norm: 8.74905986
INFO:root:[  446] Training loss: 0.07504053, Validation loss: 0.07537671, Gradient norm: 9.27675322
INFO:root:[  447] Training loss: 0.07551213, Validation loss: 0.07559225, Gradient norm: 11.81307259
INFO:root:[  448] Training loss: 0.07510646, Validation loss: 0.07579704, Gradient norm: 7.29190763
INFO:root:[  449] Training loss: 0.07513302, Validation loss: 0.07492310, Gradient norm: 8.15229758
INFO:root:[  450] Training loss: 0.07491257, Validation loss: 0.07526266, Gradient norm: 7.77634948
INFO:root:[  451] Training loss: 0.07483085, Validation loss: 0.07479164, Gradient norm: 9.14162540
INFO:root:[  452] Training loss: 0.07455457, Validation loss: 0.07523159, Gradient norm: 8.03377670
INFO:root:[  453] Training loss: 0.07459311, Validation loss: 0.07454102, Gradient norm: 10.25376512
INFO:root:[  454] Training loss: 0.07447954, Validation loss: 0.07501547, Gradient norm: 8.28160440
INFO:root:[  455] Training loss: 0.07419772, Validation loss: 0.07432078, Gradient norm: 9.07174897
INFO:root:[  456] Training loss: 0.07498943, Validation loss: 0.07480042, Gradient norm: 12.78931739
INFO:root:[  457] Training loss: 0.07486535, Validation loss: 0.07630345, Gradient norm: 9.79646719
INFO:root:[  458] Training loss: 0.07416468, Validation loss: 0.07445297, Gradient norm: 8.42947548
INFO:root:[  459] Training loss: 0.07406670, Validation loss: 0.07451868, Gradient norm: 8.61114948
INFO:root:[  460] Training loss: 0.07404269, Validation loss: 0.07728848, Gradient norm: 13.42663947
INFO:root:[  461] Training loss: 0.07409560, Validation loss: 0.07372629, Gradient norm: 13.22658490
INFO:root:[  462] Training loss: 0.07380509, Validation loss: 0.07338683, Gradient norm: 11.25611347
INFO:root:[  463] Training loss: 0.07359173, Validation loss: 0.07375609, Gradient norm: 8.84009296
INFO:root:[  464] Training loss: 0.07361377, Validation loss: 0.07358231, Gradient norm: 7.35576525
INFO:root:[  465] Training loss: 0.07395437, Validation loss: 0.07307506, Gradient norm: 12.25449951
INFO:root:[  466] Training loss: 0.07310455, Validation loss: 0.07376232, Gradient norm: 6.39178187
INFO:root:[  467] Training loss: 0.07325175, Validation loss: 0.07302081, Gradient norm: 11.67733078
INFO:root:[  468] Training loss: 0.07393516, Validation loss: 0.07362006, Gradient norm: 13.60351072
INFO:root:[  469] Training loss: 0.07302325, Validation loss: 0.07343423, Gradient norm: 7.70515018
INFO:root:[  470] Training loss: 0.07315782, Validation loss: 0.07336038, Gradient norm: 8.79815437
INFO:root:[  471] Training loss: 0.07285004, Validation loss: 0.07572364, Gradient norm: 9.41194554
INFO:root:[  472] Training loss: 0.07317638, Validation loss: 0.07258635, Gradient norm: 11.95832513
INFO:root:[  473] Training loss: 0.07288175, Validation loss: 0.07307133, Gradient norm: 9.81092269
INFO:root:[  474] Training loss: 0.07276019, Validation loss: 0.07484467, Gradient norm: 11.90604078
INFO:root:[  475] Training loss: 0.07306785, Validation loss: 0.07312466, Gradient norm: 16.28926965
INFO:root:[  476] Training loss: 0.07255947, Validation loss: 0.07284812, Gradient norm: 8.08483536
INFO:root:[  477] Training loss: 0.07229646, Validation loss: 0.07268672, Gradient norm: 8.48798466
INFO:root:[  478] Training loss: 0.07252953, Validation loss: 0.07254554, Gradient norm: 8.11115490
INFO:root:[  479] Training loss: 0.07233527, Validation loss: 0.07207354, Gradient norm: 8.84056652
INFO:root:[  480] Training loss: 0.07256120, Validation loss: 0.07215929, Gradient norm: 7.00481083
INFO:root:[  481] Training loss: 0.07219977, Validation loss: 0.07193143, Gradient norm: 11.06283891
INFO:root:[  482] Training loss: 0.07204833, Validation loss: 0.07189271, Gradient norm: 6.24986573
INFO:root:[  483] Training loss: 0.07204370, Validation loss: 0.07181428, Gradient norm: 9.52741077
INFO:root:[  484] Training loss: 0.07170879, Validation loss: 0.07307253, Gradient norm: 7.11735574
INFO:root:[  485] Training loss: 0.07238700, Validation loss: 0.07179809, Gradient norm: 15.77183875
INFO:root:[  486] Training loss: 0.07187294, Validation loss: 0.07292687, Gradient norm: 11.38059752
INFO:root:[  487] Training loss: 0.07190739, Validation loss: 0.07161849, Gradient norm: 11.02869887
INFO:root:[  488] Training loss: 0.07200754, Validation loss: 0.07316919, Gradient norm: 12.44989748
INFO:root:[  489] Training loss: 0.07167889, Validation loss: 0.07187812, Gradient norm: 11.38718113
INFO:root:[  490] Training loss: 0.07159097, Validation loss: 0.07189316, Gradient norm: 8.80241344
INFO:root:[  491] Training loss: 0.07164314, Validation loss: 0.07209856, Gradient norm: 12.01632671
INFO:root:[  492] Training loss: 0.07134774, Validation loss: 0.07181663, Gradient norm: 8.71700382
INFO:root:[  493] Training loss: 0.07140681, Validation loss: 0.07135125, Gradient norm: 6.92319857
INFO:root:[  494] Training loss: 0.07142119, Validation loss: 0.07310812, Gradient norm: 11.23597187
INFO:root:[  495] Training loss: 0.07167832, Validation loss: 0.07106979, Gradient norm: 14.14885350
INFO:root:[  496] Training loss: 0.07134216, Validation loss: 0.07102893, Gradient norm: 7.02677509
INFO:root:[  497] Training loss: 0.07125686, Validation loss: 0.07146470, Gradient norm: 9.06921203
INFO:root:[  498] Training loss: 0.07137519, Validation loss: 0.07220027, Gradient norm: 10.99804383
INFO:root:[  499] Training loss: 0.07103471, Validation loss: 0.07166010, Gradient norm: 9.07108694
INFO:root:[  500] Training loss: 0.07164362, Validation loss: 0.07171373, Gradient norm: 13.16141287
INFO:root:[  501] Training loss: 0.07098577, Validation loss: 0.07199986, Gradient norm: 9.25283240
INFO:root:[  502] Training loss: 0.07108469, Validation loss: 0.07170368, Gradient norm: 11.45598469
INFO:root:[  503] Training loss: 0.07053795, Validation loss: 0.07062743, Gradient norm: 6.61334485
INFO:root:[  504] Training loss: 0.07085222, Validation loss: 0.07124772, Gradient norm: 7.38431816
INFO:root:[  505] Training loss: 0.07096236, Validation loss: 0.07298264, Gradient norm: 8.96392252
INFO:root:[  506] Training loss: 0.07101268, Validation loss: 0.07139840, Gradient norm: 8.33505481
INFO:root:[  507] Training loss: 0.07090190, Validation loss: 0.07110884, Gradient norm: 13.66247191
INFO:root:[  508] Training loss: 0.07066620, Validation loss: 0.07160482, Gradient norm: 11.64884333
INFO:root:[  509] Training loss: 0.07091613, Validation loss: 0.07025987, Gradient norm: 10.27204052
INFO:root:[  510] Training loss: 0.07048364, Validation loss: 0.07128240, Gradient norm: 10.25560321
INFO:root:[  511] Training loss: 0.07065843, Validation loss: 0.07148079, Gradient norm: 10.08455759
INFO:root:[  512] Training loss: 0.07154719, Validation loss: 0.07116586, Gradient norm: 16.17080870
INFO:root:[  513] Training loss: 0.07049584, Validation loss: 0.07084338, Gradient norm: 11.38507891
INFO:root:[  514] Training loss: 0.07027082, Validation loss: 0.07048986, Gradient norm: 8.56027476
INFO:root:[  515] Training loss: 0.07019827, Validation loss: 0.07081284, Gradient norm: 10.07792875
INFO:root:[  516] Training loss: 0.07006911, Validation loss: 0.07015250, Gradient norm: 12.86041294
INFO:root:[  517] Training loss: 0.07001159, Validation loss: 0.07116318, Gradient norm: 7.95981026
INFO:root:[  518] Training loss: 0.07061108, Validation loss: 0.06993109, Gradient norm: 18.20626056
INFO:root:[  519] Training loss: 0.06999514, Validation loss: 0.07162838, Gradient norm: 8.25091552
INFO:root:[  520] Training loss: 0.07021663, Validation loss: 0.07080687, Gradient norm: 12.41789882
INFO:root:[  521] Training loss: 0.07036538, Validation loss: 0.07032938, Gradient norm: 16.58138699
INFO:root:[  522] Training loss: 0.07022016, Validation loss: 0.06992128, Gradient norm: 14.44566123
INFO:root:[  523] Training loss: 0.07069988, Validation loss: 0.07059960, Gradient norm: 18.19700806
INFO:root:[  524] Training loss: 0.06996913, Validation loss: 0.07022280, Gradient norm: 9.31320738
INFO:root:[  525] Training loss: 0.06979714, Validation loss: 0.06999278, Gradient norm: 7.54259490
INFO:root:[  526] Training loss: 0.07003224, Validation loss: 0.07015500, Gradient norm: 13.50648617
INFO:root:[  527] Training loss: 0.06983145, Validation loss: 0.06941460, Gradient norm: 11.82325139
INFO:root:[  528] Training loss: 0.06960244, Validation loss: 0.06971492, Gradient norm: 7.64220141
INFO:root:[  529] Training loss: 0.07027173, Validation loss: 0.07443260, Gradient norm: 15.53518776
INFO:root:[  530] Training loss: 0.07038734, Validation loss: 0.06927545, Gradient norm: 16.19008477
INFO:root:[  531] Training loss: 0.06953085, Validation loss: 0.06958444, Gradient norm: 7.66111675
INFO:root:[  532] Training loss: 0.06988809, Validation loss: 0.06990829, Gradient norm: 13.96369043
INFO:root:[  533] Training loss: 0.06993569, Validation loss: 0.06975221, Gradient norm: 13.84598684
INFO:root:[  534] Training loss: 0.06961131, Validation loss: 0.06998108, Gradient norm: 10.90261664
INFO:root:[  535] Training loss: 0.06971139, Validation loss: 0.07030840, Gradient norm: 10.85160593
INFO:root:[  536] Training loss: 0.06960196, Validation loss: 0.06976362, Gradient norm: 12.56321881
INFO:root:[  537] Training loss: 0.06962472, Validation loss: 0.07046737, Gradient norm: 10.58292430
INFO:root:[  538] Training loss: 0.06990333, Validation loss: 0.06985227, Gradient norm: 18.43882008
INFO:root:[  539] Training loss: 0.07009918, Validation loss: 0.07099854, Gradient norm: 16.70357534
INFO:root:EP 539: Early stopping
INFO:root:Training the model took 8643.627s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.52165
INFO:root:EnergyScoreTrain: 1.01765
INFO:root:CoverageTrain: 0.87955
INFO:root:IntervalWidthTrain: 0.02551
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.46385
INFO:root:EnergyScoreValidation: 0.97871
INFO:root:CoverageValidation: 0.87931
INFO:root:IntervalWidthValidation: 0.02569
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.19733
INFO:root:EnergyScoreTest: 0.80102
INFO:root:CoverageTest: 0.87908
INFO:root:IntervalWidthTest: 0.02495
INFO:root:###20 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1453326336
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.44001646, Validation loss: 1.89041400, Gradient norm: 6.99220654
INFO:root:[    2] Training loss: 1.61354482, Validation loss: 1.44917784, Gradient norm: 5.11582456
INFO:root:[    3] Training loss: 1.29529672, Validation loss: 1.19395812, Gradient norm: 4.09944101
INFO:root:[    4] Training loss: 1.12972521, Validation loss: 1.10038652, Gradient norm: 3.30873621
INFO:root:[    5] Training loss: 1.07200125, Validation loss: 1.03227758, Gradient norm: 4.00932058
INFO:root:[    6] Training loss: 1.03165213, Validation loss: 0.99921227, Gradient norm: 3.36732413
INFO:root:[    7] Training loss: 0.99229449, Validation loss: 0.96531379, Gradient norm: 3.70592073
INFO:root:[    8] Training loss: 0.95700634, Validation loss: 0.95329222, Gradient norm: 4.14143114
INFO:root:[    9] Training loss: 0.93541183, Validation loss: 0.92538060, Gradient norm: 4.68734143
INFO:root:[   10] Training loss: 0.91399523, Validation loss: 0.92561738, Gradient norm: 4.71800017
INFO:root:[   11] Training loss: 0.91603961, Validation loss: 0.90373576, Gradient norm: 5.66514603
INFO:root:[   12] Training loss: 0.91242345, Validation loss: 0.90870309, Gradient norm: 6.69138095
INFO:root:[   13] Training loss: 0.88431319, Validation loss: 0.89938543, Gradient norm: 5.87460411
INFO:root:[   14] Training loss: 0.90136353, Validation loss: 0.90395948, Gradient norm: 9.22022933
INFO:root:[   15] Training loss: 0.87243963, Validation loss: 0.88444224, Gradient norm: 8.50889325
INFO:root:[   16] Training loss: 0.87013478, Validation loss: 0.87726196, Gradient norm: 10.49428167
INFO:root:[   17] Training loss: 0.84883493, Validation loss: 0.83402825, Gradient norm: 12.81788421
INFO:root:[   18] Training loss: 0.84579401, Validation loss: 0.84099859, Gradient norm: 23.42535369
INFO:root:[   19] Training loss: 0.83027094, Validation loss: 0.83521963, Gradient norm: 24.11457748
INFO:root:[   20] Training loss: 0.81093532, Validation loss: 0.82719650, Gradient norm: 36.80141299
INFO:root:[   21] Training loss: 0.80433592, Validation loss: 0.78741542, Gradient norm: 64.12994701
INFO:root:[   22] Training loss: 0.80601395, Validation loss: 0.79221718, Gradient norm: 90.43450668
INFO:root:[   23] Training loss: 0.79148445, Validation loss: 0.77885007, Gradient norm: 106.94969347
INFO:root:[   24] Training loss: 0.77480910, Validation loss: 0.78968823, Gradient norm: 55.52902177
INFO:root:[   25] Training loss: 0.77402702, Validation loss: 0.84885600, Gradient norm: 115.81380816
INFO:root:[   26] Training loss: 0.77701010, Validation loss: 0.73813509, Gradient norm: 183.98299678
INFO:root:[   27] Training loss: 0.76231098, Validation loss: 0.76822222, Gradient norm: 131.36501819
INFO:root:[   28] Training loss: 0.75825365, Validation loss: 0.74561644, Gradient norm: 207.31192935
INFO:root:[   29] Training loss: 0.74567803, Validation loss: 0.79864606, Gradient norm: 159.97396652
INFO:root:[   30] Training loss: 0.74605211, Validation loss: 0.72841088, Gradient norm: 241.65852387
INFO:root:[   31] Training loss: 0.73773965, Validation loss: 0.72881353, Gradient norm: 243.75906182
INFO:root:[   32] Training loss: 0.73137554, Validation loss: 0.73132314, Gradient norm: 234.88729179
INFO:root:[   33] Training loss: 0.71766417, Validation loss: 0.72551299, Gradient norm: 223.73226584
INFO:root:[   34] Training loss: 0.74487245, Validation loss: 0.71739574, Gradient norm: 461.20577691
INFO:root:[   35] Training loss: 0.71323328, Validation loss: 0.71434362, Gradient norm: 339.66470965
INFO:root:[   36] Training loss: 0.70814065, Validation loss: 0.69970743, Gradient norm: 317.06494524
INFO:root:[   37] Training loss: 0.69955293, Validation loss: 0.68104532, Gradient norm: 214.80597602
INFO:root:[   38] Training loss: 0.70470651, Validation loss: 0.72161154, Gradient norm: 345.72127610
INFO:root:[   39] Training loss: 0.68749067, Validation loss: 0.77862769, Gradient norm: 288.31123652
INFO:root:[   40] Training loss: 0.71586642, Validation loss: 0.69712834, Gradient norm: 488.61405260
INFO:root:[   41] Training loss: 0.68563523, Validation loss: 0.67281753, Gradient norm: 265.46261190
INFO:root:[   42] Training loss: 0.68991666, Validation loss: 0.69130589, Gradient norm: 496.04054545
INFO:root:[   43] Training loss: 0.68787160, Validation loss: 0.67761897, Gradient norm: 458.46967073
INFO:root:[   44] Training loss: 0.67309782, Validation loss: 0.67649594, Gradient norm: 304.23920461
INFO:root:[   45] Training loss: 0.70666971, Validation loss: 0.72217152, Gradient norm: 649.94885559
INFO:root:[   46] Training loss: 0.67902022, Validation loss: 0.68002924, Gradient norm: 367.42683787
INFO:root:[   47] Training loss: 0.67372265, Validation loss: 0.66400489, Gradient norm: 467.29880770
INFO:root:[   48] Training loss: 0.67564247, Validation loss: 0.66490989, Gradient norm: 412.18745242
INFO:root:[   49] Training loss: 0.67007949, Validation loss: 0.66525346, Gradient norm: 529.14038268
INFO:root:[   50] Training loss: 0.67239622, Validation loss: 0.65175050, Gradient norm: 573.35883960
INFO:root:[   51] Training loss: 0.67439530, Validation loss: 0.65160485, Gradient norm: 618.67008018
INFO:root:[   52] Training loss: 0.66212636, Validation loss: 0.64374778, Gradient norm: 544.35987626
INFO:root:[   53] Training loss: 0.65798845, Validation loss: 0.65650737, Gradient norm: 561.42217798
INFO:root:[   54] Training loss: 0.67574113, Validation loss: 0.64863744, Gradient norm: 817.21556862
INFO:root:[   55] Training loss: 0.65668887, Validation loss: 0.64465442, Gradient norm: 569.40993326
INFO:root:[   56] Training loss: 0.65006952, Validation loss: 0.64001730, Gradient norm: 614.28938292
INFO:root:[   57] Training loss: 0.64662979, Validation loss: 0.64306850, Gradient norm: 497.80798145
INFO:root:[   58] Training loss: 0.64623542, Validation loss: 0.63117285, Gradient norm: 609.32917833
INFO:root:[   59] Training loss: 0.63158997, Validation loss: 0.62296148, Gradient norm: 465.47755697
INFO:root:[   60] Training loss: 0.63979837, Validation loss: 0.61806212, Gradient norm: 613.15399126
INFO:root:[   61] Training loss: 0.64172989, Validation loss: 0.68264921, Gradient norm: 685.59886829
INFO:root:[   62] Training loss: 0.65038462, Validation loss: 0.67936599, Gradient norm: 811.71766178
INFO:root:[   63] Training loss: 0.64859133, Validation loss: 0.61886801, Gradient norm: 853.50845209
INFO:root:[   64] Training loss: 0.62660612, Validation loss: 0.61901732, Gradient norm: 466.76026165
INFO:root:[   65] Training loss: 0.64011227, Validation loss: 0.65066558, Gradient norm: 916.56092183
INFO:root:[   66] Training loss: 0.61865880, Validation loss: 0.60592404, Gradient norm: 538.80222707
INFO:root:[   67] Training loss: 0.62667468, Validation loss: 0.60998154, Gradient norm: 683.85398730
INFO:root:[   68] Training loss: 0.62816114, Validation loss: 0.62479685, Gradient norm: 781.31177363
INFO:root:[   69] Training loss: 0.60866187, Validation loss: 0.60298448, Gradient norm: 430.74404749
INFO:root:[   70] Training loss: 0.62811927, Validation loss: 0.64365626, Gradient norm: 779.84290880
INFO:root:[   71] Training loss: 0.63105380, Validation loss: 0.60244694, Gradient norm: 949.51659361
INFO:root:[   72] Training loss: 0.61779484, Validation loss: 0.58727357, Gradient norm: 773.79458199
INFO:root:[   73] Training loss: 0.61303503, Validation loss: 0.63679419, Gradient norm: 599.50049907
INFO:root:[   74] Training loss: 0.60148775, Validation loss: 0.59746729, Gradient norm: 474.71286539
INFO:root:[   75] Training loss: 0.59885794, Validation loss: 0.60021262, Gradient norm: 614.06795174
INFO:root:[   76] Training loss: 0.60056512, Validation loss: 0.60289257, Gradient norm: 658.32067837
INFO:root:[   77] Training loss: 0.59308201, Validation loss: 0.68640617, Gradient norm: 603.83263608
INFO:root:[   78] Training loss: 0.60176333, Validation loss: 0.62030242, Gradient norm: 804.99141340
INFO:root:[   79] Training loss: 0.60536404, Validation loss: 0.57592679, Gradient norm: 796.85606622
INFO:root:[   80] Training loss: 0.59699413, Validation loss: 0.58141849, Gradient norm: 859.11782457
INFO:root:[   81] Training loss: 0.58788026, Validation loss: 0.59343987, Gradient norm: 680.26882662
INFO:root:[   82] Training loss: 0.58781887, Validation loss: 0.56915745, Gradient norm: 673.12855815
INFO:root:[   83] Training loss: 0.58406514, Validation loss: 0.57761000, Gradient norm: 689.63151854
INFO:root:[   84] Training loss: 0.57843455, Validation loss: 0.56888693, Gradient norm: 703.08673086
INFO:root:[   85] Training loss: 0.57148848, Validation loss: 0.55731176, Gradient norm: 487.60898725
INFO:root:[   86] Training loss: 0.59831736, Validation loss: 0.64860530, Gradient norm: 1191.18924567
INFO:root:[   87] Training loss: 0.57234099, Validation loss: 0.61682227, Gradient norm: 812.45987890
INFO:root:[   88] Training loss: 0.58553825, Validation loss: 0.60080475, Gradient norm: 1101.28967312
INFO:root:[   89] Training loss: 0.59825385, Validation loss: 0.61228538, Gradient norm: 1191.78593121
INFO:root:[   90] Training loss: 0.58548783, Validation loss: 0.55676345, Gradient norm: 1044.45732536
INFO:root:[   91] Training loss: 0.57268836, Validation loss: 0.56771445, Gradient norm: 818.86472210
INFO:root:[   92] Training loss: 0.56567609, Validation loss: 0.59039257, Gradient norm: 712.86079443
INFO:root:[   93] Training loss: 0.56544651, Validation loss: 0.58725409, Gradient norm: 828.12415798
INFO:root:[   94] Training loss: 0.55585196, Validation loss: 0.62740079, Gradient norm: 548.05529088
INFO:root:[   95] Training loss: 0.56707318, Validation loss: 0.57654305, Gradient norm: 1016.25482596
INFO:root:[   96] Training loss: 0.56708131, Validation loss: 0.58093178, Gradient norm: 1031.70400439
INFO:root:[   97] Training loss: 0.54940620, Validation loss: 0.54691091, Gradient norm: 683.64884596
INFO:root:[   98] Training loss: 0.54879375, Validation loss: 0.53822361, Gradient norm: 817.33208829
INFO:root:[   99] Training loss: 0.53991181, Validation loss: 0.54354101, Gradient norm: 620.87918919
INFO:root:[  100] Training loss: 0.54597939, Validation loss: 0.62448435, Gradient norm: 886.77234828
INFO:root:[  101] Training loss: 0.56364998, Validation loss: 0.64140519, Gradient norm: 1245.04797629
INFO:root:[  102] Training loss: 0.56469522, Validation loss: 0.58622251, Gradient norm: 1171.81219346
INFO:root:[  103] Training loss: 0.54594119, Validation loss: 0.53274417, Gradient norm: 875.66176463
INFO:root:[  104] Training loss: 0.54241745, Validation loss: 0.55504671, Gradient norm: 835.25386511
INFO:root:[  105] Training loss: 0.53494428, Validation loss: 0.51853406, Gradient norm: 795.57832846
INFO:root:[  106] Training loss: 0.53376396, Validation loss: 0.57698741, Gradient norm: 819.44642787
INFO:root:[  107] Training loss: 0.53093956, Validation loss: 0.52298423, Gradient norm: 790.69276511
INFO:root:[  108] Training loss: 0.53667804, Validation loss: 0.56175826, Gradient norm: 875.80264648
INFO:root:[  109] Training loss: 0.54169052, Validation loss: 0.51394317, Gradient norm: 1077.23568908
INFO:root:[  110] Training loss: 0.52694356, Validation loss: 0.58427799, Gradient norm: 803.49876517
INFO:root:[  111] Training loss: 0.53336115, Validation loss: 0.52675078, Gradient norm: 1091.47349001
INFO:root:[  112] Training loss: 0.52465325, Validation loss: 0.50962699, Gradient norm: 817.97222773
INFO:root:[  113] Training loss: 0.53419514, Validation loss: 0.50264189, Gradient norm: 1085.76287586
INFO:root:[  114] Training loss: 0.54313086, Validation loss: 0.50149246, Gradient norm: 1050.64692147
INFO:root:[  115] Training loss: 0.51856695, Validation loss: 0.56468364, Gradient norm: 747.66421611
INFO:root:[  116] Training loss: 0.52657943, Validation loss: 0.52669190, Gradient norm: 1046.76035022
INFO:root:[  117] Training loss: 0.51392201, Validation loss: 0.50104639, Gradient norm: 757.26809426
INFO:root:[  118] Training loss: 0.53039215, Validation loss: 0.50142632, Gradient norm: 1148.82584604
INFO:root:[  119] Training loss: 0.50740085, Validation loss: 0.51169475, Gradient norm: 769.59346086
INFO:root:[  120] Training loss: 0.50046425, Validation loss: 0.51497536, Gradient norm: 580.86882096
INFO:root:[  121] Training loss: 0.50412303, Validation loss: 0.50943107, Gradient norm: 693.53127955
INFO:root:[  122] Training loss: 0.52396768, Validation loss: 0.49816308, Gradient norm: 1189.02354644
INFO:root:[  123] Training loss: 0.52023074, Validation loss: 0.49432329, Gradient norm: 1106.25887758
INFO:root:[  124] Training loss: 0.50426849, Validation loss: 0.49340791, Gradient norm: 862.48641333
INFO:root:[  125] Training loss: 0.50077052, Validation loss: 0.49998611, Gradient norm: 769.89995229
INFO:root:[  126] Training loss: 0.50004187, Validation loss: 0.54738984, Gradient norm: 870.20295666
INFO:root:[  127] Training loss: 0.50748394, Validation loss: 0.48961495, Gradient norm: 1002.60235402
INFO:root:[  128] Training loss: 0.50018961, Validation loss: 0.50139996, Gradient norm: 947.90548760
INFO:root:[  129] Training loss: 0.50262990, Validation loss: 0.50234113, Gradient norm: 1046.94371579
INFO:root:[  130] Training loss: 0.48811016, Validation loss: 0.49034814, Gradient norm: 683.66787670
INFO:root:[  131] Training loss: 0.48423433, Validation loss: 0.49284352, Gradient norm: 610.99380344
INFO:root:[  132] Training loss: 0.48263284, Validation loss: 0.48424729, Gradient norm: 680.59142379
INFO:root:[  133] Training loss: 0.48365880, Validation loss: 0.49366651, Gradient norm: 677.72232212
INFO:root:[  134] Training loss: 0.48366298, Validation loss: 0.47116897, Gradient norm: 817.36669076
INFO:root:[  135] Training loss: 0.47918720, Validation loss: 0.48022224, Gradient norm: 731.22528855
INFO:root:[  136] Training loss: 0.47732830, Validation loss: 0.48592773, Gradient norm: 875.24814454
INFO:root:[  137] Training loss: 0.49611678, Validation loss: 0.46147509, Gradient norm: 1099.93205754
INFO:root:[  138] Training loss: 0.47546626, Validation loss: 0.56152010, Gradient norm: 782.83570683
INFO:root:[  139] Training loss: 0.50359212, Validation loss: 0.47500187, Gradient norm: 1285.51933777
INFO:root:[  140] Training loss: 0.46474856, Validation loss: 0.49413095, Gradient norm: 442.69625314
INFO:root:[  141] Training loss: 0.47075380, Validation loss: 0.45898179, Gradient norm: 702.23777299
INFO:root:[  142] Training loss: 0.46734531, Validation loss: 0.45568846, Gradient norm: 741.06513741
INFO:root:[  143] Training loss: 0.47347711, Validation loss: 0.45366038, Gradient norm: 870.80236204
INFO:root:[  144] Training loss: 0.47545631, Validation loss: 0.45707368, Gradient norm: 810.76971495
INFO:root:[  145] Training loss: 0.45383384, Validation loss: 0.47283337, Gradient norm: 449.74235648
INFO:root:[  146] Training loss: 0.45890138, Validation loss: 0.52956409, Gradient norm: 704.34673807
INFO:root:[  147] Training loss: 0.46602097, Validation loss: 0.44475351, Gradient norm: 955.15024280
INFO:root:[  148] Training loss: 0.45994792, Validation loss: 0.50478057, Gradient norm: 778.61167040
INFO:root:[  149] Training loss: 0.46515806, Validation loss: 0.46450795, Gradient norm: 909.86020805
INFO:root:[  150] Training loss: 0.45168543, Validation loss: 0.53447561, Gradient norm: 640.09399473
INFO:root:[  151] Training loss: 0.46196014, Validation loss: 0.52970020, Gradient norm: 1020.54628731
INFO:root:[  152] Training loss: 0.48373338, Validation loss: 0.44155866, Gradient norm: 1473.93032721
INFO:root:[  153] Training loss: 0.45709544, Validation loss: 0.49636877, Gradient norm: 931.58055405
INFO:root:[  154] Training loss: 0.45299867, Validation loss: 0.44897687, Gradient norm: 771.86324499
INFO:root:[  155] Training loss: 0.44289780, Validation loss: 0.49784406, Gradient norm: 606.75641882
INFO:root:[  156] Training loss: 0.45185087, Validation loss: 0.47702239, Gradient norm: 979.67434608
INFO:root:[  157] Training loss: 0.45864571, Validation loss: 0.43250761, Gradient norm: 1038.46788514
INFO:root:[  158] Training loss: 0.43854901, Validation loss: 0.44344206, Gradient norm: 542.21572667
INFO:root:[  159] Training loss: 0.43800156, Validation loss: 0.44110461, Gradient norm: 550.66737483
INFO:root:[  160] Training loss: 0.45081376, Validation loss: 0.43138458, Gradient norm: 1032.49006997
INFO:root:[  161] Training loss: 0.44521103, Validation loss: 0.48237684, Gradient norm: 866.08957952
INFO:root:[  162] Training loss: 0.43340187, Validation loss: 0.48239981, Gradient norm: 603.60303486
INFO:root:[  163] Training loss: 0.44766016, Validation loss: 0.44357011, Gradient norm: 1017.25038594
INFO:root:[  164] Training loss: 0.43304320, Validation loss: 0.43370684, Gradient norm: 750.93550867
INFO:root:[  165] Training loss: 0.42894308, Validation loss: 0.42614087, Gradient norm: 621.67385062
INFO:root:[  166] Training loss: 0.44238545, Validation loss: 0.44829441, Gradient norm: 1063.37008558
INFO:root:[  167] Training loss: 0.44339262, Validation loss: 0.42128324, Gradient norm: 966.18033799
INFO:root:[  168] Training loss: 0.42225426, Validation loss: 0.42189619, Gradient norm: 531.13033827
INFO:root:[  169] Training loss: 0.42943117, Validation loss: 0.46518221, Gradient norm: 811.85166654
INFO:root:[  170] Training loss: 0.42382557, Validation loss: 0.42786038, Gradient norm: 647.61017536
INFO:root:[  171] Training loss: 0.43633500, Validation loss: 0.45359893, Gradient norm: 944.07567329
INFO:root:[  172] Training loss: 0.42252824, Validation loss: 0.43151075, Gradient norm: 677.35614222
INFO:root:[  173] Training loss: 0.41926581, Validation loss: 0.41074151, Gradient norm: 671.19275099
INFO:root:[  174] Training loss: 0.41806547, Validation loss: 0.41639625, Gradient norm: 718.14234023
INFO:root:[  175] Training loss: 0.41128785, Validation loss: 0.41023772, Gradient norm: 481.76464327
INFO:root:[  176] Training loss: 0.41904463, Validation loss: 0.41416044, Gradient norm: 877.06396306
INFO:root:[  177] Training loss: 0.42568659, Validation loss: 0.41483259, Gradient norm: 1038.10598762
INFO:root:[  178] Training loss: 0.40677225, Validation loss: 0.40466492, Gradient norm: 473.54277988
INFO:root:[  179] Training loss: 0.41602771, Validation loss: 0.40187618, Gradient norm: 813.70094374
INFO:root:[  180] Training loss: 0.41584929, Validation loss: 0.40460681, Gradient norm: 917.70640494
INFO:root:[  181] Training loss: 0.40553609, Validation loss: 0.40206815, Gradient norm: 612.27052534
INFO:root:[  182] Training loss: 0.41698260, Validation loss: 0.39614031, Gradient norm: 984.90181452
INFO:root:[  183] Training loss: 0.41395038, Validation loss: 0.41220580, Gradient norm: 903.85128889
INFO:root:[  184] Training loss: 0.40116068, Validation loss: 0.39129937, Gradient norm: 625.57869645
INFO:root:[  185] Training loss: 0.40341313, Validation loss: 0.44191092, Gradient norm: 743.47785433
INFO:root:[  186] Training loss: 0.39803837, Validation loss: 0.40191146, Gradient norm: 629.39449319
INFO:root:[  187] Training loss: 0.40023083, Validation loss: 0.45140735, Gradient norm: 694.35498104
INFO:root:[  188] Training loss: 0.40305105, Validation loss: 0.42234159, Gradient norm: 807.44462449
INFO:root:[  189] Training loss: 0.39974924, Validation loss: 0.38808347, Gradient norm: 710.54357085
INFO:root:[  190] Training loss: 0.39637955, Validation loss: 0.38609276, Gradient norm: 752.88504133
INFO:root:[  191] Training loss: 0.39478751, Validation loss: 0.42272489, Gradient norm: 689.93431845
INFO:root:[  192] Training loss: 0.39526971, Validation loss: 0.38202452, Gradient norm: 799.62544365
INFO:root:[  193] Training loss: 0.39122110, Validation loss: 0.38503368, Gradient norm: 655.58374772
INFO:root:[  194] Training loss: 0.39348972, Validation loss: 0.38092183, Gradient norm: 825.94559556
INFO:root:[  195] Training loss: 0.39918050, Validation loss: 0.37965568, Gradient norm: 912.91472545
INFO:root:[  196] Training loss: 0.38600716, Validation loss: 0.40036888, Gradient norm: 609.85926644
INFO:root:[  197] Training loss: 0.38387277, Validation loss: 0.38843910, Gradient norm: 676.36425107
INFO:root:[  198] Training loss: 0.38633610, Validation loss: 0.38583965, Gradient norm: 697.55624482
INFO:root:[  199] Training loss: 0.37896937, Validation loss: 0.37468707, Gradient norm: 531.61335538
INFO:root:[  200] Training loss: 0.38073446, Validation loss: 0.37367503, Gradient norm: 600.39899374
INFO:root:[  201] Training loss: 0.37486520, Validation loss: 0.36924432, Gradient norm: 518.69297454
INFO:root:[  202] Training loss: 0.38495511, Validation loss: 0.37098590, Gradient norm: 857.08437352
INFO:root:[  203] Training loss: 0.37420454, Validation loss: 0.38120685, Gradient norm: 553.32432832
INFO:root:[  204] Training loss: 0.38298839, Validation loss: 0.40712712, Gradient norm: 837.64425237
INFO:root:[  205] Training loss: 0.37491671, Validation loss: 0.37966226, Gradient norm: 721.91021104
INFO:root:[  206] Training loss: 0.37872746, Validation loss: 0.38054322, Gradient norm: 902.17085699
INFO:root:[  207] Training loss: 0.36931260, Validation loss: 0.36271246, Gradient norm: 600.60914415
INFO:root:[  208] Training loss: 0.37976731, Validation loss: 0.39461824, Gradient norm: 945.79069373
INFO:root:[  209] Training loss: 0.38421537, Validation loss: 0.36871261, Gradient norm: 969.66970203
INFO:root:[  210] Training loss: 0.36410434, Validation loss: 0.35719223, Gradient norm: 487.06712067
INFO:root:[  211] Training loss: 0.36457105, Validation loss: 0.36232636, Gradient norm: 607.35991526
INFO:root:[  212] Training loss: 0.36132739, Validation loss: 0.42553496, Gradient norm: 463.39445178
INFO:root:[  213] Training loss: 0.37016762, Validation loss: 0.36420569, Gradient norm: 757.69820805
INFO:root:[  214] Training loss: 0.36624412, Validation loss: 0.35298450, Gradient norm: 746.42183708
INFO:root:[  215] Training loss: 0.36770250, Validation loss: 0.35220800, Gradient norm: 784.92036200
INFO:root:[  216] Training loss: 0.35263218, Validation loss: 0.35445313, Gradient norm: 381.79432057
INFO:root:[  217] Training loss: 0.35518657, Validation loss: 0.35123994, Gradient norm: 538.21888794
INFO:root:[  218] Training loss: 0.35380066, Validation loss: 0.35280879, Gradient norm: 508.14097831
INFO:root:[  219] Training loss: 0.35302086, Validation loss: 0.35148868, Gradient norm: 559.70392567
INFO:root:[  220] Training loss: 0.35243590, Validation loss: 0.37637900, Gradient norm: 573.23341924
INFO:root:[  221] Training loss: 0.36724387, Validation loss: 0.34390743, Gradient norm: 985.08322545
INFO:root:[  222] Training loss: 0.34891097, Validation loss: 0.34599169, Gradient norm: 472.45292937
INFO:root:[  223] Training loss: 0.35311532, Validation loss: 0.37653998, Gradient norm: 650.64587907
INFO:root:[  224] Training loss: 0.35278012, Validation loss: 0.34198267, Gradient norm: 627.46649473
INFO:root:[  225] Training loss: 0.35423021, Validation loss: 0.34093457, Gradient norm: 773.82319313
INFO:root:[  226] Training loss: 0.34992149, Validation loss: 0.35463842, Gradient norm: 693.86713076
INFO:root:[  227] Training loss: 0.34565478, Validation loss: 0.33613893, Gradient norm: 567.27504791
INFO:root:[  228] Training loss: 0.34346273, Validation loss: 0.34391461, Gradient norm: 599.61758726
INFO:root:[  229] Training loss: 0.34136402, Validation loss: 0.34999901, Gradient norm: 521.22067816
INFO:root:[  230] Training loss: 0.34581641, Validation loss: 0.33403893, Gradient norm: 652.22044364
INFO:root:[  231] Training loss: 0.33980533, Validation loss: 0.33377951, Gradient norm: 565.61708649
INFO:root:[  232] Training loss: 0.33449246, Validation loss: 0.34166208, Gradient norm: 453.93016212
INFO:root:[  233] Training loss: 0.33422121, Validation loss: 0.33166591, Gradient norm: 536.00095051
INFO:root:[  234] Training loss: 0.33567476, Validation loss: 0.33361845, Gradient norm: 529.42107587
INFO:root:[  235] Training loss: 0.33829190, Validation loss: 0.34220553, Gradient norm: 678.39642271
INFO:root:[  236] Training loss: 0.33648136, Validation loss: 0.33109918, Gradient norm: 662.57230419
INFO:root:[  237] Training loss: 0.32642256, Validation loss: 0.32721609, Gradient norm: 342.64349842
INFO:root:[  238] Training loss: 0.33405082, Validation loss: 0.33071760, Gradient norm: 719.83635668
INFO:root:[  239] Training loss: 0.33015993, Validation loss: 0.33193690, Gradient norm: 614.24922814
INFO:root:[  240] Training loss: 0.33315301, Validation loss: 0.31802454, Gradient norm: 740.15487989
INFO:root:[  241] Training loss: 0.32469444, Validation loss: 0.40117535, Gradient norm: 447.72648392
INFO:root:[  242] Training loss: 0.32746908, Validation loss: 0.32557708, Gradient norm: 586.52162878
INFO:root:[  243] Training loss: 0.32038910, Validation loss: 0.31688630, Gradient norm: 481.43037577
INFO:root:[  244] Training loss: 0.32693810, Validation loss: 0.34159572, Gradient norm: 709.40829627
INFO:root:[  245] Training loss: 0.32274320, Validation loss: 0.35625116, Gradient norm: 611.99990889
INFO:root:[  246] Training loss: 0.32701214, Validation loss: 0.33910250, Gradient norm: 751.25732032
INFO:root:[  247] Training loss: 0.32325165, Validation loss: 0.31180334, Gradient norm: 652.43947336
INFO:root:[  248] Training loss: 0.31668618, Validation loss: 0.31224680, Gradient norm: 474.09688759
INFO:root:[  249] Training loss: 0.31414848, Validation loss: 0.30881296, Gradient norm: 467.63404992
INFO:root:[  250] Training loss: 0.31408480, Validation loss: 0.30837189, Gradient norm: 475.96655470
INFO:root:[  251] Training loss: 0.30975163, Validation loss: 0.33671051, Gradient norm: 438.93091466
INFO:root:[  252] Training loss: 0.31349245, Validation loss: 0.31707796, Gradient norm: 553.88929857
INFO:root:[  253] Training loss: 0.32109441, Validation loss: 0.30941837, Gradient norm: 787.08676194
INFO:root:[  254] Training loss: 0.30880217, Validation loss: 0.31294771, Gradient norm: 461.28065442
INFO:root:[  255] Training loss: 0.31191252, Validation loss: 0.30944606, Gradient norm: 654.42300925
INFO:root:[  256] Training loss: 0.30543830, Validation loss: 0.35695328, Gradient norm: 472.24048325
INFO:root:[  257] Training loss: 0.30630954, Validation loss: 0.30507629, Gradient norm: 427.54481627
INFO:root:[  258] Training loss: 0.30905085, Validation loss: 0.29827759, Gradient norm: 646.01719583
INFO:root:[  259] Training loss: 0.30024927, Validation loss: 0.32676357, Gradient norm: 419.27697052
INFO:root:[  260] Training loss: 0.31032851, Validation loss: 0.29797379, Gradient norm: 676.00966898
INFO:root:[  261] Training loss: 0.29630153, Validation loss: 0.29377486, Gradient norm: 358.11886545
INFO:root:[  262] Training loss: 0.29758835, Validation loss: 0.32829930, Gradient norm: 464.59285365
INFO:root:[  263] Training loss: 0.30314208, Validation loss: 0.29511594, Gradient norm: 685.48872713
INFO:root:[  264] Training loss: 0.30032824, Validation loss: 0.34118147, Gradient norm: 628.61921736
INFO:root:[  265] Training loss: 0.30280267, Validation loss: 0.30262665, Gradient norm: 689.90719512
INFO:root:[  266] Training loss: 0.29211833, Validation loss: 0.29376040, Gradient norm: 395.52079046
INFO:root:[  267] Training loss: 0.29692051, Validation loss: 0.29530566, Gradient norm: 612.48454829
INFO:root:[  268] Training loss: 0.29306307, Validation loss: 0.31633707, Gradient norm: 524.14076807
INFO:root:[  269] Training loss: 0.29680070, Validation loss: 0.28450003, Gradient norm: 691.32524764
INFO:root:[  270] Training loss: 0.29209888, Validation loss: 0.28329818, Gradient norm: 519.33462025
INFO:root:[  271] Training loss: 0.28876975, Validation loss: 0.28525992, Gradient norm: 461.25838299
INFO:root:[  272] Training loss: 0.28649044, Validation loss: 0.28803073, Gradient norm: 435.60277206
INFO:root:[  273] Training loss: 0.28541324, Validation loss: 0.29509645, Gradient norm: 477.98242842
INFO:root:[  274] Training loss: 0.28670407, Validation loss: 0.30482362, Gradient norm: 532.04826557
INFO:root:[  275] Training loss: 0.28715916, Validation loss: 0.28093502, Gradient norm: 525.97192467
INFO:root:[  276] Training loss: 0.28153668, Validation loss: 0.27738029, Gradient norm: 414.06887947
INFO:root:[  277] Training loss: 0.28394235, Validation loss: 0.28060130, Gradient norm: 575.17839511
INFO:root:[  278] Training loss: 0.28360041, Validation loss: 0.27647544, Gradient norm: 535.09256036
INFO:root:[  279] Training loss: 0.28228782, Validation loss: 0.28781627, Gradient norm: 547.39154551
INFO:root:[  280] Training loss: 0.28383775, Validation loss: 0.27505158, Gradient norm: 609.57031811
INFO:root:[  281] Training loss: 0.27366805, Validation loss: 0.27606265, Gradient norm: 365.46579917
INFO:root:[  282] Training loss: 0.28082883, Validation loss: 0.29208848, Gradient norm: 591.99460877
INFO:root:[  283] Training loss: 0.27444632, Validation loss: 0.27133803, Gradient norm: 411.35634916
INFO:root:[  284] Training loss: 0.27657000, Validation loss: 0.27069586, Gradient norm: 509.15516304
INFO:root:[  285] Training loss: 0.27527657, Validation loss: 0.29802226, Gradient norm: 506.94261050
INFO:root:[  286] Training loss: 0.26981338, Validation loss: 0.26450716, Gradient norm: 332.68394245
INFO:root:[  287] Training loss: 0.27053723, Validation loss: 0.26787491, Gradient norm: 457.93772890
INFO:root:[  288] Training loss: 0.26796055, Validation loss: 0.27412801, Gradient norm: 418.26496649
INFO:root:[  289] Training loss: 0.27291932, Validation loss: 0.26053139, Gradient norm: 566.89709480
INFO:root:[  290] Training loss: 0.26858066, Validation loss: 0.26657725, Gradient norm: 528.80606262
INFO:root:[  291] Training loss: 0.26329147, Validation loss: 0.26490246, Gradient norm: 388.87086838
INFO:root:[  292] Training loss: 0.26122444, Validation loss: 0.29168242, Gradient norm: 336.14295501
INFO:root:[  293] Training loss: 0.27378071, Validation loss: 0.28012127, Gradient norm: 722.99961587
INFO:root:[  294] Training loss: 0.26872569, Validation loss: 0.26512656, Gradient norm: 640.52244012
INFO:root:[  295] Training loss: 0.26085459, Validation loss: 0.25560848, Gradient norm: 392.49479029
INFO:root:[  296] Training loss: 0.26325420, Validation loss: 0.25643505, Gradient norm: 497.93814571
INFO:root:[  297] Training loss: 0.25741535, Validation loss: 0.25391520, Gradient norm: 346.92198826
INFO:root:[  298] Training loss: 0.26241778, Validation loss: 0.25397159, Gradient norm: 550.56214261
INFO:root:[  299] Training loss: 0.25666478, Validation loss: 0.27054180, Gradient norm: 395.46905987
INFO:root:[  300] Training loss: 0.25526325, Validation loss: 0.27801313, Gradient norm: 328.82780787
INFO:root:[  301] Training loss: 0.25864994, Validation loss: 0.25371507, Gradient norm: 535.51737466
INFO:root:[  302] Training loss: 0.25601891, Validation loss: 0.25103320, Gradient norm: 503.11327280
INFO:root:[  303] Training loss: 0.25164456, Validation loss: 0.25887011, Gradient norm: 390.16226595
INFO:root:[  304] Training loss: 0.25563159, Validation loss: 0.24610350, Gradient norm: 554.80572276
INFO:root:[  305] Training loss: 0.25335018, Validation loss: 0.26093900, Gradient norm: 490.21889860
INFO:root:[  306] Training loss: 0.25178779, Validation loss: 0.24384012, Gradient norm: 458.21421818
INFO:root:[  307] Training loss: 0.24498502, Validation loss: 0.24351974, Gradient norm: 262.45815177
INFO:root:[  308] Training loss: 0.24549130, Validation loss: 0.24354896, Gradient norm: 361.90864273
INFO:root:[  309] Training loss: 0.25092210, Validation loss: 0.24986065, Gradient norm: 556.66977303
INFO:root:[  310] Training loss: 0.24742316, Validation loss: 0.24005624, Gradient norm: 496.36207170
INFO:root:[  311] Training loss: 0.24753150, Validation loss: 0.24449880, Gradient norm: 498.80127765
INFO:root:[  312] Training loss: 0.25221324, Validation loss: 0.24411811, Gradient norm: 593.90038896
INFO:root:[  313] Training loss: 0.24094932, Validation loss: 0.23913589, Gradient norm: 329.23593746
INFO:root:[  314] Training loss: 0.24078748, Validation loss: 0.26151794, Gradient norm: 355.99607581
INFO:root:[  315] Training loss: 0.24059488, Validation loss: 0.23511539, Gradient norm: 397.88297302
INFO:root:[  316] Training loss: 0.23934738, Validation loss: 0.24206729, Gradient norm: 369.26131141
INFO:root:[  317] Training loss: 0.23875533, Validation loss: 0.23432327, Gradient norm: 392.96362915
INFO:root:[  318] Training loss: 0.24020338, Validation loss: 0.25557159, Gradient norm: 471.86666776
INFO:root:[  319] Training loss: 0.23995455, Validation loss: 0.23546307, Gradient norm: 491.34923332
INFO:root:[  320] Training loss: 0.23313664, Validation loss: 0.23253764, Gradient norm: 279.25933224
INFO:root:[  321] Training loss: 0.24309782, Validation loss: 0.27459592, Gradient norm: 617.61398533
INFO:root:[  322] Training loss: 0.23851560, Validation loss: 0.22962461, Gradient norm: 470.61120307
INFO:root:[  323] Training loss: 0.23029780, Validation loss: 0.22822991, Gradient norm: 257.35171707
INFO:root:[  324] Training loss: 0.22956635, Validation loss: 0.24901356, Gradient norm: 268.56743797
INFO:root:[  325] Training loss: 0.23399413, Validation loss: 0.22784610, Gradient norm: 488.28238566
INFO:root:[  326] Training loss: 0.23366970, Validation loss: 0.23217741, Gradient norm: 399.48040605
INFO:root:[  327] Training loss: 0.23627239, Validation loss: 0.23004934, Gradient norm: 562.29603825
INFO:root:[  328] Training loss: 0.23262107, Validation loss: 0.22900605, Gradient norm: 478.56362664
INFO:root:[  329] Training loss: 0.23102604, Validation loss: 0.24734499, Gradient norm: 489.12891659
INFO:root:[  330] Training loss: 0.22439171, Validation loss: 0.22374658, Gradient norm: 278.98586041
INFO:root:[  331] Training loss: 0.23043284, Validation loss: 0.23771598, Gradient norm: 506.10648095
INFO:root:[  332] Training loss: 0.23210862, Validation loss: 0.22768221, Gradient norm: 536.91314486
INFO:root:[  333] Training loss: 0.22970883, Validation loss: 0.26670944, Gradient norm: 491.82841693
INFO:root:[  334] Training loss: 0.22645226, Validation loss: 0.23628293, Gradient norm: 392.18426448
INFO:root:[  335] Training loss: 0.22396672, Validation loss: 0.22277853, Gradient norm: 356.34349505
INFO:root:[  336] Training loss: 0.21971391, Validation loss: 0.21961968, Gradient norm: 225.30536513
INFO:root:[  337] Training loss: 0.22049313, Validation loss: 0.21752175, Gradient norm: 270.38550324
INFO:root:[  338] Training loss: 0.22366497, Validation loss: 0.24798270, Gradient norm: 458.73922977
INFO:root:[  339] Training loss: 0.22718896, Validation loss: 0.22776975, Gradient norm: 559.13871364
INFO:root:[  340] Training loss: 0.22063278, Validation loss: 0.21939623, Gradient norm: 389.39277501
INFO:root:[  341] Training loss: 0.22004234, Validation loss: 0.22126984, Gradient norm: 329.50090622
INFO:root:[  342] Training loss: 0.21641993, Validation loss: 0.21571744, Gradient norm: 252.43831380
INFO:root:[  343] Training loss: 0.21802346, Validation loss: 0.21233392, Gradient norm: 385.34397897
INFO:root:[  344] Training loss: 0.21746681, Validation loss: 0.21814442, Gradient norm: 366.86635153
INFO:root:[  345] Training loss: 0.22148163, Validation loss: 0.21189898, Gradient norm: 552.25201461
INFO:root:[  346] Training loss: 0.21365789, Validation loss: 0.21652209, Gradient norm: 280.43364907
INFO:root:[  347] Training loss: 0.21512594, Validation loss: 0.21332317, Gradient norm: 305.68805193
INFO:root:[  348] Training loss: 0.21522003, Validation loss: 0.21002550, Gradient norm: 398.76295804
INFO:root:[  349] Training loss: 0.21336986, Validation loss: 0.22135343, Gradient norm: 371.05769824
INFO:root:[  350] Training loss: 0.21262095, Validation loss: 0.20965473, Gradient norm: 371.58116283
INFO:root:[  351] Training loss: 0.21298668, Validation loss: 0.20725712, Gradient norm: 436.91792104
INFO:root:[  352] Training loss: 0.20818064, Validation loss: 0.21542756, Gradient norm: 197.63129876
INFO:root:[  353] Training loss: 0.21168452, Validation loss: 0.21120393, Gradient norm: 418.62931205
INFO:root:[  354] Training loss: 0.21061401, Validation loss: 0.21040351, Gradient norm: 430.52133024
INFO:root:[  355] Training loss: 0.20658570, Validation loss: 0.20798546, Gradient norm: 278.55288656
INFO:root:[  356] Training loss: 0.21649392, Validation loss: 0.20578620, Gradient norm: 560.67063202
INFO:root:[  357] Training loss: 0.21274270, Validation loss: 0.20648089, Gradient norm: 429.85825895
INFO:root:[  358] Training loss: 0.20928204, Validation loss: 0.22355239, Gradient norm: 374.96565632
INFO:root:[  359] Training loss: 0.20844091, Validation loss: 0.20489071, Gradient norm: 389.73733371
INFO:root:[  360] Training loss: 0.20777762, Validation loss: 0.20138240, Gradient norm: 383.72598381
INFO:root:[  361] Training loss: 0.20630195, Validation loss: 0.20736298, Gradient norm: 331.61761386
INFO:root:[  362] Training loss: 0.20475744, Validation loss: 0.20817646, Gradient norm: 316.38426122
INFO:root:[  363] Training loss: 0.20416187, Validation loss: 0.19966227, Gradient norm: 322.91440302
INFO:root:[  364] Training loss: 0.20122009, Validation loss: 0.20066367, Gradient norm: 176.39643608
INFO:root:[  365] Training loss: 0.20853274, Validation loss: 0.19966568, Gradient norm: 458.84667527
INFO:root:[  366] Training loss: 0.20871020, Validation loss: 0.20484987, Gradient norm: 473.56921985
INFO:root:[  367] Training loss: 0.19933515, Validation loss: 0.19812732, Gradient norm: 222.93901361
INFO:root:[  368] Training loss: 0.20256699, Validation loss: 0.19763121, Gradient norm: 335.59390298
INFO:root:[  369] Training loss: 0.19756638, Validation loss: 0.21322111, Gradient norm: 187.99695504
INFO:root:[  370] Training loss: 0.20195323, Validation loss: 0.20313338, Gradient norm: 366.87764788
INFO:root:[  371] Training loss: 0.19829200, Validation loss: 0.23540262, Gradient norm: 246.25031206
INFO:root:[  372] Training loss: 0.20072477, Validation loss: 0.19467923, Gradient norm: 353.66931974
INFO:root:[  373] Training loss: 0.19781475, Validation loss: 0.19392866, Gradient norm: 326.63665137
INFO:root:[  374] Training loss: 0.19675653, Validation loss: 0.19411292, Gradient norm: 285.62171820
INFO:root:[  375] Training loss: 0.19753408, Validation loss: 0.20641423, Gradient norm: 336.51872739
INFO:root:[  376] Training loss: 0.20262408, Validation loss: 0.19244569, Gradient norm: 454.74724312
INFO:root:[  377] Training loss: 0.19423319, Validation loss: 0.19281855, Gradient norm: 233.45414974
INFO:root:[  378] Training loss: 0.19825431, Validation loss: 0.19554348, Gradient norm: 353.00936515
INFO:root:[  379] Training loss: 0.19550404, Validation loss: 0.19730530, Gradient norm: 328.83736826
INFO:root:[  380] Training loss: 0.19566782, Validation loss: 0.19341701, Gradient norm: 301.83400647
INFO:root:[  381] Training loss: 0.19201942, Validation loss: 0.19040260, Gradient norm: 262.87144173
INFO:root:[  382] Training loss: 0.19146676, Validation loss: 0.19266725, Gradient norm: 253.49668126
INFO:root:[  383] Training loss: 0.19237068, Validation loss: 0.18915092, Gradient norm: 303.58628236
INFO:root:[  384] Training loss: 0.19123971, Validation loss: 0.19236232, Gradient norm: 288.43234710
INFO:root:[  385] Training loss: 0.19687482, Validation loss: 0.19446703, Gradient norm: 441.78021800
INFO:root:[  386] Training loss: 0.19370303, Validation loss: 0.19715046, Gradient norm: 386.66809736
INFO:root:[  387] Training loss: 0.19171696, Validation loss: 0.18739644, Gradient norm: 317.12623296
INFO:root:[  388] Training loss: 0.19398080, Validation loss: 0.18564659, Gradient norm: 409.10644516
INFO:root:[  389] Training loss: 0.18585596, Validation loss: 0.18472884, Gradient norm: 183.98533630
INFO:root:[  390] Training loss: 0.19566135, Validation loss: 0.20016570, Gradient norm: 443.35960231
INFO:root:[  391] Training loss: 0.19255946, Validation loss: 0.18462872, Gradient norm: 401.60933282
INFO:root:[  392] Training loss: 0.18877211, Validation loss: 0.19783296, Gradient norm: 295.82928909
INFO:root:[  393] Training loss: 0.18876075, Validation loss: 0.19091294, Gradient norm: 344.69534177
INFO:root:[  394] Training loss: 0.18917601, Validation loss: 0.18540949, Gradient norm: 390.38651940
INFO:root:[  395] Training loss: 0.18487850, Validation loss: 0.18193802, Gradient norm: 246.30145525
INFO:root:[  396] Training loss: 0.18852309, Validation loss: 0.18299636, Gradient norm: 356.04235600
INFO:root:[  397] Training loss: 0.18794993, Validation loss: 0.20064628, Gradient norm: 378.17027421
INFO:root:[  398] Training loss: 0.18567353, Validation loss: 0.18363928, Gradient norm: 302.29566762
INFO:root:[  399] Training loss: 0.18814936, Validation loss: 0.18208920, Gradient norm: 299.42938339
INFO:root:[  400] Training loss: 0.18787619, Validation loss: 0.18207103, Gradient norm: 407.48833519
INFO:root:[  401] Training loss: 0.18857996, Validation loss: 0.18058846, Gradient norm: 417.93712984
INFO:root:[  402] Training loss: 0.18925656, Validation loss: 0.18332852, Gradient norm: 418.56644340
INFO:root:[  403] Training loss: 0.18537490, Validation loss: 0.18089420, Gradient norm: 294.26197137
INFO:root:[  404] Training loss: 0.18478014, Validation loss: 0.18214676, Gradient norm: 359.93595204
INFO:root:[  405] Training loss: 0.18399717, Validation loss: 0.18091080, Gradient norm: 331.20561441
INFO:root:[  406] Training loss: 0.18457611, Validation loss: 0.18644980, Gradient norm: 418.75560519
INFO:root:[  407] Training loss: 0.18290921, Validation loss: 0.18500719, Gradient norm: 360.97144707
INFO:root:[  408] Training loss: 0.18152275, Validation loss: 0.19351726, Gradient norm: 340.25458130
INFO:root:[  409] Training loss: 0.18321022, Validation loss: 0.17617785, Gradient norm: 398.17668883
INFO:root:[  410] Training loss: 0.18563589, Validation loss: 0.17814273, Gradient norm: 447.87401558
INFO:root:[  411] Training loss: 0.18026175, Validation loss: 0.19339492, Gradient norm: 311.45595806
INFO:root:[  412] Training loss: 0.18322890, Validation loss: 0.18551025, Gradient norm: 406.53157725
INFO:root:[  413] Training loss: 0.18652851, Validation loss: 0.18497706, Gradient norm: 473.49207763
INFO:root:[  414] Training loss: 0.17979196, Validation loss: 0.18245957, Gradient norm: 338.11507650
INFO:root:[  415] Training loss: 0.17782248, Validation loss: 0.18313342, Gradient norm: 312.85873808
INFO:root:[  416] Training loss: 0.19217569, Validation loss: 0.18344912, Gradient norm: 651.92838250
INFO:root:[  417] Training loss: 0.17712857, Validation loss: 0.17599776, Gradient norm: 271.05922059
INFO:root:[  418] Training loss: 0.18443890, Validation loss: 0.17578359, Gradient norm: 489.00314515
INFO:root:[  419] Training loss: 0.18318851, Validation loss: 0.18345922, Gradient norm: 466.59992651
INFO:root:[  420] Training loss: 0.18313650, Validation loss: 0.18502923, Gradient norm: 458.32513294
INFO:root:[  421] Training loss: 0.18098730, Validation loss: 0.17357028, Gradient norm: 451.55487174
INFO:root:[  422] Training loss: 0.18329104, Validation loss: 0.18037280, Gradient norm: 498.98357200
INFO:root:[  423] Training loss: 0.17743225, Validation loss: 0.19789028, Gradient norm: 373.05455753
INFO:root:[  424] Training loss: 0.18045727, Validation loss: 0.17390461, Gradient norm: 438.68646967
INFO:root:[  425] Training loss: 0.18062615, Validation loss: 0.17933344, Gradient norm: 449.92633548
INFO:root:[  426] Training loss: 0.18180665, Validation loss: 0.19647894, Gradient norm: 511.56295853
INFO:root:[  427] Training loss: 0.17797441, Validation loss: 0.17381184, Gradient norm: 420.82622708
INFO:root:[  428] Training loss: 0.18686934, Validation loss: 0.17125537, Gradient norm: 607.22670327
INFO:root:[  429] Training loss: 0.17655159, Validation loss: 0.18442525, Gradient norm: 345.07840417
INFO:root:[  430] Training loss: 0.18007110, Validation loss: 0.17341125, Gradient norm: 462.98129363
INFO:root:[  431] Training loss: 0.18502322, Validation loss: 0.17865894, Gradient norm: 561.68048976
INFO:root:[  432] Training loss: 0.18337116, Validation loss: 0.17878212, Gradient norm: 505.83568301
INFO:root:[  433] Training loss: 0.18196599, Validation loss: 0.17769196, Gradient norm: 496.10252225
INFO:root:[  434] Training loss: 0.17905698, Validation loss: 0.17844903, Gradient norm: 485.66186193
INFO:root:[  435] Training loss: 0.17754434, Validation loss: 0.19827766, Gradient norm: 445.30908201
INFO:root:[  436] Training loss: 0.18474081, Validation loss: 0.17091066, Gradient norm: 606.99063440
INFO:root:[  437] Training loss: 0.17628611, Validation loss: 0.18326273, Gradient norm: 421.47383600
INFO:root:[  438] Training loss: 0.18120692, Validation loss: 0.20507648, Gradient norm: 574.26421853
INFO:root:[  439] Training loss: 0.18000015, Validation loss: 0.19507934, Gradient norm: 552.15278606
INFO:root:[  440] Training loss: 0.17928960, Validation loss: 0.19725146, Gradient norm: 463.97832371
INFO:root:[  441] Training loss: 0.17880251, Validation loss: 0.17695757, Gradient norm: 544.07055272
INFO:root:[  442] Training loss: 0.18024790, Validation loss: 0.19493966, Gradient norm: 573.78102848
INFO:root:[  443] Training loss: 0.18076723, Validation loss: 0.17339434, Gradient norm: 586.13830593
INFO:root:[  444] Training loss: 0.17925476, Validation loss: 0.17530802, Gradient norm: 584.89527464
INFO:root:[  445] Training loss: 0.17821111, Validation loss: 0.17189200, Gradient norm: 581.90917852
INFO:root:EP 445: Early stopping
INFO:root:Training the model took 7113.676s.
INFO:root:Emptying the cuda cache took 0.046s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 4.234
INFO:root:EnergyScoreTrain: 2.92119
INFO:root:CoverageTrain: 0.87694
INFO:root:IntervalWidthTrain: 0.08407
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 4.08843
INFO:root:EnergyScoreValidation: 2.82211
INFO:root:CoverageValidation: 0.87835
INFO:root:IntervalWidthValidation: 0.08473
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 3.33368
INFO:root:EnergyScoreTest: 2.29939
INFO:root:CoverageTest: 0.87757
INFO:root:IntervalWidthTest: 0.08314
INFO:root:###21 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 369098752
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.97120388, Validation loss: 2.63655365, Gradient norm: 7.62072134
INFO:root:[    2] Training loss: 2.39996260, Validation loss: 2.21411328, Gradient norm: 5.45177651
INFO:root:[    3] Training loss: 2.08891363, Validation loss: 1.97365761, Gradient norm: 4.23627996
INFO:root:[    4] Training loss: 1.88606738, Validation loss: 1.83943431, Gradient norm: 3.73205206
INFO:root:[    5] Training loss: 1.74818226, Validation loss: 1.66161478, Gradient norm: 3.56589294
INFO:root:[    6] Training loss: 1.64667642, Validation loss: 1.58206153, Gradient norm: 3.85706360
INFO:root:[    7] Training loss: 1.55022117, Validation loss: 1.49306178, Gradient norm: 3.52303452
INFO:root:[    8] Training loss: 1.49728612, Validation loss: 1.44986937, Gradient norm: 3.44357674
INFO:root:[    9] Training loss: 1.43485737, Validation loss: 1.40565612, Gradient norm: 3.41196456
INFO:root:[   10] Training loss: 1.38201819, Validation loss: 1.36666288, Gradient norm: 3.32657198
INFO:root:[   11] Training loss: 1.33786557, Validation loss: 1.37278374, Gradient norm: 4.20463361
INFO:root:[   12] Training loss: 1.34723023, Validation loss: 1.33062748, Gradient norm: 4.28132930
INFO:root:[   13] Training loss: 1.31719274, Validation loss: 1.29614095, Gradient norm: 5.22636615
INFO:root:[   14] Training loss: 1.30699515, Validation loss: 1.29587382, Gradient norm: 5.01933652
INFO:root:[   15] Training loss: 1.29807073, Validation loss: 1.29259274, Gradient norm: 6.55762865
INFO:root:[   16] Training loss: 1.30364124, Validation loss: 1.26779340, Gradient norm: 10.09864722
INFO:root:[   17] Training loss: 1.28075345, Validation loss: 1.28436424, Gradient norm: 10.66614910
INFO:root:[   18] Training loss: 1.26515622, Validation loss: 1.25581513, Gradient norm: 11.02301708
INFO:root:[   19] Training loss: 1.25885365, Validation loss: 1.25922545, Gradient norm: 16.59669669
INFO:root:[   20] Training loss: 1.24688138, Validation loss: 1.22500475, Gradient norm: 22.21960296
INFO:root:[   21] Training loss: 1.23333689, Validation loss: 1.21944767, Gradient norm: 28.01493934
INFO:root:[   22] Training loss: 1.23197567, Validation loss: 1.22072450, Gradient norm: 27.88377555
INFO:root:[   23] Training loss: 1.20049781, Validation loss: 1.19827817, Gradient norm: 45.21356325
INFO:root:[   24] Training loss: 1.19269755, Validation loss: 1.17956607, Gradient norm: 52.69107782
INFO:root:[   25] Training loss: 1.18939529, Validation loss: 1.18025585, Gradient norm: 145.27677102
INFO:root:[   26] Training loss: 1.17215706, Validation loss: 1.19915390, Gradient norm: 114.38897622
INFO:root:[   27] Training loss: 1.15572010, Validation loss: 1.13502977, Gradient norm: 111.74468189
INFO:root:[   28] Training loss: 1.14238783, Validation loss: 1.16821933, Gradient norm: 108.43212145
INFO:root:[   29] Training loss: 1.16040172, Validation loss: 1.13004287, Gradient norm: 202.48286609
INFO:root:[   30] Training loss: 1.13803989, Validation loss: 1.11366135, Gradient norm: 137.55945239
INFO:root:[   31] Training loss: 1.12313532, Validation loss: 1.14461599, Gradient norm: 184.19365276
INFO:root:[   32] Training loss: 1.12818845, Validation loss: 1.11354236, Gradient norm: 231.64592874
INFO:root:[   33] Training loss: 1.11865449, Validation loss: 1.13330100, Gradient norm: 290.29398647
INFO:root:[   34] Training loss: 1.13501710, Validation loss: 1.11847150, Gradient norm: 399.21042201
INFO:root:[   35] Training loss: 1.12753046, Validation loss: 1.12485045, Gradient norm: 419.80789056
INFO:root:[   36] Training loss: 1.09527481, Validation loss: 1.08158406, Gradient norm: 288.65384828
INFO:root:[   37] Training loss: 1.08275184, Validation loss: 1.10998034, Gradient norm: 269.83231639
INFO:root:[   38] Training loss: 1.08352350, Validation loss: 1.06479130, Gradient norm: 333.23600745
INFO:root:[   39] Training loss: 1.06593867, Validation loss: 1.06727632, Gradient norm: 218.67938900
INFO:root:[   40] Training loss: 1.06141459, Validation loss: 1.05554859, Gradient norm: 341.35377895
INFO:root:[   41] Training loss: 1.04898229, Validation loss: 1.08384756, Gradient norm: 188.31610916
INFO:root:[   42] Training loss: 1.05683499, Validation loss: 1.06732045, Gradient norm: 528.34682465
INFO:root:[   43] Training loss: 1.04542861, Validation loss: 1.03795475, Gradient norm: 351.94880540
INFO:root:[   44] Training loss: 1.04239288, Validation loss: 1.09401393, Gradient norm: 447.72821602
INFO:root:[   45] Training loss: 1.04120533, Validation loss: 1.01489736, Gradient norm: 480.47976899
INFO:root:[   46] Training loss: 1.02750805, Validation loss: 1.14166917, Gradient norm: 440.74253828
INFO:root:[   47] Training loss: 1.09173785, Validation loss: 1.04573407, Gradient norm: 987.62514347
INFO:root:[   48] Training loss: 1.01269376, Validation loss: 1.04602822, Gradient norm: 355.29671943
INFO:root:[   49] Training loss: 1.02997727, Validation loss: 0.99918898, Gradient norm: 605.93920623
INFO:root:[   50] Training loss: 1.00052999, Validation loss: 0.98293099, Gradient norm: 316.90481269
INFO:root:[   51] Training loss: 0.99544420, Validation loss: 0.99057997, Gradient norm: 345.01309523
INFO:root:[   52] Training loss: 1.00087774, Validation loss: 0.97641893, Gradient norm: 542.26329070
INFO:root:[   53] Training loss: 0.97778100, Validation loss: 0.97293173, Gradient norm: 319.76336446
INFO:root:[   54] Training loss: 0.99260240, Validation loss: 1.00781269, Gradient norm: 554.22110158
INFO:root:[   55] Training loss: 0.99910196, Validation loss: 0.97165451, Gradient norm: 684.37734432
INFO:root:[   56] Training loss: 0.97315545, Validation loss: 0.96500607, Gradient norm: 449.53875200
INFO:root:[   57] Training loss: 0.98107123, Validation loss: 0.96996312, Gradient norm: 608.32806646
INFO:root:[   58] Training loss: 0.95886560, Validation loss: 0.95569293, Gradient norm: 329.79882986
INFO:root:[   59] Training loss: 0.98429851, Validation loss: 0.96178983, Gradient norm: 827.18358935
INFO:root:[   60] Training loss: 0.96192896, Validation loss: 0.96416147, Gradient norm: 518.51837566
INFO:root:[   61] Training loss: 0.97539979, Validation loss: 0.96493443, Gradient norm: 747.11229921
INFO:root:[   62] Training loss: 0.96974330, Validation loss: 0.94835480, Gradient norm: 863.19857262
INFO:root:[   63] Training loss: 0.94667123, Validation loss: 0.94446910, Gradient norm: 437.94673646
INFO:root:[   64] Training loss: 0.94215354, Validation loss: 0.93650190, Gradient norm: 561.40757482
INFO:root:[   65] Training loss: 0.94218898, Validation loss: 0.92813460, Gradient norm: 672.72238639
INFO:root:[   66] Training loss: 0.97465170, Validation loss: 0.92058522, Gradient norm: 1109.41718107
INFO:root:[   67] Training loss: 0.92841532, Validation loss: 0.92292644, Gradient norm: 503.65578884
INFO:root:[   68] Training loss: 0.93683390, Validation loss: 0.91292995, Gradient norm: 682.03087875
INFO:root:[   69] Training loss: 0.92284886, Validation loss: 0.90779764, Gradient norm: 614.36670361
INFO:root:[   70] Training loss: 0.91003560, Validation loss: 0.91944167, Gradient norm: 436.97195679
INFO:root:[   71] Training loss: 0.92488462, Validation loss: 0.90170458, Gradient norm: 782.15531174
INFO:root:[   72] Training loss: 0.90762850, Validation loss: 0.90465418, Gradient norm: 481.38775701
INFO:root:[   73] Training loss: 0.91884385, Validation loss: 0.89264203, Gradient norm: 785.56624811
INFO:root:[   74] Training loss: 0.89901782, Validation loss: 0.88725168, Gradient norm: 598.43752451
INFO:root:[   75] Training loss: 0.90225510, Validation loss: 0.88293279, Gradient norm: 793.93166258
INFO:root:[   76] Training loss: 0.90700993, Validation loss: 0.89562175, Gradient norm: 839.37358583
INFO:root:[   77] Training loss: 0.91527468, Validation loss: 0.90086132, Gradient norm: 1077.39463612
INFO:root:[   78] Training loss: 0.87820391, Validation loss: 0.87036220, Gradient norm: 446.41624473
INFO:root:[   79] Training loss: 0.88423038, Validation loss: 0.87045915, Gradient norm: 605.89423600
INFO:root:[   80] Training loss: 0.87531135, Validation loss: 0.87999252, Gradient norm: 568.49592799
INFO:root:[   81] Training loss: 0.88143016, Validation loss: 0.86458068, Gradient norm: 681.28412639
INFO:root:[   82] Training loss: 0.86902323, Validation loss: 0.87382293, Gradient norm: 579.03389677
INFO:root:[   83] Training loss: 0.89869967, Validation loss: 0.87432785, Gradient norm: 1033.23557158
INFO:root:[   84] Training loss: 0.86069823, Validation loss: 0.85739763, Gradient norm: 422.57155157
INFO:root:[   85] Training loss: 0.85677187, Validation loss: 0.86440580, Gradient norm: 476.07323083
INFO:root:[   86] Training loss: 0.85204483, Validation loss: 0.85867242, Gradient norm: 484.20897968
INFO:root:[   87] Training loss: 0.85118483, Validation loss: 0.83563014, Gradient norm: 561.90923225
INFO:root:[   88] Training loss: 0.84563002, Validation loss: 0.89030262, Gradient norm: 487.05980850
INFO:root:[   89] Training loss: 0.85702169, Validation loss: 0.83411949, Gradient norm: 845.36662743
INFO:root:[   90] Training loss: 0.84462194, Validation loss: 0.88038018, Gradient norm: 697.66272452
INFO:root:[   91] Training loss: 0.83609097, Validation loss: 0.83098784, Gradient norm: 590.25385545
INFO:root:[   92] Training loss: 0.83666151, Validation loss: 0.84107750, Gradient norm: 615.21132444
INFO:root:[   93] Training loss: 0.84894365, Validation loss: 0.82052949, Gradient norm: 939.19603462
INFO:root:[   94] Training loss: 0.83635576, Validation loss: 0.85428686, Gradient norm: 773.52101540
INFO:root:[   95] Training loss: 0.82227499, Validation loss: 0.84898026, Gradient norm: 578.70264188
INFO:root:[   96] Training loss: 0.83590703, Validation loss: 0.82071771, Gradient norm: 913.17032075
INFO:root:[   97] Training loss: 0.85418951, Validation loss: 0.81077455, Gradient norm: 1077.90722009
INFO:root:[   98] Training loss: 0.81168914, Validation loss: 0.80161894, Gradient norm: 483.84452743
INFO:root:[   99] Training loss: 0.80827069, Validation loss: 0.88019599, Gradient norm: 404.15757997
INFO:root:[  100] Training loss: 0.80765556, Validation loss: 0.79904844, Gradient norm: 561.87305618
INFO:root:[  101] Training loss: 0.79726956, Validation loss: 0.82109399, Gradient norm: 442.95714740
INFO:root:[  102] Training loss: 0.79798855, Validation loss: 0.81536136, Gradient norm: 512.62610947
INFO:root:[  103] Training loss: 0.79358046, Validation loss: 0.78957661, Gradient norm: 418.56393311
INFO:root:[  104] Training loss: 0.79772897, Validation loss: 0.86102344, Gradient norm: 677.25132528
INFO:root:[  105] Training loss: 0.78327942, Validation loss: 0.77919945, Gradient norm: 323.15229672
INFO:root:[  106] Training loss: 0.78037096, Validation loss: 0.78094488, Gradient norm: 359.58034521
INFO:root:[  107] Training loss: 0.78018844, Validation loss: 0.80433685, Gradient norm: 488.23521983
INFO:root:[  108] Training loss: 0.77798820, Validation loss: 0.80673549, Gradient norm: 495.64341679
INFO:root:[  109] Training loss: 0.79022641, Validation loss: 0.77565433, Gradient norm: 913.60954193
INFO:root:[  110] Training loss: 0.77039577, Validation loss: 0.76977148, Gradient norm: 487.30862721
INFO:root:[  111] Training loss: 0.76544526, Validation loss: 0.76621045, Gradient norm: 487.27479895
INFO:root:[  112] Training loss: 0.76000237, Validation loss: 0.75416395, Gradient norm: 488.73891120
INFO:root:[  113] Training loss: 0.76448968, Validation loss: 0.74997339, Gradient norm: 591.25678012
INFO:root:[  114] Training loss: 0.76250580, Validation loss: 0.75282898, Gradient norm: 692.42753672
INFO:root:[  115] Training loss: 0.76264467, Validation loss: 0.82315735, Gradient norm: 632.28260260
INFO:root:[  116] Training loss: 0.75581339, Validation loss: 0.76479901, Gradient norm: 616.40000769
INFO:root:[  117] Training loss: 0.75369204, Validation loss: 0.74435007, Gradient norm: 650.92570814
INFO:root:[  118] Training loss: 0.74435685, Validation loss: 0.75703956, Gradient norm: 502.91726769
INFO:root:[  119] Training loss: 0.76532711, Validation loss: 0.82442399, Gradient norm: 955.91014922
INFO:root:[  120] Training loss: 0.75338648, Validation loss: 0.75287296, Gradient norm: 811.74070554
INFO:root:[  121] Training loss: 0.73587612, Validation loss: 0.74431389, Gradient norm: 499.06550097
INFO:root:[  122] Training loss: 0.73468174, Validation loss: 0.72646937, Gradient norm: 592.41396852
INFO:root:[  123] Training loss: 0.72023344, Validation loss: 0.72999665, Gradient norm: 311.70098206
INFO:root:[  124] Training loss: 0.72429153, Validation loss: 0.71704466, Gradient norm: 542.40923977
INFO:root:[  125] Training loss: 0.72360959, Validation loss: 0.71684556, Gradient norm: 585.96397424
INFO:root:[  126] Training loss: 0.71900707, Validation loss: 0.73401769, Gradient norm: 501.67325288
INFO:root:[  127] Training loss: 0.73174077, Validation loss: 0.72580634, Gradient norm: 786.16182310
INFO:root:[  128] Training loss: 0.70882170, Validation loss: 0.71149508, Gradient norm: 416.36856780
INFO:root:[  129] Training loss: 0.71058584, Validation loss: 0.70729169, Gradient norm: 543.01092043
INFO:root:[  130] Training loss: 0.71527008, Validation loss: 0.70092955, Gradient norm: 814.49741792
INFO:root:[  131] Training loss: 0.70488345, Validation loss: 0.69269757, Gradient norm: 620.10509740
INFO:root:[  132] Training loss: 0.70251833, Validation loss: 0.69385873, Gradient norm: 530.59340550
INFO:root:[  133] Training loss: 0.69447698, Validation loss: 0.68922042, Gradient norm: 481.69374401
INFO:root:[  134] Training loss: 0.69123126, Validation loss: 0.70622013, Gradient norm: 461.89691755
INFO:root:[  135] Training loss: 0.69180747, Validation loss: 0.71014750, Gradient norm: 551.65667699
INFO:root:[  136] Training loss: 0.68746441, Validation loss: 0.67922773, Gradient norm: 586.66083813
INFO:root:[  137] Training loss: 0.68684035, Validation loss: 0.72780201, Gradient norm: 594.99480616
INFO:root:[  138] Training loss: 0.68676973, Validation loss: 0.69120409, Gradient norm: 668.07596168
INFO:root:[  139] Training loss: 0.68398225, Validation loss: 0.69121884, Gradient norm: 673.96389472
INFO:root:[  140] Training loss: 0.68620056, Validation loss: 0.67108737, Gradient norm: 773.42424750
INFO:root:[  141] Training loss: 0.67999070, Validation loss: 0.66224353, Gradient norm: 656.73776451
INFO:root:[  142] Training loss: 0.67416713, Validation loss: 0.66496264, Gradient norm: 640.92560700
INFO:root:[  143] Training loss: 0.66823914, Validation loss: 0.65655682, Gradient norm: 530.48520784
INFO:root:[  144] Training loss: 0.65985351, Validation loss: 0.66305783, Gradient norm: 417.54325647
INFO:root:[  145] Training loss: 0.65829011, Validation loss: 0.68539943, Gradient norm: 439.04372416
INFO:root:[  146] Training loss: 0.65804808, Validation loss: 0.65411653, Gradient norm: 622.35438942
INFO:root:[  147] Training loss: 0.65603858, Validation loss: 0.65262496, Gradient norm: 590.53139840
INFO:root:[  148] Training loss: 0.64685274, Validation loss: 0.64525095, Gradient norm: 401.38843583
INFO:root:[  149] Training loss: 0.64228109, Validation loss: 0.63905546, Gradient norm: 325.77180480
INFO:root:[  150] Training loss: 0.64849298, Validation loss: 0.63467848, Gradient norm: 651.96721422
INFO:root:[  151] Training loss: 0.63755258, Validation loss: 0.63221128, Gradient norm: 390.04437352
INFO:root:[  152] Training loss: 0.64806978, Validation loss: 0.64951688, Gradient norm: 781.11852480
INFO:root:[  153] Training loss: 0.63640614, Validation loss: 0.62888874, Gradient norm: 614.75513504
INFO:root:[  154] Training loss: 0.63009329, Validation loss: 0.64806852, Gradient norm: 500.41898791
INFO:root:[  155] Training loss: 0.63782843, Validation loss: 0.62202674, Gradient norm: 687.48282829
INFO:root:[  156] Training loss: 0.63467058, Validation loss: 0.70950991, Gradient norm: 745.04433452
INFO:root:[  157] Training loss: 0.63135870, Validation loss: 0.63752898, Gradient norm: 689.41062177
INFO:root:[  158] Training loss: 0.62414594, Validation loss: 0.65302963, Gradient norm: 601.22694534
INFO:root:[  159] Training loss: 0.62154982, Validation loss: 0.61377130, Gradient norm: 550.88497856
INFO:root:[  160] Training loss: 0.61724243, Validation loss: 0.61456620, Gradient norm: 577.64257273
INFO:root:[  161] Training loss: 0.61073931, Validation loss: 0.60444649, Gradient norm: 512.40737397
INFO:root:[  162] Training loss: 0.60964416, Validation loss: 0.60221518, Gradient norm: 485.58178819
INFO:root:[  163] Training loss: 0.60448812, Validation loss: 0.60754713, Gradient norm: 497.80613081
INFO:root:[  164] Training loss: 0.60195578, Validation loss: 0.60943032, Gradient norm: 476.27760066
INFO:root:[  165] Training loss: 0.59406575, Validation loss: 0.59916970, Gradient norm: 352.15463637
INFO:root:[  166] Training loss: 0.60005678, Validation loss: 0.58902533, Gradient norm: 664.54105173
INFO:root:[  167] Training loss: 0.59547201, Validation loss: 0.62335970, Gradient norm: 507.41738261
INFO:root:[  168] Training loss: 0.60114610, Validation loss: 0.60336334, Gradient norm: 796.56569339
INFO:root:[  169] Training loss: 0.59386231, Validation loss: 0.58833271, Gradient norm: 633.66143688
INFO:root:[  170] Training loss: 0.58578298, Validation loss: 0.58076343, Gradient norm: 519.25504728
INFO:root:[  171] Training loss: 0.58997767, Validation loss: 0.59415103, Gradient norm: 682.44570333
INFO:root:[  172] Training loss: 0.58556665, Validation loss: 0.59580959, Gradient norm: 655.15078269
INFO:root:[  173] Training loss: 0.57439632, Validation loss: 0.57049762, Gradient norm: 432.72295150
INFO:root:[  174] Training loss: 0.57244630, Validation loss: 0.58637219, Gradient norm: 419.89373676
INFO:root:[  175] Training loss: 0.56898932, Validation loss: 0.56254187, Gradient norm: 468.75471864
INFO:root:[  176] Training loss: 0.56993534, Validation loss: 0.56352020, Gradient norm: 564.27293723
INFO:root:[  177] Training loss: 0.56362892, Validation loss: 0.56263875, Gradient norm: 421.12261883
INFO:root:[  178] Training loss: 0.56067480, Validation loss: 0.56119972, Gradient norm: 459.76829426
INFO:root:[  179] Training loss: 0.56414802, Validation loss: 0.63883032, Gradient norm: 661.91443229
INFO:root:[  180] Training loss: 0.57877901, Validation loss: 0.56447894, Gradient norm: 1002.01858401
INFO:root:[  181] Training loss: 0.55842322, Validation loss: 0.55342213, Gradient norm: 634.74475700
INFO:root:[  182] Training loss: 0.55621938, Validation loss: 0.54976393, Gradient norm: 625.86927711
INFO:root:[  183] Training loss: 0.54524378, Validation loss: 0.56324904, Gradient norm: 404.39179731
INFO:root:[  184] Training loss: 0.54617344, Validation loss: 0.55903701, Gradient norm: 529.60625025
INFO:root:[  185] Training loss: 0.54135936, Validation loss: 0.53415983, Gradient norm: 422.65882505
INFO:root:[  186] Training loss: 0.53794487, Validation loss: 0.53316611, Gradient norm: 429.98375266
INFO:root:[  187] Training loss: 0.54256687, Validation loss: 0.55673873, Gradient norm: 651.22638802
INFO:root:[  188] Training loss: 0.53525641, Validation loss: 0.53805546, Gradient norm: 543.67039826
INFO:root:[  189] Training loss: 0.53620532, Validation loss: 0.52794152, Gradient norm: 646.25756292
INFO:root:[  190] Training loss: 0.52529077, Validation loss: 0.52182326, Gradient norm: 380.52812413
INFO:root:[  191] Training loss: 0.52432805, Validation loss: 0.51813761, Gradient norm: 404.96880804
INFO:root:[  192] Training loss: 0.51696475, Validation loss: 0.52887602, Gradient norm: 254.99482532
INFO:root:[  193] Training loss: 0.51940463, Validation loss: 0.54025548, Gradient norm: 505.66426052
INFO:root:[  194] Training loss: 0.51751129, Validation loss: 0.51291914, Gradient norm: 466.91402854
INFO:root:[  195] Training loss: 0.51414582, Validation loss: 0.53643792, Gradient norm: 476.99041385
INFO:root:[  196] Training loss: 0.51068896, Validation loss: 0.51686692, Gradient norm: 482.27786263
INFO:root:[  197] Training loss: 0.50981339, Validation loss: 0.50829071, Gradient norm: 500.97258523
INFO:root:[  198] Training loss: 0.51860052, Validation loss: 0.49924490, Gradient norm: 816.93208208
INFO:root:[  199] Training loss: 0.50689177, Validation loss: 0.49518276, Gradient norm: 579.44014071
INFO:root:[  200] Training loss: 0.49990553, Validation loss: 0.50826517, Gradient norm: 414.30890299
INFO:root:[  201] Training loss: 0.49276483, Validation loss: 0.49013971, Gradient norm: 234.69644111
INFO:root:[  202] Training loss: 0.49064017, Validation loss: 0.52047722, Gradient norm: 283.30987293
INFO:root:[  203] Training loss: 0.49947516, Validation loss: 0.48680169, Gradient norm: 654.21171134
INFO:root:[  204] Training loss: 0.48624306, Validation loss: 0.48850267, Gradient norm: 297.89580317
INFO:root:[  205] Training loss: 0.48400121, Validation loss: 0.50056065, Gradient norm: 383.53751013
INFO:root:[  206] Training loss: 0.48954490, Validation loss: 0.47789830, Gradient norm: 612.12215631
INFO:root:[  207] Training loss: 0.48478760, Validation loss: 0.48062372, Gradient norm: 512.78359192
INFO:root:[  208] Training loss: 0.48741051, Validation loss: 0.47495031, Gradient norm: 693.90093452
INFO:root:[  209] Training loss: 0.47329457, Validation loss: 0.48966582, Gradient norm: 339.32932707
INFO:root:[  210] Training loss: 0.46966943, Validation loss: 0.47687764, Gradient norm: 301.14828965
INFO:root:[  211] Training loss: 0.47070159, Validation loss: 0.48117482, Gradient norm: 444.91518113
INFO:root:[  212] Training loss: 0.47209512, Validation loss: 0.45935363, Gradient norm: 582.33605843
INFO:root:[  213] Training loss: 0.46959268, Validation loss: 0.49867551, Gradient norm: 570.07447180
INFO:root:[  214] Training loss: 0.47149591, Validation loss: 0.46276948, Gradient norm: 623.24760987
INFO:root:[  215] Training loss: 0.45812501, Validation loss: 0.45133134, Gradient norm: 394.07835205
INFO:root:[  216] Training loss: 0.45790082, Validation loss: 0.47030604, Gradient norm: 439.85626151
INFO:root:[  217] Training loss: 0.45484819, Validation loss: 0.45116312, Gradient norm: 456.28524848
INFO:root:[  218] Training loss: 0.44947337, Validation loss: 0.44541810, Gradient norm: 378.56494716
INFO:root:[  219] Training loss: 0.45065729, Validation loss: 0.44321765, Gradient norm: 450.35798948
INFO:root:[  220] Training loss: 0.44551781, Validation loss: 0.45870316, Gradient norm: 435.09994418
INFO:root:[  221] Training loss: 0.44193783, Validation loss: 0.44523391, Gradient norm: 333.17182763
INFO:root:[  222] Training loss: 0.44833062, Validation loss: 0.45310475, Gradient norm: 657.67460946
INFO:root:[  223] Training loss: 0.43887388, Validation loss: 0.44288447, Gradient norm: 435.56266518
INFO:root:[  224] Training loss: 0.43425656, Validation loss: 0.42915603, Gradient norm: 363.36279574
INFO:root:[  225] Training loss: 0.43408196, Validation loss: 0.43596795, Gradient norm: 464.81667762
INFO:root:[  226] Training loss: 0.43137077, Validation loss: 0.42878769, Gradient norm: 421.30826905
INFO:root:[  227] Training loss: 0.42911415, Validation loss: 0.42672890, Gradient norm: 406.79992411
INFO:root:[  228] Training loss: 0.42778660, Validation loss: 0.41979595, Gradient norm: 513.74451592
INFO:root:[  229] Training loss: 0.42304301, Validation loss: 0.41753027, Gradient norm: 404.16881836
INFO:root:[  230] Training loss: 0.41753966, Validation loss: 0.44320408, Gradient norm: 274.56277221
INFO:root:[  231] Training loss: 0.41821563, Validation loss: 0.41938166, Gradient norm: 409.35677623
INFO:root:[  232] Training loss: 0.42572535, Validation loss: 0.41309807, Gradient norm: 734.03950428
INFO:root:[  233] Training loss: 0.42284534, Validation loss: 0.40826539, Gradient norm: 632.91772073
INFO:root:[  234] Training loss: 0.41079333, Validation loss: 0.44336169, Gradient norm: 440.26874387
INFO:root:[  235] Training loss: 0.40907625, Validation loss: 0.40353107, Gradient norm: 392.37452822
INFO:root:[  236] Training loss: 0.40155857, Validation loss: 0.40443767, Gradient norm: 240.81471219
INFO:root:[  237] Training loss: 0.39901521, Validation loss: 0.39714985, Gradient norm: 295.90315417
INFO:root:[  238] Training loss: 0.39872664, Validation loss: 0.39626073, Gradient norm: 363.74338529
INFO:root:[  239] Training loss: 0.39319980, Validation loss: 0.39634040, Gradient norm: 253.01786796
INFO:root:[  240] Training loss: 0.39997122, Validation loss: 0.39550793, Gradient norm: 596.55468755
INFO:root:[  241] Training loss: 0.39256253, Validation loss: 0.38622019, Gradient norm: 447.12952958
INFO:root:[  242] Training loss: 0.39013085, Validation loss: 0.39023002, Gradient norm: 477.88617786
INFO:root:[  243] Training loss: 0.38865045, Validation loss: 0.38663855, Gradient norm: 505.72551318
INFO:root:[  244] Training loss: 0.38622902, Validation loss: 0.38010670, Gradient norm: 472.76820156
INFO:root:[  245] Training loss: 0.38250134, Validation loss: 0.39522341, Gradient norm: 424.43919591
INFO:root:[  246] Training loss: 0.38664140, Validation loss: 0.37700856, Gradient norm: 576.18105963
INFO:root:[  247] Training loss: 0.37395803, Validation loss: 0.38101316, Gradient norm: 270.41467090
INFO:root:[  248] Training loss: 0.37365814, Validation loss: 0.36738252, Gradient norm: 382.72322495
INFO:root:[  249] Training loss: 0.37200774, Validation loss: 0.38382119, Gradient norm: 431.52938354
INFO:root:[  250] Training loss: 0.37222124, Validation loss: 0.36480642, Gradient norm: 526.35351219
INFO:root:[  251] Training loss: 0.36882302, Validation loss: 0.37199663, Gradient norm: 474.28097959
INFO:root:[  252] Training loss: 0.36917694, Validation loss: 0.38630125, Gradient norm: 536.03978217
INFO:root:[  253] Training loss: 0.36535592, Validation loss: 0.36088584, Gradient norm: 411.17908059
INFO:root:[  254] Training loss: 0.36310335, Validation loss: 0.36551913, Gradient norm: 459.00810700
INFO:root:[  255] Training loss: 0.36327384, Validation loss: 0.35401646, Gradient norm: 573.84332467
INFO:root:[  256] Training loss: 0.35307998, Validation loss: 0.35081050, Gradient norm: 320.51496130
INFO:root:[  257] Training loss: 0.34953148, Validation loss: 0.35412661, Gradient norm: 278.49474693
INFO:root:[  258] Training loss: 0.35121852, Validation loss: 0.34604651, Gradient norm: 436.33205179
INFO:root:[  259] Training loss: 0.35816144, Validation loss: 0.35378625, Gradient norm: 660.65064742
INFO:root:[  260] Training loss: 0.34280011, Validation loss: 0.34142228, Gradient norm: 243.12784139
INFO:root:[  261] Training loss: 0.34570132, Validation loss: 0.36516626, Gradient norm: 419.51011190
INFO:root:[  262] Training loss: 0.34627210, Validation loss: 0.34703418, Gradient norm: 476.85155049
INFO:root:[  263] Training loss: 0.34007209, Validation loss: 0.33676930, Gradient norm: 428.35526022
INFO:root:[  264] Training loss: 0.34142209, Validation loss: 0.33664272, Gradient norm: 496.27666518
INFO:root:[  265] Training loss: 0.33796832, Validation loss: 0.32984963, Gradient norm: 499.17958166
INFO:root:[  266] Training loss: 0.33342875, Validation loss: 0.33368955, Gradient norm: 417.61990210
INFO:root:[  267] Training loss: 0.32721786, Validation loss: 0.32495915, Gradient norm: 221.09822510
INFO:root:[  268] Training loss: 0.33038146, Validation loss: 0.32390103, Gradient norm: 471.10365474
INFO:root:[  269] Training loss: 0.32676263, Validation loss: 0.32259144, Gradient norm: 428.13340539
INFO:root:[  270] Training loss: 0.32378008, Validation loss: 0.31904216, Gradient norm: 371.12495963
INFO:root:[  271] Training loss: 0.32346115, Validation loss: 0.33567181, Gradient norm: 448.81675731
INFO:root:[  272] Training loss: 0.32019652, Validation loss: 0.32675508, Gradient norm: 396.02766613
INFO:root:[  273] Training loss: 0.32129626, Validation loss: 0.31517199, Gradient norm: 520.69974299
INFO:root:[  274] Training loss: 0.31630630, Validation loss: 0.31724562, Gradient norm: 452.40825198
INFO:root:[  275] Training loss: 0.31470706, Validation loss: 0.32747856, Gradient norm: 447.49430824
INFO:root:[  276] Training loss: 0.31024104, Validation loss: 0.30551493, Gradient norm: 347.16652913
INFO:root:[  277] Training loss: 0.30722479, Validation loss: 0.30362015, Gradient norm: 319.89946935
INFO:root:[  278] Training loss: 0.30782759, Validation loss: 0.30259823, Gradient norm: 398.52960296
INFO:root:[  279] Training loss: 0.30633629, Validation loss: 0.32620327, Gradient norm: 387.81215301
INFO:root:[  280] Training loss: 0.30783733, Validation loss: 0.30316781, Gradient norm: 508.01979550
INFO:root:[  281] Training loss: 0.30147737, Validation loss: 0.29505068, Gradient norm: 369.70268137
INFO:root:[  282] Training loss: 0.30077163, Validation loss: 0.29353602, Gradient norm: 395.33539696
INFO:root:[  283] Training loss: 0.30021822, Validation loss: 0.29524118, Gradient norm: 462.57015459
INFO:root:[  284] Training loss: 0.29652161, Validation loss: 0.30986557, Gradient norm: 402.21184474
INFO:root:[  285] Training loss: 0.29560751, Validation loss: 0.29127232, Gradient norm: 370.26429611
INFO:root:[  286] Training loss: 0.29047387, Validation loss: 0.31714031, Gradient norm: 311.12101866
INFO:root:[  287] Training loss: 0.28939540, Validation loss: 0.28399006, Gradient norm: 359.60426204
INFO:root:[  288] Training loss: 0.28643813, Validation loss: 0.28798103, Gradient norm: 333.86946039
INFO:root:[  289] Training loss: 0.28948257, Validation loss: 0.28497022, Gradient norm: 401.47976919
INFO:root:[  290] Training loss: 0.28280208, Validation loss: 0.28015653, Gradient norm: 297.56546049
INFO:root:[  291] Training loss: 0.28246395, Validation loss: 0.28152501, Gradient norm: 364.95657954
INFO:root:[  292] Training loss: 0.27859786, Validation loss: 0.27643886, Gradient norm: 340.22960340
INFO:root:[  293] Training loss: 0.27908740, Validation loss: 0.28392793, Gradient norm: 391.78612879
INFO:root:[  294] Training loss: 0.27953613, Validation loss: 0.27284477, Gradient norm: 480.86368991
INFO:root:[  295] Training loss: 0.27518357, Validation loss: 0.26998051, Gradient norm: 337.58221489
INFO:root:[  296] Training loss: 0.27369748, Validation loss: 0.27201378, Gradient norm: 394.38895833
INFO:root:[  297] Training loss: 0.27420320, Validation loss: 0.26903434, Gradient norm: 453.00533533
INFO:root:[  298] Training loss: 0.27169866, Validation loss: 0.27349465, Gradient norm: 374.42562911
INFO:root:[  299] Training loss: 0.26922342, Validation loss: 0.27352224, Gradient norm: 408.16476137
INFO:root:[  300] Training loss: 0.26803799, Validation loss: 0.26323964, Gradient norm: 354.03186979
INFO:root:[  301] Training loss: 0.26909620, Validation loss: 0.26670170, Gradient norm: 400.84159506
INFO:root:[  302] Training loss: 0.26885015, Validation loss: 0.26250917, Gradient norm: 405.22648849
INFO:root:[  303] Training loss: 0.26056268, Validation loss: 0.25836085, Gradient norm: 260.53932209
INFO:root:[  304] Training loss: 0.26049444, Validation loss: 0.25597358, Gradient norm: 332.90155083
INFO:root:[  305] Training loss: 0.25755597, Validation loss: 0.25509425, Gradient norm: 273.58079124
INFO:root:[  306] Training loss: 0.25760060, Validation loss: 0.25376420, Gradient norm: 303.75334629
INFO:root:[  307] Training loss: 0.25386741, Validation loss: 0.25224892, Gradient norm: 258.02735065
INFO:root:[  308] Training loss: 0.25265426, Validation loss: 0.29266464, Gradient norm: 289.48200162
INFO:root:[  309] Training loss: 0.26303780, Validation loss: 0.26073618, Gradient norm: 588.04527590
INFO:root:[  310] Training loss: 0.25072046, Validation loss: 0.24789310, Gradient norm: 276.90792534
INFO:root:[  311] Training loss: 0.24835635, Validation loss: 0.26434562, Gradient norm: 221.75540725
INFO:root:[  312] Training loss: 0.25576626, Validation loss: 0.26877999, Gradient norm: 487.76798355
INFO:root:[  313] Training loss: 0.25059857, Validation loss: 0.24821409, Gradient norm: 327.78148690
INFO:root:[  314] Training loss: 0.25014066, Validation loss: 0.24363128, Gradient norm: 409.37418118
INFO:root:[  315] Training loss: 0.25041066, Validation loss: 0.24236194, Gradient norm: 438.68607376
INFO:root:[  316] Training loss: 0.24415779, Validation loss: 0.24252975, Gradient norm: 294.33987066
INFO:root:[  317] Training loss: 0.24323299, Validation loss: 0.25882835, Gradient norm: 264.32980422
INFO:root:[  318] Training loss: 0.24287285, Validation loss: 0.24034063, Gradient norm: 310.31980815
INFO:root:[  319] Training loss: 0.24077354, Validation loss: 0.23874074, Gradient norm: 294.31129426
INFO:root:[  320] Training loss: 0.23937427, Validation loss: 0.23686353, Gradient norm: 299.33149732
INFO:root:[  321] Training loss: 0.23740008, Validation loss: 0.26086632, Gradient norm: 238.75068842
INFO:root:[  322] Training loss: 0.24773909, Validation loss: 0.23816553, Gradient norm: 493.70558204
INFO:root:[  323] Training loss: 0.23745691, Validation loss: 0.23360546, Gradient norm: 279.18835187
INFO:root:[  324] Training loss: 0.23293499, Validation loss: 0.24658742, Gradient norm: 161.59657305
INFO:root:[  325] Training loss: 0.23653159, Validation loss: 0.23312819, Gradient norm: 351.50472229
INFO:root:[  326] Training loss: 0.23347219, Validation loss: 0.26965422, Gradient norm: 284.49884904
INFO:root:[  327] Training loss: 0.23714186, Validation loss: 0.22929565, Gradient norm: 360.93103538
INFO:root:[  328] Training loss: 0.23244155, Validation loss: 0.22783275, Gradient norm: 320.42550466
INFO:root:[  329] Training loss: 0.22814828, Validation loss: 0.22836163, Gradient norm: 120.25206054
INFO:root:[  330] Training loss: 0.23100235, Validation loss: 0.23684925, Gradient norm: 290.97886254
INFO:root:[  331] Training loss: 0.22908930, Validation loss: 0.22874560, Gradient norm: 286.49219322
INFO:root:[  332] Training loss: 0.22881063, Validation loss: 0.22678146, Gradient norm: 266.78103777
INFO:root:[  333] Training loss: 0.22536759, Validation loss: 0.22461820, Gradient norm: 178.10601444
INFO:root:[  334] Training loss: 0.23287825, Validation loss: 0.23313934, Gradient norm: 450.78012571
INFO:root:[  335] Training loss: 0.22858975, Validation loss: 0.22350738, Gradient norm: 309.18951062
INFO:root:[  336] Training loss: 0.22311743, Validation loss: 0.22219721, Gradient norm: 162.47780100
INFO:root:[  337] Training loss: 0.22330577, Validation loss: 0.22318957, Gradient norm: 204.12371459
INFO:root:[  338] Training loss: 0.22899685, Validation loss: 0.22051384, Gradient norm: 410.14642683
INFO:root:[  339] Training loss: 0.22041668, Validation loss: 0.22099413, Gradient norm: 165.33918386
INFO:root:[  340] Training loss: 0.22437062, Validation loss: 0.22111757, Gradient norm: 320.11726433
INFO:root:[  341] Training loss: 0.22165824, Validation loss: 0.22283123, Gradient norm: 251.67729169
INFO:root:[  342] Training loss: 0.22177663, Validation loss: 0.21818556, Gradient norm: 309.26140428
INFO:root:[  343] Training loss: 0.22124844, Validation loss: 0.22272872, Gradient norm: 295.68451466
INFO:root:[  344] Training loss: 0.21946389, Validation loss: 0.21587681, Gradient norm: 252.86238035
INFO:root:[  345] Training loss: 0.22136912, Validation loss: 0.21627245, Gradient norm: 322.91045790
INFO:root:[  346] Training loss: 0.21790568, Validation loss: 0.24149582, Gradient norm: 258.71160172
INFO:root:[  347] Training loss: 0.22167717, Validation loss: 0.23747060, Gradient norm: 347.96250575
INFO:root:[  348] Training loss: 0.21875355, Validation loss: 0.22599875, Gradient norm: 273.75679441
INFO:root:[  349] Training loss: 0.21558320, Validation loss: 0.21412319, Gradient norm: 230.29201078
INFO:root:[  350] Training loss: 0.21678205, Validation loss: 0.21992769, Gradient norm: 213.17490701
INFO:root:[  351] Training loss: 0.21527233, Validation loss: 0.21224587, Gradient norm: 235.07331223
INFO:root:[  352] Training loss: 0.22396109, Validation loss: 0.21296859, Gradient norm: 427.92183407
INFO:root:[  353] Training loss: 0.21253718, Validation loss: 0.21359133, Gradient norm: 163.37255526
INFO:root:[  354] Training loss: 0.21314837, Validation loss: 0.21169836, Gradient norm: 164.98627017
INFO:root:[  355] Training loss: 0.21137277, Validation loss: 0.21290240, Gradient norm: 160.99395364
INFO:root:[  356] Training loss: 0.21329964, Validation loss: 0.21068177, Gradient norm: 270.71479568
INFO:root:[  357] Training loss: 0.21509020, Validation loss: 0.21287254, Gradient norm: 306.96313103
INFO:root:[  358] Training loss: 0.21008121, Validation loss: 0.21159666, Gradient norm: 189.96281624
INFO:root:[  359] Training loss: 0.21745763, Validation loss: 0.21135874, Gradient norm: 389.07324620
INFO:root:[  360] Training loss: 0.21520026, Validation loss: 0.22406260, Gradient norm: 340.54980076
INFO:root:[  361] Training loss: 0.21130866, Validation loss: 0.21022437, Gradient norm: 258.88044354
INFO:root:[  362] Training loss: 0.21054959, Validation loss: 0.20773366, Gradient norm: 266.23096140
INFO:root:[  363] Training loss: 0.21246036, Validation loss: 0.21274063, Gradient norm: 275.67340382
INFO:root:[  364] Training loss: 0.21071950, Validation loss: 0.21533591, Gradient norm: 260.56593866
INFO:root:[  365] Training loss: 0.21236138, Validation loss: 0.21604046, Gradient norm: 342.47491122
INFO:root:[  366] Training loss: 0.20982010, Validation loss: 0.20922084, Gradient norm: 287.67802361
INFO:root:[  367] Training loss: 0.21349846, Validation loss: 0.23465873, Gradient norm: 378.80802761
INFO:root:[  368] Training loss: 0.21311116, Validation loss: 0.20590947, Gradient norm: 323.15385074
INFO:root:[  369] Training loss: 0.20680247, Validation loss: 0.20538034, Gradient norm: 220.66694409
INFO:root:[  370] Training loss: 0.20789596, Validation loss: 0.20335972, Gradient norm: 242.88838667
INFO:root:[  371] Training loss: 0.20695660, Validation loss: 0.20910009, Gradient norm: 335.98917275
INFO:root:[  372] Training loss: 0.20608814, Validation loss: 0.22335527, Gradient norm: 277.87128910
INFO:root:[  373] Training loss: 0.21122640, Validation loss: 0.21081370, Gradient norm: 415.35274533
INFO:root:[  374] Training loss: 0.20538586, Validation loss: 0.20341593, Gradient norm: 280.15112132
INFO:root:[  375] Training loss: 0.20781131, Validation loss: 0.20477676, Gradient norm: 334.82187655
INFO:root:[  376] Training loss: 0.20704753, Validation loss: 0.20675342, Gradient norm: 350.14034425
INFO:root:[  377] Training loss: 0.20808915, Validation loss: 0.20348121, Gradient norm: 382.02477858
INFO:root:[  378] Training loss: 0.20600082, Validation loss: 0.20282698, Gradient norm: 286.85639333
INFO:root:[  379] Training loss: 0.20542697, Validation loss: 0.20556656, Gradient norm: 326.02828892
INFO:root:[  380] Training loss: 0.21029505, Validation loss: 0.20513319, Gradient norm: 432.47095711
INFO:root:[  381] Training loss: 0.20422830, Validation loss: 0.20265679, Gradient norm: 269.80155711
INFO:root:[  382] Training loss: 0.20471572, Validation loss: 0.20115580, Gradient norm: 381.43675317
INFO:root:[  383] Training loss: 0.20297080, Validation loss: 0.20046519, Gradient norm: 298.89299214
INFO:root:[  384] Training loss: 0.20371604, Validation loss: 0.20019454, Gradient norm: 344.97110912
INFO:root:[  385] Training loss: 0.20374086, Validation loss: 0.21614680, Gradient norm: 349.83991364
INFO:root:[  386] Training loss: 0.20705223, Validation loss: 0.20381004, Gradient norm: 470.15054139
INFO:root:[  387] Training loss: 0.20073460, Validation loss: 0.19866854, Gradient norm: 257.94376825
INFO:root:[  388] Training loss: 0.20276369, Validation loss: 0.20081984, Gradient norm: 386.03297476
INFO:root:[  389] Training loss: 0.20466712, Validation loss: 0.19722381, Gradient norm: 352.47512703
INFO:root:[  390] Training loss: 0.20194060, Validation loss: 0.19661015, Gradient norm: 385.83114101
INFO:root:[  391] Training loss: 0.20344092, Validation loss: 0.20525621, Gradient norm: 399.31429254
INFO:root:[  392] Training loss: 0.20227472, Validation loss: 0.20145697, Gradient norm: 392.68717666
INFO:root:[  393] Training loss: 0.19975868, Validation loss: 0.20156521, Gradient norm: 303.74843670
INFO:root:[  394] Training loss: 0.20478561, Validation loss: 0.20648445, Gradient norm: 464.20180942
INFO:root:[  395] Training loss: 0.20521654, Validation loss: 0.20956744, Gradient norm: 412.56711425
INFO:root:[  396] Training loss: 0.20141546, Validation loss: 0.20730496, Gradient norm: 373.98646843
INFO:root:[  397] Training loss: 0.20211175, Validation loss: 0.23169974, Gradient norm: 422.47296660
INFO:root:[  398] Training loss: 0.20503512, Validation loss: 0.21102815, Gradient norm: 421.20843282
INFO:root:[  399] Training loss: 0.20055925, Validation loss: 0.20194348, Gradient norm: 381.89805950
INFO:root:EP 399: Early stopping
INFO:root:Training the model took 6301.206s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 4.78294
INFO:root:EnergyScoreTrain: 3.28736
INFO:root:CoverageTrain: 0.88165
INFO:root:IntervalWidthTrain: 0.08429
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 4.63261
INFO:root:EnergyScoreValidation: 3.18839
INFO:root:CoverageValidation: 0.88042
INFO:root:IntervalWidthValidation: 0.08436
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 3.74049
INFO:root:EnergyScoreTest: 2.57017
INFO:root:CoverageTest: 0.88134
INFO:root:IntervalWidthTest: 0.08275
