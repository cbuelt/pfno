INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 75497472
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.07727763, Validation loss: 0.22763180, Gradient norm: 12.06598370
INFO:root:[    2] Training loss: 0.17289003, Validation loss: 0.15293612, Gradient norm: 4.14841245
INFO:root:[    3] Training loss: 0.14599712, Validation loss: 0.13868114, Gradient norm: 1.44767102
INFO:root:[    4] Training loss: 0.13446560, Validation loss: 0.13006646, Gradient norm: 1.46565225
INFO:root:[    5] Training loss: 0.12806214, Validation loss: 0.12493977, Gradient norm: 1.80949598
INFO:root:[    6] Training loss: 0.12525330, Validation loss: 0.12909602, Gradient norm: 2.23263940
INFO:root:[    7] Training loss: 0.12394844, Validation loss: 0.12580803, Gradient norm: 3.47306625
INFO:root:[    8] Training loss: 0.11650761, Validation loss: 0.11710941, Gradient norm: 1.87507015
INFO:root:[    9] Training loss: 0.11666443, Validation loss: 0.12914342, Gradient norm: 2.22966881
INFO:root:[   10] Training loss: 0.11935736, Validation loss: 0.12433595, Gradient norm: 3.96686032
INFO:root:[   11] Training loss: 0.11210085, Validation loss: 0.10833765, Gradient norm: 2.78910206
INFO:root:[   12] Training loss: 0.10900375, Validation loss: 0.11700157, Gradient norm: 2.55625479
INFO:root:[   13] Training loss: 0.11147911, Validation loss: 0.11245042, Gradient norm: 3.47472753
INFO:root:[   14] Training loss: 0.10373800, Validation loss: 0.10382117, Gradient norm: 1.87869253
INFO:root:[   15] Training loss: 0.10290318, Validation loss: 0.10152194, Gradient norm: 1.46104257
INFO:root:[   16] Training loss: 0.10239077, Validation loss: 0.09851654, Gradient norm: 2.52944521
INFO:root:[   17] Training loss: 0.09827194, Validation loss: 0.09796312, Gradient norm: 1.57690455
INFO:root:[   18] Training loss: 0.09779560, Validation loss: 0.09671079, Gradient norm: 1.64841124
INFO:root:[   19] Training loss: 0.09651394, Validation loss: 0.09413395, Gradient norm: 1.91951970
INFO:root:[   20] Training loss: 0.09665027, Validation loss: 0.09666807, Gradient norm: 2.85587393
INFO:root:[   21] Training loss: 0.09642965, Validation loss: 0.09376062, Gradient norm: 3.04375437
INFO:root:[   22] Training loss: 0.09171673, Validation loss: 0.09176964, Gradient norm: 1.93343955
INFO:root:[   23] Training loss: 0.09152302, Validation loss: 0.10006466, Gradient norm: 2.00001839
INFO:root:[   24] Training loss: 0.09146632, Validation loss: 0.09012546, Gradient norm: 2.50563912
INFO:root:[   25] Training loss: 0.09036596, Validation loss: 0.08811647, Gradient norm: 1.85044348
INFO:root:[   26] Training loss: 0.09128213, Validation loss: 0.08814187, Gradient norm: 2.85090127
INFO:root:[   27] Training loss: 0.08911806, Validation loss: 0.08898213, Gradient norm: 1.98321713
INFO:root:[   28] Training loss: 0.08734206, Validation loss: 0.09199286, Gradient norm: 1.79695531
INFO:root:[   29] Training loss: 0.08837439, Validation loss: 0.08655585, Gradient norm: 2.64319631
INFO:root:[   30] Training loss: 0.08605516, Validation loss: 0.09056102, Gradient norm: 2.21019375
INFO:root:[   31] Training loss: 0.08890155, Validation loss: 0.08644519, Gradient norm: 3.58413064
INFO:root:[   32] Training loss: 0.08556398, Validation loss: 0.08335265, Gradient norm: 2.60290172
INFO:root:[   33] Training loss: 0.08347152, Validation loss: 0.08434481, Gradient norm: 2.65937300
INFO:root:[   34] Training loss: 0.08141383, Validation loss: 0.07944251, Gradient norm: 1.99514723
INFO:root:[   35] Training loss: 0.08142276, Validation loss: 0.08394313, Gradient norm: 1.91073512
INFO:root:[   36] Training loss: 0.08350860, Validation loss: 0.07851190, Gradient norm: 2.72582939
INFO:root:[   37] Training loss: 0.08195702, Validation loss: 0.07865254, Gradient norm: 2.57435694
INFO:root:[   38] Training loss: 0.07888706, Validation loss: 0.07866123, Gradient norm: 1.57924876
INFO:root:[   39] Training loss: 0.08033153, Validation loss: 0.07878995, Gradient norm: 2.72385135
INFO:root:[   40] Training loss: 0.08115827, Validation loss: 0.07787925, Gradient norm: 2.92049367
INFO:root:[   41] Training loss: 0.07790980, Validation loss: 0.07833187, Gradient norm: 2.37311763
INFO:root:[   42] Training loss: 0.07938039, Validation loss: 0.07868355, Gradient norm: 2.54795848
INFO:root:[   43] Training loss: 0.07919686, Validation loss: 0.07430755, Gradient norm: 2.92771218
INFO:root:[   44] Training loss: 0.07804359, Validation loss: 0.08364335, Gradient norm: 2.60715217
INFO:root:[   45] Training loss: 0.07546833, Validation loss: 0.07468669, Gradient norm: 1.77721400
INFO:root:[   46] Training loss: 0.07436049, Validation loss: 0.07499011, Gradient norm: 1.27365876
INFO:root:[   47] Training loss: 0.07453189, Validation loss: 0.07715473, Gradient norm: 1.91955437
INFO:root:[   48] Training loss: 0.07257531, Validation loss: 0.07220848, Gradient norm: 1.55950562
INFO:root:[   49] Training loss: 0.07220000, Validation loss: 0.08708883, Gradient norm: 1.40715286
INFO:root:[   50] Training loss: 0.07546193, Validation loss: 0.07340409, Gradient norm: 3.07182690
INFO:root:[   51] Training loss: 0.07122428, Validation loss: 0.07352831, Gradient norm: 1.55595330
INFO:root:[   52] Training loss: 0.07175617, Validation loss: 0.07107533, Gradient norm: 1.92949327
INFO:root:[   53] Training loss: 0.07148940, Validation loss: 0.07802129, Gradient norm: 2.18055843
INFO:root:[   54] Training loss: 0.07181106, Validation loss: 0.07008389, Gradient norm: 2.73829974
INFO:root:[   55] Training loss: 0.07233701, Validation loss: 0.07646478, Gradient norm: 3.04959033
INFO:root:[   56] Training loss: 0.07138514, Validation loss: 0.06995469, Gradient norm: 2.72074899
INFO:root:[   57] Training loss: 0.06918313, Validation loss: 0.06904944, Gradient norm: 2.02164136
INFO:root:[   58] Training loss: 0.06900066, Validation loss: 0.06627378, Gradient norm: 1.95796858
INFO:root:[   59] Training loss: 0.06906045, Validation loss: 0.06749947, Gradient norm: 2.35618698
INFO:root:[   60] Training loss: 0.06890977, Validation loss: 0.06971403, Gradient norm: 2.14821997
INFO:root:[   61] Training loss: 0.06907011, Validation loss: 0.06548468, Gradient norm: 2.65144415
INFO:root:[   62] Training loss: 0.06784408, Validation loss: 0.06764061, Gradient norm: 2.19794943
INFO:root:[   63] Training loss: 0.06571216, Validation loss: 0.06721584, Gradient norm: 1.28444622
INFO:root:[   64] Training loss: 0.06795074, Validation loss: 0.06595161, Gradient norm: 2.33667294
INFO:root:[   65] Training loss: 0.06763572, Validation loss: 0.06969255, Gradient norm: 2.47926073
INFO:root:[   66] Training loss: 0.06688410, Validation loss: 0.06407288, Gradient norm: 2.52773257
INFO:root:[   67] Training loss: 0.06585322, Validation loss: 0.06513117, Gradient norm: 2.01186145
INFO:root:[   68] Training loss: 0.06623400, Validation loss: 0.07207435, Gradient norm: 2.45537106
INFO:root:[   69] Training loss: 0.06738251, Validation loss: 0.06877114, Gradient norm: 2.71901486
INFO:root:[   70] Training loss: 0.06419787, Validation loss: 0.06307803, Gradient norm: 1.67133545
INFO:root:[   71] Training loss: 0.06408220, Validation loss: 0.06483437, Gradient norm: 1.64601495
INFO:root:[   72] Training loss: 0.06372115, Validation loss: 0.06416805, Gradient norm: 1.67214422
INFO:root:[   73] Training loss: 0.06344337, Validation loss: 0.06376693, Gradient norm: 1.83581250
INFO:root:[   74] Training loss: 0.06434982, Validation loss: 0.06596983, Gradient norm: 2.22011199
INFO:root:[   75] Training loss: 0.06658956, Validation loss: 0.06161021, Gradient norm: 3.24583185
INFO:root:[   76] Training loss: 0.06557356, Validation loss: 0.06326655, Gradient norm: 2.74483134
INFO:root:[   77] Training loss: 0.06637190, Validation loss: 0.06466554, Gradient norm: 3.17095451
INFO:root:[   78] Training loss: 0.06283332, Validation loss: 0.06113052, Gradient norm: 1.84574767
INFO:root:[   79] Training loss: 0.06098274, Validation loss: 0.06684365, Gradient norm: 0.99901744
INFO:root:[   80] Training loss: 0.06300805, Validation loss: 0.06126262, Gradient norm: 1.98218631
INFO:root:[   81] Training loss: 0.06198220, Validation loss: 0.06328368, Gradient norm: 1.77420254
INFO:root:[   82] Training loss: 0.06306557, Validation loss: 0.06668402, Gradient norm: 2.52555169
INFO:root:[   83] Training loss: 0.06604573, Validation loss: 0.06173740, Gradient norm: 3.47396598
INFO:root:[   84] Training loss: 0.06295113, Validation loss: 0.06335333, Gradient norm: 2.61991260
INFO:root:[   85] Training loss: 0.06201224, Validation loss: 0.06155033, Gradient norm: 2.35795105
INFO:root:[   86] Training loss: 0.06091270, Validation loss: 0.07167583, Gradient norm: 1.88664329
INFO:root:[   87] Training loss: 0.06230078, Validation loss: 0.06399218, Gradient norm: 2.49255919
INFO:root:[   88] Training loss: 0.06173494, Validation loss: 0.05895788, Gradient norm: 2.34562172
INFO:root:[   89] Training loss: 0.06054537, Validation loss: 0.05964986, Gradient norm: 1.98457153
INFO:root:[   90] Training loss: 0.05991988, Validation loss: 0.05969040, Gradient norm: 1.46940971
INFO:root:[   91] Training loss: 0.05862983, Validation loss: 0.05846136, Gradient norm: 0.96991443
INFO:root:[   92] Training loss: 0.05993120, Validation loss: 0.05901080, Gradient norm: 1.48084127
INFO:root:[   93] Training loss: 0.05839484, Validation loss: 0.05962471, Gradient norm: 1.22169187
INFO:root:[   94] Training loss: 0.05933975, Validation loss: 0.06080365, Gradient norm: 1.61112377
INFO:root:[   95] Training loss: 0.06037378, Validation loss: 0.07572036, Gradient norm: 2.38029844
INFO:root:[   96] Training loss: 0.06430048, Validation loss: 0.05734207, Gradient norm: 3.29747859
INFO:root:[   97] Training loss: 0.05915377, Validation loss: 0.06469447, Gradient norm: 1.80402073
INFO:root:[   98] Training loss: 0.06148313, Validation loss: 0.05982132, Gradient norm: 2.77333429
INFO:root:[   99] Training loss: 0.05956080, Validation loss: 0.06190824, Gradient norm: 2.26406603
INFO:root:[  100] Training loss: 0.06089714, Validation loss: 0.05711209, Gradient norm: 2.78840037
INFO:root:[  101] Training loss: 0.05823140, Validation loss: 0.05824445, Gradient norm: 1.77014701
INFO:root:[  102] Training loss: 0.05840317, Validation loss: 0.05854590, Gradient norm: 1.78493270
INFO:root:[  103] Training loss: 0.05934467, Validation loss: 0.06126419, Gradient norm: 2.36121746
INFO:root:[  104] Training loss: 0.05880814, Validation loss: 0.05599434, Gradient norm: 2.28154088
INFO:root:[  105] Training loss: 0.05788833, Validation loss: 0.05577939, Gradient norm: 1.87963366
INFO:root:[  106] Training loss: 0.05751305, Validation loss: 0.05854507, Gradient norm: 1.66380176
INFO:root:[  107] Training loss: 0.05840515, Validation loss: 0.05670435, Gradient norm: 2.07946864
INFO:root:[  108] Training loss: 0.05728210, Validation loss: 0.06025663, Gradient norm: 1.80024856
INFO:root:[  109] Training loss: 0.05643475, Validation loss: 0.06162365, Gradient norm: 1.31651410
INFO:root:[  110] Training loss: 0.05853035, Validation loss: 0.05759967, Gradient norm: 2.42261385
INFO:root:[  111] Training loss: 0.05633357, Validation loss: 0.06035342, Gradient norm: 1.74979765
INFO:root:[  112] Training loss: 0.05615817, Validation loss: 0.05642859, Gradient norm: 1.52957201
INFO:root:[  113] Training loss: 0.05651166, Validation loss: 0.05943768, Gradient norm: 1.96205527
INFO:root:[  114] Training loss: 0.05855291, Validation loss: 0.05996772, Gradient norm: 2.77835598
INFO:root:[  115] Training loss: 0.05719428, Validation loss: 0.05539890, Gradient norm: 2.17581571
INFO:root:[  116] Training loss: 0.05579418, Validation loss: 0.05536695, Gradient norm: 1.43739628
INFO:root:[  117] Training loss: 0.05609560, Validation loss: 0.05506290, Gradient norm: 1.86191099
INFO:root:[  118] Training loss: 0.05673184, Validation loss: 0.05621616, Gradient norm: 2.30830413
INFO:root:[  119] Training loss: 0.05679618, Validation loss: 0.05382930, Gradient norm: 2.12460791
INFO:root:[  120] Training loss: 0.05665913, Validation loss: 0.05548301, Gradient norm: 2.35272005
INFO:root:[  121] Training loss: 0.05582262, Validation loss: 0.05368092, Gradient norm: 2.03701231
INFO:root:[  122] Training loss: 0.05678062, Validation loss: 0.05756599, Gradient norm: 2.44581874
INFO:root:[  123] Training loss: 0.05537522, Validation loss: 0.05614722, Gradient norm: 1.83316394
INFO:root:[  124] Training loss: 0.05557584, Validation loss: 0.05696561, Gradient norm: 1.92767506
INFO:root:[  125] Training loss: 0.05676980, Validation loss: 0.05370445, Gradient norm: 2.53816970
INFO:root:[  126] Training loss: 0.05629188, Validation loss: 0.05668771, Gradient norm: 2.43454858
INFO:root:[  127] Training loss: 0.05474297, Validation loss: 0.05640992, Gradient norm: 1.81837397
INFO:root:[  128] Training loss: 0.05419626, Validation loss: 0.05363712, Gradient norm: 1.58172849
INFO:root:[  129] Training loss: 0.05433459, Validation loss: 0.05759769, Gradient norm: 1.65543224
INFO:root:[  130] Training loss: 0.05708860, Validation loss: 0.05216243, Gradient norm: 2.78959011
INFO:root:[  131] Training loss: 0.05490710, Validation loss: 0.05407248, Gradient norm: 2.12600189
INFO:root:[  132] Training loss: 0.05417840, Validation loss: 0.05732969, Gradient norm: 1.48068234
INFO:root:[  133] Training loss: 0.05490943, Validation loss: 0.05467347, Gradient norm: 2.06679308
INFO:root:[  134] Training loss: 0.05483839, Validation loss: 0.05503069, Gradient norm: 2.25851428
INFO:root:[  135] Training loss: 0.05383035, Validation loss: 0.05287115, Gradient norm: 1.92101459
INFO:root:[  136] Training loss: 0.05251554, Validation loss: 0.05716920, Gradient norm: 1.13760629
INFO:root:[  137] Training loss: 0.05447701, Validation loss: 0.05843295, Gradient norm: 2.30439732
INFO:root:[  138] Training loss: 0.05570849, Validation loss: 0.05214623, Gradient norm: 2.90104937
INFO:root:[  139] Training loss: 0.05328254, Validation loss: 0.05348629, Gradient norm: 1.86436861
INFO:root:[  140] Training loss: 0.05295124, Validation loss: 0.05384555, Gradient norm: 1.53956663
INFO:root:[  141] Training loss: 0.05342771, Validation loss: 0.05196938, Gradient norm: 1.99921677
INFO:root:[  142] Training loss: 0.05204636, Validation loss: 0.05198674, Gradient norm: 1.29468041
INFO:root:[  143] Training loss: 0.05354292, Validation loss: 0.05165013, Gradient norm: 2.00211742
INFO:root:[  144] Training loss: 0.05246816, Validation loss: 0.05157674, Gradient norm: 1.63710320
INFO:root:[  145] Training loss: 0.05211745, Validation loss: 0.05493757, Gradient norm: 1.72593561
INFO:root:[  146] Training loss: 0.05210962, Validation loss: 0.05430567, Gradient norm: 1.86165891
INFO:root:[  147] Training loss: 0.05214558, Validation loss: 0.05061213, Gradient norm: 1.80836086
INFO:root:[  148] Training loss: 0.05179537, Validation loss: 0.05190819, Gradient norm: 1.75869269
INFO:root:[  149] Training loss: 0.05133218, Validation loss: 0.05198386, Gradient norm: 1.27466365
INFO:root:[  150] Training loss: 0.05146149, Validation loss: 0.05710846, Gradient norm: 1.58490190
INFO:root:[  151] Training loss: 0.05315161, Validation loss: 0.05407162, Gradient norm: 2.43719453
INFO:root:[  152] Training loss: 0.05299236, Validation loss: 0.05262641, Gradient norm: 2.25460711
INFO:root:[  153] Training loss: 0.05248016, Validation loss: 0.05128554, Gradient norm: 2.23396301
INFO:root:[  154] Training loss: 0.05283185, Validation loss: 0.05108276, Gradient norm: 2.43245572
INFO:root:[  155] Training loss: 0.05161428, Validation loss: 0.05049791, Gradient norm: 1.85735262
INFO:root:[  156] Training loss: 0.05088969, Validation loss: 0.04990466, Gradient norm: 1.38784480
INFO:root:[  157] Training loss: 0.05090441, Validation loss: 0.05189502, Gradient norm: 1.66288621
INFO:root:[  158] Training loss: 0.05146648, Validation loss: 0.05211801, Gradient norm: 2.08474947
INFO:root:[  159] Training loss: 0.05052853, Validation loss: 0.05219665, Gradient norm: 1.48461681
INFO:root:[  160] Training loss: 0.05184451, Validation loss: 0.05305254, Gradient norm: 2.39255803
INFO:root:[  161] Training loss: 0.05248045, Validation loss: 0.04967670, Gradient norm: 2.53229431
INFO:root:[  162] Training loss: 0.05092496, Validation loss: 0.04923875, Gradient norm: 1.97507200
INFO:root:[  163] Training loss: 0.05187929, Validation loss: 0.04978041, Gradient norm: 2.38076578
INFO:root:[  164] Training loss: 0.05095447, Validation loss: 0.04886340, Gradient norm: 2.10764944
INFO:root:[  165] Training loss: 0.05115955, Validation loss: 0.05243029, Gradient norm: 2.29792837
INFO:root:[  166] Training loss: 0.05029756, Validation loss: 0.05346027, Gradient norm: 1.74084252
INFO:root:[  167] Training loss: 0.05008975, Validation loss: 0.05197706, Gradient norm: 1.88196238
INFO:root:[  168] Training loss: 0.05114673, Validation loss: 0.05252192, Gradient norm: 2.33776216
INFO:root:[  169] Training loss: 0.05015936, Validation loss: 0.05019144, Gradient norm: 2.01544237
INFO:root:[  170] Training loss: 0.05027294, Validation loss: 0.05410509, Gradient norm: 2.25702852
INFO:root:[  171] Training loss: 0.05194966, Validation loss: 0.05058205, Gradient norm: 2.74622216
INFO:root:[  172] Training loss: 0.05067891, Validation loss: 0.04882913, Gradient norm: 2.31347575
INFO:root:[  173] Training loss: 0.04963502, Validation loss: 0.04813474, Gradient norm: 1.95843074
INFO:root:[  174] Training loss: 0.05036581, Validation loss: 0.04902925, Gradient norm: 2.27242951
INFO:root:[  175] Training loss: 0.04937083, Validation loss: 0.04800649, Gradient norm: 1.85896872
INFO:root:[  176] Training loss: 0.04818019, Validation loss: 0.04828961, Gradient norm: 1.06468958
INFO:root:[  177] Training loss: 0.04883779, Validation loss: 0.04870735, Gradient norm: 1.66738688
INFO:root:[  178] Training loss: 0.04979806, Validation loss: 0.04785000, Gradient norm: 2.15040024
INFO:root:[  179] Training loss: 0.04864904, Validation loss: 0.04761320, Gradient norm: 1.60144432
INFO:root:[  180] Training loss: 0.04940726, Validation loss: 0.05373096, Gradient norm: 1.98343362
INFO:root:[  181] Training loss: 0.04847027, Validation loss: 0.04783039, Gradient norm: 1.53335423
INFO:root:[  182] Training loss: 0.04767444, Validation loss: 0.04741769, Gradient norm: 1.07266026
INFO:root:[  183] Training loss: 0.04833393, Validation loss: 0.04831743, Gradient norm: 1.48469149
INFO:root:[  184] Training loss: 0.04819427, Validation loss: 0.04824923, Gradient norm: 1.42844790
INFO:root:[  185] Training loss: 0.04813889, Validation loss: 0.04995025, Gradient norm: 1.67126132
INFO:root:[  186] Training loss: 0.04844354, Validation loss: 0.04656331, Gradient norm: 1.88220204
INFO:root:[  187] Training loss: 0.04864561, Validation loss: 0.04750771, Gradient norm: 1.90680028
INFO:root:[  188] Training loss: 0.04909301, Validation loss: 0.04735625, Gradient norm: 2.08454071
INFO:root:[  189] Training loss: 0.04756435, Validation loss: 0.04632252, Gradient norm: 1.57161019
INFO:root:[  190] Training loss: 0.04877121, Validation loss: 0.04746825, Gradient norm: 2.00933122
INFO:root:[  191] Training loss: 0.04800437, Validation loss: 0.04654624, Gradient norm: 1.83830009
INFO:root:[  192] Training loss: 0.04735382, Validation loss: 0.04823431, Gradient norm: 1.66836572
INFO:root:[  193] Training loss: 0.04789717, Validation loss: 0.04800296, Gradient norm: 1.98304168
INFO:root:[  194] Training loss: 0.04822100, Validation loss: 0.04633688, Gradient norm: 2.04884182
INFO:root:[  195] Training loss: 0.04822408, Validation loss: 0.05081929, Gradient norm: 2.09350553
INFO:root:[  196] Training loss: 0.04744735, Validation loss: 0.04760449, Gradient norm: 1.72252946
INFO:root:[  197] Training loss: 0.04659773, Validation loss: 0.04613967, Gradient norm: 1.33751449
INFO:root:[  198] Training loss: 0.04729291, Validation loss: 0.04588701, Gradient norm: 1.76845237
INFO:root:[  199] Training loss: 0.04712870, Validation loss: 0.04596299, Gradient norm: 1.96164947
INFO:root:[  200] Training loss: 0.04854295, Validation loss: 0.05111814, Gradient norm: 2.50288796
INFO:root:[  201] Training loss: 0.04745116, Validation loss: 0.05078892, Gradient norm: 1.91171316
INFO:root:[  202] Training loss: 0.04768525, Validation loss: 0.04641606, Gradient norm: 2.11547093
INFO:root:[  203] Training loss: 0.04691547, Validation loss: 0.04548912, Gradient norm: 1.76495419
INFO:root:[  204] Training loss: 0.04584881, Validation loss: 0.04764146, Gradient norm: 0.89180015
INFO:root:[  205] Training loss: 0.04614953, Validation loss: 0.04512322, Gradient norm: 1.28054566
INFO:root:[  206] Training loss: 0.04602305, Validation loss: 0.04837265, Gradient norm: 1.16253113
INFO:root:[  207] Training loss: 0.04606816, Validation loss: 0.05267786, Gradient norm: 1.59202148
INFO:root:[  208] Training loss: 0.04812952, Validation loss: 0.04573591, Gradient norm: 2.62062605
INFO:root:[  209] Training loss: 0.04657066, Validation loss: 0.04471947, Gradient norm: 1.93361109
INFO:root:[  210] Training loss: 0.04696534, Validation loss: 0.04594198, Gradient norm: 2.17453937
INFO:root:[  211] Training loss: 0.04605097, Validation loss: 0.04915962, Gradient norm: 1.59385800
INFO:root:[  212] Training loss: 0.04624721, Validation loss: 0.04792475, Gradient norm: 1.63483304
INFO:root:[  213] Training loss: 0.04669355, Validation loss: 0.04514436, Gradient norm: 2.13967410
INFO:root:[  214] Training loss: 0.04588661, Validation loss: 0.05151429, Gradient norm: 1.67601362
INFO:root:[  215] Training loss: 0.04694984, Validation loss: 0.04860530, Gradient norm: 2.24053365
INFO:root:[  216] Training loss: 0.04572911, Validation loss: 0.04429974, Gradient norm: 1.76650559
INFO:root:[  217] Training loss: 0.04639074, Validation loss: 0.04635259, Gradient norm: 2.10019627
INFO:root:[  218] Training loss: 0.04612478, Validation loss: 0.04982954, Gradient norm: 1.93835454
INFO:root:[  219] Training loss: 0.04668668, Validation loss: 0.04419162, Gradient norm: 2.24744926
INFO:root:[  220] Training loss: 0.04525515, Validation loss: 0.04404771, Gradient norm: 1.48445992
INFO:root:[  221] Training loss: 0.04575274, Validation loss: 0.04429642, Gradient norm: 1.77942282
INFO:root:[  222] Training loss: 0.04607302, Validation loss: 0.04406279, Gradient norm: 2.03563806
INFO:root:[  223] Training loss: 0.04510509, Validation loss: 0.04633333, Gradient norm: 1.48620911
INFO:root:[  224] Training loss: 0.04535968, Validation loss: 0.04736073, Gradient norm: 1.59228400
INFO:root:[  225] Training loss: 0.04510261, Validation loss: 0.04473266, Gradient norm: 1.56355569
INFO:root:[  226] Training loss: 0.04443876, Validation loss: 0.04407362, Gradient norm: 1.18168779
INFO:root:[  227] Training loss: 0.04553762, Validation loss: 0.04478089, Gradient norm: 1.67921159
INFO:root:[  228] Training loss: 0.04686430, Validation loss: 0.04811707, Gradient norm: 2.55503417
INFO:root:[  229] Training loss: 0.04606655, Validation loss: 0.04653607, Gradient norm: 2.28074992
INFO:root:[  230] Training loss: 0.04506834, Validation loss: 0.04381900, Gradient norm: 1.94991703
INFO:root:[  231] Training loss: 0.04425859, Validation loss: 0.04376768, Gradient norm: 1.36022990
INFO:root:[  232] Training loss: 0.04455206, Validation loss: 0.04506237, Gradient norm: 1.33057102
INFO:root:[  233] Training loss: 0.04468508, Validation loss: 0.04441770, Gradient norm: 1.66826575
INFO:root:[  234] Training loss: 0.04368766, Validation loss: 0.04485359, Gradient norm: 1.13883367
INFO:root:[  235] Training loss: 0.04411826, Validation loss: 0.04505039, Gradient norm: 1.49283131
INFO:root:[  236] Training loss: 0.04447237, Validation loss: 0.04549575, Gradient norm: 1.67185111
INFO:root:[  237] Training loss: 0.04542352, Validation loss: 0.04309087, Gradient norm: 2.21290717
INFO:root:[  238] Training loss: 0.04517750, Validation loss: 0.04916172, Gradient norm: 2.07370799
INFO:root:[  239] Training loss: 0.04482641, Validation loss: 0.04353718, Gradient norm: 1.96448691
INFO:root:[  240] Training loss: 0.04481282, Validation loss: 0.04310129, Gradient norm: 2.07109623
INFO:root:[  241] Training loss: 0.04333476, Validation loss: 0.04531694, Gradient norm: 1.21630793
INFO:root:[  242] Training loss: 0.04515102, Validation loss: 0.04566723, Gradient norm: 2.26228363
INFO:root:[  243] Training loss: 0.04378026, Validation loss: 0.04596930, Gradient norm: 1.66538331
INFO:root:[  244] Training loss: 0.04428741, Validation loss: 0.04272165, Gradient norm: 1.96849772
INFO:root:[  245] Training loss: 0.04392932, Validation loss: 0.04487133, Gradient norm: 1.83633040
INFO:root:[  246] Training loss: 0.04313734, Validation loss: 0.04526585, Gradient norm: 1.37142476
INFO:root:[  247] Training loss: 0.04479011, Validation loss: 0.04286475, Gradient norm: 2.24092851
INFO:root:[  248] Training loss: 0.04366090, Validation loss: 0.04434354, Gradient norm: 1.74898094
INFO:root:[  249] Training loss: 0.04320162, Validation loss: 0.04230776, Gradient norm: 1.38411133
INFO:root:[  250] Training loss: 0.04353714, Validation loss: 0.04281364, Gradient norm: 1.60111923
INFO:root:[  251] Training loss: 0.04298610, Validation loss: 0.04318262, Gradient norm: 0.92862056
INFO:root:[  252] Training loss: 0.04359316, Validation loss: 0.04516232, Gradient norm: 1.63249703
INFO:root:[  253] Training loss: 0.04395183, Validation loss: 0.04379913, Gradient norm: 2.05104980
INFO:root:[  254] Training loss: 0.04272003, Validation loss: 0.04564393, Gradient norm: 1.26578807
INFO:root:[  255] Training loss: 0.04325744, Validation loss: 0.04317995, Gradient norm: 1.65825459
INFO:root:[  256] Training loss: 0.04284140, Validation loss: 0.04350119, Gradient norm: 1.40508858
INFO:root:[  257] Training loss: 0.04317249, Validation loss: 0.04242955, Gradient norm: 1.69981297
INFO:root:[  258] Training loss: 0.04283513, Validation loss: 0.04256013, Gradient norm: 1.47208278
INFO:root:[  259] Training loss: 0.04329599, Validation loss: 0.04153450, Gradient norm: 1.85021356
INFO:root:[  260] Training loss: 0.04277153, Validation loss: 0.04143192, Gradient norm: 1.53779720
INFO:root:[  261] Training loss: 0.04256222, Validation loss: 0.04187803, Gradient norm: 1.53306978
INFO:root:[  262] Training loss: 0.04340911, Validation loss: 0.04220931, Gradient norm: 1.90046603
INFO:root:[  263] Training loss: 0.04280024, Validation loss: 0.04656748, Gradient norm: 1.67848329
INFO:root:[  264] Training loss: 0.04445901, Validation loss: 0.04478968, Gradient norm: 2.43721741
INFO:root:[  265] Training loss: 0.04224549, Validation loss: 0.04453245, Gradient norm: 1.33846046
INFO:root:[  266] Training loss: 0.04248912, Validation loss: 0.04163690, Gradient norm: 1.57349410
INFO:root:[  267] Training loss: 0.04270930, Validation loss: 0.04386251, Gradient norm: 1.64360948
INFO:root:[  268] Training loss: 0.04257517, Validation loss: 0.04529699, Gradient norm: 1.69916779
INFO:root:[  269] Training loss: 0.04307387, Validation loss: 0.04110114, Gradient norm: 1.90458979
INFO:root:[  270] Training loss: 0.04138609, Validation loss: 0.04126369, Gradient norm: 0.61471348
INFO:root:[  271] Training loss: 0.04240809, Validation loss: 0.04553788, Gradient norm: 1.58367663
INFO:root:[  272] Training loss: 0.04289995, Validation loss: 0.04098894, Gradient norm: 2.08168305
INFO:root:[  273] Training loss: 0.04282289, Validation loss: 0.04238811, Gradient norm: 1.93104159
INFO:root:[  274] Training loss: 0.04195991, Validation loss: 0.04230110, Gradient norm: 1.49327586
INFO:root:[  275] Training loss: 0.04240843, Validation loss: 0.04202337, Gradient norm: 1.73336557
INFO:root:[  276] Training loss: 0.04202112, Validation loss: 0.04321788, Gradient norm: 1.57825619
INFO:root:[  277] Training loss: 0.04225549, Validation loss: 0.04063863, Gradient norm: 1.79834932
INFO:root:[  278] Training loss: 0.04201337, Validation loss: 0.04467981, Gradient norm: 1.76534281
INFO:root:[  279] Training loss: 0.04235686, Validation loss: 0.04092879, Gradient norm: 1.88068387
INFO:root:[  280] Training loss: 0.04148499, Validation loss: 0.04182702, Gradient norm: 1.29834521
INFO:root:[  281] Training loss: 0.04085054, Validation loss: 0.04076995, Gradient norm: 0.84812443
INFO:root:[  282] Training loss: 0.04132071, Validation loss: 0.04261129, Gradient norm: 1.12660979
INFO:root:[  283] Training loss: 0.04223867, Validation loss: 0.04085733, Gradient norm: 1.88970948
INFO:root:[  284] Training loss: 0.04185569, Validation loss: 0.04164705, Gradient norm: 1.78918676
INFO:root:[  285] Training loss: 0.04156890, Validation loss: 0.04132060, Gradient norm: 1.71864347
INFO:root:[  286] Training loss: 0.04216870, Validation loss: 0.04256183, Gradient norm: 1.90383147
INFO:root:EP 286: Early stopping
INFO:root:Training the model took 13244.007s.
INFO:root:Emptying the cuda cache took 0.107s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.86697
INFO:root:EnergyScoreTrain: 1.27876
INFO:root:CoverageTrain: 0.92978
INFO:root:IntervalWidthTrain: 0.07799
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.79149
INFO:root:EnergyScoreValidation: 1.22728
INFO:root:CoverageValidation: 0.93014
INFO:root:IntervalWidthValidation: 0.07825
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.48345
INFO:root:EnergyScoreTest: 1.01487
INFO:root:CoverageTest: 0.92726
INFO:root:IntervalWidthTest: 0.07714
INFO:root:###2 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1910505472
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.25341366, Validation loss: 0.34548666, Gradient norm: 8.27667540
INFO:root:[    2] Training loss: 0.26027486, Validation loss: 0.22070144, Gradient norm: 2.39129505
INFO:root:[    3] Training loss: 0.21258522, Validation loss: 0.20662043, Gradient norm: 1.33590365
INFO:root:[    4] Training loss: 0.19746040, Validation loss: 0.19060118, Gradient norm: 1.94855975
INFO:root:[    5] Training loss: 0.18822043, Validation loss: 0.18237979, Gradient norm: 1.74348769
INFO:root:[    6] Training loss: 0.18513586, Validation loss: 0.18037788, Gradient norm: 1.79971816
INFO:root:[    7] Training loss: 0.18054761, Validation loss: 0.17563629, Gradient norm: 2.85354788
INFO:root:[    8] Training loss: 0.17468905, Validation loss: 0.17762545, Gradient norm: 2.28255292
INFO:root:[    9] Training loss: 0.16878679, Validation loss: 0.16544567, Gradient norm: 2.13999823
INFO:root:[   10] Training loss: 0.16354235, Validation loss: 0.15806711, Gradient norm: 1.47741617
INFO:root:[   11] Training loss: 0.15985274, Validation loss: 0.15649381, Gradient norm: 1.36469291
INFO:root:[   12] Training loss: 0.15784986, Validation loss: 0.15432217, Gradient norm: 2.48136215
INFO:root:[   13] Training loss: 0.15791016, Validation loss: 0.15122806, Gradient norm: 2.63061768
INFO:root:[   14] Training loss: 0.15186922, Validation loss: 0.15967770, Gradient norm: 2.42722558
INFO:root:[   15] Training loss: 0.15084221, Validation loss: 0.14392189, Gradient norm: 2.31070694
INFO:root:[   16] Training loss: 0.14436798, Validation loss: 0.14291559, Gradient norm: 1.56019512
INFO:root:[   17] Training loss: 0.14771392, Validation loss: 0.14197497, Gradient norm: 3.00909485
INFO:root:[   18] Training loss: 0.14231982, Validation loss: 0.13845492, Gradient norm: 1.99931447
INFO:root:[   19] Training loss: 0.13823742, Validation loss: 0.13841644, Gradient norm: 1.38076598
INFO:root:[   20] Training loss: 0.13826532, Validation loss: 0.15387293, Gradient norm: 2.03279375
INFO:root:[   21] Training loss: 0.13970806, Validation loss: 0.13293546, Gradient norm: 2.45415979
INFO:root:[   22] Training loss: 0.13486423, Validation loss: 0.13405430, Gradient norm: 2.01918736
INFO:root:[   23] Training loss: 0.13515305, Validation loss: 0.14143878, Gradient norm: 2.60354025
INFO:root:[   24] Training loss: 0.13394798, Validation loss: 0.13047272, Gradient norm: 2.41871072
INFO:root:[   25] Training loss: 0.13448482, Validation loss: 0.12858528, Gradient norm: 2.91249346
INFO:root:[   26] Training loss: 0.12798916, Validation loss: 0.12612253, Gradient norm: 1.30828113
INFO:root:[   27] Training loss: 0.12963014, Validation loss: 0.12641128, Gradient norm: 2.61656739
INFO:root:[   28] Training loss: 0.12822921, Validation loss: 0.12786924, Gradient norm: 2.39431988
INFO:root:[   29] Training loss: 0.12479984, Validation loss: 0.12432027, Gradient norm: 1.60454866
INFO:root:[   30] Training loss: 0.12199549, Validation loss: 0.12021794, Gradient norm: 1.14265546
INFO:root:[   31] Training loss: 0.12103343, Validation loss: 0.11881575, Gradient norm: 1.76957966
INFO:root:[   32] Training loss: 0.11976234, Validation loss: 0.12374268, Gradient norm: 1.64139010
INFO:root:[   33] Training loss: 0.11810016, Validation loss: 0.11820256, Gradient norm: 1.46608926
INFO:root:[   34] Training loss: 0.11841639, Validation loss: 0.12236196, Gradient norm: 1.85660644
INFO:root:[   35] Training loss: 0.11661317, Validation loss: 0.12050125, Gradient norm: 1.96619148
INFO:root:[   36] Training loss: 0.12022461, Validation loss: 0.11955953, Gradient norm: 3.00723266
INFO:root:[   37] Training loss: 0.11466356, Validation loss: 0.11189092, Gradient norm: 2.08708490
INFO:root:[   38] Training loss: 0.11317646, Validation loss: 0.11105338, Gradient norm: 1.50917328
INFO:root:[   39] Training loss: 0.11415853, Validation loss: 0.11153677, Gradient norm: 2.40049059
INFO:root:[   40] Training loss: 0.11189199, Validation loss: 0.10957727, Gradient norm: 1.67655363
INFO:root:[   41] Training loss: 0.10924339, Validation loss: 0.11135887, Gradient norm: 1.18727858
INFO:root:[   42] Training loss: 0.10937930, Validation loss: 0.10853523, Gradient norm: 2.05668925
INFO:root:[   43] Training loss: 0.10836071, Validation loss: 0.10697307, Gradient norm: 2.22350861
INFO:root:[   44] Training loss: 0.10782066, Validation loss: 0.10816191, Gradient norm: 2.28421989
INFO:root:[   45] Training loss: 0.10635300, Validation loss: 0.10651931, Gradient norm: 2.00805947
INFO:root:[   46] Training loss: 0.10544406, Validation loss: 0.10467095, Gradient norm: 1.94400081
INFO:root:[   47] Training loss: 0.10354600, Validation loss: 0.10834982, Gradient norm: 1.29925797
INFO:root:[   48] Training loss: 0.11064566, Validation loss: 0.10753674, Gradient norm: 3.72459719
INFO:root:[   49] Training loss: 0.10327632, Validation loss: 0.10322773, Gradient norm: 1.63676378
INFO:root:[   50] Training loss: 0.10240141, Validation loss: 0.10004804, Gradient norm: 1.77855874
INFO:root:[   51] Training loss: 0.10275038, Validation loss: 0.10171203, Gradient norm: 1.72026799
INFO:root:[   52] Training loss: 0.10075853, Validation loss: 0.09776493, Gradient norm: 1.37903634
INFO:root:[   53] Training loss: 0.09954170, Validation loss: 0.10125088, Gradient norm: 1.69467570
INFO:root:[   54] Training loss: 0.10301991, Validation loss: 0.11041202, Gradient norm: 2.99360925
INFO:root:[   55] Training loss: 0.10086693, Validation loss: 0.10153376, Gradient norm: 2.42568278
INFO:root:[   56] Training loss: 0.10042569, Validation loss: 0.09938254, Gradient norm: 2.70237640
INFO:root:[   57] Training loss: 0.09740830, Validation loss: 0.09651833, Gradient norm: 2.30062832
INFO:root:[   58] Training loss: 0.09699778, Validation loss: 0.09971534, Gradient norm: 1.67353963
INFO:root:[   59] Training loss: 0.09936453, Validation loss: 0.09377789, Gradient norm: 2.65402967
INFO:root:[   60] Training loss: 0.09823153, Validation loss: 0.10352293, Gradient norm: 2.86281609
INFO:root:[   61] Training loss: 0.10111029, Validation loss: 0.09432685, Gradient norm: 3.44422764
INFO:root:[   62] Training loss: 0.09524248, Validation loss: 0.09820702, Gradient norm: 1.74405864
INFO:root:[   63] Training loss: 0.09562896, Validation loss: 0.09357421, Gradient norm: 2.03708640
INFO:root:[   64] Training loss: 0.09285624, Validation loss: 0.09415041, Gradient norm: 1.03065596
INFO:root:[   65] Training loss: 0.09251581, Validation loss: 0.09585007, Gradient norm: 1.73827535
INFO:root:[   66] Training loss: 0.09309244, Validation loss: 0.09301447, Gradient norm: 1.79079762
INFO:root:[   67] Training loss: 0.09335192, Validation loss: 0.09598502, Gradient norm: 2.15922760
INFO:root:[   68] Training loss: 0.09225673, Validation loss: 0.09033746, Gradient norm: 2.07518177
INFO:root:[   69] Training loss: 0.09217699, Validation loss: 0.09062308, Gradient norm: 1.60649260
INFO:root:[   70] Training loss: 0.09050324, Validation loss: 0.08902292, Gradient norm: 1.39829144
INFO:root:[   71] Training loss: 0.09216813, Validation loss: 0.09914511, Gradient norm: 2.12803028
INFO:root:[   72] Training loss: 0.09304200, Validation loss: 0.08891994, Gradient norm: 2.66813018
INFO:root:[   73] Training loss: 0.08961494, Validation loss: 0.09128704, Gradient norm: 1.69960586
INFO:root:[   74] Training loss: 0.09011036, Validation loss: 0.08961025, Gradient norm: 2.04466526
INFO:root:[   75] Training loss: 0.08963660, Validation loss: 0.08937730, Gradient norm: 1.88767204
INFO:root:[   76] Training loss: 0.08954581, Validation loss: 0.08921296, Gradient norm: 1.89099749
INFO:root:[   77] Training loss: 0.08965992, Validation loss: 0.08495710, Gradient norm: 2.41178869
INFO:root:[   78] Training loss: 0.08859300, Validation loss: 0.08922693, Gradient norm: 2.09851215
INFO:root:[   79] Training loss: 0.08924535, Validation loss: 0.09758183, Gradient norm: 2.49507477
INFO:root:[   80] Training loss: 0.08796771, Validation loss: 0.08604557, Gradient norm: 1.62388378
INFO:root:[   81] Training loss: 0.08836913, Validation loss: 0.09064853, Gradient norm: 2.35786216
INFO:root:[   82] Training loss: 0.08937133, Validation loss: 0.08632425, Gradient norm: 2.77615820
INFO:root:[   83] Training loss: 0.08520916, Validation loss: 0.08496996, Gradient norm: 0.98146301
INFO:root:[   84] Training loss: 0.08816113, Validation loss: 0.09186802, Gradient norm: 2.56750680
INFO:root:[   85] Training loss: 0.08761344, Validation loss: 0.08769199, Gradient norm: 2.54552062
INFO:root:[   86] Training loss: 0.08632608, Validation loss: 0.08450614, Gradient norm: 1.87567422
INFO:root:[   87] Training loss: 0.08583265, Validation loss: 0.08466673, Gradient norm: 1.91375167
INFO:root:[   88] Training loss: 0.08515514, Validation loss: 0.08542197, Gradient norm: 1.93898274
INFO:root:[   89] Training loss: 0.08517805, Validation loss: 0.08536953, Gradient norm: 1.88776476
INFO:root:[   90] Training loss: 0.08664345, Validation loss: 0.08417628, Gradient norm: 2.93957529
INFO:root:[   91] Training loss: 0.08589366, Validation loss: 0.08456559, Gradient norm: 2.20262498
INFO:root:[   92] Training loss: 0.08313035, Validation loss: 0.08358896, Gradient norm: 1.42927631
INFO:root:[   93] Training loss: 0.08424788, Validation loss: 0.08862508, Gradient norm: 2.15880350
INFO:root:[   94] Training loss: 0.08382638, Validation loss: 0.08778819, Gradient norm: 1.87873500
INFO:root:[   95] Training loss: 0.08245667, Validation loss: 0.08269077, Gradient norm: 1.49324239
INFO:root:[   96] Training loss: 0.08290041, Validation loss: 0.08084152, Gradient norm: 1.49017824
INFO:root:[   97] Training loss: 0.08181997, Validation loss: 0.08707273, Gradient norm: 1.62613653
INFO:root:[   98] Training loss: 0.08320022, Validation loss: 0.08252790, Gradient norm: 2.50897901
INFO:root:[   99] Training loss: 0.08359956, Validation loss: 0.07972167, Gradient norm: 2.58454812
INFO:root:[  100] Training loss: 0.08378845, Validation loss: 0.08852436, Gradient norm: 2.62629281
INFO:root:[  101] Training loss: 0.08275809, Validation loss: 0.08298383, Gradient norm: 2.30359057
INFO:root:[  102] Training loss: 0.08236738, Validation loss: 0.08108710, Gradient norm: 2.39250122
INFO:root:[  103] Training loss: 0.08306409, Validation loss: 0.07944995, Gradient norm: 2.76738586
INFO:root:[  104] Training loss: 0.08185107, Validation loss: 0.08024937, Gradient norm: 2.25052171
INFO:root:[  105] Training loss: 0.08158561, Validation loss: 0.08212626, Gradient norm: 2.17921209
INFO:root:[  106] Training loss: 0.08045175, Validation loss: 0.07903342, Gradient norm: 1.97021490
INFO:root:[  107] Training loss: 0.08015459, Validation loss: 0.07879646, Gradient norm: 1.81392191
INFO:root:[  108] Training loss: 0.07938524, Validation loss: 0.07908629, Gradient norm: 1.75463215
INFO:root:[  109] Training loss: 0.07971879, Validation loss: 0.07772075, Gradient norm: 1.78529425
INFO:root:[  110] Training loss: 0.07986539, Validation loss: 0.08363763, Gradient norm: 2.13002459
INFO:root:[  111] Training loss: 0.07904555, Validation loss: 0.07756818, Gradient norm: 1.69439988
INFO:root:[  112] Training loss: 0.08013785, Validation loss: 0.07762937, Gradient norm: 2.42634318
INFO:root:[  113] Training loss: 0.07831653, Validation loss: 0.08470859, Gradient norm: 1.65769695
INFO:root:[  114] Training loss: 0.07893035, Validation loss: 0.07901015, Gradient norm: 2.23510455
INFO:root:[  115] Training loss: 0.07856316, Validation loss: 0.07665229, Gradient norm: 2.24449745
INFO:root:[  116] Training loss: 0.07836007, Validation loss: 0.07771070, Gradient norm: 2.43496421
INFO:root:[  117] Training loss: 0.07786982, Validation loss: 0.07778183, Gradient norm: 1.88126295
INFO:root:[  118] Training loss: 0.07662576, Validation loss: 0.07579717, Gradient norm: 1.48059378
INFO:root:[  119] Training loss: 0.07667360, Validation loss: 0.08092959, Gradient norm: 1.70329613
INFO:root:[  120] Training loss: 0.07691677, Validation loss: 0.07791655, Gradient norm: 2.12036769
INFO:root:[  121] Training loss: 0.07700873, Validation loss: 0.07565053, Gradient norm: 1.90320608
INFO:root:[  122] Training loss: 0.07856334, Validation loss: 0.07484591, Gradient norm: 2.87934315
INFO:root:[  123] Training loss: 0.07757362, Validation loss: 0.07630656, Gradient norm: 2.50190571
INFO:root:[  124] Training loss: 0.07635647, Validation loss: 0.07421392, Gradient norm: 1.98941003
INFO:root:[  125] Training loss: 0.07629055, Validation loss: 0.07942632, Gradient norm: 2.10149783
INFO:root:[  126] Training loss: 0.07562017, Validation loss: 0.07501850, Gradient norm: 1.89413074
INFO:root:[  127] Training loss: 0.07552909, Validation loss: 0.07396769, Gradient norm: 2.00685144
INFO:root:[  128] Training loss: 0.07438255, Validation loss: 0.07428959, Gradient norm: 1.16066739
INFO:root:[  129] Training loss: 0.07531734, Validation loss: 0.07566882, Gradient norm: 2.02828049
INFO:root:[  130] Training loss: 0.07512837, Validation loss: 0.07368367, Gradient norm: 2.06671517
INFO:root:[  131] Training loss: 0.07523723, Validation loss: 0.07684299, Gradient norm: 2.31639494
INFO:root:[  132] Training loss: 0.07562171, Validation loss: 0.07677488, Gradient norm: 2.46286817
INFO:root:[  133] Training loss: 0.07494349, Validation loss: 0.07456251, Gradient norm: 2.36759645
INFO:root:[  134] Training loss: 0.07450521, Validation loss: 0.07313233, Gradient norm: 2.34104565
INFO:root:[  135] Training loss: 0.07327677, Validation loss: 0.07276363, Gradient norm: 1.45413898
INFO:root:[  136] Training loss: 0.07348696, Validation loss: 0.07364250, Gradient norm: 1.85468200
INFO:root:[  137] Training loss: 0.07433847, Validation loss: 0.07417893, Gradient norm: 2.31925318
INFO:root:[  138] Training loss: 0.07439757, Validation loss: 0.07164862, Gradient norm: 2.60279365
INFO:root:[  139] Training loss: 0.07317126, Validation loss: 0.07181333, Gradient norm: 1.81746395
INFO:root:[  140] Training loss: 0.07309377, Validation loss: 0.07340714, Gradient norm: 2.28212832
INFO:root:[  141] Training loss: 0.07279770, Validation loss: 0.07295440, Gradient norm: 2.08112092
INFO:root:[  142] Training loss: 0.07217554, Validation loss: 0.07109670, Gradient norm: 1.68508164
INFO:root:[  143] Training loss: 0.07134318, Validation loss: 0.07441826, Gradient norm: 1.13976117
INFO:root:[  144] Training loss: 0.07209102, Validation loss: 0.07068856, Gradient norm: 1.81631347
INFO:root:[  145] Training loss: 0.07269479, Validation loss: 0.07112108, Gradient norm: 2.38112897
INFO:root:[  146] Training loss: 0.07307593, Validation loss: 0.07365105, Gradient norm: 2.63582427
INFO:root:[  147] Training loss: 0.07264535, Validation loss: 0.07210568, Gradient norm: 2.53020595
INFO:root:[  148] Training loss: 0.07299608, Validation loss: 0.07152501, Gradient norm: 2.69086341
INFO:root:[  149] Training loss: 0.07134186, Validation loss: 0.07021321, Gradient norm: 1.92830926
INFO:root:[  150] Training loss: 0.07056808, Validation loss: 0.06991911, Gradient norm: 1.75037694
INFO:root:[  151] Training loss: 0.06971110, Validation loss: 0.07005419, Gradient norm: 1.12926626
INFO:root:[  152] Training loss: 0.07054920, Validation loss: 0.06902647, Gradient norm: 1.76767760
INFO:root:[  153] Training loss: 0.07115061, Validation loss: 0.07099880, Gradient norm: 2.13405003
INFO:root:[  154] Training loss: 0.07074711, Validation loss: 0.07435760, Gradient norm: 2.16578583
INFO:root:[  155] Training loss: 0.07091751, Validation loss: 0.07157985, Gradient norm: 2.39014770
INFO:root:[  156] Training loss: 0.07097655, Validation loss: 0.07020380, Gradient norm: 2.32630513
INFO:root:[  157] Training loss: 0.06946887, Validation loss: 0.06819186, Gradient norm: 1.46327578
INFO:root:[  158] Training loss: 0.06849383, Validation loss: 0.06994042, Gradient norm: 0.98110357
INFO:root:[  159] Training loss: 0.06907354, Validation loss: 0.06794784, Gradient norm: 1.80293486
INFO:root:[  160] Training loss: 0.06917802, Validation loss: 0.06788169, Gradient norm: 1.74423830
INFO:root:[  161] Training loss: 0.06968804, Validation loss: 0.06818687, Gradient norm: 2.25409628
INFO:root:[  162] Training loss: 0.06917640, Validation loss: 0.06953508, Gradient norm: 1.83162423
INFO:root:[  163] Training loss: 0.06830990, Validation loss: 0.07170953, Gradient norm: 1.59196619
INFO:root:[  164] Training loss: 0.06910882, Validation loss: 0.06821094, Gradient norm: 2.24349992
INFO:root:[  165] Training loss: 0.06864110, Validation loss: 0.06790924, Gradient norm: 2.20374823
INFO:root:[  166] Training loss: 0.06723979, Validation loss: 0.06729190, Gradient norm: 1.12482104
INFO:root:[  167] Training loss: 0.06794518, Validation loss: 0.06812030, Gradient norm: 1.95202625
INFO:root:[  168] Training loss: 0.06892356, Validation loss: 0.06734293, Gradient norm: 2.50932617
INFO:root:[  169] Training loss: 0.06742468, Validation loss: 0.06699389, Gradient norm: 1.45422864
INFO:root:[  170] Training loss: 0.06747425, Validation loss: 0.06689802, Gradient norm: 1.68422897
INFO:root:[  171] Training loss: 0.06675762, Validation loss: 0.06638145, Gradient norm: 1.36000608
INFO:root:[  172] Training loss: 0.06735020, Validation loss: 0.06700813, Gradient norm: 1.83155131
INFO:root:[  173] Training loss: 0.06694048, Validation loss: 0.06678823, Gradient norm: 1.78813919
INFO:root:[  174] Training loss: 0.06661521, Validation loss: 0.06618463, Gradient norm: 1.68237426
INFO:root:[  175] Training loss: 0.06604607, Validation loss: 0.06623695, Gradient norm: 1.27047770
INFO:root:[  176] Training loss: 0.06616822, Validation loss: 0.06652396, Gradient norm: 1.44669267
INFO:root:[  177] Training loss: 0.06606900, Validation loss: 0.06707932, Gradient norm: 1.53849664
INFO:root:[  178] Training loss: 0.06573707, Validation loss: 0.06517816, Gradient norm: 1.25407819
INFO:root:[  179] Training loss: 0.06495793, Validation loss: 0.06495839, Gradient norm: 0.86280527
INFO:root:[  180] Training loss: 0.06592321, Validation loss: 0.06729923, Gradient norm: 1.60163248
INFO:root:[  181] Training loss: 0.06611576, Validation loss: 0.06760433, Gradient norm: 2.15278001
INFO:root:[  182] Training loss: 0.06655519, Validation loss: 0.06539633, Gradient norm: 2.32758370
INFO:root:[  183] Training loss: 0.06563461, Validation loss: 0.06388097, Gradient norm: 1.94732164
INFO:root:[  184] Training loss: 0.06515696, Validation loss: 0.06406160, Gradient norm: 1.77611741
INFO:root:[  185] Training loss: 0.06459490, Validation loss: 0.06508001, Gradient norm: 1.38162688
INFO:root:[  186] Training loss: 0.06447048, Validation loss: 0.06485542, Gradient norm: 1.38585012
INFO:root:[  187] Training loss: 0.06443784, Validation loss: 0.06538604, Gradient norm: 1.54887004
INFO:root:[  188] Training loss: 0.06521891, Validation loss: 0.06408579, Gradient norm: 2.08257631
INFO:root:[  189] Training loss: 0.06582140, Validation loss: 0.06596361, Gradient norm: 2.48680954
INFO:root:[  190] Training loss: 0.06426993, Validation loss: 0.06718503, Gradient norm: 1.63047845
INFO:root:[  191] Training loss: 0.06399551, Validation loss: 0.06397554, Gradient norm: 1.57859144
INFO:root:[  192] Training loss: 0.06356262, Validation loss: 0.06315062, Gradient norm: 1.43999850
INFO:root:[  193] Training loss: 0.06329774, Validation loss: 0.06480163, Gradient norm: 1.16830385
INFO:root:[  194] Training loss: 0.06398343, Validation loss: 0.06460453, Gradient norm: 1.92620947
INFO:root:[  195] Training loss: 0.06394641, Validation loss: 0.06215501, Gradient norm: 1.92725409
INFO:root:[  196] Training loss: 0.06366412, Validation loss: 0.06241452, Gradient norm: 1.77434159
INFO:root:[  197] Training loss: 0.06297061, Validation loss: 0.06293560, Gradient norm: 1.21170307
INFO:root:[  198] Training loss: 0.06319311, Validation loss: 0.06294701, Gradient norm: 1.59051945
INFO:root:[  199] Training loss: 0.06315922, Validation loss: 0.06208152, Gradient norm: 1.68847761
INFO:root:[  200] Training loss: 0.06349232, Validation loss: 0.06330284, Gradient norm: 1.98184513
INFO:root:[  201] Training loss: 0.06297685, Validation loss: 0.06243556, Gradient norm: 1.84079268
INFO:root:[  202] Training loss: 0.06227291, Validation loss: 0.06288132, Gradient norm: 1.31219461
INFO:root:[  203] Training loss: 0.06264131, Validation loss: 0.06360548, Gradient norm: 1.75510469
INFO:root:[  204] Training loss: 0.06262464, Validation loss: 0.06208999, Gradient norm: 1.73853095
INFO:root:[  205] Training loss: 0.06204470, Validation loss: 0.06174631, Gradient norm: 1.46140985
INFO:root:[  206] Training loss: 0.06184696, Validation loss: 0.06393462, Gradient norm: 1.37694218
INFO:root:[  207] Training loss: 0.06216979, Validation loss: 0.06311936, Gradient norm: 1.79195636
INFO:root:[  208] Training loss: 0.06277785, Validation loss: 0.06172072, Gradient norm: 2.22982598
INFO:root:[  209] Training loss: 0.06325872, Validation loss: 0.06284721, Gradient norm: 2.38478070
INFO:root:[  210] Training loss: 0.06220086, Validation loss: 0.06178191, Gradient norm: 1.89449093
INFO:root:[  211] Training loss: 0.06154320, Validation loss: 0.06339102, Gradient norm: 1.48695346
INFO:root:[  212] Training loss: 0.06242235, Validation loss: 0.06393247, Gradient norm: 2.19908761
INFO:root:[  213] Training loss: 0.06223862, Validation loss: 0.06244988, Gradient norm: 2.09240986
INFO:root:[  214] Training loss: 0.06121393, Validation loss: 0.06183189, Gradient norm: 1.62120509
INFO:root:[  215] Training loss: 0.06129901, Validation loss: 0.06073064, Gradient norm: 1.67106824
INFO:root:[  216] Training loss: 0.06115652, Validation loss: 0.06325867, Gradient norm: 1.67444852
INFO:root:[  217] Training loss: 0.06087844, Validation loss: 0.06130094, Gradient norm: 1.52080942
INFO:root:[  218] Training loss: 0.06068561, Validation loss: 0.06083000, Gradient norm: 1.33177902
INFO:root:[  219] Training loss: 0.06092528, Validation loss: 0.06001403, Gradient norm: 1.77880066
INFO:root:[  220] Training loss: 0.06158928, Validation loss: 0.06057831, Gradient norm: 2.11554290
INFO:root:[  221] Training loss: 0.06066084, Validation loss: 0.06026656, Gradient norm: 1.63673281
INFO:root:[  222] Training loss: 0.06042012, Validation loss: 0.05951764, Gradient norm: 1.53847694
INFO:root:[  223] Training loss: 0.06020248, Validation loss: 0.06068725, Gradient norm: 1.46882536
INFO:root:[  224] Training loss: 0.06052181, Validation loss: 0.06157292, Gradient norm: 1.78142925
INFO:root:[  225] Training loss: 0.05970935, Validation loss: 0.05980618, Gradient norm: 1.30952365
INFO:root:[  226] Training loss: 0.06018567, Validation loss: 0.05916996, Gradient norm: 1.63945559
INFO:root:[  227] Training loss: 0.06026940, Validation loss: 0.06092622, Gradient norm: 1.86461953
INFO:root:[  228] Training loss: 0.06018474, Validation loss: 0.06121578, Gradient norm: 1.76139320
INFO:root:[  229] Training loss: 0.06032501, Validation loss: 0.05990344, Gradient norm: 1.92722972
INFO:root:[  230] Training loss: 0.05978023, Validation loss: 0.05955759, Gradient norm: 1.67721053
INFO:root:[  231] Training loss: 0.05963895, Validation loss: 0.05882127, Gradient norm: 1.55955125
INFO:root:[  232] Training loss: 0.05980396, Validation loss: 0.05853511, Gradient norm: 1.91092921
INFO:root:[  233] Training loss: 0.05961732, Validation loss: 0.05981776, Gradient norm: 1.72280716
INFO:root:[  234] Training loss: 0.05952984, Validation loss: 0.05963548, Gradient norm: 1.76500044
INFO:root:[  235] Training loss: 0.05965841, Validation loss: 0.05913585, Gradient norm: 1.94716459
INFO:root:[  236] Training loss: 0.05923594, Validation loss: 0.06045762, Gradient norm: 1.74234694
INFO:root:[  237] Training loss: 0.05898269, Validation loss: 0.05970939, Gradient norm: 1.51685986
INFO:root:[  238] Training loss: 0.05946907, Validation loss: 0.05958859, Gradient norm: 2.04796759
INFO:root:[  239] Training loss: 0.05903531, Validation loss: 0.05902477, Gradient norm: 1.64762045
INFO:root:[  240] Training loss: 0.05964753, Validation loss: 0.05951476, Gradient norm: 2.12025957
INFO:root:[  241] Training loss: 0.05912239, Validation loss: 0.06025071, Gradient norm: 1.99586729
INFO:root:EP 241: Early stopping
INFO:root:Training the model took 11156.231s.
INFO:root:Emptying the cuda cache took 0.112s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 2.42044
INFO:root:EnergyScoreTrain: 1.83073
INFO:root:CoverageTrain: 0.98091
INFO:root:IntervalWidthTrain: 0.1478
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 2.33169
INFO:root:EnergyScoreValidation: 1.76227
INFO:root:CoverageValidation: 0.98106
INFO:root:IntervalWidthValidation: 0.14835
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.91946
INFO:root:EnergyScoreTest: 1.45304
INFO:root:CoverageTest: 0.98091
INFO:root:IntervalWidthTest: 0.14698
INFO:root:###3 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 369098752
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.93301103, Validation loss: 0.33455139, Gradient norm: 6.62399970
INFO:root:[    2] Training loss: 0.29868646, Validation loss: 0.27783951, Gradient norm: 1.57819990
INFO:root:[    3] Training loss: 0.26197515, Validation loss: 0.24772862, Gradient norm: 1.25324547
INFO:root:[    4] Training loss: 0.23753123, Validation loss: 0.23056417, Gradient norm: 1.33073248
INFO:root:[    5] Training loss: 0.22630368, Validation loss: 0.21229838, Gradient norm: 1.52082189
INFO:root:[    6] Training loss: 0.21120722, Validation loss: 0.20634450, Gradient norm: 1.36044836
INFO:root:[    7] Training loss: 0.20595499, Validation loss: 0.20782368, Gradient norm: 2.09127707
INFO:root:[    8] Training loss: 0.20153552, Validation loss: 0.19134341, Gradient norm: 2.37121869
INFO:root:[    9] Training loss: 0.19305594, Validation loss: 0.18887379, Gradient norm: 1.79955756
INFO:root:[   10] Training loss: 0.18607247, Validation loss: 0.18408977, Gradient norm: 1.86189665
INFO:root:[   11] Training loss: 0.18134256, Validation loss: 0.18049417, Gradient norm: 1.78238248
INFO:root:[   12] Training loss: 0.17655964, Validation loss: 0.17923247, Gradient norm: 2.10141277
INFO:root:[   13] Training loss: 0.17418431, Validation loss: 0.17620283, Gradient norm: 2.06629147
INFO:root:[   14] Training loss: 0.16893389, Validation loss: 0.16194108, Gradient norm: 1.87348478
INFO:root:[   15] Training loss: 0.16673886, Validation loss: 0.16189804, Gradient norm: 2.09855239
INFO:root:[   16] Training loss: 0.16206583, Validation loss: 0.15632416, Gradient norm: 1.67840926
INFO:root:[   17] Training loss: 0.15708902, Validation loss: 0.17047335, Gradient norm: 1.68464221
INFO:root:[   18] Training loss: 0.16012277, Validation loss: 0.15056324, Gradient norm: 2.80393932
INFO:root:[   19] Training loss: 0.15617106, Validation loss: 0.14859347, Gradient norm: 3.07960991
INFO:root:[   20] Training loss: 0.15062574, Validation loss: 0.15423846, Gradient norm: 2.43361637
INFO:root:[   21] Training loss: 0.15092648, Validation loss: 0.15184177, Gradient norm: 2.27954577
INFO:root:[   22] Training loss: 0.14721624, Validation loss: 0.15009332, Gradient norm: 2.57773326
INFO:root:[   23] Training loss: 0.14635818, Validation loss: 0.14699165, Gradient norm: 2.59055427
INFO:root:[   24] Training loss: 0.14240080, Validation loss: 0.13577611, Gradient norm: 2.56431188
INFO:root:[   25] Training loss: 0.13755866, Validation loss: 0.14376563, Gradient norm: 1.34226886
INFO:root:[   26] Training loss: 0.13975990, Validation loss: 0.13261915, Gradient norm: 2.57150030
INFO:root:[   27] Training loss: 0.13663071, Validation loss: 0.13840521, Gradient norm: 2.24728967
INFO:root:[   28] Training loss: 0.13208753, Validation loss: 0.12956848, Gradient norm: 1.01735852
INFO:root:[   29] Training loss: 0.13187270, Validation loss: 0.13692452, Gradient norm: 1.71464160
INFO:root:[   30] Training loss: 0.12976827, Validation loss: 0.13820427, Gradient norm: 1.72758397
INFO:root:[   31] Training loss: 0.12984869, Validation loss: 0.12453553, Gradient norm: 2.42755695
INFO:root:[   32] Training loss: 0.12471861, Validation loss: 0.12632407, Gradient norm: 1.25975984
INFO:root:[   33] Training loss: 0.12574757, Validation loss: 0.12749356, Gradient norm: 2.30899802
INFO:root:[   34] Training loss: 0.12364435, Validation loss: 0.12994445, Gradient norm: 2.48067372
INFO:root:[   35] Training loss: 0.12275495, Validation loss: 0.12138779, Gradient norm: 1.79498445
INFO:root:[   36] Training loss: 0.12241386, Validation loss: 0.12826702, Gradient norm: 2.36844270
INFO:root:[   37] Training loss: 0.12123979, Validation loss: 0.12164943, Gradient norm: 3.04769326
INFO:root:[   38] Training loss: 0.12003560, Validation loss: 0.11841100, Gradient norm: 2.51897852
INFO:root:[   39] Training loss: 0.11806569, Validation loss: 0.11371080, Gradient norm: 2.15220895
INFO:root:[   40] Training loss: 0.11648461, Validation loss: 0.11869662, Gradient norm: 1.71472811
INFO:root:[   41] Training loss: 0.11550691, Validation loss: 0.11382417, Gradient norm: 1.88229006
INFO:root:[   42] Training loss: 0.11405328, Validation loss: 0.11144810, Gradient norm: 1.60917192
INFO:root:[   43] Training loss: 0.11280633, Validation loss: 0.11395316, Gradient norm: 1.40337750
INFO:root:[   44] Training loss: 0.11168896, Validation loss: 0.11164969, Gradient norm: 1.82689498
INFO:root:[   45] Training loss: 0.11028911, Validation loss: 0.11195764, Gradient norm: 1.52447769
INFO:root:[   46] Training loss: 0.11009527, Validation loss: 0.12044101, Gradient norm: 1.55057661
INFO:root:[   47] Training loss: 0.10931986, Validation loss: 0.10813916, Gradient norm: 1.64602292
INFO:root:[   48] Training loss: 0.10872144, Validation loss: 0.10805166, Gradient norm: 2.14792815
INFO:root:[   49] Training loss: 0.10912690, Validation loss: 0.10971727, Gradient norm: 2.55840693
INFO:root:[   50] Training loss: 0.10866884, Validation loss: 0.11296385, Gradient norm: 2.64746224
INFO:root:[   51] Training loss: 0.10643837, Validation loss: 0.11097832, Gradient norm: 2.40920656
INFO:root:[   52] Training loss: 0.10813195, Validation loss: 0.10566325, Gradient norm: 2.90290580
INFO:root:[   53] Training loss: 0.10306464, Validation loss: 0.10341332, Gradient norm: 1.33686622
INFO:root:[   54] Training loss: 0.10269327, Validation loss: 0.10380689, Gradient norm: 1.05960873
INFO:root:[   55] Training loss: 0.10255875, Validation loss: 0.10201530, Gradient norm: 1.17224571
INFO:root:[   56] Training loss: 0.10214189, Validation loss: 0.10099274, Gradient norm: 1.36703931
INFO:root:[   57] Training loss: 0.10307226, Validation loss: 0.10103802, Gradient norm: 2.02002285
INFO:root:[   58] Training loss: 0.09990180, Validation loss: 0.10286892, Gradient norm: 0.90682058
INFO:root:[   59] Training loss: 0.10198028, Validation loss: 0.10076600, Gradient norm: 1.95972748
INFO:root:[   60] Training loss: 0.09881335, Validation loss: 0.10027194, Gradient norm: 1.19003309
INFO:root:[   61] Training loss: 0.09926858, Validation loss: 0.10172972, Gradient norm: 1.70401617
INFO:root:[   62] Training loss: 0.09887301, Validation loss: 0.09670132, Gradient norm: 1.88223667
INFO:root:[   63] Training loss: 0.09832763, Validation loss: 0.09992339, Gradient norm: 1.87672043
INFO:root:[   64] Training loss: 0.09770312, Validation loss: 0.09577260, Gradient norm: 1.81226522
INFO:root:[   65] Training loss: 0.09606651, Validation loss: 0.09530221, Gradient norm: 1.00848119
INFO:root:[   66] Training loss: 0.09551474, Validation loss: 0.09391387, Gradient norm: 1.23235572
INFO:root:[   67] Training loss: 0.09633727, Validation loss: 0.09509866, Gradient norm: 1.90035615
INFO:root:[   68] Training loss: 0.09519376, Validation loss: 0.10218562, Gradient norm: 1.85748019
INFO:root:[   69] Training loss: 0.09729232, Validation loss: 0.09371268, Gradient norm: 2.35217727
INFO:root:[   70] Training loss: 0.09352716, Validation loss: 0.09434396, Gradient norm: 1.45844679
INFO:root:[   71] Training loss: 0.09476414, Validation loss: 0.09854412, Gradient norm: 1.83973511
INFO:root:[   72] Training loss: 0.09390603, Validation loss: 0.09140810, Gradient norm: 2.05206803
INFO:root:[   73] Training loss: 0.09170287, Validation loss: 0.09104707, Gradient norm: 0.95378257
INFO:root:[   74] Training loss: 0.09160898, Validation loss: 0.09299981, Gradient norm: 1.23625660
INFO:root:[   75] Training loss: 0.09236836, Validation loss: 0.09000581, Gradient norm: 1.93087439
INFO:root:[   76] Training loss: 0.09176493, Validation loss: 0.09025688, Gradient norm: 1.51102795
INFO:root:[   77] Training loss: 0.09075487, Validation loss: 0.09068468, Gradient norm: 1.46155001
INFO:root:[   78] Training loss: 0.09069003, Validation loss: 0.09075162, Gradient norm: 1.40620493
INFO:root:[   79] Training loss: 0.09081792, Validation loss: 0.08828391, Gradient norm: 1.66822650
INFO:root:[   80] Training loss: 0.08912827, Validation loss: 0.08937652, Gradient norm: 1.09825297
INFO:root:[   81] Training loss: 0.08884458, Validation loss: 0.08853112, Gradient norm: 1.26655956
INFO:root:[   82] Training loss: 0.08873853, Validation loss: 0.09049893, Gradient norm: 1.36095287
INFO:root:[   83] Training loss: 0.08769142, Validation loss: 0.08697878, Gradient norm: 1.11031289
INFO:root:[   84] Training loss: 0.08746717, Validation loss: 0.08749574, Gradient norm: 1.27309551
INFO:root:[   85] Training loss: 0.08686133, Validation loss: 0.08822721, Gradient norm: 1.07907525
INFO:root:[   86] Training loss: 0.08713229, Validation loss: 0.08595084, Gradient norm: 1.47102513
INFO:root:[   87] Training loss: 0.08624598, Validation loss: 0.08818818, Gradient norm: 1.06356549
INFO:root:[   88] Training loss: 0.08636144, Validation loss: 0.08771415, Gradient norm: 1.51040796
INFO:root:[   89] Training loss: 0.08580228, Validation loss: 0.08950149, Gradient norm: 1.44883262
INFO:root:[   90] Training loss: 0.08544921, Validation loss: 0.08543241, Gradient norm: 1.16329274
INFO:root:[   91] Training loss: 0.08535405, Validation loss: 0.08355956, Gradient norm: 1.45979300
INFO:root:[   92] Training loss: 0.08556380, Validation loss: 0.08346115, Gradient norm: 1.83672725
INFO:root:[   93] Training loss: 0.08490708, Validation loss: 0.08805232, Gradient norm: 1.66541131
INFO:root:[   94] Training loss: 0.08478848, Validation loss: 0.08423228, Gradient norm: 1.71024499
INFO:root:[   95] Training loss: 0.08465317, Validation loss: 0.08322698, Gradient norm: 2.00356439
INFO:root:[   96] Training loss: 0.08459522, Validation loss: 0.08358565, Gradient norm: 1.93025544
INFO:root:[   97] Training loss: 0.08311516, Validation loss: 0.08375201, Gradient norm: 1.48793347
INFO:root:[   98] Training loss: 0.08307928, Validation loss: 0.08367066, Gradient norm: 1.60354635
INFO:root:[   99] Training loss: 0.08279878, Validation loss: 0.08468005, Gradient norm: 1.43910677
INFO:root:[  100] Training loss: 0.08180550, Validation loss: 0.08120057, Gradient norm: 1.36956354
INFO:root:[  101] Training loss: 0.08186954, Validation loss: 0.08061506, Gradient norm: 1.49078156
INFO:root:[  102] Training loss: 0.08097874, Validation loss: 0.08288721, Gradient norm: 0.95999949
INFO:root:[  103] Training loss: 0.08117654, Validation loss: 0.08008990, Gradient norm: 1.41827434
INFO:root:[  104] Training loss: 0.07973958, Validation loss: 0.07969453, Gradient norm: 0.44996724
INFO:root:[  105] Training loss: 0.08023045, Validation loss: 0.08144387, Gradient norm: 1.21993818
INFO:root:[  106] Training loss: 0.08064937, Validation loss: 0.07968149, Gradient norm: 1.61946870
INFO:root:[  107] Training loss: 0.07942277, Validation loss: 0.08195782, Gradient norm: 1.01216918
INFO:root:[  108] Training loss: 0.08063636, Validation loss: 0.07864929, Gradient norm: 1.83550577
INFO:root:[  109] Training loss: 0.07982845, Validation loss: 0.08456891, Gradient norm: 1.45177684
INFO:root:[  110] Training loss: 0.07994639, Validation loss: 0.07877817, Gradient norm: 1.81114851
INFO:root:[  111] Training loss: 0.07966674, Validation loss: 0.07815367, Gradient norm: 1.73105346
INFO:root:[  112] Training loss: 0.07894690, Validation loss: 0.07801686, Gradient norm: 1.46133969
INFO:root:[  113] Training loss: 0.07774122, Validation loss: 0.07738792, Gradient norm: 0.74607820
INFO:root:[  114] Training loss: 0.07738495, Validation loss: 0.07731526, Gradient norm: 0.79730653
INFO:root:[  115] Training loss: 0.07773747, Validation loss: 0.07795467, Gradient norm: 1.18479513
INFO:root:[  116] Training loss: 0.07758053, Validation loss: 0.07684334, Gradient norm: 1.35545596
INFO:root:[  117] Training loss: 0.07753535, Validation loss: 0.07678166, Gradient norm: 1.51634941
INFO:root:[  118] Training loss: 0.07752369, Validation loss: 0.07722969, Gradient norm: 1.46166501
INFO:root:[  119] Training loss: 0.07637867, Validation loss: 0.07617489, Gradient norm: 0.86302866
INFO:root:[  120] Training loss: 0.07604316, Validation loss: 0.07536105, Gradient norm: 0.87021101
INFO:root:[  121] Training loss: 0.07602321, Validation loss: 0.07546668, Gradient norm: 1.01928326
INFO:root:[  122] Training loss: 0.07642944, Validation loss: 0.07608971, Gradient norm: 1.40919256
INFO:root:[  123] Training loss: 0.07557351, Validation loss: 0.07462491, Gradient norm: 1.13765705
INFO:root:[  124] Training loss: 0.07528310, Validation loss: 0.07514608, Gradient norm: 0.89716868
INFO:root:[  125] Training loss: 0.07548099, Validation loss: 0.07805950, Gradient norm: 1.32170263
INFO:root:[  126] Training loss: 0.07586968, Validation loss: 0.07541728, Gradient norm: 1.66078364
INFO:root:[  127] Training loss: 0.07589025, Validation loss: 0.07483608, Gradient norm: 1.69761119
INFO:root:[  128] Training loss: 0.07497955, Validation loss: 0.07420719, Gradient norm: 1.40217755
INFO:root:[  129] Training loss: 0.07390699, Validation loss: 0.07614623, Gradient norm: 0.52234146
INFO:root:[  130] Training loss: 0.07461616, Validation loss: 0.07342715, Gradient norm: 1.30438627
INFO:root:[  131] Training loss: 0.07351137, Validation loss: 0.07378379, Gradient norm: 0.78394806
INFO:root:[  132] Training loss: 0.07380635, Validation loss: 0.07316972, Gradient norm: 1.08676613
INFO:root:[  133] Training loss: 0.07346239, Validation loss: 0.07325045, Gradient norm: 0.95755275
INFO:root:[  134] Training loss: 0.07364441, Validation loss: 0.07354543, Gradient norm: 1.31738928
INFO:root:[  135] Training loss: 0.07301141, Validation loss: 0.07409525, Gradient norm: 0.99648550
INFO:root:[  136] Training loss: 0.07304929, Validation loss: 0.07218271, Gradient norm: 1.23790524
INFO:root:[  137] Training loss: 0.07258883, Validation loss: 0.07280951, Gradient norm: 0.85617799
INFO:root:[  138] Training loss: 0.07292648, Validation loss: 0.07219726, Gradient norm: 1.31873527
INFO:root:[  139] Training loss: 0.07297449, Validation loss: 0.07223642, Gradient norm: 1.33929225
INFO:root:[  140] Training loss: 0.07269571, Validation loss: 0.07202940, Gradient norm: 1.35746345
INFO:root:[  141] Training loss: 0.07192400, Validation loss: 0.07422436, Gradient norm: 0.83959902
INFO:root:[  142] Training loss: 0.07203908, Validation loss: 0.07223475, Gradient norm: 1.18382340
INFO:root:[  143] Training loss: 0.07160774, Validation loss: 0.07119332, Gradient norm: 1.04520620
INFO:root:[  144] Training loss: 0.07147062, Validation loss: 0.07080375, Gradient norm: 0.93268954
INFO:root:[  145] Training loss: 0.07084618, Validation loss: 0.07107813, Gradient norm: 0.55168758
INFO:root:[  146] Training loss: 0.07129203, Validation loss: 0.07145732, Gradient norm: 1.17529683
INFO:root:[  147] Training loss: 0.07096260, Validation loss: 0.07112582, Gradient norm: 1.02517102
INFO:root:[  148] Training loss: 0.07085693, Validation loss: 0.07031153, Gradient norm: 1.05762288
INFO:root:[  149] Training loss: 0.07047175, Validation loss: 0.07026072, Gradient norm: 0.87327464
INFO:root:[  150] Training loss: 0.07039294, Validation loss: 0.07048794, Gradient norm: 0.97147448
INFO:root:[  151] Training loss: 0.07043901, Validation loss: 0.07170923, Gradient norm: 1.07818167
INFO:root:[  152] Training loss: 0.07083268, Validation loss: 0.07120737, Gradient norm: 1.54936726
INFO:root:[  153] Training loss: 0.06990954, Validation loss: 0.07091501, Gradient norm: 0.88457446
INFO:root:[  154] Training loss: 0.07014598, Validation loss: 0.06936005, Gradient norm: 1.29142495
INFO:root:[  155] Training loss: 0.06975586, Validation loss: 0.07064613, Gradient norm: 1.10374617
INFO:root:[  156] Training loss: 0.06995202, Validation loss: 0.06966145, Gradient norm: 1.33116925
INFO:root:[  157] Training loss: 0.06964315, Validation loss: 0.06890168, Gradient norm: 1.12476648
INFO:root:[  158] Training loss: 0.06929924, Validation loss: 0.06899898, Gradient norm: 1.01388673
INFO:root:[  159] Training loss: 0.06918086, Validation loss: 0.06854300, Gradient norm: 1.00081123
INFO:root:[  160] Training loss: 0.06924877, Validation loss: 0.06955593, Gradient norm: 1.22827380
INFO:root:[  161] Training loss: 0.06906097, Validation loss: 0.06908424, Gradient norm: 1.26697184
INFO:root:[  162] Training loss: 0.06851440, Validation loss: 0.06880657, Gradient norm: 0.93448446
INFO:root:[  163] Training loss: 0.06849614, Validation loss: 0.06802968, Gradient norm: 1.06618991
INFO:root:[  164] Training loss: 0.06836079, Validation loss: 0.06902353, Gradient norm: 1.06117033
INFO:root:[  165] Training loss: 0.06829799, Validation loss: 0.06775030, Gradient norm: 1.03165675
INFO:root:[  166] Training loss: 0.06815717, Validation loss: 0.06836778, Gradient norm: 1.10755337
INFO:root:[  167] Training loss: 0.06861223, Validation loss: 0.06846970, Gradient norm: 1.37101072
INFO:root:[  168] Training loss: 0.06797295, Validation loss: 0.06792434, Gradient norm: 1.17176316
INFO:root:[  169] Training loss: 0.06843173, Validation loss: 0.06803727, Gradient norm: 1.49496258
INFO:root:[  170] Training loss: 0.06747804, Validation loss: 0.06783914, Gradient norm: 0.84803533
INFO:root:[  171] Training loss: 0.06745097, Validation loss: 0.06847123, Gradient norm: 1.07135757
INFO:root:[  172] Training loss: 0.06741614, Validation loss: 0.06732327, Gradient norm: 1.21808149
INFO:root:[  173] Training loss: 0.06730761, Validation loss: 0.06652647, Gradient norm: 1.16981419
INFO:root:[  174] Training loss: 0.06710927, Validation loss: 0.06679898, Gradient norm: 1.08823759
INFO:root:[  175] Training loss: 0.06661032, Validation loss: 0.06698192, Gradient norm: 0.81414631
INFO:root:[  176] Training loss: 0.06651028, Validation loss: 0.06682681, Gradient norm: 0.89449636
INFO:root:[  177] Training loss: 0.06683317, Validation loss: 0.06775385, Gradient norm: 1.22517080
INFO:root:[  178] Training loss: 0.06663349, Validation loss: 0.06683631, Gradient norm: 1.16734598
INFO:root:[  179] Training loss: 0.06640727, Validation loss: 0.06622468, Gradient norm: 1.01875668
INFO:root:[  180] Training loss: 0.06632007, Validation loss: 0.06636505, Gradient norm: 1.00351842
INFO:root:[  181] Training loss: 0.06581171, Validation loss: 0.06578234, Gradient norm: 0.68465924
INFO:root:[  182] Training loss: 0.06572853, Validation loss: 0.06567686, Gradient norm: 0.92593840
INFO:root:[  183] Training loss: 0.06612798, Validation loss: 0.06541953, Gradient norm: 1.29564317
INFO:root:[  184] Training loss: 0.06622894, Validation loss: 0.06539066, Gradient norm: 1.39088810
INFO:root:[  185] Training loss: 0.06545949, Validation loss: 0.06535764, Gradient norm: 0.88973727
INFO:root:[  186] Training loss: 0.06564498, Validation loss: 0.06523162, Gradient norm: 1.09439491
INFO:root:[  187] Training loss: 0.06534834, Validation loss: 0.06500700, Gradient norm: 1.02964584
INFO:root:[  188] Training loss: 0.06554341, Validation loss: 0.06586250, Gradient norm: 1.24582642
INFO:root:[  189] Training loss: 0.06541108, Validation loss: 0.06492235, Gradient norm: 1.24441697
INFO:root:[  190] Training loss: 0.06481006, Validation loss: 0.06478510, Gradient norm: 0.88254661
INFO:root:[  191] Training loss: 0.06459262, Validation loss: 0.06549546, Gradient norm: 0.86123731
INFO:root:[  192] Training loss: 0.06469817, Validation loss: 0.06430483, Gradient norm: 0.98075961
INFO:root:[  193] Training loss: 0.06485800, Validation loss: 0.06423478, Gradient norm: 1.19099557
INFO:root:[  194] Training loss: 0.06463120, Validation loss: 0.06442133, Gradient norm: 1.08771606
INFO:root:[  195] Training loss: 0.06442364, Validation loss: 0.06454525, Gradient norm: 1.10410465
INFO:root:[  196] Training loss: 0.06413096, Validation loss: 0.06376982, Gradient norm: 0.97398450
INFO:root:[  197] Training loss: 0.06420742, Validation loss: 0.06343441, Gradient norm: 1.20524705
INFO:root:[  198] Training loss: 0.06415169, Validation loss: 0.06365852, Gradient norm: 1.17327781
INFO:root:[  199] Training loss: 0.06391571, Validation loss: 0.06378177, Gradient norm: 1.06898700
INFO:root:[  200] Training loss: 0.06377362, Validation loss: 0.06323745, Gradient norm: 1.05403749
INFO:root:[  201] Training loss: 0.06345435, Validation loss: 0.06356397, Gradient norm: 0.85493757
INFO:root:[  202] Training loss: 0.06337505, Validation loss: 0.06357786, Gradient norm: 0.81726424
INFO:root:[  203] Training loss: 0.06330158, Validation loss: 0.06286912, Gradient norm: 0.82516070
INFO:root:[  204] Training loss: 0.06335363, Validation loss: 0.06326376, Gradient norm: 1.06128015
INFO:root:[  205] Training loss: 0.06312825, Validation loss: 0.06313064, Gradient norm: 0.81089601
INFO:root:[  206] Training loss: 0.06314262, Validation loss: 0.06308879, Gradient norm: 0.95685437
INFO:root:[  207] Training loss: 0.06316534, Validation loss: 0.06350987, Gradient norm: 1.06633680
INFO:root:[  208] Training loss: 0.06293966, Validation loss: 0.06390314, Gradient norm: 1.09959823
INFO:root:[  209] Training loss: 0.06270754, Validation loss: 0.06229962, Gradient norm: 1.02254518
INFO:root:[  210] Training loss: 0.06263220, Validation loss: 0.06264451, Gradient norm: 1.00045847
INFO:root:[  211] Training loss: 0.06248106, Validation loss: 0.06368437, Gradient norm: 0.97258057
INFO:root:[  212] Training loss: 0.06276581, Validation loss: 0.06352047, Gradient norm: 1.28170444
INFO:root:[  213] Training loss: 0.06223370, Validation loss: 0.06216420, Gradient norm: 0.83819769
INFO:root:[  214] Training loss: 0.06212443, Validation loss: 0.06233485, Gradient norm: 0.83233773
INFO:root:[  215] Training loss: 0.06215095, Validation loss: 0.06203479, Gradient norm: 1.04530009
INFO:root:[  216] Training loss: 0.06198767, Validation loss: 0.06247386, Gradient norm: 0.96294875
INFO:root:[  217] Training loss: 0.06172630, Validation loss: 0.06142155, Gradient norm: 0.91516877
INFO:root:[  218] Training loss: 0.06146777, Validation loss: 0.06264875, Gradient norm: 0.69287884
INFO:root:[  219] Training loss: 0.06178387, Validation loss: 0.06207626, Gradient norm: 1.05256279
INFO:root:[  220] Training loss: 0.06141513, Validation loss: 0.06154181, Gradient norm: 0.79651412
INFO:root:[  221] Training loss: 0.06137845, Validation loss: 0.06094055, Gradient norm: 0.92054136
INFO:root:[  222] Training loss: 0.06125966, Validation loss: 0.06164223, Gradient norm: 0.82690473
INFO:root:[  223] Training loss: 0.06134239, Validation loss: 0.06210250, Gradient norm: 1.06177427
INFO:root:[  224] Training loss: 0.06102424, Validation loss: 0.06102510, Gradient norm: 1.00939839
INFO:root:[  225] Training loss: 0.06097769, Validation loss: 0.06065065, Gradient norm: 1.01678843
INFO:root:[  226] Training loss: 0.06108314, Validation loss: 0.06086766, Gradient norm: 1.15146293
INFO:root:[  227] Training loss: 0.06093740, Validation loss: 0.06051116, Gradient norm: 1.17158027
INFO:root:[  228] Training loss: 0.06066735, Validation loss: 0.06141128, Gradient norm: 0.95523055
INFO:root:[  229] Training loss: 0.06070526, Validation loss: 0.06107189, Gradient norm: 1.18369590
INFO:root:[  230] Training loss: 0.06047658, Validation loss: 0.06008550, Gradient norm: 0.86731035
INFO:root:[  231] Training loss: 0.06065263, Validation loss: 0.06003986, Gradient norm: 1.05371979
INFO:root:[  232] Training loss: 0.06051765, Validation loss: 0.05997209, Gradient norm: 1.24185459
INFO:root:[  233] Training loss: 0.06020548, Validation loss: 0.05975612, Gradient norm: 1.04874617
INFO:root:[  234] Training loss: 0.06021573, Validation loss: 0.05983995, Gradient norm: 1.00535868
INFO:root:[  235] Training loss: 0.06025577, Validation loss: 0.05978954, Gradient norm: 1.18498015
INFO:root:[  236] Training loss: 0.05987327, Validation loss: 0.06030548, Gradient norm: 0.98119754
INFO:root:[  237] Training loss: 0.05996313, Validation loss: 0.06045290, Gradient norm: 1.06235577
INFO:root:[  238] Training loss: 0.05976303, Validation loss: 0.06049731, Gradient norm: 1.01377058
INFO:root:[  239] Training loss: 0.05963932, Validation loss: 0.05976877, Gradient norm: 0.97804235
INFO:root:[  240] Training loss: 0.05956206, Validation loss: 0.05917449, Gradient norm: 1.02972726
INFO:root:[  241] Training loss: 0.05965815, Validation loss: 0.05927804, Gradient norm: 1.16784382
INFO:root:[  242] Training loss: 0.05953792, Validation loss: 0.05974864, Gradient norm: 1.11857400
INFO:root:[  243] Training loss: 0.05909142, Validation loss: 0.05885928, Gradient norm: 0.65386098
INFO:root:[  244] Training loss: 0.05892336, Validation loss: 0.05978481, Gradient norm: 0.71827646
INFO:root:[  245] Training loss: 0.05907186, Validation loss: 0.05868424, Gradient norm: 0.86139195
INFO:root:[  246] Training loss: 0.05934374, Validation loss: 0.05881118, Gradient norm: 1.17065955
INFO:root:[  247] Training loss: 0.05894912, Validation loss: 0.05861091, Gradient norm: 1.11420271
INFO:root:[  248] Training loss: 0.05871012, Validation loss: 0.05871556, Gradient norm: 0.79908231
INFO:root:[  249] Training loss: 0.05871072, Validation loss: 0.05885739, Gradient norm: 1.01570074
INFO:root:[  250] Training loss: 0.05865390, Validation loss: 0.05876273, Gradient norm: 1.01522499
INFO:root:[  251] Training loss: 0.05845207, Validation loss: 0.05824751, Gradient norm: 0.97843573
INFO:root:[  252] Training loss: 0.05811434, Validation loss: 0.05811360, Gradient norm: 0.33177290
INFO:root:[  253] Training loss: 0.05801716, Validation loss: 0.05786040, Gradient norm: 0.57761741
INFO:root:[  254] Training loss: 0.05802702, Validation loss: 0.05849138, Gradient norm: 0.66146324
INFO:root:[  255] Training loss: 0.05811198, Validation loss: 0.05791043, Gradient norm: 0.77836444
INFO:root:[  256] Training loss: 0.05792741, Validation loss: 0.05847837, Gradient norm: 0.94134486
INFO:root:[  257] Training loss: 0.05804605, Validation loss: 0.05815178, Gradient norm: 1.12192336
INFO:root:[  258] Training loss: 0.05780744, Validation loss: 0.05788932, Gradient norm: 0.92681534
INFO:root:[  259] Training loss: 0.05789192, Validation loss: 0.05831129, Gradient norm: 1.18996441
INFO:root:[  260] Training loss: 0.05777200, Validation loss: 0.05759140, Gradient norm: 1.11569507
INFO:root:[  261] Training loss: 0.05746381, Validation loss: 0.05747124, Gradient norm: 0.92907651
INFO:root:[  262] Training loss: 0.05750524, Validation loss: 0.05835545, Gradient norm: 1.02381294
INFO:root:[  263] Training loss: 0.05744557, Validation loss: 0.05815672, Gradient norm: 1.07298839
INFO:root:[  264] Training loss: 0.05728549, Validation loss: 0.05730506, Gradient norm: 1.04103360
INFO:root:[  265] Training loss: 0.05713110, Validation loss: 0.05695699, Gradient norm: 0.88154894
INFO:root:[  266] Training loss: 0.05717826, Validation loss: 0.05723270, Gradient norm: 1.02297317
INFO:root:[  267] Training loss: 0.05722108, Validation loss: 0.05800467, Gradient norm: 1.10651638
INFO:root:[  268] Training loss: 0.05723443, Validation loss: 0.05707352, Gradient norm: 1.20228011
INFO:root:[  269] Training loss: 0.05681153, Validation loss: 0.05736141, Gradient norm: 1.03021126
INFO:root:[  270] Training loss: 0.05680086, Validation loss: 0.05710379, Gradient norm: 1.08107682
INFO:root:[  271] Training loss: 0.05671833, Validation loss: 0.05664129, Gradient norm: 0.99262888
INFO:root:[  272] Training loss: 0.05678371, Validation loss: 0.05665805, Gradient norm: 1.00292491
INFO:root:[  273] Training loss: 0.05660613, Validation loss: 0.05621674, Gradient norm: 1.05261706
INFO:root:[  274] Training loss: 0.05636455, Validation loss: 0.05644537, Gradient norm: 0.81758628
INFO:root:[  275] Training loss: 0.05602062, Validation loss: 0.05588644, Gradient norm: 0.38362085
INFO:root:[  276] Training loss: 0.05612674, Validation loss: 0.05665807, Gradient norm: 0.68911471
INFO:root:[  277] Training loss: 0.05621939, Validation loss: 0.05644412, Gradient norm: 1.01732836
INFO:root:[  278] Training loss: 0.05598641, Validation loss: 0.05583970, Gradient norm: 0.86993290
INFO:root:[  279] Training loss: 0.05589930, Validation loss: 0.05618084, Gradient norm: 0.96043690
INFO:root:[  280] Training loss: 0.05577240, Validation loss: 0.05567972, Gradient norm: 0.91417797
INFO:root:[  281] Training loss: 0.05596798, Validation loss: 0.05651356, Gradient norm: 0.75687034
INFO:root:[  282] Training loss: 0.05589152, Validation loss: 0.05640987, Gradient norm: 1.16420935
INFO:root:[  283] Training loss: 0.05559712, Validation loss: 0.05526784, Gradient norm: 1.00708778
INFO:root:[  284] Training loss: 0.05553404, Validation loss: 0.05590797, Gradient norm: 0.73856768
INFO:root:[  285] Training loss: 0.05566643, Validation loss: 0.05590439, Gradient norm: 1.14738647
INFO:root:[  286] Training loss: 0.05536591, Validation loss: 0.05509392, Gradient norm: 0.96924675
INFO:root:[  287] Training loss: 0.05531107, Validation loss: 0.05559558, Gradient norm: 0.93564106
INFO:root:[  288] Training loss: 0.05536283, Validation loss: 0.05604471, Gradient norm: 1.24891658
INFO:root:[  289] Training loss: 0.05519595, Validation loss: 0.05547534, Gradient norm: 1.11465951
INFO:root:[  290] Training loss: 0.05496765, Validation loss: 0.05549136, Gradient norm: 0.95387620
INFO:root:[  291] Training loss: 0.05489146, Validation loss: 0.05478135, Gradient norm: 0.93021255
INFO:root:[  292] Training loss: 0.05491419, Validation loss: 0.05593599, Gradient norm: 0.73062393
INFO:root:[  293] Training loss: 0.05503171, Validation loss: 0.05474290, Gradient norm: 1.15434313
INFO:root:[  294] Training loss: 0.05485218, Validation loss: 0.05477003, Gradient norm: 1.12396692
INFO:root:[  295] Training loss: 0.05478427, Validation loss: 0.05486620, Gradient norm: 0.85886657
INFO:root:[  296] Training loss: 0.05478950, Validation loss: 0.05448918, Gradient norm: 1.19820812
INFO:root:[  297] Training loss: 0.05453054, Validation loss: 0.05429375, Gradient norm: 1.17033497
INFO:root:[  298] Training loss: 0.05452396, Validation loss: 0.05495706, Gradient norm: 0.75913109
INFO:root:[  299] Training loss: 0.05472908, Validation loss: 0.05466919, Gradient norm: 1.33894166
INFO:root:[  300] Training loss: 0.05424779, Validation loss: 0.05442837, Gradient norm: 1.06144326
INFO:root:[  301] Training loss: 0.05415608, Validation loss: 0.05447458, Gradient norm: 0.71770346
INFO:root:[  302] Training loss: 0.05431153, Validation loss: 0.05401302, Gradient norm: 1.21693799
INFO:root:[  303] Training loss: 0.05387599, Validation loss: 0.05404757, Gradient norm: 0.80621791
INFO:root:[  304] Training loss: 0.05407809, Validation loss: 0.05408588, Gradient norm: 0.91494216
INFO:root:[  305] Training loss: 0.05400982, Validation loss: 0.05355602, Gradient norm: 1.20005699
INFO:root:[  306] Training loss: 0.05368311, Validation loss: 0.05370138, Gradient norm: 0.89242047
INFO:root:[  307] Training loss: 0.05353059, Validation loss: 0.05345195, Gradient norm: 0.83666857
INFO:root:[  308] Training loss: 0.05362642, Validation loss: 0.05348984, Gradient norm: 1.06995852
INFO:root:[  309] Training loss: 0.05360530, Validation loss: 0.05334261, Gradient norm: 1.14332213
INFO:root:[  310] Training loss: 0.05339155, Validation loss: 0.05388146, Gradient norm: 0.94245445
INFO:root:[  311] Training loss: 0.05334019, Validation loss: 0.05376799, Gradient norm: 0.97136973
INFO:root:[  312] Training loss: 0.05357716, Validation loss: 0.05395264, Gradient norm: 1.21054478
INFO:root:[  313] Training loss: 0.05367711, Validation loss: 0.05350100, Gradient norm: 1.38606893
INFO:root:[  314] Training loss: 0.05325904, Validation loss: 0.05382531, Gradient norm: 1.20878454
INFO:root:[  315] Training loss: 0.05309905, Validation loss: 0.05326178, Gradient norm: 1.12317912
INFO:root:[  316] Training loss: 0.05287914, Validation loss: 0.05291229, Gradient norm: 0.93224705
INFO:root:[  317] Training loss: 0.05270956, Validation loss: 0.05292549, Gradient norm: 0.67918473
INFO:root:[  318] Training loss: 0.05265260, Validation loss: 0.05267540, Gradient norm: 0.39784588
INFO:root:[  319] Training loss: 0.05258046, Validation loss: 0.05286173, Gradient norm: 0.70101647
INFO:root:[  320] Training loss: 0.05254960, Validation loss: 0.05318551, Gradient norm: 0.91867207
INFO:root:[  321] Training loss: 0.05266006, Validation loss: 0.05244690, Gradient norm: 1.06981117
INFO:root:[  322] Training loss: 0.05255462, Validation loss: 0.05258620, Gradient norm: 1.02455133
INFO:root:[  323] Training loss: 0.05278590, Validation loss: 0.05314126, Gradient norm: 1.15303755
INFO:root:[  324] Training loss: 0.05295347, Validation loss: 0.05314219, Gradient norm: 1.32471389
INFO:root:[  325] Training loss: 0.05257044, Validation loss: 0.05188463, Gradient norm: 1.43475627
INFO:root:[  326] Training loss: 0.05217386, Validation loss: 0.05219967, Gradient norm: 0.93095380
INFO:root:[  327] Training loss: 0.05214379, Validation loss: 0.05220413, Gradient norm: 0.64524240
INFO:root:[  328] Training loss: 0.05205202, Validation loss: 0.05210505, Gradient norm: 0.94698477
INFO:root:[  329] Training loss: 0.05196944, Validation loss: 0.05181951, Gradient norm: 0.70154063
INFO:root:[  330] Training loss: 0.05188827, Validation loss: 0.05192805, Gradient norm: 0.96705284
INFO:root:[  331] Training loss: 0.05171754, Validation loss: 0.05269581, Gradient norm: 0.77053130
INFO:root:[  332] Training loss: 0.05214603, Validation loss: 0.05241839, Gradient norm: 1.22781706
INFO:root:[  333] Training loss: 0.05178978, Validation loss: 0.05177190, Gradient norm: 1.24771158
INFO:root:[  334] Training loss: 0.05156280, Validation loss: 0.05188971, Gradient norm: 0.85978047
INFO:root:[  335] Training loss: 0.05161967, Validation loss: 0.05208185, Gradient norm: 1.11804474
INFO:root:[  336] Training loss: 0.05147795, Validation loss: 0.05140252, Gradient norm: 1.00261124
INFO:root:[  337] Training loss: 0.05149168, Validation loss: 0.05161737, Gradient norm: 0.94333599
INFO:root:[  338] Training loss: 0.05125531, Validation loss: 0.05147090, Gradient norm: 1.01856655
INFO:root:[  339] Training loss: 0.05122482, Validation loss: 0.05113140, Gradient norm: 1.19655673
INFO:root:[  340] Training loss: 0.05116984, Validation loss: 0.05099332, Gradient norm: 1.11406063
INFO:root:[  341] Training loss: 0.05100678, Validation loss: 0.05165408, Gradient norm: 0.78725642
INFO:root:[  342] Training loss: 0.05122918, Validation loss: 0.05079674, Gradient norm: 1.22135362
INFO:root:[  343] Training loss: 0.05083742, Validation loss: 0.05077147, Gradient norm: 0.96785573
INFO:root:[  344] Training loss: 0.05074434, Validation loss: 0.05066882, Gradient norm: 0.80666428
INFO:root:[  345] Training loss: 0.05075034, Validation loss: 0.05111533, Gradient norm: 0.89896940
INFO:root:[  346] Training loss: 0.05089664, Validation loss: 0.05087289, Gradient norm: 1.21421253
INFO:root:[  347] Training loss: 0.05076333, Validation loss: 0.05052581, Gradient norm: 1.05726232
INFO:root:[  348] Training loss: 0.05058556, Validation loss: 0.05082421, Gradient norm: 0.85120060
INFO:root:[  349] Training loss: 0.05084005, Validation loss: 0.05054442, Gradient norm: 0.64654583
INFO:root:[  350] Training loss: 0.05082722, Validation loss: 0.05103415, Gradient norm: 1.08530298
INFO:root:[  351] Training loss: 0.05033525, Validation loss: 0.05046203, Gradient norm: 1.08970931
INFO:root:[  352] Training loss: 0.05016463, Validation loss: 0.05010458, Gradient norm: 1.01874534
INFO:root:[  353] Training loss: 0.05009405, Validation loss: 0.05053492, Gradient norm: 1.08492883
INFO:root:[  354] Training loss: 0.05011286, Validation loss: 0.05039529, Gradient norm: 1.19600028
INFO:root:[  355] Training loss: 0.05005406, Validation loss: 0.04993359, Gradient norm: 1.11338061
INFO:root:[  356] Training loss: 0.05004065, Validation loss: 0.05000036, Gradient norm: 1.08776833
INFO:root:[  357] Training loss: 0.04994513, Validation loss: 0.05002538, Gradient norm: 1.08097984
INFO:root:[  358] Training loss: 0.04965492, Validation loss: 0.04954245, Gradient norm: 0.90113563
INFO:root:[  359] Training loss: 0.04968610, Validation loss: 0.04959441, Gradient norm: 0.76032562
INFO:root:[  360] Training loss: 0.04984785, Validation loss: 0.04984010, Gradient norm: 0.83464184
INFO:root:[  361] Training loss: 0.04962007, Validation loss: 0.04942852, Gradient norm: 1.17556081
INFO:root:[  362] Training loss: 0.04945307, Validation loss: 0.04978355, Gradient norm: 1.02574490
INFO:root:[  363] Training loss: 0.04930960, Validation loss: 0.04946574, Gradient norm: 0.90394877
INFO:root:[  364] Training loss: 0.04941459, Validation loss: 0.04976640, Gradient norm: 1.18482481
INFO:root:[  365] Training loss: 0.04927960, Validation loss: 0.04917258, Gradient norm: 1.16781928
INFO:root:[  366] Training loss: 0.04930547, Validation loss: 0.04978647, Gradient norm: 1.26874127
INFO:root:[  367] Training loss: 0.04916116, Validation loss: 0.04890276, Gradient norm: 1.03889659
INFO:root:[  368] Training loss: 0.04902962, Validation loss: 0.04899713, Gradient norm: 0.92217677
INFO:root:[  369] Training loss: 0.04904312, Validation loss: 0.04958670, Gradient norm: 1.16532510
INFO:root:[  370] Training loss: 0.04901685, Validation loss: 0.04911989, Gradient norm: 1.29555147
INFO:root:[  371] Training loss: 0.04871481, Validation loss: 0.04886719, Gradient norm: 0.86687200
INFO:root:[  372] Training loss: 0.04864295, Validation loss: 0.04901480, Gradient norm: 0.80713126
INFO:root:[  373] Training loss: 0.04883401, Validation loss: 0.04875037, Gradient norm: 1.18536944
INFO:root:[  374] Training loss: 0.04929106, Validation loss: 0.04943855, Gradient norm: 1.32193199
INFO:root:[  375] Training loss: 0.04872709, Validation loss: 0.04868366, Gradient norm: 1.25235335
INFO:root:[  376] Training loss: 0.04865965, Validation loss: 0.04839936, Gradient norm: 1.06960563
INFO:root:[  377] Training loss: 0.04873665, Validation loss: 0.04887687, Gradient norm: 1.11557269
INFO:root:[  378] Training loss: 0.04839130, Validation loss: 0.04840791, Gradient norm: 1.05667333
INFO:root:[  379] Training loss: 0.04842076, Validation loss: 0.04877570, Gradient norm: 0.99105483
INFO:root:[  380] Training loss: 0.04836440, Validation loss: 0.04842121, Gradient norm: 1.23894214
INFO:root:[  381] Training loss: 0.04805011, Validation loss: 0.04833662, Gradient norm: 0.91926386
INFO:root:[  382] Training loss: 0.04813637, Validation loss: 0.04783105, Gradient norm: 0.95291624
INFO:root:[  383] Training loss: 0.04790531, Validation loss: 0.04778653, Gradient norm: 0.84179903
INFO:root:[  384] Training loss: 0.04790451, Validation loss: 0.04836716, Gradient norm: 0.98426832
INFO:root:[  385] Training loss: 0.04779099, Validation loss: 0.04750629, Gradient norm: 1.08274984
INFO:root:[  386] Training loss: 0.04765986, Validation loss: 0.04810031, Gradient norm: 0.97001462
INFO:root:[  387] Training loss: 0.04766831, Validation loss: 0.04755520, Gradient norm: 1.14264327
INFO:root:[  388] Training loss: 0.04762036, Validation loss: 0.04782583, Gradient norm: 1.25481099
INFO:root:[  389] Training loss: 0.04753920, Validation loss: 0.04731624, Gradient norm: 1.24609337
INFO:root:[  390] Training loss: 0.04742530, Validation loss: 0.04778578, Gradient norm: 0.96809826
INFO:root:[  391] Training loss: 0.04767790, Validation loss: 0.04784124, Gradient norm: 1.33343831
INFO:root:[  392] Training loss: 0.04753827, Validation loss: 0.04751957, Gradient norm: 1.32850269
INFO:root:[  393] Training loss: 0.04718574, Validation loss: 0.04707128, Gradient norm: 1.16281102
INFO:root:[  394] Training loss: 0.04712632, Validation loss: 0.04722498, Gradient norm: 0.92039906
INFO:root:[  395] Training loss: 0.04736228, Validation loss: 0.04748146, Gradient norm: 1.27111412
INFO:root:[  396] Training loss: 0.04711015, Validation loss: 0.04711532, Gradient norm: 1.26549930
INFO:root:[  397] Training loss: 0.04707683, Validation loss: 0.04714635, Gradient norm: 0.95853958
INFO:root:[  398] Training loss: 0.04727412, Validation loss: 0.04747130, Gradient norm: 1.07405129
INFO:root:[  399] Training loss: 0.04699412, Validation loss: 0.04695010, Gradient norm: 1.36726018
INFO:root:[  400] Training loss: 0.04678322, Validation loss: 0.04663318, Gradient norm: 1.20638665
INFO:root:[  401] Training loss: 0.04702512, Validation loss: 0.04692116, Gradient norm: 1.27924002
INFO:root:[  402] Training loss: 0.04668888, Validation loss: 0.04656809, Gradient norm: 1.15047686
INFO:root:[  403] Training loss: 0.04662479, Validation loss: 0.04709853, Gradient norm: 1.26471073
INFO:root:[  404] Training loss: 0.04655231, Validation loss: 0.04647561, Gradient norm: 1.35890987
INFO:root:[  405] Training loss: 0.04638215, Validation loss: 0.04655152, Gradient norm: 1.05516059
INFO:root:[  406] Training loss: 0.04643726, Validation loss: 0.04650134, Gradient norm: 1.12878346
INFO:root:[  407] Training loss: 0.04633708, Validation loss: 0.04612037, Gradient norm: 1.24357856
INFO:root:[  408] Training loss: 0.04609930, Validation loss: 0.04634184, Gradient norm: 0.98346977
INFO:root:[  409] Training loss: 0.04606853, Validation loss: 0.04607930, Gradient norm: 1.06898384
INFO:root:[  410] Training loss: 0.04602363, Validation loss: 0.04588446, Gradient norm: 1.09722493
INFO:root:[  411] Training loss: 0.04592102, Validation loss: 0.04603834, Gradient norm: 1.14170150
INFO:root:[  412] Training loss: 0.04595192, Validation loss: 0.04627575, Gradient norm: 1.01691402
INFO:root:[  413] Training loss: 0.04609241, Validation loss: 0.04605647, Gradient norm: 1.36101231
INFO:root:[  414] Training loss: 0.04599317, Validation loss: 0.04638330, Gradient norm: 1.38303450
INFO:root:[  415] Training loss: 0.04582487, Validation loss: 0.04571437, Gradient norm: 1.46206425
INFO:root:[  416] Training loss: 0.04555863, Validation loss: 0.04565640, Gradient norm: 0.99089766
INFO:root:[  417] Training loss: 0.04563396, Validation loss: 0.04604893, Gradient norm: 0.95203819
INFO:root:[  418] Training loss: 0.04549554, Validation loss: 0.04562950, Gradient norm: 1.02366936
INFO:root:[  419] Training loss: 0.04547717, Validation loss: 0.04537947, Gradient norm: 1.11877244
INFO:root:[  420] Training loss: 0.04536929, Validation loss: 0.04541965, Gradient norm: 0.95829604
INFO:root:[  421] Training loss: 0.04557445, Validation loss: 0.04596526, Gradient norm: 0.95043169
INFO:root:[  422] Training loss: 0.04557158, Validation loss: 0.04618877, Gradient norm: 1.00180930
INFO:root:[  423] Training loss: 0.04558046, Validation loss: 0.04535896, Gradient norm: 1.13523417
INFO:root:[  424] Training loss: 0.04545972, Validation loss: 0.04548840, Gradient norm: 1.34017510
INFO:root:[  425] Training loss: 0.04520715, Validation loss: 0.04503525, Gradient norm: 1.04036303
INFO:root:[  426] Training loss: 0.04499841, Validation loss: 0.04526218, Gradient norm: 1.16472268
INFO:root:[  427] Training loss: 0.04491252, Validation loss: 0.04495301, Gradient norm: 1.16172449
INFO:root:[  428] Training loss: 0.04478700, Validation loss: 0.04507537, Gradient norm: 1.13645137
INFO:root:[  429] Training loss: 0.04466113, Validation loss: 0.04460230, Gradient norm: 1.13357113
INFO:root:[  430] Training loss: 0.04461444, Validation loss: 0.04456826, Gradient norm: 1.17898149
INFO:root:[  431] Training loss: 0.04460737, Validation loss: 0.04455440, Gradient norm: 1.20302890
INFO:root:[  432] Training loss: 0.04460013, Validation loss: 0.04480470, Gradient norm: 1.26123146
INFO:root:[  433] Training loss: 0.04438097, Validation loss: 0.04461212, Gradient norm: 1.06978188
INFO:root:[  434] Training loss: 0.04448552, Validation loss: 0.04479583, Gradient norm: 1.32230400
INFO:root:[  435] Training loss: 0.04434427, Validation loss: 0.04427442, Gradient norm: 1.21924054
INFO:root:[  436] Training loss: 0.04438881, Validation loss: 0.04454532, Gradient norm: 1.21141607
INFO:root:[  437] Training loss: 0.04436828, Validation loss: 0.04417940, Gradient norm: 1.47392121
INFO:root:[  438] Training loss: 0.04431546, Validation loss: 0.04425557, Gradient norm: 1.35445880
INFO:root:[  439] Training loss: 0.04398156, Validation loss: 0.04411084, Gradient norm: 0.97962836
INFO:root:[  440] Training loss: 0.04405159, Validation loss: 0.04440879, Gradient norm: 1.21937311
INFO:root:[  441] Training loss: 0.04403474, Validation loss: 0.04398970, Gradient norm: 1.55290323
INFO:root:[  442] Training loss: 0.04387697, Validation loss: 0.04373263, Gradient norm: 1.25512523
INFO:root:[  443] Training loss: 0.04392080, Validation loss: 0.04440048, Gradient norm: 1.49166825
INFO:root:[  444] Training loss: 0.04388664, Validation loss: 0.04376979, Gradient norm: 1.48802126
INFO:root:[  445] Training loss: 0.04373147, Validation loss: 0.04368403, Gradient norm: 1.41640680
INFO:root:[  446] Training loss: 0.04361424, Validation loss: 0.04371777, Gradient norm: 1.27456874
INFO:root:[  447] Training loss: 0.04357771, Validation loss: 0.04357228, Gradient norm: 1.33153001
INFO:root:[  448] Training loss: 0.04345415, Validation loss: 0.04330096, Gradient norm: 1.32172076
INFO:root:[  449] Training loss: 0.04329405, Validation loss: 0.04321015, Gradient norm: 1.08448079
INFO:root:[  450] Training loss: 0.04328729, Validation loss: 0.04329957, Gradient norm: 1.26329303
INFO:root:[  451] Training loss: 0.04333350, Validation loss: 0.04354060, Gradient norm: 1.13733456
INFO:root:[  452] Training loss: 0.04333128, Validation loss: 0.04333538, Gradient norm: 1.25174812
INFO:root:[  453] Training loss: 0.04305977, Validation loss: 0.04329877, Gradient norm: 1.38668082
INFO:root:[  454] Training loss: 0.04293960, Validation loss: 0.04318696, Gradient norm: 1.13569623
INFO:root:[  455] Training loss: 0.04303444, Validation loss: 0.04283763, Gradient norm: 1.61746927
INFO:root:[  456] Training loss: 0.04285663, Validation loss: 0.04275488, Gradient norm: 1.50066649
INFO:root:[  457] Training loss: 0.04288061, Validation loss: 0.04349599, Gradient norm: 1.51983199
INFO:root:[  458] Training loss: 0.04304627, Validation loss: 0.04313891, Gradient norm: 1.60123626
INFO:root:[  459] Training loss: 0.04353860, Validation loss: 0.04278175, Gradient norm: 1.19272660
INFO:root:[  460] Training loss: 0.04351188, Validation loss: 0.04413642, Gradient norm: 1.28416108
INFO:root:[  461] Training loss: 0.04317490, Validation loss: 0.04275410, Gradient norm: 1.06044693
INFO:root:[  462] Training loss: 0.04256729, Validation loss: 0.04228553, Gradient norm: 1.05217565
INFO:root:[  463] Training loss: 0.04236407, Validation loss: 0.04238308, Gradient norm: 1.08302970
INFO:root:[  464] Training loss: 0.04232542, Validation loss: 0.04235341, Gradient norm: 1.13330401
INFO:root:[  465] Training loss: 0.04217419, Validation loss: 0.04228292, Gradient norm: 1.09611006
INFO:root:[  466] Training loss: 0.04215412, Validation loss: 0.04218456, Gradient norm: 1.15673691
INFO:root:[  467] Training loss: 0.04207996, Validation loss: 0.04233738, Gradient norm: 1.08649601
INFO:root:[  468] Training loss: 0.04204141, Validation loss: 0.04231266, Gradient norm: 1.21393417
INFO:root:[  469] Training loss: 0.04196005, Validation loss: 0.04214419, Gradient norm: 1.20747526
INFO:root:[  470] Training loss: 0.04192598, Validation loss: 0.04207047, Gradient norm: 1.44191759
INFO:root:[  471] Training loss: 0.04201626, Validation loss: 0.04175854, Gradient norm: 1.84650467
INFO:root:[  472] Training loss: 0.04194544, Validation loss: 0.04224473, Gradient norm: 1.81582557
INFO:root:[  473] Training loss: 0.04183200, Validation loss: 0.04202458, Gradient norm: 1.68891263
INFO:root:[  474] Training loss: 0.04168298, Validation loss: 0.04157519, Gradient norm: 1.48227107
INFO:root:[  475] Training loss: 0.04166770, Validation loss: 0.04154912, Gradient norm: 1.31265943
INFO:root:[  476] Training loss: 0.04162315, Validation loss: 0.04167991, Gradient norm: 1.20792284
INFO:root:[  477] Training loss: 0.04152462, Validation loss: 0.04149153, Gradient norm: 1.08481670
INFO:root:[  478] Training loss: 0.04152733, Validation loss: 0.04162644, Gradient norm: 1.50957254
INFO:root:[  479] Training loss: 0.04139764, Validation loss: 0.04159246, Gradient norm: 1.77557298
INFO:root:[  480] Training loss: 0.04139196, Validation loss: 0.04137836, Gradient norm: 1.75145798
INFO:root:[  481] Training loss: 0.04138645, Validation loss: 0.04161476, Gradient norm: 1.74640834
INFO:root:[  482] Training loss: 0.04158875, Validation loss: 0.04130492, Gradient norm: 1.42873978
INFO:root:[  483] Training loss: 0.04113203, Validation loss: 0.04114357, Gradient norm: 1.29165861
INFO:root:[  484] Training loss: 0.04098544, Validation loss: 0.04112608, Gradient norm: 1.25823022
INFO:root:[  485] Training loss: 0.04088747, Validation loss: 0.04126776, Gradient norm: 1.18676856
INFO:root:[  486] Training loss: 0.04098662, Validation loss: 0.04125533, Gradient norm: 1.33922744
INFO:root:[  487] Training loss: 0.04115164, Validation loss: 0.04107757, Gradient norm: 1.85627631
INFO:root:[  488] Training loss: 0.04116456, Validation loss: 0.04099175, Gradient norm: 1.91752907
INFO:root:[  489] Training loss: 0.04106725, Validation loss: 0.04084618, Gradient norm: 1.55537228
INFO:root:[  490] Training loss: 0.04091471, Validation loss: 0.04048258, Gradient norm: 1.19912089
INFO:root:[  491] Training loss: 0.04076145, Validation loss: 0.04092474, Gradient norm: 1.47634754
INFO:root:[  492] Training loss: 0.04053138, Validation loss: 0.04067837, Gradient norm: 1.37908117
INFO:root:[  493] Training loss: 0.04042379, Validation loss: 0.04081492, Gradient norm: 1.25444965
INFO:root:[  494] Training loss: 0.04037132, Validation loss: 0.04060515, Gradient norm: 1.41722977
INFO:root:[  495] Training loss: 0.04031413, Validation loss: 0.04060310, Gradient norm: 1.42997011
INFO:root:[  496] Training loss: 0.04027938, Validation loss: 0.04041481, Gradient norm: 1.33141638
INFO:root:[  497] Training loss: 0.04031991, Validation loss: 0.04102793, Gradient norm: 1.56284915
INFO:root:[  498] Training loss: 0.04037487, Validation loss: 0.04025933, Gradient norm: 1.65081735
INFO:root:[  499] Training loss: 0.04024715, Validation loss: 0.04026039, Gradient norm: 1.34013021
INFO:root:[  500] Training loss: 0.04027959, Validation loss: 0.04054791, Gradient norm: 1.94073238
INFO:root:[  501] Training loss: 0.04007033, Validation loss: 0.04005984, Gradient norm: 1.75240232
INFO:root:[  502] Training loss: 0.03989670, Validation loss: 0.04003976, Gradient norm: 1.35527790
INFO:root:[  503] Training loss: 0.03988897, Validation loss: 0.04010708, Gradient norm: 1.40308276
INFO:root:[  504] Training loss: 0.04011478, Validation loss: 0.04004864, Gradient norm: 1.95710590
INFO:root:[  505] Training loss: 0.03992311, Validation loss: 0.03971084, Gradient norm: 1.90247927
INFO:root:[  506] Training loss: 0.03980060, Validation loss: 0.03998511, Gradient norm: 1.63551549
INFO:root:[  507] Training loss: 0.04011916, Validation loss: 0.04018655, Gradient norm: 1.23237058
INFO:root:[  508] Training loss: 0.04023572, Validation loss: 0.03969505, Gradient norm: 1.39615636
INFO:root:[  509] Training loss: 0.03977957, Validation loss: 0.03986818, Gradient norm: 1.25491768
INFO:root:[  510] Training loss: 0.03969472, Validation loss: 0.04002527, Gradient norm: 1.14907004
INFO:root:[  511] Training loss: 0.03963867, Validation loss: 0.03964882, Gradient norm: 1.29082978
INFO:root:[  512] Training loss: 0.03933302, Validation loss: 0.03923440, Gradient norm: 1.31070272
INFO:root:[  513] Training loss: 0.03931012, Validation loss: 0.03916982, Gradient norm: 1.43328595
INFO:root:[  514] Training loss: 0.03948991, Validation loss: 0.03915021, Gradient norm: 1.68368097
INFO:root:[  515] Training loss: 0.03921208, Validation loss: 0.03925717, Gradient norm: 1.34382008
INFO:root:[  516] Training loss: 0.03921739, Validation loss: 0.03909590, Gradient norm: 1.47134250
INFO:root:[  517] Training loss: 0.03906300, Validation loss: 0.03916195, Gradient norm: 1.34635660
INFO:root:[  518] Training loss: 0.03898687, Validation loss: 0.03902471, Gradient norm: 1.43351890
INFO:root:[  519] Training loss: 0.03904810, Validation loss: 0.03906718, Gradient norm: 1.53384691
INFO:root:[  520] Training loss: 0.03897683, Validation loss: 0.03889600, Gradient norm: 1.52676129
INFO:root:[  521] Training loss: 0.03876846, Validation loss: 0.03888749, Gradient norm: 1.39259451
INFO:root:[  522] Training loss: 0.03877634, Validation loss: 0.03937730, Gradient norm: 1.47543922
INFO:root:[  523] Training loss: 0.03886409, Validation loss: 0.03903844, Gradient norm: 1.76579659
INFO:root:[  524] Training loss: 0.03879963, Validation loss: 0.03864889, Gradient norm: 1.43328481
INFO:root:[  525] Training loss: 0.03868517, Validation loss: 0.03882125, Gradient norm: 1.48562886
INFO:root:[  526] Training loss: 0.03868961, Validation loss: 0.03857728, Gradient norm: 1.62893171
INFO:root:[  527] Training loss: 0.03866514, Validation loss: 0.03846016, Gradient norm: 1.37631870
INFO:root:[  528] Training loss: 0.03870628, Validation loss: 0.03918011, Gradient norm: 1.40092724
INFO:root:[  529] Training loss: 0.03860035, Validation loss: 0.03850327, Gradient norm: 1.35646521
INFO:root:[  530] Training loss: 0.03863709, Validation loss: 0.03868722, Gradient norm: 1.48725426
INFO:root:[  531] Training loss: 0.03837573, Validation loss: 0.03844215, Gradient norm: 1.48062560
INFO:root:[  532] Training loss: 0.03823958, Validation loss: 0.03819644, Gradient norm: 1.55941613
INFO:root:[  533] Training loss: 0.03829951, Validation loss: 0.03827985, Gradient norm: 1.25291147
INFO:root:[  534] Training loss: 0.03820568, Validation loss: 0.03808011, Gradient norm: 1.54319267
INFO:root:[  535] Training loss: 0.03810459, Validation loss: 0.03819127, Gradient norm: 1.57353834
INFO:root:[  536] Training loss: 0.03799530, Validation loss: 0.03826982, Gradient norm: 1.43821600
INFO:root:[  537] Training loss: 0.03803877, Validation loss: 0.03823133, Gradient norm: 1.65725017
INFO:root:[  538] Training loss: 0.03789044, Validation loss: 0.03775580, Gradient norm: 1.55402799
INFO:root:[  539] Training loss: 0.03780572, Validation loss: 0.03799203, Gradient norm: 1.49203343
INFO:root:[  540] Training loss: 0.03775792, Validation loss: 0.03790170, Gradient norm: 1.36450209
INFO:root:[  541] Training loss: 0.03777368, Validation loss: 0.03760171, Gradient norm: 1.41844870
INFO:root:[  542] Training loss: 0.03771056, Validation loss: 0.03800486, Gradient norm: 1.45098032
INFO:root:[  543] Training loss: 0.03778813, Validation loss: 0.03786742, Gradient norm: 1.70662102
INFO:root:[  544] Training loss: 0.03777420, Validation loss: 0.03778998, Gradient norm: 1.55231783
INFO:root:[  545] Training loss: 0.03824567, Validation loss: 0.03891151, Gradient norm: 1.39541573
INFO:root:[  546] Training loss: 0.03784625, Validation loss: 0.03761066, Gradient norm: 1.84799569
INFO:root:[  547] Training loss: 0.03754390, Validation loss: 0.03745087, Gradient norm: 1.74649314
INFO:root:[  548] Training loss: 0.03750773, Validation loss: 0.03763370, Gradient norm: 1.72271307
INFO:root:[  549] Training loss: 0.03740948, Validation loss: 0.03755848, Gradient norm: 1.47408258
INFO:root:[  550] Training loss: 0.03741541, Validation loss: 0.03749459, Gradient norm: 1.52769194
INFO:root:[  551] Training loss: 0.03737237, Validation loss: 0.03726774, Gradient norm: 1.53072972
INFO:root:[  552] Training loss: 0.03720598, Validation loss: 0.03772098, Gradient norm: 1.58373724
INFO:root:[  553] Training loss: 0.03715523, Validation loss: 0.03692913, Gradient norm: 1.68422278
INFO:root:[  554] Training loss: 0.03716709, Validation loss: 0.03724627, Gradient norm: 1.63157833
INFO:root:[  555] Training loss: 0.03709690, Validation loss: 0.03706611, Gradient norm: 1.69054781
INFO:root:[  556] Training loss: 0.03700829, Validation loss: 0.03731498, Gradient norm: 1.71491401
INFO:root:[  557] Training loss: 0.03689838, Validation loss: 0.03687332, Gradient norm: 1.79838504
INFO:root:[  558] Training loss: 0.03695786, Validation loss: 0.03706305, Gradient norm: 1.68363908
INFO:root:[  559] Training loss: 0.03688481, Validation loss: 0.03681678, Gradient norm: 1.62132671
INFO:root:[  560] Training loss: 0.03686218, Validation loss: 0.03687523, Gradient norm: 1.54905821
INFO:root:[  561] Training loss: 0.03677555, Validation loss: 0.03690168, Gradient norm: 1.87976261
INFO:root:[  562] Training loss: 0.03660035, Validation loss: 0.03684548, Gradient norm: 1.46599882
INFO:root:[  563] Training loss: 0.03661605, Validation loss: 0.03676030, Gradient norm: 1.65742011
INFO:root:[  564] Training loss: 0.03662410, Validation loss: 0.03655440, Gradient norm: 1.73582471
INFO:root:[  565] Training loss: 0.03664027, Validation loss: 0.03654675, Gradient norm: 1.68608225
INFO:root:[  566] Training loss: 0.03657586, Validation loss: 0.03671927, Gradient norm: 1.66931582
INFO:root:[  567] Training loss: 0.03655955, Validation loss: 0.03678068, Gradient norm: 1.61159996
INFO:root:[  568] Training loss: 0.03635978, Validation loss: 0.03649312, Gradient norm: 1.71245736
INFO:root:[  569] Training loss: 0.03635789, Validation loss: 0.03647056, Gradient norm: 1.76732660
INFO:root:[  570] Training loss: 0.03627546, Validation loss: 0.03613115, Gradient norm: 1.39049628
INFO:root:[  571] Training loss: 0.03631411, Validation loss: 0.03610950, Gradient norm: 1.61449759
INFO:root:[  572] Training loss: 0.03613573, Validation loss: 0.03613095, Gradient norm: 1.54118254
INFO:root:[  573] Training loss: 0.03622947, Validation loss: 0.03654777, Gradient norm: 1.91187978
INFO:root:[  574] Training loss: 0.03606726, Validation loss: 0.03640311, Gradient norm: 1.80597977
INFO:root:[  575] Training loss: 0.03609924, Validation loss: 0.03603961, Gradient norm: 1.94495019
INFO:root:[  576] Training loss: 0.03592534, Validation loss: 0.03595135, Gradient norm: 1.47378221
INFO:root:[  577] Training loss: 0.03607536, Validation loss: 0.03615277, Gradient norm: 1.39055692
INFO:root:[  578] Training loss: 0.03615144, Validation loss: 0.03578424, Gradient norm: 1.67571766
INFO:root:[  579] Training loss: 0.03595432, Validation loss: 0.03632030, Gradient norm: 1.42932498
INFO:root:[  580] Training loss: 0.03596380, Validation loss: 0.03589251, Gradient norm: 1.86646548
INFO:root:[  581] Training loss: 0.03577945, Validation loss: 0.03587402, Gradient norm: 1.66175880
INFO:root:[  582] Training loss: 0.03568346, Validation loss: 0.03567643, Gradient norm: 1.61847035
INFO:root:[  583] Training loss: 0.03570677, Validation loss: 0.03590545, Gradient norm: 1.92870299
INFO:root:[  584] Training loss: 0.03567289, Validation loss: 0.03578728, Gradient norm: 1.80190723
INFO:root:[  585] Training loss: 0.03564033, Validation loss: 0.03584258, Gradient norm: 1.66891555
INFO:root:[  586] Training loss: 0.03571539, Validation loss: 0.03605304, Gradient norm: 2.23929302
INFO:root:[  587] Training loss: 0.03559798, Validation loss: 0.03588296, Gradient norm: 2.26215053
INFO:root:[  588] Training loss: 0.03551004, Validation loss: 0.03534164, Gradient norm: 2.28108892
INFO:root:[  589] Training loss: 0.03544495, Validation loss: 0.03574710, Gradient norm: 1.67162258
INFO:root:[  590] Training loss: 0.03573353, Validation loss: 0.03607744, Gradient norm: 1.98527042
INFO:root:[  591] Training loss: 0.03546813, Validation loss: 0.03530209, Gradient norm: 1.45501112
INFO:root:[  592] Training loss: 0.03543998, Validation loss: 0.03569863, Gradient norm: 1.72138712
INFO:root:[  593] Training loss: 0.03524151, Validation loss: 0.03536129, Gradient norm: 1.86782276
INFO:root:[  594] Training loss: 0.03518970, Validation loss: 0.03519606, Gradient norm: 2.08671527
INFO:root:[  595] Training loss: 0.03520242, Validation loss: 0.03518379, Gradient norm: 1.37403837
INFO:root:[  596] Training loss: 0.03518382, Validation loss: 0.03580639, Gradient norm: 1.73925781
INFO:root:[  597] Training loss: 0.03517185, Validation loss: 0.03503975, Gradient norm: 2.08558786
INFO:root:[  598] Training loss: 0.03502448, Validation loss: 0.03519997, Gradient norm: 1.83512225
INFO:root:[  599] Training loss: 0.03492461, Validation loss: 0.03517147, Gradient norm: 1.81349330
INFO:root:[  600] Training loss: 0.03492178, Validation loss: 0.03487788, Gradient norm: 1.89807344
INFO:root:[  601] Training loss: 0.03493142, Validation loss: 0.03494494, Gradient norm: 1.99864107
INFO:root:[  602] Training loss: 0.03494389, Validation loss: 0.03488401, Gradient norm: 2.37659734
INFO:root:[  603] Training loss: 0.03478097, Validation loss: 0.03471093, Gradient norm: 2.13198875
INFO:root:[  604] Training loss: 0.03479480, Validation loss: 0.03508007, Gradient norm: 2.22839060
INFO:root:[  605] Training loss: 0.03480403, Validation loss: 0.03510187, Gradient norm: 2.00311067
INFO:root:[  606] Training loss: 0.03473511, Validation loss: 0.03486921, Gradient norm: 2.29199383
INFO:root:[  607] Training loss: 0.03471098, Validation loss: 0.03469408, Gradient norm: 1.75617441
INFO:root:[  608] Training loss: 0.03482267, Validation loss: 0.03472390, Gradient norm: 2.12314171
INFO:root:[  609] Training loss: 0.03451780, Validation loss: 0.03475161, Gradient norm: 1.72612280
INFO:root:[  610] Training loss: 0.03458853, Validation loss: 0.03489469, Gradient norm: 1.72656648
INFO:root:[  611] Training loss: 0.03453156, Validation loss: 0.03465288, Gradient norm: 2.19915276
INFO:root:[  612] Training loss: 0.03453090, Validation loss: 0.03449040, Gradient norm: 2.01615510
INFO:root:[  613] Training loss: 0.03447471, Validation loss: 0.03446717, Gradient norm: 1.77874057
INFO:root:[  614] Training loss: 0.03448202, Validation loss: 0.03441785, Gradient norm: 2.21558431
INFO:root:[  615] Training loss: 0.03430150, Validation loss: 0.03412332, Gradient norm: 2.02759095
INFO:root:[  616] Training loss: 0.03424102, Validation loss: 0.03446760, Gradient norm: 1.81329975
INFO:root:[  617] Training loss: 0.03421440, Validation loss: 0.03451485, Gradient norm: 1.90423150
INFO:root:[  618] Training loss: 0.03417934, Validation loss: 0.03409501, Gradient norm: 1.90013178
INFO:root:[  619] Training loss: 0.03413175, Validation loss: 0.03432720, Gradient norm: 1.50815075
INFO:root:[  620] Training loss: 0.03407511, Validation loss: 0.03400584, Gradient norm: 1.80100005
INFO:root:[  621] Training loss: 0.03409317, Validation loss: 0.03413510, Gradient norm: 2.13026665
INFO:root:[  622] Training loss: 0.03408701, Validation loss: 0.03384647, Gradient norm: 2.57762568
INFO:root:[  623] Training loss: 0.03398154, Validation loss: 0.03416714, Gradient norm: 1.99603878
INFO:root:[  624] Training loss: 0.03408137, Validation loss: 0.03407885, Gradient norm: 2.23445126
INFO:root:[  625] Training loss: 0.03388329, Validation loss: 0.03424566, Gradient norm: 1.91095064
INFO:root:[  626] Training loss: 0.03383565, Validation loss: 0.03422339, Gradient norm: 2.04502411
INFO:root:[  627] Training loss: 0.03395652, Validation loss: 0.03376235, Gradient norm: 2.50544740
INFO:root:[  628] Training loss: 0.03392521, Validation loss: 0.03396384, Gradient norm: 2.21247823
INFO:root:[  629] Training loss: 0.03378802, Validation loss: 0.03393496, Gradient norm: 1.90220139
INFO:root:[  630] Training loss: 0.03378837, Validation loss: 0.03378494, Gradient norm: 2.09916847
INFO:root:[  631] Training loss: 0.03371180, Validation loss: 0.03403110, Gradient norm: 2.14677249
INFO:root:[  632] Training loss: 0.03363773, Validation loss: 0.03349787, Gradient norm: 2.21359781
INFO:root:[  633] Training loss: 0.03355606, Validation loss: 0.03366858, Gradient norm: 1.74620854
INFO:root:[  634] Training loss: 0.03349090, Validation loss: 0.03359477, Gradient norm: 1.92358516
INFO:root:[  635] Training loss: 0.03356432, Validation loss: 0.03384266, Gradient norm: 2.39839078
INFO:root:[  636] Training loss: 0.03348040, Validation loss: 0.03332682, Gradient norm: 2.54616164
INFO:root:[  637] Training loss: 0.03347526, Validation loss: 0.03361514, Gradient norm: 2.05942582
INFO:root:[  638] Training loss: 0.03334986, Validation loss: 0.03335443, Gradient norm: 2.12598203
INFO:root:[  639] Training loss: 0.03329503, Validation loss: 0.03333473, Gradient norm: 2.16492089
INFO:root:[  640] Training loss: 0.03325769, Validation loss: 0.03379428, Gradient norm: 2.00334057
INFO:root:[  641] Training loss: 0.03337939, Validation loss: 0.03317356, Gradient norm: 2.58919087
INFO:root:[  642] Training loss: 0.03338925, Validation loss: 0.03330653, Gradient norm: 2.32359458
INFO:root:[  643] Training loss: 0.03349413, Validation loss: 0.03377430, Gradient norm: 1.88899999
INFO:root:[  644] Training loss: 0.03357511, Validation loss: 0.03458979, Gradient norm: 1.74451197
INFO:root:[  645] Training loss: 0.03376688, Validation loss: 0.03312502, Gradient norm: 2.13726404
INFO:root:[  646] Training loss: 0.03330317, Validation loss: 0.03304209, Gradient norm: 1.95400213
INFO:root:[  647] Training loss: 0.03304235, Validation loss: 0.03331973, Gradient norm: 2.00871714
INFO:root:[  648] Training loss: 0.03305047, Validation loss: 0.03302507, Gradient norm: 2.02168843
INFO:root:[  649] Training loss: 0.03306637, Validation loss: 0.03325576, Gradient norm: 2.04002283
INFO:root:[  650] Training loss: 0.03303813, Validation loss: 0.03297023, Gradient norm: 2.53143300
INFO:root:[  651] Training loss: 0.03291731, Validation loss: 0.03281133, Gradient norm: 2.22890418
INFO:root:[  652] Training loss: 0.03286927, Validation loss: 0.03272019, Gradient norm: 1.85543031
INFO:root:[  653] Training loss: 0.03283211, Validation loss: 0.03285376, Gradient norm: 2.37528572
INFO:root:[  654] Training loss: 0.03292779, Validation loss: 0.03323748, Gradient norm: 2.76788868
INFO:root:[  655] Training loss: 0.03289405, Validation loss: 0.03305454, Gradient norm: 2.56985237
INFO:root:[  656] Training loss: 0.03271035, Validation loss: 0.03266677, Gradient norm: 2.52089346
INFO:root:[  657] Training loss: 0.03265243, Validation loss: 0.03256667, Gradient norm: 1.88104885
INFO:root:[  658] Training loss: 0.03263672, Validation loss: 0.03308245, Gradient norm: 1.51794708
INFO:root:[  659] Training loss: 0.03280559, Validation loss: 0.03307378, Gradient norm: 2.47086863
INFO:root:[  660] Training loss: 0.03272823, Validation loss: 0.03283606, Gradient norm: 2.75070755
INFO:root:[  661] Training loss: 0.03262533, Validation loss: 0.03263270, Gradient norm: 2.07592043
INFO:root:[  662] Training loss: 0.03248554, Validation loss: 0.03253041, Gradient norm: 1.69236511
INFO:root:[  663] Training loss: 0.03241638, Validation loss: 0.03246751, Gradient norm: 1.93722019
INFO:root:[  664] Training loss: 0.03249838, Validation loss: 0.03277697, Gradient norm: 2.42796113
INFO:root:[  665] Training loss: 0.03246673, Validation loss: 0.03281635, Gradient norm: 2.12952460
INFO:root:[  666] Training loss: 0.03253246, Validation loss: 0.03285128, Gradient norm: 2.74877638
INFO:root:[  667] Training loss: 0.03237446, Validation loss: 0.03249100, Gradient norm: 2.43251810
INFO:root:[  668] Training loss: 0.03232867, Validation loss: 0.03226765, Gradient norm: 2.35788316
INFO:root:[  669] Training loss: 0.03237476, Validation loss: 0.03261042, Gradient norm: 2.45411452
INFO:root:[  670] Training loss: 0.03217632, Validation loss: 0.03260832, Gradient norm: 1.97749977
INFO:root:[  671] Training loss: 0.03235115, Validation loss: 0.03281696, Gradient norm: 2.88138276
INFO:root:[  672] Training loss: 0.03238545, Validation loss: 0.03232487, Gradient norm: 2.46170669
INFO:root:[  673] Training loss: 0.03218208, Validation loss: 0.03222444, Gradient norm: 2.25793829
INFO:root:[  674] Training loss: 0.03207423, Validation loss: 0.03222680, Gradient norm: 2.35337045
INFO:root:[  675] Training loss: 0.03213757, Validation loss: 0.03192879, Gradient norm: 2.67164608
INFO:root:[  676] Training loss: 0.03207101, Validation loss: 0.03257816, Gradient norm: 2.68559531
INFO:root:[  677] Training loss: 0.03208037, Validation loss: 0.03216946, Gradient norm: 3.12318798
INFO:root:[  678] Training loss: 0.03194250, Validation loss: 0.03221196, Gradient norm: 2.52445009
INFO:root:[  679] Training loss: 0.03200586, Validation loss: 0.03209132, Gradient norm: 2.50357449
INFO:root:[  680] Training loss: 0.03189530, Validation loss: 0.03201649, Gradient norm: 2.18842480
INFO:root:[  681] Training loss: 0.03191926, Validation loss: 0.03194202, Gradient norm: 2.55081848
INFO:root:[  682] Training loss: 0.03188290, Validation loss: 0.03194864, Gradient norm: 2.51006731
INFO:root:[  683] Training loss: 0.03181503, Validation loss: 0.03200992, Gradient norm: 2.14034034
INFO:root:[  684] Training loss: 0.03191463, Validation loss: 0.03196339, Gradient norm: 2.87533339
INFO:root:EP 684: Early stopping
INFO:root:Training the model took 31582.004s.
INFO:root:Emptying the cuda cache took 0.109s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.36125
INFO:root:EnergyScoreTrain: 0.99942
INFO:root:CoverageTrain: 0.98417
INFO:root:IntervalWidthTrain: 0.07329
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.30874
INFO:root:EnergyScoreValidation: 0.96161
INFO:root:CoverageValidation: 0.98428
INFO:root:IntervalWidthValidation: 0.07364
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.07701
INFO:root:EnergyScoreTest: 0.79054
INFO:root:CoverageTest: 0.98421
INFO:root:IntervalWidthTest: 0.07268
INFO:root:###4 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 2329935872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.48244761, Validation loss: 0.32613887, Gradient norm: 6.24149491
INFO:root:[    2] Training loss: 0.30277480, Validation loss: 0.29967235, Gradient norm: 1.55639518
INFO:root:[    3] Training loss: 0.28671418, Validation loss: 0.27888240, Gradient norm: 1.59988956
INFO:root:[    4] Training loss: 0.26683270, Validation loss: 0.25084020, Gradient norm: 1.75731434
INFO:root:[    5] Training loss: 0.25644205, Validation loss: 0.24729336, Gradient norm: 1.65890647
INFO:root:[    6] Training loss: 0.24665652, Validation loss: 0.24361756, Gradient norm: 1.66120309
INFO:root:[    7] Training loss: 0.23423149, Validation loss: 0.22802899, Gradient norm: 1.06144295
INFO:root:[    8] Training loss: 0.22825177, Validation loss: 0.22997416, Gradient norm: 1.23708643
INFO:root:[    9] Training loss: 0.22671113, Validation loss: 0.22473178, Gradient norm: 1.61614864
INFO:root:[   10] Training loss: 0.22419977, Validation loss: 0.21199924, Gradient norm: 1.96948420
INFO:root:[   11] Training loss: 0.21707273, Validation loss: 0.21898114, Gradient norm: 1.90399151
INFO:root:[   12] Training loss: 0.20901377, Validation loss: 0.20657704, Gradient norm: 1.85703107
INFO:root:[   13] Training loss: 0.20360344, Validation loss: 0.20634221, Gradient norm: 1.21572250
INFO:root:[   14] Training loss: 0.20072277, Validation loss: 0.20045593, Gradient norm: 1.54812612
INFO:root:[   15] Training loss: 0.19865741, Validation loss: 0.19395133, Gradient norm: 1.64593822
INFO:root:[   16] Training loss: 0.18825829, Validation loss: 0.19202384, Gradient norm: 1.27671318
INFO:root:[   17] Training loss: 0.18646548, Validation loss: 0.18271471, Gradient norm: 1.25486689
INFO:root:[   18] Training loss: 0.18267854, Validation loss: 0.17725267, Gradient norm: 1.53528079
INFO:root:[   19] Training loss: 0.17987589, Validation loss: 0.17692018, Gradient norm: 1.35059386
INFO:root:[   20] Training loss: 0.17991250, Validation loss: 0.17402610, Gradient norm: 1.94767351
INFO:root:[   21] Training loss: 0.17620498, Validation loss: 0.17137908, Gradient norm: 2.08202450
INFO:root:[   22] Training loss: 0.16926848, Validation loss: 0.16896588, Gradient norm: 0.88457386
INFO:root:[   23] Training loss: 0.16774391, Validation loss: 0.16386677, Gradient norm: 1.56973772
INFO:root:[   24] Training loss: 0.16588827, Validation loss: 0.17110990, Gradient norm: 1.29719994
INFO:root:[   25] Training loss: 0.16383721, Validation loss: 0.16612379, Gradient norm: 1.75348147
INFO:root:[   26] Training loss: 0.16054427, Validation loss: 0.16024601, Gradient norm: 1.19928883
INFO:root:[   27] Training loss: 0.15917580, Validation loss: 0.16047491, Gradient norm: 1.66098025
INFO:root:[   28] Training loss: 0.15652466, Validation loss: 0.16314299, Gradient norm: 1.14871364
INFO:root:[   29] Training loss: 0.15821786, Validation loss: 0.15367078, Gradient norm: 2.31013610
INFO:root:[   30] Training loss: 0.15299813, Validation loss: 0.15786713, Gradient norm: 1.50858629
INFO:root:[   31] Training loss: 0.15314652, Validation loss: 0.15123349, Gradient norm: 1.48495131
INFO:root:[   32] Training loss: 0.14998468, Validation loss: 0.14772960, Gradient norm: 1.09210343
INFO:root:[   33] Training loss: 0.14831922, Validation loss: 0.14781297, Gradient norm: 1.01341339
INFO:root:[   34] Training loss: 0.14537822, Validation loss: 0.14544162, Gradient norm: 0.89568284
INFO:root:[   35] Training loss: 0.14345690, Validation loss: 0.15412364, Gradient norm: 1.16501117
INFO:root:[   36] Training loss: 0.14349477, Validation loss: 0.14123178, Gradient norm: 1.64473810
INFO:root:[   37] Training loss: 0.14164979, Validation loss: 0.13908198, Gradient norm: 1.41523451
INFO:root:[   38] Training loss: 0.14042781, Validation loss: 0.13845910, Gradient norm: 1.26938346
INFO:root:[   39] Training loss: 0.13756568, Validation loss: 0.13856181, Gradient norm: 1.02777557
INFO:root:[   40] Training loss: 0.13879415, Validation loss: 0.13957690, Gradient norm: 1.45827996
INFO:root:[   41] Training loss: 0.13747674, Validation loss: 0.13536583, Gradient norm: 1.51656907
INFO:root:[   42] Training loss: 0.13480286, Validation loss: 0.13369997, Gradient norm: 1.22204392
INFO:root:[   43] Training loss: 0.13382394, Validation loss: 0.13507773, Gradient norm: 0.85940256
INFO:root:[   44] Training loss: 0.13374093, Validation loss: 0.13125477, Gradient norm: 1.34152029
INFO:root:[   45] Training loss: 0.13096322, Validation loss: 0.13025932, Gradient norm: 0.91785390
INFO:root:[   46] Training loss: 0.13162748, Validation loss: 0.12886561, Gradient norm: 1.54032327
INFO:root:[   47] Training loss: 0.12891307, Validation loss: 0.12884720, Gradient norm: 1.32140198
INFO:root:[   48] Training loss: 0.12871455, Validation loss: 0.12846412, Gradient norm: 1.18434396
INFO:root:[   49] Training loss: 0.12733089, Validation loss: 0.12891141, Gradient norm: 0.92293421
INFO:root:[   50] Training loss: 0.12693682, Validation loss: 0.13321586, Gradient norm: 1.20730689
INFO:root:[   51] Training loss: 0.12712966, Validation loss: 0.12389562, Gradient norm: 1.80360496
INFO:root:[   52] Training loss: 0.12600123, Validation loss: 0.12421258, Gradient norm: 1.64154958
INFO:root:[   53] Training loss: 0.12321810, Validation loss: 0.13204439, Gradient norm: 0.94950656
INFO:root:[   54] Training loss: 0.12625331, Validation loss: 0.12226899, Gradient norm: 2.26749477
INFO:root:[   55] Training loss: 0.12122417, Validation loss: 0.12323249, Gradient norm: 0.91668928
INFO:root:[   56] Training loss: 0.12143103, Validation loss: 0.12045871, Gradient norm: 1.00773839
INFO:root:[   57] Training loss: 0.12045689, Validation loss: 0.11954306, Gradient norm: 1.09773763
INFO:root:[   58] Training loss: 0.12055233, Validation loss: 0.11907967, Gradient norm: 1.42214000
INFO:root:[   59] Training loss: 0.11820640, Validation loss: 0.11929903, Gradient norm: 0.90135662
INFO:root:[   60] Training loss: 0.11947718, Validation loss: 0.12166413, Gradient norm: 1.54693464
INFO:root:[   61] Training loss: 0.11871776, Validation loss: 0.11904148, Gradient norm: 1.38179365
INFO:root:[   62] Training loss: 0.11762562, Validation loss: 0.11945954, Gradient norm: 1.41549440
INFO:root:[   63] Training loss: 0.11784266, Validation loss: 0.11675273, Gradient norm: 1.20015030
INFO:root:[   64] Training loss: 0.11576596, Validation loss: 0.11772359, Gradient norm: 1.46041522
INFO:root:[   65] Training loss: 0.11673630, Validation loss: 0.11418925, Gradient norm: 1.81754645
INFO:root:[   66] Training loss: 0.11485155, Validation loss: 0.11513465, Gradient norm: 0.75592542
INFO:root:[   67] Training loss: 0.11388504, Validation loss: 0.11314385, Gradient norm: 1.17486082
INFO:root:[   68] Training loss: 0.11445331, Validation loss: 0.11254778, Gradient norm: 0.95984653
INFO:root:[   69] Training loss: 0.11467035, Validation loss: 0.11414621, Gradient norm: 1.59125067
INFO:root:[   70] Training loss: 0.11373170, Validation loss: 0.11265510, Gradient norm: 1.61345961
INFO:root:[   71] Training loss: 0.11249875, Validation loss: 0.11225243, Gradient norm: 1.63914488
INFO:root:[   72] Training loss: 0.11142663, Validation loss: 0.11146842, Gradient norm: 1.28254089
INFO:root:[   73] Training loss: 0.11150997, Validation loss: 0.11146282, Gradient norm: 1.64629640
INFO:root:[   74] Training loss: 0.11026871, Validation loss: 0.11004996, Gradient norm: 1.51024106
INFO:root:[   75] Training loss: 0.11082100, Validation loss: 0.11034647, Gradient norm: 1.32878474
INFO:root:[   76] Training loss: 0.11017175, Validation loss: 0.11085377, Gradient norm: 1.15153763
INFO:root:[   77] Training loss: 0.10979283, Validation loss: 0.10901970, Gradient norm: 1.50374906
INFO:root:[   78] Training loss: 0.10914203, Validation loss: 0.11078217, Gradient norm: 1.41253822
INFO:root:[   79] Training loss: 0.10865804, Validation loss: 0.10828225, Gradient norm: 1.46198187
INFO:root:[   80] Training loss: 0.10821108, Validation loss: 0.11165748, Gradient norm: 1.38144535
INFO:root:[   81] Training loss: 0.10885661, Validation loss: 0.10836605, Gradient norm: 1.48072064
INFO:root:[   82] Training loss: 0.10828342, Validation loss: 0.10759437, Gradient norm: 1.41454349
INFO:root:[   83] Training loss: 0.10756114, Validation loss: 0.10815883, Gradient norm: 1.50224284
INFO:root:[   84] Training loss: 0.10648068, Validation loss: 0.10646557, Gradient norm: 1.40179772
INFO:root:[   85] Training loss: 0.10581021, Validation loss: 0.10652402, Gradient norm: 1.23627500
INFO:root:[   86] Training loss: 0.10562578, Validation loss: 0.10538588, Gradient norm: 0.98606779
INFO:root:[   87] Training loss: 0.10494750, Validation loss: 0.10540356, Gradient norm: 1.01360883
INFO:root:[   88] Training loss: 0.10532452, Validation loss: 0.10622461, Gradient norm: 1.34138202
INFO:root:[   89] Training loss: 0.10437791, Validation loss: 0.10475558, Gradient norm: 1.40371916
INFO:root:[   90] Training loss: 0.10476859, Validation loss: 0.10420860, Gradient norm: 1.74789855
INFO:root:[   91] Training loss: 0.10430615, Validation loss: 0.10455138, Gradient norm: 1.26985108
INFO:root:[   92] Training loss: 0.10485847, Validation loss: 0.10465028, Gradient norm: 2.23174881
INFO:root:[   93] Training loss: 0.10369433, Validation loss: 0.10455407, Gradient norm: 1.59128794
INFO:root:[   94] Training loss: 0.10272259, Validation loss: 0.10258625, Gradient norm: 0.90733888
INFO:root:[   95] Training loss: 0.10277731, Validation loss: 0.10335991, Gradient norm: 1.21544002
INFO:root:[   96] Training loss: 0.10280885, Validation loss: 0.10220428, Gradient norm: 1.73501521
INFO:root:[   97] Training loss: 0.10205410, Validation loss: 0.10137098, Gradient norm: 1.24769889
INFO:root:[   98] Training loss: 0.10131104, Validation loss: 0.10195019, Gradient norm: 0.98074477
INFO:root:[   99] Training loss: 0.10132166, Validation loss: 0.10196398, Gradient norm: 0.73207276
INFO:root:[  100] Training loss: 0.10145071, Validation loss: 0.10047664, Gradient norm: 1.49282177
INFO:root:[  101] Training loss: 0.10112516, Validation loss: 0.10006682, Gradient norm: 1.79480461
INFO:root:[  102] Training loss: 0.10045205, Validation loss: 0.09973467, Gradient norm: 1.69859273
INFO:root:[  103] Training loss: 0.10024239, Validation loss: 0.10115006, Gradient norm: 1.70792142
INFO:root:[  104] Training loss: 0.09953131, Validation loss: 0.09957524, Gradient norm: 1.26635393
INFO:root:[  105] Training loss: 0.09947292, Validation loss: 0.09951647, Gradient norm: 1.27787250
INFO:root:[  106] Training loss: 0.09964695, Validation loss: 0.09817097, Gradient norm: 1.66436889
INFO:root:[  107] Training loss: 0.09963789, Validation loss: 0.09922270, Gradient norm: 1.19980620
INFO:root:[  108] Training loss: 0.09941754, Validation loss: 0.09931207, Gradient norm: 2.15653371
INFO:root:[  109] Training loss: 0.09905935, Validation loss: 0.09852250, Gradient norm: 2.07588287
INFO:root:[  110] Training loss: 0.09822191, Validation loss: 0.09696698, Gradient norm: 1.29992000
INFO:root:[  111] Training loss: 0.09730259, Validation loss: 0.09715844, Gradient norm: 0.90520008
INFO:root:[  112] Training loss: 0.09713353, Validation loss: 0.09693353, Gradient norm: 1.98653977
INFO:root:[  113] Training loss: 0.09734806, Validation loss: 0.09720880, Gradient norm: 1.54632519
INFO:root:[  114] Training loss: 0.09699954, Validation loss: 0.09767759, Gradient norm: 1.10973569
INFO:root:[  115] Training loss: 0.09645974, Validation loss: 0.09660823, Gradient norm: 1.61235887
INFO:root:[  116] Training loss: 0.09650965, Validation loss: 0.09592049, Gradient norm: 1.85013742
INFO:root:[  117] Training loss: 0.09599483, Validation loss: 0.09595699, Gradient norm: 1.33672864
INFO:root:[  118] Training loss: 0.09578166, Validation loss: 0.09568383, Gradient norm: 1.37025132
INFO:root:[  119] Training loss: 0.09595150, Validation loss: 0.09588600, Gradient norm: 1.61987089
INFO:root:[  120] Training loss: 0.09564747, Validation loss: 0.09534618, Gradient norm: 1.50940711
INFO:root:[  121] Training loss: 0.09506979, Validation loss: 0.09575280, Gradient norm: 1.09933902
INFO:root:[  122] Training loss: 0.09496833, Validation loss: 0.09505877, Gradient norm: 1.13532742
INFO:root:[  123] Training loss: 0.09518974, Validation loss: 0.09604217, Gradient norm: 1.36988949
INFO:root:[  124] Training loss: 0.09474148, Validation loss: 0.09612896, Gradient norm: 2.12311708
INFO:root:[  125] Training loss: 0.09527638, Validation loss: 0.09586213, Gradient norm: 2.32916560
INFO:root:[  126] Training loss: 0.09440678, Validation loss: 0.09435888, Gradient norm: 2.55108272
INFO:root:[  127] Training loss: 0.09389427, Validation loss: 0.09329533, Gradient norm: 1.73772392
INFO:root:[  128] Training loss: 0.09402644, Validation loss: 0.09622318, Gradient norm: 1.60238093
INFO:root:[  129] Training loss: 0.09440431, Validation loss: 0.09336260, Gradient norm: 3.59435599
INFO:root:[  130] Training loss: 0.09334774, Validation loss: 0.09430919, Gradient norm: 2.06187581
INFO:root:[  131] Training loss: 0.09311143, Validation loss: 0.09383513, Gradient norm: 1.86878464
INFO:root:[  132] Training loss: 0.09302584, Validation loss: 0.09247420, Gradient norm: 2.29567567
INFO:root:[  133] Training loss: 0.09265376, Validation loss: 0.09279951, Gradient norm: 2.24992833
INFO:root:[  134] Training loss: 0.09233480, Validation loss: 0.09242452, Gradient norm: 2.01219091
INFO:root:[  135] Training loss: 0.09220804, Validation loss: 0.09248569, Gradient norm: 1.73173377
INFO:root:[  136] Training loss: 0.09231990, Validation loss: 0.09309198, Gradient norm: 2.37819100
INFO:root:[  137] Training loss: 0.09222468, Validation loss: 0.09122272, Gradient norm: 2.24413579
INFO:root:[  138] Training loss: 0.09236766, Validation loss: 0.09157510, Gradient norm: 2.65097617
INFO:root:[  139] Training loss: 0.09188558, Validation loss: 0.09212531, Gradient norm: 2.07026188
INFO:root:[  140] Training loss: 0.09219615, Validation loss: 0.09252393, Gradient norm: 3.37976798
INFO:root:[  141] Training loss: 0.09116992, Validation loss: 0.09214365, Gradient norm: 1.50795655
INFO:root:[  142] Training loss: 0.09207411, Validation loss: 0.09155230, Gradient norm: 2.36713588
INFO:root:[  143] Training loss: 0.09126499, Validation loss: 0.09157758, Gradient norm: 2.30984840
INFO:root:[  144] Training loss: 0.09094042, Validation loss: 0.09171500, Gradient norm: 1.75239501
INFO:root:[  145] Training loss: 0.09121293, Validation loss: 0.09152783, Gradient norm: 1.60454550
INFO:root:[  146] Training loss: 0.09144451, Validation loss: 0.09219510, Gradient norm: 2.54104319
INFO:root:[  147] Training loss: 0.09129837, Validation loss: 0.09121548, Gradient norm: 3.53143716
INFO:root:[  148] Training loss: 0.09105579, Validation loss: 0.09031058, Gradient norm: 2.94822856
INFO:root:[  149] Training loss: 0.09096706, Validation loss: 0.09133959, Gradient norm: 3.87834359
INFO:root:[  150] Training loss: 0.09092537, Validation loss: 0.09377440, Gradient norm: 3.05551768
INFO:root:[  151] Training loss: 0.09142905, Validation loss: 0.09117810, Gradient norm: 4.75622233
INFO:root:[  152] Training loss: 0.09060351, Validation loss: 0.08997273, Gradient norm: 3.79755124
INFO:root:[  153] Training loss: 0.09029918, Validation loss: 0.08985165, Gradient norm: 3.04376718
INFO:root:[  154] Training loss: 0.09036177, Validation loss: 0.08958205, Gradient norm: 1.84536092
INFO:root:[  155] Training loss: 0.09097959, Validation loss: 0.09015901, Gradient norm: 4.34428179
INFO:root:[  156] Training loss: 0.09045044, Validation loss: 0.09020525, Gradient norm: 3.90363258
INFO:root:[  157] Training loss: 0.09001010, Validation loss: 0.09069477, Gradient norm: 3.66895490
INFO:root:[  158] Training loss: 0.08977354, Validation loss: 0.08977945, Gradient norm: 3.33813848
INFO:root:[  159] Training loss: 0.08992457, Validation loss: 0.08989397, Gradient norm: 4.69083071
INFO:root:[  160] Training loss: 0.08938685, Validation loss: 0.08982478, Gradient norm: 2.47299022
INFO:root:[  161] Training loss: 0.08967544, Validation loss: 0.08952895, Gradient norm: 2.82430760
INFO:root:[  162] Training loss: 0.09036858, Validation loss: 0.09016304, Gradient norm: 4.12699943
INFO:root:[  163] Training loss: 0.08995787, Validation loss: 0.08989936, Gradient norm: 3.52085503
INFO:root:[  164] Training loss: 0.09062188, Validation loss: 0.09197402, Gradient norm: 5.21866238
INFO:root:[  165] Training loss: 0.08996890, Validation loss: 0.08928089, Gradient norm: 5.35214531
INFO:root:[  166] Training loss: 0.09103074, Validation loss: 0.09004039, Gradient norm: 6.84985977
INFO:root:[  167] Training loss: 0.09073724, Validation loss: 0.09002536, Gradient norm: 7.39002078
INFO:root:[  168] Training loss: 0.08912907, Validation loss: 0.08977488, Gradient norm: 4.16014413
INFO:root:[  169] Training loss: 0.08929612, Validation loss: 0.09061327, Gradient norm: 3.51333201
INFO:root:[  170] Training loss: 0.08897183, Validation loss: 0.08993892, Gradient norm: 3.81377105
INFO:root:[  171] Training loss: 0.08963261, Validation loss: 0.09000189, Gradient norm: 4.68932290
INFO:root:[  172] Training loss: 0.08953088, Validation loss: 0.09105830, Gradient norm: 6.33574155
INFO:root:[  173] Training loss: 0.08885971, Validation loss: 0.08834253, Gradient norm: 4.59334193
INFO:root:[  174] Training loss: 0.08828046, Validation loss: 0.08912183, Gradient norm: 2.09128630
INFO:root:[  175] Training loss: 0.08828539, Validation loss: 0.08850331, Gradient norm: 3.30741834
INFO:root:[  176] Training loss: 0.08893617, Validation loss: 0.08891054, Gradient norm: 3.63785060
INFO:root:[  177] Training loss: 0.08881762, Validation loss: 0.08847296, Gradient norm: 2.97479738
INFO:root:[  178] Training loss: 0.08851481, Validation loss: 0.08837222, Gradient norm: 3.95096184
INFO:root:[  179] Training loss: 0.08882505, Validation loss: 0.08938672, Gradient norm: 3.18793430
INFO:root:[  180] Training loss: 0.08954227, Validation loss: 0.08916006, Gradient norm: 7.63785972
INFO:root:[  181] Training loss: 0.08869171, Validation loss: 0.08866443, Gradient norm: 4.12928642
INFO:root:[  182] Training loss: 0.08906081, Validation loss: 0.08880505, Gradient norm: 6.21478838
INFO:root:EP 182: Early stopping
INFO:root:Training the model took 8440.272s.
INFO:root:Emptying the cuda cache took 0.109s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 3.53467
INFO:root:EnergyScoreTrain: 2.7598
INFO:root:CoverageTrain: 0.98831
INFO:root:IntervalWidthTrain: 0.23811
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 3.39571
INFO:root:EnergyScoreValidation: 2.65085
INFO:root:CoverageValidation: 0.98834
INFO:root:IntervalWidthValidation: 0.23827
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.80701
INFO:root:EnergyScoreTest: 2.19092
INFO:root:CoverageTest: 0.98828
INFO:root:IntervalWidthTest: 0.23663
INFO:root:###5 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 32, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 8, 'projection_channels': 32, 'lifting_channels': 16, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14532089
INFO:root:Memory allocated: 1652555776
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.29081120, Validation loss: 0.53933764, Gradient norm: 8.95818713
INFO:root:[    2] Training loss: 0.47783136, Validation loss: 0.44631027, Gradient norm: 2.10521652
INFO:root:[    3] Training loss: 0.43182671, Validation loss: 0.39109435, Gradient norm: 1.78991222
INFO:root:[    4] Training loss: 0.39775876, Validation loss: 0.39402132, Gradient norm: 1.68234022
INFO:root:[    5] Training loss: 0.37446788, Validation loss: 0.35980079, Gradient norm: 2.01998930
INFO:root:[    6] Training loss: 0.36054947, Validation loss: 0.35400672, Gradient norm: 2.04539424
INFO:root:[    7] Training loss: 0.34293255, Validation loss: 0.34419941, Gradient norm: 1.80502798
INFO:root:[    8] Training loss: 0.32149237, Validation loss: 0.31176729, Gradient norm: 1.93716039
INFO:root:[    9] Training loss: 0.30804230, Validation loss: 0.31644610, Gradient norm: 1.65529553
INFO:root:[   10] Training loss: 0.30271483, Validation loss: 0.30459591, Gradient norm: 2.58855655
INFO:root:[   11] Training loss: 0.28258408, Validation loss: 0.28405227, Gradient norm: 1.60222818
INFO:root:[   12] Training loss: 0.27477955, Validation loss: 0.27221124, Gradient norm: 1.35301565
INFO:root:[   13] Training loss: 0.27066856, Validation loss: 0.26657720, Gradient norm: 1.88027777
INFO:root:[   14] Training loss: 0.26412985, Validation loss: 0.26085152, Gradient norm: 1.85369816
INFO:root:[   15] Training loss: 0.25704727, Validation loss: 0.25448509, Gradient norm: 2.49034273
INFO:root:[   16] Training loss: 0.25732370, Validation loss: 0.25159660, Gradient norm: 2.56974860
INFO:root:[   17] Training loss: 0.24539222, Validation loss: 0.24493806, Gradient norm: 1.32289582
INFO:root:[   18] Training loss: 0.23982844, Validation loss: 0.24129849, Gradient norm: 1.13006554
INFO:root:[   19] Training loss: 0.23911181, Validation loss: 0.24223889, Gradient norm: 2.64273543
INFO:root:[   20] Training loss: 0.24004397, Validation loss: 0.23146977, Gradient norm: 3.08023910
INFO:root:[   21] Training loss: 0.22506021, Validation loss: 0.22351912, Gradient norm: 1.28872628
INFO:root:[   22] Training loss: 0.22658956, Validation loss: 0.22270070, Gradient norm: 2.21770454
INFO:root:[   23] Training loss: 0.22171403, Validation loss: 0.22021557, Gradient norm: 1.90178369
INFO:root:[   24] Training loss: 0.21811550, Validation loss: 0.21892683, Gradient norm: 1.78860278
INFO:root:[   25] Training loss: 0.21409996, Validation loss: 0.20935777, Gradient norm: 1.82892435
INFO:root:[   26] Training loss: 0.20944332, Validation loss: 0.21079846, Gradient norm: 1.65941437
INFO:root:[   27] Training loss: 0.20675335, Validation loss: 0.20663432, Gradient norm: 1.67356627
INFO:root:[   28] Training loss: 0.20555263, Validation loss: 0.19866527, Gradient norm: 1.37525451
INFO:root:[   29] Training loss: 0.20554586, Validation loss: 0.21007435, Gradient norm: 1.31242743
INFO:root:[   30] Training loss: 0.20265618, Validation loss: 0.19881297, Gradient norm: 1.99184374
INFO:root:[   31] Training loss: 0.19836525, Validation loss: 0.19554956, Gradient norm: 1.17391820
INFO:root:[   32] Training loss: 0.19830897, Validation loss: 0.20197697, Gradient norm: 1.98433014
INFO:root:[   33] Training loss: 0.19500854, Validation loss: 0.19858885, Gradient norm: 1.66605557
INFO:root:[   34] Training loss: 0.19185841, Validation loss: 0.18785190, Gradient norm: 1.74753315
INFO:root:[   35] Training loss: 0.18898656, Validation loss: 0.18645841, Gradient norm: 1.64858644
INFO:root:[   36] Training loss: 0.18815514, Validation loss: 0.18721341, Gradient norm: 1.61420951
INFO:root:[   37] Training loss: 0.18464012, Validation loss: 0.18643641, Gradient norm: 1.48074520
INFO:root:[   38] Training loss: 0.18288018, Validation loss: 0.18173869, Gradient norm: 1.91056197
INFO:root:[   39] Training loss: 0.18078794, Validation loss: 0.17885044, Gradient norm: 1.72374478
INFO:root:[   40] Training loss: 0.17962736, Validation loss: 0.17774858, Gradient norm: 1.72425524
INFO:root:[   41] Training loss: 0.17674927, Validation loss: 0.17297515, Gradient norm: 1.23512887
INFO:root:[   42] Training loss: 0.17527182, Validation loss: 0.17307943, Gradient norm: 1.43800897
INFO:root:[   43] Training loss: 0.17385929, Validation loss: 0.17290751, Gradient norm: 1.47644085
INFO:root:[   44] Training loss: 0.17178899, Validation loss: 0.17052243, Gradient norm: 1.43356661
INFO:root:[   45] Training loss: 0.16896221, Validation loss: 0.17027691, Gradient norm: 0.89411993
INFO:root:[   46] Training loss: 0.16581150, Validation loss: 0.16685328, Gradient norm: 1.07546922
INFO:root:[   47] Training loss: 0.16611562, Validation loss: 0.16395561, Gradient norm: 1.26650593
INFO:root:[   48] Training loss: 0.16469049, Validation loss: 0.16636193, Gradient norm: 1.80538764
INFO:root:[   49] Training loss: 0.16214701, Validation loss: 0.15839958, Gradient norm: 1.63903714
INFO:root:[   50] Training loss: 0.16213730, Validation loss: 0.16422686, Gradient norm: 1.71681482
INFO:root:[   51] Training loss: 0.15946584, Validation loss: 0.15738507, Gradient norm: 1.97449887
INFO:root:[   52] Training loss: 0.15759831, Validation loss: 0.15843020, Gradient norm: 1.24722688
INFO:root:[   53] Training loss: 0.15692480, Validation loss: 0.15588815, Gradient norm: 1.76513641
INFO:root:[   54] Training loss: 0.15414366, Validation loss: 0.15423452, Gradient norm: 1.05950408
INFO:root:[   55] Training loss: 0.15248405, Validation loss: 0.15505668, Gradient norm: 1.10554662
INFO:root:[   56] Training loss: 0.15165288, Validation loss: 0.15048595, Gradient norm: 1.31866186
INFO:root:[   57] Training loss: 0.15065598, Validation loss: 0.15802279, Gradient norm: 1.49339255
INFO:root:[   58] Training loss: 0.15075315, Validation loss: 0.14974220, Gradient norm: 1.67232823
INFO:root:[   59] Training loss: 0.14820928, Validation loss: 0.14773216, Gradient norm: 1.07622824
INFO:root:[   60] Training loss: 0.14723008, Validation loss: 0.14545689, Gradient norm: 1.79901277
INFO:root:[   61] Training loss: 0.14505822, Validation loss: 0.14566228, Gradient norm: 1.20727771
INFO:root:[   62] Training loss: 0.14526574, Validation loss: 0.14279192, Gradient norm: 1.55859219
INFO:root:[   63] Training loss: 0.14313697, Validation loss: 0.13929165, Gradient norm: 1.43816011
INFO:root:[   64] Training loss: 0.14172466, Validation loss: 0.14076972, Gradient norm: 1.19417452
INFO:root:[   65] Training loss: 0.13995817, Validation loss: 0.13850803, Gradient norm: 1.22021360
INFO:root:[   66] Training loss: 0.13999283, Validation loss: 0.13800472, Gradient norm: 1.65887186
INFO:root:[   67] Training loss: 0.14000351, Validation loss: 0.14329826, Gradient norm: 2.23128461
INFO:root:[   68] Training loss: 0.13709934, Validation loss: 0.13582824, Gradient norm: 1.65475674
INFO:root:[   69] Training loss: 0.13550388, Validation loss: 0.13437361, Gradient norm: 1.30930237
INFO:root:[   70] Training loss: 0.13476540, Validation loss: 0.13364558, Gradient norm: 1.34495005
INFO:root:[   71] Training loss: 0.13405995, Validation loss: 0.13338732, Gradient norm: 1.38745614
INFO:root:[   72] Training loss: 0.13308297, Validation loss: 0.13014872, Gradient norm: 1.82289021
INFO:root:[   73] Training loss: 0.13163929, Validation loss: 0.13082823, Gradient norm: 0.83379049
INFO:root:[   74] Training loss: 0.13003091, Validation loss: 0.13120353, Gradient norm: 0.96175019
INFO:root:[   75] Training loss: 0.12920424, Validation loss: 0.12742990, Gradient norm: 1.02434136
INFO:root:[   76] Training loss: 0.12811578, Validation loss: 0.12811205, Gradient norm: 1.39174797
INFO:root:[   77] Training loss: 0.12693401, Validation loss: 0.12935663, Gradient norm: 1.10048563
INFO:root:[   78] Training loss: 0.12647745, Validation loss: 0.12545334, Gradient norm: 1.10458806
INFO:root:[   79] Training loss: 0.12629354, Validation loss: 0.12679538, Gradient norm: 1.45877233
INFO:root:[   80] Training loss: 0.12484923, Validation loss: 0.12394038, Gradient norm: 1.57327647
INFO:root:[   81] Training loss: 0.12426274, Validation loss: 0.12437485, Gradient norm: 1.36478095
INFO:root:[   82] Training loss: 0.12277441, Validation loss: 0.12218739, Gradient norm: 1.17581946
INFO:root:[   83] Training loss: 0.12154912, Validation loss: 0.12074328, Gradient norm: 0.72180938
INFO:root:[   84] Training loss: 0.12109391, Validation loss: 0.12087277, Gradient norm: 1.42585886
INFO:root:[   85] Training loss: 0.12021123, Validation loss: 0.12024900, Gradient norm: 1.14258627
INFO:root:[   86] Training loss: 0.12061693, Validation loss: 0.11918385, Gradient norm: 2.04475995
INFO:root:[   87] Training loss: 0.11898244, Validation loss: 0.11833935, Gradient norm: 1.10005847
INFO:root:[   88] Training loss: 0.11837723, Validation loss: 0.11829131, Gradient norm: 1.20252568
INFO:root:[   89] Training loss: 0.11774487, Validation loss: 0.11939832, Gradient norm: 1.18182138
INFO:root:[   90] Training loss: 0.11704134, Validation loss: 0.11645860, Gradient norm: 1.35532925
INFO:root:[   91] Training loss: 0.11655685, Validation loss: 0.11506875, Gradient norm: 1.34139997
INFO:root:[   92] Training loss: 0.11533463, Validation loss: 0.11563441, Gradient norm: 1.24739999
INFO:root:[   93] Training loss: 0.11481665, Validation loss: 0.11462623, Gradient norm: 1.41466926
INFO:root:[   94] Training loss: 0.11534783, Validation loss: 0.11484840, Gradient norm: 2.26412488
INFO:root:[   95] Training loss: 0.11344524, Validation loss: 0.11304856, Gradient norm: 1.13691403
INFO:root:[   96] Training loss: 0.11251935, Validation loss: 0.11161623, Gradient norm: 1.06548882
INFO:root:[   97] Training loss: 0.11234173, Validation loss: 0.11156682, Gradient norm: 1.48325909
INFO:root:[   98] Training loss: 0.11189078, Validation loss: 0.11119082, Gradient norm: 1.48755637
INFO:root:[   99] Training loss: 0.11087961, Validation loss: 0.11113904, Gradient norm: 1.45472431
INFO:root:[  100] Training loss: 0.11017996, Validation loss: 0.11028398, Gradient norm: 1.31722656
INFO:root:[  101] Training loss: 0.11019847, Validation loss: 0.10949575, Gradient norm: 1.64177077
INFO:root:[  102] Training loss: 0.10909652, Validation loss: 0.10819589, Gradient norm: 1.20415885
INFO:root:[  103] Training loss: 0.10828484, Validation loss: 0.10785363, Gradient norm: 1.31176415
INFO:root:[  104] Training loss: 0.10818542, Validation loss: 0.10776008, Gradient norm: 1.39279589
INFO:root:[  105] Training loss: 0.10720220, Validation loss: 0.10802452, Gradient norm: 1.11416432
INFO:root:[  106] Training loss: 0.10661282, Validation loss: 0.10754091, Gradient norm: 0.94592231
INFO:root:[  107] Training loss: 0.10653555, Validation loss: 0.10716895, Gradient norm: 1.39736452
INFO:root:[  108] Training loss: 0.10561912, Validation loss: 0.10590383, Gradient norm: 1.32003918
INFO:root:[  109] Training loss: 0.10494335, Validation loss: 0.10420644, Gradient norm: 1.52585307
INFO:root:[  110] Training loss: 0.10477888, Validation loss: 0.10390227, Gradient norm: 1.39227552
INFO:root:[  111] Training loss: 0.10369639, Validation loss: 0.10369294, Gradient norm: 1.26808014
INFO:root:[  112] Training loss: 0.10362436, Validation loss: 0.10501182, Gradient norm: 1.55413547
INFO:root:[  113] Training loss: 0.10373527, Validation loss: 0.10268326, Gradient norm: 1.99751284
INFO:root:[  114] Training loss: 0.10304089, Validation loss: 0.10511457, Gradient norm: 2.10420273
INFO:root:[  115] Training loss: 0.10231397, Validation loss: 0.10108659, Gradient norm: 1.66343343
INFO:root:[  116] Training loss: 0.10105957, Validation loss: 0.10088690, Gradient norm: 0.86507858
INFO:root:[  117] Training loss: 0.10079949, Validation loss: 0.10079484, Gradient norm: 1.15336989
INFO:root:[  118] Training loss: 0.10030846, Validation loss: 0.09972526, Gradient norm: 1.33843914
INFO:root:[  119] Training loss: 0.09969730, Validation loss: 0.09953460, Gradient norm: 1.05608539
INFO:root:[  120] Training loss: 0.09903725, Validation loss: 0.09902806, Gradient norm: 1.07752975
INFO:root:[  121] Training loss: 0.09916893, Validation loss: 0.09877421, Gradient norm: 1.39795679
INFO:root:[  122] Training loss: 0.09862877, Validation loss: 0.09917154, Gradient norm: 1.21704826
INFO:root:[  123] Training loss: 0.09772455, Validation loss: 0.09721893, Gradient norm: 0.81689896
INFO:root:[  124] Training loss: 0.09781299, Validation loss: 0.09810209, Gradient norm: 1.60869529
INFO:root:[  125] Training loss: 0.09676743, Validation loss: 0.09695986, Gradient norm: 1.04284060
INFO:root:[  126] Training loss: 0.09672175, Validation loss: 0.09679197, Gradient norm: 1.10721917
INFO:root:[  127] Training loss: 0.09619997, Validation loss: 0.09532260, Gradient norm: 1.33401751
INFO:root:[  128] Training loss: 0.09547699, Validation loss: 0.09589363, Gradient norm: 0.93239902
INFO:root:[  129] Training loss: 0.09551421, Validation loss: 0.09467733, Gradient norm: 1.64181820
INFO:root:[  130] Training loss: 0.09445952, Validation loss: 0.09437285, Gradient norm: 0.96813467
INFO:root:[  131] Training loss: 0.09427455, Validation loss: 0.09376112, Gradient norm: 1.12896691
INFO:root:[  132] Training loss: 0.09397635, Validation loss: 0.09366593, Gradient norm: 1.24461842
INFO:root:[  133] Training loss: 0.09350949, Validation loss: 0.09339303, Gradient norm: 0.89608672
INFO:root:[  134] Training loss: 0.09329065, Validation loss: 0.09339348, Gradient norm: 1.22644353
INFO:root:[  135] Training loss: 0.09312867, Validation loss: 0.09235389, Gradient norm: 1.74252095
INFO:root:[  136] Training loss: 0.09240686, Validation loss: 0.09319288, Gradient norm: 1.55816859
INFO:root:[  137] Training loss: 0.09219081, Validation loss: 0.09342258, Gradient norm: 1.66353416
INFO:root:[  138] Training loss: 0.09173237, Validation loss: 0.09103691, Gradient norm: 1.66507779
INFO:root:[  139] Training loss: 0.09122929, Validation loss: 0.09146771, Gradient norm: 1.37299509
INFO:root:[  140] Training loss: 0.09103067, Validation loss: 0.09105908, Gradient norm: 1.72404156
INFO:root:[  141] Training loss: 0.09054651, Validation loss: 0.08987116, Gradient norm: 1.30869683
INFO:root:[  142] Training loss: 0.09015696, Validation loss: 0.09038125, Gradient norm: 1.68355819
INFO:root:[  143] Training loss: 0.08986147, Validation loss: 0.08989595, Gradient norm: 0.96563244
INFO:root:[  144] Training loss: 0.08963753, Validation loss: 0.08949457, Gradient norm: 1.53300021
INFO:root:[  145] Training loss: 0.08933143, Validation loss: 0.08974636, Gradient norm: 1.10441206
INFO:root:[  146] Training loss: 0.08895484, Validation loss: 0.08884152, Gradient norm: 1.41316222
INFO:root:[  147] Training loss: 0.08858970, Validation loss: 0.08879441, Gradient norm: 1.46176845
INFO:root:[  148] Training loss: 0.08813668, Validation loss: 0.08784109, Gradient norm: 0.94285260
INFO:root:[  149] Training loss: 0.08784397, Validation loss: 0.08760280, Gradient norm: 0.95936734
INFO:root:[  150] Training loss: 0.08757709, Validation loss: 0.08812459, Gradient norm: 1.44276010
INFO:root:[  151] Training loss: 0.08739384, Validation loss: 0.08740712, Gradient norm: 1.47101712
INFO:root:[  152] Training loss: 0.08734261, Validation loss: 0.08686384, Gradient norm: 1.73267144
INFO:root:[  153] Training loss: 0.08655520, Validation loss: 0.08718797, Gradient norm: 0.94227777
INFO:root:[  154] Training loss: 0.08658933, Validation loss: 0.08683664, Gradient norm: 1.47232763
INFO:root:[  155] Training loss: 0.08609567, Validation loss: 0.08659474, Gradient norm: 0.75074844
INFO:root:[  156] Training loss: 0.08582116, Validation loss: 0.08677212, Gradient norm: 1.58851764
INFO:root:[  157] Training loss: 0.08552833, Validation loss: 0.08522405, Gradient norm: 1.56167569
INFO:root:[  158] Training loss: 0.08512567, Validation loss: 0.08522410, Gradient norm: 1.47963361
INFO:root:[  159] Training loss: 0.08502837, Validation loss: 0.08446598, Gradient norm: 2.15139242
INFO:root:[  160] Training loss: 0.08452848, Validation loss: 0.08493699, Gradient norm: 1.06251279
INFO:root:[  161] Training loss: 0.08464455, Validation loss: 0.08541819, Gradient norm: 1.72609546
INFO:root:[  162] Training loss: 0.08422713, Validation loss: 0.08416854, Gradient norm: 1.83916713
INFO:root:[  163] Training loss: 0.08414741, Validation loss: 0.08430875, Gradient norm: 1.88222864
INFO:root:[  164] Training loss: 0.08347865, Validation loss: 0.08366609, Gradient norm: 1.32573993
INFO:root:[  165] Training loss: 0.08355984, Validation loss: 0.08443730, Gradient norm: 1.46883763
INFO:root:[  166] Training loss: 0.08373198, Validation loss: 0.08363024, Gradient norm: 2.49825561
INFO:root:[  167] Training loss: 0.08311678, Validation loss: 0.08292085, Gradient norm: 2.18517016
INFO:root:[  168] Training loss: 0.08269823, Validation loss: 0.08244911, Gradient norm: 1.17780035
INFO:root:[  169] Training loss: 0.08260971, Validation loss: 0.08234662, Gradient norm: 1.43047597
INFO:root:[  170] Training loss: 0.08274222, Validation loss: 0.08270908, Gradient norm: 2.64805966
INFO:root:[  171] Training loss: 0.08204103, Validation loss: 0.08227363, Gradient norm: 1.57162368
INFO:root:[  172] Training loss: 0.08199567, Validation loss: 0.08176782, Gradient norm: 1.10805257
INFO:root:[  173] Training loss: 0.08174926, Validation loss: 0.08195788, Gradient norm: 1.56494344
INFO:root:[  174] Training loss: 0.08150931, Validation loss: 0.08187043, Gradient norm: 1.06711344
INFO:root:[  175] Training loss: 0.08152026, Validation loss: 0.08120240, Gradient norm: 2.11119107
INFO:root:[  176] Training loss: 0.08132720, Validation loss: 0.08090344, Gradient norm: 2.39408346
INFO:root:[  177] Training loss: 0.08089186, Validation loss: 0.08152493, Gradient norm: 2.02376511
INFO:root:[  178] Training loss: 0.08105888, Validation loss: 0.08014530, Gradient norm: 2.35498721
INFO:root:[  179] Training loss: 0.08085368, Validation loss: 0.08113104, Gradient norm: 2.78155081
INFO:root:[  180] Training loss: 0.08037955, Validation loss: 0.08079870, Gradient norm: 1.87215178
INFO:root:[  181] Training loss: 0.08019214, Validation loss: 0.08010049, Gradient norm: 2.12397672
INFO:root:[  182] Training loss: 0.08010227, Validation loss: 0.08054638, Gradient norm: 2.27993319
INFO:root:[  183] Training loss: 0.08003550, Validation loss: 0.07929346, Gradient norm: 2.97800158
INFO:root:[  184] Training loss: 0.07982329, Validation loss: 0.07966054, Gradient norm: 2.73928606
INFO:root:[  185] Training loss: 0.07927316, Validation loss: 0.08009937, Gradient norm: 1.86946774
INFO:root:[  186] Training loss: 0.07918245, Validation loss: 0.07897518, Gradient norm: 2.03243436
INFO:root:[  187] Training loss: 0.07888526, Validation loss: 0.07950248, Gradient norm: 1.49331624
INFO:root:[  188] Training loss: 0.07883924, Validation loss: 0.07898171, Gradient norm: 1.34969766
INFO:root:[  189] Training loss: 0.07871923, Validation loss: 0.07837764, Gradient norm: 1.75094176
INFO:root:[  190] Training loss: 0.07854275, Validation loss: 0.07798197, Gradient norm: 2.09604740
INFO:root:[  191] Training loss: 0.07851987, Validation loss: 0.07799808, Gradient norm: 2.95548007
INFO:root:[  192] Training loss: 0.07807732, Validation loss: 0.07779040, Gradient norm: 2.26864462
INFO:root:[  193] Training loss: 0.07780069, Validation loss: 0.07811215, Gradient norm: 1.75987430
INFO:root:[  194] Training loss: 0.07779326, Validation loss: 0.07882293, Gradient norm: 1.44154779
INFO:root:[  195] Training loss: 0.07765742, Validation loss: 0.07759979, Gradient norm: 2.22630012
INFO:root:[  196] Training loss: 0.07741985, Validation loss: 0.07744691, Gradient norm: 1.96699069
INFO:root:[  197] Training loss: 0.07727327, Validation loss: 0.07752406, Gradient norm: 1.95748533
INFO:root:[  198] Training loss: 0.07695466, Validation loss: 0.07726230, Gradient norm: 1.87576012
INFO:root:[  199] Training loss: 0.07677763, Validation loss: 0.07630958, Gradient norm: 1.83422253
INFO:root:[  200] Training loss: 0.07672312, Validation loss: 0.07665081, Gradient norm: 2.59631364
INFO:root:[  201] Training loss: 0.07668379, Validation loss: 0.07667166, Gradient norm: 2.88810226
INFO:root:[  202] Training loss: 0.07610540, Validation loss: 0.07629910, Gradient norm: 1.94393891
INFO:root:[  203] Training loss: 0.07593401, Validation loss: 0.07608321, Gradient norm: 1.08027875
INFO:root:[  204] Training loss: 0.07603083, Validation loss: 0.07598369, Gradient norm: 1.22126449
INFO:root:[  205] Training loss: 0.07604120, Validation loss: 0.07643092, Gradient norm: 2.23533250
INFO:root:[  206] Training loss: 0.07580680, Validation loss: 0.07610002, Gradient norm: 2.12305958
INFO:root:[  207] Training loss: 0.07571289, Validation loss: 0.07595889, Gradient norm: 2.22742091
INFO:root:[  208] Training loss: 0.07556530, Validation loss: 0.07502336, Gradient norm: 2.06515009
INFO:root:[  209] Training loss: 0.07518292, Validation loss: 0.07543718, Gradient norm: 1.71087284
INFO:root:[  210] Training loss: 0.07519561, Validation loss: 0.07523863, Gradient norm: 2.67281406
INFO:root:[  211] Training loss: 0.07496564, Validation loss: 0.07466527, Gradient norm: 2.05970898
INFO:root:[  212] Training loss: 0.07464047, Validation loss: 0.07510896, Gradient norm: 2.06845484
INFO:root:[  213] Training loss: 0.07444255, Validation loss: 0.07519389, Gradient norm: 1.41446376
INFO:root:[  214] Training loss: 0.07449715, Validation loss: 0.07458253, Gradient norm: 2.94847776
INFO:root:[  215] Training loss: 0.07421520, Validation loss: 0.07380366, Gradient norm: 1.99980098
INFO:root:[  216] Training loss: 0.07401117, Validation loss: 0.07442232, Gradient norm: 1.33498246
INFO:root:[  217] Training loss: 0.07405988, Validation loss: 0.07354975, Gradient norm: 2.56399979
INFO:root:[  218] Training loss: 0.07387771, Validation loss: 0.07477945, Gradient norm: 1.93704928
INFO:root:[  219] Training loss: 0.07362951, Validation loss: 0.07431705, Gradient norm: 2.80841035
INFO:root:[  220] Training loss: 0.07346218, Validation loss: 0.07380199, Gradient norm: 2.41522260
INFO:root:[  221] Training loss: 0.07326393, Validation loss: 0.07372769, Gradient norm: 2.05198667
INFO:root:[  222] Training loss: 0.07298230, Validation loss: 0.07303689, Gradient norm: 2.30163267
INFO:root:[  223] Training loss: 0.07286592, Validation loss: 0.07280976, Gradient norm: 2.45638148
INFO:root:[  224] Training loss: 0.07300728, Validation loss: 0.07242567, Gradient norm: 2.15658419
INFO:root:[  225] Training loss: 0.07272090, Validation loss: 0.07319583, Gradient norm: 2.80056375
INFO:root:[  226] Training loss: 0.07240165, Validation loss: 0.07212139, Gradient norm: 2.97929129
INFO:root:[  227] Training loss: 0.07213966, Validation loss: 0.07207486, Gradient norm: 2.26336089
INFO:root:[  228] Training loss: 0.07218856, Validation loss: 0.07304927, Gradient norm: 2.59115010
INFO:root:[  229] Training loss: 0.07223152, Validation loss: 0.07207970, Gradient norm: 3.19544348
INFO:root:[  230] Training loss: 0.07224142, Validation loss: 0.07207832, Gradient norm: 3.21954222
INFO:root:[  231] Training loss: 0.07196181, Validation loss: 0.07211422, Gradient norm: 3.32465884
INFO:root:[  232] Training loss: 0.07151361, Validation loss: 0.07131204, Gradient norm: 2.43576012
INFO:root:[  233] Training loss: 0.07128957, Validation loss: 0.07174081, Gradient norm: 1.94737963
INFO:root:[  234] Training loss: 0.07137094, Validation loss: 0.07159481, Gradient norm: 2.21489025
INFO:root:[  235] Training loss: 0.07116391, Validation loss: 0.07154604, Gradient norm: 3.15994647
INFO:root:[  236] Training loss: 0.07109911, Validation loss: 0.07085271, Gradient norm: 2.95423068
INFO:root:[  237] Training loss: 0.07058730, Validation loss: 0.07067592, Gradient norm: 1.42762912
INFO:root:[  238] Training loss: 0.07059804, Validation loss: 0.07085375, Gradient norm: 2.74995344
INFO:root:[  239] Training loss: 0.07058616, Validation loss: 0.07073347, Gradient norm: 3.20159738
INFO:root:[  240] Training loss: 0.07032391, Validation loss: 0.07061166, Gradient norm: 1.67390641
INFO:root:[  241] Training loss: 0.07040675, Validation loss: 0.07129696, Gradient norm: 2.41768313
INFO:root:[  242] Training loss: 0.07022192, Validation loss: 0.07027933, Gradient norm: 3.38154139
INFO:root:[  243] Training loss: 0.06984549, Validation loss: 0.07027832, Gradient norm: 2.24619345
INFO:root:[  244] Training loss: 0.07005846, Validation loss: 0.07043059, Gradient norm: 3.07172162
INFO:root:[  245] Training loss: 0.06974835, Validation loss: 0.07004142, Gradient norm: 3.05015077
INFO:root:[  246] Training loss: 0.06967232, Validation loss: 0.07092437, Gradient norm: 2.73191844
INFO:root:[  247] Training loss: 0.06967960, Validation loss: 0.07008417, Gradient norm: 4.17038239
INFO:root:[  248] Training loss: 0.06941632, Validation loss: 0.07069861, Gradient norm: 3.28788676
INFO:root:[  249] Training loss: 0.06962814, Validation loss: 0.06941251, Gradient norm: 3.55891932
INFO:root:[  250] Training loss: 0.06933070, Validation loss: 0.06876874, Gradient norm: 4.07560514
INFO:root:[  251] Training loss: 0.06898941, Validation loss: 0.06884911, Gradient norm: 3.43870966
INFO:root:[  252] Training loss: 0.06864169, Validation loss: 0.06884423, Gradient norm: 1.64715524
INFO:root:[  253] Training loss: 0.06882992, Validation loss: 0.06937663, Gradient norm: 3.86092006
INFO:root:[  254] Training loss: 0.06863162, Validation loss: 0.06840089, Gradient norm: 3.31302108
INFO:root:[  255] Training loss: 0.06841969, Validation loss: 0.06824780, Gradient norm: 3.26352882
INFO:root:[  256] Training loss: 0.06827952, Validation loss: 0.06864251, Gradient norm: 3.28601023
INFO:root:[  257] Training loss: 0.06818349, Validation loss: 0.06801917, Gradient norm: 4.26757110
INFO:root:[  258] Training loss: 0.06806197, Validation loss: 0.06912632, Gradient norm: 2.17829061
INFO:root:[  259] Training loss: 0.06815207, Validation loss: 0.06837590, Gradient norm: 4.33580001
INFO:root:[  260] Training loss: 0.06804992, Validation loss: 0.06806111, Gradient norm: 3.78378252
INFO:root:[  261] Training loss: 0.06783892, Validation loss: 0.06744373, Gradient norm: 4.63103958
INFO:root:[  262] Training loss: 0.06741860, Validation loss: 0.06761111, Gradient norm: 2.50593531
INFO:root:[  263] Training loss: 0.06741866, Validation loss: 0.06723577, Gradient norm: 4.11708035
INFO:root:[  264] Training loss: 0.06709703, Validation loss: 0.06789963, Gradient norm: 3.55336392
INFO:root:[  265] Training loss: 0.06713673, Validation loss: 0.06794683, Gradient norm: 3.08173955
INFO:root:[  266] Training loss: 0.06696841, Validation loss: 0.06750667, Gradient norm: 3.94119893
INFO:root:[  267] Training loss: 0.06674223, Validation loss: 0.06772316, Gradient norm: 3.23556878
INFO:root:[  268] Training loss: 0.06694071, Validation loss: 0.06690308, Gradient norm: 3.91702537
INFO:root:[  269] Training loss: 0.06669881, Validation loss: 0.06654864, Gradient norm: 2.77616682
INFO:root:[  270] Training loss: 0.06635230, Validation loss: 0.06638867, Gradient norm: 2.14145543
INFO:root:[  271] Training loss: 0.06627033, Validation loss: 0.06622237, Gradient norm: 3.92411592
INFO:root:[  272] Training loss: 0.06616231, Validation loss: 0.06605153, Gradient norm: 2.92413109
INFO:root:[  273] Training loss: 0.06613934, Validation loss: 0.06627088, Gradient norm: 3.77075985
INFO:root:[  274] Training loss: 0.06577998, Validation loss: 0.06570368, Gradient norm: 2.47976324
INFO:root:[  275] Training loss: 0.06593995, Validation loss: 0.06541213, Gradient norm: 3.97619257
INFO:root:[  276] Training loss: 0.06582118, Validation loss: 0.06574548, Gradient norm: 5.00468282
INFO:root:[  277] Training loss: 0.06550308, Validation loss: 0.06549191, Gradient norm: 5.51031205
INFO:root:[  278] Training loss: 0.06544205, Validation loss: 0.06610877, Gradient norm: 4.87919779
INFO:root:[  279] Training loss: 0.06528288, Validation loss: 0.06555196, Gradient norm: 3.71324732
INFO:root:[  280] Training loss: 0.06516892, Validation loss: 0.06539429, Gradient norm: 3.48073354
INFO:root:[  281] Training loss: 0.06519137, Validation loss: 0.06507630, Gradient norm: 4.94534774
INFO:root:[  282] Training loss: 0.06528527, Validation loss: 0.06706382, Gradient norm: 5.57351748
INFO:root:[  283] Training loss: 0.06496327, Validation loss: 0.06477990, Gradient norm: 4.66951386
INFO:root:[  284] Training loss: 0.06493216, Validation loss: 0.06448591, Gradient norm: 3.94883979
INFO:root:[  285] Training loss: 0.06464251, Validation loss: 0.06517568, Gradient norm: 2.29568695
INFO:root:[  286] Training loss: 0.06476178, Validation loss: 0.06479017, Gradient norm: 4.75343515
INFO:root:[  287] Training loss: 0.06450869, Validation loss: 0.06425217, Gradient norm: 4.13507274
INFO:root:[  288] Training loss: 0.06427569, Validation loss: 0.06395297, Gradient norm: 5.72197394
INFO:root:[  289] Training loss: 0.06384568, Validation loss: 0.06374017, Gradient norm: 2.85315000
INFO:root:[  290] Training loss: 0.06410910, Validation loss: 0.06484681, Gradient norm: 5.34556319
INFO:root:[  291] Training loss: 0.06385923, Validation loss: 0.06424279, Gradient norm: 5.66850364
INFO:root:[  292] Training loss: 0.06361589, Validation loss: 0.06422519, Gradient norm: 4.80421350
INFO:root:[  293] Training loss: 0.06366115, Validation loss: 0.06308180, Gradient norm: 5.90073964
INFO:root:[  294] Training loss: 0.06330155, Validation loss: 0.06357199, Gradient norm: 2.64738404
INFO:root:[  295] Training loss: 0.06359135, Validation loss: 0.06338801, Gradient norm: 6.09615137
INFO:root:[  296] Training loss: 0.06310998, Validation loss: 0.06300668, Gradient norm: 4.73238982
INFO:root:[  297] Training loss: 0.06311102, Validation loss: 0.06338071, Gradient norm: 3.75810132
INFO:root:[  298] Training loss: 0.06312407, Validation loss: 0.06330509, Gradient norm: 4.43909234
INFO:root:[  299] Training loss: 0.06324587, Validation loss: 0.06324560, Gradient norm: 3.40169471
INFO:root:[  300] Training loss: 0.06271669, Validation loss: 0.06239008, Gradient norm: 4.64709663
INFO:root:[  301] Training loss: 0.06267938, Validation loss: 0.06408796, Gradient norm: 2.47356924
INFO:root:[  302] Training loss: 0.06309042, Validation loss: 0.06298046, Gradient norm: 10.28341870
INFO:root:[  303] Training loss: 0.06227285, Validation loss: 0.06323925, Gradient norm: 5.35409450
INFO:root:[  304] Training loss: 0.06230968, Validation loss: 0.06241812, Gradient norm: 5.06163304
INFO:root:[  305] Training loss: 0.06205865, Validation loss: 0.06204404, Gradient norm: 4.79153963
INFO:root:[  306] Training loss: 0.06193887, Validation loss: 0.06230253, Gradient norm: 3.71725621
INFO:root:[  307] Training loss: 0.06165014, Validation loss: 0.06197898, Gradient norm: 3.15999388
INFO:root:[  308] Training loss: 0.06177925, Validation loss: 0.06172867, Gradient norm: 2.92764912
INFO:root:[  309] Training loss: 0.06178350, Validation loss: 0.06308178, Gradient norm: 7.07511312
INFO:root:[  310] Training loss: 0.06165668, Validation loss: 0.06136803, Gradient norm: 5.18695330
INFO:root:[  311] Training loss: 0.06148845, Validation loss: 0.06122459, Gradient norm: 5.21591282
INFO:root:[  312] Training loss: 0.06104304, Validation loss: 0.06148259, Gradient norm: 4.82305181
INFO:root:[  313] Training loss: 0.06105165, Validation loss: 0.06072663, Gradient norm: 3.69742987
INFO:root:[  314] Training loss: 0.06093581, Validation loss: 0.06157690, Gradient norm: 4.31957312
INFO:root:[  315] Training loss: 0.06126469, Validation loss: 0.06249495, Gradient norm: 7.01372077
INFO:root:[  316] Training loss: 0.06106696, Validation loss: 0.06109162, Gradient norm: 6.81298591
INFO:root:[  317] Training loss: 0.06054760, Validation loss: 0.06081685, Gradient norm: 3.48676126
INFO:root:[  318] Training loss: 0.06080370, Validation loss: 0.06078079, Gradient norm: 4.87263137
INFO:root:[  319] Training loss: 0.06055333, Validation loss: 0.06025301, Gradient norm: 5.45648237
INFO:root:[  320] Training loss: 0.06021670, Validation loss: 0.06069179, Gradient norm: 6.28370294
INFO:root:[  321] Training loss: 0.06036541, Validation loss: 0.06041948, Gradient norm: 5.31479355
INFO:root:[  322] Training loss: 0.06020061, Validation loss: 0.05984578, Gradient norm: 5.21239885
INFO:root:[  323] Training loss: 0.05977598, Validation loss: 0.06030108, Gradient norm: 4.72946777
INFO:root:[  324] Training loss: 0.05996447, Validation loss: 0.06043182, Gradient norm: 4.76633457
INFO:root:[  325] Training loss: 0.05996368, Validation loss: 0.05954962, Gradient norm: 7.39638182
INFO:root:[  326] Training loss: 0.05986931, Validation loss: 0.06007917, Gradient norm: 6.84846003
INFO:root:[  327] Training loss: 0.05953991, Validation loss: 0.05952257, Gradient norm: 5.15941733
INFO:root:[  328] Training loss: 0.05966919, Validation loss: 0.06077293, Gradient norm: 6.28016546
INFO:root:[  329] Training loss: 0.05919736, Validation loss: 0.05903249, Gradient norm: 4.76455223
INFO:root:[  330] Training loss: 0.05906904, Validation loss: 0.05910918, Gradient norm: 5.81590850
INFO:root:[  331] Training loss: 0.05908174, Validation loss: 0.05888263, Gradient norm: 6.72857702
INFO:root:[  332] Training loss: 0.05911299, Validation loss: 0.05971594, Gradient norm: 7.77381139
INFO:root:[  333] Training loss: 0.05860833, Validation loss: 0.05881311, Gradient norm: 4.42493326
INFO:root:[  334] Training loss: 0.05863810, Validation loss: 0.05976521, Gradient norm: 5.53191360
INFO:root:[  335] Training loss: 0.05902669, Validation loss: 0.05906456, Gradient norm: 10.51189763
INFO:root:[  336] Training loss: 0.05846654, Validation loss: 0.05866286, Gradient norm: 6.57527786
INFO:root:[  337] Training loss: 0.05844892, Validation loss: 0.05927848, Gradient norm: 6.02205612
INFO:root:[  338] Training loss: 0.05867639, Validation loss: 0.05892478, Gradient norm: 4.28958584
INFO:root:[  339] Training loss: 0.05830033, Validation loss: 0.05811667, Gradient norm: 5.33013863
INFO:root:[  340] Training loss: 0.05805307, Validation loss: 0.05855571, Gradient norm: 4.57633391
INFO:root:[  341] Training loss: 0.05835900, Validation loss: 0.05830600, Gradient norm: 7.87499869
INFO:root:[  342] Training loss: 0.05774146, Validation loss: 0.05762157, Gradient norm: 6.60233548
INFO:root:[  343] Training loss: 0.05761824, Validation loss: 0.05742427, Gradient norm: 6.25493928
INFO:root:[  344] Training loss: 0.05795853, Validation loss: 0.05748242, Gradient norm: 7.05350698
INFO:root:[  345] Training loss: 0.05786071, Validation loss: 0.05746038, Gradient norm: 9.94975300
INFO:root:[  346] Training loss: 0.05761594, Validation loss: 0.05894867, Gradient norm: 6.32235954
INFO:root:[  347] Training loss: 0.05730979, Validation loss: 0.05738585, Gradient norm: 7.89656936
INFO:root:[  348] Training loss: 0.05722059, Validation loss: 0.05860217, Gradient norm: 8.30551818
INFO:root:[  349] Training loss: 0.05730591, Validation loss: 0.05695150, Gradient norm: 10.54933601
INFO:root:[  350] Training loss: 0.05670325, Validation loss: 0.05697289, Gradient norm: 4.70884871
INFO:root:[  351] Training loss: 0.05683435, Validation loss: 0.05702118, Gradient norm: 5.85688827
INFO:root:[  352] Training loss: 0.05684972, Validation loss: 0.05665842, Gradient norm: 7.22119786
INFO:root:[  353] Training loss: 0.05668107, Validation loss: 0.05667949, Gradient norm: 5.31615425
INFO:root:[  354] Training loss: 0.05666683, Validation loss: 0.05654320, Gradient norm: 7.44110820
INFO:root:[  355] Training loss: 0.05661296, Validation loss: 0.05645917, Gradient norm: 7.21794507
INFO:root:[  356] Training loss: 0.05649230, Validation loss: 0.05609577, Gradient norm: 5.71073804
INFO:root:[  357] Training loss: 0.05626725, Validation loss: 0.05577781, Gradient norm: 3.01874534
INFO:root:[  358] Training loss: 0.05639438, Validation loss: 0.05636546, Gradient norm: 8.23004075
INFO:root:[  359] Training loss: 0.05606107, Validation loss: 0.05562522, Gradient norm: 7.87947098
INFO:root:[  360] Training loss: 0.05596344, Validation loss: 0.05579166, Gradient norm: 7.27775338
INFO:root:[  361] Training loss: 0.05600861, Validation loss: 0.05595899, Gradient norm: 7.87521505
INFO:root:[  362] Training loss: 0.05595443, Validation loss: 0.05609665, Gradient norm: 9.39060047
INFO:root:[  363] Training loss: 0.05567268, Validation loss: 0.05584469, Gradient norm: 6.30388029
INFO:root:[  364] Training loss: 0.05566380, Validation loss: 0.05631999, Gradient norm: 6.60355915
INFO:root:[  365] Training loss: 0.05567000, Validation loss: 0.05610837, Gradient norm: 5.38813956
INFO:root:[  366] Training loss: 0.05584260, Validation loss: 0.05693619, Gradient norm: 8.39849376
INFO:root:[  367] Training loss: 0.05614061, Validation loss: 0.05486117, Gradient norm: 16.32203090
INFO:root:[  368] Training loss: 0.05502990, Validation loss: 0.05503460, Gradient norm: 5.87361773
INFO:root:[  369] Training loss: 0.05508224, Validation loss: 0.05501433, Gradient norm: 9.66149433
INFO:root:[  370] Training loss: 0.05490537, Validation loss: 0.05513136, Gradient norm: 6.35886729
INFO:root:[  371] Training loss: 0.05520280, Validation loss: 0.05563005, Gradient norm: 12.12852081
INFO:root:[  372] Training loss: 0.05482823, Validation loss: 0.05437345, Gradient norm: 10.17882620
INFO:root:[  373] Training loss: 0.05517233, Validation loss: 0.05454510, Gradient norm: 15.14546508
INFO:root:[  374] Training loss: 0.05453212, Validation loss: 0.05432694, Gradient norm: 8.73323070
INFO:root:[  375] Training loss: 0.05448978, Validation loss: 0.05503856, Gradient norm: 9.39636015
INFO:root:[  376] Training loss: 0.05447547, Validation loss: 0.05412867, Gradient norm: 7.88402300
INFO:root:[  377] Training loss: 0.05418406, Validation loss: 0.05520503, Gradient norm: 5.34024849
INFO:root:[  378] Training loss: 0.05436803, Validation loss: 0.05471158, Gradient norm: 7.88510761
INFO:root:[  379] Training loss: 0.05447336, Validation loss: 0.05459242, Gradient norm: 7.92256837
INFO:root:[  380] Training loss: 0.05411321, Validation loss: 0.05395977, Gradient norm: 9.04760467
INFO:root:[  381] Training loss: 0.05416527, Validation loss: 0.05430282, Gradient norm: 12.53725242
INFO:root:[  382] Training loss: 0.05403096, Validation loss: 0.05447881, Gradient norm: 11.56022775
INFO:root:[  383] Training loss: 0.05384034, Validation loss: 0.05383895, Gradient norm: 11.35926061
INFO:root:[  384] Training loss: 0.05359225, Validation loss: 0.05436801, Gradient norm: 7.07632791
INFO:root:[  385] Training loss: 0.05353358, Validation loss: 0.05410304, Gradient norm: 8.52161861
INFO:root:[  386] Training loss: 0.05346582, Validation loss: 0.05403546, Gradient norm: 11.85297812
INFO:root:[  387] Training loss: 0.05376777, Validation loss: 0.05300380, Gradient norm: 14.85311966
INFO:root:[  388] Training loss: 0.05304861, Validation loss: 0.05379032, Gradient norm: 7.91547454
INFO:root:[  389] Training loss: 0.05308142, Validation loss: 0.05279650, Gradient norm: 9.50814140
INFO:root:[  390] Training loss: 0.05335832, Validation loss: 0.05293577, Gradient norm: 12.32381069
INFO:root:[  391] Training loss: 0.05311560, Validation loss: 0.05345963, Gradient norm: 11.00404726
INFO:root:[  392] Training loss: 0.05296144, Validation loss: 0.05371540, Gradient norm: 12.33688817
INFO:root:[  393] Training loss: 0.05292519, Validation loss: 0.05279297, Gradient norm: 10.81822780
INFO:root:[  394] Training loss: 0.05260913, Validation loss: 0.05284596, Gradient norm: 7.06449368
INFO:root:[  395] Training loss: 0.05263830, Validation loss: 0.05348275, Gradient norm: 10.36289538
INFO:root:[  396] Training loss: 0.05291285, Validation loss: 0.05491332, Gradient norm: 12.31443810
INFO:root:[  397] Training loss: 0.05269542, Validation loss: 0.05257603, Gradient norm: 16.17734552
INFO:root:[  398] Training loss: 0.05236716, Validation loss: 0.05289097, Gradient norm: 9.42052409
INFO:root:[  399] Training loss: 0.05250774, Validation loss: 0.05263442, Gradient norm: 12.90924786
INFO:root:[  400] Training loss: 0.05228671, Validation loss: 0.05224943, Gradient norm: 10.29793551
INFO:root:[  401] Training loss: 0.05245234, Validation loss: 0.05384348, Gradient norm: 15.39709679
INFO:root:[  402] Training loss: 0.05212476, Validation loss: 0.05215661, Gradient norm: 11.74937463
INFO:root:[  403] Training loss: 0.05193682, Validation loss: 0.05177404, Gradient norm: 8.24220411
INFO:root:[  404] Training loss: 0.05171421, Validation loss: 0.05229990, Gradient norm: 8.65117116
INFO:root:[  405] Training loss: 0.05207114, Validation loss: 0.05186737, Gradient norm: 14.99818192
INFO:root:[  406] Training loss: 0.05180617, Validation loss: 0.05207340, Gradient norm: 12.58854320
INFO:root:[  407] Training loss: 0.05176112, Validation loss: 0.05216759, Gradient norm: 13.18488714
INFO:root:[  408] Training loss: 0.05161806, Validation loss: 0.05129464, Gradient norm: 11.92216754
INFO:root:[  409] Training loss: 0.05144656, Validation loss: 0.05168903, Gradient norm: 12.32572065
INFO:root:[  410] Training loss: 0.05137325, Validation loss: 0.05182356, Gradient norm: 8.19627890
INFO:root:[  411] Training loss: 0.05148614, Validation loss: 0.05373594, Gradient norm: 11.94618939
INFO:root:[  412] Training loss: 0.05242096, Validation loss: 0.05199059, Gradient norm: 27.49023686
INFO:root:[  413] Training loss: 0.05135565, Validation loss: 0.05312496, Gradient norm: 15.90127108
INFO:root:[  414] Training loss: 0.05117692, Validation loss: 0.05272270, Gradient norm: 12.84675080
INFO:root:[  415] Training loss: 0.05093822, Validation loss: 0.05067745, Gradient norm: 10.70815568
INFO:root:[  416] Training loss: 0.05103847, Validation loss: 0.05334955, Gradient norm: 14.98433352
INFO:root:[  417] Training loss: 0.05103784, Validation loss: 0.05056589, Gradient norm: 18.10392042
INFO:root:[  418] Training loss: 0.05097322, Validation loss: 0.05085177, Gradient norm: 18.72159409
INFO:root:[  419] Training loss: 0.05088347, Validation loss: 0.05126335, Gradient norm: 18.95747194
INFO:root:[  420] Training loss: 0.05048059, Validation loss: 0.05115510, Gradient norm: 11.47529913
INFO:root:[  421] Training loss: 0.05037759, Validation loss: 0.05059455, Gradient norm: 13.46878135
INFO:root:[  422] Training loss: 0.05031079, Validation loss: 0.05019321, Gradient norm: 9.02755502
INFO:root:[  423] Training loss: 0.05034143, Validation loss: 0.05015450, Gradient norm: 10.71587350
INFO:root:[  424] Training loss: 0.05028416, Validation loss: 0.05173451, Gradient norm: 11.87434354
INFO:root:[  425] Training loss: 0.05043034, Validation loss: 0.05029823, Gradient norm: 18.63847135
INFO:root:[  426] Training loss: 0.05050371, Validation loss: 0.05027024, Gradient norm: 16.80493078
INFO:root:[  427] Training loss: 0.05025670, Validation loss: 0.05002158, Gradient norm: 17.18704395
INFO:root:[  428] Training loss: 0.05045330, Validation loss: 0.04984397, Gradient norm: 22.14372899
INFO:root:[  429] Training loss: 0.05033185, Validation loss: 0.05010827, Gradient norm: 20.86476939
INFO:root:[  430] Training loss: 0.04974777, Validation loss: 0.04968077, Gradient norm: 11.78959390
INFO:root:[  431] Training loss: 0.04974678, Validation loss: 0.04988481, Gradient norm: 10.83407498
INFO:root:[  432] Training loss: 0.04968333, Validation loss: 0.05036609, Gradient norm: 13.26656108
INFO:root:[  433] Training loss: 0.04998917, Validation loss: 0.04957400, Gradient norm: 20.32673300
INFO:root:[  434] Training loss: 0.04965824, Validation loss: 0.05090923, Gradient norm: 14.74815179
INFO:root:[  435] Training loss: 0.04960979, Validation loss: 0.05197073, Gradient norm: 18.48058171
INFO:root:[  436] Training loss: 0.04990702, Validation loss: 0.05168497, Gradient norm: 23.31542047
INFO:root:[  437] Training loss: 0.04954483, Validation loss: 0.04928241, Gradient norm: 18.32623112
INFO:root:[  438] Training loss: 0.04944683, Validation loss: 0.04943409, Gradient norm: 13.31412811
INFO:root:[  439] Training loss: 0.04967353, Validation loss: 0.05192915, Gradient norm: 24.92699641
INFO:root:[  440] Training loss: 0.04932567, Validation loss: 0.04907362, Gradient norm: 18.72066053
INFO:root:[  441] Training loss: 0.04888520, Validation loss: 0.04923290, Gradient norm: 11.37187128
INFO:root:[  442] Training loss: 0.04926186, Validation loss: 0.04915921, Gradient norm: 20.56787252
INFO:root:[  443] Training loss: 0.04902218, Validation loss: 0.04900496, Gradient norm: 6.69720543
INFO:root:[  444] Training loss: 0.04921997, Validation loss: 0.04954482, Gradient norm: 19.50570760
INFO:root:[  445] Training loss: 0.04917308, Validation loss: 0.04961493, Gradient norm: 17.07722443
INFO:root:[  446] Training loss: 0.04917102, Validation loss: 0.04890076, Gradient norm: 17.99195373
INFO:root:[  447] Training loss: 0.04905767, Validation loss: 0.04869270, Gradient norm: 21.13059089
INFO:root:[  448] Training loss: 0.04884407, Validation loss: 0.04884345, Gradient norm: 18.22154370
INFO:root:[  449] Training loss: 0.04863591, Validation loss: 0.04835644, Gradient norm: 11.84566825
INFO:root:[  450] Training loss: 0.04870483, Validation loss: 0.04908842, Gradient norm: 20.78771471
INFO:root:[  451] Training loss: 0.04907634, Validation loss: 0.05085707, Gradient norm: 23.90263739
INFO:root:[  452] Training loss: 0.04887948, Validation loss: 0.04810301, Gradient norm: 27.08659638
INFO:root:[  453] Training loss: 0.04821335, Validation loss: 0.04863079, Gradient norm: 10.78690128
INFO:root:[  454] Training loss: 0.04839516, Validation loss: 0.04833253, Gradient norm: 16.44716476
INFO:root:[  455] Training loss: 0.04876137, Validation loss: 0.04834474, Gradient norm: 26.16111489
INFO:root:[  456] Training loss: 0.04872177, Validation loss: 0.05016413, Gradient norm: 24.43345697
INFO:root:[  457] Training loss: 0.04837057, Validation loss: 0.04791720, Gradient norm: 20.43735264
INFO:root:[  458] Training loss: 0.04823096, Validation loss: 0.04792847, Gradient norm: 15.67742936
INFO:root:[  459] Training loss: 0.04801840, Validation loss: 0.04802579, Gradient norm: 17.81212280
INFO:root:[  460] Training loss: 0.04892880, Validation loss: 0.05047689, Gradient norm: 35.38529542
INFO:root:[  461] Training loss: 0.04832931, Validation loss: 0.04821666, Gradient norm: 27.02577924
INFO:root:[  462] Training loss: 0.04808484, Validation loss: 0.04771590, Gradient norm: 23.27359019
INFO:root:[  463] Training loss: 0.04784284, Validation loss: 0.04783902, Gradient norm: 19.28774745
INFO:root:[  464] Training loss: 0.04752547, Validation loss: 0.04853337, Gradient norm: 12.11402048
INFO:root:[  465] Training loss: 0.04797652, Validation loss: 0.04850787, Gradient norm: 24.21628290
INFO:root:[  466] Training loss: 0.04783865, Validation loss: 0.04728244, Gradient norm: 23.23660182
INFO:root:[  467] Training loss: 0.04807835, Validation loss: 0.04947136, Gradient norm: 24.82384165
INFO:root:[  468] Training loss: 0.04820927, Validation loss: 0.04736067, Gradient norm: 32.28514790
INFO:root:[  469] Training loss: 0.04762845, Validation loss: 0.04717370, Gradient norm: 19.95047074
INFO:root:[  470] Training loss: 0.04757298, Validation loss: 0.04870769, Gradient norm: 24.59113543
INFO:root:[  471] Training loss: 0.04762714, Validation loss: 0.04751347, Gradient norm: 21.61802839
INFO:root:[  472] Training loss: 0.04730857, Validation loss: 0.04718729, Gradient norm: 21.83396314
INFO:root:[  473] Training loss: 0.04721144, Validation loss: 0.04735299, Gradient norm: 13.31079789
INFO:root:[  474] Training loss: 0.04761874, Validation loss: 0.05017612, Gradient norm: 26.52382317
INFO:root:[  475] Training loss: 0.04813704, Validation loss: 0.04692299, Gradient norm: 36.07507759
INFO:root:[  476] Training loss: 0.04718796, Validation loss: 0.04681895, Gradient norm: 20.32573581
INFO:root:[  477] Training loss: 0.04687889, Validation loss: 0.04740488, Gradient norm: 16.43861576
INFO:root:[  478] Training loss: 0.04741724, Validation loss: 0.04967667, Gradient norm: 28.10045650
INFO:root:[  479] Training loss: 0.04745362, Validation loss: 0.04672193, Gradient norm: 26.75215765
INFO:root:[  480] Training loss: 0.04703685, Validation loss: 0.04828857, Gradient norm: 25.80442126
INFO:root:[  481] Training loss: 0.04694512, Validation loss: 0.04687639, Gradient norm: 21.46597140
INFO:root:[  482] Training loss: 0.04693319, Validation loss: 0.04902601, Gradient norm: 19.51648303
INFO:root:[  483] Training loss: 0.04784904, Validation loss: 0.04657770, Gradient norm: 40.97432535
INFO:root:[  484] Training loss: 0.04674275, Validation loss: 0.04905121, Gradient norm: 19.72900714
INFO:root:[  485] Training loss: 0.04720957, Validation loss: 0.04670595, Gradient norm: 31.69238707
INFO:root:[  486] Training loss: 0.04652409, Validation loss: 0.04639682, Gradient norm: 22.60920473
INFO:root:[  487] Training loss: 0.04644883, Validation loss: 0.04650208, Gradient norm: 16.69593069
INFO:root:[  488] Training loss: 0.04700034, Validation loss: 0.04703817, Gradient norm: 26.52536547
INFO:root:[  489] Training loss: 0.04644429, Validation loss: 0.04687062, Gradient norm: 18.32151173
INFO:root:[  490] Training loss: 0.04695602, Validation loss: 0.04608939, Gradient norm: 32.35294488
INFO:root:[  491] Training loss: 0.04660082, Validation loss: 0.04611535, Gradient norm: 27.66293771
INFO:root:[  492] Training loss: 0.04669445, Validation loss: 0.04640509, Gradient norm: 31.45022527
INFO:root:[  493] Training loss: 0.04675631, Validation loss: 0.04616654, Gradient norm: 30.75749498
INFO:root:[  494] Training loss: 0.04606871, Validation loss: 0.04662669, Gradient norm: 16.07727794
INFO:root:[  495] Training loss: 0.04726050, Validation loss: 0.04824062, Gradient norm: 40.26488647
INFO:root:[  496] Training loss: 0.04623209, Validation loss: 0.04684821, Gradient norm: 24.56930783
INFO:root:[  497] Training loss: 0.04654932, Validation loss: 0.04592345, Gradient norm: 32.72343630
INFO:root:[  498] Training loss: 0.04603773, Validation loss: 0.04665868, Gradient norm: 21.25190677
INFO:root:[  499] Training loss: 0.04688657, Validation loss: 0.04582842, Gradient norm: 37.00697706
INFO:root:[  500] Training loss: 0.04584741, Validation loss: 0.04581842, Gradient norm: 13.89247297
INFO:root:[  501] Training loss: 0.04594107, Validation loss: 0.04619939, Gradient norm: 23.78002967
INFO:root:[  502] Training loss: 0.04639437, Validation loss: 0.04559474, Gradient norm: 36.56409971
INFO:root:[  503] Training loss: 0.04584582, Validation loss: 0.04610248, Gradient norm: 19.51686046
INFO:root:[  504] Training loss: 0.04595544, Validation loss: 0.04826383, Gradient norm: 28.90285512
INFO:root:[  505] Training loss: 0.04631714, Validation loss: 0.04789636, Gradient norm: 36.73743076
INFO:root:[  506] Training loss: 0.04609816, Validation loss: 0.04548683, Gradient norm: 36.73903797
INFO:root:[  507] Training loss: 0.04559724, Validation loss: 0.04616496, Gradient norm: 14.59619046
INFO:root:[  508] Training loss: 0.04543374, Validation loss: 0.04554524, Gradient norm: 20.41663008
INFO:root:[  509] Training loss: 0.04693107, Validation loss: 0.04804067, Gradient norm: 50.86323691
INFO:root:[  510] Training loss: 0.04555620, Validation loss: 0.04606288, Gradient norm: 31.05480788
INFO:root:[  511] Training loss: 0.04689977, Validation loss: 0.04565655, Gradient norm: 51.63748155
INFO:root:[  512] Training loss: 0.04542696, Validation loss: 0.04526959, Gradient norm: 25.36536344
INFO:root:[  513] Training loss: 0.04552501, Validation loss: 0.04542697, Gradient norm: 25.26443280
INFO:root:[  514] Training loss: 0.04542894, Validation loss: 0.04621927, Gradient norm: 28.14231709
INFO:root:[  515] Training loss: 0.04513864, Validation loss: 0.04511087, Gradient norm: 26.59807714
INFO:root:[  516] Training loss: 0.04522521, Validation loss: 0.04505108, Gradient norm: 26.79284054
INFO:root:[  517] Training loss: 0.04520926, Validation loss: 0.04496457, Gradient norm: 19.13971243
INFO:root:[  518] Training loss: 0.04566832, Validation loss: 0.04525929, Gradient norm: 35.36960462
INFO:root:[  519] Training loss: 0.04486419, Validation loss: 0.04528533, Gradient norm: 15.74546763
INFO:root:[  520] Training loss: 0.04553837, Validation loss: 0.04750266, Gradient norm: 34.08171220
INFO:root:[  521] Training loss: 0.04544077, Validation loss: 0.04475642, Gradient norm: 32.61733984
INFO:root:[  522] Training loss: 0.04533762, Validation loss: 0.04545843, Gradient norm: 28.33075685
INFO:root:[  523] Training loss: 0.04673851, Validation loss: 0.04524610, Gradient norm: 58.88093714
INFO:root:[  524] Training loss: 0.04470143, Validation loss: 0.04472605, Gradient norm: 20.40407232
INFO:root:[  525] Training loss: 0.04475095, Validation loss: 0.04590026, Gradient norm: 21.33255727
INFO:root:[  526] Training loss: 0.04523741, Validation loss: 0.04471294, Gradient norm: 38.60529303
INFO:root:[  527] Training loss: 0.04460134, Validation loss: 0.04553171, Gradient norm: 25.91633803
INFO:root:[  528] Training loss: 0.04568398, Validation loss: 0.04531338, Gradient norm: 46.51147651
INFO:root:[  529] Training loss: 0.04455080, Validation loss: 0.04476834, Gradient norm: 20.25092117
INFO:root:[  530] Training loss: 0.04476677, Validation loss: 0.04762492, Gradient norm: 32.73615264
INFO:root:[  531] Training loss: 0.04504343, Validation loss: 0.04660985, Gradient norm: 40.10065440
INFO:root:[  532] Training loss: 0.04538456, Validation loss: 0.04454613, Gradient norm: 43.31989969
INFO:root:[  533] Training loss: 0.04443442, Validation loss: 0.04420625, Gradient norm: 23.63292387
INFO:root:[  534] Training loss: 0.04440232, Validation loss: 0.04562985, Gradient norm: 28.41901569
INFO:root:[  535] Training loss: 0.04473169, Validation loss: 0.04409190, Gradient norm: 35.65721496
INFO:root:[  536] Training loss: 0.04466184, Validation loss: 0.05305574, Gradient norm: 24.28458423
INFO:root:[  537] Training loss: 0.04688567, Validation loss: 0.04447661, Gradient norm: 70.38063598
INFO:root:[  538] Training loss: 0.04411469, Validation loss: 0.04436603, Gradient norm: 21.72034402
INFO:root:[  539] Training loss: 0.04409484, Validation loss: 0.04604817, Gradient norm: 19.31669035
INFO:root:[  540] Training loss: 0.04479457, Validation loss: 0.04387665, Gradient norm: 37.67469325
INFO:root:[  541] Training loss: 0.04427479, Validation loss: 0.04397762, Gradient norm: 31.77483890
INFO:root:[  542] Training loss: 0.04426788, Validation loss: 0.04623393, Gradient norm: 33.61261333
INFO:root:[  543] Training loss: 0.04488543, Validation loss: 0.04705895, Gradient norm: 45.09622101
INFO:root:[  544] Training loss: 0.04453007, Validation loss: 0.04498506, Gradient norm: 39.10781667
INFO:root:[  545] Training loss: 0.04397813, Validation loss: 0.04552410, Gradient norm: 27.89837556
INFO:root:[  546] Training loss: 0.04408934, Validation loss: 0.04449757, Gradient norm: 31.55295619
INFO:root:[  547] Training loss: 0.04453710, Validation loss: 0.04429778, Gradient norm: 41.90381636
INFO:root:[  548] Training loss: 0.04400231, Validation loss: 0.04375028, Gradient norm: 29.30180131
INFO:root:[  549] Training loss: 0.04433251, Validation loss: 0.04491140, Gradient norm: 38.40468549
