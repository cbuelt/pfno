INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 2097152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99398727, Validation loss: 0.98846023, Gradient norm: 0.02319189
INFO:root:[    2] Training loss: 0.97852067, Validation loss: 0.96928538, Gradient norm: 0.09828555
INFO:root:[    3] Training loss: 0.95754668, Validation loss: 0.94563171, Gradient norm: 0.15268158
INFO:root:[    4] Training loss: 0.93931052, Validation loss: 0.93076027, Gradient norm: 0.19462113
INFO:root:[    5] Training loss: 0.92538991, Validation loss: 0.92121352, Gradient norm: 0.21228913
INFO:root:[    6] Training loss: 0.91457456, Validation loss: 0.91011152, Gradient norm: 0.25257728
INFO:root:[    7] Training loss: 0.90618728, Validation loss: 0.90173988, Gradient norm: 0.24386751
INFO:root:[    8] Training loss: 0.90052881, Validation loss: 0.89817371, Gradient norm: 0.25699044
INFO:root:[    9] Training loss: 0.89491088, Validation loss: 0.89434440, Gradient norm: 0.25368736
INFO:root:[   10] Training loss: 0.89148014, Validation loss: 0.89469254, Gradient norm: 0.26592353
INFO:root:[   11] Training loss: 0.88755899, Validation loss: 0.88668446, Gradient norm: 0.22800819
INFO:root:[   12] Training loss: 0.88451668, Validation loss: 0.89154619, Gradient norm: 0.26341535
INFO:root:[   13] Training loss: 0.88243105, Validation loss: 0.88277936, Gradient norm: 0.24425533
INFO:root:[   14] Training loss: 0.87859921, Validation loss: 0.88478287, Gradient norm: 0.27217261
INFO:root:[   15] Training loss: 0.87667146, Validation loss: 0.88136484, Gradient norm: 0.24624800
INFO:root:[   16] Training loss: 0.87339884, Validation loss: 0.87926435, Gradient norm: 0.23825333
INFO:root:[   17] Training loss: 0.87254057, Validation loss: 0.87524235, Gradient norm: 0.30306367
INFO:root:[   18] Training loss: 0.86926748, Validation loss: 0.87816741, Gradient norm: 0.25811790
INFO:root:[   19] Training loss: 0.86866486, Validation loss: 0.87295297, Gradient norm: 0.27162398
INFO:root:[   20] Training loss: 0.86587820, Validation loss: 0.87420271, Gradient norm: 0.28552110
INFO:root:[   21] Training loss: 0.86526009, Validation loss: 0.87361293, Gradient norm: 0.30451219
INFO:root:[   22] Training loss: 0.86224153, Validation loss: 0.87289662, Gradient norm: 0.24944300
INFO:root:[   23] Training loss: 0.86212187, Validation loss: 0.86935862, Gradient norm: 0.28913049
INFO:root:[   24] Training loss: 0.85874190, Validation loss: 0.87302520, Gradient norm: 0.23653873
INFO:root:[   25] Training loss: 0.85853811, Validation loss: 0.86964699, Gradient norm: 0.34746162
INFO:root:[   26] Training loss: 0.85762446, Validation loss: 0.87156004, Gradient norm: 0.30046948
INFO:root:[   27] Training loss: 0.85475553, Validation loss: 0.86822002, Gradient norm: 0.28412480
INFO:root:[   28] Training loss: 0.85428844, Validation loss: 0.86798798, Gradient norm: 0.25957369
INFO:root:[   29] Training loss: 0.85278480, Validation loss: 0.87556387, Gradient norm: 0.28582221
INFO:root:[   30] Training loss: 0.85238688, Validation loss: 0.86687165, Gradient norm: 0.38180465
INFO:root:[   31] Training loss: 0.85030279, Validation loss: 0.86532408, Gradient norm: 0.28201141
INFO:root:[   32] Training loss: 0.84893336, Validation loss: 0.86705912, Gradient norm: 0.29952948
INFO:root:[   33] Training loss: 0.84865564, Validation loss: 0.86340425, Gradient norm: 0.34972471
INFO:root:[   34] Training loss: 0.84595225, Validation loss: 0.86429482, Gradient norm: 0.30296233
INFO:root:[   35] Training loss: 0.84658977, Validation loss: 0.86457959, Gradient norm: 0.26685830
INFO:root:[   36] Training loss: 0.84518106, Validation loss: 0.86322265, Gradient norm: 0.35032328
INFO:root:[   37] Training loss: 0.84444532, Validation loss: 0.86315181, Gradient norm: 0.37307914
INFO:root:[   38] Training loss: 0.84243095, Validation loss: 0.86199328, Gradient norm: 0.29457234
INFO:root:[   39] Training loss: 0.84037925, Validation loss: 0.86145699, Gradient norm: 0.26879146
INFO:root:[   40] Training loss: 0.84004886, Validation loss: 0.86441500, Gradient norm: 0.30793000
INFO:root:[   41] Training loss: 0.84077024, Validation loss: 0.86284755, Gradient norm: 0.37044537
INFO:root:[   42] Training loss: 0.83785845, Validation loss: 0.86206883, Gradient norm: 0.30352832
INFO:root:[   43] Training loss: 0.83738795, Validation loss: 0.86243621, Gradient norm: 0.36405868
INFO:root:[   44] Training loss: 0.83750007, Validation loss: 0.86244321, Gradient norm: 0.25716494
INFO:root:[   45] Training loss: 0.83521022, Validation loss: 0.86115930, Gradient norm: 0.32746786
INFO:root:[   46] Training loss: 0.83525148, Validation loss: 0.86566296, Gradient norm: 0.39880811
INFO:root:[   47] Training loss: 0.83351358, Validation loss: 0.86097907, Gradient norm: 0.34277619
INFO:root:[   48] Training loss: 0.83192699, Validation loss: 0.86284545, Gradient norm: 0.30018998
INFO:root:[   49] Training loss: 0.83237514, Validation loss: 0.86412100, Gradient norm: 0.32627053
INFO:root:[   50] Training loss: 0.83167294, Validation loss: 0.86556260, Gradient norm: 0.31548279
INFO:root:[   51] Training loss: 0.83040523, Validation loss: 0.86454408, Gradient norm: 0.37375322
INFO:root:[   52] Training loss: 0.82982971, Validation loss: 0.86454381, Gradient norm: 0.34380457
INFO:root:[   53] Training loss: 0.82778686, Validation loss: 0.86778804, Gradient norm: 0.30123796
INFO:root:[   54] Training loss: 0.82780317, Validation loss: 0.86725636, Gradient norm: 0.32255905
INFO:root:[   55] Training loss: 0.82831061, Validation loss: 0.86381849, Gradient norm: 0.44205956
INFO:root:[   56] Training loss: 0.82662016, Validation loss: 0.86420016, Gradient norm: 0.37485723
INFO:root:[   57] Training loss: 0.82497269, Validation loss: 0.86654370, Gradient norm: 0.37175302
INFO:root:[   58] Training loss: 0.82327416, Validation loss: 0.86335689, Gradient norm: 0.23232745
INFO:root:[   59] Training loss: 0.82405802, Validation loss: 0.86822256, Gradient norm: 0.35434528
INFO:root:[   60] Training loss: 0.82408804, Validation loss: 0.86585367, Gradient norm: 0.41234338
INFO:root:[   61] Training loss: 0.82158970, Validation loss: 0.86980979, Gradient norm: 0.34467244
INFO:root:[   62] Training loss: 0.82234256, Validation loss: 0.86758345, Gradient norm: 0.40267748
INFO:root:[   63] Training loss: 0.82020978, Validation loss: 0.86802008, Gradient norm: 0.34932860
INFO:root:[   64] Training loss: 0.82026042, Validation loss: 0.86495693, Gradient norm: 0.38833877
INFO:root:[   65] Training loss: 0.81928630, Validation loss: 0.86983143, Gradient norm: 0.38295851
INFO:root:[   66] Training loss: 0.81817632, Validation loss: 0.86661480, Gradient norm: 0.36844118
INFO:root:[   67] Training loss: 0.81849600, Validation loss: 0.87092193, Gradient norm: 0.40490518
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 2229.578s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.83403
INFO:root:EnergyScoreTrain: 0.71811
INFO:root:CRPSTrain: 0.59094
INFO:root:Gaussian NLLTrain: 1208.28145
INFO:root:CoverageTrain: 0.18676
INFO:root:IntervalWidthTrain: 0.29353
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86935
INFO:root:EnergyScoreValidation: 0.75316
INFO:root:CRPSValidation: 0.62068
INFO:root:Gaussian NLLValidation: 1498.04867
INFO:root:CoverageValidation: 0.17841
INFO:root:IntervalWidthValidation: 0.29358
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86991
INFO:root:EnergyScoreTest: 0.75645
INFO:root:CRPSTest: 0.62276
INFO:root:Gaussian NLLTest: 1308.19507
INFO:root:CoverageTest: 0.17524
INFO:root:IntervalWidthTest: 0.28712
INFO:root:###2 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99403043, Validation loss: 0.98849287, Gradient norm: 0.02256553
INFO:root:[    2] Training loss: 0.97852888, Validation loss: 0.96640010, Gradient norm: 0.08450400
INFO:root:[    3] Training loss: 0.95843661, Validation loss: 0.94535753, Gradient norm: 0.16172200
INFO:root:[    4] Training loss: 0.93995154, Validation loss: 0.93050721, Gradient norm: 0.16947793
INFO:root:[    5] Training loss: 0.92639724, Validation loss: 0.91767572, Gradient norm: 0.19776212
INFO:root:[    6] Training loss: 0.91523209, Validation loss: 0.90969490, Gradient norm: 0.22695488
INFO:root:[    7] Training loss: 0.90796968, Validation loss: 0.90147676, Gradient norm: 0.20785871
INFO:root:[    8] Training loss: 0.90092410, Validation loss: 0.89672543, Gradient norm: 0.20042007
INFO:root:[    9] Training loss: 0.89680388, Validation loss: 0.89373238, Gradient norm: 0.21482149
INFO:root:[   10] Training loss: 0.89325275, Validation loss: 0.89238604, Gradient norm: 0.22588916
INFO:root:[   11] Training loss: 0.88799132, Validation loss: 0.88831436, Gradient norm: 0.16436753
INFO:root:[   12] Training loss: 0.88562639, Validation loss: 0.88617688, Gradient norm: 0.20187442
INFO:root:[   13] Training loss: 0.88243108, Validation loss: 0.88380796, Gradient norm: 0.18260387
INFO:root:[   14] Training loss: 0.88066659, Validation loss: 0.88229647, Gradient norm: 0.24636675
INFO:root:[   15] Training loss: 0.87683802, Validation loss: 0.87906472, Gradient norm: 0.18386455
INFO:root:[   16] Training loss: 0.87478367, Validation loss: 0.87613451, Gradient norm: 0.19969870
INFO:root:[   17] Training loss: 0.87344383, Validation loss: 0.87582986, Gradient norm: 0.22793043
INFO:root:[   18] Training loss: 0.87019123, Validation loss: 0.87228049, Gradient norm: 0.14754628
INFO:root:[   19] Training loss: 0.86772969, Validation loss: 0.87146878, Gradient norm: 0.13905169
INFO:root:[   20] Training loss: 0.86833384, Validation loss: 0.87095573, Gradient norm: 0.28511814
INFO:root:[   21] Training loss: 0.86534554, Validation loss: 0.86794898, Gradient norm: 0.22433294
INFO:root:[   22] Training loss: 0.86331057, Validation loss: 0.86989691, Gradient norm: 0.24605951
INFO:root:[   23] Training loss: 0.86156718, Validation loss: 0.86607503, Gradient norm: 0.16626138
INFO:root:[   24] Training loss: 0.86005917, Validation loss: 0.86743822, Gradient norm: 0.22488771
INFO:root:[   25] Training loss: 0.85858126, Validation loss: 0.86606829, Gradient norm: 0.22211403
INFO:root:[   26] Training loss: 0.85798561, Validation loss: 0.86525508, Gradient norm: 0.27026740
INFO:root:[   27] Training loss: 0.85533420, Validation loss: 0.86364126, Gradient norm: 0.19142326
INFO:root:[   28] Training loss: 0.85321607, Validation loss: 0.86398908, Gradient norm: 0.18509379
INFO:root:[   29] Training loss: 0.85309749, Validation loss: 0.86408468, Gradient norm: 0.20478822
INFO:root:[   30] Training loss: 0.85207077, Validation loss: 0.86590087, Gradient norm: 0.25452754
INFO:root:[   31] Training loss: 0.85056514, Validation loss: 0.86035081, Gradient norm: 0.21178215
INFO:root:[   32] Training loss: 0.84873583, Validation loss: 0.86100186, Gradient norm: 0.20018590
INFO:root:[   33] Training loss: 0.84912262, Validation loss: 0.85977441, Gradient norm: 0.27916486
INFO:root:[   34] Training loss: 0.84718010, Validation loss: 0.86055126, Gradient norm: 0.26636478
INFO:root:[   35] Training loss: 0.84616563, Validation loss: 0.86072508, Gradient norm: 0.25067815
INFO:root:[   36] Training loss: 0.84440025, Validation loss: 0.85765138, Gradient norm: 0.21765763
INFO:root:[   37] Training loss: 0.84394372, Validation loss: 0.85925724, Gradient norm: 0.24859243
INFO:root:[   38] Training loss: 0.84202396, Validation loss: 0.85927728, Gradient norm: 0.21488297
INFO:root:[   39] Training loss: 0.84006796, Validation loss: 0.85839653, Gradient norm: 0.14334220
INFO:root:[   40] Training loss: 0.84073447, Validation loss: 0.85908091, Gradient norm: 0.28224625
INFO:root:[   41] Training loss: 0.83947306, Validation loss: 0.85930452, Gradient norm: 0.21340231
INFO:root:[   42] Training loss: 0.83754397, Validation loss: 0.85751881, Gradient norm: 0.22311535
INFO:root:[   43] Training loss: 0.83823062, Validation loss: 0.86352901, Gradient norm: 0.27770922
INFO:root:[   44] Training loss: 0.83668737, Validation loss: 0.85649693, Gradient norm: 0.23889450
INFO:root:[   45] Training loss: 0.83537006, Validation loss: 0.86061463, Gradient norm: 0.25563904
INFO:root:[   46] Training loss: 0.83380977, Validation loss: 0.85894521, Gradient norm: 0.25846627
INFO:root:[   47] Training loss: 0.83274240, Validation loss: 0.85788820, Gradient norm: 0.19673342
INFO:root:[   48] Training loss: 0.83286222, Validation loss: 0.85841992, Gradient norm: 0.23770557
INFO:root:[   49] Training loss: 0.83215846, Validation loss: 0.86269519, Gradient norm: 0.29289395
INFO:root:[   50] Training loss: 0.83138484, Validation loss: 0.85921776, Gradient norm: 0.25654146
INFO:root:[   51] Training loss: 0.82792690, Validation loss: 0.85822766, Gradient norm: 0.15090503
INFO:root:[   52] Training loss: 0.82966228, Validation loss: 0.85964067, Gradient norm: 0.29247973
INFO:root:[   53] Training loss: 0.82859612, Validation loss: 0.86233778, Gradient norm: 0.28609742
INFO:root:[   54] Training loss: 0.82837787, Validation loss: 0.86110147, Gradient norm: 0.28346395
INFO:root:[   55] Training loss: 0.82642407, Validation loss: 0.86062874, Gradient norm: 0.25598588
INFO:root:[   56] Training loss: 0.82523078, Validation loss: 0.86075888, Gradient norm: 0.23302992
INFO:root:[   57] Training loss: 0.82512172, Validation loss: 0.86054959, Gradient norm: 0.30635351
INFO:root:[   58] Training loss: 0.82298908, Validation loss: 0.86080556, Gradient norm: 0.22171702
INFO:root:[   59] Training loss: 0.82432142, Validation loss: 0.86197849, Gradient norm: 0.29642569
INFO:root:[   60] Training loss: 0.82183215, Validation loss: 0.86655640, Gradient norm: 0.27102733
INFO:root:[   61] Training loss: 0.82131390, Validation loss: 0.86272812, Gradient norm: 0.25550409
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1963.924s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.83209
INFO:root:EnergyScoreTrain: 0.72557
INFO:root:CRPSTrain: 0.59601
INFO:root:Gaussian NLLTrain: 2242.87726
INFO:root:CoverageTrain: 0.17507
INFO:root:IntervalWidthTrain: 0.2644
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86531
INFO:root:EnergyScoreValidation: 0.75673
INFO:root:CRPSValidation: 0.62398
INFO:root:Gaussian NLLValidation: 1918.18881
INFO:root:CoverageValidation: 0.16665
INFO:root:IntervalWidthValidation: 0.26706
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86491
INFO:root:EnergyScoreTest: 0.75946
INFO:root:CRPSTest: 0.62337
INFO:root:Gaussian NLLTest: 2517.59682
INFO:root:CoverageTest: 0.16783
INFO:root:IntervalWidthTest: 0.264
INFO:root:###3 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99408244, Validation loss: 0.98854105, Gradient norm: 0.02149530
INFO:root:[    2] Training loss: 0.97929985, Validation loss: 0.96853674, Gradient norm: 0.08871162
INFO:root:[    3] Training loss: 0.95943154, Validation loss: 0.94742798, Gradient norm: 0.12985049
INFO:root:[    4] Training loss: 0.94160753, Validation loss: 0.93155164, Gradient norm: 0.14781112
INFO:root:[    5] Training loss: 0.92855355, Validation loss: 0.92234818, Gradient norm: 0.16154337
INFO:root:[    6] Training loss: 0.91790900, Validation loss: 0.91228070, Gradient norm: 0.19912253
INFO:root:[    7] Training loss: 0.91034496, Validation loss: 0.90281475, Gradient norm: 0.18073790
INFO:root:[    8] Training loss: 0.90355779, Validation loss: 0.89942804, Gradient norm: 0.16169671
INFO:root:[    9] Training loss: 0.89959052, Validation loss: 0.89508229, Gradient norm: 0.21205593
INFO:root:[   10] Training loss: 0.89544251, Validation loss: 0.89107439, Gradient norm: 0.18146311
INFO:root:[   11] Training loss: 0.89127192, Validation loss: 0.88605500, Gradient norm: 0.16309061
INFO:root:[   12] Training loss: 0.88843909, Validation loss: 0.88605874, Gradient norm: 0.16724197
INFO:root:[   13] Training loss: 0.88545812, Validation loss: 0.88341939, Gradient norm: 0.18761789
INFO:root:[   14] Training loss: 0.88199962, Validation loss: 0.87983749, Gradient norm: 0.16254739
INFO:root:[   15] Training loss: 0.88022642, Validation loss: 0.87979774, Gradient norm: 0.19942168
INFO:root:[   16] Training loss: 0.87751823, Validation loss: 0.87491138, Gradient norm: 0.16361728
INFO:root:[   17] Training loss: 0.87485920, Validation loss: 0.87454627, Gradient norm: 0.17935364
INFO:root:[   18] Training loss: 0.87352665, Validation loss: 0.87280650, Gradient norm: 0.19852769
INFO:root:[   19] Training loss: 0.87084842, Validation loss: 0.87274917, Gradient norm: 0.13397576
INFO:root:[   20] Training loss: 0.86945565, Validation loss: 0.87513151, Gradient norm: 0.21161230
INFO:root:[   21] Training loss: 0.86746516, Validation loss: 0.86794179, Gradient norm: 0.15011054
INFO:root:[   22] Training loss: 0.86532148, Validation loss: 0.87142499, Gradient norm: 0.17145865
INFO:root:[   23] Training loss: 0.86352673, Validation loss: 0.86700164, Gradient norm: 0.13861262
INFO:root:[   24] Training loss: 0.86332167, Validation loss: 0.86559154, Gradient norm: 0.23883251
INFO:root:[   25] Training loss: 0.86046646, Validation loss: 0.86461731, Gradient norm: 0.13270532
INFO:root:[   26] Training loss: 0.85975575, Validation loss: 0.86374926, Gradient norm: 0.19942663
INFO:root:[   27] Training loss: 0.85765429, Validation loss: 0.86299570, Gradient norm: 0.16211458
INFO:root:[   28] Training loss: 0.85633650, Validation loss: 0.86261041, Gradient norm: 0.17617546
INFO:root:[   29] Training loss: 0.85550782, Validation loss: 0.86493322, Gradient norm: 0.16758669
INFO:root:[   30] Training loss: 0.85483017, Validation loss: 0.86264393, Gradient norm: 0.21659692
INFO:root:[   31] Training loss: 0.85220804, Validation loss: 0.86231318, Gradient norm: 0.16852231
INFO:root:[   32] Training loss: 0.85181589, Validation loss: 0.86151552, Gradient norm: 0.17330195
INFO:root:[   33] Training loss: 0.85175458, Validation loss: 0.86065976, Gradient norm: 0.23840748
INFO:root:[   34] Training loss: 0.85007782, Validation loss: 0.85878187, Gradient norm: 0.20182075
INFO:root:[   35] Training loss: 0.84864419, Validation loss: 0.85964896, Gradient norm: 0.16314849
INFO:root:[   36] Training loss: 0.84602874, Validation loss: 0.85819774, Gradient norm: 0.13200631
INFO:root:[   37] Training loss: 0.84593615, Validation loss: 0.85897847, Gradient norm: 0.15346627
INFO:root:[   38] Training loss: 0.84579687, Validation loss: 0.85748808, Gradient norm: 0.19775310
INFO:root:[   39] Training loss: 0.84399677, Validation loss: 0.86091368, Gradient norm: 0.15223623
INFO:root:[   40] Training loss: 0.84358589, Validation loss: 0.85811595, Gradient norm: 0.18480804
INFO:root:[   41] Training loss: 0.84349951, Validation loss: 0.85765125, Gradient norm: 0.22566378
INFO:root:[   42] Training loss: 0.84067415, Validation loss: 0.85758355, Gradient norm: 0.16095756
INFO:root:[   43] Training loss: 0.84077321, Validation loss: 0.85920039, Gradient norm: 0.20103556
INFO:root:[   44] Training loss: 0.84122722, Validation loss: 0.85665051, Gradient norm: 0.24453499
INFO:root:[   45] Training loss: 0.83791511, Validation loss: 0.85808724, Gradient norm: 0.15106726
INFO:root:[   46] Training loss: 0.83776071, Validation loss: 0.85847549, Gradient norm: 0.20895823
INFO:root:[   47] Training loss: 0.83821685, Validation loss: 0.85707341, Gradient norm: 0.19708839
INFO:root:[   48] Training loss: 0.83604263, Validation loss: 0.85744224, Gradient norm: 0.17574578
INFO:root:[   49] Training loss: 0.83631361, Validation loss: 0.85685458, Gradient norm: 0.22840488
INFO:root:[   50] Training loss: 0.83544837, Validation loss: 0.85736323, Gradient norm: 0.22091843
INFO:root:[   51] Training loss: 0.83302772, Validation loss: 0.85677515, Gradient norm: 0.17245866
INFO:root:[   52] Training loss: 0.83369791, Validation loss: 0.85778323, Gradient norm: 0.17626842
INFO:root:[   53] Training loss: 0.83147238, Validation loss: 0.85879677, Gradient norm: 0.16929722
INFO:root:[   54] Training loss: 0.83337562, Validation loss: 0.85830021, Gradient norm: 0.24948078
INFO:root:[   55] Training loss: 0.83065982, Validation loss: 0.86007235, Gradient norm: 0.19621101
INFO:root:[   56] Training loss: 0.82957773, Validation loss: 0.85879856, Gradient norm: 0.17704777
INFO:root:[   57] Training loss: 0.82863451, Validation loss: 0.85754469, Gradient norm: 0.18551352
INFO:root:[   58] Training loss: 0.82965578, Validation loss: 0.85933535, Gradient norm: 0.24400656
INFO:root:[   59] Training loss: 0.82857643, Validation loss: 0.85939428, Gradient norm: 0.21409622
INFO:root:[   60] Training loss: 0.82680929, Validation loss: 0.86065044, Gradient norm: 0.20368207
INFO:root:[   61] Training loss: 0.82577951, Validation loss: 0.85936340, Gradient norm: 0.17763581
INFO:root:[   62] Training loss: 0.82752867, Validation loss: 0.86035685, Gradient norm: 0.24656370
INFO:root:[   63] Training loss: 0.82548633, Validation loss: 0.85997018, Gradient norm: 0.22701276
INFO:root:[   64] Training loss: 0.82489306, Validation loss: 0.85986318, Gradient norm: 0.23055900
INFO:root:[   65] Training loss: 0.82347933, Validation loss: 0.85976747, Gradient norm: 0.20046297
INFO:root:[   66] Training loss: 0.82244396, Validation loss: 0.86013378, Gradient norm: 0.16654265
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 2097.902s.
INFO:root:Emptying the cuda cache took 0.018s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.8323
INFO:root:EnergyScoreTrain: 0.72211
INFO:root:CRPSTrain: 0.59364
INFO:root:Gaussian NLLTrain: 2378.49741
INFO:root:CoverageTrain: 0.18028
INFO:root:IntervalWidthTrain: 0.27196
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86311
INFO:root:EnergyScoreValidation: 0.75309
INFO:root:CRPSValidation: 0.61862
INFO:root:Gaussian NLLValidation: 1702.15045
INFO:root:CoverageValidation: 0.1732
INFO:root:IntervalWidthValidation: 0.27254
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86521
INFO:root:EnergyScoreTest: 0.75431
INFO:root:CRPSTest: 0.62184
INFO:root:Gaussian NLLTest: 2323.97226
INFO:root:CoverageTest: 0.1712
INFO:root:IntervalWidthTest: 0.27218
INFO:root:###4 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99410686, Validation loss: 0.98839087, Gradient norm: 0.02123430
INFO:root:[    2] Training loss: 0.97953393, Validation loss: 0.96776258, Gradient norm: 0.07981394
INFO:root:[    3] Training loss: 0.96022308, Validation loss: 0.94683517, Gradient norm: 0.11876937
INFO:root:[    4] Training loss: 0.94259242, Validation loss: 0.93118326, Gradient norm: 0.12642831
INFO:root:[    5] Training loss: 0.93033902, Validation loss: 0.91923484, Gradient norm: 0.14134864
INFO:root:[    6] Training loss: 0.92031533, Validation loss: 0.91133417, Gradient norm: 0.17055353
INFO:root:[    7] Training loss: 0.91283418, Validation loss: 0.90296099, Gradient norm: 0.13860788
INFO:root:[    8] Training loss: 0.90725905, Validation loss: 0.89840179, Gradient norm: 0.16343887
INFO:root:[    9] Training loss: 0.90213855, Validation loss: 0.89461686, Gradient norm: 0.15612073
INFO:root:[   10] Training loss: 0.89887195, Validation loss: 0.89119367, Gradient norm: 0.15084788
INFO:root:[   11] Training loss: 0.89437956, Validation loss: 0.88677538, Gradient norm: 0.13165384
INFO:root:[   12] Training loss: 0.89103924, Validation loss: 0.88622671, Gradient norm: 0.11257506
INFO:root:[   13] Training loss: 0.88877983, Validation loss: 0.88352655, Gradient norm: 0.14010734
INFO:root:[   14] Training loss: 0.88541466, Validation loss: 0.88001439, Gradient norm: 0.14479637
INFO:root:[   15] Training loss: 0.88353916, Validation loss: 0.87782637, Gradient norm: 0.14796321
INFO:root:[   16] Training loss: 0.88086641, Validation loss: 0.87499508, Gradient norm: 0.13531559
INFO:root:[   17] Training loss: 0.87877247, Validation loss: 0.87283754, Gradient norm: 0.14024477
INFO:root:[   18] Training loss: 0.87733475, Validation loss: 0.87221792, Gradient norm: 0.13609581
INFO:root:[   19] Training loss: 0.87443165, Validation loss: 0.87185102, Gradient norm: 0.10841627
INFO:root:[   20] Training loss: 0.87347138, Validation loss: 0.87270532, Gradient norm: 0.16697537
INFO:root:[   21] Training loss: 0.87140893, Validation loss: 0.86793941, Gradient norm: 0.13363432
INFO:root:[   22] Training loss: 0.87001735, Validation loss: 0.86990400, Gradient norm: 0.15228730
INFO:root:[   23] Training loss: 0.86800335, Validation loss: 0.86546932, Gradient norm: 0.12768177
INFO:root:[   24] Training loss: 0.86630621, Validation loss: 0.86825078, Gradient norm: 0.11993077
INFO:root:[   25] Training loss: 0.86575745, Validation loss: 0.86333599, Gradient norm: 0.16356617
INFO:root:[   26] Training loss: 0.86443899, Validation loss: 0.86301919, Gradient norm: 0.14859804
INFO:root:[   27] Training loss: 0.86211295, Validation loss: 0.86362749, Gradient norm: 0.11756200
INFO:root:[   28] Training loss: 0.86132835, Validation loss: 0.86112423, Gradient norm: 0.15962501
INFO:root:[   29] Training loss: 0.86094992, Validation loss: 0.86014474, Gradient norm: 0.16251437
INFO:root:[   30] Training loss: 0.85812910, Validation loss: 0.85850693, Gradient norm: 0.12261073
INFO:root:[   31] Training loss: 0.85675683, Validation loss: 0.85870718, Gradient norm: 0.12507780
INFO:root:[   32] Training loss: 0.85749626, Validation loss: 0.85963592, Gradient norm: 0.16174935
INFO:root:[   33] Training loss: 0.85492851, Validation loss: 0.85764149, Gradient norm: 0.11415334
INFO:root:[   34] Training loss: 0.85594692, Validation loss: 0.85762913, Gradient norm: 0.19832022
INFO:root:[   35] Training loss: 0.85320647, Validation loss: 0.85938824, Gradient norm: 0.13085685
INFO:root:[   36] Training loss: 0.85236857, Validation loss: 0.85534544, Gradient norm: 0.14178284
INFO:root:[   37] Training loss: 0.85175840, Validation loss: 0.85395992, Gradient norm: 0.14364243
INFO:root:[   38] Training loss: 0.85078124, Validation loss: 0.85508741, Gradient norm: 0.16002580
INFO:root:[   39] Training loss: 0.84980042, Validation loss: 0.85351462, Gradient norm: 0.14730098
INFO:root:[   40] Training loss: 0.84922105, Validation loss: 0.85363102, Gradient norm: 0.14187719
INFO:root:[   41] Training loss: 0.84776575, Validation loss: 0.85979309, Gradient norm: 0.14394014
INFO:root:[   42] Training loss: 0.84781457, Validation loss: 0.85291644, Gradient norm: 0.14611661
INFO:root:[   43] Training loss: 0.84563888, Validation loss: 0.85265899, Gradient norm: 0.11660807
INFO:root:[   44] Training loss: 0.84620820, Validation loss: 0.85309946, Gradient norm: 0.15842153
INFO:root:[   45] Training loss: 0.84509853, Validation loss: 0.85559788, Gradient norm: 0.15462062
INFO:root:[   46] Training loss: 0.84474444, Validation loss: 0.85222113, Gradient norm: 0.16721192
INFO:root:[   47] Training loss: 0.84295929, Validation loss: 0.85231923, Gradient norm: 0.13591669
INFO:root:[   48] Training loss: 0.84202075, Validation loss: 0.85295756, Gradient norm: 0.14574147
INFO:root:[   49] Training loss: 0.84202605, Validation loss: 0.85152324, Gradient norm: 0.14166844
INFO:root:[   50] Training loss: 0.84184037, Validation loss: 0.85315163, Gradient norm: 0.16958792
INFO:root:[   51] Training loss: 0.84048435, Validation loss: 0.85085131, Gradient norm: 0.15400771
INFO:root:[   52] Training loss: 0.84063387, Validation loss: 0.85300619, Gradient norm: 0.16986731
INFO:root:[   53] Training loss: 0.83819384, Validation loss: 0.85157801, Gradient norm: 0.13161261
INFO:root:[   54] Training loss: 0.83915328, Validation loss: 0.85260638, Gradient norm: 0.16702919
INFO:root:[   55] Training loss: 0.83718287, Validation loss: 0.85145018, Gradient norm: 0.13888949
INFO:root:[   56] Training loss: 0.83749120, Validation loss: 0.85138288, Gradient norm: 0.16250373
INFO:root:[   57] Training loss: 0.83576849, Validation loss: 0.85244833, Gradient norm: 0.12537800
INFO:root:[   58] Training loss: 0.83602052, Validation loss: 0.85197871, Gradient norm: 0.15820117
INFO:root:[   59] Training loss: 0.83545595, Validation loss: 0.85124011, Gradient norm: 0.16031352
INFO:root:[   60] Training loss: 0.83410524, Validation loss: 0.85142236, Gradient norm: 0.14317033
INFO:root:[   61] Training loss: 0.83437640, Validation loss: 0.84991642, Gradient norm: 0.15918739
INFO:root:[   62] Training loss: 0.83378865, Validation loss: 0.85179229, Gradient norm: 0.16132389
INFO:root:[   63] Training loss: 0.83254163, Validation loss: 0.85075003, Gradient norm: 0.13270568
INFO:root:[   64] Training loss: 0.83300993, Validation loss: 0.85128186, Gradient norm: 0.16909787
INFO:root:[   65] Training loss: 0.83106443, Validation loss: 0.85117210, Gradient norm: 0.11834284
INFO:root:[   66] Training loss: 0.83203907, Validation loss: 0.85073553, Gradient norm: 0.18116456
INFO:root:[   67] Training loss: 0.83103905, Validation loss: 0.85012730, Gradient norm: 0.17885527
INFO:root:[   68] Training loss: 0.83077133, Validation loss: 0.85074577, Gradient norm: 0.16264010
INFO:root:[   69] Training loss: 0.83046435, Validation loss: 0.85451516, Gradient norm: 0.16981051
INFO:root:[   70] Training loss: 0.82974440, Validation loss: 0.85572066, Gradient norm: 0.17141013
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 2239.387s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.81724
INFO:root:EnergyScoreTrain: 0.7272
INFO:root:CRPSTrain: 0.58524
INFO:root:Gaussian NLLTrain: 2295.49375
INFO:root:CoverageTrain: 0.1673
INFO:root:IntervalWidthTrain: 0.23094
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.8546
INFO:root:EnergyScoreValidation: 0.76765
INFO:root:CRPSValidation: 0.61837
INFO:root:Gaussian NLLValidation: 2970.76086
INFO:root:CoverageValidation: 0.15558
INFO:root:IntervalWidthValidation: 0.22492
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.85483
INFO:root:EnergyScoreTest: 0.75958
INFO:root:CRPSTest: 0.61567
INFO:root:Gaussian NLLTest: 2607.86128
INFO:root:CoverageTest: 0.16665
INFO:root:IntervalWidthTest: 0.24299
INFO:root:###5 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99434437, Validation loss: 0.98851846, Gradient norm: 0.01992011
INFO:root:[    2] Training loss: 0.98079731, Validation loss: 0.96935267, Gradient norm: 0.06062259
INFO:root:[    3] Training loss: 0.96492979, Validation loss: 0.95129787, Gradient norm: 0.09648645
INFO:root:[    4] Training loss: 0.94962413, Validation loss: 0.93508255, Gradient norm: 0.09652310
INFO:root:[    5] Training loss: 0.93801025, Validation loss: 0.92450963, Gradient norm: 0.08612107
INFO:root:[    6] Training loss: 0.92898436, Validation loss: 0.91530998, Gradient norm: 0.12708294
INFO:root:[    7] Training loss: 0.92166386, Validation loss: 0.90733828, Gradient norm: 0.08677628
INFO:root:[    8] Training loss: 0.91550887, Validation loss: 0.90197055, Gradient norm: 0.11555495
INFO:root:[    9] Training loss: 0.91005298, Validation loss: 0.89718666, Gradient norm: 0.08735966
INFO:root:[   10] Training loss: 0.90693972, Validation loss: 0.89526892, Gradient norm: 0.10652219
INFO:root:[   11] Training loss: 0.90315435, Validation loss: 0.89081489, Gradient norm: 0.09824739
INFO:root:[   12] Training loss: 0.90054511, Validation loss: 0.88759730, Gradient norm: 0.09520761
INFO:root:[   13] Training loss: 0.89812547, Validation loss: 0.88753659, Gradient norm: 0.09621529
INFO:root:[   14] Training loss: 0.89506730, Validation loss: 0.88218202, Gradient norm: 0.10725770
INFO:root:[   15] Training loss: 0.89353683, Validation loss: 0.88056757, Gradient norm: 0.10315541
INFO:root:[   16] Training loss: 0.89021945, Validation loss: 0.87959145, Gradient norm: 0.07338141
INFO:root:[   17] Training loss: 0.88925082, Validation loss: 0.87756037, Gradient norm: 0.10525656
INFO:root:[   18] Training loss: 0.88738905, Validation loss: 0.87728650, Gradient norm: 0.10738115
INFO:root:[   19] Training loss: 0.88499611, Validation loss: 0.87385745, Gradient norm: 0.09238134
INFO:root:[   20] Training loss: 0.88321267, Validation loss: 0.87213102, Gradient norm: 0.07860266
INFO:root:[   21] Training loss: 0.88255108, Validation loss: 0.87153122, Gradient norm: 0.11428332
INFO:root:[   22] Training loss: 0.88094768, Validation loss: 0.87409928, Gradient norm: 0.09629567
INFO:root:[   23] Training loss: 0.87983344, Validation loss: 0.86996814, Gradient norm: 0.11589465
INFO:root:[   24] Training loss: 0.87783035, Validation loss: 0.87115657, Gradient norm: 0.08643457
INFO:root:[   25] Training loss: 0.87772417, Validation loss: 0.86964375, Gradient norm: 0.10875674
INFO:root:[   26] Training loss: 0.87546932, Validation loss: 0.86636263, Gradient norm: 0.09255012
INFO:root:[   27] Training loss: 0.87395208, Validation loss: 0.86361164, Gradient norm: 0.08956213
INFO:root:[   28] Training loss: 0.87317505, Validation loss: 0.86366568, Gradient norm: 0.09703647
INFO:root:[   29] Training loss: 0.87285593, Validation loss: 0.86283097, Gradient norm: 0.10637274
INFO:root:[   30] Training loss: 0.87208719, Validation loss: 0.86291958, Gradient norm: 0.11819478
INFO:root:[   31] Training loss: 0.86978492, Validation loss: 0.85983384, Gradient norm: 0.09594621
INFO:root:[   32] Training loss: 0.86968464, Validation loss: 0.85975019, Gradient norm: 0.10805632
INFO:root:[   33] Training loss: 0.86897707, Validation loss: 0.86197336, Gradient norm: 0.10647440
INFO:root:[   34] Training loss: 0.86795916, Validation loss: 0.85856300, Gradient norm: 0.09403412
INFO:root:[   35] Training loss: 0.86728823, Validation loss: 0.85740996, Gradient norm: 0.11181486
INFO:root:[   36] Training loss: 0.86572772, Validation loss: 0.85783375, Gradient norm: 0.09765958
INFO:root:[   37] Training loss: 0.86527718, Validation loss: 0.85565745, Gradient norm: 0.10068238
INFO:root:[   38] Training loss: 0.86492636, Validation loss: 0.85514469, Gradient norm: 0.10761969
INFO:root:[   39] Training loss: 0.86401653, Validation loss: 0.85461121, Gradient norm: 0.09970675
INFO:root:[   40] Training loss: 0.86256327, Validation loss: 0.85567693, Gradient norm: 0.09238274
INFO:root:[   41] Training loss: 0.86246872, Validation loss: 0.85861812, Gradient norm: 0.11596526
INFO:root:[   42] Training loss: 0.86188347, Validation loss: 0.85344413, Gradient norm: 0.10911009
INFO:root:[   43] Training loss: 0.86013663, Validation loss: 0.85360576, Gradient norm: 0.09450103
INFO:root:[   44] Training loss: 0.86195241, Validation loss: 0.85462470, Gradient norm: 0.13297980
INFO:root:[   45] Training loss: 0.86023674, Validation loss: 0.85678860, Gradient norm: 0.11577300
INFO:root:[   46] Training loss: 0.85907877, Validation loss: 0.85219962, Gradient norm: 0.11342808
INFO:root:[   47] Training loss: 0.85907900, Validation loss: 0.85196029, Gradient norm: 0.11889866
INFO:root:[   48] Training loss: 0.85729187, Validation loss: 0.85091876, Gradient norm: 0.09132243
INFO:root:[   49] Training loss: 0.85829972, Validation loss: 0.85014304, Gradient norm: 0.12941952
INFO:root:[   50] Training loss: 0.85698847, Validation loss: 0.85113684, Gradient norm: 0.10624809
INFO:root:[   51] Training loss: 0.85585551, Validation loss: 0.85188308, Gradient norm: 0.09277476
INFO:root:[   52] Training loss: 0.85743272, Validation loss: 0.84884645, Gradient norm: 0.13255826
INFO:root:[   53] Training loss: 0.85521665, Validation loss: 0.85051715, Gradient norm: 0.10559160
INFO:root:[   54] Training loss: 0.85524023, Validation loss: 0.84932850, Gradient norm: 0.12548257
INFO:root:[   55] Training loss: 0.85448791, Validation loss: 0.84774448, Gradient norm: 0.12051836
INFO:root:[   56] Training loss: 0.85334555, Validation loss: 0.84859114, Gradient norm: 0.11112290
INFO:root:[   57] Training loss: 0.85375113, Validation loss: 0.84940927, Gradient norm: 0.12439080
INFO:root:[   58] Training loss: 0.85320678, Validation loss: 0.84690004, Gradient norm: 0.11832608
INFO:root:[   59] Training loss: 0.85231784, Validation loss: 0.84901345, Gradient norm: 0.10857142
INFO:root:[   60] Training loss: 0.85231304, Validation loss: 0.84758911, Gradient norm: 0.11800757
INFO:root:[   61] Training loss: 0.85130716, Validation loss: 0.84860388, Gradient norm: 0.11146357
INFO:root:[   62] Training loss: 0.85213292, Validation loss: 0.84768948, Gradient norm: 0.13332234
INFO:root:[   63] Training loss: 0.85116142, Validation loss: 0.84646050, Gradient norm: 0.12576322
INFO:root:[   64] Training loss: 0.85164140, Validation loss: 0.84671927, Gradient norm: 0.14420293
INFO:root:[   65] Training loss: 0.84969666, Validation loss: 0.84640735, Gradient norm: 0.10634949
INFO:root:[   66] Training loss: 0.84985360, Validation loss: 0.84820873, Gradient norm: 0.12050411
INFO:root:[   67] Training loss: 0.84917970, Validation loss: 0.84591817, Gradient norm: 0.11709607
INFO:root:[   68] Training loss: 0.84909085, Validation loss: 0.84501183, Gradient norm: 0.12292148
INFO:root:[   69] Training loss: 0.84973718, Validation loss: 0.84880326, Gradient norm: 0.13748532
INFO:root:[   70] Training loss: 0.84842204, Validation loss: 0.84575424, Gradient norm: 0.12450671
INFO:root:[   71] Training loss: 0.84807176, Validation loss: 0.84554505, Gradient norm: 0.12674449
INFO:root:[   72] Training loss: 0.84703388, Validation loss: 0.84576044, Gradient norm: 0.11213357
INFO:root:[   73] Training loss: 0.84832413, Validation loss: 0.84635686, Gradient norm: 0.14658466
INFO:root:[   74] Training loss: 0.84678219, Validation loss: 0.84656490, Gradient norm: 0.13238873
INFO:root:[   75] Training loss: 0.84648041, Validation loss: 0.84355453, Gradient norm: 0.12717523
INFO:root:[   76] Training loss: 0.84668068, Validation loss: 0.84359157, Gradient norm: 0.13396652
INFO:root:[   77] Training loss: 0.84537140, Validation loss: 0.84274250, Gradient norm: 0.11069580
INFO:root:[   78] Training loss: 0.84595254, Validation loss: 0.84336868, Gradient norm: 0.13718887
INFO:root:[   79] Training loss: 0.84547768, Validation loss: 0.84371722, Gradient norm: 0.13422550
INFO:root:[   80] Training loss: 0.84535527, Validation loss: 0.84546687, Gradient norm: 0.13660163
INFO:root:[   81] Training loss: 0.84527406, Validation loss: 0.84690535, Gradient norm: 0.13527509
INFO:root:[   82] Training loss: 0.84452658, Validation loss: 0.84492814, Gradient norm: 0.13656839
INFO:root:[   83] Training loss: 0.84409370, Validation loss: 0.84486178, Gradient norm: 0.13035589
INFO:root:[   84] Training loss: 0.84351409, Validation loss: 0.84336423, Gradient norm: 0.12372346
INFO:root:[   85] Training loss: 0.84318944, Validation loss: 0.84495921, Gradient norm: 0.12542768
INFO:root:[   86] Training loss: 0.84437970, Validation loss: 0.84451519, Gradient norm: 0.16192007
INFO:root:EP 86: Early stopping
INFO:root:Training the model took 2738.434s.
INFO:root:Emptying the cuda cache took 0.021s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82112
INFO:root:EnergyScoreTrain: 0.74505
INFO:root:CRPSTrain: 0.59481
INFO:root:Gaussian NLLTrain: 6183.39917
INFO:root:CoverageTrain: 0.14949
INFO:root:IntervalWidthTrain: 0.19498
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.84747
INFO:root:EnergyScoreValidation: 0.77335
INFO:root:CRPSValidation: 0.61772
INFO:root:Gaussian NLLValidation: 4033.35905
INFO:root:CoverageValidation: 0.14236
INFO:root:IntervalWidthValidation: 0.19375
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.84824
INFO:root:EnergyScoreTest: 0.77504
INFO:root:CRPSTest: 0.61907
INFO:root:Gaussian NLLTest: 5267.49596
INFO:root:CoverageTest: 0.14082
INFO:root:IntervalWidthTest: 0.19072
INFO:root:###6 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99467106, Validation loss: 0.98914830, Gradient norm: 0.01680704
INFO:root:[    2] Training loss: 0.98320437, Validation loss: 0.97140012, Gradient norm: 0.04731193
INFO:root:[    3] Training loss: 0.96817949, Validation loss: 0.95489655, Gradient norm: 0.07596195
INFO:root:[    4] Training loss: 0.95606684, Validation loss: 0.94315162, Gradient norm: 0.07755614
INFO:root:[    5] Training loss: 0.94645755, Validation loss: 0.93164823, Gradient norm: 0.08184207
INFO:root:[    6] Training loss: 0.93843732, Validation loss: 0.92376783, Gradient norm: 0.08945750
INFO:root:[    7] Training loss: 0.93184729, Validation loss: 0.91481425, Gradient norm: 0.08138242
INFO:root:[    8] Training loss: 0.92593936, Validation loss: 0.91068170, Gradient norm: 0.09420375
INFO:root:[    9] Training loss: 0.92094800, Validation loss: 0.90414592, Gradient norm: 0.08090277
INFO:root:[   10] Training loss: 0.91694400, Validation loss: 0.89914821, Gradient norm: 0.08335165
INFO:root:[   11] Training loss: 0.91306900, Validation loss: 0.89642164, Gradient norm: 0.07997443
INFO:root:[   12] Training loss: 0.90995016, Validation loss: 0.89286153, Gradient norm: 0.07040439
INFO:root:[   13] Training loss: 0.90740912, Validation loss: 0.89164191, Gradient norm: 0.08389685
INFO:root:[   14] Training loss: 0.90484558, Validation loss: 0.88635105, Gradient norm: 0.07666188
INFO:root:[   15] Training loss: 0.90246203, Validation loss: 0.88654751, Gradient norm: 0.08875933
INFO:root:[   16] Training loss: 0.90041873, Validation loss: 0.88258175, Gradient norm: 0.07733313
INFO:root:[   17] Training loss: 0.89898940, Validation loss: 0.87966995, Gradient norm: 0.09658999
INFO:root:[   18] Training loss: 0.89671923, Validation loss: 0.87959740, Gradient norm: 0.07953997
INFO:root:[   19] Training loss: 0.89550088, Validation loss: 0.87840715, Gradient norm: 0.08612349
INFO:root:[   20] Training loss: 0.89376079, Validation loss: 0.87594198, Gradient norm: 0.08726958
INFO:root:[   21] Training loss: 0.89275824, Validation loss: 0.87374701, Gradient norm: 0.10880514
INFO:root:[   22] Training loss: 0.89098577, Validation loss: 0.87860305, Gradient norm: 0.09075239
INFO:root:[   23] Training loss: 0.89014896, Validation loss: 0.87107913, Gradient norm: 0.08980634
INFO:root:[   24] Training loss: 0.88826486, Validation loss: 0.87002933, Gradient norm: 0.10129075
INFO:root:[   25] Training loss: 0.88780548, Validation loss: 0.86871928, Gradient norm: 0.10459880
INFO:root:[   26] Training loss: 0.88628570, Validation loss: 0.86736832, Gradient norm: 0.08622396
INFO:root:[   27] Training loss: 0.88529209, Validation loss: 0.86655395, Gradient norm: 0.09595718
INFO:root:[   28] Training loss: 0.88530599, Validation loss: 0.86566096, Gradient norm: 0.10986238
INFO:root:[   29] Training loss: 0.88386581, Validation loss: 0.86498324, Gradient norm: 0.10888011
INFO:root:[   30] Training loss: 0.88324910, Validation loss: 0.86374600, Gradient norm: 0.12696067
INFO:root:[   31] Training loss: 0.88261461, Validation loss: 0.86231569, Gradient norm: 0.12475354
INFO:root:[   32] Training loss: 0.88131349, Validation loss: 0.86155894, Gradient norm: 0.10921295
INFO:root:[   33] Training loss: 0.88140751, Validation loss: 0.86271567, Gradient norm: 0.10887944
INFO:root:[   34] Training loss: 0.88025069, Validation loss: 0.86224973, Gradient norm: 0.12015456
INFO:root:[   35] Training loss: 0.87989337, Validation loss: 0.86114323, Gradient norm: 0.12431947
INFO:root:[   36] Training loss: 0.87930334, Validation loss: 0.85953909, Gradient norm: 0.12051750
INFO:root:[   37] Training loss: 0.87870758, Validation loss: 0.85763133, Gradient norm: 0.13774127
INFO:root:[   38] Training loss: 0.87856460, Validation loss: 0.85963678, Gradient norm: 0.12799671
INFO:root:[   39] Training loss: 0.87717818, Validation loss: 0.85826760, Gradient norm: 0.14454023
INFO:root:[   40] Training loss: 0.87644070, Validation loss: 0.85989646, Gradient norm: 0.12610353
INFO:root:[   41] Training loss: 0.87673002, Validation loss: 0.85583623, Gradient norm: 0.14124741
INFO:root:[   42] Training loss: 0.87674011, Validation loss: 0.85719714, Gradient norm: 0.14304377
INFO:root:[   43] Training loss: 0.87503170, Validation loss: 0.85778545, Gradient norm: 0.14064517
INFO:root:[   44] Training loss: 0.87554276, Validation loss: 0.85738815, Gradient norm: 0.15697307
INFO:root:[   45] Training loss: 0.87472137, Validation loss: 0.85557513, Gradient norm: 0.15626045
INFO:root:[   46] Training loss: 0.87370541, Validation loss: 0.85493683, Gradient norm: 0.14036561
INFO:root:[   47] Training loss: 0.87354579, Validation loss: 0.85431905, Gradient norm: 0.15970402
INFO:root:[   48] Training loss: 0.87316655, Validation loss: 0.85367815, Gradient norm: 0.15959547
INFO:root:[   49] Training loss: 0.87322594, Validation loss: 0.85874268, Gradient norm: 0.16930449
INFO:root:[   50] Training loss: 0.87300657, Validation loss: 0.85562663, Gradient norm: 0.15965632
INFO:root:[   51] Training loss: 0.87251126, Validation loss: 0.85708255, Gradient norm: 0.21481314
INFO:root:[   52] Training loss: 0.87244630, Validation loss: 0.85617144, Gradient norm: 0.17912735
INFO:root:[   53] Training loss: 0.87142095, Validation loss: 0.85228967, Gradient norm: 0.15886911
INFO:root:[   54] Training loss: 0.87123680, Validation loss: 0.85207941, Gradient norm: 0.16543671
INFO:root:[   55] Training loss: 0.87165400, Validation loss: 0.85426204, Gradient norm: 0.20857018
INFO:root:[   56] Training loss: 0.87157526, Validation loss: 0.85145514, Gradient norm: 0.20876505
INFO:root:[   57] Training loss: 0.86999455, Validation loss: 0.85196464, Gradient norm: 0.19416473
INFO:root:[   58] Training loss: 0.87109647, Validation loss: 0.85187952, Gradient norm: 0.20744092
INFO:root:[   59] Training loss: 0.86990084, Validation loss: 0.85116711, Gradient norm: 0.19096228
INFO:root:[   60] Training loss: 0.87018456, Validation loss: 0.85180047, Gradient norm: 0.22808202
INFO:root:[   61] Training loss: 0.87018091, Validation loss: 0.85374903, Gradient norm: 0.25865086
INFO:root:[   62] Training loss: 0.87002152, Validation loss: 0.85224180, Gradient norm: 0.21274788
INFO:root:[   63] Training loss: 0.86859831, Validation loss: 0.85164809, Gradient norm: 0.19356037
INFO:root:[   64] Training loss: 0.87047636, Validation loss: 0.85047143, Gradient norm: 0.28970303
INFO:root:[   65] Training loss: 0.86819117, Validation loss: 0.85296396, Gradient norm: 0.19098388
INFO:root:[   66] Training loss: 0.86851725, Validation loss: 0.84920759, Gradient norm: 0.22785722
INFO:root:[   67] Training loss: 0.86923203, Validation loss: 0.85299967, Gradient norm: 0.27466288
INFO:root:[   68] Training loss: 0.86850448, Validation loss: 0.85018755, Gradient norm: 0.28307266
INFO:root:[   69] Training loss: 0.86898381, Validation loss: 0.84985638, Gradient norm: 0.24113398
INFO:root:[   70] Training loss: 0.86836931, Validation loss: 0.85044066, Gradient norm: 0.26467277
INFO:root:[   71] Training loss: 0.86857806, Validation loss: 0.85089405, Gradient norm: 0.31231589
INFO:root:[   72] Training loss: 0.86812145, Validation loss: 0.85167661, Gradient norm: 0.29280078
INFO:root:[   73] Training loss: 0.86795828, Validation loss: 0.85105609, Gradient norm: 0.31408920
INFO:root:[   74] Training loss: 0.86751218, Validation loss: 0.85269438, Gradient norm: 0.31316317
INFO:root:[   75] Training loss: 0.86672261, Validation loss: 0.84961837, Gradient norm: 0.27378048
INFO:root:[   76] Training loss: 0.86711654, Validation loss: 0.84804531, Gradient norm: 0.28160681
INFO:root:[   77] Training loss: 0.86626209, Validation loss: 0.84788127, Gradient norm: 0.28422275
INFO:root:[   78] Training loss: 0.86716529, Validation loss: 0.84875514, Gradient norm: 0.34322096
INFO:root:[   79] Training loss: 0.86769291, Validation loss: 0.84751321, Gradient norm: 0.35910036
INFO:root:[   80] Training loss: 0.86699127, Validation loss: 0.85048990, Gradient norm: 0.35791145
INFO:root:[   81] Training loss: 0.86644361, Validation loss: 0.84802425, Gradient norm: 0.29081192
INFO:root:[   82] Training loss: 0.86625718, Validation loss: 0.84894173, Gradient norm: 0.36899705
INFO:root:[   83] Training loss: 0.86612631, Validation loss: 0.84983898, Gradient norm: 0.37828239
INFO:root:[   84] Training loss: 0.86705548, Validation loss: 0.84981452, Gradient norm: 0.39639103
INFO:root:[   85] Training loss: 0.86654406, Validation loss: 0.85350393, Gradient norm: 0.40397389
INFO:root:[   86] Training loss: 0.86543461, Validation loss: 0.84954489, Gradient norm: 0.34995932
INFO:root:[   87] Training loss: 0.86485851, Validation loss: 0.84549792, Gradient norm: 0.31073328
INFO:root:[   88] Training loss: 0.86556432, Validation loss: 0.85297583, Gradient norm: 0.39602389
INFO:root:[   89] Training loss: 0.86550766, Validation loss: 0.84933823, Gradient norm: 0.37391076
INFO:root:[   90] Training loss: 0.86564671, Validation loss: 0.84879054, Gradient norm: 0.37421957
INFO:root:[   91] Training loss: 0.86519429, Validation loss: 0.84918789, Gradient norm: 0.36811216
INFO:root:[   92] Training loss: 0.86696979, Validation loss: 0.84643217, Gradient norm: 0.50656672
INFO:root:[   93] Training loss: 0.86518858, Validation loss: 0.84771965, Gradient norm: 0.43436789
INFO:root:[   94] Training loss: 0.86466759, Validation loss: 0.84834000, Gradient norm: 0.36263360
INFO:root:[   95] Training loss: 0.86595526, Validation loss: 0.84923367, Gradient norm: 0.46173934
INFO:root:[   96] Training loss: 0.86568331, Validation loss: 0.84634770, Gradient norm: 0.48302080
INFO:root:[   97] Training loss: 0.86540078, Validation loss: 0.84547352, Gradient norm: 0.44999055
INFO:root:[   98] Training loss: 0.86450877, Validation loss: 0.84843037, Gradient norm: 0.43346439
INFO:root:[   99] Training loss: 0.86463876, Validation loss: 0.85055416, Gradient norm: 0.43251642
INFO:root:[  100] Training loss: 0.86577763, Validation loss: 0.84923314, Gradient norm: 0.52778020
INFO:root:[  101] Training loss: 0.86486831, Validation loss: 0.84916498, Gradient norm: 0.49525416
INFO:root:[  102] Training loss: 0.86602608, Validation loss: 0.85201218, Gradient norm: 0.58036324
INFO:root:[  103] Training loss: 0.86399075, Validation loss: 0.84718149, Gradient norm: 0.37808559
INFO:root:[  104] Training loss: 0.86412045, Validation loss: 0.84857713, Gradient norm: 0.48890306
INFO:root:[  105] Training loss: 0.86608166, Validation loss: 0.84946483, Gradient norm: 0.60417382
INFO:root:[  106] Training loss: 0.86375338, Validation loss: 0.84695054, Gradient norm: 0.48746859
INFO:root:[  107] Training loss: 0.86380025, Validation loss: 0.84528924, Gradient norm: 0.46847035
INFO:root:[  108] Training loss: 0.86412978, Validation loss: 0.84418970, Gradient norm: 0.57100455
INFO:root:[  109] Training loss: 0.86549888, Validation loss: 0.84617720, Gradient norm: 0.61244195
INFO:root:[  110] Training loss: 0.86426339, Validation loss: 0.84581001, Gradient norm: 0.57323875
INFO:root:[  111] Training loss: 0.86305761, Validation loss: 0.84643023, Gradient norm: 0.46071916
INFO:root:[  112] Training loss: 0.86452491, Validation loss: 0.84747854, Gradient norm: 0.62035095
INFO:root:[  113] Training loss: 0.86371916, Validation loss: 0.84677753, Gradient norm: 0.53483307
INFO:root:[  114] Training loss: 0.86471287, Validation loss: 0.84410093, Gradient norm: 0.60470620
INFO:root:[  115] Training loss: 0.86441062, Validation loss: 0.85318447, Gradient norm: 0.60736400
INFO:root:[  116] Training loss: 0.86481393, Validation loss: 0.84632593, Gradient norm: 0.66916855
INFO:root:[  117] Training loss: 0.86361643, Validation loss: 0.84711603, Gradient norm: 0.58363118
INFO:root:[  118] Training loss: 0.86333263, Validation loss: 0.84922869, Gradient norm: 0.61285171
INFO:root:[  119] Training loss: 0.86454559, Validation loss: 0.84503273, Gradient norm: 0.64987538
INFO:root:[  120] Training loss: 0.86407026, Validation loss: 0.85008585, Gradient norm: 0.68291981
INFO:root:[  121] Training loss: 0.86314891, Validation loss: 0.85186736, Gradient norm: 0.59289921
INFO:root:[  122] Training loss: 0.86409005, Validation loss: 0.84939618, Gradient norm: 0.68988712
INFO:root:[  123] Training loss: 0.86357300, Validation loss: 0.84962673, Gradient norm: 0.62086403
INFO:root:EP 123: Early stopping
INFO:root:Training the model took 3877.134s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.83644
INFO:root:EnergyScoreTrain: 0.73958
INFO:root:CRPSTrain: 0.60556
INFO:root:Gaussian NLLTrain: 25405.6346
INFO:root:CoverageTrain: 0.15862
INFO:root:IntervalWidthTrain: 0.22566
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.85077
INFO:root:EnergyScoreValidation: 0.75639
INFO:root:CRPSValidation: 0.61859
INFO:root:Gaussian NLLValidation: 7472.07843
INFO:root:CoverageValidation: 0.15339
INFO:root:IntervalWidthValidation: 0.22217
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.85297
INFO:root:EnergyScoreTest: 0.75289
INFO:root:CRPSTest: 0.61961
INFO:root:Gaussian NLLTest: 9905.36989
INFO:root:CoverageTest: 0.1579
INFO:root:IntervalWidthTest: 0.23165
INFO:root:###7 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99496417, Validation loss: 0.98984502, Gradient norm: 0.01546231
INFO:root:[    2] Training loss: 0.98524252, Validation loss: 0.97212109, Gradient norm: 0.04339313
INFO:root:[    3] Training loss: 0.97045530, Validation loss: 0.95504356, Gradient norm: 0.06094444
INFO:root:[    4] Training loss: 0.95948138, Validation loss: 0.94574212, Gradient norm: 0.07721994
INFO:root:[    5] Training loss: 0.95132536, Validation loss: 0.93883270, Gradient norm: 0.07189200
INFO:root:[    6] Training loss: 0.94494434, Validation loss: 0.93021029, Gradient norm: 0.08759980
INFO:root:[    7] Training loss: 0.93952131, Validation loss: 0.92132355, Gradient norm: 0.07707514
INFO:root:[    8] Training loss: 0.93384608, Validation loss: 0.91644622, Gradient norm: 0.07443767
INFO:root:[    9] Training loss: 0.92956256, Validation loss: 0.91024890, Gradient norm: 0.08815676
INFO:root:[   10] Training loss: 0.92578997, Validation loss: 0.90553549, Gradient norm: 0.09071099
INFO:root:[   11] Training loss: 0.92174305, Validation loss: 0.90241190, Gradient norm: 0.07751099
INFO:root:[   12] Training loss: 0.91850425, Validation loss: 0.89820377, Gradient norm: 0.07282128
INFO:root:[   13] Training loss: 0.91618630, Validation loss: 0.89671867, Gradient norm: 0.09630881
INFO:root:[   14] Training loss: 0.91358712, Validation loss: 0.89227854, Gradient norm: 0.09916815
INFO:root:[   15] Training loss: 0.91162165, Validation loss: 0.89168212, Gradient norm: 0.09521701
INFO:root:[   16] Training loss: 0.90962647, Validation loss: 0.88902530, Gradient norm: 0.08979312
INFO:root:[   17] Training loss: 0.90833674, Validation loss: 0.88620664, Gradient norm: 0.12467219
INFO:root:[   18] Training loss: 0.90650518, Validation loss: 0.88540662, Gradient norm: 0.12003449
INFO:root:[   19] Training loss: 0.90565212, Validation loss: 0.88445790, Gradient norm: 0.13101287
INFO:root:[   20] Training loss: 0.90439775, Validation loss: 0.88269339, Gradient norm: 0.14327725
INFO:root:[   21] Training loss: 0.90341527, Validation loss: 0.88226866, Gradient norm: 0.13777016
INFO:root:[   22] Training loss: 0.90288634, Validation loss: 0.88283588, Gradient norm: 0.20818995
INFO:root:[   23] Training loss: 0.90165363, Validation loss: 0.87821653, Gradient norm: 0.17073958
INFO:root:[   24] Training loss: 0.90127775, Validation loss: 0.88528501, Gradient norm: 0.22468612
INFO:root:[   25] Training loss: 0.90075057, Validation loss: 0.87937057, Gradient norm: 0.21302280
INFO:root:[   26] Training loss: 0.90003328, Validation loss: 0.87962984, Gradient norm: 0.26592464
INFO:root:[   27] Training loss: 0.90009496, Validation loss: 0.87695020, Gradient norm: 0.32610783
INFO:root:[   28] Training loss: 0.89836264, Validation loss: 0.87638712, Gradient norm: 0.28329044
INFO:root:[   29] Training loss: 0.89834619, Validation loss: 0.87450698, Gradient norm: 0.30763675
INFO:root:[   30] Training loss: 0.89873854, Validation loss: 0.87818442, Gradient norm: 0.39428671
INFO:root:[   31] Training loss: 0.89700086, Validation loss: 0.87783067, Gradient norm: 0.36213211
INFO:root:[   32] Training loss: 0.89752887, Validation loss: 0.87346599, Gradient norm: 0.40245064
INFO:root:[   33] Training loss: 0.89775276, Validation loss: 0.87519116, Gradient norm: 0.48028340
INFO:root:[   34] Training loss: 0.89712652, Validation loss: 0.87250401, Gradient norm: 0.47709687
INFO:root:[   35] Training loss: 0.89625659, Validation loss: 0.87412907, Gradient norm: 0.50819423
INFO:root:[   36] Training loss: 0.89591124, Validation loss: 0.87540188, Gradient norm: 0.52977558
INFO:root:[   37] Training loss: 0.89587320, Validation loss: 0.87321142, Gradient norm: 0.55258626
INFO:root:[   38] Training loss: 0.89623385, Validation loss: 0.87196661, Gradient norm: 0.62081603
INFO:root:[   39] Training loss: 0.89532091, Validation loss: 0.86904522, Gradient norm: 0.63540506
INFO:root:[   40] Training loss: 0.89515376, Validation loss: 0.86900913, Gradient norm: 0.65620308
INFO:root:[   41] Training loss: 0.89416782, Validation loss: 0.86860795, Gradient norm: 0.63874157
INFO:root:[   42] Training loss: 0.89449369, Validation loss: 0.87284341, Gradient norm: 0.71559151
INFO:root:[   43] Training loss: 0.89392942, Validation loss: 0.86926479, Gradient norm: 0.70102986
INFO:root:[   44] Training loss: 0.89385556, Validation loss: 0.87058801, Gradient norm: 0.72847256
INFO:root:[   45] Training loss: 0.89426340, Validation loss: 0.87117871, Gradient norm: 0.81289140
INFO:root:[   46] Training loss: 0.89381297, Validation loss: 0.87007797, Gradient norm: 0.83559959
INFO:root:[   47] Training loss: 0.89380705, Validation loss: 0.87002255, Gradient norm: 0.87411397
INFO:root:[   48] Training loss: 0.89387017, Validation loss: 0.86598444, Gradient norm: 0.91572968
INFO:root:[   49] Training loss: 0.89309225, Validation loss: 0.86463198, Gradient norm: 0.90283481
INFO:root:[   50] Training loss: 0.89330326, Validation loss: 0.87055848, Gradient norm: 0.98703215
INFO:root:[   51] Training loss: 0.89246190, Validation loss: 0.87191580, Gradient norm: 0.93271795
INFO:root:[   52] Training loss: 0.89223703, Validation loss: 0.86677353, Gradient norm: 0.96654657
INFO:root:[   53] Training loss: 0.89263917, Validation loss: 0.86566433, Gradient norm: 1.04679333
INFO:root:[   54] Training loss: 0.89171155, Validation loss: 0.86995001, Gradient norm: 1.00121699
INFO:root:[   55] Training loss: 0.89202513, Validation loss: 0.86885300, Gradient norm: 1.09250947
INFO:root:[   56] Training loss: 0.89211589, Validation loss: 0.86665328, Gradient norm: 1.12005370
INFO:root:[   57] Training loss: 0.89153204, Validation loss: 0.86909921, Gradient norm: 1.10745055
INFO:root:[   58] Training loss: 0.89142601, Validation loss: 0.86688109, Gradient norm: 1.12802605
INFO:root:[   59] Training loss: 0.89139718, Validation loss: 0.86958870, Gradient norm: 1.17665741
INFO:root:[   60] Training loss: 0.89177701, Validation loss: 0.86529619, Gradient norm: 1.18943820
INFO:root:[   61] Training loss: 0.89092404, Validation loss: 0.86838208, Gradient norm: 1.26234045
INFO:root:[   62] Training loss: 0.89142594, Validation loss: 0.86805268, Gradient norm: 1.28896130
INFO:root:[   63] Training loss: 0.89126478, Validation loss: 0.86656923, Gradient norm: 1.29321324
INFO:root:[   64] Training loss: 0.89050327, Validation loss: 0.86892863, Gradient norm: 1.26277499
INFO:root:[   65] Training loss: 0.89018836, Validation loss: 0.86718707, Gradient norm: 1.27020874
INFO:root:[   66] Training loss: 0.89074790, Validation loss: 0.86582889, Gradient norm: 1.35916567
INFO:root:[   67] Training loss: 0.89075032, Validation loss: 0.86539982, Gradient norm: 1.37981064
INFO:root:[   68] Training loss: 0.89044361, Validation loss: 0.86442817, Gradient norm: 1.37533375
INFO:root:[   69] Training loss: 0.89050075, Validation loss: 0.86359922, Gradient norm: 1.44385774
INFO:root:[   70] Training loss: 0.89064983, Validation loss: 0.86926501, Gradient norm: 1.42273398
INFO:root:[   71] Training loss: 0.88994585, Validation loss: 0.86309917, Gradient norm: 1.24138907
INFO:root:[   72] Training loss: 0.89049537, Validation loss: 0.86729615, Gradient norm: 1.61664958
INFO:root:[   73] Training loss: 0.88977955, Validation loss: 0.86794657, Gradient norm: 1.48539362
INFO:root:[   74] Training loss: 0.88979384, Validation loss: 0.86418772, Gradient norm: 1.58248707
INFO:root:[   75] Training loss: 0.88932843, Validation loss: 0.86487194, Gradient norm: 1.50447322
INFO:root:[   76] Training loss: 0.89024838, Validation loss: 0.86447705, Gradient norm: 1.63985713
INFO:root:[   77] Training loss: 0.88980375, Validation loss: 0.86496558, Gradient norm: 1.67756576
INFO:root:[   78] Training loss: 0.88980044, Validation loss: 0.86479282, Gradient norm: 1.66844473
INFO:root:[   79] Training loss: 0.88921034, Validation loss: 0.86374359, Gradient norm: 1.61833483
INFO:root:[   80] Training loss: 0.88963092, Validation loss: 0.86533313, Gradient norm: 1.71491862
INFO:root:EP 80: Early stopping
INFO:root:Training the model took 2519.471s.
INFO:root:Emptying the cuda cache took 0.022s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.86062
INFO:root:EnergyScoreTrain: 0.80071
INFO:root:CRPSTrain: 0.6507
INFO:root:Gaussian NLLTrain: 54806.25184
INFO:root:CoverageTrain: 0.10386
INFO:root:IntervalWidthTrain: 0.14159
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86578
INFO:root:EnergyScoreValidation: 0.81028
INFO:root:CRPSValidation: 0.65685
INFO:root:Gaussian NLLValidation: 33434.49973
INFO:root:CoverageValidation: 0.09815
INFO:root:IntervalWidthValidation: 0.1351
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86743
INFO:root:EnergyScoreTest: 0.81271
INFO:root:CRPSTest: 0.65992
INFO:root:Gaussian NLLTest: 45980.65603
INFO:root:CoverageTest: 0.09536
INFO:root:IntervalWidthTest: 0.13181
INFO:root:###8 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99529707, Validation loss: 0.99080334, Gradient norm: 0.01402344
INFO:root:[    2] Training loss: 0.98792444, Validation loss: 0.97644094, Gradient norm: 0.03155764
INFO:root:[    3] Training loss: 0.97463307, Validation loss: 0.95902582, Gradient norm: 0.05733920
INFO:root:[    4] Training loss: 0.96270365, Validation loss: 0.94592211, Gradient norm: 0.06260954
INFO:root:[    5] Training loss: 0.95302281, Validation loss: 0.93699573, Gradient norm: 0.06183370
INFO:root:[    6] Training loss: 0.94609702, Validation loss: 0.92728214, Gradient norm: 0.07401068
INFO:root:[    7] Training loss: 0.94065338, Validation loss: 0.91911139, Gradient norm: 0.07024221
INFO:root:[    8] Training loss: 0.93578450, Validation loss: 0.91505778, Gradient norm: 0.08024850
INFO:root:[    9] Training loss: 0.93197735, Validation loss: 0.90879961, Gradient norm: 0.08960520
INFO:root:[   10] Training loss: 0.92878309, Validation loss: 0.90488332, Gradient norm: 0.09653494
INFO:root:[   11] Training loss: 0.92613142, Validation loss: 0.90306268, Gradient norm: 0.10764137
INFO:root:[   12] Training loss: 0.92468645, Validation loss: 0.90237650, Gradient norm: 0.15373579
INFO:root:[   13] Training loss: 0.92224489, Validation loss: 0.89857567, Gradient norm: 0.12382203
INFO:root:[   14] Training loss: 0.92110170, Validation loss: 0.89577342, Gradient norm: 0.18387289
INFO:root:[   15] Training loss: 0.91946548, Validation loss: 0.89416689, Gradient norm: 0.19522350
INFO:root:[   16] Training loss: 0.91849360, Validation loss: 0.89232573, Gradient norm: 0.24408890
INFO:root:[   17] Training loss: 0.91734072, Validation loss: 0.89130055, Gradient norm: 0.26886335
INFO:root:[   18] Training loss: 0.91643692, Validation loss: 0.88841344, Gradient norm: 0.32527624
INFO:root:[   19] Training loss: 0.91492348, Validation loss: 0.88978535, Gradient norm: 0.29500164
INFO:root:[   20] Training loss: 0.91532479, Validation loss: 0.88898821, Gradient norm: 0.39894908
INFO:root:[   21] Training loss: 0.91291363, Validation loss: 0.89019708, Gradient norm: 0.33287206
INFO:root:[   22] Training loss: 0.91351019, Validation loss: 0.88639067, Gradient norm: 0.46709316
INFO:root:[   23] Training loss: 0.91191768, Validation loss: 0.88755150, Gradient norm: 0.45844297
INFO:root:[   24] Training loss: 0.91183183, Validation loss: 0.88839658, Gradient norm: 0.49957679
INFO:root:[   25] Training loss: 0.91138974, Validation loss: 0.88096935, Gradient norm: 0.54042950
INFO:root:[   26] Training loss: 0.91098241, Validation loss: 0.88087835, Gradient norm: 0.58819934
INFO:root:[   27] Training loss: 0.91063366, Validation loss: 0.88489750, Gradient norm: 0.61270601
INFO:root:[   28] Training loss: 0.90908614, Validation loss: 0.88151039, Gradient norm: 0.61115094
INFO:root:[   29] Training loss: 0.90903923, Validation loss: 0.88288500, Gradient norm: 0.65580216
INFO:root:[   30] Training loss: 0.90882933, Validation loss: 0.88245339, Gradient norm: 0.70252469
INFO:root:[   31] Training loss: 0.90877691, Validation loss: 0.88588643, Gradient norm: 0.77150371
INFO:root:[   32] Training loss: 0.90844755, Validation loss: 0.88146256, Gradient norm: 0.79543544
INFO:root:[   33] Training loss: 0.90738573, Validation loss: 0.87875640, Gradient norm: 0.76331675
INFO:root:[   34] Training loss: 0.90701063, Validation loss: 0.88120780, Gradient norm: 0.80043985
INFO:root:[   35] Training loss: 0.90662653, Validation loss: 0.87805406, Gradient norm: 0.83977007
INFO:root:[   36] Training loss: 0.90646737, Validation loss: 0.87771311, Gradient norm: 0.89281859
INFO:root:[   37] Training loss: 0.90638187, Validation loss: 0.87589310, Gradient norm: 0.92397337
INFO:root:[   38] Training loss: 0.90561163, Validation loss: 0.87991684, Gradient norm: 0.93070818
INFO:root:[   39] Training loss: 0.90591987, Validation loss: 0.87692202, Gradient norm: 1.00503132
INFO:root:[   40] Training loss: 0.90509603, Validation loss: 0.87986323, Gradient norm: 0.94908572
INFO:root:[   41] Training loss: 0.90465316, Validation loss: 0.87735699, Gradient norm: 1.01689817
INFO:root:[   42] Training loss: 0.90505078, Validation loss: 0.87658499, Gradient norm: 1.07702502
INFO:root:[   43] Training loss: 0.90413697, Validation loss: 0.87753639, Gradient norm: 1.04852880
INFO:root:[   44] Training loss: 0.90455235, Validation loss: 0.88246185, Gradient norm: 1.10828804
INFO:root:[   45] Training loss: 0.90455510, Validation loss: 0.87774049, Gradient norm: 1.16286961
INFO:root:[   46] Training loss: 0.90391917, Validation loss: 0.87625852, Gradient norm: 1.11181587
INFO:root:[   47] Training loss: 0.90355240, Validation loss: 0.87472921, Gradient norm: 1.14908621
INFO:root:[   48] Training loss: 0.90340545, Validation loss: 0.87457824, Gradient norm: 1.23386950
INFO:root:[   49] Training loss: 0.90368373, Validation loss: 0.87564065, Gradient norm: 1.25520022
INFO:root:[   50] Training loss: 0.90315700, Validation loss: 0.87319513, Gradient norm: 1.27863270
INFO:root:[   51] Training loss: 0.90304266, Validation loss: 0.87556444, Gradient norm: 1.29441537
INFO:root:[   52] Training loss: 0.90332932, Validation loss: 0.87562719, Gradient norm: 1.42838251
INFO:root:[   53] Training loss: 0.90270107, Validation loss: 0.87527017, Gradient norm: 1.37537168
INFO:root:[   54] Training loss: 0.90244103, Validation loss: 0.87678894, Gradient norm: 1.37422646
INFO:root:[   55] Training loss: 0.90247589, Validation loss: 0.87531233, Gradient norm: 1.43150710
INFO:root:[   56] Training loss: 0.90203051, Validation loss: 0.87507148, Gradient norm: 1.44412481
INFO:root:[   57] Training loss: 0.90229782, Validation loss: 0.87426747, Gradient norm: 1.50237535
INFO:root:[   58] Training loss: 0.90185049, Validation loss: 0.87361538, Gradient norm: 1.47167225
INFO:root:[   59] Training loss: 0.90211646, Validation loss: 0.87393443, Gradient norm: 1.52957473
INFO:root:[   60] Training loss: 0.90166988, Validation loss: 0.87796188, Gradient norm: 1.53511218
INFO:root:[   61] Training loss: 0.90159708, Validation loss: 0.87236817, Gradient norm: 1.55084532
INFO:root:[   62] Training loss: 0.90112488, Validation loss: 0.87535040, Gradient norm: 1.53776705
INFO:root:[   63] Training loss: 0.90125602, Validation loss: 0.87398603, Gradient norm: 1.67606226
INFO:root:[   64] Training loss: 0.90112236, Validation loss: 0.87386469, Gradient norm: 1.65402521
INFO:root:[   65] Training loss: 0.90112383, Validation loss: 0.87155792, Gradient norm: 1.74072812
INFO:root:[   66] Training loss: 0.90130485, Validation loss: 0.87167297, Gradient norm: 1.71569278
INFO:root:[   67] Training loss: 0.90134791, Validation loss: 0.87376866, Gradient norm: 1.86459271
INFO:root:[   68] Training loss: 0.90076844, Validation loss: 0.87115917, Gradient norm: 1.82539864
INFO:root:[   69] Training loss: 0.90138139, Validation loss: 0.87200043, Gradient norm: 1.87358505
INFO:root:[   70] Training loss: 0.90108199, Validation loss: 0.86912168, Gradient norm: 1.92655761
INFO:root:[   71] Training loss: 0.90105351, Validation loss: 0.87097303, Gradient norm: 2.07469062
INFO:root:[   72] Training loss: 0.90103756, Validation loss: 0.87161666, Gradient norm: 2.11473548
INFO:root:[   73] Training loss: 0.90090778, Validation loss: 0.87377365, Gradient norm: 2.12248431
INFO:root:[   74] Training loss: 0.90168136, Validation loss: 0.87333032, Gradient norm: 2.27221648
INFO:root:[   75] Training loss: 0.90074636, Validation loss: 0.87053734, Gradient norm: 2.31280394
INFO:root:[   76] Training loss: 0.90132878, Validation loss: 0.87545482, Gradient norm: 2.33658769
INFO:root:[   77] Training loss: 0.90147087, Validation loss: 0.87080416, Gradient norm: 2.43652874
INFO:root:[   78] Training loss: 0.90054134, Validation loss: 0.86994441, Gradient norm: 2.31878101
INFO:root:[   79] Training loss: 0.90060508, Validation loss: 0.87111128, Gradient norm: 2.37160577
INFO:root:[   80] Training loss: 0.90048445, Validation loss: 0.86911412, Gradient norm: 2.39996490
INFO:root:[   81] Training loss: 0.90078656, Validation loss: 0.87521233, Gradient norm: 2.53722245
INFO:root:[   82] Training loss: 0.90042216, Validation loss: 0.87494792, Gradient norm: 2.44865108
INFO:root:[   83] Training loss: 0.90060359, Validation loss: 0.87359944, Gradient norm: 2.63659774
INFO:root:[   84] Training loss: 0.90064369, Validation loss: 0.87080067, Gradient norm: 2.60024893
INFO:root:[   85] Training loss: 0.90084623, Validation loss: 0.87196614, Gradient norm: 2.85487501
INFO:root:[   86] Training loss: 0.90062617, Validation loss: 0.87428493, Gradient norm: 2.83446746
INFO:root:[   87] Training loss: 0.90050993, Validation loss: 0.87141910, Gradient norm: 2.87442963
INFO:root:[   88] Training loss: 0.90072761, Validation loss: 0.87271371, Gradient norm: 3.05340380
INFO:root:[   89] Training loss: 0.90083173, Validation loss: 0.86928542, Gradient norm: 3.03416579
INFO:root:EP 89: Early stopping
INFO:root:Training the model took 2801.325s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.86873
INFO:root:EnergyScoreTrain: 0.80665
INFO:root:CRPSTrain: 0.66047
INFO:root:Gaussian NLLTrain: 269159.42194
INFO:root:CoverageTrain: 0.0973
INFO:root:IntervalWidthTrain: 0.13986
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.87197
INFO:root:EnergyScoreValidation: 0.81188
INFO:root:CRPSValidation: 0.66367
INFO:root:Gaussian NLLValidation: 167883.31135
INFO:root:CoverageValidation: 0.09572
INFO:root:IntervalWidthValidation: 0.13773
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.87301
INFO:root:EnergyScoreTest: 0.81202
INFO:root:CRPSTest: 0.66548
INFO:root:Gaussian NLLTest: 101441.53012
INFO:root:CoverageTest: 0.09476
INFO:root:IntervalWidthTest: 0.13746
INFO:root:###9 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99592193, Validation loss: 0.99124912, Gradient norm: 0.01322509
INFO:root:[    2] Training loss: 0.99079537, Validation loss: 0.98416380, Gradient norm: 0.02566975
INFO:root:[    3] Training loss: 0.98144482, Validation loss: 0.96725261, Gradient norm: 0.05096014
INFO:root:[    4] Training loss: 0.97156132, Validation loss: 0.95792444, Gradient norm: 0.05579884
INFO:root:[    5] Training loss: 0.96454066, Validation loss: 0.94848400, Gradient norm: 0.06625964
INFO:root:[    6] Training loss: 0.95901038, Validation loss: 0.94207404, Gradient norm: 0.05451320
INFO:root:[    7] Training loss: 0.95519666, Validation loss: 0.93791111, Gradient norm: 0.06476956
INFO:root:[    8] Training loss: 0.95197686, Validation loss: 0.93691168, Gradient norm: 0.06307698
INFO:root:[    9] Training loss: 0.94957869, Validation loss: 0.92960133, Gradient norm: 0.09806868
INFO:root:[   10] Training loss: 0.94755696, Validation loss: 0.92614587, Gradient norm: 0.08583533
INFO:root:[   11] Training loss: 0.94574515, Validation loss: 0.92423551, Gradient norm: 0.11604105
INFO:root:[   12] Training loss: 0.94467121, Validation loss: 0.92486051, Gradient norm: 0.14349538
INFO:root:[   13] Training loss: 0.94337937, Validation loss: 0.92209760, Gradient norm: 0.20584831
INFO:root:[   14] Training loss: 0.94189631, Validation loss: 0.91852509, Gradient norm: 0.22977027
INFO:root:[   15] Training loss: 0.94061238, Validation loss: 0.91649549, Gradient norm: 0.26343803
INFO:root:[   16] Training loss: 0.93905402, Validation loss: 0.91411371, Gradient norm: 0.29704617
INFO:root:[   17] Training loss: 0.93895624, Validation loss: 0.92108119, Gradient norm: 0.37485307
INFO:root:[   18] Training loss: 0.93872116, Validation loss: 0.91260643, Gradient norm: 0.42645252
INFO:root:[   19] Training loss: 0.93721202, Validation loss: 0.91299352, Gradient norm: 0.45399651
INFO:root:[   20] Training loss: 0.93744706, Validation loss: 0.91858641, Gradient norm: 0.52566508
INFO:root:[   21] Training loss: 0.93699234, Validation loss: 0.91552592, Gradient norm: 0.57587105
INFO:root:[   22] Training loss: 0.93614098, Validation loss: 0.91046182, Gradient norm: 0.59860236
INFO:root:[   23] Training loss: 0.93605837, Validation loss: 0.91185471, Gradient norm: 0.69931147
INFO:root:[   24] Training loss: 0.93620629, Validation loss: 0.90752362, Gradient norm: 0.75151777
INFO:root:[   25] Training loss: 0.93607850, Validation loss: 0.90987608, Gradient norm: 0.86760502
INFO:root:[   26] Training loss: 0.93551033, Validation loss: 0.90801606, Gradient norm: 0.88835722
INFO:root:[   27] Training loss: 0.93488732, Validation loss: 0.90770893, Gradient norm: 0.95757614
INFO:root:[   28] Training loss: 0.93451901, Validation loss: 0.91090395, Gradient norm: 0.97862448
INFO:root:[   29] Training loss: 0.93500062, Validation loss: 0.90452039, Gradient norm: 1.13729141
INFO:root:[   30] Training loss: 0.93465015, Validation loss: 0.90535356, Gradient norm: 1.20445020
INFO:root:[   31] Training loss: 0.93412636, Validation loss: 0.90388837, Gradient norm: 1.23261424
INFO:root:[   32] Training loss: 0.93367552, Validation loss: 0.91021826, Gradient norm: 1.36665012
INFO:root:[   33] Training loss: 0.93432040, Validation loss: 0.90983382, Gradient norm: 1.45829067
INFO:root:[   34] Training loss: 0.93386619, Validation loss: 0.91019593, Gradient norm: 1.48066739
INFO:root:[   35] Training loss: 0.93404231, Validation loss: 0.90323163, Gradient norm: 1.65044225
INFO:root:[   36] Training loss: 0.93373501, Validation loss: 0.90582633, Gradient norm: 1.69636022
INFO:root:[   37] Training loss: 0.93343063, Validation loss: 0.90545612, Gradient norm: 1.80697170
INFO:root:[   38] Training loss: 0.93361513, Validation loss: 0.90240285, Gradient norm: 1.90457965
INFO:root:[   39] Training loss: 0.93354216, Validation loss: 0.90645766, Gradient norm: 1.92172033
INFO:root:[   40] Training loss: 0.93396141, Validation loss: 0.90356104, Gradient norm: 2.18280423
INFO:root:[   41] Training loss: 0.93385289, Validation loss: 0.90780285, Gradient norm: 2.32032906
INFO:root:[   42] Training loss: 0.93349638, Validation loss: 0.90151723, Gradient norm: 2.38339192
INFO:root:[   43] Training loss: 0.93357701, Validation loss: 0.90546422, Gradient norm: 2.60463101
INFO:root:[   44] Training loss: 0.93335722, Validation loss: 0.90372493, Gradient norm: 2.55347970
INFO:root:[   45] Training loss: 0.93464057, Validation loss: 0.90776472, Gradient norm: 3.10695999
INFO:root:[   46] Training loss: 0.93410625, Validation loss: 0.90981971, Gradient norm: 3.24277100
INFO:root:[   47] Training loss: 0.93493828, Validation loss: 0.90715456, Gradient norm: 3.36520205
INFO:root:[   48] Training loss: 0.93336046, Validation loss: 0.90330598, Gradient norm: 3.29163443
INFO:root:[   49] Training loss: 0.93481228, Validation loss: 0.91063627, Gradient norm: 3.82986163
INFO:root:[   50] Training loss: 0.93432674, Validation loss: 0.90633797, Gradient norm: 3.98150979
INFO:root:[   51] Training loss: 0.93493945, Validation loss: 0.90408588, Gradient norm: 4.09538667
INFO:root:[   52] Training loss: 0.93570269, Validation loss: 0.90326921, Gradient norm: 4.42317810
INFO:root:[   53] Training loss: 0.93616446, Validation loss: 0.90663265, Gradient norm: 4.50541411
INFO:root:[   54] Training loss: 0.93745762, Validation loss: 0.90593969, Gradient norm: 4.71405284
INFO:root:[   55] Training loss: 0.93604462, Validation loss: 0.90658268, Gradient norm: 4.94070423
INFO:root:[   56] Training loss: 0.93983169, Validation loss: 0.91706189, Gradient norm: 5.46902691
INFO:root:[   57] Training loss: 0.93779901, Validation loss: 0.90547125, Gradient norm: 5.34724020
INFO:root:[   58] Training loss: 0.94334492, Validation loss: 0.90714879, Gradient norm: 6.36969794
INFO:root:[   59] Training loss: 0.93780531, Validation loss: 0.91241138, Gradient norm: 5.61788068
INFO:root:[   60] Training loss: 0.93925557, Validation loss: 0.90799292, Gradient norm: 6.00453849
INFO:root:[   61] Training loss: 0.94103361, Validation loss: 0.90969604, Gradient norm: 6.47641738
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1934.788s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.90406
INFO:root:EnergyScoreTrain: 0.84739
INFO:root:CRPSTrain: 0.70977
INFO:root:Gaussian NLLTrain: 153559.19746
INFO:root:CoverageTrain: 0.0734
INFO:root:IntervalWidthTrain: 0.1205
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.90437
INFO:root:EnergyScoreValidation: 0.84797
INFO:root:CRPSValidation: 0.71001
INFO:root:Gaussian NLLValidation: 164798.9459
INFO:root:CoverageValidation: 0.0735
INFO:root:IntervalWidthValidation: 0.12091
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.90586
INFO:root:EnergyScoreTest: 0.84714
INFO:root:CRPSTest: 0.7113
INFO:root:Gaussian NLLTest: 146523.44662
INFO:root:CoverageTest: 0.07542
INFO:root:IntervalWidthTest: 0.12418
INFO:root:###10 out of 36 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 190840832
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99398727, Validation loss: 0.98848113, Gradient norm: 0.02319189
INFO:root:[    2] Training loss: 0.97858290, Validation loss: 0.96840295, Gradient norm: 0.09880822
INFO:root:[    3] Training loss: 0.95777744, Validation loss: 0.94640321, Gradient norm: 0.16065255
INFO:root:[    4] Training loss: 0.93893651, Validation loss: 0.92997267, Gradient norm: 0.17028342
INFO:root:[    5] Training loss: 0.92571058, Validation loss: 0.92142230, Gradient norm: 0.24593804
INFO:root:[    6] Training loss: 0.91439383, Validation loss: 0.91001956, Gradient norm: 0.26527513
INFO:root:[    7] Training loss: 0.90627426, Validation loss: 0.90258947, Gradient norm: 0.21805860
INFO:root:[    8] Training loss: 0.90042152, Validation loss: 0.90040742, Gradient norm: 0.26437901
INFO:root:[    9] Training loss: 0.89546302, Validation loss: 0.89501867, Gradient norm: 0.25948355
INFO:root:[   10] Training loss: 0.89135775, Validation loss: 0.89479855, Gradient norm: 0.24962021
INFO:root:[   11] Training loss: 0.88822148, Validation loss: 0.88884707, Gradient norm: 0.23988767
INFO:root:[   12] Training loss: 0.88446929, Validation loss: 0.89312271, Gradient norm: 0.25317375
INFO:root:[   13] Training loss: 0.88267741, Validation loss: 0.88632927, Gradient norm: 0.28143533
INFO:root:[   14] Training loss: 0.87813707, Validation loss: 0.88412009, Gradient norm: 0.20502607
INFO:root:[   15] Training loss: 0.87707021, Validation loss: 0.88149639, Gradient norm: 0.28278991
INFO:root:[   16] Training loss: 0.87345713, Validation loss: 0.87790485, Gradient norm: 0.24744656
INFO:root:[   17] Training loss: 0.87185388, Validation loss: 0.87950762, Gradient norm: 0.27376742
INFO:root:[   18] Training loss: 0.87000357, Validation loss: 0.87793821, Gradient norm: 0.29179555
INFO:root:[   19] Training loss: 0.86805495, Validation loss: 0.87540511, Gradient norm: 0.25132757
INFO:root:[   20] Training loss: 0.86437786, Validation loss: 0.87378503, Gradient norm: 0.15215745
INFO:root:[   21] Training loss: 0.86495472, Validation loss: 0.87283548, Gradient norm: 0.28676077
INFO:root:[   22] Training loss: 0.86354446, Validation loss: 0.87514118, Gradient norm: 0.33778910
INFO:root:[   23] Training loss: 0.86118600, Validation loss: 0.87130177, Gradient norm: 0.24473857
INFO:root:[   24] Training loss: 0.85908967, Validation loss: 0.87309617, Gradient norm: 0.27867994
INFO:root:[   25] Training loss: 0.85841365, Validation loss: 0.87053629, Gradient norm: 0.33666207
INFO:root:[   26] Training loss: 0.85641869, Validation loss: 0.86907015, Gradient norm: 0.25537853
INFO:root:[   27] Training loss: 0.85533319, Validation loss: 0.86879895, Gradient norm: 0.30985959
INFO:root:[   28] Training loss: 0.85291573, Validation loss: 0.86846558, Gradient norm: 0.20011102
INFO:root:[   29] Training loss: 0.85224917, Validation loss: 0.86862101, Gradient norm: 0.29034151
INFO:root:[   30] Training loss: 0.85145506, Validation loss: 0.86587566, Gradient norm: 0.31023679
INFO:root:[   31] Training loss: 0.84974482, Validation loss: 0.86624483, Gradient norm: 0.27744777
INFO:root:[   32] Training loss: 0.84890512, Validation loss: 0.86754282, Gradient norm: 0.29702403
INFO:root:[   33] Training loss: 0.84728524, Validation loss: 0.86579354, Gradient norm: 0.29135480
INFO:root:[   34] Training loss: 0.84630049, Validation loss: 0.86798452, Gradient norm: 0.29537790
INFO:root:[   35] Training loss: 0.84498575, Validation loss: 0.86747519, Gradient norm: 0.27318275
INFO:root:[   36] Training loss: 0.84459529, Validation loss: 0.86757717, Gradient norm: 0.31105518
INFO:root:[   37] Training loss: 0.84320684, Validation loss: 0.86740306, Gradient norm: 0.32912740
INFO:root:[   38] Training loss: 0.84143811, Validation loss: 0.86298852, Gradient norm: 0.24814577
INFO:root:[   39] Training loss: 0.84111155, Validation loss: 0.86381430, Gradient norm: 0.30353711
INFO:root:[   40] Training loss: 0.84146787, Validation loss: 0.86332795, Gradient norm: 0.38493257
INFO:root:[   41] Training loss: 0.83789581, Validation loss: 0.86239176, Gradient norm: 0.30002062
INFO:root:[   42] Training loss: 0.83863709, Validation loss: 0.86408299, Gradient norm: 0.33747193
INFO:root:[   43] Training loss: 0.83684876, Validation loss: 0.86313188, Gradient norm: 0.31364870
INFO:root:[   44] Training loss: 0.83644304, Validation loss: 0.86353228, Gradient norm: 0.28954803
INFO:root:[   45] Training loss: 0.83470194, Validation loss: 0.86383934, Gradient norm: 0.28762871
INFO:root:[   46] Training loss: 0.83389733, Validation loss: 0.86619323, Gradient norm: 0.33460270
INFO:root:[   47] Training loss: 0.83388817, Validation loss: 0.86474843, Gradient norm: 0.28320182
INFO:root:[   48] Training loss: 0.83254969, Validation loss: 0.86930845, Gradient norm: 0.37693407
INFO:root:[   49] Training loss: 0.83216370, Validation loss: 0.86870985, Gradient norm: 0.36573931
INFO:root:[   50] Training loss: 0.83067659, Validation loss: 0.86694057, Gradient norm: 0.33232784
INFO:root:[   51] Training loss: 0.82954825, Validation loss: 0.86577825, Gradient norm: 0.30952280
INFO:root:[   52] Training loss: 0.82841178, Validation loss: 0.86662325, Gradient norm: 0.31437116
INFO:root:[   53] Training loss: 0.82900168, Validation loss: 0.86444486, Gradient norm: 0.39323648
INFO:root:[   54] Training loss: 0.82732910, Validation loss: 0.86687669, Gradient norm: 0.33765566
INFO:root:[   55] Training loss: 0.82590932, Validation loss: 0.86865815, Gradient norm: 0.32719685
INFO:root:[   56] Training loss: 0.82437803, Validation loss: 0.86733493, Gradient norm: 0.30151951
INFO:root:[   57] Training loss: 0.82499966, Validation loss: 0.86827790, Gradient norm: 0.35387185
INFO:root:[   58] Training loss: 0.82431063, Validation loss: 0.86667887, Gradient norm: 0.36002207
INFO:root:[   59] Training loss: 0.82249526, Validation loss: 0.86738397, Gradient norm: 0.24682502
INFO:root:[   60] Training loss: 0.82295447, Validation loss: 0.87286876, Gradient norm: 0.41834706
INFO:root:[   61] Training loss: 0.82180912, Validation loss: 0.86593314, Gradient norm: 0.35656187
INFO:root:[   62] Training loss: 0.82077406, Validation loss: 0.87136193, Gradient norm: 0.31751872
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1962.19s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
