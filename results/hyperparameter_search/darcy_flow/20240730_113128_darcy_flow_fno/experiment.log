INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 4194304
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05207661, Validation loss: 0.03895100, Gradient norm: 0.78149785
INFO:root:[    2] Training loss: 0.02869356, Validation loss: 0.02284292, Gradient norm: 0.78173171
INFO:root:[    3] Training loss: 0.01995251, Validation loss: 0.01519835, Gradient norm: 0.67324386
INFO:root:[    4] Training loss: 0.01655360, Validation loss: 0.01409647, Gradient norm: 0.57061305
INFO:root:[    5] Training loss: 0.01544316, Validation loss: 0.01400017, Gradient norm: 0.55499303
INFO:root:[    6] Training loss: 0.01552264, Validation loss: 0.01302614, Gradient norm: 0.60556090
INFO:root:[    7] Training loss: 0.01409238, Validation loss: 0.01376287, Gradient norm: 0.59105403
INFO:root:[    8] Training loss: 0.01212360, Validation loss: 0.01567410, Gradient norm: 0.49139360
INFO:root:[    9] Training loss: 0.01245792, Validation loss: 0.01550341, Gradient norm: 0.52770406
INFO:root:[   10] Training loss: 0.01159120, Validation loss: 0.01533730, Gradient norm: 0.50306837
INFO:root:[   11] Training loss: 0.01168845, Validation loss: 0.01171748, Gradient norm: 0.50499998
INFO:root:[   12] Training loss: 0.01034691, Validation loss: 0.01133870, Gradient norm: 0.41720535
INFO:root:[   13] Training loss: 0.01113681, Validation loss: 0.00994583, Gradient norm: 0.47491696
INFO:root:[   14] Training loss: 0.01053598, Validation loss: 0.01169850, Gradient norm: 0.45638268
INFO:root:[   15] Training loss: 0.00953602, Validation loss: 0.01155408, Gradient norm: 0.38686478
INFO:root:[   16] Training loss: 0.00911370, Validation loss: 0.01040083, Gradient norm: 0.31644005
INFO:root:[   17] Training loss: 0.01041288, Validation loss: 0.01070693, Gradient norm: 0.46718311
INFO:root:[   18] Training loss: 0.00881405, Validation loss: 0.01219844, Gradient norm: 0.35991929
INFO:root:[   19] Training loss: 0.00880464, Validation loss: 0.00948461, Gradient norm: 0.37637039
INFO:root:[   20] Training loss: 0.00983456, Validation loss: 0.00962515, Gradient norm: 0.47955400
INFO:root:[   21] Training loss: 0.00790127, Validation loss: 0.01228259, Gradient norm: 0.27144574
INFO:root:[   22] Training loss: 0.00924744, Validation loss: 0.01309980, Gradient norm: 0.43915318
INFO:root:[   23] Training loss: 0.00897241, Validation loss: 0.00887661, Gradient norm: 0.44556522
INFO:root:[   24] Training loss: 0.00934842, Validation loss: 0.01266760, Gradient norm: 0.43530277
INFO:root:[   25] Training loss: 0.00820381, Validation loss: 0.01189235, Gradient norm: 0.40404183
INFO:root:[   26] Training loss: 0.00737966, Validation loss: 0.00916445, Gradient norm: 0.32072389
INFO:root:[   27] Training loss: 0.00696814, Validation loss: 0.01031925, Gradient norm: 0.26077605
INFO:root:[   28] Training loss: 0.00802670, Validation loss: 0.01022602, Gradient norm: 0.39130453
INFO:root:[   29] Training loss: 0.00706057, Validation loss: 0.01036848, Gradient norm: 0.30766051
INFO:root:[   30] Training loss: 0.00707213, Validation loss: 0.01143556, Gradient norm: 0.35163370
INFO:root:[   31] Training loss: 0.00805088, Validation loss: 0.01020528, Gradient norm: 0.40573677
INFO:root:[   32] Training loss: 0.00669432, Validation loss: 0.01020593, Gradient norm: 0.28574475
INFO:root:[   33] Training loss: 0.00814193, Validation loss: 0.01012510, Gradient norm: 0.44552035
INFO:root:[   34] Training loss: 0.00678934, Validation loss: 0.01057627, Gradient norm: 0.28674599
INFO:root:[   35] Training loss: 0.00673762, Validation loss: 0.00849799, Gradient norm: 0.33899928
INFO:root:[   36] Training loss: 0.00678443, Validation loss: 0.01054464, Gradient norm: 0.36402340
INFO:root:[   37] Training loss: 0.00696786, Validation loss: 0.01094874, Gradient norm: 0.38107770
INFO:root:[   38] Training loss: 0.00683341, Validation loss: 0.01006454, Gradient norm: 0.36520836
INFO:root:[   39] Training loss: 0.00647299, Validation loss: 0.00861372, Gradient norm: 0.36957294
INFO:root:[   40] Training loss: 0.00566713, Validation loss: 0.00893909, Gradient norm: 0.23362724
INFO:root:[   41] Training loss: 0.00638566, Validation loss: 0.01121374, Gradient norm: 0.32924473
INFO:root:[   42] Training loss: 0.00633418, Validation loss: 0.00943321, Gradient norm: 0.33364894
INFO:root:[   43] Training loss: 0.00599005, Validation loss: 0.00993703, Gradient norm: 0.33595429
INFO:root:[   44] Training loss: 0.00599187, Validation loss: 0.00867309, Gradient norm: 0.33086526
INFO:root:[   45] Training loss: 0.00607893, Validation loss: 0.00973087, Gradient norm: 0.33828750
INFO:root:[   46] Training loss: 0.00605311, Validation loss: 0.00890626, Gradient norm: 0.34448748
INFO:root:[   47] Training loss: 0.00567355, Validation loss: 0.00850708, Gradient norm: 0.29685898
INFO:root:[   48] Training loss: 0.00566605, Validation loss: 0.00997730, Gradient norm: 0.29653232
INFO:root:[   49] Training loss: 0.00598275, Validation loss: 0.00989140, Gradient norm: 0.34379739
INFO:root:[   50] Training loss: 0.00615449, Validation loss: 0.01020280, Gradient norm: 0.38859859
INFO:root:[   51] Training loss: 0.00561852, Validation loss: 0.00915137, Gradient norm: 0.30474438
INFO:root:[   52] Training loss: 0.00534476, Validation loss: 0.00927518, Gradient norm: 0.31049869
INFO:root:[   53] Training loss: 0.00552849, Validation loss: 0.01032333, Gradient norm: 0.32484493
INFO:root:[   54] Training loss: 0.00515502, Validation loss: 0.00867472, Gradient norm: 0.22695414
INFO:root:[   55] Training loss: 0.00551348, Validation loss: 0.00947355, Gradient norm: 0.29102430
INFO:root:[   56] Training loss: 0.00594826, Validation loss: 0.01042852, Gradient norm: 0.33987284
INFO:root:[   57] Training loss: 0.00521457, Validation loss: 0.00972755, Gradient norm: 0.27071652
INFO:root:[   58] Training loss: 0.00551691, Validation loss: 0.00939096, Gradient norm: 0.37269195
INFO:root:[   59] Training loss: 0.00546418, Validation loss: 0.00981469, Gradient norm: 0.36849224
INFO:root:[   60] Training loss: 0.00521561, Validation loss: 0.00966961, Gradient norm: 0.28684494
INFO:root:[   61] Training loss: 0.00509826, Validation loss: 0.00976971, Gradient norm: 0.30805458
INFO:root:[   62] Training loss: 0.00519413, Validation loss: 0.00830810, Gradient norm: 0.27644332
INFO:root:[   63] Training loss: 0.00494205, Validation loss: 0.00844233, Gradient norm: 0.27557441
INFO:root:[   64] Training loss: 0.00473583, Validation loss: 0.00953483, Gradient norm: 0.23748530
INFO:root:[   65] Training loss: 0.00536267, Validation loss: 0.00911130, Gradient norm: 0.37256331
INFO:root:[   66] Training loss: 0.00471496, Validation loss: 0.00869157, Gradient norm: 0.27619393
INFO:root:[   67] Training loss: 0.00506153, Validation loss: 0.00878611, Gradient norm: 0.32120787
INFO:root:[   68] Training loss: 0.00488369, Validation loss: 0.00880932, Gradient norm: 0.26867089
INFO:root:[   69] Training loss: 0.00454138, Validation loss: 0.00840276, Gradient norm: 0.24696583
INFO:root:[   70] Training loss: 0.00462389, Validation loss: 0.00866032, Gradient norm: 0.28397203
INFO:root:[   71] Training loss: 0.00495628, Validation loss: 0.00904877, Gradient norm: 0.31842511
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 2496.538s.
INFO:root:Emptying the cuda cache took 0.068s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00536
INFO:root:EnergyScoreTrain: 0.00391
INFO:root:CoverageTrain: 0.99003
INFO:root:IntervalWidthTrain: 0.04127
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01123
INFO:root:EnergyScoreValidation: 0.00843
INFO:root:CoverageValidation: 0.91852
INFO:root:IntervalWidthValidation: 0.04109
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01133
INFO:root:EnergyScoreTest: 0.0085
INFO:root:CoverageTest: 0.90915
INFO:root:IntervalWidthTest: 0.04074
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05137040, Validation loss: 0.02799083, Gradient norm: 0.72338887
INFO:root:[    2] Training loss: 0.02386103, Validation loss: 0.01928018, Gradient norm: 0.61227470
INFO:root:[    3] Training loss: 0.01773411, Validation loss: 0.01372030, Gradient norm: 0.47725196
INFO:root:[    4] Training loss: 0.01700253, Validation loss: 0.01908421, Gradient norm: 0.54427609
INFO:root:[    5] Training loss: 0.01503872, Validation loss: 0.01276038, Gradient norm: 0.46008000
INFO:root:[    6] Training loss: 0.01168586, Validation loss: 0.01346167, Gradient norm: 0.27227110
INFO:root:[    7] Training loss: 0.01390918, Validation loss: 0.01341921, Gradient norm: 0.47377531
INFO:root:[    8] Training loss: 0.01376983, Validation loss: 0.01099223, Gradient norm: 0.45213799
INFO:root:[    9] Training loss: 0.01229267, Validation loss: 0.01322080, Gradient norm: 0.39786882
INFO:root:[   10] Training loss: 0.01329557, Validation loss: 0.01252222, Gradient norm: 0.41576448
INFO:root:[   11] Training loss: 0.01077653, Validation loss: 0.01142382, Gradient norm: 0.28499714
INFO:root:[   12] Training loss: 0.01114724, Validation loss: 0.00965901, Gradient norm: 0.33154988
INFO:root:[   13] Training loss: 0.01039380, Validation loss: 0.01321019, Gradient norm: 0.33207466
INFO:root:[   14] Training loss: 0.01048741, Validation loss: 0.01012031, Gradient norm: 0.37045619
INFO:root:[   15] Training loss: 0.01032808, Validation loss: 0.00996751, Gradient norm: 0.34182671
INFO:root:[   16] Training loss: 0.00968542, Validation loss: 0.01129623, Gradient norm: 0.30064747
INFO:root:[   17] Training loss: 0.01079817, Validation loss: 0.01298282, Gradient norm: 0.33291619
INFO:root:[   18] Training loss: 0.01040635, Validation loss: 0.01479341, Gradient norm: 0.39599358
INFO:root:[   19] Training loss: 0.00980263, Validation loss: 0.01058196, Gradient norm: 0.34043357
INFO:root:[   20] Training loss: 0.00878544, Validation loss: 0.01134883, Gradient norm: 0.25291075
INFO:root:[   21] Training loss: 0.01018203, Validation loss: 0.00958919, Gradient norm: 0.38823107
INFO:root:[   22] Training loss: 0.00921697, Validation loss: 0.01157634, Gradient norm: 0.31999169
INFO:root:[   23] Training loss: 0.00917685, Validation loss: 0.01055065, Gradient norm: 0.34073020
INFO:root:[   24] Training loss: 0.00844738, Validation loss: 0.01401248, Gradient norm: 0.27875460
INFO:root:[   25] Training loss: 0.00900154, Validation loss: 0.01042146, Gradient norm: 0.32747519
INFO:root:[   26] Training loss: 0.00867482, Validation loss: 0.00892282, Gradient norm: 0.27214791
INFO:root:[   27] Training loss: 0.00834011, Validation loss: 0.01090671, Gradient norm: 0.28794915
INFO:root:[   28] Training loss: 0.00757921, Validation loss: 0.00937659, Gradient norm: 0.20431824
INFO:root:[   29] Training loss: 0.00869830, Validation loss: 0.01055137, Gradient norm: 0.36073804
INFO:root:[   30] Training loss: 0.00815991, Validation loss: 0.01107060, Gradient norm: 0.28429493
INFO:root:[   31] Training loss: 0.00859216, Validation loss: 0.01081424, Gradient norm: 0.31810893
INFO:root:[   32] Training loss: 0.00730318, Validation loss: 0.01061872, Gradient norm: 0.22662521
INFO:root:[   33] Training loss: 0.00848373, Validation loss: 0.01048560, Gradient norm: 0.34436142
INFO:root:[   34] Training loss: 0.00783433, Validation loss: 0.00920484, Gradient norm: 0.31068558
INFO:root:[   35] Training loss: 0.00732633, Validation loss: 0.00918669, Gradient norm: 0.25445512
INFO:root:[   36] Training loss: 0.00727369, Validation loss: 0.00931449, Gradient norm: 0.26991694
INFO:root:[   37] Training loss: 0.00705406, Validation loss: 0.00934800, Gradient norm: 0.24553217
INFO:root:[   38] Training loss: 0.00771028, Validation loss: 0.00921269, Gradient norm: 0.30952592
INFO:root:[   39] Training loss: 0.00683323, Validation loss: 0.01063025, Gradient norm: 0.23100240
INFO:root:[   40] Training loss: 0.00747992, Validation loss: 0.01118591, Gradient norm: 0.31054999
INFO:root:[   41] Training loss: 0.00764311, Validation loss: 0.00922740, Gradient norm: 0.31802180
INFO:root:[   42] Training loss: 0.00663452, Validation loss: 0.01009657, Gradient norm: 0.20115260
INFO:root:[   43] Training loss: 0.00751842, Validation loss: 0.00884708, Gradient norm: 0.34160054
INFO:root:[   44] Training loss: 0.00653154, Validation loss: 0.01099095, Gradient norm: 0.21951211
INFO:root:[   45] Training loss: 0.00737073, Validation loss: 0.01147312, Gradient norm: 0.32288321
INFO:root:[   46] Training loss: 0.00657446, Validation loss: 0.00859447, Gradient norm: 0.26895380
INFO:root:[   47] Training loss: 0.00671993, Validation loss: 0.00883262, Gradient norm: 0.23698158
INFO:root:[   48] Training loss: 0.00705019, Validation loss: 0.00943619, Gradient norm: 0.28967838
INFO:root:[   49] Training loss: 0.00677146, Validation loss: 0.00930266, Gradient norm: 0.28386963
INFO:root:[   50] Training loss: 0.00712702, Validation loss: 0.00935047, Gradient norm: 0.30672512
INFO:root:[   51] Training loss: 0.00663317, Validation loss: 0.00919282, Gradient norm: 0.28307299
INFO:root:[   52] Training loss: 0.00626189, Validation loss: 0.00886152, Gradient norm: 0.29180775
INFO:root:[   53] Training loss: 0.00682696, Validation loss: 0.00885112, Gradient norm: 0.31595878
INFO:root:[   54] Training loss: 0.00621978, Validation loss: 0.01069615, Gradient norm: 0.21843165
INFO:root:[   55] Training loss: 0.00658563, Validation loss: 0.00828218, Gradient norm: 0.26163564
INFO:root:[   56] Training loss: 0.00592680, Validation loss: 0.00863546, Gradient norm: 0.21658044
INFO:root:[   57] Training loss: 0.00641758, Validation loss: 0.00888951, Gradient norm: 0.22368429
INFO:root:[   58] Training loss: 0.00573081, Validation loss: 0.01029543, Gradient norm: 0.22576272
INFO:root:[   59] Training loss: 0.00615999, Validation loss: 0.00836288, Gradient norm: 0.28769423
INFO:root:[   60] Training loss: 0.00587171, Validation loss: 0.00843154, Gradient norm: 0.25479658
INFO:root:[   61] Training loss: 0.00657609, Validation loss: 0.00892229, Gradient norm: 0.25762193
INFO:root:[   62] Training loss: 0.00612690, Validation loss: 0.00921104, Gradient norm: 0.29392185
INFO:root:[   63] Training loss: 0.00654178, Validation loss: 0.01099614, Gradient norm: 0.30131220
INFO:root:[   64] Training loss: 0.00570754, Validation loss: 0.00937565, Gradient norm: 0.23649215
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 2135.964s.
INFO:root:Emptying the cuda cache took 0.072s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0064
INFO:root:EnergyScoreTrain: 0.00513
INFO:root:CoverageTrain: 0.996
INFO:root:IntervalWidthTrain: 0.04772
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01137
INFO:root:EnergyScoreValidation: 0.00838
INFO:root:CoverageValidation: 0.95964
INFO:root:IntervalWidthValidation: 0.04751
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01134
INFO:root:EnergyScoreTest: 0.00837
INFO:root:CoverageTest: 0.95503
INFO:root:IntervalWidthTest: 0.0476
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04339964, Validation loss: 0.02554966, Gradient norm: 0.63362542
INFO:root:[    2] Training loss: 0.02321751, Validation loss: 0.02042736, Gradient norm: 0.51728331
INFO:root:[    3] Training loss: 0.01655811, Validation loss: 0.01734287, Gradient norm: 0.32377647
INFO:root:[    4] Training loss: 0.01766056, Validation loss: 0.01415754, Gradient norm: 0.46706314
INFO:root:[    5] Training loss: 0.01546238, Validation loss: 0.01383731, Gradient norm: 0.38637032
INFO:root:[    6] Training loss: 0.01404453, Validation loss: 0.02303453, Gradient norm: 0.37371134
INFO:root:[    7] Training loss: 0.01261794, Validation loss: 0.01195429, Gradient norm: 0.29494276
INFO:root:[    8] Training loss: 0.01241430, Validation loss: 0.01621098, Gradient norm: 0.33242219
INFO:root:[    9] Training loss: 0.01210557, Validation loss: 0.01100154, Gradient norm: 0.31802879
INFO:root:[   10] Training loss: 0.01325513, Validation loss: 0.01125706, Gradient norm: 0.39516852
INFO:root:[   11] Training loss: 0.01182939, Validation loss: 0.01102526, Gradient norm: 0.34347958
INFO:root:[   12] Training loss: 0.01083777, Validation loss: 0.01080894, Gradient norm: 0.27100908
INFO:root:[   13] Training loss: 0.01103943, Validation loss: 0.01070096, Gradient norm: 0.32049930
INFO:root:[   14] Training loss: 0.01001374, Validation loss: 0.01139934, Gradient norm: 0.22673219
INFO:root:[   15] Training loss: 0.01032639, Validation loss: 0.00984943, Gradient norm: 0.31369486
INFO:root:[   16] Training loss: 0.01024136, Validation loss: 0.00944332, Gradient norm: 0.27012958
INFO:root:[   17] Training loss: 0.01011230, Validation loss: 0.01079215, Gradient norm: 0.29031286
INFO:root:[   18] Training loss: 0.00960502, Validation loss: 0.00915958, Gradient norm: 0.24637272
INFO:root:[   19] Training loss: 0.00985363, Validation loss: 0.01033765, Gradient norm: 0.27865932
INFO:root:[   20] Training loss: 0.00973133, Validation loss: 0.00927214, Gradient norm: 0.25856320
INFO:root:[   21] Training loss: 0.00943481, Validation loss: 0.00994433, Gradient norm: 0.26568837
INFO:root:[   22] Training loss: 0.00820515, Validation loss: 0.00968566, Gradient norm: 0.20712710
INFO:root:[   23] Training loss: 0.00855975, Validation loss: 0.00889970, Gradient norm: 0.22956457
INFO:root:[   24] Training loss: 0.00958376, Validation loss: 0.01068743, Gradient norm: 0.33524053
INFO:root:[   25] Training loss: 0.00871455, Validation loss: 0.01020925, Gradient norm: 0.22346643
INFO:root:[   26] Training loss: 0.00906625, Validation loss: 0.01104060, Gradient norm: 0.25489073
INFO:root:[   27] Training loss: 0.00784646, Validation loss: 0.00905615, Gradient norm: 0.18610096
INFO:root:[   28] Training loss: 0.00874380, Validation loss: 0.00950816, Gradient norm: 0.25211586
INFO:root:[   29] Training loss: 0.00836381, Validation loss: 0.01264343, Gradient norm: 0.25277042
INFO:root:[   30] Training loss: 0.00815730, Validation loss: 0.01072414, Gradient norm: 0.25059570
INFO:root:[   31] Training loss: 0.00873275, Validation loss: 0.01390842, Gradient norm: 0.28459244
INFO:root:[   32] Training loss: 0.00783068, Validation loss: 0.00888022, Gradient norm: 0.22352851
INFO:root:[   33] Training loss: 0.00833620, Validation loss: 0.00951490, Gradient norm: 0.25306933
INFO:root:[   34] Training loss: 0.00724294, Validation loss: 0.00918362, Gradient norm: 0.22328727
INFO:root:[   35] Training loss: 0.00790264, Validation loss: 0.01013116, Gradient norm: 0.23848549
INFO:root:[   36] Training loss: 0.00762630, Validation loss: 0.00987349, Gradient norm: 0.22417606
INFO:root:[   37] Training loss: 0.00764112, Validation loss: 0.00968459, Gradient norm: 0.22115149
INFO:root:[   38] Training loss: 0.00750788, Validation loss: 0.00892627, Gradient norm: 0.24865432
INFO:root:[   39] Training loss: 0.00803294, Validation loss: 0.00916935, Gradient norm: 0.26230239
INFO:root:[   40] Training loss: 0.00761446, Validation loss: 0.00888928, Gradient norm: 0.21642551
INFO:root:[   41] Training loss: 0.00768941, Validation loss: 0.00876180, Gradient norm: 0.24546733
INFO:root:[   42] Training loss: 0.00740978, Validation loss: 0.00888329, Gradient norm: 0.22145030
INFO:root:[   43] Training loss: 0.00708122, Validation loss: 0.00940506, Gradient norm: 0.25937602
INFO:root:[   44] Training loss: 0.00675093, Validation loss: 0.00906973, Gradient norm: 0.18606223
INFO:root:[   45] Training loss: 0.00719357, Validation loss: 0.01236358, Gradient norm: 0.26458250
INFO:root:[   46] Training loss: 0.00725907, Validation loss: 0.00853482, Gradient norm: 0.28150266
INFO:root:[   47] Training loss: 0.00693078, Validation loss: 0.00945477, Gradient norm: 0.23398079
INFO:root:[   48] Training loss: 0.00753282, Validation loss: 0.01021851, Gradient norm: 0.25659459
INFO:root:[   49] Training loss: 0.00684496, Validation loss: 0.00906307, Gradient norm: 0.22086918
INFO:root:[   50] Training loss: 0.00668625, Validation loss: 0.00947374, Gradient norm: 0.23952540
INFO:root:[   51] Training loss: 0.00679041, Validation loss: 0.00969585, Gradient norm: 0.22273504
INFO:root:[   52] Training loss: 0.00690402, Validation loss: 0.00893162, Gradient norm: 0.25347217
INFO:root:[   53] Training loss: 0.00620176, Validation loss: 0.00884923, Gradient norm: 0.19956740
INFO:root:[   54] Training loss: 0.00650674, Validation loss: 0.00948090, Gradient norm: 0.23135199
INFO:root:[   55] Training loss: 0.00679295, Validation loss: 0.00964818, Gradient norm: 0.27983781
INFO:root:[   56] Training loss: 0.00674373, Validation loss: 0.00873866, Gradient norm: 0.24880740
INFO:root:[   57] Training loss: 0.00669646, Validation loss: 0.00879871, Gradient norm: 0.25429806
INFO:root:[   58] Training loss: 0.00667100, Validation loss: 0.00907167, Gradient norm: 0.21264867
INFO:root:[   59] Training loss: 0.00645720, Validation loss: 0.00928239, Gradient norm: 0.24565357
INFO:root:[   60] Training loss: 0.00621368, Validation loss: 0.01269460, Gradient norm: 0.23068646
INFO:root:[   61] Training loss: 0.00671516, Validation loss: 0.00945214, Gradient norm: 0.27032605
INFO:root:[   62] Training loss: 0.00620575, Validation loss: 0.01061863, Gradient norm: 0.22461236
INFO:root:[   63] Training loss: 0.00632689, Validation loss: 0.00915637, Gradient norm: 0.26616924
INFO:root:[   64] Training loss: 0.00600702, Validation loss: 0.00976211, Gradient norm: 0.22050900
INFO:root:[   65] Training loss: 0.00602898, Validation loss: 0.00889121, Gradient norm: 0.21194467
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 2187.657s.
INFO:root:Emptying the cuda cache took 0.069s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00739
INFO:root:EnergyScoreTrain: 0.00593
INFO:root:CoverageTrain: 0.99737
INFO:root:IntervalWidthTrain: 0.05222
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01187
INFO:root:EnergyScoreValidation: 0.00869
INFO:root:CoverageValidation: 0.97474
INFO:root:IntervalWidthValidation: 0.05198
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01177
INFO:root:EnergyScoreTest: 0.00865
INFO:root:CoverageTest: 0.97097
INFO:root:IntervalWidthTest: 0.052
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04627346, Validation loss: 0.02238292, Gradient norm: 0.54935809
INFO:root:[    2] Training loss: 0.02233547, Validation loss: 0.01674862, Gradient norm: 0.39292359
INFO:root:[    3] Training loss: 0.01737976, Validation loss: 0.01524900, Gradient norm: 0.34225004
INFO:root:[    4] Training loss: 0.01822006, Validation loss: 0.01761616, Gradient norm: 0.41651693
INFO:root:[    5] Training loss: 0.01522046, Validation loss: 0.01558837, Gradient norm: 0.32306536
INFO:root:[    6] Training loss: 0.01438579, Validation loss: 0.01182803, Gradient norm: 0.31472782
INFO:root:[    7] Training loss: 0.01360080, Validation loss: 0.01404108, Gradient norm: 0.29998176
INFO:root:[    8] Training loss: 0.01254508, Validation loss: 0.01284593, Gradient norm: 0.29968619
INFO:root:[    9] Training loss: 0.01267088, Validation loss: 0.01197452, Gradient norm: 0.31445082
INFO:root:[   10] Training loss: 0.01147993, Validation loss: 0.01202006, Gradient norm: 0.23380928
INFO:root:[   11] Training loss: 0.01232273, Validation loss: 0.01340265, Gradient norm: 0.28421952
INFO:root:[   12] Training loss: 0.01195049, Validation loss: 0.01126266, Gradient norm: 0.30837348
INFO:root:[   13] Training loss: 0.01062631, Validation loss: 0.01287519, Gradient norm: 0.22998991
INFO:root:[   14] Training loss: 0.01103501, Validation loss: 0.01180055, Gradient norm: 0.24869012
INFO:root:[   15] Training loss: 0.01082142, Validation loss: 0.01101671, Gradient norm: 0.26583024
INFO:root:[   16] Training loss: 0.00935037, Validation loss: 0.01067531, Gradient norm: 0.16435042
INFO:root:[   17] Training loss: 0.01094377, Validation loss: 0.01345141, Gradient norm: 0.29613702
INFO:root:[   18] Training loss: 0.01113407, Validation loss: 0.01253987, Gradient norm: 0.29553558
INFO:root:[   19] Training loss: 0.01011785, Validation loss: 0.00994669, Gradient norm: 0.22251201
INFO:root:[   20] Training loss: 0.00893265, Validation loss: 0.00961649, Gradient norm: 0.19177184
INFO:root:[   21] Training loss: 0.01013968, Validation loss: 0.01060897, Gradient norm: 0.26651419
INFO:root:[   22] Training loss: 0.00957953, Validation loss: 0.00980755, Gradient norm: 0.22446122
INFO:root:[   23] Training loss: 0.00953466, Validation loss: 0.01116032, Gradient norm: 0.25113470
INFO:root:[   24] Training loss: 0.01023730, Validation loss: 0.01000287, Gradient norm: 0.30508238
INFO:root:[   25] Training loss: 0.00892086, Validation loss: 0.00942331, Gradient norm: 0.20927217
INFO:root:[   26] Training loss: 0.00911696, Validation loss: 0.01100628, Gradient norm: 0.22235857
INFO:root:[   27] Training loss: 0.00945854, Validation loss: 0.00998710, Gradient norm: 0.26996820
INFO:root:[   28] Training loss: 0.00812816, Validation loss: 0.00977841, Gradient norm: 0.17665054
INFO:root:[   29] Training loss: 0.00894340, Validation loss: 0.01039469, Gradient norm: 0.26635411
INFO:root:[   30] Training loss: 0.00901251, Validation loss: 0.00963620, Gradient norm: 0.20400087
INFO:root:[   31] Training loss: 0.00846880, Validation loss: 0.00997723, Gradient norm: 0.22067378
INFO:root:[   32] Training loss: 0.00864591, Validation loss: 0.01107252, Gradient norm: 0.25858715
INFO:root:[   33] Training loss: 0.00795995, Validation loss: 0.01287772, Gradient norm: 0.21581303
INFO:root:[   34] Training loss: 0.00827752, Validation loss: 0.01031198, Gradient norm: 0.22279248
INFO:root:[   35] Training loss: 0.00851142, Validation loss: 0.00921346, Gradient norm: 0.22187971
INFO:root:[   36] Training loss: 0.00859187, Validation loss: 0.01052426, Gradient norm: 0.27287144
INFO:root:[   37] Training loss: 0.00803153, Validation loss: 0.01073002, Gradient norm: 0.24062503
INFO:root:[   38] Training loss: 0.00800018, Validation loss: 0.00928027, Gradient norm: 0.26448161
INFO:root:[   39] Training loss: 0.00796473, Validation loss: 0.00968294, Gradient norm: 0.22047401
INFO:root:[   40] Training loss: 0.00788766, Validation loss: 0.00919577, Gradient norm: 0.20839004
INFO:root:[   41] Training loss: 0.00763009, Validation loss: 0.01072431, Gradient norm: 0.25163920
INFO:root:[   42] Training loss: 0.00791852, Validation loss: 0.00954602, Gradient norm: 0.25134948
INFO:root:[   43] Training loss: 0.00787452, Validation loss: 0.01022783, Gradient norm: 0.23803197
INFO:root:[   44] Training loss: 0.00759154, Validation loss: 0.01054957, Gradient norm: 0.20843456
INFO:root:[   45] Training loss: 0.00755443, Validation loss: 0.01106359, Gradient norm: 0.24698276
INFO:root:[   46] Training loss: 0.00792725, Validation loss: 0.00924154, Gradient norm: 0.27604370
INFO:root:[   47] Training loss: 0.00668088, Validation loss: 0.00941719, Gradient norm: 0.16264472
INFO:root:[   48] Training loss: 0.00747537, Validation loss: 0.01012298, Gradient norm: 0.21584234
INFO:root:[   49] Training loss: 0.00710532, Validation loss: 0.01043515, Gradient norm: 0.22591282
INFO:root:[   50] Training loss: 0.00702558, Validation loss: 0.00872667, Gradient norm: 0.18272219
INFO:root:[   51] Training loss: 0.00703130, Validation loss: 0.01006690, Gradient norm: 0.24528084
INFO:root:[   52] Training loss: 0.00692574, Validation loss: 0.00849318, Gradient norm: 0.24596807
INFO:root:[   53] Training loss: 0.00763411, Validation loss: 0.00888859, Gradient norm: 0.26664538
INFO:root:[   54] Training loss: 0.00720611, Validation loss: 0.01108916, Gradient norm: 0.24763005
INFO:root:[   55] Training loss: 0.00722308, Validation loss: 0.00964804, Gradient norm: 0.26726039
INFO:root:[   56] Training loss: 0.00724021, Validation loss: 0.00962698, Gradient norm: 0.24424125
INFO:root:[   57] Training loss: 0.00654314, Validation loss: 0.00955442, Gradient norm: 0.19159276
INFO:root:[   58] Training loss: 0.00691044, Validation loss: 0.00882369, Gradient norm: 0.25340621
INFO:root:[   59] Training loss: 0.00664319, Validation loss: 0.00886032, Gradient norm: 0.22684050
INFO:root:[   60] Training loss: 0.00695366, Validation loss: 0.00899437, Gradient norm: 0.24136652
INFO:root:[   61] Training loss: 0.00701122, Validation loss: 0.00941006, Gradient norm: 0.24241445
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 2084.318s.
INFO:root:Emptying the cuda cache took 0.07s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00759
INFO:root:EnergyScoreTrain: 0.00621
INFO:root:CoverageTrain: 0.99731
INFO:root:IntervalWidthTrain: 0.05597
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01188
INFO:root:EnergyScoreValidation: 0.00874
INFO:root:CoverageValidation: 0.98071
INFO:root:IntervalWidthValidation: 0.05527
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01187
INFO:root:EnergyScoreTest: 0.00879
INFO:root:CoverageTest: 0.97703
INFO:root:IntervalWidthTest: 0.05564
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 60817408
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04356288, Validation loss: 0.03115323, Gradient norm: 0.51416216
INFO:root:[    2] Training loss: 0.02401811, Validation loss: 0.01798826, Gradient norm: 0.42484498
INFO:root:[    3] Training loss: 0.01949166, Validation loss: 0.01543410, Gradient norm: 0.37163915
INFO:root:[    4] Training loss: 0.01808179, Validation loss: 0.01590823, Gradient norm: 0.39379885
INFO:root:[    5] Training loss: 0.01554408, Validation loss: 0.01773313, Gradient norm: 0.29432653
INFO:root:[    6] Training loss: 0.01561215, Validation loss: 0.01294219, Gradient norm: 0.35619739
INFO:root:[    7] Training loss: 0.01375373, Validation loss: 0.01265981, Gradient norm: 0.28938011
INFO:root:[    8] Training loss: 0.01363686, Validation loss: 0.01557919, Gradient norm: 0.29951287
INFO:root:[    9] Training loss: 0.01278273, Validation loss: 0.01204962, Gradient norm: 0.30676584
INFO:root:[   10] Training loss: 0.01242232, Validation loss: 0.01178482, Gradient norm: 0.27217211
INFO:root:[   11] Training loss: 0.01319143, Validation loss: 0.01101771, Gradient norm: 0.29786351
INFO:root:[   12] Training loss: 0.01148161, Validation loss: 0.01016973, Gradient norm: 0.25933397
INFO:root:[   13] Training loss: 0.01258671, Validation loss: 0.01131047, Gradient norm: 0.30182204
INFO:root:[   14] Training loss: 0.01125322, Validation loss: 0.01059624, Gradient norm: 0.25751212
INFO:root:[   15] Training loss: 0.01172193, Validation loss: 0.01150131, Gradient norm: 0.33713845
INFO:root:[   16] Training loss: 0.01083039, Validation loss: 0.01136892, Gradient norm: 0.26275965
INFO:root:[   17] Training loss: 0.01028069, Validation loss: 0.01011690, Gradient norm: 0.21923823
INFO:root:[   18] Training loss: 0.01081491, Validation loss: 0.00969905, Gradient norm: 0.26304970
INFO:root:[   19] Training loss: 0.01051270, Validation loss: 0.01038932, Gradient norm: 0.27862948
INFO:root:[   20] Training loss: 0.01000662, Validation loss: 0.01173821, Gradient norm: 0.22698455
INFO:root:[   21] Training loss: 0.00957527, Validation loss: 0.01689643, Gradient norm: 0.22746994
INFO:root:[   22] Training loss: 0.01015571, Validation loss: 0.01113707, Gradient norm: 0.23789662
INFO:root:[   23] Training loss: 0.00996147, Validation loss: 0.01024312, Gradient norm: 0.24491974
INFO:root:[   24] Training loss: 0.00953815, Validation loss: 0.01154221, Gradient norm: 0.20414552
INFO:root:[   25] Training loss: 0.01060696, Validation loss: 0.00957116, Gradient norm: 0.29628713
INFO:root:[   26] Training loss: 0.00989855, Validation loss: 0.01078866, Gradient norm: 0.27551934
INFO:root:[   27] Training loss: 0.00922152, Validation loss: 0.01158675, Gradient norm: 0.24879675
INFO:root:[   28] Training loss: 0.00972323, Validation loss: 0.00918952, Gradient norm: 0.21916127
INFO:root:[   29] Training loss: 0.00944591, Validation loss: 0.01175702, Gradient norm: 0.22153868
INFO:root:[   30] Training loss: 0.00930244, Validation loss: 0.01031252, Gradient norm: 0.25803842
INFO:root:[   31] Training loss: 0.00877270, Validation loss: 0.01179210, Gradient norm: 0.23212396
INFO:root:[   32] Training loss: 0.00921237, Validation loss: 0.00888141, Gradient norm: 0.23665509
INFO:root:[   33] Training loss: 0.00842681, Validation loss: 0.00949887, Gradient norm: 0.22579191
INFO:root:[   34] Training loss: 0.00862549, Validation loss: 0.00996449, Gradient norm: 0.23097210
INFO:root:[   35] Training loss: 0.00817397, Validation loss: 0.00913290, Gradient norm: 0.19978718
INFO:root:[   36] Training loss: 0.00859625, Validation loss: 0.00940974, Gradient norm: 0.22529325
INFO:root:[   37] Training loss: 0.00875722, Validation loss: 0.01142157, Gradient norm: 0.20986835
INFO:root:[   38] Training loss: 0.00866988, Validation loss: 0.00909152, Gradient norm: 0.26927911
INFO:root:[   39] Training loss: 0.00821890, Validation loss: 0.01071012, Gradient norm: 0.22357432
INFO:root:[   40] Training loss: 0.00864052, Validation loss: 0.00998008, Gradient norm: 0.24107163
INFO:root:[   41] Training loss: 0.00770482, Validation loss: 0.01012040, Gradient norm: 0.22185179
INFO:root:[   42] Training loss: 0.00832185, Validation loss: 0.00881882, Gradient norm: 0.24850183
INFO:root:[   43] Training loss: 0.00895219, Validation loss: 0.01067504, Gradient norm: 0.29891113
INFO:root:[   44] Training loss: 0.00820791, Validation loss: 0.01144414, Gradient norm: 0.23295569
INFO:root:[   45] Training loss: 0.00825963, Validation loss: 0.01038915, Gradient norm: 0.25550083
INFO:root:[   46] Training loss: 0.00853668, Validation loss: 0.01000439, Gradient norm: 0.28117977
INFO:root:[   47] Training loss: 0.00771858, Validation loss: 0.01056534, Gradient norm: 0.24525820
INFO:root:[   48] Training loss: 0.00803387, Validation loss: 0.00989236, Gradient norm: 0.24861533
INFO:root:[   49] Training loss: 0.00780710, Validation loss: 0.01249870, Gradient norm: 0.22013427
INFO:root:[   50] Training loss: 0.00771610, Validation loss: 0.01009250, Gradient norm: 0.20822477
INFO:root:[   51] Training loss: 0.00741412, Validation loss: 0.00875923, Gradient norm: 0.22803949
INFO:root:[   52] Training loss: 0.00714015, Validation loss: 0.00933531, Gradient norm: 0.17854822
INFO:root:[   53] Training loss: 0.00707569, Validation loss: 0.00921973, Gradient norm: 0.21477573
INFO:root:[   54] Training loss: 0.00773060, Validation loss: 0.00903060, Gradient norm: 0.26139077
INFO:root:[   55] Training loss: 0.00815134, Validation loss: 0.01127320, Gradient norm: 0.23801581
INFO:root:[   56] Training loss: 0.00726756, Validation loss: 0.00966326, Gradient norm: 0.19575826
INFO:root:[   57] Training loss: 0.00799172, Validation loss: 0.00927654, Gradient norm: 0.26809024
INFO:root:[   58] Training loss: 0.00731192, Validation loss: 0.00893029, Gradient norm: 0.23271466
INFO:root:[   59] Training loss: 0.00766018, Validation loss: 0.00960358, Gradient norm: 0.27614635
INFO:root:[   60] Training loss: 0.00764409, Validation loss: 0.01013993, Gradient norm: 0.25349670
INFO:root:[   61] Training loss: 0.00668125, Validation loss: 0.00902843, Gradient norm: 0.18756039
INFO:root:[   62] Training loss: 0.00744622, Validation loss: 0.01008734, Gradient norm: 0.27884275
INFO:root:[   63] Training loss: 0.00673200, Validation loss: 0.00919713, Gradient norm: 0.20100062
INFO:root:[   64] Training loss: 0.00738854, Validation loss: 0.01055852, Gradient norm: 0.23933910
INFO:root:[   65] Training loss: 0.00705874, Validation loss: 0.00906438, Gradient norm: 0.23500943
INFO:root:[   66] Training loss: 0.00697127, Validation loss: 0.00901816, Gradient norm: 0.19481115
INFO:root:[   67] Training loss: 0.00705226, Validation loss: 0.00980359, Gradient norm: 0.19197162
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 2259.132s.
INFO:root:Emptying the cuda cache took 0.07s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00772
INFO:root:EnergyScoreTrain: 0.00645
INFO:root:CoverageTrain: 0.9984
INFO:root:IntervalWidthTrain: 0.0588
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01192
INFO:root:EnergyScoreValidation: 0.00885
INFO:root:CoverageValidation: 0.98309
INFO:root:IntervalWidthValidation: 0.05839
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01194
INFO:root:EnergyScoreTest: 0.00892
INFO:root:CoverageTest: 0.98114
INFO:root:IntervalWidthTest: 0.05852
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.04407499, Validation loss: 0.02250497, Gradient norm: 0.40438625
INFO:root:[    2] Training loss: 0.02390852, Validation loss: 0.02965418, Gradient norm: 0.34420907
INFO:root:[    3] Training loss: 0.02087806, Validation loss: 0.02027188, Gradient norm: 0.33684548
INFO:root:[    4] Training loss: 0.01782558, Validation loss: 0.01626747, Gradient norm: 0.31289599
INFO:root:[    5] Training loss: 0.01692183, Validation loss: 0.01433193, Gradient norm: 0.30074548
INFO:root:[    6] Training loss: 0.01429067, Validation loss: 0.01550177, Gradient norm: 0.24630139
INFO:root:[    7] Training loss: 0.01392104, Validation loss: 0.01607812, Gradient norm: 0.24093250
INFO:root:[    8] Training loss: 0.01392826, Validation loss: 0.01377578, Gradient norm: 0.29847448
INFO:root:[    9] Training loss: 0.01308366, Validation loss: 0.01185082, Gradient norm: 0.23701686
INFO:root:[   10] Training loss: 0.01347487, Validation loss: 0.01471187, Gradient norm: 0.30474019
INFO:root:[   11] Training loss: 0.01235991, Validation loss: 0.01195894, Gradient norm: 0.24800511
INFO:root:[   12] Training loss: 0.01172312, Validation loss: 0.01421489, Gradient norm: 0.22016296
INFO:root:[   13] Training loss: 0.01282471, Validation loss: 0.01075748, Gradient norm: 0.30280955
INFO:root:[   14] Training loss: 0.01064825, Validation loss: 0.01157819, Gradient norm: 0.17132948
INFO:root:[   15] Training loss: 0.01195094, Validation loss: 0.01149830, Gradient norm: 0.26918162
INFO:root:[   16] Training loss: 0.01242820, Validation loss: 0.01087473, Gradient norm: 0.28624955
INFO:root:[   17] Training loss: 0.01071606, Validation loss: 0.01077574, Gradient norm: 0.21065429
INFO:root:[   18] Training loss: 0.01025249, Validation loss: 0.01410902, Gradient norm: 0.22142102
INFO:root:[   19] Training loss: 0.01098258, Validation loss: 0.00999737, Gradient norm: 0.24744481
INFO:root:[   20] Training loss: 0.00998249, Validation loss: 0.01229200, Gradient norm: 0.20172199
INFO:root:[   21] Training loss: 0.01109973, Validation loss: 0.01025846, Gradient norm: 0.28041123
INFO:root:[   22] Training loss: 0.01024740, Validation loss: 0.01312479, Gradient norm: 0.22771574
INFO:root:[   23] Training loss: 0.01023812, Validation loss: 0.01034542, Gradient norm: 0.24571136
INFO:root:[   24] Training loss: 0.00992931, Validation loss: 0.01075666, Gradient norm: 0.22324198
INFO:root:[   25] Training loss: 0.01046727, Validation loss: 0.01293136, Gradient norm: 0.24965987
INFO:root:[   26] Training loss: 0.01074982, Validation loss: 0.01147728, Gradient norm: 0.28684691
INFO:root:[   27] Training loss: 0.00941516, Validation loss: 0.00897030, Gradient norm: 0.21970262
INFO:root:[   28] Training loss: 0.01063397, Validation loss: 0.01234915, Gradient norm: 0.27708230
INFO:root:[   29] Training loss: 0.00984839, Validation loss: 0.01002532, Gradient norm: 0.24443284
INFO:root:[   30] Training loss: 0.01039345, Validation loss: 0.01047696, Gradient norm: 0.26948333
INFO:root:[   31] Training loss: 0.00916088, Validation loss: 0.00974475, Gradient norm: 0.22475875
INFO:root:[   32] Training loss: 0.00921557, Validation loss: 0.00950044, Gradient norm: 0.23423601
INFO:root:[   33] Training loss: 0.00971236, Validation loss: 0.01017052, Gradient norm: 0.26477027
INFO:root:[   34] Training loss: 0.00936646, Validation loss: 0.00960161, Gradient norm: 0.24364424
INFO:root:[   35] Training loss: 0.00939608, Validation loss: 0.01260800, Gradient norm: 0.22396965
INFO:root:[   36] Training loss: 0.00903464, Validation loss: 0.00899902, Gradient norm: 0.24983812
INFO:root:[   37] Training loss: 0.00901020, Validation loss: 0.00953381, Gradient norm: 0.22597223
INFO:root:[   38] Training loss: 0.00948697, Validation loss: 0.01136931, Gradient norm: 0.26878517
INFO:root:[   39] Training loss: 0.00888603, Validation loss: 0.01010031, Gradient norm: 0.21200398
INFO:root:[   40] Training loss: 0.00893820, Validation loss: 0.00961736, Gradient norm: 0.23044019
INFO:root:[   41] Training loss: 0.00824958, Validation loss: 0.01136547, Gradient norm: 0.21682301
INFO:root:[   42] Training loss: 0.00977293, Validation loss: 0.01296610, Gradient norm: 0.27589299
INFO:root:[   43] Training loss: 0.00867690, Validation loss: 0.01380157, Gradient norm: 0.24133763
INFO:root:[   44] Training loss: 0.00928286, Validation loss: 0.00936396, Gradient norm: 0.28545984
INFO:root:[   45] Training loss: 0.00868730, Validation loss: 0.01181014, Gradient norm: 0.24124660
INFO:root:[   46] Training loss: 0.00846431, Validation loss: 0.00949691, Gradient norm: 0.21534512
INFO:root:[   47] Training loss: 0.00861628, Validation loss: 0.00975624, Gradient norm: 0.24929222
INFO:root:[   48] Training loss: 0.00839558, Validation loss: 0.00967249, Gradient norm: 0.22921829
INFO:root:[   49] Training loss: 0.00824183, Validation loss: 0.01022812, Gradient norm: 0.19272852
INFO:root:[   50] Training loss: 0.00826175, Validation loss: 0.00895321, Gradient norm: 0.25733044
INFO:root:[   51] Training loss: 0.00883596, Validation loss: 0.01056511, Gradient norm: 0.28921176
INFO:root:[   52] Training loss: 0.00856559, Validation loss: 0.00969186, Gradient norm: 0.24810586
INFO:root:[   53] Training loss: 0.00831356, Validation loss: 0.00968605, Gradient norm: 0.21857762
INFO:root:[   54] Training loss: 0.00769189, Validation loss: 0.01074777, Gradient norm: 0.21449549
INFO:root:[   55] Training loss: 0.00789369, Validation loss: 0.01001925, Gradient norm: 0.24420753
INFO:root:[   56] Training loss: 0.00744165, Validation loss: 0.00936355, Gradient norm: 0.20838625
INFO:root:[   57] Training loss: 0.00799499, Validation loss: 0.01089311, Gradient norm: 0.25957048
INFO:root:[   58] Training loss: 0.00827277, Validation loss: 0.00873367, Gradient norm: 0.20382678
INFO:root:[   59] Training loss: 0.00821106, Validation loss: 0.00977685, Gradient norm: 0.24809566
INFO:root:[   60] Training loss: 0.00732225, Validation loss: 0.00893561, Gradient norm: 0.19506188
INFO:root:[   61] Training loss: 0.00845189, Validation loss: 0.01010713, Gradient norm: 0.28087309
INFO:root:[   62] Training loss: 0.00785561, Validation loss: 0.01072346, Gradient norm: 0.23905134
INFO:root:[   63] Training loss: 0.00833403, Validation loss: 0.00877875, Gradient norm: 0.27471847
INFO:root:[   64] Training loss: 0.00724752, Validation loss: 0.00863181, Gradient norm: 0.17461191
INFO:root:[   65] Training loss: 0.00760209, Validation loss: 0.01110386, Gradient norm: 0.22574162
INFO:root:[   66] Training loss: 0.00757001, Validation loss: 0.00847834, Gradient norm: 0.23728940
INFO:root:[   67] Training loss: 0.00763541, Validation loss: 0.00925266, Gradient norm: 0.21074842
INFO:root:[   68] Training loss: 0.00731369, Validation loss: 0.01096842, Gradient norm: 0.22864405
INFO:root:[   69] Training loss: 0.00783400, Validation loss: 0.00888894, Gradient norm: 0.25156511
INFO:root:[   70] Training loss: 0.00731580, Validation loss: 0.00887184, Gradient norm: 0.19268720
INFO:root:[   71] Training loss: 0.00745169, Validation loss: 0.00954963, Gradient norm: 0.23451909
INFO:root:[   72] Training loss: 0.00740939, Validation loss: 0.00969089, Gradient norm: 0.24114114
INFO:root:[   73] Training loss: 0.00720672, Validation loss: 0.00866036, Gradient norm: 0.22956610
INFO:root:[   74] Training loss: 0.00806933, Validation loss: 0.01000858, Gradient norm: 0.24304321
INFO:root:[   75] Training loss: 0.00760477, Validation loss: 0.01284422, Gradient norm: 0.22696853
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 2532.709s.
INFO:root:Emptying the cuda cache took 0.071s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00763
INFO:root:EnergyScoreTrain: 0.00658
INFO:root:CoverageTrain: 0.99885
INFO:root:IntervalWidthTrain: 0.06252
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0114
INFO:root:EnergyScoreValidation: 0.00868
INFO:root:CoverageValidation: 0.99009
INFO:root:IntervalWidthValidation: 0.06215
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01116
INFO:root:EnergyScoreTest: 0.0085
INFO:root:CoverageTest: 0.991
INFO:root:IntervalWidthTest: 0.06219
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05529636, Validation loss: 0.03184795, Gradient norm: 0.65527438
INFO:root:[    2] Training loss: 0.02449240, Validation loss: 0.01956860, Gradient norm: 0.57656724
INFO:root:[    3] Training loss: 0.01900858, Validation loss: 0.01740338, Gradient norm: 0.50846336
INFO:root:[    4] Training loss: 0.01653842, Validation loss: 0.01123441, Gradient norm: 0.45677515
INFO:root:[    5] Training loss: 0.01516755, Validation loss: 0.01255236, Gradient norm: 0.41606541
INFO:root:[    6] Training loss: 0.01532505, Validation loss: 0.01507399, Gradient norm: 0.46140604
INFO:root:[    7] Training loss: 0.01402078, Validation loss: 0.01158661, Gradient norm: 0.41241784
INFO:root:[    8] Training loss: 0.01355114, Validation loss: 0.01650385, Gradient norm: 0.37375349
INFO:root:[    9] Training loss: 0.01304040, Validation loss: 0.00987151, Gradient norm: 0.39451637
INFO:root:[   10] Training loss: 0.01112124, Validation loss: 0.01109263, Gradient norm: 0.32707594
INFO:root:[   11] Training loss: 0.01153811, Validation loss: 0.01446474, Gradient norm: 0.36167025
INFO:root:[   12] Training loss: 0.01125780, Validation loss: 0.00997810, Gradient norm: 0.32525426
INFO:root:[   13] Training loss: 0.01138017, Validation loss: 0.01116578, Gradient norm: 0.34506083
INFO:root:[   14] Training loss: 0.01064154, Validation loss: 0.01077210, Gradient norm: 0.34442063
INFO:root:[   15] Training loss: 0.00973603, Validation loss: 0.01029142, Gradient norm: 0.30505490
INFO:root:[   16] Training loss: 0.01128297, Validation loss: 0.01040574, Gradient norm: 0.39038823
INFO:root:[   17] Training loss: 0.01024940, Validation loss: 0.01130933, Gradient norm: 0.31433668
INFO:root:[   18] Training loss: 0.00916465, Validation loss: 0.00926993, Gradient norm: 0.28260655
INFO:root:[   19] Training loss: 0.01014020, Validation loss: 0.01149828, Gradient norm: 0.32695797
INFO:root:[   20] Training loss: 0.00989093, Validation loss: 0.01220376, Gradient norm: 0.31342843
INFO:root:[   21] Training loss: 0.00972818, Validation loss: 0.01133052, Gradient norm: 0.26449960
INFO:root:[   22] Training loss: 0.00890073, Validation loss: 0.01231714, Gradient norm: 0.30482519
INFO:root:[   23] Training loss: 0.00956613, Validation loss: 0.01187304, Gradient norm: 0.34438964
INFO:root:[   24] Training loss: 0.00912603, Validation loss: 0.00904597, Gradient norm: 0.31400038
INFO:root:[   25] Training loss: 0.00915525, Validation loss: 0.01213765, Gradient norm: 0.33002977
INFO:root:[   26] Training loss: 0.00867856, Validation loss: 0.01069321, Gradient norm: 0.25149438
INFO:root:[   27] Training loss: 0.00840849, Validation loss: 0.00907781, Gradient norm: 0.29981253
INFO:root:[   28] Training loss: 0.00828396, Validation loss: 0.00890631, Gradient norm: 0.27836667
INFO:root:[   29] Training loss: 0.00849496, Validation loss: 0.01011552, Gradient norm: 0.34159055
INFO:root:[   30] Training loss: 0.00802697, Validation loss: 0.00936133, Gradient norm: 0.27288715
INFO:root:[   31] Training loss: 0.00845310, Validation loss: 0.01187603, Gradient norm: 0.32302489
INFO:root:[   32] Training loss: 0.00875214, Validation loss: 0.00969125, Gradient norm: 0.35058988
INFO:root:[   33] Training loss: 0.00783704, Validation loss: 0.01147579, Gradient norm: 0.27055369
INFO:root:[   34] Training loss: 0.00803498, Validation loss: 0.01076452, Gradient norm: 0.30256889
INFO:root:[   35] Training loss: 0.00787432, Validation loss: 0.00966145, Gradient norm: 0.31494509
INFO:root:[   36] Training loss: 0.00782845, Validation loss: 0.00987553, Gradient norm: 0.29439468
INFO:root:[   37] Training loss: 0.00728225, Validation loss: 0.00904843, Gradient norm: 0.25523784
INFO:root:[   38] Training loss: 0.00740684, Validation loss: 0.00880105, Gradient norm: 0.25662841
INFO:root:[   39] Training loss: 0.00712122, Validation loss: 0.01093062, Gradient norm: 0.22932406
INFO:root:[   40] Training loss: 0.00772458, Validation loss: 0.00906190, Gradient norm: 0.32822096
INFO:root:[   41] Training loss: 0.00747049, Validation loss: 0.00996907, Gradient norm: 0.33724502
INFO:root:[   42] Training loss: 0.00793305, Validation loss: 0.00930773, Gradient norm: 0.30367464
INFO:root:[   43] Training loss: 0.00750349, Validation loss: 0.01147000, Gradient norm: 0.30566280
INFO:root:[   44] Training loss: 0.00815023, Validation loss: 0.00950447, Gradient norm: 0.36253758
INFO:root:[   45] Training loss: 0.00719607, Validation loss: 0.01026227, Gradient norm: 0.25269566
INFO:root:[   46] Training loss: 0.00762884, Validation loss: 0.00947622, Gradient norm: 0.34405792
INFO:root:[   47] Training loss: 0.00674925, Validation loss: 0.00919432, Gradient norm: 0.27815451
INFO:root:[   48] Training loss: 0.00711111, Validation loss: 0.01056112, Gradient norm: 0.31463962
INFO:root:[   49] Training loss: 0.00696962, Validation loss: 0.00860826, Gradient norm: 0.31322600
INFO:root:[   50] Training loss: 0.00675880, Validation loss: 0.01193173, Gradient norm: 0.29351452
INFO:root:[   51] Training loss: 0.00702023, Validation loss: 0.00999479, Gradient norm: 0.29405518
INFO:root:[   52] Training loss: 0.00704044, Validation loss: 0.00982454, Gradient norm: 0.32207934
INFO:root:[   53] Training loss: 0.00643436, Validation loss: 0.00960752, Gradient norm: 0.25871747
INFO:root:[   54] Training loss: 0.00694745, Validation loss: 0.01084272, Gradient norm: 0.32443331
INFO:root:[   55] Training loss: 0.00688631, Validation loss: 0.00981924, Gradient norm: 0.31687262
INFO:root:[   56] Training loss: 0.00731668, Validation loss: 0.01312807, Gradient norm: 0.30328585
INFO:root:[   57] Training loss: 0.00658489, Validation loss: 0.00896340, Gradient norm: 0.29811706
INFO:root:[   58] Training loss: 0.00687773, Validation loss: 0.00900479, Gradient norm: 0.30145959
INFO:root:[   59] Training loss: 0.00694050, Validation loss: 0.00993171, Gradient norm: 0.30615258
INFO:root:[   60] Training loss: 0.00674038, Validation loss: 0.00951772, Gradient norm: 0.32828793
INFO:root:[   61] Training loss: 0.00652133, Validation loss: 0.01001064, Gradient norm: 0.30471442
INFO:root:[   62] Training loss: 0.00679657, Validation loss: 0.00963346, Gradient norm: 0.31002051
INFO:root:[   63] Training loss: 0.00599416, Validation loss: 0.00854189, Gradient norm: 0.30550668
INFO:root:[   64] Training loss: 0.00659310, Validation loss: 0.01013535, Gradient norm: 0.31170876
INFO:root:[   65] Training loss: 0.00662966, Validation loss: 0.00971698, Gradient norm: 0.31920716
INFO:root:[   66] Training loss: 0.00631357, Validation loss: 0.00913565, Gradient norm: 0.26928509
INFO:root:[   67] Training loss: 0.00651412, Validation loss: 0.01036615, Gradient norm: 0.32070090
INFO:root:[   68] Training loss: 0.00646577, Validation loss: 0.01134916, Gradient norm: 0.29706175
INFO:root:[   69] Training loss: 0.00675922, Validation loss: 0.01015790, Gradient norm: 0.29661932
INFO:root:[   70] Training loss: 0.00639201, Validation loss: 0.01008362, Gradient norm: 0.33548287
INFO:root:[   71] Training loss: 0.00668113, Validation loss: 0.01126128, Gradient norm: 0.32434813
INFO:root:[   72] Training loss: 0.00642029, Validation loss: 0.00921787, Gradient norm: 0.31714652
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 1537.643s.
INFO:root:Emptying the cuda cache took 0.044s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00617
INFO:root:EnergyScoreTrain: 0.00457
INFO:root:CoverageTrain: 0.88832
INFO:root:IntervalWidthTrain: 0.0246
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0111
INFO:root:EnergyScoreValidation: 0.00852
INFO:root:CoverageValidation: 0.78678
INFO:root:IntervalWidthValidation: 0.02468
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01114
INFO:root:EnergyScoreTest: 0.00858
INFO:root:CoverageTest: 0.78066
INFO:root:IntervalWidthTest: 0.02459
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05467189, Validation loss: 0.03122395, Gradient norm: 0.66774295
INFO:root:[    2] Training loss: 0.02436391, Validation loss: 0.01795609, Gradient norm: 0.42815623
INFO:root:[    3] Training loss: 0.02229316, Validation loss: 0.01628734, Gradient norm: 0.46720736
INFO:root:[    4] Training loss: 0.01984652, Validation loss: 0.01619504, Gradient norm: 0.42117502
INFO:root:[    5] Training loss: 0.01674762, Validation loss: 0.01510740, Gradient norm: 0.38948147
INFO:root:[    6] Training loss: 0.01699291, Validation loss: 0.01974718, Gradient norm: 0.35290248
INFO:root:[    7] Training loss: 0.01430829, Validation loss: 0.01236863, Gradient norm: 0.28419336
INFO:root:[    8] Training loss: 0.01506486, Validation loss: 0.01194635, Gradient norm: 0.37277064
INFO:root:[    9] Training loss: 0.01426926, Validation loss: 0.01390258, Gradient norm: 0.34841412
INFO:root:[   10] Training loss: 0.01325979, Validation loss: 0.01297545, Gradient norm: 0.28550578
INFO:root:[   11] Training loss: 0.01446875, Validation loss: 0.01139469, Gradient norm: 0.36694861
INFO:root:[   12] Training loss: 0.01272690, Validation loss: 0.01692220, Gradient norm: 0.31523294
INFO:root:[   13] Training loss: 0.01210308, Validation loss: 0.01039595, Gradient norm: 0.26857490
INFO:root:[   14] Training loss: 0.01255450, Validation loss: 0.01060599, Gradient norm: 0.31527352
INFO:root:[   15] Training loss: 0.01155050, Validation loss: 0.01241938, Gradient norm: 0.29410761
INFO:root:[   16] Training loss: 0.01229517, Validation loss: 0.01005843, Gradient norm: 0.30170970
INFO:root:[   17] Training loss: 0.01143833, Validation loss: 0.01011899, Gradient norm: 0.28057577
INFO:root:[   18] Training loss: 0.01196140, Validation loss: 0.01063882, Gradient norm: 0.30207732
INFO:root:[   19] Training loss: 0.01108484, Validation loss: 0.00951385, Gradient norm: 0.26163620
INFO:root:[   20] Training loss: 0.01105769, Validation loss: 0.00991386, Gradient norm: 0.27128509
INFO:root:[   21] Training loss: 0.01087963, Validation loss: 0.00877198, Gradient norm: 0.28378536
INFO:root:[   22] Training loss: 0.01167769, Validation loss: 0.01217078, Gradient norm: 0.28354838
INFO:root:[   23] Training loss: 0.01046483, Validation loss: 0.01030267, Gradient norm: 0.28078970
INFO:root:[   24] Training loss: 0.01050844, Validation loss: 0.01273728, Gradient norm: 0.31527887
INFO:root:[   25] Training loss: 0.01111587, Validation loss: 0.00986758, Gradient norm: 0.30024450
INFO:root:[   26] Training loss: 0.01026472, Validation loss: 0.01362921, Gradient norm: 0.27102008
INFO:root:[   27] Training loss: 0.01026204, Validation loss: 0.00959354, Gradient norm: 0.28459399
INFO:root:[   28] Training loss: 0.01057829, Validation loss: 0.01113738, Gradient norm: 0.31869051
INFO:root:[   29] Training loss: 0.01053059, Validation loss: 0.00998413, Gradient norm: 0.28951479
INFO:root:[   30] Training loss: 0.01016141, Validation loss: 0.01123782, Gradient norm: 0.28695078
INFO:root:[   31] Training loss: 0.01029640, Validation loss: 0.01129608, Gradient norm: 0.29498083
INFO:root:[   32] Training loss: 0.01000419, Validation loss: 0.01229277, Gradient norm: 0.28935643
INFO:root:[   33] Training loss: 0.01014342, Validation loss: 0.00948485, Gradient norm: 0.31684006
INFO:root:[   34] Training loss: 0.00930693, Validation loss: 0.01087105, Gradient norm: 0.27191890
INFO:root:[   35] Training loss: 0.00959723, Validation loss: 0.00984851, Gradient norm: 0.27942781
INFO:root:[   36] Training loss: 0.01009663, Validation loss: 0.01049314, Gradient norm: 0.30658602
INFO:root:[   37] Training loss: 0.00932345, Validation loss: 0.00936468, Gradient norm: 0.26851881
INFO:root:[   38] Training loss: 0.00987355, Validation loss: 0.01131180, Gradient norm: 0.30597315
INFO:root:[   39] Training loss: 0.00975453, Validation loss: 0.01021016, Gradient norm: 0.29386470
INFO:root:[   40] Training loss: 0.00904888, Validation loss: 0.01044490, Gradient norm: 0.27335020
INFO:root:[   41] Training loss: 0.00907465, Validation loss: 0.01103286, Gradient norm: 0.26628821
INFO:root:[   42] Training loss: 0.00916692, Validation loss: 0.00928812, Gradient norm: 0.29909210
INFO:root:[   43] Training loss: 0.00870750, Validation loss: 0.01041322, Gradient norm: 0.28513094
INFO:root:[   44] Training loss: 0.00963010, Validation loss: 0.00970649, Gradient norm: 0.31183920
INFO:root:[   45] Training loss: 0.00888210, Validation loss: 0.01088146, Gradient norm: 0.28063409
INFO:root:[   46] Training loss: 0.00897939, Validation loss: 0.00885475, Gradient norm: 0.29380771
INFO:root:[   47] Training loss: 0.00848511, Validation loss: 0.00916451, Gradient norm: 0.24079539
INFO:root:[   48] Training loss: 0.00853448, Validation loss: 0.00840929, Gradient norm: 0.27698120
INFO:root:[   49] Training loss: 0.00839508, Validation loss: 0.00979502, Gradient norm: 0.27846981
INFO:root:[   50] Training loss: 0.00871038, Validation loss: 0.01082776, Gradient norm: 0.27854848
INFO:root:[   51] Training loss: 0.00861922, Validation loss: 0.01006501, Gradient norm: 0.28244614
INFO:root:[   52] Training loss: 0.00922209, Validation loss: 0.00892296, Gradient norm: 0.31964794
INFO:root:[   53] Training loss: 0.00874365, Validation loss: 0.00919693, Gradient norm: 0.30435985
INFO:root:[   54] Training loss: 0.00787083, Validation loss: 0.01055721, Gradient norm: 0.29221647
INFO:root:[   55] Training loss: 0.00847710, Validation loss: 0.01076138, Gradient norm: 0.27201898
INFO:root:[   56] Training loss: 0.00805883, Validation loss: 0.01034916, Gradient norm: 0.29030008
INFO:root:[   57] Training loss: 0.00818382, Validation loss: 0.01144433, Gradient norm: 0.28041950
INFO:root:[   58] Training loss: 0.00845091, Validation loss: 0.00997838, Gradient norm: 0.30227859
INFO:root:[   59] Training loss: 0.00832344, Validation loss: 0.01007706, Gradient norm: 0.29917147
INFO:root:[   60] Training loss: 0.00838976, Validation loss: 0.00899160, Gradient norm: 0.28022272
INFO:root:[   61] Training loss: 0.00739944, Validation loss: 0.00943552, Gradient norm: 0.21630495
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1333.063s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00796
INFO:root:EnergyScoreTrain: 0.00591
INFO:root:CoverageTrain: 0.91325
INFO:root:IntervalWidthTrain: 0.03601
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01143
INFO:root:EnergyScoreValidation: 0.0085
INFO:root:CoverageValidation: 0.85262
INFO:root:IntervalWidthValidation: 0.03598
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01151
INFO:root:EnergyScoreTest: 0.00856
INFO:root:CoverageTest: 0.84914
INFO:root:IntervalWidthTest: 0.03599
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05984518, Validation loss: 0.02768406, Gradient norm: 0.53324957
INFO:root:[    2] Training loss: 0.02665036, Validation loss: 0.01528035, Gradient norm: 0.40124565
INFO:root:[    3] Training loss: 0.02422345, Validation loss: 0.01631633, Gradient norm: 0.45870918
INFO:root:[    4] Training loss: 0.01991533, Validation loss: 0.01971300, Gradient norm: 0.36060809
INFO:root:[    5] Training loss: 0.01894568, Validation loss: 0.01467580, Gradient norm: 0.35089554
INFO:root:[    6] Training loss: 0.01914525, Validation loss: 0.01288011, Gradient norm: 0.42424830
INFO:root:[    7] Training loss: 0.01626231, Validation loss: 0.01624676, Gradient norm: 0.33349141
INFO:root:[    8] Training loss: 0.01611458, Validation loss: 0.01238671, Gradient norm: 0.33198602
INFO:root:[    9] Training loss: 0.01597715, Validation loss: 0.01186627, Gradient norm: 0.34584181
INFO:root:[   10] Training loss: 0.01397480, Validation loss: 0.01070535, Gradient norm: 0.30878367
INFO:root:[   11] Training loss: 0.01514559, Validation loss: 0.01376165, Gradient norm: 0.36814891
INFO:root:[   12] Training loss: 0.01416342, Validation loss: 0.01191626, Gradient norm: 0.29786179
INFO:root:[   13] Training loss: 0.01363793, Validation loss: 0.00962003, Gradient norm: 0.29374461
INFO:root:[   14] Training loss: 0.01449944, Validation loss: 0.01109572, Gradient norm: 0.31754090
INFO:root:[   15] Training loss: 0.01250478, Validation loss: 0.01327948, Gradient norm: 0.23276915
INFO:root:[   16] Training loss: 0.01285909, Validation loss: 0.00930462, Gradient norm: 0.30600379
INFO:root:[   17] Training loss: 0.01269955, Validation loss: 0.01460188, Gradient norm: 0.30065392
INFO:root:[   18] Training loss: 0.01285162, Validation loss: 0.00997457, Gradient norm: 0.32925515
INFO:root:[   19] Training loss: 0.01230543, Validation loss: 0.01371231, Gradient norm: 0.30947160
INFO:root:[   20] Training loss: 0.01274274, Validation loss: 0.01308516, Gradient norm: 0.31534741
INFO:root:[   21] Training loss: 0.01272685, Validation loss: 0.01077022, Gradient norm: 0.32129968
INFO:root:[   22] Training loss: 0.01197930, Validation loss: 0.01328234, Gradient norm: 0.28169323
INFO:root:[   23] Training loss: 0.01169760, Validation loss: 0.01089339, Gradient norm: 0.29700821
INFO:root:[   24] Training loss: 0.01051289, Validation loss: 0.00850590, Gradient norm: 0.24478756
INFO:root:[   25] Training loss: 0.01282655, Validation loss: 0.00889863, Gradient norm: 0.26870295
INFO:root:[   26] Training loss: 0.01107858, Validation loss: 0.01141473, Gradient norm: 0.25411493
INFO:root:[   27] Training loss: 0.01109041, Validation loss: 0.00918411, Gradient norm: 0.25107217
INFO:root:[   28] Training loss: 0.01117333, Validation loss: 0.01363277, Gradient norm: 0.30128033
INFO:root:[   29] Training loss: 0.01073634, Validation loss: 0.00967301, Gradient norm: 0.28357509
INFO:root:[   30] Training loss: 0.01096951, Validation loss: 0.00929443, Gradient norm: 0.29311177
INFO:root:[   31] Training loss: 0.01073853, Validation loss: 0.00920311, Gradient norm: 0.25835111
INFO:root:[   32] Training loss: 0.01048190, Validation loss: 0.00919579, Gradient norm: 0.23927918
INFO:root:[   33] Training loss: 0.01098165, Validation loss: 0.01113334, Gradient norm: 0.26655517
INFO:root:[   34] Training loss: 0.01046496, Validation loss: 0.01144830, Gradient norm: 0.28625772
INFO:root:[   35] Training loss: 0.01101220, Validation loss: 0.01009940, Gradient norm: 0.30669947
INFO:root:[   36] Training loss: 0.01032085, Validation loss: 0.00942426, Gradient norm: 0.22115010
INFO:root:[   37] Training loss: 0.00958184, Validation loss: 0.01240120, Gradient norm: 0.22799085
INFO:root:[   38] Training loss: 0.00983437, Validation loss: 0.01019707, Gradient norm: 0.30467445
INFO:root:[   39] Training loss: 0.01073144, Validation loss: 0.00993343, Gradient norm: 0.28762360
INFO:root:[   40] Training loss: 0.01048231, Validation loss: 0.01507674, Gradient norm: 0.27901123
INFO:root:[   41] Training loss: 0.00977464, Validation loss: 0.01120200, Gradient norm: 0.27836554
INFO:root:[   42] Training loss: 0.00917346, Validation loss: 0.01072389, Gradient norm: 0.25106671
INFO:root:[   43] Training loss: 0.01008738, Validation loss: 0.00862984, Gradient norm: 0.30822118
INFO:root:[   44] Training loss: 0.00958760, Validation loss: 0.00886920, Gradient norm: 0.27658995
INFO:root:[   45] Training loss: 0.00973461, Validation loss: 0.00822723, Gradient norm: 0.27119349
INFO:root:[   46] Training loss: 0.00952196, Validation loss: 0.01006764, Gradient norm: 0.24330094
INFO:root:[   47] Training loss: 0.00870406, Validation loss: 0.00974594, Gradient norm: 0.25112227
INFO:root:[   48] Training loss: 0.00961506, Validation loss: 0.01063383, Gradient norm: 0.29011037
INFO:root:[   49] Training loss: 0.00908676, Validation loss: 0.01035800, Gradient norm: 0.29218841
INFO:root:[   50] Training loss: 0.00911737, Validation loss: 0.01031286, Gradient norm: 0.27550597
INFO:root:[   51] Training loss: 0.00945109, Validation loss: 0.00998392, Gradient norm: 0.28299977
INFO:root:[   52] Training loss: 0.00876238, Validation loss: 0.00900760, Gradient norm: 0.25980514
INFO:root:[   53] Training loss: 0.00878238, Validation loss: 0.01243477, Gradient norm: 0.26026858
INFO:root:[   54] Training loss: 0.00950948, Validation loss: 0.00994617, Gradient norm: 0.27601697
INFO:root:[   55] Training loss: 0.00908056, Validation loss: 0.01196664, Gradient norm: 0.23797161
INFO:root:[   56] Training loss: 0.00836592, Validation loss: 0.01093481, Gradient norm: 0.23921523
INFO:root:[   57] Training loss: 0.00834263, Validation loss: 0.00982069, Gradient norm: 0.29037890
INFO:root:[   58] Training loss: 0.00922800, Validation loss: 0.01155188, Gradient norm: 0.29899336
INFO:root:[   59] Training loss: 0.01005993, Validation loss: 0.01367150, Gradient norm: 0.29596883
INFO:root:[   60] Training loss: 0.00880742, Validation loss: 0.00930348, Gradient norm: 0.25338292
INFO:root:[   61] Training loss: 0.00920010, Validation loss: 0.00872214, Gradient norm: 0.28912897
INFO:root:[   62] Training loss: 0.00873741, Validation loss: 0.01104804, Gradient norm: 0.26382139
INFO:root:[   63] Training loss: 0.00830341, Validation loss: 0.00862034, Gradient norm: 0.28290128
INFO:root:[   64] Training loss: 0.00825925, Validation loss: 0.00900618, Gradient norm: 0.19502172
INFO:root:[   65] Training loss: 0.00887026, Validation loss: 0.01134957, Gradient norm: 0.26198242
INFO:root:[   66] Training loss: 0.00837899, Validation loss: 0.00989677, Gradient norm: 0.26490762
INFO:root:[   67] Training loss: 0.00849901, Validation loss: 0.00903133, Gradient norm: 0.31973979
INFO:root:[   68] Training loss: 0.00863363, Validation loss: 0.01356600, Gradient norm: 0.28748557
INFO:root:[   69] Training loss: 0.00885060, Validation loss: 0.01448496, Gradient norm: 0.31807845
INFO:root:[   70] Training loss: 0.00917308, Validation loss: 0.01022103, Gradient norm: 0.23911545
INFO:root:[   71] Training loss: 0.00866734, Validation loss: 0.00951164, Gradient norm: 0.30624387
INFO:root:[   72] Training loss: 0.00859042, Validation loss: 0.01204092, Gradient norm: 0.28296518
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 1599.763s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0074
INFO:root:EnergyScoreTrain: 0.00567
INFO:root:CoverageTrain: 0.9458
INFO:root:IntervalWidthTrain: 0.03637
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01109
INFO:root:EnergyScoreValidation: 0.00838
INFO:root:CoverageValidation: 0.8861
INFO:root:IntervalWidthValidation: 0.03627
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01109
INFO:root:EnergyScoreTest: 0.00836
INFO:root:CoverageTest: 0.88101
INFO:root:IntervalWidthTest: 0.0363
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05828825, Validation loss: 0.02307096, Gradient norm: 0.47114183
INFO:root:[    2] Training loss: 0.03029227, Validation loss: 0.02329148, Gradient norm: 0.43984618
INFO:root:[    3] Training loss: 0.02336565, Validation loss: 0.01727300, Gradient norm: 0.36325164
INFO:root:[    4] Training loss: 0.02265694, Validation loss: 0.01851553, Gradient norm: 0.39827724
INFO:root:[    5] Training loss: 0.02097416, Validation loss: 0.01692381, Gradient norm: 0.36604212
INFO:root:[    6] Training loss: 0.01861476, Validation loss: 0.01676834, Gradient norm: 0.34158626
INFO:root:[    7] Training loss: 0.01839705, Validation loss: 0.01339843, Gradient norm: 0.34035937
INFO:root:[    8] Training loss: 0.01653610, Validation loss: 0.01375747, Gradient norm: 0.29229011
INFO:root:[    9] Training loss: 0.01790343, Validation loss: 0.01496783, Gradient norm: 0.30860668
INFO:root:[   10] Training loss: 0.01504193, Validation loss: 0.01140155, Gradient norm: 0.27234645
INFO:root:[   11] Training loss: 0.01567206, Validation loss: 0.01133378, Gradient norm: 0.30195400
INFO:root:[   12] Training loss: 0.01457072, Validation loss: 0.01269343, Gradient norm: 0.26902253
INFO:root:[   13] Training loss: 0.01528824, Validation loss: 0.01274250, Gradient norm: 0.33882719
INFO:root:[   14] Training loss: 0.01455114, Validation loss: 0.01043754, Gradient norm: 0.26105772
INFO:root:[   15] Training loss: 0.01402877, Validation loss: 0.01025938, Gradient norm: 0.26993476
INFO:root:[   16] Training loss: 0.01309357, Validation loss: 0.01178585, Gradient norm: 0.23744966
INFO:root:[   17] Training loss: 0.01491162, Validation loss: 0.01071845, Gradient norm: 0.34739093
INFO:root:[   18] Training loss: 0.01401334, Validation loss: 0.01459868, Gradient norm: 0.29702617
INFO:root:[   19] Training loss: 0.01338145, Validation loss: 0.01573520, Gradient norm: 0.25713969
INFO:root:[   20] Training loss: 0.01361243, Validation loss: 0.00958940, Gradient norm: 0.29538893
INFO:root:[   21] Training loss: 0.01327992, Validation loss: 0.01377707, Gradient norm: 0.31080425
INFO:root:[   22] Training loss: 0.01370378, Validation loss: 0.01123814, Gradient norm: 0.32417634
INFO:root:[   23] Training loss: 0.01217610, Validation loss: 0.01053198, Gradient norm: 0.28386521
INFO:root:[   24] Training loss: 0.01295098, Validation loss: 0.01154685, Gradient norm: 0.29682332
INFO:root:[   25] Training loss: 0.01122479, Validation loss: 0.00939087, Gradient norm: 0.22685225
INFO:root:[   26] Training loss: 0.01295032, Validation loss: 0.01638918, Gradient norm: 0.29976828
INFO:root:[   27] Training loss: 0.01308639, Validation loss: 0.01628972, Gradient norm: 0.33373983
INFO:root:[   28] Training loss: 0.01250878, Validation loss: 0.00932897, Gradient norm: 0.30105142
INFO:root:[   29] Training loss: 0.01196514, Validation loss: 0.00941279, Gradient norm: 0.28314085
INFO:root:[   30] Training loss: 0.01160648, Validation loss: 0.00967993, Gradient norm: 0.29110766
INFO:root:[   31] Training loss: 0.01229744, Validation loss: 0.01249908, Gradient norm: 0.27941766
INFO:root:[   32] Training loss: 0.01224429, Validation loss: 0.01110582, Gradient norm: 0.28431817
INFO:root:[   33] Training loss: 0.01194593, Validation loss: 0.01077987, Gradient norm: 0.30965858
INFO:root:[   34] Training loss: 0.01121371, Validation loss: 0.01071208, Gradient norm: 0.28401248
INFO:root:[   35] Training loss: 0.01147326, Validation loss: 0.01212360, Gradient norm: 0.24954094
INFO:root:[   36] Training loss: 0.01141879, Validation loss: 0.01233131, Gradient norm: 0.27993680
INFO:root:[   37] Training loss: 0.01254754, Validation loss: 0.00993361, Gradient norm: 0.26952522
INFO:root:[   38] Training loss: 0.01119855, Validation loss: 0.00985231, Gradient norm: 0.28335628
INFO:root:[   39] Training loss: 0.01122717, Validation loss: 0.01069056, Gradient norm: 0.32521222
INFO:root:[   40] Training loss: 0.01143561, Validation loss: 0.01089928, Gradient norm: 0.30110220
INFO:root:[   41] Training loss: 0.01095907, Validation loss: 0.00991265, Gradient norm: 0.24522904
INFO:root:[   42] Training loss: 0.01066480, Validation loss: 0.01348477, Gradient norm: 0.24984472
INFO:root:[   43] Training loss: 0.01130059, Validation loss: 0.01163073, Gradient norm: 0.31870101
INFO:root:[   44] Training loss: 0.01102019, Validation loss: 0.01098576, Gradient norm: 0.30007916
INFO:root:[   45] Training loss: 0.01027687, Validation loss: 0.01013643, Gradient norm: 0.28595494
INFO:root:[   46] Training loss: 0.01101187, Validation loss: 0.01461353, Gradient norm: 0.28236118
INFO:root:[   47] Training loss: 0.01006766, Validation loss: 0.01200687, Gradient norm: 0.25238085
INFO:root:[   48] Training loss: 0.01068838, Validation loss: 0.01011744, Gradient norm: 0.27682851
INFO:root:[   49] Training loss: 0.01049500, Validation loss: 0.00948964, Gradient norm: 0.21868348
INFO:root:[   50] Training loss: 0.01053985, Validation loss: 0.01043170, Gradient norm: 0.29169654
INFO:root:[   51] Training loss: 0.01022224, Validation loss: 0.00966646, Gradient norm: 0.29746607
INFO:root:[   52] Training loss: 0.01036145, Validation loss: 0.01164210, Gradient norm: 0.29194663
INFO:root:[   53] Training loss: 0.01017151, Validation loss: 0.01084936, Gradient norm: 0.25786642
INFO:root:[   54] Training loss: 0.01013272, Validation loss: 0.01096747, Gradient norm: 0.22139043
INFO:root:[   55] Training loss: 0.01040685, Validation loss: 0.00973721, Gradient norm: 0.27553252
INFO:root:[   56] Training loss: 0.01042924, Validation loss: 0.01016308, Gradient norm: 0.27402158
INFO:root:[   57] Training loss: 0.01022736, Validation loss: 0.01199517, Gradient norm: 0.28492720
INFO:root:[   58] Training loss: 0.01034717, Validation loss: 0.01092090, Gradient norm: 0.30196752
INFO:root:[   59] Training loss: 0.00968420, Validation loss: 0.00966900, Gradient norm: 0.27082713
INFO:root:[   60] Training loss: 0.00985524, Validation loss: 0.00933967, Gradient norm: 0.27411479
INFO:root:[   61] Training loss: 0.01003853, Validation loss: 0.01103963, Gradient norm: 0.27747972
INFO:root:[   62] Training loss: 0.00990186, Validation loss: 0.01181839, Gradient norm: 0.29116366
INFO:root:[   63] Training loss: 0.00958317, Validation loss: 0.00978743, Gradient norm: 0.22810987
INFO:root:[   64] Training loss: 0.00967917, Validation loss: 0.01212492, Gradient norm: 0.28245858
INFO:root:[   65] Training loss: 0.00999449, Validation loss: 0.01229386, Gradient norm: 0.28211208
INFO:root:[   66] Training loss: 0.00898238, Validation loss: 0.00988406, Gradient norm: 0.27100307
INFO:root:[   67] Training loss: 0.00996070, Validation loss: 0.00990979, Gradient norm: 0.28783245
INFO:root:[   68] Training loss: 0.00986691, Validation loss: 0.01228589, Gradient norm: 0.27416229
INFO:root:[   69] Training loss: 0.00970916, Validation loss: 0.01038885, Gradient norm: 0.25842032
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 1511.903s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01077
INFO:root:EnergyScoreTrain: 0.00803
INFO:root:CoverageTrain: 0.9036
INFO:root:IntervalWidthTrain: 0.04442
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01272
INFO:root:EnergyScoreValidation: 0.00937
INFO:root:CoverageValidation: 0.86177
INFO:root:IntervalWidthValidation: 0.04441
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01321
INFO:root:EnergyScoreTest: 0.00974
INFO:root:CoverageTest: 0.85021
INFO:root:IntervalWidthTest: 0.04434
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06211521, Validation loss: 0.02498138, Gradient norm: 0.42288748
INFO:root:[    2] Training loss: 0.03071413, Validation loss: 0.01678423, Gradient norm: 0.38628664
INFO:root:[    3] Training loss: 0.02590025, Validation loss: 0.02610116, Gradient norm: 0.37407485
INFO:root:[    4] Training loss: 0.02258092, Validation loss: 0.01749826, Gradient norm: 0.33379315
INFO:root:[    5] Training loss: 0.02056519, Validation loss: 0.01746521, Gradient norm: 0.33497851
INFO:root:[    6] Training loss: 0.01992477, Validation loss: 0.01188592, Gradient norm: 0.35423674
INFO:root:[    7] Training loss: 0.01868028, Validation loss: 0.01642836, Gradient norm: 0.33142825
INFO:root:[    8] Training loss: 0.01790025, Validation loss: 0.01168958, Gradient norm: 0.30983420
INFO:root:[    9] Training loss: 0.01709701, Validation loss: 0.01122582, Gradient norm: 0.29125428
INFO:root:[   10] Training loss: 0.01636238, Validation loss: 0.01307769, Gradient norm: 0.31119181
INFO:root:[   11] Training loss: 0.01550102, Validation loss: 0.01274114, Gradient norm: 0.28094926
INFO:root:[   12] Training loss: 0.01573149, Validation loss: 0.01194333, Gradient norm: 0.32116607
INFO:root:[   13] Training loss: 0.01468893, Validation loss: 0.01266335, Gradient norm: 0.25634611
INFO:root:[   14] Training loss: 0.01465758, Validation loss: 0.01478503, Gradient norm: 0.28888892
INFO:root:[   15] Training loss: 0.01526351, Validation loss: 0.01334348, Gradient norm: 0.32765872
INFO:root:[   16] Training loss: 0.01441930, Validation loss: 0.01111078, Gradient norm: 0.29334079
INFO:root:[   17] Training loss: 0.01448816, Validation loss: 0.01014666, Gradient norm: 0.27802365
INFO:root:[   18] Training loss: 0.01409564, Validation loss: 0.01196721, Gradient norm: 0.27338424
INFO:root:[   19] Training loss: 0.01340803, Validation loss: 0.01585437, Gradient norm: 0.26015069
INFO:root:[   20] Training loss: 0.01424761, Validation loss: 0.01549330, Gradient norm: 0.28393151
INFO:root:[   21] Training loss: 0.01316754, Validation loss: 0.01040190, Gradient norm: 0.27115308
INFO:root:[   22] Training loss: 0.01392455, Validation loss: 0.01442999, Gradient norm: 0.33315585
INFO:root:[   23] Training loss: 0.01260820, Validation loss: 0.01382860, Gradient norm: 0.26254709
INFO:root:[   24] Training loss: 0.01253223, Validation loss: 0.01743725, Gradient norm: 0.23968258
INFO:root:[   25] Training loss: 0.01330962, Validation loss: 0.01597038, Gradient norm: 0.31429773
INFO:root:[   26] Training loss: 0.01340003, Validation loss: 0.01914991, Gradient norm: 0.30812738
INFO:root:[   27] Training loss: 0.01327185, Validation loss: 0.01226906, Gradient norm: 0.31197675
INFO:root:[   28] Training loss: 0.01265309, Validation loss: 0.01619746, Gradient norm: 0.28950589
INFO:root:[   29] Training loss: 0.01169471, Validation loss: 0.00930083, Gradient norm: 0.25467000
INFO:root:[   30] Training loss: 0.01262606, Validation loss: 0.00950984, Gradient norm: 0.26911240
INFO:root:[   31] Training loss: 0.01255968, Validation loss: 0.00985169, Gradient norm: 0.25140974
INFO:root:[   32] Training loss: 0.01176993, Validation loss: 0.01353509, Gradient norm: 0.23023160
INFO:root:[   33] Training loss: 0.01237165, Validation loss: 0.01094800, Gradient norm: 0.29216469
INFO:root:[   34] Training loss: 0.01221070, Validation loss: 0.01030211, Gradient norm: 0.30390031
INFO:root:[   35] Training loss: 0.01259566, Validation loss: 0.01015560, Gradient norm: 0.27184740
INFO:root:[   36] Training loss: 0.01194515, Validation loss: 0.01163175, Gradient norm: 0.28633913
INFO:root:[   37] Training loss: 0.01190325, Validation loss: 0.01274756, Gradient norm: 0.29779626
INFO:root:[   38] Training loss: 0.01140865, Validation loss: 0.01056277, Gradient norm: 0.25989031
INFO:root:[   39] Training loss: 0.01123543, Validation loss: 0.01034141, Gradient norm: 0.26860656
INFO:root:[   40] Training loss: 0.01171325, Validation loss: 0.00996215, Gradient norm: 0.28294812
INFO:root:[   41] Training loss: 0.01177908, Validation loss: 0.01200191, Gradient norm: 0.28582976
INFO:root:[   42] Training loss: 0.01149829, Validation loss: 0.01254185, Gradient norm: 0.21852143
INFO:root:[   43] Training loss: 0.01139279, Validation loss: 0.01198199, Gradient norm: 0.25673331
INFO:root:[   44] Training loss: 0.01113018, Validation loss: 0.01152949, Gradient norm: 0.27254190
INFO:root:[   45] Training loss: 0.01169800, Validation loss: 0.00956622, Gradient norm: 0.30940455
INFO:root:[   46] Training loss: 0.01139624, Validation loss: 0.00961328, Gradient norm: 0.25908288
INFO:root:[   47] Training loss: 0.01114184, Validation loss: 0.01050513, Gradient norm: 0.29445804
INFO:root:[   48] Training loss: 0.01146271, Validation loss: 0.01254621, Gradient norm: 0.28511743
INFO:root:[   49] Training loss: 0.01151691, Validation loss: 0.01076191, Gradient norm: 0.26405643
INFO:root:[   50] Training loss: 0.01073128, Validation loss: 0.01394879, Gradient norm: 0.27048076
INFO:root:[   51] Training loss: 0.01072902, Validation loss: 0.01171104, Gradient norm: 0.23971170
INFO:root:[   52] Training loss: 0.01092330, Validation loss: 0.01388782, Gradient norm: 0.24496020
INFO:root:[   53] Training loss: 0.00996144, Validation loss: 0.00948674, Gradient norm: 0.25412603
INFO:root:[   54] Training loss: 0.01082190, Validation loss: 0.01233056, Gradient norm: 0.26333745
INFO:root:[   55] Training loss: 0.01026661, Validation loss: 0.01369276, Gradient norm: 0.28049991
INFO:root:[   56] Training loss: 0.01086491, Validation loss: 0.01013332, Gradient norm: 0.25335354
INFO:root:[   57] Training loss: 0.01083240, Validation loss: 0.01257071, Gradient norm: 0.27627445
INFO:root:[   58] Training loss: 0.01041270, Validation loss: 0.00882947, Gradient norm: 0.29117978
INFO:root:[   59] Training loss: 0.01089694, Validation loss: 0.01425687, Gradient norm: 0.28531061
INFO:root:[   60] Training loss: 0.00940210, Validation loss: 0.01035364, Gradient norm: 0.20200878
INFO:root:[   61] Training loss: 0.01047821, Validation loss: 0.01042496, Gradient norm: 0.26207944
INFO:root:[   62] Training loss: 0.01073220, Validation loss: 0.00989627, Gradient norm: 0.28531095
INFO:root:[   63] Training loss: 0.01006519, Validation loss: 0.01468157, Gradient norm: 0.25070703
INFO:root:[   64] Training loss: 0.01047154, Validation loss: 0.01037458, Gradient norm: 0.27032625
INFO:root:[   65] Training loss: 0.00993852, Validation loss: 0.00900001, Gradient norm: 0.27791287
INFO:root:[   66] Training loss: 0.00945978, Validation loss: 0.00866984, Gradient norm: 0.23196385
INFO:root:[   67] Training loss: 0.00983793, Validation loss: 0.01020522, Gradient norm: 0.25452620
INFO:root:[   68] Training loss: 0.00935796, Validation loss: 0.00904731, Gradient norm: 0.25403017
INFO:root:[   69] Training loss: 0.01007145, Validation loss: 0.00986937, Gradient norm: 0.21844099
INFO:root:[   70] Training loss: 0.01016854, Validation loss: 0.01188549, Gradient norm: 0.27575682
INFO:root:[   71] Training loss: 0.01010607, Validation loss: 0.01172768, Gradient norm: 0.26897114
INFO:root:[   72] Training loss: 0.00951305, Validation loss: 0.00911952, Gradient norm: 0.26164039
INFO:root:[   73] Training loss: 0.00994329, Validation loss: 0.01063392, Gradient norm: 0.25988375
INFO:root:[   74] Training loss: 0.00972915, Validation loss: 0.00959898, Gradient norm: 0.21545052
INFO:root:[   75] Training loss: 0.00905543, Validation loss: 0.01164440, Gradient norm: 0.20930004
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 1667.844s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0078
INFO:root:EnergyScoreTrain: 0.00575
INFO:root:CoverageTrain: 0.89544
INFO:root:IntervalWidthTrain: 0.02743
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01142
INFO:root:EnergyScoreValidation: 0.00864
INFO:root:CoverageValidation: 0.79956
INFO:root:IntervalWidthValidation: 0.02739
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01151
INFO:root:EnergyScoreTest: 0.00872
INFO:root:CoverageTest: 0.79279
INFO:root:IntervalWidthTest: 0.02743
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 719010
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06368022, Validation loss: 0.04709499, Gradient norm: 0.41069563
INFO:root:[    2] Training loss: 0.03476340, Validation loss: 0.03163980, Gradient norm: 0.37559966
INFO:root:[    3] Training loss: 0.03001247, Validation loss: 0.02249985, Gradient norm: 0.40901520
INFO:root:[    4] Training loss: 0.02861739, Validation loss: 0.01813006, Gradient norm: 0.44049916
INFO:root:[    5] Training loss: 0.02462935, Validation loss: 0.01621576, Gradient norm: 0.40110120
INFO:root:[    6] Training loss: 0.02168973, Validation loss: 0.01841022, Gradient norm: 0.35130508
INFO:root:[    7] Training loss: 0.02217738, Validation loss: 0.01772897, Gradient norm: 0.38239947
INFO:root:[    8] Training loss: 0.02033026, Validation loss: 0.01310706, Gradient norm: 0.33582394
INFO:root:[    9] Training loss: 0.01996621, Validation loss: 0.02162243, Gradient norm: 0.35749530
INFO:root:[   10] Training loss: 0.01798889, Validation loss: 0.01583967, Gradient norm: 0.29367147
INFO:root:[   11] Training loss: 0.01890656, Validation loss: 0.01206839, Gradient norm: 0.32873549
INFO:root:[   12] Training loss: 0.01776054, Validation loss: 0.01875000, Gradient norm: 0.30425640
INFO:root:[   13] Training loss: 0.01821566, Validation loss: 0.01775967, Gradient norm: 0.32277681
INFO:root:[   14] Training loss: 0.01650708, Validation loss: 0.02161370, Gradient norm: 0.29316624
INFO:root:[   15] Training loss: 0.01744804, Validation loss: 0.01950913, Gradient norm: 0.29969388
INFO:root:[   16] Training loss: 0.01631301, Validation loss: 0.01545418, Gradient norm: 0.29839510
INFO:root:[   17] Training loss: 0.01561301, Validation loss: 0.01667346, Gradient norm: 0.28460087
INFO:root:[   18] Training loss: 0.01511829, Validation loss: 0.01419013, Gradient norm: 0.28186413
INFO:root:[   19] Training loss: 0.01594585, Validation loss: 0.01785280, Gradient norm: 0.30315206
INFO:root:[   20] Training loss: 0.01445068, Validation loss: 0.01671192, Gradient norm: 0.27193450
INFO:root:[   21] Training loss: 0.01468176, Validation loss: 0.01195609, Gradient norm: 0.24748319
INFO:root:[   22] Training loss: 0.01498452, Validation loss: 0.01267579, Gradient norm: 0.29799495
INFO:root:[   23] Training loss: 0.01509464, Validation loss: 0.01751154, Gradient norm: 0.29139001
INFO:root:[   24] Training loss: 0.01405793, Validation loss: 0.01679331, Gradient norm: 0.27451034
INFO:root:[   25] Training loss: 0.01382848, Validation loss: 0.01566307, Gradient norm: 0.25809936
INFO:root:[   26] Training loss: 0.01507415, Validation loss: 0.01386155, Gradient norm: 0.27777849
INFO:root:[   27] Training loss: 0.01379366, Validation loss: 0.01426577, Gradient norm: 0.26969755
INFO:root:[   28] Training loss: 0.01336993, Validation loss: 0.01365559, Gradient norm: 0.25360006
INFO:root:[   29] Training loss: 0.01433930, Validation loss: 0.01872369, Gradient norm: 0.27927129
INFO:root:[   30] Training loss: 0.01376975, Validation loss: 0.01626786, Gradient norm: 0.23682676
INFO:root:[   31] Training loss: 0.01354427, Validation loss: 0.01265867, Gradient norm: 0.26810051
INFO:root:[   32] Training loss: 0.01344179, Validation loss: 0.01194704, Gradient norm: 0.27133887
INFO:root:[   33] Training loss: 0.01310327, Validation loss: 0.01362558, Gradient norm: 0.22339046
INFO:root:[   34] Training loss: 0.01362677, Validation loss: 0.01289088, Gradient norm: 0.29055809
INFO:root:[   35] Training loss: 0.01326614, Validation loss: 0.01429381, Gradient norm: 0.27757273
INFO:root:[   36] Training loss: 0.01312400, Validation loss: 0.01184067, Gradient norm: 0.24254898
INFO:root:[   37] Training loss: 0.01327018, Validation loss: 0.01535130, Gradient norm: 0.29057594
INFO:root:[   38] Training loss: 0.01313408, Validation loss: 0.01409705, Gradient norm: 0.29781306
INFO:root:[   39] Training loss: 0.01265749, Validation loss: 0.01594319, Gradient norm: 0.26359692
INFO:root:[   40] Training loss: 0.01291274, Validation loss: 0.01158826, Gradient norm: 0.21699605
INFO:root:[   41] Training loss: 0.01281781, Validation loss: 0.01246888, Gradient norm: 0.25290689
INFO:root:[   42] Training loss: 0.01205988, Validation loss: 0.01465326, Gradient norm: 0.26868501
INFO:root:[   43] Training loss: 0.01245380, Validation loss: 0.01390794, Gradient norm: 0.26115514
INFO:root:[   44] Training loss: 0.01288327, Validation loss: 0.01212423, Gradient norm: 0.29345709
INFO:root:[   45] Training loss: 0.01319889, Validation loss: 0.01250998, Gradient norm: 0.26053484
INFO:root:[   46] Training loss: 0.01161043, Validation loss: 0.01619721, Gradient norm: 0.24427650
INFO:root:[   47] Training loss: 0.01182866, Validation loss: 0.01265318, Gradient norm: 0.25575180
INFO:root:[   48] Training loss: 0.01183532, Validation loss: 0.01240783, Gradient norm: 0.25028970
INFO:root:[   49] Training loss: 0.01204608, Validation loss: 0.01935088, Gradient norm: 0.26126496
INFO:root:[   50] Training loss: 0.01211133, Validation loss: 0.01561013, Gradient norm: 0.25963345
INFO:root:[   51] Training loss: 0.01208989, Validation loss: 0.01355303, Gradient norm: 0.23365513
INFO:root:[   52] Training loss: 0.01174193, Validation loss: 0.01427091, Gradient norm: 0.24597035
INFO:root:[   53] Training loss: 0.01112521, Validation loss: 0.01364572, Gradient norm: 0.20601602
INFO:root:[   54] Training loss: 0.01216455, Validation loss: 0.01320801, Gradient norm: 0.24480094
INFO:root:[   55] Training loss: 0.01202026, Validation loss: 0.01193947, Gradient norm: 0.27646043
INFO:root:[   56] Training loss: 0.01199027, Validation loss: 0.01363280, Gradient norm: 0.26530621
INFO:root:[   57] Training loss: 0.01202214, Validation loss: 0.01302320, Gradient norm: 0.26855467
INFO:root:[   58] Training loss: 0.01117040, Validation loss: 0.01625753, Gradient norm: 0.20514909
INFO:root:[   59] Training loss: 0.01104569, Validation loss: 0.01277588, Gradient norm: 0.21865186
INFO:root:[   60] Training loss: 0.01161357, Validation loss: 0.01262898, Gradient norm: 0.26463452
INFO:root:[   61] Training loss: 0.01149206, Validation loss: 0.01402151, Gradient norm: 0.25117800
INFO:root:[   62] Training loss: 0.01132855, Validation loss: 0.01344170, Gradient norm: 0.25437666
INFO:root:[   63] Training loss: 0.01166115, Validation loss: 0.01253925, Gradient norm: 0.29768846
INFO:root:[   64] Training loss: 0.01162722, Validation loss: 0.01340418, Gradient norm: 0.27936711
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 1416.797s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01304
INFO:root:EnergyScoreTrain: 0.00997
INFO:root:CoverageTrain: 0.7696
INFO:root:IntervalWidthTrain: 0.02911
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01495
INFO:root:EnergyScoreValidation: 0.01156
INFO:root:CoverageValidation: 0.7084
INFO:root:IntervalWidthValidation: 0.02912
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01514
INFO:root:EnergyScoreTest: 0.01176
INFO:root:CoverageTest: 0.70881
INFO:root:IntervalWidthTest: 0.02912
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 69206016
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05662198, Validation loss: 0.03515734, Gradient norm: 0.74071437
INFO:root:[    2] Training loss: 0.03218773, Validation loss: 0.03314456, Gradient norm: 0.68494182
INFO:root:[    3] Training loss: 0.02670319, Validation loss: 0.02082659, Gradient norm: 0.69135864
INFO:root:[    4] Training loss: 0.02299176, Validation loss: 0.01918634, Gradient norm: 0.61963751
INFO:root:[    5] Training loss: 0.02123393, Validation loss: 0.01893387, Gradient norm: 0.57894480
INFO:root:[    6] Training loss: 0.02042179, Validation loss: 0.01852517, Gradient norm: 0.58953451
INFO:root:[    7] Training loss: 0.01869708, Validation loss: 0.01710533, Gradient norm: 0.56602101
INFO:root:[    8] Training loss: 0.01708818, Validation loss: 0.01610035, Gradient norm: 0.49629063
INFO:root:[    9] Training loss: 0.01639986, Validation loss: 0.01341237, Gradient norm: 0.41835337
INFO:root:[   10] Training loss: 0.01619205, Validation loss: 0.01778764, Gradient norm: 0.45866128
INFO:root:[   11] Training loss: 0.01511593, Validation loss: 0.01605834, Gradient norm: 0.42086681
INFO:root:[   12] Training loss: 0.01591999, Validation loss: 0.01581142, Gradient norm: 0.49302098
INFO:root:[   13] Training loss: 0.01670810, Validation loss: 0.01644614, Gradient norm: 0.53247194
INFO:root:[   14] Training loss: 0.01504881, Validation loss: 0.01556843, Gradient norm: 0.46043792
INFO:root:[   15] Training loss: 0.01386125, Validation loss: 0.01368248, Gradient norm: 0.40258473
INFO:root:[   16] Training loss: 0.01509446, Validation loss: 0.01396642, Gradient norm: 0.47429570
INFO:root:[   17] Training loss: 0.01308677, Validation loss: 0.01294236, Gradient norm: 0.35469877
INFO:root:[   18] Training loss: 0.01396873, Validation loss: 0.01338712, Gradient norm: 0.44710584
INFO:root:[   19] Training loss: 0.01329951, Validation loss: 0.01423361, Gradient norm: 0.39976063
INFO:root:[   20] Training loss: 0.01392741, Validation loss: 0.01300796, Gradient norm: 0.44122915
INFO:root:[   21] Training loss: 0.01315309, Validation loss: 0.01611247, Gradient norm: 0.42957843
INFO:root:[   22] Training loss: 0.01197939, Validation loss: 0.01170606, Gradient norm: 0.30671198
INFO:root:[   23] Training loss: 0.01293007, Validation loss: 0.01474058, Gradient norm: 0.41955426
INFO:root:[   24] Training loss: 0.01204115, Validation loss: 0.01444520, Gradient norm: 0.32227246
INFO:root:[   25] Training loss: 0.01291044, Validation loss: 0.01292263, Gradient norm: 0.45423941
INFO:root:[   26] Training loss: 0.01175626, Validation loss: 0.01267965, Gradient norm: 0.34922573
INFO:root:[   27] Training loss: 0.01191544, Validation loss: 0.01584127, Gradient norm: 0.35613253
INFO:root:[   28] Training loss: 0.01238679, Validation loss: 0.01246224, Gradient norm: 0.42194219
INFO:root:[   29] Training loss: 0.01212103, Validation loss: 0.01323316, Gradient norm: 0.42496277
INFO:root:[   30] Training loss: 0.01143956, Validation loss: 0.01307364, Gradient norm: 0.32476408
INFO:root:[   31] Training loss: 0.01177944, Validation loss: 0.01317229, Gradient norm: 0.42636719
INFO:root:[   32] Training loss: 0.01145673, Validation loss: 0.01606672, Gradient norm: 0.39603993
INFO:root:[   33] Training loss: 0.01111423, Validation loss: 0.01141622, Gradient norm: 0.38359467
INFO:root:[   34] Training loss: 0.01102086, Validation loss: 0.01222184, Gradient norm: 0.38137552
INFO:root:[   35] Training loss: 0.01168904, Validation loss: 0.01186446, Gradient norm: 0.35255660
INFO:root:[   36] Training loss: 0.01082269, Validation loss: 0.01523187, Gradient norm: 0.34908930
INFO:root:[   37] Training loss: 0.01061972, Validation loss: 0.01243129, Gradient norm: 0.40566485
INFO:root:[   38] Training loss: 0.01078118, Validation loss: 0.01148337, Gradient norm: 0.40738842
INFO:root:[   39] Training loss: 0.01031025, Validation loss: 0.01279475, Gradient norm: 0.37416759
INFO:root:[   40] Training loss: 0.01061480, Validation loss: 0.01233260, Gradient norm: 0.40743520
INFO:root:[   41] Training loss: 0.01018272, Validation loss: 0.01308420, Gradient norm: 0.34922749
INFO:root:[   42] Training loss: 0.01001158, Validation loss: 0.01397872, Gradient norm: 0.35230718
INFO:root:[   43] Training loss: 0.01089683, Validation loss: 0.01349710, Gradient norm: 0.43111602
INFO:root:[   44] Training loss: 0.01024221, Validation loss: 0.01152029, Gradient norm: 0.34984943
INFO:root:[   45] Training loss: 0.01066910, Validation loss: 0.01326417, Gradient norm: 0.37396533
INFO:root:[   46] Training loss: 0.01005659, Validation loss: 0.01177455, Gradient norm: 0.43176591
INFO:root:[   47] Training loss: 0.01005876, Validation loss: 0.01200275, Gradient norm: 0.37564405
INFO:root:[   48] Training loss: 0.00961334, Validation loss: 0.01144831, Gradient norm: 0.42355320
INFO:root:[   49] Training loss: 0.01112691, Validation loss: 0.01288898, Gradient norm: 0.42786162
INFO:root:[   50] Training loss: 0.01002689, Validation loss: 0.01394141, Gradient norm: 0.36914018
INFO:root:[   51] Training loss: 0.00941949, Validation loss: 0.01298306, Gradient norm: 0.36451218
INFO:root:[   52] Training loss: 0.01013814, Validation loss: 0.01164202, Gradient norm: 0.42168652
INFO:root:[   53] Training loss: 0.00886051, Validation loss: 0.01134260, Gradient norm: 0.33243810
INFO:root:[   54] Training loss: 0.00962250, Validation loss: 0.01161934, Gradient norm: 0.40715729
INFO:root:[   55] Training loss: 0.00922017, Validation loss: 0.01354799, Gradient norm: 0.36323580
INFO:root:[   56] Training loss: 0.00952466, Validation loss: 0.01145942, Gradient norm: 0.35597893
INFO:root:[   57] Training loss: 0.00874463, Validation loss: 0.01275010, Gradient norm: 0.36951500
INFO:root:[   58] Training loss: 0.00951467, Validation loss: 0.01126027, Gradient norm: 0.38896975
INFO:root:[   59] Training loss: 0.00976358, Validation loss: 0.01249196, Gradient norm: 0.40045945
INFO:root:[   60] Training loss: 0.00889299, Validation loss: 0.01263690, Gradient norm: 0.38498725
INFO:root:[   61] Training loss: 0.00911884, Validation loss: 0.01266637, Gradient norm: 0.36973801
INFO:root:[   62] Training loss: 0.00872105, Validation loss: 0.01095864, Gradient norm: 0.37213649
INFO:root:[   63] Training loss: 0.00841799, Validation loss: 0.01275213, Gradient norm: 0.40829222
INFO:root:[   64] Training loss: 0.00965466, Validation loss: 0.01188752, Gradient norm: 0.41393862
INFO:root:[   65] Training loss: 0.00945449, Validation loss: 0.01341663, Gradient norm: 0.39898284
INFO:root:[   66] Training loss: 0.00895963, Validation loss: 0.01226258, Gradient norm: 0.35752773
INFO:root:[   67] Training loss: 0.00888068, Validation loss: 0.01304969, Gradient norm: 0.43751513
INFO:root:[   68] Training loss: 0.00905564, Validation loss: 0.01404363, Gradient norm: 0.36589063
INFO:root:[   69] Training loss: 0.00896446, Validation loss: 0.01316763, Gradient norm: 0.39165486
INFO:root:[   70] Training loss: 0.00880657, Validation loss: 0.01355304, Gradient norm: 0.37766096
INFO:root:[   71] Training loss: 0.00891452, Validation loss: 0.01244588, Gradient norm: 0.43161046
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 1399.27s.
INFO:root:Emptying the cuda cache took 0.033s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00856
INFO:root:EnergyScoreTrain: 0.00464
INFO:root:CoverageTrain: 0.64572
INFO:root:IntervalWidthTrain: 0.01817
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01306
INFO:root:EnergyScoreValidation: 0.00836
INFO:root:CoverageValidation: 0.53083
INFO:root:IntervalWidthValidation: 0.01768
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01248
INFO:root:EnergyScoreTest: 0.00777
INFO:root:CoverageTest: 0.5533
INFO:root:IntervalWidthTest: 0.01814
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1149239296
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05795179, Validation loss: 0.03315237, Gradient norm: 0.62247834
INFO:root:[    2] Training loss: 0.03562905, Validation loss: 0.02041023, Gradient norm: 0.64108230
INFO:root:[    3] Training loss: 0.02833623, Validation loss: 0.02910401, Gradient norm: 0.56014256
INFO:root:[    4] Training loss: 0.02662585, Validation loss: 0.02006059, Gradient norm: 0.57201740
INFO:root:[    5] Training loss: 0.02415972, Validation loss: 0.01629813, Gradient norm: 0.55305489
INFO:root:[    6] Training loss: 0.02335280, Validation loss: 0.01729093, Gradient norm: 0.54147056
INFO:root:[    7] Training loss: 0.02207846, Validation loss: 0.02034783, Gradient norm: 0.45242349
INFO:root:[    8] Training loss: 0.02086251, Validation loss: 0.01450676, Gradient norm: 0.52407421
INFO:root:[    9] Training loss: 0.01903375, Validation loss: 0.01298408, Gradient norm: 0.43486156
INFO:root:[   10] Training loss: 0.01885417, Validation loss: 0.01650055, Gradient norm: 0.47679325
INFO:root:[   11] Training loss: 0.01786427, Validation loss: 0.01587826, Gradient norm: 0.41251043
INFO:root:[   12] Training loss: 0.01797592, Validation loss: 0.01384061, Gradient norm: 0.39473818
INFO:root:[   13] Training loss: 0.01737491, Validation loss: 0.01876877, Gradient norm: 0.45759769
INFO:root:[   14] Training loss: 0.01616013, Validation loss: 0.01555969, Gradient norm: 0.40119424
INFO:root:[   15] Training loss: 0.01690820, Validation loss: 0.01627604, Gradient norm: 0.44545247
INFO:root:[   16] Training loss: 0.01719954, Validation loss: 0.01990521, Gradient norm: 0.47076071
INFO:root:[   17] Training loss: 0.01611346, Validation loss: 0.02074087, Gradient norm: 0.39339041
INFO:root:[   18] Training loss: 0.01571836, Validation loss: 0.01423727, Gradient norm: 0.41011047
INFO:root:[   19] Training loss: 0.01527890, Validation loss: 0.01327091, Gradient norm: 0.37384810
INFO:root:[   20] Training loss: 0.01527381, Validation loss: 0.01194045, Gradient norm: 0.39115230
INFO:root:[   21] Training loss: 0.01509526, Validation loss: 0.01190090, Gradient norm: 0.38818881
INFO:root:[   22] Training loss: 0.01462927, Validation loss: 0.01310340, Gradient norm: 0.36769938
INFO:root:[   23] Training loss: 0.01401451, Validation loss: 0.01266277, Gradient norm: 0.38351840
INFO:root:[   24] Training loss: 0.01472989, Validation loss: 0.01549470, Gradient norm: 0.35457781
INFO:root:[   25] Training loss: 0.01498670, Validation loss: 0.01677109, Gradient norm: 0.42800940
INFO:root:[   26] Training loss: 0.01561666, Validation loss: 0.01556707, Gradient norm: 0.40172223
INFO:root:[   27] Training loss: 0.01417405, Validation loss: 0.01582408, Gradient norm: 0.33769432
INFO:root:[   28] Training loss: 0.01412131, Validation loss: 0.01558688, Gradient norm: 0.35699875
INFO:root:[   29] Training loss: 0.01423492, Validation loss: 0.01606193, Gradient norm: 0.41936590
INFO:root:[   30] Training loss: 0.01411202, Validation loss: 0.01358597, Gradient norm: 0.44133443
INFO:root:[   31] Training loss: 0.01306587, Validation loss: 0.01219716, Gradient norm: 0.35041001
INFO:root:[   32] Training loss: 0.01306951, Validation loss: 0.01472168, Gradient norm: 0.37229620
INFO:root:[   33] Training loss: 0.01362965, Validation loss: 0.01549816, Gradient norm: 0.40154656
INFO:root:[   34] Training loss: 0.01269707, Validation loss: 0.01436155, Gradient norm: 0.39119646
INFO:root:[   35] Training loss: 0.01351391, Validation loss: 0.01171790, Gradient norm: 0.42826743
INFO:root:[   36] Training loss: 0.01302142, Validation loss: 0.01546376, Gradient norm: 0.32546317
INFO:root:[   37] Training loss: 0.01373659, Validation loss: 0.01377884, Gradient norm: 0.47245916
INFO:root:[   38] Training loss: 0.01317521, Validation loss: 0.01442583, Gradient norm: 0.36060915
INFO:root:[   39] Training loss: 0.01261262, Validation loss: 0.01294570, Gradient norm: 0.34096691
INFO:root:[   40] Training loss: 0.01281775, Validation loss: 0.01525989, Gradient norm: 0.43450223
INFO:root:[   41] Training loss: 0.01178295, Validation loss: 0.01241674, Gradient norm: 0.34146751
INFO:root:[   42] Training loss: 0.01158776, Validation loss: 0.01366888, Gradient norm: 0.39255462
INFO:root:[   43] Training loss: 0.01241116, Validation loss: 0.01153989, Gradient norm: 0.39121576
INFO:root:[   44] Training loss: 0.01266519, Validation loss: 0.01467136, Gradient norm: 0.39824576
INFO:root:[   45] Training loss: 0.01240773, Validation loss: 0.01199159, Gradient norm: 0.33621634
INFO:root:[   46] Training loss: 0.01210120, Validation loss: 0.01379987, Gradient norm: 0.37501488
INFO:root:[   47] Training loss: 0.01146080, Validation loss: 0.01336011, Gradient norm: 0.40035695
INFO:root:[   48] Training loss: 0.01134866, Validation loss: 0.01236353, Gradient norm: 0.40152994
INFO:root:[   49] Training loss: 0.01142890, Validation loss: 0.01166075, Gradient norm: 0.39581458
INFO:root:[   50] Training loss: 0.01172850, Validation loss: 0.01367158, Gradient norm: 0.43137831
INFO:root:[   51] Training loss: 0.01233580, Validation loss: 0.01171475, Gradient norm: 0.43828424
INFO:root:[   52] Training loss: 0.01128224, Validation loss: 0.01445967, Gradient norm: 0.38043244
INFO:root:[   53] Training loss: 0.01146336, Validation loss: 0.01143992, Gradient norm: 0.41875218
INFO:root:[   54] Training loss: 0.01170640, Validation loss: 0.01282980, Gradient norm: 0.36003717
INFO:root:[   55] Training loss: 0.01055440, Validation loss: 0.01211916, Gradient norm: 0.35736960
INFO:root:[   56] Training loss: 0.01067891, Validation loss: 0.01366129, Gradient norm: 0.39346592
INFO:root:[   57] Training loss: 0.01155020, Validation loss: 0.01205745, Gradient norm: 0.42840975
INFO:root:[   58] Training loss: 0.01084087, Validation loss: 0.01416031, Gradient norm: 0.39135895
INFO:root:[   59] Training loss: 0.01119987, Validation loss: 0.01567937, Gradient norm: 0.41090820
INFO:root:[   60] Training loss: 0.01135234, Validation loss: 0.01378225, Gradient norm: 0.40912697
INFO:root:[   61] Training loss: 0.01026411, Validation loss: 0.01289510, Gradient norm: 0.37901910
INFO:root:[   62] Training loss: 0.01107802, Validation loss: 0.01520111, Gradient norm: 0.43974009
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 1222.575s.
INFO:root:Emptying the cuda cache took 0.014s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01104
INFO:root:EnergyScoreTrain: 0.00622
INFO:root:CoverageTrain: 0.57392
INFO:root:IntervalWidthTrain: 0.02265
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01418
INFO:root:EnergyScoreValidation: 0.00869
INFO:root:CoverageValidation: 0.5207
INFO:root:IntervalWidthValidation: 0.02155
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01472
INFO:root:EnergyScoreTest: 0.00842
INFO:root:CoverageTest: 0.53872
INFO:root:IntervalWidthTest: 0.02565
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1147142144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06348390, Validation loss: 0.04063590, Gradient norm: 0.57429060
INFO:root:[    2] Training loss: 0.03815464, Validation loss: 0.03293647, Gradient norm: 0.52159863
INFO:root:[    3] Training loss: 0.03075224, Validation loss: 0.02422270, Gradient norm: 0.49177435
INFO:root:[    4] Training loss: 0.02792644, Validation loss: 0.02573767, Gradient norm: 0.52482297
INFO:root:[    5] Training loss: 0.02267320, Validation loss: 0.01745431, Gradient norm: 0.34371705
INFO:root:[    6] Training loss: 0.02604469, Validation loss: 0.02534537, Gradient norm: 0.53984296
INFO:root:[    7] Training loss: 0.02468169, Validation loss: 0.01593225, Gradient norm: 0.51232887
INFO:root:[    8] Training loss: 0.02190066, Validation loss: 0.02208039, Gradient norm: 0.38745216
INFO:root:[    9] Training loss: 0.02157013, Validation loss: 0.01499287, Gradient norm: 0.42977235
INFO:root:[   10] Training loss: 0.01987507, Validation loss: 0.01691852, Gradient norm: 0.40965237
INFO:root:[   11] Training loss: 0.02103774, Validation loss: 0.01602778, Gradient norm: 0.42208321
INFO:root:[   12] Training loss: 0.01940409, Validation loss: 0.01688888, Gradient norm: 0.41770891
INFO:root:[   13] Training loss: 0.02062926, Validation loss: 0.01593542, Gradient norm: 0.43905871
INFO:root:[   14] Training loss: 0.01784412, Validation loss: 0.01488383, Gradient norm: 0.38963509
INFO:root:[   15] Training loss: 0.01852681, Validation loss: 0.01376795, Gradient norm: 0.39185570
INFO:root:[   16] Training loss: 0.01822617, Validation loss: 0.01507156, Gradient norm: 0.41046698
INFO:root:[   17] Training loss: 0.01658229, Validation loss: 0.01803421, Gradient norm: 0.30683233
INFO:root:[   18] Training loss: 0.01711132, Validation loss: 0.01786564, Gradient norm: 0.39440522
INFO:root:[   19] Training loss: 0.01673094, Validation loss: 0.01728584, Gradient norm: 0.33721120
INFO:root:[   20] Training loss: 0.01683095, Validation loss: 0.01306774, Gradient norm: 0.27973375
INFO:root:[   21] Training loss: 0.01620129, Validation loss: 0.02008950, Gradient norm: 0.31292699
INFO:root:[   22] Training loss: 0.01619935, Validation loss: 0.01543092, Gradient norm: 0.40612226
INFO:root:[   23] Training loss: 0.01800014, Validation loss: 0.01302145, Gradient norm: 0.39870246
INFO:root:[   24] Training loss: 0.01598753, Validation loss: 0.01426882, Gradient norm: 0.37647328
INFO:root:[   25] Training loss: 0.01616422, Validation loss: 0.01669974, Gradient norm: 0.38843476
INFO:root:[   26] Training loss: 0.01644644, Validation loss: 0.02235862, Gradient norm: 0.38136178
INFO:root:[   27] Training loss: 0.01625733, Validation loss: 0.01453834, Gradient norm: 0.39619482
INFO:root:[   28] Training loss: 0.01562850, Validation loss: 0.01428122, Gradient norm: 0.35824985
INFO:root:[   29] Training loss: 0.01548131, Validation loss: 0.01478863, Gradient norm: 0.37253413
INFO:root:[   30] Training loss: 0.01510623, Validation loss: 0.01518960, Gradient norm: 0.35616253
INFO:root:[   31] Training loss: 0.01498048, Validation loss: 0.01799279, Gradient norm: 0.32920757
INFO:root:[   32] Training loss: 0.01530303, Validation loss: 0.01213585, Gradient norm: 0.41449449
INFO:root:[   33] Training loss: 0.01441924, Validation loss: 0.01381186, Gradient norm: 0.35627866
INFO:root:[   34] Training loss: 0.01558871, Validation loss: 0.01383704, Gradient norm: 0.39377834
INFO:root:[   35] Training loss: 0.01412394, Validation loss: 0.01273767, Gradient norm: 0.28433122
INFO:root:[   36] Training loss: 0.01373645, Validation loss: 0.01544541, Gradient norm: 0.28174144
INFO:root:[   37] Training loss: 0.01376252, Validation loss: 0.01431257, Gradient norm: 0.39143824
INFO:root:[   38] Training loss: 0.01382810, Validation loss: 0.01545935, Gradient norm: 0.33599602
INFO:root:[   39] Training loss: 0.01310575, Validation loss: 0.01322285, Gradient norm: 0.25021805
INFO:root:[   40] Training loss: 0.01481246, Validation loss: 0.01591203, Gradient norm: 0.42632269
INFO:root:[   41] Training loss: 0.01425219, Validation loss: 0.01658596, Gradient norm: 0.36929735
INFO:root:[   42] Training loss: 0.01414879, Validation loss: 0.01384133, Gradient norm: 0.37180242
INFO:root:[   43] Training loss: 0.01506514, Validation loss: 0.01447419, Gradient norm: 0.35529913
INFO:root:[   44] Training loss: 0.01351349, Validation loss: 0.01424973, Gradient norm: 0.30049673
INFO:root:[   45] Training loss: 0.01255345, Validation loss: 0.01474754, Gradient norm: 0.38151915
INFO:root:[   46] Training loss: 0.01284754, Validation loss: 0.01151039, Gradient norm: 0.37452031
INFO:root:[   47] Training loss: 0.01332488, Validation loss: 0.01278854, Gradient norm: 0.37861060
INFO:root:[   48] Training loss: 0.01271616, Validation loss: 0.01549419, Gradient norm: 0.34366482
INFO:root:[   49] Training loss: 0.01323357, Validation loss: 0.01168287, Gradient norm: 0.30575501
INFO:root:[   50] Training loss: 0.01286790, Validation loss: 0.01644084, Gradient norm: 0.37230330
INFO:root:[   51] Training loss: 0.01318461, Validation loss: 0.01209666, Gradient norm: 0.37330553
INFO:root:[   52] Training loss: 0.01283481, Validation loss: 0.01212081, Gradient norm: 0.32652634
INFO:root:[   53] Training loss: 0.01183343, Validation loss: 0.01318660, Gradient norm: 0.26276929
INFO:root:[   54] Training loss: 0.01242967, Validation loss: 0.01274047, Gradient norm: 0.36950580
INFO:root:[   55] Training loss: 0.01289885, Validation loss: 0.01696728, Gradient norm: 0.36618290
INFO:root:[   56] Training loss: 0.01271279, Validation loss: 0.01473308, Gradient norm: 0.35615807
INFO:root:[   57] Training loss: 0.01268713, Validation loss: 0.01354682, Gradient norm: 0.37383047
INFO:root:[   58] Training loss: 0.01206367, Validation loss: 0.01325262, Gradient norm: 0.37498172
INFO:root:[   59] Training loss: 0.01231489, Validation loss: 0.01835721, Gradient norm: 0.35414508
INFO:root:[   60] Training loss: 0.01265971, Validation loss: 0.01502167, Gradient norm: 0.35271363
INFO:root:[   61] Training loss: 0.01220945, Validation loss: 0.01553887, Gradient norm: 0.36314356
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1206.793s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01267
INFO:root:EnergyScoreTrain: 0.00672
INFO:root:CoverageTrain: 0.62886
INFO:root:IntervalWidthTrain: 0.02717
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01503
INFO:root:EnergyScoreValidation: 0.00872
INFO:root:CoverageValidation: 0.58594
INFO:root:IntervalWidthValidation: 0.02523
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01563
INFO:root:EnergyScoreTest: 0.00942
INFO:root:CoverageTest: 0.58238
INFO:root:IntervalWidthTest: 0.02715
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1147142144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06263434, Validation loss: 0.03042623, Gradient norm: 0.48245038
INFO:root:[    2] Training loss: 0.03902580, Validation loss: 0.02896030, Gradient norm: 0.50923323
INFO:root:[    3] Training loss: 0.03253229, Validation loss: 0.03994668, Gradient norm: 0.53973456
INFO:root:[    4] Training loss: 0.02936852, Validation loss: 0.01755466, Gradient norm: 0.47768678
INFO:root:[    5] Training loss: 0.02764146, Validation loss: 0.03174311, Gradient norm: 0.47986702
INFO:root:[    6] Training loss: 0.02507942, Validation loss: 0.01545002, Gradient norm: 0.42762001
INFO:root:[    7] Training loss: 0.02217903, Validation loss: 0.02073393, Gradient norm: 0.39744184
INFO:root:[    8] Training loss: 0.02277955, Validation loss: 0.01852266, Gradient norm: 0.39399457
INFO:root:[    9] Training loss: 0.02201491, Validation loss: 0.02727582, Gradient norm: 0.46753936
INFO:root:[   10] Training loss: 0.02158639, Validation loss: 0.02370295, Gradient norm: 0.44493153
INFO:root:[   11] Training loss: 0.02013824, Validation loss: 0.01827218, Gradient norm: 0.39607453
INFO:root:[   12] Training loss: 0.02033093, Validation loss: 0.02325604, Gradient norm: 0.38942280
INFO:root:[   13] Training loss: 0.01948772, Validation loss: 0.01483679, Gradient norm: 0.38096407
INFO:root:[   14] Training loss: 0.02067712, Validation loss: 0.01793582, Gradient norm: 0.41410580
INFO:root:[   15] Training loss: 0.02049460, Validation loss: 0.01518688, Gradient norm: 0.42546024
INFO:root:[   16] Training loss: 0.01842107, Validation loss: 0.01610303, Gradient norm: 0.33394157
INFO:root:[   17] Training loss: 0.01746161, Validation loss: 0.01607706, Gradient norm: 0.34771035
INFO:root:[   18] Training loss: 0.01974071, Validation loss: 0.01399466, Gradient norm: 0.43949302
INFO:root:[   19] Training loss: 0.01709094, Validation loss: 0.01433036, Gradient norm: 0.34705742
INFO:root:[   20] Training loss: 0.01857710, Validation loss: 0.01484012, Gradient norm: 0.44282282
INFO:root:[   21] Training loss: 0.01696523, Validation loss: 0.02786057, Gradient norm: 0.34689947
INFO:root:[   22] Training loss: 0.01799915, Validation loss: 0.02117378, Gradient norm: 0.45241335
INFO:root:[   23] Training loss: 0.01773624, Validation loss: 0.01290573, Gradient norm: 0.36633268
INFO:root:[   24] Training loss: 0.01581155, Validation loss: 0.01496594, Gradient norm: 0.31305365
INFO:root:[   25] Training loss: 0.01623347, Validation loss: 0.01449582, Gradient norm: 0.38419515
INFO:root:[   26] Training loss: 0.01752938, Validation loss: 0.01424359, Gradient norm: 0.43433712
INFO:root:[   27] Training loss: 0.01649624, Validation loss: 0.01540495, Gradient norm: 0.39108681
INFO:root:[   28] Training loss: 0.01727110, Validation loss: 0.01591652, Gradient norm: 0.40202108
INFO:root:[   29] Training loss: 0.01611996, Validation loss: 0.01486549, Gradient norm: 0.38223797
INFO:root:[   30] Training loss: 0.01641269, Validation loss: 0.01163308, Gradient norm: 0.41813137
INFO:root:[   31] Training loss: 0.01445580, Validation loss: 0.01627231, Gradient norm: 0.27146623
INFO:root:[   32] Training loss: 0.01728263, Validation loss: 0.01296891, Gradient norm: 0.43422369
INFO:root:[   33] Training loss: 0.01615490, Validation loss: 0.01245661, Gradient norm: 0.40757155
INFO:root:[   34] Training loss: 0.01451428, Validation loss: 0.01313318, Gradient norm: 0.31474115
INFO:root:[   35] Training loss: 0.01526185, Validation loss: 0.01186907, Gradient norm: 0.37226253
INFO:root:[   36] Training loss: 0.01640489, Validation loss: 0.01509730, Gradient norm: 0.40822725
INFO:root:[   37] Training loss: 0.01473634, Validation loss: 0.01933396, Gradient norm: 0.39632431
INFO:root:[   38] Training loss: 0.01594367, Validation loss: 0.01149199, Gradient norm: 0.40158360
INFO:root:[   39] Training loss: 0.01561066, Validation loss: 0.01207764, Gradient norm: 0.38862788
INFO:root:[   40] Training loss: 0.01466600, Validation loss: 0.01265775, Gradient norm: 0.35822659
INFO:root:[   41] Training loss: 0.01398970, Validation loss: 0.01507398, Gradient norm: 0.35110015
INFO:root:[   42] Training loss: 0.01494629, Validation loss: 0.01358593, Gradient norm: 0.38075719
INFO:root:[   43] Training loss: 0.01508032, Validation loss: 0.01382280, Gradient norm: 0.32399718
INFO:root:[   44] Training loss: 0.01443235, Validation loss: 0.01596001, Gradient norm: 0.37677702
INFO:root:[   45] Training loss: 0.01391993, Validation loss: 0.01662693, Gradient norm: 0.32481489
INFO:root:[   46] Training loss: 0.01423589, Validation loss: 0.01344520, Gradient norm: 0.40801335
INFO:root:[   47] Training loss: 0.01350705, Validation loss: 0.01227766, Gradient norm: 0.35574918
INFO:root:[   48] Training loss: 0.01360905, Validation loss: 0.02244367, Gradient norm: 0.35130465
INFO:root:[   49] Training loss: 0.01483998, Validation loss: 0.02006379, Gradient norm: 0.40595852
INFO:root:[   50] Training loss: 0.01448903, Validation loss: 0.01113305, Gradient norm: 0.38009990
INFO:root:[   51] Training loss: 0.01470388, Validation loss: 0.01376738, Gradient norm: 0.39783134
INFO:root:[   52] Training loss: 0.01418706, Validation loss: 0.01412006, Gradient norm: 0.38909780
INFO:root:[   53] Training loss: 0.01396420, Validation loss: 0.01968458, Gradient norm: 0.31619266
INFO:root:[   54] Training loss: 0.01347934, Validation loss: 0.01231116, Gradient norm: 0.30910718
INFO:root:[   55] Training loss: 0.01385286, Validation loss: 0.01493770, Gradient norm: 0.43398476
INFO:root:[   56] Training loss: 0.01399496, Validation loss: 0.01330441, Gradient norm: 0.40577618
INFO:root:[   57] Training loss: 0.01316870, Validation loss: 0.01263205, Gradient norm: 0.30444342
INFO:root:[   58] Training loss: 0.01397206, Validation loss: 0.01741255, Gradient norm: 0.41167181
INFO:root:[   59] Training loss: 0.01297273, Validation loss: 0.01511767, Gradient norm: 0.28874053
INFO:root:[   60] Training loss: 0.01309767, Validation loss: 0.01239155, Gradient norm: 0.29833268
INFO:root:[   61] Training loss: 0.01298406, Validation loss: 0.01111082, Gradient norm: 0.35098387
INFO:root:[   62] Training loss: 0.01373035, Validation loss: 0.01432161, Gradient norm: 0.34238762
INFO:root:[   63] Training loss: 0.01258048, Validation loss: 0.01322539, Gradient norm: 0.32007714
INFO:root:[   64] Training loss: 0.01315828, Validation loss: 0.01248084, Gradient norm: 0.37614273
INFO:root:[   65] Training loss: 0.01232760, Validation loss: 0.01153962, Gradient norm: 0.33963669
INFO:root:[   66] Training loss: 0.01352235, Validation loss: 0.01459334, Gradient norm: 0.42692192
INFO:root:[   67] Training loss: 0.01312776, Validation loss: 0.01304832, Gradient norm: 0.36579271
INFO:root:[   68] Training loss: 0.01264309, Validation loss: 0.01395879, Gradient norm: 0.35652060
INFO:root:[   69] Training loss: 0.01270857, Validation loss: 0.01823346, Gradient norm: 0.35553000
INFO:root:[   70] Training loss: 0.01212759, Validation loss: 0.01279724, Gradient norm: 0.35677322
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1382.97s.
INFO:root:Emptying the cuda cache took 0.021s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01176
INFO:root:EnergyScoreTrain: 0.00653
INFO:root:CoverageTrain: 0.59961
INFO:root:IntervalWidthTrain: 0.02524
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01448
INFO:root:EnergyScoreValidation: 0.00803
INFO:root:CoverageValidation: 0.55301
INFO:root:IntervalWidthValidation: 0.02588
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0165
INFO:root:EnergyScoreTest: 0.01022
INFO:root:CoverageTest: 0.52652
INFO:root:IntervalWidthTest: 0.02652
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1147142144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06327820, Validation loss: 0.03536855, Gradient norm: 0.49807605
INFO:root:[    2] Training loss: 0.04410841, Validation loss: 0.02962216, Gradient norm: 0.61429633
INFO:root:[    3] Training loss: 0.03580069, Validation loss: 0.02038701, Gradient norm: 0.53560924
INFO:root:[    4] Training loss: 0.03191950, Validation loss: 0.02850822, Gradient norm: 0.52885018
INFO:root:[    5] Training loss: 0.02945592, Validation loss: 0.01864603, Gradient norm: 0.53170650
INFO:root:[    6] Training loss: 0.02736007, Validation loss: 0.03181120, Gradient norm: 0.48601393
INFO:root:[    7] Training loss: 0.02565550, Validation loss: 0.01804748, Gradient norm: 0.45438219
INFO:root:[    8] Training loss: 0.02413240, Validation loss: 0.01733618, Gradient norm: 0.42701828
INFO:root:[    9] Training loss: 0.02432898, Validation loss: 0.01931189, Gradient norm: 0.45670881
INFO:root:[   10] Training loss: 0.02206713, Validation loss: 0.02141257, Gradient norm: 0.38865706
INFO:root:[   11] Training loss: 0.02217973, Validation loss: 0.01743678, Gradient norm: 0.43120315
INFO:root:[   12] Training loss: 0.02162419, Validation loss: 0.02396552, Gradient norm: 0.43346304
INFO:root:[   13] Training loss: 0.02122577, Validation loss: 0.01688857, Gradient norm: 0.41069899
INFO:root:[   14] Training loss: 0.02114375, Validation loss: 0.01530008, Gradient norm: 0.38519327
INFO:root:[   15] Training loss: 0.01972070, Validation loss: 0.01716135, Gradient norm: 0.39445974
INFO:root:[   16] Training loss: 0.01951696, Validation loss: 0.01451389, Gradient norm: 0.38262384
INFO:root:[   17] Training loss: 0.02230265, Validation loss: 0.02008333, Gradient norm: 0.44847188
INFO:root:[   18] Training loss: 0.01927832, Validation loss: 0.01300913, Gradient norm: 0.35866800
INFO:root:[   19] Training loss: 0.01926104, Validation loss: 0.02043319, Gradient norm: 0.38199763
INFO:root:[   20] Training loss: 0.01893673, Validation loss: 0.01694390, Gradient norm: 0.38021457
INFO:root:[   21] Training loss: 0.01933225, Validation loss: 0.01711795, Gradient norm: 0.42728837
INFO:root:[   22] Training loss: 0.01856506, Validation loss: 0.03008731, Gradient norm: 0.37378278
INFO:root:[   23] Training loss: 0.01972887, Validation loss: 0.01346725, Gradient norm: 0.39985096
INFO:root:[   24] Training loss: 0.01824931, Validation loss: 0.01346950, Gradient norm: 0.39769193
INFO:root:[   25] Training loss: 0.01743721, Validation loss: 0.01521945, Gradient norm: 0.34376426
INFO:root:[   26] Training loss: 0.01689864, Validation loss: 0.01383667, Gradient norm: 0.33824877
INFO:root:[   27] Training loss: 0.01799014, Validation loss: 0.01857244, Gradient norm: 0.40974305
INFO:root:[   28] Training loss: 0.01727065, Validation loss: 0.01646342, Gradient norm: 0.37568059
INFO:root:[   29] Training loss: 0.01748377, Validation loss: 0.01591788, Gradient norm: 0.35220985
INFO:root:[   30] Training loss: 0.01698448, Validation loss: 0.01767069, Gradient norm: 0.39009648
INFO:root:[   31] Training loss: 0.01814689, Validation loss: 0.02164555, Gradient norm: 0.39754882
INFO:root:[   32] Training loss: 0.01688426, Validation loss: 0.02015762, Gradient norm: 0.31924548
INFO:root:[   33] Training loss: 0.01784913, Validation loss: 0.01855171, Gradient norm: 0.37243495
INFO:root:[   34] Training loss: 0.01692341, Validation loss: 0.01581617, Gradient norm: 0.36839398
INFO:root:[   35] Training loss: 0.01627109, Validation loss: 0.01455998, Gradient norm: 0.38353563
INFO:root:[   36] Training loss: 0.01610918, Validation loss: 0.01381345, Gradient norm: 0.36816658
INFO:root:[   37] Training loss: 0.01641706, Validation loss: 0.01726019, Gradient norm: 0.37757643
INFO:root:[   38] Training loss: 0.01656052, Validation loss: 0.01950305, Gradient norm: 0.31271176
INFO:root:[   39] Training loss: 0.01581145, Validation loss: 0.01707185, Gradient norm: 0.35134166
INFO:root:[   40] Training loss: 0.01605826, Validation loss: 0.01371748, Gradient norm: 0.38209823
INFO:root:[   41] Training loss: 0.01605868, Validation loss: 0.01682956, Gradient norm: 0.34914032
INFO:root:[   42] Training loss: 0.01594252, Validation loss: 0.01344659, Gradient norm: 0.36219865
INFO:root:[   43] Training loss: 0.01579775, Validation loss: 0.01672642, Gradient norm: 0.38586430
INFO:root:[   44] Training loss: 0.01452629, Validation loss: 0.01282665, Gradient norm: 0.28269428
INFO:root:[   45] Training loss: 0.01592889, Validation loss: 0.01442814, Gradient norm: 0.37541899
INFO:root:[   46] Training loss: 0.01581226, Validation loss: 0.01145824, Gradient norm: 0.41326664
INFO:root:[   47] Training loss: 0.01579274, Validation loss: 0.01705965, Gradient norm: 0.35307755
INFO:root:[   48] Training loss: 0.01460023, Validation loss: 0.01205695, Gradient norm: 0.34376594
INFO:root:[   49] Training loss: 0.01503736, Validation loss: 0.01808443, Gradient norm: 0.36147435
INFO:root:[   50] Training loss: 0.01533399, Validation loss: 0.01264146, Gradient norm: 0.39419164
INFO:root:[   51] Training loss: 0.01467981, Validation loss: 0.01398018, Gradient norm: 0.37230510
INFO:root:[   52] Training loss: 0.01529769, Validation loss: 0.01537646, Gradient norm: 0.38923998
INFO:root:[   53] Training loss: 0.01495229, Validation loss: 0.01475218, Gradient norm: 0.30589539
INFO:root:[   54] Training loss: 0.01423235, Validation loss: 0.01178209, Gradient norm: 0.33542339
INFO:root:[   55] Training loss: 0.01538882, Validation loss: 0.01963293, Gradient norm: 0.37269025
INFO:root:[   56] Training loss: 0.01505716, Validation loss: 0.01718901, Gradient norm: 0.31797633
INFO:root:[   57] Training loss: 0.01444421, Validation loss: 0.01652321, Gradient norm: 0.29647272
INFO:root:[   58] Training loss: 0.01441893, Validation loss: 0.01471284, Gradient norm: 0.35525428
INFO:root:[   59] Training loss: 0.01426589, Validation loss: 0.01579825, Gradient norm: 0.28914978
INFO:root:[   60] Training loss: 0.01402241, Validation loss: 0.01476374, Gradient norm: 0.38640133
INFO:root:[   61] Training loss: 0.01438223, Validation loss: 0.01206379, Gradient norm: 0.29918412
INFO:root:[   62] Training loss: 0.01416778, Validation loss: 0.01437639, Gradient norm: 0.36091542
INFO:root:[   63] Training loss: 0.01508648, Validation loss: 0.01293266, Gradient norm: 0.39962514
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 1249.449s.
INFO:root:Emptying the cuda cache took 0.019s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01391
INFO:root:EnergyScoreTrain: 0.00778
INFO:root:CoverageTrain: 0.60375
INFO:root:IntervalWidthTrain: 0.02853
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0152
INFO:root:EnergyScoreValidation: 0.00891
INFO:root:CoverageValidation: 0.59731
INFO:root:IntervalWidthValidation: 0.02912
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0151
INFO:root:EnergyScoreTest: 0.00898
INFO:root:CoverageTest: 0.61522
INFO:root:IntervalWidthTest: 0.02908
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1147142144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06699714, Validation loss: 0.03853866, Gradient norm: 0.46032303
INFO:root:[    2] Training loss: 0.04306638, Validation loss: 0.04341812, Gradient norm: 0.47061776
INFO:root:[    3] Training loss: 0.03982357, Validation loss: 0.04235121, Gradient norm: 0.57184160
INFO:root:[    4] Training loss: 0.03304034, Validation loss: 0.02888192, Gradient norm: 0.44982329
INFO:root:[    5] Training loss: 0.03376757, Validation loss: 0.03317273, Gradient norm: 0.52148170
INFO:root:[    6] Training loss: 0.02984322, Validation loss: 0.02769271, Gradient norm: 0.46129207
INFO:root:[    7] Training loss: 0.02709005, Validation loss: 0.02233008, Gradient norm: 0.42343234
INFO:root:[    8] Training loss: 0.02630375, Validation loss: 0.01869526, Gradient norm: 0.40396469
INFO:root:[    9] Training loss: 0.02504492, Validation loss: 0.01746646, Gradient norm: 0.42496968
INFO:root:[   10] Training loss: 0.02488147, Validation loss: 0.02429207, Gradient norm: 0.36769558
INFO:root:[   11] Training loss: 0.02474912, Validation loss: 0.02549703, Gradient norm: 0.43315481
INFO:root:[   12] Training loss: 0.02363134, Validation loss: 0.03090313, Gradient norm: 0.34090526
INFO:root:[   13] Training loss: 0.02264826, Validation loss: 0.02130896, Gradient norm: 0.36585619
INFO:root:[   14] Training loss: 0.02211358, Validation loss: 0.02432598, Gradient norm: 0.36096524
INFO:root:[   15] Training loss: 0.02274476, Validation loss: 0.02596204, Gradient norm: 0.42963227
INFO:root:[   16] Training loss: 0.02123500, Validation loss: 0.01665276, Gradient norm: 0.39435885
INFO:root:[   17] Training loss: 0.02254396, Validation loss: 0.01742168, Gradient norm: 0.41872195
INFO:root:[   18] Training loss: 0.02100275, Validation loss: 0.02561103, Gradient norm: 0.37616144
INFO:root:[   19] Training loss: 0.02157866, Validation loss: 0.01657732, Gradient norm: 0.37368947
INFO:root:[   20] Training loss: 0.02105278, Validation loss: 0.01956752, Gradient norm: 0.38816509
INFO:root:[   21] Training loss: 0.01969460, Validation loss: 0.02285003, Gradient norm: 0.35888601
INFO:root:[   22] Training loss: 0.02056680, Validation loss: 0.02009160, Gradient norm: 0.38101809
INFO:root:[   23] Training loss: 0.01930262, Validation loss: 0.02971784, Gradient norm: 0.32279970
INFO:root:[   24] Training loss: 0.01991619, Validation loss: 0.01829123, Gradient norm: 0.37802598
INFO:root:[   25] Training loss: 0.01950605, Validation loss: 0.02257065, Gradient norm: 0.39049274
INFO:root:[   26] Training loss: 0.01921260, Validation loss: 0.01396230, Gradient norm: 0.33858478
INFO:root:[   27] Training loss: 0.01971904, Validation loss: 0.01754034, Gradient norm: 0.39711074
INFO:root:[   28] Training loss: 0.01946776, Validation loss: 0.01838075, Gradient norm: 0.39669678
INFO:root:[   29] Training loss: 0.01950440, Validation loss: 0.01983797, Gradient norm: 0.33035449
INFO:root:[   30] Training loss: 0.01892729, Validation loss: 0.02066560, Gradient norm: 0.36910210
INFO:root:[   31] Training loss: 0.01919683, Validation loss: 0.02147401, Gradient norm: 0.32814450
INFO:root:[   32] Training loss: 0.01795138, Validation loss: 0.01969947, Gradient norm: 0.34970885
INFO:root:[   33] Training loss: 0.01744933, Validation loss: 0.01837160, Gradient norm: 0.38257942
INFO:root:[   34] Training loss: 0.01786394, Validation loss: 0.01592854, Gradient norm: 0.33680332
INFO:root:[   35] Training loss: 0.01750728, Validation loss: 0.01773949, Gradient norm: 0.36914373
INFO:root:[   36] Training loss: 0.01817888, Validation loss: 0.02032278, Gradient norm: 0.33942300
INFO:root:[   37] Training loss: 0.01936565, Validation loss: 0.01719963, Gradient norm: 0.44581483
INFO:root:[   38] Training loss: 0.01690234, Validation loss: 0.01861264, Gradient norm: 0.33943268
INFO:root:[   39] Training loss: 0.01759236, Validation loss: 0.02164234, Gradient norm: 0.37812018
INFO:root:[   40] Training loss: 0.01735237, Validation loss: 0.01619290, Gradient norm: 0.36653391
INFO:root:[   41] Training loss: 0.01758379, Validation loss: 0.01570423, Gradient norm: 0.35039195
INFO:root:[   42] Training loss: 0.01933252, Validation loss: 0.01563426, Gradient norm: 0.39014863
INFO:root:[   43] Training loss: 0.01711385, Validation loss: 0.01584945, Gradient norm: 0.35781706
INFO:root:[   44] Training loss: 0.01705877, Validation loss: 0.01344897, Gradient norm: 0.34368104
INFO:root:[   45] Training loss: 0.01642159, Validation loss: 0.01972070, Gradient norm: 0.37847396
INFO:root:[   46] Training loss: 0.01650700, Validation loss: 0.01607096, Gradient norm: 0.32952366
INFO:root:[   47] Training loss: 0.01723421, Validation loss: 0.01710681, Gradient norm: 0.35360901
INFO:root:[   48] Training loss: 0.01647142, Validation loss: 0.01515962, Gradient norm: 0.38079155
INFO:root:[   49] Training loss: 0.01638148, Validation loss: 0.01732324, Gradient norm: 0.35897388
INFO:root:[   50] Training loss: 0.01611378, Validation loss: 0.01650264, Gradient norm: 0.33725778
INFO:root:[   51] Training loss: 0.01715098, Validation loss: 0.01714086, Gradient norm: 0.35452625
INFO:root:[   52] Training loss: 0.01675952, Validation loss: 0.01586307, Gradient norm: 0.35483440
INFO:root:[   53] Training loss: 0.01554494, Validation loss: 0.01663760, Gradient norm: 0.30244760
INFO:root:[   54] Training loss: 0.01646604, Validation loss: 0.01709945, Gradient norm: 0.37264686
INFO:root:[   55] Training loss: 0.01774844, Validation loss: 0.01813249, Gradient norm: 0.40071689
INFO:root:[   56] Training loss: 0.01581449, Validation loss: 0.02002191, Gradient norm: 0.33838713
INFO:root:[   57] Training loss: 0.01627492, Validation loss: 0.01613633, Gradient norm: 0.39550564
INFO:root:[   58] Training loss: 0.01624332, Validation loss: 0.01481870, Gradient norm: 0.31990340
INFO:root:[   59] Training loss: 0.01549956, Validation loss: 0.01882804, Gradient norm: 0.32133338
INFO:root:[   60] Training loss: 0.01587245, Validation loss: 0.01618671, Gradient norm: 0.34290773
INFO:root:[   61] Training loss: 0.01518342, Validation loss: 0.01467637, Gradient norm: 0.33392691
INFO:root:[   62] Training loss: 0.01605244, Validation loss: 0.01834739, Gradient norm: 0.38806056
INFO:root:[   63] Training loss: 0.01464173, Validation loss: 0.01621909, Gradient norm: 0.30614286
INFO:root:[   64] Training loss: 0.01551475, Validation loss: 0.01554353, Gradient norm: 0.35636687
INFO:root:[   65] Training loss: 0.01525096, Validation loss: 0.01481445, Gradient norm: 0.33709648
INFO:root:[   66] Training loss: 0.01562874, Validation loss: 0.01740793, Gradient norm: 0.35522054
INFO:root:[   67] Training loss: 0.01499466, Validation loss: 0.01543395, Gradient norm: 0.30229917
INFO:root:[   68] Training loss: 0.01608672, Validation loss: 0.01917710, Gradient norm: 0.36797767
INFO:root:[   69] Training loss: 0.01563832, Validation loss: 0.01762974, Gradient norm: 0.35789518
INFO:root:[   70] Training loss: 0.01561717, Validation loss: 0.02066298, Gradient norm: 0.33323355
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1389.357s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01556
INFO:root:EnergyScoreTrain: 0.00908
INFO:root:CoverageTrain: 0.53286
INFO:root:IntervalWidthTrain: 0.02719
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01708
INFO:root:EnergyScoreValidation: 0.01044
INFO:root:CoverageValidation: 0.49007
INFO:root:IntervalWidthValidation: 0.02685
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01895
INFO:root:EnergyScoreTest: 0.01137
INFO:root:CoverageTest: 0.48424
INFO:root:IntervalWidthTest: 0.02923
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 1147142144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05567575, Validation loss: 0.03998865, Gradient norm: 0.61366248
INFO:root:[    2] Training loss: 0.03090138, Validation loss: 0.02338467, Gradient norm: 0.64758024
INFO:root:[    3] Training loss: 0.02660251, Validation loss: 0.03691044, Gradient norm: 0.65078209
INFO:root:[    4] Training loss: 0.02438169, Validation loss: 0.02720117, Gradient norm: 0.68303711
INFO:root:[    5] Training loss: 0.02262454, Validation loss: 0.02067739, Gradient norm: 0.57208334
INFO:root:[    6] Training loss: 0.01996312, Validation loss: 0.01773038, Gradient norm: 0.57357293
INFO:root:[    7] Training loss: 0.01802355, Validation loss: 0.02014773, Gradient norm: 0.46992513
INFO:root:[    8] Training loss: 0.01829629, Validation loss: 0.01668147, Gradient norm: 0.54862969
INFO:root:[    9] Training loss: 0.01681072, Validation loss: 0.01811908, Gradient norm: 0.44859874
INFO:root:[   10] Training loss: 0.01647827, Validation loss: 0.01667389, Gradient norm: 0.49345712
INFO:root:[   11] Training loss: 0.01539318, Validation loss: 0.01452743, Gradient norm: 0.44571137
INFO:root:[   12] Training loss: 0.01424804, Validation loss: 0.01487434, Gradient norm: 0.35376796
INFO:root:[   13] Training loss: 0.01641686, Validation loss: 0.01485437, Gradient norm: 0.43796695
INFO:root:[   14] Training loss: 0.01413015, Validation loss: 0.01421646, Gradient norm: 0.44949157
INFO:root:[   15] Training loss: 0.01447495, Validation loss: 0.01488934, Gradient norm: 0.44608676
INFO:root:[   16] Training loss: 0.01398659, Validation loss: 0.01579604, Gradient norm: 0.43550527
INFO:root:[   17] Training loss: 0.01354206, Validation loss: 0.01545660, Gradient norm: 0.34551904
INFO:root:[   18] Training loss: 0.01461091, Validation loss: 0.01686744, Gradient norm: 0.45269462
INFO:root:[   19] Training loss: 0.01403053, Validation loss: 0.01413091, Gradient norm: 0.45985998
INFO:root:[   20] Training loss: 0.01249148, Validation loss: 0.01548733, Gradient norm: 0.36277037
INFO:root:[   21] Training loss: 0.01312228, Validation loss: 0.01446271, Gradient norm: 0.42662058
INFO:root:[   22] Training loss: 0.01247889, Validation loss: 0.01406133, Gradient norm: 0.38358281
INFO:root:[   23] Training loss: 0.01167679, Validation loss: 0.01316903, Gradient norm: 0.34006104
INFO:root:[   24] Training loss: 0.01269722, Validation loss: 0.01276082, Gradient norm: 0.41867440
INFO:root:[   25] Training loss: 0.01309943, Validation loss: 0.01237254, Gradient norm: 0.38733390
INFO:root:[   26] Training loss: 0.01180419, Validation loss: 0.01383739, Gradient norm: 0.36225395
INFO:root:[   27] Training loss: 0.01234797, Validation loss: 0.01500232, Gradient norm: 0.43701631
INFO:root:[   28] Training loss: 0.01219948, Validation loss: 0.01373131, Gradient norm: 0.43813463
INFO:root:[   29] Training loss: 0.01155796, Validation loss: 0.01403400, Gradient norm: 0.33050901
INFO:root:[   30] Training loss: 0.01095527, Validation loss: 0.01301713, Gradient norm: 0.35721766
INFO:root:[   31] Training loss: 0.01040606, Validation loss: 0.01446966, Gradient norm: 0.36181710
INFO:root:[   32] Training loss: 0.01178669, Validation loss: 0.01394474, Gradient norm: 0.39677317
INFO:root:[   33] Training loss: 0.01204255, Validation loss: 0.01313998, Gradient norm: 0.37028765
INFO:root:[   34] Training loss: 0.01063052, Validation loss: 0.01507891, Gradient norm: 0.39062549
INFO:root:[   35] Training loss: 0.01162186, Validation loss: 0.01254623, Gradient norm: 0.35712005
INFO:root:[   36] Training loss: 0.01082808, Validation loss: 0.01266205, Gradient norm: 0.40776307
INFO:root:[   37] Training loss: 0.01111598, Validation loss: 0.01262260, Gradient norm: 0.41789239
INFO:root:[   38] Training loss: 0.01077313, Validation loss: 0.01329515, Gradient norm: 0.33247869
INFO:root:[   39] Training loss: 0.01029252, Validation loss: 0.01426069, Gradient norm: 0.37470334
INFO:root:[   40] Training loss: 0.01044353, Validation loss: 0.01319947, Gradient norm: 0.42334086
INFO:root:[   41] Training loss: 0.01089356, Validation loss: 0.01470814, Gradient norm: 0.39939140
INFO:root:[   42] Training loss: 0.01074096, Validation loss: 0.01357988, Gradient norm: 0.35159807
INFO:root:[   43] Training loss: 0.00994481, Validation loss: 0.01421489, Gradient norm: 0.32294138
INFO:root:[   44] Training loss: 0.01007273, Validation loss: 0.01464335, Gradient norm: 0.42161098
INFO:root:[   45] Training loss: 0.01038962, Validation loss: 0.01414185, Gradient norm: 0.37638484
INFO:root:[   46] Training loss: 0.01002425, Validation loss: 0.01258638, Gradient norm: 0.33789074
INFO:root:[   47] Training loss: 0.00943412, Validation loss: 0.01230537, Gradient norm: 0.37577230
INFO:root:[   48] Training loss: 0.00940529, Validation loss: 0.01331794, Gradient norm: 0.33712146
INFO:root:[   49] Training loss: 0.00899005, Validation loss: 0.01407389, Gradient norm: 0.34784786
INFO:root:[   50] Training loss: 0.00952968, Validation loss: 0.01347493, Gradient norm: 0.41094291
INFO:root:[   51] Training loss: 0.00963874, Validation loss: 0.01356232, Gradient norm: 0.41988074
INFO:root:[   52] Training loss: 0.00978362, Validation loss: 0.01559682, Gradient norm: 0.39189334
INFO:root:[   53] Training loss: 0.00961158, Validation loss: 0.01264340, Gradient norm: 0.36070667
INFO:root:[   54] Training loss: 0.00938361, Validation loss: 0.01327434, Gradient norm: 0.43878964
INFO:root:[   55] Training loss: 0.00990531, Validation loss: 0.01487296, Gradient norm: 0.44393211
INFO:root:[   56] Training loss: 0.01002854, Validation loss: 0.01347958, Gradient norm: 0.41656557
INFO:root:[   57] Training loss: 0.00935509, Validation loss: 0.01357416, Gradient norm: 0.41890976
INFO:root:[   58] Training loss: 0.00899464, Validation loss: 0.01307445, Gradient norm: 0.38194790
INFO:root:[   59] Training loss: 0.00908966, Validation loss: 0.01289155, Gradient norm: 0.40662677
INFO:root:[   60] Training loss: 0.00874711, Validation loss: 0.01252692, Gradient norm: 0.34223954
INFO:root:[   61] Training loss: 0.00854462, Validation loss: 0.01446520, Gradient norm: 0.36681796
INFO:root:[   62] Training loss: 0.00937474, Validation loss: 0.01455097, Gradient norm: 0.41236388
INFO:root:[   63] Training loss: 0.00909099, Validation loss: 0.01328294, Gradient norm: 0.44156574
INFO:root:[   64] Training loss: 0.00863382, Validation loss: 0.01460286, Gradient norm: 0.37876634
INFO:root:[   65] Training loss: 0.00941511, Validation loss: 0.01424647, Gradient norm: 0.42260246
INFO:root:[   66] Training loss: 0.00906798, Validation loss: 0.01513438, Gradient norm: 0.38662138
INFO:root:[   67] Training loss: 0.00883810, Validation loss: 0.01322780, Gradient norm: 0.41555139
INFO:root:[   68] Training loss: 0.00890514, Validation loss: 0.01281940, Gradient norm: 0.37641492
INFO:root:[   69] Training loss: 0.00852092, Validation loss: 0.01348560, Gradient norm: 0.39404937
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 1397.166s.
INFO:root:Emptying the cuda cache took 0.015s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0075
INFO:root:EnergyScoreTrain: 0.00571
INFO:root:CoverageTrain: 0.76849
INFO:root:IntervalWidthTrain: 0.01663
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01164
INFO:root:EnergyScoreValidation: 0.00958
INFO:root:CoverageValidation: 0.62092
INFO:root:IntervalWidthValidation: 0.01644
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01199
INFO:root:EnergyScoreTest: 0.00991
INFO:root:CoverageTest: 0.6201
INFO:root:IntervalWidthTest: 0.0165
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.05806390, Validation loss: 0.03127846, Gradient norm: 0.55390034
INFO:root:[    2] Training loss: 0.03664193, Validation loss: 0.03631015, Gradient norm: 0.53298488
INFO:root:[    3] Training loss: 0.03008756, Validation loss: 0.02665243, Gradient norm: 0.56540732
INFO:root:[    4] Training loss: 0.02427403, Validation loss: 0.02057764, Gradient norm: 0.47323273
INFO:root:[    5] Training loss: 0.02351652, Validation loss: 0.02244880, Gradient norm: 0.50169976
INFO:root:[    6] Training loss: 0.02441953, Validation loss: 0.02121788, Gradient norm: 0.55757968
INFO:root:[    7] Training loss: 0.02141454, Validation loss: 0.02165547, Gradient norm: 0.48780252
INFO:root:[    8] Training loss: 0.01965613, Validation loss: 0.01805071, Gradient norm: 0.45320040
INFO:root:[    9] Training loss: 0.02022436, Validation loss: 0.01825442, Gradient norm: 0.41814185
INFO:root:[   10] Training loss: 0.01936173, Validation loss: 0.01797692, Gradient norm: 0.47649061
INFO:root:[   11] Training loss: 0.01797367, Validation loss: 0.01984836, Gradient norm: 0.40552092
INFO:root:[   12] Training loss: 0.01715831, Validation loss: 0.02172136, Gradient norm: 0.34909029
INFO:root:[   13] Training loss: 0.01791258, Validation loss: 0.02196363, Gradient norm: 0.44940378
INFO:root:[   14] Training loss: 0.01693959, Validation loss: 0.01545098, Gradient norm: 0.41802128
INFO:root:[   15] Training loss: 0.01673699, Validation loss: 0.02358777, Gradient norm: 0.41092417
INFO:root:[   16] Training loss: 0.01476712, Validation loss: 0.01486600, Gradient norm: 0.33550976
INFO:root:[   17] Training loss: 0.01652057, Validation loss: 0.01414439, Gradient norm: 0.41804045
INFO:root:[   18] Training loss: 0.01622397, Validation loss: 0.01598391, Gradient norm: 0.35087278
INFO:root:[   19] Training loss: 0.01447231, Validation loss: 0.01891815, Gradient norm: 0.33978460
INFO:root:[   20] Training loss: 0.01512902, Validation loss: 0.01552376, Gradient norm: 0.41187432
INFO:root:[   21] Training loss: 0.01432922, Validation loss: 0.01527829, Gradient norm: 0.31995034
INFO:root:[   22] Training loss: 0.01553354, Validation loss: 0.01522137, Gradient norm: 0.41818669
INFO:root:[   23] Training loss: 0.01413784, Validation loss: 0.01513702, Gradient norm: 0.38483005
INFO:root:[   24] Training loss: 0.01405516, Validation loss: 0.01460306, Gradient norm: 0.34034550
INFO:root:[   25] Training loss: 0.01463269, Validation loss: 0.01763897, Gradient norm: 0.36979251
INFO:root:[   26] Training loss: 0.01353269, Validation loss: 0.01572099, Gradient norm: 0.37364708
INFO:root:[   27] Training loss: 0.01496208, Validation loss: 0.01346642, Gradient norm: 0.40698830
INFO:root:[   28] Training loss: 0.01383197, Validation loss: 0.01849084, Gradient norm: 0.39514787
INFO:root:[   29] Training loss: 0.01359895, Validation loss: 0.01427977, Gradient norm: 0.41091317
INFO:root:[   30] Training loss: 0.01350093, Validation loss: 0.01485667, Gradient norm: 0.40268444
INFO:root:[   31] Training loss: 0.01310999, Validation loss: 0.01382570, Gradient norm: 0.32791866
INFO:root:[   32] Training loss: 0.01286580, Validation loss: 0.01329570, Gradient norm: 0.38682511
INFO:root:[   33] Training loss: 0.01293202, Validation loss: 0.01365747, Gradient norm: 0.29975324
INFO:root:[   34] Training loss: 0.01288553, Validation loss: 0.01540432, Gradient norm: 0.37294607
INFO:root:[   35] Training loss: 0.01245427, Validation loss: 0.01298199, Gradient norm: 0.33403962
INFO:root:[   36] Training loss: 0.01315532, Validation loss: 0.01482801, Gradient norm: 0.41474253
INFO:root:[   37] Training loss: 0.01200772, Validation loss: 0.01336508, Gradient norm: 0.38996585
INFO:root:[   38] Training loss: 0.01235973, Validation loss: 0.01458013, Gradient norm: 0.30386065
INFO:root:[   39] Training loss: 0.01220836, Validation loss: 0.01382256, Gradient norm: 0.36559236
INFO:root:[   40] Training loss: 0.01292688, Validation loss: 0.01302978, Gradient norm: 0.35145051
INFO:root:[   41] Training loss: 0.01235194, Validation loss: 0.01428748, Gradient norm: 0.40050051
INFO:root:[   42] Training loss: 0.01215675, Validation loss: 0.01494799, Gradient norm: 0.37631630
INFO:root:[   43] Training loss: 0.01226503, Validation loss: 0.01490710, Gradient norm: 0.36052378
INFO:root:[   44] Training loss: 0.01217374, Validation loss: 0.01583204, Gradient norm: 0.38694959
INFO:root:[   45] Training loss: 0.01174943, Validation loss: 0.01385262, Gradient norm: 0.34685602
INFO:root:[   46] Training loss: 0.01082012, Validation loss: 0.01349805, Gradient norm: 0.33988747
INFO:root:[   47] Training loss: 0.01193067, Validation loss: 0.01402412, Gradient norm: 0.40417304
INFO:root:[   48] Training loss: 0.01123176, Validation loss: 0.01398065, Gradient norm: 0.34445125
INFO:root:[   49] Training loss: 0.01182661, Validation loss: 0.01427616, Gradient norm: 0.33693826
INFO:root:[   50] Training loss: 0.01112598, Validation loss: 0.01419838, Gradient norm: 0.38172758
INFO:root:[   51] Training loss: 0.01137037, Validation loss: 0.01565442, Gradient norm: 0.38794960
INFO:root:[   52] Training loss: 0.01045229, Validation loss: 0.01255200, Gradient norm: 0.32669571
INFO:root:[   53] Training loss: 0.01037629, Validation loss: 0.01553122, Gradient norm: 0.31876436
INFO:root:[   54] Training loss: 0.01086068, Validation loss: 0.01762896, Gradient norm: 0.38563375
INFO:root:[   55] Training loss: 0.01137745, Validation loss: 0.01380540, Gradient norm: 0.33965646
INFO:root:[   56] Training loss: 0.01071793, Validation loss: 0.01363989, Gradient norm: 0.38914112
INFO:root:[   57] Training loss: 0.00975192, Validation loss: 0.01386691, Gradient norm: 0.33139106
INFO:root:[   58] Training loss: 0.01080805, Validation loss: 0.01299462, Gradient norm: 0.37023682
INFO:root:[   59] Training loss: 0.01071686, Validation loss: 0.01610776, Gradient norm: 0.35330237
INFO:root:[   60] Training loss: 0.01084653, Validation loss: 0.01395794, Gradient norm: 0.37986034
INFO:root:[   61] Training loss: 0.01017355, Validation loss: 0.01621286, Gradient norm: 0.29958939
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1357.506s.
INFO:root:Emptying the cuda cache took 0.075s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00731
INFO:root:EnergyScoreTrain: 0.00536
INFO:root:CoverageTrain: 0.89747
INFO:root:IntervalWidthTrain: 0.0215
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01086
INFO:root:EnergyScoreValidation: 0.00849
INFO:root:CoverageValidation: 0.78987
INFO:root:IntervalWidthValidation: 0.02127
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01084
INFO:root:EnergyScoreTest: 0.00846
INFO:root:CoverageTest: 0.7889
INFO:root:IntervalWidthTest: 0.02139
INFO:root:###21 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06288683, Validation loss: 0.05643251, Gradient norm: 0.52870705
INFO:root:[    2] Training loss: 0.03689119, Validation loss: 0.02948510, Gradient norm: 0.57959797
INFO:root:[    3] Training loss: 0.03081531, Validation loss: 0.02904478, Gradient norm: 0.49008704
INFO:root:[    4] Training loss: 0.02740065, Validation loss: 0.02873853, Gradient norm: 0.48055388
INFO:root:[    5] Training loss: 0.02363892, Validation loss: 0.02391707, Gradient norm: 0.44446768
INFO:root:[    6] Training loss: 0.02398940, Validation loss: 0.02530405, Gradient norm: 0.49950787
INFO:root:[    7] Training loss: 0.02269171, Validation loss: 0.02188952, Gradient norm: 0.46083300
INFO:root:[    8] Training loss: 0.02093135, Validation loss: 0.01866073, Gradient norm: 0.40518568
INFO:root:[    9] Training loss: 0.02079325, Validation loss: 0.02043952, Gradient norm: 0.36775687
INFO:root:[   10] Training loss: 0.02017743, Validation loss: 0.02072616, Gradient norm: 0.42714784
INFO:root:[   11] Training loss: 0.01955786, Validation loss: 0.01923989, Gradient norm: 0.41700740
INFO:root:[   12] Training loss: 0.01908921, Validation loss: 0.02551980, Gradient norm: 0.43236747
INFO:root:[   13] Training loss: 0.01858708, Validation loss: 0.01799705, Gradient norm: 0.37021114
INFO:root:[   14] Training loss: 0.01804671, Validation loss: 0.01718159, Gradient norm: 0.36616705
INFO:root:[   15] Training loss: 0.01639503, Validation loss: 0.01719919, Gradient norm: 0.34089153
INFO:root:[   16] Training loss: 0.01652718, Validation loss: 0.01848186, Gradient norm: 0.33756956
INFO:root:[   17] Training loss: 0.01698419, Validation loss: 0.01881587, Gradient norm: 0.38127384
INFO:root:[   18] Training loss: 0.01723212, Validation loss: 0.01704471, Gradient norm: 0.41951583
INFO:root:[   19] Training loss: 0.01617206, Validation loss: 0.01870049, Gradient norm: 0.39277458
INFO:root:[   20] Training loss: 0.01600696, Validation loss: 0.01836261, Gradient norm: 0.38443511
INFO:root:[   21] Training loss: 0.01606133, Validation loss: 0.01499986, Gradient norm: 0.37190644
INFO:root:[   22] Training loss: 0.01593624, Validation loss: 0.01716750, Gradient norm: 0.31567510
INFO:root:[   23] Training loss: 0.01587746, Validation loss: 0.01839657, Gradient norm: 0.42027550
INFO:root:[   24] Training loss: 0.01508345, Validation loss: 0.01561295, Gradient norm: 0.36566141
INFO:root:[   25] Training loss: 0.01427355, Validation loss: 0.01400493, Gradient norm: 0.32090895
INFO:root:[   26] Training loss: 0.01478227, Validation loss: 0.01448403, Gradient norm: 0.36687914
INFO:root:[   27] Training loss: 0.01503938, Validation loss: 0.01668212, Gradient norm: 0.37614894
INFO:root:[   28] Training loss: 0.01487439, Validation loss: 0.01797269, Gradient norm: 0.34565557
INFO:root:[   29] Training loss: 0.01458162, Validation loss: 0.01720148, Gradient norm: 0.34992793
INFO:root:[   30] Training loss: 0.01448733, Validation loss: 0.01625368, Gradient norm: 0.39944139
INFO:root:[   31] Training loss: 0.01563903, Validation loss: 0.01593314, Gradient norm: 0.41751184
INFO:root:[   32] Training loss: 0.01436846, Validation loss: 0.01504170, Gradient norm: 0.37226309
INFO:root:[   33] Training loss: 0.01458637, Validation loss: 0.01636166, Gradient norm: 0.43468046
INFO:root:[   34] Training loss: 0.01325488, Validation loss: 0.01544401, Gradient norm: 0.31537648
INFO:root:[   35] Training loss: 0.01361821, Validation loss: 0.01597242, Gradient norm: 0.33904453
INFO:root:[   36] Training loss: 0.01453523, Validation loss: 0.02040424, Gradient norm: 0.42850573
INFO:root:[   37] Training loss: 0.01425649, Validation loss: 0.01461054, Gradient norm: 0.37816835
INFO:root:[   38] Training loss: 0.01346053, Validation loss: 0.01540271, Gradient norm: 0.33929903
INFO:root:[   39] Training loss: 0.01379376, Validation loss: 0.01385050, Gradient norm: 0.37452225
INFO:root:[   40] Training loss: 0.01340302, Validation loss: 0.01580200, Gradient norm: 0.35892110
INFO:root:[   41] Training loss: 0.01288314, Validation loss: 0.01335963, Gradient norm: 0.31469765
INFO:root:[   42] Training loss: 0.01314984, Validation loss: 0.01678271, Gradient norm: 0.32761913
INFO:root:[   43] Training loss: 0.01334919, Validation loss: 0.01455909, Gradient norm: 0.37709897
INFO:root:[   44] Training loss: 0.01281616, Validation loss: 0.01720049, Gradient norm: 0.37984164
INFO:root:[   45] Training loss: 0.01347288, Validation loss: 0.01587440, Gradient norm: 0.37981163
INFO:root:[   46] Training loss: 0.01300256, Validation loss: 0.01742376, Gradient norm: 0.40051581
INFO:root:[   47] Training loss: 0.01272474, Validation loss: 0.01466828, Gradient norm: 0.33825210
INFO:root:[   48] Training loss: 0.01301574, Validation loss: 0.01492792, Gradient norm: 0.38670445
INFO:root:[   49] Training loss: 0.01180819, Validation loss: 0.01454925, Gradient norm: 0.33611080
INFO:root:[   50] Training loss: 0.01171340, Validation loss: 0.01433580, Gradient norm: 0.36869409
INFO:root:[   51] Training loss: 0.01245098, Validation loss: 0.01432168, Gradient norm: 0.35658734
INFO:root:[   52] Training loss: 0.01230465, Validation loss: 0.01348361, Gradient norm: 0.31034626
INFO:root:[   53] Training loss: 0.01164699, Validation loss: 0.01433191, Gradient norm: 0.40447392
INFO:root:[   54] Training loss: 0.01307380, Validation loss: 0.01348937, Gradient norm: 0.35662964
INFO:root:[   55] Training loss: 0.01208862, Validation loss: 0.01353459, Gradient norm: 0.36186182
INFO:root:[   56] Training loss: 0.01259726, Validation loss: 0.01430327, Gradient norm: 0.36853080
INFO:root:[   57] Training loss: 0.01214541, Validation loss: 0.01481926, Gradient norm: 0.36750005
INFO:root:[   58] Training loss: 0.01172961, Validation loss: 0.01485393, Gradient norm: 0.33832887
INFO:root:[   59] Training loss: 0.01211678, Validation loss: 0.01715190, Gradient norm: 0.38990265
INFO:root:[   60] Training loss: 0.01175557, Validation loss: 0.01609009, Gradient norm: 0.36280275
INFO:root:[   61] Training loss: 0.01193461, Validation loss: 0.01604155, Gradient norm: 0.35065977
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1306.055s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00895
INFO:root:EnergyScoreTrain: 0.00652
INFO:root:CoverageTrain: 0.89647
INFO:root:IntervalWidthTrain: 0.02643
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0115
INFO:root:EnergyScoreValidation: 0.00867
INFO:root:CoverageValidation: 0.81093
INFO:root:IntervalWidthValidation: 0.02622
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01156
INFO:root:EnergyScoreTest: 0.00873
INFO:root:CoverageTest: 0.81288
INFO:root:IntervalWidthTest: 0.0263
INFO:root:###22 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06430911, Validation loss: 0.05347005, Gradient norm: 0.56039081
INFO:root:[    2] Training loss: 0.04101652, Validation loss: 0.03905420, Gradient norm: 0.56119842
INFO:root:[    3] Training loss: 0.03498588, Validation loss: 0.02629413, Gradient norm: 0.56641841
INFO:root:[    4] Training loss: 0.02937651, Validation loss: 0.03058229, Gradient norm: 0.43409541
INFO:root:[    5] Training loss: 0.02773233, Validation loss: 0.02538057, Gradient norm: 0.47134398
INFO:root:[    6] Training loss: 0.02656978, Validation loss: 0.02465872, Gradient norm: 0.49514649
INFO:root:[    7] Training loss: 0.02480477, Validation loss: 0.02615780, Gradient norm: 0.44470515
INFO:root:[    8] Training loss: 0.02464985, Validation loss: 0.02091191, Gradient norm: 0.46815789
INFO:root:[    9] Training loss: 0.02282575, Validation loss: 0.02535271, Gradient norm: 0.41417247
INFO:root:[   10] Training loss: 0.02078303, Validation loss: 0.02410650, Gradient norm: 0.38838362
INFO:root:[   11] Training loss: 0.02309054, Validation loss: 0.02111300, Gradient norm: 0.46246416
INFO:root:[   12] Training loss: 0.02130207, Validation loss: 0.01906045, Gradient norm: 0.43778827
INFO:root:[   13] Training loss: 0.02083987, Validation loss: 0.02150612, Gradient norm: 0.43542323
INFO:root:[   14] Training loss: 0.02087941, Validation loss: 0.01841879, Gradient norm: 0.42920667
INFO:root:[   15] Training loss: 0.01879908, Validation loss: 0.01853976, Gradient norm: 0.36935722
INFO:root:[   16] Training loss: 0.01902512, Validation loss: 0.02073267, Gradient norm: 0.38732254
INFO:root:[   17] Training loss: 0.01868316, Validation loss: 0.01853161, Gradient norm: 0.39967076
INFO:root:[   18] Training loss: 0.01819776, Validation loss: 0.01731863, Gradient norm: 0.37473217
INFO:root:[   19] Training loss: 0.01731573, Validation loss: 0.01765750, Gradient norm: 0.34882349
INFO:root:[   20] Training loss: 0.01723824, Validation loss: 0.02191486, Gradient norm: 0.34767444
INFO:root:[   21] Training loss: 0.01820239, Validation loss: 0.01878807, Gradient norm: 0.40981252
INFO:root:[   22] Training loss: 0.01844038, Validation loss: 0.02053807, Gradient norm: 0.41867018
INFO:root:[   23] Training loss: 0.01738172, Validation loss: 0.01654459, Gradient norm: 0.37778133
INFO:root:[   24] Training loss: 0.01674862, Validation loss: 0.01622376, Gradient norm: 0.37000875
INFO:root:[   25] Training loss: 0.01838199, Validation loss: 0.01848439, Gradient norm: 0.44895175
INFO:root:[   26] Training loss: 0.01691576, Validation loss: 0.01691566, Gradient norm: 0.38708996
INFO:root:[   27] Training loss: 0.01663976, Validation loss: 0.01728737, Gradient norm: 0.33763382
INFO:root:[   28] Training loss: 0.01659544, Validation loss: 0.01919704, Gradient norm: 0.41026409
INFO:root:[   29] Training loss: 0.01673866, Validation loss: 0.01486430, Gradient norm: 0.38068561
INFO:root:[   30] Training loss: 0.01617317, Validation loss: 0.02212845, Gradient norm: 0.39909784
INFO:root:[   31] Training loss: 0.01652873, Validation loss: 0.01982271, Gradient norm: 0.37198446
INFO:root:[   32] Training loss: 0.01655474, Validation loss: 0.01829305, Gradient norm: 0.42363875
INFO:root:[   33] Training loss: 0.01560658, Validation loss: 0.01852536, Gradient norm: 0.35870213
INFO:root:[   34] Training loss: 0.01432766, Validation loss: 0.01528425, Gradient norm: 0.31073179
INFO:root:[   35] Training loss: 0.01629374, Validation loss: 0.01886034, Gradient norm: 0.42535711
INFO:root:[   36] Training loss: 0.01492311, Validation loss: 0.01836545, Gradient norm: 0.33919402
INFO:root:[   37] Training loss: 0.01596850, Validation loss: 0.01594762, Gradient norm: 0.41275047
INFO:root:[   38] Training loss: 0.01434775, Validation loss: 0.01592679, Gradient norm: 0.32990712
INFO:root:[   39] Training loss: 0.01594543, Validation loss: 0.01564508, Gradient norm: 0.37862146
INFO:root:[   40] Training loss: 0.01590327, Validation loss: 0.01635208, Gradient norm: 0.40169148
INFO:root:[   41] Training loss: 0.01492290, Validation loss: 0.01757576, Gradient norm: 0.31450032
INFO:root:[   42] Training loss: 0.01583321, Validation loss: 0.01741678, Gradient norm: 0.42820935
INFO:root:[   43] Training loss: 0.01445554, Validation loss: 0.01600591, Gradient norm: 0.32410579
INFO:root:[   44] Training loss: 0.01478004, Validation loss: 0.01676373, Gradient norm: 0.37880317
INFO:root:[   45] Training loss: 0.01334696, Validation loss: 0.01922464, Gradient norm: 0.30944993
INFO:root:[   46] Training loss: 0.01396369, Validation loss: 0.01542238, Gradient norm: 0.33375517
INFO:root:[   47] Training loss: 0.01467094, Validation loss: 0.01737710, Gradient norm: 0.33417387
INFO:root:[   48] Training loss: 0.01524618, Validation loss: 0.01520159, Gradient norm: 0.39965999
INFO:root:[   49] Training loss: 0.01450737, Validation loss: 0.01483263, Gradient norm: 0.37007536
INFO:root:[   50] Training loss: 0.01444656, Validation loss: 0.01614809, Gradient norm: 0.39038667
INFO:root:[   51] Training loss: 0.01265133, Validation loss: 0.01431304, Gradient norm: 0.24192358
INFO:root:[   52] Training loss: 0.01413776, Validation loss: 0.01620017, Gradient norm: 0.39339364
INFO:root:[   53] Training loss: 0.01359489, Validation loss: 0.01642546, Gradient norm: 0.30837149
INFO:root:[   54] Training loss: 0.01400842, Validation loss: 0.01580014, Gradient norm: 0.40313738
INFO:root:[   55] Training loss: 0.01357799, Validation loss: 0.01474814, Gradient norm: 0.30789975
INFO:root:[   56] Training loss: 0.01324086, Validation loss: 0.01438673, Gradient norm: 0.38259345
INFO:root:[   57] Training loss: 0.01411436, Validation loss: 0.01578613, Gradient norm: 0.41213725
INFO:root:[   58] Training loss: 0.01382469, Validation loss: 0.01373804, Gradient norm: 0.37919675
INFO:root:[   59] Training loss: 0.01373881, Validation loss: 0.01787979, Gradient norm: 0.37139868
INFO:root:[   60] Training loss: 0.01262008, Validation loss: 0.01461908, Gradient norm: 0.35515988
INFO:root:[   61] Training loss: 0.01260537, Validation loss: 0.01492708, Gradient norm: 0.35522952
INFO:root:[   62] Training loss: 0.01377801, Validation loss: 0.01649829, Gradient norm: 0.37636765
INFO:root:[   63] Training loss: 0.01257635, Validation loss: 0.01558486, Gradient norm: 0.30741485
INFO:root:[   64] Training loss: 0.01297344, Validation loss: 0.01814183, Gradient norm: 0.33935735
INFO:root:[   65] Training loss: 0.01323142, Validation loss: 0.01455144, Gradient norm: 0.36149600
INFO:root:[   66] Training loss: 0.01200011, Validation loss: 0.01739472, Gradient norm: 0.36844982
INFO:root:[   67] Training loss: 0.01377262, Validation loss: 0.01374595, Gradient norm: 0.36413112
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 1358.586s.
INFO:root:Emptying the cuda cache took 0.033s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00755
INFO:root:EnergyScoreTrain: 0.00555
INFO:root:CoverageTrain: 0.95953
INFO:root:IntervalWidthTrain: 0.02963
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01134
INFO:root:EnergyScoreValidation: 0.00849
INFO:root:CoverageValidation: 0.87058
INFO:root:IntervalWidthValidation: 0.02932
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01132
INFO:root:EnergyScoreTest: 0.00847
INFO:root:CoverageTest: 0.8679
INFO:root:IntervalWidthTest: 0.02941
INFO:root:###23 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.06711375, Validation loss: 0.04551055, Gradient norm: 0.53463733
INFO:root:[    2] Training loss: 0.04239598, Validation loss: 0.03482012, Gradient norm: 0.52500751
INFO:root:[    3] Training loss: 0.03647265, Validation loss: 0.03029932, Gradient norm: 0.55393742
INFO:root:[    4] Training loss: 0.02968757, Validation loss: 0.03433574, Gradient norm: 0.42831217
INFO:root:[    5] Training loss: 0.02949239, Validation loss: 0.02724496, Gradient norm: 0.47131642
INFO:root:[    6] Training loss: 0.02771108, Validation loss: 0.02407798, Gradient norm: 0.46574566
INFO:root:[    7] Training loss: 0.02621104, Validation loss: 0.02520677, Gradient norm: 0.44253213
INFO:root:[    8] Training loss: 0.02365264, Validation loss: 0.02179396, Gradient norm: 0.41604910
INFO:root:[    9] Training loss: 0.02384383, Validation loss: 0.02700934, Gradient norm: 0.43424323
INFO:root:[   10] Training loss: 0.02287761, Validation loss: 0.02662767, Gradient norm: 0.38636816
INFO:root:[   11] Training loss: 0.02236218, Validation loss: 0.02581277, Gradient norm: 0.34956788
INFO:root:[   12] Training loss: 0.02145247, Validation loss: 0.01879702, Gradient norm: 0.32729371
INFO:root:[   13] Training loss: 0.02139556, Validation loss: 0.02505996, Gradient norm: 0.42752055
INFO:root:[   14] Training loss: 0.02102211, Validation loss: 0.01831228, Gradient norm: 0.41648034
INFO:root:[   15] Training loss: 0.01915352, Validation loss: 0.02044028, Gradient norm: 0.33314912
INFO:root:[   16] Training loss: 0.02025601, Validation loss: 0.02678223, Gradient norm: 0.38709612
INFO:root:[   17] Training loss: 0.01869458, Validation loss: 0.02390535, Gradient norm: 0.35250326
INFO:root:[   18] Training loss: 0.01987982, Validation loss: 0.01732413, Gradient norm: 0.39949248
INFO:root:[   19] Training loss: 0.01890507, Validation loss: 0.02061533, Gradient norm: 0.38252097
INFO:root:[   20] Training loss: 0.01999733, Validation loss: 0.01908757, Gradient norm: 0.34973507
INFO:root:[   21] Training loss: 0.01765056, Validation loss: 0.01774856, Gradient norm: 0.32402230
INFO:root:[   22] Training loss: 0.01880321, Validation loss: 0.02081183, Gradient norm: 0.40031651
INFO:root:[   23] Training loss: 0.01820105, Validation loss: 0.01701012, Gradient norm: 0.34845373
INFO:root:[   24] Training loss: 0.01933932, Validation loss: 0.02181612, Gradient norm: 0.35440897
INFO:root:[   25] Training loss: 0.01870743, Validation loss: 0.01732530, Gradient norm: 0.38902442
INFO:root:[   26] Training loss: 0.01656094, Validation loss: 0.01793021, Gradient norm: 0.33580189
INFO:root:[   27] Training loss: 0.01709739, Validation loss: 0.01883634, Gradient norm: 0.33695241
INFO:root:[   28] Training loss: 0.01760870, Validation loss: 0.01690972, Gradient norm: 0.36123134
INFO:root:[   29] Training loss: 0.01655366, Validation loss: 0.01999810, Gradient norm: 0.34056317
INFO:root:[   30] Training loss: 0.01665254, Validation loss: 0.01846886, Gradient norm: 0.34732720
INFO:root:[   31] Training loss: 0.01666430, Validation loss: 0.01637358, Gradient norm: 0.35835062
INFO:root:[   32] Training loss: 0.01645851, Validation loss: 0.01761348, Gradient norm: 0.33986874
INFO:root:[   33] Training loss: 0.01689090, Validation loss: 0.01605290, Gradient norm: 0.32474231
INFO:root:[   34] Training loss: 0.01613741, Validation loss: 0.02673531, Gradient norm: 0.33130697
INFO:root:[   35] Training loss: 0.01715627, Validation loss: 0.01790680, Gradient norm: 0.36939057
INFO:root:[   36] Training loss: 0.01618118, Validation loss: 0.01781307, Gradient norm: 0.33942136
INFO:root:[   37] Training loss: 0.01664103, Validation loss: 0.01665978, Gradient norm: 0.40437449
INFO:root:[   38] Training loss: 0.01536858, Validation loss: 0.01661381, Gradient norm: 0.26093670
INFO:root:[   39] Training loss: 0.01646984, Validation loss: 0.01945859, Gradient norm: 0.33516818
INFO:root:[   40] Training loss: 0.01571648, Validation loss: 0.01529848, Gradient norm: 0.40174403
INFO:root:[   41] Training loss: 0.01492160, Validation loss: 0.01696520, Gradient norm: 0.30889705
INFO:root:[   42] Training loss: 0.01571077, Validation loss: 0.01686514, Gradient norm: 0.35274068
INFO:root:[   43] Training loss: 0.01584437, Validation loss: 0.01829801, Gradient norm: 0.38676157
INFO:root:[   44] Training loss: 0.01484670, Validation loss: 0.01561916, Gradient norm: 0.28085382
INFO:root:[   45] Training loss: 0.01519435, Validation loss: 0.01647742, Gradient norm: 0.36580680
INFO:root:[   46] Training loss: 0.01610474, Validation loss: 0.01573932, Gradient norm: 0.36773677
INFO:root:[   47] Training loss: 0.01486782, Validation loss: 0.01720917, Gradient norm: 0.35129562
INFO:root:[   48] Training loss: 0.01546084, Validation loss: 0.01662117, Gradient norm: 0.38184684
INFO:root:[   49] Training loss: 0.01607127, Validation loss: 0.01882801, Gradient norm: 0.41455571
INFO:root:[   50] Training loss: 0.01500895, Validation loss: 0.01624878, Gradient norm: 0.32278855
INFO:root:[   51] Training loss: 0.01471557, Validation loss: 0.01628793, Gradient norm: 0.35967596
INFO:root:[   52] Training loss: 0.01483420, Validation loss: 0.01639309, Gradient norm: 0.36834966
INFO:root:[   53] Training loss: 0.01535052, Validation loss: 0.01883645, Gradient norm: 0.34853763
INFO:root:[   54] Training loss: 0.01424113, Validation loss: 0.01532001, Gradient norm: 0.29891144
INFO:root:[   55] Training loss: 0.01444150, Validation loss: 0.01711078, Gradient norm: 0.33705450
INFO:root:[   56] Training loss: 0.01473744, Validation loss: 0.01588517, Gradient norm: 0.33560770
INFO:root:[   57] Training loss: 0.01418367, Validation loss: 0.01549179, Gradient norm: 0.34093985
INFO:root:[   58] Training loss: 0.01413112, Validation loss: 0.01695659, Gradient norm: 0.30299458
INFO:root:[   59] Training loss: 0.01446487, Validation loss: 0.01637860, Gradient norm: 0.33197030
INFO:root:[   60] Training loss: 0.01474403, Validation loss: 0.01508947, Gradient norm: 0.40505783
INFO:root:[   61] Training loss: 0.01279321, Validation loss: 0.01623846, Gradient norm: 0.26415718
INFO:root:[   62] Training loss: 0.01463795, Validation loss: 0.01545018, Gradient norm: 0.35982275
INFO:root:[   63] Training loss: 0.01474069, Validation loss: 0.01633091, Gradient norm: 0.33763656
INFO:root:[   64] Training loss: 0.01455845, Validation loss: 0.01405505, Gradient norm: 0.35869949
INFO:root:[   65] Training loss: 0.01345248, Validation loss: 0.01668035, Gradient norm: 0.34510723
INFO:root:[   66] Training loss: 0.01480538, Validation loss: 0.01504180, Gradient norm: 0.37870416
INFO:root:[   67] Training loss: 0.01373749, Validation loss: 0.01645520, Gradient norm: 0.38031682
INFO:root:[   68] Training loss: 0.01341145, Validation loss: 0.01558179, Gradient norm: 0.27551713
INFO:root:[   69] Training loss: 0.01292154, Validation loss: 0.01452422, Gradient norm: 0.30703735
INFO:root:[   70] Training loss: 0.01389775, Validation loss: 0.01694138, Gradient norm: 0.35961264
INFO:root:[   71] Training loss: 0.01400694, Validation loss: 0.01583412, Gradient norm: 0.35572614
INFO:root:[   72] Training loss: 0.01324003, Validation loss: 0.01742608, Gradient norm: 0.30673040
INFO:root:[   73] Training loss: 0.01304051, Validation loss: 0.01504441, Gradient norm: 0.36298253
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 1480.894s.
INFO:root:Emptying the cuda cache took 0.034s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00829
INFO:root:EnergyScoreTrain: 0.00611
INFO:root:CoverageTrain: 0.95665
INFO:root:IntervalWidthTrain: 0.03198
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01109
INFO:root:EnergyScoreValidation: 0.00831
INFO:root:CoverageValidation: 0.89522
INFO:root:IntervalWidthValidation: 0.03163
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0114
INFO:root:EnergyScoreTest: 0.00857
INFO:root:CoverageTest: 0.88861
INFO:root:IntervalWidthTest: 0.03175
INFO:root:###24 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 256, 'lifting_channels': 256, 'n_modes': (12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 710305
INFO:root:Memory allocated: 71303168
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.07288789, Validation loss: 0.05194833, Gradient norm: 0.49823126
INFO:root:[    2] Training loss: 0.04397928, Validation loss: 0.04096844, Gradient norm: 0.42465667
INFO:root:[    3] Training loss: 0.03903172, Validation loss: 0.03256607, Gradient norm: 0.50604496
INFO:root:[    4] Training loss: 0.03482785, Validation loss: 0.03506039, Gradient norm: 0.50136778
INFO:root:[    5] Training loss: 0.03244753, Validation loss: 0.03460003, Gradient norm: 0.49955190
INFO:root:[    6] Training loss: 0.03015715, Validation loss: 0.02943485, Gradient norm: 0.46310000
INFO:root:[    7] Training loss: 0.02615768, Validation loss: 0.02546437, Gradient norm: 0.32206938
INFO:root:[    8] Training loss: 0.02722546, Validation loss: 0.02372272, Gradient norm: 0.45513108
INFO:root:[    9] Training loss: 0.02587982, Validation loss: 0.02542880, Gradient norm: 0.41803807
INFO:root:[   10] Training loss: 0.02490648, Validation loss: 0.02627526, Gradient norm: 0.40574384
INFO:root:[   11] Training loss: 0.02395641, Validation loss: 0.02350819, Gradient norm: 0.36673207
INFO:root:[   12] Training loss: 0.02341968, Validation loss: 0.02169798, Gradient norm: 0.38260596
INFO:root:[   13] Training loss: 0.02363374, Validation loss: 0.02115633, Gradient norm: 0.40716218
INFO:root:[   14] Training loss: 0.02345008, Validation loss: 0.02107642, Gradient norm: 0.39600451
INFO:root:[   15] Training loss: 0.02281839, Validation loss: 0.02145877, Gradient norm: 0.38763110
INFO:root:[   16] Training loss: 0.02193629, Validation loss: 0.02339302, Gradient norm: 0.37059614
INFO:root:[   17] Training loss: 0.02206135, Validation loss: 0.02694225, Gradient norm: 0.39950275
INFO:root:[   18] Training loss: 0.02088393, Validation loss: 0.01957444, Gradient norm: 0.35092505
INFO:root:[   19] Training loss: 0.02055882, Validation loss: 0.02030456, Gradient norm: 0.36571314
INFO:root:[   20] Training loss: 0.02011789, Validation loss: 0.02051061, Gradient norm: 0.33963380
INFO:root:[   21] Training loss: 0.02098357, Validation loss: 0.01874665, Gradient norm: 0.40677954
INFO:root:[   22] Training loss: 0.02088046, Validation loss: 0.02138635, Gradient norm: 0.37804376
INFO:root:[   23] Training loss: 0.01990455, Validation loss: 0.01860015, Gradient norm: 0.36119356
INFO:root:[   24] Training loss: 0.02005584, Validation loss: 0.01778634, Gradient norm: 0.38995699
INFO:root:[   25] Training loss: 0.01914404, Validation loss: 0.01977049, Gradient norm: 0.32902746
INFO:root:[   26] Training loss: 0.02026619, Validation loss: 0.01704885, Gradient norm: 0.38404172
INFO:root:[   27] Training loss: 0.01960476, Validation loss: 0.01920228, Gradient norm: 0.36977484
INFO:root:[   28] Training loss: 0.01730851, Validation loss: 0.02118863, Gradient norm: 0.28864739
INFO:root:[   29] Training loss: 0.02028259, Validation loss: 0.01910917, Gradient norm: 0.38598328
INFO:root:[   30] Training loss: 0.01888226, Validation loss: 0.01779891, Gradient norm: 0.37366093
INFO:root:[   31] Training loss: 0.01770037, Validation loss: 0.01998501, Gradient norm: 0.32501207
INFO:root:[   32] Training loss: 0.01944644, Validation loss: 0.02195882, Gradient norm: 0.36647695
INFO:root:[   33] Training loss: 0.01894835, Validation loss: 0.02140825, Gradient norm: 0.35152411
INFO:root:[   34] Training loss: 0.01859042, Validation loss: 0.02007749, Gradient norm: 0.37878516
INFO:root:[   35] Training loss: 0.01861868, Validation loss: 0.01879329, Gradient norm: 0.38308673
INFO:root:[   36] Training loss: 0.01792842, Validation loss: 0.02092070, Gradient norm: 0.37633192
INFO:root:[   37] Training loss: 0.01747095, Validation loss: 0.01999886, Gradient norm: 0.32779999
INFO:root:[   38] Training loss: 0.01885504, Validation loss: 0.01750816, Gradient norm: 0.35758405
INFO:root:[   39] Training loss: 0.01641865, Validation loss: 0.01695899, Gradient norm: 0.25126819
INFO:root:[   40] Training loss: 0.01825805, Validation loss: 0.01983973, Gradient norm: 0.37718396
INFO:root:[   41] Training loss: 0.01717064, Validation loss: 0.01580185, Gradient norm: 0.34140464
INFO:root:[   42] Training loss: 0.01788450, Validation loss: 0.01642520, Gradient norm: 0.35090226
INFO:root:[   43] Training loss: 0.01651825, Validation loss: 0.01778933, Gradient norm: 0.32428410
INFO:root:[   44] Training loss: 0.01722848, Validation loss: 0.01683333, Gradient norm: 0.35411058
INFO:root:[   45] Training loss: 0.01758671, Validation loss: 0.01921833, Gradient norm: 0.39026982
INFO:root:[   46] Training loss: 0.01711851, Validation loss: 0.01590233, Gradient norm: 0.36243884
INFO:root:[   47] Training loss: 0.01715279, Validation loss: 0.01935881, Gradient norm: 0.38147194
INFO:root:[   48] Training loss: 0.01728530, Validation loss: 0.02009648, Gradient norm: 0.31036303
INFO:root:[   49] Training loss: 0.01641329, Validation loss: 0.01824667, Gradient norm: 0.30573917
INFO:root:[   50] Training loss: 0.01686192, Validation loss: 0.01876838, Gradient norm: 0.33258367
INFO:root:[   51] Training loss: 0.01628086, Validation loss: 0.01955287, Gradient norm: 0.37796313
INFO:root:[   52] Training loss: 0.01577688, Validation loss: 0.01657548, Gradient norm: 0.30002691
INFO:root:[   53] Training loss: 0.01629157, Validation loss: 0.02173690, Gradient norm: 0.36050138
