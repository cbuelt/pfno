[META]
results_path = results/
data_path = data/
experiment_name = swe_uno

[TRAININGPARAMETERS]
model = 'UNO'
uncertainty_quantification = ['scoring-rule-dropout', 'scoring-rule-reparam', 'laplace', 'dropout']  # 'laplace', 'dropout', 'scoring-rule-dropout', 'scoring-rule-reparam'
batch_size =  [16]
n_epochs = 1000
early_stopping = 10
init = 'default' # he, xavier, default
learning_rate = 0.001
lr_schedule = 'step' # 'no', 'step'
optimizer = 'adam'
gradient_clipping = 1
layer_normalization = True
data_loader_pin_memory = False 
data_loader_num_workers = [0]
distributed_training = False
alpha = 0.05
n_samples_uq = 100
weight_decay = 0
### Model Parameters
dropout = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]
fourier_dropout = None
hidden_channels = 16
projection_channels = 32
lifting_channels = 32
uno_out_channels = [16, 32, 64, 128, 64, 32, 16]  # has to be a list (hyperparameter search does not iterate over it)
uno_scalings= [[1.0,0.75,0.75], [1.0,0.67,0.67], [1.0,0.5,0.5], [1.0,1.0,1.0], [1.0, 2.0,2.0], [1.0,1.5,1.5], [1.0, 1.33, 1.33]]  # has to be a list (hyperparameter search does not iterate over it)
uno_n_modes= [[4,20,20],[4,14,14],[4,6,6], [7,6,6], [7,6,6], [10,14,14], [10,20,20]]  # has to be a list (hyperparameter search does not iterate over it)
### PUNO
n_samples = 3

[DATAPARAMETERS]
dataset_name = ['SWE']
max_training_set_size = 1000
downscaling_factor = 1  # int
temporal_downscaling = 4
init_steps = 10 # Initial steps
t_start = 0 # Where to start input
pred_horizon = 10 # Prediction horizon across time domain
ood = False # Whether to use out of distribution dataset