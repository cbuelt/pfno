INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10, 'ood': False}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 75497472
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.37673787, Validation loss: 0.20782198, Gradient norm: 8.54507647
INFO:root:[    2] Training loss: 0.12435851, Validation loss: 0.09065894, Gradient norm: 6.06400515
INFO:root:[    3] Training loss: 0.10518878, Validation loss: 0.13533604, Gradient norm: 5.24475014
INFO:root:[    4] Training loss: 0.13866448, Validation loss: 0.12749270, Gradient norm: 8.89703778
INFO:root:[    5] Training loss: 0.10891427, Validation loss: 0.12143712, Gradient norm: 7.50815050
INFO:root:[    6] Training loss: 0.09835728, Validation loss: 0.09570853, Gradient norm: 6.67639037
INFO:root:[    7] Training loss: 0.08399413, Validation loss: 0.07719263, Gradient norm: 4.03211556
INFO:root:[    8] Training loss: 0.07955402, Validation loss: 0.09683640, Gradient norm: 4.12517692
INFO:root:[    9] Training loss: 0.09164685, Validation loss: 0.07637440, Gradient norm: 6.74058916
INFO:root:[   10] Training loss: 0.08034239, Validation loss: 0.07528760, Gradient norm: 5.50676817
INFO:root:[   11] Training loss: 0.07547654, Validation loss: 0.07493145, Gradient norm: 4.79779534
INFO:root:[   12] Training loss: 0.07305622, Validation loss: 0.06826272, Gradient norm: 4.76039911
INFO:root:[   13] Training loss: 0.06750227, Validation loss: 0.06750119, Gradient norm: 4.03684272
INFO:root:[   14] Training loss: 0.06933264, Validation loss: 0.05594874, Gradient norm: 4.49789230
INFO:root:[   15] Training loss: 0.06671435, Validation loss: 0.05816504, Gradient norm: 3.97870275
INFO:root:[   16] Training loss: 0.06332676, Validation loss: 0.05740321, Gradient norm: 3.66006030
INFO:root:[   17] Training loss: 0.06028372, Validation loss: 0.06880294, Gradient norm: 3.58255742
INFO:root:[   18] Training loss: 0.05990594, Validation loss: 0.06726029, Gradient norm: 3.69126072
INFO:root:[   19] Training loss: 0.05800987, Validation loss: 0.05630621, Gradient norm: 3.21860351
INFO:root:[   20] Training loss: 0.05483743, Validation loss: 0.05644467, Gradient norm: 3.09296668
INFO:root:[   21] Training loss: 0.05375325, Validation loss: 0.05222064, Gradient norm: 2.92644291
INFO:root:[   22] Training loss: 0.05373037, Validation loss: 0.05255427, Gradient norm: 3.05868499
INFO:root:[   23] Training loss: 0.05199066, Validation loss: 0.04804670, Gradient norm: 2.99060582
INFO:root:[   24] Training loss: 0.05185835, Validation loss: 0.05195949, Gradient norm: 2.98959117
INFO:root:[   25] Training loss: 0.04985089, Validation loss: 0.05156105, Gradient norm: 2.48646908
INFO:root:[   26] Training loss: 0.05238445, Validation loss: 0.04751206, Gradient norm: 3.13216566
INFO:root:[   27] Training loss: 0.04793178, Validation loss: 0.04573988, Gradient norm: 2.13872641
INFO:root:[   28] Training loss: 0.04682617, Validation loss: 0.04922644, Gradient norm: 2.25135391
INFO:root:[   29] Training loss: 0.04547292, Validation loss: 0.04422339, Gradient norm: 2.19019040
INFO:root:[   30] Training loss: 0.04789699, Validation loss: 0.04877114, Gradient norm: 2.38546704
INFO:root:[   31] Training loss: 0.04526736, Validation loss: 0.04544652, Gradient norm: 1.75715816
INFO:root:[   32] Training loss: 0.04412479, Validation loss: 0.04011583, Gradient norm: 1.89311605
INFO:root:[   33] Training loss: 0.04369921, Validation loss: 0.03875179, Gradient norm: 2.32752222
INFO:root:[   34] Training loss: 0.04250638, Validation loss: 0.04317823, Gradient norm: 2.03370939
INFO:root:[   35] Training loss: 0.04282676, Validation loss: 0.03822499, Gradient norm: 1.99435853
INFO:root:[   36] Training loss: 0.04295153, Validation loss: 0.03997897, Gradient norm: 1.96364359
INFO:root:[   37] Training loss: 0.04171776, Validation loss: 0.03789187, Gradient norm: 2.00761351
INFO:root:[   38] Training loss: 0.04232019, Validation loss: 0.04180720, Gradient norm: 2.07734846
INFO:root:[   39] Training loss: 0.04113558, Validation loss: 0.04468295, Gradient norm: 1.62062345
INFO:root:[   40] Training loss: 0.04201967, Validation loss: 0.03646850, Gradient norm: 2.14721395
INFO:root:[   41] Training loss: 0.04004985, Validation loss: 0.04268046, Gradient norm: 1.75861936
INFO:root:[   42] Training loss: 0.04002363, Validation loss: 0.04001908, Gradient norm: 1.82925697
INFO:root:[   43] Training loss: 0.03818916, Validation loss: 0.04171820, Gradient norm: 1.71243730
INFO:root:[   44] Training loss: 0.03844848, Validation loss: 0.03864176, Gradient norm: 1.88544786
INFO:root:[   45] Training loss: 0.03850443, Validation loss: 0.03747724, Gradient norm: 1.66256338
INFO:root:[   46] Training loss: 0.03972663, Validation loss: 0.03577731, Gradient norm: 1.55459852
INFO:root:[   47] Training loss: 0.03872338, Validation loss: 0.03809941, Gradient norm: 1.91967200
INFO:root:[   48] Training loss: 0.03844777, Validation loss: 0.04097539, Gradient norm: 1.79884905
INFO:root:[   49] Training loss: 0.03778035, Validation loss: 0.03421621, Gradient norm: 1.85358648
INFO:root:[   50] Training loss: 0.03715589, Validation loss: 0.03812729, Gradient norm: 1.80956374
INFO:root:[   51] Training loss: 0.03713319, Validation loss: 0.03914172, Gradient norm: 1.85319367
INFO:root:[   52] Training loss: 0.03924662, Validation loss: 0.04217982, Gradient norm: 1.62482542
INFO:root:[   53] Training loss: 0.03844135, Validation loss: 0.03752824, Gradient norm: 1.73306582
INFO:root:[   54] Training loss: 0.03740019, Validation loss: 0.03679681, Gradient norm: 1.62539806
INFO:root:[   55] Training loss: 0.03677582, Validation loss: 0.04050073, Gradient norm: 1.63506635
INFO:root:[   56] Training loss: 0.03684384, Validation loss: 0.03290448, Gradient norm: 1.59848056
INFO:root:[   57] Training loss: 0.03684916, Validation loss: 0.03256149, Gradient norm: 1.84529291
INFO:root:[   58] Training loss: 0.03610134, Validation loss: 0.03804198, Gradient norm: 1.51148671
INFO:root:[   59] Training loss: 0.03723151, Validation loss: 0.03438329, Gradient norm: 1.59166298
INFO:root:[   60] Training loss: 0.03638431, Validation loss: 0.03622896, Gradient norm: 1.42172518
INFO:root:[   61] Training loss: 0.03477971, Validation loss: 0.03418715, Gradient norm: 1.61646909
INFO:root:[   62] Training loss: 0.03446510, Validation loss: 0.03381621, Gradient norm: 1.74493815
INFO:root:[   63] Training loss: 0.03491813, Validation loss: 0.03640336, Gradient norm: 1.59260449
INFO:root:[   64] Training loss: 0.03516010, Validation loss: 0.03278438, Gradient norm: 1.64940057
INFO:root:[   65] Training loss: 0.03502223, Validation loss: 0.03603477, Gradient norm: 1.61536783
INFO:root:[   66] Training loss: 0.03557890, Validation loss: 0.03452219, Gradient norm: 1.81121791
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 3591.086s.
INFO:root:Emptying the cuda cache took 0.09s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04234
INFO:root:EnergyScoreTrain: 0.03269
INFO:root:CoverageTrain: 0.98581
INFO:root:IntervalWidthTrain: 0.08417
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04213
INFO:root:EnergyScoreValidation: 0.03259
INFO:root:CoverageValidation: 0.98614
INFO:root:IntervalWidthValidation: 0.08419
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04207
INFO:root:EnergyScoreTest: 0.03245
INFO:root:CoverageTest: 0.98573
INFO:root:IntervalWidthTest: 0.08346
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1516240896
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.40947662, Validation loss: 0.25335962, Gradient norm: 6.19872824
INFO:root:[    2] Training loss: 0.17261926, Validation loss: 0.13616800, Gradient norm: 4.11978467
INFO:root:[    3] Training loss: 0.14191638, Validation loss: 0.14868155, Gradient norm: 3.57696925
INFO:root:[    4] Training loss: 0.12961982, Validation loss: 0.10622628, Gradient norm: 3.47354141
INFO:root:[    5] Training loss: 0.11585133, Validation loss: 0.11566324, Gradient norm: 3.25522805
INFO:root:[    6] Training loss: 0.10491628, Validation loss: 0.10832766, Gradient norm: 3.04624019
INFO:root:[    7] Training loss: 0.09348075, Validation loss: 0.09555961, Gradient norm: 2.41671156
INFO:root:[    8] Training loss: 0.08777264, Validation loss: 0.08273703, Gradient norm: 2.20994475
INFO:root:[    9] Training loss: 0.07838164, Validation loss: 0.08233956, Gradient norm: 1.89719191
INFO:root:[   10] Training loss: 0.07366217, Validation loss: 0.07290258, Gradient norm: 1.22993920
INFO:root:[   11] Training loss: 0.07230886, Validation loss: 0.07031123, Gradient norm: 1.36288955
INFO:root:[   12] Training loss: 0.06836193, Validation loss: 0.06477365, Gradient norm: 1.54856336
INFO:root:[   13] Training loss: 0.06424299, Validation loss: 0.06586167, Gradient norm: 1.07976171
INFO:root:[   14] Training loss: 0.06340411, Validation loss: 0.05942382, Gradient norm: 1.03066802
INFO:root:[   15] Training loss: 0.06118259, Validation loss: 0.06207883, Gradient norm: 0.93962758
INFO:root:[   16] Training loss: 0.05979496, Validation loss: 0.05796376, Gradient norm: 1.03809866
INFO:root:[   17] Training loss: 0.05799114, Validation loss: 0.05590654, Gradient norm: 0.98406526
INFO:root:[   18] Training loss: 0.05783960, Validation loss: 0.05506911, Gradient norm: 1.19299501
INFO:root:[   19] Training loss: 0.05665850, Validation loss: 0.05985834, Gradient norm: 1.06451990
INFO:root:[   20] Training loss: 0.05660516, Validation loss: 0.05370063, Gradient norm: 0.95662038
INFO:root:[   21] Training loss: 0.05516228, Validation loss: 0.05811258, Gradient norm: 0.93129131
INFO:root:[   22] Training loss: 0.05333461, Validation loss: 0.05096071, Gradient norm: 0.91722029
INFO:root:[   23] Training loss: 0.05399442, Validation loss: 0.05059092, Gradient norm: 0.74251079
INFO:root:[   24] Training loss: 0.05208295, Validation loss: 0.05190416, Gradient norm: 0.88049910
INFO:root:[   25] Training loss: 0.05175251, Validation loss: 0.05100825, Gradient norm: 0.68947920
INFO:root:[   26] Training loss: 0.05169893, Validation loss: 0.04903224, Gradient norm: 0.84903163
INFO:root:[   27] Training loss: 0.05056624, Validation loss: 0.05251460, Gradient norm: 0.86873014
INFO:root:[   28] Training loss: 0.05039194, Validation loss: 0.04849979, Gradient norm: 0.60350631
INFO:root:[   29] Training loss: 0.04956742, Validation loss: 0.04768751, Gradient norm: 0.79505469
INFO:root:[   30] Training loss: 0.04915815, Validation loss: 0.04754014, Gradient norm: 0.75808288
INFO:root:[   31] Training loss: 0.04985411, Validation loss: 0.04974933, Gradient norm: 0.79643703
INFO:root:[   32] Training loss: 0.04863857, Validation loss: 0.04692381, Gradient norm: 0.82324160
INFO:root:[   33] Training loss: 0.04890612, Validation loss: 0.05111449, Gradient norm: 0.88991052
INFO:root:[   34] Training loss: 0.04797418, Validation loss: 0.04513961, Gradient norm: 0.60420952
INFO:root:[   35] Training loss: 0.04648316, Validation loss: 0.04489332, Gradient norm: 0.78224457
INFO:root:[   36] Training loss: 0.04731298, Validation loss: 0.04612242, Gradient norm: 0.88660570
INFO:root:[   37] Training loss: 0.04620471, Validation loss: 0.04812574, Gradient norm: 0.91697331
INFO:root:[   38] Training loss: 0.04598500, Validation loss: 0.04526507, Gradient norm: 1.03312936
INFO:root:[   39] Training loss: 0.04634823, Validation loss: 0.04695047, Gradient norm: 0.92302115
INFO:root:[   40] Training loss: 0.04539279, Validation loss: 0.04308729, Gradient norm: 0.90054405
INFO:root:[   41] Training loss: 0.04391190, Validation loss: 0.04526437, Gradient norm: 0.58958813
INFO:root:[   42] Training loss: 0.04469079, Validation loss: 0.04494427, Gradient norm: 1.13636892
INFO:root:[   43] Training loss: 0.04396960, Validation loss: 0.04301341, Gradient norm: 0.89750259
INFO:root:[   44] Training loss: 0.04292474, Validation loss: 0.04302438, Gradient norm: 0.66845318
INFO:root:[   45] Training loss: 0.04371959, Validation loss: 0.04396513, Gradient norm: 1.13537095
INFO:root:[   46] Training loss: 0.04272241, Validation loss: 0.04303502, Gradient norm: 0.81350380
INFO:root:[   47] Training loss: 0.04335521, Validation loss: 0.04464960, Gradient norm: 1.00463410
INFO:root:[   48] Training loss: 0.04263704, Validation loss: 0.04428929, Gradient norm: 1.00630967
INFO:root:[   49] Training loss: 0.04146516, Validation loss: 0.04069511, Gradient norm: 1.07833583
INFO:root:[   50] Training loss: 0.04189823, Validation loss: 0.04010108, Gradient norm: 1.10788943
INFO:root:[   51] Training loss: 0.04113738, Validation loss: 0.04159272, Gradient norm: 0.97812973
INFO:root:[   52] Training loss: 0.04152780, Validation loss: 0.03957093, Gradient norm: 0.83231615
INFO:root:[   53] Training loss: 0.04080349, Validation loss: 0.03949875, Gradient norm: 0.95325027
INFO:root:[   54] Training loss: 0.04020152, Validation loss: 0.04060621, Gradient norm: 1.03975388
INFO:root:[   55] Training loss: 0.04085607, Validation loss: 0.03792825, Gradient norm: 0.75797676
INFO:root:[   56] Training loss: 0.03876134, Validation loss: 0.03881957, Gradient norm: 0.73899583
INFO:root:[   57] Training loss: 0.03916689, Validation loss: 0.03771837, Gradient norm: 1.09217155
INFO:root:[   58] Training loss: 0.04016932, Validation loss: 0.03916339, Gradient norm: 1.10844390
INFO:root:[   59] Training loss: 0.03871841, Validation loss: 0.04089395, Gradient norm: 1.16034703
INFO:root:[   60] Training loss: 0.03902717, Validation loss: 0.03945104, Gradient norm: 1.08685450
INFO:root:[   61] Training loss: 0.03877629, Validation loss: 0.04028747, Gradient norm: 1.12397198
INFO:root:[   62] Training loss: 0.03868254, Validation loss: 0.03608321, Gradient norm: 1.04236269
INFO:root:[   63] Training loss: 0.03794417, Validation loss: 0.03894048, Gradient norm: 1.22415149
INFO:root:[   64] Training loss: 0.03776965, Validation loss: 0.03768204, Gradient norm: 1.22161214
INFO:root:[   65] Training loss: 0.03710638, Validation loss: 0.03864808, Gradient norm: 1.08612709
INFO:root:[   66] Training loss: 0.03725366, Validation loss: 0.03801891, Gradient norm: 1.19431208
INFO:root:[   67] Training loss: 0.03766869, Validation loss: 0.03744121, Gradient norm: 1.08880061
INFO:root:[   68] Training loss: 0.03734687, Validation loss: 0.03454778, Gradient norm: 1.31840326
INFO:root:[   69] Training loss: 0.03617792, Validation loss: 0.03559880, Gradient norm: 1.21890763
INFO:root:[   70] Training loss: 0.03698043, Validation loss: 0.03827985, Gradient norm: 1.39686001
INFO:root:[   71] Training loss: 0.03642789, Validation loss: 0.03551870, Gradient norm: 1.21308767
INFO:root:[   72] Training loss: 0.03601863, Validation loss: 0.03739632, Gradient norm: 1.20729562
INFO:root:[   73] Training loss: 0.03541187, Validation loss: 0.03427141, Gradient norm: 1.21866485
INFO:root:[   74] Training loss: 0.03469923, Validation loss: 0.03492810, Gradient norm: 1.04020696
INFO:root:[   75] Training loss: 0.03546099, Validation loss: 0.03715531, Gradient norm: 1.29712251
INFO:root:[   76] Training loss: 0.03516945, Validation loss: 0.03589821, Gradient norm: 1.30994639
INFO:root:[   77] Training loss: 0.03565858, Validation loss: 0.03436005, Gradient norm: 1.38769796
INFO:root:[   78] Training loss: 0.03585160, Validation loss: 0.03767252, Gradient norm: 1.18739635
INFO:root:[   79] Training loss: 0.03494223, Validation loss: 0.03514644, Gradient norm: 1.34023199
INFO:root:[   80] Training loss: 0.03479849, Validation loss: 0.03274169, Gradient norm: 1.38617604
INFO:root:[   81] Training loss: 0.03331688, Validation loss: 0.03472574, Gradient norm: 1.33399160
INFO:root:[   82] Training loss: 0.03325572, Validation loss: 0.03362449, Gradient norm: 1.35279122
INFO:root:[   83] Training loss: 0.03452141, Validation loss: 0.03353547, Gradient norm: 1.30482932
INFO:root:[   84] Training loss: 0.03309431, Validation loss: 0.03041156, Gradient norm: 1.44907301
INFO:root:[   85] Training loss: 0.03319082, Validation loss: 0.03208419, Gradient norm: 1.43017669
INFO:root:[   86] Training loss: 0.03329728, Validation loss: 0.03189397, Gradient norm: 1.40906145
INFO:root:[   87] Training loss: 0.03345977, Validation loss: 0.03231979, Gradient norm: 1.31758900
INFO:root:[   88] Training loss: 0.03256414, Validation loss: 0.03358789, Gradient norm: 1.48048835
INFO:root:[   89] Training loss: 0.03251895, Validation loss: 0.03470007, Gradient norm: 1.43216651
INFO:root:[   90] Training loss: 0.03316830, Validation loss: 0.03489129, Gradient norm: 1.52434654
INFO:root:[   91] Training loss: 0.03771998, Validation loss: 0.03604789, Gradient norm: 1.67504425
INFO:root:[   92] Training loss: 0.03240447, Validation loss: 0.03127424, Gradient norm: 1.56333693
INFO:root:[   93] Training loss: 0.03213239, Validation loss: 0.03011662, Gradient norm: 1.53070467
INFO:root:[   94] Training loss: 0.03172935, Validation loss: 0.03177819, Gradient norm: 1.54668688
INFO:root:[   95] Training loss: 0.03118636, Validation loss: 0.03147325, Gradient norm: 1.41982883
INFO:root:[   96] Training loss: 0.03094138, Validation loss: 0.03163304, Gradient norm: 1.32539207
INFO:root:[   97] Training loss: 0.03183700, Validation loss: 0.03106320, Gradient norm: 1.59025631
INFO:root:[   98] Training loss: 0.03180919, Validation loss: 0.03007766, Gradient norm: 1.55138853
INFO:root:[   99] Training loss: 0.03082679, Validation loss: 0.03150892, Gradient norm: 1.54008096
INFO:root:[  100] Training loss: 0.03194146, Validation loss: 0.03051567, Gradient norm: 1.55920379
INFO:root:[  101] Training loss: 0.03079996, Validation loss: 0.03095820, Gradient norm: 1.35421604
INFO:root:[  102] Training loss: 0.03110911, Validation loss: 0.03094416, Gradient norm: 1.60209130
INFO:root:[  103] Training loss: 0.03082355, Validation loss: 0.02905928, Gradient norm: 1.58924370
INFO:root:[  104] Training loss: 0.03106256, Validation loss: 0.02857611, Gradient norm: 1.78070308
INFO:root:[  105] Training loss: 0.03058859, Validation loss: 0.02952136, Gradient norm: 1.72835147
INFO:root:[  106] Training loss: 0.03056727, Validation loss: 0.02866886, Gradient norm: 1.53442249
INFO:root:[  107] Training loss: 0.02979246, Validation loss: 0.02783498, Gradient norm: 1.70738202
INFO:root:[  108] Training loss: 0.02938060, Validation loss: 0.03117415, Gradient norm: 1.67366137
INFO:root:[  109] Training loss: 0.02978059, Validation loss: 0.03011235, Gradient norm: 1.62162241
INFO:root:[  110] Training loss: 0.02918408, Validation loss: 0.02946296, Gradient norm: 1.60492759
INFO:root:[  111] Training loss: 0.03011181, Validation loss: 0.02935657, Gradient norm: 1.57192949
INFO:root:[  112] Training loss: 0.02983752, Validation loss: 0.02950225, Gradient norm: 1.61370374
INFO:root:[  113] Training loss: 0.02859485, Validation loss: 0.02713180, Gradient norm: 1.70406277
INFO:root:[  114] Training loss: 0.02918929, Validation loss: 0.02959720, Gradient norm: 1.78189864
INFO:root:[  115] Training loss: 0.03018988, Validation loss: 0.02880555, Gradient norm: 1.86499874
INFO:root:[  116] Training loss: 0.02822429, Validation loss: 0.02819203, Gradient norm: 1.70040199
INFO:root:[  117] Training loss: 0.02830623, Validation loss: 0.02775726, Gradient norm: 1.84896472
INFO:root:[  118] Training loss: 0.02838581, Validation loss: 0.02825855, Gradient norm: 1.79164827
INFO:root:[  119] Training loss: 0.02878267, Validation loss: 0.03043330, Gradient norm: 1.68947659
INFO:root:[  120] Training loss: 0.02830724, Validation loss: 0.02713196, Gradient norm: 1.73905911
INFO:root:[  121] Training loss: 0.02746475, Validation loss: 0.02668773, Gradient norm: 1.71715282
INFO:root:[  122] Training loss: 0.02794606, Validation loss: 0.02937776, Gradient norm: 1.78262399
INFO:root:[  123] Training loss: 0.02920825, Validation loss: 0.02757126, Gradient norm: 1.87446165
INFO:root:[  124] Training loss: 0.02786528, Validation loss: 0.02733068, Gradient norm: 1.84116020
INFO:root:[  125] Training loss: 0.02747059, Validation loss: 0.02776919, Gradient norm: 1.92923252
INFO:root:[  126] Training loss: 0.02856086, Validation loss: 0.02602630, Gradient norm: 1.85696043
INFO:root:[  127] Training loss: 0.02738625, Validation loss: 0.02765622, Gradient norm: 1.65498212
INFO:root:[  128] Training loss: 0.02689692, Validation loss: 0.02726440, Gradient norm: 1.87460888
INFO:root:[  129] Training loss: 0.02780603, Validation loss: 0.02700926, Gradient norm: 2.01286355
INFO:root:[  130] Training loss: 0.02666560, Validation loss: 0.02483427, Gradient norm: 1.90010791
INFO:root:[  131] Training loss: 0.02723204, Validation loss: 0.02517925, Gradient norm: 2.06656433
INFO:root:[  132] Training loss: 0.02806383, Validation loss: 0.02884747, Gradient norm: 1.79633041
INFO:root:[  133] Training loss: 0.02783416, Validation loss: 0.02705249, Gradient norm: 1.95224461
INFO:root:[  134] Training loss: 0.02759793, Validation loss: 0.02773798, Gradient norm: 1.79979325
INFO:root:[  135] Training loss: 0.02652209, Validation loss: 0.02468696, Gradient norm: 1.80798361
INFO:root:[  136] Training loss: 0.02650224, Validation loss: 0.02573468, Gradient norm: 1.72115451
INFO:root:[  137] Training loss: 0.02664030, Validation loss: 0.02687193, Gradient norm: 1.66658074
INFO:root:[  138] Training loss: 0.02607879, Validation loss: 0.02618673, Gradient norm: 1.76855853
INFO:root:[  139] Training loss: 0.02600313, Validation loss: 0.02493359, Gradient norm: 1.92469187
INFO:root:[  140] Training loss: 0.02600854, Validation loss: 0.02641241, Gradient norm: 1.86626775
INFO:root:[  141] Training loss: 0.02629206, Validation loss: 0.02628250, Gradient norm: 1.94899049
INFO:root:[  142] Training loss: 0.02692737, Validation loss: 0.02822456, Gradient norm: 1.84399561
INFO:root:[  143] Training loss: 0.02641510, Validation loss: 0.02457953, Gradient norm: 2.40355929
INFO:root:[  144] Training loss: 0.02552457, Validation loss: 0.02402221, Gradient norm: 1.81848160
INFO:root:[  145] Training loss: 0.02568724, Validation loss: 0.02608939, Gradient norm: 1.94665332
INFO:root:[  146] Training loss: 0.02573672, Validation loss: 0.02809371, Gradient norm: 2.14244687
INFO:root:[  147] Training loss: 0.02498025, Validation loss: 0.02495867, Gradient norm: 2.26202757
INFO:root:[  148] Training loss: 0.02509597, Validation loss: 0.02671777, Gradient norm: 2.17778440
INFO:root:[  149] Training loss: 0.02562874, Validation loss: 0.02329888, Gradient norm: 2.19191776
INFO:root:[  150] Training loss: 0.02555844, Validation loss: 0.02557198, Gradient norm: 2.19463299
INFO:root:[  151] Training loss: 0.02555495, Validation loss: 0.02538031, Gradient norm: 2.27704544
INFO:root:[  152] Training loss: 0.02477454, Validation loss: 0.02492253, Gradient norm: 2.21499349
INFO:root:[  153] Training loss: 0.02505773, Validation loss: 0.02450091, Gradient norm: 2.21394754
INFO:root:[  154] Training loss: 0.02422091, Validation loss: 0.02481386, Gradient norm: 2.19217906
INFO:root:[  155] Training loss: 0.02425763, Validation loss: 0.02647360, Gradient norm: 2.28645752
INFO:root:[  156] Training loss: 0.02608231, Validation loss: 0.02342279, Gradient norm: 2.44163405
INFO:root:[  157] Training loss: 0.02380205, Validation loss: 0.02331597, Gradient norm: 2.12009094
INFO:root:[  158] Training loss: 0.02413884, Validation loss: 0.02560685, Gradient norm: 2.38024167
INFO:root:EP 158: Early stopping
INFO:root:Training the model took 8020.374s.
INFO:root:Emptying the cuda cache took 0.106s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02936
INFO:root:EnergyScoreTrain: 0.02287
INFO:root:CoverageTrain: 0.99481
INFO:root:IntervalWidthTrain: 0.0578
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02936
INFO:root:EnergyScoreValidation: 0.02287
INFO:root:CoverageValidation: 0.99482
INFO:root:IntervalWidthValidation: 0.05779
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02897
INFO:root:EnergyScoreTest: 0.02253
INFO:root:CoverageTest: 0.99483
INFO:root:IntervalWidthTest: 0.05713
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.34724122, Validation loss: 0.16620378, Gradient norm: 4.47622729
INFO:root:[    2] Training loss: 0.15625174, Validation loss: 0.12004202, Gradient norm: 3.57166956
INFO:root:[    3] Training loss: 0.11854260, Validation loss: 0.10760717, Gradient norm: 2.61287278
INFO:root:[    4] Training loss: 0.09944954, Validation loss: 0.09185937, Gradient norm: 1.94476744
INFO:root:[    5] Training loss: 0.08803185, Validation loss: 0.08698666, Gradient norm: 1.47667996
INFO:root:[    6] Training loss: 0.08192285, Validation loss: 0.08176401, Gradient norm: 1.16632434
INFO:root:[    7] Training loss: 0.07745806, Validation loss: 0.07579610, Gradient norm: 1.52655435
INFO:root:[    8] Training loss: 0.07502169, Validation loss: 0.07212707, Gradient norm: 1.30445695
INFO:root:[    9] Training loss: 0.07152332, Validation loss: 0.06796148, Gradient norm: 1.21701252
INFO:root:[   10] Training loss: 0.06956373, Validation loss: 0.06637230, Gradient norm: 1.11248582
INFO:root:[   11] Training loss: 0.06740007, Validation loss: 0.06727643, Gradient norm: 0.96636925
INFO:root:[   12] Training loss: 0.06634252, Validation loss: 0.06529809, Gradient norm: 0.94614582
INFO:root:[   13] Training loss: 0.06557963, Validation loss: 0.06557999, Gradient norm: 1.07531616
INFO:root:[   14] Training loss: 0.06290935, Validation loss: 0.06282241, Gradient norm: 0.72823400
INFO:root:[   15] Training loss: 0.06236935, Validation loss: 0.06234867, Gradient norm: 0.87340380
INFO:root:[   16] Training loss: 0.06197092, Validation loss: 0.05978657, Gradient norm: 1.09094848
INFO:root:[   17] Training loss: 0.06214597, Validation loss: 0.05961520, Gradient norm: 1.14090112
INFO:root:[   18] Training loss: 0.06040298, Validation loss: 0.05864364, Gradient norm: 0.92322738
INFO:root:[   19] Training loss: 0.05937850, Validation loss: 0.05690710, Gradient norm: 0.85029053
INFO:root:[   20] Training loss: 0.05888895, Validation loss: 0.05658014, Gradient norm: 1.14321200
INFO:root:[   21] Training loss: 0.05742405, Validation loss: 0.05575661, Gradient norm: 0.83449603
INFO:root:[   22] Training loss: 0.05655995, Validation loss: 0.05651018, Gradient norm: 0.95471246
INFO:root:[   23] Training loss: 0.05598582, Validation loss: 0.05517019, Gradient norm: 1.09475985
INFO:root:[   24] Training loss: 0.05557818, Validation loss: 0.05736637, Gradient norm: 0.94964092
INFO:root:[   25] Training loss: 0.05486903, Validation loss: 0.05212537, Gradient norm: 0.74166480
INFO:root:[   26] Training loss: 0.05413965, Validation loss: 0.05457111, Gradient norm: 1.27157392
INFO:root:[   27] Training loss: 0.05358564, Validation loss: 0.05056824, Gradient norm: 0.64081508
INFO:root:[   28] Training loss: 0.05348349, Validation loss: 0.05290692, Gradient norm: 0.96587780
INFO:root:[   29] Training loss: 0.05183972, Validation loss: 0.05371398, Gradient norm: 1.15458844
INFO:root:[   30] Training loss: 0.05231403, Validation loss: 0.05223219, Gradient norm: 0.97171208
INFO:root:[   31] Training loss: 0.05140217, Validation loss: 0.05121297, Gradient norm: 1.16519372
INFO:root:[   32] Training loss: 0.05091194, Validation loss: 0.05161462, Gradient norm: 1.32878663
INFO:root:[   33] Training loss: 0.05036927, Validation loss: 0.04989588, Gradient norm: 1.29689711
INFO:root:[   34] Training loss: 0.04944397, Validation loss: 0.04890708, Gradient norm: 1.01048251
INFO:root:[   35] Training loss: 0.04889556, Validation loss: 0.04910287, Gradient norm: 0.79710632
INFO:root:[   36] Training loss: 0.04735685, Validation loss: 0.04493194, Gradient norm: 0.76978958
INFO:root:[   37] Training loss: 0.04753794, Validation loss: 0.04600228, Gradient norm: 1.42529584
INFO:root:[   38] Training loss: 0.04735913, Validation loss: 0.04426088, Gradient norm: 1.24028818
INFO:root:[   39] Training loss: 0.04635959, Validation loss: 0.04715350, Gradient norm: 1.10406343
INFO:root:[   40] Training loss: 0.04751337, Validation loss: 0.04851939, Gradient norm: 1.17277802
INFO:root:[   41] Training loss: 0.04601156, Validation loss: 0.04551231, Gradient norm: 1.09422798
INFO:root:[   42] Training loss: 0.04539633, Validation loss: 0.04443397, Gradient norm: 1.05309329
INFO:root:[   43] Training loss: 0.04486134, Validation loss: 0.04448263, Gradient norm: 1.12466564
INFO:root:[   44] Training loss: 0.04432455, Validation loss: 0.04587286, Gradient norm: 1.22695786
INFO:root:[   45] Training loss: 0.04357947, Validation loss: 0.04352455, Gradient norm: 1.21431856
INFO:root:[   46] Training loss: 0.04358827, Validation loss: 0.04260417, Gradient norm: 1.25784856
INFO:root:[   47] Training loss: 0.04332516, Validation loss: 0.04477500, Gradient norm: 1.40398209
INFO:root:[   48] Training loss: 0.04285190, Validation loss: 0.04237276, Gradient norm: 1.17553960
INFO:root:[   49] Training loss: 0.04294009, Validation loss: 0.04219925, Gradient norm: 1.33976409
INFO:root:[   50] Training loss: 0.04152859, Validation loss: 0.04089422, Gradient norm: 0.84421980
INFO:root:[   51] Training loss: 0.04133995, Validation loss: 0.04111918, Gradient norm: 1.24571473
INFO:root:[   52] Training loss: 0.04057586, Validation loss: 0.04235688, Gradient norm: 1.32187171
INFO:root:[   53] Training loss: 0.04108024, Validation loss: 0.03921631, Gradient norm: 1.33169357
INFO:root:[   54] Training loss: 0.03954520, Validation loss: 0.04085602, Gradient norm: 1.16763395
INFO:root:[   55] Training loss: 0.04017385, Validation loss: 0.04319099, Gradient norm: 1.26006652
INFO:root:[   56] Training loss: 0.03975332, Validation loss: 0.04054753, Gradient norm: 1.17847346
INFO:root:[   57] Training loss: 0.03894168, Validation loss: 0.03909724, Gradient norm: 1.08093947
INFO:root:[   58] Training loss: 0.03800739, Validation loss: 0.03934132, Gradient norm: 1.24029238
INFO:root:[   59] Training loss: 0.03849040, Validation loss: 0.03808554, Gradient norm: 1.30391881
INFO:root:[   60] Training loss: 0.03807169, Validation loss: 0.03637182, Gradient norm: 1.29294473
INFO:root:[   61] Training loss: 0.03713913, Validation loss: 0.03639856, Gradient norm: 1.27355116
INFO:root:[   62] Training loss: 0.03674294, Validation loss: 0.03698849, Gradient norm: 1.29710815
INFO:root:[   63] Training loss: 0.03842294, Validation loss: 0.03739906, Gradient norm: 1.34598968
INFO:root:[   64] Training loss: 0.03693911, Validation loss: 0.03426958, Gradient norm: 1.33375281
INFO:root:[   65] Training loss: 0.03641434, Validation loss: 0.03780967, Gradient norm: 1.18355498
INFO:root:[   66] Training loss: 0.03674947, Validation loss: 0.03477988, Gradient norm: 1.26039535
INFO:root:[   67] Training loss: 0.03570191, Validation loss: 0.03465142, Gradient norm: 1.33207018
INFO:root:[   68] Training loss: 0.03624895, Validation loss: 0.03300669, Gradient norm: 1.24782658
INFO:root:[   69] Training loss: 0.03478926, Validation loss: 0.03383133, Gradient norm: 1.36596948
INFO:root:[   70] Training loss: 0.03503213, Validation loss: 0.03587997, Gradient norm: 1.27806992
INFO:root:[   71] Training loss: 0.03547164, Validation loss: 0.03689358, Gradient norm: 1.26168073
INFO:root:[   72] Training loss: 0.03513208, Validation loss: 0.03491648, Gradient norm: 1.24114393
INFO:root:[   73] Training loss: 0.03402656, Validation loss: 0.03178315, Gradient norm: 1.32303268
INFO:root:[   74] Training loss: 0.03333098, Validation loss: 0.03462263, Gradient norm: 1.21589016
INFO:root:[   75] Training loss: 0.03397919, Validation loss: 0.03107535, Gradient norm: 1.27432632
INFO:root:[   76] Training loss: 0.03296528, Validation loss: 0.03509458, Gradient norm: 1.29388821
INFO:root:[   77] Training loss: 0.03274244, Validation loss: 0.03047218, Gradient norm: 1.43102616
INFO:root:[   78] Training loss: 0.03277377, Validation loss: 0.03262415, Gradient norm: 1.29576147
INFO:root:[   79] Training loss: 0.03239307, Validation loss: 0.03199319, Gradient norm: 1.25240630
INFO:root:[   80] Training loss: 0.03280726, Validation loss: 0.03652319, Gradient norm: 1.27868577
INFO:root:[   81] Training loss: 0.03298810, Validation loss: 0.03222362, Gradient norm: 1.41882038
INFO:root:[   82] Training loss: 0.03183771, Validation loss: 0.03024421, Gradient norm: 1.38758831
INFO:root:[   83] Training loss: 0.03284657, Validation loss: 0.03491025, Gradient norm: 1.44117742
INFO:root:[   84] Training loss: 0.03075209, Validation loss: 0.03138197, Gradient norm: 1.32386980
INFO:root:[   85] Training loss: 0.03196235, Validation loss: 0.02877486, Gradient norm: 1.25114196
INFO:root:[   86] Training loss: 0.03105136, Validation loss: 0.03210262, Gradient norm: 1.33578026
INFO:root:[   87] Training loss: 0.03078941, Validation loss: 0.02974492, Gradient norm: 1.27915615
INFO:root:[   88] Training loss: 0.03044687, Validation loss: 0.03012304, Gradient norm: 1.41083021
INFO:root:[   89] Training loss: 0.03084407, Validation loss: 0.03135255, Gradient norm: 1.24131978
INFO:root:[   90] Training loss: 0.03011545, Validation loss: 0.02895813, Gradient norm: 1.27982118
INFO:root:[   91] Training loss: 0.03021475, Validation loss: 0.03099333, Gradient norm: 1.38836307
INFO:root:[   92] Training loss: 0.03014755, Validation loss: 0.02792018, Gradient norm: 1.31704128
INFO:root:[   93] Training loss: 0.02961883, Validation loss: 0.02978779, Gradient norm: 1.50407129
INFO:root:[   94] Training loss: 0.02925035, Validation loss: 0.02788439, Gradient norm: 1.41858670
INFO:root:[   95] Training loss: 0.03012313, Validation loss: 0.02887114, Gradient norm: 1.29842296
INFO:root:[   96] Training loss: 0.02970918, Validation loss: 0.02656483, Gradient norm: 1.43445879
INFO:root:[   97] Training loss: 0.02817560, Validation loss: 0.02907897, Gradient norm: 1.32403928
INFO:root:[   98] Training loss: 0.02896526, Validation loss: 0.02634906, Gradient norm: 1.42600532
INFO:root:[   99] Training loss: 0.02851868, Validation loss: 0.02689883, Gradient norm: 1.38505211
INFO:root:[  100] Training loss: 0.02925900, Validation loss: 0.03805862, Gradient norm: 1.35580637
INFO:root:[  101] Training loss: 0.02861648, Validation loss: 0.02685775, Gradient norm: 1.52504836
INFO:root:[  102] Training loss: 0.02921426, Validation loss: 0.02976294, Gradient norm: 1.30237052
INFO:root:[  103] Training loss: 0.02786750, Validation loss: 0.02821983, Gradient norm: 1.36823604
INFO:root:[  104] Training loss: 0.02770564, Validation loss: 0.02693848, Gradient norm: 1.41361663
INFO:root:[  105] Training loss: 0.02845991, Validation loss: 0.02769896, Gradient norm: 1.37844943
INFO:root:[  106] Training loss: 0.02768971, Validation loss: 0.02504751, Gradient norm: 1.46039324
INFO:root:[  107] Training loss: 0.02709458, Validation loss: 0.02779199, Gradient norm: 1.40224427
INFO:root:[  108] Training loss: 0.02726839, Validation loss: 0.02785392, Gradient norm: 0.99295819
INFO:root:[  109] Training loss: 0.02775796, Validation loss: 0.02728833, Gradient norm: 1.24681736
INFO:root:[  110] Training loss: 0.02721169, Validation loss: 0.02837809, Gradient norm: 1.36516760
INFO:root:[  111] Training loss: 0.02693920, Validation loss: 0.02587388, Gradient norm: 1.46750933
INFO:root:[  112] Training loss: 0.02701092, Validation loss: 0.02560810, Gradient norm: 1.48501742
INFO:root:[  113] Training loss: 0.02647271, Validation loss: 0.02774909, Gradient norm: 1.43310077
INFO:root:[  114] Training loss: 0.02752978, Validation loss: 0.02821395, Gradient norm: 1.27609455
INFO:root:[  115] Training loss: 0.02646914, Validation loss: 0.02603244, Gradient norm: 1.41028433
INFO:root:EP 115: Early stopping
INFO:root:Training the model took 5853.633s.
INFO:root:Emptying the cuda cache took 0.097s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03102
INFO:root:EnergyScoreTrain: 0.02511
INFO:root:CoverageTrain: 0.99585
INFO:root:IntervalWidthTrain: 0.05929
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03087
INFO:root:EnergyScoreValidation: 0.02506
INFO:root:CoverageValidation: 0.9959
INFO:root:IntervalWidthValidation: 0.05929
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03077
INFO:root:EnergyScoreTest: 0.02479
INFO:root:CoverageTest: 0.99583
INFO:root:IntervalWidthTest: 0.05838
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.45655067, Validation loss: 0.30480972, Gradient norm: 5.43675685
INFO:root:[    2] Training loss: 0.22169569, Validation loss: 0.22174232, Gradient norm: 3.54123574
INFO:root:[    3] Training loss: 0.18857457, Validation loss: 0.17296709, Gradient norm: 3.47344477
INFO:root:[    4] Training loss: 0.15693990, Validation loss: 0.14504560, Gradient norm: 2.60971287
INFO:root:[    5] Training loss: 0.14255028, Validation loss: 0.13065163, Gradient norm: 2.92792827
INFO:root:[    6] Training loss: 0.12907012, Validation loss: 0.12019329, Gradient norm: 2.29871951
INFO:root:[    7] Training loss: 0.11937043, Validation loss: 0.12992378, Gradient norm: 1.95506614
INFO:root:[    8] Training loss: 0.11191320, Validation loss: 0.11620445, Gradient norm: 1.82598513
INFO:root:[    9] Training loss: 0.10652140, Validation loss: 0.10670229, Gradient norm: 1.67296949
INFO:root:[   10] Training loss: 0.10479511, Validation loss: 0.09683377, Gradient norm: 2.11863775
INFO:root:[   11] Training loss: 0.10028434, Validation loss: 0.09456819, Gradient norm: 1.76852346
INFO:root:[   12] Training loss: 0.09739264, Validation loss: 0.09140880, Gradient norm: 1.71784703
INFO:root:[   13] Training loss: 0.09238560, Validation loss: 0.08974450, Gradient norm: 1.04993264
INFO:root:[   14] Training loss: 0.09142655, Validation loss: 0.08745004, Gradient norm: 1.10080664
INFO:root:[   15] Training loss: 0.08967445, Validation loss: 0.08672083, Gradient norm: 1.40356210
INFO:root:[   16] Training loss: 0.08749540, Validation loss: 0.08459224, Gradient norm: 1.18818387
INFO:root:[   17] Training loss: 0.08683644, Validation loss: 0.08464997, Gradient norm: 1.41257164
INFO:root:[   18] Training loss: 0.08513471, Validation loss: 0.08290842, Gradient norm: 1.10347272
INFO:root:[   19] Training loss: 0.08333263, Validation loss: 0.08142420, Gradient norm: 1.16164056
INFO:root:[   20] Training loss: 0.08226001, Validation loss: 0.07996860, Gradient norm: 0.99896429
INFO:root:[   21] Training loss: 0.08081188, Validation loss: 0.07904028, Gradient norm: 0.84678925
INFO:root:[   22] Training loss: 0.07876643, Validation loss: 0.07784730, Gradient norm: 1.18519347
INFO:root:[   23] Training loss: 0.07838607, Validation loss: 0.07793525, Gradient norm: 0.98441635
INFO:root:[   24] Training loss: 0.07726917, Validation loss: 0.07865967, Gradient norm: 1.07421366
INFO:root:[   25] Training loss: 0.07645847, Validation loss: 0.07465386, Gradient norm: 1.30455105
INFO:root:[   26] Training loss: 0.07505563, Validation loss: 0.07580100, Gradient norm: 1.22527322
INFO:root:[   27] Training loss: 0.07345443, Validation loss: 0.07611587, Gradient norm: 1.13974171
INFO:root:[   28] Training loss: 0.07350153, Validation loss: 0.07345364, Gradient norm: 1.12192638
INFO:root:[   29] Training loss: 0.07203059, Validation loss: 0.07073484, Gradient norm: 1.00784492
INFO:root:[   30] Training loss: 0.07103497, Validation loss: 0.07025509, Gradient norm: 1.20577760
INFO:root:[   31] Training loss: 0.06990907, Validation loss: 0.06881411, Gradient norm: 1.26727702
INFO:root:[   32] Training loss: 0.07008180, Validation loss: 0.06970250, Gradient norm: 1.19070368
INFO:root:[   33] Training loss: 0.06787458, Validation loss: 0.06497202, Gradient norm: 1.13562085
INFO:root:[   34] Training loss: 0.06717137, Validation loss: 0.06601581, Gradient norm: 1.33314311
INFO:root:[   35] Training loss: 0.06683862, Validation loss: 0.06484258, Gradient norm: 1.13413204
INFO:root:[   36] Training loss: 0.06476026, Validation loss: 0.06435374, Gradient norm: 1.13932155
INFO:root:[   37] Training loss: 0.06459333, Validation loss: 0.06287833, Gradient norm: 1.28920400
INFO:root:[   38] Training loss: 0.06298656, Validation loss: 0.06307027, Gradient norm: 1.09512698
INFO:root:[   39] Training loss: 0.06202675, Validation loss: 0.06233849, Gradient norm: 1.28223230
INFO:root:[   40] Training loss: 0.06132242, Validation loss: 0.05954476, Gradient norm: 1.15478943
INFO:root:[   41] Training loss: 0.06071721, Validation loss: 0.06092158, Gradient norm: 1.28573175
INFO:root:[   42] Training loss: 0.05992749, Validation loss: 0.05818216, Gradient norm: 1.33077421
INFO:root:[   43] Training loss: 0.05970915, Validation loss: 0.06178754, Gradient norm: 1.22186829
INFO:root:[   44] Training loss: 0.05837728, Validation loss: 0.05631384, Gradient norm: 1.16418324
INFO:root:[   45] Training loss: 0.05726742, Validation loss: 0.05730405, Gradient norm: 1.19925061
INFO:root:[   46] Training loss: 0.05663301, Validation loss: 0.05477781, Gradient norm: 1.28684373
INFO:root:[   47] Training loss: 0.05614025, Validation loss: 0.05787140, Gradient norm: 1.31566293
INFO:root:[   48] Training loss: 0.05524711, Validation loss: 0.05394809, Gradient norm: 1.32685938
INFO:root:[   49] Training loss: 0.05433173, Validation loss: 0.05246790, Gradient norm: 1.20059978
INFO:root:[   50] Training loss: 0.05371927, Validation loss: 0.05334750, Gradient norm: 1.27667626
INFO:root:[   51] Training loss: 0.05262559, Validation loss: 0.05329087, Gradient norm: 1.30720725
INFO:root:[   52] Training loss: 0.05379617, Validation loss: 0.05524210, Gradient norm: 1.30122191
INFO:root:[   53] Training loss: 0.05167965, Validation loss: 0.04966031, Gradient norm: 1.29721006
INFO:root:[   54] Training loss: 0.05152584, Validation loss: 0.04989447, Gradient norm: 1.32069037
INFO:root:[   55] Training loss: 0.05006123, Validation loss: 0.04901273, Gradient norm: 1.30397495
INFO:root:[   56] Training loss: 0.05005127, Validation loss: 0.05056198, Gradient norm: 1.34385261
INFO:root:[   57] Training loss: 0.04959816, Validation loss: 0.04656917, Gradient norm: 1.45696306
INFO:root:[   58] Training loss: 0.04844215, Validation loss: 0.04616305, Gradient norm: 1.32473541
INFO:root:[   59] Training loss: 0.04822135, Validation loss: 0.04689967, Gradient norm: 1.27715862
INFO:root:[   60] Training loss: 0.04740851, Validation loss: 0.04617147, Gradient norm: 1.29387466
INFO:root:[   61] Training loss: 0.04612756, Validation loss: 0.04436025, Gradient norm: 1.30032768
INFO:root:[   62] Training loss: 0.04781904, Validation loss: 0.04700734, Gradient norm: 1.38249448
INFO:root:[   63] Training loss: 0.04538404, Validation loss: 0.04396543, Gradient norm: 1.35019611
INFO:root:[   64] Training loss: 0.04459493, Validation loss: 0.04684789, Gradient norm: 1.39438074
INFO:root:[   65] Training loss: 0.04516325, Validation loss: 0.04517600, Gradient norm: 1.19947199
INFO:root:[   66] Training loss: 0.04338497, Validation loss: 0.04369853, Gradient norm: 1.34428422
INFO:root:[   67] Training loss: 0.04319948, Validation loss: 0.04307176, Gradient norm: 1.42334210
INFO:root:[   68] Training loss: 0.04391319, Validation loss: 0.04432045, Gradient norm: 1.36552538
INFO:root:[   69] Training loss: 0.04191851, Validation loss: 0.04284124, Gradient norm: 1.40221208
INFO:root:[   70] Training loss: 0.04276134, Validation loss: 0.04561284, Gradient norm: 1.35424201
INFO:root:[   71] Training loss: 0.04169600, Validation loss: 0.03892924, Gradient norm: 1.43678105
INFO:root:[   72] Training loss: 0.04105363, Validation loss: 0.03905872, Gradient norm: 1.27794650
INFO:root:[   73] Training loss: 0.03997574, Validation loss: 0.04068740, Gradient norm: 1.34637482
INFO:root:[   74] Training loss: 0.04021980, Validation loss: 0.04029101, Gradient norm: 1.42360119
INFO:root:[   75] Training loss: 0.04063969, Validation loss: 0.04086149, Gradient norm: 1.23233977
INFO:root:[   76] Training loss: 0.03931676, Validation loss: 0.04202308, Gradient norm: 1.31901056
INFO:root:[   77] Training loss: 0.03970174, Validation loss: 0.03681060, Gradient norm: 1.13498624
INFO:root:[   78] Training loss: 0.03832389, Validation loss: 0.03728767, Gradient norm: 1.42954673
INFO:root:[   79] Training loss: 0.03956980, Validation loss: 0.03802184, Gradient norm: 1.38641350
INFO:root:[   80] Training loss: 0.03726476, Validation loss: 0.03648784, Gradient norm: 1.15509619
INFO:root:[   81] Training loss: 0.03665817, Validation loss: 0.03533539, Gradient norm: 1.34634151
INFO:root:[   82] Training loss: 0.03751693, Validation loss: 0.04230321, Gradient norm: 1.42040299
INFO:root:[   83] Training loss: 0.03765195, Validation loss: 0.03626833, Gradient norm: 1.60381300
INFO:root:[   84] Training loss: 0.03575686, Validation loss: 0.03828448, Gradient norm: 1.28042552
INFO:root:[   85] Training loss: 0.03788192, Validation loss: 0.03562297, Gradient norm: 1.27309090
INFO:root:[   86] Training loss: 0.03548746, Validation loss: 0.03757331, Gradient norm: 1.22959182
INFO:root:[   87] Training loss: 0.03553173, Validation loss: 0.03261187, Gradient norm: 1.49007390
INFO:root:[   88] Training loss: 0.03507400, Validation loss: 0.03777556, Gradient norm: 1.34671230
INFO:root:[   89] Training loss: 0.03420454, Validation loss: 0.03300279, Gradient norm: 1.39884069
INFO:root:[   90] Training loss: 0.03362850, Validation loss: 0.03378602, Gradient norm: 1.40813861
INFO:root:[   91] Training loss: 0.03410628, Validation loss: 0.03463070, Gradient norm: 1.40733121
INFO:root:[   92] Training loss: 0.03359183, Validation loss: 0.03572449, Gradient norm: 1.44061141
INFO:root:[   93] Training loss: 0.03459475, Validation loss: 0.03467917, Gradient norm: 1.59289822
INFO:root:[   94] Training loss: 0.03289976, Validation loss: 0.03854038, Gradient norm: 1.31414474
INFO:root:[   95] Training loss: 0.03315682, Validation loss: 0.03088132, Gradient norm: 1.51444286
INFO:root:[   96] Training loss: 0.03239958, Validation loss: 0.03028646, Gradient norm: 1.53458281
INFO:root:[   97] Training loss: 0.03186923, Validation loss: 0.03128134, Gradient norm: 1.41922133
INFO:root:[   98] Training loss: 0.03239532, Validation loss: 0.03509691, Gradient norm: 1.35631740
INFO:root:[   99] Training loss: 0.03153855, Validation loss: 0.03175350, Gradient norm: 1.43912910
INFO:root:[  100] Training loss: 0.03097533, Validation loss: 0.02939131, Gradient norm: 1.33837857
INFO:root:[  101] Training loss: 0.03145997, Validation loss: 0.02915752, Gradient norm: 1.36509344
INFO:root:[  102] Training loss: 0.03000372, Validation loss: 0.03228170, Gradient norm: 1.59250409
INFO:root:[  103] Training loss: 0.03215936, Validation loss: 0.02892112, Gradient norm: 1.76503466
INFO:root:[  104] Training loss: 0.02980480, Validation loss: 0.03121497, Gradient norm: 1.46342508
INFO:root:[  105] Training loss: 0.03097499, Validation loss: 0.03176730, Gradient norm: 1.47844890
INFO:root:[  106] Training loss: 0.03126869, Validation loss: 0.03184787, Gradient norm: 1.57325321
INFO:root:[  107] Training loss: 0.03038660, Validation loss: 0.03034582, Gradient norm: 1.66924060
INFO:root:[  108] Training loss: 0.03030882, Validation loss: 0.02894576, Gradient norm: 1.46931534
INFO:root:[  109] Training loss: 0.02872366, Validation loss: 0.02817384, Gradient norm: 1.31418941
INFO:root:[  110] Training loss: 0.02961898, Validation loss: 0.03075042, Gradient norm: 1.65772431
INFO:root:[  111] Training loss: 0.02915717, Validation loss: 0.03033969, Gradient norm: 1.56646152
INFO:root:[  112] Training loss: 0.02802821, Validation loss: 0.02955557, Gradient norm: 1.48338290
INFO:root:[  113] Training loss: 0.02839518, Validation loss: 0.02963607, Gradient norm: 1.53575810
INFO:root:[  114] Training loss: 0.02936006, Validation loss: 0.02978140, Gradient norm: 1.55090357
INFO:root:[  115] Training loss: 0.02748631, Validation loss: 0.03172042, Gradient norm: 1.25104124
INFO:root:[  116] Training loss: 0.02910386, Validation loss: 0.02893021, Gradient norm: 1.52600177
INFO:root:[  117] Training loss: 0.02751414, Validation loss: 0.02502309, Gradient norm: 1.61527441
INFO:root:[  118] Training loss: 0.02914791, Validation loss: 0.03262657, Gradient norm: 1.59629504
INFO:root:[  119] Training loss: 0.02859662, Validation loss: 0.02801863, Gradient norm: 1.51313403
INFO:root:[  120] Training loss: 0.02779447, Validation loss: 0.02753412, Gradient norm: 1.36609437
INFO:root:[  121] Training loss: 0.02756854, Validation loss: 0.02659446, Gradient norm: 1.56917559
INFO:root:[  122] Training loss: 0.02688573, Validation loss: 0.02525605, Gradient norm: 1.56639025
INFO:root:[  123] Training loss: 0.02800603, Validation loss: 0.02833729, Gradient norm: 1.77154155
INFO:root:[  124] Training loss: 0.02794259, Validation loss: 0.02773748, Gradient norm: 1.40277951
INFO:root:[  125] Training loss: 0.02818118, Validation loss: 0.02932793, Gradient norm: 1.65839078
INFO:root:[  126] Training loss: 0.02741871, Validation loss: 0.02528369, Gradient norm: 1.66537236
INFO:root:EP 126: Early stopping
INFO:root:Training the model took 6417.7s.
INFO:root:Emptying the cuda cache took 0.101s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03019
INFO:root:EnergyScoreTrain: 0.02478
INFO:root:CoverageTrain: 0.99775
INFO:root:IntervalWidthTrain: 0.05181
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0301
INFO:root:EnergyScoreValidation: 0.02475
INFO:root:CoverageValidation: 0.99775
INFO:root:IntervalWidthValidation: 0.05176
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02975
INFO:root:EnergyScoreTest: 0.02437
INFO:root:CoverageTest: 0.99783
INFO:root:IntervalWidthTest: 0.05087
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.50346222, Validation loss: 0.26393321, Gradient norm: 5.06954537
INFO:root:[    2] Training loss: 0.26729706, Validation loss: 0.21987195, Gradient norm: 4.94018873
INFO:root:[    3] Training loss: 0.22290880, Validation loss: 0.19851042, Gradient norm: 5.66002713
INFO:root:[    4] Training loss: 0.20810361, Validation loss: 0.20299323, Gradient norm: 6.07031520
INFO:root:[    5] Training loss: 0.18717869, Validation loss: 0.19025879, Gradient norm: 5.32930526
INFO:root:[    6] Training loss: 0.16644940, Validation loss: 0.17488809, Gradient norm: 3.93754072
INFO:root:[    7] Training loss: 0.15925584, Validation loss: 0.15059352, Gradient norm: 4.80646174
INFO:root:[    8] Training loss: 0.14891605, Validation loss: 0.13520336, Gradient norm: 4.20733274
INFO:root:[    9] Training loss: 0.13299559, Validation loss: 0.13329027, Gradient norm: 2.83492584
INFO:root:[   10] Training loss: 0.12952488, Validation loss: 0.12171508, Gradient norm: 3.52967236
INFO:root:[   11] Training loss: 0.12823716, Validation loss: 0.11824882, Gradient norm: 4.18207487
INFO:root:[   12] Training loss: 0.11721950, Validation loss: 0.11868358, Gradient norm: 2.71540440
INFO:root:[   13] Training loss: 0.11314140, Validation loss: 0.10871637, Gradient norm: 2.68049302
INFO:root:[   14] Training loss: 0.11292308, Validation loss: 0.10546034, Gradient norm: 3.83041391
INFO:root:[   15] Training loss: 0.10438034, Validation loss: 0.10102110, Gradient norm: 2.24731592
INFO:root:[   16] Training loss: 0.10670913, Validation loss: 0.10378893, Gradient norm: 3.89034774
INFO:root:[   17] Training loss: 0.10115098, Validation loss: 0.09594669, Gradient norm: 3.08165080
INFO:root:[   18] Training loss: 0.09725802, Validation loss: 0.09309307, Gradient norm: 2.59653747
INFO:root:[   19] Training loss: 0.09463150, Validation loss: 0.09071182, Gradient norm: 2.58112807
INFO:root:[   20] Training loss: 0.09297860, Validation loss: 0.08799936, Gradient norm: 2.84177586
INFO:root:[   21] Training loss: 0.08787199, Validation loss: 0.08749763, Gradient norm: 1.76672044
INFO:root:[   22] Training loss: 0.08740594, Validation loss: 0.08514626, Gradient norm: 2.26890606
INFO:root:[   23] Training loss: 0.08543691, Validation loss: 0.08640918, Gradient norm: 2.25493246
INFO:root:[   24] Training loss: 0.08343805, Validation loss: 0.07996340, Gradient norm: 2.56192837
INFO:root:[   25] Training loss: 0.08141810, Validation loss: 0.08129518, Gradient norm: 2.35024502
INFO:root:[   26] Training loss: 0.08108266, Validation loss: 0.07889947, Gradient norm: 2.97370296
INFO:root:[   27] Training loss: 0.07825110, Validation loss: 0.07628283, Gradient norm: 2.37417157
INFO:root:[   28] Training loss: 0.07649073, Validation loss: 0.07549656, Gradient norm: 2.28741175
INFO:root:[   29] Training loss: 0.07558012, Validation loss: 0.07881360, Gradient norm: 2.71297788
INFO:root:[   30] Training loss: 0.07542056, Validation loss: 0.07290080, Gradient norm: 2.99986403
INFO:root:[   31] Training loss: 0.07280267, Validation loss: 0.07184647, Gradient norm: 2.31605924
INFO:root:[   32] Training loss: 0.07162424, Validation loss: 0.06985866, Gradient norm: 2.95526527
INFO:root:[   33] Training loss: 0.07009831, Validation loss: 0.06893828, Gradient norm: 2.75250852
INFO:root:[   34] Training loss: 0.06979732, Validation loss: 0.06766694, Gradient norm: 3.11646021
INFO:root:[   35] Training loss: 0.06786910, Validation loss: 0.06814274, Gradient norm: 2.82445299
INFO:root:[   36] Training loss: 0.06677487, Validation loss: 0.06599670, Gradient norm: 3.02524760
INFO:root:[   37] Training loss: 0.06629796, Validation loss: 0.06696941, Gradient norm: 3.03823238
INFO:root:[   38] Training loss: 0.06507326, Validation loss: 0.06315439, Gradient norm: 3.00050190
INFO:root:[   39] Training loss: 0.06318167, Validation loss: 0.06305815, Gradient norm: 2.96731865
INFO:root:[   40] Training loss: 0.06325928, Validation loss: 0.06289496, Gradient norm: 3.31353204
INFO:root:[   41] Training loss: 0.06176304, Validation loss: 0.06236905, Gradient norm: 2.84362388
INFO:root:[   42] Training loss: 0.06072337, Validation loss: 0.05964634, Gradient norm: 3.20432692
INFO:root:[   43] Training loss: 0.05972978, Validation loss: 0.05957821, Gradient norm: 3.27765287
INFO:root:[   44] Training loss: 0.05837653, Validation loss: 0.05705383, Gradient norm: 3.24909167
INFO:root:[   45] Training loss: 0.05796297, Validation loss: 0.05777452, Gradient norm: 2.80455714
INFO:root:[   46] Training loss: 0.05744442, Validation loss: 0.05854657, Gradient norm: 3.12892980
INFO:root:[   47] Training loss: 0.05617413, Validation loss: 0.05501133, Gradient norm: 2.84025846
INFO:root:[   48] Training loss: 0.05506358, Validation loss: 0.05372901, Gradient norm: 3.26818451
INFO:root:[   49] Training loss: 0.05428032, Validation loss: 0.05382156, Gradient norm: 3.37288342
INFO:root:[   50] Training loss: 0.05358743, Validation loss: 0.05204383, Gradient norm: 3.28768030
INFO:root:[   51] Training loss: 0.05176007, Validation loss: 0.05112773, Gradient norm: 3.03291863
INFO:root:[   52] Training loss: 0.05232153, Validation loss: 0.05190824, Gradient norm: 3.21280076
INFO:root:[   53] Training loss: 0.05083551, Validation loss: 0.04918867, Gradient norm: 3.26232742
INFO:root:[   54] Training loss: 0.04946429, Validation loss: 0.04852231, Gradient norm: 3.30140405
INFO:root:[   55] Training loss: 0.04939521, Validation loss: 0.04991796, Gradient norm: 3.46519830
INFO:root:[   56] Training loss: 0.04848290, Validation loss: 0.04654943, Gradient norm: 2.99260515
INFO:root:[   57] Training loss: 0.05146295, Validation loss: 0.04877099, Gradient norm: 2.19956499
INFO:root:[   58] Training loss: 0.04742563, Validation loss: 0.04716003, Gradient norm: 3.33869307
INFO:root:[   59] Training loss: 0.04648279, Validation loss: 0.04418747, Gradient norm: 3.34746095
INFO:root:[   60] Training loss: 0.04574663, Validation loss: 0.04518149, Gradient norm: 3.06942140
INFO:root:[   61] Training loss: 0.04504898, Validation loss: 0.04289405, Gradient norm: 3.10526854
INFO:root:[   62] Training loss: 0.04469956, Validation loss: 0.04459701, Gradient norm: 2.96429381
INFO:root:[   63] Training loss: 0.04453387, Validation loss: 0.04491720, Gradient norm: 3.01564289
INFO:root:[   64] Training loss: 0.04340764, Validation loss: 0.04303147, Gradient norm: 3.01245295
INFO:root:[   65] Training loss: 0.04304486, Validation loss: 0.05181266, Gradient norm: 2.98641925
INFO:root:[   66] Training loss: 0.04807492, Validation loss: 0.04347729, Gradient norm: 3.28792202
INFO:root:[   67] Training loss: 0.04290681, Validation loss: 0.04361252, Gradient norm: 3.15586545
INFO:root:[   68] Training loss: 0.04247066, Validation loss: 0.04129352, Gradient norm: 2.81074811
INFO:root:[   69] Training loss: 0.04289626, Validation loss: 0.04444446, Gradient norm: 2.46555992
INFO:root:[   70] Training loss: 0.04042849, Validation loss: 0.03947676, Gradient norm: 2.87289717
INFO:root:[   71] Training loss: 0.03983163, Validation loss: 0.03951731, Gradient norm: 3.08665010
INFO:root:[   72] Training loss: 0.03913052, Validation loss: 0.03901319, Gradient norm: 3.03024010
INFO:root:[   73] Training loss: 0.03893274, Validation loss: 0.03889941, Gradient norm: 3.02415296
INFO:root:[   74] Training loss: 0.03877854, Validation loss: 0.03639844, Gradient norm: 2.92830179
INFO:root:[   75] Training loss: 0.03764253, Validation loss: 0.03568786, Gradient norm: 3.18637154
INFO:root:[   76] Training loss: 0.03695666, Validation loss: 0.03598614, Gradient norm: 2.95927534
INFO:root:[   77] Training loss: 0.04152360, Validation loss: 0.04359075, Gradient norm: 2.89978740
INFO:root:[   78] Training loss: 0.03880642, Validation loss: 0.03671385, Gradient norm: 3.10131109
INFO:root:[   79] Training loss: 0.03766663, Validation loss: 0.03706567, Gradient norm: 3.30383686
INFO:root:[   80] Training loss: 0.03841337, Validation loss: 0.03822606, Gradient norm: 2.75756188
INFO:root:[   81] Training loss: 0.03703207, Validation loss: 0.03546200, Gradient norm: 2.64022338
INFO:root:[   82] Training loss: 0.03510357, Validation loss: 0.03347422, Gradient norm: 3.06873313
INFO:root:[   83] Training loss: 0.03505706, Validation loss: 0.03465428, Gradient norm: 3.23518439
INFO:root:[   84] Training loss: 0.03493162, Validation loss: 0.03479922, Gradient norm: 3.13882787
INFO:root:[   85] Training loss: 0.03648188, Validation loss: 0.04067463, Gradient norm: 2.90943350
INFO:root:[   86] Training loss: 0.03643256, Validation loss: 0.03192381, Gradient norm: 2.91606639
INFO:root:[   87] Training loss: 0.03364709, Validation loss: 0.03292164, Gradient norm: 2.99526972
INFO:root:[   88] Training loss: 0.03565537, Validation loss: 0.03289090, Gradient norm: 3.19656109
INFO:root:[   89] Training loss: 0.03342926, Validation loss: 0.03496776, Gradient norm: 2.99549147
INFO:root:[   90] Training loss: 0.03643159, Validation loss: 0.03552906, Gradient norm: 2.98334614
INFO:root:[   91] Training loss: 0.03453302, Validation loss: 0.03270769, Gradient norm: 3.18257276
INFO:root:[   92] Training loss: 0.03337450, Validation loss: 0.03278800, Gradient norm: 2.50707976
INFO:root:[   93] Training loss: 0.03431921, Validation loss: 0.03515370, Gradient norm: 3.14689398
INFO:root:[   94] Training loss: 0.03156292, Validation loss: 0.03194490, Gradient norm: 2.90694836
INFO:root:[   95] Training loss: 0.03172556, Validation loss: 0.03014012, Gradient norm: 3.11401282
INFO:root:[   96] Training loss: 0.03178842, Validation loss: 0.03367391, Gradient norm: 2.93549696
INFO:root:[   97] Training loss: 0.03349067, Validation loss: 0.03245751, Gradient norm: 3.08775483
INFO:root:[   98] Training loss: 0.03215529, Validation loss: 0.03566614, Gradient norm: 2.09343349
INFO:root:[   99] Training loss: 0.03286155, Validation loss: 0.02927030, Gradient norm: 3.12290391
INFO:root:[  100] Training loss: 0.03163963, Validation loss: 0.03129006, Gradient norm: 2.47281308
INFO:root:[  101] Training loss: 0.03225776, Validation loss: 0.03363851, Gradient norm: 3.79901132
INFO:root:[  102] Training loss: 0.03174742, Validation loss: 0.03229001, Gradient norm: 2.34166618
INFO:root:[  103] Training loss: 0.03218279, Validation loss: 0.03217691, Gradient norm: 2.98260970
INFO:root:[  104] Training loss: 0.03257863, Validation loss: 0.03616720, Gradient norm: 3.96244906
INFO:root:[  105] Training loss: 0.03095160, Validation loss: 0.03311308, Gradient norm: 3.73547014
INFO:root:[  106] Training loss: 0.02999113, Validation loss: 0.02921828, Gradient norm: 3.31251055
INFO:root:[  107] Training loss: 0.03020166, Validation loss: 0.02942570, Gradient norm: 3.37579333
INFO:root:[  108] Training loss: 0.03103830, Validation loss: 0.03410549, Gradient norm: 3.17181724
INFO:root:[  109] Training loss: 0.03197983, Validation loss: 0.03072697, Gradient norm: 3.14735104
INFO:root:[  110] Training loss: 0.03125770, Validation loss: 0.03141667, Gradient norm: 3.43522770
INFO:root:[  111] Training loss: 0.02962258, Validation loss: 0.03026407, Gradient norm: 3.22150636
INFO:root:[  112] Training loss: 0.03192899, Validation loss: 0.03092808, Gradient norm: 3.62298051
INFO:root:[  113] Training loss: 0.03052628, Validation loss: 0.02928395, Gradient norm: 3.49564131
INFO:root:[  114] Training loss: 0.03011851, Validation loss: 0.02793839, Gradient norm: 2.91775631
INFO:root:[  115] Training loss: 0.03001024, Validation loss: 0.03409450, Gradient norm: 2.81938298
INFO:root:[  116] Training loss: 0.03053483, Validation loss: 0.02994005, Gradient norm: 3.37300022
INFO:root:[  117] Training loss: 0.02893873, Validation loss: 0.03257262, Gradient norm: 2.25700880
INFO:root:[  118] Training loss: 0.03066267, Validation loss: 0.02835035, Gradient norm: 3.77192376
INFO:root:[  119] Training loss: 0.02936730, Validation loss: 0.03207187, Gradient norm: 3.15924409
INFO:root:[  120] Training loss: 0.03024150, Validation loss: 0.03089153, Gradient norm: 2.82228268
INFO:root:[  121] Training loss: 0.03019525, Validation loss: 0.03101499, Gradient norm: 3.12591471
INFO:root:[  122] Training loss: 0.02889361, Validation loss: 0.02882449, Gradient norm: 2.93434073
INFO:root:[  123] Training loss: 0.03010749, Validation loss: 0.02936236, Gradient norm: 3.05284494
INFO:root:EP 123: Early stopping
INFO:root:Training the model took 6269.119s.
INFO:root:Emptying the cuda cache took 0.097s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03496
INFO:root:EnergyScoreTrain: 0.04256
INFO:root:CoverageTrain: 0.94897
INFO:root:IntervalWidthTrain: 0.04358
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03474
INFO:root:EnergyScoreValidation: 0.04256
INFO:root:CoverageValidation: 0.94747
INFO:root:IntervalWidthValidation: 0.04348
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03445
INFO:root:EnergyScoreTest: 0.04255
INFO:root:CoverageTest: 0.94858
INFO:root:IntervalWidthTest: 0.04245
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.55306247, Validation loss: 0.38336039, Gradient norm: 4.20546414
INFO:root:[    2] Training loss: 0.30927688, Validation loss: 0.31032334, Gradient norm: 2.68945584
INFO:root:[    3] Training loss: 0.24371163, Validation loss: 0.23562859, Gradient norm: 3.00740800
INFO:root:[    4] Training loss: 0.20821219, Validation loss: 0.18105208, Gradient norm: 3.53676418
INFO:root:[    5] Training loss: 0.17797301, Validation loss: 0.17947974, Gradient norm: 2.69919472
INFO:root:[    6] Training loss: 0.16236054, Validation loss: 0.14431284, Gradient norm: 2.56997784
INFO:root:[    7] Training loss: 0.14615915, Validation loss: 0.13264198, Gradient norm: 2.47142249
INFO:root:[    8] Training loss: 0.13248895, Validation loss: 0.12798502, Gradient norm: 1.79694231
INFO:root:[    9] Training loss: 0.12328457, Validation loss: 0.12283060, Gradient norm: 1.36834910
INFO:root:[   10] Training loss: 0.11714730, Validation loss: 0.11437485, Gradient norm: 1.50598015
INFO:root:[   11] Training loss: 0.10988648, Validation loss: 0.10696844, Gradient norm: 0.74490210
INFO:root:[   12] Training loss: 0.10480396, Validation loss: 0.10108390, Gradient norm: 0.85231073
INFO:root:[   13] Training loss: 0.10093679, Validation loss: 0.09829202, Gradient norm: 1.11833518
INFO:root:[   14] Training loss: 0.09793543, Validation loss: 0.09880779, Gradient norm: 1.55635781
INFO:root:[   15] Training loss: 0.09444145, Validation loss: 0.09073527, Gradient norm: 1.19315557
INFO:root:[   16] Training loss: 0.08938464, Validation loss: 0.08937943, Gradient norm: 0.97875735
INFO:root:[   17] Training loss: 0.08764188, Validation loss: 0.08624494, Gradient norm: 1.09536580
INFO:root:[   18] Training loss: 0.08539698, Validation loss: 0.08491127, Gradient norm: 1.14371696
INFO:root:[   19] Training loss: 0.08248295, Validation loss: 0.07919558, Gradient norm: 1.27382331
INFO:root:[   20] Training loss: 0.07974050, Validation loss: 0.08006001, Gradient norm: 1.20225463
INFO:root:[   21] Training loss: 0.07751643, Validation loss: 0.07454814, Gradient norm: 1.49855307
INFO:root:[   22] Training loss: 0.07549184, Validation loss: 0.07316869, Gradient norm: 1.23020918
INFO:root:[   23] Training loss: 0.07300534, Validation loss: 0.07150874, Gradient norm: 1.15374469
INFO:root:[   24] Training loss: 0.07083638, Validation loss: 0.06824047, Gradient norm: 1.37842777
INFO:root:[   25] Training loss: 0.06809621, Validation loss: 0.06543541, Gradient norm: 1.39830153
INFO:root:[   26] Training loss: 0.06671112, Validation loss: 0.06735044, Gradient norm: 1.59727392
INFO:root:[   27] Training loss: 0.06509425, Validation loss: 0.06529053, Gradient norm: 1.47398966
INFO:root:[   28] Training loss: 0.06306286, Validation loss: 0.05975561, Gradient norm: 1.75707330
INFO:root:[   29] Training loss: 0.06146022, Validation loss: 0.06034543, Gradient norm: 1.65458863
INFO:root:[   30] Training loss: 0.06082245, Validation loss: 0.05879981, Gradient norm: 1.91013869
INFO:root:[   31] Training loss: 0.05874945, Validation loss: 0.05947529, Gradient norm: 1.65249260
INFO:root:[   32] Training loss: 0.05680271, Validation loss: 0.05467854, Gradient norm: 1.83810150
INFO:root:[   33] Training loss: 0.05606938, Validation loss: 0.05691579, Gradient norm: 1.64589909
INFO:root:[   34] Training loss: 0.05445182, Validation loss: 0.05398145, Gradient norm: 1.46633640
INFO:root:[   35] Training loss: 0.05370904, Validation loss: 0.04928515, Gradient norm: 1.65516332
INFO:root:[   36] Training loss: 0.05243130, Validation loss: 0.05008546, Gradient norm: 2.20951529
INFO:root:[   37] Training loss: 0.05165370, Validation loss: 0.04792279, Gradient norm: 1.96083586
INFO:root:[   38] Training loss: 0.05105735, Validation loss: 0.04797673, Gradient norm: 1.85481329
INFO:root:[   39] Training loss: 0.04919278, Validation loss: 0.05065861, Gradient norm: 2.02361505
INFO:root:[   40] Training loss: 0.04835441, Validation loss: 0.04605213, Gradient norm: 2.05790370
INFO:root:[   41] Training loss: 0.04741573, Validation loss: 0.04957933, Gradient norm: 1.68986791
INFO:root:[   42] Training loss: 0.04779020, Validation loss: 0.04653395, Gradient norm: 1.99189797
INFO:root:[   43] Training loss: 0.04603800, Validation loss: 0.05023515, Gradient norm: 2.02284718
INFO:root:[   44] Training loss: 0.04904874, Validation loss: 0.04706777, Gradient norm: 1.92442729
INFO:root:[   45] Training loss: 0.04355933, Validation loss: 0.04345060, Gradient norm: 1.63620048
INFO:root:[   46] Training loss: 0.04307786, Validation loss: 0.03947308, Gradient norm: 2.07201145
INFO:root:[   47] Training loss: 0.04234659, Validation loss: 0.04057035, Gradient norm: 2.11544754
INFO:root:[   48] Training loss: 0.04231272, Validation loss: 0.04085027, Gradient norm: 2.02285636
INFO:root:[   49] Training loss: 0.04241884, Validation loss: 0.04503206, Gradient norm: 1.80212761
INFO:root:[   50] Training loss: 0.04244417, Validation loss: 0.03879521, Gradient norm: 1.97962872
INFO:root:[   51] Training loss: 0.04031006, Validation loss: 0.04148726, Gradient norm: 1.42603912
INFO:root:[   52] Training loss: 0.03943431, Validation loss: 0.03680668, Gradient norm: 2.16258229
INFO:root:[   53] Training loss: 0.03886500, Validation loss: 0.04195278, Gradient norm: 2.02418164
INFO:root:[   54] Training loss: 0.04052661, Validation loss: 0.04307653, Gradient norm: 1.93746590
INFO:root:[   55] Training loss: 0.04001351, Validation loss: 0.03839520, Gradient norm: 1.64617365
INFO:root:[   56] Training loss: 0.03781641, Validation loss: 0.03423674, Gradient norm: 2.22983771
INFO:root:[   57] Training loss: 0.03841522, Validation loss: 0.03988050, Gradient norm: 2.19309460
INFO:root:[   58] Training loss: 0.03945417, Validation loss: 0.03515640, Gradient norm: 2.31990091
INFO:root:[   59] Training loss: 0.03772255, Validation loss: 0.03363692, Gradient norm: 2.40042456
INFO:root:[   60] Training loss: 0.03539485, Validation loss: 0.03444458, Gradient norm: 2.10513731
INFO:root:[   61] Training loss: 0.03637462, Validation loss: 0.03881740, Gradient norm: 1.95576658
INFO:root:[   62] Training loss: 0.03739436, Validation loss: 0.03550929, Gradient norm: 2.23108498
INFO:root:[   63] Training loss: 0.03627156, Validation loss: 0.03651051, Gradient norm: 2.14012013
INFO:root:[   64] Training loss: 0.03623171, Validation loss: 0.03508711, Gradient norm: 2.45379919
INFO:root:[   65] Training loss: 0.03530141, Validation loss: 0.03570604, Gradient norm: 2.37749128
INFO:root:[   66] Training loss: 0.03653925, Validation loss: 0.03625166, Gradient norm: 2.68926121
INFO:root:[   67] Training loss: 0.03632097, Validation loss: 0.03603498, Gradient norm: 2.60194493
INFO:root:[   68] Training loss: 0.03688015, Validation loss: 0.03855258, Gradient norm: 1.97227934
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 3489.29s.
INFO:root:Emptying the cuda cache took 0.099s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03994
INFO:root:EnergyScoreTrain: 0.03405
INFO:root:CoverageTrain: 0.99811
INFO:root:IntervalWidthTrain: 0.06458
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03952
INFO:root:EnergyScoreValidation: 0.0339
INFO:root:CoverageValidation: 0.99818
INFO:root:IntervalWidthValidation: 0.06446
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03945
INFO:root:EnergyScoreTest: 0.0335
INFO:root:CoverageTest: 0.9981
INFO:root:IntervalWidthTest: 0.0632
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.40705975, Validation loss: 0.12952888, Gradient norm: 7.25967953
INFO:root:[    2] Training loss: 0.17417752, Validation loss: 0.10190161, Gradient norm: 5.91789937
INFO:root:[    3] Training loss: 0.14376761, Validation loss: 0.08018235, Gradient norm: 4.87189216
INFO:root:[    4] Training loss: 0.12447026, Validation loss: 0.08101175, Gradient norm: 3.33843682
INFO:root:[    5] Training loss: 0.11349312, Validation loss: 0.06951279, Gradient norm: 3.57284706
INFO:root:[    6] Training loss: 0.10275035, Validation loss: 0.06930631, Gradient norm: 2.50216217
INFO:root:[    7] Training loss: 0.09518642, Validation loss: 0.06167604, Gradient norm: 2.78859415
INFO:root:[    8] Training loss: 0.08953641, Validation loss: 0.06314812, Gradient norm: 2.31417711
INFO:root:[    9] Training loss: 0.08429298, Validation loss: 0.05768331, Gradient norm: 2.08005049
INFO:root:[   10] Training loss: 0.08233513, Validation loss: 0.05367887, Gradient norm: 1.92335841
INFO:root:[   11] Training loss: 0.07759265, Validation loss: 0.06000404, Gradient norm: 1.76401398
INFO:root:[   12] Training loss: 0.07492820, Validation loss: 0.06402029, Gradient norm: 1.63925403
INFO:root:[   13] Training loss: 0.07429115, Validation loss: 0.06706393, Gradient norm: 1.21072251
INFO:root:[   14] Training loss: 0.07187179, Validation loss: 0.06255767, Gradient norm: 1.27833261
INFO:root:[   15] Training loss: 0.06957803, Validation loss: 0.05558200, Gradient norm: 1.30405295
INFO:root:[   16] Training loss: 0.06835440, Validation loss: 0.04698953, Gradient norm: 0.78966048
INFO:root:[   17] Training loss: 0.06849011, Validation loss: 0.05395731, Gradient norm: 1.26678356
INFO:root:[   18] Training loss: 0.06725281, Validation loss: 0.05340665, Gradient norm: 1.12235044
INFO:root:[   19] Training loss: 0.06643263, Validation loss: 0.04630813, Gradient norm: 1.04092176
INFO:root:[   20] Training loss: 0.06464892, Validation loss: 0.04232769, Gradient norm: 1.05983767
INFO:root:[   21] Training loss: 0.06423931, Validation loss: 0.04541118, Gradient norm: 0.90591345
INFO:root:[   22] Training loss: 0.06355803, Validation loss: 0.05790060, Gradient norm: 1.03920247
INFO:root:[   23] Training loss: 0.06305261, Validation loss: 0.04494990, Gradient norm: 1.27317015
INFO:root:[   24] Training loss: 0.06191717, Validation loss: 0.05052957, Gradient norm: 1.03550486
INFO:root:[   25] Training loss: 0.06129351, Validation loss: 0.04310321, Gradient norm: 0.82299407
INFO:root:[   26] Training loss: 0.05978370, Validation loss: 0.04615564, Gradient norm: 0.76124430
INFO:root:[   27] Training loss: 0.05986383, Validation loss: 0.04877354, Gradient norm: 1.20798099
INFO:root:[   28] Training loss: 0.05978008, Validation loss: 0.04442104, Gradient norm: 1.13836370
INFO:root:[   29] Training loss: 0.05813483, Validation loss: 0.04468788, Gradient norm: 0.63930672
INFO:root:[   30] Training loss: 0.05761460, Validation loss: 0.04884694, Gradient norm: 0.73947791
INFO:root:[   31] Training loss: 0.05794005, Validation loss: 0.04518071, Gradient norm: 0.82788012
INFO:root:[   32] Training loss: 0.05800767, Validation loss: 0.05054300, Gradient norm: 1.12622061
INFO:root:[   33] Training loss: 0.05642217, Validation loss: 0.05150463, Gradient norm: 1.08934460
INFO:root:[   34] Training loss: 0.05605490, Validation loss: 0.04261053, Gradient norm: 1.13534455
INFO:root:[   35] Training loss: 0.05580685, Validation loss: 0.03841716, Gradient norm: 1.08083541
INFO:root:[   36] Training loss: 0.05570128, Validation loss: 0.03658820, Gradient norm: 1.09022787
INFO:root:[   37] Training loss: 0.05475910, Validation loss: 0.03560767, Gradient norm: 1.04908184
INFO:root:[   38] Training loss: 0.05449956, Validation loss: 0.03334867, Gradient norm: 1.22702359
INFO:root:[   39] Training loss: 0.05381018, Validation loss: 0.04897403, Gradient norm: 1.18318505
INFO:root:[   40] Training loss: 0.05394628, Validation loss: 0.03251945, Gradient norm: 1.30531625
INFO:root:[   41] Training loss: 0.05241092, Validation loss: 0.04657941, Gradient norm: 1.16190604
INFO:root:[   42] Training loss: 0.05360360, Validation loss: 0.04614009, Gradient norm: 1.07667407
INFO:root:[   43] Training loss: 0.05274634, Validation loss: 0.03336552, Gradient norm: 1.32236839
INFO:root:[   44] Training loss: 0.05140530, Validation loss: 0.04895037, Gradient norm: 1.20911878
INFO:root:[   45] Training loss: 0.05138413, Validation loss: 0.03259666, Gradient norm: 1.14813888
INFO:root:[   46] Training loss: 0.05125364, Validation loss: 0.04705707, Gradient norm: 1.36757951
INFO:root:[   47] Training loss: 0.05065592, Validation loss: 0.03115907, Gradient norm: 1.29177535
INFO:root:[   48] Training loss: 0.05056543, Validation loss: 0.04627196, Gradient norm: 1.34498199
INFO:root:[   49] Training loss: 0.05031576, Validation loss: 0.03375434, Gradient norm: 1.10798730
INFO:root:[   50] Training loss: 0.04990058, Validation loss: 0.03831212, Gradient norm: 1.11802858
INFO:root:[   51] Training loss: 0.04996821, Validation loss: 0.03348976, Gradient norm: 1.40476689
INFO:root:[   52] Training loss: 0.04908846, Validation loss: 0.03516363, Gradient norm: 1.20960564
INFO:root:[   53] Training loss: 0.04853742, Validation loss: 0.04304023, Gradient norm: 1.19461962
INFO:root:[   54] Training loss: 0.04865735, Validation loss: 0.03044944, Gradient norm: 1.43316067
INFO:root:[   55] Training loss: 0.04888939, Validation loss: 0.04407922, Gradient norm: 1.35912254
INFO:root:[   56] Training loss: 0.04796929, Validation loss: 0.03076401, Gradient norm: 1.28041419
INFO:root:[   57] Training loss: 0.04827230, Validation loss: 0.03417286, Gradient norm: 1.18322008
INFO:root:[   58] Training loss: 0.04735281, Validation loss: 0.02995412, Gradient norm: 1.07550044
INFO:root:[   59] Training loss: 0.04697340, Validation loss: 0.03770987, Gradient norm: 1.40637441
INFO:root:[   60] Training loss: 0.04669468, Validation loss: 0.04030357, Gradient norm: 1.33210216
INFO:root:[   61] Training loss: 0.04676400, Validation loss: 0.04223580, Gradient norm: 1.17389714
INFO:root:[   62] Training loss: 0.04671133, Validation loss: 0.03176831, Gradient norm: 1.52513410
INFO:root:[   63] Training loss: 0.04592013, Validation loss: 0.02969991, Gradient norm: 1.45410960
INFO:root:[   64] Training loss: 0.04620971, Validation loss: 0.03092215, Gradient norm: 1.40061211
INFO:root:[   65] Training loss: 0.04617836, Validation loss: 0.03694947, Gradient norm: 1.54610578
INFO:root:[   66] Training loss: 0.04590081, Validation loss: 0.03669989, Gradient norm: 1.34562972
INFO:root:[   67] Training loss: 0.04564158, Validation loss: 0.02820341, Gradient norm: 1.48428983
INFO:root:[   68] Training loss: 0.04446566, Validation loss: 0.04355176, Gradient norm: 1.49749804
INFO:root:[   69] Training loss: 0.04453693, Validation loss: 0.04141684, Gradient norm: 1.37792234
INFO:root:[   70] Training loss: 0.04481592, Validation loss: 0.04202673, Gradient norm: 1.38383002
INFO:root:[   71] Training loss: 0.04427457, Validation loss: 0.04218402, Gradient norm: 1.41443787
INFO:root:[   72] Training loss: 0.04464735, Validation loss: 0.03349957, Gradient norm: 1.46327458
INFO:root:[   73] Training loss: 0.04360723, Validation loss: 0.02883617, Gradient norm: 1.63163868
INFO:root:[   74] Training loss: 0.04376057, Validation loss: 0.02998335, Gradient norm: 1.51590264
INFO:root:[   75] Training loss: 0.04346587, Validation loss: 0.03839451, Gradient norm: 1.52542825
INFO:root:[   76] Training loss: 0.04379052, Validation loss: 0.04616648, Gradient norm: 1.46642821
INFO:root:EP 76: Early stopping
INFO:root:Training the model took 1473.169s.
INFO:root:Emptying the cuda cache took 0.042s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03869
INFO:root:EnergyScoreTrain: 0.02823
INFO:root:CoverageTrain: 0.90335
INFO:root:IntervalWidthTrain: 0.05641
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03847
INFO:root:EnergyScoreValidation: 0.02809
INFO:root:CoverageValidation: 0.90364
INFO:root:IntervalWidthValidation: 0.05639
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03849
INFO:root:EnergyScoreTest: 0.02808
INFO:root:CoverageTest: 0.90598
INFO:root:IntervalWidthTest: 0.05626
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.52453250, Validation loss: 0.16008168, Gradient norm: 5.21923882
INFO:root:[    2] Training loss: 0.27845384, Validation loss: 0.11452327, Gradient norm: 5.32537352
INFO:root:[    3] Training loss: 0.23032052, Validation loss: 0.09711395, Gradient norm: 4.75973648
INFO:root:[    4] Training loss: 0.19311744, Validation loss: 0.09953814, Gradient norm: 3.18382376
INFO:root:[    5] Training loss: 0.17596136, Validation loss: 0.10792354, Gradient norm: 3.49454966
INFO:root:[    6] Training loss: 0.16036224, Validation loss: 0.11547309, Gradient norm: 3.23060313
INFO:root:[    7] Training loss: 0.15260199, Validation loss: 0.11003372, Gradient norm: 3.04567868
INFO:root:[    8] Training loss: 0.14576868, Validation loss: 0.07745320, Gradient norm: 3.12166869
INFO:root:[    9] Training loss: 0.13440989, Validation loss: 0.07458371, Gradient norm: 2.50839894
INFO:root:[   10] Training loss: 0.12749941, Validation loss: 0.08606257, Gradient norm: 1.80657986
INFO:root:[   11] Training loss: 0.12525307, Validation loss: 0.10438980, Gradient norm: 2.22324184
INFO:root:[   12] Training loss: 0.12090509, Validation loss: 0.10807096, Gradient norm: 1.82133789
INFO:root:[   13] Training loss: 0.12046000, Validation loss: 0.11031876, Gradient norm: 2.75107456
INFO:root:[   14] Training loss: 0.11620176, Validation loss: 0.11715183, Gradient norm: 2.44416792
INFO:root:[   15] Training loss: 0.11225236, Validation loss: 0.07345583, Gradient norm: 1.74520921
INFO:root:[   16] Training loss: 0.11077867, Validation loss: 0.07644788, Gradient norm: 2.07913423
INFO:root:[   17] Training loss: 0.10910242, Validation loss: 0.08086771, Gradient norm: 1.92297882
INFO:root:[   18] Training loss: 0.10536126, Validation loss: 0.08719655, Gradient norm: 1.64251442
INFO:root:[   19] Training loss: 0.10412987, Validation loss: 0.09605179, Gradient norm: 2.05966045
INFO:root:[   20] Training loss: 0.10394151, Validation loss: 0.09165547, Gradient norm: 1.57231835
INFO:root:[   21] Training loss: 0.10185984, Validation loss: 0.06852903, Gradient norm: 1.85697045
INFO:root:[   22] Training loss: 0.09850512, Validation loss: 0.08127438, Gradient norm: 1.73593151
INFO:root:[   23] Training loss: 0.09685366, Validation loss: 0.09953922, Gradient norm: 1.90758265
INFO:root:[   24] Training loss: 0.09539865, Validation loss: 0.09674805, Gradient norm: 1.94411553
INFO:root:[   25] Training loss: 0.09410494, Validation loss: 0.07993746, Gradient norm: 2.00928589
INFO:root:[   26] Training loss: 0.09308751, Validation loss: 0.06543718, Gradient norm: 1.71982365
INFO:root:[   27] Training loss: 0.09200189, Validation loss: 0.09934900, Gradient norm: 1.88001661
INFO:root:[   28] Training loss: 0.08943622, Validation loss: 0.09721610, Gradient norm: 1.96955696
INFO:root:[   29] Training loss: 0.08871079, Validation loss: 0.07115820, Gradient norm: 1.90763636
INFO:root:[   30] Training loss: 0.08622378, Validation loss: 0.10024614, Gradient norm: 1.64726118
INFO:root:[   31] Training loss: 0.08603661, Validation loss: 0.09927152, Gradient norm: 2.19736421
INFO:root:[   32] Training loss: 0.08362086, Validation loss: 0.06561043, Gradient norm: 1.73099479
INFO:root:[   33] Training loss: 0.08334025, Validation loss: 0.09258726, Gradient norm: 1.82488456
INFO:root:[   34] Training loss: 0.08197216, Validation loss: 0.06184039, Gradient norm: 1.88282655
INFO:root:[   35] Training loss: 0.07944955, Validation loss: 0.08383655, Gradient norm: 1.82935829
INFO:root:[   36] Training loss: 0.07809837, Validation loss: 0.05547699, Gradient norm: 1.86744273
INFO:root:[   37] Training loss: 0.07719955, Validation loss: 0.08205684, Gradient norm: 1.77477211
INFO:root:[   38] Training loss: 0.07603419, Validation loss: 0.07460622, Gradient norm: 1.98653281
INFO:root:[   39] Training loss: 0.07422433, Validation loss: 0.05152806, Gradient norm: 1.93106548
INFO:root:[   40] Training loss: 0.07322420, Validation loss: 0.06534832, Gradient norm: 2.21042922
INFO:root:[   41] Training loss: 0.07214363, Validation loss: 0.06077231, Gradient norm: 1.90374852
INFO:root:[   42] Training loss: 0.07150448, Validation loss: 0.08491326, Gradient norm: 1.84667491
INFO:root:[   43] Training loss: 0.07090927, Validation loss: 0.04674990, Gradient norm: 2.02986933
INFO:root:[   44] Training loss: 0.06937012, Validation loss: 0.08025486, Gradient norm: 2.07189207
INFO:root:[   45] Training loss: 0.06834155, Validation loss: 0.05170436, Gradient norm: 2.08227319
INFO:root:[   46] Training loss: 0.06691776, Validation loss: 0.07778793, Gradient norm: 1.96009845
INFO:root:[   47] Training loss: 0.06685247, Validation loss: 0.06897515, Gradient norm: 2.27882900
INFO:root:[   48] Training loss: 0.06517392, Validation loss: 0.04475465, Gradient norm: 2.01148736
INFO:root:[   49] Training loss: 0.06333028, Validation loss: 0.06589481, Gradient norm: 2.04202549
INFO:root:[   50] Training loss: 0.06381941, Validation loss: 0.07102465, Gradient norm: 1.98239501
INFO:root:[   51] Training loss: 0.06213375, Validation loss: 0.05442179, Gradient norm: 2.21440390
INFO:root:[   52] Training loss: 0.06032327, Validation loss: 0.04003037, Gradient norm: 1.93855563
INFO:root:[   53] Training loss: 0.06008031, Validation loss: 0.08470745, Gradient norm: 1.90920417
INFO:root:[   54] Training loss: 0.05964213, Validation loss: 0.05313706, Gradient norm: 2.25076238
INFO:root:[   55] Training loss: 0.06124132, Validation loss: 0.03984259, Gradient norm: 1.84342787
INFO:root:[   56] Training loss: 0.05674363, Validation loss: 0.06059328, Gradient norm: 1.84294954
INFO:root:[   57] Training loss: 0.05683086, Validation loss: 0.06237717, Gradient norm: 2.12342837
INFO:root:[   58] Training loss: 0.05555855, Validation loss: 0.03460310, Gradient norm: 1.94554885
INFO:root:[   59] Training loss: 0.05591933, Validation loss: 0.07060924, Gradient norm: 1.84995054
INFO:root:[   60] Training loss: 0.05465337, Validation loss: 0.04331902, Gradient norm: 2.04909661
INFO:root:[   61] Training loss: 0.05319227, Validation loss: 0.04719362, Gradient norm: 1.74495435
INFO:root:[   62] Training loss: 0.05227187, Validation loss: 0.03414365, Gradient norm: 1.70759650
INFO:root:[   63] Training loss: 0.05111999, Validation loss: 0.06134523, Gradient norm: 2.13689727
INFO:root:[   64] Training loss: 0.04945504, Validation loss: 0.03501463, Gradient norm: 1.84861055
INFO:root:[   65] Training loss: 0.05100884, Validation loss: 0.04190437, Gradient norm: 2.05153456
INFO:root:[   66] Training loss: 0.05101154, Validation loss: 0.05841587, Gradient norm: 1.23209638
INFO:root:[   67] Training loss: 0.04988909, Validation loss: 0.05560152, Gradient norm: 1.95313265
INFO:root:[   68] Training loss: 0.04935762, Validation loss: 0.05737846, Gradient norm: 2.40379941
INFO:root:[   69] Training loss: 0.04767400, Validation loss: 0.03051509, Gradient norm: 1.99692708
INFO:root:[   70] Training loss: 0.04629005, Validation loss: 0.03621252, Gradient norm: 2.00472618
INFO:root:[   71] Training loss: 0.04803826, Validation loss: 0.04236045, Gradient norm: 2.08363206
INFO:root:[   72] Training loss: 0.04626956, Validation loss: 0.04292374, Gradient norm: 2.15794269
INFO:root:[   73] Training loss: 0.04593007, Validation loss: 0.03216230, Gradient norm: 2.12140984
INFO:root:[   74] Training loss: 0.04494489, Validation loss: 0.03154379, Gradient norm: 1.71748884
INFO:root:[   75] Training loss: 0.04626290, Validation loss: 0.05071650, Gradient norm: 2.02464593
INFO:root:[   76] Training loss: 0.04541340, Validation loss: 0.03371041, Gradient norm: 2.11720291
INFO:root:[   77] Training loss: 0.04308778, Validation loss: 0.03968534, Gradient norm: 1.75899256
INFO:root:[   78] Training loss: 0.04270897, Validation loss: 0.04280575, Gradient norm: 1.92930004
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 1524.985s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04357
INFO:root:EnergyScoreTrain: 0.0306
INFO:root:CoverageTrain: 0.81253
INFO:root:IntervalWidthTrain: 0.03646
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04332
INFO:root:EnergyScoreValidation: 0.03039
INFO:root:CoverageValidation: 0.81276
INFO:root:IntervalWidthValidation: 0.0364
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04336
INFO:root:EnergyScoreTest: 0.03044
INFO:root:CoverageTest: 0.81519
INFO:root:IntervalWidthTest: 0.03637
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 662700032
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.63121340, Validation loss: 0.20781011, Gradient norm: 4.92491533
INFO:root:[    2] Training loss: 0.36099647, Validation loss: 0.15679970, Gradient norm: 4.22831617
INFO:root:[    3] Training loss: 0.28195966, Validation loss: 0.19462947, Gradient norm: 4.02170111
INFO:root:[    4] Training loss: 0.22239998, Validation loss: 0.11593921, Gradient norm: 1.90468304
INFO:root:[    5] Training loss: 0.19623373, Validation loss: 0.17195187, Gradient norm: 2.01640620
INFO:root:[    6] Training loss: 0.17535651, Validation loss: 0.21606536, Gradient norm: 1.17757867
INFO:root:[    7] Training loss: 0.16601008, Validation loss: 0.14461361, Gradient norm: 1.34779323
INFO:root:[    8] Training loss: 0.15636386, Validation loss: 0.17631360, Gradient norm: 0.96511183
INFO:root:[    9] Training loss: 0.14862770, Validation loss: 0.17099293, Gradient norm: 0.87593892
INFO:root:[   10] Training loss: 0.14469522, Validation loss: 0.21906675, Gradient norm: 0.99495040
INFO:root:[   11] Training loss: 0.13884000, Validation loss: 0.21645819, Gradient norm: 0.53584630
INFO:root:[   12] Training loss: 0.13470819, Validation loss: 0.18091089, Gradient norm: 1.07215054
INFO:root:[   13] Training loss: 0.13089334, Validation loss: 0.20582007, Gradient norm: 1.19486881
INFO:root:[   14] Training loss: 0.12671281, Validation loss: 0.22125295, Gradient norm: 0.96791972
INFO:root:[   15] Training loss: 0.12308348, Validation loss: 0.17893094, Gradient norm: 1.15996843
INFO:root:[   16] Training loss: 0.12016139, Validation loss: 0.16355211, Gradient norm: 1.10640511
INFO:root:[   17] Training loss: 0.11594326, Validation loss: 0.15062760, Gradient norm: 0.80480336
INFO:root:[   18] Training loss: 0.11283981, Validation loss: 0.17570374, Gradient norm: 0.89498427
INFO:root:[   19] Training loss: 0.10873620, Validation loss: 0.17749386, Gradient norm: 1.07701551
INFO:root:[   20] Training loss: 0.10653742, Validation loss: 0.19727400, Gradient norm: 1.14051798
INFO:root:[   21] Training loss: 0.10374196, Validation loss: 0.18595321, Gradient norm: 1.33016358
INFO:root:[   22] Training loss: 0.10038985, Validation loss: 0.13016104, Gradient norm: 1.00373651
INFO:root:[   23] Training loss: 0.09722804, Validation loss: 0.12201882, Gradient norm: 1.22612367
INFO:root:[   24] Training loss: 0.09467901, Validation loss: 0.10871688, Gradient norm: 1.30015145
INFO:root:[   25] Training loss: 0.09252637, Validation loss: 0.12983128, Gradient norm: 1.22837175
INFO:root:[   26] Training loss: 0.09031056, Validation loss: 0.13865994, Gradient norm: 1.22261202
INFO:root:[   27] Training loss: 0.08709966, Validation loss: 0.13813127, Gradient norm: 1.25686773
INFO:root:[   28] Training loss: 0.08394098, Validation loss: 0.13495699, Gradient norm: 1.15841572
INFO:root:[   29] Training loss: 0.08233914, Validation loss: 0.11308790, Gradient norm: 1.31688212
INFO:root:[   30] Training loss: 0.07918349, Validation loss: 0.14385421, Gradient norm: 1.16710370
INFO:root:[   31] Training loss: 0.07768419, Validation loss: 0.13012211, Gradient norm: 1.35652491
INFO:root:[   32] Training loss: 0.07595888, Validation loss: 0.08949033, Gradient norm: 1.11445925
INFO:root:[   33] Training loss: 0.07356898, Validation loss: 0.12580610, Gradient norm: 1.34432465
INFO:root:[   34] Training loss: 0.07165450, Validation loss: 0.09520371, Gradient norm: 1.30876832
INFO:root:[   35] Training loss: 0.06990673, Validation loss: 0.10211973, Gradient norm: 1.29208594
INFO:root:[   36] Training loss: 0.06722562, Validation loss: 0.09787641, Gradient norm: 1.28727410
INFO:root:[   37] Training loss: 0.06712451, Validation loss: 0.09356638, Gradient norm: 1.42416302
INFO:root:[   38] Training loss: 0.06364339, Validation loss: 0.06374013, Gradient norm: 1.16149033
INFO:root:[   39] Training loss: 0.06249158, Validation loss: 0.09534850, Gradient norm: 1.36557428
INFO:root:[   40] Training loss: 0.06226532, Validation loss: 0.07059846, Gradient norm: 1.52710250
INFO:root:[   41] Training loss: 0.05951324, Validation loss: 0.08119111, Gradient norm: 1.03244774
INFO:root:[   42] Training loss: 0.05737300, Validation loss: 0.07955803, Gradient norm: 1.04992120
INFO:root:[   43] Training loss: 0.05701427, Validation loss: 0.08276008, Gradient norm: 1.34817680
INFO:root:[   44] Training loss: 0.05526348, Validation loss: 0.07228429, Gradient norm: 1.13972314
INFO:root:[   45] Training loss: 0.05497933, Validation loss: 0.05711798, Gradient norm: 1.29267973
INFO:root:[   46] Training loss: 0.05361864, Validation loss: 0.05201843, Gradient norm: 1.35171638
INFO:root:[   47] Training loss: 0.05165440, Validation loss: 0.04273728, Gradient norm: 1.49246502
INFO:root:[   48] Training loss: 0.05175405, Validation loss: 0.07312274, Gradient norm: 1.45835426
INFO:root:[   49] Training loss: 0.04933370, Validation loss: 0.06465034, Gradient norm: 1.50045376
INFO:root:[   50] Training loss: 0.05124249, Validation loss: 0.05846463, Gradient norm: 1.38227168
INFO:root:[   51] Training loss: 0.04802650, Validation loss: 0.04146519, Gradient norm: 1.70368374
INFO:root:[   52] Training loss: 0.04907943, Validation loss: 0.04530648, Gradient norm: 1.47228804
INFO:root:[   53] Training loss: 0.04660672, Validation loss: 0.05031398, Gradient norm: 1.50487925
INFO:root:[   54] Training loss: 0.04838841, Validation loss: 0.04853607, Gradient norm: 1.75427749
INFO:root:[   55] Training loss: 0.04500728, Validation loss: 0.03425473, Gradient norm: 1.68848463
INFO:root:[   56] Training loss: 0.04610206, Validation loss: 0.03749849, Gradient norm: 1.46638732
INFO:root:[   57] Training loss: 0.04486785, Validation loss: 0.04264659, Gradient norm: 1.59260538
INFO:root:[   58] Training loss: 0.04520323, Validation loss: 0.03924001, Gradient norm: 1.64654575
INFO:root:[   59] Training loss: 0.04458800, Validation loss: 0.04790135, Gradient norm: 1.76440567
INFO:root:[   60] Training loss: 0.04351760, Validation loss: 0.03807506, Gradient norm: 1.83555268
INFO:root:[   61] Training loss: 0.04442143, Validation loss: 0.04715122, Gradient norm: 1.95795696
INFO:root:[   62] Training loss: 0.04465204, Validation loss: 0.03676556, Gradient norm: 1.87078176
INFO:root:[   63] Training loss: 0.04330042, Validation loss: 0.04157853, Gradient norm: 1.65594352
INFO:root:[   64] Training loss: 0.04425554, Validation loss: 0.04055288, Gradient norm: 1.79815417
INFO:root:[   65] Training loss: 0.04206949, Validation loss: 0.02927410, Gradient norm: 1.80993609
INFO:root:[   66] Training loss: 0.04184008, Validation loss: 0.03690279, Gradient norm: 1.95978715
INFO:root:[   67] Training loss: 0.04217745, Validation loss: 0.04884952, Gradient norm: 1.66960003
INFO:root:[   68] Training loss: 0.04289929, Validation loss: 0.03692565, Gradient norm: 2.03073744
INFO:root:[   69] Training loss: 0.04216627, Validation loss: 0.03370922, Gradient norm: 1.96674193
INFO:root:[   70] Training loss: 0.04186036, Validation loss: 0.03056381, Gradient norm: 1.87565398
INFO:root:[   71] Training loss: 0.04153069, Validation loss: 0.04253822, Gradient norm: 1.94821885
INFO:root:[   72] Training loss: 0.04165577, Validation loss: 0.02733166, Gradient norm: 2.01276787
INFO:root:[   73] Training loss: 0.04130823, Validation loss: 0.03413761, Gradient norm: 2.08919289
INFO:root:[   74] Training loss: 0.04098454, Validation loss: 0.03192944, Gradient norm: 2.27733018
INFO:root:[   75] Training loss: 0.04111800, Validation loss: 0.03531539, Gradient norm: 2.16754715
INFO:root:[   76] Training loss: 0.04057557, Validation loss: 0.03032656, Gradient norm: 1.83986662
INFO:root:[   77] Training loss: 0.04051996, Validation loss: 0.03042110, Gradient norm: 2.13495300
INFO:root:[   78] Training loss: 0.04131128, Validation loss: 0.03954073, Gradient norm: 2.38068430
INFO:root:[   79] Training loss: 0.03987952, Validation loss: 0.04023211, Gradient norm: 2.04132267
INFO:root:[   80] Training loss: 0.04146905, Validation loss: 0.02628596, Gradient norm: 2.46237481
INFO:root:[   81] Training loss: 0.04086486, Validation loss: 0.04059737, Gradient norm: 2.10694834
INFO:root:[   82] Training loss: 0.04032601, Validation loss: 0.04418716, Gradient norm: 2.19263345
INFO:root:[   83] Training loss: 0.03997803, Validation loss: 0.04113516, Gradient norm: 2.42107837
INFO:root:[   84] Training loss: 0.03987116, Validation loss: 0.02766470, Gradient norm: 1.94051921
INFO:root:[   85] Training loss: 0.03903890, Validation loss: 0.02727494, Gradient norm: 1.85874672
INFO:root:[   86] Training loss: 0.03868969, Validation loss: 0.02723409, Gradient norm: 2.38897211
INFO:root:[   87] Training loss: 0.03845535, Validation loss: 0.04105946, Gradient norm: 2.27185137
INFO:root:[   88] Training loss: 0.03868858, Validation loss: 0.03102927, Gradient norm: 2.45194643
INFO:root:[   89] Training loss: 0.03868319, Validation loss: 0.03522266, Gradient norm: 2.42431820
INFO:root:EP 89: Early stopping
INFO:root:Training the model took 1731.076s.
INFO:root:Emptying the cuda cache took 0.047s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03644
INFO:root:EnergyScoreTrain: 0.02711
INFO:root:CoverageTrain: 0.04078
INFO:root:IntervalWidthTrain: 0.00687
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03621
INFO:root:EnergyScoreValidation: 0.02685
INFO:root:CoverageValidation: 0.04126
INFO:root:IntervalWidthValidation: 0.00694
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03612
INFO:root:EnergyScoreTest: 0.02751
INFO:root:CoverageTest: 0.04022
INFO:root:IntervalWidthTest: 0.00672
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.75862587, Validation loss: 0.21755732, Gradient norm: 5.33679297
INFO:root:[    2] Training loss: 0.42042095, Validation loss: 0.16295397, Gradient norm: 3.61929814
INFO:root:[    3] Training loss: 0.34695053, Validation loss: 0.15474962, Gradient norm: 4.47007425
INFO:root:[    4] Training loss: 0.29414043, Validation loss: 0.15608979, Gradient norm: 3.68839478
INFO:root:[    5] Training loss: 0.25982667, Validation loss: 0.14263750, Gradient norm: 3.38834225
INFO:root:[    6] Training loss: 0.23078051, Validation loss: 0.14466800, Gradient norm: 2.22737031
INFO:root:[    7] Training loss: 0.21618602, Validation loss: 0.28330794, Gradient norm: 2.51172183
INFO:root:[    8] Training loss: 0.20405648, Validation loss: 0.25342086, Gradient norm: 2.48271605
INFO:root:[    9] Training loss: 0.19294225, Validation loss: 0.25439888, Gradient norm: 1.57474835
INFO:root:[   10] Training loss: 0.18536493, Validation loss: 0.30169575, Gradient norm: 1.98272837
INFO:root:[   11] Training loss: 0.17548208, Validation loss: 0.29450161, Gradient norm: 1.47450303
INFO:root:[   12] Training loss: 0.16910090, Validation loss: 0.29913955, Gradient norm: 1.74179922
INFO:root:[   13] Training loss: 0.16289925, Validation loss: 0.27924252, Gradient norm: 1.80119090
INFO:root:[   14] Training loss: 0.15713439, Validation loss: 0.23922763, Gradient norm: 1.93483419
INFO:root:[   15] Training loss: 0.15170520, Validation loss: 0.21526581, Gradient norm: 1.74209012
INFO:root:[   16] Training loss: 0.14769150, Validation loss: 0.25030115, Gradient norm: 2.38026174
INFO:root:[   17] Training loss: 0.14056736, Validation loss: 0.22552240, Gradient norm: 1.93052162
INFO:root:[   18] Training loss: 0.13596025, Validation loss: 0.26787614, Gradient norm: 2.14634007
INFO:root:[   19] Training loss: 0.13106379, Validation loss: 0.27719766, Gradient norm: 2.11705002
INFO:root:[   20] Training loss: 0.12609792, Validation loss: 0.21609637, Gradient norm: 2.00344958
INFO:root:[   21] Training loss: 0.12278126, Validation loss: 0.23850336, Gradient norm: 2.22703094
INFO:root:[   22] Training loss: 0.11760249, Validation loss: 0.18832565, Gradient norm: 2.01867451
INFO:root:[   23] Training loss: 0.11399903, Validation loss: 0.22835870, Gradient norm: 2.05119350
INFO:root:[   24] Training loss: 0.10950374, Validation loss: 0.24304591, Gradient norm: 2.41487318
INFO:root:[   25] Training loss: 0.10579732, Validation loss: 0.23700942, Gradient norm: 2.26938508
INFO:root:[   26] Training loss: 0.10109825, Validation loss: 0.21328028, Gradient norm: 2.07123556
INFO:root:[   27] Training loss: 0.09690349, Validation loss: 0.20724081, Gradient norm: 1.95891724
INFO:root:[   28] Training loss: 0.09360691, Validation loss: 0.15715810, Gradient norm: 2.36065079
INFO:root:[   29] Training loss: 0.08925707, Validation loss: 0.15548327, Gradient norm: 2.38927925
INFO:root:[   30] Training loss: 0.08703072, Validation loss: 0.13186180, Gradient norm: 2.22153613
INFO:root:[   31] Training loss: 0.08376763, Validation loss: 0.13121679, Gradient norm: 2.77500654
INFO:root:[   32] Training loss: 0.08030374, Validation loss: 0.11474553, Gradient norm: 2.72427572
INFO:root:[   33] Training loss: 0.07803910, Validation loss: 0.11315787, Gradient norm: 2.04202634
INFO:root:[   34] Training loss: 0.07390383, Validation loss: 0.12022772, Gradient norm: 2.09092418
INFO:root:[   35] Training loss: 0.07154159, Validation loss: 0.10049093, Gradient norm: 2.57586777
INFO:root:[   36] Training loss: 0.06905121, Validation loss: 0.09194181, Gradient norm: 2.22382825
INFO:root:[   37] Training loss: 0.07012501, Validation loss: 0.11452654, Gradient norm: 2.23333014
INFO:root:[   38] Training loss: 0.06560623, Validation loss: 0.10805858, Gradient norm: 2.05088468
INFO:root:[   39] Training loss: 0.06425607, Validation loss: 0.07154635, Gradient norm: 2.52704477
INFO:root:[   40] Training loss: 0.06266421, Validation loss: 0.09117685, Gradient norm: 2.06648789
INFO:root:[   41] Training loss: 0.05844044, Validation loss: 0.09712679, Gradient norm: 2.00840004
INFO:root:[   42] Training loss: 0.05802341, Validation loss: 0.07505766, Gradient norm: 2.10952489
INFO:root:[   43] Training loss: 0.05607287, Validation loss: 0.06157440, Gradient norm: 2.30916004
INFO:root:[   44] Training loss: 0.05570052, Validation loss: 0.06344198, Gradient norm: 2.47844768
INFO:root:[   45] Training loss: 0.05359843, Validation loss: 0.06297351, Gradient norm: 2.43638761
INFO:root:[   46] Training loss: 0.05356800, Validation loss: 0.07334775, Gradient norm: 2.67858901
INFO:root:[   47] Training loss: 0.05346916, Validation loss: 0.06804504, Gradient norm: 2.82105947
INFO:root:[   48] Training loss: 0.05402360, Validation loss: 0.05728391, Gradient norm: 2.61338576
INFO:root:[   49] Training loss: 0.05116354, Validation loss: 0.06670207, Gradient norm: 2.55321454
INFO:root:[   50] Training loss: 0.05218410, Validation loss: 0.05620559, Gradient norm: 2.73214364
INFO:root:[   51] Training loss: 0.05282719, Validation loss: 0.05122106, Gradient norm: 2.57053202
INFO:root:[   52] Training loss: 0.05121176, Validation loss: 0.06859061, Gradient norm: 2.48611106
INFO:root:[   53] Training loss: 0.05188937, Validation loss: 0.05483512, Gradient norm: 2.28812290
INFO:root:[   54] Training loss: 0.05069986, Validation loss: 0.05642723, Gradient norm: 2.34584406
INFO:root:[   55] Training loss: 0.04992196, Validation loss: 0.05699151, Gradient norm: 2.27140656
INFO:root:[   56] Training loss: 0.04831205, Validation loss: 0.05155332, Gradient norm: 2.63483996
INFO:root:[   57] Training loss: 0.04862025, Validation loss: 0.05247015, Gradient norm: 2.34557618
INFO:root:[   58] Training loss: 0.04897239, Validation loss: 0.06397496, Gradient norm: 2.38312368
INFO:root:[   59] Training loss: 0.04951814, Validation loss: 0.04887620, Gradient norm: 2.49945849
INFO:root:[   60] Training loss: 0.05183264, Validation loss: 0.05491357, Gradient norm: 2.94667715
INFO:root:[   61] Training loss: 0.04915534, Validation loss: 0.06556119, Gradient norm: 2.07477595
INFO:root:[   62] Training loss: 0.04987672, Validation loss: 0.06595595, Gradient norm: 2.84272670
INFO:root:[   63] Training loss: 0.05015914, Validation loss: 0.04955027, Gradient norm: 2.64536964
INFO:root:[   64] Training loss: 0.04853423, Validation loss: 0.05194644, Gradient norm: 2.09194519
INFO:root:[   65] Training loss: 0.04804113, Validation loss: 0.05584147, Gradient norm: 2.26854248
INFO:root:[   66] Training loss: 0.04594504, Validation loss: 0.04923146, Gradient norm: 2.31204614
INFO:root:[   67] Training loss: 0.04830457, Validation loss: 0.06604142, Gradient norm: 2.73288108
INFO:root:[   68] Training loss: 0.04769181, Validation loss: 0.05120219, Gradient norm: 2.54486287
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 1329.488s.
INFO:root:Emptying the cuda cache took 0.043s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04969
INFO:root:EnergyScoreTrain: 0.04683
INFO:root:CoverageTrain: 0.00085
INFO:root:IntervalWidthTrain: 1e-05
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04915
INFO:root:EnergyScoreValidation: 0.04617
INFO:root:CoverageValidation: 0.00085
INFO:root:IntervalWidthValidation: 1e-05
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04908
INFO:root:EnergyScoreTest: 0.04644
INFO:root:CoverageTest: 0.00086
INFO:root:IntervalWidthTest: 1e-05
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.79625388, Validation loss: 0.25170003, Gradient norm: 4.14594712
INFO:root:[    2] Training loss: 0.43626703, Validation loss: 0.30243070, Gradient norm: 3.22172086
INFO:root:[    3] Training loss: 0.34122945, Validation loss: 0.16435344, Gradient norm: 2.39555072
INFO:root:[    4] Training loss: 0.28186495, Validation loss: 0.20355446, Gradient norm: 1.22481000
INFO:root:[    5] Training loss: 0.25805775, Validation loss: 0.22029465, Gradient norm: 1.42400953
INFO:root:[    6] Training loss: 0.23803998, Validation loss: 0.32424520, Gradient norm: 1.11612669
INFO:root:[    7] Training loss: 0.22286048, Validation loss: 0.39402197, Gradient norm: 0.85620812
INFO:root:[    8] Training loss: 0.21010266, Validation loss: 0.32510247, Gradient norm: 0.67843044
INFO:root:[    9] Training loss: 0.20095764, Validation loss: 0.37713432, Gradient norm: 0.64325574
INFO:root:[   10] Training loss: 0.19149140, Validation loss: 0.34987915, Gradient norm: 0.43709843
INFO:root:[   11] Training loss: 0.18396692, Validation loss: 0.33182278, Gradient norm: 0.63239001
INFO:root:[   12] Training loss: 0.17629465, Validation loss: 0.34360854, Gradient norm: 0.72221055
INFO:root:[   13] Training loss: 0.16901371, Validation loss: 0.36736328, Gradient norm: 0.70510006
INFO:root:[   14] Training loss: 0.15992778, Validation loss: 0.36352000, Gradient norm: 0.50873894
INFO:root:[   15] Training loss: 0.15385315, Validation loss: 0.30550658, Gradient norm: 0.66667981
INFO:root:[   16] Training loss: 0.14772386, Validation loss: 0.34907308, Gradient norm: 0.67678987
INFO:root:[   17] Training loss: 0.14019333, Validation loss: 0.34314910, Gradient norm: 0.75038718
INFO:root:[   18] Training loss: 0.13419976, Validation loss: 0.35245881, Gradient norm: 0.79955570
INFO:root:[   19] Training loss: 0.12548108, Validation loss: 0.28790957, Gradient norm: 0.40487011
INFO:root:[   20] Training loss: 0.12100767, Validation loss: 0.25668967, Gradient norm: 0.86829094
INFO:root:[   21] Training loss: 0.11660983, Validation loss: 0.27351996, Gradient norm: 0.75979029
INFO:root:[   22] Training loss: 0.10859381, Validation loss: 0.27569558, Gradient norm: 0.48753058
INFO:root:[   23] Training loss: 0.10309469, Validation loss: 0.23572197, Gradient norm: 0.76080822
INFO:root:[   24] Training loss: 0.09818435, Validation loss: 0.20835397, Gradient norm: 0.78942159
INFO:root:[   25] Training loss: 0.09264883, Validation loss: 0.20849220, Gradient norm: 0.70767565
INFO:root:[   26] Training loss: 0.08895864, Validation loss: 0.17595303, Gradient norm: 0.77895250
INFO:root:[   27] Training loss: 0.08393254, Validation loss: 0.18256555, Gradient norm: 0.86347108
INFO:root:[   28] Training loss: 0.08028398, Validation loss: 0.17591393, Gradient norm: 1.02491868
INFO:root:[   29] Training loss: 0.07729583, Validation loss: 0.17587744, Gradient norm: 0.88813676
INFO:root:[   30] Training loss: 0.07238751, Validation loss: 0.13419775, Gradient norm: 0.70401424
INFO:root:[   31] Training loss: 0.06884867, Validation loss: 0.13146638, Gradient norm: 0.56636283
INFO:root:[   32] Training loss: 0.07103595, Validation loss: 0.11088823, Gradient norm: 1.00967527
INFO:root:[   33] Training loss: 0.06823740, Validation loss: 0.08765553, Gradient norm: 0.93055683
INFO:root:[   34] Training loss: 0.06573188, Validation loss: 0.07700981, Gradient norm: 0.91919928
INFO:root:[   35] Training loss: 0.06278794, Validation loss: 0.08619160, Gradient norm: 0.86374759
INFO:root:[   36] Training loss: 0.06102639, Validation loss: 0.08990594, Gradient norm: 0.96070224
INFO:root:[   37] Training loss: 0.06063117, Validation loss: 0.05934676, Gradient norm: 0.98340849
INFO:root:[   38] Training loss: 0.05705829, Validation loss: 0.06142399, Gradient norm: 0.86970556
INFO:root:[   39] Training loss: 0.05827056, Validation loss: 0.06787336, Gradient norm: 1.06438134
INFO:root:[   40] Training loss: 0.05867665, Validation loss: 0.05377835, Gradient norm: 1.13414363
INFO:root:[   41] Training loss: 0.05621950, Validation loss: 0.06223270, Gradient norm: 0.99833324
INFO:root:[   42] Training loss: 0.05578792, Validation loss: 0.06016862, Gradient norm: 1.00567941
INFO:root:[   43] Training loss: 0.05356983, Validation loss: 0.05144697, Gradient norm: 0.98479292
INFO:root:[   44] Training loss: 0.05416729, Validation loss: 0.05773457, Gradient norm: 0.97044828
INFO:root:[   45] Training loss: 0.05528727, Validation loss: 0.04519510, Gradient norm: 1.13224987
INFO:root:[   46] Training loss: 0.05386249, Validation loss: 0.06093528, Gradient norm: 1.12526521
INFO:root:[   47] Training loss: 0.05409234, Validation loss: 0.04238687, Gradient norm: 1.18149063
INFO:root:[   48] Training loss: 0.05414532, Validation loss: 0.04734465, Gradient norm: 1.13100094
INFO:root:[   49] Training loss: 0.05287252, Validation loss: 0.05931361, Gradient norm: 1.13554982
INFO:root:[   50] Training loss: 0.05369848, Validation loss: 0.05100433, Gradient norm: 1.24177009
INFO:root:[   51] Training loss: 0.05281012, Validation loss: 0.06691468, Gradient norm: 1.15924613
INFO:root:[   52] Training loss: 0.05273971, Validation loss: 0.05114937, Gradient norm: 1.16427275
INFO:root:[   53] Training loss: 0.05170200, Validation loss: 0.07144581, Gradient norm: 1.14105769
INFO:root:[   54] Training loss: 0.05291501, Validation loss: 0.04934106, Gradient norm: 1.26689594
INFO:root:[   55] Training loss: 0.05195140, Validation loss: 0.05163242, Gradient norm: 1.05089673
INFO:root:[   56] Training loss: 0.05139866, Validation loss: 0.04941163, Gradient norm: 1.12940316
INFO:root:[   57] Training loss: 0.05047431, Validation loss: 0.06655481, Gradient norm: 1.05200233
INFO:root:[   58] Training loss: 0.05085516, Validation loss: 0.06894686, Gradient norm: 1.23267101
INFO:root:[   59] Training loss: 0.05082316, Validation loss: 0.05790981, Gradient norm: 1.30329673
INFO:root:[   60] Training loss: 0.05077948, Validation loss: 0.04962825, Gradient norm: 1.14022375
INFO:root:[   61] Training loss: 0.05119013, Validation loss: 0.04542941, Gradient norm: 1.31463771
INFO:root:[   62] Training loss: 0.05235039, Validation loss: 0.07012951, Gradient norm: 1.45129794
INFO:root:[   63] Training loss: 0.05089431, Validation loss: 0.06250267, Gradient norm: 1.40327845
INFO:root:[   64] Training loss: 0.04983259, Validation loss: 0.04797412, Gradient norm: 1.17923710
INFO:root:[   65] Training loss: 0.04922302, Validation loss: 0.05699622, Gradient norm: 1.21954710
INFO:root:[   66] Training loss: 0.05100350, Validation loss: 0.05861951, Gradient norm: 1.38656182
INFO:root:[   67] Training loss: 0.04880701, Validation loss: 0.05368723, Gradient norm: 1.24082848
INFO:root:[   68] Training loss: 0.04969571, Validation loss: 0.06624066, Gradient norm: 1.32710632
INFO:root:[   69] Training loss: 0.04977161, Validation loss: 0.04093622, Gradient norm: 1.38557619
INFO:root:[   70] Training loss: 0.04980404, Validation loss: 0.06204878, Gradient norm: 1.37495706
INFO:root:[   71] Training loss: 0.04902201, Validation loss: 0.07492935, Gradient norm: 1.33836041
INFO:root:[   72] Training loss: 0.04967565, Validation loss: 0.06690602, Gradient norm: 1.43364354
INFO:root:[   73] Training loss: 0.04864693, Validation loss: 0.04874432, Gradient norm: 1.36187721
INFO:root:[   74] Training loss: 0.04803183, Validation loss: 0.07500153, Gradient norm: 1.17123452
INFO:root:[   75] Training loss: 0.04834015, Validation loss: 0.04096077, Gradient norm: 1.34171517
INFO:root:[   76] Training loss: 0.04769640, Validation loss: 0.06747731, Gradient norm: 1.25631252
INFO:root:[   77] Training loss: 0.04815981, Validation loss: 0.05656824, Gradient norm: 1.40347248
INFO:root:[   78] Training loss: 0.04780583, Validation loss: 0.05968740, Gradient norm: 1.37629621
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 1523.163s.
INFO:root:Emptying the cuda cache took 0.043s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05397
INFO:root:EnergyScoreTrain: 0.03148
INFO:root:CoverageTrain: 0.02746
INFO:root:IntervalWidthTrain: 0.00495
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05368
INFO:root:EnergyScoreValidation: 0.03076
INFO:root:CoverageValidation: 0.02753
INFO:root:IntervalWidthValidation: 0.00498
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.05393
INFO:root:EnergyScoreTest: 0.03249
INFO:root:CoverageTest: 0.0267
INFO:root:IntervalWidthTest: 0.00477
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 662700032
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.88480273, Validation loss: 0.22542278, Gradient norm: 3.70864792
INFO:root:[    2] Training loss: 0.46324918, Validation loss: 0.28259250, Gradient norm: 2.21103956
INFO:root:[    3] Training loss: 0.36869368, Validation loss: 0.24448156, Gradient norm: 2.05870502
INFO:root:[    4] Training loss: 0.31285412, Validation loss: 0.36664061, Gradient norm: 2.11580922
INFO:root:[    5] Training loss: 0.26847283, Validation loss: 0.53828548, Gradient norm: 1.24245733
INFO:root:[    6] Training loss: 0.24437550, Validation loss: 0.52316048, Gradient norm: 1.10518236
INFO:root:[    7] Training loss: 0.22269312, Validation loss: 0.57283886, Gradient norm: 0.90336138
INFO:root:[    8] Training loss: 0.20744255, Validation loss: 0.47991541, Gradient norm: 1.14916914
INFO:root:[    9] Training loss: 0.19141328, Validation loss: 0.55844342, Gradient norm: 0.83834914
INFO:root:[   10] Training loss: 0.17819340, Validation loss: 0.52970703, Gradient norm: 1.07681517
INFO:root:[   11] Training loss: 0.16473929, Validation loss: 0.57038888, Gradient norm: 1.02024454
INFO:root:[   12] Training loss: 0.15364968, Validation loss: 0.52183781, Gradient norm: 1.46896095
INFO:root:[   13] Training loss: 0.14266559, Validation loss: 0.48852631, Gradient norm: 1.03670775
INFO:root:[   14] Training loss: 0.13180261, Validation loss: 0.44135639, Gradient norm: 1.63815326
INFO:root:[   15] Training loss: 0.12387309, Validation loss: 0.46321829, Gradient norm: 1.56101987
INFO:root:[   16] Training loss: 0.11480609, Validation loss: 0.43343208, Gradient norm: 1.79476146
INFO:root:[   17] Training loss: 0.10663727, Validation loss: 0.40765364, Gradient norm: 1.38869723
INFO:root:[   18] Training loss: 0.09924565, Validation loss: 0.34767378, Gradient norm: 1.40430517
INFO:root:[   19] Training loss: 0.09344809, Validation loss: 0.33290830, Gradient norm: 1.72833538
INFO:root:[   20] Training loss: 0.08882903, Validation loss: 0.27223648, Gradient norm: 1.94458538
INFO:root:[   21] Training loss: 0.08453828, Validation loss: 0.20621281, Gradient norm: 1.72395976
INFO:root:[   22] Training loss: 0.07832668, Validation loss: 0.20877509, Gradient norm: 1.42695726
INFO:root:[   23] Training loss: 0.07501271, Validation loss: 0.17082241, Gradient norm: 1.41009579
INFO:root:[   24] Training loss: 0.07392788, Validation loss: 0.14038793, Gradient norm: 1.79285913
INFO:root:[   25] Training loss: 0.07046212, Validation loss: 0.14388618, Gradient norm: 1.63262804
INFO:root:[   26] Training loss: 0.07235512, Validation loss: 0.16157845, Gradient norm: 1.95489954
INFO:root:[   27] Training loss: 0.07089187, Validation loss: 0.12939288, Gradient norm: 1.97003128
INFO:root:[   28] Training loss: 0.06957913, Validation loss: 0.12781445, Gradient norm: 1.65581171
INFO:root:[   29] Training loss: 0.06954107, Validation loss: 0.12851803, Gradient norm: 1.87692313
INFO:root:[   30] Training loss: 0.06876288, Validation loss: 0.11778658, Gradient norm: 1.84808876
INFO:root:[   31] Training loss: 0.06871880, Validation loss: 0.14687657, Gradient norm: 1.72171574
INFO:root:[   32] Training loss: 0.06860284, Validation loss: 0.14404988, Gradient norm: 1.90683023
INFO:root:[   33] Training loss: 0.06600433, Validation loss: 0.13712856, Gradient norm: 1.56278004
INFO:root:[   34] Training loss: 0.06610512, Validation loss: 0.16711204, Gradient norm: 1.91848258
INFO:root:[   35] Training loss: 0.06793066, Validation loss: 0.15804258, Gradient norm: 1.97030273
INFO:root:[   36] Training loss: 0.06668175, Validation loss: 0.13528802, Gradient norm: 1.76030794
INFO:root:[   37] Training loss: 0.06525040, Validation loss: 0.14166409, Gradient norm: 1.88588925
INFO:root:[   38] Training loss: 0.06560077, Validation loss: 0.12507950, Gradient norm: 1.77515248
INFO:root:[   39] Training loss: 0.06593994, Validation loss: 0.12385212, Gradient norm: 1.94328348
INFO:root:[   40] Training loss: 0.06500838, Validation loss: 0.11923332, Gradient norm: 2.07618685
INFO:root:[   41] Training loss: 0.06441350, Validation loss: 0.11892242, Gradient norm: 1.71764301
INFO:root:[   42] Training loss: 0.06391451, Validation loss: 0.13795520, Gradient norm: 1.69203145
INFO:root:[   43] Training loss: 0.06365960, Validation loss: 0.17388337, Gradient norm: 1.68837650
INFO:root:[   44] Training loss: 0.06377230, Validation loss: 0.13724731, Gradient norm: 2.12590253
INFO:root:[   45] Training loss: 0.06482354, Validation loss: 0.16731727, Gradient norm: 1.90972656
INFO:root:[   46] Training loss: 0.06321270, Validation loss: 0.13697827, Gradient norm: 2.00467502
INFO:root:[   47] Training loss: 0.06421113, Validation loss: 0.18393101, Gradient norm: 1.92400880
INFO:root:[   48] Training loss: 0.06233610, Validation loss: 0.12247239, Gradient norm: 1.80359384
INFO:root:[   49] Training loss: 0.06321105, Validation loss: 0.13563746, Gradient norm: 2.10400064
INFO:root:[   50] Training loss: 0.06400844, Validation loss: 0.12740559, Gradient norm: 2.01673113
INFO:root:[   51] Training loss: 0.06234189, Validation loss: 0.17063724, Gradient norm: 1.53291018
INFO:root:[   52] Training loss: 0.06258937, Validation loss: 0.15109303, Gradient norm: 1.47154623
INFO:root:[   53] Training loss: 0.06052245, Validation loss: 0.14853982, Gradient norm: 1.38739067
INFO:root:[   54] Training loss: 0.06044877, Validation loss: 0.16065905, Gradient norm: 1.60172174
INFO:root:[   55] Training loss: 0.06166342, Validation loss: 0.15417587, Gradient norm: 1.73348782
INFO:root:[   56] Training loss: 0.05913928, Validation loss: 0.14758121, Gradient norm: 1.76461274
INFO:root:[   57] Training loss: 0.05961617, Validation loss: 0.14897498, Gradient norm: 1.76054275
INFO:root:[   58] Training loss: 0.06026915, Validation loss: 0.15446829, Gradient norm: 1.69612036
INFO:root:[   59] Training loss: 0.06042645, Validation loss: 0.12210370, Gradient norm: 1.37284568
INFO:root:[   60] Training loss: 0.05848987, Validation loss: 0.12590278, Gradient norm: 1.71406494
INFO:root:[   61] Training loss: 0.06059086, Validation loss: 0.11657886, Gradient norm: 1.85491340
INFO:root:[   62] Training loss: 0.06060401, Validation loss: 0.12577686, Gradient norm: 1.85774849
INFO:root:[   63] Training loss: 0.06088910, Validation loss: 0.12869202, Gradient norm: 1.84203954
INFO:root:[   64] Training loss: 0.05985997, Validation loss: 0.16264699, Gradient norm: 1.35288756
INFO:root:[   65] Training loss: 0.05961444, Validation loss: 0.17226241, Gradient norm: 1.48736992
INFO:root:[   66] Training loss: 0.06026252, Validation loss: 0.17330405, Gradient norm: 1.57495179
INFO:root:[   67] Training loss: 0.05860406, Validation loss: 0.15226922, Gradient norm: 1.38472907
INFO:root:[   68] Training loss: 0.05896284, Validation loss: 0.14631412, Gradient norm: 1.62895816
INFO:root:[   69] Training loss: 0.05792747, Validation loss: 0.13347178, Gradient norm: 1.39784860
INFO:root:[   70] Training loss: 0.05891640, Validation loss: 0.16038536, Gradient norm: 1.67888104
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1368.734s.
INFO:root:Emptying the cuda cache took 0.044s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.11746
INFO:root:EnergyScoreTrain: 0.11413
INFO:root:CoverageTrain: 1e-05
INFO:root:IntervalWidthTrain: 2e-05
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.11686
INFO:root:EnergyScoreValidation: 0.11366
INFO:root:CoverageValidation: 1e-05
INFO:root:IntervalWidthValidation: 2e-05
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.11851
INFO:root:EnergyScoreTest: 0.11569
INFO:root:CoverageTest: 1e-05
INFO:root:IntervalWidthTest: 2e-05
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.47818082, Validation loss: 0.19610960, Gradient norm: 8.12073517
INFO:root:[    2] Training loss: 0.25707970, Validation loss: 0.12787458, Gradient norm: 11.20858965
INFO:root:[    3] Training loss: 0.20462375, Validation loss: 0.12746287, Gradient norm: 7.98135825
INFO:root:[    4] Training loss: 0.18243665, Validation loss: 0.09473105, Gradient norm: 7.05603400
INFO:root:[    5] Training loss: 0.17322742, Validation loss: 0.09577956, Gradient norm: 7.69865280
INFO:root:[    6] Training loss: 0.14668750, Validation loss: 0.09373952, Gradient norm: 5.54600703
INFO:root:[    7] Training loss: 0.13824980, Validation loss: 0.10417487, Gradient norm: 5.70856789
INFO:root:[    8] Training loss: 0.12856852, Validation loss: 0.07700129, Gradient norm: 5.08598054
INFO:root:[    9] Training loss: 0.12278648, Validation loss: 0.08861925, Gradient norm: 3.23338141
INFO:root:[   10] Training loss: 0.11526828, Validation loss: 0.06842409, Gradient norm: 4.43728924
INFO:root:[   11] Training loss: 0.11041990, Validation loss: 0.06646774, Gradient norm: 4.24763069
INFO:root:[   12] Training loss: 0.10434032, Validation loss: 0.06632634, Gradient norm: 3.15280740
INFO:root:[   13] Training loss: 0.10356263, Validation loss: 0.06472519, Gradient norm: 3.62164116
INFO:root:[   14] Training loss: 0.09967770, Validation loss: 0.06507295, Gradient norm: 2.07774591
INFO:root:[   15] Training loss: 0.09721925, Validation loss: 0.06341266, Gradient norm: 2.71404747
INFO:root:[   16] Training loss: 0.09446381, Validation loss: 0.07226886, Gradient norm: 2.25046498
INFO:root:[   17] Training loss: 0.09310463, Validation loss: 0.07037563, Gradient norm: 1.76841590
INFO:root:[   18] Training loss: 0.09113721, Validation loss: 0.07037607, Gradient norm: 2.31006378
INFO:root:[   19] Training loss: 0.08920501, Validation loss: 0.08163928, Gradient norm: 2.59849216
INFO:root:[   20] Training loss: 0.08924986, Validation loss: 0.05507047, Gradient norm: 1.92614368
INFO:root:[   21] Training loss: 0.08616967, Validation loss: 0.06582610, Gradient norm: 2.17902074
INFO:root:[   22] Training loss: 0.08651090, Validation loss: 0.06414176, Gradient norm: 2.24175706
INFO:root:[   23] Training loss: 0.08495527, Validation loss: 0.05302094, Gradient norm: 2.00909311
INFO:root:[   24] Training loss: 0.08541976, Validation loss: 0.06024598, Gradient norm: 1.87858501
INFO:root:[   25] Training loss: 0.08413363, Validation loss: 0.05511957, Gradient norm: 1.87650315
INFO:root:[   26] Training loss: 0.08223890, Validation loss: 0.04998446, Gradient norm: 2.36462983
INFO:root:[   27] Training loss: 0.08133467, Validation loss: 0.05310169, Gradient norm: 2.20489699
INFO:root:[   28] Training loss: 0.07963227, Validation loss: 0.05914603, Gradient norm: 1.89349245
INFO:root:[   29] Training loss: 0.07939620, Validation loss: 0.05165778, Gradient norm: 2.30589549
INFO:root:[   30] Training loss: 0.07786399, Validation loss: 0.06741409, Gradient norm: 2.13196158
INFO:root:[   31] Training loss: 0.08011040, Validation loss: 0.05767293, Gradient norm: 1.52110897
INFO:root:[   32] Training loss: 0.07791548, Validation loss: 0.06778221, Gradient norm: 2.08170149
INFO:root:[   33] Training loss: 0.07637420, Validation loss: 0.04448878, Gradient norm: 2.23510359
INFO:root:[   34] Training loss: 0.07544809, Validation loss: 0.04673304, Gradient norm: 1.86240804
INFO:root:[   35] Training loss: 0.07365974, Validation loss: 0.04370087, Gradient norm: 2.01009311
INFO:root:[   36] Training loss: 0.07295144, Validation loss: 0.04506532, Gradient norm: 2.14227165
INFO:root:[   37] Training loss: 0.07397190, Validation loss: 0.04260943, Gradient norm: 2.17481500
INFO:root:[   38] Training loss: 0.07247735, Validation loss: 0.05479589, Gradient norm: 2.17602819
INFO:root:[   39] Training loss: 0.07436764, Validation loss: 0.04874399, Gradient norm: 2.31985426
INFO:root:[   40] Training loss: 0.07325686, Validation loss: 0.04467748, Gradient norm: 1.77429270
INFO:root:[   41] Training loss: 0.07234140, Validation loss: 0.05391758, Gradient norm: 2.08126447
INFO:root:[   42] Training loss: 0.07294535, Validation loss: 0.04357948, Gradient norm: 1.82062092
INFO:root:[   43] Training loss: 0.07123567, Validation loss: 0.04816159, Gradient norm: 2.50568954
INFO:root:[   44] Training loss: 0.06969964, Validation loss: 0.04475114, Gradient norm: 2.10478709
INFO:root:[   45] Training loss: 0.06929274, Validation loss: 0.05678510, Gradient norm: 2.18739677
INFO:root:[   46] Training loss: 0.06754672, Validation loss: 0.05471935, Gradient norm: 2.46123418
INFO:root:[   47] Training loss: 0.06721563, Validation loss: 0.04292111, Gradient norm: 2.62733652
INFO:root:[   48] Training loss: 0.06801524, Validation loss: 0.04696521, Gradient norm: 2.20666495
INFO:root:[   49] Training loss: 0.06836672, Validation loss: 0.05531820, Gradient norm: 2.44088146
INFO:root:[   50] Training loss: 0.06656158, Validation loss: 0.05825809, Gradient norm: 2.62041061
INFO:root:[   51] Training loss: 0.06612489, Validation loss: 0.04068036, Gradient norm: 2.48530879
INFO:root:[   52] Training loss: 0.06520860, Validation loss: 0.03900850, Gradient norm: 2.77825109
INFO:root:[   53] Training loss: 0.06497284, Validation loss: 0.05648529, Gradient norm: 2.93936870
INFO:root:[   54] Training loss: 0.06419304, Validation loss: 0.05522735, Gradient norm: 2.93113994
INFO:root:[   55] Training loss: 0.06446958, Validation loss: 0.04612324, Gradient norm: 2.68139968
INFO:root:[   56] Training loss: 0.06500244, Validation loss: 0.04066218, Gradient norm: 2.27651358
INFO:root:[   57] Training loss: 0.06355769, Validation loss: 0.05268852, Gradient norm: 2.87679594
INFO:root:[   58] Training loss: 0.06276126, Validation loss: 0.05144893, Gradient norm: 3.04228606
INFO:root:[   59] Training loss: 0.06305544, Validation loss: 0.03970663, Gradient norm: 2.81956646
INFO:root:[   60] Training loss: 0.06268635, Validation loss: 0.04276831, Gradient norm: 2.56387670
INFO:root:[   61] Training loss: 0.06339276, Validation loss: 0.04649210, Gradient norm: 2.82234717
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1144.092s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0414
INFO:root:EnergyScoreTrain: 0.03173
INFO:root:CoverageTrain: 0.40151
INFO:root:IntervalWidthTrain: 0.01012
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04107
INFO:root:EnergyScoreValidation: 0.03035
INFO:root:CoverageValidation: 0.46303
INFO:root:IntervalWidthValidation: 0.01176
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0416
INFO:root:EnergyScoreTest: 0.03253
INFO:root:CoverageTest: 0.39218
INFO:root:IntervalWidthTest: 0.00938
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 652214272
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72000838, Validation loss: 0.20900026, Gradient norm: 7.34054869
INFO:root:[    2] Training loss: 0.39183235, Validation loss: 0.14191385, Gradient norm: 7.05685042
INFO:root:[    3] Training loss: 0.32149387, Validation loss: 0.19411823, Gradient norm: 4.72291413
INFO:root:[    4] Training loss: 0.28565557, Validation loss: 0.15054090, Gradient norm: 6.24287909
INFO:root:[    5] Training loss: 0.22961050, Validation loss: 0.16449607, Gradient norm: 3.55677096
INFO:root:[    6] Training loss: 0.21152486, Validation loss: 0.11530962, Gradient norm: 3.41448849
INFO:root:[    7] Training loss: 0.19551849, Validation loss: 0.14828879, Gradient norm: 2.68511180
INFO:root:[    8] Training loss: 0.18292254, Validation loss: 0.17417578, Gradient norm: 2.03795892
INFO:root:[    9] Training loss: 0.17551882, Validation loss: 0.12693635, Gradient norm: 1.92222301
INFO:root:[   10] Training loss: 0.17074820, Validation loss: 0.15898070, Gradient norm: 1.59280601
INFO:root:[   11] Training loss: 0.16404241, Validation loss: 0.14646543, Gradient norm: 1.58485509
INFO:root:[   12] Training loss: 0.16009310, Validation loss: 0.16842323, Gradient norm: 1.17036213
INFO:root:[   13] Training loss: 0.15552804, Validation loss: 0.12996365, Gradient norm: 1.43744435
INFO:root:[   14] Training loss: 0.15358748, Validation loss: 0.11673665, Gradient norm: 1.53126746
INFO:root:[   15] Training loss: 0.14882065, Validation loss: 0.12106806, Gradient norm: 1.52158052
INFO:root:[   16] Training loss: 0.14619738, Validation loss: 0.15145064, Gradient norm: 1.15561051
INFO:root:[   17] Training loss: 0.14320470, Validation loss: 0.14903053, Gradient norm: 1.54587242
INFO:root:[   18] Training loss: 0.13941328, Validation loss: 0.15291608, Gradient norm: 1.70318584
INFO:root:[   19] Training loss: 0.13828159, Validation loss: 0.11976102, Gradient norm: 1.10218543
INFO:root:[   20] Training loss: 0.13368998, Validation loss: 0.15495471, Gradient norm: 1.57637434
INFO:root:[   21] Training loss: 0.13195515, Validation loss: 0.13915029, Gradient norm: 1.11978137
INFO:root:[   22] Training loss: 0.12835378, Validation loss: 0.16051809, Gradient norm: 1.60228446
INFO:root:[   23] Training loss: 0.12543776, Validation loss: 0.11106719, Gradient norm: 1.68421713
INFO:root:[   24] Training loss: 0.12243241, Validation loss: 0.13751962, Gradient norm: 1.59204782
INFO:root:[   25] Training loss: 0.11994533, Validation loss: 0.12860426, Gradient norm: 1.88591398
INFO:root:[   26] Training loss: 0.11846352, Validation loss: 0.13414755, Gradient norm: 1.50859283
INFO:root:[   27] Training loss: 0.11471132, Validation loss: 0.13964716, Gradient norm: 1.66536852
INFO:root:[   28] Training loss: 0.11216400, Validation loss: 0.13318032, Gradient norm: 1.89671957
INFO:root:[   29] Training loss: 0.11021902, Validation loss: 0.10815630, Gradient norm: 1.55033679
INFO:root:[   30] Training loss: 0.10869334, Validation loss: 0.10046416, Gradient norm: 1.93213603
INFO:root:[   31] Training loss: 0.10654516, Validation loss: 0.13075153, Gradient norm: 1.66021797
INFO:root:[   32] Training loss: 0.10311874, Validation loss: 0.11733739, Gradient norm: 2.22536861
INFO:root:[   33] Training loss: 0.10050352, Validation loss: 0.08600165, Gradient norm: 2.13491157
INFO:root:[   34] Training loss: 0.10259606, Validation loss: 0.10954688, Gradient norm: 2.40502634
INFO:root:[   35] Training loss: 0.09687348, Validation loss: 0.09120534, Gradient norm: 1.78632164
INFO:root:[   36] Training loss: 0.09622532, Validation loss: 0.10304555, Gradient norm: 1.94674019
INFO:root:[   37] Training loss: 0.09677693, Validation loss: 0.08990980, Gradient norm: 2.21525445
INFO:root:[   38] Training loss: 0.09049451, Validation loss: 0.10710390, Gradient norm: 1.85916942
INFO:root:[   39] Training loss: 0.08871958, Validation loss: 0.07915455, Gradient norm: 2.20154319
INFO:root:[   40] Training loss: 0.08737322, Validation loss: 0.07973930, Gradient norm: 2.16893759
INFO:root:[   41] Training loss: 0.08758809, Validation loss: 0.06686705, Gradient norm: 2.53105466
INFO:root:[   42] Training loss: 0.08315959, Validation loss: 0.09280870, Gradient norm: 2.32830202
INFO:root:[   43] Training loss: 0.08817649, Validation loss: 0.07135315, Gradient norm: 2.51414043
INFO:root:[   44] Training loss: 0.08226575, Validation loss: 0.07987324, Gradient norm: 2.50418977
INFO:root:[   45] Training loss: 0.08030956, Validation loss: 0.08422260, Gradient norm: 2.17528284
INFO:root:[   46] Training loss: 0.07784446, Validation loss: 0.08903505, Gradient norm: 2.57694657
INFO:root:[   47] Training loss: 0.07664902, Validation loss: 0.08611625, Gradient norm: 2.77836445
INFO:root:[   48] Training loss: 0.07432492, Validation loss: 0.07720515, Gradient norm: 2.45772731
INFO:root:[   49] Training loss: 0.07699423, Validation loss: 0.06243426, Gradient norm: 2.89845811
INFO:root:[   50] Training loss: 0.07249573, Validation loss: 0.05688902, Gradient norm: 2.38386323
INFO:root:[   51] Training loss: 0.07036565, Validation loss: 0.07558862, Gradient norm: 2.56367803
INFO:root:[   52] Training loss: 0.06872624, Validation loss: 0.07138786, Gradient norm: 2.76640185
INFO:root:[   53] Training loss: 0.07333346, Validation loss: 0.05793394, Gradient norm: 3.30447179
INFO:root:[   54] Training loss: 0.06726902, Validation loss: 0.05467006, Gradient norm: 2.54274984
INFO:root:[   55] Training loss: 0.06765475, Validation loss: 0.05281227, Gradient norm: 2.49897016
INFO:root:[   56] Training loss: 0.06577180, Validation loss: 0.06004315, Gradient norm: 2.85354434
INFO:root:[   57] Training loss: 0.06402917, Validation loss: 0.06853828, Gradient norm: 2.82170326
INFO:root:[   58] Training loss: 0.06590447, Validation loss: 0.05985073, Gradient norm: 3.08783334
INFO:root:[   59] Training loss: 0.06291380, Validation loss: 0.04302905, Gradient norm: 2.66193973
INFO:root:[   60] Training loss: 0.06045410, Validation loss: 0.04005685, Gradient norm: 3.55938661
INFO:root:[   61] Training loss: 0.05974312, Validation loss: 0.06580248, Gradient norm: 3.39625065
INFO:root:[   62] Training loss: 0.05984492, Validation loss: 0.05859231, Gradient norm: 3.05863383
INFO:root:[   63] Training loss: 0.06022245, Validation loss: 0.04100708, Gradient norm: 3.24381894
INFO:root:[   64] Training loss: 0.05837234, Validation loss: 0.04819575, Gradient norm: 2.32250052
INFO:root:[   65] Training loss: 0.05748980, Validation loss: 0.05540796, Gradient norm: 3.73790459
INFO:root:[   66] Training loss: 0.05641085, Validation loss: 0.05592918, Gradient norm: 4.00654019
INFO:root:[   67] Training loss: 0.05635033, Validation loss: 0.04324438, Gradient norm: 3.71874027
INFO:root:[   68] Training loss: 0.05612430, Validation loss: 0.04063054, Gradient norm: 3.82575231
INFO:root:[   69] Training loss: 0.05383870, Validation loss: 0.05013742, Gradient norm: 3.85749557
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 1288.715s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04225
INFO:root:EnergyScoreTrain: 0.03272
INFO:root:CoverageTrain: 0.24873
INFO:root:IntervalWidthTrain: 0.00755
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04057
INFO:root:EnergyScoreValidation: 0.0301
INFO:root:CoverageValidation: 0.37522
INFO:root:IntervalWidthValidation: 0.00964
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0415
INFO:root:EnergyScoreTest: 0.03432
INFO:root:CoverageTest: 0.18851
INFO:root:IntervalWidthTest: 0.00566
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1033895936
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.76681219, Validation loss: 0.35569613, Gradient norm: 6.67918113
INFO:root:[    2] Training loss: 0.39138394, Validation loss: 0.13899615, Gradient norm: 3.52492525
INFO:root:[    3] Training loss: 0.30486818, Validation loss: 0.12104077, Gradient norm: 2.91512519
INFO:root:[    4] Training loss: 0.26149487, Validation loss: 0.19286275, Gradient norm: 2.16193892
INFO:root:[    5] Training loss: 0.23553551, Validation loss: 0.27975236, Gradient norm: 1.69063972
INFO:root:[    6] Training loss: 0.22074495, Validation loss: 0.24302437, Gradient norm: 2.11092551
INFO:root:[    7] Training loss: 0.20642648, Validation loss: 0.26871470, Gradient norm: 1.40296731
INFO:root:[    8] Training loss: 0.19867345, Validation loss: 0.23999051, Gradient norm: 1.08727700
INFO:root:[    9] Training loss: 0.19136662, Validation loss: 0.20480673, Gradient norm: 1.18643633
INFO:root:[   10] Training loss: 0.18411582, Validation loss: 0.26531351, Gradient norm: 1.23963318
INFO:root:[   11] Training loss: 0.17756099, Validation loss: 0.22315039, Gradient norm: 1.30792025
INFO:root:[   12] Training loss: 0.17260358, Validation loss: 0.22618360, Gradient norm: 1.26657973
INFO:root:[   13] Training loss: 0.16642156, Validation loss: 0.20632757, Gradient norm: 1.52186295
INFO:root:[   14] Training loss: 0.16129175, Validation loss: 0.24333258, Gradient norm: 1.58269115
INFO:root:[   15] Training loss: 0.15628068, Validation loss: 0.21259331, Gradient norm: 1.46158713
INFO:root:[   16] Training loss: 0.15053366, Validation loss: 0.19111487, Gradient norm: 1.33571357
INFO:root:[   17] Training loss: 0.14594695, Validation loss: 0.20864144, Gradient norm: 1.21018851
INFO:root:[   18] Training loss: 0.14162521, Validation loss: 0.22651882, Gradient norm: 1.60456916
INFO:root:[   19] Training loss: 0.13735117, Validation loss: 0.21738600, Gradient norm: 1.81042550
INFO:root:[   20] Training loss: 0.13389383, Validation loss: 0.20523729, Gradient norm: 1.78440441
INFO:root:[   21] Training loss: 0.12907343, Validation loss: 0.17834244, Gradient norm: 1.59304581
INFO:root:[   22] Training loss: 0.12629432, Validation loss: 0.13212336, Gradient norm: 1.80765590
INFO:root:[   23] Training loss: 0.12108813, Validation loss: 0.17338432, Gradient norm: 1.78530288
INFO:root:[   24] Training loss: 0.11899073, Validation loss: 0.17303861, Gradient norm: 1.98325223
INFO:root:[   25] Training loss: 0.11443837, Validation loss: 0.13509263, Gradient norm: 1.88817162
INFO:root:[   26] Training loss: 0.11049003, Validation loss: 0.12101041, Gradient norm: 2.07470974
INFO:root:[   27] Training loss: 0.10680088, Validation loss: 0.10803242, Gradient norm: 2.14481616
INFO:root:[   28] Training loss: 0.10278807, Validation loss: 0.12145438, Gradient norm: 2.27683136
INFO:root:[   29] Training loss: 0.10171385, Validation loss: 0.11589688, Gradient norm: 2.33265067
INFO:root:[   30] Training loss: 0.09615141, Validation loss: 0.12837443, Gradient norm: 2.15342039
INFO:root:[   31] Training loss: 0.09398642, Validation loss: 0.12887514, Gradient norm: 2.43797519
INFO:root:[   32] Training loss: 0.09274627, Validation loss: 0.09780760, Gradient norm: 2.88678646
INFO:root:[   33] Training loss: 0.09090897, Validation loss: 0.10388185, Gradient norm: 2.55584081
INFO:root:[   34] Training loss: 0.08733549, Validation loss: 0.08889535, Gradient norm: 2.90587909
INFO:root:[   35] Training loss: 0.08625043, Validation loss: 0.10498407, Gradient norm: 3.18667550
INFO:root:[   36] Training loss: 0.08410780, Validation loss: 0.07107548, Gradient norm: 2.99303751
INFO:root:[   37] Training loss: 0.08167172, Validation loss: 0.07016868, Gradient norm: 3.30770916
INFO:root:[   38] Training loss: 0.07959190, Validation loss: 0.06304611, Gradient norm: 3.06252430
INFO:root:[   39] Training loss: 0.08220982, Validation loss: 0.09350876, Gradient norm: 3.35484151
INFO:root:[   40] Training loss: 0.07664199, Validation loss: 0.07651411, Gradient norm: 3.81905242
INFO:root:[   41] Training loss: 0.07507865, Validation loss: 0.05781330, Gradient norm: 4.19060506
INFO:root:[   42] Training loss: 0.07582276, Validation loss: 0.06392107, Gradient norm: 3.56476980
INFO:root:[   43] Training loss: 0.07454083, Validation loss: 0.05642407, Gradient norm: 3.98357551
INFO:root:[   44] Training loss: 0.07246803, Validation loss: 0.07043941, Gradient norm: 4.27606127
INFO:root:[   45] Training loss: 0.07372893, Validation loss: 0.06751664, Gradient norm: 4.04169684
INFO:root:[   46] Training loss: 0.06901928, Validation loss: 0.05044376, Gradient norm: 4.29758518
INFO:root:[   47] Training loss: 0.07185972, Validation loss: 0.05200506, Gradient norm: 3.67355006
INFO:root:[   48] Training loss: 0.06763316, Validation loss: 0.05845392, Gradient norm: 3.64277282
INFO:root:[   49] Training loss: 0.06872510, Validation loss: 0.04596530, Gradient norm: 4.95232205
INFO:root:[   50] Training loss: 0.06920224, Validation loss: 0.05980970, Gradient norm: 4.58330942
INFO:root:[   51] Training loss: 0.06734959, Validation loss: 0.05482618, Gradient norm: 5.21264707
INFO:root:[   52] Training loss: 0.06932925, Validation loss: 0.05487970, Gradient norm: 4.96583632
INFO:root:[   53] Training loss: 0.06720044, Validation loss: 0.06895260, Gradient norm: 5.45730925
INFO:root:[   54] Training loss: 0.06597830, Validation loss: 0.04712332, Gradient norm: 5.06648132
INFO:root:[   55] Training loss: 0.06905325, Validation loss: 0.04953461, Gradient norm: 6.52300441
INFO:root:[   56] Training loss: 0.06826097, Validation loss: 0.07330195, Gradient norm: 6.35330538
INFO:root:[   57] Training loss: 0.06798606, Validation loss: 0.05944587, Gradient norm: 5.80012844
INFO:root:[   58] Training loss: 0.06565729, Validation loss: 0.04940751, Gradient norm: 6.47147290
INFO:root:[   59] Training loss: 0.06421740, Validation loss: 0.04593470, Gradient norm: 7.43454553
INFO:root:[   60] Training loss: 0.06669995, Validation loss: 0.05032769, Gradient norm: 6.71841194
INFO:root:[   61] Training loss: 0.07158462, Validation loss: 0.05254927, Gradient norm: 8.71410822
INFO:root:[   62] Training loss: 0.06551865, Validation loss: 0.05172231, Gradient norm: 6.67507379
INFO:root:[   63] Training loss: 0.06298959, Validation loss: 0.04537504, Gradient norm: 7.12614122
INFO:root:[   64] Training loss: 0.06331919, Validation loss: 0.04575696, Gradient norm: 7.35400659
INFO:root:[   65] Training loss: 0.06691453, Validation loss: 0.04282723, Gradient norm: 8.19280921
INFO:root:[   66] Training loss: 0.06264627, Validation loss: 0.04952928, Gradient norm: 8.09165431
INFO:root:[   67] Training loss: 0.06838954, Validation loss: 0.04262842, Gradient norm: 9.32104345
INFO:root:[   68] Training loss: 0.06226050, Validation loss: 0.06190164, Gradient norm: 8.04728153
INFO:root:[   69] Training loss: 0.06288719, Validation loss: 0.04704220, Gradient norm: 6.00610060
INFO:root:[   70] Training loss: 0.06323938, Validation loss: 0.04372873, Gradient norm: 8.44513200
INFO:root:[   71] Training loss: 0.06375061, Validation loss: 0.04423849, Gradient norm: 9.60407248
INFO:root:[   72] Training loss: 0.06476128, Validation loss: 0.05484128, Gradient norm: 8.21059739
INFO:root:[   73] Training loss: 0.07300682, Validation loss: 0.04781164, Gradient norm: 11.15613227
INFO:root:[   74] Training loss: 0.06798610, Validation loss: 0.04938866, Gradient norm: 10.52157443
INFO:root:[   75] Training loss: 0.06741222, Validation loss: 0.04207887, Gradient norm: 11.19587640
INFO:root:[   76] Training loss: 0.06636272, Validation loss: 0.04293123, Gradient norm: 8.41803319
INFO:root:[   77] Training loss: 0.06329204, Validation loss: 0.06161818, Gradient norm: 7.98070818
INFO:root:[   78] Training loss: 0.06661133, Validation loss: 0.05192956, Gradient norm: 10.70154457
INFO:root:[   79] Training loss: 0.06209633, Validation loss: 0.04921366, Gradient norm: 7.84387534
INFO:root:[   80] Training loss: 0.06289919, Validation loss: 0.04540173, Gradient norm: 7.06818358
INFO:root:[   81] Training loss: 0.06528168, Validation loss: 0.04594692, Gradient norm: 8.13268160
INFO:root:[   82] Training loss: 0.06336967, Validation loss: 0.04626219, Gradient norm: 9.74262855
INFO:root:[   83] Training loss: 0.06556813, Validation loss: 0.07061063, Gradient norm: 8.34768221
INFO:root:[   84] Training loss: 0.06796259, Validation loss: 0.04134340, Gradient norm: 11.08178415
INFO:root:[   85] Training loss: 0.06197085, Validation loss: 0.04695609, Gradient norm: 7.57320465
INFO:root:[   86] Training loss: 0.06478737, Validation loss: 0.04683333, Gradient norm: 7.80172819
INFO:root:[   87] Training loss: 0.06535528, Validation loss: 0.04425498, Gradient norm: 10.44070407
INFO:root:[   88] Training loss: 0.06564005, Validation loss: 0.04635396, Gradient norm: 10.18155536
INFO:root:[   89] Training loss: 0.07009256, Validation loss: 0.06403859, Gradient norm: 13.14768114
INFO:root:[   90] Training loss: 0.07068514, Validation loss: 0.05478297, Gradient norm: 13.21118701
INFO:root:[   91] Training loss: 0.06827156, Validation loss: 0.05097955, Gradient norm: 10.33677394
INFO:root:[   92] Training loss: 0.06528213, Validation loss: 0.04352524, Gradient norm: 8.80844096
INFO:root:[   93] Training loss: 0.06541742, Validation loss: 0.04480885, Gradient norm: 10.90252143
INFO:root:EP 93: Early stopping
INFO:root:Training the model took 1726.16s.
INFO:root:Emptying the cuda cache took 0.033s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05734
INFO:root:EnergyScoreTrain: 0.03302
INFO:root:CoverageTrain: 0.143
INFO:root:IntervalWidthTrain: 0.01723
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05463
INFO:root:EnergyScoreValidation: 0.03112
INFO:root:CoverageValidation: 0.14196
INFO:root:IntervalWidthValidation: 0.01659
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.05644
INFO:root:EnergyScoreTest: 0.03148
INFO:root:CoverageTest: 0.15489
INFO:root:IntervalWidthTest: 0.01802
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1409286144
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.80850522, Validation loss: 0.33210835, Gradient norm: 6.37129812
INFO:root:[    2] Training loss: 0.40405692, Validation loss: 0.33331854, Gradient norm: 3.18392761
INFO:root:[    3] Training loss: 0.31883510, Validation loss: 0.38403457, Gradient norm: 2.46107643
INFO:root:[    4] Training loss: 0.27098509, Validation loss: 0.29395788, Gradient norm: 2.18068015
INFO:root:[    5] Training loss: 0.24280777, Validation loss: 0.35489545, Gradient norm: 0.95352105
INFO:root:[    6] Training loss: 0.22919740, Validation loss: 0.31772397, Gradient norm: 1.54118200
INFO:root:[    7] Training loss: 0.21433384, Validation loss: 0.32496254, Gradient norm: 0.74056386
INFO:root:[    8] Training loss: 0.20363163, Validation loss: 0.36561096, Gradient norm: 1.22780130
INFO:root:[    9] Training loss: 0.19273118, Validation loss: 0.36955682, Gradient norm: 0.92303813
INFO:root:[   10] Training loss: 0.18394089, Validation loss: 0.35939445, Gradient norm: 1.13651380
INFO:root:[   11] Training loss: 0.17471375, Validation loss: 0.36181717, Gradient norm: 1.39047898
INFO:root:[   12] Training loss: 0.16794130, Validation loss: 0.29157372, Gradient norm: 1.28005547
INFO:root:[   13] Training loss: 0.15901197, Validation loss: 0.33917727, Gradient norm: 1.18485941
INFO:root:[   14] Training loss: 0.15327412, Validation loss: 0.26578735, Gradient norm: 1.09752994
INFO:root:[   15] Training loss: 0.14463915, Validation loss: 0.27131986, Gradient norm: 1.52666558
INFO:root:[   16] Training loss: 0.13968353, Validation loss: 0.24829860, Gradient norm: 1.35048672
INFO:root:[   17] Training loss: 0.13354361, Validation loss: 0.23531958, Gradient norm: 1.56352241
INFO:root:[   18] Training loss: 0.12579447, Validation loss: 0.21300663, Gradient norm: 1.42623888
INFO:root:[   19] Training loss: 0.12032101, Validation loss: 0.18748916, Gradient norm: 1.54973891
INFO:root:[   20] Training loss: 0.11593051, Validation loss: 0.21867041, Gradient norm: 1.55311877
INFO:root:[   21] Training loss: 0.10997887, Validation loss: 0.16923123, Gradient norm: 1.55301322
INFO:root:[   22] Training loss: 0.10652935, Validation loss: 0.17524213, Gradient norm: 1.44368833
INFO:root:[   23] Training loss: 0.10538501, Validation loss: 0.14376887, Gradient norm: 1.89893445
INFO:root:[   24] Training loss: 0.09918284, Validation loss: 0.13858037, Gradient norm: 1.62829698
INFO:root:[   25] Training loss: 0.09641642, Validation loss: 0.13967668, Gradient norm: 1.68637297
INFO:root:[   26] Training loss: 0.09283323, Validation loss: 0.12955849, Gradient norm: 1.90507161
INFO:root:[   27] Training loss: 0.08851638, Validation loss: 0.10836458, Gradient norm: 1.50509718
INFO:root:[   28] Training loss: 0.08787009, Validation loss: 0.12085985, Gradient norm: 1.89572555
INFO:root:[   29] Training loss: 0.08374121, Validation loss: 0.12702333, Gradient norm: 1.70134325
INFO:root:[   30] Training loss: 0.08099727, Validation loss: 0.11641486, Gradient norm: 1.22651108
INFO:root:[   31] Training loss: 0.08066716, Validation loss: 0.09034336, Gradient norm: 1.89315501
INFO:root:[   32] Training loss: 0.07815093, Validation loss: 0.08383482, Gradient norm: 2.18336607
INFO:root:[   33] Training loss: 0.07829512, Validation loss: 0.07436100, Gradient norm: 2.17769554
INFO:root:[   34] Training loss: 0.07800910, Validation loss: 0.07178005, Gradient norm: 2.24417515
INFO:root:[   35] Training loss: 0.07502500, Validation loss: 0.07162016, Gradient norm: 1.78070546
INFO:root:[   36] Training loss: 0.07336094, Validation loss: 0.07385790, Gradient norm: 1.73731118
INFO:root:[   37] Training loss: 0.07342649, Validation loss: 0.06874370, Gradient norm: 1.97318950
INFO:root:[   38] Training loss: 0.07182008, Validation loss: 0.07992842, Gradient norm: 2.27659697
INFO:root:[   39] Training loss: 0.07239198, Validation loss: 0.08335684, Gradient norm: 1.98193787
INFO:root:[   40] Training loss: 0.07178765, Validation loss: 0.06849583, Gradient norm: 1.74523455
INFO:root:[   41] Training loss: 0.06957905, Validation loss: 0.06726305, Gradient norm: 1.98439046
INFO:root:[   42] Training loss: 0.07046664, Validation loss: 0.05799530, Gradient norm: 2.34503933
INFO:root:[   43] Training loss: 0.07123682, Validation loss: 0.06702616, Gradient norm: 2.04264706
INFO:root:[   44] Training loss: 0.06994728, Validation loss: 0.08621878, Gradient norm: 2.41344567
INFO:root:[   45] Training loss: 0.06866330, Validation loss: 0.08571984, Gradient norm: 2.20351498
INFO:root:[   46] Training loss: 0.06934347, Validation loss: 0.07238568, Gradient norm: 2.05283467
INFO:root:[   47] Training loss: 0.06898289, Validation loss: 0.08739256, Gradient norm: 2.14154014
INFO:root:[   48] Training loss: 0.06762197, Validation loss: 0.07119991, Gradient norm: 2.51531207
INFO:root:[   49] Training loss: 0.06864344, Validation loss: 0.06710308, Gradient norm: 2.25593530
INFO:root:[   50] Training loss: 0.06735143, Validation loss: 0.06375099, Gradient norm: 2.35458979
INFO:root:[   51] Training loss: 0.06688382, Validation loss: 0.06264305, Gradient norm: 2.33032215
INFO:root:[   52] Training loss: 0.06668387, Validation loss: 0.08032102, Gradient norm: 2.21288667
INFO:root:[   53] Training loss: 0.06980900, Validation loss: 0.08371421, Gradient norm: 3.20164291
INFO:root:[   54] Training loss: 0.06628504, Validation loss: 0.07412461, Gradient norm: 2.39621809
INFO:root:[   55] Training loss: 0.06659042, Validation loss: 0.06742912, Gradient norm: 2.76744384
INFO:root:[   56] Training loss: 0.06701243, Validation loss: 0.05336921, Gradient norm: 2.70587716
INFO:root:[   57] Training loss: 0.06525480, Validation loss: 0.05317111, Gradient norm: 2.53048410
INFO:root:[   58] Training loss: 0.06562914, Validation loss: 0.07589130, Gradient norm: 2.61645395
INFO:root:[   59] Training loss: 0.06663466, Validation loss: 0.06043980, Gradient norm: 2.37384106
INFO:root:[   60] Training loss: 0.06504372, Validation loss: 0.08201757, Gradient norm: 2.67642126
INFO:root:[   61] Training loss: 0.06478368, Validation loss: 0.06086956, Gradient norm: 2.57108542
INFO:root:[   62] Training loss: 0.06543912, Validation loss: 0.05262454, Gradient norm: 2.34910067
INFO:root:[   63] Training loss: 0.06378088, Validation loss: 0.06877336, Gradient norm: 2.50004736
INFO:root:[   64] Training loss: 0.06407797, Validation loss: 0.07235891, Gradient norm: 2.64062931
INFO:root:[   65] Training loss: 0.06367760, Validation loss: 0.06495194, Gradient norm: 2.88608509
INFO:root:[   66] Training loss: 0.06287353, Validation loss: 0.06487164, Gradient norm: 2.28424443
INFO:root:[   67] Training loss: 0.06431375, Validation loss: 0.08140547, Gradient norm: 2.87751107
INFO:root:[   68] Training loss: 0.06407571, Validation loss: 0.06045258, Gradient norm: 2.37304000
INFO:root:[   69] Training loss: 0.06345084, Validation loss: 0.04905115, Gradient norm: 3.22842635
INFO:root:[   70] Training loss: 0.06088533, Validation loss: 0.06561741, Gradient norm: 2.75202435
INFO:root:[   71] Training loss: 0.06362146, Validation loss: 0.07540003, Gradient norm: 2.84513909
INFO:root:[   72] Training loss: 0.06245417, Validation loss: 0.07289755, Gradient norm: 3.11083837
INFO:root:[   73] Training loss: 0.06204557, Validation loss: 0.05782824, Gradient norm: 2.82343699
INFO:root:[   74] Training loss: 0.06038243, Validation loss: 0.07266036, Gradient norm: 2.63209420
INFO:root:[   75] Training loss: 0.06133334, Validation loss: 0.06956970, Gradient norm: 3.18597656
INFO:root:[   76] Training loss: 0.06074660, Validation loss: 0.07807313, Gradient norm: 2.85333534
INFO:root:[   77] Training loss: 0.06237616, Validation loss: 0.05362533, Gradient norm: 3.10949956
INFO:root:[   78] Training loss: 0.06165481, Validation loss: 0.06647655, Gradient norm: 3.16033539
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 1455.296s.
INFO:root:Emptying the cuda cache took 0.034s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05065
INFO:root:EnergyScoreTrain: 0.03869
INFO:root:CoverageTrain: 0.11814
INFO:root:IntervalWidthTrain: 0.00566
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05088
INFO:root:EnergyScoreValidation: 0.03851
INFO:root:CoverageValidation: 0.12028
INFO:root:IntervalWidthValidation: 0.00599
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.05089
INFO:root:EnergyScoreTest: 0.03821
INFO:root:CoverageTest: 0.1283
INFO:root:IntervalWidthTest: 0.00602
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 922746880
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.09183749, Validation loss: 0.20748026, Gradient norm: 7.25555294
INFO:root:[    2] Training loss: 0.60344368, Validation loss: 0.20890643, Gradient norm: 4.45075947
INFO:root:[    3] Training loss: 0.47318474, Validation loss: 0.36644516, Gradient norm: 4.38903549
INFO:root:[    4] Training loss: 0.38143860, Validation loss: 0.46985842, Gradient norm: 2.91588232
INFO:root:[    5] Training loss: 0.33103463, Validation loss: 0.46115991, Gradient norm: 1.84700129
INFO:root:[    6] Training loss: 0.30135078, Validation loss: 0.52725121, Gradient norm: 1.22690718
INFO:root:[    7] Training loss: 0.28637816, Validation loss: 0.54977280, Gradient norm: 1.63873887
INFO:root:[    8] Training loss: 0.26978718, Validation loss: 0.52514716, Gradient norm: 1.11736440
INFO:root:[    9] Training loss: 0.25735733, Validation loss: 0.58316453, Gradient norm: 1.21697728
INFO:root:[   10] Training loss: 0.24669762, Validation loss: 0.48902208, Gradient norm: 1.34850398
INFO:root:[   11] Training loss: 0.23365186, Validation loss: 0.50350459, Gradient norm: 1.39341317
INFO:root:[   12] Training loss: 0.22352343, Validation loss: 0.50442302, Gradient norm: 1.93757559
INFO:root:[   13] Training loss: 0.21399653, Validation loss: 0.46208245, Gradient norm: 1.35026095
INFO:root:[   14] Training loss: 0.20165970, Validation loss: 0.49100780, Gradient norm: 1.51490893
INFO:root:[   15] Training loss: 0.19080998, Validation loss: 0.41950172, Gradient norm: 2.16407871
INFO:root:[   16] Training loss: 0.18193587, Validation loss: 0.43385290, Gradient norm: 1.71951718
INFO:root:[   17] Training loss: 0.17138235, Validation loss: 0.34951756, Gradient norm: 2.45908624
INFO:root:[   18] Training loss: 0.16309261, Validation loss: 0.38584765, Gradient norm: 2.17298261
INFO:root:[   19] Training loss: 0.15408840, Validation loss: 0.33685006, Gradient norm: 1.87826735
INFO:root:[   20] Training loss: 0.14543612, Validation loss: 0.34670749, Gradient norm: 2.59135441
INFO:root:[   21] Training loss: 0.13746960, Validation loss: 0.31848699, Gradient norm: 2.49304844
INFO:root:[   22] Training loss: 0.12991440, Validation loss: 0.29681686, Gradient norm: 2.38152624
INFO:root:[   23] Training loss: 0.12689489, Validation loss: 0.25382302, Gradient norm: 2.49133639
INFO:root:[   24] Training loss: 0.11657266, Validation loss: 0.24385483, Gradient norm: 2.74988643
INFO:root:[   25] Training loss: 0.11167527, Validation loss: 0.19483723, Gradient norm: 2.25557292
INFO:root:[   26] Training loss: 0.10614462, Validation loss: 0.21708273, Gradient norm: 2.59237965
INFO:root:[   27] Training loss: 0.10374775, Validation loss: 0.15334166, Gradient norm: 2.73218429
INFO:root:[   28] Training loss: 0.09749567, Validation loss: 0.14177457, Gradient norm: 2.96747997
INFO:root:[   29] Training loss: 0.09496843, Validation loss: 0.12127598, Gradient norm: 2.96688451
INFO:root:[   30] Training loss: 0.09005127, Validation loss: 0.12811519, Gradient norm: 2.52639778
INFO:root:[   31] Training loss: 0.08704263, Validation loss: 0.11378641, Gradient norm: 2.92653617
INFO:root:[   32] Training loss: 0.08506295, Validation loss: 0.10468135, Gradient norm: 2.65492272
INFO:root:[   33] Training loss: 0.08429595, Validation loss: 0.10454884, Gradient norm: 3.85153692
INFO:root:[   34] Training loss: 0.08431853, Validation loss: 0.08252071, Gradient norm: 3.24955388
INFO:root:[   35] Training loss: 0.08367079, Validation loss: 0.09224683, Gradient norm: 2.78040224
INFO:root:[   36] Training loss: 0.08259261, Validation loss: 0.07318324, Gradient norm: 3.86340911
INFO:root:[   37] Training loss: 0.07979845, Validation loss: 0.07611111, Gradient norm: 3.42986269
INFO:root:[   38] Training loss: 0.07865900, Validation loss: 0.08173843, Gradient norm: 4.08465887
INFO:root:[   39] Training loss: 0.07895503, Validation loss: 0.09362354, Gradient norm: 2.86864653
INFO:root:[   40] Training loss: 0.08140535, Validation loss: 0.07450913, Gradient norm: 3.12614953
INFO:root:[   41] Training loss: 0.07703692, Validation loss: 0.06952543, Gradient norm: 3.44950301
INFO:root:[   42] Training loss: 0.07707643, Validation loss: 0.08065303, Gradient norm: 4.06348052
INFO:root:[   43] Training loss: 0.07869317, Validation loss: 0.09994164, Gradient norm: 4.81092613
INFO:root:[   44] Training loss: 0.07559534, Validation loss: 0.09207071, Gradient norm: 4.13903692
INFO:root:[   45] Training loss: 0.07999106, Validation loss: 0.08442584, Gradient norm: 3.96842712
INFO:root:[   46] Training loss: 0.07447441, Validation loss: 0.07459891, Gradient norm: 4.00789131
INFO:root:[   47] Training loss: 0.07525176, Validation loss: 0.06818214, Gradient norm: 3.92274873
INFO:root:[   48] Training loss: 0.07312184, Validation loss: 0.09312131, Gradient norm: 3.53502624
INFO:root:[   49] Training loss: 0.07576408, Validation loss: 0.08260879, Gradient norm: 4.43348264
INFO:root:[   50] Training loss: 0.07594575, Validation loss: 0.08258620, Gradient norm: 3.51743502
INFO:root:[   51] Training loss: 0.07469907, Validation loss: 0.07032801, Gradient norm: 4.09908846
INFO:root:[   52] Training loss: 0.07260854, Validation loss: 0.08616222, Gradient norm: 3.65762513
INFO:root:[   53] Training loss: 0.07503864, Validation loss: 0.10582462, Gradient norm: 3.51225421
INFO:root:[   54] Training loss: 0.07434934, Validation loss: 0.07531482, Gradient norm: 3.46551787
INFO:root:[   55] Training loss: 0.07212767, Validation loss: 0.09973120, Gradient norm: 3.57647844
INFO:root:[   56] Training loss: 0.07162823, Validation loss: 0.07416632, Gradient norm: 4.18823995
INFO:root:[   57] Training loss: 0.07167958, Validation loss: 0.09103278, Gradient norm: 3.86946036
INFO:root:[   58] Training loss: 0.07064326, Validation loss: 0.07480882, Gradient norm: 3.80959760
INFO:root:[   59] Training loss: 0.07234100, Validation loss: 0.08120080, Gradient norm: 4.18634198
INFO:root:[   60] Training loss: 0.07003372, Validation loss: 0.06595380, Gradient norm: 3.31879396
INFO:root:[   61] Training loss: 0.06929114, Validation loss: 0.10775364, Gradient norm: 3.89043789
INFO:root:[   62] Training loss: 0.07236138, Validation loss: 0.10073480, Gradient norm: 4.17129660
INFO:root:[   63] Training loss: 0.06975467, Validation loss: 0.07396389, Gradient norm: 3.09889574
INFO:root:[   64] Training loss: 0.06838691, Validation loss: 0.08773953, Gradient norm: 3.94888343
INFO:root:[   65] Training loss: 0.06987887, Validation loss: 0.11153136, Gradient norm: 3.70645599
INFO:root:[   66] Training loss: 0.06859227, Validation loss: 0.07568509, Gradient norm: 3.78400717
INFO:root:[   67] Training loss: 0.06683987, Validation loss: 0.08560597, Gradient norm: 2.69339396
INFO:root:[   68] Training loss: 0.06941551, Validation loss: 0.06425347, Gradient norm: 3.70756739
INFO:root:[   69] Training loss: 0.06804808, Validation loss: 0.09236314, Gradient norm: 3.66449816
INFO:root:[   70] Training loss: 0.06685103, Validation loss: 0.07713515, Gradient norm: 3.96070733
INFO:root:[   71] Training loss: 0.06748698, Validation loss: 0.08955349, Gradient norm: 3.61923106
INFO:root:[   72] Training loss: 0.06767543, Validation loss: 0.09196773, Gradient norm: 2.89794308
INFO:root:[   73] Training loss: 0.06780879, Validation loss: 0.07173156, Gradient norm: 3.97801645
INFO:root:[   74] Training loss: 0.06406848, Validation loss: 0.07204677, Gradient norm: 3.23257466
INFO:root:[   75] Training loss: 0.06572803, Validation loss: 0.10067594, Gradient norm: 3.01552968
INFO:root:[   76] Training loss: 0.06604143, Validation loss: 0.09789223, Gradient norm: 3.68989650
INFO:root:[   77] Training loss: 0.06579299, Validation loss: 0.09040004, Gradient norm: 3.27388448
INFO:root:EP 77: Early stopping
INFO:root:Training the model took 1435.166s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06705
INFO:root:EnergyScoreTrain: 0.0533
INFO:root:CoverageTrain: 0.0861
INFO:root:IntervalWidthTrain: 0.00662
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06448
INFO:root:EnergyScoreValidation: 0.05144
INFO:root:CoverageValidation: 0.089
INFO:root:IntervalWidthValidation: 0.00622
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06541
INFO:root:EnergyScoreTest: 0.05307
INFO:root:CoverageTest: 0.08568
INFO:root:IntervalWidthTest: 0.00589
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 748683264
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.24201343, Validation loss: 0.34465892, Gradient norm: 5.12341088
INFO:root:[    2] Training loss: 0.80180700, Validation loss: 0.25689203, Gradient norm: 5.98111291
INFO:root:[    3] Training loss: 0.61389752, Validation loss: 0.24055819, Gradient norm: 2.68930969
INFO:root:[    4] Training loss: 0.52283680, Validation loss: 0.41403601, Gradient norm: 2.81797951
INFO:root:[    5] Training loss: 0.46506280, Validation loss: 0.56170931, Gradient norm: 2.98203822
INFO:root:[    6] Training loss: 0.42162275, Validation loss: 0.46871586, Gradient norm: 2.16360506
INFO:root:[    7] Training loss: 0.39196012, Validation loss: 0.54267070, Gradient norm: 3.00334688
INFO:root:[    8] Training loss: 0.36560487, Validation loss: 0.70471198, Gradient norm: 1.95876425
INFO:root:[    9] Training loss: 0.34939446, Validation loss: 0.52925785, Gradient norm: 2.73451223
INFO:root:[   10] Training loss: 0.33312612, Validation loss: 0.72426194, Gradient norm: 2.75519583
INFO:root:[   11] Training loss: 0.31406459, Validation loss: 0.59306650, Gradient norm: 2.18243991
INFO:root:[   12] Training loss: 0.29886764, Validation loss: 0.64616127, Gradient norm: 2.35861439
INFO:root:[   13] Training loss: 0.28193309, Validation loss: 0.62810927, Gradient norm: 1.82344774
INFO:root:[   14] Training loss: 0.26659134, Validation loss: 0.59022585, Gradient norm: 2.00972486
INFO:root:[   15] Training loss: 0.25113946, Validation loss: 0.57958331, Gradient norm: 2.17342680
INFO:root:[   16] Training loss: 0.23869013, Validation loss: 0.60329773, Gradient norm: 3.79291493
INFO:root:[   17] Training loss: 0.22391548, Validation loss: 0.52630076, Gradient norm: 2.85691427
INFO:root:[   18] Training loss: 0.21093377, Validation loss: 0.51936993, Gradient norm: 3.68505510
INFO:root:[   19] Training loss: 0.19708464, Validation loss: 0.51991155, Gradient norm: 3.78739614
INFO:root:[   20] Training loss: 0.18331444, Validation loss: 0.44570441, Gradient norm: 2.23389835
INFO:root:[   21] Training loss: 0.17095005, Validation loss: 0.36942826, Gradient norm: 3.04720121
INFO:root:[   22] Training loss: 0.16002788, Validation loss: 0.38625098, Gradient norm: 5.49930466
INFO:root:[   23] Training loss: 0.14790957, Validation loss: 0.32450645, Gradient norm: 3.35429940
INFO:root:[   24] Training loss: 0.13973180, Validation loss: 0.28420613, Gradient norm: 3.42000230
INFO:root:[   25] Training loss: 0.13269430, Validation loss: 0.23470446, Gradient norm: 5.44658912
INFO:root:[   26] Training loss: 0.12533146, Validation loss: 0.25565110, Gradient norm: 4.54171863
INFO:root:[   27] Training loss: 0.11807702, Validation loss: 0.22220073, Gradient norm: 4.92197003
INFO:root:[   28] Training loss: 0.11358580, Validation loss: 0.16472206, Gradient norm: 4.39750468
INFO:root:[   29] Training loss: 0.11161862, Validation loss: 0.14129444, Gradient norm: 4.01240261
INFO:root:[   30] Training loss: 0.10490328, Validation loss: 0.13385348, Gradient norm: 4.08780757
INFO:root:[   31] Training loss: 0.10135729, Validation loss: 0.11612749, Gradient norm: 4.46252804
INFO:root:[   32] Training loss: 0.10034906, Validation loss: 0.10027857, Gradient norm: 4.82756523
INFO:root:[   33] Training loss: 0.09893370, Validation loss: 0.10172443, Gradient norm: 4.85179730
INFO:root:[   34] Training loss: 0.09678943, Validation loss: 0.08902880, Gradient norm: 3.30300565
INFO:root:[   35] Training loss: 0.09849623, Validation loss: 0.08545394, Gradient norm: 4.20800376
INFO:root:[   36] Training loss: 0.09383372, Validation loss: 0.08239152, Gradient norm: 3.40634945
INFO:root:[   37] Training loss: 0.09537069, Validation loss: 0.08078658, Gradient norm: 4.57308901
INFO:root:[   38] Training loss: 0.09526326, Validation loss: 0.09603011, Gradient norm: 6.11585803
INFO:root:[   39] Training loss: 0.09162616, Validation loss: 0.08777166, Gradient norm: 4.13872293
INFO:root:[   40] Training loss: 0.09398229, Validation loss: 0.08402613, Gradient norm: 4.46804742
INFO:root:[   41] Training loss: 0.09224577, Validation loss: 0.08274353, Gradient norm: 3.54231140
INFO:root:[   42] Training loss: 0.09053735, Validation loss: 0.09629816, Gradient norm: 5.07319408
INFO:root:[   43] Training loss: 0.09031564, Validation loss: 0.10341796, Gradient norm: 4.45775446
INFO:root:[   44] Training loss: 0.09005902, Validation loss: 0.09762402, Gradient norm: 5.78103031
INFO:root:[   45] Training loss: 0.08922646, Validation loss: 0.08629816, Gradient norm: 5.23947530
INFO:root:[   46] Training loss: 0.08784457, Validation loss: 0.07930663, Gradient norm: 3.61330885
INFO:root:[   47] Training loss: 0.08899709, Validation loss: 0.09319367, Gradient norm: 5.79251375
INFO:root:[   48] Training loss: 0.08715150, Validation loss: 0.08197346, Gradient norm: 4.29921483
INFO:root:[   49] Training loss: 0.08697949, Validation loss: 0.09530686, Gradient norm: 3.98772715
INFO:root:[   50] Training loss: 0.08495432, Validation loss: 0.07345203, Gradient norm: 5.03757448
INFO:root:[   51] Training loss: 0.08582370, Validation loss: 0.08788071, Gradient norm: 6.03969789
INFO:root:[   52] Training loss: 0.08400955, Validation loss: 0.09461431, Gradient norm: 4.43710150
INFO:root:[   53] Training loss: 0.08516538, Validation loss: 0.08428493, Gradient norm: 4.66848507
INFO:root:[   54] Training loss: 0.08464430, Validation loss: 0.10529615, Gradient norm: 5.11700897
INFO:root:[   55] Training loss: 0.08274043, Validation loss: 0.08052844, Gradient norm: 3.82222648
INFO:root:[   56] Training loss: 0.08902502, Validation loss: 0.08524537, Gradient norm: 5.68808860
INFO:root:[   57] Training loss: 0.08425235, Validation loss: 0.08288364, Gradient norm: 4.27654666
INFO:root:[   58] Training loss: 0.08326508, Validation loss: 0.11664361, Gradient norm: 5.13707190
INFO:root:[   59] Training loss: 0.08352528, Validation loss: 0.08181212, Gradient norm: 4.52848773
INFO:root:[   60] Training loss: 0.08132010, Validation loss: 0.10961318, Gradient norm: 5.21998455
INFO:root:[   61] Training loss: 0.08257242, Validation loss: 0.09095576, Gradient norm: 5.26105147
INFO:root:[   62] Training loss: 0.08240441, Validation loss: 0.10295105, Gradient norm: 4.18345211
INFO:root:[   63] Training loss: 0.08062152, Validation loss: 0.08413267, Gradient norm: 4.84063994
INFO:root:[   64] Training loss: 0.08172520, Validation loss: 0.07604890, Gradient norm: 5.80622486
INFO:root:[   65] Training loss: 0.08038386, Validation loss: 0.09079789, Gradient norm: 3.67468604
INFO:root:[   66] Training loss: 0.08077456, Validation loss: 0.10958627, Gradient norm: 6.03652349
INFO:root:[   67] Training loss: 0.08095091, Validation loss: 0.07830080, Gradient norm: 5.84114309
INFO:root:[   68] Training loss: 0.08028089, Validation loss: 0.10156968, Gradient norm: 4.02744265
INFO:root:[   69] Training loss: 0.07931273, Validation loss: 0.08861588, Gradient norm: 3.63130283
INFO:root:[   70] Training loss: 0.07765918, Validation loss: 0.10324060, Gradient norm: 4.23284949
INFO:root:[   71] Training loss: 0.07808548, Validation loss: 0.08371434, Gradient norm: 5.09975733
INFO:root:[   72] Training loss: 0.08740807, Validation loss: 0.10048119, Gradient norm: 6.23469786
INFO:root:[   73] Training loss: 0.07740041, Validation loss: 0.10314281, Gradient norm: 3.89886894
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 1365.061s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.08964
INFO:root:EnergyScoreTrain: 0.05954
INFO:root:CoverageTrain: 0.11617
INFO:root:IntervalWidthTrain: 0.02073
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0857
INFO:root:EnergyScoreValidation: 0.0536
INFO:root:CoverageValidation: 0.12144
INFO:root:IntervalWidthValidation: 0.02123
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.09061
INFO:root:EnergyScoreTest: 0.05714
INFO:root:CoverageTest: 0.10895
INFO:root:IntervalWidthTest: 0.01916
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1015021568
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.49903324, Validation loss: 0.24013221, Gradient norm: 8.71414440
INFO:root:[    2] Training loss: 0.27158800, Validation loss: 0.19795680, Gradient norm: 7.43865361
INFO:root:[    3] Training loss: 0.24591933, Validation loss: 0.22293964, Gradient norm: 8.23311095
INFO:root:[    4] Training loss: 0.22451215, Validation loss: 0.23852982, Gradient norm: 8.13922370
INFO:root:[    5] Training loss: 0.19950377, Validation loss: 0.17732149, Gradient norm: 6.70161325
INFO:root:[    6] Training loss: 0.18309554, Validation loss: 0.17088218, Gradient norm: 6.38131135
INFO:root:[    7] Training loss: 0.17296735, Validation loss: 0.18165044, Gradient norm: 5.68520743
INFO:root:[    8] Training loss: 0.15916146, Validation loss: 0.13893895, Gradient norm: 4.55460219
INFO:root:[    9] Training loss: 0.15037256, Validation loss: 0.16981532, Gradient norm: 4.09104231
INFO:root:[   10] Training loss: 0.14370121, Validation loss: 0.13108800, Gradient norm: 4.68976938
INFO:root:[   11] Training loss: 0.13485687, Validation loss: 0.14169138, Gradient norm: 3.94500018
INFO:root:[   12] Training loss: 0.12840463, Validation loss: 0.12258708, Gradient norm: 3.60888755
INFO:root:[   13] Training loss: 0.12316855, Validation loss: 0.12588518, Gradient norm: 3.56176619
INFO:root:[   14] Training loss: 0.11760422, Validation loss: 0.11692386, Gradient norm: 3.41144084
INFO:root:[   15] Training loss: 0.11345725, Validation loss: 0.11176304, Gradient norm: 3.15406179
INFO:root:[   16] Training loss: 0.11075772, Validation loss: 0.10986598, Gradient norm: 2.90025144
INFO:root:[   17] Training loss: 0.10622710, Validation loss: 0.11031009, Gradient norm: 2.82508204
INFO:root:[   18] Training loss: 0.10448625, Validation loss: 0.09718536, Gradient norm: 2.70527454
INFO:root:[   19] Training loss: 0.10433862, Validation loss: 0.10534403, Gradient norm: 2.71948791
INFO:root:[   20] Training loss: 0.09961434, Validation loss: 0.10108098, Gradient norm: 2.02943751
INFO:root:[   21] Training loss: 0.09756910, Validation loss: 0.10609720, Gradient norm: 2.25010098
INFO:root:[   22] Training loss: 0.09854606, Validation loss: 0.09093051, Gradient norm: 2.63373796
INFO:root:[   23] Training loss: 0.09921760, Validation loss: 0.10329428, Gradient norm: 2.92188504
INFO:root:[   24] Training loss: 0.09526265, Validation loss: 0.09792694, Gradient norm: 2.33265378
INFO:root:[   25] Training loss: 0.09425229, Validation loss: 0.09275637, Gradient norm: 2.06230799
INFO:root:[   26] Training loss: 0.09120857, Validation loss: 0.09205866, Gradient norm: 2.05847511
INFO:root:[   27] Training loss: 0.09068156, Validation loss: 0.08768494, Gradient norm: 1.99470040
INFO:root:[   28] Training loss: 0.09024669, Validation loss: 0.09436837, Gradient norm: 2.09657724
INFO:root:[   29] Training loss: 0.08915840, Validation loss: 0.09019031, Gradient norm: 2.17166875
INFO:root:[   30] Training loss: 0.08739568, Validation loss: 0.08574661, Gradient norm: 2.03765285
INFO:root:[   31] Training loss: 0.08596174, Validation loss: 0.08489397, Gradient norm: 1.88699157
INFO:root:[   32] Training loss: 0.08609119, Validation loss: 0.08358921, Gradient norm: 1.94300616
INFO:root:[   33] Training loss: 0.08632778, Validation loss: 0.09091395, Gradient norm: 2.30416940
INFO:root:[   34] Training loss: 0.08563312, Validation loss: 0.08754078, Gradient norm: 2.02401534
INFO:root:[   35] Training loss: 0.08397831, Validation loss: 0.08188398, Gradient norm: 1.75324726
INFO:root:[   36] Training loss: 0.08273401, Validation loss: 0.08139632, Gradient norm: 2.00064967
INFO:root:[   37] Training loss: 0.08227558, Validation loss: 0.08811475, Gradient norm: 1.89032050
INFO:root:[   38] Training loss: 0.08359583, Validation loss: 0.08738046, Gradient norm: 2.07806334
INFO:root:[   39] Training loss: 0.08010940, Validation loss: 0.08388518, Gradient norm: 1.81342989
INFO:root:[   40] Training loss: 0.08019024, Validation loss: 0.08156685, Gradient norm: 1.92597678
INFO:root:[   41] Training loss: 0.08028682, Validation loss: 0.08410370, Gradient norm: 2.11677478
INFO:root:[   42] Training loss: 0.07919508, Validation loss: 0.07651712, Gradient norm: 2.11740690
INFO:root:[   43] Training loss: 0.07855768, Validation loss: 0.07994320, Gradient norm: 1.96362146
INFO:root:[   44] Training loss: 0.07812377, Validation loss: 0.07990213, Gradient norm: 2.00787156
INFO:root:[   45] Training loss: 0.07635067, Validation loss: 0.08039546, Gradient norm: 1.79082280
INFO:root:[   46] Training loss: 0.07705479, Validation loss: 0.07313589, Gradient norm: 2.12446785
INFO:root:[   47] Training loss: 0.07626716, Validation loss: 0.07954441, Gradient norm: 1.90446201
INFO:root:[   48] Training loss: 0.07726526, Validation loss: 0.07319810, Gradient norm: 2.16072034
INFO:root:[   49] Training loss: 0.07509641, Validation loss: 0.07265981, Gradient norm: 1.82234328
INFO:root:[   50] Training loss: 0.07393413, Validation loss: 0.07620905, Gradient norm: 1.99357980
INFO:root:[   51] Training loss: 0.07274363, Validation loss: 0.07359566, Gradient norm: 1.94293035
INFO:root:[   52] Training loss: 0.07287733, Validation loss: 0.06936707, Gradient norm: 2.05576605
INFO:root:[   53] Training loss: 0.07237128, Validation loss: 0.06917063, Gradient norm: 1.95834638
INFO:root:[   54] Training loss: 0.07260238, Validation loss: 0.06812106, Gradient norm: 2.01882780
INFO:root:[   55] Training loss: 0.07131973, Validation loss: 0.07662530, Gradient norm: 1.88137391
INFO:root:[   56] Training loss: 0.07227263, Validation loss: 0.07046321, Gradient norm: 2.15013737
INFO:root:[   57] Training loss: 0.07246084, Validation loss: 0.07580034, Gradient norm: 2.09926867
INFO:root:[   58] Training loss: 0.07007332, Validation loss: 0.07138920, Gradient norm: 2.05267817
INFO:root:[   59] Training loss: 0.07060079, Validation loss: 0.06764228, Gradient norm: 1.90650028
INFO:root:[   60] Training loss: 0.06845598, Validation loss: 0.06721877, Gradient norm: 1.77182148
INFO:root:[   61] Training loss: 0.06888323, Validation loss: 0.06800049, Gradient norm: 2.03270134
INFO:root:[   62] Training loss: 0.06874679, Validation loss: 0.06520220, Gradient norm: 2.03887833
INFO:root:[   63] Training loss: 0.06822136, Validation loss: 0.06736613, Gradient norm: 2.28068923
INFO:root:[   64] Training loss: 0.06739100, Validation loss: 0.06723878, Gradient norm: 2.16974553
INFO:root:[   65] Training loss: 0.06714922, Validation loss: 0.06744005, Gradient norm: 2.17417773
INFO:root:[   66] Training loss: 0.06784293, Validation loss: 0.07096928, Gradient norm: 2.18082546
INFO:root:[   67] Training loss: 0.06822356, Validation loss: 0.06712279, Gradient norm: 2.09457142
INFO:root:[   68] Training loss: 0.06583156, Validation loss: 0.06698556, Gradient norm: 2.07816562
INFO:root:[   69] Training loss: 0.06640093, Validation loss: 0.06284424, Gradient norm: 2.26927984
INFO:root:[   70] Training loss: 0.06510283, Validation loss: 0.06272029, Gradient norm: 1.97867630
INFO:root:[   71] Training loss: 0.06420903, Validation loss: 0.06742143, Gradient norm: 1.94429411
INFO:root:[   72] Training loss: 0.06502901, Validation loss: 0.06311957, Gradient norm: 2.12218235
INFO:root:[   73] Training loss: 0.06455088, Validation loss: 0.06314503, Gradient norm: 2.03491403
INFO:root:[   74] Training loss: 0.06394647, Validation loss: 0.06109594, Gradient norm: 2.05625810
INFO:root:[   75] Training loss: 0.06390251, Validation loss: 0.06316313, Gradient norm: 2.16536999
INFO:root:[   76] Training loss: 0.06371989, Validation loss: 0.06660767, Gradient norm: 2.28917816
INFO:root:[   77] Training loss: 0.06410663, Validation loss: 0.06300441, Gradient norm: 2.37490041
INFO:root:[   78] Training loss: 0.06289807, Validation loss: 0.06076726, Gradient norm: 1.97871466
INFO:root:[   79] Training loss: 0.06237268, Validation loss: 0.06306908, Gradient norm: 2.26957205
INFO:root:[   80] Training loss: 0.06193470, Validation loss: 0.06166910, Gradient norm: 2.01300967
INFO:root:[   81] Training loss: 0.06157031, Validation loss: 0.05854028, Gradient norm: 2.26525068
INFO:root:[   82] Training loss: 0.06110506, Validation loss: 0.06011927, Gradient norm: 2.25590734
INFO:root:[   83] Training loss: 0.06083993, Validation loss: 0.05777596, Gradient norm: 2.37015537
INFO:root:[   84] Training loss: 0.05982013, Validation loss: 0.06134951, Gradient norm: 2.03677621
INFO:root:[   85] Training loss: 0.06023009, Validation loss: 0.06111135, Gradient norm: 2.20896665
INFO:root:[   86] Training loss: 0.06084378, Validation loss: 0.06269703, Gradient norm: 2.06137248
INFO:root:[   87] Training loss: 0.06001232, Validation loss: 0.05913514, Gradient norm: 2.30920068
INFO:root:[   88] Training loss: 0.06004693, Validation loss: 0.05945616, Gradient norm: 2.51796312
INFO:root:[   89] Training loss: 0.05859661, Validation loss: 0.05625988, Gradient norm: 2.38102697
INFO:root:[   90] Training loss: 0.06035224, Validation loss: 0.05922418, Gradient norm: 2.10382485
INFO:root:[   91] Training loss: 0.05781881, Validation loss: 0.05788917, Gradient norm: 2.59244707
INFO:root:[   92] Training loss: 0.05687136, Validation loss: 0.05608265, Gradient norm: 2.69982900
INFO:root:[   93] Training loss: 0.05698642, Validation loss: 0.05812562, Gradient norm: 2.70668901
INFO:root:[   94] Training loss: 0.05702750, Validation loss: 0.05667189, Gradient norm: 2.69909121
INFO:root:[   95] Training loss: 0.05656643, Validation loss: 0.05755043, Gradient norm: 2.83506865
INFO:root:[   96] Training loss: 0.05683908, Validation loss: 0.05536674, Gradient norm: 2.85334234
INFO:root:[   97] Training loss: 0.05653370, Validation loss: 0.05826123, Gradient norm: 2.80869823
INFO:root:[   98] Training loss: 0.05637619, Validation loss: 0.05695197, Gradient norm: 2.88754071
INFO:root:[   99] Training loss: 0.05605423, Validation loss: 0.05577209, Gradient norm: 2.85222222
INFO:root:[  100] Training loss: 0.05610942, Validation loss: 0.05602487, Gradient norm: 2.91107798
INFO:root:[  101] Training loss: 0.05622336, Validation loss: 0.05666776, Gradient norm: 2.66045672
INFO:root:[  102] Training loss: 0.05526493, Validation loss: 0.05463432, Gradient norm: 3.09646367
INFO:root:[  103] Training loss: 0.05491145, Validation loss: 0.05407214, Gradient norm: 2.87965412
INFO:root:[  104] Training loss: 0.05486680, Validation loss: 0.05332979, Gradient norm: 3.10877039
INFO:root:[  105] Training loss: 0.05465874, Validation loss: 0.05441748, Gradient norm: 2.93650884
INFO:root:[  106] Training loss: 0.05452225, Validation loss: 0.05201220, Gradient norm: 3.20308778
INFO:root:[  107] Training loss: 0.05428889, Validation loss: 0.05374336, Gradient norm: 3.11356314
INFO:root:[  108] Training loss: 0.05355645, Validation loss: 0.05211208, Gradient norm: 3.23864887
INFO:root:[  109] Training loss: 0.05368502, Validation loss: 0.05283090, Gradient norm: 3.07057199
INFO:root:[  110] Training loss: 0.05411367, Validation loss: 0.05266373, Gradient norm: 3.30470324
INFO:root:[  111] Training loss: 0.05318759, Validation loss: 0.05489850, Gradient norm: 3.25201010
INFO:root:[  112] Training loss: 0.05577618, Validation loss: 0.05458141, Gradient norm: 2.92463381
INFO:root:[  113] Training loss: 0.05315023, Validation loss: 0.05229667, Gradient norm: 3.29553960
INFO:root:[  114] Training loss: 0.05356888, Validation loss: 0.05440128, Gradient norm: 2.92745886
INFO:root:[  115] Training loss: 0.05262837, Validation loss: 0.05283039, Gradient norm: 3.41932552
INFO:root:EP 115: Early stopping
INFO:root:Training the model took 2146.262s.
INFO:root:Emptying the cuda cache took 0.036s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03464
INFO:root:EnergyScoreTrain: 0.02159
INFO:root:CoverageTrain: 0.9386
INFO:root:IntervalWidthTrain: 0.04371
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03449
INFO:root:EnergyScoreValidation: 0.0214
INFO:root:CoverageValidation: 0.93953
INFO:root:IntervalWidthValidation: 0.04371
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03432
INFO:root:EnergyScoreTest: 0.02137
INFO:root:CoverageTest: 0.9385
INFO:root:IntervalWidthTest: 0.0433
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.70034317, Validation loss: 0.56724315, Gradient norm: 7.55977875
INFO:root:[    2] Training loss: 0.38339799, Validation loss: 0.40722899, Gradient norm: 4.91328115
INFO:root:[    3] Training loss: 0.34014697, Validation loss: 0.30344594, Gradient norm: 5.97382702
INFO:root:[    4] Training loss: 0.30498265, Validation loss: 0.34748389, Gradient norm: 5.74688835
INFO:root:[    5] Training loss: 0.26726762, Validation loss: 0.26140396, Gradient norm: 4.31151649
INFO:root:[    6] Training loss: 0.24380948, Validation loss: 0.23227362, Gradient norm: 3.87212462
INFO:root:[    7] Training loss: 0.22078706, Validation loss: 0.20949454, Gradient norm: 2.65930779
INFO:root:[    8] Training loss: 0.21365704, Validation loss: 0.20011473, Gradient norm: 3.04711392
INFO:root:[    9] Training loss: 0.20029811, Validation loss: 0.19093612, Gradient norm: 2.42695275
INFO:root:[   10] Training loss: 0.19024811, Validation loss: 0.18491006, Gradient norm: 1.46949603
INFO:root:[   11] Training loss: 0.18606030, Validation loss: 0.17860999, Gradient norm: 2.02847803
INFO:root:[   12] Training loss: 0.17723912, Validation loss: 0.17987309, Gradient norm: 1.37153837
INFO:root:[   13] Training loss: 0.17596167, Validation loss: 0.17183220, Gradient norm: 2.03482236
INFO:root:[   14] Training loss: 0.16984611, Validation loss: 0.16852227, Gradient norm: 1.69616933
INFO:root:[   15] Training loss: 0.16644088, Validation loss: 0.16138541, Gradient norm: 1.55361440
INFO:root:[   16] Training loss: 0.16337050, Validation loss: 0.16035727, Gradient norm: 1.67513866
INFO:root:[   17] Training loss: 0.15918117, Validation loss: 0.15854055, Gradient norm: 1.63595327
INFO:root:[   18] Training loss: 0.15842612, Validation loss: 0.15262574, Gradient norm: 2.04931312
INFO:root:[   19] Training loss: 0.15419594, Validation loss: 0.15483783, Gradient norm: 1.73487701
INFO:root:[   20] Training loss: 0.15152293, Validation loss: 0.15205510, Gradient norm: 1.72653112
INFO:root:[   21] Training loss: 0.14875166, Validation loss: 0.14736625, Gradient norm: 1.75341870
INFO:root:[   22] Training loss: 0.14549861, Validation loss: 0.14473802, Gradient norm: 1.56481639
INFO:root:[   23] Training loss: 0.14340667, Validation loss: 0.13850067, Gradient norm: 1.62923429
INFO:root:[   24] Training loss: 0.14086161, Validation loss: 0.13935654, Gradient norm: 1.72350656
INFO:root:[   25] Training loss: 0.13775817, Validation loss: 0.13799626, Gradient norm: 1.35040927
INFO:root:[   26] Training loss: 0.13598948, Validation loss: 0.13192385, Gradient norm: 1.44419867
INFO:root:[   27] Training loss: 0.13414203, Validation loss: 0.12946373, Gradient norm: 1.73993499
INFO:root:[   28] Training loss: 0.13186455, Validation loss: 0.13063187, Gradient norm: 1.72229537
INFO:root:[   29] Training loss: 0.12896602, Validation loss: 0.12727115, Gradient norm: 1.48929334
INFO:root:[   30] Training loss: 0.12582601, Validation loss: 0.12243361, Gradient norm: 1.68197703
INFO:root:[   31] Training loss: 0.12418843, Validation loss: 0.12442715, Gradient norm: 1.63319888
INFO:root:[   32] Training loss: 0.12286888, Validation loss: 0.12484488, Gradient norm: 1.76775334
INFO:root:[   33] Training loss: 0.12093027, Validation loss: 0.12127356, Gradient norm: 1.51022635
INFO:root:[   34] Training loss: 0.11868564, Validation loss: 0.11611933, Gradient norm: 1.62519362
INFO:root:[   35] Training loss: 0.11620417, Validation loss: 0.11351439, Gradient norm: 1.66752944
INFO:root:[   36] Training loss: 0.11395417, Validation loss: 0.11311856, Gradient norm: 1.87094307
INFO:root:[   37] Training loss: 0.11234000, Validation loss: 0.11179821, Gradient norm: 1.70537448
INFO:root:[   38] Training loss: 0.10956224, Validation loss: 0.10610068, Gradient norm: 1.93587961
INFO:root:[   39] Training loss: 0.10739054, Validation loss: 0.10630704, Gradient norm: 1.63917421
INFO:root:[   40] Training loss: 0.10668707, Validation loss: 0.10221117, Gradient norm: 2.06735474
INFO:root:[   41] Training loss: 0.10410286, Validation loss: 0.10609736, Gradient norm: 1.62460699
INFO:root:[   42] Training loss: 0.10215141, Validation loss: 0.10127907, Gradient norm: 1.98348529
INFO:root:[   43] Training loss: 0.09958045, Validation loss: 0.09931319, Gradient norm: 1.83716069
INFO:root:[   44] Training loss: 0.09829010, Validation loss: 0.09961408, Gradient norm: 2.04441860
INFO:root:[   45] Training loss: 0.09689965, Validation loss: 0.09645283, Gradient norm: 2.23235168
INFO:root:[   46] Training loss: 0.09557743, Validation loss: 0.09306271, Gradient norm: 2.00938622
INFO:root:[   47] Training loss: 0.09377968, Validation loss: 0.09492719, Gradient norm: 1.83136356
INFO:root:[   48] Training loss: 0.09227075, Validation loss: 0.08981158, Gradient norm: 2.09857773
INFO:root:[   49] Training loss: 0.09039115, Validation loss: 0.08673734, Gradient norm: 1.94386986
INFO:root:[   50] Training loss: 0.08883924, Validation loss: 0.08804765, Gradient norm: 2.33970300
INFO:root:[   51] Training loss: 0.08659942, Validation loss: 0.08643592, Gradient norm: 1.98854604
INFO:root:[   52] Training loss: 0.08532843, Validation loss: 0.08199952, Gradient norm: 1.94383533
INFO:root:[   53] Training loss: 0.08536254, Validation loss: 0.09018704, Gradient norm: 2.19473176
INFO:root:[   54] Training loss: 0.08446720, Validation loss: 0.08254365, Gradient norm: 2.24712119
INFO:root:[   55] Training loss: 0.08157580, Validation loss: 0.08096008, Gradient norm: 2.62007264
INFO:root:[   56] Training loss: 0.08254471, Validation loss: 0.07810411, Gradient norm: 2.43598019
INFO:root:[   57] Training loss: 0.08054324, Validation loss: 0.07557230, Gradient norm: 2.24874086
INFO:root:[   58] Training loss: 0.07779636, Validation loss: 0.07748484, Gradient norm: 2.16145398
INFO:root:[   59] Training loss: 0.07888906, Validation loss: 0.08117428, Gradient norm: 2.45651582
INFO:root:[   60] Training loss: 0.07583421, Validation loss: 0.07188380, Gradient norm: 2.55266541
INFO:root:[   61] Training loss: 0.07484098, Validation loss: 0.07615146, Gradient norm: 2.18105343
INFO:root:[   62] Training loss: 0.07351151, Validation loss: 0.07146499, Gradient norm: 2.37574992
INFO:root:[   63] Training loss: 0.07177235, Validation loss: 0.06757134, Gradient norm: 2.30610664
INFO:root:[   64] Training loss: 0.06914833, Validation loss: 0.06946646, Gradient norm: 2.07365388
INFO:root:[   65] Training loss: 0.07044434, Validation loss: 0.06847973, Gradient norm: 2.42800049
INFO:root:[   66] Training loss: 0.06847347, Validation loss: 0.06839472, Gradient norm: 2.45566088
INFO:root:[   67] Training loss: 0.06894118, Validation loss: 0.06813857, Gradient norm: 2.23998304
INFO:root:[   68] Training loss: 0.06623889, Validation loss: 0.06417595, Gradient norm: 2.36728901
INFO:root:[   69] Training loss: 0.06658168, Validation loss: 0.07197833, Gradient norm: 2.58311948
INFO:root:[   70] Training loss: 0.06832588, Validation loss: 0.06546751, Gradient norm: 2.07811202
INFO:root:[   71] Training loss: 0.06360045, Validation loss: 0.06281488, Gradient norm: 2.34642556
INFO:root:[   72] Training loss: 0.06243342, Validation loss: 0.06191950, Gradient norm: 2.49049144
INFO:root:[   73] Training loss: 0.06353080, Validation loss: 0.06435867, Gradient norm: 2.28258338
INFO:root:[   74] Training loss: 0.06250169, Validation loss: 0.05923349, Gradient norm: 2.48841323
INFO:root:[   75] Training loss: 0.05874224, Validation loss: 0.05984542, Gradient norm: 2.94436193
INFO:root:[   76] Training loss: 0.05880832, Validation loss: 0.05834660, Gradient norm: 3.22116389
INFO:root:[   77] Training loss: 0.06194077, Validation loss: 0.05941942, Gradient norm: 3.03663852
INFO:root:[   78] Training loss: 0.06016835, Validation loss: 0.05705862, Gradient norm: 2.76233317
INFO:root:[   79] Training loss: 0.05818586, Validation loss: 0.05771430, Gradient norm: 2.70385360
INFO:root:[   80] Training loss: 0.05562138, Validation loss: 0.05416539, Gradient norm: 3.21156728
INFO:root:[   81] Training loss: 0.05623932, Validation loss: 0.05566009, Gradient norm: 3.25317448
INFO:root:[   82] Training loss: 0.05512776, Validation loss: 0.05331961, Gradient norm: 3.31073742
INFO:root:[   83] Training loss: 0.05471502, Validation loss: 0.05450653, Gradient norm: 3.40084694
INFO:root:[   84] Training loss: 0.05487319, Validation loss: 0.05693758, Gradient norm: 3.51840375
INFO:root:[   85] Training loss: 0.05538187, Validation loss: 0.05591421, Gradient norm: 3.12494085
INFO:root:[   86] Training loss: 0.05584621, Validation loss: 0.05776793, Gradient norm: 3.47442813
INFO:root:[   87] Training loss: 0.05500426, Validation loss: 0.05511403, Gradient norm: 2.53294023
INFO:root:[   88] Training loss: 0.05347113, Validation loss: 0.05345333, Gradient norm: 3.71139139
INFO:root:[   89] Training loss: 0.05242785, Validation loss: 0.04885203, Gradient norm: 3.66000844
INFO:root:[   90] Training loss: 0.05269866, Validation loss: 0.05047605, Gradient norm: 4.05716872
INFO:root:[   91] Training loss: 0.05258992, Validation loss: 0.05136029, Gradient norm: 3.03583848
INFO:root:[   92] Training loss: 0.05176057, Validation loss: 0.05061960, Gradient norm: 4.03696111
INFO:root:[   93] Training loss: 0.05149406, Validation loss: 0.05008808, Gradient norm: 3.70414741
INFO:root:[   94] Training loss: 0.05306877, Validation loss: 0.05137422, Gradient norm: 4.27868499
INFO:root:[   95] Training loss: 0.05018857, Validation loss: 0.05102732, Gradient norm: 3.78881808
INFO:root:[   96] Training loss: 0.05233492, Validation loss: 0.05076855, Gradient norm: 3.96357430
INFO:root:[   97] Training loss: 0.05026404, Validation loss: 0.04988430, Gradient norm: 4.28111814
INFO:root:[   98] Training loss: 0.04977173, Validation loss: 0.05191768, Gradient norm: 4.15877469
INFO:root:[   99] Training loss: 0.04975633, Validation loss: 0.04864109, Gradient norm: 4.68846797
INFO:root:[  100] Training loss: 0.05033920, Validation loss: 0.05087491, Gradient norm: 3.74185378
INFO:root:[  101] Training loss: 0.05169769, Validation loss: 0.04765781, Gradient norm: 4.00152921
INFO:root:[  102] Training loss: 0.05003977, Validation loss: 0.04814390, Gradient norm: 3.90364915
INFO:root:[  103] Training loss: 0.04897184, Validation loss: 0.05314843, Gradient norm: 4.37388669
INFO:root:[  104] Training loss: 0.04915939, Validation loss: 0.04709840, Gradient norm: 4.72953476
INFO:root:[  105] Training loss: 0.04832874, Validation loss: 0.04528694, Gradient norm: 3.94829512
INFO:root:[  106] Training loss: 0.04918369, Validation loss: 0.05268732, Gradient norm: 4.31071547
INFO:root:[  107] Training loss: 0.05224810, Validation loss: 0.05750343, Gradient norm: 6.59838942
INFO:root:[  108] Training loss: 0.05025261, Validation loss: 0.04754267, Gradient norm: 5.30804475
INFO:root:[  109] Training loss: 0.04814765, Validation loss: 0.04785219, Gradient norm: 4.78900736
INFO:root:[  110] Training loss: 0.05123776, Validation loss: 0.04675065, Gradient norm: 6.26060887
INFO:root:[  111] Training loss: 0.04687706, Validation loss: 0.04483448, Gradient norm: 4.51623604
INFO:root:[  112] Training loss: 0.05026064, Validation loss: 0.05931324, Gradient norm: 6.37058398
INFO:root:[  113] Training loss: 0.04934787, Validation loss: 0.04484435, Gradient norm: 6.44943015
INFO:root:[  114] Training loss: 0.04883173, Validation loss: 0.04799353, Gradient norm: 5.73200085
INFO:root:[  115] Training loss: 0.04871953, Validation loss: 0.04712401, Gradient norm: 7.38117447
INFO:root:[  116] Training loss: 0.04913485, Validation loss: 0.05031619, Gradient norm: 6.50113210
INFO:root:[  117] Training loss: 0.04842667, Validation loss: 0.04865442, Gradient norm: 5.91526187
INFO:root:[  118] Training loss: 0.04973397, Validation loss: 0.04975226, Gradient norm: 7.02432315
INFO:root:[  119] Training loss: 0.04755997, Validation loss: 0.05414748, Gradient norm: 6.05465542
INFO:root:[  120] Training loss: 0.04657593, Validation loss: 0.05349783, Gradient norm: 6.61264342
INFO:root:EP 120: Early stopping
INFO:root:Training the model took 2246.427s.
INFO:root:Emptying the cuda cache took 0.036s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03055
INFO:root:EnergyScoreTrain: 0.03088
INFO:root:CoverageTrain: 0.80651
INFO:root:IntervalWidthTrain: 0.02092
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03049
INFO:root:EnergyScoreValidation: 0.03066
INFO:root:CoverageValidation: 0.80465
INFO:root:IntervalWidthValidation: 0.02088
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03004
INFO:root:EnergyScoreTest: 0.03128
INFO:root:CoverageTest: 0.80297
INFO:root:IntervalWidthTest: 0.02037
INFO:root:###21 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72916230, Validation loss: 0.46808420, Gradient norm: 5.39906927
INFO:root:[    2] Training loss: 0.41285478, Validation loss: 0.35452748, Gradient norm: 3.41165079
INFO:root:[    3] Training loss: 0.32895099, Validation loss: 0.27910626, Gradient norm: 3.29402565
INFO:root:[    4] Training loss: 0.26025189, Validation loss: 0.24468644, Gradient norm: 1.99005370
INFO:root:[    5] Training loss: 0.23640890, Validation loss: 0.22043961, Gradient norm: 2.41332321
INFO:root:[    6] Training loss: 0.21560716, Validation loss: 0.20636249, Gradient norm: 1.07354602
INFO:root:[    7] Training loss: 0.20263348, Validation loss: 0.19882236, Gradient norm: 1.54249570
INFO:root:[    8] Training loss: 0.19393962, Validation loss: 0.18642338, Gradient norm: 1.37177850
INFO:root:[    9] Training loss: 0.18695152, Validation loss: 0.18552330, Gradient norm: 1.61150081
INFO:root:[   10] Training loss: 0.18154296, Validation loss: 0.17780800, Gradient norm: 1.38809861
INFO:root:[   11] Training loss: 0.17497967, Validation loss: 0.16861456, Gradient norm: 1.60231574
INFO:root:[   12] Training loss: 0.16861878, Validation loss: 0.16584328, Gradient norm: 1.71921879
INFO:root:[   13] Training loss: 0.16491467, Validation loss: 0.16339351, Gradient norm: 1.38715368
INFO:root:[   14] Training loss: 0.15950634, Validation loss: 0.15335448, Gradient norm: 1.32920738
INFO:root:[   15] Training loss: 0.15351642, Validation loss: 0.14748414, Gradient norm: 1.67084888
INFO:root:[   16] Training loss: 0.14782207, Validation loss: 0.14241215, Gradient norm: 1.72538459
INFO:root:[   17] Training loss: 0.14279725, Validation loss: 0.13912959, Gradient norm: 1.51360868
INFO:root:[   18] Training loss: 0.13969365, Validation loss: 0.13592241, Gradient norm: 1.71245949
INFO:root:[   19] Training loss: 0.13481288, Validation loss: 0.13132427, Gradient norm: 1.45374856
INFO:root:[   20] Training loss: 0.13152065, Validation loss: 0.12751445, Gradient norm: 1.63418281
INFO:root:[   21] Training loss: 0.12571568, Validation loss: 0.12660272, Gradient norm: 1.25244510
INFO:root:[   22] Training loss: 0.12149754, Validation loss: 0.11828192, Gradient norm: 1.89241384
INFO:root:[   23] Training loss: 0.11690391, Validation loss: 0.11299310, Gradient norm: 1.94171193
INFO:root:[   24] Training loss: 0.11574177, Validation loss: 0.11106801, Gradient norm: 2.00862690
INFO:root:[   25] Training loss: 0.11212332, Validation loss: 0.11091203, Gradient norm: 2.10745289
INFO:root:[   26] Training loss: 0.10722255, Validation loss: 0.10987906, Gradient norm: 2.04644357
INFO:root:[   27] Training loss: 0.10648722, Validation loss: 0.10087197, Gradient norm: 2.48348149
INFO:root:[   28] Training loss: 0.10160400, Validation loss: 0.10198341, Gradient norm: 2.11693083
INFO:root:[   29] Training loss: 0.09875771, Validation loss: 0.10133635, Gradient norm: 2.33856255
INFO:root:[   30] Training loss: 0.09633965, Validation loss: 0.09034984, Gradient norm: 2.55855753
INFO:root:[   31] Training loss: 0.09550694, Validation loss: 0.09066825, Gradient norm: 2.21815766
INFO:root:[   32] Training loss: 0.09131987, Validation loss: 0.08482316, Gradient norm: 2.34583978
INFO:root:[   33] Training loss: 0.08793060, Validation loss: 0.08530824, Gradient norm: 2.05977947
INFO:root:[   34] Training loss: 0.08503573, Validation loss: 0.08366341, Gradient norm: 1.97528123
INFO:root:[   35] Training loss: 0.08091491, Validation loss: 0.07951871, Gradient norm: 2.19582354
INFO:root:[   36] Training loss: 0.08219386, Validation loss: 0.08456476, Gradient norm: 2.30467810
INFO:root:[   37] Training loss: 0.07965258, Validation loss: 0.08018047, Gradient norm: 2.46073085
INFO:root:[   38] Training loss: 0.07720746, Validation loss: 0.07623089, Gradient norm: 2.44535298
INFO:root:[   39] Training loss: 0.07763638, Validation loss: 0.07233863, Gradient norm: 2.71085293
INFO:root:[   40] Training loss: 0.07436751, Validation loss: 0.07175128, Gradient norm: 2.41654552
INFO:root:[   41] Training loss: 0.07317681, Validation loss: 0.07860927, Gradient norm: 2.06330148
INFO:root:[   42] Training loss: 0.07152437, Validation loss: 0.07037692, Gradient norm: 2.58495422
INFO:root:[   43] Training loss: 0.07079947, Validation loss: 0.07652690, Gradient norm: 2.76349086
INFO:root:[   44] Training loss: 0.06876704, Validation loss: 0.06575269, Gradient norm: 3.06855428
INFO:root:[   45] Training loss: 0.06823445, Validation loss: 0.06451538, Gradient norm: 3.22875725
INFO:root:[   46] Training loss: 0.06627059, Validation loss: 0.06855346, Gradient norm: 3.09862918
INFO:root:[   47] Training loss: 0.06698972, Validation loss: 0.07031468, Gradient norm: 3.20583765
INFO:root:[   48] Training loss: 0.06638212, Validation loss: 0.07050258, Gradient norm: 2.84784612
INFO:root:[   49] Training loss: 0.06334892, Validation loss: 0.06160813, Gradient norm: 3.50991153
INFO:root:[   50] Training loss: 0.06321082, Validation loss: 0.07214295, Gradient norm: 3.41028350
INFO:root:[   51] Training loss: 0.06372350, Validation loss: 0.06311479, Gradient norm: 2.98943753
INFO:root:[   52] Training loss: 0.06350358, Validation loss: 0.06399455, Gradient norm: 3.07603718
INFO:root:[   53] Training loss: 0.06557328, Validation loss: 0.06555673, Gradient norm: 4.32621562
INFO:root:[   54] Training loss: 0.06290804, Validation loss: 0.06152475, Gradient norm: 3.80211111
INFO:root:[   55] Training loss: 0.06164276, Validation loss: 0.06456179, Gradient norm: 3.86352587
INFO:root:[   56] Training loss: 0.06228639, Validation loss: 0.06687985, Gradient norm: 3.63754635
INFO:root:[   57] Training loss: 0.06479379, Validation loss: 0.06315851, Gradient norm: 4.51841253
INFO:root:[   58] Training loss: 0.06164818, Validation loss: 0.06338438, Gradient norm: 3.49498494
INFO:root:[   59] Training loss: 0.06020473, Validation loss: 0.05933525, Gradient norm: 3.92284756
INFO:root:[   60] Training loss: 0.06162512, Validation loss: 0.05977988, Gradient norm: 4.88185761
INFO:root:[   61] Training loss: 0.06028403, Validation loss: 0.06042937, Gradient norm: 4.07510632
INFO:root:[   62] Training loss: 0.06185025, Validation loss: 0.06687298, Gradient norm: 5.93185006
INFO:root:[   63] Training loss: 0.06221699, Validation loss: 0.06147871, Gradient norm: 5.04308906
INFO:root:[   64] Training loss: 0.05961619, Validation loss: 0.06023459, Gradient norm: 4.32015795
INFO:root:[   65] Training loss: 0.06202190, Validation loss: 0.06278740, Gradient norm: 5.59692657
INFO:root:[   66] Training loss: 0.06152368, Validation loss: 0.06365857, Gradient norm: 5.88693770
INFO:root:[   67] Training loss: 0.06107970, Validation loss: 0.05812038, Gradient norm: 4.11544608
INFO:root:[   68] Training loss: 0.05879010, Validation loss: 0.06282315, Gradient norm: 5.62146468
INFO:root:[   69] Training loss: 0.05990778, Validation loss: 0.05859606, Gradient norm: 6.54978380
INFO:root:[   70] Training loss: 0.05737548, Validation loss: 0.05488864, Gradient norm: 4.76556392
INFO:root:[   71] Training loss: 0.05877046, Validation loss: 0.05642884, Gradient norm: 6.70924131
INFO:root:[   72] Training loss: 0.06137405, Validation loss: 0.05607793, Gradient norm: 6.15716101
INFO:root:[   73] Training loss: 0.05732088, Validation loss: 0.05363676, Gradient norm: 5.71249576
INFO:root:[   74] Training loss: 0.05773119, Validation loss: 0.05822857, Gradient norm: 5.98936581
INFO:root:[   75] Training loss: 0.05739174, Validation loss: 0.05508051, Gradient norm: 6.12443702
INFO:root:[   76] Training loss: 0.05930237, Validation loss: 0.07007072, Gradient norm: 7.54402094
INFO:root:[   77] Training loss: 0.06392298, Validation loss: 0.05897176, Gradient norm: 7.64684327
INFO:root:[   78] Training loss: 0.05979926, Validation loss: 0.05751459, Gradient norm: 7.69388978
INFO:root:[   79] Training loss: 0.05723219, Validation loss: 0.05506312, Gradient norm: 5.18947856
INFO:root:[   80] Training loss: 0.05777622, Validation loss: 0.05501451, Gradient norm: 6.48316429
INFO:root:[   81] Training loss: 0.05755195, Validation loss: 0.05604946, Gradient norm: 6.74874710
INFO:root:[   82] Training loss: 0.05684196, Validation loss: 0.08889638, Gradient norm: 6.84211134
INFO:root:EP 82: Early stopping
INFO:root:Training the model took 1538.334s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03537
INFO:root:EnergyScoreTrain: 0.02622
INFO:root:CoverageTrain: 0.77451
INFO:root:IntervalWidthTrain: 0.02483
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03517
INFO:root:EnergyScoreValidation: 0.02626
INFO:root:CoverageValidation: 0.77304
INFO:root:IntervalWidthValidation: 0.02478
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03501
INFO:root:EnergyScoreTest: 0.02605
INFO:root:CoverageTest: 0.77291
INFO:root:IntervalWidthTest: 0.02416
INFO:root:###22 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.85076161, Validation loss: 0.53158461, Gradient norm: 5.93690625
INFO:root:[    2] Training loss: 0.47959163, Validation loss: 0.38635333, Gradient norm: 4.98150270
INFO:root:[    3] Training loss: 0.36273972, Validation loss: 0.33245645, Gradient norm: 2.89918617
INFO:root:[    4] Training loss: 0.31087544, Validation loss: 0.29068329, Gradient norm: 1.94345456
INFO:root:[    5] Training loss: 0.28279612, Validation loss: 0.27135422, Gradient norm: 1.90543861
INFO:root:[    6] Training loss: 0.26482919, Validation loss: 0.25251343, Gradient norm: 1.96977654
INFO:root:[    7] Training loss: 0.24848676, Validation loss: 0.24108621, Gradient norm: 1.37648051
INFO:root:[    8] Training loss: 0.23686189, Validation loss: 0.22842872, Gradient norm: 1.42278432
INFO:root:[    9] Training loss: 0.22503598, Validation loss: 0.22224825, Gradient norm: 1.52781056
INFO:root:[   10] Training loss: 0.21594981, Validation loss: 0.21332864, Gradient norm: 1.14014609
INFO:root:[   11] Training loss: 0.20844925, Validation loss: 0.20303664, Gradient norm: 1.15665719
INFO:root:[   12] Training loss: 0.19977087, Validation loss: 0.20028324, Gradient norm: 1.46501939
INFO:root:[   13] Training loss: 0.19152829, Validation loss: 0.18594585, Gradient norm: 1.60697121
INFO:root:[   14] Training loss: 0.18348350, Validation loss: 0.17995408, Gradient norm: 1.36976751
INFO:root:[   15] Training loss: 0.17634175, Validation loss: 0.17265743, Gradient norm: 1.93914486
INFO:root:[   16] Training loss: 0.17018075, Validation loss: 0.16653012, Gradient norm: 1.33047481
INFO:root:[   17] Training loss: 0.16253028, Validation loss: 0.15981506, Gradient norm: 1.31734930
INFO:root:[   18] Training loss: 0.15499329, Validation loss: 0.14906210, Gradient norm: 2.04470344
INFO:root:[   19] Training loss: 0.14864405, Validation loss: 0.14376178, Gradient norm: 2.22247219
INFO:root:[   20] Training loss: 0.14270387, Validation loss: 0.14040387, Gradient norm: 2.33531844
INFO:root:[   21] Training loss: 0.13638948, Validation loss: 0.13227731, Gradient norm: 1.60024941
INFO:root:[   22] Training loss: 0.13037570, Validation loss: 0.12807596, Gradient norm: 2.29708571
INFO:root:[   23] Training loss: 0.12657351, Validation loss: 0.12007500, Gradient norm: 2.69497595
INFO:root:[   24] Training loss: 0.11907454, Validation loss: 0.11918402, Gradient norm: 2.01848743
INFO:root:[   25] Training loss: 0.11506133, Validation loss: 0.11115394, Gradient norm: 2.49749878
INFO:root:[   26] Training loss: 0.11179504, Validation loss: 0.10887087, Gradient norm: 3.36441774
INFO:root:[   27] Training loss: 0.10720643, Validation loss: 0.10764221, Gradient norm: 2.84821607
INFO:root:[   28] Training loss: 0.10539276, Validation loss: 0.10359266, Gradient norm: 3.18812192
INFO:root:[   29] Training loss: 0.10399164, Validation loss: 0.09694335, Gradient norm: 2.73123213
INFO:root:[   30] Training loss: 0.09557443, Validation loss: 0.09459796, Gradient norm: 2.85238062
INFO:root:[   31] Training loss: 0.09337645, Validation loss: 0.09159759, Gradient norm: 2.96610659
INFO:root:[   32] Training loss: 0.08870123, Validation loss: 0.09169133, Gradient norm: 3.03127979
INFO:root:[   33] Training loss: 0.08604024, Validation loss: 0.08306648, Gradient norm: 2.72476553
INFO:root:[   34] Training loss: 0.08557317, Validation loss: 0.08494474, Gradient norm: 3.36268548
INFO:root:[   35] Training loss: 0.08052565, Validation loss: 0.08415950, Gradient norm: 3.51912586
INFO:root:[   36] Training loss: 0.07927977, Validation loss: 0.08177486, Gradient norm: 3.61855885
INFO:root:[   37] Training loss: 0.07837785, Validation loss: 0.07842205, Gradient norm: 3.08906729
INFO:root:[   38] Training loss: 0.07879751, Validation loss: 0.07597432, Gradient norm: 2.85113386
INFO:root:[   39] Training loss: 0.07644339, Validation loss: 0.07314694, Gradient norm: 3.08569824
INFO:root:[   40] Training loss: 0.07651143, Validation loss: 0.07363200, Gradient norm: 4.85810835
INFO:root:[   41] Training loss: 0.07444087, Validation loss: 0.07542772, Gradient norm: 3.97840685
INFO:root:[   42] Training loss: 0.07673467, Validation loss: 0.07332416, Gradient norm: 4.37104347
INFO:root:[   43] Training loss: 0.07327199, Validation loss: 0.07478480, Gradient norm: 5.06524662
INFO:root:[   44] Training loss: 0.07262352, Validation loss: 0.06990363, Gradient norm: 4.99434237
INFO:root:[   45] Training loss: 0.07257358, Validation loss: 0.07062962, Gradient norm: 4.22569144
INFO:root:[   46] Training loss: 0.07274714, Validation loss: 0.07909264, Gradient norm: 4.26671038
INFO:root:[   47] Training loss: 0.07345779, Validation loss: 0.07219960, Gradient norm: 3.67314774
INFO:root:[   48] Training loss: 0.07257739, Validation loss: 0.06830440, Gradient norm: 5.65966696
INFO:root:[   49] Training loss: 0.07204408, Validation loss: 0.07283768, Gradient norm: 4.46150477
INFO:root:[   50] Training loss: 0.07053860, Validation loss: 0.06646078, Gradient norm: 5.40687949
INFO:root:[   51] Training loss: 0.07054977, Validation loss: 0.07312510, Gradient norm: 5.81539211
INFO:root:[   52] Training loss: 0.07033846, Validation loss: 0.07443230, Gradient norm: 5.22983561
INFO:root:[   53] Training loss: 0.07165577, Validation loss: 0.06797101, Gradient norm: 6.03044580
INFO:root:[   54] Training loss: 0.07173993, Validation loss: 0.06910783, Gradient norm: 6.67204131
INFO:root:[   55] Training loss: 0.07118772, Validation loss: 0.06728610, Gradient norm: 6.10265537
INFO:root:[   56] Training loss: 0.06947075, Validation loss: 0.06728446, Gradient norm: 5.73432887
INFO:root:[   57] Training loss: 0.06955226, Validation loss: 0.06746470, Gradient norm: 6.62443874
INFO:root:[   58] Training loss: 0.06816523, Validation loss: 0.06441528, Gradient norm: 5.46535446
INFO:root:[   59] Training loss: 0.07075275, Validation loss: 0.06815439, Gradient norm: 7.80037603
INFO:root:[   60] Training loss: 0.06875280, Validation loss: 0.06562714, Gradient norm: 5.94683108
INFO:root:[   61] Training loss: 0.06877855, Validation loss: 0.06795323, Gradient norm: 6.23971588
INFO:root:[   62] Training loss: 0.06730041, Validation loss: 0.06411387, Gradient norm: 7.12758230
INFO:root:[   63] Training loss: 0.07227838, Validation loss: 0.06526267, Gradient norm: 8.01958737
INFO:root:[   64] Training loss: 0.06992096, Validation loss: 0.06607714, Gradient norm: 8.20611512
INFO:root:[   65] Training loss: 0.06777309, Validation loss: 0.06451855, Gradient norm: 5.80541779
INFO:root:[   66] Training loss: 0.06669664, Validation loss: 0.07183966, Gradient norm: 7.09629180
INFO:root:[   67] Training loss: 0.06950559, Validation loss: 0.07032854, Gradient norm: 8.51149359
INFO:root:[   68] Training loss: 0.06663401, Validation loss: 0.06530577, Gradient norm: 6.66732563
INFO:root:[   69] Training loss: 0.06501105, Validation loss: 0.06291352, Gradient norm: 6.64198869
INFO:root:[   70] Training loss: 0.06811204, Validation loss: 0.06432026, Gradient norm: 8.19094338
INFO:root:[   71] Training loss: 0.06614712, Validation loss: 0.07436096, Gradient norm: 6.40254913
INFO:root:[   72] Training loss: 0.06986828, Validation loss: 0.06351697, Gradient norm: 9.16655016
INFO:root:[   73] Training loss: 0.06592350, Validation loss: 0.06432750, Gradient norm: 5.98678062
INFO:root:[   74] Training loss: 0.07043854, Validation loss: 0.07417780, Gradient norm: 7.61009494
INFO:root:[   75] Training loss: 0.06699037, Validation loss: 0.06430111, Gradient norm: 8.16890206
INFO:root:[   76] Training loss: 0.06643791, Validation loss: 0.06339401, Gradient norm: 7.30300390
INFO:root:[   77] Training loss: 0.06570284, Validation loss: 0.06229777, Gradient norm: 7.90930184
INFO:root:[   78] Training loss: 0.06456066, Validation loss: 0.06335106, Gradient norm: 6.63098186
INFO:root:[   79] Training loss: 0.06287310, Validation loss: 0.06461437, Gradient norm: 5.43008798
INFO:root:[   80] Training loss: 0.06468243, Validation loss: 0.06302015, Gradient norm: 8.30591427
INFO:root:[   81] Training loss: 0.06729556, Validation loss: 0.06291133, Gradient norm: 8.07646818
INFO:root:[   82] Training loss: 0.06487177, Validation loss: 0.06220855, Gradient norm: 6.65597911
INFO:root:[   83] Training loss: 0.06266560, Validation loss: 0.06253015, Gradient norm: 5.27522989
INFO:root:[   84] Training loss: 0.06836302, Validation loss: 0.06186244, Gradient norm: 8.11591099
INFO:root:[   85] Training loss: 0.06227429, Validation loss: 0.06052983, Gradient norm: 5.87575431
INFO:root:[   86] Training loss: 0.06130572, Validation loss: 0.05979934, Gradient norm: 5.96250413
INFO:root:[   87] Training loss: 0.06850416, Validation loss: 0.06696491, Gradient norm: 9.05939986
INFO:root:[   88] Training loss: 0.06210772, Validation loss: 0.06294834, Gradient norm: 5.84246280
INFO:root:[   89] Training loss: 0.06524041, Validation loss: 0.06699481, Gradient norm: 7.22333513
INFO:root:[   90] Training loss: 0.06238524, Validation loss: 0.06469309, Gradient norm: 5.12398386
INFO:root:[   91] Training loss: 0.06148236, Validation loss: 0.06168115, Gradient norm: 6.39293410
INFO:root:[   92] Training loss: 0.06327521, Validation loss: 0.06649625, Gradient norm: 6.59316559
INFO:root:[   93] Training loss: 0.06275884, Validation loss: 0.06476812, Gradient norm: 6.14234576
INFO:root:[   94] Training loss: 0.06239659, Validation loss: 0.06125643, Gradient norm: 5.67929539
INFO:root:[   95] Training loss: 0.06288449, Validation loss: 0.05899558, Gradient norm: 6.71710019
INFO:root:[   96] Training loss: 0.06338895, Validation loss: 0.06383354, Gradient norm: 6.53340331
INFO:root:[   97] Training loss: 0.06180892, Validation loss: 0.06399623, Gradient norm: 5.76667107
INFO:root:[   98] Training loss: 0.06294276, Validation loss: 0.05849040, Gradient norm: 5.82836338
INFO:root:[   99] Training loss: 0.06159302, Validation loss: 0.06041415, Gradient norm: 5.29184367
INFO:root:[  100] Training loss: 0.06139358, Validation loss: 0.06657823, Gradient norm: 6.29738392
INFO:root:[  101] Training loss: 0.06059181, Validation loss: 0.06041192, Gradient norm: 4.88389567
INFO:root:[  102] Training loss: 0.06075623, Validation loss: 0.05973823, Gradient norm: 5.29208144
INFO:root:[  103] Training loss: 0.06000233, Validation loss: 0.06016745, Gradient norm: 6.06410200
INFO:root:[  104] Training loss: 0.06203945, Validation loss: 0.06050487, Gradient norm: 5.45268445
INFO:root:[  105] Training loss: 0.06296702, Validation loss: 0.05832384, Gradient norm: 6.57951920
INFO:root:[  106] Training loss: 0.06083043, Validation loss: 0.06997038, Gradient norm: 5.22434866
INFO:root:[  107] Training loss: 0.06197825, Validation loss: 0.07108322, Gradient norm: 5.50674734
INFO:root:[  108] Training loss: 0.05988384, Validation loss: 0.05791925, Gradient norm: 4.95646219
INFO:root:[  109] Training loss: 0.05921197, Validation loss: 0.06126928, Gradient norm: 5.12726170
INFO:root:[  110] Training loss: 0.06172155, Validation loss: 0.05747061, Gradient norm: 6.31493896
INFO:root:[  111] Training loss: 0.05832913, Validation loss: 0.05806921, Gradient norm: 4.73956150
INFO:root:[  112] Training loss: 0.06000535, Validation loss: 0.05988545, Gradient norm: 4.14960889
INFO:root:[  113] Training loss: 0.06066111, Validation loss: 0.05820859, Gradient norm: 4.68313919
INFO:root:[  114] Training loss: 0.05841696, Validation loss: 0.06203113, Gradient norm: 4.31003877
INFO:root:[  115] Training loss: 0.06027682, Validation loss: 0.05755704, Gradient norm: 5.36657401
INFO:root:[  116] Training loss: 0.05828165, Validation loss: 0.05765412, Gradient norm: 3.96062772
INFO:root:[  117] Training loss: 0.05923631, Validation loss: 0.07070190, Gradient norm: 4.93825804
INFO:root:[  118] Training loss: 0.06111303, Validation loss: 0.05592211, Gradient norm: 5.48077828
INFO:root:[  119] Training loss: 0.05840807, Validation loss: 0.06599896, Gradient norm: 4.85864019
INFO:root:[  120] Training loss: 0.06111947, Validation loss: 0.06011839, Gradient norm: 5.18876853
INFO:root:[  121] Training loss: 0.05927777, Validation loss: 0.05816294, Gradient norm: 4.67048351
INFO:root:[  122] Training loss: 0.05805542, Validation loss: 0.06314011, Gradient norm: 4.55876743
INFO:root:[  123] Training loss: 0.06366542, Validation loss: 0.05870493, Gradient norm: 5.92826286
INFO:root:[  124] Training loss: 0.06000190, Validation loss: 0.06274440, Gradient norm: 5.27455152
INFO:root:[  125] Training loss: 0.05839182, Validation loss: 0.05663320, Gradient norm: 4.42052663
INFO:root:[  126] Training loss: 0.06012780, Validation loss: 0.06040418, Gradient norm: 4.43506960
INFO:root:[  127] Training loss: 0.05660145, Validation loss: 0.05650550, Gradient norm: 3.06314932
INFO:root:[  128] Training loss: 0.05786169, Validation loss: 0.05575651, Gradient norm: 4.57960539
INFO:root:[  129] Training loss: 0.05919198, Validation loss: 0.05672459, Gradient norm: 5.37104553
INFO:root:[  130] Training loss: 0.05739451, Validation loss: 0.05590216, Gradient norm: 3.89647957
INFO:root:[  131] Training loss: 0.05811994, Validation loss: 0.05644838, Gradient norm: 4.87637484
INFO:root:[  132] Training loss: 0.05726246, Validation loss: 0.05990266, Gradient norm: 4.14085400
INFO:root:[  133] Training loss: 0.05731385, Validation loss: 0.05705296, Gradient norm: 4.41607487
INFO:root:[  134] Training loss: 0.05661056, Validation loss: 0.05693391, Gradient norm: 3.22569155
INFO:root:[  135] Training loss: 0.06070790, Validation loss: 0.05502641, Gradient norm: 5.38093232
INFO:root:[  136] Training loss: 0.05745702, Validation loss: 0.05910196, Gradient norm: 4.77820434
INFO:root:[  137] Training loss: 0.05642954, Validation loss: 0.05763257, Gradient norm: 4.26322313
INFO:root:[  138] Training loss: 0.05639358, Validation loss: 0.05540820, Gradient norm: 4.07438318
INFO:root:[  139] Training loss: 0.05853378, Validation loss: 0.06043575, Gradient norm: 4.15621636
INFO:root:[  140] Training loss: 0.05709275, Validation loss: 0.05843504, Gradient norm: 4.55939969
INFO:root:[  141] Training loss: 0.05900739, Validation loss: 0.06450330, Gradient norm: 4.71853240
INFO:root:[  142] Training loss: 0.05913704, Validation loss: 0.05847820, Gradient norm: 4.68251082
INFO:root:[  143] Training loss: 0.05754957, Validation loss: 0.05883145, Gradient norm: 4.29093457
INFO:root:[  144] Training loss: 0.05690237, Validation loss: 0.05671854, Gradient norm: 4.36797369
INFO:root:EP 144: Early stopping
INFO:root:Training the model took 2677.142s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03342
INFO:root:EnergyScoreTrain: -0.00469
INFO:root:CoverageTrain: 0.63617
INFO:root:IntervalWidthTrain: 0.02532
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03334
INFO:root:EnergyScoreValidation: -0.0051
INFO:root:CoverageValidation: 0.63243
INFO:root:IntervalWidthValidation: 0.02536
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03295
INFO:root:EnergyScoreTest: -0.00438
INFO:root:CoverageTest: 0.64991
INFO:root:IntervalWidthTest: 0.02452
INFO:root:###23 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.03364024, Validation loss: 0.60693933, Gradient norm: 5.53569749
INFO:root:[    2] Training loss: 0.53149031, Validation loss: 0.42872985, Gradient norm: 4.58660573
INFO:root:[    3] Training loss: 0.40224848, Validation loss: 0.35180462, Gradient norm: 3.84459511
INFO:root:[    4] Training loss: 0.33474249, Validation loss: 0.30912791, Gradient norm: 2.94333293
INFO:root:[    5] Training loss: 0.29268201, Validation loss: 0.27359443, Gradient norm: 1.83972258
INFO:root:[    6] Training loss: 0.26573927, Validation loss: 0.25575888, Gradient norm: 1.48005862
INFO:root:[    7] Training loss: 0.25021956, Validation loss: 0.23876535, Gradient norm: 1.73289215
INFO:root:[    8] Training loss: 0.23453495, Validation loss: 0.22904171, Gradient norm: 1.41333994
INFO:root:[    9] Training loss: 0.22575975, Validation loss: 0.21628721, Gradient norm: 2.02728293
INFO:root:[   10] Training loss: 0.21192814, Validation loss: 0.20385397, Gradient norm: 1.00080045
INFO:root:[   11] Training loss: 0.19924601, Validation loss: 0.19277842, Gradient norm: 1.25754394
INFO:root:[   12] Training loss: 0.18971775, Validation loss: 0.18490257, Gradient norm: 1.82977718
INFO:root:[   13] Training loss: 0.17974904, Validation loss: 0.17375945, Gradient norm: 1.60209660
INFO:root:[   14] Training loss: 0.17088459, Validation loss: 0.16524869, Gradient norm: 1.46729818
INFO:root:[   15] Training loss: 0.16123019, Validation loss: 0.15513471, Gradient norm: 1.95213081
INFO:root:[   16] Training loss: 0.15338567, Validation loss: 0.14739622, Gradient norm: 1.94677500
INFO:root:[   17] Training loss: 0.14534212, Validation loss: 0.14265661, Gradient norm: 1.95415402
INFO:root:[   18] Training loss: 0.13990569, Validation loss: 0.13052052, Gradient norm: 2.43566333
INFO:root:[   19] Training loss: 0.13128508, Validation loss: 0.12664449, Gradient norm: 2.47443705
INFO:root:[   20] Training loss: 0.12554695, Validation loss: 0.12010784, Gradient norm: 2.48666149
INFO:root:[   21] Training loss: 0.12322280, Validation loss: 0.12664015, Gradient norm: 2.21013661
INFO:root:[   22] Training loss: 0.11907545, Validation loss: 0.11159027, Gradient norm: 2.55040228
INFO:root:[   23] Training loss: 0.11103791, Validation loss: 0.11016803, Gradient norm: 2.18203543
INFO:root:[   24] Training loss: 0.10403214, Validation loss: 0.10437860, Gradient norm: 1.83640331
INFO:root:[   25] Training loss: 0.10318897, Validation loss: 0.09910261, Gradient norm: 2.46751283
INFO:root:[   26] Training loss: 0.09959703, Validation loss: 0.09573571, Gradient norm: 2.28348189
INFO:root:[   27] Training loss: 0.09285939, Validation loss: 0.09330701, Gradient norm: 1.95613953
INFO:root:[   28] Training loss: 0.09015012, Validation loss: 0.08631884, Gradient norm: 2.00794342
INFO:root:[   29] Training loss: 0.08733517, Validation loss: 0.09005473, Gradient norm: 2.37342953
INFO:root:[   30] Training loss: 0.08681152, Validation loss: 0.08532828, Gradient norm: 2.46344463
INFO:root:[   31] Training loss: 0.08459574, Validation loss: 0.08312860, Gradient norm: 2.50612012
INFO:root:[   32] Training loss: 0.08209095, Validation loss: 0.07871631, Gradient norm: 2.27816333
INFO:root:[   33] Training loss: 0.08133020, Validation loss: 0.08385051, Gradient norm: 2.04965564
INFO:root:[   34] Training loss: 0.08342365, Validation loss: 0.08075948, Gradient norm: 2.90029287
INFO:root:[   35] Training loss: 0.08137422, Validation loss: 0.08056456, Gradient norm: 2.67747991
INFO:root:[   36] Training loss: 0.08037496, Validation loss: 0.08467599, Gradient norm: 2.34307735
INFO:root:[   37] Training loss: 0.08047365, Validation loss: 0.07701930, Gradient norm: 2.52910795
INFO:root:[   38] Training loss: 0.07830964, Validation loss: 0.07453773, Gradient norm: 2.57022232
INFO:root:[   39] Training loss: 0.07794392, Validation loss: 0.07883397, Gradient norm: 2.36450235
INFO:root:[   40] Training loss: 0.07824401, Validation loss: 0.07861351, Gradient norm: 2.24159881
INFO:root:[   41] Training loss: 0.07905254, Validation loss: 0.07543381, Gradient norm: 2.54902620
INFO:root:[   42] Training loss: 0.07588559, Validation loss: 0.07606055, Gradient norm: 2.41353711
INFO:root:[   43] Training loss: 0.07707684, Validation loss: 0.07703513, Gradient norm: 2.77113558
INFO:root:[   44] Training loss: 0.07735676, Validation loss: 0.07505421, Gradient norm: 2.45040430
INFO:root:[   45] Training loss: 0.07740371, Validation loss: 0.07462824, Gradient norm: 2.68849725
INFO:root:[   46] Training loss: 0.07509510, Validation loss: 0.07444103, Gradient norm: 2.18767675
INFO:root:[   47] Training loss: 0.07572263, Validation loss: 0.07369231, Gradient norm: 2.45400095
INFO:root:[   48] Training loss: 0.07440978, Validation loss: 0.07351285, Gradient norm: 2.42668189
INFO:root:[   49] Training loss: 0.07357488, Validation loss: 0.07268493, Gradient norm: 2.78871052
INFO:root:[   50] Training loss: 0.07530329, Validation loss: 0.07474639, Gradient norm: 2.59001777
INFO:root:[   51] Training loss: 0.07357145, Validation loss: 0.07599754, Gradient norm: 1.99679117
INFO:root:[   52] Training loss: 0.07226023, Validation loss: 0.07256651, Gradient norm: 2.72666962
INFO:root:[   53] Training loss: 0.07454239, Validation loss: 0.07205640, Gradient norm: 2.90765592
INFO:root:[   54] Training loss: 0.07267359, Validation loss: 0.07096598, Gradient norm: 2.42827993
INFO:root:[   55] Training loss: 0.07240176, Validation loss: 0.07210551, Gradient norm: 2.54801892
INFO:root:[   56] Training loss: 0.07256770, Validation loss: 0.07095431, Gradient norm: 2.84621877
INFO:root:[   57] Training loss: 0.07036849, Validation loss: 0.06714684, Gradient norm: 2.58321012
INFO:root:[   58] Training loss: 0.06990278, Validation loss: 0.07637375, Gradient norm: 2.83689533
INFO:root:[   59] Training loss: 0.07341987, Validation loss: 0.06860587, Gradient norm: 2.75679029
INFO:root:[   60] Training loss: 0.07163480, Validation loss: 0.07451782, Gradient norm: 2.62474177
INFO:root:[   61] Training loss: 0.07043720, Validation loss: 0.06899576, Gradient norm: 2.79944331
INFO:root:[   62] Training loss: 0.07067740, Validation loss: 0.06638266, Gradient norm: 2.83608750
INFO:root:[   63] Training loss: 0.07137056, Validation loss: 0.07024324, Gradient norm: 2.62054737
INFO:root:[   64] Training loss: 0.06877215, Validation loss: 0.06824665, Gradient norm: 2.30670821
INFO:root:[   65] Training loss: 0.06918255, Validation loss: 0.06790049, Gradient norm: 2.87871392
INFO:root:[   66] Training loss: 0.06860198, Validation loss: 0.06866140, Gradient norm: 2.88624921
INFO:root:[   67] Training loss: 0.06777997, Validation loss: 0.06539731, Gradient norm: 2.79202631
INFO:root:[   68] Training loss: 0.06867102, Validation loss: 0.06727145, Gradient norm: 2.96365516
INFO:root:[   69] Training loss: 0.06878657, Validation loss: 0.06969422, Gradient norm: 2.79365667
INFO:root:[   70] Training loss: 0.06747664, Validation loss: 0.06925092, Gradient norm: 2.62844809
INFO:root:[   71] Training loss: 0.06803631, Validation loss: 0.06567387, Gradient norm: 2.41500999
INFO:root:[   72] Training loss: 0.06839546, Validation loss: 0.06604242, Gradient norm: 3.26571883
INFO:root:[   73] Training loss: 0.06764646, Validation loss: 0.06887654, Gradient norm: 2.72218698
INFO:root:[   74] Training loss: 0.06739286, Validation loss: 0.06612986, Gradient norm: 2.75137836
INFO:root:[   75] Training loss: 0.06775264, Validation loss: 0.07207906, Gradient norm: 2.71396804
INFO:root:[   76] Training loss: 0.06640631, Validation loss: 0.06388555, Gradient norm: 2.97618219
INFO:root:[   77] Training loss: 0.06723790, Validation loss: 0.06848721, Gradient norm: 2.98006955
INFO:root:[   78] Training loss: 0.06727095, Validation loss: 0.06552876, Gradient norm: 3.10469280
INFO:root:[   79] Training loss: 0.06705914, Validation loss: 0.06484526, Gradient norm: 2.81144158
INFO:root:[   80] Training loss: 0.06638302, Validation loss: 0.06562080, Gradient norm: 2.85363356
INFO:root:[   81] Training loss: 0.06628658, Validation loss: 0.06411169, Gradient norm: 2.98101538
INFO:root:[   82] Training loss: 0.06659998, Validation loss: 0.06462526, Gradient norm: 2.68982638
INFO:root:[   83] Training loss: 0.06594870, Validation loss: 0.06424500, Gradient norm: 2.89396119
INFO:root:[   84] Training loss: 0.06464297, Validation loss: 0.06426209, Gradient norm: 2.77654557
INFO:root:[   85] Training loss: 0.06479259, Validation loss: 0.06863977, Gradient norm: 2.90375967
INFO:root:[   86] Training loss: 0.06507701, Validation loss: 0.06341178, Gradient norm: 2.56042339
INFO:root:[   87] Training loss: 0.06506364, Validation loss: 0.06565604, Gradient norm: 3.05143347
INFO:root:[   88] Training loss: 0.06374317, Validation loss: 0.06676976, Gradient norm: 3.13465568
INFO:root:[   89] Training loss: 0.06455782, Validation loss: 0.06223896, Gradient norm: 2.82018623
INFO:root:[   90] Training loss: 0.06374589, Validation loss: 0.06398331, Gradient norm: 2.86164954
INFO:root:[   91] Training loss: 0.06565234, Validation loss: 0.06534886, Gradient norm: 2.95812861
INFO:root:[   92] Training loss: 0.06438267, Validation loss: 0.06919235, Gradient norm: 2.89667660
INFO:root:[   93] Training loss: 0.06571715, Validation loss: 0.07119586, Gradient norm: 2.50277003
INFO:root:[   94] Training loss: 0.06495828, Validation loss: 0.06190743, Gradient norm: 3.01483843
INFO:root:[   95] Training loss: 0.06495764, Validation loss: 0.06294514, Gradient norm: 2.69975092
INFO:root:[   96] Training loss: 0.06232635, Validation loss: 0.06611037, Gradient norm: 2.73298549
INFO:root:[   97] Training loss: 0.06462978, Validation loss: 0.06420801, Gradient norm: 2.86877525
INFO:root:[   98] Training loss: 0.06401933, Validation loss: 0.06157810, Gradient norm: 2.82035075
INFO:root:[   99] Training loss: 0.06279981, Validation loss: 0.06329652, Gradient norm: 2.75364644
INFO:root:[  100] Training loss: 0.06278240, Validation loss: 0.06354848, Gradient norm: 2.92772157
INFO:root:[  101] Training loss: 0.06424852, Validation loss: 0.06009666, Gradient norm: 3.08063503
INFO:root:[  102] Training loss: 0.06208184, Validation loss: 0.06339439, Gradient norm: 2.54908813
INFO:root:[  103] Training loss: 0.06333819, Validation loss: 0.07006039, Gradient norm: 2.76195693
INFO:root:[  104] Training loss: 0.06401274, Validation loss: 0.06726340, Gradient norm: 2.38339728
INFO:root:[  105] Training loss: 0.06357930, Validation loss: 0.06606010, Gradient norm: 3.07904961
INFO:root:[  106] Training loss: 0.06317702, Validation loss: 0.06278612, Gradient norm: 2.73402702
INFO:root:[  107] Training loss: 0.06355871, Validation loss: 0.06351543, Gradient norm: 3.39561153
INFO:root:[  108] Training loss: 0.06293309, Validation loss: 0.06259966, Gradient norm: 2.86170249
INFO:root:[  109] Training loss: 0.06321697, Validation loss: 0.06162487, Gradient norm: 2.68599602
INFO:root:[  110] Training loss: 0.06188823, Validation loss: 0.06095396, Gradient norm: 2.60654619
INFO:root:EP 110: Early stopping
INFO:root:Training the model took 2054.467s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03667
INFO:root:EnergyScoreTrain: -0.0027
INFO:root:CoverageTrain: 0.74891
INFO:root:IntervalWidthTrain: 0.03009
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03642
INFO:root:EnergyScoreValidation: -0.00309
INFO:root:CoverageValidation: 0.74598
INFO:root:IntervalWidthValidation: 0.03007
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0364
INFO:root:EnergyScoreTest: -0.00285
INFO:root:CoverageTest: 0.753
INFO:root:IntervalWidthTest: 0.02927
INFO:root:###24 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 870318080
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.14072380, Validation loss: 0.80301732, Gradient norm: 4.21641170
INFO:root:[    2] Training loss: 0.68070490, Validation loss: 0.55146007, Gradient norm: 3.34041224
INFO:root:[    3] Training loss: 0.50469251, Validation loss: 0.47179687, Gradient norm: 2.50451926
INFO:root:[    4] Training loss: 0.42520115, Validation loss: 0.39186154, Gradient norm: 2.71504046
INFO:root:[    5] Training loss: 0.38311028, Validation loss: 0.34715176, Gradient norm: 2.93261722
INFO:root:[    6] Training loss: 0.34200268, Validation loss: 0.32411076, Gradient norm: 2.07446196
INFO:root:[    7] Training loss: 0.31586765, Validation loss: 0.30329864, Gradient norm: 1.70330565
INFO:root:[    8] Training loss: 0.29401641, Validation loss: 0.28175509, Gradient norm: 1.38961841
INFO:root:[    9] Training loss: 0.27615458, Validation loss: 0.26322222, Gradient norm: 1.20820502
INFO:root:[   10] Training loss: 0.25991526, Validation loss: 0.25593080, Gradient norm: 1.49632254
INFO:root:[   11] Training loss: 0.24370911, Validation loss: 0.23352821, Gradient norm: 1.13204562
INFO:root:[   12] Training loss: 0.22881921, Validation loss: 0.21831023, Gradient norm: 1.07309274
INFO:root:[   13] Training loss: 0.21515014, Validation loss: 0.20752054, Gradient norm: 1.27115917
INFO:root:[   14] Training loss: 0.20104281, Validation loss: 0.19345185, Gradient norm: 1.26928216
INFO:root:[   15] Training loss: 0.18754169, Validation loss: 0.17798622, Gradient norm: 0.95589740
INFO:root:[   16] Training loss: 0.17500096, Validation loss: 0.16557073, Gradient norm: 1.13879850
INFO:root:[   17] Training loss: 0.16266103, Validation loss: 0.15398112, Gradient norm: 1.29661434
INFO:root:[   18] Training loss: 0.15107388, Validation loss: 0.14557694, Gradient norm: 1.76372527
INFO:root:[   19] Training loss: 0.14042090, Validation loss: 0.13397171, Gradient norm: 1.39019057
INFO:root:[   20] Training loss: 0.13059476, Validation loss: 0.12753449, Gradient norm: 1.65061273
INFO:root:[   21] Training loss: 0.12343228, Validation loss: 0.11646772, Gradient norm: 1.91305867
INFO:root:[   22] Training loss: 0.11575124, Validation loss: 0.11863432, Gradient norm: 2.15830368
INFO:root:[   23] Training loss: 0.11326592, Validation loss: 0.10854362, Gradient norm: 2.24487006
INFO:root:[   24] Training loss: 0.10529998, Validation loss: 0.10036586, Gradient norm: 2.05073684
INFO:root:[   25] Training loss: 0.10124512, Validation loss: 0.09622542, Gradient norm: 1.96096498
INFO:root:[   26] Training loss: 0.09811748, Validation loss: 0.09835350, Gradient norm: 2.50147846
INFO:root:[   27] Training loss: 0.09781278, Validation loss: 0.09705366, Gradient norm: 2.38086742
INFO:root:[   28] Training loss: 0.09597643, Validation loss: 0.09151035, Gradient norm: 2.32986186
INFO:root:[   29] Training loss: 0.09393078, Validation loss: 0.09209947, Gradient norm: 2.26337387
INFO:root:[   30] Training loss: 0.09372296, Validation loss: 0.09121404, Gradient norm: 2.42829489
INFO:root:[   31] Training loss: 0.09333436, Validation loss: 0.09443131, Gradient norm: 2.23370822
INFO:root:[   32] Training loss: 0.09012715, Validation loss: 0.09001477, Gradient norm: 2.53778245
INFO:root:[   33] Training loss: 0.09066350, Validation loss: 0.09172818, Gradient norm: 2.71554872
INFO:root:[   34] Training loss: 0.09060389, Validation loss: 0.09138458, Gradient norm: 2.45951094
INFO:root:[   35] Training loss: 0.08946442, Validation loss: 0.08879286, Gradient norm: 2.48178900
INFO:root:[   36] Training loss: 0.08953662, Validation loss: 0.08891577, Gradient norm: 3.10107982
INFO:root:[   37] Training loss: 0.08956829, Validation loss: 0.08703753, Gradient norm: 2.61290817
INFO:root:[   38] Training loss: 0.08705253, Validation loss: 0.08746205, Gradient norm: 2.62360913
INFO:root:[   39] Training loss: 0.08857204, Validation loss: 0.08766514, Gradient norm: 2.51482959
INFO:root:[   40] Training loss: 0.08984748, Validation loss: 0.09492959, Gradient norm: 3.05553292
INFO:root:[   41] Training loss: 0.08924029, Validation loss: 0.09314186, Gradient norm: 2.95793168
INFO:root:[   42] Training loss: 0.08777233, Validation loss: 0.08562307, Gradient norm: 2.37062082
INFO:root:[   43] Training loss: 0.08454605, Validation loss: 0.08378037, Gradient norm: 2.72131416
INFO:root:[   44] Training loss: 0.08672404, Validation loss: 0.08475855, Gradient norm: 2.89609759
INFO:root:[   45] Training loss: 0.08554999, Validation loss: 0.08450549, Gradient norm: 3.15615300
INFO:root:[   46] Training loss: 0.08358845, Validation loss: 0.08594247, Gradient norm: 3.03017223
INFO:root:[   47] Training loss: 0.08405560, Validation loss: 0.08759991, Gradient norm: 2.97817508
INFO:root:[   48] Training loss: 0.08674010, Validation loss: 0.08435951, Gradient norm: 3.65041165
INFO:root:[   49] Training loss: 0.08406380, Validation loss: 0.08284086, Gradient norm: 3.40017965
INFO:root:[   50] Training loss: 0.08543183, Validation loss: 0.09213503, Gradient norm: 3.03589282
INFO:root:[   51] Training loss: 0.08404783, Validation loss: 0.09315462, Gradient norm: 2.71802411
INFO:root:[   52] Training loss: 0.08618462, Validation loss: 0.08715149, Gradient norm: 2.49349247
INFO:root:[   53] Training loss: 0.08374016, Validation loss: 0.08217426, Gradient norm: 3.40301310
INFO:root:[   54] Training loss: 0.08184383, Validation loss: 0.08253527, Gradient norm: 3.45488553
INFO:root:[   55] Training loss: 0.08325958, Validation loss: 0.08224231, Gradient norm: 3.37844552
INFO:root:[   56] Training loss: 0.08157359, Validation loss: 0.08144013, Gradient norm: 3.20528608
INFO:root:[   57] Training loss: 0.08077362, Validation loss: 0.08049064, Gradient norm: 2.93421972
INFO:root:[   58] Training loss: 0.08264079, Validation loss: 0.08243961, Gradient norm: 3.40039228
INFO:root:[   59] Training loss: 0.08183152, Validation loss: 0.08187123, Gradient norm: 2.89505669
INFO:root:[   60] Training loss: 0.08109731, Validation loss: 0.08277041, Gradient norm: 3.15215930
INFO:root:[   61] Training loss: 0.08232512, Validation loss: 0.07827736, Gradient norm: 3.49155962
INFO:root:[   62] Training loss: 0.08017837, Validation loss: 0.07707605, Gradient norm: 3.06071615
INFO:root:[   63] Training loss: 0.08029084, Validation loss: 0.08261656, Gradient norm: 3.67589009
INFO:root:[   64] Training loss: 0.08183411, Validation loss: 0.08012601, Gradient norm: 3.64232907
INFO:root:[   65] Training loss: 0.07958454, Validation loss: 0.07963039, Gradient norm: 3.12092713
INFO:root:[   66] Training loss: 0.07918652, Validation loss: 0.07840386, Gradient norm: 3.44342395
INFO:root:[   67] Training loss: 0.07859806, Validation loss: 0.07660052, Gradient norm: 2.94737651
INFO:root:[   68] Training loss: 0.07965049, Validation loss: 0.07715498, Gradient norm: 3.63914069
INFO:root:[   69] Training loss: 0.07835352, Validation loss: 0.07488495, Gradient norm: 2.96522331
INFO:root:[   70] Training loss: 0.07924519, Validation loss: 0.07786010, Gradient norm: 4.05543980
INFO:root:[   71] Training loss: 0.07842231, Validation loss: 0.08010882, Gradient norm: 3.59920099
INFO:root:[   72] Training loss: 0.07776929, Validation loss: 0.07716644, Gradient norm: 3.70600147
INFO:root:[   73] Training loss: 0.07822963, Validation loss: 0.08134900, Gradient norm: 3.71365006
INFO:root:[   74] Training loss: 0.07831005, Validation loss: 0.08015671, Gradient norm: 3.18471776
INFO:root:[   75] Training loss: 0.07954609, Validation loss: 0.07768867, Gradient norm: 3.70447454
INFO:root:[   76] Training loss: 0.07838271, Validation loss: 0.07578260, Gradient norm: 3.70699593
INFO:root:[   77] Training loss: 0.07734638, Validation loss: 0.07607757, Gradient norm: 3.56026866
INFO:root:[   78] Training loss: 0.07781294, Validation loss: 0.07947410, Gradient norm: 2.43142590
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 1467.742s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04677
INFO:root:EnergyScoreTrain: 0.03085
INFO:root:CoverageTrain: 0.87546
INFO:root:IntervalWidthTrain: 0.03634
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04649
INFO:root:EnergyScoreValidation: 0.03063
INFO:root:CoverageValidation: 0.87404
INFO:root:IntervalWidthValidation: 0.03634
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04648
INFO:root:EnergyScoreTest: 0.03065
INFO:root:CoverageTest: 0.87411
INFO:root:IntervalWidthTest: 0.03533
