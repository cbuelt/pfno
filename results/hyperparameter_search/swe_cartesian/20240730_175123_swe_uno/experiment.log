INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/uno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10, 'ood': True}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 75497472
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.35550040, Validation loss: 0.18122407, Gradient norm: 6.56400222
INFO:root:[    2] Training loss: 0.12892987, Validation loss: 0.09809602, Gradient norm: 6.39875072
INFO:root:[    3] Training loss: 0.11217095, Validation loss: 0.08753942, Gradient norm: 5.85843939
INFO:root:[    4] Training loss: 0.10225496, Validation loss: 0.18810870, Gradient norm: 5.14377196
INFO:root:[    5] Training loss: 0.10885763, Validation loss: 0.08216640, Gradient norm: 6.38716389
INFO:root:[    6] Training loss: 0.12961771, Validation loss: 0.08143075, Gradient norm: 7.89111942
INFO:root:[    7] Training loss: 0.11124026, Validation loss: 0.08137068, Gradient norm: 7.32246084
INFO:root:[    8] Training loss: 0.08304938, Validation loss: 0.12948371, Gradient norm: 3.35612056
INFO:root:[    9] Training loss: 0.08717687, Validation loss: 0.17672786, Gradient norm: 5.04541436
INFO:root:[   10] Training loss: 0.11335165, Validation loss: 0.09421253, Gradient norm: 8.60345582
INFO:root:[   11] Training loss: 0.08961102, Validation loss: 0.08699139, Gradient norm: 6.45515475
INFO:root:[   12] Training loss: 0.08414004, Validation loss: 0.07279181, Gradient norm: 6.10730085
INFO:root:[   13] Training loss: 0.08015000, Validation loss: 0.06458405, Gradient norm: 5.22182574
INFO:root:[   14] Training loss: 0.07906616, Validation loss: 0.06814195, Gradient norm: 5.44689526
INFO:root:[   15] Training loss: 0.07668132, Validation loss: 0.06425925, Gradient norm: 5.64575181
INFO:root:[   16] Training loss: 0.07512300, Validation loss: 0.08505584, Gradient norm: 5.32138636
INFO:root:[   17] Training loss: 0.07191747, Validation loss: 0.06301990, Gradient norm: 4.79504689
INFO:root:[   18] Training loss: 0.06903343, Validation loss: 0.06425812, Gradient norm: 4.99599356
INFO:root:[   19] Training loss: 0.06507184, Validation loss: 0.05799522, Gradient norm: 4.26086904
INFO:root:[   20] Training loss: 0.06407021, Validation loss: 0.07907251, Gradient norm: 4.39304162
INFO:root:[   21] Training loss: 0.06332874, Validation loss: 0.06007831, Gradient norm: 4.34087276
INFO:root:[   22] Training loss: 0.06020353, Validation loss: 0.05806711, Gradient norm: 4.02846879
INFO:root:[   23] Training loss: 0.05952472, Validation loss: 0.05518227, Gradient norm: 2.47214425
INFO:root:[   24] Training loss: 0.06018806, Validation loss: 0.05957644, Gradient norm: 4.21104256
INFO:root:[   25] Training loss: 0.05465548, Validation loss: 0.06391145, Gradient norm: 3.26834343
INFO:root:[   26] Training loss: 0.05547427, Validation loss: 0.04822372, Gradient norm: 3.74596297
INFO:root:[   27] Training loss: 0.05298425, Validation loss: 0.04978493, Gradient norm: 3.33215996
INFO:root:[   28] Training loss: 0.05223962, Validation loss: 0.05892046, Gradient norm: 3.31540431
INFO:root:[   29] Training loss: 0.05099423, Validation loss: 0.04872042, Gradient norm: 3.36340717
INFO:root:[   30] Training loss: 0.04965983, Validation loss: 0.05488163, Gradient norm: 2.95872521
INFO:root:[   31] Training loss: 0.05005762, Validation loss: 0.04875430, Gradient norm: 3.04662163
INFO:root:[   32] Training loss: 0.04657085, Validation loss: 0.04154714, Gradient norm: 2.66327980
INFO:root:[   33] Training loss: 0.04766618, Validation loss: 0.04209837, Gradient norm: 2.69813262
INFO:root:[   34] Training loss: 0.04597686, Validation loss: 0.05066712, Gradient norm: 2.24936725
INFO:root:[   35] Training loss: 0.04532559, Validation loss: 0.04421489, Gradient norm: 2.63477197
INFO:root:[   36] Training loss: 0.04406561, Validation loss: 0.04441776, Gradient norm: 2.50389105
INFO:root:[   37] Training loss: 0.04371124, Validation loss: 0.04173236, Gradient norm: 2.59507450
INFO:root:[   38] Training loss: 0.04516971, Validation loss: 0.04864442, Gradient norm: 2.63181562
INFO:root:[   39] Training loss: 0.04404918, Validation loss: 0.04289658, Gradient norm: 2.47457387
INFO:root:[   40] Training loss: 0.04438474, Validation loss: 0.05097753, Gradient norm: 2.45611530
INFO:root:[   41] Training loss: 0.04225561, Validation loss: 0.03899732, Gradient norm: 2.11091777
INFO:root:[   42] Training loss: 0.04307347, Validation loss: 0.05836022, Gradient norm: 2.38138103
INFO:root:[   43] Training loss: 0.04395555, Validation loss: 0.04647406, Gradient norm: 2.57088789
INFO:root:[   44] Training loss: 0.04225814, Validation loss: 0.04823810, Gradient norm: 2.26109343
INFO:root:[   45] Training loss: 0.04247299, Validation loss: 0.03829476, Gradient norm: 2.38865845
INFO:root:[   46] Training loss: 0.04132850, Validation loss: 0.03660255, Gradient norm: 2.10897130
INFO:root:[   47] Training loss: 0.04013710, Validation loss: 0.03996879, Gradient norm: 1.99050430
INFO:root:[   48] Training loss: 0.04114060, Validation loss: 0.03593999, Gradient norm: 2.29649319
INFO:root:[   49] Training loss: 0.04035967, Validation loss: 0.04494608, Gradient norm: 2.04617560
INFO:root:[   50] Training loss: 0.03970713, Validation loss: 0.03900249, Gradient norm: 2.06700863
INFO:root:[   51] Training loss: 0.03998581, Validation loss: 0.03875240, Gradient norm: 2.12701087
INFO:root:[   52] Training loss: 0.04089269, Validation loss: 0.04589665, Gradient norm: 2.27763250
INFO:root:[   53] Training loss: 0.03934157, Validation loss: 0.03498785, Gradient norm: 2.17113755
INFO:root:[   54] Training loss: 0.03883254, Validation loss: 0.04465602, Gradient norm: 2.07216635
INFO:root:[   55] Training loss: 0.03885301, Validation loss: 0.04038763, Gradient norm: 2.05256329
INFO:root:[   56] Training loss: 0.03933677, Validation loss: 0.03607857, Gradient norm: 1.91077406
INFO:root:[   57] Training loss: 0.03785545, Validation loss: 0.04391677, Gradient norm: 1.78817252
INFO:root:[   58] Training loss: 0.03846199, Validation loss: 0.03814086, Gradient norm: 2.06820003
INFO:root:[   59] Training loss: 0.03729513, Validation loss: 0.03496532, Gradient norm: 1.79735958
INFO:root:[   60] Training loss: 0.03825684, Validation loss: 0.03699563, Gradient norm: 1.73965844
INFO:root:[   61] Training loss: 0.03800465, Validation loss: 0.03785684, Gradient norm: 1.97578414
INFO:root:[   62] Training loss: 0.03696950, Validation loss: 0.03636061, Gradient norm: 1.78350974
INFO:root:[   63] Training loss: 0.03705084, Validation loss: 0.03463680, Gradient norm: 1.65689215
INFO:root:[   64] Training loss: 0.03706131, Validation loss: 0.03324450, Gradient norm: 1.81162653
INFO:root:[   65] Training loss: 0.03709159, Validation loss: 0.03533085, Gradient norm: 1.86339711
INFO:root:[   66] Training loss: 0.03661426, Validation loss: 0.03910217, Gradient norm: 1.86159371
INFO:root:[   67] Training loss: 0.03620669, Validation loss: 0.03508222, Gradient norm: 1.60446217
INFO:root:[   68] Training loss: 0.03609207, Validation loss: 0.03742909, Gradient norm: 1.70915016
INFO:root:[   69] Training loss: 0.03558648, Validation loss: 0.03884939, Gradient norm: 1.76947118
INFO:root:[   70] Training loss: 0.03573869, Validation loss: 0.03881665, Gradient norm: 1.66819400
INFO:root:[   71] Training loss: 0.03521152, Validation loss: 0.03762701, Gradient norm: 1.73857776
INFO:root:[   72] Training loss: 0.03481136, Validation loss: 0.03314330, Gradient norm: 1.66101269
INFO:root:[   73] Training loss: 0.03564809, Validation loss: 0.03535486, Gradient norm: 1.75130198
INFO:root:[   74] Training loss: 0.03520390, Validation loss: 0.03482253, Gradient norm: 1.70572881
INFO:root:[   75] Training loss: 0.03439761, Validation loss: 0.03384187, Gradient norm: 1.62814673
INFO:root:[   76] Training loss: 0.03468761, Validation loss: 0.03618521, Gradient norm: 1.72148081
INFO:root:[   77] Training loss: 0.03483103, Validation loss: 0.03528581, Gradient norm: 1.73623521
INFO:root:[   78] Training loss: 0.03449415, Validation loss: 0.03239437, Gradient norm: 1.69950699
INFO:root:[   79] Training loss: 0.03430228, Validation loss: 0.03275592, Gradient norm: 1.62725038
INFO:root:[   80] Training loss: 0.03426144, Validation loss: 0.03591463, Gradient norm: 1.57062072
INFO:root:[   81] Training loss: 0.03519906, Validation loss: 0.03396426, Gradient norm: 1.80299362
INFO:root:[   82] Training loss: 0.03508084, Validation loss: 0.03066435, Gradient norm: 1.81437724
INFO:root:[   83] Training loss: 0.03371314, Validation loss: 0.03365072, Gradient norm: 1.66804655
INFO:root:[   84] Training loss: 0.03372419, Validation loss: 0.03515545, Gradient norm: 1.55129066
INFO:root:[   85] Training loss: 0.03459000, Validation loss: 0.03923184, Gradient norm: 1.84603336
INFO:root:[   86] Training loss: 0.03308075, Validation loss: 0.03154504, Gradient norm: 1.68684474
INFO:root:[   87] Training loss: 0.03416848, Validation loss: 0.03659128, Gradient norm: 1.58623157
INFO:root:[   88] Training loss: 0.03319996, Validation loss: 0.03334989, Gradient norm: 1.59382091
INFO:root:[   89] Training loss: 0.03402640, Validation loss: 0.03472810, Gradient norm: 1.74864344
INFO:root:[   90] Training loss: 0.03415290, Validation loss: 0.03351974, Gradient norm: 1.68076746
INFO:root:[   91] Training loss: 0.03272077, Validation loss: 0.03560730, Gradient norm: 1.64601410
INFO:root:EP 91: Early stopping
INFO:root:Training the model took 4784.896s.
INFO:root:Emptying the cuda cache took 0.093s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03924
INFO:root:EnergyScoreTrain: 0.03089
INFO:root:CoverageTrain: 0.98011
INFO:root:IntervalWidthTrain: 0.08424
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03882
INFO:root:EnergyScoreValidation: 0.03062
INFO:root:CoverageValidation: 0.98037
INFO:root:IntervalWidthValidation: 0.08377
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04874
INFO:root:EnergyScoreTest: 0.03574
INFO:root:CoverageTest: 0.96926
INFO:root:IntervalWidthTest: 0.08314
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1210056704
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.35518732, Validation loss: 0.15321556, Gradient norm: 5.97741352
INFO:root:[    2] Training loss: 0.16991058, Validation loss: 0.24330045, Gradient norm: 4.82703129
INFO:root:[    3] Training loss: 0.15164066, Validation loss: 0.16026424, Gradient norm: 4.70150707
INFO:root:[    4] Training loss: 0.12884254, Validation loss: 0.16908281, Gradient norm: 4.11358124
INFO:root:[    5] Training loss: 0.12028075, Validation loss: 0.10403122, Gradient norm: 4.05523032
INFO:root:[    6] Training loss: 0.10739175, Validation loss: 0.10446099, Gradient norm: 3.27673391
INFO:root:[    7] Training loss: 0.10372863, Validation loss: 0.15167183, Gradient norm: 3.81801726
INFO:root:[    8] Training loss: 0.09495258, Validation loss: 0.08703125, Gradient norm: 3.29595863
INFO:root:[    9] Training loss: 0.08936800, Validation loss: 0.08083824, Gradient norm: 3.09679810
INFO:root:[   10] Training loss: 0.08010405, Validation loss: 0.08677640, Gradient norm: 2.31560247
INFO:root:[   11] Training loss: 0.07771488, Validation loss: 0.06972730, Gradient norm: 2.20239978
INFO:root:[   12] Training loss: 0.07743803, Validation loss: 0.07780755, Gradient norm: 2.55340281
INFO:root:[   13] Training loss: 0.07248481, Validation loss: 0.06880669, Gradient norm: 2.06692433
INFO:root:[   14] Training loss: 0.06742483, Validation loss: 0.07582417, Gradient norm: 1.59989246
INFO:root:[   15] Training loss: 0.06678106, Validation loss: 0.06453178, Gradient norm: 1.65922488
INFO:root:[   16] Training loss: 0.06512274, Validation loss: 0.06070680, Gradient norm: 1.55704864
INFO:root:[   17] Training loss: 0.06426648, Validation loss: 0.05961074, Gradient norm: 1.59977140
INFO:root:[   18] Training loss: 0.06208536, Validation loss: 0.05782638, Gradient norm: 1.54064570
INFO:root:[   19] Training loss: 0.06259846, Validation loss: 0.06371339, Gradient norm: 1.49609637
INFO:root:[   20] Training loss: 0.06110488, Validation loss: 0.05744058, Gradient norm: 1.44828012
INFO:root:[   21] Training loss: 0.05956794, Validation loss: 0.06338314, Gradient norm: 1.10715327
INFO:root:[   22] Training loss: 0.05862337, Validation loss: 0.05571552, Gradient norm: 1.27306942
INFO:root:[   23] Training loss: 0.05708930, Validation loss: 0.05538912, Gradient norm: 1.19295140
INFO:root:[   24] Training loss: 0.05739494, Validation loss: 0.05567269, Gradient norm: 1.32355220
INFO:root:[   25] Training loss: 0.05698751, Validation loss: 0.05754380, Gradient norm: 1.33061963
INFO:root:[   26] Training loss: 0.05470112, Validation loss: 0.05196622, Gradient norm: 1.17531134
INFO:root:[   27] Training loss: 0.05538435, Validation loss: 0.05692349, Gradient norm: 1.20871421
INFO:root:[   28] Training loss: 0.05588739, Validation loss: 0.05777198, Gradient norm: 1.15936251
INFO:root:[   29] Training loss: 0.05489192, Validation loss: 0.05632741, Gradient norm: 1.10284499
INFO:root:[   30] Training loss: 0.05390681, Validation loss: 0.05208700, Gradient norm: 0.89188947
INFO:root:[   31] Training loss: 0.05286105, Validation loss: 0.05040150, Gradient norm: 1.07955387
INFO:root:[   32] Training loss: 0.05236118, Validation loss: 0.05230310, Gradient norm: 1.01551261
INFO:root:[   33] Training loss: 0.05221693, Validation loss: 0.05109285, Gradient norm: 0.90344765
INFO:root:[   34] Training loss: 0.05150021, Validation loss: 0.05418555, Gradient norm: 0.92014100
INFO:root:[   35] Training loss: 0.05179027, Validation loss: 0.05002764, Gradient norm: 1.00789644
INFO:root:[   36] Training loss: 0.05078844, Validation loss: 0.04876593, Gradient norm: 1.09801414
INFO:root:[   37] Training loss: 0.05010423, Validation loss: 0.05062710, Gradient norm: 1.04322571
INFO:root:[   38] Training loss: 0.05140894, Validation loss: 0.04969930, Gradient norm: 0.83083106
INFO:root:[   39] Training loss: 0.04974998, Validation loss: 0.04821411, Gradient norm: 1.03428230
INFO:root:[   40] Training loss: 0.04940745, Validation loss: 0.05049875, Gradient norm: 1.02366135
INFO:root:[   41] Training loss: 0.04882460, Validation loss: 0.04961284, Gradient norm: 0.97733409
INFO:root:[   42] Training loss: 0.04945716, Validation loss: 0.05130535, Gradient norm: 0.97141084
INFO:root:[   43] Training loss: 0.04871907, Validation loss: 0.04882162, Gradient norm: 1.11980983
INFO:root:[   44] Training loss: 0.04948700, Validation loss: 0.05038054, Gradient norm: 0.99831425
INFO:root:[   45] Training loss: 0.04851380, Validation loss: 0.04801956, Gradient norm: 0.95180184
INFO:root:[   46] Training loss: 0.04699090, Validation loss: 0.04667135, Gradient norm: 1.00232430
INFO:root:[   47] Training loss: 0.04737959, Validation loss: 0.04871523, Gradient norm: 0.86533951
INFO:root:[   48] Training loss: 0.04818153, Validation loss: 0.04797429, Gradient norm: 1.05445387
INFO:root:[   49] Training loss: 0.04674840, Validation loss: 0.04723369, Gradient norm: 0.77729665
INFO:root:[   50] Training loss: 0.04644013, Validation loss: 0.04439446, Gradient norm: 0.76248198
INFO:root:[   51] Training loss: 0.04570936, Validation loss: 0.04644770, Gradient norm: 0.86657379
INFO:root:[   52] Training loss: 0.04497252, Validation loss: 0.04947207, Gradient norm: 0.89861475
INFO:root:[   53] Training loss: 0.04555326, Validation loss: 0.04697922, Gradient norm: 0.98144584
INFO:root:[   54] Training loss: 0.04554140, Validation loss: 0.04611290, Gradient norm: 0.85512419
INFO:root:[   55] Training loss: 0.04518303, Validation loss: 0.04464468, Gradient norm: 0.85271587
INFO:root:[   56] Training loss: 0.04360231, Validation loss: 0.04564729, Gradient norm: 0.97548930
INFO:root:[   57] Training loss: 0.04345589, Validation loss: 0.04610064, Gradient norm: 0.94787935
INFO:root:[   58] Training loss: 0.04337678, Validation loss: 0.04258831, Gradient norm: 0.94958164
INFO:root:[   59] Training loss: 0.04281851, Validation loss: 0.04184511, Gradient norm: 0.77172960
INFO:root:[   60] Training loss: 0.04271376, Validation loss: 0.04249967, Gradient norm: 0.70534685
INFO:root:[   61] Training loss: 0.04239742, Validation loss: 0.04070121, Gradient norm: 0.90607752
INFO:root:[   62] Training loss: 0.04246706, Validation loss: 0.04182154, Gradient norm: 1.06314595
INFO:root:[   63] Training loss: 0.04311129, Validation loss: 0.04265486, Gradient norm: 1.02128326
INFO:root:[   64] Training loss: 0.04173914, Validation loss: 0.04095953, Gradient norm: 0.99685285
INFO:root:[   65] Training loss: 0.04137101, Validation loss: 0.04055354, Gradient norm: 0.96669711
INFO:root:[   66] Training loss: 0.04074282, Validation loss: 0.04048044, Gradient norm: 0.93456141
INFO:root:[   67] Training loss: 0.04105945, Validation loss: 0.04136708, Gradient norm: 1.00004034
INFO:root:[   68] Training loss: 0.04130928, Validation loss: 0.03939693, Gradient norm: 0.97853347
INFO:root:[   69] Training loss: 0.04014321, Validation loss: 0.04016800, Gradient norm: 0.83688553
INFO:root:[   70] Training loss: 0.04007822, Validation loss: 0.04189485, Gradient norm: 1.01513835
INFO:root:[   71] Training loss: 0.04043511, Validation loss: 0.03767835, Gradient norm: 1.02378303
INFO:root:[   72] Training loss: 0.03939440, Validation loss: 0.03953979, Gradient norm: 1.10831676
INFO:root:[   73] Training loss: 0.03926312, Validation loss: 0.03767370, Gradient norm: 1.06175063
INFO:root:[   74] Training loss: 0.03941307, Validation loss: 0.03874602, Gradient norm: 1.06977182
INFO:root:[   75] Training loss: 0.03864754, Validation loss: 0.03881540, Gradient norm: 1.06016411
INFO:root:[   76] Training loss: 0.03873292, Validation loss: 0.03822642, Gradient norm: 1.10558876
INFO:root:[   77] Training loss: 0.03895615, Validation loss: 0.03948927, Gradient norm: 1.22507969
INFO:root:[   78] Training loss: 0.03888717, Validation loss: 0.03863635, Gradient norm: 1.01385626
INFO:root:[   79] Training loss: 0.03792640, Validation loss: 0.03881169, Gradient norm: 1.13167940
INFO:root:[   80] Training loss: 0.03807918, Validation loss: 0.03852968, Gradient norm: 1.18833801
INFO:root:[   81] Training loss: 0.03727772, Validation loss: 0.03909317, Gradient norm: 1.14885307
INFO:root:[   82] Training loss: 0.03730560, Validation loss: 0.03796091, Gradient norm: 1.27553795
INFO:root:[   83] Training loss: 0.03656492, Validation loss: 0.03614855, Gradient norm: 1.29925361
INFO:root:[   84] Training loss: 0.03688241, Validation loss: 0.03776836, Gradient norm: 1.15930242
INFO:root:[   85] Training loss: 0.03715563, Validation loss: 0.03917686, Gradient norm: 1.35538834
INFO:root:[   86] Training loss: 0.03734489, Validation loss: 0.03567294, Gradient norm: 1.46655721
INFO:root:[   87] Training loss: 0.03623345, Validation loss: 0.03540131, Gradient norm: 1.43678399
INFO:root:[   88] Training loss: 0.03591064, Validation loss: 0.03676694, Gradient norm: 1.38866974
INFO:root:[   89] Training loss: 0.03596021, Validation loss: 0.03440785, Gradient norm: 1.45008068
INFO:root:[   90] Training loss: 0.03563500, Validation loss: 0.03518344, Gradient norm: 1.32285620
INFO:root:[   91] Training loss: 0.03558730, Validation loss: 0.03535681, Gradient norm: 1.43533513
INFO:root:[   92] Training loss: 0.03540785, Validation loss: 0.03390352, Gradient norm: 1.30903770
INFO:root:[   93] Training loss: 0.03550756, Validation loss: 0.03461096, Gradient norm: 1.48217590
INFO:root:[   94] Training loss: 0.03502678, Validation loss: 0.03385945, Gradient norm: 1.30917507
INFO:root:[   95] Training loss: 0.03465659, Validation loss: 0.03381451, Gradient norm: 1.57505900
INFO:root:[   96] Training loss: 0.03434301, Validation loss: 0.03282934, Gradient norm: 1.39581336
INFO:root:[   97] Training loss: 0.03463706, Validation loss: 0.03492029, Gradient norm: 1.28680783
INFO:root:[   98] Training loss: 0.03413117, Validation loss: 0.03273774, Gradient norm: 1.47731390
INFO:root:[   99] Training loss: 0.03386229, Validation loss: 0.03255490, Gradient norm: 1.37544622
INFO:root:[  100] Training loss: 0.03362174, Validation loss: 0.03183346, Gradient norm: 1.62635704
INFO:root:[  101] Training loss: 0.03351305, Validation loss: 0.03175330, Gradient norm: 1.62884383
INFO:root:[  102] Training loss: 0.03316916, Validation loss: 0.03307353, Gradient norm: 1.41582288
INFO:root:[  103] Training loss: 0.03349372, Validation loss: 0.03263903, Gradient norm: 1.62360886
INFO:root:[  104] Training loss: 0.03299770, Validation loss: 0.03338503, Gradient norm: 1.62642982
INFO:root:[  105] Training loss: 0.03257970, Validation loss: 0.02986601, Gradient norm: 1.68031832
INFO:root:[  106] Training loss: 0.03237104, Validation loss: 0.03163590, Gradient norm: 1.66615937
INFO:root:[  107] Training loss: 0.03243261, Validation loss: 0.03350552, Gradient norm: 1.58214368
INFO:root:[  108] Training loss: 0.03249579, Validation loss: 0.03057769, Gradient norm: 1.76530595
INFO:root:[  109] Training loss: 0.03228281, Validation loss: 0.03238842, Gradient norm: 1.65733481
INFO:root:[  110] Training loss: 0.03201912, Validation loss: 0.03262327, Gradient norm: 1.79957137
INFO:root:[  111] Training loss: 0.03222383, Validation loss: 0.03185561, Gradient norm: 1.80192811
INFO:root:[  112] Training loss: 0.03193812, Validation loss: 0.03358092, Gradient norm: 1.75490398
INFO:root:[  113] Training loss: 0.03165033, Validation loss: 0.03247619, Gradient norm: 1.74412074
INFO:root:[  114] Training loss: 0.03095417, Validation loss: 0.03284906, Gradient norm: 1.82353035
INFO:root:EP 114: Early stopping
INFO:root:Training the model took 5704.561s.
INFO:root:Emptying the cuda cache took 0.092s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03338
INFO:root:EnergyScoreTrain: 0.03008
INFO:root:CoverageTrain: 0.99442
INFO:root:IntervalWidthTrain: 0.09157
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03305
INFO:root:EnergyScoreValidation: 0.02985
INFO:root:CoverageValidation: 0.99449
INFO:root:IntervalWidthValidation: 0.09108
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04043
INFO:root:EnergyScoreTest: 0.03287
INFO:root:CoverageTest: 0.99188
INFO:root:IntervalWidthTest: 0.09115
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1377828864
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.37846816, Validation loss: 0.21321983, Gradient norm: 4.73210470
INFO:root:[    2] Training loss: 0.18880853, Validation loss: 0.17470707, Gradient norm: 4.27295373
INFO:root:[    3] Training loss: 0.15073457, Validation loss: 0.12370160, Gradient norm: 3.00016592
INFO:root:[    4] Training loss: 0.13701636, Validation loss: 0.11159560, Gradient norm: 2.98265711
INFO:root:[    5] Training loss: 0.11235464, Validation loss: 0.10179725, Gradient norm: 1.61586498
INFO:root:[    6] Training loss: 0.10737123, Validation loss: 0.09231182, Gradient norm: 2.39554641
INFO:root:[    7] Training loss: 0.09701919, Validation loss: 0.08617363, Gradient norm: 1.94824758
INFO:root:[    8] Training loss: 0.08992069, Validation loss: 0.08534557, Gradient norm: 1.70833141
INFO:root:[    9] Training loss: 0.08601674, Validation loss: 0.07971974, Gradient norm: 1.69464138
INFO:root:[   10] Training loss: 0.08074393, Validation loss: 0.08549907, Gradient norm: 1.35961831
INFO:root:[   11] Training loss: 0.07923949, Validation loss: 0.07251813, Gradient norm: 1.50430636
INFO:root:[   12] Training loss: 0.07717316, Validation loss: 0.07538981, Gradient norm: 1.33803081
INFO:root:[   13] Training loss: 0.07440544, Validation loss: 0.07151571, Gradient norm: 1.27829771
INFO:root:[   14] Training loss: 0.07230237, Validation loss: 0.06877784, Gradient norm: 1.11187748
INFO:root:[   15] Training loss: 0.07141451, Validation loss: 0.07212787, Gradient norm: 1.13057725
INFO:root:[   16] Training loss: 0.06839048, Validation loss: 0.06658917, Gradient norm: 0.88255940
INFO:root:[   17] Training loss: 0.06855303, Validation loss: 0.06670026, Gradient norm: 0.96005812
INFO:root:[   18] Training loss: 0.06702136, Validation loss: 0.06606836, Gradient norm: 0.84522800
INFO:root:[   19] Training loss: 0.06726241, Validation loss: 0.06881591, Gradient norm: 1.02964357
INFO:root:[   20] Training loss: 0.06544423, Validation loss: 0.06450531, Gradient norm: 0.52094214
INFO:root:[   21] Training loss: 0.06424147, Validation loss: 0.06531903, Gradient norm: 0.82651805
INFO:root:[   22] Training loss: 0.06446678, Validation loss: 0.06050118, Gradient norm: 0.91563061
INFO:root:[   23] Training loss: 0.06326042, Validation loss: 0.06841071, Gradient norm: 0.93792624
INFO:root:[   24] Training loss: 0.06284079, Validation loss: 0.06247506, Gradient norm: 0.87785953
INFO:root:[   25] Training loss: 0.06159460, Validation loss: 0.06089816, Gradient norm: 0.67454065
INFO:root:[   26] Training loss: 0.06033889, Validation loss: 0.05808329, Gradient norm: 0.34329164
INFO:root:[   27] Training loss: 0.05972155, Validation loss: 0.06137174, Gradient norm: 0.76145041
INFO:root:[   28] Training loss: 0.06030703, Validation loss: 0.05977980, Gradient norm: 0.99220896
INFO:root:[   29] Training loss: 0.05929431, Validation loss: 0.05856234, Gradient norm: 0.94449928
INFO:root:[   30] Training loss: 0.05813378, Validation loss: 0.05515927, Gradient norm: 0.91666309
INFO:root:[   31] Training loss: 0.05790550, Validation loss: 0.05944198, Gradient norm: 0.82388678
INFO:root:[   32] Training loss: 0.05852603, Validation loss: 0.05876839, Gradient norm: 0.90824431
INFO:root:[   33] Training loss: 0.05759940, Validation loss: 0.05685857, Gradient norm: 0.85531587
INFO:root:[   34] Training loss: 0.05665319, Validation loss: 0.06211314, Gradient norm: 0.76493951
INFO:root:[   35] Training loss: 0.05692430, Validation loss: 0.05682712, Gradient norm: 0.95516840
INFO:root:[   36] Training loss: 0.05574897, Validation loss: 0.05276324, Gradient norm: 0.98316956
INFO:root:[   37] Training loss: 0.05449381, Validation loss: 0.05254800, Gradient norm: 0.91340202
INFO:root:[   38] Training loss: 0.05449211, Validation loss: 0.05367993, Gradient norm: 0.97401862
INFO:root:[   39] Training loss: 0.05404723, Validation loss: 0.05484842, Gradient norm: 0.91135018
INFO:root:[   40] Training loss: 0.05307980, Validation loss: 0.05274932, Gradient norm: 0.91735820
INFO:root:[   41] Training loss: 0.05220547, Validation loss: 0.05340960, Gradient norm: 1.05833572
INFO:root:[   42] Training loss: 0.05219098, Validation loss: 0.05135932, Gradient norm: 0.91722040
INFO:root:[   43] Training loss: 0.05121906, Validation loss: 0.05193215, Gradient norm: 0.85851970
INFO:root:[   44] Training loss: 0.05120414, Validation loss: 0.04985953, Gradient norm: 0.95805346
INFO:root:[   45] Training loss: 0.05075040, Validation loss: 0.05008605, Gradient norm: 0.88637902
INFO:root:[   46] Training loss: 0.04929507, Validation loss: 0.04790994, Gradient norm: 0.93888869
INFO:root:[   47] Training loss: 0.04858419, Validation loss: 0.04976297, Gradient norm: 1.02110130
INFO:root:[   48] Training loss: 0.04839327, Validation loss: 0.04871535, Gradient norm: 0.95903153
INFO:root:[   49] Training loss: 0.04837568, Validation loss: 0.04586079, Gradient norm: 0.94885589
INFO:root:[   50] Training loss: 0.04776666, Validation loss: 0.04703057, Gradient norm: 0.85142822
INFO:root:[   51] Training loss: 0.04700631, Validation loss: 0.04541763, Gradient norm: 1.08351436
INFO:root:[   52] Training loss: 0.04664567, Validation loss: 0.04614033, Gradient norm: 0.97740373
INFO:root:[   53] Training loss: 0.04699818, Validation loss: 0.04879184, Gradient norm: 1.05284341
INFO:root:[   54] Training loss: 0.04578541, Validation loss: 0.04356931, Gradient norm: 1.02721770
INFO:root:[   55] Training loss: 0.04553950, Validation loss: 0.04463252, Gradient norm: 1.12959655
INFO:root:[   56] Training loss: 0.04495776, Validation loss: 0.04331514, Gradient norm: 1.08658927
INFO:root:[   57] Training loss: 0.04506351, Validation loss: 0.04364502, Gradient norm: 1.10166932
INFO:root:[   58] Training loss: 0.04535650, Validation loss: 0.04397767, Gradient norm: 1.10365208
INFO:root:[   59] Training loss: 0.04314953, Validation loss: 0.04218351, Gradient norm: 1.10302326
INFO:root:[   60] Training loss: 0.04328136, Validation loss: 0.04275171, Gradient norm: 1.12406144
INFO:root:[   61] Training loss: 0.04352371, Validation loss: 0.04186946, Gradient norm: 1.08183024
INFO:root:[   62] Training loss: 0.04246913, Validation loss: 0.04287626, Gradient norm: 1.08339553
INFO:root:[   63] Training loss: 0.04214290, Validation loss: 0.04310186, Gradient norm: 1.01147461
INFO:root:[   64] Training loss: 0.04165336, Validation loss: 0.04030720, Gradient norm: 1.16711545
INFO:root:[   65] Training loss: 0.04108128, Validation loss: 0.04213600, Gradient norm: 1.10028821
INFO:root:[   66] Training loss: 0.04111674, Validation loss: 0.04169825, Gradient norm: 1.02281532
INFO:root:[   67] Training loss: 0.04111960, Validation loss: 0.04155704, Gradient norm: 1.17070743
INFO:root:[   68] Training loss: 0.03979393, Validation loss: 0.03789983, Gradient norm: 1.13890519
INFO:root:[   69] Training loss: 0.04055775, Validation loss: 0.03988207, Gradient norm: 1.23385083
INFO:root:[   70] Training loss: 0.03966317, Validation loss: 0.03814531, Gradient norm: 1.02790835
INFO:root:[   71] Training loss: 0.03922990, Validation loss: 0.03836515, Gradient norm: 1.30077488
INFO:root:[   72] Training loss: 0.03914982, Validation loss: 0.03863188, Gradient norm: 1.13201414
INFO:root:[   73] Training loss: 0.03860808, Validation loss: 0.03830936, Gradient norm: 1.12410526
INFO:root:[   74] Training loss: 0.03784944, Validation loss: 0.03761765, Gradient norm: 1.17803933
INFO:root:[   75] Training loss: 0.03765262, Validation loss: 0.03765592, Gradient norm: 1.18207554
INFO:root:[   76] Training loss: 0.03715135, Validation loss: 0.03768346, Gradient norm: 1.16141730
INFO:root:[   77] Training loss: 0.03676575, Validation loss: 0.03665872, Gradient norm: 1.15446354
INFO:root:[   78] Training loss: 0.03702174, Validation loss: 0.03513699, Gradient norm: 1.21103369
INFO:root:[   79] Training loss: 0.03570984, Validation loss: 0.03542727, Gradient norm: 1.16637922
INFO:root:[   80] Training loss: 0.03606760, Validation loss: 0.03638049, Gradient norm: 1.18646949
INFO:root:[   81] Training loss: 0.03587240, Validation loss: 0.03605623, Gradient norm: 1.30376361
INFO:root:[   82] Training loss: 0.03514006, Validation loss: 0.03498673, Gradient norm: 1.32664777
INFO:root:[   83] Training loss: 0.03496717, Validation loss: 0.03502213, Gradient norm: 1.26433874
INFO:root:[   84] Training loss: 0.03609434, Validation loss: 0.03582654, Gradient norm: 0.96100446
INFO:root:[   85] Training loss: 0.03479096, Validation loss: 0.03450958, Gradient norm: 1.32050353
INFO:root:[   86] Training loss: 0.03497566, Validation loss: 0.03519202, Gradient norm: 1.14467836
INFO:root:[   87] Training loss: 0.03489285, Validation loss: 0.03200254, Gradient norm: 1.11289277
INFO:root:[   88] Training loss: 0.03386213, Validation loss: 0.03478861, Gradient norm: 1.31821191
INFO:root:[   89] Training loss: 0.03331558, Validation loss: 0.03191425, Gradient norm: 1.42968849
INFO:root:[   90] Training loss: 0.03362896, Validation loss: 0.03391002, Gradient norm: 1.21926332
INFO:root:[   91] Training loss: 0.03393856, Validation loss: 0.03142972, Gradient norm: 1.14816714
INFO:root:[   92] Training loss: 0.03294695, Validation loss: 0.03327679, Gradient norm: 1.36432658
INFO:root:[   93] Training loss: 0.03344979, Validation loss: 0.03054603, Gradient norm: 1.26012645
INFO:root:[   94] Training loss: 0.03189498, Validation loss: 0.03281325, Gradient norm: 1.23320270
INFO:root:[   95] Training loss: 0.03250809, Validation loss: 0.03081920, Gradient norm: 1.40301853
INFO:root:[   96] Training loss: 0.03184029, Validation loss: 0.03574529, Gradient norm: 1.24926741
INFO:root:[   97] Training loss: 0.03254829, Validation loss: 0.03187922, Gradient norm: 1.20393452
INFO:root:[   98] Training loss: 0.03095922, Validation loss: 0.02909425, Gradient norm: 1.31832357
INFO:root:[   99] Training loss: 0.03132593, Validation loss: 0.03541826, Gradient norm: 1.37076479
INFO:root:[  100] Training loss: 0.03180851, Validation loss: 0.02975747, Gradient norm: 1.39787608
INFO:root:[  101] Training loss: 0.03062718, Validation loss: 0.03031273, Gradient norm: 1.31214074
INFO:root:[  102] Training loss: 0.03069954, Validation loss: 0.02992322, Gradient norm: 1.30190575
INFO:root:[  103] Training loss: 0.03056103, Validation loss: 0.03194589, Gradient norm: 1.26309523
INFO:root:[  104] Training loss: 0.03020892, Validation loss: 0.03034794, Gradient norm: 1.36427581
INFO:root:[  105] Training loss: 0.03029033, Validation loss: 0.02758182, Gradient norm: 1.25140043
INFO:root:[  106] Training loss: 0.03021610, Validation loss: 0.02858624, Gradient norm: 1.30821548
INFO:root:[  107] Training loss: 0.02966041, Validation loss: 0.03073215, Gradient norm: 1.25544328
INFO:root:[  108] Training loss: 0.02917506, Validation loss: 0.02916217, Gradient norm: 1.35895826
INFO:root:[  109] Training loss: 0.02818784, Validation loss: 0.02793469, Gradient norm: 1.29804046
INFO:root:[  110] Training loss: 0.03071701, Validation loss: 0.02981157, Gradient norm: 0.99355990
INFO:root:[  111] Training loss: 0.02868105, Validation loss: 0.02697242, Gradient norm: 1.38718623
INFO:root:[  112] Training loss: 0.02861481, Validation loss: 0.02836274, Gradient norm: 1.52897122
INFO:root:[  113] Training loss: 0.02906004, Validation loss: 0.02898285, Gradient norm: 1.44320395
INFO:root:[  114] Training loss: 0.02774545, Validation loss: 0.02852293, Gradient norm: 1.30705102
INFO:root:[  115] Training loss: 0.02805233, Validation loss: 0.02778538, Gradient norm: 1.53075456
INFO:root:[  116] Training loss: 0.02740366, Validation loss: 0.02535881, Gradient norm: 1.45925811
INFO:root:[  117] Training loss: 0.02767738, Validation loss: 0.02723419, Gradient norm: 1.34488630
INFO:root:[  118] Training loss: 0.02786224, Validation loss: 0.02724295, Gradient norm: 1.32495037
INFO:root:[  119] Training loss: 0.02799839, Validation loss: 0.02569454, Gradient norm: 1.45446210
INFO:root:[  120] Training loss: 0.02717673, Validation loss: 0.02561074, Gradient norm: 1.37633600
INFO:root:[  121] Training loss: 0.02695094, Validation loss: 0.02669565, Gradient norm: 1.30558269
INFO:root:[  122] Training loss: 0.02686356, Validation loss: 0.03093835, Gradient norm: 1.39066781
INFO:root:[  123] Training loss: 0.02708897, Validation loss: 0.02449594, Gradient norm: 1.42713021
INFO:root:[  124] Training loss: 0.02711639, Validation loss: 0.02638474, Gradient norm: 1.50038860
INFO:root:[  125] Training loss: 0.02621725, Validation loss: 0.02594106, Gradient norm: 1.37046422
INFO:root:[  126] Training loss: 0.02654073, Validation loss: 0.02602691, Gradient norm: 1.35301146
INFO:root:[  127] Training loss: 0.02684015, Validation loss: 0.02673678, Gradient norm: 1.35803149
INFO:root:[  128] Training loss: 0.02583048, Validation loss: 0.02823573, Gradient norm: 1.43665117
INFO:root:[  129] Training loss: 0.02576205, Validation loss: 0.02786254, Gradient norm: 1.50497630
INFO:root:[  130] Training loss: 0.02691425, Validation loss: 0.02467784, Gradient norm: 1.71489058
INFO:root:[  131] Training loss: 0.02535670, Validation loss: 0.02440840, Gradient norm: 1.44261725
INFO:root:[  132] Training loss: 0.02535676, Validation loss: 0.02309640, Gradient norm: 1.36345719
INFO:root:[  133] Training loss: 0.02495841, Validation loss: 0.02781004, Gradient norm: 1.26997481
INFO:root:[  134] Training loss: 0.02634498, Validation loss: 0.02745331, Gradient norm: 1.56480933
INFO:root:[  135] Training loss: 0.02487528, Validation loss: 0.02506433, Gradient norm: 1.51291992
INFO:root:[  136] Training loss: 0.02675201, Validation loss: 0.02698709, Gradient norm: 1.31705908
INFO:root:[  137] Training loss: 0.02524107, Validation loss: 0.02448559, Gradient norm: 1.36116143
INFO:root:[  138] Training loss: 0.02508112, Validation loss: 0.02530780, Gradient norm: 1.39832820
INFO:root:[  139] Training loss: 0.02465656, Validation loss: 0.02221719, Gradient norm: 1.53316115
INFO:root:[  140] Training loss: 0.02440713, Validation loss: 0.02493321, Gradient norm: 1.55825697
INFO:root:[  141] Training loss: 0.02434683, Validation loss: 0.02229124, Gradient norm: 1.42530515
INFO:root:[  142] Training loss: 0.02416076, Validation loss: 0.02467548, Gradient norm: 1.48219907
INFO:root:[  143] Training loss: 0.02486585, Validation loss: 0.02202315, Gradient norm: 1.49363302
INFO:root:[  144] Training loss: 0.02425472, Validation loss: 0.02456748, Gradient norm: 1.56633127
INFO:root:[  145] Training loss: 0.02413537, Validation loss: 0.02178051, Gradient norm: 1.47820310
INFO:root:[  146] Training loss: 0.02341730, Validation loss: 0.02393838, Gradient norm: 1.39255256
INFO:root:[  147] Training loss: 0.02496710, Validation loss: 0.02425544, Gradient norm: 1.48286940
INFO:root:[  148] Training loss: 0.02420963, Validation loss: 0.02569430, Gradient norm: 1.54950401
INFO:root:[  149] Training loss: 0.02443751, Validation loss: 0.02382022, Gradient norm: 1.68621511
INFO:root:[  150] Training loss: 0.02425268, Validation loss: 0.02446191, Gradient norm: 1.62253518
INFO:root:[  151] Training loss: 0.02401162, Validation loss: 0.02413574, Gradient norm: 1.49275500
INFO:root:[  152] Training loss: 0.02437381, Validation loss: 0.02302436, Gradient norm: 1.71552463
INFO:root:[  153] Training loss: 0.02326557, Validation loss: 0.02402075, Gradient norm: 1.45203567
INFO:root:[  154] Training loss: 0.02508622, Validation loss: 0.02617201, Gradient norm: 1.52941462
INFO:root:EP 154: Early stopping
INFO:root:Training the model took 7685.981s.
INFO:root:Emptying the cuda cache took 0.095s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02742
INFO:root:EnergyScoreTrain: 0.02162
INFO:root:CoverageTrain: 0.99572
INFO:root:IntervalWidthTrain: 0.04409
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02714
INFO:root:EnergyScoreValidation: 0.02136
INFO:root:CoverageValidation: 0.99576
INFO:root:IntervalWidthValidation: 0.04352
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03244
INFO:root:EnergyScoreTest: 0.02392
INFO:root:CoverageTest: 0.99206
INFO:root:IntervalWidthTest: 0.04325
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.45290465, Validation loss: 0.22527344, Gradient norm: 5.67021857
INFO:root:[    2] Training loss: 0.22791358, Validation loss: 0.24214433, Gradient norm: 3.74998140
INFO:root:[    3] Training loss: 0.19701331, Validation loss: 0.18454926, Gradient norm: 3.33514218
INFO:root:[    4] Training loss: 0.17860889, Validation loss: 0.14360908, Gradient norm: 3.30997262
INFO:root:[    5] Training loss: 0.15318017, Validation loss: 0.15347063, Gradient norm: 2.54910323
INFO:root:[    6] Training loss: 0.13739918, Validation loss: 0.12442333, Gradient norm: 2.19923026
INFO:root:[    7] Training loss: 0.12771907, Validation loss: 0.11897088, Gradient norm: 2.20034484
INFO:root:[    8] Training loss: 0.11959946, Validation loss: 0.11444029, Gradient norm: 2.04143180
INFO:root:[    9] Training loss: 0.11527843, Validation loss: 0.10465219, Gradient norm: 2.08644742
INFO:root:[   10] Training loss: 0.10228126, Validation loss: 0.10070006, Gradient norm: 1.05373335
INFO:root:[   11] Training loss: 0.10139971, Validation loss: 0.09444542, Gradient norm: 1.31100329
INFO:root:[   12] Training loss: 0.09656084, Validation loss: 0.09371163, Gradient norm: 1.09338072
INFO:root:[   13] Training loss: 0.09450521, Validation loss: 0.09538741, Gradient norm: 1.16737165
INFO:root:[   14] Training loss: 0.09290297, Validation loss: 0.09018928, Gradient norm: 1.23953694
INFO:root:[   15] Training loss: 0.08901813, Validation loss: 0.08501633, Gradient norm: 1.03037276
INFO:root:[   16] Training loss: 0.08601854, Validation loss: 0.08227706, Gradient norm: 0.52244159
INFO:root:[   17] Training loss: 0.08472069, Validation loss: 0.08333547, Gradient norm: 0.76582207
INFO:root:[   18] Training loss: 0.08315553, Validation loss: 0.07950991, Gradient norm: 0.82731909
INFO:root:[   19] Training loss: 0.08219766, Validation loss: 0.08196466, Gradient norm: 0.86530039
INFO:root:[   20] Training loss: 0.08037409, Validation loss: 0.08089192, Gradient norm: 0.70926292
INFO:root:[   21] Training loss: 0.07888531, Validation loss: 0.07667022, Gradient norm: 0.84114239
INFO:root:[   22] Training loss: 0.07758481, Validation loss: 0.07437599, Gradient norm: 0.64435381
INFO:root:[   23] Training loss: 0.07468208, Validation loss: 0.07303798, Gradient norm: 0.33600081
INFO:root:[   24] Training loss: 0.07324520, Validation loss: 0.07341574, Gradient norm: 0.48363696
INFO:root:[   25] Training loss: 0.07377543, Validation loss: 0.07306610, Gradient norm: 0.73179113
INFO:root:[   26] Training loss: 0.07255579, Validation loss: 0.06975974, Gradient norm: 0.76642291
INFO:root:[   27] Training loss: 0.07098224, Validation loss: 0.06982465, Gradient norm: 0.61778174
INFO:root:[   28] Training loss: 0.06997898, Validation loss: 0.06957009, Gradient norm: 0.72228569
INFO:root:[   29] Training loss: 0.06853240, Validation loss: 0.06682890, Gradient norm: 0.49349600
INFO:root:[   30] Training loss: 0.06672446, Validation loss: 0.06740941, Gradient norm: 0.49729892
INFO:root:[   31] Training loss: 0.06755719, Validation loss: 0.06803227, Gradient norm: 0.78948275
INFO:root:[   32] Training loss: 0.06628869, Validation loss: 0.06489639, Gradient norm: 0.73022493
INFO:root:[   33] Training loss: 0.06500299, Validation loss: 0.06490084, Gradient norm: 0.83340404
INFO:root:[   34] Training loss: 0.06463324, Validation loss: 0.06276669, Gradient norm: 0.70135537
INFO:root:[   35] Training loss: 0.06371046, Validation loss: 0.06229471, Gradient norm: 0.74166518
INFO:root:[   36] Training loss: 0.06260638, Validation loss: 0.06233702, Gradient norm: 0.81631803
INFO:root:[   37] Training loss: 0.06178795, Validation loss: 0.06440466, Gradient norm: 0.88480630
INFO:root:[   38] Training loss: 0.06096313, Validation loss: 0.05739099, Gradient norm: 0.67638435
INFO:root:[   39] Training loss: 0.06013405, Validation loss: 0.06200933, Gradient norm: 0.78340531
INFO:root:[   40] Training loss: 0.05888336, Validation loss: 0.05869712, Gradient norm: 0.72927888
INFO:root:[   41] Training loss: 0.05817135, Validation loss: 0.06099070, Gradient norm: 0.84612101
INFO:root:[   42] Training loss: 0.05807175, Validation loss: 0.05690802, Gradient norm: 0.82933894
INFO:root:[   43] Training loss: 0.05612030, Validation loss: 0.05745312, Gradient norm: 0.71144510
INFO:root:[   44] Training loss: 0.05554129, Validation loss: 0.05573434, Gradient norm: 0.79620484
INFO:root:[   45] Training loss: 0.05541505, Validation loss: 0.05497326, Gradient norm: 0.68191866
INFO:root:[   46] Training loss: 0.05419864, Validation loss: 0.05497249, Gradient norm: 0.78560280
INFO:root:[   47] Training loss: 0.05355024, Validation loss: 0.05398218, Gradient norm: 0.90733362
INFO:root:[   48] Training loss: 0.05268758, Validation loss: 0.05206427, Gradient norm: 0.91285043
INFO:root:[   49] Training loss: 0.05193114, Validation loss: 0.05213941, Gradient norm: 0.61269747
INFO:root:[   50] Training loss: 0.05129224, Validation loss: 0.04877455, Gradient norm: 0.91101685
INFO:root:[   51] Training loss: 0.05097300, Validation loss: 0.05043985, Gradient norm: 0.93898235
INFO:root:[   52] Training loss: 0.05148732, Validation loss: 0.04888629, Gradient norm: 1.02953561
INFO:root:[   53] Training loss: 0.04952244, Validation loss: 0.04852279, Gradient norm: 0.87298941
INFO:root:[   54] Training loss: 0.04898057, Validation loss: 0.04908057, Gradient norm: 0.75055944
INFO:root:[   55] Training loss: 0.04865863, Validation loss: 0.04848677, Gradient norm: 1.01871341
INFO:root:[   56] Training loss: 0.04734673, Validation loss: 0.05232379, Gradient norm: 0.88726327
INFO:root:[   57] Training loss: 0.04847293, Validation loss: 0.04572887, Gradient norm: 1.09045810
INFO:root:[   58] Training loss: 0.04672773, Validation loss: 0.04550017, Gradient norm: 0.94368784
INFO:root:[   59] Training loss: 0.04666059, Validation loss: 0.04725358, Gradient norm: 0.99184114
INFO:root:[   60] Training loss: 0.04643129, Validation loss: 0.04444397, Gradient norm: 1.10397793
INFO:root:[   61] Training loss: 0.04469623, Validation loss: 0.04290102, Gradient norm: 1.05305970
INFO:root:[   62] Training loss: 0.04438908, Validation loss: 0.04428089, Gradient norm: 0.83955648
INFO:root:[   63] Training loss: 0.04392536, Validation loss: 0.04362252, Gradient norm: 0.97419790
INFO:root:[   64] Training loss: 0.04335439, Validation loss: 0.04310959, Gradient norm: 0.99030922
INFO:root:[   65] Training loss: 0.04315678, Validation loss: 0.04193011, Gradient norm: 1.11327505
INFO:root:[   66] Training loss: 0.04281948, Validation loss: 0.04093847, Gradient norm: 1.10227721
INFO:root:[   67] Training loss: 0.04363448, Validation loss: 0.03907912, Gradient norm: 1.16140625
INFO:root:[   68] Training loss: 0.04115202, Validation loss: 0.04056811, Gradient norm: 1.06072354
INFO:root:[   69] Training loss: 0.04372409, Validation loss: 0.03882206, Gradient norm: 1.10797388
INFO:root:[   70] Training loss: 0.03989748, Validation loss: 0.03980814, Gradient norm: 1.05628919
INFO:root:[   71] Training loss: 0.04038018, Validation loss: 0.03759333, Gradient norm: 1.28552925
INFO:root:[   72] Training loss: 0.03917394, Validation loss: 0.03792393, Gradient norm: 1.14289746
INFO:root:[   73] Training loss: 0.04158389, Validation loss: 0.03698341, Gradient norm: 1.09434898
INFO:root:[   74] Training loss: 0.03981291, Validation loss: 0.03939605, Gradient norm: 1.36433865
INFO:root:[   75] Training loss: 0.04044863, Validation loss: 0.03936456, Gradient norm: 1.30166040
INFO:root:[   76] Training loss: 0.03904216, Validation loss: 0.03833551, Gradient norm: 1.09147355
INFO:root:[   77] Training loss: 0.03753851, Validation loss: 0.03694033, Gradient norm: 1.12656649
INFO:root:[   78] Training loss: 0.03749597, Validation loss: 0.03871248, Gradient norm: 1.21391775
INFO:root:[   79] Training loss: 0.03712858, Validation loss: 0.03615338, Gradient norm: 1.16692098
INFO:root:[   80] Training loss: 0.03738222, Validation loss: 0.03652697, Gradient norm: 1.14520777
INFO:root:[   81] Training loss: 0.03598694, Validation loss: 0.03548633, Gradient norm: 1.02264260
INFO:root:[   82] Training loss: 0.03664162, Validation loss: 0.03471127, Gradient norm: 1.16263670
INFO:root:[   83] Training loss: 0.03592867, Validation loss: 0.03359217, Gradient norm: 1.13761816
INFO:root:[   84] Training loss: 0.03561373, Validation loss: 0.03388697, Gradient norm: 1.21525264
INFO:root:[   85] Training loss: 0.03509998, Validation loss: 0.03331574, Gradient norm: 1.13230412
INFO:root:[   86] Training loss: 0.03495830, Validation loss: 0.03386763, Gradient norm: 1.31089125
INFO:root:[   87] Training loss: 0.03454065, Validation loss: 0.03475056, Gradient norm: 1.22713862
INFO:root:[   88] Training loss: 0.03514107, Validation loss: 0.03396716, Gradient norm: 1.43989987
INFO:root:[   89] Training loss: 0.03351409, Validation loss: 0.03486565, Gradient norm: 1.35620993
INFO:root:[   90] Training loss: 0.03517700, Validation loss: 0.03243426, Gradient norm: 1.15535102
INFO:root:[   91] Training loss: 0.03426442, Validation loss: 0.03161786, Gradient norm: 1.19212086
INFO:root:[   92] Training loss: 0.03259589, Validation loss: 0.03055172, Gradient norm: 1.28486450
INFO:root:[   93] Training loss: 0.03316042, Validation loss: 0.03520223, Gradient norm: 1.21212903
INFO:root:[   94] Training loss: 0.03266068, Validation loss: 0.03047517, Gradient norm: 1.14343466
INFO:root:[   95] Training loss: 0.03208855, Validation loss: 0.03411387, Gradient norm: 1.24734112
INFO:root:[   96] Training loss: 0.03235954, Validation loss: 0.03201229, Gradient norm: 1.24791843
INFO:root:[   97] Training loss: 0.03245301, Validation loss: 0.03020383, Gradient norm: 1.14080110
INFO:root:[   98] Training loss: 0.03209098, Validation loss: 0.03330721, Gradient norm: 1.27211778
INFO:root:[   99] Training loss: 0.03175886, Validation loss: 0.03281757, Gradient norm: 1.37586050
INFO:root:[  100] Training loss: 0.03199712, Validation loss: 0.03065023, Gradient norm: 1.28964343
INFO:root:[  101] Training loss: 0.03175242, Validation loss: 0.03164366, Gradient norm: 1.31263555
INFO:root:[  102] Training loss: 0.03041275, Validation loss: 0.03426944, Gradient norm: 1.15096080
INFO:root:[  103] Training loss: 0.03066868, Validation loss: 0.03040944, Gradient norm: 1.37089451
INFO:root:[  104] Training loss: 0.02973743, Validation loss: 0.02882064, Gradient norm: 1.22238822
INFO:root:[  105] Training loss: 0.02965326, Validation loss: 0.02805569, Gradient norm: 1.22658063
INFO:root:[  106] Training loss: 0.03037174, Validation loss: 0.03041934, Gradient norm: 1.39256144
INFO:root:[  107] Training loss: 0.02991277, Validation loss: 0.03028030, Gradient norm: 1.38253812
INFO:root:[  108] Training loss: 0.03004968, Validation loss: 0.02981466, Gradient norm: 1.37392212
INFO:root:[  109] Training loss: 0.02985641, Validation loss: 0.02907366, Gradient norm: 1.32575896
INFO:root:[  110] Training loss: 0.02886211, Validation loss: 0.02760271, Gradient norm: 1.22689309
INFO:root:[  111] Training loss: 0.02915972, Validation loss: 0.03023835, Gradient norm: 1.37824218
INFO:root:[  112] Training loss: 0.03021167, Validation loss: 0.02845098, Gradient norm: 1.40894939
INFO:root:[  113] Training loss: 0.02947074, Validation loss: 0.03314886, Gradient norm: 1.57313704
INFO:root:[  114] Training loss: 0.02928576, Validation loss: 0.02993137, Gradient norm: 1.36482200
INFO:root:[  115] Training loss: 0.02932393, Validation loss: 0.03448524, Gradient norm: 1.41081134
INFO:root:[  116] Training loss: 0.02943711, Validation loss: 0.02628786, Gradient norm: 1.46573055
INFO:root:[  117] Training loss: 0.02808630, Validation loss: 0.02675491, Gradient norm: 1.27866833
INFO:root:[  118] Training loss: 0.02777675, Validation loss: 0.02755658, Gradient norm: 1.31151686
INFO:root:[  119] Training loss: 0.02840162, Validation loss: 0.02686209, Gradient norm: 1.33150553
INFO:root:[  120] Training loss: 0.02823990, Validation loss: 0.03097911, Gradient norm: 1.42543574
INFO:root:[  121] Training loss: 0.02820624, Validation loss: 0.03028455, Gradient norm: 1.41359774
INFO:root:[  122] Training loss: 0.02777115, Validation loss: 0.02622767, Gradient norm: 1.37055786
INFO:root:[  123] Training loss: 0.02787981, Validation loss: 0.03240369, Gradient norm: 1.38742514
INFO:root:[  124] Training loss: 0.02811081, Validation loss: 0.02768207, Gradient norm: 1.41410708
INFO:root:[  125] Training loss: 0.02689294, Validation loss: 0.02678963, Gradient norm: 1.32621881
INFO:root:[  126] Training loss: 0.02666970, Validation loss: 0.02607621, Gradient norm: 1.38496805
INFO:root:[  127] Training loss: 0.02755238, Validation loss: 0.02784759, Gradient norm: 1.48397347
INFO:root:[  128] Training loss: 0.02702652, Validation loss: 0.02585957, Gradient norm: 1.37339121
INFO:root:[  129] Training loss: 0.02792623, Validation loss: 0.02787394, Gradient norm: 1.50207375
INFO:root:[  130] Training loss: 0.02684951, Validation loss: 0.02843297, Gradient norm: 1.53813820
INFO:root:[  131] Training loss: 0.02696460, Validation loss: 0.02768576, Gradient norm: 1.58474604
INFO:root:[  132] Training loss: 0.02740309, Validation loss: 0.02643079, Gradient norm: 1.47896227
INFO:root:[  133] Training loss: 0.02742237, Validation loss: 0.02793394, Gradient norm: 1.45805304
INFO:root:[  134] Training loss: 0.02674722, Validation loss: 0.02500367, Gradient norm: 1.36580134
INFO:root:[  135] Training loss: 0.02648629, Validation loss: 0.02505191, Gradient norm: 1.47073322
INFO:root:[  136] Training loss: 0.02652469, Validation loss: 0.02730958, Gradient norm: 1.51580147
INFO:root:[  137] Training loss: 0.02667100, Validation loss: 0.02719643, Gradient norm: 1.63169980
INFO:root:[  138] Training loss: 0.02644893, Validation loss: 0.02756451, Gradient norm: 1.41927553
INFO:root:[  139] Training loss: 0.02663391, Validation loss: 0.02574371, Gradient norm: 1.42823992
INFO:root:[  140] Training loss: 0.02667767, Validation loss: 0.02483677, Gradient norm: 1.65556654
INFO:root:[  141] Training loss: 0.02648029, Validation loss: 0.02518061, Gradient norm: 1.64489118
INFO:root:[  142] Training loss: 0.02607612, Validation loss: 0.02838146, Gradient norm: 1.31501322
INFO:root:[  143] Training loss: 0.02604006, Validation loss: 0.02724176, Gradient norm: 1.51570381
INFO:root:[  144] Training loss: 0.02598264, Validation loss: 0.02754440, Gradient norm: 1.40770600
INFO:root:[  145] Training loss: 0.02596939, Validation loss: 0.02246660, Gradient norm: 1.70703788
INFO:root:[  146] Training loss: 0.02651413, Validation loss: 0.02453061, Gradient norm: 1.79421014
INFO:root:[  147] Training loss: 0.02613962, Validation loss: 0.03060958, Gradient norm: 1.56092880
INFO:root:[  148] Training loss: 0.02664865, Validation loss: 0.02645016, Gradient norm: 1.49478106
INFO:root:[  149] Training loss: 0.02549585, Validation loss: 0.02571271, Gradient norm: 1.50178641
INFO:root:[  150] Training loss: 0.02628210, Validation loss: 0.02576318, Gradient norm: 1.55859441
INFO:root:[  151] Training loss: 0.02589688, Validation loss: 0.02761955, Gradient norm: 1.53866926
INFO:root:[  152] Training loss: 0.02598442, Validation loss: 0.02651506, Gradient norm: 1.48875993
INFO:root:[  153] Training loss: 0.02637421, Validation loss: 0.02456806, Gradient norm: 1.60864870
INFO:root:[  154] Training loss: 0.02514069, Validation loss: 0.02403550, Gradient norm: 1.37815414
INFO:root:EP 154: Early stopping
INFO:root:Training the model took 7689.674s.
INFO:root:Emptying the cuda cache took 0.098s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02685
INFO:root:EnergyScoreTrain: 0.03015
INFO:root:CoverageTrain: 0.96945
INFO:root:IntervalWidthTrain: 0.03681
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0266
INFO:root:EnergyScoreValidation: 0.03044
INFO:root:CoverageValidation: 0.96866
INFO:root:IntervalWidthValidation: 0.03613
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03361
INFO:root:EnergyScoreTest: 0.03511
INFO:root:CoverageTest: 0.96126
INFO:root:IntervalWidthTest: 0.03561
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1197473792
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.56661015, Validation loss: 0.29153405, Gradient norm: 5.86436349
INFO:root:[    2] Training loss: 0.29407862, Validation loss: 0.23769768, Gradient norm: 5.08106973
INFO:root:[    3] Training loss: 0.23947211, Validation loss: 0.23229699, Gradient norm: 3.83274006
INFO:root:[    4] Training loss: 0.20931528, Validation loss: 0.29526354, Gradient norm: 3.33474162
INFO:root:[    5] Training loss: 0.19628987, Validation loss: 0.16423607, Gradient norm: 3.99658602
INFO:root:[    6] Training loss: 0.17862141, Validation loss: 0.15934639, Gradient norm: 3.92680351
INFO:root:[    7] Training loss: 0.16231033, Validation loss: 0.14451964, Gradient norm: 3.78678066
INFO:root:[    8] Training loss: 0.14831424, Validation loss: 0.13698238, Gradient norm: 3.21270833
INFO:root:[    9] Training loss: 0.14529422, Validation loss: 0.12989089, Gradient norm: 3.92669711
INFO:root:[   10] Training loss: 0.13747422, Validation loss: 0.16066014, Gradient norm: 3.79948446
INFO:root:[   11] Training loss: 0.13090164, Validation loss: 0.11881631, Gradient norm: 3.44906905
INFO:root:[   12] Training loss: 0.12050145, Validation loss: 0.12237690, Gradient norm: 2.38173397
INFO:root:[   13] Training loss: 0.11537328, Validation loss: 0.11021669, Gradient norm: 2.09657667
INFO:root:[   14] Training loss: 0.11289045, Validation loss: 0.12930004, Gradient norm: 2.11464704
INFO:root:[   15] Training loss: 0.10873627, Validation loss: 0.10595335, Gradient norm: 1.98337235
INFO:root:[   16] Training loss: 0.10549797, Validation loss: 0.11021042, Gradient norm: 1.98070092
INFO:root:[   17] Training loss: 0.10424787, Validation loss: 0.10073425, Gradient norm: 2.38400760
INFO:root:[   18] Training loss: 0.09985131, Validation loss: 0.09725929, Gradient norm: 1.74222427
INFO:root:[   19] Training loss: 0.09859844, Validation loss: 0.09479836, Gradient norm: 1.92352354
INFO:root:[   20] Training loss: 0.09634942, Validation loss: 0.09521317, Gradient norm: 1.83284964
INFO:root:[   21] Training loss: 0.09429121, Validation loss: 0.09131423, Gradient norm: 1.73890268
INFO:root:[   22] Training loss: 0.09259893, Validation loss: 0.09082528, Gradient norm: 1.87457718
INFO:root:[   23] Training loss: 0.09066349, Validation loss: 0.09347073, Gradient norm: 1.67047160
INFO:root:[   24] Training loss: 0.08991785, Validation loss: 0.08738674, Gradient norm: 1.90458661
INFO:root:[   25] Training loss: 0.08692526, Validation loss: 0.08441538, Gradient norm: 1.52128006
INFO:root:[   26] Training loss: 0.08463062, Validation loss: 0.08371680, Gradient norm: 1.31061137
INFO:root:[   27] Training loss: 0.08388238, Validation loss: 0.08068432, Gradient norm: 1.72759112
INFO:root:[   28] Training loss: 0.08250894, Validation loss: 0.07906362, Gradient norm: 1.82390118
INFO:root:[   29] Training loss: 0.08110397, Validation loss: 0.07917896, Gradient norm: 1.84313313
INFO:root:[   30] Training loss: 0.07984824, Validation loss: 0.07756213, Gradient norm: 1.64842361
INFO:root:[   31] Training loss: 0.07870998, Validation loss: 0.08222020, Gradient norm: 1.87094819
INFO:root:[   32] Training loss: 0.07692133, Validation loss: 0.07583925, Gradient norm: 1.79043494
INFO:root:[   33] Training loss: 0.07461821, Validation loss: 0.07373924, Gradient norm: 1.51769683
INFO:root:[   34] Training loss: 0.07316582, Validation loss: 0.07567656, Gradient norm: 1.60835301
INFO:root:[   35] Training loss: 0.07339589, Validation loss: 0.07349458, Gradient norm: 1.90721058
INFO:root:[   36] Training loss: 0.07129628, Validation loss: 0.06895140, Gradient norm: 1.72302867
INFO:root:[   37] Training loss: 0.07022712, Validation loss: 0.06875990, Gradient norm: 1.91300091
INFO:root:[   38] Training loss: 0.06848450, Validation loss: 0.06702431, Gradient norm: 1.79902057
INFO:root:[   39] Training loss: 0.06859990, Validation loss: 0.06803378, Gradient norm: 2.03033937
INFO:root:[   40] Training loss: 0.06701056, Validation loss: 0.06710083, Gradient norm: 2.14394743
INFO:root:[   41] Training loss: 0.06560466, Validation loss: 0.06522325, Gradient norm: 1.92974995
INFO:root:[   42] Training loss: 0.06506434, Validation loss: 0.06376458, Gradient norm: 1.67705399
INFO:root:[   43] Training loss: 0.06283163, Validation loss: 0.06257975, Gradient norm: 1.89336105
INFO:root:[   44] Training loss: 0.06228124, Validation loss: 0.06240537, Gradient norm: 2.01620072
INFO:root:[   45] Training loss: 0.06164760, Validation loss: 0.06325966, Gradient norm: 1.87610622
INFO:root:[   46] Training loss: 0.06094596, Validation loss: 0.05822534, Gradient norm: 2.17667878
INFO:root:[   47] Training loss: 0.05962819, Validation loss: 0.05964343, Gradient norm: 1.97172052
INFO:root:[   48] Training loss: 0.05766609, Validation loss: 0.05982160, Gradient norm: 2.07064751
INFO:root:[   49] Training loss: 0.05781015, Validation loss: 0.05829744, Gradient norm: 1.96818010
INFO:root:[   50] Training loss: 0.05704781, Validation loss: 0.05783261, Gradient norm: 2.22761922
INFO:root:[   51] Training loss: 0.05548508, Validation loss: 0.05409531, Gradient norm: 2.09465873
INFO:root:[   52] Training loss: 0.05496733, Validation loss: 0.05642288, Gradient norm: 2.07410795
INFO:root:[   53] Training loss: 0.05376782, Validation loss: 0.05375238, Gradient norm: 1.84183245
INFO:root:[   54] Training loss: 0.05288129, Validation loss: 0.05271978, Gradient norm: 1.97711036
INFO:root:[   55] Training loss: 0.05210479, Validation loss: 0.04981372, Gradient norm: 1.92602474
INFO:root:[   56] Training loss: 0.05029757, Validation loss: 0.04974989, Gradient norm: 1.91020901
INFO:root:[   57] Training loss: 0.05027384, Validation loss: 0.04943249, Gradient norm: 1.89893174
INFO:root:[   58] Training loss: 0.04947308, Validation loss: 0.04947468, Gradient norm: 2.03566823
INFO:root:[   59] Training loss: 0.04852073, Validation loss: 0.04696116, Gradient norm: 2.17571447
INFO:root:[   60] Training loss: 0.04815659, Validation loss: 0.04530361, Gradient norm: 1.91082121
INFO:root:[   61] Training loss: 0.04752247, Validation loss: 0.04689539, Gradient norm: 1.95802359
INFO:root:[   62] Training loss: 0.04638839, Validation loss: 0.04432120, Gradient norm: 1.94287307
INFO:root:[   63] Training loss: 0.04537827, Validation loss: 0.04339056, Gradient norm: 2.08233149
INFO:root:[   64] Training loss: 0.04579131, Validation loss: 0.04901782, Gradient norm: 1.90661634
INFO:root:[   65] Training loss: 0.04510609, Validation loss: 0.04254401, Gradient norm: 2.00385317
INFO:root:[   66] Training loss: 0.04725105, Validation loss: 0.05294510, Gradient norm: 2.05227071
INFO:root:[   67] Training loss: 0.04476425, Validation loss: 0.04118652, Gradient norm: 2.10604532
INFO:root:[   68] Training loss: 0.04270953, Validation loss: 0.04132224, Gradient norm: 1.97185887
INFO:root:[   69] Training loss: 0.04318935, Validation loss: 0.04064168, Gradient norm: 2.11320081
INFO:root:[   70] Training loss: 0.04244414, Validation loss: 0.04306626, Gradient norm: 2.27133888
INFO:root:[   71] Training loss: 0.04205541, Validation loss: 0.03979549, Gradient norm: 1.96081085
INFO:root:[   72] Training loss: 0.04057638, Validation loss: 0.04114645, Gradient norm: 1.89447201
INFO:root:[   73] Training loss: 0.04172612, Validation loss: 0.04438463, Gradient norm: 1.97300750
INFO:root:[   74] Training loss: 0.04084516, Validation loss: 0.03919471, Gradient norm: 1.76379619
INFO:root:[   75] Training loss: 0.03920400, Validation loss: 0.04104071, Gradient norm: 2.06170366
INFO:root:[   76] Training loss: 0.03900651, Validation loss: 0.03796706, Gradient norm: 2.01009264
INFO:root:[   77] Training loss: 0.03961085, Validation loss: 0.03731814, Gradient norm: 2.19792766
INFO:root:[   78] Training loss: 0.03788132, Validation loss: 0.04002902, Gradient norm: 1.79256232
INFO:root:[   79] Training loss: 0.03751190, Validation loss: 0.03903521, Gradient norm: 1.85306403
INFO:root:[   80] Training loss: 0.03778891, Validation loss: 0.04450790, Gradient norm: 2.12564643
INFO:root:[   81] Training loss: 0.03794998, Validation loss: 0.03695895, Gradient norm: 1.99974231
INFO:root:[   82] Training loss: 0.03666244, Validation loss: 0.03562272, Gradient norm: 1.89733537
INFO:root:[   83] Training loss: 0.03549237, Validation loss: 0.03414939, Gradient norm: 1.88123370
INFO:root:[   84] Training loss: 0.03768690, Validation loss: 0.03490053, Gradient norm: 1.91146547
INFO:root:[   85] Training loss: 0.03547045, Validation loss: 0.03340874, Gradient norm: 1.85531827
INFO:root:[   86] Training loss: 0.03465732, Validation loss: 0.03527098, Gradient norm: 1.96287472
INFO:root:[   87] Training loss: 0.03417955, Validation loss: 0.03305423, Gradient norm: 1.90311830
INFO:root:[   88] Training loss: 0.03443610, Validation loss: 0.03114771, Gradient norm: 2.12087602
INFO:root:[   89] Training loss: 0.03286734, Validation loss: 0.03667760, Gradient norm: 1.74907267
INFO:root:[   90] Training loss: 0.03554029, Validation loss: 0.03133416, Gradient norm: 2.26451118
INFO:root:[   91] Training loss: 0.03410270, Validation loss: 0.03371155, Gradient norm: 1.78745505
INFO:root:[   92] Training loss: 0.03275571, Validation loss: 0.03157758, Gradient norm: 1.78789404
INFO:root:[   93] Training loss: 0.03196936, Validation loss: 0.03700218, Gradient norm: 1.93361406
INFO:root:[   94] Training loss: 0.03512987, Validation loss: 0.03569134, Gradient norm: 2.21860527
INFO:root:[   95] Training loss: 0.03279937, Validation loss: 0.03287489, Gradient norm: 1.79410117
INFO:root:[   96] Training loss: 0.03188938, Validation loss: 0.03339587, Gradient norm: 2.03878875
INFO:root:[   97] Training loss: 0.03218286, Validation loss: 0.03167876, Gradient norm: 1.86228437
INFO:root:EP 97: Early stopping
INFO:root:Training the model took 4864.14s.
INFO:root:Emptying the cuda cache took 0.107s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0387
INFO:root:EnergyScoreTrain: 0.03156
INFO:root:CoverageTrain: 0.99683
INFO:root:IntervalWidthTrain: 0.07689
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03814
INFO:root:EnergyScoreValidation: 0.03116
INFO:root:CoverageValidation: 0.99697
INFO:root:IntervalWidthValidation: 0.07611
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04495
INFO:root:EnergyScoreTest: 0.03438
INFO:root:CoverageTest: 0.99453
INFO:root:IntervalWidthTest: 0.07679
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.61872451, Validation loss: 0.35337112, Gradient norm: 5.29572961
INFO:root:[    2] Training loss: 0.31080191, Validation loss: 0.26495886, Gradient norm: 2.87449662
INFO:root:[    3] Training loss: 0.25864993, Validation loss: 0.22740005, Gradient norm: 3.27118493
INFO:root:[    4] Training loss: 0.22060694, Validation loss: 0.20851544, Gradient norm: 2.96278643
INFO:root:[    5] Training loss: 0.18846179, Validation loss: 0.17377077, Gradient norm: 2.23230760
INFO:root:[    6] Training loss: 0.17033214, Validation loss: 0.16853071, Gradient norm: 1.75092383
INFO:root:[    7] Training loss: 0.15690534, Validation loss: 0.14627001, Gradient norm: 1.66227454
INFO:root:[    8] Training loss: 0.14319348, Validation loss: 0.14512191, Gradient norm: 0.99767101
INFO:root:[    9] Training loss: 0.13738826, Validation loss: 0.13114166, Gradient norm: 1.48693930
INFO:root:[   10] Training loss: 0.12848874, Validation loss: 0.12293723, Gradient norm: 1.03512602
INFO:root:[   11] Training loss: 0.12302632, Validation loss: 0.12210358, Gradient norm: 0.99030284
INFO:root:[   12] Training loss: 0.11902852, Validation loss: 0.11399362, Gradient norm: 1.13171848
INFO:root:[   13] Training loss: 0.11416768, Validation loss: 0.11136449, Gradient norm: 0.89646661
INFO:root:[   14] Training loss: 0.11037666, Validation loss: 0.10844726, Gradient norm: 0.79116140
INFO:root:[   15] Training loss: 0.10730981, Validation loss: 0.10362941, Gradient norm: 0.79066264
INFO:root:[   16] Training loss: 0.10499476, Validation loss: 0.10182279, Gradient norm: 0.81461166
INFO:root:[   17] Training loss: 0.10168854, Validation loss: 0.09783986, Gradient norm: 0.79033469
INFO:root:[   18] Training loss: 0.09785711, Validation loss: 0.09562435, Gradient norm: 0.45759299
INFO:root:[   19] Training loss: 0.09563714, Validation loss: 0.09342672, Gradient norm: 0.47573097
INFO:root:[   20] Training loss: 0.09364578, Validation loss: 0.09240436, Gradient norm: 0.70725398
INFO:root:[   21] Training loss: 0.09150139, Validation loss: 0.09011645, Gradient norm: 0.80156759
INFO:root:[   22] Training loss: 0.09007791, Validation loss: 0.08779972, Gradient norm: 0.99219289
INFO:root:[   23] Training loss: 0.08657197, Validation loss: 0.08369016, Gradient norm: 0.76078945
INFO:root:[   24] Training loss: 0.08450687, Validation loss: 0.08292478, Gradient norm: 0.75243972
INFO:root:[   25] Training loss: 0.08225578, Validation loss: 0.08178124, Gradient norm: 0.75451884
INFO:root:[   26] Training loss: 0.08040310, Validation loss: 0.07909791, Gradient norm: 0.83467539
INFO:root:[   27] Training loss: 0.07767280, Validation loss: 0.07844335, Gradient norm: 0.62176199
INFO:root:[   28] Training loss: 0.07717654, Validation loss: 0.07667444, Gradient norm: 0.82930197
INFO:root:[   29] Training loss: 0.07496835, Validation loss: 0.07575044, Gradient norm: 0.75652471
INFO:root:[   30] Training loss: 0.07213041, Validation loss: 0.07074045, Gradient norm: 0.88716612
INFO:root:[   31] Training loss: 0.07082868, Validation loss: 0.06912366, Gradient norm: 0.91540844
INFO:root:[   32] Training loss: 0.06917198, Validation loss: 0.06831144, Gradient norm: 0.92263698
INFO:root:[   33] Training loss: 0.06675808, Validation loss: 0.06461861, Gradient norm: 0.91508205
INFO:root:[   34] Training loss: 0.06555075, Validation loss: 0.06289377, Gradient norm: 0.91759416
INFO:root:[   35] Training loss: 0.06458383, Validation loss: 0.06022558, Gradient norm: 0.89947468
INFO:root:[   36] Training loss: 0.06226917, Validation loss: 0.06153414, Gradient norm: 0.90053960
INFO:root:[   37] Training loss: 0.06043965, Validation loss: 0.06021724, Gradient norm: 0.88038376
INFO:root:[   38] Training loss: 0.05967137, Validation loss: 0.05658866, Gradient norm: 1.08523009
INFO:root:[   39] Training loss: 0.05695220, Validation loss: 0.05501052, Gradient norm: 1.07278222
INFO:root:[   40] Training loss: 0.05796643, Validation loss: 0.05518602, Gradient norm: 0.95820514
INFO:root:[   41] Training loss: 0.05443891, Validation loss: 0.05134507, Gradient norm: 0.94664615
INFO:root:[   42] Training loss: 0.05421185, Validation loss: 0.05324726, Gradient norm: 0.90120297
INFO:root:[   43] Training loss: 0.05151449, Validation loss: 0.05182721, Gradient norm: 1.13107434
INFO:root:[   44] Training loss: 0.05242229, Validation loss: 0.05236693, Gradient norm: 0.91584357
INFO:root:[   45] Training loss: 0.05044147, Validation loss: 0.05234535, Gradient norm: 1.07661584
INFO:root:[   46] Training loss: 0.04864289, Validation loss: 0.04651664, Gradient norm: 1.17961970
INFO:root:[   47] Training loss: 0.04906051, Validation loss: 0.04839525, Gradient norm: 1.19769407
INFO:root:[   48] Training loss: 0.04633512, Validation loss: 0.04760869, Gradient norm: 0.97054982
INFO:root:[   49] Training loss: 0.04680045, Validation loss: 0.04625361, Gradient norm: 1.16050867
INFO:root:[   50] Training loss: 0.04538509, Validation loss: 0.04247195, Gradient norm: 1.07956125
INFO:root:[   51] Training loss: 0.04270037, Validation loss: 0.04185448, Gradient norm: 1.08684625
INFO:root:[   52] Training loss: 0.04616659, Validation loss: 0.04258749, Gradient norm: 1.14490250
INFO:root:[   53] Training loss: 0.04186295, Validation loss: 0.03965499, Gradient norm: 1.04563628
INFO:root:[   54] Training loss: 0.04208200, Validation loss: 0.03901989, Gradient norm: 1.17487121
INFO:root:[   55] Training loss: 0.04069136, Validation loss: 0.04227256, Gradient norm: 0.96244711
INFO:root:[   56] Training loss: 0.03966363, Validation loss: 0.04120397, Gradient norm: 1.05715670
INFO:root:[   57] Training loss: 0.04008405, Validation loss: 0.03975287, Gradient norm: 1.33106549
INFO:root:[   58] Training loss: 0.04096110, Validation loss: 0.04007209, Gradient norm: 1.19608954
INFO:root:[   59] Training loss: 0.03883616, Validation loss: 0.03672055, Gradient norm: 1.00662581
INFO:root:[   60] Training loss: 0.03845348, Validation loss: 0.03838966, Gradient norm: 1.19980658
INFO:root:[   61] Training loss: 0.03782455, Validation loss: 0.03692011, Gradient norm: 1.33447844
INFO:root:[   62] Training loss: 0.03634324, Validation loss: 0.04306184, Gradient norm: 1.07604420
INFO:root:[   63] Training loss: 0.03808084, Validation loss: 0.03694851, Gradient norm: 1.47515207
INFO:root:[   64] Training loss: 0.03895302, Validation loss: 0.03665923, Gradient norm: 1.33955846
INFO:root:[   65] Training loss: 0.03589005, Validation loss: 0.03366266, Gradient norm: 1.16376618
INFO:root:[   66] Training loss: 0.03648518, Validation loss: 0.03586179, Gradient norm: 1.18324130
INFO:root:[   67] Training loss: 0.03573379, Validation loss: 0.04152454, Gradient norm: 1.30220318
INFO:root:[   68] Training loss: 0.03800325, Validation loss: 0.03626657, Gradient norm: 1.63742427
INFO:root:[   69] Training loss: 0.03633048, Validation loss: 0.03219466, Gradient norm: 1.28833179
INFO:root:[   70] Training loss: 0.03518799, Validation loss: 0.03386046, Gradient norm: 1.15833402
INFO:root:[   71] Training loss: 0.03600761, Validation loss: 0.03255842, Gradient norm: 1.23770027
INFO:root:[   72] Training loss: 0.03499103, Validation loss: 0.03717648, Gradient norm: 1.37458296
INFO:root:[   73] Training loss: 0.03605442, Validation loss: 0.03384009, Gradient norm: 1.48005352
INFO:root:[   74] Training loss: 0.03523846, Validation loss: 0.03617662, Gradient norm: 1.01057109
INFO:root:[   75] Training loss: 0.03437299, Validation loss: 0.03486436, Gradient norm: 1.25900794
INFO:root:[   76] Training loss: 0.03367915, Validation loss: 0.03679747, Gradient norm: 1.31272565
INFO:root:[   77] Training loss: 0.03423899, Validation loss: 0.03496029, Gradient norm: 1.24344608
INFO:root:[   78] Training loss: 0.03318040, Validation loss: 0.03156524, Gradient norm: 1.19052305
INFO:root:[   79] Training loss: 0.03425047, Validation loss: 0.03760568, Gradient norm: 1.45466730
INFO:root:[   80] Training loss: 0.03516581, Validation loss: 0.03104532, Gradient norm: 1.36161697
INFO:root:[   81] Training loss: 0.03317346, Validation loss: 0.03211262, Gradient norm: 1.17993251
INFO:root:[   82] Training loss: 0.03393232, Validation loss: 0.03409214, Gradient norm: 1.30271221
INFO:root:[   83] Training loss: 0.03354856, Validation loss: 0.03738317, Gradient norm: 1.18786481
INFO:root:[   84] Training loss: 0.03340731, Validation loss: 0.03460749, Gradient norm: 1.37876591
INFO:root:[   85] Training loss: 0.03398515, Validation loss: 0.03428228, Gradient norm: 1.41251133
INFO:root:[   86] Training loss: 0.03436589, Validation loss: 0.03905122, Gradient norm: 1.28078451
INFO:root:[   87] Training loss: 0.03372165, Validation loss: 0.03268239, Gradient norm: 1.34145032
INFO:root:[   88] Training loss: 0.03316595, Validation loss: 0.03251635, Gradient norm: 1.22718705
INFO:root:[   89] Training loss: 0.03480818, Validation loss: 0.03482843, Gradient norm: 1.38450563
INFO:root:EP 89: Early stopping
INFO:root:Training the model took 4468.902s.
INFO:root:Emptying the cuda cache took 0.099s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03914
INFO:root:EnergyScoreTrain: 0.03752
INFO:root:CoverageTrain: 0.9984
INFO:root:IntervalWidthTrain: 0.05221
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03857
INFO:root:EnergyScoreValidation: 0.03726
INFO:root:CoverageValidation: 0.99845
INFO:root:IntervalWidthValidation: 0.05136
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04415
INFO:root:EnergyScoreTest: 0.03828
INFO:root:CoverageTest: 0.99665
INFO:root:IntervalWidthTest: 0.05165
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.49608044, Validation loss: 0.18970564, Gradient norm: 6.67379875
INFO:root:[    2] Training loss: 0.20417884, Validation loss: 0.28892494, Gradient norm: 5.77373121
INFO:root:[    3] Training loss: 0.19491247, Validation loss: 0.10726964, Gradient norm: 6.67336236
INFO:root:[    4] Training loss: 0.15792703, Validation loss: 0.09054721, Gradient norm: 5.34022906
INFO:root:[    5] Training loss: 0.13780928, Validation loss: 0.17608415, Gradient norm: 4.67764593
INFO:root:[    6] Training loss: 0.12814707, Validation loss: 0.08767258, Gradient norm: 4.27002368
INFO:root:[    7] Training loss: 0.11152982, Validation loss: 0.08211538, Gradient norm: 3.90365298
INFO:root:[    8] Training loss: 0.10626858, Validation loss: 0.11453590, Gradient norm: 3.64348890
INFO:root:[    9] Training loss: 0.10234750, Validation loss: 0.08668313, Gradient norm: 3.42835479
INFO:root:[   10] Training loss: 0.09448688, Validation loss: 0.06054490, Gradient norm: 3.16892212
INFO:root:[   11] Training loss: 0.08385003, Validation loss: 0.06629007, Gradient norm: 2.00697444
INFO:root:[   12] Training loss: 0.08004289, Validation loss: 0.05119606, Gradient norm: 1.95002062
INFO:root:[   13] Training loss: 0.07630663, Validation loss: 0.05878042, Gradient norm: 2.03532844
INFO:root:[   14] Training loss: 0.07415538, Validation loss: 0.06167048, Gradient norm: 1.98970166
INFO:root:[   15] Training loss: 0.07183092, Validation loss: 0.05104649, Gradient norm: 1.66534363
INFO:root:[   16] Training loss: 0.06818140, Validation loss: 0.05421772, Gradient norm: 1.31297870
INFO:root:[   17] Training loss: 0.06834949, Validation loss: 0.05519672, Gradient norm: 1.65590804
INFO:root:[   18] Training loss: 0.06592420, Validation loss: 0.04311446, Gradient norm: 1.34150718
INFO:root:[   19] Training loss: 0.06360764, Validation loss: 0.04506671, Gradient norm: 1.23604309
INFO:root:[   20] Training loss: 0.06419954, Validation loss: 0.05199826, Gradient norm: 1.33648048
INFO:root:[   21] Training loss: 0.06205436, Validation loss: 0.04165079, Gradient norm: 1.34604481
INFO:root:[   22] Training loss: 0.06123880, Validation loss: 0.04720972, Gradient norm: 1.21473250
INFO:root:[   23] Training loss: 0.06115852, Validation loss: 0.03975948, Gradient norm: 1.27572964
INFO:root:[   24] Training loss: 0.05960034, Validation loss: 0.04439930, Gradient norm: 1.33798567
INFO:root:[   25] Training loss: 0.06076719, Validation loss: 0.04724075, Gradient norm: 1.16383569
INFO:root:[   26] Training loss: 0.05869641, Validation loss: 0.04091115, Gradient norm: 1.17322505
INFO:root:[   27] Training loss: 0.05845954, Validation loss: 0.05053819, Gradient norm: 1.18800929
INFO:root:[   28] Training loss: 0.05712429, Validation loss: 0.04383095, Gradient norm: 1.33190559
INFO:root:[   29] Training loss: 0.05740209, Validation loss: 0.03944983, Gradient norm: 1.27331348
INFO:root:[   30] Training loss: 0.05691070, Validation loss: 0.04151664, Gradient norm: 1.12389413
INFO:root:[   31] Training loss: 0.05605454, Validation loss: 0.05016050, Gradient norm: 1.18376102
INFO:root:[   32] Training loss: 0.05527794, Validation loss: 0.04551000, Gradient norm: 1.29587069
INFO:root:[   33] Training loss: 0.05352135, Validation loss: 0.03399494, Gradient norm: 1.20829561
INFO:root:[   34] Training loss: 0.05585496, Validation loss: 0.04463996, Gradient norm: 1.13700170
INFO:root:[   35] Training loss: 0.05304741, Validation loss: 0.04845723, Gradient norm: 0.86437418
INFO:root:[   36] Training loss: 0.05457646, Validation loss: 0.05298884, Gradient norm: 1.47246450
INFO:root:[   37] Training loss: 0.05170933, Validation loss: 0.03331304, Gradient norm: 1.28503944
INFO:root:[   38] Training loss: 0.05345229, Validation loss: 0.03739896, Gradient norm: 1.46413824
INFO:root:[   39] Training loss: 0.05285709, Validation loss: 0.03396100, Gradient norm: 1.27380514
INFO:root:[   40] Training loss: 0.05259478, Validation loss: 0.04768992, Gradient norm: 1.13328662
INFO:root:[   41] Training loss: 0.05195879, Validation loss: 0.03463536, Gradient norm: 1.01733006
INFO:root:[   42] Training loss: 0.05077433, Validation loss: 0.03568358, Gradient norm: 1.08893294
INFO:root:[   43] Training loss: 0.05080698, Validation loss: 0.03279538, Gradient norm: 1.51226111
INFO:root:[   44] Training loss: 0.04935592, Validation loss: 0.03044840, Gradient norm: 1.40086793
INFO:root:[   45] Training loss: 0.04972362, Validation loss: 0.04502226, Gradient norm: 1.31429126
INFO:root:[   46] Training loss: 0.04838771, Validation loss: 0.03444585, Gradient norm: 1.41895856
INFO:root:[   47] Training loss: 0.04909592, Validation loss: 0.03155573, Gradient norm: 1.49305475
INFO:root:[   48] Training loss: 0.04872082, Validation loss: 0.03696773, Gradient norm: 1.51825954
INFO:root:[   49] Training loss: 0.04957805, Validation loss: 0.04105412, Gradient norm: 1.23175117
INFO:root:[   50] Training loss: 0.04815902, Validation loss: 0.03193259, Gradient norm: 1.20672715
INFO:root:[   51] Training loss: 0.04773602, Validation loss: 0.04181453, Gradient norm: 1.31435018
INFO:root:[   52] Training loss: 0.04809312, Validation loss: 0.03150358, Gradient norm: 1.51216322
INFO:root:[   53] Training loss: 0.04645493, Validation loss: 0.04298945, Gradient norm: 1.32709428
INFO:root:[   54] Training loss: 0.04680726, Validation loss: 0.02990832, Gradient norm: 1.35366802
INFO:root:[   55] Training loss: 0.04586029, Validation loss: 0.04512320, Gradient norm: 1.29369797
INFO:root:[   56] Training loss: 0.04592995, Validation loss: 0.03006086, Gradient norm: 1.42717730
INFO:root:[   57] Training loss: 0.04539948, Validation loss: 0.03253434, Gradient norm: 1.29784682
INFO:root:[   58] Training loss: 0.04531622, Validation loss: 0.03437873, Gradient norm: 1.57926574
INFO:root:[   59] Training loss: 0.04448936, Validation loss: 0.03071649, Gradient norm: 1.45853570
INFO:root:[   60] Training loss: 0.04441948, Validation loss: 0.03696367, Gradient norm: 1.47944418
INFO:root:[   61] Training loss: 0.04527885, Validation loss: 0.03388587, Gradient norm: 1.51287444
INFO:root:[   62] Training loss: 0.04495505, Validation loss: 0.03226520, Gradient norm: 1.48667373
INFO:root:[   63] Training loss: 0.04473216, Validation loss: 0.02783088, Gradient norm: 1.37222662
INFO:root:[   64] Training loss: 0.04427231, Validation loss: 0.04481420, Gradient norm: 1.58926134
INFO:root:[   65] Training loss: 0.04438814, Validation loss: 0.02888683, Gradient norm: 1.49571021
INFO:root:[   66] Training loss: 0.04329551, Validation loss: 0.02720724, Gradient norm: 1.40875038
INFO:root:[   67] Training loss: 0.04317177, Validation loss: 0.03943663, Gradient norm: 1.53044998
INFO:root:[   68] Training loss: 0.04531727, Validation loss: 0.03598378, Gradient norm: 1.45820339
INFO:root:[   69] Training loss: 0.04259202, Validation loss: 0.03994657, Gradient norm: 1.53304463
INFO:root:[   70] Training loss: 0.04295534, Validation loss: 0.04037713, Gradient norm: 1.34107917
INFO:root:[   71] Training loss: 0.04146378, Validation loss: 0.04211411, Gradient norm: 1.45954920
INFO:root:[   72] Training loss: 0.04181082, Validation loss: 0.04238675, Gradient norm: 1.54242412
INFO:root:[   73] Training loss: 0.04196315, Validation loss: 0.03786657, Gradient norm: 1.52269613
INFO:root:[   74] Training loss: 0.04142521, Validation loss: 0.04154531, Gradient norm: 1.51499383
INFO:root:[   75] Training loss: 0.04182652, Validation loss: 0.03693629, Gradient norm: 1.63946783
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 1430.513s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03867
INFO:root:EnergyScoreTrain: 0.02762
INFO:root:CoverageTrain: 0.85423
INFO:root:IntervalWidthTrain: 0.04378
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03815
INFO:root:EnergyScoreValidation: 0.02729
INFO:root:CoverageValidation: 0.85723
INFO:root:IntervalWidthValidation: 0.04362
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04544
INFO:root:EnergyScoreTest: 0.03208
INFO:root:CoverageTest: 0.8341
INFO:root:IntervalWidthTest: 0.0435
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.59768745, Validation loss: 0.27131948, Gradient norm: 6.37337590
INFO:root:[    2] Training loss: 0.37667639, Validation loss: 0.23137230, Gradient norm: 5.88575046
INFO:root:[    3] Training loss: 0.28429929, Validation loss: 0.15568536, Gradient norm: 3.82495193
INFO:root:[    4] Training loss: 0.24820519, Validation loss: 0.13481619, Gradient norm: 4.24217685
INFO:root:[    5] Training loss: 0.21328546, Validation loss: 0.13251370, Gradient norm: 3.77618247
INFO:root:[    6] Training loss: 0.19160491, Validation loss: 0.09733286, Gradient norm: 4.34567480
INFO:root:[    7] Training loss: 0.17273648, Validation loss: 0.10310686, Gradient norm: 3.50038664
INFO:root:[    8] Training loss: 0.15932021, Validation loss: 0.08070879, Gradient norm: 3.43315371
INFO:root:[    9] Training loss: 0.14911032, Validation loss: 0.09799353, Gradient norm: 2.87184360
INFO:root:[   10] Training loss: 0.14039925, Validation loss: 0.08563951, Gradient norm: 2.56108098
INFO:root:[   11] Training loss: 0.13632164, Validation loss: 0.07107143, Gradient norm: 2.63836372
INFO:root:[   12] Training loss: 0.12904272, Validation loss: 0.06980642, Gradient norm: 2.00611843
INFO:root:[   13] Training loss: 0.12529938, Validation loss: 0.06992482, Gradient norm: 2.10539963
INFO:root:[   14] Training loss: 0.12343212, Validation loss: 0.07127812, Gradient norm: 2.32751593
INFO:root:[   15] Training loss: 0.11962313, Validation loss: 0.07868966, Gradient norm: 2.10587934
INFO:root:[   16] Training loss: 0.11744011, Validation loss: 0.06939710, Gradient norm: 2.02584568
INFO:root:[   17] Training loss: 0.11378882, Validation loss: 0.06733994, Gradient norm: 1.59405482
INFO:root:[   18] Training loss: 0.11196775, Validation loss: 0.06310809, Gradient norm: 1.83014080
INFO:root:[   19] Training loss: 0.10873386, Validation loss: 0.06378564, Gradient norm: 1.55150684
INFO:root:[   20] Training loss: 0.10747543, Validation loss: 0.07103632, Gradient norm: 1.72618063
INFO:root:[   21] Training loss: 0.10605920, Validation loss: 0.07616216, Gradient norm: 1.36658163
INFO:root:[   22] Training loss: 0.10419286, Validation loss: 0.08397308, Gradient norm: 1.71929251
INFO:root:[   23] Training loss: 0.10241344, Validation loss: 0.08596964, Gradient norm: 1.40224498
INFO:root:[   24] Training loss: 0.10167875, Validation loss: 0.06720133, Gradient norm: 1.38745062
INFO:root:[   25] Training loss: 0.09923486, Validation loss: 0.08785502, Gradient norm: 1.70940320
INFO:root:[   26] Training loss: 0.09753480, Validation loss: 0.09221113, Gradient norm: 1.60993023
INFO:root:[   27] Training loss: 0.09634535, Validation loss: 0.06206004, Gradient norm: 1.68256364
INFO:root:[   28] Training loss: 0.09446688, Validation loss: 0.06333232, Gradient norm: 1.47130151
INFO:root:[   29] Training loss: 0.09283924, Validation loss: 0.05963227, Gradient norm: 1.50211292
INFO:root:[   30] Training loss: 0.09292859, Validation loss: 0.07247573, Gradient norm: 1.17717594
INFO:root:[   31] Training loss: 0.08971232, Validation loss: 0.05513100, Gradient norm: 1.61282407
INFO:root:[   32] Training loss: 0.08867535, Validation loss: 0.06032152, Gradient norm: 1.65370827
INFO:root:[   33] Training loss: 0.08765165, Validation loss: 0.09105548, Gradient norm: 1.79912751
INFO:root:[   34] Training loss: 0.08594014, Validation loss: 0.06212195, Gradient norm: 1.64776592
INFO:root:[   35] Training loss: 0.08558216, Validation loss: 0.06114915, Gradient norm: 1.39851528
INFO:root:[   36] Training loss: 0.08339893, Validation loss: 0.05105173, Gradient norm: 1.64447119
INFO:root:[   37] Training loss: 0.08257539, Validation loss: 0.06386881, Gradient norm: 1.72729966
INFO:root:[   38] Training loss: 0.08060469, Validation loss: 0.07803416, Gradient norm: 1.56590462
INFO:root:[   39] Training loss: 0.07912744, Validation loss: 0.08053347, Gradient norm: 1.73837817
INFO:root:[   40] Training loss: 0.07830567, Validation loss: 0.07749004, Gradient norm: 1.40772014
INFO:root:[   41] Training loss: 0.07713375, Validation loss: 0.06115138, Gradient norm: 1.56711115
INFO:root:[   42] Training loss: 0.07522003, Validation loss: 0.06398767, Gradient norm: 1.72470085
INFO:root:[   43] Training loss: 0.07390873, Validation loss: 0.08236979, Gradient norm: 1.63112152
INFO:root:[   44] Training loss: 0.07244106, Validation loss: 0.05126852, Gradient norm: 1.55514007
INFO:root:[   45] Training loss: 0.07163652, Validation loss: 0.05175553, Gradient norm: 1.77576585
INFO:root:[   46] Training loss: 0.07128582, Validation loss: 0.06750271, Gradient norm: 1.86179576
INFO:root:[   47] Training loss: 0.06957975, Validation loss: 0.07302968, Gradient norm: 1.74912627
INFO:root:[   48] Training loss: 0.06916273, Validation loss: 0.06207361, Gradient norm: 1.58281012
INFO:root:[   49] Training loss: 0.06791060, Validation loss: 0.05355165, Gradient norm: 1.63568223
INFO:root:[   50] Training loss: 0.06686868, Validation loss: 0.05282932, Gradient norm: 1.68748549
INFO:root:[   51] Training loss: 0.06490475, Validation loss: 0.04637519, Gradient norm: 1.58089308
INFO:root:[   52] Training loss: 0.06402824, Validation loss: 0.04221278, Gradient norm: 1.72212299
INFO:root:[   53] Training loss: 0.06260039, Validation loss: 0.04355065, Gradient norm: 1.83103187
INFO:root:[   54] Training loss: 0.06247005, Validation loss: 0.03969876, Gradient norm: 1.71240638
INFO:root:[   55] Training loss: 0.06109616, Validation loss: 0.03825176, Gradient norm: 1.77307597
INFO:root:[   56] Training loss: 0.06128662, Validation loss: 0.04310758, Gradient norm: 1.74201660
INFO:root:[   57] Training loss: 0.05918608, Validation loss: 0.06726037, Gradient norm: 1.48324240
INFO:root:[   58] Training loss: 0.05937697, Validation loss: 0.06376257, Gradient norm: 1.82131574
INFO:root:[   59] Training loss: 0.05706472, Validation loss: 0.06458897, Gradient norm: 1.67668049
INFO:root:[   60] Training loss: 0.05719305, Validation loss: 0.03562723, Gradient norm: 1.55780844
INFO:root:[   61] Training loss: 0.05578133, Validation loss: 0.04703404, Gradient norm: 1.68250846
INFO:root:[   62] Training loss: 0.05402897, Validation loss: 0.05864074, Gradient norm: 1.70258417
INFO:root:[   63] Training loss: 0.05361252, Validation loss: 0.04615926, Gradient norm: 1.74841325
INFO:root:[   64] Training loss: 0.05342492, Validation loss: 0.03476596, Gradient norm: 1.75793832
INFO:root:[   65] Training loss: 0.05314992, Validation loss: 0.03945113, Gradient norm: 1.75676398
INFO:root:[   66] Training loss: 0.05192081, Validation loss: 0.05284400, Gradient norm: 1.69524642
INFO:root:[   67] Training loss: 0.05176572, Validation loss: 0.04047173, Gradient norm: 1.79989321
INFO:root:[   68] Training loss: 0.04998947, Validation loss: 0.04148357, Gradient norm: 1.83248713
INFO:root:[   69] Training loss: 0.04833403, Validation loss: 0.03142173, Gradient norm: 1.71596702
INFO:root:[   70] Training loss: 0.04935356, Validation loss: 0.03179332, Gradient norm: 1.71783215
INFO:root:[   71] Training loss: 0.04853468, Validation loss: 0.03110804, Gradient norm: 2.02377347
INFO:root:[   72] Training loss: 0.05040348, Validation loss: 0.05067330, Gradient norm: 2.04312661
INFO:root:[   73] Training loss: 0.04701588, Validation loss: 0.03420943, Gradient norm: 1.70957296
INFO:root:[   74] Training loss: 0.04601703, Validation loss: 0.04407151, Gradient norm: 1.61164867
INFO:root:[   75] Training loss: 0.04574837, Validation loss: 0.03642678, Gradient norm: 1.86146844
INFO:root:[   76] Training loss: 0.04560115, Validation loss: 0.04324603, Gradient norm: 1.71888914
INFO:root:[   77] Training loss: 0.04473671, Validation loss: 0.04715667, Gradient norm: 1.59901960
INFO:root:[   78] Training loss: 0.04472976, Validation loss: 0.03625097, Gradient norm: 1.71525838
INFO:root:[   79] Training loss: 0.04301964, Validation loss: 0.03212813, Gradient norm: 1.78012090
INFO:root:[   80] Training loss: 0.04429870, Validation loss: 0.04398359, Gradient norm: 1.95085718
INFO:root:[   81] Training loss: 0.04344542, Validation loss: 0.02869748, Gradient norm: 1.90209362
INFO:root:[   82] Training loss: 0.04342133, Validation loss: 0.03364502, Gradient norm: 1.78815864
INFO:root:[   83] Training loss: 0.04358001, Validation loss: 0.04356999, Gradient norm: 1.85227745
INFO:root:[   84] Training loss: 0.04175287, Validation loss: 0.03327597, Gradient norm: 1.63694080
INFO:root:[   85] Training loss: 0.04075156, Validation loss: 0.02790911, Gradient norm: 1.75817047
INFO:root:[   86] Training loss: 0.04016644, Validation loss: 0.02995398, Gradient norm: 1.56846044
INFO:root:[   87] Training loss: 0.04096711, Validation loss: 0.03146641, Gradient norm: 1.63483346
INFO:root:[   88] Training loss: 0.04125645, Validation loss: 0.03049802, Gradient norm: 1.69192701
INFO:root:[   89] Training loss: 0.03927185, Validation loss: 0.02757227, Gradient norm: 1.85684100
INFO:root:[   90] Training loss: 0.03949673, Validation loss: 0.02668659, Gradient norm: 1.91679087
INFO:root:[   91] Training loss: 0.03924158, Validation loss: 0.03680009, Gradient norm: 1.77553778
INFO:root:[   92] Training loss: 0.03924735, Validation loss: 0.03417550, Gradient norm: 1.82347518
INFO:root:[   93] Training loss: 0.03851857, Validation loss: 0.03093648, Gradient norm: 1.85869752
INFO:root:[   94] Training loss: 0.03807476, Validation loss: 0.03145950, Gradient norm: 1.68821878
INFO:root:[   95] Training loss: 0.03780426, Validation loss: 0.03326743, Gradient norm: 1.71994428
INFO:root:[   96] Training loss: 0.03871677, Validation loss: 0.02530960, Gradient norm: 1.95745200
INFO:root:[   97] Training loss: 0.03837990, Validation loss: 0.03242110, Gradient norm: 1.74420351
INFO:root:[   98] Training loss: 0.03718629, Validation loss: 0.02731496, Gradient norm: 1.64273072
INFO:root:[   99] Training loss: 0.03731832, Validation loss: 0.02521058, Gradient norm: 2.08187278
INFO:root:[  100] Training loss: 0.03682219, Validation loss: 0.03519803, Gradient norm: 1.84551969
INFO:root:[  101] Training loss: 0.03701170, Validation loss: 0.03217447, Gradient norm: 1.75181381
INFO:root:[  102] Training loss: 0.03627746, Validation loss: 0.02668634, Gradient norm: 2.00509425
INFO:root:[  103] Training loss: 0.03661074, Validation loss: 0.02652210, Gradient norm: 2.13619035
INFO:root:[  104] Training loss: 0.03517066, Validation loss: 0.02674545, Gradient norm: 1.91098016
INFO:root:[  105] Training loss: 0.03514226, Validation loss: 0.03313783, Gradient norm: 2.06413272
INFO:root:[  106] Training loss: 0.03542594, Validation loss: 0.02791690, Gradient norm: 2.00762798
INFO:root:[  107] Training loss: 0.03516997, Validation loss: 0.02536242, Gradient norm: 2.14076904
INFO:root:[  108] Training loss: 0.03503099, Validation loss: 0.02946542, Gradient norm: 1.93805879
INFO:root:EP 108: Early stopping
INFO:root:Training the model took 2051.016s.
INFO:root:Emptying the cuda cache took 0.043s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03522
INFO:root:EnergyScoreTrain: 0.03422
INFO:root:CoverageTrain: 0.31745
INFO:root:IntervalWidthTrain: 0.01416
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03492
INFO:root:EnergyScoreValidation: 0.03448
INFO:root:CoverageValidation: 0.3206
INFO:root:IntervalWidthValidation: 0.01403
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04116
INFO:root:EnergyScoreTest: 0.03237
INFO:root:CoverageTest: 0.28846
INFO:root:IntervalWidthTest: 0.01404
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 473956352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.69502359, Validation loss: 0.21557130, Gradient norm: 5.49985572
INFO:root:[    2] Training loss: 0.37337839, Validation loss: 0.20022387, Gradient norm: 4.38190694
INFO:root:[    3] Training loss: 0.30192705, Validation loss: 0.28193287, Gradient norm: 3.57145392
INFO:root:[    4] Training loss: 0.24184128, Validation loss: 0.17700375, Gradient norm: 2.11496231
INFO:root:[    5] Training loss: 0.20751761, Validation loss: 0.17441971, Gradient norm: 1.52981895
INFO:root:[    6] Training loss: 0.18416167, Validation loss: 0.18329259, Gradient norm: 0.93806748
INFO:root:[    7] Training loss: 0.17080165, Validation loss: 0.20096968, Gradient norm: 1.21529500
INFO:root:[    8] Training loss: 0.16077999, Validation loss: 0.22781441, Gradient norm: 1.01695147
INFO:root:[    9] Training loss: 0.15409344, Validation loss: 0.19472270, Gradient norm: 0.68684721
INFO:root:[   10] Training loss: 0.15060751, Validation loss: 0.26889984, Gradient norm: 0.86975232
INFO:root:[   11] Training loss: 0.14386075, Validation loss: 0.21325989, Gradient norm: 0.56086667
INFO:root:[   12] Training loss: 0.14025580, Validation loss: 0.25122409, Gradient norm: 0.76535538
INFO:root:[   13] Training loss: 0.13533463, Validation loss: 0.25368145, Gradient norm: 0.62901116
INFO:root:[   14] Training loss: 0.13234860, Validation loss: 0.26231678, Gradient norm: 0.70869154
INFO:root:[   15] Training loss: 0.12910855, Validation loss: 0.26372531, Gradient norm: 0.83789403
INFO:root:[   16] Training loss: 0.12495792, Validation loss: 0.23580365, Gradient norm: 0.86922437
INFO:root:[   17] Training loss: 0.12056764, Validation loss: 0.25258284, Gradient norm: 0.78848562
INFO:root:[   18] Training loss: 0.11765923, Validation loss: 0.23653723, Gradient norm: 0.98330525
INFO:root:[   19] Training loss: 0.11512465, Validation loss: 0.23257756, Gradient norm: 1.06784245
INFO:root:[   20] Training loss: 0.11136971, Validation loss: 0.21338340, Gradient norm: 0.91629054
INFO:root:[   21] Training loss: 0.10798631, Validation loss: 0.23023904, Gradient norm: 0.95635727
INFO:root:[   22] Training loss: 0.10402183, Validation loss: 0.19447168, Gradient norm: 0.98277173
INFO:root:[   23] Training loss: 0.10139519, Validation loss: 0.17550219, Gradient norm: 0.98967088
INFO:root:[   24] Training loss: 0.09935358, Validation loss: 0.16108289, Gradient norm: 0.92872603
INFO:root:[   25] Training loss: 0.09631065, Validation loss: 0.15002819, Gradient norm: 0.73123257
INFO:root:[   26] Training loss: 0.09257447, Validation loss: 0.18132415, Gradient norm: 1.02595519
INFO:root:[   27] Training loss: 0.09000395, Validation loss: 0.18028755, Gradient norm: 1.05551766
INFO:root:[   28] Training loss: 0.08898916, Validation loss: 0.12594567, Gradient norm: 0.87115413
INFO:root:[   29] Training loss: 0.08665426, Validation loss: 0.12448755, Gradient norm: 1.13771778
INFO:root:[   30] Training loss: 0.08328293, Validation loss: 0.14731640, Gradient norm: 0.94147399
INFO:root:[   31] Training loss: 0.07959081, Validation loss: 0.16418011, Gradient norm: 1.12261698
INFO:root:[   32] Training loss: 0.07887436, Validation loss: 0.13715874, Gradient norm: 0.76202805
INFO:root:[   33] Training loss: 0.07622903, Validation loss: 0.14462021, Gradient norm: 0.96982196
INFO:root:[   34] Training loss: 0.07432679, Validation loss: 0.10971798, Gradient norm: 0.91270920
INFO:root:[   35] Training loss: 0.07103239, Validation loss: 0.13017740, Gradient norm: 1.26523900
INFO:root:[   36] Training loss: 0.07252414, Validation loss: 0.13684069, Gradient norm: 1.21418384
INFO:root:[   37] Training loss: 0.06981614, Validation loss: 0.11087001, Gradient norm: 0.59671535
INFO:root:[   38] Training loss: 0.06790871, Validation loss: 0.07260397, Gradient norm: 1.15786383
INFO:root:[   39] Training loss: 0.06397212, Validation loss: 0.08715498, Gradient norm: 1.01702204
INFO:root:[   40] Training loss: 0.06264927, Validation loss: 0.11300015, Gradient norm: 1.08092830
INFO:root:[   41] Training loss: 0.06148409, Validation loss: 0.08555412, Gradient norm: 1.08794271
INFO:root:[   42] Training loss: 0.06085796, Validation loss: 0.06573536, Gradient norm: 1.10563051
INFO:root:[   43] Training loss: 0.05764684, Validation loss: 0.07244303, Gradient norm: 1.02597101
INFO:root:[   44] Training loss: 0.05548320, Validation loss: 0.06150753, Gradient norm: 1.08033811
INFO:root:[   45] Training loss: 0.05624019, Validation loss: 0.06328642, Gradient norm: 1.30655889
INFO:root:[   46] Training loss: 0.05551963, Validation loss: 0.07360153, Gradient norm: 1.28510998
INFO:root:[   47] Training loss: 0.05265664, Validation loss: 0.04882958, Gradient norm: 1.10497654
INFO:root:[   48] Training loss: 0.05171153, Validation loss: 0.06079123, Gradient norm: 1.31437196
INFO:root:[   49] Training loss: 0.04948171, Validation loss: 0.07054807, Gradient norm: 1.17921795
INFO:root:[   50] Training loss: 0.04952264, Validation loss: 0.05967691, Gradient norm: 1.27072219
INFO:root:[   51] Training loss: 0.05009152, Validation loss: 0.07762159, Gradient norm: 1.44015290
INFO:root:[   52] Training loss: 0.04850308, Validation loss: 0.05003015, Gradient norm: 1.43233449
INFO:root:[   53] Training loss: 0.04812358, Validation loss: 0.04493652, Gradient norm: 1.25750305
INFO:root:[   54] Training loss: 0.04709996, Validation loss: 0.04598028, Gradient norm: 1.35339212
INFO:root:[   55] Training loss: 0.04465285, Validation loss: 0.04876702, Gradient norm: 1.36316941
INFO:root:[   56] Training loss: 0.04713719, Validation loss: 0.04273078, Gradient norm: 1.37007533
INFO:root:[   57] Training loss: 0.04582555, Validation loss: 0.05617796, Gradient norm: 1.39683220
INFO:root:[   58] Training loss: 0.04510983, Validation loss: 0.04077972, Gradient norm: 1.35079479
INFO:root:[   59] Training loss: 0.04396059, Validation loss: 0.06912676, Gradient norm: 1.45836457
INFO:root:[   60] Training loss: 0.04687663, Validation loss: 0.05089549, Gradient norm: 1.65394858
INFO:root:[   61] Training loss: 0.04483864, Validation loss: 0.05833392, Gradient norm: 1.46774723
INFO:root:[   62] Training loss: 0.04440442, Validation loss: 0.04659772, Gradient norm: 1.47287870
INFO:root:[   63] Training loss: 0.04266070, Validation loss: 0.05715762, Gradient norm: 1.35193008
INFO:root:[   64] Training loss: 0.04300754, Validation loss: 0.05511692, Gradient norm: 1.35753019
INFO:root:[   65] Training loss: 0.04380853, Validation loss: 0.05131345, Gradient norm: 1.67996695
INFO:root:[   66] Training loss: 0.04160215, Validation loss: 0.04398656, Gradient norm: 1.50398706
INFO:root:[   67] Training loss: 0.04347055, Validation loss: 0.03673750, Gradient norm: 1.46965936
INFO:root:[   68] Training loss: 0.04311819, Validation loss: 0.03662258, Gradient norm: 1.48986012
INFO:root:[   69] Training loss: 0.04381112, Validation loss: 0.05919697, Gradient norm: 1.64581594
INFO:root:[   70] Training loss: 0.04222143, Validation loss: 0.05054323, Gradient norm: 1.55768053
INFO:root:[   71] Training loss: 0.04117793, Validation loss: 0.05516060, Gradient norm: 1.59066854
INFO:root:[   72] Training loss: 0.04170237, Validation loss: 0.04234126, Gradient norm: 1.60129313
INFO:root:[   73] Training loss: 0.04203207, Validation loss: 0.07025420, Gradient norm: 1.83601229
INFO:root:[   74] Training loss: 0.04318955, Validation loss: 0.03957550, Gradient norm: 1.94287825
INFO:root:[   75] Training loss: 0.04199387, Validation loss: 0.04978968, Gradient norm: 1.60102423
INFO:root:[   76] Training loss: 0.04132548, Validation loss: 0.03673389, Gradient norm: 1.59044467
INFO:root:[   77] Training loss: 0.04128582, Validation loss: 0.04142129, Gradient norm: 2.00064045
INFO:root:EP 77: Early stopping
INFO:root:Training the model took 1468.352s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04894
INFO:root:EnergyScoreTrain: 0.03075
INFO:root:CoverageTrain: 0.03974
INFO:root:IntervalWidthTrain: 0.00581
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0485
INFO:root:EnergyScoreValidation: 0.03116
INFO:root:CoverageValidation: 0.03928
INFO:root:IntervalWidthValidation: 0.00564
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0562
INFO:root:EnergyScoreTest: 0.04114
INFO:root:CoverageTest: 0.03797
INFO:root:IntervalWidthTest: 0.00633
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.74136553, Validation loss: 0.25340000, Gradient norm: 5.32154526
INFO:root:[    2] Training loss: 0.44265561, Validation loss: 0.21179821, Gradient norm: 5.49038610
INFO:root:[    3] Training loss: 0.35149344, Validation loss: 0.25390017, Gradient norm: 4.19226275
INFO:root:[    4] Training loss: 0.29005983, Validation loss: 0.33000892, Gradient norm: 3.46623975
INFO:root:[    5] Training loss: 0.26001307, Validation loss: 0.17024402, Gradient norm: 3.37335328
INFO:root:[    6] Training loss: 0.23591925, Validation loss: 0.14063651, Gradient norm: 2.83294219
INFO:root:[    7] Training loss: 0.21940165, Validation loss: 0.29313131, Gradient norm: 2.38516870
INFO:root:[    8] Training loss: 0.20187525, Validation loss: 0.30402125, Gradient norm: 1.53221007
INFO:root:[    9] Training loss: 0.19044431, Validation loss: 0.27909731, Gradient norm: 1.63386737
INFO:root:[   10] Training loss: 0.18204238, Validation loss: 0.38217495, Gradient norm: 1.59382604
INFO:root:[   11] Training loss: 0.17549890, Validation loss: 0.31243757, Gradient norm: 1.81292487
INFO:root:[   12] Training loss: 0.16786761, Validation loss: 0.42360660, Gradient norm: 1.80911642
INFO:root:[   13] Training loss: 0.16186301, Validation loss: 0.35061428, Gradient norm: 1.90379081
INFO:root:[   14] Training loss: 0.15439195, Validation loss: 0.35545386, Gradient norm: 1.43524547
INFO:root:[   15] Training loss: 0.14878459, Validation loss: 0.31101600, Gradient norm: 1.52751039
INFO:root:[   16] Training loss: 0.14305896, Validation loss: 0.26996498, Gradient norm: 1.90589113
INFO:root:[   17] Training loss: 0.13804905, Validation loss: 0.27634759, Gradient norm: 2.03057697
INFO:root:[   18] Training loss: 0.13397340, Validation loss: 0.31761563, Gradient norm: 1.42091581
INFO:root:[   19] Training loss: 0.12883147, Validation loss: 0.27526088, Gradient norm: 2.25206424
INFO:root:[   20] Training loss: 0.12269859, Validation loss: 0.29386804, Gradient norm: 1.59519675
INFO:root:[   21] Training loss: 0.11891048, Validation loss: 0.23810940, Gradient norm: 2.08693893
INFO:root:[   22] Training loss: 0.11318742, Validation loss: 0.27295747, Gradient norm: 2.09844094
INFO:root:[   23] Training loss: 0.10841069, Validation loss: 0.26894532, Gradient norm: 2.73375321
INFO:root:[   24] Training loss: 0.10552590, Validation loss: 0.22481753, Gradient norm: 2.66905085
INFO:root:[   25] Training loss: 0.10235685, Validation loss: 0.26074810, Gradient norm: 2.84451251
INFO:root:[   26] Training loss: 0.09901132, Validation loss: 0.22825392, Gradient norm: 2.68856812
INFO:root:[   27] Training loss: 0.09616663, Validation loss: 0.23435987, Gradient norm: 1.93017420
INFO:root:[   28] Training loss: 0.08992292, Validation loss: 0.24474614, Gradient norm: 3.20800349
INFO:root:[   29] Training loss: 0.08676014, Validation loss: 0.21284310, Gradient norm: 3.04542492
INFO:root:[   30] Training loss: 0.08262050, Validation loss: 0.14597095, Gradient norm: 3.37352989
INFO:root:[   31] Training loss: 0.08018805, Validation loss: 0.12278922, Gradient norm: 3.37462100
INFO:root:[   32] Training loss: 0.07963704, Validation loss: 0.17165511, Gradient norm: 3.21529392
INFO:root:[   33] Training loss: 0.07670346, Validation loss: 0.16056405, Gradient norm: 2.83241955
INFO:root:[   34] Training loss: 0.07301252, Validation loss: 0.14963064, Gradient norm: 2.96649718
INFO:root:[   35] Training loss: 0.06992952, Validation loss: 0.15382405, Gradient norm: 3.38212601
INFO:root:[   36] Training loss: 0.06837299, Validation loss: 0.14399709, Gradient norm: 2.88001813
INFO:root:[   37] Training loss: 0.06753158, Validation loss: 0.16221598, Gradient norm: 2.99255340
INFO:root:[   38] Training loss: 0.06695778, Validation loss: 0.12596089, Gradient norm: 3.00556517
INFO:root:[   39] Training loss: 0.06152293, Validation loss: 0.09109824, Gradient norm: 2.47831635
INFO:root:[   40] Training loss: 0.06101622, Validation loss: 0.07396442, Gradient norm: 2.14808529
INFO:root:[   41] Training loss: 0.06004217, Validation loss: 0.11292164, Gradient norm: 3.00139342
INFO:root:[   42] Training loss: 0.05851553, Validation loss: 0.07485175, Gradient norm: 2.96460928
INFO:root:[   43] Training loss: 0.05646037, Validation loss: 0.06493653, Gradient norm: 2.56457255
INFO:root:[   44] Training loss: 0.05568673, Validation loss: 0.06804370, Gradient norm: 2.67561053
INFO:root:[   45] Training loss: 0.05489062, Validation loss: 0.06984805, Gradient norm: 2.83099808
INFO:root:[   46] Training loss: 0.05347827, Validation loss: 0.06791862, Gradient norm: 2.97665950
INFO:root:[   47] Training loss: 0.05313066, Validation loss: 0.05824361, Gradient norm: 3.05682966
INFO:root:[   48] Training loss: 0.05499351, Validation loss: 0.06900864, Gradient norm: 2.74166706
INFO:root:[   49] Training loss: 0.05339331, Validation loss: 0.06219351, Gradient norm: 2.88014618
INFO:root:[   50] Training loss: 0.05112965, Validation loss: 0.05847853, Gradient norm: 3.02585029
INFO:root:[   51] Training loss: 0.05008086, Validation loss: 0.04883465, Gradient norm: 2.84031551
INFO:root:[   52] Training loss: 0.04962513, Validation loss: 0.04700939, Gradient norm: 3.10205337
INFO:root:[   53] Training loss: 0.04951391, Validation loss: 0.05967028, Gradient norm: 2.68626659
INFO:root:[   54] Training loss: 0.05069671, Validation loss: 0.04729274, Gradient norm: 2.95410982
INFO:root:[   55] Training loss: 0.05051898, Validation loss: 0.07181465, Gradient norm: 3.00807118
INFO:root:[   56] Training loss: 0.05003275, Validation loss: 0.06667317, Gradient norm: 2.98479855
INFO:root:[   57] Training loss: 0.05033122, Validation loss: 0.04501503, Gradient norm: 2.62555457
INFO:root:[   58] Training loss: 0.04896321, Validation loss: 0.05893511, Gradient norm: 3.28426938
INFO:root:[   59] Training loss: 0.04763694, Validation loss: 0.06936783, Gradient norm: 3.35633626
INFO:root:[   60] Training loss: 0.04767723, Validation loss: 0.04544339, Gradient norm: 3.47614460
INFO:root:[   61] Training loss: 0.04994347, Validation loss: 0.07308168, Gradient norm: 3.08876973
INFO:root:[   62] Training loss: 0.04889428, Validation loss: 0.05539753, Gradient norm: 2.42295101
INFO:root:[   63] Training loss: 0.04902088, Validation loss: 0.05979309, Gradient norm: 3.15750033
INFO:root:[   64] Training loss: 0.04725096, Validation loss: 0.03964863, Gradient norm: 2.94386592
INFO:root:[   65] Training loss: 0.04794840, Validation loss: 0.05963889, Gradient norm: 2.59947208
INFO:root:[   66] Training loss: 0.04871067, Validation loss: 0.04627805, Gradient norm: 2.63422763
INFO:root:[   67] Training loss: 0.04620604, Validation loss: 0.06613625, Gradient norm: 2.80344358
INFO:root:[   68] Training loss: 0.04673784, Validation loss: 0.07055354, Gradient norm: 2.79665628
INFO:root:[   69] Training loss: 0.04847937, Validation loss: 0.05630718, Gradient norm: 2.59217964
INFO:root:[   70] Training loss: 0.04694338, Validation loss: 0.03767408, Gradient norm: 2.94372356
INFO:root:[   71] Training loss: 0.04614103, Validation loss: 0.04980895, Gradient norm: 3.08044755
INFO:root:[   72] Training loss: 0.04704330, Validation loss: 0.06119955, Gradient norm: 2.68449184
INFO:root:[   73] Training loss: 0.04728879, Validation loss: 0.06659824, Gradient norm: 2.88551212
INFO:root:[   74] Training loss: 0.04669645, Validation loss: 0.05464663, Gradient norm: 2.88794646
INFO:root:[   75] Training loss: 0.04548338, Validation loss: 0.05335486, Gradient norm: 2.70697949
INFO:root:[   76] Training loss: 0.04527526, Validation loss: 0.06197615, Gradient norm: 2.90844605
INFO:root:[   77] Training loss: 0.04463724, Validation loss: 0.04424574, Gradient norm: 2.73371102
INFO:root:[   78] Training loss: 0.04623107, Validation loss: 0.05854280, Gradient norm: 3.13544476
INFO:root:[   79] Training loss: 0.04591176, Validation loss: 0.05669770, Gradient norm: 2.77084033
INFO:root:EP 79: Early stopping
INFO:root:Training the model took 1514.069s.
INFO:root:Emptying the cuda cache took 0.042s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0447
INFO:root:EnergyScoreTrain: 0.01757
INFO:root:CoverageTrain: 0.01744
INFO:root:IntervalWidthTrain: 0.00172
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04446
INFO:root:EnergyScoreValidation: 0.01807
INFO:root:CoverageValidation: 0.01699
INFO:root:IntervalWidthValidation: 0.00168
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.05318
INFO:root:EnergyScoreTest: 0.03153
INFO:root:CoverageTest: 0.01572
INFO:root:IntervalWidthTest: 0.00171
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.67393814, Validation loss: 0.21731068, Gradient norm: 3.72757088
INFO:root:[    2] Training loss: 0.40202880, Validation loss: 0.15590212, Gradient norm: 2.49100240
INFO:root:[    3] Training loss: 0.31120401, Validation loss: 0.16271869, Gradient norm: 2.64642931
INFO:root:[    4] Training loss: 0.25331435, Validation loss: 0.26010939, Gradient norm: 1.58815902
INFO:root:[    5] Training loss: 0.22654253, Validation loss: 0.19300805, Gradient norm: 1.60391829
INFO:root:[    6] Training loss: 0.20812813, Validation loss: 0.22941833, Gradient norm: 1.49222960
INFO:root:[    7] Training loss: 0.19469600, Validation loss: 0.31100103, Gradient norm: 1.32648174
INFO:root:[    8] Training loss: 0.18370944, Validation loss: 0.28637213, Gradient norm: 1.09709688
INFO:root:[    9] Training loss: 0.17217374, Validation loss: 0.26824178, Gradient norm: 0.83595135
INFO:root:[   10] Training loss: 0.16322362, Validation loss: 0.35457330, Gradient norm: 1.06637608
INFO:root:[   11] Training loss: 0.15504179, Validation loss: 0.29454744, Gradient norm: 1.07412926
INFO:root:[   12] Training loss: 0.14716012, Validation loss: 0.27640418, Gradient norm: 0.90800238
INFO:root:[   13] Training loss: 0.13842187, Validation loss: 0.33543639, Gradient norm: 1.00126268
INFO:root:[   14] Training loss: 0.13111640, Validation loss: 0.33677687, Gradient norm: 1.09549571
INFO:root:[   15] Training loss: 0.12491537, Validation loss: 0.29056093, Gradient norm: 0.91823160
INFO:root:[   16] Training loss: 0.11841714, Validation loss: 0.21968672, Gradient norm: 1.28033404
INFO:root:[   17] Training loss: 0.11150445, Validation loss: 0.27413940, Gradient norm: 1.13213061
INFO:root:[   18] Training loss: 0.10514495, Validation loss: 0.23680058, Gradient norm: 1.33447095
INFO:root:[   19] Training loss: 0.10068620, Validation loss: 0.19765027, Gradient norm: 1.23874138
INFO:root:[   20] Training loss: 0.09424139, Validation loss: 0.22507585, Gradient norm: 1.25786957
INFO:root:[   21] Training loss: 0.08835693, Validation loss: 0.20136183, Gradient norm: 1.07928744
INFO:root:[   22] Training loss: 0.08429593, Validation loss: 0.19870189, Gradient norm: 1.42689490
INFO:root:[   23] Training loss: 0.08100071, Validation loss: 0.19410321, Gradient norm: 1.19442630
INFO:root:[   24] Training loss: 0.07648266, Validation loss: 0.17945972, Gradient norm: 1.47544746
INFO:root:[   25] Training loss: 0.07352696, Validation loss: 0.14506733, Gradient norm: 1.55735924
INFO:root:[   26] Training loss: 0.07161822, Validation loss: 0.16791782, Gradient norm: 1.52850180
INFO:root:[   27] Training loss: 0.06995855, Validation loss: 0.15430334, Gradient norm: 1.82050339
INFO:root:[   28] Training loss: 0.06586996, Validation loss: 0.11474302, Gradient norm: 1.35541615
INFO:root:[   29] Training loss: 0.06443388, Validation loss: 0.11113481, Gradient norm: 1.45341480
INFO:root:[   30] Training loss: 0.06377434, Validation loss: 0.09372509, Gradient norm: 1.93380282
INFO:root:[   31] Training loss: 0.06133642, Validation loss: 0.08874527, Gradient norm: 1.57576996
INFO:root:[   32] Training loss: 0.05915786, Validation loss: 0.07741299, Gradient norm: 1.60519118
INFO:root:[   33] Training loss: 0.05735457, Validation loss: 0.09307043, Gradient norm: 1.82980191
INFO:root:[   34] Training loss: 0.05771245, Validation loss: 0.06753955, Gradient norm: 1.93060222
INFO:root:[   35] Training loss: 0.05684292, Validation loss: 0.07430866, Gradient norm: 1.78579952
INFO:root:[   36] Training loss: 0.05667048, Validation loss: 0.08274238, Gradient norm: 1.68720255
INFO:root:[   37] Training loss: 0.05922329, Validation loss: 0.08494599, Gradient norm: 2.29441231
INFO:root:[   38] Training loss: 0.05742146, Validation loss: 0.07718701, Gradient norm: 1.69548595
INFO:root:[   39] Training loss: 0.05749146, Validation loss: 0.06194222, Gradient norm: 1.95082069
INFO:root:[   40] Training loss: 0.05609094, Validation loss: 0.06608416, Gradient norm: 1.84691577
INFO:root:[   41] Training loss: 0.05528801, Validation loss: 0.06930545, Gradient norm: 1.71115832
INFO:root:[   42] Training loss: 0.05519577, Validation loss: 0.07479244, Gradient norm: 1.72161600
INFO:root:[   43] Training loss: 0.05464959, Validation loss: 0.05797247, Gradient norm: 2.11443783
INFO:root:[   44] Training loss: 0.05416410, Validation loss: 0.07851492, Gradient norm: 2.16955946
INFO:root:[   45] Training loss: 0.05476949, Validation loss: 0.07525662, Gradient norm: 2.20391038
INFO:root:[   46] Training loss: 0.05439350, Validation loss: 0.06787037, Gradient norm: 1.84744087
INFO:root:[   47] Training loss: 0.05511047, Validation loss: 0.06650019, Gradient norm: 2.35726965
INFO:root:[   48] Training loss: 0.05560891, Validation loss: 0.08195178, Gradient norm: 2.00271455
INFO:root:[   49] Training loss: 0.05353299, Validation loss: 0.06177514, Gradient norm: 1.79458738
INFO:root:[   50] Training loss: 0.05419433, Validation loss: 0.08320580, Gradient norm: 1.82293955
INFO:root:[   51] Training loss: 0.05305825, Validation loss: 0.06039849, Gradient norm: 1.94941183
INFO:root:[   52] Training loss: 0.05376182, Validation loss: 0.05689965, Gradient norm: 2.24567032
INFO:root:[   53] Training loss: 0.05328394, Validation loss: 0.08726957, Gradient norm: 1.94201593
INFO:root:[   54] Training loss: 0.05370084, Validation loss: 0.05662371, Gradient norm: 1.82949283
INFO:root:[   55] Training loss: 0.05254197, Validation loss: 0.07440144, Gradient norm: 2.39966137
INFO:root:[   56] Training loss: 0.05259051, Validation loss: 0.06337364, Gradient norm: 2.05200603
INFO:root:[   57] Training loss: 0.05262902, Validation loss: 0.07031584, Gradient norm: 1.86171238
INFO:root:[   58] Training loss: 0.05236102, Validation loss: 0.05801587, Gradient norm: 1.94481901
INFO:root:[   59] Training loss: 0.05291301, Validation loss: 0.05850806, Gradient norm: 2.07499040
INFO:root:[   60] Training loss: 0.05177104, Validation loss: 0.06108687, Gradient norm: 1.84692275
INFO:root:[   61] Training loss: 0.05033371, Validation loss: 0.09399171, Gradient norm: 1.85064684
INFO:root:[   62] Training loss: 0.05227849, Validation loss: 0.07543742, Gradient norm: 2.00938390
INFO:root:[   63] Training loss: 0.05136293, Validation loss: 0.07384411, Gradient norm: 1.82340978
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 1205.031s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05676
INFO:root:EnergyScoreTrain: 0.03442
INFO:root:CoverageTrain: 0.00025
INFO:root:IntervalWidthTrain: 1e-05
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05666
INFO:root:EnergyScoreValidation: 0.0347
INFO:root:CoverageValidation: 0.00025
INFO:root:IntervalWidthValidation: 1e-05
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06598
INFO:root:EnergyScoreTest: 0.04569
INFO:root:CoverageTest: 0.00036
INFO:root:IntervalWidthTest: 2e-05
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14758546
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.81008243, Validation loss: 0.51506674, Gradient norm: 3.28732296
INFO:root:[    2] Training loss: 0.48163742, Validation loss: 0.23355356, Gradient norm: 3.45503663
INFO:root:[    3] Training loss: 0.35850165, Validation loss: 0.15046777, Gradient norm: 2.43420952
INFO:root:[    4] Training loss: 0.29993030, Validation loss: 0.19178039, Gradient norm: 2.24680503
INFO:root:[    5] Training loss: 0.26907528, Validation loss: 0.14244629, Gradient norm: 2.52670123
INFO:root:[    6] Training loss: 0.24431528, Validation loss: 0.39141001, Gradient norm: 2.08046842
INFO:root:[    7] Training loss: 0.22789824, Validation loss: 0.31459939, Gradient norm: 2.26976008
INFO:root:[    8] Training loss: 0.20931206, Validation loss: 0.39142807, Gradient norm: 1.27923188
INFO:root:[    9] Training loss: 0.19813832, Validation loss: 0.37097328, Gradient norm: 2.10232694
INFO:root:[   10] Training loss: 0.18719172, Validation loss: 0.45959258, Gradient norm: 2.35280517
INFO:root:[   11] Training loss: 0.17696216, Validation loss: 0.41571530, Gradient norm: 2.78427923
INFO:root:[   12] Training loss: 0.16601002, Validation loss: 0.38173324, Gradient norm: 2.34476579
INFO:root:[   13] Training loss: 0.15733659, Validation loss: 0.38743494, Gradient norm: 3.16015983
INFO:root:[   14] Training loss: 0.14859806, Validation loss: 0.31756413, Gradient norm: 3.06417436
INFO:root:[   15] Training loss: 0.14168160, Validation loss: 0.33544466, Gradient norm: 3.00034293
INFO:root:[   16] Training loss: 0.12985339, Validation loss: 0.32810883, Gradient norm: 2.06282677
INFO:root:[   17] Training loss: 0.12262511, Validation loss: 0.25807924, Gradient norm: 2.79448663
INFO:root:[   18] Training loss: 0.11489342, Validation loss: 0.24591047, Gradient norm: 4.12498940
INFO:root:[   19] Training loss: 0.10747589, Validation loss: 0.21638760, Gradient norm: 3.15304033
INFO:root:[   20] Training loss: 0.10206616, Validation loss: 0.22410014, Gradient norm: 4.16127513
INFO:root:[   21] Training loss: 0.09775858, Validation loss: 0.18141116, Gradient norm: 3.32854825
INFO:root:[   22] Training loss: 0.09343759, Validation loss: 0.18132973, Gradient norm: 3.63999943
INFO:root:[   23] Training loss: 0.08863425, Validation loss: 0.23100703, Gradient norm: 3.55289306
INFO:root:[   24] Training loss: 0.08579751, Validation loss: 0.15956436, Gradient norm: 3.31946531
INFO:root:[   25] Training loss: 0.08115808, Validation loss: 0.12554691, Gradient norm: 3.18128635
INFO:root:[   26] Training loss: 0.07840052, Validation loss: 0.13560825, Gradient norm: 3.14884835
INFO:root:[   27] Training loss: 0.07729156, Validation loss: 0.11877690, Gradient norm: 2.06981046
INFO:root:[   28] Training loss: 0.07558945, Validation loss: 0.10477356, Gradient norm: 2.75398014
INFO:root:[   29] Training loss: 0.07360189, Validation loss: 0.09273956, Gradient norm: 3.84422930
INFO:root:[   30] Training loss: 0.07089323, Validation loss: 0.10921355, Gradient norm: 3.29105888
INFO:root:[   31] Training loss: 0.07040747, Validation loss: 0.09086813, Gradient norm: 3.19743303
INFO:root:[   32] Training loss: 0.07072127, Validation loss: 0.09357641, Gradient norm: 3.67103126
INFO:root:[   33] Training loss: 0.06901549, Validation loss: 0.08659080, Gradient norm: 2.90551334
INFO:root:[   34] Training loss: 0.06829317, Validation loss: 0.10566985, Gradient norm: 3.22358272
INFO:root:[   35] Training loss: 0.06995872, Validation loss: 0.08275915, Gradient norm: 3.28086071
INFO:root:[   36] Training loss: 0.06669960, Validation loss: 0.09068751, Gradient norm: 3.30501452
INFO:root:[   37] Training loss: 0.06936862, Validation loss: 0.08966658, Gradient norm: 3.45898747
INFO:root:[   38] Training loss: 0.06757601, Validation loss: 0.08872096, Gradient norm: 2.21115091
INFO:root:[   39] Training loss: 0.06782079, Validation loss: 0.09861362, Gradient norm: 2.22050468
INFO:root:[   40] Training loss: 0.06629915, Validation loss: 0.08148030, Gradient norm: 3.43366457
INFO:root:[   41] Training loss: 0.06607561, Validation loss: 0.08854145, Gradient norm: 3.55777728
INFO:root:[   42] Training loss: 0.06325726, Validation loss: 0.08876527, Gradient norm: 3.24626847
INFO:root:[   43] Training loss: 0.06378373, Validation loss: 0.08943854, Gradient norm: 3.11405851
INFO:root:[   44] Training loss: 0.06406918, Validation loss: 0.07408190, Gradient norm: 2.78605682
INFO:root:[   45] Training loss: 0.06249752, Validation loss: 0.07263210, Gradient norm: 3.23525734
INFO:root:[   46] Training loss: 0.06298701, Validation loss: 0.07421408, Gradient norm: 3.45707974
INFO:root:[   47] Training loss: 0.06326706, Validation loss: 0.08735738, Gradient norm: 2.88876266
INFO:root:[   48] Training loss: 0.06336713, Validation loss: 0.08312716, Gradient norm: 3.02594759
INFO:root:[   49] Training loss: 0.06255153, Validation loss: 0.07639130, Gradient norm: 3.25335952
INFO:root:[   50] Training loss: 0.06327450, Validation loss: 0.09315673, Gradient norm: 3.21378481
INFO:root:[   51] Training loss: 0.06113373, Validation loss: 0.06992015, Gradient norm: 2.72934706
INFO:root:[   52] Training loss: 0.06019389, Validation loss: 0.07420420, Gradient norm: 2.90014754
INFO:root:[   53] Training loss: 0.05998566, Validation loss: 0.06953397, Gradient norm: 2.69496120
INFO:root:[   54] Training loss: 0.06117542, Validation loss: 0.06988361, Gradient norm: 3.17064789
INFO:root:[   55] Training loss: 0.06037602, Validation loss: 0.08385550, Gradient norm: 2.01695807
INFO:root:[   56] Training loss: 0.05993907, Validation loss: 0.06942099, Gradient norm: 2.94057590
INFO:root:[   57] Training loss: 0.05967985, Validation loss: 0.08026047, Gradient norm: 2.91666981
INFO:root:[   58] Training loss: 0.06030359, Validation loss: 0.06972148, Gradient norm: 2.58587044
INFO:root:[   59] Training loss: 0.05857719, Validation loss: 0.07778248, Gradient norm: 3.08547419
INFO:root:[   60] Training loss: 0.06075651, Validation loss: 0.09488915, Gradient norm: 2.58879658
INFO:root:[   61] Training loss: 0.05942478, Validation loss: 0.08006298, Gradient norm: 2.27540612
INFO:root:[   62] Training loss: 0.05891116, Validation loss: 0.08513319, Gradient norm: 3.01777102
INFO:root:[   63] Training loss: 0.06007445, Validation loss: 0.06720580, Gradient norm: 2.79077364
INFO:root:[   64] Training loss: 0.05750438, Validation loss: 0.08207297, Gradient norm: 2.28586738
INFO:root:[   65] Training loss: 0.05934879, Validation loss: 0.07093959, Gradient norm: 1.90539635
INFO:root:[   66] Training loss: 0.05883744, Validation loss: 0.08600515, Gradient norm: 2.07569191
INFO:root:[   67] Training loss: 0.05759367, Validation loss: 0.08702824, Gradient norm: 2.47398190
INFO:root:[   68] Training loss: 0.05886520, Validation loss: 0.08562926, Gradient norm: 2.46465257
INFO:root:[   69] Training loss: 0.05661673, Validation loss: 0.09419170, Gradient norm: 2.38946231
INFO:root:[   70] Training loss: 0.05957074, Validation loss: 0.07619768, Gradient norm: 2.61675915
INFO:root:[   71] Training loss: 0.05953420, Validation loss: 0.08938057, Gradient norm: 2.17594088
INFO:root:[   72] Training loss: 0.05733043, Validation loss: 0.08518145, Gradient norm: 2.25072195
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 1384.076s.
INFO:root:Emptying the cuda cache took 0.045s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06793
INFO:root:EnergyScoreTrain: 0.06024
INFO:root:CoverageTrain: 0.0004
INFO:root:IntervalWidthTrain: 4e-05
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06764
INFO:root:EnergyScoreValidation: 0.05942
INFO:root:CoverageValidation: 0.00041
INFO:root:IntervalWidthValidation: 4e-05
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07745
INFO:root:EnergyScoreTest: 0.06651
INFO:root:CoverageTest: 0.00026
INFO:root:IntervalWidthTest: 4e-05
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 473956352
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.44902309, Validation loss: 0.14457983, Gradient norm: 7.29679228
INFO:root:[    2] Training loss: 0.25836472, Validation loss: 0.17364425, Gradient norm: 9.84427843
INFO:root:[    3] Training loss: 0.19748717, Validation loss: 0.14602952, Gradient norm: 5.88776524
INFO:root:[    4] Training loss: 0.16599040, Validation loss: 0.16649086, Gradient norm: 5.34101387
INFO:root:[    5] Training loss: 0.15449098, Validation loss: 0.09393647, Gradient norm: 5.51621866
INFO:root:[    6] Training loss: 0.14233794, Validation loss: 0.07831237, Gradient norm: 4.92674231
INFO:root:[    7] Training loss: 0.14892631, Validation loss: 0.11558297, Gradient norm: 6.86850774
INFO:root:[    8] Training loss: 0.13158926, Validation loss: 0.08896103, Gradient norm: 5.22588958
INFO:root:[    9] Training loss: 0.12160659, Validation loss: 0.08387631, Gradient norm: 5.77011076
INFO:root:[   10] Training loss: 0.11284619, Validation loss: 0.08004541, Gradient norm: 4.76447401
INFO:root:[   11] Training loss: 0.10634768, Validation loss: 0.07373653, Gradient norm: 4.02328337
INFO:root:[   12] Training loss: 0.10183090, Validation loss: 0.08495545, Gradient norm: 3.63672434
INFO:root:[   13] Training loss: 0.09806367, Validation loss: 0.06026170, Gradient norm: 3.00038834
INFO:root:[   14] Training loss: 0.09610481, Validation loss: 0.06777002, Gradient norm: 3.40102057
INFO:root:[   15] Training loss: 0.09468052, Validation loss: 0.06608829, Gradient norm: 2.80726323
INFO:root:[   16] Training loss: 0.09302933, Validation loss: 0.06441075, Gradient norm: 2.68534109
INFO:root:[   17] Training loss: 0.08966126, Validation loss: 0.07427136, Gradient norm: 2.50352215
INFO:root:[   18] Training loss: 0.09014675, Validation loss: 0.06638068, Gradient norm: 2.50113487
INFO:root:[   19] Training loss: 0.08781319, Validation loss: 0.06118915, Gradient norm: 2.26861514
INFO:root:[   20] Training loss: 0.08676083, Validation loss: 0.06544576, Gradient norm: 2.05435449
INFO:root:[   21] Training loss: 0.08701060, Validation loss: 0.06954917, Gradient norm: 2.22479038
INFO:root:[   22] Training loss: 0.08321292, Validation loss: 0.05658719, Gradient norm: 2.23780475
INFO:root:[   23] Training loss: 0.08474346, Validation loss: 0.07166502, Gradient norm: 1.87167763
INFO:root:[   24] Training loss: 0.08292436, Validation loss: 0.05137052, Gradient norm: 2.72742121
INFO:root:[   25] Training loss: 0.08107281, Validation loss: 0.07697431, Gradient norm: 2.12507373
INFO:root:[   26] Training loss: 0.08226033, Validation loss: 0.05247614, Gradient norm: 2.53681898
INFO:root:[   27] Training loss: 0.08157250, Validation loss: 0.06051137, Gradient norm: 2.04377492
INFO:root:[   28] Training loss: 0.07992999, Validation loss: 0.07293642, Gradient norm: 2.49968580
INFO:root:[   29] Training loss: 0.07823224, Validation loss: 0.05641833, Gradient norm: 2.51506455
INFO:root:[   30] Training loss: 0.07855234, Validation loss: 0.05008289, Gradient norm: 2.47152121
INFO:root:[   31] Training loss: 0.07789991, Validation loss: 0.05780925, Gradient norm: 2.24729751
INFO:root:[   32] Training loss: 0.07620644, Validation loss: 0.04957705, Gradient norm: 2.45684455
INFO:root:[   33] Training loss: 0.07621927, Validation loss: 0.05511384, Gradient norm: 2.67055494
INFO:root:[   34] Training loss: 0.07602565, Validation loss: 0.05813369, Gradient norm: 1.93092187
INFO:root:[   35] Training loss: 0.07669024, Validation loss: 0.06093246, Gradient norm: 2.01069285
INFO:root:[   36] Training loss: 0.07608945, Validation loss: 0.05095163, Gradient norm: 2.44687379
INFO:root:[   37] Training loss: 0.07551254, Validation loss: 0.04662778, Gradient norm: 2.12816259
INFO:root:[   38] Training loss: 0.07283463, Validation loss: 0.04257136, Gradient norm: 2.19022749
INFO:root:[   39] Training loss: 0.07033279, Validation loss: 0.04296376, Gradient norm: 2.66408449
INFO:root:[   40] Training loss: 0.07018198, Validation loss: 0.05651323, Gradient norm: 2.66744733
INFO:root:[   41] Training loss: 0.07193915, Validation loss: 0.05714782, Gradient norm: 2.41706527
INFO:root:[   42] Training loss: 0.06984967, Validation loss: 0.04247556, Gradient norm: 2.79280842
INFO:root:[   43] Training loss: 0.07029166, Validation loss: 0.05273390, Gradient norm: 2.85298645
INFO:root:[   44] Training loss: 0.06876080, Validation loss: 0.04854848, Gradient norm: 2.79483115
INFO:root:[   45] Training loss: 0.06997197, Validation loss: 0.04415830, Gradient norm: 2.69503202
INFO:root:[   46] Training loss: 0.06780979, Validation loss: 0.05340681, Gradient norm: 2.78230141
INFO:root:[   47] Training loss: 0.06691535, Validation loss: 0.05118106, Gradient norm: 3.10004124
INFO:root:[   48] Training loss: 0.06730881, Validation loss: 0.03925984, Gradient norm: 2.87227202
INFO:root:[   49] Training loss: 0.06758904, Validation loss: 0.04427030, Gradient norm: 2.80209737
INFO:root:[   50] Training loss: 0.06880103, Validation loss: 0.04103294, Gradient norm: 2.77087962
INFO:root:[   51] Training loss: 0.06533709, Validation loss: 0.03960554, Gradient norm: 3.24629305
INFO:root:[   52] Training loss: 0.06469665, Validation loss: 0.04953890, Gradient norm: 3.19651483
INFO:root:[   53] Training loss: 0.06447628, Validation loss: 0.05715857, Gradient norm: 3.06306884
INFO:root:[   54] Training loss: 0.06441481, Validation loss: 0.03888933, Gradient norm: 3.03700608
INFO:root:[   55] Training loss: 0.06406243, Validation loss: 0.03751626, Gradient norm: 3.28485922
INFO:root:[   56] Training loss: 0.06302785, Validation loss: 0.05039077, Gradient norm: 3.29306302
INFO:root:[   57] Training loss: 0.06278723, Validation loss: 0.05090827, Gradient norm: 3.53166662
INFO:root:[   58] Training loss: 0.06356554, Validation loss: 0.04278027, Gradient norm: 3.00043445
INFO:root:[   59] Training loss: 0.06396421, Validation loss: 0.03758585, Gradient norm: 3.24430447
INFO:root:[   60] Training loss: 0.06208417, Validation loss: 0.05010303, Gradient norm: 3.46166178
INFO:root:[   61] Training loss: 0.06229244, Validation loss: 0.04010865, Gradient norm: 3.31644132
INFO:root:[   62] Training loss: 0.06460814, Validation loss: 0.04884058, Gradient norm: 2.76537677
INFO:root:[   63] Training loss: 0.06063526, Validation loss: 0.03712968, Gradient norm: 3.56811259
INFO:root:[   64] Training loss: 0.06082984, Validation loss: 0.03640013, Gradient norm: 3.39714741
INFO:root:[   65] Training loss: 0.05962530, Validation loss: 0.04633788, Gradient norm: 3.44684829
INFO:root:[   66] Training loss: 0.06023496, Validation loss: 0.04499301, Gradient norm: 3.34581224
INFO:root:[   67] Training loss: 0.05990870, Validation loss: 0.03558922, Gradient norm: 3.45041230
INFO:root:[   68] Training loss: 0.06112115, Validation loss: 0.04376936, Gradient norm: 2.84418536
INFO:root:[   69] Training loss: 0.05995934, Validation loss: 0.05095024, Gradient norm: 3.55500241
INFO:root:[   70] Training loss: 0.05943676, Validation loss: 0.04519865, Gradient norm: 3.70029956
INFO:root:[   71] Training loss: 0.05841663, Validation loss: 0.03416376, Gradient norm: 3.75748293
INFO:root:[   72] Training loss: 0.05873095, Validation loss: 0.03522771, Gradient norm: 3.42255152
INFO:root:[   73] Training loss: 0.05815635, Validation loss: 0.04624172, Gradient norm: 3.80806139
INFO:root:[   74] Training loss: 0.05828666, Validation loss: 0.04311831, Gradient norm: 3.52592399
INFO:root:[   75] Training loss: 0.05823332, Validation loss: 0.03741631, Gradient norm: 3.51592071
INFO:root:[   76] Training loss: 0.05769020, Validation loss: 0.03694819, Gradient norm: 3.71294830
INFO:root:[   77] Training loss: 0.05815998, Validation loss: 0.05007710, Gradient norm: 3.45717795
INFO:root:[   78] Training loss: 0.05713626, Validation loss: 0.04343478, Gradient norm: 4.00266724
INFO:root:[   79] Training loss: 0.05608197, Validation loss: 0.03476062, Gradient norm: 3.88553871
INFO:root:[   80] Training loss: 0.05977959, Validation loss: 0.03977873, Gradient norm: 2.79235052
INFO:root:EP 80: Early stopping
INFO:root:Training the model took 1463.809s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03594
INFO:root:EnergyScoreTrain: 0.02834
INFO:root:CoverageTrain: 0.43952
INFO:root:IntervalWidthTrain: 0.00732
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03556
INFO:root:EnergyScoreValidation: 0.02765
INFO:root:CoverageValidation: 0.44691
INFO:root:IntervalWidthValidation: 0.00753
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04431
INFO:root:EnergyScoreTest: 0.03605
INFO:root:CoverageTest: 0.38104
INFO:root:IntervalWidthTest: 0.00849
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 664797184
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.73916812, Validation loss: 0.27458538, Gradient norm: 7.68150495
INFO:root:[    2] Training loss: 0.41243299, Validation loss: 0.16925722, Gradient norm: 6.76310561
INFO:root:[    3] Training loss: 0.32232262, Validation loss: 0.15720157, Gradient norm: 6.00105980
INFO:root:[    4] Training loss: 0.27688268, Validation loss: 0.14071032, Gradient norm: 4.78068174
INFO:root:[    5] Training loss: 0.25549194, Validation loss: 0.10875533, Gradient norm: 5.89331323
INFO:root:[    6] Training loss: 0.22067682, Validation loss: 0.09553404, Gradient norm: 3.58858086
INFO:root:[    7] Training loss: 0.21087160, Validation loss: 0.16714816, Gradient norm: 4.80364776
INFO:root:[    8] Training loss: 0.19269363, Validation loss: 0.09942075, Gradient norm: 2.90663624
INFO:root:[    9] Training loss: 0.19028934, Validation loss: 0.11035682, Gradient norm: 3.35937853
INFO:root:[   10] Training loss: 0.17768544, Validation loss: 0.08590738, Gradient norm: 2.16401324
INFO:root:[   11] Training loss: 0.17296466, Validation loss: 0.11180116, Gradient norm: 2.66092157
INFO:root:[   12] Training loss: 0.16786939, Validation loss: 0.16475393, Gradient norm: 2.10593313
INFO:root:[   13] Training loss: 0.16462106, Validation loss: 0.14514799, Gradient norm: 2.52396207
INFO:root:[   14] Training loss: 0.15994410, Validation loss: 0.14774886, Gradient norm: 2.50821235
INFO:root:[   15] Training loss: 0.15775175, Validation loss: 0.14155017, Gradient norm: 1.95246687
INFO:root:[   16] Training loss: 0.15709229, Validation loss: 0.10508516, Gradient norm: 2.43090158
INFO:root:[   17] Training loss: 0.15140223, Validation loss: 0.10196563, Gradient norm: 1.89490898
INFO:root:[   18] Training loss: 0.14811733, Validation loss: 0.14426989, Gradient norm: 2.19768897
INFO:root:[   19] Training loss: 0.14746930, Validation loss: 0.15689968, Gradient norm: 1.69415037
INFO:root:[   20] Training loss: 0.14275333, Validation loss: 0.14615271, Gradient norm: 2.01941898
INFO:root:[   21] Training loss: 0.13919608, Validation loss: 0.15894773, Gradient norm: 1.95350482
INFO:root:[   22] Training loss: 0.13727263, Validation loss: 0.14138409, Gradient norm: 2.28420950
INFO:root:[   23] Training loss: 0.13461068, Validation loss: 0.14803010, Gradient norm: 2.10749922
INFO:root:[   24] Training loss: 0.13152475, Validation loss: 0.13460119, Gradient norm: 2.20746224
INFO:root:[   25] Training loss: 0.13037955, Validation loss: 0.12741146, Gradient norm: 2.44137449
INFO:root:[   26] Training loss: 0.12879147, Validation loss: 0.10815590, Gradient norm: 2.26896653
INFO:root:[   27] Training loss: 0.12671237, Validation loss: 0.08860339, Gradient norm: 2.12751571
INFO:root:[   28] Training loss: 0.12340548, Validation loss: 0.10603689, Gradient norm: 1.95592592
INFO:root:[   29] Training loss: 0.12153307, Validation loss: 0.13788753, Gradient norm: 2.42357372
INFO:root:[   30] Training loss: 0.11862605, Validation loss: 0.10238352, Gradient norm: 2.26100617
INFO:root:[   31] Training loss: 0.11783181, Validation loss: 0.12400856, Gradient norm: 1.87206236
INFO:root:[   32] Training loss: 0.11554781, Validation loss: 0.11320835, Gradient norm: 2.51673496
INFO:root:[   33] Training loss: 0.11288830, Validation loss: 0.11694035, Gradient norm: 2.29918689
INFO:root:[   34] Training loss: 0.11002274, Validation loss: 0.13508809, Gradient norm: 2.78605596
INFO:root:[   35] Training loss: 0.11103471, Validation loss: 0.11471136, Gradient norm: 2.52338334
INFO:root:[   36] Training loss: 0.10666626, Validation loss: 0.11214418, Gradient norm: 2.63332980
INFO:root:[   37] Training loss: 0.10456672, Validation loss: 0.11691226, Gradient norm: 2.73526097
INFO:root:[   38] Training loss: 0.10270185, Validation loss: 0.08443361, Gradient norm: 2.78313660
INFO:root:[   39] Training loss: 0.10053046, Validation loss: 0.07426173, Gradient norm: 2.80716352
INFO:root:[   40] Training loss: 0.10023603, Validation loss: 0.10702210, Gradient norm: 2.75873648
INFO:root:[   41] Training loss: 0.09753928, Validation loss: 0.10590539, Gradient norm: 2.95162735
INFO:root:[   42] Training loss: 0.09667485, Validation loss: 0.07218182, Gradient norm: 2.88299491
INFO:root:[   43] Training loss: 0.09369130, Validation loss: 0.06926485, Gradient norm: 3.14843921
INFO:root:[   44] Training loss: 0.09174784, Validation loss: 0.09514749, Gradient norm: 3.24479733
INFO:root:[   45] Training loss: 0.10077370, Validation loss: 0.11131428, Gradient norm: 3.20683183
INFO:root:[   46] Training loss: 0.09160156, Validation loss: 0.10037461, Gradient norm: 2.95371353
INFO:root:[   47] Training loss: 0.09152129, Validation loss: 0.10111830, Gradient norm: 2.09352464
INFO:root:[   48] Training loss: 0.09000406, Validation loss: 0.06578652, Gradient norm: 2.58060499
INFO:root:[   49] Training loss: 0.08584682, Validation loss: 0.08900820, Gradient norm: 2.79682738
INFO:root:[   50] Training loss: 0.08466288, Validation loss: 0.09156238, Gradient norm: 2.95132466
INFO:root:[   51] Training loss: 0.08238320, Validation loss: 0.06241278, Gradient norm: 3.01274240
INFO:root:[   52] Training loss: 0.08106966, Validation loss: 0.06046895, Gradient norm: 3.27036114
INFO:root:[   53] Training loss: 0.07941619, Validation loss: 0.08218675, Gradient norm: 3.36469041
INFO:root:[   54] Training loss: 0.07877008, Validation loss: 0.08709479, Gradient norm: 2.89538166
INFO:root:[   55] Training loss: 0.07834605, Validation loss: 0.08797786, Gradient norm: 3.01034192
INFO:root:[   56] Training loss: 0.08066704, Validation loss: 0.05109067, Gradient norm: 2.62402765
INFO:root:[   57] Training loss: 0.07503571, Validation loss: 0.05911364, Gradient norm: 2.85790120
INFO:root:[   58] Training loss: 0.07400642, Validation loss: 0.07357903, Gradient norm: 3.58683785
INFO:root:[   59] Training loss: 0.07252389, Validation loss: 0.07803763, Gradient norm: 3.20436787
INFO:root:[   60] Training loss: 0.07154083, Validation loss: 0.07717460, Gradient norm: 3.26653096
INFO:root:[   61] Training loss: 0.07750756, Validation loss: 0.05654808, Gradient norm: 2.87164327
INFO:root:[   62] Training loss: 0.07027957, Validation loss: 0.05454707, Gradient norm: 3.28281023
INFO:root:[   63] Training loss: 0.07404008, Validation loss: 0.07053120, Gradient norm: 2.87604834
INFO:root:[   64] Training loss: 0.06763478, Validation loss: 0.06521418, Gradient norm: 2.93250516
INFO:root:[   65] Training loss: 0.06630111, Validation loss: 0.05375526, Gradient norm: 3.31575237
INFO:root:[   66] Training loss: 0.06730664, Validation loss: 0.05076436, Gradient norm: 3.31308620
INFO:root:[   67] Training loss: 0.06436166, Validation loss: 0.04524504, Gradient norm: 3.40911585
INFO:root:[   68] Training loss: 0.06294424, Validation loss: 0.06357226, Gradient norm: 3.37467462
INFO:root:[   69] Training loss: 0.06307113, Validation loss: 0.06185726, Gradient norm: 3.18227718
INFO:root:[   70] Training loss: 0.06769296, Validation loss: 0.06254096, Gradient norm: 3.05409098
INFO:root:[   71] Training loss: 0.06196588, Validation loss: 0.05128331, Gradient norm: 2.82031608
INFO:root:[   72] Training loss: 0.06566472, Validation loss: 0.05105473, Gradient norm: 2.89824277
INFO:root:[   73] Training loss: 0.06072300, Validation loss: 0.04645565, Gradient norm: 2.95343078
INFO:root:[   74] Training loss: 0.06203126, Validation loss: 0.03816353, Gradient norm: 3.37368968
INFO:root:[   75] Training loss: 0.05821054, Validation loss: 0.05610522, Gradient norm: 3.42942621
INFO:root:[   76] Training loss: 0.05766480, Validation loss: 0.05138464, Gradient norm: 3.51182061
INFO:root:[   77] Training loss: 0.05704145, Validation loss: 0.04781967, Gradient norm: 3.70808816
INFO:root:[   78] Training loss: 0.05747271, Validation loss: 0.03835582, Gradient norm: 3.36876482
INFO:root:[   79] Training loss: 0.05608647, Validation loss: 0.03979126, Gradient norm: 3.84898198
INFO:root:[   80] Training loss: 0.05539122, Validation loss: 0.05011606, Gradient norm: 3.65329179
INFO:root:[   81] Training loss: 0.05516875, Validation loss: 0.05676269, Gradient norm: 3.69297281
INFO:root:[   82] Training loss: 0.05669852, Validation loss: 0.04865438, Gradient norm: 3.69085005
INFO:root:[   83] Training loss: 0.05512778, Validation loss: 0.03868874, Gradient norm: 3.31049967
INFO:root:[   84] Training loss: 0.05404262, Validation loss: 0.03783827, Gradient norm: 3.77053106
INFO:root:[   85] Training loss: 0.05286452, Validation loss: 0.04786240, Gradient norm: 3.94776699
INFO:root:[   86] Training loss: 0.05830697, Validation loss: 0.04375001, Gradient norm: 3.15017928
INFO:root:[   87] Training loss: 0.05320209, Validation loss: 0.03435859, Gradient norm: 4.33556493
INFO:root:[   88] Training loss: 0.05223186, Validation loss: 0.04691220, Gradient norm: 3.91754967
INFO:root:[   89] Training loss: 0.05268600, Validation loss: 0.05440780, Gradient norm: 3.97072171
INFO:root:[   90] Training loss: 0.05314580, Validation loss: 0.03633799, Gradient norm: 3.89128654
INFO:root:[   91] Training loss: 0.05393537, Validation loss: 0.05055510, Gradient norm: 3.12039468
INFO:root:[   92] Training loss: 0.05168986, Validation loss: 0.04518808, Gradient norm: 4.52498278
INFO:root:[   93] Training loss: 0.04976060, Validation loss: 0.03721267, Gradient norm: 4.20393284
INFO:root:[   94] Training loss: 0.04987037, Validation loss: 0.03393383, Gradient norm: 4.40676936
INFO:root:[   95] Training loss: 0.05003222, Validation loss: 0.04227857, Gradient norm: 4.11517531
INFO:root:[   96] Training loss: 0.05046292, Validation loss: 0.04717515, Gradient norm: 4.24600802
INFO:root:[   97] Training loss: 0.05115448, Validation loss: 0.04202485, Gradient norm: 3.65978343
INFO:root:[   98] Training loss: 0.05045556, Validation loss: 0.04334209, Gradient norm: 4.61506731
INFO:root:[   99] Training loss: 0.05028108, Validation loss: 0.04844585, Gradient norm: 3.49970460
INFO:root:[  100] Training loss: 0.05172030, Validation loss: 0.03342282, Gradient norm: 4.50400802
INFO:root:[  101] Training loss: 0.04888106, Validation loss: 0.03750037, Gradient norm: 3.83245220
INFO:root:[  102] Training loss: 0.04925578, Validation loss: 0.04018315, Gradient norm: 4.87417083
INFO:root:[  103] Training loss: 0.05046055, Validation loss: 0.03247900, Gradient norm: 4.68886676
INFO:root:[  104] Training loss: 0.04850174, Validation loss: 0.04395192, Gradient norm: 4.98828040
INFO:root:[  105] Training loss: 0.04810765, Validation loss: 0.03621325, Gradient norm: 4.80617531
INFO:root:[  106] Training loss: 0.04832740, Validation loss: 0.03348230, Gradient norm: 3.79344891
INFO:root:[  107] Training loss: 0.04833641, Validation loss: 0.04719434, Gradient norm: 2.94828185
INFO:root:[  108] Training loss: 0.04882065, Validation loss: 0.03271622, Gradient norm: 4.68742351
INFO:root:[  109] Training loss: 0.04783317, Validation loss: 0.03667624, Gradient norm: 4.58720475
INFO:root:[  110] Training loss: 0.04778846, Validation loss: 0.03347862, Gradient norm: 4.71331581
INFO:root:[  111] Training loss: 0.04701130, Validation loss: 0.04418237, Gradient norm: 4.81115528
INFO:root:[  112] Training loss: 0.04887258, Validation loss: 0.04172127, Gradient norm: 5.42004577
INFO:root:[  113] Training loss: 0.04790767, Validation loss: 0.02970969, Gradient norm: 4.20483146
INFO:root:[  114] Training loss: 0.05037583, Validation loss: 0.03146682, Gradient norm: 5.89609951
INFO:root:[  115] Training loss: 0.04666609, Validation loss: 0.03322055, Gradient norm: 4.73400321
INFO:root:[  116] Training loss: 0.04617320, Validation loss: 0.03094164, Gradient norm: 4.47439625
INFO:root:[  117] Training loss: 0.04789484, Validation loss: 0.03950811, Gradient norm: 6.67028117
INFO:root:[  118] Training loss: 0.04535807, Validation loss: 0.03176286, Gradient norm: 4.54232055
INFO:root:[  119] Training loss: 0.04759319, Validation loss: 0.03985218, Gradient norm: 6.92165988
INFO:root:[  120] Training loss: 0.04603494, Validation loss: 0.03691427, Gradient norm: 5.71240991
INFO:root:[  121] Training loss: 0.04585158, Validation loss: 0.03293784, Gradient norm: 5.18657747
INFO:root:[  122] Training loss: 0.04714054, Validation loss: 0.03131322, Gradient norm: 6.21965877
INFO:root:EP 122: Early stopping
INFO:root:Training the model took 2220.644s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0349
INFO:root:EnergyScoreTrain: 0.02203
INFO:root:CoverageTrain: 0.17909
INFO:root:IntervalWidthTrain: 0.00823
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03488
INFO:root:EnergyScoreValidation: 0.02222
INFO:root:CoverageValidation: 0.16356
INFO:root:IntervalWidthValidation: 0.00783
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04177
INFO:root:EnergyScoreTest: 0.03018
INFO:root:CoverageTest: 0.13897
INFO:root:IntervalWidthTest: 0.00599
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1000341504
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.90406947, Validation loss: 0.18885953, Gradient norm: 6.96867147
INFO:root:[    2] Training loss: 0.52891397, Validation loss: 0.36240977, Gradient norm: 5.99250590
INFO:root:[    3] Training loss: 0.47190915, Validation loss: 0.17452855, Gradient norm: 7.35472554
INFO:root:[    4] Training loss: 0.38936215, Validation loss: 0.12190255, Gradient norm: 5.25592416
INFO:root:[    5] Training loss: 0.32949559, Validation loss: 0.12521929, Gradient norm: 3.12396977
INFO:root:[    6] Training loss: 0.30050400, Validation loss: 0.11412777, Gradient norm: 3.22623304
INFO:root:[    7] Training loss: 0.28009623, Validation loss: 0.12571073, Gradient norm: 3.50614284
INFO:root:[    8] Training loss: 0.25923492, Validation loss: 0.12978637, Gradient norm: 2.47579974
INFO:root:[    9] Training loss: 0.24522990, Validation loss: 0.16865580, Gradient norm: 1.96689017
INFO:root:[   10] Training loss: 0.23487075, Validation loss: 0.14641642, Gradient norm: 1.79597617
INFO:root:[   11] Training loss: 0.22818294, Validation loss: 0.21682148, Gradient norm: 1.78123257
INFO:root:[   12] Training loss: 0.21886737, Validation loss: 0.21242137, Gradient norm: 1.03425191
INFO:root:[   13] Training loss: 0.21334532, Validation loss: 0.21866259, Gradient norm: 1.28903763
INFO:root:[   14] Training loss: 0.20805080, Validation loss: 0.24499969, Gradient norm: 1.40136417
INFO:root:[   15] Training loss: 0.20186416, Validation loss: 0.17737212, Gradient norm: 1.28953237
INFO:root:[   16] Training loss: 0.19644831, Validation loss: 0.18751886, Gradient norm: 1.34711528
INFO:root:[   17] Training loss: 0.19206046, Validation loss: 0.22017885, Gradient norm: 1.13883714
INFO:root:[   18] Training loss: 0.18662641, Validation loss: 0.22367221, Gradient norm: 1.16516337
INFO:root:[   19] Training loss: 0.18094077, Validation loss: 0.16821245, Gradient norm: 1.06000846
INFO:root:[   20] Training loss: 0.17653991, Validation loss: 0.19701435, Gradient norm: 1.29236688
INFO:root:[   21] Training loss: 0.17229303, Validation loss: 0.20803695, Gradient norm: 1.43713730
INFO:root:[   22] Training loss: 0.16709303, Validation loss: 0.19246948, Gradient norm: 1.34477753
INFO:root:[   23] Training loss: 0.16248272, Validation loss: 0.22388727, Gradient norm: 1.14865678
INFO:root:[   24] Training loss: 0.15837446, Validation loss: 0.18757658, Gradient norm: 1.20604682
INFO:root:[   25] Training loss: 0.15317816, Validation loss: 0.15776350, Gradient norm: 1.41340988
INFO:root:[   26] Training loss: 0.14904684, Validation loss: 0.14083595, Gradient norm: 1.39519437
INFO:root:[   27] Training loss: 0.14700853, Validation loss: 0.19631522, Gradient norm: 1.88521263
INFO:root:[   28] Training loss: 0.14010336, Validation loss: 0.17274865, Gradient norm: 1.43515271
INFO:root:[   29] Training loss: 0.13813342, Validation loss: 0.14047726, Gradient norm: 1.21019535
INFO:root:[   30] Training loss: 0.13219422, Validation loss: 0.16230777, Gradient norm: 1.48911069
INFO:root:[   31] Training loss: 0.12905180, Validation loss: 0.19306186, Gradient norm: 1.55820017
INFO:root:[   32] Training loss: 0.12504449, Validation loss: 0.14828950, Gradient norm: 1.50577195
INFO:root:[   33] Training loss: 0.12136210, Validation loss: 0.12120771, Gradient norm: 1.62597999
INFO:root:[   34] Training loss: 0.11901292, Validation loss: 0.15313082, Gradient norm: 1.54480290
INFO:root:[   35] Training loss: 0.11243887, Validation loss: 0.12425548, Gradient norm: 1.39775761
INFO:root:[   36] Training loss: 0.11220939, Validation loss: 0.13055701, Gradient norm: 1.51144290
INFO:root:[   37] Training loss: 0.10838809, Validation loss: 0.14390002, Gradient norm: 1.52032112
INFO:root:[   38] Training loss: 0.10504723, Validation loss: 0.11644821, Gradient norm: 1.72453362
INFO:root:[   39] Training loss: 0.10013878, Validation loss: 0.11651850, Gradient norm: 1.65102771
INFO:root:[   40] Training loss: 0.09932101, Validation loss: 0.11002840, Gradient norm: 1.94267643
INFO:root:[   41] Training loss: 0.09825827, Validation loss: 0.08151627, Gradient norm: 1.82347819
INFO:root:[   42] Training loss: 0.09201153, Validation loss: 0.09611213, Gradient norm: 1.95336015
INFO:root:[   43] Training loss: 0.09269491, Validation loss: 0.10324890, Gradient norm: 1.38115627
INFO:root:[   44] Training loss: 0.08720198, Validation loss: 0.07820256, Gradient norm: 1.61873655
INFO:root:[   45] Training loss: 0.08424310, Validation loss: 0.07342551, Gradient norm: 1.81951836
INFO:root:[   46] Training loss: 0.08459584, Validation loss: 0.07134213, Gradient norm: 1.84359552
INFO:root:[   47] Training loss: 0.08120637, Validation loss: 0.06990702, Gradient norm: 2.11830536
INFO:root:[   48] Training loss: 0.08093128, Validation loss: 0.08700692, Gradient norm: 2.50660132
INFO:root:[   49] Training loss: 0.07869082, Validation loss: 0.09009850, Gradient norm: 2.27629036
INFO:root:[   50] Training loss: 0.07637830, Validation loss: 0.07427514, Gradient norm: 2.04005286
INFO:root:[   51] Training loss: 0.07345141, Validation loss: 0.07512898, Gradient norm: 2.20810750
INFO:root:[   52] Training loss: 0.07160894, Validation loss: 0.06463673, Gradient norm: 2.30890935
INFO:root:[   53] Training loss: 0.06976333, Validation loss: 0.05227949, Gradient norm: 2.17369405
INFO:root:[   54] Training loss: 0.06979998, Validation loss: 0.08171555, Gradient norm: 2.46819314
INFO:root:[   55] Training loss: 0.06914031, Validation loss: 0.06673573, Gradient norm: 2.99972906
INFO:root:[   56] Training loss: 0.06725692, Validation loss: 0.05071417, Gradient norm: 2.66699018
INFO:root:[   57] Training loss: 0.06581785, Validation loss: 0.05310202, Gradient norm: 2.87101169
INFO:root:[   58] Training loss: 0.06516210, Validation loss: 0.06352300, Gradient norm: 2.85410431
INFO:root:[   59] Training loss: 0.06530013, Validation loss: 0.04797914, Gradient norm: 3.51781225
INFO:root:[   60] Training loss: 0.06311771, Validation loss: 0.04614594, Gradient norm: 3.02930974
INFO:root:[   61] Training loss: 0.06215156, Validation loss: 0.05549065, Gradient norm: 3.57986743
INFO:root:[   62] Training loss: 0.06130136, Validation loss: 0.05446054, Gradient norm: 3.33565006
INFO:root:[   63] Training loss: 0.06148833, Validation loss: 0.04602870, Gradient norm: 3.75659724
INFO:root:[   64] Training loss: 0.06202568, Validation loss: 0.04920999, Gradient norm: 3.62208957
INFO:root:[   65] Training loss: 0.06447701, Validation loss: 0.04666107, Gradient norm: 3.65060111
INFO:root:[   66] Training loss: 0.06034644, Validation loss: 0.04264847, Gradient norm: 3.54264182
INFO:root:[   67] Training loss: 0.06204496, Validation loss: 0.04369683, Gradient norm: 3.79030541
INFO:root:[   68] Training loss: 0.05961015, Validation loss: 0.04771177, Gradient norm: 4.28818444
INFO:root:[   69] Training loss: 0.06062382, Validation loss: 0.04668301, Gradient norm: 3.58951308
INFO:root:[   70] Training loss: 0.06076553, Validation loss: 0.04983753, Gradient norm: 3.07384987
INFO:root:[   71] Training loss: 0.05961841, Validation loss: 0.04168369, Gradient norm: 3.81171089
INFO:root:[   72] Training loss: 0.05937913, Validation loss: 0.04402923, Gradient norm: 3.74058796
INFO:root:[   73] Training loss: 0.05889474, Validation loss: 0.05342758, Gradient norm: 4.15151812
INFO:root:[   74] Training loss: 0.06032217, Validation loss: 0.03986596, Gradient norm: 5.02500604
INFO:root:[   75] Training loss: 0.05971852, Validation loss: 0.04499983, Gradient norm: 4.94308281
INFO:root:[   76] Training loss: 0.05913770, Validation loss: 0.04916709, Gradient norm: 3.77087787
INFO:root:[   77] Training loss: 0.05826677, Validation loss: 0.04045349, Gradient norm: 4.62328232
INFO:root:[   78] Training loss: 0.05971900, Validation loss: 0.04678745, Gradient norm: 4.93437063
INFO:root:[   79] Training loss: 0.05753035, Validation loss: 0.05338813, Gradient norm: 4.86369018
INFO:root:[   80] Training loss: 0.05723423, Validation loss: 0.04050142, Gradient norm: 5.09487302
INFO:root:[   81] Training loss: 0.06021647, Validation loss: 0.05711819, Gradient norm: 6.04537030
INFO:root:[   82] Training loss: 0.05815565, Validation loss: 0.04804030, Gradient norm: 5.76749922
INFO:root:[   83] Training loss: 0.05712704, Validation loss: 0.04380172, Gradient norm: 4.13477826
INFO:root:EP 83: Early stopping
INFO:root:Training the model took 1521.744s.
INFO:root:Emptying the cuda cache took 0.043s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04844
INFO:root:EnergyScoreTrain: 0.03216
INFO:root:CoverageTrain: 0.17026
INFO:root:IntervalWidthTrain: 0.00948
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04446
INFO:root:EnergyScoreValidation: 0.03009
INFO:root:CoverageValidation: 0.17272
INFO:root:IntervalWidthValidation: 0.00854
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.05185
INFO:root:EnergyScoreTest: 0.03931
INFO:root:CoverageTest: 0.12735
INFO:root:IntervalWidthTest: 0.00633
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 601882624
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.86706719, Validation loss: 0.37020434, Gradient norm: 5.76511927
INFO:root:[    2] Training loss: 0.56373098, Validation loss: 0.19727939, Gradient norm: 7.61859291
INFO:root:[    3] Training loss: 0.44909378, Validation loss: 0.24518660, Gradient norm: 6.05217532
INFO:root:[    4] Training loss: 0.38672161, Validation loss: 0.16752421, Gradient norm: 5.76878054
INFO:root:[    5] Training loss: 0.32732814, Validation loss: 0.21432255, Gradient norm: 3.71288350
INFO:root:[    6] Training loss: 0.29610432, Validation loss: 0.26748799, Gradient norm: 3.08922812
INFO:root:[    7] Training loss: 0.26926058, Validation loss: 0.37186782, Gradient norm: 2.50502017
INFO:root:[    8] Training loss: 0.25449834, Validation loss: 0.32286052, Gradient norm: 2.95458009
INFO:root:[    9] Training loss: 0.24017521, Validation loss: 0.32590492, Gradient norm: 2.21305545
INFO:root:[   10] Training loss: 0.22918484, Validation loss: 0.29091376, Gradient norm: 2.46071148
INFO:root:[   11] Training loss: 0.22012185, Validation loss: 0.26234408, Gradient norm: 2.23976882
INFO:root:[   12] Training loss: 0.21086960, Validation loss: 0.35450448, Gradient norm: 2.30994465
INFO:root:[   13] Training loss: 0.20373279, Validation loss: 0.32313387, Gradient norm: 2.83616194
INFO:root:[   14] Training loss: 0.19434683, Validation loss: 0.26693379, Gradient norm: 2.31469859
INFO:root:[   15] Training loss: 0.18736132, Validation loss: 0.31240353, Gradient norm: 2.66001366
INFO:root:[   16] Training loss: 0.18045791, Validation loss: 0.23775935, Gradient norm: 2.41213183
INFO:root:[   17] Training loss: 0.17296816, Validation loss: 0.28607445, Gradient norm: 2.68396554
INFO:root:[   18] Training loss: 0.16547999, Validation loss: 0.24680449, Gradient norm: 2.44396102
INFO:root:[   19] Training loss: 0.15863255, Validation loss: 0.24345050, Gradient norm: 1.78686417
INFO:root:[   20] Training loss: 0.15113588, Validation loss: 0.25010491, Gradient norm: 2.80097626
INFO:root:[   21] Training loss: 0.14524576, Validation loss: 0.21898039, Gradient norm: 3.16177337
INFO:root:[   22] Training loss: 0.13953527, Validation loss: 0.22579052, Gradient norm: 3.38392148
INFO:root:[   23] Training loss: 0.13564471, Validation loss: 0.18224350, Gradient norm: 2.15133834
INFO:root:[   24] Training loss: 0.12809536, Validation loss: 0.19170321, Gradient norm: 3.70330510
INFO:root:[   25] Training loss: 0.12390246, Validation loss: 0.17843717, Gradient norm: 4.54781022
INFO:root:[   26] Training loss: 0.11967874, Validation loss: 0.20889065, Gradient norm: 4.11818602
INFO:root:[   27] Training loss: 0.11446634, Validation loss: 0.18486097, Gradient norm: 4.82688059
INFO:root:[   28] Training loss: 0.11161944, Validation loss: 0.12552733, Gradient norm: 3.74826273
INFO:root:[   29] Training loss: 0.10725581, Validation loss: 0.12843745, Gradient norm: 5.23058849
INFO:root:[   30] Training loss: 0.10307525, Validation loss: 0.15453837, Gradient norm: 4.41089059
INFO:root:[   31] Training loss: 0.09985486, Validation loss: 0.15697522, Gradient norm: 4.51791869
INFO:root:[   32] Training loss: 0.09782400, Validation loss: 0.10602923, Gradient norm: 3.64468891
INFO:root:[   33] Training loss: 0.09527520, Validation loss: 0.10382594, Gradient norm: 4.54141497
INFO:root:[   34] Training loss: 0.09370982, Validation loss: 0.11460491, Gradient norm: 3.34590587
INFO:root:[   35] Training loss: 0.09031764, Validation loss: 0.07875651, Gradient norm: 3.39736936
INFO:root:[   36] Training loss: 0.08470824, Validation loss: 0.09253303, Gradient norm: 3.27573850
INFO:root:[   37] Training loss: 0.08564259, Validation loss: 0.06536400, Gradient norm: 4.13486336
INFO:root:[   38] Training loss: 0.08736949, Validation loss: 0.07012834, Gradient norm: 6.79431439
INFO:root:[   39] Training loss: 0.08388033, Validation loss: 0.08625690, Gradient norm: 6.24133058
INFO:root:[   40] Training loss: 0.08258740, Validation loss: 0.06849207, Gradient norm: 5.50791908
INFO:root:[   41] Training loss: 0.08008785, Validation loss: 0.07465833, Gradient norm: 5.91080883
INFO:root:[   42] Training loss: 0.08317430, Validation loss: 0.06710234, Gradient norm: 7.09880354
INFO:root:[   43] Training loss: 0.08067163, Validation loss: 0.06925806, Gradient norm: 6.11170900
INFO:root:[   44] Training loss: 0.07819023, Validation loss: 0.05828447, Gradient norm: 7.48559626
INFO:root:[   45] Training loss: 0.08056492, Validation loss: 0.05456373, Gradient norm: 7.80754222
INFO:root:[   46] Training loss: 0.07978841, Validation loss: 0.05759580, Gradient norm: 9.11551568
INFO:root:[   47] Training loss: 0.07799935, Validation loss: 0.05814755, Gradient norm: 7.83499721
INFO:root:[   48] Training loss: 0.08099593, Validation loss: 0.05561771, Gradient norm: 10.58260559
INFO:root:[   49] Training loss: 0.07822184, Validation loss: 0.05551840, Gradient norm: 7.97195088
INFO:root:[   50] Training loss: 0.07898577, Validation loss: 0.06428696, Gradient norm: 8.65427583
INFO:root:[   51] Training loss: 0.07693742, Validation loss: 0.07004244, Gradient norm: 8.84535631
INFO:root:[   52] Training loss: 0.08113317, Validation loss: 0.05077184, Gradient norm: 10.74381777
INFO:root:[   53] Training loss: 0.07422537, Validation loss: 0.05035482, Gradient norm: 7.23537674
INFO:root:[   54] Training loss: 0.07876462, Validation loss: 0.05450082, Gradient norm: 12.21721806
INFO:root:[   55] Training loss: 0.07666205, Validation loss: 0.04973517, Gradient norm: 9.94984055
INFO:root:[   56] Training loss: 0.07858119, Validation loss: 0.04857569, Gradient norm: 10.29916750
INFO:root:[   57] Training loss: 0.07466018, Validation loss: 0.05251387, Gradient norm: 9.04008600
INFO:root:[   58] Training loss: 0.07670767, Validation loss: 0.06708766, Gradient norm: 10.21512633
INFO:root:[   59] Training loss: 0.07465123, Validation loss: 0.05098180, Gradient norm: 10.09123811
INFO:root:[   60] Training loss: 0.07614466, Validation loss: 0.05798249, Gradient norm: 11.10868401
INFO:root:[   61] Training loss: 0.07169125, Validation loss: 0.05327640, Gradient norm: 8.46197176
INFO:root:[   62] Training loss: 0.07530707, Validation loss: 0.06085445, Gradient norm: 11.24840396
INFO:root:[   63] Training loss: 0.07542097, Validation loss: 0.07553960, Gradient norm: 13.62112478
INFO:root:[   64] Training loss: 0.07628072, Validation loss: 0.05961971, Gradient norm: 9.39868503
INFO:root:[   65] Training loss: 0.07659151, Validation loss: 0.05404121, Gradient norm: 11.14461331
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 1193.134s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05068
INFO:root:EnergyScoreTrain: 0.04076
INFO:root:CoverageTrain: 0.06243
INFO:root:IntervalWidthTrain: 0.00423
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05004
INFO:root:EnergyScoreValidation: 0.04066
INFO:root:CoverageValidation: 0.06494
INFO:root:IntervalWidthValidation: 0.00419
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06012
INFO:root:EnergyScoreTest: 0.04933
INFO:root:CoverageTest: 0.057
INFO:root:IntervalWidthTest: 0.00442
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1059061760
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.31874589, Validation loss: 0.78943959, Gradient norm: 9.17754223
INFO:root:[    2] Training loss: 0.78775260, Validation loss: 0.30975841, Gradient norm: 5.47087008
INFO:root:[    3] Training loss: 0.57502340, Validation loss: 0.42143953, Gradient norm: 4.81239985
INFO:root:[    4] Training loss: 0.47273999, Validation loss: 0.21775543, Gradient norm: 4.88557453
INFO:root:[    5] Training loss: 0.40570043, Validation loss: 0.46206138, Gradient norm: 3.52463363
INFO:root:[    6] Training loss: 0.36658619, Validation loss: 0.34706160, Gradient norm: 3.11347565
INFO:root:[    7] Training loss: 0.33249107, Validation loss: 0.46478669, Gradient norm: 2.57163661
INFO:root:[    8] Training loss: 0.30818304, Validation loss: 0.41810991, Gradient norm: 2.01976552
INFO:root:[    9] Training loss: 0.28920218, Validation loss: 0.51392566, Gradient norm: 1.88323365
INFO:root:[   10] Training loss: 0.27442111, Validation loss: 0.47095016, Gradient norm: 1.62267011
INFO:root:[   11] Training loss: 0.25930781, Validation loss: 0.40260616, Gradient norm: 1.69100758
INFO:root:[   12] Training loss: 0.24925974, Validation loss: 0.49874621, Gradient norm: 1.72073378
INFO:root:[   13] Training loss: 0.23577193, Validation loss: 0.47574221, Gradient norm: 1.34396894
INFO:root:[   14] Training loss: 0.22422462, Validation loss: 0.47294647, Gradient norm: 1.26960189
INFO:root:[   15] Training loss: 0.21368420, Validation loss: 0.45490515, Gradient norm: 1.62341694
INFO:root:[   16] Training loss: 0.20406272, Validation loss: 0.46009961, Gradient norm: 2.06198458
INFO:root:[   17] Training loss: 0.19330193, Validation loss: 0.42425201, Gradient norm: 1.64563128
INFO:root:[   18] Training loss: 0.18495619, Validation loss: 0.42365695, Gradient norm: 1.44513613
INFO:root:[   19] Training loss: 0.17372915, Validation loss: 0.38907389, Gradient norm: 1.33885799
INFO:root:[   20] Training loss: 0.16448843, Validation loss: 0.37301691, Gradient norm: 1.58348372
INFO:root:[   21] Training loss: 0.15422279, Validation loss: 0.37082814, Gradient norm: 2.22378310
INFO:root:[   22] Training loss: 0.14743920, Validation loss: 0.31425449, Gradient norm: 1.96651577
INFO:root:[   23] Training loss: 0.13843953, Validation loss: 0.30820106, Gradient norm: 2.11212721
INFO:root:[   24] Training loss: 0.13029298, Validation loss: 0.30095903, Gradient norm: 2.07159423
INFO:root:[   25] Training loss: 0.12600154, Validation loss: 0.20998652, Gradient norm: 2.83420149
INFO:root:[   26] Training loss: 0.11840927, Validation loss: 0.21569789, Gradient norm: 2.38825521
INFO:root:[   27] Training loss: 0.11369390, Validation loss: 0.20822329, Gradient norm: 2.21531994
INFO:root:[   28] Training loss: 0.10698099, Validation loss: 0.18532495, Gradient norm: 2.01779468
INFO:root:[   29] Training loss: 0.10194971, Validation loss: 0.16695249, Gradient norm: 2.40716532
INFO:root:[   30] Training loss: 0.09964017, Validation loss: 0.14306674, Gradient norm: 2.49753761
INFO:root:[   31] Training loss: 0.09472948, Validation loss: 0.13423305, Gradient norm: 2.60179849
INFO:root:[   32] Training loss: 0.09131402, Validation loss: 0.11594952, Gradient norm: 2.42671920
INFO:root:[   33] Training loss: 0.08911032, Validation loss: 0.08613640, Gradient norm: 3.03382718
INFO:root:[   34] Training loss: 0.08936489, Validation loss: 0.09420765, Gradient norm: 3.53571862
INFO:root:[   35] Training loss: 0.08814806, Validation loss: 0.07726897, Gradient norm: 3.55310828
INFO:root:[   36] Training loss: 0.08574215, Validation loss: 0.07902336, Gradient norm: 3.52354410
INFO:root:[   37] Training loss: 0.08579609, Validation loss: 0.07001266, Gradient norm: 3.82833721
INFO:root:[   38] Training loss: 0.08345204, Validation loss: 0.08398222, Gradient norm: 3.81347336
INFO:root:[   39] Training loss: 0.08333843, Validation loss: 0.07502103, Gradient norm: 3.45061339
INFO:root:[   40] Training loss: 0.08444333, Validation loss: 0.06963119, Gradient norm: 4.14543147
INFO:root:[   41] Training loss: 0.08272445, Validation loss: 0.09101604, Gradient norm: 4.89212842
INFO:root:[   42] Training loss: 0.08191781, Validation loss: 0.08340567, Gradient norm: 4.40790359
INFO:root:[   43] Training loss: 0.08073197, Validation loss: 0.08199130, Gradient norm: 4.13823123
INFO:root:[   44] Training loss: 0.08158602, Validation loss: 0.07413210, Gradient norm: 3.59068759
INFO:root:[   45] Training loss: 0.08075558, Validation loss: 0.07495787, Gradient norm: 5.12611794
INFO:root:[   46] Training loss: 0.08015156, Validation loss: 0.07898116, Gradient norm: 4.74017532
INFO:root:[   47] Training loss: 0.08059703, Validation loss: 0.07824919, Gradient norm: 4.58530262
INFO:root:[   48] Training loss: 0.07894713, Validation loss: 0.06597058, Gradient norm: 3.89115083
INFO:root:[   49] Training loss: 0.07961714, Validation loss: 0.07866751, Gradient norm: 4.35264595
INFO:root:[   50] Training loss: 0.07659270, Validation loss: 0.08939553, Gradient norm: 4.26612043
INFO:root:[   51] Training loss: 0.07766497, Validation loss: 0.08239818, Gradient norm: 4.53277503
INFO:root:[   52] Training loss: 0.07842262, Validation loss: 0.07364511, Gradient norm: 4.64688373
INFO:root:[   53] Training loss: 0.07882718, Validation loss: 0.09244743, Gradient norm: 3.96234445
INFO:root:[   54] Training loss: 0.07820005, Validation loss: 0.09116175, Gradient norm: 4.69784188
INFO:root:[   55] Training loss: 0.07749534, Validation loss: 0.06705974, Gradient norm: 5.60811336
INFO:root:[   56] Training loss: 0.07563889, Validation loss: 0.07126226, Gradient norm: 5.24597920
INFO:root:[   57] Training loss: 0.07438567, Validation loss: 0.06255922, Gradient norm: 4.94120638
INFO:root:[   58] Training loss: 0.07605766, Validation loss: 0.06625827, Gradient norm: 5.23771469
INFO:root:[   59] Training loss: 0.07684927, Validation loss: 0.07303239, Gradient norm: 5.60980070
INFO:root:[   60] Training loss: 0.07554167, Validation loss: 0.08024044, Gradient norm: 5.44111552
INFO:root:[   61] Training loss: 0.07525113, Validation loss: 0.09470213, Gradient norm: 4.76630844
INFO:root:[   62] Training loss: 0.07597669, Validation loss: 0.08726149, Gradient norm: 4.19760359
INFO:root:[   63] Training loss: 0.07298777, Validation loss: 0.08288192, Gradient norm: 3.92160790
INFO:root:[   64] Training loss: 0.07439132, Validation loss: 0.05878479, Gradient norm: 5.86280228
INFO:root:[   65] Training loss: 0.07444288, Validation loss: 0.07541827, Gradient norm: 5.29276327
INFO:root:[   66] Training loss: 0.07299736, Validation loss: 0.08175224, Gradient norm: 5.49182320
INFO:root:[   67] Training loss: 0.07210460, Validation loss: 0.07066194, Gradient norm: 4.71404222
INFO:root:[   68] Training loss: 0.07097544, Validation loss: 0.08250302, Gradient norm: 3.80214775
INFO:root:[   69] Training loss: 0.07493080, Validation loss: 0.08009565, Gradient norm: 5.66274533
INFO:root:[   70] Training loss: 0.07311840, Validation loss: 0.07602413, Gradient norm: 4.34256487
INFO:root:[   71] Training loss: 0.07122495, Validation loss: 0.06243660, Gradient norm: 4.98890302
INFO:root:[   72] Training loss: 0.07141507, Validation loss: 0.06760641, Gradient norm: 4.53247523
INFO:root:[   73] Training loss: 0.07187567, Validation loss: 0.05649431, Gradient norm: 4.73351425
INFO:root:[   74] Training loss: 0.07255674, Validation loss: 0.06083726, Gradient norm: 5.57229539
INFO:root:[   75] Training loss: 0.07033794, Validation loss: 0.07623175, Gradient norm: 4.11154116
INFO:root:[   76] Training loss: 0.06895418, Validation loss: 0.07120877, Gradient norm: 4.51017328
INFO:root:[   77] Training loss: 0.06920220, Validation loss: 0.10026960, Gradient norm: 4.60127173
INFO:root:[   78] Training loss: 0.07155155, Validation loss: 0.10361417, Gradient norm: 5.07291554
INFO:root:[   79] Training loss: 0.07134518, Validation loss: 0.08541887, Gradient norm: 4.81745791
INFO:root:[   80] Training loss: 0.07003425, Validation loss: 0.07702571, Gradient norm: 3.83028035
INFO:root:[   81] Training loss: 0.06974835, Validation loss: 0.05859019, Gradient norm: 4.33813264
INFO:root:[   82] Training loss: 0.06902431, Validation loss: 0.07402779, Gradient norm: 3.71027033
INFO:root:EP 82: Early stopping
INFO:root:Training the model took 1499.809s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06812
INFO:root:EnergyScoreTrain: 0.04403
INFO:root:CoverageTrain: 0.12153
INFO:root:IntervalWidthTrain: 0.01365
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06425
INFO:root:EnergyScoreValidation: 0.0405
INFO:root:CoverageValidation: 0.12505
INFO:root:IntervalWidthValidation: 0.01435
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07796
INFO:root:EnergyScoreTest: 0.05454
INFO:root:CoverageTest: 0.09574
INFO:root:IntervalWidthTest: 0.01211
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 1025507328
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.15355499, Validation loss: 0.59908294, Gradient norm: 5.20212257
INFO:root:[    2] Training loss: 0.66504073, Validation loss: 0.17405951, Gradient norm: 4.52233845
INFO:root:[    3] Training loss: 0.48630479, Validation loss: 0.15320887, Gradient norm: 3.25818232
INFO:root:[    4] Training loss: 0.40068857, Validation loss: 0.20461701, Gradient norm: 2.73073767
INFO:root:[    5] Training loss: 0.36098518, Validation loss: 0.17856150, Gradient norm: 3.19612045
INFO:root:[    6] Training loss: 0.32521237, Validation loss: 0.22155050, Gradient norm: 2.23687713
INFO:root:[    7] Training loss: 0.30092436, Validation loss: 0.27900295, Gradient norm: 1.91684864
INFO:root:[    8] Training loss: 0.28243934, Validation loss: 0.36019216, Gradient norm: 2.88058116
INFO:root:[    9] Training loss: 0.26388005, Validation loss: 0.40821962, Gradient norm: 2.58287194
INFO:root:[   10] Training loss: 0.25029969, Validation loss: 0.38862732, Gradient norm: 3.81101160
INFO:root:[   11] Training loss: 0.23319594, Validation loss: 0.42887773, Gradient norm: 3.30411870
INFO:root:[   12] Training loss: 0.22052731, Validation loss: 0.39254997, Gradient norm: 4.37947326
INFO:root:[   13] Training loss: 0.20785719, Validation loss: 0.32023894, Gradient norm: 3.40427797
INFO:root:[   14] Training loss: 0.19831449, Validation loss: 0.31033156, Gradient norm: 6.35401297
INFO:root:[   15] Training loss: 0.18588302, Validation loss: 0.33723148, Gradient norm: 4.11694589
INFO:root:[   16] Training loss: 0.17829996, Validation loss: 0.28898217, Gradient norm: 6.27001382
INFO:root:[   17] Training loss: 0.16802634, Validation loss: 0.28592018, Gradient norm: 5.15900429
INFO:root:[   18] Training loss: 0.15787512, Validation loss: 0.24513538, Gradient norm: 3.64624369
INFO:root:[   19] Training loss: 0.15210094, Validation loss: 0.24025052, Gradient norm: 6.03839054
INFO:root:[   20] Training loss: 0.14165959, Validation loss: 0.24866685, Gradient norm: 4.11769248
INFO:root:[   21] Training loss: 0.13776815, Validation loss: 0.20973275, Gradient norm: 7.63289987
INFO:root:[   22] Training loss: 0.13107585, Validation loss: 0.19918000, Gradient norm: 6.75213730
INFO:root:[   23] Training loss: 0.12707101, Validation loss: 0.14199942, Gradient norm: 7.46338353
INFO:root:[   24] Training loss: 0.12008812, Validation loss: 0.15469908, Gradient norm: 4.96593292
INFO:root:[   25] Training loss: 0.11610178, Validation loss: 0.12023057, Gradient norm: 5.97865252
INFO:root:[   26] Training loss: 0.11275794, Validation loss: 0.11243239, Gradient norm: 5.99776263
INFO:root:[   27] Training loss: 0.10999055, Validation loss: 0.10330267, Gradient norm: 6.72682415
INFO:root:[   28] Training loss: 0.11007743, Validation loss: 0.11406968, Gradient norm: 8.86266359
INFO:root:[   29] Training loss: 0.10928802, Validation loss: 0.09361829, Gradient norm: 8.38483038
INFO:root:[   30] Training loss: 0.10502160, Validation loss: 0.08706352, Gradient norm: 7.79892369
INFO:root:[   31] Training loss: 0.10677917, Validation loss: 0.08701020, Gradient norm: 9.72693874
INFO:root:[   32] Training loss: 0.10671195, Validation loss: 0.08630020, Gradient norm: 9.93553099
INFO:root:[   33] Training loss: 0.10238971, Validation loss: 0.07981744, Gradient norm: 6.76876778
INFO:root:[   34] Training loss: 0.10559562, Validation loss: 0.08152750, Gradient norm: 10.99498813
INFO:root:[   35] Training loss: 0.10455322, Validation loss: 0.07843801, Gradient norm: 9.24418602
INFO:root:[   36] Training loss: 0.10415087, Validation loss: 0.07351755, Gradient norm: 9.25733480
INFO:root:[   37] Training loss: 0.10201848, Validation loss: 0.07060201, Gradient norm: 8.60236770
INFO:root:[   38] Training loss: 0.10486869, Validation loss: 0.09053781, Gradient norm: 11.39177437
INFO:root:[   39] Training loss: 0.10245405, Validation loss: 0.09211568, Gradient norm: 8.58794992
INFO:root:[   40] Training loss: 0.09955385, Validation loss: 0.09447469, Gradient norm: 8.60157244
INFO:root:[   41] Training loss: 0.10045117, Validation loss: 0.09512700, Gradient norm: 10.40622549
INFO:root:[   42] Training loss: 0.10241301, Validation loss: 0.08537161, Gradient norm: 11.40994570
INFO:root:[   43] Training loss: 0.09991252, Validation loss: 0.07646224, Gradient norm: 9.31228484
INFO:root:[   44] Training loss: 0.10114226, Validation loss: 0.06848399, Gradient norm: 10.66327600
INFO:root:[   45] Training loss: 0.09887055, Validation loss: 0.07991158, Gradient norm: 10.85214024
INFO:root:[   46] Training loss: 0.10524860, Validation loss: 0.12626187, Gradient norm: 11.80680150
INFO:root:[   47] Training loss: 0.10153298, Validation loss: 0.09806249, Gradient norm: 11.80872616
INFO:root:[   48] Training loss: 0.09583180, Validation loss: 0.12735164, Gradient norm: 9.46335456
INFO:root:[   49] Training loss: 0.10122858, Validation loss: 0.09077932, Gradient norm: 12.64777291
INFO:root:[   50] Training loss: 0.09989431, Validation loss: 0.12485855, Gradient norm: 12.48412288
INFO:root:[   51] Training loss: 0.10081759, Validation loss: 0.07964780, Gradient norm: 11.59308979
INFO:root:[   52] Training loss: 0.09915992, Validation loss: 0.13125113, Gradient norm: 12.35930472
INFO:root:[   53] Training loss: 0.09791157, Validation loss: 0.09946890, Gradient norm: 10.13391094
INFO:root:[   54] Training loss: 0.09512755, Validation loss: 0.06777374, Gradient norm: 11.97778778
INFO:root:[   55] Training loss: 0.09738057, Validation loss: 0.07913487, Gradient norm: 11.35612829
INFO:root:[   56] Training loss: 0.09976545, Validation loss: 0.09390439, Gradient norm: 13.66237425
INFO:root:[   57] Training loss: 0.09944031, Validation loss: 0.07065027, Gradient norm: 11.29701080
INFO:root:[   58] Training loss: 0.09483342, Validation loss: 0.10143607, Gradient norm: 10.69277325
INFO:root:[   59] Training loss: 0.09447493, Validation loss: 0.05868234, Gradient norm: 10.92070956
INFO:root:[   60] Training loss: 0.09605647, Validation loss: 0.08326797, Gradient norm: 11.81166194
INFO:root:[   61] Training loss: 0.09239112, Validation loss: 0.12077208, Gradient norm: 10.75606786
INFO:root:[   62] Training loss: 0.09417300, Validation loss: 0.07093836, Gradient norm: 13.38556178
INFO:root:[   63] Training loss: 0.09524869, Validation loss: 0.06266256, Gradient norm: 12.72583739
INFO:root:[   64] Training loss: 0.09541457, Validation loss: 0.09162142, Gradient norm: 13.78376119
INFO:root:[   65] Training loss: 0.09266255, Validation loss: 0.07865246, Gradient norm: 11.52864047
INFO:root:[   66] Training loss: 0.09888235, Validation loss: 0.06522800, Gradient norm: 14.02145191
INFO:root:[   67] Training loss: 0.09155167, Validation loss: 0.07524804, Gradient norm: 8.90491328
INFO:root:[   68] Training loss: 0.09536064, Validation loss: 0.05739521, Gradient norm: 12.28493519
INFO:root:[   69] Training loss: 0.09118807, Validation loss: 0.09674374, Gradient norm: 8.84319874
INFO:root:[   70] Training loss: 0.09337000, Validation loss: 0.07583052, Gradient norm: 11.35016827
INFO:root:[   71] Training loss: 0.09080994, Validation loss: 0.08760512, Gradient norm: 10.04811800
INFO:root:[   72] Training loss: 0.09552635, Validation loss: 0.09936738, Gradient norm: 14.01716635
INFO:root:[   73] Training loss: 0.09298099, Validation loss: 0.09479371, Gradient norm: 12.44723294
INFO:root:[   74] Training loss: 0.09119478, Validation loss: 0.09392657, Gradient norm: 10.74451379
INFO:root:[   75] Training loss: 0.09027377, Validation loss: 0.07738348, Gradient norm: 10.24841381
INFO:root:[   76] Training loss: 0.09090050, Validation loss: 0.05814134, Gradient norm: 12.17211501
INFO:root:[   77] Training loss: 0.09231332, Validation loss: 0.06374449, Gradient norm: 12.29191855
INFO:root:EP 77: Early stopping
INFO:root:Training the model took 1412.332s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05851
INFO:root:EnergyScoreTrain: 0.05097
INFO:root:CoverageTrain: 0.03895
INFO:root:IntervalWidthTrain: 0.00264
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05797
INFO:root:EnergyScoreValidation: 0.05164
INFO:root:CoverageValidation: 0.03347
INFO:root:IntervalWidthValidation: 0.00221
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06255
INFO:root:EnergyScoreTest: 0.05685
INFO:root:CoverageTest: 0.03592
INFO:root:IntervalWidthTest: 0.00194
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 836763648
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.51427487, Validation loss: 0.26035692, Gradient norm: 7.23976594
INFO:root:[    2] Training loss: 0.24108312, Validation loss: 0.20038977, Gradient norm: 7.63672642
INFO:root:[    3] Training loss: 0.20824410, Validation loss: 0.15961733, Gradient norm: 7.33182522
INFO:root:[    4] Training loss: 0.17916134, Validation loss: 0.15896385, Gradient norm: 6.18222770
INFO:root:[    5] Training loss: 0.16000567, Validation loss: 0.20379044, Gradient norm: 4.85374795
INFO:root:[    6] Training loss: 0.15162891, Validation loss: 0.13398260, Gradient norm: 4.86275003
INFO:root:[    7] Training loss: 0.13491199, Validation loss: 0.12720491, Gradient norm: 3.81263878
INFO:root:[    8] Training loss: 0.12926947, Validation loss: 0.11551412, Gradient norm: 3.62181063
INFO:root:[    9] Training loss: 0.12042840, Validation loss: 0.10964548, Gradient norm: 3.46334515
INFO:root:[   10] Training loss: 0.11549381, Validation loss: 0.10319018, Gradient norm: 2.99689947
INFO:root:[   11] Training loss: 0.11159330, Validation loss: 0.10901491, Gradient norm: 2.93835294
INFO:root:[   12] Training loss: 0.10878372, Validation loss: 0.10182507, Gradient norm: 2.82628596
INFO:root:[   13] Training loss: 0.10377445, Validation loss: 0.11987413, Gradient norm: 2.46104731
INFO:root:[   14] Training loss: 0.10037472, Validation loss: 0.09229425, Gradient norm: 2.32055945
INFO:root:[   15] Training loss: 0.09618974, Validation loss: 0.10196173, Gradient norm: 1.89275761
INFO:root:[   16] Training loss: 0.09537586, Validation loss: 0.09933876, Gradient norm: 2.11522287
INFO:root:[   17] Training loss: 0.09232695, Validation loss: 0.08592884, Gradient norm: 1.93399646
INFO:root:[   18] Training loss: 0.08977151, Validation loss: 0.08414985, Gradient norm: 1.71376933
INFO:root:[   19] Training loss: 0.09250541, Validation loss: 0.08492634, Gradient norm: 1.80987009
INFO:root:[   20] Training loss: 0.08959280, Validation loss: 0.08835962, Gradient norm: 1.39887845
INFO:root:[   21] Training loss: 0.08870753, Validation loss: 0.08485663, Gradient norm: 1.60634593
INFO:root:[   22] Training loss: 0.08638382, Validation loss: 0.08256698, Gradient norm: 1.68253659
INFO:root:[   23] Training loss: 0.08779772, Validation loss: 0.08492312, Gradient norm: 1.72871065
INFO:root:[   24] Training loss: 0.08364107, Validation loss: 0.07919878, Gradient norm: 1.40377789
INFO:root:[   25] Training loss: 0.08323973, Validation loss: 0.07871566, Gradient norm: 1.52428716
INFO:root:[   26] Training loss: 0.08350756, Validation loss: 0.08130745, Gradient norm: 1.69067122
INFO:root:[   27] Training loss: 0.08314012, Validation loss: 0.07707588, Gradient norm: 1.52758646
INFO:root:[   28] Training loss: 0.08070846, Validation loss: 0.07821133, Gradient norm: 1.70177123
INFO:root:[   29] Training loss: 0.08073501, Validation loss: 0.07621007, Gradient norm: 1.32968266
INFO:root:[   30] Training loss: 0.07940688, Validation loss: 0.07475585, Gradient norm: 1.41389886
INFO:root:[   31] Training loss: 0.07876697, Validation loss: 0.08252504, Gradient norm: 1.47185986
INFO:root:[   32] Training loss: 0.07831839, Validation loss: 0.08313235, Gradient norm: 1.61093360
INFO:root:[   33] Training loss: 0.08021322, Validation loss: 0.08020315, Gradient norm: 1.98110225
INFO:root:[   34] Training loss: 0.07775922, Validation loss: 0.07843293, Gradient norm: 1.59024732
INFO:root:[   35] Training loss: 0.07641440, Validation loss: 0.07782575, Gradient norm: 1.47596883
INFO:root:[   36] Training loss: 0.07475551, Validation loss: 0.07379439, Gradient norm: 1.61539498
INFO:root:[   37] Training loss: 0.07473848, Validation loss: 0.07593864, Gradient norm: 1.49118269
INFO:root:[   38] Training loss: 0.07458252, Validation loss: 0.07602729, Gradient norm: 1.66391150
INFO:root:[   39] Training loss: 0.07474877, Validation loss: 0.07212150, Gradient norm: 1.34029763
INFO:root:[   40] Training loss: 0.07157128, Validation loss: 0.06910844, Gradient norm: 1.38484364
INFO:root:[   41] Training loss: 0.07136506, Validation loss: 0.06787068, Gradient norm: 1.56188264
INFO:root:[   42] Training loss: 0.07044567, Validation loss: 0.06824349, Gradient norm: 1.58357207
INFO:root:[   43] Training loss: 0.06948572, Validation loss: 0.06791056, Gradient norm: 1.60539435
INFO:root:[   44] Training loss: 0.07165263, Validation loss: 0.07177941, Gradient norm: 1.38017637
INFO:root:[   45] Training loss: 0.06919312, Validation loss: 0.06574546, Gradient norm: 1.78583003
INFO:root:[   46] Training loss: 0.06938670, Validation loss: 0.07025085, Gradient norm: 1.69124914
INFO:root:[   47] Training loss: 0.06797938, Validation loss: 0.06570124, Gradient norm: 1.70148068
INFO:root:[   48] Training loss: 0.06885719, Validation loss: 0.06880559, Gradient norm: 1.84206628
INFO:root:[   49] Training loss: 0.06713424, Validation loss: 0.06496849, Gradient norm: 1.61185947
INFO:root:[   50] Training loss: 0.06564960, Validation loss: 0.06511289, Gradient norm: 1.67099459
INFO:root:[   51] Training loss: 0.06575794, Validation loss: 0.06757336, Gradient norm: 1.88285064
INFO:root:[   52] Training loss: 0.06577381, Validation loss: 0.06538000, Gradient norm: 1.75892124
INFO:root:[   53] Training loss: 0.06533618, Validation loss: 0.06611709, Gradient norm: 1.97413129
INFO:root:[   54] Training loss: 0.06549074, Validation loss: 0.06722542, Gradient norm: 1.68124529
INFO:root:[   55] Training loss: 0.06529421, Validation loss: 0.06333683, Gradient norm: 1.80950386
INFO:root:[   56] Training loss: 0.06444112, Validation loss: 0.06652574, Gradient norm: 1.92432041
INFO:root:[   57] Training loss: 0.06338982, Validation loss: 0.06493115, Gradient norm: 1.78326751
INFO:root:[   58] Training loss: 0.06350359, Validation loss: 0.06320847, Gradient norm: 1.68751801
INFO:root:[   59] Training loss: 0.06195242, Validation loss: 0.06182094, Gradient norm: 1.87341826
INFO:root:[   60] Training loss: 0.06194956, Validation loss: 0.05950235, Gradient norm: 1.93708373
INFO:root:[   61] Training loss: 0.06173123, Validation loss: 0.05845395, Gradient norm: 2.04867594
INFO:root:[   62] Training loss: 0.06180030, Validation loss: 0.05862737, Gradient norm: 2.11002587
INFO:root:[   63] Training loss: 0.06087486, Validation loss: 0.05894451, Gradient norm: 2.12723331
INFO:root:[   64] Training loss: 0.06126767, Validation loss: 0.06217461, Gradient norm: 2.28405923
INFO:root:[   65] Training loss: 0.06099308, Validation loss: 0.05721240, Gradient norm: 2.07142746
INFO:root:[   66] Training loss: 0.05963477, Validation loss: 0.06068143, Gradient norm: 1.98588940
INFO:root:[   67] Training loss: 0.05938294, Validation loss: 0.06064164, Gradient norm: 1.98685837
INFO:root:[   68] Training loss: 0.05902192, Validation loss: 0.05960500, Gradient norm: 1.97152810
INFO:root:[   69] Training loss: 0.05849997, Validation loss: 0.06075570, Gradient norm: 2.18772343
INFO:root:[   70] Training loss: 0.05921834, Validation loss: 0.06040913, Gradient norm: 2.16322929
INFO:root:[   71] Training loss: 0.05774868, Validation loss: 0.05928113, Gradient norm: 2.29420238
INFO:root:[   72] Training loss: 0.05705625, Validation loss: 0.05774225, Gradient norm: 2.13007611
INFO:root:[   73] Training loss: 0.05810717, Validation loss: 0.05547383, Gradient norm: 2.17242915
INFO:root:[   74] Training loss: 0.05585460, Validation loss: 0.05478574, Gradient norm: 1.98692344
INFO:root:[   75] Training loss: 0.05634103, Validation loss: 0.05661556, Gradient norm: 2.27643761
INFO:root:[   76] Training loss: 0.05574797, Validation loss: 0.05538960, Gradient norm: 2.15454768
INFO:root:[   77] Training loss: 0.05611986, Validation loss: 0.05492379, Gradient norm: 2.18740210
INFO:root:[   78] Training loss: 0.05467519, Validation loss: 0.05472134, Gradient norm: 2.49678989
INFO:root:[   79] Training loss: 0.05399325, Validation loss: 0.05411593, Gradient norm: 2.53820001
INFO:root:[   80] Training loss: 0.05397355, Validation loss: 0.05427252, Gradient norm: 2.50138471
INFO:root:[   81] Training loss: 0.05389819, Validation loss: 0.05354387, Gradient norm: 2.74574241
INFO:root:[   82] Training loss: 0.05394412, Validation loss: 0.05366058, Gradient norm: 2.70455464
INFO:root:[   83] Training loss: 0.05334582, Validation loss: 0.05364021, Gradient norm: 2.87817904
INFO:root:[   84] Training loss: 0.05398574, Validation loss: 0.05556977, Gradient norm: 2.90556696
INFO:root:[   85] Training loss: 0.05458620, Validation loss: 0.05405765, Gradient norm: 2.91234728
INFO:root:[   86] Training loss: 0.05250385, Validation loss: 0.05212492, Gradient norm: 2.93977473
INFO:root:[   87] Training loss: 0.05340987, Validation loss: 0.05271888, Gradient norm: 2.75273733
INFO:root:[   88] Training loss: 0.05198930, Validation loss: 0.05177922, Gradient norm: 3.17754708
INFO:root:[   89] Training loss: 0.05220480, Validation loss: 0.05341014, Gradient norm: 2.99869410
INFO:root:[   90] Training loss: 0.05387027, Validation loss: 0.05219476, Gradient norm: 2.86941543
INFO:root:[   91] Training loss: 0.05156973, Validation loss: 0.05458771, Gradient norm: 3.03924997
INFO:root:[   92] Training loss: 0.05162598, Validation loss: 0.05193729, Gradient norm: 3.00457518
INFO:root:[   93] Training loss: 0.05117534, Validation loss: 0.05028960, Gradient norm: 3.23759112
INFO:root:[   94] Training loss: 0.05100985, Validation loss: 0.04940564, Gradient norm: 3.35244547
INFO:root:[   95] Training loss: 0.05040165, Validation loss: 0.05036854, Gradient norm: 3.33990862
INFO:root:[   96] Training loss: 0.05023090, Validation loss: 0.05114725, Gradient norm: 3.40799751
INFO:root:[   97] Training loss: 0.05104841, Validation loss: 0.05000075, Gradient norm: 3.14548354
INFO:root:[   98] Training loss: 0.05070164, Validation loss: 0.04859890, Gradient norm: 3.26447997
INFO:root:[   99] Training loss: 0.05025425, Validation loss: 0.04911001, Gradient norm: 3.32763543
INFO:root:[  100] Training loss: 0.04953456, Validation loss: 0.05016467, Gradient norm: 3.51257128
INFO:root:[  101] Training loss: 0.04942549, Validation loss: 0.05130834, Gradient norm: 3.52313897
INFO:root:[  102] Training loss: 0.04965572, Validation loss: 0.04918015, Gradient norm: 3.27816779
INFO:root:[  103] Training loss: 0.04960540, Validation loss: 0.05106798, Gradient norm: 3.29790539
INFO:root:[  104] Training loss: 0.04926192, Validation loss: 0.04815597, Gradient norm: 3.69536767
INFO:root:[  105] Training loss: 0.04918971, Validation loss: 0.04856008, Gradient norm: 3.36688236
INFO:root:[  106] Training loss: 0.04870467, Validation loss: 0.04724039, Gradient norm: 3.74884283
INFO:root:[  107] Training loss: 0.04814670, Validation loss: 0.04736944, Gradient norm: 3.97637544
INFO:root:[  108] Training loss: 0.04945470, Validation loss: 0.04983901, Gradient norm: 2.53926964
INFO:root:[  109] Training loss: 0.04939452, Validation loss: 0.04793661, Gradient norm: 3.54449170
INFO:root:[  110] Training loss: 0.04843608, Validation loss: 0.04652398, Gradient norm: 3.85545794
INFO:root:[  111] Training loss: 0.04775503, Validation loss: 0.04861447, Gradient norm: 3.72842482
INFO:root:[  112] Training loss: 0.04804639, Validation loss: 0.04730849, Gradient norm: 4.01070307
INFO:root:[  113] Training loss: 0.04795446, Validation loss: 0.05234785, Gradient norm: 3.58510661
INFO:root:[  114] Training loss: 0.04748554, Validation loss: 0.04499781, Gradient norm: 3.90487426
INFO:root:[  115] Training loss: 0.04726579, Validation loss: 0.04789114, Gradient norm: 3.93474498
INFO:root:[  116] Training loss: 0.04926977, Validation loss: 0.04729359, Gradient norm: 3.40678993
INFO:root:[  117] Training loss: 0.04682730, Validation loss: 0.04617111, Gradient norm: 3.92154509
INFO:root:[  118] Training loss: 0.04664897, Validation loss: 0.04731954, Gradient norm: 3.96885781
INFO:root:[  119] Training loss: 0.04880631, Validation loss: 0.04659534, Gradient norm: 2.77531806
INFO:root:[  120] Training loss: 0.04742667, Validation loss: 0.04654755, Gradient norm: 3.93846418
INFO:root:[  121] Training loss: 0.04638575, Validation loss: 0.04373491, Gradient norm: 4.17785613
INFO:root:[  122] Training loss: 0.04578455, Validation loss: 0.04483547, Gradient norm: 3.97361422
INFO:root:[  123] Training loss: 0.04607408, Validation loss: 0.04539370, Gradient norm: 4.29563982
INFO:root:[  124] Training loss: 0.04593091, Validation loss: 0.04612689, Gradient norm: 4.15975367
INFO:root:[  125] Training loss: 0.04581096, Validation loss: 0.04541403, Gradient norm: 3.79377679
INFO:root:[  126] Training loss: 0.04554963, Validation loss: 0.04540377, Gradient norm: 4.18545123
INFO:root:[  127] Training loss: 0.04552463, Validation loss: 0.04618336, Gradient norm: 4.01755943
INFO:root:[  128] Training loss: 0.04916885, Validation loss: 0.04737594, Gradient norm: 3.47730580
INFO:root:[  129] Training loss: 0.04540509, Validation loss: 0.04413263, Gradient norm: 4.21323361
INFO:root:[  130] Training loss: 0.04491261, Validation loss: 0.04647767, Gradient norm: 4.31701059
INFO:root:EP 130: Early stopping
INFO:root:Training the model took 2378.246s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03136
INFO:root:EnergyScoreTrain: 0.0233
INFO:root:CoverageTrain: 0.9203
INFO:root:IntervalWidthTrain: 0.03466
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03108
INFO:root:EnergyScoreValidation: 0.02309
INFO:root:CoverageValidation: 0.91992
INFO:root:IntervalWidthValidation: 0.03439
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03849
INFO:root:EnergyScoreTest: 0.02729
INFO:root:CoverageTest: 0.89571
INFO:root:IntervalWidthTest: 0.03471
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 832569344
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.73754044, Validation loss: 0.59355265, Gradient norm: 8.68615700
INFO:root:[    2] Training loss: 0.41435845, Validation loss: 0.36878496, Gradient norm: 8.36399194
INFO:root:[    3] Training loss: 0.34175868, Validation loss: 0.35158308, Gradient norm: 5.96762179
INFO:root:[    4] Training loss: 0.28718927, Validation loss: 0.27613196, Gradient norm: 4.54509047
INFO:root:[    5] Training loss: 0.26211605, Validation loss: 0.23575440, Gradient norm: 5.68157395
INFO:root:[    6] Training loss: 0.24160962, Validation loss: 0.22359616, Gradient norm: 5.85480061
INFO:root:[    7] Training loss: 0.21913403, Validation loss: 0.22186518, Gradient norm: 4.04766998
INFO:root:[    8] Training loss: 0.20496803, Validation loss: 0.19110300, Gradient norm: 3.77508108
INFO:root:[    9] Training loss: 0.19059139, Validation loss: 0.18599571, Gradient norm: 2.50317711
INFO:root:[   10] Training loss: 0.18282303, Validation loss: 0.17377387, Gradient norm: 2.40759168
INFO:root:[   11] Training loss: 0.17579529, Validation loss: 0.16753216, Gradient norm: 2.29136865
INFO:root:[   12] Training loss: 0.16908065, Validation loss: 0.16788288, Gradient norm: 1.58267221
INFO:root:[   13] Training loss: 0.16487990, Validation loss: 0.16411501, Gradient norm: 2.06611015
INFO:root:[   14] Training loss: 0.15996488, Validation loss: 0.15642623, Gradient norm: 2.08112097
INFO:root:[   15] Training loss: 0.15477503, Validation loss: 0.15140115, Gradient norm: 1.87740917
INFO:root:[   16] Training loss: 0.15151959, Validation loss: 0.15035565, Gradient norm: 1.99863009
INFO:root:[   17] Training loss: 0.14915912, Validation loss: 0.14487340, Gradient norm: 1.35242649
INFO:root:[   18] Training loss: 0.14534168, Validation loss: 0.14265127, Gradient norm: 0.89989997
INFO:root:[   19] Training loss: 0.14015527, Validation loss: 0.13722081, Gradient norm: 1.50279442
INFO:root:[   20] Training loss: 0.13703599, Validation loss: 0.13299747, Gradient norm: 2.02300595
INFO:root:[   21] Training loss: 0.13467975, Validation loss: 0.13573389, Gradient norm: 2.05443768
INFO:root:[   22] Training loss: 0.13195855, Validation loss: 0.12815631, Gradient norm: 1.74357043
INFO:root:[   23] Training loss: 0.12798793, Validation loss: 0.12671123, Gradient norm: 1.65612069
INFO:root:[   24] Training loss: 0.12565387, Validation loss: 0.12299928, Gradient norm: 1.84290454
INFO:root:[   25] Training loss: 0.12308848, Validation loss: 0.12118240, Gradient norm: 1.91659471
INFO:root:[   26] Training loss: 0.12009653, Validation loss: 0.11859653, Gradient norm: 2.07259911
INFO:root:[   27] Training loss: 0.11773206, Validation loss: 0.11470865, Gradient norm: 2.19377922
INFO:root:[   28] Training loss: 0.11534368, Validation loss: 0.11087719, Gradient norm: 2.16934660
INFO:root:[   29] Training loss: 0.11243533, Validation loss: 0.11132400, Gradient norm: 2.13127920
INFO:root:[   30] Training loss: 0.11016752, Validation loss: 0.11086529, Gradient norm: 2.03802137
INFO:root:[   31] Training loss: 0.10909106, Validation loss: 0.10773178, Gradient norm: 2.62757799
INFO:root:[   32] Training loss: 0.10619947, Validation loss: 0.10612007, Gradient norm: 2.46859162
INFO:root:[   33] Training loss: 0.10391655, Validation loss: 0.10155799, Gradient norm: 2.62390847
INFO:root:[   34] Training loss: 0.10113481, Validation loss: 0.10238389, Gradient norm: 2.53037659
INFO:root:[   35] Training loss: 0.10077318, Validation loss: 0.09848956, Gradient norm: 2.79642794
INFO:root:[   36] Training loss: 0.09845421, Validation loss: 0.09624711, Gradient norm: 2.83485735
INFO:root:[   37] Training loss: 0.09660670, Validation loss: 0.09551352, Gradient norm: 2.68849414
INFO:root:[   38] Training loss: 0.09648600, Validation loss: 0.09611182, Gradient norm: 2.59981301
INFO:root:[   39] Training loss: 0.09321366, Validation loss: 0.09198285, Gradient norm: 2.89896074
INFO:root:[   40] Training loss: 0.09152316, Validation loss: 0.09093211, Gradient norm: 2.80179785
INFO:root:[   41] Training loss: 0.09001507, Validation loss: 0.08773436, Gradient norm: 2.95757488
INFO:root:[   42] Training loss: 0.08834389, Validation loss: 0.08782627, Gradient norm: 3.22026696
INFO:root:[   43] Training loss: 0.08716143, Validation loss: 0.08506172, Gradient norm: 3.05415441
INFO:root:[   44] Training loss: 0.08547422, Validation loss: 0.08557172, Gradient norm: 2.67496398
INFO:root:[   45] Training loss: 0.08585341, Validation loss: 0.08191131, Gradient norm: 2.85093343
INFO:root:[   46] Training loss: 0.08180821, Validation loss: 0.08302782, Gradient norm: 2.75368707
INFO:root:[   47] Training loss: 0.08465041, Validation loss: 0.09187567, Gradient norm: 2.45348163
INFO:root:[   48] Training loss: 0.08188603, Validation loss: 0.07686122, Gradient norm: 3.22837614
INFO:root:[   49] Training loss: 0.07743556, Validation loss: 0.08200256, Gradient norm: 3.24985659
INFO:root:[   50] Training loss: 0.08233230, Validation loss: 0.07520878, Gradient norm: 3.08216156
INFO:root:[   51] Training loss: 0.07936778, Validation loss: 0.08047757, Gradient norm: 2.37423226
INFO:root:[   52] Training loss: 0.07587827, Validation loss: 0.07559176, Gradient norm: 2.94466825
INFO:root:[   53] Training loss: 0.07475839, Validation loss: 0.07529435, Gradient norm: 2.44170735
INFO:root:[   54] Training loss: 0.07137959, Validation loss: 0.06842388, Gradient norm: 2.94566755
INFO:root:[   55] Training loss: 0.07018217, Validation loss: 0.06745973, Gradient norm: 2.86355323
INFO:root:[   56] Training loss: 0.07040779, Validation loss: 0.06873339, Gradient norm: 2.90018752
INFO:root:[   57] Training loss: 0.06856767, Validation loss: 0.06858372, Gradient norm: 2.69870647
INFO:root:[   58] Training loss: 0.06814519, Validation loss: 0.07082357, Gradient norm: 3.20059052
INFO:root:[   59] Training loss: 0.07109831, Validation loss: 0.06639439, Gradient norm: 3.35223616
INFO:root:[   60] Training loss: 0.06703096, Validation loss: 0.06533963, Gradient norm: 2.47348853
INFO:root:[   61] Training loss: 0.06762759, Validation loss: 0.06577755, Gradient norm: 2.77308967
INFO:root:[   62] Training loss: 0.06285403, Validation loss: 0.06396539, Gradient norm: 2.79497307
INFO:root:[   63] Training loss: 0.06441154, Validation loss: 0.06149607, Gradient norm: 2.76551800
INFO:root:[   64] Training loss: 0.06165960, Validation loss: 0.06172891, Gradient norm: 2.91671892
INFO:root:[   65] Training loss: 0.06048550, Validation loss: 0.05956599, Gradient norm: 3.04506277
INFO:root:[   66] Training loss: 0.06480040, Validation loss: 0.05931576, Gradient norm: 2.73480192
INFO:root:[   67] Training loss: 0.05871467, Validation loss: 0.05847518, Gradient norm: 2.92833770
INFO:root:[   68] Training loss: 0.05988822, Validation loss: 0.05842678, Gradient norm: 3.66514790
INFO:root:[   69] Training loss: 0.06035336, Validation loss: 0.05643758, Gradient norm: 3.14410490
INFO:root:[   70] Training loss: 0.05758494, Validation loss: 0.05582633, Gradient norm: 3.06805852
INFO:root:[   71] Training loss: 0.06087613, Validation loss: 0.06942884, Gradient norm: 2.80089517
INFO:root:[   72] Training loss: 0.06059595, Validation loss: 0.05614143, Gradient norm: 2.74189981
INFO:root:[   73] Training loss: 0.05502167, Validation loss: 0.05347302, Gradient norm: 3.66413291
INFO:root:[   74] Training loss: 0.05493890, Validation loss: 0.05314951, Gradient norm: 3.79986950
INFO:root:[   75] Training loss: 0.05462342, Validation loss: 0.05291340, Gradient norm: 3.40313879
INFO:root:[   76] Training loss: 0.05417154, Validation loss: 0.05313624, Gradient norm: 3.58355701
INFO:root:[   77] Training loss: 0.05356945, Validation loss: 0.05011475, Gradient norm: 3.85860835
INFO:root:[   78] Training loss: 0.05481036, Validation loss: 0.05530604, Gradient norm: 3.94329791
INFO:root:[   79] Training loss: 0.05460097, Validation loss: 0.05264801, Gradient norm: 3.39759930
INFO:root:[   80] Training loss: 0.05231117, Validation loss: 0.05294471, Gradient norm: 4.24328613
INFO:root:[   81] Training loss: 0.05383943, Validation loss: 0.05322154, Gradient norm: 3.40808009
INFO:root:[   82] Training loss: 0.05269333, Validation loss: 0.05599802, Gradient norm: 3.74612506
INFO:root:[   83] Training loss: 0.05158913, Validation loss: 0.05130527, Gradient norm: 4.44178920
INFO:root:[   84] Training loss: 0.05297517, Validation loss: 0.05010638, Gradient norm: 3.67823434
INFO:root:[   85] Training loss: 0.05257615, Validation loss: 0.05752173, Gradient norm: 2.61689694
INFO:root:[   86] Training loss: 0.05124129, Validation loss: 0.05226750, Gradient norm: 3.56685152
INFO:root:[   87] Training loss: 0.05088779, Validation loss: 0.05026879, Gradient norm: 3.36051864
INFO:root:[   88] Training loss: 0.05097116, Validation loss: 0.05525349, Gradient norm: 3.52884992
INFO:root:[   89] Training loss: 0.05079334, Validation loss: 0.05115496, Gradient norm: 4.24015619
INFO:root:[   90] Training loss: 0.05153570, Validation loss: 0.04791097, Gradient norm: 3.15149447
INFO:root:[   91] Training loss: 0.05126034, Validation loss: 0.05159714, Gradient norm: 5.07534460
INFO:root:[   92] Training loss: 0.04831420, Validation loss: 0.05340552, Gradient norm: 3.96613544
INFO:root:[   93] Training loss: 0.05000816, Validation loss: 0.05344643, Gradient norm: 4.58154304
INFO:root:[   94] Training loss: 0.05164954, Validation loss: 0.05113494, Gradient norm: 4.33427248
INFO:root:[   95] Training loss: 0.04912849, Validation loss: 0.04728685, Gradient norm: 4.49705069
INFO:root:[   96] Training loss: 0.04940408, Validation loss: 0.04952854, Gradient norm: 4.25822678
INFO:root:[   97] Training loss: 0.04937950, Validation loss: 0.04479322, Gradient norm: 5.01784866
INFO:root:[   98] Training loss: 0.04806767, Validation loss: 0.04880455, Gradient norm: 4.12070386
INFO:root:[   99] Training loss: 0.04728707, Validation loss: 0.04397894, Gradient norm: 4.54577156
INFO:root:[  100] Training loss: 0.04880926, Validation loss: 0.04578559, Gradient norm: 5.35368762
INFO:root:[  101] Training loss: 0.04795142, Validation loss: 0.04868250, Gradient norm: 4.08845340
INFO:root:[  102] Training loss: 0.04765713, Validation loss: 0.04616779, Gradient norm: 4.18008481
INFO:root:[  103] Training loss: 0.04837047, Validation loss: 0.04530565, Gradient norm: 5.16877424
INFO:root:[  104] Training loss: 0.04746680, Validation loss: 0.05040208, Gradient norm: 4.68425511
INFO:root:[  105] Training loss: 0.04773481, Validation loss: 0.04720627, Gradient norm: 4.70363812
INFO:root:[  106] Training loss: 0.04643307, Validation loss: 0.04389840, Gradient norm: 4.67091156
INFO:root:[  107] Training loss: 0.04874885, Validation loss: 0.04689014, Gradient norm: 5.68053754
INFO:root:[  108] Training loss: 0.04412654, Validation loss: 0.04493214, Gradient norm: 4.51216038
INFO:root:[  109] Training loss: 0.04757538, Validation loss: 0.05364733, Gradient norm: 5.58829061
INFO:root:[  110] Training loss: 0.04675387, Validation loss: 0.04566174, Gradient norm: 4.58663662
INFO:root:[  111] Training loss: 0.04603034, Validation loss: 0.04857468, Gradient norm: 4.80621707
INFO:root:[  112] Training loss: 0.04721691, Validation loss: 0.05125881, Gradient norm: 6.19653084
INFO:root:[  113] Training loss: 0.04806133, Validation loss: 0.04320623, Gradient norm: 5.19540982
INFO:root:[  114] Training loss: 0.04523738, Validation loss: 0.04628077, Gradient norm: 5.04389952
INFO:root:[  115] Training loss: 0.04559455, Validation loss: 0.04503797, Gradient norm: 5.76461305
INFO:root:[  116] Training loss: 0.05044850, Validation loss: 0.04353050, Gradient norm: 6.70168743
INFO:root:[  117] Training loss: 0.04541669, Validation loss: 0.04493988, Gradient norm: 4.51365437
INFO:root:[  118] Training loss: 0.04552762, Validation loss: 0.04651756, Gradient norm: 3.97079366
INFO:root:[  119] Training loss: 0.04669280, Validation loss: 0.04834667, Gradient norm: 5.81075231
INFO:root:[  120] Training loss: 0.04661221, Validation loss: 0.04407806, Gradient norm: 5.15476709
INFO:root:[  121] Training loss: 0.04427101, Validation loss: 0.04586129, Gradient norm: 4.82660310
INFO:root:[  122] Training loss: 0.04516849, Validation loss: 0.04258590, Gradient norm: 5.82380837
INFO:root:[  123] Training loss: 0.04454787, Validation loss: 0.04344369, Gradient norm: 5.26718028
INFO:root:[  124] Training loss: 0.04498882, Validation loss: 0.04081141, Gradient norm: 5.48571419
INFO:root:[  125] Training loss: 0.04646891, Validation loss: 0.04643801, Gradient norm: 5.48957131
INFO:root:[  126] Training loss: 0.04489718, Validation loss: 0.04398519, Gradient norm: 4.90893698
INFO:root:[  127] Training loss: 0.04458114, Validation loss: 0.04324743, Gradient norm: 4.95522639
INFO:root:[  128] Training loss: 0.04516538, Validation loss: 0.04487070, Gradient norm: 5.21654790
INFO:root:[  129] Training loss: 0.04363210, Validation loss: 0.04157009, Gradient norm: 6.01108486
INFO:root:[  130] Training loss: 0.04462546, Validation loss: 0.04968845, Gradient norm: 5.15579275
INFO:root:[  131] Training loss: 0.04352239, Validation loss: 0.04168685, Gradient norm: 5.42743849
INFO:root:[  132] Training loss: 0.04439595, Validation loss: 0.04112514, Gradient norm: 4.73853616
INFO:root:[  133] Training loss: 0.04620072, Validation loss: 0.04617122, Gradient norm: 5.93333703
INFO:root:EP 133: Early stopping
INFO:root:Training the model took 2438.418s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02745
INFO:root:EnergyScoreTrain: 0.02031
INFO:root:CoverageTrain: 0.56791
INFO:root:IntervalWidthTrain: 0.01835
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02724
INFO:root:EnergyScoreValidation: 0.02017
INFO:root:CoverageValidation: 0.56752
INFO:root:IntervalWidthValidation: 0.01798
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03543
INFO:root:EnergyScoreTest: 0.02789
INFO:root:CoverageTest: 0.59812
INFO:root:IntervalWidthTest: 0.01811
INFO:root:###21 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.85306241, Validation loss: 0.53805567, Gradient norm: 9.66385477
INFO:root:[    2] Training loss: 0.49402552, Validation loss: 0.44034312, Gradient norm: 7.47665564
INFO:root:[    3] Training loss: 0.39981955, Validation loss: 0.35521223, Gradient norm: 6.28891849
INFO:root:[    4] Training loss: 0.33278106, Validation loss: 0.30386331, Gradient norm: 4.71381706
INFO:root:[    5] Training loss: 0.30599480, Validation loss: 0.27822975, Gradient norm: 5.05881355
INFO:root:[    6] Training loss: 0.27450373, Validation loss: 0.27703016, Gradient norm: 2.96459767
INFO:root:[    7] Training loss: 0.25500570, Validation loss: 0.24533016, Gradient norm: 2.13311185
INFO:root:[    8] Training loss: 0.24440975, Validation loss: 0.23367774, Gradient norm: 2.81523464
INFO:root:[    9] Training loss: 0.23233343, Validation loss: 0.22535716, Gradient norm: 1.74579972
INFO:root:[   10] Training loss: 0.22527772, Validation loss: 0.24267646, Gradient norm: 2.18097419
INFO:root:[   11] Training loss: 0.21981058, Validation loss: 0.21282583, Gradient norm: 2.66198445
INFO:root:[   12] Training loss: 0.21116399, Validation loss: 0.20699451, Gradient norm: 1.51118067
INFO:root:[   13] Training loss: 0.20607654, Validation loss: 0.20048145, Gradient norm: 2.00252959
INFO:root:[   14] Training loss: 0.19887781, Validation loss: 0.19535324, Gradient norm: 1.56826384
INFO:root:[   15] Training loss: 0.19404389, Validation loss: 0.19165190, Gradient norm: 1.81908394
INFO:root:[   16] Training loss: 0.18915529, Validation loss: 0.18565494, Gradient norm: 2.18703485
INFO:root:[   17] Training loss: 0.18451091, Validation loss: 0.18231291, Gradient norm: 1.95887682
INFO:root:[   18] Training loss: 0.17973424, Validation loss: 0.17646709, Gradient norm: 1.55378800
INFO:root:[   19] Training loss: 0.17265421, Validation loss: 0.16930729, Gradient norm: 2.02803451
INFO:root:[   20] Training loss: 0.16718405, Validation loss: 0.16302004, Gradient norm: 1.99936748
INFO:root:[   21] Training loss: 0.16231392, Validation loss: 0.15888442, Gradient norm: 2.17788272
INFO:root:[   22] Training loss: 0.15823710, Validation loss: 0.15337840, Gradient norm: 2.14649913
INFO:root:[   23] Training loss: 0.15233207, Validation loss: 0.14668118, Gradient norm: 1.84441227
INFO:root:[   24] Training loss: 0.14816862, Validation loss: 0.14299523, Gradient norm: 2.33338774
INFO:root:[   25] Training loss: 0.14341737, Validation loss: 0.14075578, Gradient norm: 2.78381380
INFO:root:[   26] Training loss: 0.13907863, Validation loss: 0.13535447, Gradient norm: 2.28272105
INFO:root:[   27] Training loss: 0.13617450, Validation loss: 0.13388950, Gradient norm: 2.11331231
INFO:root:[   28] Training loss: 0.12952527, Validation loss: 0.12563998, Gradient norm: 2.23461138
INFO:root:[   29] Training loss: 0.12591090, Validation loss: 0.12238677, Gradient norm: 2.46219732
INFO:root:[   30] Training loss: 0.12082996, Validation loss: 0.11951359, Gradient norm: 2.65838053
INFO:root:[   31] Training loss: 0.11822714, Validation loss: 0.11954994, Gradient norm: 2.45189161
INFO:root:[   32] Training loss: 0.11487921, Validation loss: 0.11578270, Gradient norm: 2.42905731
INFO:root:[   33] Training loss: 0.11198852, Validation loss: 0.10644279, Gradient norm: 2.50769524
INFO:root:[   34] Training loss: 0.10623642, Validation loss: 0.10646113, Gradient norm: 3.26823116
INFO:root:[   35] Training loss: 0.10312372, Validation loss: 0.09891629, Gradient norm: 3.37221721
INFO:root:[   36] Training loss: 0.09948098, Validation loss: 0.09790730, Gradient norm: 2.71013817
INFO:root:[   37] Training loss: 0.09674838, Validation loss: 0.09248524, Gradient norm: 3.71300745
INFO:root:[   38] Training loss: 0.09656791, Validation loss: 0.09436424, Gradient norm: 2.95423349
INFO:root:[   39] Training loss: 0.09522721, Validation loss: 0.09062181, Gradient norm: 2.92232724
INFO:root:[   40] Training loss: 0.08841471, Validation loss: 0.08561089, Gradient norm: 3.09615404
INFO:root:[   41] Training loss: 0.08604834, Validation loss: 0.08536014, Gradient norm: 3.06407846
INFO:root:[   42] Training loss: 0.08328201, Validation loss: 0.08069944, Gradient norm: 3.24550264
INFO:root:[   43] Training loss: 0.08217377, Validation loss: 0.07815028, Gradient norm: 3.47971120
INFO:root:[   44] Training loss: 0.08132478, Validation loss: 0.07804707, Gradient norm: 2.89432206
INFO:root:[   45] Training loss: 0.08115549, Validation loss: 0.07896068, Gradient norm: 3.12072515
INFO:root:[   46] Training loss: 0.07869780, Validation loss: 0.07788879, Gradient norm: 2.74153923
INFO:root:[   47] Training loss: 0.07670810, Validation loss: 0.07294897, Gradient norm: 2.54733528
INFO:root:[   48] Training loss: 0.07539080, Validation loss: 0.07872521, Gradient norm: 3.44951900
INFO:root:[   49] Training loss: 0.07227893, Validation loss: 0.07249730, Gradient norm: 3.07614058
INFO:root:[   50] Training loss: 0.07158885, Validation loss: 0.06577926, Gradient norm: 2.35105840
INFO:root:[   51] Training loss: 0.06721997, Validation loss: 0.06637936, Gradient norm: 2.42063592
INFO:root:[   52] Training loss: 0.06825524, Validation loss: 0.06821198, Gradient norm: 2.70514889
INFO:root:[   53] Training loss: 0.06608431, Validation loss: 0.06159787, Gradient norm: 2.93400979
INFO:root:[   54] Training loss: 0.06683699, Validation loss: 0.06391595, Gradient norm: 2.85281797
INFO:root:[   55] Training loss: 0.06470384, Validation loss: 0.06366510, Gradient norm: 2.82986216
INFO:root:[   56] Training loss: 0.06373942, Validation loss: 0.06388347, Gradient norm: 2.85261068
INFO:root:[   57] Training loss: 0.06249993, Validation loss: 0.06142349, Gradient norm: 2.81097660
INFO:root:[   58] Training loss: 0.06308950, Validation loss: 0.06110472, Gradient norm: 3.38899947
INFO:root:[   59] Training loss: 0.06113105, Validation loss: 0.06070642, Gradient norm: 3.15693397
INFO:root:[   60] Training loss: 0.05989499, Validation loss: 0.06206321, Gradient norm: 3.31948460
INFO:root:[   61] Training loss: 0.06050953, Validation loss: 0.05790745, Gradient norm: 3.21683174
INFO:root:[   62] Training loss: 0.06004564, Validation loss: 0.06242592, Gradient norm: 3.64329505
INFO:root:[   63] Training loss: 0.05953592, Validation loss: 0.05800714, Gradient norm: 3.88798027
INFO:root:[   64] Training loss: 0.05887119, Validation loss: 0.06046325, Gradient norm: 3.71000770
INFO:root:[   65] Training loss: 0.06138720, Validation loss: 0.05878426, Gradient norm: 4.12907227
INFO:root:[   66] Training loss: 0.05995124, Validation loss: 0.05731774, Gradient norm: 3.79006361
INFO:root:[   67] Training loss: 0.05942331, Validation loss: 0.05801248, Gradient norm: 3.37966394
INFO:root:[   68] Training loss: 0.05949129, Validation loss: 0.05984469, Gradient norm: 3.41317690
INFO:root:[   69] Training loss: 0.06070599, Validation loss: 0.05855715, Gradient norm: 3.73230236
INFO:root:[   70] Training loss: 0.05951548, Validation loss: 0.05783120, Gradient norm: 4.19216239
INFO:root:[   71] Training loss: 0.05943164, Validation loss: 0.06432896, Gradient norm: 4.73419007
INFO:root:[   72] Training loss: 0.05786802, Validation loss: 0.05806620, Gradient norm: 3.52999435
INFO:root:[   73] Training loss: 0.05841431, Validation loss: 0.05481619, Gradient norm: 3.81365622
INFO:root:[   74] Training loss: 0.05866539, Validation loss: 0.06027729, Gradient norm: 4.70730974
INFO:root:[   75] Training loss: 0.05788645, Validation loss: 0.05433285, Gradient norm: 4.85753076
INFO:root:[   76] Training loss: 0.05834986, Validation loss: 0.05708248, Gradient norm: 4.95570714
INFO:root:[   77] Training loss: 0.05803881, Validation loss: 0.05535849, Gradient norm: 5.71799184
INFO:root:[   78] Training loss: 0.06080769, Validation loss: 0.05866801, Gradient norm: 4.80028389
INFO:root:[   79] Training loss: 0.05756393, Validation loss: 0.05603439, Gradient norm: 5.49204234
INFO:root:[   80] Training loss: 0.05732893, Validation loss: 0.05491886, Gradient norm: 5.37214450
INFO:root:[   81] Training loss: 0.05521370, Validation loss: 0.05299756, Gradient norm: 4.98002551
INFO:root:[   82] Training loss: 0.05818162, Validation loss: 0.05361047, Gradient norm: 6.31086486
INFO:root:[   83] Training loss: 0.05677661, Validation loss: 0.05566130, Gradient norm: 4.39498788
INFO:root:[   84] Training loss: 0.05736561, Validation loss: 0.05764841, Gradient norm: 5.53789197
INFO:root:[   85] Training loss: 0.05559010, Validation loss: 0.05422827, Gradient norm: 4.29831592
INFO:root:[   86] Training loss: 0.05817055, Validation loss: 0.05350872, Gradient norm: 5.96262192
INFO:root:[   87] Training loss: 0.05598238, Validation loss: 0.05403877, Gradient norm: 5.06836220
INFO:root:[   88] Training loss: 0.05636025, Validation loss: 0.05538018, Gradient norm: 6.63869311
INFO:root:[   89] Training loss: 0.05613864, Validation loss: 0.05132206, Gradient norm: 5.50458296
INFO:root:[   90] Training loss: 0.05429420, Validation loss: 0.06009384, Gradient norm: 5.26822122
INFO:root:[   91] Training loss: 0.05803166, Validation loss: 0.05838967, Gradient norm: 6.98072379
INFO:root:[   92] Training loss: 0.05379970, Validation loss: 0.05347490, Gradient norm: 4.42181434
INFO:root:[   93] Training loss: 0.05525874, Validation loss: 0.06980662, Gradient norm: 6.43739803
INFO:root:[   94] Training loss: 0.05725449, Validation loss: 0.05999451, Gradient norm: 5.25638484
INFO:root:[   95] Training loss: 0.05499276, Validation loss: 0.05378194, Gradient norm: 6.06751063
INFO:root:[   96] Training loss: 0.05350332, Validation loss: 0.05396200, Gradient norm: 4.99189737
INFO:root:[   97] Training loss: 0.05604731, Validation loss: 0.05594799, Gradient norm: 6.71411140
INFO:root:[   98] Training loss: 0.05423173, Validation loss: 0.07680773, Gradient norm: 5.54171881
INFO:root:[   99] Training loss: 0.05819759, Validation loss: 0.05131049, Gradient norm: 6.71240355
INFO:root:[  100] Training loss: 0.05209625, Validation loss: 0.05242710, Gradient norm: 4.41472776
INFO:root:[  101] Training loss: 0.05262635, Validation loss: 0.05757765, Gradient norm: 4.67611402
INFO:root:[  102] Training loss: 0.05490097, Validation loss: 0.05413251, Gradient norm: 5.65332525
INFO:root:[  103] Training loss: 0.05181184, Validation loss: 0.05012360, Gradient norm: 3.78210692
INFO:root:[  104] Training loss: 0.05348443, Validation loss: 0.05224898, Gradient norm: 5.48123265
INFO:root:[  105] Training loss: 0.05310281, Validation loss: 0.05261967, Gradient norm: 5.62215932
INFO:root:[  106] Training loss: 0.05328303, Validation loss: 0.05700801, Gradient norm: 5.85839985
INFO:root:[  107] Training loss: 0.05326633, Validation loss: 0.05040574, Gradient norm: 5.57716374
INFO:root:[  108] Training loss: 0.05276945, Validation loss: 0.05097189, Gradient norm: 4.96621431
INFO:root:[  109] Training loss: 0.05246100, Validation loss: 0.05545814, Gradient norm: 5.18483389
INFO:root:[  110] Training loss: 0.05246585, Validation loss: 0.05993841, Gradient norm: 5.44280222
INFO:root:[  111] Training loss: 0.05396164, Validation loss: 0.05289145, Gradient norm: 5.45258256
INFO:root:[  112] Training loss: 0.05325041, Validation loss: 0.05077609, Gradient norm: 5.74221683
INFO:root:EP 112: Early stopping
INFO:root:Training the model took 2056.954s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03326
INFO:root:EnergyScoreTrain: 0.00736
INFO:root:CoverageTrain: 0.78189
INFO:root:IntervalWidthTrain: 0.02346
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03292
INFO:root:EnergyScoreValidation: 0.00713
INFO:root:CoverageValidation: 0.7871
INFO:root:IntervalWidthValidation: 0.02296
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04094
INFO:root:EnergyScoreTest: 0.01372
INFO:root:CoverageTest: 0.78452
INFO:root:IntervalWidthTest: 0.02296
INFO:root:###22 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 216006656
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.04095369, Validation loss: 0.79162588, Gradient norm: 6.86613297
INFO:root:[    2] Training loss: 0.60912111, Validation loss: 0.50923524, Gradient norm: 5.33382599
INFO:root:[    3] Training loss: 0.46141011, Validation loss: 0.40890859, Gradient norm: 3.20732253
INFO:root:[    4] Training loss: 0.38996709, Validation loss: 0.35708806, Gradient norm: 2.71862238
INFO:root:[    5] Training loss: 0.35014818, Validation loss: 0.32545356, Gradient norm: 2.87459746
INFO:root:[    6] Training loss: 0.31719321, Validation loss: 0.30190695, Gradient norm: 1.95379953
INFO:root:[    7] Training loss: 0.29551077, Validation loss: 0.28597204, Gradient norm: 1.69656106
INFO:root:[    8] Training loss: 0.27962656, Validation loss: 0.26855032, Gradient norm: 1.47450248
INFO:root:[    9] Training loss: 0.26646218, Validation loss: 0.26131945, Gradient norm: 1.16333054
INFO:root:[   10] Training loss: 0.25548563, Validation loss: 0.24988945, Gradient norm: 1.42673077
INFO:root:[   11] Training loss: 0.24615404, Validation loss: 0.23857743, Gradient norm: 1.18926735
INFO:root:[   12] Training loss: 0.23689920, Validation loss: 0.22959151, Gradient norm: 1.10150362
INFO:root:[   13] Training loss: 0.22658352, Validation loss: 0.21916219, Gradient norm: 0.98570494
INFO:root:[   14] Training loss: 0.21791066, Validation loss: 0.21132143, Gradient norm: 1.30876731
INFO:root:[   15] Training loss: 0.20974798, Validation loss: 0.20775990, Gradient norm: 1.34149510
INFO:root:[   16] Training loss: 0.20289372, Validation loss: 0.20197980, Gradient norm: 1.53760427
INFO:root:[   17] Training loss: 0.19382552, Validation loss: 0.19039484, Gradient norm: 1.41354672
INFO:root:[   18] Training loss: 0.18518571, Validation loss: 0.17890432, Gradient norm: 1.09130113
INFO:root:[   19] Training loss: 0.17820918, Validation loss: 0.17571521, Gradient norm: 1.71266801
INFO:root:[   20] Training loss: 0.17014117, Validation loss: 0.16842165, Gradient norm: 1.36520765
INFO:root:[   21] Training loss: 0.16390461, Validation loss: 0.16098671, Gradient norm: 1.43998249
INFO:root:[   22] Training loss: 0.15557923, Validation loss: 0.14882782, Gradient norm: 1.52925033
INFO:root:[   23] Training loss: 0.14953937, Validation loss: 0.14290771, Gradient norm: 1.54532568
INFO:root:[   24] Training loss: 0.14106951, Validation loss: 0.13592496, Gradient norm: 1.73384507
INFO:root:[   25] Training loss: 0.13581300, Validation loss: 0.13337448, Gradient norm: 1.74632724
INFO:root:[   26] Training loss: 0.13121974, Validation loss: 0.12840188, Gradient norm: 2.31910968
INFO:root:[   27] Training loss: 0.12518904, Validation loss: 0.12228436, Gradient norm: 1.12374198
INFO:root:[   28] Training loss: 0.11849718, Validation loss: 0.11239468, Gradient norm: 1.60932757
INFO:root:[   29] Training loss: 0.11295577, Validation loss: 0.11054795, Gradient norm: 1.43847643
INFO:root:[   30] Training loss: 0.10780212, Validation loss: 0.10095740, Gradient norm: 2.01971469
INFO:root:[   31] Training loss: 0.10225157, Validation loss: 0.09850120, Gradient norm: 2.21471250
INFO:root:[   32] Training loss: 0.09930434, Validation loss: 0.10414157, Gradient norm: 1.91995368
INFO:root:[   33] Training loss: 0.09719924, Validation loss: 0.09291935, Gradient norm: 2.18489012
INFO:root:[   34] Training loss: 0.09262721, Validation loss: 0.09195949, Gradient norm: 2.09958634
INFO:root:[   35] Training loss: 0.09107436, Validation loss: 0.08915375, Gradient norm: 2.48050928
INFO:root:[   36] Training loss: 0.08837546, Validation loss: 0.08409257, Gradient norm: 2.18297758
INFO:root:[   37] Training loss: 0.08300366, Validation loss: 0.07836270, Gradient norm: 2.33131219
INFO:root:[   38] Training loss: 0.07921074, Validation loss: 0.07679269, Gradient norm: 2.27326534
INFO:root:[   39] Training loss: 0.07683161, Validation loss: 0.07095901, Gradient norm: 2.42446907
INFO:root:[   40] Training loss: 0.07450640, Validation loss: 0.07151996, Gradient norm: 2.82401224
INFO:root:[   41] Training loss: 0.07672960, Validation loss: 0.07757211, Gradient norm: 2.88036381
INFO:root:[   42] Training loss: 0.07484932, Validation loss: 0.07555525, Gradient norm: 2.84250212
INFO:root:[   43] Training loss: 0.07456249, Validation loss: 0.06899861, Gradient norm: 2.81965982
INFO:root:[   44] Training loss: 0.07138645, Validation loss: 0.07368324, Gradient norm: 2.34320064
INFO:root:[   45] Training loss: 0.07293537, Validation loss: 0.06745976, Gradient norm: 3.05729444
INFO:root:[   46] Training loss: 0.06870331, Validation loss: 0.06832191, Gradient norm: 3.11918388
INFO:root:[   47] Training loss: 0.07072504, Validation loss: 0.07484876, Gradient norm: 2.69061376
INFO:root:[   48] Training loss: 0.06891499, Validation loss: 0.06945285, Gradient norm: 3.93683369
INFO:root:[   49] Training loss: 0.06866752, Validation loss: 0.06822315, Gradient norm: 3.72832366
INFO:root:[   50] Training loss: 0.07178807, Validation loss: 0.06759689, Gradient norm: 3.41468113
INFO:root:[   51] Training loss: 0.06720811, Validation loss: 0.06655334, Gradient norm: 2.92564122
INFO:root:[   52] Training loss: 0.07080747, Validation loss: 0.07315216, Gradient norm: 2.71711886
INFO:root:[   53] Training loss: 0.06820650, Validation loss: 0.06568327, Gradient norm: 3.44516206
INFO:root:[   54] Training loss: 0.06658376, Validation loss: 0.06607565, Gradient norm: 4.41233250
INFO:root:[   55] Training loss: 0.06874664, Validation loss: 0.06234090, Gradient norm: 3.82984943
INFO:root:[   56] Training loss: 0.06551486, Validation loss: 0.06412462, Gradient norm: 3.83340948
INFO:root:[   57] Training loss: 0.06593153, Validation loss: 0.06354868, Gradient norm: 3.83519425
INFO:root:[   58] Training loss: 0.06477473, Validation loss: 0.06870977, Gradient norm: 4.05942781
INFO:root:[   59] Training loss: 0.06849538, Validation loss: 0.06379279, Gradient norm: 4.01208035
INFO:root:[   60] Training loss: 0.06603478, Validation loss: 0.06477925, Gradient norm: 3.28951023
INFO:root:[   61] Training loss: 0.06395076, Validation loss: 0.06791253, Gradient norm: 3.43003839
INFO:root:[   62] Training loss: 0.06654541, Validation loss: 0.06516127, Gradient norm: 3.93727700
INFO:root:[   63] Training loss: 0.06469150, Validation loss: 0.06066035, Gradient norm: 3.79878687
INFO:root:[   64] Training loss: 0.06363149, Validation loss: 0.06665508, Gradient norm: 3.97146406
INFO:root:[   65] Training loss: 0.06592250, Validation loss: 0.06088234, Gradient norm: 4.02595862
INFO:root:[   66] Training loss: 0.06820833, Validation loss: 0.06323530, Gradient norm: 3.98699934
INFO:root:[   67] Training loss: 0.06471962, Validation loss: 0.06711432, Gradient norm: 2.65384675
INFO:root:[   68] Training loss: 0.06429903, Validation loss: 0.06229399, Gradient norm: 2.95491063
INFO:root:[   69] Training loss: 0.06429115, Validation loss: 0.06745274, Gradient norm: 3.23867769
INFO:root:[   70] Training loss: 0.06556840, Validation loss: 0.06291544, Gradient norm: 3.83143057
INFO:root:[   71] Training loss: 0.06264178, Validation loss: 0.05968041, Gradient norm: 3.82148120
INFO:root:[   72] Training loss: 0.06182133, Validation loss: 0.06067548, Gradient norm: 4.70371859
INFO:root:[   73] Training loss: 0.06294023, Validation loss: 0.06361863, Gradient norm: 4.00218262
INFO:root:[   74] Training loss: 0.06090193, Validation loss: 0.05818660, Gradient norm: 4.07624558
INFO:root:[   75] Training loss: 0.06117512, Validation loss: 0.06432472, Gradient norm: 4.37340387
INFO:root:[   76] Training loss: 0.06228459, Validation loss: 0.05918066, Gradient norm: 3.68580981
INFO:root:[   77] Training loss: 0.06298445, Validation loss: 0.05832471, Gradient norm: 4.25521695
INFO:root:[   78] Training loss: 0.06090650, Validation loss: 0.06194834, Gradient norm: 3.92994413
INFO:root:[   79] Training loss: 0.06136013, Validation loss: 0.05837912, Gradient norm: 3.92329258
INFO:root:[   80] Training loss: 0.06003083, Validation loss: 0.05916716, Gradient norm: 3.27509683
INFO:root:[   81] Training loss: 0.06265820, Validation loss: 0.05801608, Gradient norm: 4.05764293
INFO:root:[   82] Training loss: 0.06123996, Validation loss: 0.05940750, Gradient norm: 2.79177344
INFO:root:[   83] Training loss: 0.06229191, Validation loss: 0.06466048, Gradient norm: 3.73854896
INFO:root:[   84] Training loss: 0.06136893, Validation loss: 0.05945582, Gradient norm: 4.20868218
INFO:root:[   85] Training loss: 0.05918284, Validation loss: 0.05946482, Gradient norm: 3.28683427
INFO:root:[   86] Training loss: 0.06011120, Validation loss: 0.06174765, Gradient norm: 3.68708905
INFO:root:[   87] Training loss: 0.05956927, Validation loss: 0.05613723, Gradient norm: 4.32576947
INFO:root:[   88] Training loss: 0.06040314, Validation loss: 0.06121898, Gradient norm: 4.10224888
INFO:root:[   89] Training loss: 0.05964246, Validation loss: 0.06463199, Gradient norm: 3.05459218
INFO:root:[   90] Training loss: 0.06078500, Validation loss: 0.06344869, Gradient norm: 4.52738091
INFO:root:[   91] Training loss: 0.05920757, Validation loss: 0.05752563, Gradient norm: 3.33469834
INFO:root:[   92] Training loss: 0.05952234, Validation loss: 0.06349491, Gradient norm: 3.72120033
INFO:root:[   93] Training loss: 0.06175752, Validation loss: 0.06126021, Gradient norm: 3.62065754
INFO:root:[   94] Training loss: 0.05987583, Validation loss: 0.05875746, Gradient norm: 2.47766235
INFO:root:[   95] Training loss: 0.05999112, Validation loss: 0.06536540, Gradient norm: 3.62405374
INFO:root:[   96] Training loss: 0.05913452, Validation loss: 0.07038284, Gradient norm: 3.47626027
INFO:root:EP 96: Early stopping
INFO:root:Training the model took 1765.575s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03551
INFO:root:EnergyScoreTrain: 0.02054
INFO:root:CoverageTrain: 0.88271
INFO:root:IntervalWidthTrain: 0.02758
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03518
INFO:root:EnergyScoreValidation: 0.02082
INFO:root:CoverageValidation: 0.88581
INFO:root:IntervalWidthValidation: 0.02703
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04022
INFO:root:EnergyScoreTest: 0.0279
INFO:root:CoverageTest: 0.8824
INFO:root:IntervalWidthTest: 0.02687
INFO:root:###23 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 941621248
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.00206667, Validation loss: 0.83335917, Gradient norm: 6.51538303
INFO:root:[    2] Training loss: 0.57062486, Validation loss: 0.47884298, Gradient norm: 4.06355611
INFO:root:[    3] Training loss: 0.44935782, Validation loss: 0.42722812, Gradient norm: 3.70836691
INFO:root:[    4] Training loss: 0.39379008, Validation loss: 0.35419481, Gradient norm: 4.50535961
INFO:root:[    5] Training loss: 0.35029572, Validation loss: 0.33395989, Gradient norm: 3.78061083
INFO:root:[    6] Training loss: 0.31530211, Validation loss: 0.29702566, Gradient norm: 2.47772278
INFO:root:[    7] Training loss: 0.29251803, Validation loss: 0.28489944, Gradient norm: 2.32150130
INFO:root:[    8] Training loss: 0.27457562, Validation loss: 0.26561382, Gradient norm: 1.75540323
INFO:root:[    9] Training loss: 0.25944670, Validation loss: 0.26447642, Gradient norm: 1.87916417
INFO:root:[   10] Training loss: 0.25008642, Validation loss: 0.24355784, Gradient norm: 3.19943008
INFO:root:[   11] Training loss: 0.23663311, Validation loss: 0.22934237, Gradient norm: 1.80811995
INFO:root:[   12] Training loss: 0.22518625, Validation loss: 0.21581918, Gradient norm: 1.55792549
INFO:root:[   13] Training loss: 0.21314420, Validation loss: 0.20642548, Gradient norm: 2.03195291
INFO:root:[   14] Training loss: 0.20173919, Validation loss: 0.19720657, Gradient norm: 1.97240526
INFO:root:[   15] Training loss: 0.19170201, Validation loss: 0.18456952, Gradient norm: 2.57789437
INFO:root:[   16] Training loss: 0.18085992, Validation loss: 0.17415274, Gradient norm: 2.35960299
INFO:root:[   17] Training loss: 0.17286656, Validation loss: 0.16696290, Gradient norm: 3.02386156
INFO:root:[   18] Training loss: 0.16381154, Validation loss: 0.17451499, Gradient norm: 2.42868180
INFO:root:[   19] Training loss: 0.15753020, Validation loss: 0.15019205, Gradient norm: 3.75599390
INFO:root:[   20] Training loss: 0.14668147, Validation loss: 0.14492355, Gradient norm: 3.09137040
INFO:root:[   21] Training loss: 0.14028351, Validation loss: 0.13458829, Gradient norm: 4.28997586
INFO:root:[   22] Training loss: 0.13278955, Validation loss: 0.12730984, Gradient norm: 2.57170243
INFO:root:[   23] Training loss: 0.12692872, Validation loss: 0.12358324, Gradient norm: 3.45771151
INFO:root:[   24] Training loss: 0.12098667, Validation loss: 0.11894446, Gradient norm: 5.51882452
INFO:root:[   25] Training loss: 0.11576393, Validation loss: 0.11315850, Gradient norm: 3.58250585
INFO:root:[   26] Training loss: 0.11066055, Validation loss: 0.10434267, Gradient norm: 4.01585745
INFO:root:[   27] Training loss: 0.10524858, Validation loss: 0.09941682, Gradient norm: 4.96686122
INFO:root:[   28] Training loss: 0.10198662, Validation loss: 0.10089420, Gradient norm: 4.56685731
INFO:root:[   29] Training loss: 0.09793052, Validation loss: 0.09311304, Gradient norm: 3.86775973
INFO:root:[   30] Training loss: 0.09514904, Validation loss: 0.10020475, Gradient norm: 3.10941149
INFO:root:[   31] Training loss: 0.09297658, Validation loss: 0.08713034, Gradient norm: 4.34648238
INFO:root:[   32] Training loss: 0.08933385, Validation loss: 0.08448835, Gradient norm: 4.09807938
INFO:root:[   33] Training loss: 0.08618667, Validation loss: 0.08366912, Gradient norm: 4.02714077
INFO:root:[   34] Training loss: 0.08572870, Validation loss: 0.08599663, Gradient norm: 4.30044648
INFO:root:[   35] Training loss: 0.08388619, Validation loss: 0.08640279, Gradient norm: 4.36979342
INFO:root:[   36] Training loss: 0.08166494, Validation loss: 0.08558554, Gradient norm: 3.67937980
INFO:root:[   37] Training loss: 0.08098584, Validation loss: 0.07766236, Gradient norm: 4.95255327
INFO:root:[   38] Training loss: 0.08147355, Validation loss: 0.07867053, Gradient norm: 4.87552571
INFO:root:[   39] Training loss: 0.08125787, Validation loss: 0.08342701, Gradient norm: 4.31671064
INFO:root:[   40] Training loss: 0.07956293, Validation loss: 0.07788719, Gradient norm: 2.85793751
INFO:root:[   41] Training loss: 0.07838909, Validation loss: 0.07697983, Gradient norm: 4.72742991
INFO:root:[   42] Training loss: 0.07673540, Validation loss: 0.07834559, Gradient norm: 3.79684857
INFO:root:[   43] Training loss: 0.07969622, Validation loss: 0.07470376, Gradient norm: 5.30917331
INFO:root:[   44] Training loss: 0.07545919, Validation loss: 0.07375306, Gradient norm: 4.37946820
INFO:root:[   45] Training loss: 0.07499584, Validation loss: 0.07475386, Gradient norm: 3.81668007
INFO:root:[   46] Training loss: 0.07563918, Validation loss: 0.07241311, Gradient norm: 5.71099752
INFO:root:[   47] Training loss: 0.07760193, Validation loss: 0.07467640, Gradient norm: 4.99010155
INFO:root:[   48] Training loss: 0.07359906, Validation loss: 0.06944255, Gradient norm: 5.00288140
INFO:root:[   49] Training loss: 0.07508846, Validation loss: 0.07934760, Gradient norm: 5.54566694
INFO:root:[   50] Training loss: 0.07422727, Validation loss: 0.07114816, Gradient norm: 4.81622706
INFO:root:[   51] Training loss: 0.07361086, Validation loss: 0.06927267, Gradient norm: 4.23295772
INFO:root:[   52] Training loss: 0.07468575, Validation loss: 0.06866374, Gradient norm: 5.94587247
INFO:root:[   53] Training loss: 0.07262241, Validation loss: 0.06991545, Gradient norm: 4.74620550
INFO:root:[   54] Training loss: 0.07181381, Validation loss: 0.06796123, Gradient norm: 4.60473654
INFO:root:[   55] Training loss: 0.07428265, Validation loss: 0.08953487, Gradient norm: 6.80932942
INFO:root:[   56] Training loss: 0.07307471, Validation loss: 0.06845390, Gradient norm: 4.75905338
INFO:root:[   57] Training loss: 0.07033926, Validation loss: 0.07042772, Gradient norm: 4.11947790
INFO:root:[   58] Training loss: 0.07135056, Validation loss: 0.06845314, Gradient norm: 5.03101119
INFO:root:[   59] Training loss: 0.07097690, Validation loss: 0.07268726, Gradient norm: 5.35889822
INFO:root:[   60] Training loss: 0.07053055, Validation loss: 0.06929146, Gradient norm: 4.91897978
INFO:root:[   61] Training loss: 0.07370014, Validation loss: 0.07011239, Gradient norm: 4.44137622
INFO:root:[   62] Training loss: 0.06960792, Validation loss: 0.06785287, Gradient norm: 4.76296567
INFO:root:[   63] Training loss: 0.07203302, Validation loss: 0.06651711, Gradient norm: 5.17351623
INFO:root:[   64] Training loss: 0.06976488, Validation loss: 0.06764823, Gradient norm: 5.04588952
INFO:root:[   65] Training loss: 0.07106961, Validation loss: 0.06964820, Gradient norm: 4.91782400
INFO:root:[   66] Training loss: 0.07121169, Validation loss: 0.06639005, Gradient norm: 5.49282324
INFO:root:[   67] Training loss: 0.06920009, Validation loss: 0.06561750, Gradient norm: 5.03430918
INFO:root:[   68] Training loss: 0.07002944, Validation loss: 0.07251582, Gradient norm: 5.57071949
INFO:root:[   69] Training loss: 0.07016905, Validation loss: 0.07005354, Gradient norm: 4.71072773
INFO:root:[   70] Training loss: 0.06962343, Validation loss: 0.06578917, Gradient norm: 4.89645030
INFO:root:[   71] Training loss: 0.06890724, Validation loss: 0.06867675, Gradient norm: 5.19743765
INFO:root:[   72] Training loss: 0.06986120, Validation loss: 0.07054187, Gradient norm: 5.13100837
INFO:root:[   73] Training loss: 0.06811151, Validation loss: 0.06634829, Gradient norm: 3.86833902
INFO:root:[   74] Training loss: 0.06873878, Validation loss: 0.07698229, Gradient norm: 4.18622671
INFO:root:[   75] Training loss: 0.06937271, Validation loss: 0.07245035, Gradient norm: 4.97421836
INFO:root:[   76] Training loss: 0.06924158, Validation loss: 0.06754235, Gradient norm: 5.60603264
INFO:root:EP 76: Early stopping
INFO:root:Training the model took 1413.321s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04161
INFO:root:EnergyScoreTrain: 0.01071
INFO:root:CoverageTrain: 0.94584
INFO:root:IntervalWidthTrain: 0.03273
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04101
INFO:root:EnergyScoreValidation: 0.01034
INFO:root:CoverageValidation: 0.94571
INFO:root:IntervalWidthValidation: 0.03212
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04749
INFO:root:EnergyScoreTest: 0.0178
INFO:root:CoverageTest: 0.92394
INFO:root:IntervalWidthTest: 0.03144
INFO:root:###24 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'UNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 16, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [16, 32, 64, 128, 64, 32, 16], 'uno_scalings': [[1.0, 0.75, 0.75], [1.0, 0.67, 0.67], [1.0, 0.5, 0.5], [1.0, 1.0, 1.0], [1.0, 2.0, 2.0], [1.0, 1.5, 1.5], [1.0, 1.33, 1.33]], 'uno_n_modes': [[4, 20, 20], [4, 14, 14], [4, 6, 6], [7, 6, 6], [7, 6, 6], [10, 14, 14], [10, 20, 20]]}
INFO:root:NumberParameters: 14757969
INFO:root:Memory allocated: 845152256
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.32775135, Validation loss: 0.84240288, Gradient norm: 5.34119088
INFO:root:[    2] Training loss: 0.74477649, Validation loss: 0.67111134, Gradient norm: 3.50362193
INFO:root:[    3] Training loss: 0.60354637, Validation loss: 0.56193879, Gradient norm: 3.56989533
INFO:root:[    4] Training loss: 0.51322983, Validation loss: 0.47675950, Gradient norm: 3.41300823
INFO:root:[    5] Training loss: 0.45375598, Validation loss: 0.42118209, Gradient norm: 3.05023260
INFO:root:[    6] Training loss: 0.40808982, Validation loss: 0.38579431, Gradient norm: 2.19961308
INFO:root:[    7] Training loss: 0.37589470, Validation loss: 0.36087092, Gradient norm: 1.80017414
INFO:root:[    8] Training loss: 0.34838607, Validation loss: 0.33366230, Gradient norm: 1.99411890
INFO:root:[    9] Training loss: 0.32869532, Validation loss: 0.31745431, Gradient norm: 2.73144076
INFO:root:[   10] Training loss: 0.31067729, Validation loss: 0.29709807, Gradient norm: 2.41792882
INFO:root:[   11] Training loss: 0.29307226, Validation loss: 0.28931930, Gradient norm: 2.45112190
INFO:root:[   12] Training loss: 0.27515593, Validation loss: 0.26659271, Gradient norm: 2.37639966
INFO:root:[   13] Training loss: 0.26065253, Validation loss: 0.24930911, Gradient norm: 2.83234385
INFO:root:[   14] Training loss: 0.24614017, Validation loss: 0.24007504, Gradient norm: 3.22687718
INFO:root:[   15] Training loss: 0.23183894, Validation loss: 0.22136678, Gradient norm: 3.45164342
INFO:root:[   16] Training loss: 0.21873500, Validation loss: 0.21057202, Gradient norm: 5.02376331
INFO:root:[   17] Training loss: 0.20473711, Validation loss: 0.19837382, Gradient norm: 4.03028000
INFO:root:[   18] Training loss: 0.19203781, Validation loss: 0.18841890, Gradient norm: 3.48307305
INFO:root:[   19] Training loss: 0.18385254, Validation loss: 0.17421637, Gradient norm: 6.02563577
INFO:root:[   20] Training loss: 0.17015316, Validation loss: 0.16149218, Gradient norm: 3.72506109
INFO:root:[   21] Training loss: 0.15970519, Validation loss: 0.15385179, Gradient norm: 3.87354163
INFO:root:[   22] Training loss: 0.15256923, Validation loss: 0.14967934, Gradient norm: 6.18675399
INFO:root:[   23] Training loss: 0.14109041, Validation loss: 0.13480908, Gradient norm: 3.83149072
INFO:root:[   24] Training loss: 0.13413065, Validation loss: 0.13411314, Gradient norm: 5.33922274
INFO:root:[   25] Training loss: 0.12996360, Validation loss: 0.13048288, Gradient norm: 6.27893424
INFO:root:[   26] Training loss: 0.12031452, Validation loss: 0.12655376, Gradient norm: 4.45524081
INFO:root:[   27] Training loss: 0.11378098, Validation loss: 0.11064653, Gradient norm: 4.02159290
INFO:root:[   28] Training loss: 0.10932625, Validation loss: 0.11812624, Gradient norm: 4.87041460
INFO:root:[   29] Training loss: 0.10734387, Validation loss: 0.10134085, Gradient norm: 5.55980912
INFO:root:[   30] Training loss: 0.10354349, Validation loss: 0.10132569, Gradient norm: 3.78928089
INFO:root:[   31] Training loss: 0.10023702, Validation loss: 0.09766586, Gradient norm: 3.60414374
INFO:root:[   32] Training loss: 0.09968841, Validation loss: 0.09804245, Gradient norm: 5.07022246
INFO:root:[   33] Training loss: 0.09964239, Validation loss: 0.09987333, Gradient norm: 5.43033846
INFO:root:[   34] Training loss: 0.09793432, Validation loss: 0.09361338, Gradient norm: 6.05394255
INFO:root:[   35] Training loss: 0.09632450, Validation loss: 0.10431716, Gradient norm: 4.55569245
INFO:root:[   36] Training loss: 0.09886313, Validation loss: 0.10290695, Gradient norm: 5.12808678
INFO:root:[   37] Training loss: 0.09665073, Validation loss: 0.09376294, Gradient norm: 5.87049300
INFO:root:[   38] Training loss: 0.09315286, Validation loss: 0.09388618, Gradient norm: 5.10511930
INFO:root:[   39] Training loss: 0.09436287, Validation loss: 0.09633865, Gradient norm: 5.95376523
INFO:root:[   40] Training loss: 0.09405761, Validation loss: 0.09079754, Gradient norm: 5.93301790
INFO:root:[   41] Training loss: 0.09218742, Validation loss: 0.09215265, Gradient norm: 3.96523586
INFO:root:[   42] Training loss: 0.09174170, Validation loss: 0.08834826, Gradient norm: 4.86025248
INFO:root:[   43] Training loss: 0.09246365, Validation loss: 0.08615570, Gradient norm: 5.87538862
INFO:root:[   44] Training loss: 0.09179328, Validation loss: 0.08587676, Gradient norm: 5.56796440
INFO:root:[   45] Training loss: 0.08960665, Validation loss: 0.10129459, Gradient norm: 4.08677346
INFO:root:[   46] Training loss: 0.10258827, Validation loss: 0.09034779, Gradient norm: 7.78252818
INFO:root:[   47] Training loss: 0.09014626, Validation loss: 0.08562444, Gradient norm: 3.89899074
INFO:root:[   48] Training loss: 0.08687777, Validation loss: 0.08853484, Gradient norm: 4.88981369
INFO:root:[   49] Training loss: 0.08756065, Validation loss: 0.08961759, Gradient norm: 4.43875837
INFO:root:[   50] Training loss: 0.08883262, Validation loss: 0.08397203, Gradient norm: 4.51882624
INFO:root:[   51] Training loss: 0.08928568, Validation loss: 0.08451686, Gradient norm: 4.87182360
INFO:root:[   52] Training loss: 0.08643144, Validation loss: 0.08556187, Gradient norm: 5.21424817
INFO:root:[   53] Training loss: 0.08517341, Validation loss: 0.08530224, Gradient norm: 4.75229602
INFO:root:[   54] Training loss: 0.08969386, Validation loss: 0.09450223, Gradient norm: 5.58703266
INFO:root:[   55] Training loss: 0.08577728, Validation loss: 0.08362042, Gradient norm: 3.99963464
INFO:root:[   56] Training loss: 0.08558411, Validation loss: 0.09633730, Gradient norm: 5.21651921
INFO:root:[   57] Training loss: 0.09473009, Validation loss: 0.08274585, Gradient norm: 6.22332146
INFO:root:[   58] Training loss: 0.08370373, Validation loss: 0.08244066, Gradient norm: 4.17205065
INFO:root:[   59] Training loss: 0.08508655, Validation loss: 0.10311979, Gradient norm: 4.32699646
INFO:root:[   60] Training loss: 0.08594903, Validation loss: 0.08149460, Gradient norm: 6.65054454
INFO:root:[   61] Training loss: 0.08289470, Validation loss: 0.07866852, Gradient norm: 5.26147423
INFO:root:[   62] Training loss: 0.08309421, Validation loss: 0.08523338, Gradient norm: 4.25401108
INFO:root:[   63] Training loss: 0.08483254, Validation loss: 0.08341040, Gradient norm: 5.10388334
INFO:root:[   64] Training loss: 0.08303542, Validation loss: 0.09047693, Gradient norm: 5.16740313
INFO:root:[   65] Training loss: 0.08365738, Validation loss: 0.08404709, Gradient norm: 4.79799974
INFO:root:[   66] Training loss: 0.08234777, Validation loss: 0.08053392, Gradient norm: 5.86066381
INFO:root:[   67] Training loss: 0.08267413, Validation loss: 0.08126550, Gradient norm: 4.89716204
INFO:root:[   68] Training loss: 0.08208779, Validation loss: 0.08495657, Gradient norm: 4.81461981
INFO:root:[   69] Training loss: 0.08175519, Validation loss: 0.07958241, Gradient norm: 5.78392013
INFO:root:[   70] Training loss: 0.08298255, Validation loss: 0.09906885, Gradient norm: 6.08598967
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1294.909s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05188
INFO:root:EnergyScoreTrain: 0.03249
INFO:root:CoverageTrain: 0.85387
INFO:root:IntervalWidthTrain: 0.03799
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05134
INFO:root:EnergyScoreValidation: 0.03175
INFO:root:CoverageValidation: 0.85215
INFO:root:IntervalWidthValidation: 0.03727
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.05632
INFO:root:EnergyScoreTest: 0.03281
INFO:root:CoverageTest: 0.84498
INFO:root:IntervalWidthTest: 0.03716
