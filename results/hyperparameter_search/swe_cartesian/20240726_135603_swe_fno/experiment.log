INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10, 'ood': False}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.58049730, Validation loss: 0.08831568, Gradient norm: 6.23604176
INFO:root:[    2] Training loss: 0.09243291, Validation loss: 0.07367067, Gradient norm: 3.87603181
INFO:root:[    3] Training loss: 0.08189132, Validation loss: 0.06180616, Gradient norm: 5.13347923
INFO:root:[    4] Training loss: 0.06087808, Validation loss: 0.05058123, Gradient norm: 3.03299833
INFO:root:[    5] Training loss: 0.05603545, Validation loss: 0.04380806, Gradient norm: 3.84120356
INFO:root:[    6] Training loss: 0.05215497, Validation loss: 0.05181379, Gradient norm: 3.70136829
INFO:root:[    7] Training loss: 0.04935166, Validation loss: 0.04201113, Gradient norm: 3.58185224
INFO:root:[    8] Training loss: 0.04357609, Validation loss: 0.04162308, Gradient norm: 1.77621738
INFO:root:[    9] Training loss: 0.04684171, Validation loss: 0.04649190, Gradient norm: 3.69166349
INFO:root:[   10] Training loss: 0.04404888, Validation loss: 0.06261117, Gradient norm: 3.32671427
INFO:root:[   11] Training loss: 0.04406000, Validation loss: 0.04749333, Gradient norm: 3.25416366
INFO:root:[   12] Training loss: 0.04354933, Validation loss: 0.04612418, Gradient norm: 3.29616860
INFO:root:[   13] Training loss: 0.03748368, Validation loss: 0.03689643, Gradient norm: 2.20419088
INFO:root:[   14] Training loss: 0.03768071, Validation loss: 0.03435555, Gradient norm: 2.52535139
INFO:root:[   15] Training loss: 0.03647044, Validation loss: 0.03841277, Gradient norm: 2.54769452
INFO:root:[   16] Training loss: 0.03827400, Validation loss: 0.04501273, Gradient norm: 2.87553876
INFO:root:[   17] Training loss: 0.03505394, Validation loss: 0.04241732, Gradient norm: 2.26951200
INFO:root:[   18] Training loss: 0.03797984, Validation loss: 0.03483115, Gradient norm: 2.76931770
INFO:root:[   19] Training loss: 0.03631743, Validation loss: 0.02918040, Gradient norm: 2.38011464
INFO:root:[   20] Training loss: 0.03365652, Validation loss: 0.04304970, Gradient norm: 2.27893642
INFO:root:[   21] Training loss: 0.03612663, Validation loss: 0.02840354, Gradient norm: 2.88815712
INFO:root:[   22] Training loss: 0.03164020, Validation loss: 0.03188225, Gradient norm: 1.96278324
INFO:root:[   23] Training loss: 0.03019056, Validation loss: 0.03022627, Gradient norm: 2.06568437
INFO:root:[   24] Training loss: 0.02951364, Validation loss: 0.03143119, Gradient norm: 2.16876550
INFO:root:[   25] Training loss: 0.02977469, Validation loss: 0.04005745, Gradient norm: 1.79668865
INFO:root:[   26] Training loss: 0.03342712, Validation loss: 0.03474343, Gradient norm: 2.24082808
INFO:root:[   27] Training loss: 0.02944142, Validation loss: 0.02485239, Gradient norm: 2.17417766
INFO:root:[   28] Training loss: 0.02934424, Validation loss: 0.03091402, Gradient norm: 2.04212271
INFO:root:[   29] Training loss: 0.02801626, Validation loss: 0.03253031, Gradient norm: 1.88747182
INFO:root:[   30] Training loss: 0.02803227, Validation loss: 0.02665140, Gradient norm: 2.09009393
INFO:root:[   31] Training loss: 0.02784578, Validation loss: 0.03016511, Gradient norm: 2.13595941
INFO:root:[   32] Training loss: 0.02819467, Validation loss: 0.03285819, Gradient norm: 2.38775038
INFO:root:[   33] Training loss: 0.02675453, Validation loss: 0.03128252, Gradient norm: 2.07628461
INFO:root:[   34] Training loss: 0.02768573, Validation loss: 0.03318525, Gradient norm: 2.19463473
INFO:root:[   35] Training loss: 0.02807878, Validation loss: 0.02876202, Gradient norm: 1.78106084
INFO:root:[   36] Training loss: 0.02888492, Validation loss: 0.03045389, Gradient norm: 2.25573862
INFO:root:[   37] Training loss: 0.02636392, Validation loss: 0.02871594, Gradient norm: 2.29404820
INFO:root:[   38] Training loss: 0.02735041, Validation loss: 0.02137853, Gradient norm: 2.45113572
INFO:root:[   39] Training loss: 0.02583138, Validation loss: 0.02046148, Gradient norm: 2.29876326
INFO:root:[   40] Training loss: 0.02463389, Validation loss: 0.02430584, Gradient norm: 2.12306806
INFO:root:[   41] Training loss: 0.02488328, Validation loss: 0.02715844, Gradient norm: 2.00105713
INFO:root:[   42] Training loss: 0.02696392, Validation loss: 0.02556410, Gradient norm: 2.15490227
INFO:root:[   43] Training loss: 0.02456831, Validation loss: 0.02785130, Gradient norm: 2.17858446
INFO:root:[   44] Training loss: 0.02412232, Validation loss: 0.02311647, Gradient norm: 2.01792755
INFO:root:[   45] Training loss: 0.02735601, Validation loss: 0.02132143, Gradient norm: 2.39673214
INFO:root:[   46] Training loss: 0.02441326, Validation loss: 0.02472519, Gradient norm: 2.06170377
INFO:root:[   47] Training loss: 0.02355784, Validation loss: 0.01951574, Gradient norm: 2.06423376
INFO:root:[   48] Training loss: 0.02297814, Validation loss: 0.02860701, Gradient norm: 1.97605352
INFO:root:[   49] Training loss: 0.02364562, Validation loss: 0.01935702, Gradient norm: 2.08497266
INFO:root:[   50] Training loss: 0.02328521, Validation loss: 0.02847401, Gradient norm: 1.99168080
INFO:root:[   51] Training loss: 0.02417192, Validation loss: 0.01869039, Gradient norm: 1.90822393
INFO:root:[   52] Training loss: 0.02336368, Validation loss: 0.02286053, Gradient norm: 1.97394597
INFO:root:[   53] Training loss: 0.02323051, Validation loss: 0.02505186, Gradient norm: 2.00938504
INFO:root:[   54] Training loss: 0.02387243, Validation loss: 0.02675863, Gradient norm: 2.06967529
INFO:root:[   55] Training loss: 0.02235071, Validation loss: 0.01841841, Gradient norm: 1.95900970
INFO:root:[   56] Training loss: 0.02323881, Validation loss: 0.01879526, Gradient norm: 1.98214458
INFO:root:[   57] Training loss: 0.02112278, Validation loss: 0.02494806, Gradient norm: 1.77463045
INFO:root:[   58] Training loss: 0.02185260, Validation loss: 0.02387967, Gradient norm: 1.99184004
INFO:root:[   59] Training loss: 0.02263426, Validation loss: 0.02422010, Gradient norm: 1.96208700
INFO:root:[   60] Training loss: 0.02187239, Validation loss: 0.02260773, Gradient norm: 1.79877436
INFO:root:[   61] Training loss: 0.02255949, Validation loss: 0.01900863, Gradient norm: 2.02973993
INFO:root:[   62] Training loss: 0.01990581, Validation loss: 0.02059148, Gradient norm: 1.62215627
INFO:root:[   63] Training loss: 0.02041322, Validation loss: 0.02086474, Gradient norm: 1.72282057
INFO:root:[   64] Training loss: 0.02057992, Validation loss: 0.02100762, Gradient norm: 1.86952444
INFO:root:[   65] Training loss: 0.02018731, Validation loss: 0.01688972, Gradient norm: 1.76364337
INFO:root:[   66] Training loss: 0.02265679, Validation loss: 0.02533692, Gradient norm: 1.93563137
INFO:root:[   67] Training loss: 0.02227344, Validation loss: 0.02227277, Gradient norm: 1.89725413
INFO:root:[   68] Training loss: 0.02053772, Validation loss: 0.02331718, Gradient norm: 1.84810356
INFO:root:[   69] Training loss: 0.02099037, Validation loss: 0.02388331, Gradient norm: 1.83300549
INFO:root:[   70] Training loss: 0.02151020, Validation loss: 0.02063984, Gradient norm: 1.90176284
INFO:root:[   71] Training loss: 0.02028917, Validation loss: 0.01713260, Gradient norm: 1.54043959
INFO:root:[   72] Training loss: 0.02046839, Validation loss: 0.01601936, Gradient norm: 1.68280293
INFO:root:[   73] Training loss: 0.01962826, Validation loss: 0.01542026, Gradient norm: 1.78214516
INFO:root:[   74] Training loss: 0.02005966, Validation loss: 0.01891658, Gradient norm: 1.85224821
INFO:root:[   75] Training loss: 0.01957949, Validation loss: 0.01992166, Gradient norm: 1.71956161
INFO:root:[   76] Training loss: 0.02036509, Validation loss: 0.02039437, Gradient norm: 1.74544804
INFO:root:[   77] Training loss: 0.02104719, Validation loss: 0.01786087, Gradient norm: 1.91226105
INFO:root:[   78] Training loss: 0.02082209, Validation loss: 0.02357009, Gradient norm: 1.84742250
INFO:root:[   79] Training loss: 0.02128598, Validation loss: 0.02212998, Gradient norm: 1.93309792
INFO:root:[   80] Training loss: 0.01984608, Validation loss: 0.01536788, Gradient norm: 1.79519308
INFO:root:[   81] Training loss: 0.01964987, Validation loss: 0.02137843, Gradient norm: 1.76884998
INFO:root:[   82] Training loss: 0.01982483, Validation loss: 0.02102563, Gradient norm: 1.82540324
INFO:root:[   83] Training loss: 0.01947040, Validation loss: 0.01822661, Gradient norm: 1.69958965
INFO:root:[   84] Training loss: 0.01941031, Validation loss: 0.01815092, Gradient norm: 1.63502186
INFO:root:[   85] Training loss: 0.01942609, Validation loss: 0.01549046, Gradient norm: 1.82679909
INFO:root:[   86] Training loss: 0.01890066, Validation loss: 0.02177975, Gradient norm: 1.76446233
INFO:root:[   87] Training loss: 0.01956157, Validation loss: 0.02101913, Gradient norm: 1.73333570
INFO:root:[   88] Training loss: 0.01880356, Validation loss: 0.01825278, Gradient norm: 1.62469901
INFO:root:[   89] Training loss: 0.01963900, Validation loss: 0.01901920, Gradient norm: 1.73670714
INFO:root:EP 89: Early stopping
INFO:root:Training the model took 3346.749s.
INFO:root:Emptying the cuda cache took 0.082s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0149
INFO:root:EnergyScoreTrain: 0.01528
INFO:root:CoverageTrain: 0.99927
INFO:root:IntervalWidthTrain: 0.0517
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0147
INFO:root:EnergyScoreValidation: 0.01519
INFO:root:CoverageValidation: 0.99938
INFO:root:IntervalWidthValidation: 0.05164
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01469
INFO:root:EnergyScoreTest: 0.01513
INFO:root:CoverageTest: 0.99923
INFO:root:IntervalWidthTest: 0.05134
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 457179136
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.58333305, Validation loss: 0.12474433, Gradient norm: 4.18979189
INFO:root:[    2] Training loss: 0.10170408, Validation loss: 0.09924827, Gradient norm: 1.74042327
INFO:root:[    3] Training loss: 0.08632815, Validation loss: 0.06890764, Gradient norm: 2.52516538
INFO:root:[    4] Training loss: 0.07392399, Validation loss: 0.06003532, Gradient norm: 2.43818070
INFO:root:[    5] Training loss: 0.06056510, Validation loss: 0.06044003, Gradient norm: 1.60555673
INFO:root:[    6] Training loss: 0.05830357, Validation loss: 0.05006688, Gradient norm: 1.77997615
INFO:root:[    7] Training loss: 0.05406754, Validation loss: 0.06006516, Gradient norm: 1.80641759
INFO:root:[    8] Training loss: 0.05196402, Validation loss: 0.04869203, Gradient norm: 1.77247122
INFO:root:[    9] Training loss: 0.05134134, Validation loss: 0.04402759, Gradient norm: 1.92758089
INFO:root:[   10] Training loss: 0.05061081, Validation loss: 0.04363856, Gradient norm: 1.85250334
INFO:root:[   11] Training loss: 0.04570791, Validation loss: 0.04584649, Gradient norm: 1.49674106
INFO:root:[   12] Training loss: 0.04483509, Validation loss: 0.04023674, Gradient norm: 1.57058588
INFO:root:[   13] Training loss: 0.04470319, Validation loss: 0.04536306, Gradient norm: 1.55669802
INFO:root:[   14] Training loss: 0.04458128, Validation loss: 0.04362868, Gradient norm: 1.61559743
INFO:root:[   15] Training loss: 0.04434175, Validation loss: 0.04840763, Gradient norm: 1.50052583
INFO:root:[   16] Training loss: 0.04280283, Validation loss: 0.04431184, Gradient norm: 1.63496144
INFO:root:[   17] Training loss: 0.04310485, Validation loss: 0.04341386, Gradient norm: 1.75081371
INFO:root:[   18] Training loss: 0.04088117, Validation loss: 0.03880601, Gradient norm: 1.55057434
INFO:root:[   19] Training loss: 0.04412908, Validation loss: 0.03929019, Gradient norm: 1.95405641
INFO:root:[   20] Training loss: 0.04047634, Validation loss: 0.04277869, Gradient norm: 1.50436895
INFO:root:[   21] Training loss: 0.03832877, Validation loss: 0.04355145, Gradient norm: 1.41190562
INFO:root:[   22] Training loss: 0.03844507, Validation loss: 0.03919250, Gradient norm: 1.55378141
INFO:root:[   23] Training loss: 0.03760072, Validation loss: 0.03791733, Gradient norm: 1.48694082
INFO:root:[   24] Training loss: 0.03941424, Validation loss: 0.03426812, Gradient norm: 1.66069341
INFO:root:[   25] Training loss: 0.03822010, Validation loss: 0.04161010, Gradient norm: 1.53601742
INFO:root:[   26] Training loss: 0.03659130, Validation loss: 0.04045550, Gradient norm: 1.55112297
INFO:root:[   27] Training loss: 0.03610393, Validation loss: 0.03191878, Gradient norm: 1.59634815
INFO:root:[   28] Training loss: 0.03577489, Validation loss: 0.03543705, Gradient norm: 1.55255475
INFO:root:[   29] Training loss: 0.03629434, Validation loss: 0.03498125, Gradient norm: 1.60950580
INFO:root:[   30] Training loss: 0.03487794, Validation loss: 0.03501380, Gradient norm: 1.43798963
INFO:root:[   31] Training loss: 0.03439548, Validation loss: 0.03220336, Gradient norm: 1.56698145
INFO:root:[   32] Training loss: 0.03386019, Validation loss: 0.03591762, Gradient norm: 1.57179658
INFO:root:[   33] Training loss: 0.03523332, Validation loss: 0.03477554, Gradient norm: 1.73920582
INFO:root:[   34] Training loss: 0.03441301, Validation loss: 0.03676851, Gradient norm: 1.45267761
INFO:root:[   35] Training loss: 0.03352022, Validation loss: 0.03607369, Gradient norm: 1.37533916
INFO:root:[   36] Training loss: 0.03396498, Validation loss: 0.03169776, Gradient norm: 1.57208803
INFO:root:[   37] Training loss: 0.03364906, Validation loss: 0.03948277, Gradient norm: 1.59840161
INFO:root:[   38] Training loss: 0.03429963, Validation loss: 0.02748441, Gradient norm: 1.82381617
INFO:root:[   39] Training loss: 0.03207329, Validation loss: 0.03184667, Gradient norm: 1.54461541
INFO:root:[   40] Training loss: 0.03283236, Validation loss: 0.03394147, Gradient norm: 1.65790052
INFO:root:[   41] Training loss: 0.03421814, Validation loss: 0.03114223, Gradient norm: 1.62645527
INFO:root:[   42] Training loss: 0.03213277, Validation loss: 0.03014763, Gradient norm: 1.47861508
INFO:root:[   43] Training loss: 0.03073714, Validation loss: 0.03273698, Gradient norm: 1.49278477
INFO:root:[   44] Training loss: 0.02959098, Validation loss: 0.03375809, Gradient norm: 1.36513188
INFO:root:[   45] Training loss: 0.03080729, Validation loss: 0.03108368, Gradient norm: 1.59615361
INFO:root:[   46] Training loss: 0.03013885, Validation loss: 0.02884668, Gradient norm: 1.50521540
INFO:root:[   47] Training loss: 0.03201558, Validation loss: 0.03230686, Gradient norm: 1.67208274
INFO:root:[   48] Training loss: 0.03004290, Validation loss: 0.03039129, Gradient norm: 1.41850853
INFO:root:[   49] Training loss: 0.03010976, Validation loss: 0.02902407, Gradient norm: 1.42515133
INFO:root:[   50] Training loss: 0.03040631, Validation loss: 0.02598690, Gradient norm: 1.50596946
INFO:root:[   51] Training loss: 0.03024158, Validation loss: 0.03001938, Gradient norm: 1.49529883
INFO:root:[   52] Training loss: 0.03012323, Validation loss: 0.02766197, Gradient norm: 1.56563361
INFO:root:[   53] Training loss: 0.02922272, Validation loss: 0.03396102, Gradient norm: 1.50221580
INFO:root:[   54] Training loss: 0.02999554, Validation loss: 0.03105133, Gradient norm: 1.63659834
INFO:root:[   55] Training loss: 0.02906135, Validation loss: 0.02734544, Gradient norm: 1.50427431
INFO:root:[   56] Training loss: 0.02885176, Validation loss: 0.02482547, Gradient norm: 1.37398882
INFO:root:[   57] Training loss: 0.02974469, Validation loss: 0.03299596, Gradient norm: 1.50372088
INFO:root:[   58] Training loss: 0.02952358, Validation loss: 0.03171870, Gradient norm: 1.56998515
INFO:root:[   59] Training loss: 0.02848168, Validation loss: 0.02751905, Gradient norm: 1.54852319
INFO:root:[   60] Training loss: 0.02838215, Validation loss: 0.02376066, Gradient norm: 1.57120075
INFO:root:[   61] Training loss: 0.02703594, Validation loss: 0.02395867, Gradient norm: 1.37884078
INFO:root:[   62] Training loss: 0.02772403, Validation loss: 0.03020560, Gradient norm: 1.51699396
INFO:root:[   63] Training loss: 0.02817609, Validation loss: 0.02470949, Gradient norm: 1.55417757
INFO:root:[   64] Training loss: 0.02833127, Validation loss: 0.02805274, Gradient norm: 1.47794795
INFO:root:[   65] Training loss: 0.02842424, Validation loss: 0.02471275, Gradient norm: 1.47729772
INFO:root:[   66] Training loss: 0.02732388, Validation loss: 0.02623439, Gradient norm: 1.49832726
INFO:root:[   67] Training loss: 0.02701986, Validation loss: 0.02961620, Gradient norm: 1.54203743
INFO:root:[   68] Training loss: 0.02626579, Validation loss: 0.02838665, Gradient norm: 1.44741576
INFO:root:[   69] Training loss: 0.02764818, Validation loss: 0.03197908, Gradient norm: 1.58319986
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2423.1s.
INFO:root:Emptying the cuda cache took 0.084s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01754
INFO:root:EnergyScoreTrain: 0.02381
INFO:root:CoverageTrain: 0.99998
INFO:root:IntervalWidthTrain: 0.08825
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01738
INFO:root:EnergyScoreValidation: 0.02376
INFO:root:CoverageValidation: 0.99998
INFO:root:IntervalWidthValidation: 0.08821
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01718
INFO:root:EnergyScoreTest: 0.02359
INFO:root:CoverageTest: 0.99997
INFO:root:IntervalWidthTest: 0.08773
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.62009615, Validation loss: 0.15211235, Gradient norm: 4.46104051
INFO:root:[    2] Training loss: 0.12123851, Validation loss: 0.09791175, Gradient norm: 1.67034771
INFO:root:[    3] Training loss: 0.09119446, Validation loss: 0.07744925, Gradient norm: 1.04747163
INFO:root:[    4] Training loss: 0.08239362, Validation loss: 0.08898007, Gradient norm: 1.72335200
INFO:root:[    5] Training loss: 0.07568029, Validation loss: 0.07179925, Gradient norm: 1.83321153
INFO:root:[    6] Training loss: 0.06843810, Validation loss: 0.05974794, Gradient norm: 1.54919775
INFO:root:[    7] Training loss: 0.06146417, Validation loss: 0.06615867, Gradient norm: 1.03065814
INFO:root:[    8] Training loss: 0.06002711, Validation loss: 0.05553675, Gradient norm: 1.24308826
INFO:root:[    9] Training loss: 0.05910052, Validation loss: 0.05437170, Gradient norm: 1.59341720
INFO:root:[   10] Training loss: 0.05637602, Validation loss: 0.05380259, Gradient norm: 1.53494895
INFO:root:[   11] Training loss: 0.05477910, Validation loss: 0.05087359, Gradient norm: 1.44319954
INFO:root:[   12] Training loss: 0.05589220, Validation loss: 0.05842299, Gradient norm: 1.57419810
INFO:root:[   13] Training loss: 0.05327499, Validation loss: 0.04662131, Gradient norm: 1.37304496
INFO:root:[   14] Training loss: 0.05165759, Validation loss: 0.05026310, Gradient norm: 1.46355152
INFO:root:[   15] Training loss: 0.05221895, Validation loss: 0.04513550, Gradient norm: 1.64394096
INFO:root:[   16] Training loss: 0.04947603, Validation loss: 0.04876785, Gradient norm: 1.33377502
INFO:root:[   17] Training loss: 0.04921699, Validation loss: 0.05077982, Gradient norm: 1.46473575
INFO:root:[   18] Training loss: 0.04928278, Validation loss: 0.04767606, Gradient norm: 1.41621926
INFO:root:[   19] Training loss: 0.04841925, Validation loss: 0.04534230, Gradient norm: 1.30794942
INFO:root:[   20] Training loss: 0.04797861, Validation loss: 0.04286410, Gradient norm: 1.44575615
INFO:root:[   21] Training loss: 0.04691315, Validation loss: 0.04306934, Gradient norm: 1.36766501
INFO:root:[   22] Training loss: 0.04590556, Validation loss: 0.04783141, Gradient norm: 1.24764050
INFO:root:[   23] Training loss: 0.04596082, Validation loss: 0.04464842, Gradient norm: 1.40729659
INFO:root:[   24] Training loss: 0.04571701, Validation loss: 0.04507789, Gradient norm: 1.48101545
INFO:root:[   25] Training loss: 0.04490149, Validation loss: 0.04363325, Gradient norm: 1.45701543
INFO:root:[   26] Training loss: 0.04480604, Validation loss: 0.04017616, Gradient norm: 1.37754860
INFO:root:[   27] Training loss: 0.04570978, Validation loss: 0.04466990, Gradient norm: 1.42802484
INFO:root:[   28] Training loss: 0.04430028, Validation loss: 0.04340047, Gradient norm: 1.34369338
INFO:root:[   29] Training loss: 0.04365952, Validation loss: 0.04164879, Gradient norm: 1.38233954
INFO:root:[   30] Training loss: 0.04252173, Validation loss: 0.04661073, Gradient norm: 1.25315043
INFO:root:[   31] Training loss: 0.04304352, Validation loss: 0.04293361, Gradient norm: 1.49540401
INFO:root:[   32] Training loss: 0.04292990, Validation loss: 0.03908039, Gradient norm: 1.48450798
INFO:root:[   33] Training loss: 0.04219186, Validation loss: 0.03838913, Gradient norm: 1.35042821
INFO:root:[   34] Training loss: 0.04298744, Validation loss: 0.03641021, Gradient norm: 1.48648535
INFO:root:[   35] Training loss: 0.04236385, Validation loss: 0.04977460, Gradient norm: 1.43072200
INFO:root:[   36] Training loss: 0.04308060, Validation loss: 0.03598890, Gradient norm: 1.53336555
INFO:root:[   37] Training loss: 0.04093098, Validation loss: 0.04232336, Gradient norm: 1.38124539
INFO:root:[   38] Training loss: 0.04033514, Validation loss: 0.04339828, Gradient norm: 1.32022304
INFO:root:[   39] Training loss: 0.04234926, Validation loss: 0.05090937, Gradient norm: 1.70834994
INFO:root:[   40] Training loss: 0.04284674, Validation loss: 0.04106673, Gradient norm: 1.65444310
INFO:root:[   41] Training loss: 0.04167367, Validation loss: 0.04120666, Gradient norm: 1.30086069
INFO:root:[   42] Training loss: 0.04122300, Validation loss: 0.03803151, Gradient norm: 1.62466060
INFO:root:[   43] Training loss: 0.03997313, Validation loss: 0.04259977, Gradient norm: 1.48130160
INFO:root:[   44] Training loss: 0.04017571, Validation loss: 0.04491599, Gradient norm: 1.57998491
INFO:root:[   45] Training loss: 0.04055288, Validation loss: 0.04548115, Gradient norm: 1.60373342
INFO:root:[   46] Training loss: 0.04018858, Validation loss: 0.04188276, Gradient norm: 1.36297497
INFO:root:[   47] Training loss: 0.04054896, Validation loss: 0.03892624, Gradient norm: 1.55845235
INFO:root:[   48] Training loss: 0.03895662, Validation loss: 0.03492408, Gradient norm: 1.52861765
INFO:root:[   49] Training loss: 0.03873317, Validation loss: 0.03568212, Gradient norm: 1.53729851
INFO:root:[   50] Training loss: 0.03916818, Validation loss: 0.03562656, Gradient norm: 1.58794434
INFO:root:[   51] Training loss: 0.03801530, Validation loss: 0.03402052, Gradient norm: 1.41009028
INFO:root:[   52] Training loss: 0.03785226, Validation loss: 0.03814080, Gradient norm: 1.30911606
INFO:root:[   53] Training loss: 0.03776364, Validation loss: 0.03527344, Gradient norm: 1.27076140
INFO:root:[   54] Training loss: 0.03778892, Validation loss: 0.04072450, Gradient norm: 1.27853892
INFO:root:[   55] Training loss: 0.03719454, Validation loss: 0.03837274, Gradient norm: 1.35603709
INFO:root:[   56] Training loss: 0.03802735, Validation loss: 0.03753959, Gradient norm: 1.56380997
INFO:root:[   57] Training loss: 0.03775492, Validation loss: 0.03522398, Gradient norm: 1.62666195
INFO:root:[   58] Training loss: 0.03710598, Validation loss: 0.03697475, Gradient norm: 1.55220977
INFO:root:[   59] Training loss: 0.03560545, Validation loss: 0.03461511, Gradient norm: 1.30011443
INFO:root:[   60] Training loss: 0.03716117, Validation loss: 0.03601535, Gradient norm: 1.51236491
INFO:root:[   61] Training loss: 0.03798689, Validation loss: 0.03936651, Gradient norm: 1.40285290
INFO:root:[   62] Training loss: 0.03614452, Validation loss: 0.03655805, Gradient norm: 1.29490071
INFO:root:[   63] Training loss: 0.03502899, Validation loss: 0.03431772, Gradient norm: 1.26816042
INFO:root:[   64] Training loss: 0.03604192, Validation loss: 0.04343027, Gradient norm: 1.37669016
INFO:root:[   65] Training loss: 0.03768373, Validation loss: 0.04159852, Gradient norm: 1.65482850
INFO:root:[   66] Training loss: 0.03613052, Validation loss: 0.04225876, Gradient norm: 1.61692205
INFO:root:[   67] Training loss: 0.03576154, Validation loss: 0.03992038, Gradient norm: 1.59736805
INFO:root:[   68] Training loss: 0.03538149, Validation loss: 0.03985576, Gradient norm: 1.57482486
INFO:root:[   69] Training loss: 0.03506091, Validation loss: 0.04081319, Gradient norm: 1.51336703
INFO:root:[   70] Training loss: 0.03531773, Validation loss: 0.03939772, Gradient norm: 1.55026090
INFO:root:[   71] Training loss: 0.03440992, Validation loss: 0.03708179, Gradient norm: 1.34716575
INFO:root:[   72] Training loss: 0.03515717, Validation loss: 0.03636137, Gradient norm: 1.33624421
INFO:root:[   73] Training loss: 0.03511845, Validation loss: 0.03059317, Gradient norm: 1.24724463
INFO:root:[   74] Training loss: 0.03428806, Validation loss: 0.03712184, Gradient norm: 1.37326538
INFO:root:[   75] Training loss: 0.03461887, Validation loss: 0.03777556, Gradient norm: 1.44905016
INFO:root:[   76] Training loss: 0.03366461, Validation loss: 0.02993878, Gradient norm: 1.32028464
INFO:root:[   77] Training loss: 0.03319852, Validation loss: 0.03703143, Gradient norm: 1.34081271
INFO:root:[   78] Training loss: 0.03255257, Validation loss: 0.03477336, Gradient norm: 1.29224611
INFO:root:[   79] Training loss: 0.03410112, Validation loss: 0.04087391, Gradient norm: 1.60860958
INFO:root:[   80] Training loss: 0.03460112, Validation loss: 0.03930252, Gradient norm: 1.64133830
INFO:root:[   81] Training loss: 0.03438224, Validation loss: 0.03855956, Gradient norm: 1.65028396
INFO:root:[   82] Training loss: 0.03354099, Validation loss: 0.03730804, Gradient norm: 1.59834767
INFO:root:[   83] Training loss: 0.03365379, Validation loss: 0.03514628, Gradient norm: 1.45026156
INFO:root:[   84] Training loss: 0.03216145, Validation loss: 0.02818127, Gradient norm: 1.25152998
INFO:root:[   85] Training loss: 0.03201954, Validation loss: 0.03311978, Gradient norm: 1.36461232
INFO:root:[   86] Training loss: 0.03187635, Validation loss: 0.02895469, Gradient norm: 1.21820204
INFO:root:[   87] Training loss: 0.03247135, Validation loss: 0.02789351, Gradient norm: 1.29434117
INFO:root:[   88] Training loss: 0.03128223, Validation loss: 0.03422388, Gradient norm: 1.17465043
INFO:root:[   89] Training loss: 0.03237340, Validation loss: 0.03234920, Gradient norm: 1.27449870
INFO:root:[   90] Training loss: 0.03129037, Validation loss: 0.02860989, Gradient norm: 1.31977793
INFO:root:[   91] Training loss: 0.03142754, Validation loss: 0.02961271, Gradient norm: 1.36742642
INFO:root:[   92] Training loss: 0.03102973, Validation loss: 0.02699591, Gradient norm: 1.35206601
INFO:root:[   93] Training loss: 0.03084057, Validation loss: 0.02800807, Gradient norm: 1.31873687
INFO:root:[   94] Training loss: 0.03076811, Validation loss: 0.02788647, Gradient norm: 1.30680172
INFO:root:[   95] Training loss: 0.03088334, Validation loss: 0.02768348, Gradient norm: 1.36717812
INFO:root:[   96] Training loss: 0.02987435, Validation loss: 0.03131619, Gradient norm: 1.25641667
INFO:root:[   97] Training loss: 0.03085909, Validation loss: 0.02806872, Gradient norm: 1.44563737
INFO:root:[   98] Training loss: 0.03055526, Validation loss: 0.02674354, Gradient norm: 1.43433298
INFO:root:[   99] Training loss: 0.03172880, Validation loss: 0.03218938, Gradient norm: 1.60916432
INFO:root:[  100] Training loss: 0.03205431, Validation loss: 0.03028988, Gradient norm: 1.73184359
INFO:root:[  101] Training loss: 0.03118904, Validation loss: 0.03164414, Gradient norm: 1.63051694
INFO:root:[  102] Training loss: 0.03127772, Validation loss: 0.03011025, Gradient norm: 1.63182328
INFO:root:[  103] Training loss: 0.03111095, Validation loss: 0.02877475, Gradient norm: 1.64643481
INFO:root:[  104] Training loss: 0.03048308, Validation loss: 0.02930102, Gradient norm: 1.61397320
INFO:root:[  105] Training loss: 0.03055611, Validation loss: 0.02826795, Gradient norm: 1.58369794
INFO:root:[  106] Training loss: 0.02936500, Validation loss: 0.03126025, Gradient norm: 1.33257431
INFO:root:[  107] Training loss: 0.02844551, Validation loss: 0.02700934, Gradient norm: 1.25889354
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 3739.119s.
INFO:root:Emptying the cuda cache took 0.082s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01957
INFO:root:EnergyScoreTrain: 0.02674
INFO:root:CoverageTrain: 0.99998
INFO:root:IntervalWidthTrain: 0.10016
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01958
INFO:root:EnergyScoreValidation: 0.02674
INFO:root:CoverageValidation: 0.99998
INFO:root:IntervalWidthValidation: 0.10015
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01924
INFO:root:EnergyScoreTest: 0.02656
INFO:root:CoverageTest: 0.99998
INFO:root:IntervalWidthTest: 0.09974
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.67249345, Validation loss: 0.16467435, Gradient norm: 4.25716130
INFO:root:[    2] Training loss: 0.13112005, Validation loss: 0.10602687, Gradient norm: 1.55064807
INFO:root:[    3] Training loss: 0.10540664, Validation loss: 0.09279219, Gradient norm: 1.80063289
INFO:root:[    4] Training loss: 0.08284835, Validation loss: 0.08141117, Gradient norm: 1.10044276
INFO:root:[    5] Training loss: 0.07855895, Validation loss: 0.07352074, Gradient norm: 1.72776091
INFO:root:[    6] Training loss: 0.07093971, Validation loss: 0.07079573, Gradient norm: 1.44832084
INFO:root:[    7] Training loss: 0.06915660, Validation loss: 0.06658758, Gradient norm: 1.60357058
INFO:root:[    8] Training loss: 0.06703858, Validation loss: 0.06481998, Gradient norm: 1.43540992
INFO:root:[    9] Training loss: 0.06101116, Validation loss: 0.05768861, Gradient norm: 0.99918219
INFO:root:[   10] Training loss: 0.06395932, Validation loss: 0.06335611, Gradient norm: 1.56952449
INFO:root:[   11] Training loss: 0.05956145, Validation loss: 0.06216615, Gradient norm: 1.08810238
INFO:root:[   12] Training loss: 0.06092311, Validation loss: 0.06434749, Gradient norm: 1.42828497
INFO:root:[   13] Training loss: 0.06012072, Validation loss: 0.05555252, Gradient norm: 1.48990840
INFO:root:[   14] Training loss: 0.05812170, Validation loss: 0.05483307, Gradient norm: 1.36564951
INFO:root:[   15] Training loss: 0.05692615, Validation loss: 0.05275892, Gradient norm: 1.26365477
INFO:root:[   16] Training loss: 0.05805167, Validation loss: 0.05346651, Gradient norm: 1.42719681
INFO:root:[   17] Training loss: 0.05597362, Validation loss: 0.05203556, Gradient norm: 1.26749339
INFO:root:[   18] Training loss: 0.05450013, Validation loss: 0.05873781, Gradient norm: 1.23792739
INFO:root:[   19] Training loss: 0.05543171, Validation loss: 0.05342510, Gradient norm: 1.45596477
INFO:root:[   20] Training loss: 0.05410867, Validation loss: 0.05275491, Gradient norm: 1.36474866
INFO:root:[   21] Training loss: 0.05334097, Validation loss: 0.05265663, Gradient norm: 1.37764640
INFO:root:[   22] Training loss: 0.05339025, Validation loss: 0.05018997, Gradient norm: 1.39753769
INFO:root:[   23] Training loss: 0.05287148, Validation loss: 0.05080548, Gradient norm: 1.39524581
INFO:root:[   24] Training loss: 0.05263368, Validation loss: 0.05449959, Gradient norm: 1.32653483
INFO:root:[   25] Training loss: 0.05198146, Validation loss: 0.05212014, Gradient norm: 1.44785232
INFO:root:[   26] Training loss: 0.05149072, Validation loss: 0.05228050, Gradient norm: 1.42908185
INFO:root:[   27] Training loss: 0.05036743, Validation loss: 0.05055661, Gradient norm: 1.35939316
INFO:root:[   28] Training loss: 0.05046182, Validation loss: 0.05680577, Gradient norm: 1.30058039
INFO:root:[   29] Training loss: 0.05027559, Validation loss: 0.05412345, Gradient norm: 1.41653716
INFO:root:[   30] Training loss: 0.05034025, Validation loss: 0.04626595, Gradient norm: 1.54423380
INFO:root:[   31] Training loss: 0.04962740, Validation loss: 0.04839343, Gradient norm: 1.46323750
INFO:root:[   32] Training loss: 0.04873766, Validation loss: 0.05201971, Gradient norm: 1.40470956
INFO:root:[   33] Training loss: 0.04852648, Validation loss: 0.04783207, Gradient norm: 1.39253935
INFO:root:[   34] Training loss: 0.04856267, Validation loss: 0.05020651, Gradient norm: 1.43317298
INFO:root:[   35] Training loss: 0.04857689, Validation loss: 0.04589594, Gradient norm: 1.33317879
INFO:root:[   36] Training loss: 0.04668707, Validation loss: 0.04836341, Gradient norm: 1.32486396
INFO:root:[   37] Training loss: 0.04636361, Validation loss: 0.04849670, Gradient norm: 1.39362590
INFO:root:[   38] Training loss: 0.04646071, Validation loss: 0.05197863, Gradient norm: 1.47243443
INFO:root:[   39] Training loss: 0.04682725, Validation loss: 0.04132459, Gradient norm: 1.58006561
INFO:root:[   40] Training loss: 0.04500577, Validation loss: 0.04069700, Gradient norm: 1.38276756
INFO:root:[   41] Training loss: 0.04533666, Validation loss: 0.04850298, Gradient norm: 1.46355710
INFO:root:[   42] Training loss: 0.04409490, Validation loss: 0.04631588, Gradient norm: 1.30616558
INFO:root:[   43] Training loss: 0.04427466, Validation loss: 0.04024408, Gradient norm: 1.33262854
INFO:root:[   44] Training loss: 0.04378337, Validation loss: 0.04028943, Gradient norm: 1.39135037
INFO:root:[   45] Training loss: 0.04419712, Validation loss: 0.03923412, Gradient norm: 1.43002887
INFO:root:[   46] Training loss: 0.04369637, Validation loss: 0.03900974, Gradient norm: 1.46826182
INFO:root:[   47] Training loss: 0.04327189, Validation loss: 0.04614424, Gradient norm: 1.43351596
INFO:root:[   48] Training loss: 0.04231765, Validation loss: 0.04287052, Gradient norm: 1.33961115
INFO:root:[   49] Training loss: 0.04335517, Validation loss: 0.03901528, Gradient norm: 1.45148287
INFO:root:[   50] Training loss: 0.04230491, Validation loss: 0.03801052, Gradient norm: 1.42522960
INFO:root:[   51] Training loss: 0.04192960, Validation loss: 0.04066296, Gradient norm: 1.45360565
INFO:root:[   52] Training loss: 0.04172308, Validation loss: 0.04224017, Gradient norm: 1.44388914
INFO:root:[   53] Training loss: 0.04155389, Validation loss: 0.04398333, Gradient norm: 1.44890982
INFO:root:[   54] Training loss: 0.04100261, Validation loss: 0.04390311, Gradient norm: 1.38802142
INFO:root:[   55] Training loss: 0.04022219, Validation loss: 0.04015476, Gradient norm: 1.33553470
INFO:root:[   56] Training loss: 0.04069784, Validation loss: 0.04213114, Gradient norm: 1.45638121
INFO:root:[   57] Training loss: 0.03979486, Validation loss: 0.04202689, Gradient norm: 1.42710616
INFO:root:[   58] Training loss: 0.03935008, Validation loss: 0.03701024, Gradient norm: 1.38781356
INFO:root:[   59] Training loss: 0.03956353, Validation loss: 0.03556000, Gradient norm: 1.46238951
INFO:root:[   60] Training loss: 0.04027982, Validation loss: 0.03553965, Gradient norm: 1.44213758
INFO:root:[   61] Training loss: 0.04076984, Validation loss: 0.04499262, Gradient norm: 1.48067604
INFO:root:[   62] Training loss: 0.03950796, Validation loss: 0.03862203, Gradient norm: 1.43910506
INFO:root:[   63] Training loss: 0.03811201, Validation loss: 0.04117528, Gradient norm: 1.34160394
INFO:root:[   64] Training loss: 0.03762366, Validation loss: 0.04154714, Gradient norm: 1.35489556
INFO:root:[   65] Training loss: 0.03781888, Validation loss: 0.03937244, Gradient norm: 1.42094123
INFO:root:[   66] Training loss: 0.03775144, Validation loss: 0.03901047, Gradient norm: 1.42004520
INFO:root:[   67] Training loss: 0.03775440, Validation loss: 0.03718428, Gradient norm: 1.43756533
INFO:root:[   68] Training loss: 0.03782295, Validation loss: 0.04159235, Gradient norm: 1.50460239
INFO:root:[   69] Training loss: 0.03713504, Validation loss: 0.04029165, Gradient norm: 1.43969417
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2423.846s.
INFO:root:Emptying the cuda cache took 0.084s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0249
INFO:root:EnergyScoreTrain: 0.03556
INFO:root:CoverageTrain: 0.99999
INFO:root:IntervalWidthTrain: 0.13483
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02502
INFO:root:EnergyScoreValidation: 0.03558
INFO:root:CoverageValidation: 0.99999
INFO:root:IntervalWidthValidation: 0.13478
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02477
INFO:root:EnergyScoreTest: 0.03539
INFO:root:CoverageTest: 0.99999
INFO:root:IntervalWidthTest: 0.13424
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.83434200, Validation loss: 0.12429546, Gradient norm: 3.39061772
INFO:root:[    2] Training loss: 0.12031193, Validation loss: 0.10675410, Gradient norm: 1.31127035
INFO:root:[    3] Training loss: 0.10548622, Validation loss: 0.09580352, Gradient norm: 1.15007905
INFO:root:[    4] Training loss: 0.09508660, Validation loss: 0.08565533, Gradient norm: 1.24620381
INFO:root:[    5] Training loss: 0.08966510, Validation loss: 0.08459585, Gradient norm: 1.54884705
INFO:root:[    6] Training loss: 0.08250901, Validation loss: 0.08933132, Gradient norm: 1.29002931
INFO:root:[    7] Training loss: 0.08019849, Validation loss: 0.07129783, Gradient norm: 1.43171078
INFO:root:[    8] Training loss: 0.07553722, Validation loss: 0.07653779, Gradient norm: 1.31744149
INFO:root:[    9] Training loss: 0.07117179, Validation loss: 0.07660926, Gradient norm: 1.12022008
INFO:root:[   10] Training loss: 0.06934846, Validation loss: 0.06697761, Gradient norm: 1.21713555
INFO:root:[   11] Training loss: 0.06970288, Validation loss: 0.06851038, Gradient norm: 1.32232376
INFO:root:[   12] Training loss: 0.06783202, Validation loss: 0.06614997, Gradient norm: 1.43013206
INFO:root:[   13] Training loss: 0.06662763, Validation loss: 0.06504231, Gradient norm: 1.30317521
INFO:root:[   14] Training loss: 0.06505554, Validation loss: 0.06075506, Gradient norm: 1.21568753
INFO:root:[   15] Training loss: 0.06383392, Validation loss: 0.06078918, Gradient norm: 1.31435343
INFO:root:[   16] Training loss: 0.06374151, Validation loss: 0.06274849, Gradient norm: 1.37614958
INFO:root:[   17] Training loss: 0.06319722, Validation loss: 0.06860018, Gradient norm: 1.24834709
INFO:root:[   18] Training loss: 0.06113614, Validation loss: 0.06010493, Gradient norm: 1.11723311
INFO:root:[   19] Training loss: 0.06068321, Validation loss: 0.05604784, Gradient norm: 1.24721793
INFO:root:[   20] Training loss: 0.05950804, Validation loss: 0.05754443, Gradient norm: 1.13355013
INFO:root:[   21] Training loss: 0.06035700, Validation loss: 0.05607312, Gradient norm: 1.37661447
INFO:root:[   22] Training loss: 0.05899915, Validation loss: 0.05565424, Gradient norm: 1.22269021
INFO:root:[   23] Training loss: 0.05893017, Validation loss: 0.05412238, Gradient norm: 1.31298298
INFO:root:[   24] Training loss: 0.05859307, Validation loss: 0.05354049, Gradient norm: 1.30061896
INFO:root:[   25] Training loss: 0.05719677, Validation loss: 0.05925180, Gradient norm: 1.20499127
INFO:root:[   26] Training loss: 0.05595781, Validation loss: 0.05998518, Gradient norm: 1.13579015
INFO:root:[   27] Training loss: 0.05728769, Validation loss: 0.05625226, Gradient norm: 1.29132556
INFO:root:[   28] Training loss: 0.05501760, Validation loss: 0.05979925, Gradient norm: 1.08993649
INFO:root:[   29] Training loss: 0.05541075, Validation loss: 0.06040826, Gradient norm: 1.16519392
INFO:root:[   30] Training loss: 0.05559476, Validation loss: 0.05689910, Gradient norm: 1.16832088
INFO:root:[   31] Training loss: 0.05432323, Validation loss: 0.05760597, Gradient norm: 1.15069643
INFO:root:[   32] Training loss: 0.05400825, Validation loss: 0.05070990, Gradient norm: 1.12930350
INFO:root:[   33] Training loss: 0.05432527, Validation loss: 0.05173217, Gradient norm: 1.33373926
INFO:root:[   34] Training loss: 0.05214290, Validation loss: 0.04899351, Gradient norm: 1.13329392
INFO:root:[   35] Training loss: 0.05164373, Validation loss: 0.05673361, Gradient norm: 1.12108151
INFO:root:[   36] Training loss: 0.05326047, Validation loss: 0.05707634, Gradient norm: 1.33996999
INFO:root:[   37] Training loss: 0.05241626, Validation loss: 0.06085528, Gradient norm: 1.31008573
INFO:root:[   38] Training loss: 0.05119035, Validation loss: 0.05588148, Gradient norm: 1.21071036
INFO:root:[   39] Training loss: 0.05126079, Validation loss: 0.05594913, Gradient norm: 1.33497169
INFO:root:[   40] Training loss: 0.05035670, Validation loss: 0.05002133, Gradient norm: 1.20004912
INFO:root:[   41] Training loss: 0.05016604, Validation loss: 0.05026841, Gradient norm: 1.12221816
INFO:root:[   42] Training loss: 0.04961752, Validation loss: 0.05056400, Gradient norm: 1.12972137
INFO:root:[   43] Training loss: 0.05016108, Validation loss: 0.04686675, Gradient norm: 1.19779870
INFO:root:[   44] Training loss: 0.04904123, Validation loss: 0.04701270, Gradient norm: 1.14855011
INFO:root:[   45] Training loss: 0.04754842, Validation loss: 0.05125835, Gradient norm: 1.07064440
INFO:root:[   46] Training loss: 0.04804712, Validation loss: 0.04580477, Gradient norm: 1.12919282
INFO:root:[   47] Training loss: 0.04739713, Validation loss: 0.04828352, Gradient norm: 1.12823684
INFO:root:[   48] Training loss: 0.04894057, Validation loss: 0.05414374, Gradient norm: 1.29249838
INFO:root:[   49] Training loss: 0.04778092, Validation loss: 0.05362438, Gradient norm: 1.22319767
INFO:root:[   50] Training loss: 0.04753817, Validation loss: 0.04655165, Gradient norm: 1.23995696
INFO:root:[   51] Training loss: 0.04669746, Validation loss: 0.04358378, Gradient norm: 1.23968253
INFO:root:[   52] Training loss: 0.04729252, Validation loss: 0.04374419, Gradient norm: 1.37117473
INFO:root:[   53] Training loss: 0.04625010, Validation loss: 0.04412007, Gradient norm: 1.34166409
INFO:root:[   54] Training loss: 0.04626267, Validation loss: 0.04536078, Gradient norm: 1.33142851
INFO:root:[   55] Training loss: 0.04507411, Validation loss: 0.04447486, Gradient norm: 1.10423512
INFO:root:[   56] Training loss: 0.04519888, Validation loss: 0.04327750, Gradient norm: 1.31844123
INFO:root:[   57] Training loss: 0.04465319, Validation loss: 0.04121750, Gradient norm: 1.17080659
INFO:root:[   58] Training loss: 0.04423908, Validation loss: 0.04055387, Gradient norm: 1.11995197
INFO:root:[   59] Training loss: 0.04506635, Validation loss: 0.04217413, Gradient norm: 1.29062352
INFO:root:[   60] Training loss: 0.04536743, Validation loss: 0.03954952, Gradient norm: 1.36299391
INFO:root:[   61] Training loss: 0.04406832, Validation loss: 0.03977152, Gradient norm: 1.16093891
INFO:root:[   62] Training loss: 0.04334611, Validation loss: 0.04147405, Gradient norm: 1.19133261
INFO:root:[   63] Training loss: 0.04373365, Validation loss: 0.04084315, Gradient norm: 1.34516660
INFO:root:[   64] Training loss: 0.04389233, Validation loss: 0.03930819, Gradient norm: 1.33366773
INFO:root:[   65] Training loss: 0.04256105, Validation loss: 0.04074424, Gradient norm: 1.25376519
INFO:root:[   66] Training loss: 0.04293922, Validation loss: 0.03986689, Gradient norm: 1.35595463
INFO:root:[   67] Training loss: 0.04241077, Validation loss: 0.03842634, Gradient norm: 1.28819906
INFO:root:[   68] Training loss: 0.04249810, Validation loss: 0.04050034, Gradient norm: 1.27308261
INFO:root:[   69] Training loss: 0.04342733, Validation loss: 0.03904888, Gradient norm: 1.26406026
INFO:root:[   70] Training loss: 0.04098083, Validation loss: 0.03731618, Gradient norm: 1.21455453
INFO:root:[   71] Training loss: 0.04119600, Validation loss: 0.03846513, Gradient norm: 1.31434205
INFO:root:[   72] Training loss: 0.04052807, Validation loss: 0.03888657, Gradient norm: 1.24833827
INFO:root:[   73] Training loss: 0.04110719, Validation loss: 0.03976896, Gradient norm: 1.30743376
INFO:root:[   74] Training loss: 0.04094554, Validation loss: 0.03905985, Gradient norm: 1.23144766
INFO:root:[   75] Training loss: 0.04002735, Validation loss: 0.03990314, Gradient norm: 1.30528874
INFO:root:[   76] Training loss: 0.04014306, Validation loss: 0.03783129, Gradient norm: 1.30760283
INFO:root:[   77] Training loss: 0.03883741, Validation loss: 0.04093809, Gradient norm: 1.08854124
INFO:root:[   78] Training loss: 0.03894671, Validation loss: 0.03670698, Gradient norm: 1.12983665
INFO:root:[   79] Training loss: 0.03892329, Validation loss: 0.03478827, Gradient norm: 1.22416138
INFO:root:[   80] Training loss: 0.03831879, Validation loss: 0.03898563, Gradient norm: 1.19515921
INFO:root:[   81] Training loss: 0.03987104, Validation loss: 0.03882464, Gradient norm: 1.43028885
INFO:root:[   82] Training loss: 0.03933184, Validation loss: 0.03860330, Gradient norm: 1.32775899
INFO:root:[   83] Training loss: 0.03843789, Validation loss: 0.04191445, Gradient norm: 1.16882513
INFO:root:[   84] Training loss: 0.03714478, Validation loss: 0.03685780, Gradient norm: 1.17229236
INFO:root:[   85] Training loss: 0.03819021, Validation loss: 0.03599997, Gradient norm: 1.37567942
INFO:root:[   86] Training loss: 0.03805013, Validation loss: 0.03744433, Gradient norm: 1.35678775
INFO:root:[   87] Training loss: 0.03785970, Validation loss: 0.03418482, Gradient norm: 1.35889776
INFO:root:[   88] Training loss: 0.03715020, Validation loss: 0.03421004, Gradient norm: 1.35063146
INFO:root:[   89] Training loss: 0.03706788, Validation loss: 0.03317374, Gradient norm: 1.33544392
INFO:root:[   90] Training loss: 0.03586273, Validation loss: 0.03285972, Gradient norm: 1.12178660
INFO:root:[   91] Training loss: 0.03652401, Validation loss: 0.04085428, Gradient norm: 1.17231674
INFO:root:[   92] Training loss: 0.03618034, Validation loss: 0.03612836, Gradient norm: 1.18095366
INFO:root:[   93] Training loss: 0.03491008, Validation loss: 0.03424074, Gradient norm: 1.12683200
INFO:root:[   94] Training loss: 0.03549692, Validation loss: 0.03529897, Gradient norm: 1.12440188
INFO:root:[   95] Training loss: 0.03559827, Validation loss: 0.03617421, Gradient norm: 1.22985995
INFO:root:[   96] Training loss: 0.03466231, Validation loss: 0.03326708, Gradient norm: 1.16251883
INFO:root:[   97] Training loss: 0.03603578, Validation loss: 0.03264374, Gradient norm: 1.48838767
INFO:root:[   98] Training loss: 0.03532898, Validation loss: 0.03280305, Gradient norm: 1.39365955
INFO:root:[   99] Training loss: 0.03483870, Validation loss: 0.03382495, Gradient norm: 1.23241830
INFO:root:[  100] Training loss: 0.03543067, Validation loss: 0.03333437, Gradient norm: 1.33774375
INFO:root:[  101] Training loss: 0.03405462, Validation loss: 0.03293592, Gradient norm: 1.29437281
INFO:root:[  102] Training loss: 0.03456047, Validation loss: 0.03249177, Gradient norm: 1.44619193
INFO:root:[  103] Training loss: 0.03430044, Validation loss: 0.03110970, Gradient norm: 1.39494252
INFO:root:[  104] Training loss: 0.03303984, Validation loss: 0.03701423, Gradient norm: 1.15980476
INFO:root:[  105] Training loss: 0.03360365, Validation loss: 0.03960306, Gradient norm: 1.33245550
INFO:root:[  106] Training loss: 0.03384134, Validation loss: 0.03672849, Gradient norm: 1.42242028
INFO:root:[  107] Training loss: 0.03446174, Validation loss: 0.03099292, Gradient norm: 1.28436321
INFO:root:[  108] Training loss: 0.03362350, Validation loss: 0.03218296, Gradient norm: 1.42914645
INFO:root:[  109] Training loss: 0.03222222, Validation loss: 0.02930573, Gradient norm: 1.16383980
INFO:root:[  110] Training loss: 0.03135405, Validation loss: 0.03576976, Gradient norm: 1.13768865
INFO:root:[  111] Training loss: 0.03229696, Validation loss: 0.03560981, Gradient norm: 1.30015300
INFO:root:[  112] Training loss: 0.03186692, Validation loss: 0.02769465, Gradient norm: 1.16997643
INFO:root:[  113] Training loss: 0.03187755, Validation loss: 0.03019973, Gradient norm: 1.24880210
INFO:root:[  114] Training loss: 0.03128179, Validation loss: 0.03301891, Gradient norm: 1.22039987
INFO:root:[  115] Training loss: 0.03171244, Validation loss: 0.03580724, Gradient norm: 1.35331469
INFO:root:[  116] Training loss: 0.03189554, Validation loss: 0.03461529, Gradient norm: 1.43803561
INFO:root:[  117] Training loss: 0.03109121, Validation loss: 0.03378330, Gradient norm: 1.33042977
INFO:root:[  118] Training loss: 0.03051743, Validation loss: 0.03168819, Gradient norm: 1.21975990
INFO:root:[  119] Training loss: 0.02997524, Validation loss: 0.02650833, Gradient norm: 1.14308814
INFO:root:[  120] Training loss: 0.02997366, Validation loss: 0.03152883, Gradient norm: 1.23057552
INFO:root:[  121] Training loss: 0.03037929, Validation loss: 0.03088192, Gradient norm: 1.31791200
INFO:root:[  122] Training loss: 0.03034618, Validation loss: 0.02933006, Gradient norm: 1.34508039
INFO:root:[  123] Training loss: 0.02986753, Validation loss: 0.03225743, Gradient norm: 1.18875791
INFO:root:[  124] Training loss: 0.02963486, Validation loss: 0.02742180, Gradient norm: 1.31055540
INFO:root:[  125] Training loss: 0.03003244, Validation loss: 0.02602050, Gradient norm: 1.36025524
INFO:root:[  126] Training loss: 0.02943453, Validation loss: 0.02794207, Gradient norm: 1.31161001
INFO:root:[  127] Training loss: 0.02962706, Validation loss: 0.02910444, Gradient norm: 1.40456373
INFO:root:[  128] Training loss: 0.02983233, Validation loss: 0.02765863, Gradient norm: 1.43749653
INFO:root:[  129] Training loss: 0.02936852, Validation loss: 0.02764492, Gradient norm: 1.40015789
INFO:root:[  130] Training loss: 0.02883824, Validation loss: 0.03171958, Gradient norm: 1.21922159
INFO:root:[  131] Training loss: 0.02920359, Validation loss: 0.03290034, Gradient norm: 1.40644111
INFO:root:[  132] Training loss: 0.02871213, Validation loss: 0.03213607, Gradient norm: 1.41474942
INFO:root:[  133] Training loss: 0.02777718, Validation loss: 0.03072750, Gradient norm: 1.24321792
INFO:root:[  134] Training loss: 0.02789760, Validation loss: 0.02881973, Gradient norm: 1.27277998
INFO:root:EP 134: Early stopping
INFO:root:Training the model took 4669.372s.
INFO:root:Emptying the cuda cache took 0.085s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01812
INFO:root:EnergyScoreTrain: 0.02606
INFO:root:CoverageTrain: 0.99994
INFO:root:IntervalWidthTrain: 0.09803
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01797
INFO:root:EnergyScoreValidation: 0.02602
INFO:root:CoverageValidation: 0.99995
INFO:root:IntervalWidthValidation: 0.09803
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01797
INFO:root:EnergyScoreTest: 0.02588
INFO:root:CoverageTest: 0.99994
INFO:root:IntervalWidthTest: 0.09749
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.85929130, Validation loss: 0.24953811, Gradient norm: 4.45235642
INFO:root:[    2] Training loss: 0.19293738, Validation loss: 0.17495255, Gradient norm: 1.57444278
INFO:root:[    3] Training loss: 0.14174597, Validation loss: 0.12141681, Gradient norm: 1.93010265
INFO:root:[    4] Training loss: 0.11455003, Validation loss: 0.10281202, Gradient norm: 1.14265654
INFO:root:[    5] Training loss: 0.10623491, Validation loss: 0.09436100, Gradient norm: 1.63580802
INFO:root:[    6] Training loss: 0.09508813, Validation loss: 0.09085031, Gradient norm: 1.29153039
INFO:root:[    7] Training loss: 0.09086752, Validation loss: 0.09023406, Gradient norm: 1.28078052
INFO:root:[    8] Training loss: 0.08743286, Validation loss: 0.08802562, Gradient norm: 1.30426193
INFO:root:[    9] Training loss: 0.08556926, Validation loss: 0.08003204, Gradient norm: 1.32204215
INFO:root:[   10] Training loss: 0.08454333, Validation loss: 0.08107447, Gradient norm: 1.47498348
INFO:root:[   11] Training loss: 0.08276093, Validation loss: 0.07778927, Gradient norm: 1.40164186
INFO:root:[   12] Training loss: 0.07928768, Validation loss: 0.08440342, Gradient norm: 1.11807328
INFO:root:[   13] Training loss: 0.07908385, Validation loss: 0.07415217, Gradient norm: 1.27969699
INFO:root:[   14] Training loss: 0.07698016, Validation loss: 0.08227474, Gradient norm: 1.16432575
INFO:root:[   15] Training loss: 0.07637227, Validation loss: 0.07207821, Gradient norm: 1.22888681
INFO:root:[   16] Training loss: 0.07601786, Validation loss: 0.07966259, Gradient norm: 1.36102075
INFO:root:[   17] Training loss: 0.07529176, Validation loss: 0.07467673, Gradient norm: 1.22938074
INFO:root:[   18] Training loss: 0.07513767, Validation loss: 0.07878783, Gradient norm: 1.41766265
INFO:root:[   19] Training loss: 0.07506540, Validation loss: 0.07784275, Gradient norm: 1.52862611
INFO:root:[   20] Training loss: 0.07329665, Validation loss: 0.07976092, Gradient norm: 1.44998470
INFO:root:[   21] Training loss: 0.07267909, Validation loss: 0.06640323, Gradient norm: 1.38342596
INFO:root:[   22] Training loss: 0.06936645, Validation loss: 0.07092302, Gradient norm: 1.03748508
INFO:root:[   23] Training loss: 0.07064785, Validation loss: 0.07664791, Gradient norm: 1.32161435
INFO:root:[   24] Training loss: 0.07045161, Validation loss: 0.07365978, Gradient norm: 1.45144470
INFO:root:[   25] Training loss: 0.06922488, Validation loss: 0.07285723, Gradient norm: 1.32697176
INFO:root:[   26] Training loss: 0.06778847, Validation loss: 0.07054115, Gradient norm: 1.27857530
INFO:root:[   27] Training loss: 0.06775324, Validation loss: 0.07489008, Gradient norm: 1.37952236
INFO:root:[   28] Training loss: 0.06795294, Validation loss: 0.07094510, Gradient norm: 1.38451856
INFO:root:[   29] Training loss: 0.06641034, Validation loss: 0.07104108, Gradient norm: 1.34915223
INFO:root:[   30] Training loss: 0.06606900, Validation loss: 0.07120163, Gradient norm: 1.41085141
INFO:root:[   31] Training loss: 0.06499506, Validation loss: 0.07100833, Gradient norm: 1.29149031
INFO:root:[   32] Training loss: 0.06449705, Validation loss: 0.07086088, Gradient norm: 1.35280022
INFO:root:[   33] Training loss: 0.06476862, Validation loss: 0.06709159, Gradient norm: 1.38861794
INFO:root:[   34] Training loss: 0.06350392, Validation loss: 0.07033473, Gradient norm: 1.30578668
INFO:root:[   35] Training loss: 0.06327771, Validation loss: 0.06815033, Gradient norm: 1.36668399
INFO:root:[   36] Training loss: 0.06333831, Validation loss: 0.06499157, Gradient norm: 1.30142723
INFO:root:[   37] Training loss: 0.06123469, Validation loss: 0.06348044, Gradient norm: 1.12189253
INFO:root:[   38] Training loss: 0.06142299, Validation loss: 0.06675018, Gradient norm: 1.27695795
INFO:root:[   39] Training loss: 0.06110543, Validation loss: 0.06358381, Gradient norm: 1.35346147
INFO:root:[   40] Training loss: 0.05980954, Validation loss: 0.05794210, Gradient norm: 1.22801665
INFO:root:[   41] Training loss: 0.06001529, Validation loss: 0.05748064, Gradient norm: 1.40058127
INFO:root:[   42] Training loss: 0.05897665, Validation loss: 0.05392188, Gradient norm: 1.27649487
INFO:root:[   43] Training loss: 0.05911164, Validation loss: 0.05579641, Gradient norm: 1.38709390
INFO:root:[   44] Training loss: 0.05804993, Validation loss: 0.05448713, Gradient norm: 1.36487563
INFO:root:[   45] Training loss: 0.05692878, Validation loss: 0.05312818, Gradient norm: 1.17371607
INFO:root:[   46] Training loss: 0.05617370, Validation loss: 0.05921271, Gradient norm: 1.17392269
INFO:root:[   47] Training loss: 0.05666372, Validation loss: 0.05596664, Gradient norm: 1.24910871
INFO:root:[   48] Training loss: 0.05683013, Validation loss: 0.05444420, Gradient norm: 1.31570789
INFO:root:[   49] Training loss: 0.05547528, Validation loss: 0.05088839, Gradient norm: 1.26005619
INFO:root:[   50] Training loss: 0.05452660, Validation loss: 0.05334856, Gradient norm: 1.25861427
INFO:root:[   51] Training loss: 0.05526002, Validation loss: 0.05206755, Gradient norm: 1.48656859
INFO:root:[   52] Training loss: 0.05415829, Validation loss: 0.05445859, Gradient norm: 1.35415632
INFO:root:[   53] Training loss: 0.05451457, Validation loss: 0.05130970, Gradient norm: 1.42038835
INFO:root:[   54] Training loss: 0.05373756, Validation loss: 0.05224272, Gradient norm: 1.33338141
INFO:root:[   55] Training loss: 0.05365362, Validation loss: 0.05085577, Gradient norm: 1.37693166
INFO:root:[   56] Training loss: 0.05158988, Validation loss: 0.05435765, Gradient norm: 1.20922563
INFO:root:[   57] Training loss: 0.05186766, Validation loss: 0.04984838, Gradient norm: 1.18222747
INFO:root:[   58] Training loss: 0.05183017, Validation loss: 0.04911007, Gradient norm: 1.46933898
INFO:root:[   59] Training loss: 0.05127080, Validation loss: 0.04838752, Gradient norm: 1.43840119
INFO:root:[   60] Training loss: 0.05081399, Validation loss: 0.04780248, Gradient norm: 1.40739555
INFO:root:[   61] Training loss: 0.04949697, Validation loss: 0.04653859, Gradient norm: 1.33200930
INFO:root:[   62] Training loss: 0.04955586, Validation loss: 0.04647268, Gradient norm: 1.41701819
INFO:root:[   63] Training loss: 0.04895678, Validation loss: 0.04465543, Gradient norm: 1.34535239
INFO:root:[   64] Training loss: 0.04841662, Validation loss: 0.04504079, Gradient norm: 1.32734990
INFO:root:[   65] Training loss: 0.04823519, Validation loss: 0.04466273, Gradient norm: 1.37804841
INFO:root:[   66] Training loss: 0.04654302, Validation loss: 0.04437773, Gradient norm: 1.16386898
INFO:root:[   67] Training loss: 0.04710400, Validation loss: 0.04855862, Gradient norm: 1.12709410
INFO:root:[   68] Training loss: 0.04610951, Validation loss: 0.04834587, Gradient norm: 0.99553878
INFO:root:[   69] Training loss: 0.04694030, Validation loss: 0.05077429, Gradient norm: 1.45843481
INFO:root:[   70] Training loss: 0.04660090, Validation loss: 0.05077336, Gradient norm: 1.49319217
INFO:root:[   71] Training loss: 0.04622057, Validation loss: 0.04928824, Gradient norm: 1.46426006
INFO:root:[   72] Training loss: 0.04450948, Validation loss: 0.03992468, Gradient norm: 1.18908754
INFO:root:[   73] Training loss: 0.04439633, Validation loss: 0.04275429, Gradient norm: 1.29962976
INFO:root:[   74] Training loss: 0.04389542, Validation loss: 0.04445085, Gradient norm: 1.27711871
INFO:root:[   75] Training loss: 0.04333403, Validation loss: 0.05047837, Gradient norm: 1.26422351
INFO:root:[   76] Training loss: 0.04394383, Validation loss: 0.05164470, Gradient norm: 1.41822194
INFO:root:[   77] Training loss: 0.04305952, Validation loss: 0.04171219, Gradient norm: 1.29757342
INFO:root:[   78] Training loss: 0.04187198, Validation loss: 0.03792285, Gradient norm: 1.23294329
INFO:root:[   79] Training loss: 0.04146185, Validation loss: 0.04294089, Gradient norm: 1.18627946
INFO:root:[   80] Training loss: 0.04120387, Validation loss: 0.04365452, Gradient norm: 1.22658253
INFO:root:[   81] Training loss: 0.04048085, Validation loss: 0.03671372, Gradient norm: 1.18526030
INFO:root:[   82] Training loss: 0.04025438, Validation loss: 0.04143035, Gradient norm: 1.19393302
INFO:root:[   83] Training loss: 0.04194051, Validation loss: 0.03585807, Gradient norm: 1.31966731
INFO:root:[   84] Training loss: 0.03966150, Validation loss: 0.04259038, Gradient norm: 1.20548628
INFO:root:[   85] Training loss: 0.04043308, Validation loss: 0.04594637, Gradient norm: 1.45344102
INFO:root:[   86] Training loss: 0.04008154, Validation loss: 0.04466038, Gradient norm: 1.45419486
INFO:root:[   87] Training loss: 0.03936541, Validation loss: 0.03858244, Gradient norm: 1.37721929
INFO:root:[   88] Training loss: 0.03808958, Validation loss: 0.03816271, Gradient norm: 1.11051502
INFO:root:[   89] Training loss: 0.03864717, Validation loss: 0.04549890, Gradient norm: 1.25003692
INFO:root:[   90] Training loss: 0.03762083, Validation loss: 0.03384078, Gradient norm: 1.24007862
INFO:root:[   91] Training loss: 0.03809695, Validation loss: 0.03548063, Gradient norm: 1.39368396
INFO:root:[   92] Training loss: 0.03830468, Validation loss: 0.03518334, Gradient norm: 1.45074247
INFO:root:[   93] Training loss: 0.03723489, Validation loss: 0.03265007, Gradient norm: 1.34097296
INFO:root:[   94] Training loss: 0.03587587, Validation loss: 0.03537152, Gradient norm: 1.11747563
INFO:root:[   95] Training loss: 0.03588734, Validation loss: 0.03872379, Gradient norm: 1.18535397
INFO:root:[   96] Training loss: 0.03608204, Validation loss: 0.03445516, Gradient norm: 1.27619609
INFO:root:[   97] Training loss: 0.03502519, Validation loss: 0.03815719, Gradient norm: 1.17215905
INFO:root:[   98] Training loss: 0.03550466, Validation loss: 0.03277576, Gradient norm: 1.32509096
INFO:root:[   99] Training loss: 0.03568650, Validation loss: 0.03344752, Gradient norm: 1.42765997
INFO:root:[  100] Training loss: 0.03461208, Validation loss: 0.03486477, Gradient norm: 1.19250949
INFO:root:[  101] Training loss: 0.03514329, Validation loss: 0.03411122, Gradient norm: 1.41838050
INFO:root:[  102] Training loss: 0.03477539, Validation loss: 0.03177013, Gradient norm: 1.41878960
INFO:root:[  103] Training loss: 0.03376338, Validation loss: 0.03399146, Gradient norm: 1.25104510
INFO:root:[  104] Training loss: 0.03397715, Validation loss: 0.03234263, Gradient norm: 1.38136091
INFO:root:[  105] Training loss: 0.03400656, Validation loss: 0.03067542, Gradient norm: 1.36585532
INFO:root:[  106] Training loss: 0.03299881, Validation loss: 0.03629320, Gradient norm: 1.12983856
INFO:root:[  107] Training loss: 0.03291448, Validation loss: 0.03658914, Gradient norm: 1.34309913
INFO:root:[  108] Training loss: 0.03258750, Validation loss: 0.03697697, Gradient norm: 1.33048773
INFO:root:[  109] Training loss: 0.03189553, Validation loss: 0.03392324, Gradient norm: 1.23707459
INFO:root:[  110] Training loss: 0.03120587, Validation loss: 0.02797294, Gradient norm: 1.20112478
INFO:root:[  111] Training loss: 0.03186273, Validation loss: 0.02939381, Gradient norm: 1.40264166
INFO:root:[  112] Training loss: 0.03105362, Validation loss: 0.02906318, Gradient norm: 1.33908145
INFO:root:[  113] Training loss: 0.03113125, Validation loss: 0.02845922, Gradient norm: 1.37410038
INFO:root:[  114] Training loss: 0.03152285, Validation loss: 0.03185922, Gradient norm: 1.28050171
INFO:root:[  115] Training loss: 0.03060978, Validation loss: 0.02801291, Gradient norm: 1.09716179
INFO:root:[  116] Training loss: 0.02969143, Validation loss: 0.03019170, Gradient norm: 1.18800377
INFO:root:[  117] Training loss: 0.03051542, Validation loss: 0.02803898, Gradient norm: 1.39040780
INFO:root:[  118] Training loss: 0.02975985, Validation loss: 0.03194509, Gradient norm: 1.16450208
INFO:root:[  119] Training loss: 0.02951260, Validation loss: 0.03458607, Gradient norm: 1.31492074
INFO:root:EP 119: Early stopping
INFO:root:Training the model took 4155.203s.
INFO:root:Emptying the cuda cache took 0.086s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01919
INFO:root:EnergyScoreTrain: 0.02803
INFO:root:CoverageTrain: 0.99999
INFO:root:IntervalWidthTrain: 0.10449
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01913
INFO:root:EnergyScoreValidation: 0.02799
INFO:root:CoverageValidation: 1.0
INFO:root:IntervalWidthValidation: 0.10438
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01906
INFO:root:EnergyScoreTest: 0.0278
INFO:root:CoverageTest: 0.99999
INFO:root:IntervalWidthTest: 0.10373
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.64741748, Validation loss: 0.13400256, Gradient norm: 4.42150369
INFO:root:[    2] Training loss: 0.12615601, Validation loss: 0.09004934, Gradient norm: 3.64311493
INFO:root:[    3] Training loss: 0.10137530, Validation loss: 0.06547340, Gradient norm: 4.08453030
INFO:root:[    4] Training loss: 0.08732488, Validation loss: 0.05551263, Gradient norm: 3.66395153
INFO:root:[    5] Training loss: 0.07673439, Validation loss: 0.05497486, Gradient norm: 3.37041037
INFO:root:[    6] Training loss: 0.06986104, Validation loss: 0.04222970, Gradient norm: 2.99241484
INFO:root:[    7] Training loss: 0.06128772, Validation loss: 0.04565384, Gradient norm: 2.28885491
INFO:root:[    8] Training loss: 0.05954388, Validation loss: 0.04191548, Gradient norm: 2.58015130
INFO:root:[    9] Training loss: 0.05836169, Validation loss: 0.03659345, Gradient norm: 2.59249375
INFO:root:[   10] Training loss: 0.05676153, Validation loss: 0.03575715, Gradient norm: 2.72286615
INFO:root:[   11] Training loss: 0.05493262, Validation loss: 0.05601059, Gradient norm: 2.61385147
INFO:root:[   12] Training loss: 0.05145549, Validation loss: 0.04740012, Gradient norm: 2.21182879
INFO:root:[   13] Training loss: 0.05315898, Validation loss: 0.04137194, Gradient norm: 2.17877255
INFO:root:[   14] Training loss: 0.04849339, Validation loss: 0.03125657, Gradient norm: 1.99378157
INFO:root:[   15] Training loss: 0.04888197, Validation loss: 0.04429588, Gradient norm: 2.41009465
INFO:root:[   16] Training loss: 0.04744291, Validation loss: 0.03422583, Gradient norm: 2.23021636
INFO:root:[   17] Training loss: 0.04911310, Validation loss: 0.03681691, Gradient norm: 2.48875559
INFO:root:[   18] Training loss: 0.04667819, Validation loss: 0.02806963, Gradient norm: 2.27007265
INFO:root:[   19] Training loss: 0.04467579, Validation loss: 0.05134727, Gradient norm: 2.12368186
INFO:root:[   20] Training loss: 0.04430916, Validation loss: 0.03410261, Gradient norm: 2.28236483
INFO:root:[   21] Training loss: 0.04398353, Validation loss: 0.02524418, Gradient norm: 2.27618100
INFO:root:[   22] Training loss: 0.04173012, Validation loss: 0.02789090, Gradient norm: 2.07479983
INFO:root:[   23] Training loss: 0.04297021, Validation loss: 0.04886753, Gradient norm: 2.18062495
INFO:root:[   24] Training loss: 0.04294523, Validation loss: 0.02638691, Gradient norm: 2.15981014
INFO:root:[   25] Training loss: 0.04319953, Validation loss: 0.04796271, Gradient norm: 2.30489391
INFO:root:[   26] Training loss: 0.04164909, Validation loss: 0.02118907, Gradient norm: 2.25864818
INFO:root:[   27] Training loss: 0.04078028, Validation loss: 0.02670795, Gradient norm: 2.21825159
INFO:root:[   28] Training loss: 0.04015251, Validation loss: 0.03274024, Gradient norm: 2.20914285
INFO:root:[   29] Training loss: 0.04100259, Validation loss: 0.02959106, Gradient norm: 2.21842862
INFO:root:[   30] Training loss: 0.04061148, Validation loss: 0.03784451, Gradient norm: 2.11009884
INFO:root:[   31] Training loss: 0.03885084, Validation loss: 0.02021173, Gradient norm: 2.02400320
INFO:root:[   32] Training loss: 0.03860072, Validation loss: 0.04317438, Gradient norm: 2.13949901
INFO:root:[   33] Training loss: 0.03787491, Validation loss: 0.02442269, Gradient norm: 2.10607811
INFO:root:[   34] Training loss: 0.03696273, Validation loss: 0.02085335, Gradient norm: 2.01109851
INFO:root:[   35] Training loss: 0.03813394, Validation loss: 0.02938113, Gradient norm: 2.25366565
INFO:root:[   36] Training loss: 0.03762476, Validation loss: 0.03312184, Gradient norm: 2.12786016
INFO:root:[   37] Training loss: 0.03782434, Validation loss: 0.02046423, Gradient norm: 2.06951040
INFO:root:[   38] Training loss: 0.03787837, Validation loss: 0.03893077, Gradient norm: 2.13720461
INFO:root:[   39] Training loss: 0.03829490, Validation loss: 0.03536437, Gradient norm: 2.03126914
INFO:root:[   40] Training loss: 0.03699884, Validation loss: 0.02029181, Gradient norm: 1.90954671
INFO:root:[   41] Training loss: 0.03614954, Validation loss: 0.04126189, Gradient norm: 2.16048103
INFO:root:[   42] Training loss: 0.03729504, Validation loss: 0.02575401, Gradient norm: 2.30209524
INFO:root:[   43] Training loss: 0.03588457, Validation loss: 0.02077002, Gradient norm: 1.99230555
INFO:root:[   44] Training loss: 0.03508065, Validation loss: 0.04050798, Gradient norm: 2.05869725
INFO:root:[   45] Training loss: 0.03570689, Validation loss: 0.01751113, Gradient norm: 1.93407890
INFO:root:[   46] Training loss: 0.03497061, Validation loss: 0.04233242, Gradient norm: 2.02978177
INFO:root:[   47] Training loss: 0.03509150, Validation loss: 0.02032177, Gradient norm: 2.09372426
INFO:root:[   48] Training loss: 0.03530752, Validation loss: 0.03115720, Gradient norm: 2.09699650
INFO:root:[   49] Training loss: 0.03554991, Validation loss: 0.04405747, Gradient norm: 2.14025570
INFO:root:[   50] Training loss: 0.03414209, Validation loss: 0.01608639, Gradient norm: 2.03864576
INFO:root:[   51] Training loss: 0.03343858, Validation loss: 0.03515577, Gradient norm: 2.00506104
INFO:root:[   52] Training loss: 0.03408998, Validation loss: 0.02062049, Gradient norm: 2.08850995
INFO:root:[   53] Training loss: 0.03426224, Validation loss: 0.01630802, Gradient norm: 2.08823526
INFO:root:[   54] Training loss: 0.03520050, Validation loss: 0.02101374, Gradient norm: 2.08227991
INFO:root:[   55] Training loss: 0.03395051, Validation loss: 0.03891337, Gradient norm: 1.95754589
INFO:root:[   56] Training loss: 0.03336378, Validation loss: 0.01972906, Gradient norm: 2.03010249
INFO:root:[   57] Training loss: 0.03252564, Validation loss: 0.02828485, Gradient norm: 1.96965941
INFO:root:[   58] Training loss: 0.03280934, Validation loss: 0.03514217, Gradient norm: 2.01951648
INFO:root:[   59] Training loss: 0.03317096, Validation loss: 0.01942179, Gradient norm: 2.01531437
INFO:root:[   60] Training loss: 0.03330422, Validation loss: 0.03732408, Gradient norm: 1.98933250
INFO:root:[   61] Training loss: 0.03299729, Validation loss: 0.03416166, Gradient norm: 2.02387577
INFO:root:[   62] Training loss: 0.03283419, Validation loss: 0.01424282, Gradient norm: 2.02585846
INFO:root:[   63] Training loss: 0.03229057, Validation loss: 0.04035187, Gradient norm: 1.87511215
INFO:root:[   64] Training loss: 0.03202489, Validation loss: 0.01541982, Gradient norm: 1.88492715
INFO:root:[   65] Training loss: 0.03248349, Validation loss: 0.02744737, Gradient norm: 2.09701520
INFO:root:[   66] Training loss: 0.03132268, Validation loss: 0.01645839, Gradient norm: 1.98548070
INFO:root:[   67] Training loss: 0.03170970, Validation loss: 0.03398064, Gradient norm: 1.99764295
INFO:root:[   68] Training loss: 0.03187704, Validation loss: 0.03256703, Gradient norm: 2.03744509
INFO:root:[   69] Training loss: 0.03133837, Validation loss: 0.01536780, Gradient norm: 1.99645046
INFO:root:[   70] Training loss: 0.03199789, Validation loss: 0.03106288, Gradient norm: 2.05528129
INFO:root:[   71] Training loss: 0.03118421, Validation loss: 0.02352802, Gradient norm: 1.99765861
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 1083.115s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01436
INFO:root:EnergyScoreTrain: 0.01432
INFO:root:CoverageTrain: 0.85058
INFO:root:IntervalWidthTrain: 0.04167
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01421
INFO:root:EnergyScoreValidation: 0.01423
INFO:root:CoverageValidation: 0.85018
INFO:root:IntervalWidthValidation: 0.04164
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01421
INFO:root:EnergyScoreTest: 0.01419
INFO:root:CoverageTest: 0.85642
INFO:root:IntervalWidthTest: 0.0418
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.71776217, Validation loss: 0.17662143, Gradient norm: 3.19276696
INFO:root:[    2] Training loss: 0.16126063, Validation loss: 0.09862447, Gradient norm: 1.76056073
INFO:root:[    3] Training loss: 0.12078885, Validation loss: 0.13775786, Gradient norm: 1.63433829
INFO:root:[    4] Training loss: 0.10950512, Validation loss: 0.13098963, Gradient norm: 2.19452164
INFO:root:[    5] Training loss: 0.09615539, Validation loss: 0.05838829, Gradient norm: 1.64746225
INFO:root:[    6] Training loss: 0.08748779, Validation loss: 0.05157479, Gradient norm: 1.43130890
INFO:root:[    7] Training loss: 0.08655928, Validation loss: 0.13315257, Gradient norm: 1.78344729
INFO:root:[    8] Training loss: 0.08433175, Validation loss: 0.07391042, Gradient norm: 1.83432575
INFO:root:[    9] Training loss: 0.08132828, Validation loss: 0.05337613, Gradient norm: 1.77137424
INFO:root:[   10] Training loss: 0.07910646, Validation loss: 0.11736327, Gradient norm: 1.65423007
INFO:root:[   11] Training loss: 0.07769428, Validation loss: 0.08806876, Gradient norm: 1.71661610
INFO:root:[   12] Training loss: 0.07597968, Validation loss: 0.07517199, Gradient norm: 1.65571318
INFO:root:[   13] Training loss: 0.07449334, Validation loss: 0.11953179, Gradient norm: 1.67666977
INFO:root:[   14] Training loss: 0.07453896, Validation loss: 0.07383963, Gradient norm: 1.79786494
INFO:root:[   15] Training loss: 0.07188377, Validation loss: 0.13402017, Gradient norm: 1.59498267
INFO:root:[   16] Training loss: 0.07064008, Validation loss: 0.06690749, Gradient norm: 1.70495317
INFO:root:[   17] Training loss: 0.06949259, Validation loss: 0.13164690, Gradient norm: 1.70790798
INFO:root:[   18] Training loss: 0.06899624, Validation loss: 0.05646195, Gradient norm: 1.76998628
INFO:root:[   19] Training loss: 0.06796744, Validation loss: 0.09723575, Gradient norm: 1.77445281
INFO:root:[   20] Training loss: 0.06729369, Validation loss: 0.10732655, Gradient norm: 1.80862511
INFO:root:[   21] Training loss: 0.06434718, Validation loss: 0.11069044, Gradient norm: 1.59406555
INFO:root:[   22] Training loss: 0.06433694, Validation loss: 0.08015741, Gradient norm: 1.64993765
INFO:root:[   23] Training loss: 0.06521158, Validation loss: 0.09636621, Gradient norm: 1.81268275
INFO:root:[   24] Training loss: 0.06401474, Validation loss: 0.09307816, Gradient norm: 1.74151220
INFO:root:[   25] Training loss: 0.06171236, Validation loss: 0.05299137, Gradient norm: 1.70005169
INFO:root:[   26] Training loss: 0.06105595, Validation loss: 0.06373074, Gradient norm: 1.64225490
INFO:root:[   27] Training loss: 0.06134715, Validation loss: 0.07058930, Gradient norm: 1.69937533
INFO:root:[   28] Training loss: 0.06072343, Validation loss: 0.12657193, Gradient norm: 1.73358307
INFO:root:[   29] Training loss: 0.05927323, Validation loss: 0.12322416, Gradient norm: 1.66994504
INFO:root:[   30] Training loss: 0.05926273, Validation loss: 0.09896290, Gradient norm: 1.77761470
INFO:root:[   31] Training loss: 0.05787101, Validation loss: 0.10335003, Gradient norm: 1.74360327
INFO:root:[   32] Training loss: 0.05723858, Validation loss: 0.09520945, Gradient norm: 1.74357611
INFO:root:[   33] Training loss: 0.05732477, Validation loss: 0.07601242, Gradient norm: 1.79239546
INFO:root:[   34] Training loss: 0.05690264, Validation loss: 0.07704759, Gradient norm: 1.64329103
INFO:root:[   35] Training loss: 0.05573423, Validation loss: 0.09050123, Gradient norm: 1.77480947
INFO:root:[   36] Training loss: 0.05496869, Validation loss: 0.08630714, Gradient norm: 1.78161493
INFO:root:[   37] Training loss: 0.05517808, Validation loss: 0.06069967, Gradient norm: 1.87545653
INFO:root:[   38] Training loss: 0.05481491, Validation loss: 0.04803706, Gradient norm: 1.87232140
INFO:root:[   39] Training loss: 0.05435650, Validation loss: 0.07998393, Gradient norm: 1.89261114
INFO:root:[   40] Training loss: 0.05318893, Validation loss: 0.08892450, Gradient norm: 1.80086428
INFO:root:[   41] Training loss: 0.05238274, Validation loss: 0.07258197, Gradient norm: 1.76644956
INFO:root:[   42] Training loss: 0.05295684, Validation loss: 0.09538727, Gradient norm: 1.85971116
INFO:root:[   43] Training loss: 0.05243076, Validation loss: 0.12486802, Gradient norm: 1.88450054
INFO:root:[   44] Training loss: 0.05148066, Validation loss: 0.12203719, Gradient norm: 1.80142473
INFO:root:[   45] Training loss: 0.05117343, Validation loss: 0.12234481, Gradient norm: 1.77100714
INFO:root:[   46] Training loss: 0.05151282, Validation loss: 0.11590464, Gradient norm: 1.82916958
INFO:root:[   47] Training loss: 0.05094039, Validation loss: 0.07861283, Gradient norm: 1.88569945
INFO:root:[   48] Training loss: 0.05020274, Validation loss: 0.05548014, Gradient norm: 1.88706386
INFO:root:[   49] Training loss: 0.04955056, Validation loss: 0.05292898, Gradient norm: 1.85048435
INFO:root:[   50] Training loss: 0.04940607, Validation loss: 0.04780250, Gradient norm: 1.85611250
INFO:root:[   51] Training loss: 0.04959009, Validation loss: 0.07118961, Gradient norm: 1.90773941
INFO:root:[   52] Training loss: 0.04862088, Validation loss: 0.08244247, Gradient norm: 1.83640419
INFO:root:[   53] Training loss: 0.04816691, Validation loss: 0.08460502, Gradient norm: 1.82259883
INFO:root:[   54] Training loss: 0.04786928, Validation loss: 0.09795820, Gradient norm: 1.86908662
INFO:root:[   55] Training loss: 0.04779868, Validation loss: 0.11728036, Gradient norm: 1.89268717
INFO:root:[   56] Training loss: 0.04718344, Validation loss: 0.11797703, Gradient norm: 1.83823386
INFO:root:[   57] Training loss: 0.04661708, Validation loss: 0.10204114, Gradient norm: 1.80922047
INFO:root:[   58] Training loss: 0.04709308, Validation loss: 0.11935662, Gradient norm: 1.96075245
INFO:root:[   59] Training loss: 0.04619076, Validation loss: 0.11675913, Gradient norm: 1.83043214
INFO:root:[   60] Training loss: 0.04589410, Validation loss: 0.11837277, Gradient norm: 1.84803547
INFO:root:[   61] Training loss: 0.04586262, Validation loss: 0.11650841, Gradient norm: 1.81812794
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 931.921s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06791
INFO:root:EnergyScoreTrain: 0.04776
INFO:root:CoverageTrain: 0.8109
INFO:root:IntervalWidthTrain: 0.0724
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06795
INFO:root:EnergyScoreValidation: 0.04779
INFO:root:CoverageValidation: 0.80967
INFO:root:IntervalWidthValidation: 0.0724
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06789
INFO:root:EnergyScoreTest: 0.04773
INFO:root:CoverageTest: 0.81666
INFO:root:IntervalWidthTest: 0.07249
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.74288751, Validation loss: 0.13419570, Gradient norm: 2.57067704
INFO:root:[    2] Training loss: 0.17381891, Validation loss: 0.10352834, Gradient norm: 1.12490092
INFO:root:[    3] Training loss: 0.13534935, Validation loss: 0.13857414, Gradient norm: 1.11883222
INFO:root:[    4] Training loss: 0.12275403, Validation loss: 0.14783294, Gradient norm: 1.64451566
INFO:root:[    5] Training loss: 0.11303911, Validation loss: 0.14822198, Gradient norm: 1.55270327
INFO:root:[    6] Training loss: 0.10911549, Validation loss: 0.13392177, Gradient norm: 1.65662281
INFO:root:[    7] Training loss: 0.10234339, Validation loss: 0.14341479, Gradient norm: 1.39307191
INFO:root:[    8] Training loss: 0.10135986, Validation loss: 0.16297459, Gradient norm: 1.70989456
INFO:root:[    9] Training loss: 0.09699830, Validation loss: 0.19176431, Gradient norm: 1.41577703
INFO:root:[   10] Training loss: 0.09366672, Validation loss: 0.14120855, Gradient norm: 1.28660950
INFO:root:[   11] Training loss: 0.09266470, Validation loss: 0.14338754, Gradient norm: 1.41274119
INFO:root:[   12] Training loss: 0.09076556, Validation loss: 0.22709002, Gradient norm: 1.41088518
INFO:root:[   13] Training loss: 0.09121073, Validation loss: 0.21769928, Gradient norm: 1.56800271
INFO:root:[   14] Training loss: 0.08862917, Validation loss: 0.21402826, Gradient norm: 1.50310389
INFO:root:[   15] Training loss: 0.08742506, Validation loss: 0.24135104, Gradient norm: 1.53774115
INFO:root:[   16] Training loss: 0.08691885, Validation loss: 0.22209118, Gradient norm: 1.55370123
INFO:root:[   17] Training loss: 0.08469992, Validation loss: 0.18914305, Gradient norm: 1.47803765
INFO:root:[   18] Training loss: 0.08384295, Validation loss: 0.14618455, Gradient norm: 1.42240831
INFO:root:[   19] Training loss: 0.08280954, Validation loss: 0.13023393, Gradient norm: 1.54852734
INFO:root:[   20] Training loss: 0.08246411, Validation loss: 0.13347701, Gradient norm: 1.64403194
INFO:root:[   21] Training loss: 0.08195409, Validation loss: 0.13364062, Gradient norm: 1.60157849
INFO:root:[   22] Training loss: 0.08091114, Validation loss: 0.14064596, Gradient norm: 1.50475784
INFO:root:[   23] Training loss: 0.07915978, Validation loss: 0.18055697, Gradient norm: 1.59569154
INFO:root:[   24] Training loss: 0.07741119, Validation loss: 0.21010629, Gradient norm: 1.49107177
INFO:root:[   25] Training loss: 0.07636442, Validation loss: 0.13279208, Gradient norm: 1.50135901
INFO:root:[   26] Training loss: 0.07556243, Validation loss: 0.16810411, Gradient norm: 1.51369441
INFO:root:[   27] Training loss: 0.07461158, Validation loss: 0.20181246, Gradient norm: 1.53515511
INFO:root:[   28] Training loss: 0.07369597, Validation loss: 0.12228336, Gradient norm: 1.50416350
INFO:root:[   29] Training loss: 0.07514322, Validation loss: 0.13137499, Gradient norm: 1.70002218
INFO:root:[   30] Training loss: 0.07414533, Validation loss: 0.15503907, Gradient norm: 1.63408127
INFO:root:[   31] Training loss: 0.07337065, Validation loss: 0.19052108, Gradient norm: 1.51844369
INFO:root:[   32] Training loss: 0.07085425, Validation loss: 0.16167780, Gradient norm: 1.53864684
INFO:root:[   33] Training loss: 0.06965660, Validation loss: 0.13441281, Gradient norm: 1.51328951
INFO:root:[   34] Training loss: 0.06891897, Validation loss: 0.16906807, Gradient norm: 1.48570431
INFO:root:[   35] Training loss: 0.06811084, Validation loss: 0.19367673, Gradient norm: 1.36757677
INFO:root:[   36] Training loss: 0.06709288, Validation loss: 0.12141069, Gradient norm: 1.47397424
INFO:root:[   37] Training loss: 0.06688908, Validation loss: 0.15290261, Gradient norm: 1.56350661
INFO:root:[   38] Training loss: 0.06690409, Validation loss: 0.14434862, Gradient norm: 1.64613775
INFO:root:[   39] Training loss: 0.06641012, Validation loss: 0.17426278, Gradient norm: 1.61939499
INFO:root:[   40] Training loss: 0.06511461, Validation loss: 0.13879384, Gradient norm: 1.62760113
INFO:root:[   41] Training loss: 0.06416336, Validation loss: 0.15594089, Gradient norm: 1.62654473
INFO:root:[   42] Training loss: 0.06361001, Validation loss: 0.15318658, Gradient norm: 1.63506807
INFO:root:[   43] Training loss: 0.06288818, Validation loss: 0.11563439, Gradient norm: 1.58320815
INFO:root:[   44] Training loss: 0.06262178, Validation loss: 0.16987792, Gradient norm: 1.67286724
INFO:root:[   45] Training loss: 0.06221209, Validation loss: 0.13428183, Gradient norm: 1.63451700
INFO:root:[   46] Training loss: 0.06098697, Validation loss: 0.18545637, Gradient norm: 1.55247649
INFO:root:[   47] Training loss: 0.06093327, Validation loss: 0.10636040, Gradient norm: 1.65923311
INFO:root:[   48] Training loss: 0.06077501, Validation loss: 0.16809205, Gradient norm: 1.65561663
INFO:root:[   49] Training loss: 0.05877993, Validation loss: 0.10533509, Gradient norm: 1.57525607
INFO:root:[   50] Training loss: 0.05909859, Validation loss: 0.17556209, Gradient norm: 1.64664417
INFO:root:[   51] Training loss: 0.05869363, Validation loss: 0.10472047, Gradient norm: 1.67223934
INFO:root:[   52] Training loss: 0.05732498, Validation loss: 0.17415310, Gradient norm: 1.58252895
INFO:root:[   53] Training loss: 0.05617248, Validation loss: 0.15206110, Gradient norm: 1.58861587
INFO:root:[   54] Training loss: 0.05536802, Validation loss: 0.10093226, Gradient norm: 1.54014452
INFO:root:[   55] Training loss: 0.05662620, Validation loss: 0.16403274, Gradient norm: 1.74314520
INFO:root:[   56] Training loss: 0.05656133, Validation loss: 0.09626584, Gradient norm: 1.59229206
INFO:root:[   57] Training loss: 0.05541104, Validation loss: 0.13469060, Gradient norm: 1.61619572
INFO:root:[   58] Training loss: 0.05457592, Validation loss: 0.12618659, Gradient norm: 1.69833785
INFO:root:[   59] Training loss: 0.05428749, Validation loss: 0.16114974, Gradient norm: 1.78593395
INFO:root:[   60] Training loss: 0.05292854, Validation loss: 0.11756596, Gradient norm: 1.65436744
INFO:root:[   61] Training loss: 0.05273283, Validation loss: 0.16659086, Gradient norm: 1.74443106
INFO:root:[   62] Training loss: 0.05220859, Validation loss: 0.09695733, Gradient norm: 1.67451198
INFO:root:[   63] Training loss: 0.05165063, Validation loss: 0.14433269, Gradient norm: 1.59447960
INFO:root:[   64] Training loss: 0.05085224, Validation loss: 0.11821734, Gradient norm: 1.64314554
INFO:root:[   65] Training loss: 0.05112408, Validation loss: 0.14229919, Gradient norm: 1.70717225
INFO:root:[   66] Training loss: 0.05051406, Validation loss: 0.09036899, Gradient norm: 1.73619883
INFO:root:[   67] Training loss: 0.04916677, Validation loss: 0.16101484, Gradient norm: 1.64948126
INFO:root:[   68] Training loss: 0.04890218, Validation loss: 0.09119658, Gradient norm: 1.66959571
INFO:root:[   69] Training loss: 0.04816396, Validation loss: 0.15858973, Gradient norm: 1.57429805
INFO:root:[   70] Training loss: 0.04801023, Validation loss: 0.07870272, Gradient norm: 1.63892222
INFO:root:[   71] Training loss: 0.04764596, Validation loss: 0.15192868, Gradient norm: 1.66718423
INFO:root:[   72] Training loss: 0.04790272, Validation loss: 0.10319587, Gradient norm: 1.79710113
INFO:root:[   73] Training loss: 0.04702227, Validation loss: 0.12352275, Gradient norm: 1.63573864
INFO:root:[   74] Training loss: 0.04577907, Validation loss: 0.08508506, Gradient norm: 1.63232338
INFO:root:[   75] Training loss: 0.04544459, Validation loss: 0.14465591, Gradient norm: 1.61991931
INFO:root:[   76] Training loss: 0.04509637, Validation loss: 0.08108714, Gradient norm: 1.67210965
INFO:root:[   77] Training loss: 0.04468453, Validation loss: 0.14239953, Gradient norm: 1.68145759
INFO:root:[   78] Training loss: 0.04455129, Validation loss: 0.10872494, Gradient norm: 1.79216420
INFO:root:[   79] Training loss: 0.04429674, Validation loss: 0.07258266, Gradient norm: 1.67947904
INFO:root:[   80] Training loss: 0.04349474, Validation loss: 0.14323211, Gradient norm: 1.62215379
INFO:root:[   81] Training loss: 0.04306409, Validation loss: 0.07030275, Gradient norm: 1.58878434
INFO:root:[   82] Training loss: 0.04362577, Validation loss: 0.12528354, Gradient norm: 1.68319240
INFO:root:[   83] Training loss: 0.04349816, Validation loss: 0.07768277, Gradient norm: 1.58569478
INFO:root:[   84] Training loss: 0.04152168, Validation loss: 0.10646547, Gradient norm: 1.67569855
INFO:root:[   85] Training loss: 0.04132968, Validation loss: 0.11331876, Gradient norm: 1.63222751
INFO:root:[   86] Training loss: 0.04044845, Validation loss: 0.06657430, Gradient norm: 1.65050471
INFO:root:[   87] Training loss: 0.04039861, Validation loss: 0.13201926, Gradient norm: 1.64692420
INFO:root:[   88] Training loss: 0.03945625, Validation loss: 0.06826735, Gradient norm: 1.54273618
INFO:root:[   89] Training loss: 0.03960524, Validation loss: 0.10211465, Gradient norm: 1.58798058
INFO:root:[   90] Training loss: 0.03893357, Validation loss: 0.08254579, Gradient norm: 1.64092229
INFO:root:[   91] Training loss: 0.03890926, Validation loss: 0.12290770, Gradient norm: 1.69433183
INFO:root:[   92] Training loss: 0.03802442, Validation loss: 0.07188598, Gradient norm: 1.53147772
INFO:root:[   93] Training loss: 0.03774345, Validation loss: 0.11952537, Gradient norm: 1.62385987
INFO:root:[   94] Training loss: 0.03742990, Validation loss: 0.06187971, Gradient norm: 1.61711485
INFO:root:[   95] Training loss: 0.03682728, Validation loss: 0.11917859, Gradient norm: 1.59204370
INFO:root:[   96] Training loss: 0.03662855, Validation loss: 0.05523297, Gradient norm: 1.64278168
INFO:root:[   97] Training loss: 0.03633724, Validation loss: 0.11967608, Gradient norm: 1.61477491
INFO:root:[   98] Training loss: 0.03579081, Validation loss: 0.05647585, Gradient norm: 1.58220173
INFO:root:[   99] Training loss: 0.03582142, Validation loss: 0.11975434, Gradient norm: 1.59914501
INFO:root:[  100] Training loss: 0.03542831, Validation loss: 0.05506907, Gradient norm: 1.61336126
INFO:root:[  101] Training loss: 0.03490233, Validation loss: 0.10736186, Gradient norm: 1.60807492
INFO:root:[  102] Training loss: 0.03466522, Validation loss: 0.08575103, Gradient norm: 1.67552058
INFO:root:[  103] Training loss: 0.03430663, Validation loss: 0.06389409, Gradient norm: 1.59538891
INFO:root:[  104] Training loss: 0.03430943, Validation loss: 0.10168465, Gradient norm: 1.54522156
INFO:root:[  105] Training loss: 0.03413688, Validation loss: 0.04997333, Gradient norm: 1.57603463
INFO:root:[  106] Training loss: 0.03332014, Validation loss: 0.10446376, Gradient norm: 1.55779345
INFO:root:[  107] Training loss: 0.03345965, Validation loss: 0.10604631, Gradient norm: 1.65717391
INFO:root:[  108] Training loss: 0.03232733, Validation loss: 0.04787975, Gradient norm: 1.52453084
INFO:root:[  109] Training loss: 0.03272896, Validation loss: 0.06943869, Gradient norm: 1.64561318
INFO:root:[  110] Training loss: 0.03207550, Validation loss: 0.10075173, Gradient norm: 1.59158026
INFO:root:[  111] Training loss: 0.03144351, Validation loss: 0.04867115, Gradient norm: 1.53045347
INFO:root:[  112] Training loss: 0.03182845, Validation loss: 0.05017407, Gradient norm: 1.57538704
INFO:root:[  113] Training loss: 0.03068623, Validation loss: 0.09639703, Gradient norm: 1.53867323
INFO:root:[  114] Training loss: 0.03069886, Validation loss: 0.05297212, Gradient norm: 1.56047221
INFO:root:[  115] Training loss: 0.02990886, Validation loss: 0.07926126, Gradient norm: 1.50338602
INFO:root:[  116] Training loss: 0.02959045, Validation loss: 0.06094745, Gradient norm: 1.47727276
INFO:root:[  117] Training loss: 0.02923818, Validation loss: 0.08250092, Gradient norm: 1.44068416
INFO:root:EP 117: Early stopping
INFO:root:Training the model took 1784.075s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0651
INFO:root:EnergyScoreTrain: 0.04788
INFO:root:CoverageTrain: 0.71288
INFO:root:IntervalWidthTrain: 0.04682
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0651
INFO:root:EnergyScoreValidation: 0.04788
INFO:root:CoverageValidation: 0.71527
INFO:root:IntervalWidthValidation: 0.04681
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06507
INFO:root:EnergyScoreTest: 0.04792
INFO:root:CoverageTest: 0.70713
INFO:root:IntervalWidthTest: 0.04656
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.66583059, Validation loss: 0.13246121, Gradient norm: 3.09335314
INFO:root:[    2] Training loss: 0.18933327, Validation loss: 0.16039522, Gradient norm: 1.67010702
INFO:root:[    3] Training loss: 0.14844311, Validation loss: 0.15623081, Gradient norm: 1.32762299
INFO:root:[    4] Training loss: 0.13145886, Validation loss: 0.29510650, Gradient norm: 1.33783399
INFO:root:[    5] Training loss: 0.12271374, Validation loss: 0.30517620, Gradient norm: 1.35438909
INFO:root:[    6] Training loss: 0.11686373, Validation loss: 0.20351413, Gradient norm: 1.44899727
INFO:root:[    7] Training loss: 0.11102589, Validation loss: 0.26698562, Gradient norm: 1.29124176
INFO:root:[    8] Training loss: 0.10573457, Validation loss: 0.31450125, Gradient norm: 1.14953981
INFO:root:[    9] Training loss: 0.10422112, Validation loss: 0.26720980, Gradient norm: 1.43439418
INFO:root:[   10] Training loss: 0.10279650, Validation loss: 0.23546998, Gradient norm: 1.52032523
INFO:root:[   11] Training loss: 0.10025327, Validation loss: 0.24440265, Gradient norm: 1.49239863
INFO:root:[   12] Training loss: 0.09798366, Validation loss: 0.29505747, Gradient norm: 1.50807963
INFO:root:[   13] Training loss: 0.09524846, Validation loss: 0.33520941, Gradient norm: 1.36273130
INFO:root:[   14] Training loss: 0.09420024, Validation loss: 0.34604856, Gradient norm: 1.51120638
INFO:root:[   15] Training loss: 0.09583535, Validation loss: 0.32360405, Gradient norm: 1.87852832
INFO:root:[   16] Training loss: 0.09436422, Validation loss: 0.24530751, Gradient norm: 1.81915677
INFO:root:[   17] Training loss: 0.09059119, Validation loss: 0.28204594, Gradient norm: 1.51125824
INFO:root:[   18] Training loss: 0.08908268, Validation loss: 0.26952931, Gradient norm: 1.42004093
INFO:root:[   19] Training loss: 0.08839493, Validation loss: 0.28604521, Gradient norm: 1.57474625
INFO:root:[   20] Training loss: 0.08608392, Validation loss: 0.24559135, Gradient norm: 1.45964422
INFO:root:[   21] Training loss: 0.08634135, Validation loss: 0.29964991, Gradient norm: 1.66375174
INFO:root:[   22] Training loss: 0.08359285, Validation loss: 0.22751047, Gradient norm: 1.41828715
INFO:root:[   23] Training loss: 0.08226335, Validation loss: 0.23774796, Gradient norm: 1.55465030
INFO:root:[   24] Training loss: 0.08061181, Validation loss: 0.30518116, Gradient norm: 1.46568712
INFO:root:[   25] Training loss: 0.08029621, Validation loss: 0.32086711, Gradient norm: 1.61483136
INFO:root:[   26] Training loss: 0.07946717, Validation loss: 0.30883206, Gradient norm: 1.66265472
INFO:root:[   27] Training loss: 0.07766930, Validation loss: 0.24885563, Gradient norm: 1.56700241
INFO:root:[   28] Training loss: 0.07669758, Validation loss: 0.21705310, Gradient norm: 1.60437680
INFO:root:[   29] Training loss: 0.07546720, Validation loss: 0.25178471, Gradient norm: 1.60447499
INFO:root:[   30] Training loss: 0.07394632, Validation loss: 0.29652782, Gradient norm: 1.56157711
INFO:root:[   31] Training loss: 0.07291566, Validation loss: 0.29330537, Gradient norm: 1.60072317
INFO:root:[   32] Training loss: 0.07096607, Validation loss: 0.21576663, Gradient norm: 1.50576847
INFO:root:[   33] Training loss: 0.07024291, Validation loss: 0.29248001, Gradient norm: 1.57699941
INFO:root:[   34] Training loss: 0.06933699, Validation loss: 0.24412549, Gradient norm: 1.66466718
INFO:root:[   35] Training loss: 0.06822565, Validation loss: 0.20902828, Gradient norm: 1.60387994
INFO:root:[   36] Training loss: 0.06657157, Validation loss: 0.28530370, Gradient norm: 1.55467333
INFO:root:[   37] Training loss: 0.06595526, Validation loss: 0.22970832, Gradient norm: 1.66285042
INFO:root:[   38] Training loss: 0.06460292, Validation loss: 0.22245630, Gradient norm: 1.60101557
INFO:root:[   39] Training loss: 0.06379009, Validation loss: 0.25789894, Gradient norm: 1.60525606
INFO:root:[   40] Training loss: 0.06275435, Validation loss: 0.19709450, Gradient norm: 1.58221270
INFO:root:[   41] Training loss: 0.06104417, Validation loss: 0.21593359, Gradient norm: 1.53459032
INFO:root:[   42] Training loss: 0.06086606, Validation loss: 0.21372947, Gradient norm: 1.67016267
INFO:root:[   43] Training loss: 0.05916490, Validation loss: 0.18708058, Gradient norm: 1.47957193
INFO:root:[   44] Training loss: 0.05902355, Validation loss: 0.24884466, Gradient norm: 1.68456874
INFO:root:[   45] Training loss: 0.05850914, Validation loss: 0.20633628, Gradient norm: 1.72085219
INFO:root:[   46] Training loss: 0.05711969, Validation loss: 0.20410695, Gradient norm: 1.62808915
INFO:root:[   47] Training loss: 0.05614266, Validation loss: 0.22139197, Gradient norm: 1.64084942
INFO:root:[   48] Training loss: 0.05537237, Validation loss: 0.16705710, Gradient norm: 1.65112675
INFO:root:[   49] Training loss: 0.05445505, Validation loss: 0.23667521, Gradient norm: 1.62025607
INFO:root:[   50] Training loss: 0.05342001, Validation loss: 0.16658460, Gradient norm: 1.57920550
INFO:root:[   51] Training loss: 0.05280119, Validation loss: 0.21718703, Gradient norm: 1.64754354
INFO:root:[   52] Training loss: 0.05215820, Validation loss: 0.17154452, Gradient norm: 1.59462531
INFO:root:[   53] Training loss: 0.05112047, Validation loss: 0.15977752, Gradient norm: 1.50680821
INFO:root:[   54] Training loss: 0.05037857, Validation loss: 0.21822680, Gradient norm: 1.59038659
INFO:root:[   55] Training loss: 0.05017272, Validation loss: 0.14571698, Gradient norm: 1.67050336
INFO:root:[   56] Training loss: 0.04944193, Validation loss: 0.20514545, Gradient norm: 1.62528305
INFO:root:[   57] Training loss: 0.04888693, Validation loss: 0.14751560, Gradient norm: 1.65983320
INFO:root:[   58] Training loss: 0.04776702, Validation loss: 0.17785176, Gradient norm: 1.63257463
INFO:root:[   59] Training loss: 0.04697669, Validation loss: 0.14430518, Gradient norm: 1.57387177
INFO:root:[   60] Training loss: 0.04630141, Validation loss: 0.19026204, Gradient norm: 1.59508193
INFO:root:[   61] Training loss: 0.04584080, Validation loss: 0.15820225, Gradient norm: 1.65580104
INFO:root:[   62] Training loss: 0.04501190, Validation loss: 0.13449519, Gradient norm: 1.60821318
INFO:root:[   63] Training loss: 0.04436475, Validation loss: 0.18419405, Gradient norm: 1.55278940
INFO:root:[   64] Training loss: 0.04341653, Validation loss: 0.17141446, Gradient norm: 1.44780643
INFO:root:[   65] Training loss: 0.04272150, Validation loss: 0.11334910, Gradient norm: 1.50932004
INFO:root:[   66] Training loss: 0.04244104, Validation loss: 0.17961868, Gradient norm: 1.61434167
INFO:root:[   67] Training loss: 0.04198009, Validation loss: 0.11742424, Gradient norm: 1.63603949
INFO:root:[   68] Training loss: 0.04146880, Validation loss: 0.15667417, Gradient norm: 1.61463136
INFO:root:[   69] Training loss: 0.04079513, Validation loss: 0.13076218, Gradient norm: 1.59952650
INFO:root:[   70] Training loss: 0.03989014, Validation loss: 0.13228252, Gradient norm: 1.55417291
INFO:root:[   71] Training loss: 0.03925208, Validation loss: 0.11568086, Gradient norm: 1.50589668
INFO:root:[   72] Training loss: 0.03817392, Validation loss: 0.15757232, Gradient norm: 1.44126529
INFO:root:[   73] Training loss: 0.03838651, Validation loss: 0.09023866, Gradient norm: 1.56382743
INFO:root:[   74] Training loss: 0.03807015, Validation loss: 0.13570567, Gradient norm: 1.62896284
INFO:root:[   75] Training loss: 0.03740077, Validation loss: 0.15427888, Gradient norm: 1.60547371
INFO:root:[   76] Training loss: 0.03721311, Validation loss: 0.11890669, Gradient norm: 1.63040336
INFO:root:[   77] Training loss: 0.03669905, Validation loss: 0.09192747, Gradient norm: 1.48120812
INFO:root:[   78] Training loss: 0.03572256, Validation loss: 0.13270293, Gradient norm: 1.56808291
INFO:root:[   79] Training loss: 0.03524170, Validation loss: 0.12175284, Gradient norm: 1.49972050
INFO:root:[   80] Training loss: 0.03584799, Validation loss: 0.11749345, Gradient norm: 1.60195646
INFO:root:[   81] Training loss: 0.03447704, Validation loss: 0.08512099, Gradient norm: 1.50840365
INFO:root:[   82] Training loss: 0.03404489, Validation loss: 0.07080396, Gradient norm: 1.54355989
INFO:root:[   83] Training loss: 0.03358346, Validation loss: 0.09369840, Gradient norm: 1.54484596
INFO:root:[   84] Training loss: 0.03294650, Validation loss: 0.12332810, Gradient norm: 1.46821826
INFO:root:[   85] Training loss: 0.03227127, Validation loss: 0.12324883, Gradient norm: 1.47519503
INFO:root:[   86] Training loss: 0.03226168, Validation loss: 0.09418190, Gradient norm: 1.49292510
INFO:root:[   87] Training loss: 0.03152847, Validation loss: 0.08882583, Gradient norm: 1.49085527
INFO:root:[   88] Training loss: 0.03078767, Validation loss: 0.06815881, Gradient norm: 1.40983548
INFO:root:[   89] Training loss: 0.03105622, Validation loss: 0.05572162, Gradient norm: 1.46900281
INFO:root:[   90] Training loss: 0.03017842, Validation loss: 0.07881063, Gradient norm: 1.41232095
INFO:root:[   91] Training loss: 0.02939200, Validation loss: 0.10495860, Gradient norm: 1.36779413
INFO:root:[   92] Training loss: 0.03009179, Validation loss: 0.10809545, Gradient norm: 1.46905645
INFO:root:[   93] Training loss: 0.02901584, Validation loss: 0.05577275, Gradient norm: 1.27488903
INFO:root:[   94] Training loss: 0.02870441, Validation loss: 0.05318815, Gradient norm: 1.41002386
INFO:root:[   95] Training loss: 0.02852954, Validation loss: 0.05422825, Gradient norm: 1.44303084
INFO:root:[   96] Training loss: 0.02788847, Validation loss: 0.07392179, Gradient norm: 1.30038419
INFO:root:[   97] Training loss: 0.02736934, Validation loss: 0.08894305, Gradient norm: 1.34596230
INFO:root:[   98] Training loss: 0.02843143, Validation loss: 0.06702168, Gradient norm: 1.42123588
INFO:root:[   99] Training loss: 0.02770811, Validation loss: 0.04046855, Gradient norm: 1.38222508
INFO:root:[  100] Training loss: 0.02668887, Validation loss: 0.04178489, Gradient norm: 1.33713641
INFO:root:[  101] Training loss: 0.02632295, Validation loss: 0.04402031, Gradient norm: 1.31243471
INFO:root:[  102] Training loss: 0.02628454, Validation loss: 0.05876543, Gradient norm: 1.32007780
INFO:root:[  103] Training loss: 0.02556136, Validation loss: 0.04118640, Gradient norm: 1.20726897
INFO:root:[  104] Training loss: 0.02580629, Validation loss: 0.03373153, Gradient norm: 1.22452110
INFO:root:[  105] Training loss: 0.02600905, Validation loss: 0.06856044, Gradient norm: 1.39604897
INFO:root:[  106] Training loss: 0.02638711, Validation loss: 0.03918237, Gradient norm: 1.45042840
INFO:root:[  107] Training loss: 0.02470217, Validation loss: 0.05005390, Gradient norm: 1.18316788
INFO:root:[  108] Training loss: 0.02534356, Validation loss: 0.04087445, Gradient norm: 1.30222807
INFO:root:[  109] Training loss: 0.02525575, Validation loss: 0.06529427, Gradient norm: 1.33555920
INFO:root:[  110] Training loss: 0.02392686, Validation loss: 0.03181940, Gradient norm: 1.20055722
INFO:root:[  111] Training loss: 0.02375477, Validation loss: 0.03502476, Gradient norm: 1.08722773
INFO:root:[  112] Training loss: 0.02343223, Validation loss: 0.06465040, Gradient norm: 1.22833935
INFO:root:[  113] Training loss: 0.02304274, Validation loss: 0.05052909, Gradient norm: 1.04524535
INFO:root:[  114] Training loss: 0.02352090, Validation loss: 0.05410877, Gradient norm: 1.07082091
INFO:root:[  115] Training loss: 0.02360420, Validation loss: 0.06598275, Gradient norm: 1.16520785
INFO:root:[  116] Training loss: 0.02282435, Validation loss: 0.06247585, Gradient norm: 1.05647861
INFO:root:[  117] Training loss: 0.02443209, Validation loss: 0.05540239, Gradient norm: 1.29048492
INFO:root:[  118] Training loss: 0.02332632, Validation loss: 0.02635281, Gradient norm: 1.21222111
INFO:root:[  119] Training loss: 0.02289932, Validation loss: 0.06176079, Gradient norm: 1.03402399
INFO:root:[  120] Training loss: 0.02368020, Validation loss: 0.03140237, Gradient norm: 1.08194718
INFO:root:[  121] Training loss: 0.02322488, Validation loss: 0.04711828, Gradient norm: 1.21795155
INFO:root:[  122] Training loss: 0.02137582, Validation loss: 0.02774127, Gradient norm: 1.00356695
INFO:root:[  123] Training loss: 0.02268120, Validation loss: 0.03913230, Gradient norm: 0.98725210
INFO:root:[  124] Training loss: 0.02159403, Validation loss: 0.03554669, Gradient norm: 0.98873917
INFO:root:[  125] Training loss: 0.02286565, Validation loss: 0.06107893, Gradient norm: 1.04315293
INFO:root:[  126] Training loss: 0.02251744, Validation loss: 0.03290267, Gradient norm: 1.08174377
INFO:root:[  127] Training loss: 0.02243819, Validation loss: 0.04593690, Gradient norm: 1.08050174
INFO:root:[  128] Training loss: 0.02329684, Validation loss: 0.02270028, Gradient norm: 1.01654370
INFO:root:[  129] Training loss: 0.02339230, Validation loss: 0.04913203, Gradient norm: 1.08793495
INFO:root:[  130] Training loss: 0.02298965, Validation loss: 0.04763025, Gradient norm: 0.96699497
INFO:root:[  131] Training loss: 0.02144778, Validation loss: 0.02266563, Gradient norm: 0.97458054
INFO:root:[  132] Training loss: 0.02149236, Validation loss: 0.05712747, Gradient norm: 1.00259650
INFO:root:[  133] Training loss: 0.02149185, Validation loss: 0.03282586, Gradient norm: 0.99175551
INFO:root:[  134] Training loss: 0.02252594, Validation loss: 0.05441839, Gradient norm: 0.97398507
INFO:root:[  135] Training loss: 0.02113050, Validation loss: 0.02591094, Gradient norm: 1.00066179
INFO:root:[  136] Training loss: 0.02259994, Validation loss: 0.04319314, Gradient norm: 1.02666959
INFO:root:[  137] Training loss: 0.02190513, Validation loss: 0.04905996, Gradient norm: 1.00400530
INFO:root:[  138] Training loss: 0.02128347, Validation loss: 0.04595023, Gradient norm: 1.02808988
INFO:root:[  139] Training loss: 0.02202201, Validation loss: 0.03949540, Gradient norm: 1.04724344
INFO:root:[  140] Training loss: 0.02157238, Validation loss: 0.03055882, Gradient norm: 1.04806820
INFO:root:EP 140: Early stopping
INFO:root:Training the model took 2106.256s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.03044
INFO:root:EnergyScoreTrain: 0.03078
INFO:root:CoverageTrain: 0.86923
INFO:root:IntervalWidthTrain: 0.01784
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.03053
INFO:root:EnergyScoreValidation: 0.03174
INFO:root:CoverageValidation: 0.86745
INFO:root:IntervalWidthValidation: 0.01782
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02987
INFO:root:EnergyScoreTest: 0.02952
INFO:root:CoverageTest: 0.87453
INFO:root:IntervalWidthTest: 0.01768
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.67029547, Validation loss: 0.12288453, Gradient norm: 1.77917370
INFO:root:[    2] Training loss: 0.18469372, Validation loss: 0.10651268, Gradient norm: 1.19316169
INFO:root:[    3] Training loss: 0.16098875, Validation loss: 0.13741477, Gradient norm: 1.57019285
INFO:root:[    4] Training loss: 0.14074501, Validation loss: 0.14857078, Gradient norm: 0.71568607
INFO:root:[    5] Training loss: 0.13314991, Validation loss: 0.11714478, Gradient norm: 1.25194600
INFO:root:[    6] Training loss: 0.12753152, Validation loss: 0.21968816, Gradient norm: 1.22833573
INFO:root:[    7] Training loss: 0.12272485, Validation loss: 0.14022899, Gradient norm: 1.16920888
INFO:root:[    8] Training loss: 0.11899585, Validation loss: 0.20459845, Gradient norm: 1.26571024
INFO:root:[    9] Training loss: 0.11506048, Validation loss: 0.15500943, Gradient norm: 1.15297919
INFO:root:[   10] Training loss: 0.11209771, Validation loss: 0.23970743, Gradient norm: 1.09628714
INFO:root:[   11] Training loss: 0.11091008, Validation loss: 0.16260388, Gradient norm: 1.21705810
INFO:root:[   12] Training loss: 0.10809528, Validation loss: 0.21098691, Gradient norm: 1.31813944
INFO:root:[   13] Training loss: 0.10748717, Validation loss: 0.16207646, Gradient norm: 1.36122934
INFO:root:[   14] Training loss: 0.10440193, Validation loss: 0.23489161, Gradient norm: 1.21340099
INFO:root:[   15] Training loss: 0.10231163, Validation loss: 0.14578602, Gradient norm: 1.14654099
INFO:root:[   16] Training loss: 0.10157127, Validation loss: 0.24939889, Gradient norm: 1.26363696
INFO:root:[   17] Training loss: 0.09806694, Validation loss: 0.14687969, Gradient norm: 1.20849457
INFO:root:[   18] Training loss: 0.09689724, Validation loss: 0.25043155, Gradient norm: 1.28347819
INFO:root:[   19] Training loss: 0.09433518, Validation loss: 0.17307754, Gradient norm: 1.07447167
INFO:root:[   20] Training loss: 0.09348983, Validation loss: 0.20740694, Gradient norm: 1.28881584
INFO:root:[   21] Training loss: 0.09157188, Validation loss: 0.21990471, Gradient norm: 1.24224470
INFO:root:[   22] Training loss: 0.09234794, Validation loss: 0.21082904, Gradient norm: 1.33398629
INFO:root:[   23] Training loss: 0.08981663, Validation loss: 0.18248604, Gradient norm: 1.23754212
INFO:root:[   24] Training loss: 0.08744913, Validation loss: 0.16319903, Gradient norm: 1.20393107
INFO:root:[   25] Training loss: 0.08679959, Validation loss: 0.16183560, Gradient norm: 1.29485270
INFO:root:[   26] Training loss: 0.08414618, Validation loss: 0.22317698, Gradient norm: 1.25290663
INFO:root:[   27] Training loss: 0.08245951, Validation loss: 0.17983601, Gradient norm: 1.09829018
INFO:root:[   28] Training loss: 0.08153013, Validation loss: 0.15332599, Gradient norm: 1.15185464
INFO:root:[   29] Training loss: 0.08014480, Validation loss: 0.16884019, Gradient norm: 1.20344262
INFO:root:[   30] Training loss: 0.07891095, Validation loss: 0.19110736, Gradient norm: 1.22571276
INFO:root:[   31] Training loss: 0.07741155, Validation loss: 0.13914269, Gradient norm: 1.22978384
INFO:root:[   32] Training loss: 0.07696459, Validation loss: 0.19646659, Gradient norm: 1.42367238
INFO:root:[   33] Training loss: 0.07431257, Validation loss: 0.12429008, Gradient norm: 1.27011346
INFO:root:[   34] Training loss: 0.07342267, Validation loss: 0.19315173, Gradient norm: 1.27614692
INFO:root:[   35] Training loss: 0.07227089, Validation loss: 0.18490175, Gradient norm: 1.31922864
INFO:root:[   36] Training loss: 0.07099650, Validation loss: 0.13929154, Gradient norm: 1.44150475
INFO:root:[   37] Training loss: 0.07010430, Validation loss: 0.18821505, Gradient norm: 1.33439740
INFO:root:[   38] Training loss: 0.06819072, Validation loss: 0.13712096, Gradient norm: 1.17609497
INFO:root:[   39] Training loss: 0.06728030, Validation loss: 0.16136339, Gradient norm: 1.32350067
INFO:root:[   40] Training loss: 0.06569090, Validation loss: 0.14408111, Gradient norm: 1.37412858
INFO:root:[   41] Training loss: 0.06501428, Validation loss: 0.13746655, Gradient norm: 1.32082016
INFO:root:[   42] Training loss: 0.06333535, Validation loss: 0.16442804, Gradient norm: 1.28794067
INFO:root:[   43] Training loss: 0.06314370, Validation loss: 0.12833989, Gradient norm: 1.48870308
INFO:root:[   44] Training loss: 0.06154223, Validation loss: 0.15129272, Gradient norm: 1.44508529
INFO:root:[   45] Training loss: 0.06040004, Validation loss: 0.12207676, Gradient norm: 1.49239529
INFO:root:[   46] Training loss: 0.05932094, Validation loss: 0.15640360, Gradient norm: 1.46387441
INFO:root:[   47] Training loss: 0.05776786, Validation loss: 0.12782989, Gradient norm: 1.41168247
INFO:root:[   48] Training loss: 0.05616106, Validation loss: 0.10509119, Gradient norm: 1.18900943
INFO:root:[   49] Training loss: 0.05573658, Validation loss: 0.14689084, Gradient norm: 1.39613189
INFO:root:[   50] Training loss: 0.05420293, Validation loss: 0.14807782, Gradient norm: 1.33482028
INFO:root:[   51] Training loss: 0.05298231, Validation loss: 0.10175748, Gradient norm: 1.37120073
INFO:root:[   52] Training loss: 0.05256841, Validation loss: 0.13982871, Gradient norm: 1.43417234
INFO:root:[   53] Training loss: 0.05240078, Validation loss: 0.10497802, Gradient norm: 1.49097544
INFO:root:[   54] Training loss: 0.05058466, Validation loss: 0.11178083, Gradient norm: 1.34438364
INFO:root:[   55] Training loss: 0.05012656, Validation loss: 0.13568701, Gradient norm: 1.36174818
INFO:root:[   56] Training loss: 0.04703467, Validation loss: 0.13269864, Gradient norm: 1.14300362
INFO:root:[   57] Training loss: 0.04713212, Validation loss: 0.12265229, Gradient norm: 1.23951461
INFO:root:[   58] Training loss: 0.04634996, Validation loss: 0.07618570, Gradient norm: 1.34754371
INFO:root:[   59] Training loss: 0.04569085, Validation loss: 0.11578156, Gradient norm: 1.32639874
INFO:root:[   60] Training loss: 0.04569235, Validation loss: 0.12476241, Gradient norm: 1.25106099
INFO:root:[   61] Training loss: 0.04425286, Validation loss: 0.13498283, Gradient norm: 1.27956931
INFO:root:[   62] Training loss: 0.04442571, Validation loss: 0.07172430, Gradient norm: 1.59814758
INFO:root:[   63] Training loss: 0.04345994, Validation loss: 0.11372237, Gradient norm: 1.41327613
INFO:root:[   64] Training loss: 0.04146438, Validation loss: 0.11384714, Gradient norm: 1.32275814
INFO:root:[   65] Training loss: 0.04053865, Validation loss: 0.09423992, Gradient norm: 1.25822620
INFO:root:[   66] Training loss: 0.03926991, Validation loss: 0.07262044, Gradient norm: 1.23427329
INFO:root:[   67] Training loss: 0.03980545, Validation loss: 0.07744280, Gradient norm: 1.21166481
INFO:root:[   68] Training loss: 0.04018767, Validation loss: 0.10231659, Gradient norm: 1.48176134
INFO:root:[   69] Training loss: 0.03895873, Validation loss: 0.11554494, Gradient norm: 1.29220690
INFO:root:[   70] Training loss: 0.03871407, Validation loss: 0.07767216, Gradient norm: 1.31226499
INFO:root:[   71] Training loss: 0.03706826, Validation loss: 0.05093542, Gradient norm: 1.32058182
INFO:root:[   72] Training loss: 0.03721821, Validation loss: 0.11269192, Gradient norm: 1.43253152
INFO:root:[   73] Training loss: 0.03664755, Validation loss: 0.04895263, Gradient norm: 1.43274253
INFO:root:[   74] Training loss: 0.03574818, Validation loss: 0.08859436, Gradient norm: 1.37900469
INFO:root:[   75] Training loss: 0.03692155, Validation loss: 0.05031007, Gradient norm: 1.26189040
INFO:root:[   76] Training loss: 0.03462321, Validation loss: 0.09143498, Gradient norm: 1.20094484
INFO:root:[   77] Training loss: 0.03396828, Validation loss: 0.07208737, Gradient norm: 1.24930603
INFO:root:[   78] Training loss: 0.03355290, Validation loss: 0.04551779, Gradient norm: 1.17413598
INFO:root:[   79] Training loss: 0.03300381, Validation loss: 0.08090098, Gradient norm: 1.18366577
INFO:root:[   80] Training loss: 0.03247420, Validation loss: 0.08503578, Gradient norm: 1.20953210
INFO:root:[   81] Training loss: 0.03226187, Validation loss: 0.04296118, Gradient norm: 1.27568991
INFO:root:[   82] Training loss: 0.03194635, Validation loss: 0.07792827, Gradient norm: 1.23737990
INFO:root:[   83] Training loss: 0.03065284, Validation loss: 0.05167219, Gradient norm: 1.12192392
INFO:root:[   84] Training loss: 0.03004254, Validation loss: 0.04750780, Gradient norm: 1.04275885
INFO:root:[   85] Training loss: 0.03073928, Validation loss: 0.07309060, Gradient norm: 1.24064609
INFO:root:[   86] Training loss: 0.03180909, Validation loss: 0.03373684, Gradient norm: 1.14909151
INFO:root:[   87] Training loss: 0.02896379, Validation loss: 0.04192376, Gradient norm: 1.03267135
INFO:root:[   88] Training loss: 0.02890185, Validation loss: 0.06559184, Gradient norm: 1.06653354
INFO:root:[   89] Training loss: 0.02932959, Validation loss: 0.03464208, Gradient norm: 1.07475482
INFO:root:[   90] Training loss: 0.03041408, Validation loss: 0.06801188, Gradient norm: 1.19868129
INFO:root:[   91] Training loss: 0.02877749, Validation loss: 0.02649921, Gradient norm: 1.05390267
INFO:root:[   92] Training loss: 0.02933194, Validation loss: 0.04587425, Gradient norm: 1.10257305
INFO:root:[   93] Training loss: 0.02743028, Validation loss: 0.04645971, Gradient norm: 0.88212519
INFO:root:[   94] Training loss: 0.02758424, Validation loss: 0.02625610, Gradient norm: 0.93592203
INFO:root:[   95] Training loss: 0.02716839, Validation loss: 0.03916786, Gradient norm: 1.00858336
INFO:root:[   96] Training loss: 0.02819663, Validation loss: 0.05265766, Gradient norm: 1.06187394
INFO:root:[   97] Training loss: 0.02823632, Validation loss: 0.02675455, Gradient norm: 1.03290431
INFO:root:[   98] Training loss: 0.02807830, Validation loss: 0.05846141, Gradient norm: 0.96903901
INFO:root:[   99] Training loss: 0.02708411, Validation loss: 0.03178159, Gradient norm: 0.97430270
INFO:root:[  100] Training loss: 0.02898922, Validation loss: 0.02248309, Gradient norm: 1.01874968
INFO:root:[  101] Training loss: 0.02830645, Validation loss: 0.06039122, Gradient norm: 0.89327415
INFO:root:[  102] Training loss: 0.02781407, Validation loss: 0.05683706, Gradient norm: 0.96889504
INFO:root:[  103] Training loss: 0.02721038, Validation loss: 0.04639100, Gradient norm: 0.92408544
INFO:root:[  104] Training loss: 0.02739237, Validation loss: 0.02111249, Gradient norm: 0.98422503
INFO:root:[  105] Training loss: 0.02600145, Validation loss: 0.04527958, Gradient norm: 0.92441879
INFO:root:[  106] Training loss: 0.02669079, Validation loss: 0.04856065, Gradient norm: 0.92669619
INFO:root:[  107] Training loss: 0.02610675, Validation loss: 0.02313619, Gradient norm: 0.88223832
INFO:root:[  108] Training loss: 0.02667715, Validation loss: 0.03351206, Gradient norm: 0.95276912
INFO:root:[  109] Training loss: 0.02590656, Validation loss: 0.03707284, Gradient norm: 0.87572342
INFO:root:[  110] Training loss: 0.02616688, Validation loss: 0.02569855, Gradient norm: 0.82500672
INFO:root:[  111] Training loss: 0.02654152, Validation loss: 0.04924309, Gradient norm: 0.88465412
INFO:root:[  112] Training loss: 0.02633735, Validation loss: 0.03684795, Gradient norm: 0.91079015
INFO:root:[  113] Training loss: 0.02759009, Validation loss: 0.04888498, Gradient norm: 0.93998712
INFO:root:EP 113: Early stopping
INFO:root:Training the model took 1704.292s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02993
INFO:root:EnergyScoreTrain: 0.02142
INFO:root:CoverageTrain: 0.84664
INFO:root:IntervalWidthTrain: 0.02176
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02979
INFO:root:EnergyScoreValidation: 0.0213
INFO:root:CoverageValidation: 0.84644
INFO:root:IntervalWidthValidation: 0.02173
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02992
INFO:root:EnergyScoreTest: 0.02153
INFO:root:CoverageTest: 0.84873
INFO:root:IntervalWidthTest: 0.02166
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.78049342, Validation loss: 0.17228995, Gradient norm: 1.75615212
INFO:root:[    2] Training loss: 0.22485988, Validation loss: 0.31891724, Gradient norm: 1.02302319
INFO:root:[    3] Training loss: 0.18663919, Validation loss: 0.39489917, Gradient norm: 0.75841757
INFO:root:[    4] Training loss: 0.17119440, Validation loss: 0.51853309, Gradient norm: 0.79313201
INFO:root:[    5] Training loss: 0.15941444, Validation loss: 0.52733037, Gradient norm: 0.77866157
INFO:root:[    6] Training loss: 0.14839307, Validation loss: 0.53938327, Gradient norm: 0.38053225
INFO:root:[    7] Training loss: 0.14694733, Validation loss: 0.65560689, Gradient norm: 0.97540378
INFO:root:[    8] Training loss: 0.14430087, Validation loss: 0.64583916, Gradient norm: 1.12636700
INFO:root:[    9] Training loss: 0.13837204, Validation loss: 0.56979791, Gradient norm: 0.99133791
INFO:root:[   10] Training loss: 0.13533689, Validation loss: 0.51142167, Gradient norm: 1.04403190
INFO:root:[   11] Training loss: 0.13182269, Validation loss: 0.52264357, Gradient norm: 1.04758126
INFO:root:[   12] Training loss: 0.12924350, Validation loss: 0.53775267, Gradient norm: 1.21043946
INFO:root:[   13] Training loss: 0.12291723, Validation loss: 0.51246985, Gradient norm: 0.97631986
INFO:root:[   14] Training loss: 0.11917924, Validation loss: 0.57784869, Gradient norm: 0.93781090
INFO:root:[   15] Training loss: 0.11545216, Validation loss: 0.51045177, Gradient norm: 0.89492631
INFO:root:[   16] Training loss: 0.11345853, Validation loss: 0.58482042, Gradient norm: 1.16419770
INFO:root:[   17] Training loss: 0.10937192, Validation loss: 0.53643939, Gradient norm: 1.09156582
INFO:root:[   18] Training loss: 0.10672499, Validation loss: 0.49754214, Gradient norm: 1.15688496
INFO:root:[   19] Training loss: 0.10402364, Validation loss: 0.55865578, Gradient norm: 1.23538321
INFO:root:[   20] Training loss: 0.10003258, Validation loss: 0.48346991, Gradient norm: 1.11421965
INFO:root:[   21] Training loss: 0.09778862, Validation loss: 0.53689618, Gradient norm: 1.16447836
INFO:root:[   22] Training loss: 0.09422434, Validation loss: 0.45441034, Gradient norm: 1.10092071
INFO:root:[   23] Training loss: 0.09193933, Validation loss: 0.53352958, Gradient norm: 1.21277555
INFO:root:[   24] Training loss: 0.08958555, Validation loss: 0.42434514, Gradient norm: 1.26178922
INFO:root:[   25] Training loss: 0.08648021, Validation loss: 0.47074598, Gradient norm: 1.09201240
INFO:root:[   26] Training loss: 0.08423044, Validation loss: 0.49520835, Gradient norm: 1.12732470
INFO:root:[   27] Training loss: 0.08226989, Validation loss: 0.40250796, Gradient norm: 1.17790597
INFO:root:[   28] Training loss: 0.07957622, Validation loss: 0.46064559, Gradient norm: 1.19315641
INFO:root:[   29] Training loss: 0.07824337, Validation loss: 0.39486804, Gradient norm: 1.38271925
INFO:root:[   30] Training loss: 0.07595207, Validation loss: 0.41654793, Gradient norm: 1.28890957
INFO:root:[   31] Training loss: 0.07224226, Validation loss: 0.44210168, Gradient norm: 0.98714559
INFO:root:[   32] Training loss: 0.07118295, Validation loss: 0.37413517, Gradient norm: 1.21493660
INFO:root:[   33] Training loss: 0.06985194, Validation loss: 0.39721576, Gradient norm: 1.34871470
INFO:root:[   34] Training loss: 0.06797855, Validation loss: 0.36623716, Gradient norm: 1.32126841
INFO:root:[   35] Training loss: 0.06520333, Validation loss: 0.32457360, Gradient norm: 1.11201706
INFO:root:[   36] Training loss: 0.06393279, Validation loss: 0.37383826, Gradient norm: 1.20170286
INFO:root:[   37] Training loss: 0.06163722, Validation loss: 0.36579016, Gradient norm: 1.21191038
INFO:root:[   38] Training loss: 0.05958919, Validation loss: 0.29686370, Gradient norm: 1.18617746
INFO:root:[   39] Training loss: 0.05775311, Validation loss: 0.28513237, Gradient norm: 1.05973737
INFO:root:[   40] Training loss: 0.05807079, Validation loss: 0.34617991, Gradient norm: 1.37626744
INFO:root:[   41] Training loss: 0.05551084, Validation loss: 0.29817218, Gradient norm: 1.25530197
INFO:root:[   42] Training loss: 0.05410814, Validation loss: 0.25331525, Gradient norm: 1.19269631
INFO:root:[   43] Training loss: 0.05265249, Validation loss: 0.31214828, Gradient norm: 1.16530566
INFO:root:[   44] Training loss: 0.05103929, Validation loss: 0.29804783, Gradient norm: 1.14039431
INFO:root:[   45] Training loss: 0.04955569, Validation loss: 0.26972603, Gradient norm: 1.12568806
INFO:root:[   46] Training loss: 0.04894435, Validation loss: 0.27900530, Gradient norm: 1.18531712
INFO:root:[   47] Training loss: 0.04719339, Validation loss: 0.26270405, Gradient norm: 1.18457361
INFO:root:[   48] Training loss: 0.04661964, Validation loss: 0.21492115, Gradient norm: 1.21285138
INFO:root:[   49] Training loss: 0.04578470, Validation loss: 0.18970327, Gradient norm: 1.16783245
INFO:root:[   50] Training loss: 0.04473818, Validation loss: 0.22810931, Gradient norm: 1.22620740
INFO:root:[   51] Training loss: 0.04506360, Validation loss: 0.18370142, Gradient norm: 1.25177511
INFO:root:[   52] Training loss: 0.04356359, Validation loss: 0.16054499, Gradient norm: 1.09317330
INFO:root:[   53] Training loss: 0.04231472, Validation loss: 0.20771954, Gradient norm: 1.13458772
INFO:root:[   54] Training loss: 0.04082034, Validation loss: 0.19961916, Gradient norm: 1.07142697
INFO:root:[   55] Training loss: 0.03995195, Validation loss: 0.18749702, Gradient norm: 1.08980043
INFO:root:[   56] Training loss: 0.04051070, Validation loss: 0.12317369, Gradient norm: 1.20341942
INFO:root:[   57] Training loss: 0.04101678, Validation loss: 0.16501555, Gradient norm: 1.14368896
INFO:root:[   58] Training loss: 0.03906556, Validation loss: 0.13657617, Gradient norm: 1.07317009
INFO:root:[   59] Training loss: 0.03872830, Validation loss: 0.11625189, Gradient norm: 1.11041905
INFO:root:[   60] Training loss: 0.03911996, Validation loss: 0.13876082, Gradient norm: 1.12620334
INFO:root:[   61] Training loss: 0.03769531, Validation loss: 0.10158657, Gradient norm: 1.11786560
INFO:root:[   62] Training loss: 0.03698380, Validation loss: 0.08154170, Gradient norm: 1.01663586
INFO:root:[   63] Training loss: 0.03858891, Validation loss: 0.13855476, Gradient norm: 1.10651638
INFO:root:[   64] Training loss: 0.03728820, Validation loss: 0.09390061, Gradient norm: 1.07133426
INFO:root:[   65] Training loss: 0.03703756, Validation loss: 0.08224965, Gradient norm: 1.13169315
INFO:root:[   66] Training loss: 0.03777195, Validation loss: 0.10524962, Gradient norm: 1.06271591
INFO:root:[   67] Training loss: 0.03686514, Validation loss: 0.08018104, Gradient norm: 1.01911292
INFO:root:[   68] Training loss: 0.03680292, Validation loss: 0.09863093, Gradient norm: 1.03670210
INFO:root:[   69] Training loss: 0.03743509, Validation loss: 0.11771130, Gradient norm: 1.03525866
INFO:root:[   70] Training loss: 0.03733534, Validation loss: 0.09392907, Gradient norm: 1.16810190
INFO:root:[   71] Training loss: 0.03670680, Validation loss: 0.07119349, Gradient norm: 1.00071410
INFO:root:[   72] Training loss: 0.03605921, Validation loss: 0.09040200, Gradient norm: 1.08781147
INFO:root:[   73] Training loss: 0.03512820, Validation loss: 0.10300607, Gradient norm: 1.03163573
INFO:root:[   74] Training loss: 0.03679496, Validation loss: 0.06934073, Gradient norm: 1.19555686
INFO:root:[   75] Training loss: 0.03630638, Validation loss: 0.09092795, Gradient norm: 1.09421507
INFO:root:[   76] Training loss: 0.03553728, Validation loss: 0.10353476, Gradient norm: 1.07113920
INFO:root:[   77] Training loss: 0.03648887, Validation loss: 0.06248909, Gradient norm: 1.10138033
INFO:root:[   78] Training loss: 0.03629882, Validation loss: 0.10018085, Gradient norm: 1.13005495
INFO:root:[   79] Training loss: 0.03696746, Validation loss: 0.10633634, Gradient norm: 1.15587994
INFO:root:[   80] Training loss: 0.03771038, Validation loss: 0.05895625, Gradient norm: 1.25651571
INFO:root:[   81] Training loss: 0.03737694, Validation loss: 0.06265636, Gradient norm: 1.17483101
INFO:root:[   82] Training loss: 0.03632056, Validation loss: 0.05584564, Gradient norm: 1.08536907
INFO:root:[   83] Training loss: 0.03716948, Validation loss: 0.08193767, Gradient norm: 1.20623549
INFO:root:[   84] Training loss: 0.03707309, Validation loss: 0.09046473, Gradient norm: 1.18152918
INFO:root:[   85] Training loss: 0.03768234, Validation loss: 0.04874369, Gradient norm: 1.22572859
INFO:root:[   86] Training loss: 0.03759964, Validation loss: 0.09573590, Gradient norm: 1.22242539
INFO:root:[   87] Training loss: 0.03703429, Validation loss: 0.06114580, Gradient norm: 1.16844191
INFO:root:[   88] Training loss: 0.03661473, Validation loss: 0.06234855, Gradient norm: 1.16481612
INFO:root:[   89] Training loss: 0.03648636, Validation loss: 0.08025206, Gradient norm: 1.15316203
INFO:root:[   90] Training loss: 0.03733125, Validation loss: 0.08442826, Gradient norm: 1.27991027
INFO:root:[   91] Training loss: 0.03740160, Validation loss: 0.07478785, Gradient norm: 1.23749353
INFO:root:[   92] Training loss: 0.03744800, Validation loss: 0.06896185, Gradient norm: 1.19579697
INFO:root:[   93] Training loss: 0.03694907, Validation loss: 0.09456510, Gradient norm: 1.15236997
INFO:root:[   94] Training loss: 0.03631266, Validation loss: 0.05831526, Gradient norm: 1.21799139
INFO:root:EP 94: Early stopping
INFO:root:Training the model took 1421.447s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05782
INFO:root:EnergyScoreTrain: 0.02389
INFO:root:CoverageTrain: 0.02895
INFO:root:IntervalWidthTrain: 0.00604
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0579
INFO:root:EnergyScoreValidation: 0.0234
INFO:root:CoverageValidation: 0.02875
INFO:root:IntervalWidthValidation: 0.00603
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.05762
INFO:root:EnergyScoreTest: 0.02265
INFO:root:CoverageTest: 0.02762
INFO:root:IntervalWidthTest: 0.00595
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.86826865, Validation loss: 0.11955937, Gradient norm: 5.19302162
INFO:root:[    2] Training loss: 0.15295059, Validation loss: 0.10072220, Gradient norm: 2.23574332
INFO:root:[    3] Training loss: 0.12125390, Validation loss: 0.07298911, Gradient norm: 2.53803980
INFO:root:[    4] Training loss: 0.10425052, Validation loss: 0.07896702, Gradient norm: 2.83574440
INFO:root:[    5] Training loss: 0.09225078, Validation loss: 0.06052066, Gradient norm: 2.52992500
INFO:root:[    6] Training loss: 0.08705025, Validation loss: 0.06202889, Gradient norm: 2.64064891
INFO:root:[    7] Training loss: 0.08299774, Validation loss: 0.08389680, Gradient norm: 2.23263502
INFO:root:[    8] Training loss: 0.08186786, Validation loss: 0.05899631, Gradient norm: 2.68170632
INFO:root:[    9] Training loss: 0.07706142, Validation loss: 0.06643480, Gradient norm: 2.67671117
INFO:root:[   10] Training loss: 0.07283857, Validation loss: 0.04989158, Gradient norm: 2.19602722
INFO:root:[   11] Training loss: 0.07326648, Validation loss: 0.04975427, Gradient norm: 2.46676019
INFO:root:[   12] Training loss: 0.07185486, Validation loss: 0.06762347, Gradient norm: 2.48530627
INFO:root:[   13] Training loss: 0.06901814, Validation loss: 0.06591606, Gradient norm: 2.33251703
INFO:root:[   14] Training loss: 0.06668640, Validation loss: 0.06194406, Gradient norm: 2.17790499
INFO:root:[   15] Training loss: 0.06824867, Validation loss: 0.06355419, Gradient norm: 2.39445803
INFO:root:[   16] Training loss: 0.06562065, Validation loss: 0.05765988, Gradient norm: 2.26475882
INFO:root:[   17] Training loss: 0.06364949, Validation loss: 0.04757639, Gradient norm: 2.04917894
INFO:root:[   18] Training loss: 0.06346194, Validation loss: 0.04925366, Gradient norm: 2.17211359
INFO:root:[   19] Training loss: 0.06244081, Validation loss: 0.05759790, Gradient norm: 2.11691009
INFO:root:[   20] Training loss: 0.06205100, Validation loss: 0.04802468, Gradient norm: 2.24739990
INFO:root:[   21] Training loss: 0.05777529, Validation loss: 0.05149543, Gradient norm: 2.14383644
INFO:root:[   22] Training loss: 0.05689794, Validation loss: 0.04247753, Gradient norm: 2.19878580
INFO:root:[   23] Training loss: 0.05527088, Validation loss: 0.04022202, Gradient norm: 1.92516011
INFO:root:[   24] Training loss: 0.05697008, Validation loss: 0.03228836, Gradient norm: 2.22765058
INFO:root:[   25] Training loss: 0.05414908, Validation loss: 0.04539874, Gradient norm: 2.21128042
INFO:root:[   26] Training loss: 0.05312499, Validation loss: 0.05388085, Gradient norm: 2.48643530
INFO:root:[   27] Training loss: 0.05332135, Validation loss: 0.04533330, Gradient norm: 2.42487904
INFO:root:[   28] Training loss: 0.05405621, Validation loss: 0.03869685, Gradient norm: 2.31678050
INFO:root:[   29] Training loss: 0.05292085, Validation loss: 0.04546303, Gradient norm: 2.24853001
INFO:root:[   30] Training loss: 0.05024647, Validation loss: 0.04305685, Gradient norm: 2.23275815
INFO:root:[   31] Training loss: 0.04956446, Validation loss: 0.02175046, Gradient norm: 2.59910945
INFO:root:[   32] Training loss: 0.04852489, Validation loss: 0.02303153, Gradient norm: 2.49626717
INFO:root:[   33] Training loss: 0.04952765, Validation loss: 0.02199516, Gradient norm: 2.31364207
INFO:root:[   34] Training loss: 0.04796917, Validation loss: 0.04162201, Gradient norm: 2.52835708
INFO:root:[   35] Training loss: 0.04675019, Validation loss: 0.04121921, Gradient norm: 2.47866261
INFO:root:[   36] Training loss: 0.04883749, Validation loss: 0.04933926, Gradient norm: 2.33469314
INFO:root:[   37] Training loss: 0.04961878, Validation loss: 0.04009566, Gradient norm: 2.34765122
INFO:root:[   38] Training loss: 0.04677682, Validation loss: 0.03676282, Gradient norm: 2.33820342
INFO:root:[   39] Training loss: 0.04527527, Validation loss: 0.04686481, Gradient norm: 2.33341074
INFO:root:[   40] Training loss: 0.04749617, Validation loss: 0.03702357, Gradient norm: 2.38854138
INFO:root:[   41] Training loss: 0.04769389, Validation loss: 0.01741502, Gradient norm: 2.50935095
INFO:root:[   42] Training loss: 0.04536958, Validation loss: 0.03516848, Gradient norm: 2.35777829
INFO:root:[   43] Training loss: 0.04462202, Validation loss: 0.04146544, Gradient norm: 2.43979094
INFO:root:[   44] Training loss: 0.04411494, Validation loss: 0.01716474, Gradient norm: 2.64595616
INFO:root:[   45] Training loss: 0.04331179, Validation loss: 0.01836150, Gradient norm: 2.58033780
INFO:root:[   46] Training loss: 0.04325965, Validation loss: 0.03870949, Gradient norm: 2.59726520
INFO:root:[   47] Training loss: 0.04308465, Validation loss: 0.04265448, Gradient norm: 2.45592191
INFO:root:[   48] Training loss: 0.04315359, Validation loss: 0.01488827, Gradient norm: 2.65384471
INFO:root:[   49] Training loss: 0.04256460, Validation loss: 0.01726246, Gradient norm: 2.58326074
INFO:root:[   50] Training loss: 0.04206683, Validation loss: 0.03775519, Gradient norm: 2.45460612
INFO:root:[   51] Training loss: 0.04266386, Validation loss: 0.04264391, Gradient norm: 2.49145590
INFO:root:[   52] Training loss: 0.04838144, Validation loss: 0.04209813, Gradient norm: 2.54290384
INFO:root:[   53] Training loss: 0.04301808, Validation loss: 0.04084049, Gradient norm: 2.13220815
INFO:root:[   54] Training loss: 0.04193148, Validation loss: 0.02076077, Gradient norm: 2.36352861
INFO:root:[   55] Training loss: 0.04155706, Validation loss: 0.01582369, Gradient norm: 2.67364727
INFO:root:[   56] Training loss: 0.04110566, Validation loss: 0.03755144, Gradient norm: 2.69999483
INFO:root:[   57] Training loss: 0.04098936, Validation loss: 0.04201378, Gradient norm: 2.64530735
INFO:root:[   58] Training loss: 0.04040959, Validation loss: 0.01362229, Gradient norm: 2.61126698
INFO:root:[   59] Training loss: 0.04061642, Validation loss: 0.01436434, Gradient norm: 2.64179863
INFO:root:[   60] Training loss: 0.04012385, Validation loss: 0.03779496, Gradient norm: 2.66072949
INFO:root:[   61] Training loss: 0.04017689, Validation loss: 0.04552020, Gradient norm: 2.59204847
INFO:root:[   62] Training loss: 0.04525524, Validation loss: 0.02430611, Gradient norm: 2.22257561
INFO:root:[   63] Training loss: 0.04516876, Validation loss: 0.04028118, Gradient norm: 2.20604361
INFO:root:[   64] Training loss: 0.04192476, Validation loss: 0.04621218, Gradient norm: 2.08056852
INFO:root:[   65] Training loss: 0.03975978, Validation loss: 0.01240820, Gradient norm: 2.62392157
INFO:root:[   66] Training loss: 0.03934042, Validation loss: 0.01684321, Gradient norm: 2.76737736
INFO:root:[   67] Training loss: 0.03887274, Validation loss: 0.03943676, Gradient norm: 2.60131017
INFO:root:[   68] Training loss: 0.03913944, Validation loss: 0.03936273, Gradient norm: 2.61875593
INFO:root:[   69] Training loss: 0.03895530, Validation loss: 0.01539234, Gradient norm: 2.61159406
INFO:root:[   70] Training loss: 0.03834253, Validation loss: 0.01220057, Gradient norm: 2.65469933
INFO:root:[   71] Training loss: 0.03855153, Validation loss: 0.03719784, Gradient norm: 2.66593928
INFO:root:[   72] Training loss: 0.03836850, Validation loss: 0.04088757, Gradient norm: 2.64124685
INFO:root:[   73] Training loss: 0.03992127, Validation loss: 0.04262967, Gradient norm: 2.48360577
INFO:root:[   74] Training loss: 0.03837084, Validation loss: 0.01149790, Gradient norm: 2.58164545
INFO:root:[   75] Training loss: 0.03809391, Validation loss: 0.01288586, Gradient norm: 2.53844831
INFO:root:[   76] Training loss: 0.03814998, Validation loss: 0.03472407, Gradient norm: 2.75482654
INFO:root:[   77] Training loss: 0.03851897, Validation loss: 0.03694454, Gradient norm: 2.50890932
INFO:root:[   78] Training loss: 0.03906967, Validation loss: 0.03723822, Gradient norm: 2.38067928
INFO:root:[   79] Training loss: 0.04045247, Validation loss: 0.01339772, Gradient norm: 2.38391718
INFO:root:[   80] Training loss: 0.03941776, Validation loss: 0.01784813, Gradient norm: 2.46133732
INFO:root:[   81] Training loss: 0.03923220, Validation loss: 0.04101947, Gradient norm: 2.54747546
INFO:root:[   82] Training loss: 0.03744916, Validation loss: 0.01192561, Gradient norm: 2.70859431
INFO:root:[   83] Training loss: 0.03714327, Validation loss: 0.01166574, Gradient norm: 2.66005711
INFO:root:EP 83: Early stopping
INFO:root:Training the model took 1083.018s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.05649
INFO:root:EnergyScoreTrain: 0.02808
INFO:root:CoverageTrain: 0.74906
INFO:root:IntervalWidthTrain: 0.05422
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06763
INFO:root:EnergyScoreValidation: 0.03755
INFO:root:CoverageValidation: 0.61539
INFO:root:IntervalWidthValidation: 0.06056
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.04909
INFO:root:EnergyScoreTest: 0.02564
INFO:root:CoverageTest: 0.68785
INFO:root:IntervalWidthTest: 0.04421
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1816133632
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.49235903, Validation loss: 0.13108651, Gradient norm: 3.44676157
INFO:root:[    2] Training loss: 0.18260182, Validation loss: 0.11530924, Gradient norm: 1.87115264
INFO:root:[    3] Training loss: 0.15593091, Validation loss: 0.09491524, Gradient norm: 1.64898354
INFO:root:[    4] Training loss: 0.13998919, Validation loss: 0.08544821, Gradient norm: 2.34819043
INFO:root:[    5] Training loss: 0.12902032, Validation loss: 0.06665496, Gradient norm: 2.53169949
INFO:root:[    6] Training loss: 0.11777929, Validation loss: 0.06469187, Gradient norm: 1.82460534
INFO:root:[    7] Training loss: 0.11053352, Validation loss: 0.05854200, Gradient norm: 2.03817445
INFO:root:[    8] Training loss: 0.10437494, Validation loss: 0.06108137, Gradient norm: 1.75843345
INFO:root:[    9] Training loss: 0.10007404, Validation loss: 0.08133480, Gradient norm: 1.65282951
INFO:root:[   10] Training loss: 0.09785268, Validation loss: 0.05724113, Gradient norm: 1.84828613
INFO:root:[   11] Training loss: 0.09638888, Validation loss: 0.09960765, Gradient norm: 2.08830921
INFO:root:[   12] Training loss: 0.09358130, Validation loss: 0.09898162, Gradient norm: 2.05608173
INFO:root:[   13] Training loss: 0.09361176, Validation loss: 0.07591124, Gradient norm: 2.21004987
INFO:root:[   14] Training loss: 0.08914080, Validation loss: 0.04846754, Gradient norm: 1.95290530
INFO:root:[   15] Training loss: 0.08853849, Validation loss: 0.04335511, Gradient norm: 2.08452164
INFO:root:[   16] Training loss: 0.08760515, Validation loss: 0.05324008, Gradient norm: 2.02312603
INFO:root:[   17] Training loss: 0.08662700, Validation loss: 0.06753328, Gradient norm: 2.02027442
INFO:root:[   18] Training loss: 0.08372856, Validation loss: 0.06788163, Gradient norm: 1.98877230
INFO:root:[   19] Training loss: 0.08338950, Validation loss: 0.03582022, Gradient norm: 1.97525457
INFO:root:[   20] Training loss: 0.08240765, Validation loss: 0.08963277, Gradient norm: 2.00960863
INFO:root:[   21] Training loss: 0.08155047, Validation loss: 0.10059714, Gradient norm: 2.09369598
INFO:root:[   22] Training loss: 0.08109743, Validation loss: 0.10270447, Gradient norm: 2.28706526
INFO:root:[   23] Training loss: 0.07716650, Validation loss: 0.04049124, Gradient norm: 1.88106071
INFO:root:[   24] Training loss: 0.07940146, Validation loss: 0.02838968, Gradient norm: 2.15641591
INFO:root:[   25] Training loss: 0.07910818, Validation loss: 0.04130424, Gradient norm: 2.26317353
INFO:root:[   26] Training loss: 0.07781916, Validation loss: 0.03430775, Gradient norm: 2.27680198
INFO:root:[   27] Training loss: 0.07493196, Validation loss: 0.03302088, Gradient norm: 2.07976262
INFO:root:[   28] Training loss: 0.07266884, Validation loss: 0.08522375, Gradient norm: 1.84572507
INFO:root:[   29] Training loss: 0.07360594, Validation loss: 0.08384801, Gradient norm: 2.08476933
INFO:root:[   30] Training loss: 0.07212297, Validation loss: 0.02736192, Gradient norm: 1.97423992
INFO:root:[   31] Training loss: 0.07198730, Validation loss: 0.07386824, Gradient norm: 2.11108227
INFO:root:[   32] Training loss: 0.07031414, Validation loss: 0.07120126, Gradient norm: 1.98119078
INFO:root:[   33] Training loss: 0.07186415, Validation loss: 0.03012949, Gradient norm: 2.19348764
INFO:root:[   34] Training loss: 0.07068970, Validation loss: 0.03713778, Gradient norm: 2.15607247
INFO:root:[   35] Training loss: 0.07173835, Validation loss: 0.04542932, Gradient norm: 2.20305011
INFO:root:[   36] Training loss: 0.07043154, Validation loss: 0.03805836, Gradient norm: 2.03381027
INFO:root:[   37] Training loss: 0.06981332, Validation loss: 0.05446041, Gradient norm: 2.30676475
INFO:root:[   38] Training loss: 0.06715895, Validation loss: 0.08838853, Gradient norm: 1.94490016
INFO:root:[   39] Training loss: 0.06682776, Validation loss: 0.03817736, Gradient norm: 1.92212734
INFO:root:[   40] Training loss: 0.06734649, Validation loss: 0.09138222, Gradient norm: 2.15851907
INFO:root:[   41] Training loss: 0.06583353, Validation loss: 0.04875792, Gradient norm: 2.04441821
INFO:root:[   42] Training loss: 0.06499337, Validation loss: 0.06696189, Gradient norm: 1.99454759
INFO:root:[   43] Training loss: 0.06553304, Validation loss: 0.07490809, Gradient norm: 2.09632849
INFO:root:[   44] Training loss: 0.06529469, Validation loss: 0.03327852, Gradient norm: 2.20534564
INFO:root:[   45] Training loss: 0.06555216, Validation loss: 0.03019219, Gradient norm: 2.11973677
INFO:root:[   46] Training loss: 0.06386086, Validation loss: 0.07601152, Gradient norm: 2.01206952
INFO:root:[   47] Training loss: 0.06421052, Validation loss: 0.02781752, Gradient norm: 2.14168773
INFO:root:[   48] Training loss: 0.06466421, Validation loss: 0.04180280, Gradient norm: 2.24976399
INFO:root:[   49] Training loss: 0.06225590, Validation loss: 0.08908627, Gradient norm: 2.06837205
INFO:root:[   50] Training loss: 0.06306103, Validation loss: 0.04501965, Gradient norm: 2.22505292
INFO:root:[   51] Training loss: 0.06311073, Validation loss: 0.04973098, Gradient norm: 2.35699318
INFO:root:[   52] Training loss: 0.06311174, Validation loss: 0.05129121, Gradient norm: 2.11779501
INFO:root:[   53] Training loss: 0.06196828, Validation loss: 0.07701661, Gradient norm: 1.95976182
INFO:root:[   54] Training loss: 0.05906076, Validation loss: 0.02857401, Gradient norm: 1.89077318
INFO:root:[   55] Training loss: 0.06005978, Validation loss: 0.07973664, Gradient norm: 2.14037227
INFO:root:[   56] Training loss: 0.05903738, Validation loss: 0.03339672, Gradient norm: 2.05368801
INFO:root:[   57] Training loss: 0.05929809, Validation loss: 0.08518709, Gradient norm: 2.08008170
INFO:root:[   58] Training loss: 0.05884599, Validation loss: 0.02773517, Gradient norm: 2.13948828
INFO:root:[   59] Training loss: 0.05903799, Validation loss: 0.08432527, Gradient norm: 2.03909727
INFO:root:[   60] Training loss: 0.05857587, Validation loss: 0.04279391, Gradient norm: 2.05951083
INFO:root:[   61] Training loss: 0.05940322, Validation loss: 0.07959199, Gradient norm: 2.22253171
INFO:root:[   62] Training loss: 0.05777758, Validation loss: 0.02363814, Gradient norm: 2.08184384
INFO:root:[   63] Training loss: 0.05637772, Validation loss: 0.08030106, Gradient norm: 2.05702893
INFO:root:[   64] Training loss: 0.05627777, Validation loss: 0.02723043, Gradient norm: 2.15909204
INFO:root:[   65] Training loss: 0.05614296, Validation loss: 0.08302932, Gradient norm: 2.16554174
INFO:root:[   66] Training loss: 0.05658310, Validation loss: 0.02047709, Gradient norm: 2.10865459
INFO:root:[   67] Training loss: 0.05654614, Validation loss: 0.08348108, Gradient norm: 2.10654228
INFO:root:[   68] Training loss: 0.05536733, Validation loss: 0.02527993, Gradient norm: 2.15911331
INFO:root:[   69] Training loss: 0.05425671, Validation loss: 0.07919304, Gradient norm: 2.02729320
INFO:root:[   70] Training loss: 0.05455281, Validation loss: 0.05585134, Gradient norm: 1.97283346
INFO:root:[   71] Training loss: 0.05552444, Validation loss: 0.07816081, Gradient norm: 2.24927879
INFO:root:[   72] Training loss: 0.05330612, Validation loss: 0.02607951, Gradient norm: 2.04268078
INFO:root:[   73] Training loss: 0.05221651, Validation loss: 0.03473338, Gradient norm: 2.05661360
INFO:root:[   74] Training loss: 0.05258288, Validation loss: 0.07272087, Gradient norm: 1.97247444
INFO:root:[   75] Training loss: 0.05285607, Validation loss: 0.04906520, Gradient norm: 2.14907858
INFO:root:EP 75: Early stopping
INFO:root:Training the model took 976.005s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.07991
INFO:root:EnergyScoreTrain: 0.0396
INFO:root:CoverageTrain: 0.70425
INFO:root:IntervalWidthTrain: 0.07312
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.07125
INFO:root:EnergyScoreValidation: 0.03352
INFO:root:CoverageValidation: 0.84549
INFO:root:IntervalWidthValidation: 0.09476
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06744
INFO:root:EnergyScoreTest: 0.03008
INFO:root:CoverageTest: 0.79966
INFO:root:IntervalWidthTest: 0.07318
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1845493760
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.69898957, Validation loss: 0.23411712, Gradient norm: 4.19932943
INFO:root:[    2] Training loss: 0.22351383, Validation loss: 0.24605842, Gradient norm: 2.07774293
INFO:root:[    3] Training loss: 0.18199799, Validation loss: 0.27708144, Gradient norm: 1.81217098
INFO:root:[    4] Training loss: 0.15492367, Validation loss: 0.24865195, Gradient norm: 1.41992438
INFO:root:[    5] Training loss: 0.14723617, Validation loss: 0.25134241, Gradient norm: 1.95720384
INFO:root:[    6] Training loss: 0.13728923, Validation loss: 0.21008051, Gradient norm: 1.81440011
INFO:root:[    7] Training loss: 0.13124909, Validation loss: 0.32055426, Gradient norm: 1.67536712
INFO:root:[    8] Training loss: 0.12766495, Validation loss: 0.30634390, Gradient norm: 1.78021721
INFO:root:[    9] Training loss: 0.12597140, Validation loss: 0.27960717, Gradient norm: 2.01377305
INFO:root:[   10] Training loss: 0.12212862, Validation loss: 0.25529374, Gradient norm: 1.95252730
INFO:root:[   11] Training loss: 0.11892602, Validation loss: 0.22478672, Gradient norm: 1.99922777
INFO:root:[   12] Training loss: 0.11770449, Validation loss: 0.22573917, Gradient norm: 2.18431420
INFO:root:[   13] Training loss: 0.11366129, Validation loss: 0.21821363, Gradient norm: 1.94108452
INFO:root:[   14] Training loss: 0.11176319, Validation loss: 0.24025402, Gradient norm: 2.06334759
INFO:root:[   15] Training loss: 0.10985137, Validation loss: 0.25747308, Gradient norm: 2.09627815
INFO:root:[   16] Training loss: 0.10908624, Validation loss: 0.23023871, Gradient norm: 2.26027212
INFO:root:[   17] Training loss: 0.10639758, Validation loss: 0.27834517, Gradient norm: 2.14503873
INFO:root:[   18] Training loss: 0.10392833, Validation loss: 0.28927946, Gradient norm: 2.08941256
INFO:root:[   19] Training loss: 0.10314981, Validation loss: 0.28563106, Gradient norm: 2.24466606
INFO:root:[   20] Training loss: 0.10004485, Validation loss: 0.22667866, Gradient norm: 2.01388092
INFO:root:[   21] Training loss: 0.09753480, Validation loss: 0.28535585, Gradient norm: 1.88493761
INFO:root:[   22] Training loss: 0.09860889, Validation loss: 0.25222792, Gradient norm: 2.45520652
INFO:root:[   23] Training loss: 0.09524549, Validation loss: 0.19931180, Gradient norm: 2.15631844
INFO:root:[   24] Training loss: 0.09367197, Validation loss: 0.26375040, Gradient norm: 2.13251785
INFO:root:[   25] Training loss: 0.09199674, Validation loss: 0.22527525, Gradient norm: 2.12903708
INFO:root:[   26] Training loss: 0.09049622, Validation loss: 0.19648618, Gradient norm: 2.06946927
INFO:root:[   27] Training loss: 0.08815390, Validation loss: 0.18356093, Gradient norm: 1.90985377
INFO:root:[   28] Training loss: 0.08912798, Validation loss: 0.26147947, Gradient norm: 2.48318550
INFO:root:[   29] Training loss: 0.08628448, Validation loss: 0.23756848, Gradient norm: 2.18163988
INFO:root:[   30] Training loss: 0.08585205, Validation loss: 0.17667638, Gradient norm: 2.30542648
INFO:root:[   31] Training loss: 0.08453505, Validation loss: 0.22455701, Gradient norm: 2.34334335
INFO:root:[   32] Training loss: 0.08221574, Validation loss: 0.24521994, Gradient norm: 2.14374045
INFO:root:[   33] Training loss: 0.08199998, Validation loss: 0.22360066, Gradient norm: 2.34844986
INFO:root:[   34] Training loss: 0.08063000, Validation loss: 0.17435266, Gradient norm: 2.29446525
INFO:root:[   35] Training loss: 0.07880005, Validation loss: 0.17059597, Gradient norm: 2.16300055
INFO:root:[   36] Training loss: 0.07851721, Validation loss: 0.17955577, Gradient norm: 2.27562699
INFO:root:[   37] Training loss: 0.07690471, Validation loss: 0.17111763, Gradient norm: 2.16060306
INFO:root:[   38] Training loss: 0.07722629, Validation loss: 0.19954727, Gradient norm: 2.45143021
INFO:root:[   39] Training loss: 0.07627436, Validation loss: 0.23126181, Gradient norm: 2.39829734
INFO:root:[   40] Training loss: 0.07429839, Validation loss: 0.21969050, Gradient norm: 2.24030325
INFO:root:[   41] Training loss: 0.07324681, Validation loss: 0.20641407, Gradient norm: 2.24703803
INFO:root:[   42] Training loss: 0.07299741, Validation loss: 0.17406146, Gradient norm: 2.32816325
INFO:root:[   43] Training loss: 0.07099297, Validation loss: 0.20260510, Gradient norm: 2.13025018
INFO:root:[   44] Training loss: 0.07075881, Validation loss: 0.21143771, Gradient norm: 2.17287214
INFO:root:[   45] Training loss: 0.06951750, Validation loss: 0.21412780, Gradient norm: 2.22597107
INFO:root:[   46] Training loss: 0.06959193, Validation loss: 0.20583252, Gradient norm: 2.33629715
INFO:root:[   47] Training loss: 0.06849329, Validation loss: 0.18886984, Gradient norm: 2.34475344
INFO:root:[   48] Training loss: 0.06767785, Validation loss: 0.17260414, Gradient norm: 2.29361510
INFO:root:[   49] Training loss: 0.06705038, Validation loss: 0.14462594, Gradient norm: 2.33579762
INFO:root:[   50] Training loss: 0.06609652, Validation loss: 0.14578256, Gradient norm: 2.22836249
INFO:root:[   51] Training loss: 0.06508873, Validation loss: 0.14689686, Gradient norm: 2.28166046
INFO:root:[   52] Training loss: 0.06421428, Validation loss: 0.14012476, Gradient norm: 2.20634197
INFO:root:[   53] Training loss: 0.06486765, Validation loss: 0.18179588, Gradient norm: 2.42514009
INFO:root:[   54] Training loss: 0.06309066, Validation loss: 0.19360069, Gradient norm: 2.27810401
INFO:root:[   55] Training loss: 0.06265928, Validation loss: 0.18872510, Gradient norm: 2.22370781
INFO:root:[   56] Training loss: 0.06199713, Validation loss: 0.18238001, Gradient norm: 2.26621922
INFO:root:[   57] Training loss: 0.06105327, Validation loss: 0.16195959, Gradient norm: 2.30764003
INFO:root:[   58] Training loss: 0.06028577, Validation loss: 0.12706817, Gradient norm: 2.26138544
INFO:root:[   59] Training loss: 0.05949750, Validation loss: 0.12413053, Gradient norm: 2.27760206
INFO:root:[   60] Training loss: 0.05875559, Validation loss: 0.13886134, Gradient norm: 2.25963066
INFO:root:[   61] Training loss: 0.05839815, Validation loss: 0.17708971, Gradient norm: 2.28634004
INFO:root:[   62] Training loss: 0.05726777, Validation loss: 0.17522746, Gradient norm: 2.22586478
INFO:root:[   63] Training loss: 0.05663520, Validation loss: 0.17034037, Gradient norm: 2.21147760
INFO:root:[   64] Training loss: 0.05617327, Validation loss: 0.16178086, Gradient norm: 2.18749104
INFO:root:[   65] Training loss: 0.05554458, Validation loss: 0.14679013, Gradient norm: 2.14093365
INFO:root:[   66] Training loss: 0.05528466, Validation loss: 0.12779770, Gradient norm: 2.18481553
INFO:root:[   67] Training loss: 0.05408437, Validation loss: 0.11560235, Gradient norm: 2.21724264
INFO:root:[   68] Training loss: 0.05306200, Validation loss: 0.11323817, Gradient norm: 2.09041348
INFO:root:[   69] Training loss: 0.05217435, Validation loss: 0.13568050, Gradient norm: 2.10165084
INFO:root:[   70] Training loss: 0.05219664, Validation loss: 0.13912540, Gradient norm: 2.16740165
INFO:root:[   71] Training loss: 0.05243418, Validation loss: 0.10249426, Gradient norm: 2.21597059
INFO:root:[   72] Training loss: 0.05141308, Validation loss: 0.12591863, Gradient norm: 2.30224610
INFO:root:[   73] Training loss: 0.04997829, Validation loss: 0.14070529, Gradient norm: 2.10177193
INFO:root:[   74] Training loss: 0.05050465, Validation loss: 0.14137042, Gradient norm: 2.27929257
INFO:root:[   75] Training loss: 0.04926109, Validation loss: 0.12193142, Gradient norm: 2.13403068
INFO:root:[   76] Training loss: 0.04892846, Validation loss: 0.09750434, Gradient norm: 2.19028508
INFO:root:[   77] Training loss: 0.04767918, Validation loss: 0.10080792, Gradient norm: 2.01181604
INFO:root:[   78] Training loss: 0.04701904, Validation loss: 0.09598331, Gradient norm: 2.07602849
INFO:root:[   79] Training loss: 0.04717137, Validation loss: 0.11712787, Gradient norm: 2.23430003
INFO:root:[   80] Training loss: 0.04650193, Validation loss: 0.14101239, Gradient norm: 2.07437918
INFO:root:[   81] Training loss: 0.04610755, Validation loss: 0.13771350, Gradient norm: 1.97501111
INFO:root:[   82] Training loss: 0.04553614, Validation loss: 0.13622535, Gradient norm: 2.03491619
INFO:root:[   83] Training loss: 0.04480639, Validation loss: 0.11260264, Gradient norm: 2.19861191
INFO:root:[   84] Training loss: 0.04448421, Validation loss: 0.08548637, Gradient norm: 2.14452958
INFO:root:[   85] Training loss: 0.04408300, Validation loss: 0.13265359, Gradient norm: 2.18413947
INFO:root:[   86] Training loss: 0.04341564, Validation loss: 0.11646382, Gradient norm: 2.11698336
INFO:root:[   87] Training loss: 0.04317065, Validation loss: 0.07988613, Gradient norm: 2.08406698
INFO:root:[   88] Training loss: 0.04306181, Validation loss: 0.12301057, Gradient norm: 2.15980330
INFO:root:[   89] Training loss: 0.04199921, Validation loss: 0.10236543, Gradient norm: 2.13322766
INFO:root:[   90] Training loss: 0.04135453, Validation loss: 0.08020095, Gradient norm: 2.10266237
INFO:root:[   91] Training loss: 0.04175756, Validation loss: 0.10918256, Gradient norm: 2.19462079
INFO:root:[   92] Training loss: 0.04126992, Validation loss: 0.09892419, Gradient norm: 2.17112862
INFO:root:[   93] Training loss: 0.04013575, Validation loss: 0.10020752, Gradient norm: 2.08730393
INFO:root:[   94] Training loss: 0.04033346, Validation loss: 0.09849366, Gradient norm: 2.09204453
INFO:root:[   95] Training loss: 0.03942891, Validation loss: 0.08069839, Gradient norm: 2.03803643
INFO:root:[   96] Training loss: 0.03859371, Validation loss: 0.09226817, Gradient norm: 2.05972010
INFO:root:EP 96: Early stopping
INFO:root:Training the model took 1248.906s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.11888
INFO:root:EnergyScoreTrain: 0.06368
INFO:root:CoverageTrain: 0.60237
INFO:root:IntervalWidthTrain: 0.09086
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.09055
INFO:root:EnergyScoreValidation: 0.04223
INFO:root:CoverageValidation: 0.7823
INFO:root:IntervalWidthValidation: 0.09675
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.12408
INFO:root:EnergyScoreTest: 0.05743
INFO:root:CoverageTest: 0.71753
INFO:root:IntervalWidthTest: 0.09777
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1801453568
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.00146005, Validation loss: 0.15398661, Gradient norm: 3.61434672
INFO:root:[    2] Training loss: 0.26845039, Validation loss: 0.18322034, Gradient norm: 1.73662274
INFO:root:[    3] Training loss: 0.22333650, Validation loss: 0.34016304, Gradient norm: 1.76319417
INFO:root:[    4] Training loss: 0.19929649, Validation loss: 0.33084818, Gradient norm: 1.89069935
INFO:root:[    5] Training loss: 0.18135947, Validation loss: 0.31684986, Gradient norm: 1.57983738
INFO:root:[    6] Training loss: 0.17405111, Validation loss: 0.45074913, Gradient norm: 1.87937948
INFO:root:[    7] Training loss: 0.16878249, Validation loss: 0.42128411, Gradient norm: 1.85117131
INFO:root:[    8] Training loss: 0.16241100, Validation loss: 0.37017577, Gradient norm: 1.70929704
INFO:root:[    9] Training loss: 0.15869039, Validation loss: 0.37233218, Gradient norm: 1.75815750
INFO:root:[   10] Training loss: 0.15579091, Validation loss: 0.45325660, Gradient norm: 1.99586004
INFO:root:[   11] Training loss: 0.14980086, Validation loss: 0.40237311, Gradient norm: 1.43876128
INFO:root:[   12] Training loss: 0.14943140, Validation loss: 0.36537787, Gradient norm: 1.75652412
INFO:root:[   13] Training loss: 0.14697092, Validation loss: 0.36439331, Gradient norm: 1.93826138
INFO:root:[   14] Training loss: 0.14529909, Validation loss: 0.40781280, Gradient norm: 1.94804035
INFO:root:[   15] Training loss: 0.14322706, Validation loss: 0.46035184, Gradient norm: 2.05967390
INFO:root:[   16] Training loss: 0.13939636, Validation loss: 0.45026425, Gradient norm: 1.93133355
INFO:root:[   17] Training loss: 0.13732090, Validation loss: 0.43217337, Gradient norm: 2.00841292
INFO:root:[   18] Training loss: 0.13557168, Validation loss: 0.40575615, Gradient norm: 1.98557327
INFO:root:[   19] Training loss: 0.13354168, Validation loss: 0.34304780, Gradient norm: 2.03984623
INFO:root:[   20] Training loss: 0.13066065, Validation loss: 0.39324527, Gradient norm: 1.69911047
INFO:root:[   21] Training loss: 0.12893218, Validation loss: 0.44358444, Gradient norm: 1.75743416
INFO:root:[   22] Training loss: 0.12760175, Validation loss: 0.42854972, Gradient norm: 2.14213086
INFO:root:[   23] Training loss: 0.12477477, Validation loss: 0.35640769, Gradient norm: 1.98997522
INFO:root:[   24] Training loss: 0.12224909, Validation loss: 0.32533053, Gradient norm: 1.95411482
INFO:root:[   25] Training loss: 0.12125015, Validation loss: 0.32881452, Gradient norm: 2.22580489
INFO:root:[   26] Training loss: 0.11975354, Validation loss: 0.32210683, Gradient norm: 2.22500928
INFO:root:[   27] Training loss: 0.11684557, Validation loss: 0.32911304, Gradient norm: 2.08053150
INFO:root:[   28] Training loss: 0.11379241, Validation loss: 0.40149805, Gradient norm: 1.81882671
INFO:root:[   29] Training loss: 0.11306199, Validation loss: 0.32704270, Gradient norm: 2.01789081
INFO:root:[   30] Training loss: 0.11179970, Validation loss: 0.32003356, Gradient norm: 2.19000920
INFO:root:[   31] Training loss: 0.10894722, Validation loss: 0.37635336, Gradient norm: 1.92119432
INFO:root:[   32] Training loss: 0.10923167, Validation loss: 0.36974439, Gradient norm: 2.16999960
INFO:root:[   33] Training loss: 0.10581178, Validation loss: 0.30385177, Gradient norm: 2.03343855
INFO:root:[   34] Training loss: 0.10277965, Validation loss: 0.36822593, Gradient norm: 1.94197146
INFO:root:[   35] Training loss: 0.10181939, Validation loss: 0.28582396, Gradient norm: 2.11491757
INFO:root:[   36] Training loss: 0.10019268, Validation loss: 0.36185168, Gradient norm: 2.09744869
INFO:root:[   37] Training loss: 0.09863257, Validation loss: 0.28167731, Gradient norm: 2.06554986
INFO:root:[   38] Training loss: 0.09789156, Validation loss: 0.32728974, Gradient norm: 2.19083927
INFO:root:[   39] Training loss: 0.09491820, Validation loss: 0.27649603, Gradient norm: 1.92043384
INFO:root:[   40] Training loss: 0.09371399, Validation loss: 0.34467252, Gradient norm: 2.06069752
INFO:root:[   41] Training loss: 0.09295016, Validation loss: 0.27205784, Gradient norm: 2.14972272
INFO:root:[   42] Training loss: 0.09176112, Validation loss: 0.31004840, Gradient norm: 2.25132978
INFO:root:[   43] Training loss: 0.08959231, Validation loss: 0.28857642, Gradient norm: 2.07336406
INFO:root:[   44] Training loss: 0.08800102, Validation loss: 0.29847770, Gradient norm: 2.03721938
INFO:root:[   45] Training loss: 0.08631446, Validation loss: 0.25024848, Gradient norm: 2.01005281
INFO:root:[   46] Training loss: 0.08542810, Validation loss: 0.30668949, Gradient norm: 2.13032575
INFO:root:[   47] Training loss: 0.08485028, Validation loss: 0.23875340, Gradient norm: 2.00073359
INFO:root:[   48] Training loss: 0.08393696, Validation loss: 0.30729181, Gradient norm: 1.96755356
INFO:root:[   49] Training loss: 0.08146956, Validation loss: 0.23879555, Gradient norm: 2.09231795
INFO:root:[   50] Training loss: 0.07949943, Validation loss: 0.24819244, Gradient norm: 1.96725053
INFO:root:[   51] Training loss: 0.07890499, Validation loss: 0.28966957, Gradient norm: 2.03517235
INFO:root:[   52] Training loss: 0.07671370, Validation loss: 0.25698876, Gradient norm: 1.97694373
INFO:root:[   53] Training loss: 0.07545622, Validation loss: 0.21602612, Gradient norm: 1.98816801
INFO:root:[   54] Training loss: 0.07518648, Validation loss: 0.23582566, Gradient norm: 2.19649223
INFO:root:[   55] Training loss: 0.07439218, Validation loss: 0.22753404, Gradient norm: 2.25801838
INFO:root:[   56] Training loss: 0.07262926, Validation loss: 0.25091030, Gradient norm: 2.17844897
INFO:root:[   57] Training loss: 0.07105021, Validation loss: 0.23250077, Gradient norm: 2.13119378
INFO:root:[   58] Training loss: 0.06994690, Validation loss: 0.19724010, Gradient norm: 2.03499694
INFO:root:[   59] Training loss: 0.06864900, Validation loss: 0.25290267, Gradient norm: 2.07663646
INFO:root:[   60] Training loss: 0.06782535, Validation loss: 0.18212191, Gradient norm: 2.14705801
INFO:root:[   61] Training loss: 0.06802261, Validation loss: 0.23283029, Gradient norm: 2.25421497
INFO:root:[   62] Training loss: 0.06597692, Validation loss: 0.19466762, Gradient norm: 2.18271485
INFO:root:[   63] Training loss: 0.06460735, Validation loss: 0.20036496, Gradient norm: 2.06850518
INFO:root:[   64] Training loss: 0.06388146, Validation loss: 0.20887946, Gradient norm: 2.05202628
INFO:root:[   65] Training loss: 0.06206714, Validation loss: 0.19399524, Gradient norm: 2.01102631
INFO:root:[   66] Training loss: 0.06156861, Validation loss: 0.21750845, Gradient norm: 2.12673016
INFO:root:[   67] Training loss: 0.06082951, Validation loss: 0.15608759, Gradient norm: 2.13163019
INFO:root:[   68] Training loss: 0.05932594, Validation loss: 0.21146084, Gradient norm: 2.05471014
INFO:root:[   69] Training loss: 0.05852840, Validation loss: 0.15157450, Gradient norm: 1.96130680
INFO:root:[   70] Training loss: 0.05793824, Validation loss: 0.18161519, Gradient norm: 2.23103681
INFO:root:[   71] Training loss: 0.05657986, Validation loss: 0.18210202, Gradient norm: 2.04880924
INFO:root:[   72] Training loss: 0.05506871, Validation loss: 0.16929988, Gradient norm: 2.04345952
INFO:root:[   73] Training loss: 0.05495378, Validation loss: 0.19173899, Gradient norm: 2.08459390
INFO:root:[   74] Training loss: 0.05411001, Validation loss: 0.13465946, Gradient norm: 2.04762841
INFO:root:[   75] Training loss: 0.05356999, Validation loss: 0.17564455, Gradient norm: 2.08350293
INFO:root:[   76] Training loss: 0.05272808, Validation loss: 0.17407498, Gradient norm: 1.96654711
INFO:root:[   77] Training loss: 0.05241800, Validation loss: 0.14407401, Gradient norm: 2.12724375
INFO:root:[   78] Training loss: 0.05095768, Validation loss: 0.12363679, Gradient norm: 2.04735596
INFO:root:[   79] Training loss: 0.05036918, Validation loss: 0.15286701, Gradient norm: 1.99966743
INFO:root:[   80] Training loss: 0.04784405, Validation loss: 0.12571498, Gradient norm: 1.84467166
INFO:root:[   81] Training loss: 0.04780140, Validation loss: 0.14611372, Gradient norm: 2.02327026
INFO:root:[   82] Training loss: 0.04705247, Validation loss: 0.14107432, Gradient norm: 1.95529971
INFO:root:[   83] Training loss: 0.04593258, Validation loss: 0.13369780, Gradient norm: 1.93475143
INFO:root:[   84] Training loss: 0.04598343, Validation loss: 0.14974763, Gradient norm: 1.91414531
INFO:root:[   85] Training loss: 0.04510675, Validation loss: 0.10130404, Gradient norm: 1.95950486
INFO:root:[   86] Training loss: 0.04479877, Validation loss: 0.10306329, Gradient norm: 2.07398393
INFO:root:[   87] Training loss: 0.04390437, Validation loss: 0.12076059, Gradient norm: 2.04635504
INFO:root:[   88] Training loss: 0.04252370, Validation loss: 0.12959036, Gradient norm: 1.81823883
INFO:root:[   89] Training loss: 0.04343950, Validation loss: 0.14686090, Gradient norm: 2.13001861
INFO:root:[   90] Training loss: 0.04141355, Validation loss: 0.09094882, Gradient norm: 1.88409454
INFO:root:[   91] Training loss: 0.04108229, Validation loss: 0.09258230, Gradient norm: 1.86162479
INFO:root:[   92] Training loss: 0.04077874, Validation loss: 0.10106416, Gradient norm: 1.96271503
INFO:root:[   93] Training loss: 0.04061290, Validation loss: 0.10469586, Gradient norm: 1.93862324
INFO:root:[   94] Training loss: 0.04015348, Validation loss: 0.07559667, Gradient norm: 1.99366945
INFO:root:[   95] Training loss: 0.03880465, Validation loss: 0.07932400, Gradient norm: 1.92461477
INFO:root:[   96] Training loss: 0.03809738, Validation loss: 0.09860186, Gradient norm: 1.71431951
INFO:root:[   97] Training loss: 0.03880498, Validation loss: 0.07154382, Gradient norm: 1.90640249
INFO:root:[   98] Training loss: 0.03731655, Validation loss: 0.07307962, Gradient norm: 1.87150278
INFO:root:[   99] Training loss: 0.03839716, Validation loss: 0.10494204, Gradient norm: 1.96998716
INFO:root:[  100] Training loss: 0.03778266, Validation loss: 0.06777003, Gradient norm: 1.99285629
INFO:root:[  101] Training loss: 0.03704856, Validation loss: 0.10156431, Gradient norm: 1.91287607
INFO:root:[  102] Training loss: 0.03572716, Validation loss: 0.09589417, Gradient norm: 1.75135695
INFO:root:[  103] Training loss: 0.03530420, Validation loss: 0.06481390, Gradient norm: 1.66121568
INFO:root:[  104] Training loss: 0.03471033, Validation loss: 0.05725068, Gradient norm: 1.66675037
INFO:root:[  105] Training loss: 0.03413930, Validation loss: 0.06889580, Gradient norm: 1.80667276
INFO:root:[  106] Training loss: 0.03361203, Validation loss: 0.09428701, Gradient norm: 1.69936236
INFO:root:[  107] Training loss: 0.03433344, Validation loss: 0.07436155, Gradient norm: 1.67306909
INFO:root:[  108] Training loss: 0.03603816, Validation loss: 0.08380032, Gradient norm: 1.91732803
INFO:root:[  109] Training loss: 0.03284909, Validation loss: 0.04732836, Gradient norm: 1.63580645
INFO:root:[  110] Training loss: 0.03344732, Validation loss: 0.08018772, Gradient norm: 1.64528816
INFO:root:[  111] Training loss: 0.03302583, Validation loss: 0.04296085, Gradient norm: 1.64291511
INFO:root:[  112] Training loss: 0.03340655, Validation loss: 0.07541427, Gradient norm: 1.68451379
INFO:root:[  113] Training loss: 0.03262935, Validation loss: 0.06357972, Gradient norm: 1.75470697
INFO:root:[  114] Training loss: 0.03265284, Validation loss: 0.05486446, Gradient norm: 1.65331210
INFO:root:[  115] Training loss: 0.03097933, Validation loss: 0.05588891, Gradient norm: 1.54510604
INFO:root:[  116] Training loss: 0.03130764, Validation loss: 0.03601572, Gradient norm: 1.53388737
INFO:root:[  117] Training loss: 0.03068050, Validation loss: 0.07067489, Gradient norm: 1.50773925
INFO:root:[  118] Training loss: 0.03300648, Validation loss: 0.06192548, Gradient norm: 1.69586960
INFO:root:[  119] Training loss: 0.03145656, Validation loss: 0.03427287, Gradient norm: 1.52275170
INFO:root:[  120] Training loss: 0.03046544, Validation loss: 0.05729954, Gradient norm: 1.47282873
INFO:root:[  121] Training loss: 0.03247829, Validation loss: 0.02964684, Gradient norm: 1.66967820
INFO:root:[  122] Training loss: 0.03307392, Validation loss: 0.07886860, Gradient norm: 1.75087865
INFO:root:[  123] Training loss: 0.03270277, Validation loss: 0.06836097, Gradient norm: 1.45711458
INFO:root:[  124] Training loss: 0.03038724, Validation loss: 0.06834704, Gradient norm: 1.52748211
INFO:root:[  125] Training loss: 0.03124239, Validation loss: 0.03342748, Gradient norm: 1.67312693
INFO:root:[  126] Training loss: 0.03120516, Validation loss: 0.03341379, Gradient norm: 1.47169493
INFO:root:[  127] Training loss: 0.03069043, Validation loss: 0.05398800, Gradient norm: 1.45114513
INFO:root:[  128] Training loss: 0.03001849, Validation loss: 0.03078924, Gradient norm: 1.36311566
INFO:root:[  129] Training loss: 0.03080272, Validation loss: 0.05929361, Gradient norm: 1.55934691
INFO:root:[  130] Training loss: 0.03080283, Validation loss: 0.06277973, Gradient norm: 1.52648560
INFO:root:[  131] Training loss: 0.03125044, Validation loss: 0.02912716, Gradient norm: 1.56620453
INFO:root:[  132] Training loss: 0.03080169, Validation loss: 0.06750788, Gradient norm: 1.54095895
INFO:root:[  133] Training loss: 0.03097069, Validation loss: 0.03819205, Gradient norm: 1.55608273
INFO:root:[  134] Training loss: 0.03016192, Validation loss: 0.06503549, Gradient norm: 1.52021292
INFO:root:[  135] Training loss: 0.03021497, Validation loss: 0.04330176, Gradient norm: 1.52682573
INFO:root:[  136] Training loss: 0.03024496, Validation loss: 0.06134240, Gradient norm: 1.53458971
INFO:root:[  137] Training loss: 0.03027862, Validation loss: 0.05777559, Gradient norm: 1.41234007
INFO:root:[  138] Training loss: 0.02984655, Validation loss: 0.03459745, Gradient norm: 1.48789761
INFO:root:[  139] Training loss: 0.02973627, Validation loss: 0.05863890, Gradient norm: 1.45743885
INFO:root:[  140] Training loss: 0.03021100, Validation loss: 0.04339333, Gradient norm: 1.47950996
INFO:root:EP 140: Early stopping
INFO:root:Training the model took 1808.243s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0801
INFO:root:EnergyScoreTrain: 0.04079
INFO:root:CoverageTrain: 0.71883
INFO:root:IntervalWidthTrain: 0.0442
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08772
INFO:root:EnergyScoreValidation: 0.04577
INFO:root:CoverageValidation: 0.67942
INFO:root:IntervalWidthValidation: 0.04223
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.11624
INFO:root:EnergyScoreTest: 0.0649
INFO:root:CoverageTest: 0.73691
INFO:root:IntervalWidthTest: 0.04994
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1845493760
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.15243549, Validation loss: 0.13645822, Gradient norm: 4.22381402
INFO:root:[    2] Training loss: 0.29299343, Validation loss: 0.16540079, Gradient norm: 1.84601273
INFO:root:[    3] Training loss: 0.24333748, Validation loss: 0.27223957, Gradient norm: 1.12613204
INFO:root:[    4] Training loss: 0.21612152, Validation loss: 0.33484484, Gradient norm: 1.09562899
INFO:root:[    5] Training loss: 0.20508844, Validation loss: 0.34272229, Gradient norm: 1.45096132
INFO:root:[    6] Training loss: 0.19307637, Validation loss: 0.41325226, Gradient norm: 1.51459033
INFO:root:[    7] Training loss: 0.18535128, Validation loss: 0.38869968, Gradient norm: 1.25831144
INFO:root:[    8] Training loss: 0.17869132, Validation loss: 0.40562749, Gradient norm: 1.40074603
INFO:root:[    9] Training loss: 0.17401822, Validation loss: 0.33319731, Gradient norm: 1.46774711
INFO:root:[   10] Training loss: 0.16989243, Validation loss: 0.36093417, Gradient norm: 1.55623435
INFO:root:[   11] Training loss: 0.16664854, Validation loss: 0.41684347, Gradient norm: 1.62534767
INFO:root:[   12] Training loss: 0.16558024, Validation loss: 0.42537006, Gradient norm: 1.78472906
INFO:root:[   13] Training loss: 0.15971842, Validation loss: 0.43273692, Gradient norm: 1.53840396
INFO:root:[   14] Training loss: 0.15552409, Validation loss: 0.37209051, Gradient norm: 1.45355527
INFO:root:[   15] Training loss: 0.15222420, Validation loss: 0.40785127, Gradient norm: 1.44104585
INFO:root:[   16] Training loss: 0.14997935, Validation loss: 0.42888415, Gradient norm: 1.53718963
INFO:root:[   17] Training loss: 0.14569262, Validation loss: 0.35607778, Gradient norm: 1.33699853
INFO:root:[   18] Training loss: 0.14427247, Validation loss: 0.43332124, Gradient norm: 1.59136473
INFO:root:[   19] Training loss: 0.14220026, Validation loss: 0.33424419, Gradient norm: 1.54028636
INFO:root:[   20] Training loss: 0.13864860, Validation loss: 0.40601305, Gradient norm: 1.50033314
INFO:root:[   21] Training loss: 0.13697665, Validation loss: 0.32623945, Gradient norm: 1.62940702
INFO:root:[   22] Training loss: 0.13408357, Validation loss: 0.40941981, Gradient norm: 1.62616395
INFO:root:[   23] Training loss: 0.13047913, Validation loss: 0.32515960, Gradient norm: 1.40084919
INFO:root:[   24] Training loss: 0.12888111, Validation loss: 0.38884616, Gradient norm: 1.66844915
INFO:root:[   25] Training loss: 0.12658475, Validation loss: 0.34360639, Gradient norm: 1.67231845
INFO:root:[   26] Training loss: 0.12589477, Validation loss: 0.39602963, Gradient norm: 1.78841028
INFO:root:[   27] Training loss: 0.12294509, Validation loss: 0.33335932, Gradient norm: 1.63789854
INFO:root:[   28] Training loss: 0.11980046, Validation loss: 0.34781728, Gradient norm: 1.66494673
INFO:root:[   29] Training loss: 0.11828269, Validation loss: 0.33576779, Gradient norm: 1.67509387
INFO:root:[   30] Training loss: 0.11462869, Validation loss: 0.35547148, Gradient norm: 1.42402150
INFO:root:[   31] Training loss: 0.11393277, Validation loss: 0.28188834, Gradient norm: 1.68211253
INFO:root:[   32] Training loss: 0.11128330, Validation loss: 0.35710903, Gradient norm: 1.73833536
INFO:root:[   33] Training loss: 0.10905463, Validation loss: 0.27992703, Gradient norm: 1.67055191
INFO:root:[   34] Training loss: 0.10783054, Validation loss: 0.35484276, Gradient norm: 1.68187955
INFO:root:[   35] Training loss: 0.10514797, Validation loss: 0.26572755, Gradient norm: 1.69892207
INFO:root:[   36] Training loss: 0.10287742, Validation loss: 0.32744609, Gradient norm: 1.62877661
INFO:root:[   37] Training loss: 0.10053160, Validation loss: 0.25737214, Gradient norm: 1.53576550
INFO:root:[   38] Training loss: 0.10067652, Validation loss: 0.28968967, Gradient norm: 1.89311821
INFO:root:[   39] Training loss: 0.09809485, Validation loss: 0.31445058, Gradient norm: 1.77200092
INFO:root:[   40] Training loss: 0.09605002, Validation loss: 0.23847625, Gradient norm: 1.70928486
INFO:root:[   41] Training loss: 0.09404608, Validation loss: 0.31090919, Gradient norm: 1.70437105
INFO:root:[   42] Training loss: 0.09306488, Validation loss: 0.25705748, Gradient norm: 1.68287137
INFO:root:[   43] Training loss: 0.09325355, Validation loss: 0.29132171, Gradient norm: 1.90447611
INFO:root:[   44] Training loss: 0.08998862, Validation loss: 0.25543694, Gradient norm: 1.82586661
INFO:root:[   45] Training loss: 0.08582706, Validation loss: 0.27830363, Gradient norm: 1.63490624
INFO:root:[   46] Training loss: 0.08551548, Validation loss: 0.25328012, Gradient norm: 1.86805863
INFO:root:[   47] Training loss: 0.08404951, Validation loss: 0.20836063, Gradient norm: 1.85181284
INFO:root:[   48] Training loss: 0.08043209, Validation loss: 0.22634492, Gradient norm: 1.64618492
INFO:root:[   49] Training loss: 0.08066753, Validation loss: 0.23372911, Gradient norm: 1.84991888
INFO:root:[   50] Training loss: 0.07733257, Validation loss: 0.18939467, Gradient norm: 1.65893109
INFO:root:[   51] Training loss: 0.07676312, Validation loss: 0.26220548, Gradient norm: 1.82875111
INFO:root:[   52] Training loss: 0.07494605, Validation loss: 0.20675452, Gradient norm: 1.76610332
INFO:root:[   53] Training loss: 0.07347378, Validation loss: 0.25640700, Gradient norm: 1.88704661
INFO:root:[   54] Training loss: 0.07211384, Validation loss: 0.18358305, Gradient norm: 1.86997156
INFO:root:[   55] Training loss: 0.07003257, Validation loss: 0.20273886, Gradient norm: 1.76789075
INFO:root:[   56] Training loss: 0.06926057, Validation loss: 0.23462807, Gradient norm: 1.74206960
INFO:root:[   57] Training loss: 0.06725919, Validation loss: 0.18885153, Gradient norm: 1.78110887
INFO:root:[   58] Training loss: 0.06491930, Validation loss: 0.16557787, Gradient norm: 1.73981347
INFO:root:[   59] Training loss: 0.06396577, Validation loss: 0.18129852, Gradient norm: 1.79230968
INFO:root:[   60] Training loss: 0.06100725, Validation loss: 0.15829443, Gradient norm: 1.51172223
INFO:root:[   61] Training loss: 0.06083475, Validation loss: 0.15943216, Gradient norm: 1.77509245
INFO:root:[   62] Training loss: 0.05895684, Validation loss: 0.15297941, Gradient norm: 1.69989424
INFO:root:[   63] Training loss: 0.05964769, Validation loss: 0.18742663, Gradient norm: 1.97634480
INFO:root:[   64] Training loss: 0.05774434, Validation loss: 0.19718786, Gradient norm: 1.84577441
INFO:root:[   65] Training loss: 0.05606432, Validation loss: 0.17726438, Gradient norm: 1.79377800
INFO:root:[   66] Training loss: 0.05427779, Validation loss: 0.18259956, Gradient norm: 1.67064051
INFO:root:[   67] Training loss: 0.05490569, Validation loss: 0.12600619, Gradient norm: 1.84936132
INFO:root:[   68] Training loss: 0.05251548, Validation loss: 0.16083380, Gradient norm: 1.59746924
INFO:root:[   69] Training loss: 0.05261158, Validation loss: 0.12231746, Gradient norm: 1.92319771
INFO:root:[   70] Training loss: 0.05073082, Validation loss: 0.11707589, Gradient norm: 1.68039996
INFO:root:[   71] Training loss: 0.04956961, Validation loss: 0.11102365, Gradient norm: 1.68121954
INFO:root:[   72] Training loss: 0.05087464, Validation loss: 0.12468156, Gradient norm: 1.59355462
INFO:root:[   73] Training loss: 0.04850464, Validation loss: 0.11306726, Gradient norm: 1.70552320
INFO:root:[   74] Training loss: 0.04653130, Validation loss: 0.10726326, Gradient norm: 1.83287771
INFO:root:[   75] Training loss: 0.04647131, Validation loss: 0.09578989, Gradient norm: 1.73851227
INFO:root:[   76] Training loss: 0.04549096, Validation loss: 0.11822049, Gradient norm: 1.73849466
INFO:root:[   77] Training loss: 0.04497996, Validation loss: 0.13523545, Gradient norm: 1.67220187
INFO:root:[   78] Training loss: 0.04409888, Validation loss: 0.11998686, Gradient norm: 1.74459524
INFO:root:[   79] Training loss: 0.04395305, Validation loss: 0.08030325, Gradient norm: 1.67799106
INFO:root:[   80] Training loss: 0.04369690, Validation loss: 0.11692058, Gradient norm: 1.67940789
INFO:root:[   81] Training loss: 0.04103178, Validation loss: 0.10922005, Gradient norm: 1.49599558
INFO:root:[   82] Training loss: 0.04171832, Validation loss: 0.07137086, Gradient norm: 1.61842876
INFO:root:[   83] Training loss: 0.04119625, Validation loss: 0.07640738, Gradient norm: 1.46090939
INFO:root:[   84] Training loss: 0.04177290, Validation loss: 0.06342357, Gradient norm: 1.73958620
INFO:root:[   85] Training loss: 0.03986215, Validation loss: 0.10703323, Gradient norm: 1.57044832
INFO:root:[   86] Training loss: 0.04019138, Validation loss: 0.06879987, Gradient norm: 1.57579138
INFO:root:[   87] Training loss: 0.03954878, Validation loss: 0.10433281, Gradient norm: 1.43555546
INFO:root:[   88] Training loss: 0.03874317, Validation loss: 0.05318351, Gradient norm: 1.47953767
INFO:root:[   89] Training loss: 0.04003795, Validation loss: 0.06516158, Gradient norm: 1.70635067
INFO:root:[   90] Training loss: 0.03798426, Validation loss: 0.05502089, Gradient norm: 1.42487464
INFO:root:[   91] Training loss: 0.03829073, Validation loss: 0.08607144, Gradient norm: 1.24151360
INFO:root:[   92] Training loss: 0.03864697, Validation loss: 0.06960436, Gradient norm: 1.49509135
INFO:root:[   93] Training loss: 0.03629018, Validation loss: 0.07988112, Gradient norm: 1.28750720
INFO:root:[   94] Training loss: 0.03807184, Validation loss: 0.08031747, Gradient norm: 1.36665927
INFO:root:[   95] Training loss: 0.03676882, Validation loss: 0.08041571, Gradient norm: 1.22862208
INFO:root:[   96] Training loss: 0.03754557, Validation loss: 0.06374508, Gradient norm: 1.43456007
INFO:root:[   97] Training loss: 0.03693749, Validation loss: 0.05415548, Gradient norm: 1.31805615
INFO:root:[   98] Training loss: 0.03704654, Validation loss: 0.04577534, Gradient norm: 1.35862582
INFO:root:[   99] Training loss: 0.03666176, Validation loss: 0.05677269, Gradient norm: 1.35945046
INFO:root:[  100] Training loss: 0.03631840, Validation loss: 0.07758079, Gradient norm: 1.24125131
INFO:root:[  101] Training loss: 0.03542681, Validation loss: 0.07737919, Gradient norm: 1.18845330
INFO:root:[  102] Training loss: 0.03553166, Validation loss: 0.05221865, Gradient norm: 1.32419135
INFO:root:[  103] Training loss: 0.03455924, Validation loss: 0.06275839, Gradient norm: 1.08979263
INFO:root:[  104] Training loss: 0.03548195, Validation loss: 0.07873208, Gradient norm: 1.26365212
INFO:root:[  105] Training loss: 0.03664063, Validation loss: 0.05783115, Gradient norm: 1.19257017
INFO:root:[  106] Training loss: 0.03510383, Validation loss: 0.05621036, Gradient norm: 1.25352199
INFO:root:[  107] Training loss: 0.03518965, Validation loss: 0.05443330, Gradient norm: 1.23359571
INFO:root:EP 107: Early stopping
INFO:root:Training the model took 1388.54s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.08015
INFO:root:EnergyScoreTrain: 0.04254
INFO:root:CoverageTrain: 0.56767
INFO:root:IntervalWidthTrain: 0.03994
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08537
INFO:root:EnergyScoreValidation: 0.04293
INFO:root:CoverageValidation: 0.59227
INFO:root:IntervalWidthValidation: 0.04153
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.10426
INFO:root:EnergyScoreTest: 0.06476
INFO:root:CoverageTest: 0.3456
INFO:root:IntervalWidthTest: 0.04062
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1845493760
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.10582317, Validation loss: 0.44630810, Gradient norm: 3.70811008
INFO:root:[    2] Training loss: 0.35712764, Validation loss: 0.63549731, Gradient norm: 1.88688297
INFO:root:[    3] Training loss: 0.28484155, Validation loss: 0.80332109, Gradient norm: 1.10233738
INFO:root:[    4] Training loss: 0.26238991, Validation loss: 0.86659345, Gradient norm: 1.48862441
INFO:root:[    5] Training loss: 0.24251085, Validation loss: 0.86600501, Gradient norm: 0.98271548
INFO:root:[    6] Training loss: 0.23166720, Validation loss: 0.92178171, Gradient norm: 1.52704188
INFO:root:[    7] Training loss: 0.22294060, Validation loss: 0.89853176, Gradient norm: 1.54972014
INFO:root:[    8] Training loss: 0.21275995, Validation loss: 0.82541779, Gradient norm: 1.13657326
INFO:root:[    9] Training loss: 0.20922852, Validation loss: 0.83838319, Gradient norm: 1.76684198
INFO:root:[   10] Training loss: 0.20058832, Validation loss: 0.84800859, Gradient norm: 1.57378413
INFO:root:[   11] Training loss: 0.19339642, Validation loss: 0.88920738, Gradient norm: 1.44291191
INFO:root:[   12] Training loss: 0.19138649, Validation loss: 0.85645289, Gradient norm: 2.18838768
INFO:root:[   13] Training loss: 0.18147906, Validation loss: 0.81621241, Gradient norm: 1.57264514
INFO:root:[   14] Training loss: 0.17742110, Validation loss: 0.77720938, Gradient norm: 1.86256245
INFO:root:[   15] Training loss: 0.17307377, Validation loss: 0.84788066, Gradient norm: 2.00299669
INFO:root:[   16] Training loss: 0.16679197, Validation loss: 0.76494531, Gradient norm: 1.81030237
INFO:root:[   17] Training loss: 0.16262730, Validation loss: 0.72761953, Gradient norm: 1.94564861
INFO:root:[   18] Training loss: 0.15753192, Validation loss: 0.79662632, Gradient norm: 1.93893593
INFO:root:[   19] Training loss: 0.15247097, Validation loss: 0.77265099, Gradient norm: 1.85006749
INFO:root:[   20] Training loss: 0.14887201, Validation loss: 0.76717183, Gradient norm: 1.92240103
INFO:root:[   21] Training loss: 0.14420129, Validation loss: 0.74209873, Gradient norm: 1.98010933
INFO:root:[   22] Training loss: 0.13988295, Validation loss: 0.71333399, Gradient norm: 1.94358451
INFO:root:[   23] Training loss: 0.13650911, Validation loss: 0.71898558, Gradient norm: 2.09014813
INFO:root:[   24] Training loss: 0.13271340, Validation loss: 0.69882670, Gradient norm: 2.15412869
INFO:root:[   25] Training loss: 0.12818480, Validation loss: 0.67461167, Gradient norm: 2.12345198
INFO:root:[   26] Training loss: 0.12365056, Validation loss: 0.59420094, Gradient norm: 1.83711875
INFO:root:[   27] Training loss: 0.12059738, Validation loss: 0.62690223, Gradient norm: 2.05028132
INFO:root:[   28] Training loss: 0.11780945, Validation loss: 0.63681113, Gradient norm: 2.14988973
INFO:root:[   29] Training loss: 0.11390354, Validation loss: 0.61413915, Gradient norm: 2.18016256
INFO:root:[   30] Training loss: 0.11123305, Validation loss: 0.56144041, Gradient norm: 2.25607278
INFO:root:[   31] Training loss: 0.10578848, Validation loss: 0.58215840, Gradient norm: 1.77851505
INFO:root:[   32] Training loss: 0.10369045, Validation loss: 0.50520729, Gradient norm: 2.09470566
INFO:root:[   33] Training loss: 0.10071683, Validation loss: 0.52623593, Gradient norm: 2.15563802
INFO:root:[   34] Training loss: 0.09718889, Validation loss: 0.54225138, Gradient norm: 2.17802598
INFO:root:[   35] Training loss: 0.09415194, Validation loss: 0.47113975, Gradient norm: 2.14677712
INFO:root:[   36] Training loss: 0.09072314, Validation loss: 0.50636113, Gradient norm: 2.09462640
INFO:root:[   37] Training loss: 0.08877030, Validation loss: 0.42558894, Gradient norm: 2.22882737
INFO:root:[   38] Training loss: 0.08553950, Validation loss: 0.43737974, Gradient norm: 2.21096065
INFO:root:[   39] Training loss: 0.08260131, Validation loss: 0.46017951, Gradient norm: 2.19739174
INFO:root:[   40] Training loss: 0.07968424, Validation loss: 0.40215295, Gradient norm: 2.02934828
INFO:root:[   41] Training loss: 0.07699134, Validation loss: 0.38319926, Gradient norm: 2.01277024
INFO:root:[   42] Training loss: 0.07622227, Validation loss: 0.40721776, Gradient norm: 2.29753563
INFO:root:[   43] Training loss: 0.07263106, Validation loss: 0.33256173, Gradient norm: 2.07592585
INFO:root:[   44] Training loss: 0.07218532, Validation loss: 0.32266803, Gradient norm: 2.47713155
INFO:root:[   45] Training loss: 0.06820651, Validation loss: 0.31090011, Gradient norm: 1.90174071
INFO:root:[   46] Training loss: 0.06698801, Validation loss: 0.30488801, Gradient norm: 1.96541296
INFO:root:[   47] Training loss: 0.06472056, Validation loss: 0.32469970, Gradient norm: 2.05450509
INFO:root:[   48] Training loss: 0.06267820, Validation loss: 0.30580519, Gradient norm: 2.11033225
INFO:root:[   49] Training loss: 0.06220803, Validation loss: 0.27141136, Gradient norm: 2.36837129
INFO:root:[   50] Training loss: 0.06021625, Validation loss: 0.23059108, Gradient norm: 2.19814934
INFO:root:[   51] Training loss: 0.05948086, Validation loss: 0.20458900, Gradient norm: 2.20314641
INFO:root:[   52] Training loss: 0.05870446, Validation loss: 0.20937926, Gradient norm: 2.32122628
INFO:root:[   53] Training loss: 0.05636544, Validation loss: 0.18146221, Gradient norm: 2.01588907
INFO:root:[   54] Training loss: 0.05600753, Validation loss: 0.19729602, Gradient norm: 2.34078801
INFO:root:[   55] Training loss: 0.05672562, Validation loss: 0.17425434, Gradient norm: 2.57414120
INFO:root:[   56] Training loss: 0.05518256, Validation loss: 0.15916426, Gradient norm: 2.21071077
INFO:root:[   57] Training loss: 0.05499468, Validation loss: 0.12673918, Gradient norm: 2.59249880
INFO:root:[   58] Training loss: 0.05664041, Validation loss: 0.11766312, Gradient norm: 2.67487748
INFO:root:[   59] Training loss: 0.05264179, Validation loss: 0.11512199, Gradient norm: 2.20050317
INFO:root:[   60] Training loss: 0.05188561, Validation loss: 0.12480455, Gradient norm: 2.63737434
INFO:root:[   61] Training loss: 0.05228113, Validation loss: 0.12742649, Gradient norm: 2.99114496
INFO:root:[   62] Training loss: 0.05327667, Validation loss: 0.10949859, Gradient norm: 2.95638673
INFO:root:[   63] Training loss: 0.05427211, Validation loss: 0.10265716, Gradient norm: 3.14457462
INFO:root:[   64] Training loss: 0.05286001, Validation loss: 0.10089609, Gradient norm: 2.85036455
INFO:root:[   65] Training loss: 0.05568546, Validation loss: 0.08092298, Gradient norm: 3.49742442
INFO:root:[   66] Training loss: 0.05428485, Validation loss: 0.06142955, Gradient norm: 2.62763949
INFO:root:[   67] Training loss: 0.05474098, Validation loss: 0.07584163, Gradient norm: 3.39149263
INFO:root:[   68] Training loss: 0.05593647, Validation loss: 0.04790425, Gradient norm: 3.50173171
INFO:root:[   69] Training loss: 0.05523886, Validation loss: 0.04138301, Gradient norm: 3.53894283
INFO:root:[   70] Training loss: 0.05517935, Validation loss: 0.05862811, Gradient norm: 3.56934750
INFO:root:[   71] Training loss: 0.05558484, Validation loss: 0.08027138, Gradient norm: 3.84460827
INFO:root:[   72] Training loss: 0.05760102, Validation loss: 0.04796675, Gradient norm: 3.72584821
INFO:root:[   73] Training loss: 0.05482680, Validation loss: 0.04884469, Gradient norm: 3.89444514
INFO:root:[   74] Training loss: 0.05523025, Validation loss: 0.04758236, Gradient norm: 3.67548317
INFO:root:[   75] Training loss: 0.05484898, Validation loss: 0.03552872, Gradient norm: 3.72268928
INFO:root:[   76] Training loss: 0.05751361, Validation loss: 0.03745874, Gradient norm: 4.43853701
INFO:root:[   77] Training loss: 0.05631519, Validation loss: 0.06189447, Gradient norm: 3.48670472
INFO:root:[   78] Training loss: 0.05415503, Validation loss: 0.04246898, Gradient norm: 3.91002041
INFO:root:[   79] Training loss: 0.05472571, Validation loss: 0.05097999, Gradient norm: 4.18895335
INFO:root:[   80] Training loss: 0.05431440, Validation loss: 0.06209541, Gradient norm: 4.17276298
INFO:root:[   81] Training loss: 0.05381167, Validation loss: 0.05143280, Gradient norm: 3.88796812
INFO:root:[   82] Training loss: 0.05627768, Validation loss: 0.06266282, Gradient norm: 4.05375221
INFO:root:[   83] Training loss: 0.05456534, Validation loss: 0.06860332, Gradient norm: 4.11097950
INFO:root:[   84] Training loss: 0.05399171, Validation loss: 0.05965833, Gradient norm: 4.32349529
INFO:root:[   85] Training loss: 0.05342215, Validation loss: 0.03282512, Gradient norm: 4.21629183
INFO:root:[   86] Training loss: 0.05724483, Validation loss: 0.05356197, Gradient norm: 4.73352787
INFO:root:[   87] Training loss: 0.05610635, Validation loss: 0.03407971, Gradient norm: 3.90425200
INFO:root:[   88] Training loss: 0.05428961, Validation loss: 0.04491585, Gradient norm: 4.66502844
INFO:root:[   89] Training loss: 0.05312893, Validation loss: 0.06108265, Gradient norm: 3.65940621
INFO:root:[   90] Training loss: 0.05317341, Validation loss: 0.07018970, Gradient norm: 3.99788341
INFO:root:[   91] Training loss: 0.05402379, Validation loss: 0.07306397, Gradient norm: 4.41218353
INFO:root:[   92] Training loss: 0.05627689, Validation loss: 0.06290925, Gradient norm: 4.79693054
INFO:root:[   93] Training loss: 0.05321062, Validation loss: 0.05093510, Gradient norm: 4.25155425
INFO:root:[   94] Training loss: 0.05302835, Validation loss: 0.04884582, Gradient norm: 4.30804616
INFO:root:EP 94: Early stopping
INFO:root:Training the model took 1221.472s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06038
INFO:root:EnergyScoreTrain: 0.03108
INFO:root:CoverageTrain: 0.69356
INFO:root:IntervalWidthTrain: 0.02741
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06069
INFO:root:EnergyScoreValidation: 0.03189
INFO:root:CoverageValidation: 0.70474
INFO:root:IntervalWidthValidation: 0.02973
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.05968
INFO:root:EnergyScoreTest: 0.03179
INFO:root:CoverageTest: 0.56333
INFO:root:IntervalWidthTest: 0.02421
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1845493760
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.54274390, Validation loss: 0.15248719, Gradient norm: 4.87565234
INFO:root:[    2] Training loss: 0.14495661, Validation loss: 0.12472493, Gradient norm: 2.75438571
INFO:root:[    3] Training loss: 0.11593556, Validation loss: 0.10617088, Gradient norm: 3.06257204
INFO:root:[    4] Training loss: 0.10147596, Validation loss: 0.08911159, Gradient norm: 3.15106876
INFO:root:[    5] Training loss: 0.08917269, Validation loss: 0.08223973, Gradient norm: 2.75768477
INFO:root:[    6] Training loss: 0.08130992, Validation loss: 0.09512451, Gradient norm: 2.31973824
INFO:root:[    7] Training loss: 0.07819772, Validation loss: 0.07264712, Gradient norm: 2.56404051
INFO:root:[    8] Training loss: 0.07634830, Validation loss: 0.07029335, Gradient norm: 2.48141086
INFO:root:[    9] Training loss: 0.07223368, Validation loss: 0.06945309, Gradient norm: 2.35912977
INFO:root:[   10] Training loss: 0.07482920, Validation loss: 0.07076119, Gradient norm: 2.88987269
INFO:root:[   11] Training loss: 0.07125007, Validation loss: 0.06279962, Gradient norm: 2.54427877
INFO:root:[   12] Training loss: 0.06916424, Validation loss: 0.07076759, Gradient norm: 2.52934525
INFO:root:[   13] Training loss: 0.06428108, Validation loss: 0.06032890, Gradient norm: 2.13295332
INFO:root:[   14] Training loss: 0.06364988, Validation loss: 0.06039300, Gradient norm: 2.31305205
INFO:root:[   15] Training loss: 0.06058627, Validation loss: 0.05957228, Gradient norm: 2.25018843
INFO:root:[   16] Training loss: 0.06064069, Validation loss: 0.05645672, Gradient norm: 2.44196311
INFO:root:[   17] Training loss: 0.05949968, Validation loss: 0.05503379, Gradient norm: 2.32298038
INFO:root:[   18] Training loss: 0.05887212, Validation loss: 0.05670355, Gradient norm: 2.38656314
INFO:root:[   19] Training loss: 0.05989637, Validation loss: 0.05805912, Gradient norm: 2.51011881
INFO:root:[   20] Training loss: 0.05683342, Validation loss: 0.05546832, Gradient norm: 2.22824018
INFO:root:[   21] Training loss: 0.05619570, Validation loss: 0.05601856, Gradient norm: 2.28337211
INFO:root:[   22] Training loss: 0.05722167, Validation loss: 0.05802617, Gradient norm: 2.61654212
INFO:root:[   23] Training loss: 0.05420364, Validation loss: 0.05104673, Gradient norm: 2.76438061
INFO:root:[   24] Training loss: 0.05301388, Validation loss: 0.05475760, Gradient norm: 2.55839317
INFO:root:[   25] Training loss: 0.05308415, Validation loss: 0.05260970, Gradient norm: 2.58269389
INFO:root:[   26] Training loss: 0.05186962, Validation loss: 0.05333865, Gradient norm: 2.74181585
INFO:root:[   27] Training loss: 0.05136638, Validation loss: 0.05052350, Gradient norm: 2.60995515
INFO:root:[   28] Training loss: 0.05107872, Validation loss: 0.05008917, Gradient norm: 2.64601585
INFO:root:[   29] Training loss: 0.05029649, Validation loss: 0.04596403, Gradient norm: 2.67837671
INFO:root:[   30] Training loss: 0.04994512, Validation loss: 0.04893111, Gradient norm: 2.68215384
INFO:root:[   31] Training loss: 0.05173654, Validation loss: 0.05177848, Gradient norm: 2.82504563
INFO:root:[   32] Training loss: 0.05056643, Validation loss: 0.04578694, Gradient norm: 2.63483377
INFO:root:[   33] Training loss: 0.04761273, Validation loss: 0.05041642, Gradient norm: 2.56659188
INFO:root:[   34] Training loss: 0.04928726, Validation loss: 0.04365326, Gradient norm: 2.55952377
INFO:root:[   35] Training loss: 0.05172513, Validation loss: 0.04592560, Gradient norm: 2.83913421
INFO:root:[   36] Training loss: 0.04844069, Validation loss: 0.04438112, Gradient norm: 2.59600801
INFO:root:[   37] Training loss: 0.04730988, Validation loss: 0.04809761, Gradient norm: 2.45349391
INFO:root:[   38] Training loss: 0.04622823, Validation loss: 0.04148085, Gradient norm: 2.46564260
INFO:root:[   39] Training loss: 0.04613527, Validation loss: 0.04670342, Gradient norm: 2.34359230
INFO:root:[   40] Training loss: 0.04500037, Validation loss: 0.04731992, Gradient norm: 2.54787460
INFO:root:[   41] Training loss: 0.04463233, Validation loss: 0.04661260, Gradient norm: 2.65323852
INFO:root:[   42] Training loss: 0.04482908, Validation loss: 0.04088946, Gradient norm: 2.67761861
INFO:root:[   43] Training loss: 0.04429188, Validation loss: 0.04549387, Gradient norm: 2.85642831
INFO:root:[   44] Training loss: 0.04376389, Validation loss: 0.04075545, Gradient norm: 2.76498412
INFO:root:[   45] Training loss: 0.04397024, Validation loss: 0.04726882, Gradient norm: 2.51098722
INFO:root:[   46] Training loss: 0.04637225, Validation loss: 0.04953369, Gradient norm: 2.73115002
INFO:root:[   47] Training loss: 0.04324478, Validation loss: 0.04234023, Gradient norm: 2.64004075
INFO:root:[   48] Training loss: 0.04258872, Validation loss: 0.04183468, Gradient norm: 2.74650846
INFO:root:[   49] Training loss: 0.04224899, Validation loss: 0.04346812, Gradient norm: 2.73346860
INFO:root:[   50] Training loss: 0.04215053, Validation loss: 0.03844191, Gradient norm: 2.87102281
INFO:root:[   51] Training loss: 0.04151167, Validation loss: 0.04275011, Gradient norm: 2.79920209
INFO:root:[   52] Training loss: 0.04216412, Validation loss: 0.04258957, Gradient norm: 2.63731096
INFO:root:[   53] Training loss: 0.04394705, Validation loss: 0.03909234, Gradient norm: 2.58549279
INFO:root:[   54] Training loss: 0.04193030, Validation loss: 0.04175380, Gradient norm: 2.62154784
INFO:root:[   55] Training loss: 0.04110825, Validation loss: 0.03777407, Gradient norm: 2.71168721
INFO:root:[   56] Training loss: 0.04076995, Validation loss: 0.04200384, Gradient norm: 2.65745389
INFO:root:[   57] Training loss: 0.04052117, Validation loss: 0.04213675, Gradient norm: 2.64972852
INFO:root:[   58] Training loss: 0.04044291, Validation loss: 0.03967776, Gradient norm: 2.83565952
INFO:root:[   59] Training loss: 0.04013358, Validation loss: 0.04147480, Gradient norm: 2.83842332
INFO:root:[   60] Training loss: 0.04027450, Validation loss: 0.04159595, Gradient norm: 2.90111044
INFO:root:[   61] Training loss: 0.03957987, Validation loss: 0.03821187, Gradient norm: 2.80471426
INFO:root:[   62] Training loss: 0.03954261, Validation loss: 0.03939481, Gradient norm: 2.81912504
INFO:root:[   63] Training loss: 0.03951460, Validation loss: 0.04241272, Gradient norm: 2.65092325
INFO:root:[   64] Training loss: 0.04109725, Validation loss: 0.03841309, Gradient norm: 2.74091831
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 848.065s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01823
INFO:root:EnergyScoreTrain: 0.01429
INFO:root:CoverageTrain: 0.99486
INFO:root:IntervalWidthTrain: 0.03763
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01811
INFO:root:EnergyScoreValidation: 0.01425
INFO:root:CoverageValidation: 0.99503
INFO:root:IntervalWidthValidation: 0.03763
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01819
INFO:root:EnergyScoreTest: 0.01427
INFO:root:CoverageTest: 0.99462
INFO:root:IntervalWidthTest: 0.03737
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.65645165, Validation loss: 0.22007150, Gradient norm: 4.18029927
INFO:root:[    2] Training loss: 0.19524744, Validation loss: 0.17847999, Gradient norm: 2.57002193
INFO:root:[    3] Training loss: 0.15637603, Validation loss: 0.13888202, Gradient norm: 1.91323780
INFO:root:[    4] Training loss: 0.13174556, Validation loss: 0.11798099, Gradient norm: 1.92575810
INFO:root:[    5] Training loss: 0.12171936, Validation loss: 0.11308930, Gradient norm: 2.10585433
INFO:root:[    6] Training loss: 0.11840383, Validation loss: 0.11645519, Gradient norm: 2.35937075
INFO:root:[    7] Training loss: 0.10918002, Validation loss: 0.10332846, Gradient norm: 1.87042870
INFO:root:[    8] Training loss: 0.10737287, Validation loss: 0.10897229, Gradient norm: 2.28661879
INFO:root:[    9] Training loss: 0.10487986, Validation loss: 0.09959852, Gradient norm: 2.19569524
INFO:root:[   10] Training loss: 0.09942666, Validation loss: 0.10005867, Gradient norm: 1.90902371
INFO:root:[   11] Training loss: 0.09680045, Validation loss: 0.10064913, Gradient norm: 1.78286667
INFO:root:[   12] Training loss: 0.09764621, Validation loss: 0.09625344, Gradient norm: 2.20508892
INFO:root:[   13] Training loss: 0.09563047, Validation loss: 0.10113986, Gradient norm: 2.14651553
INFO:root:[   14] Training loss: 0.09581257, Validation loss: 0.09818385, Gradient norm: 2.49199555
INFO:root:[   15] Training loss: 0.09256003, Validation loss: 0.09809359, Gradient norm: 2.19019795
INFO:root:[   16] Training loss: 0.09141816, Validation loss: 0.08404399, Gradient norm: 2.17920808
INFO:root:[   17] Training loss: 0.08901034, Validation loss: 0.09221053, Gradient norm: 2.02156795
INFO:root:[   18] Training loss: 0.09037373, Validation loss: 0.09580964, Gradient norm: 2.49279770
INFO:root:[   19] Training loss: 0.08842492, Validation loss: 0.08029600, Gradient norm: 2.31970677
INFO:root:[   20] Training loss: 0.08631788, Validation loss: 0.08790871, Gradient norm: 2.28576854
INFO:root:[   21] Training loss: 0.08505567, Validation loss: 0.07993228, Gradient norm: 2.18065289
INFO:root:[   22] Training loss: 0.08553609, Validation loss: 0.08356346, Gradient norm: 2.30435221
INFO:root:[   23] Training loss: 0.08192199, Validation loss: 0.08397931, Gradient norm: 2.02721818
INFO:root:[   24] Training loss: 0.08229610, Validation loss: 0.07749262, Gradient norm: 2.17853852
INFO:root:[   25] Training loss: 0.08159928, Validation loss: 0.08651062, Gradient norm: 2.32992683
INFO:root:[   26] Training loss: 0.07985022, Validation loss: 0.07436697, Gradient norm: 2.16920426
INFO:root:[   27] Training loss: 0.07984849, Validation loss: 0.07566398, Gradient norm: 2.35246878
INFO:root:[   28] Training loss: 0.08024630, Validation loss: 0.07995058, Gradient norm: 2.32392927
INFO:root:[   29] Training loss: 0.07659317, Validation loss: 0.07515082, Gradient norm: 1.95462175
INFO:root:[   30] Training loss: 0.07864436, Validation loss: 0.07308049, Gradient norm: 2.27200248
INFO:root:[   31] Training loss: 0.07724389, Validation loss: 0.07874997, Gradient norm: 2.39274435
INFO:root:[   32] Training loss: 0.07459162, Validation loss: 0.07451019, Gradient norm: 2.22724298
INFO:root:[   33] Training loss: 0.07267141, Validation loss: 0.06987953, Gradient norm: 2.02389423
INFO:root:[   34] Training loss: 0.07150131, Validation loss: 0.07455853, Gradient norm: 2.02257984
INFO:root:[   35] Training loss: 0.07256677, Validation loss: 0.06927505, Gradient norm: 2.17933753
INFO:root:[   36] Training loss: 0.07030575, Validation loss: 0.07312653, Gradient norm: 2.13392378
INFO:root:[   37] Training loss: 0.07088699, Validation loss: 0.07182684, Gradient norm: 2.27479311
INFO:root:[   38] Training loss: 0.06932602, Validation loss: 0.06698410, Gradient norm: 2.30697879
INFO:root:[   39] Training loss: 0.06843756, Validation loss: 0.07155655, Gradient norm: 2.23292018
INFO:root:[   40] Training loss: 0.07275092, Validation loss: 0.07394859, Gradient norm: 2.71031689
INFO:root:[   41] Training loss: 0.06933570, Validation loss: 0.06678771, Gradient norm: 2.28852445
INFO:root:[   42] Training loss: 0.06671758, Validation loss: 0.06620190, Gradient norm: 2.12986783
INFO:root:[   43] Training loss: 0.06607723, Validation loss: 0.06522850, Gradient norm: 2.35238004
INFO:root:[   44] Training loss: 0.06566241, Validation loss: 0.06269887, Gradient norm: 2.28554639
INFO:root:[   45] Training loss: 0.06494180, Validation loss: 0.06730072, Gradient norm: 2.34356978
INFO:root:[   46] Training loss: 0.06439159, Validation loss: 0.06533701, Gradient norm: 2.31256693
INFO:root:[   47] Training loss: 0.06487651, Validation loss: 0.06038037, Gradient norm: 2.54363543
INFO:root:[   48] Training loss: 0.06359694, Validation loss: 0.06605435, Gradient norm: 2.52396016
INFO:root:[   49] Training loss: 0.06319860, Validation loss: 0.06128311, Gradient norm: 2.58175640
INFO:root:[   50] Training loss: 0.06244562, Validation loss: 0.06409452, Gradient norm: 2.54475359
INFO:root:[   51] Training loss: 0.06196760, Validation loss: 0.05839090, Gradient norm: 2.52095847
INFO:root:[   52] Training loss: 0.06167529, Validation loss: 0.06394122, Gradient norm: 2.52829991
INFO:root:[   53] Training loss: 0.06118585, Validation loss: 0.05859889, Gradient norm: 2.50336131
INFO:root:[   54] Training loss: 0.06138180, Validation loss: 0.06138043, Gradient norm: 2.54721494
INFO:root:[   55] Training loss: 0.05982756, Validation loss: 0.06067461, Gradient norm: 2.48466667
INFO:root:[   56] Training loss: 0.05948823, Validation loss: 0.05961855, Gradient norm: 2.63908706
INFO:root:[   57] Training loss: 0.05862035, Validation loss: 0.05764109, Gradient norm: 2.56801360
INFO:root:[   58] Training loss: 0.05804828, Validation loss: 0.06057992, Gradient norm: 2.43246494
INFO:root:[   59] Training loss: 0.05853873, Validation loss: 0.05570910, Gradient norm: 2.65648477
INFO:root:[   60] Training loss: 0.05752902, Validation loss: 0.05767133, Gradient norm: 2.51439287
INFO:root:[   61] Training loss: 0.05714102, Validation loss: 0.05448286, Gradient norm: 2.61234508
INFO:root:[   62] Training loss: 0.05643634, Validation loss: 0.05675618, Gradient norm: 2.60309005
INFO:root:[   63] Training loss: 0.05597171, Validation loss: 0.05707604, Gradient norm: 2.52446508
INFO:root:[   64] Training loss: 0.05602984, Validation loss: 0.05682011, Gradient norm: 2.70041023
INFO:root:[   65] Training loss: 0.05530079, Validation loss: 0.05285607, Gradient norm: 2.50034586
INFO:root:[   66] Training loss: 0.05470992, Validation loss: 0.05350258, Gradient norm: 2.58603636
INFO:root:[   67] Training loss: 0.05439096, Validation loss: 0.05424084, Gradient norm: 2.58551326
INFO:root:[   68] Training loss: 0.05406426, Validation loss: 0.05425564, Gradient norm: 2.53418845
INFO:root:[   69] Training loss: 0.05377896, Validation loss: 0.05207386, Gradient norm: 2.69779302
INFO:root:[   70] Training loss: 0.05344088, Validation loss: 0.05490736, Gradient norm: 2.55244559
INFO:root:[   71] Training loss: 0.05295280, Validation loss: 0.05089208, Gradient norm: 2.49351106
INFO:root:[   72] Training loss: 0.05203744, Validation loss: 0.05293693, Gradient norm: 2.57339803
INFO:root:[   73] Training loss: 0.05186752, Validation loss: 0.04966118, Gradient norm: 2.64761282
INFO:root:[   74] Training loss: 0.05123372, Validation loss: 0.04974673, Gradient norm: 2.50285924
INFO:root:[   75] Training loss: 0.05100106, Validation loss: 0.05107927, Gradient norm: 2.54516825
INFO:root:[   76] Training loss: 0.05074723, Validation loss: 0.05119722, Gradient norm: 2.60445466
INFO:root:[   77] Training loss: 0.05024292, Validation loss: 0.04807477, Gradient norm: 2.59504835
INFO:root:[   78] Training loss: 0.04987328, Validation loss: 0.05100277, Gradient norm: 2.54447474
INFO:root:[   79] Training loss: 0.04975488, Validation loss: 0.04836170, Gradient norm: 2.54616351
INFO:root:[   80] Training loss: 0.04920319, Validation loss: 0.05171464, Gradient norm: 2.51769036
INFO:root:[   81] Training loss: 0.04886414, Validation loss: 0.04878774, Gradient norm: 2.54147197
INFO:root:[   82] Training loss: 0.04893864, Validation loss: 0.05308959, Gradient norm: 2.35201702
INFO:root:[   83] Training loss: 0.04960007, Validation loss: 0.04743695, Gradient norm: 2.35178062
INFO:root:[   84] Training loss: 0.04814650, Validation loss: 0.04940682, Gradient norm: 2.57997899
INFO:root:[   85] Training loss: 0.04748699, Validation loss: 0.04591853, Gradient norm: 2.58744359
INFO:root:[   86] Training loss: 0.04738947, Validation loss: 0.04920649, Gradient norm: 2.52422474
INFO:root:[   87] Training loss: 0.04701969, Validation loss: 0.04599345, Gradient norm: 2.44807855
INFO:root:[   88] Training loss: 0.04664773, Validation loss: 0.04879984, Gradient norm: 2.54279528
INFO:root:[   89] Training loss: 0.04611779, Validation loss: 0.04555746, Gradient norm: 2.50393742
INFO:root:[   90] Training loss: 0.04640803, Validation loss: 0.04847336, Gradient norm: 2.51430703
INFO:root:[   91] Training loss: 0.04614263, Validation loss: 0.04450111, Gradient norm: 2.46708048
INFO:root:[   92] Training loss: 0.04539009, Validation loss: 0.04622091, Gradient norm: 2.40556699
INFO:root:[   93] Training loss: 0.04516650, Validation loss: 0.04344537, Gradient norm: 2.47788184
INFO:root:[   94] Training loss: 0.04464702, Validation loss: 0.04665404, Gradient norm: 2.48414423
INFO:root:[   95] Training loss: 0.04448182, Validation loss: 0.04249941, Gradient norm: 2.53910309
INFO:root:[   96] Training loss: 0.04401420, Validation loss: 0.04419614, Gradient norm: 2.39739879
INFO:root:[   97] Training loss: 0.04423330, Validation loss: 0.04169948, Gradient norm: 2.53835222
INFO:root:[   98] Training loss: 0.04327391, Validation loss: 0.04384075, Gradient norm: 2.43866447
INFO:root:[   99] Training loss: 0.04285826, Validation loss: 0.04207735, Gradient norm: 2.40210364
INFO:root:[  100] Training loss: 0.04285035, Validation loss: 0.04479357, Gradient norm: 2.52933875
INFO:root:[  101] Training loss: 0.04264262, Validation loss: 0.04113651, Gradient norm: 2.47197430
INFO:root:[  102] Training loss: 0.04253764, Validation loss: 0.04370197, Gradient norm: 2.27193162
INFO:root:[  103] Training loss: 0.04236102, Validation loss: 0.04105954, Gradient norm: 2.44916818
INFO:root:[  104] Training loss: 0.04193369, Validation loss: 0.04333307, Gradient norm: 2.40079520
INFO:root:[  105] Training loss: 0.04162989, Validation loss: 0.04013418, Gradient norm: 2.46523325
INFO:root:[  106] Training loss: 0.04142914, Validation loss: 0.04408036, Gradient norm: 2.37681728
INFO:root:[  107] Training loss: 0.04141575, Validation loss: 0.03979271, Gradient norm: 2.36797462
INFO:root:[  108] Training loss: 0.04071896, Validation loss: 0.04191989, Gradient norm: 2.44024627
INFO:root:[  109] Training loss: 0.04045959, Validation loss: 0.04078587, Gradient norm: 2.34965044
INFO:root:[  110] Training loss: 0.04025240, Validation loss: 0.04054555, Gradient norm: 2.38014253
INFO:root:[  111] Training loss: 0.04028020, Validation loss: 0.03834620, Gradient norm: 2.37160552
INFO:root:[  112] Training loss: 0.04010752, Validation loss: 0.03957297, Gradient norm: 2.24757450
INFO:root:[  113] Training loss: 0.03936014, Validation loss: 0.03795217, Gradient norm: 2.49672167
INFO:root:[  114] Training loss: 0.03931835, Validation loss: 0.04152912, Gradient norm: 2.30149211
INFO:root:[  115] Training loss: 0.04229280, Validation loss: 0.03550715, Gradient norm: 2.40649816
INFO:root:[  116] Training loss: 0.03968663, Validation loss: 0.03809191, Gradient norm: 2.34180653
INFO:root:[  117] Training loss: 0.03859993, Validation loss: 0.03890260, Gradient norm: 2.06796272
INFO:root:[  118] Training loss: 0.03941364, Validation loss: 0.03684574, Gradient norm: 2.05040958
INFO:root:[  119] Training loss: 0.03805120, Validation loss: 0.04017359, Gradient norm: 2.19177510
INFO:root:[  120] Training loss: 0.03782751, Validation loss: 0.04431154, Gradient norm: 2.17994760
INFO:root:[  121] Training loss: 0.03848382, Validation loss: 0.03809913, Gradient norm: 2.34096075
INFO:root:[  122] Training loss: 0.03700392, Validation loss: 0.03730828, Gradient norm: 2.35338879
INFO:root:[  123] Training loss: 0.03659949, Validation loss: 0.03551547, Gradient norm: 2.44801314
INFO:root:[  124] Training loss: 0.03617210, Validation loss: 0.03713485, Gradient norm: 2.32035578
INFO:root:EP 124: Early stopping
INFO:root:Training the model took 1629.282s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01223
INFO:root:EnergyScoreTrain: 0.01183
INFO:root:CoverageTrain: 0.99776
INFO:root:IntervalWidthTrain: 0.03866
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01211
INFO:root:EnergyScoreValidation: 0.01179
INFO:root:CoverageValidation: 0.99777
INFO:root:IntervalWidthValidation: 0.03866
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01191
INFO:root:EnergyScoreTest: 0.01167
INFO:root:CoverageTest: 0.99771
INFO:root:IntervalWidthTest: 0.03843
INFO:root:###21 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.78179396, Validation loss: 0.27119926, Gradient norm: 3.97847508
INFO:root:[    2] Training loss: 0.22906614, Validation loss: 0.20503682, Gradient norm: 1.57130203
INFO:root:[    3] Training loss: 0.19649724, Validation loss: 0.17926759, Gradient norm: 1.52444608
INFO:root:[    4] Training loss: 0.17580587, Validation loss: 0.16017456, Gradient norm: 2.01619193
INFO:root:[    5] Training loss: 0.16069399, Validation loss: 0.15653368, Gradient norm: 1.17919913
INFO:root:[    6] Training loss: 0.15035447, Validation loss: 0.15051692, Gradient norm: 1.54563931
INFO:root:[    7] Training loss: 0.14678044, Validation loss: 0.14460465, Gradient norm: 1.85989717
INFO:root:[    8] Training loss: 0.14094392, Validation loss: 0.13923772, Gradient norm: 1.77184925
INFO:root:[    9] Training loss: 0.13717583, Validation loss: 0.12802905, Gradient norm: 1.76820626
INFO:root:[   10] Training loss: 0.13104450, Validation loss: 0.14240829, Gradient norm: 1.63215145
INFO:root:[   11] Training loss: 0.13123555, Validation loss: 0.12484313, Gradient norm: 1.99643906
INFO:root:[   12] Training loss: 0.12576631, Validation loss: 0.12929881, Gradient norm: 1.51375117
INFO:root:[   13] Training loss: 0.12327115, Validation loss: 0.11750447, Gradient norm: 1.62139698
INFO:root:[   14] Training loss: 0.12240998, Validation loss: 0.12213197, Gradient norm: 1.83240238
INFO:root:[   15] Training loss: 0.12170875, Validation loss: 0.11435444, Gradient norm: 1.92430851
INFO:root:[   16] Training loss: 0.11670685, Validation loss: 0.11232597, Gradient norm: 1.60039891
INFO:root:[   17] Training loss: 0.11672979, Validation loss: 0.12026691, Gradient norm: 1.88732642
INFO:root:[   18] Training loss: 0.11561751, Validation loss: 0.12152680, Gradient norm: 1.89246125
INFO:root:[   19] Training loss: 0.11454557, Validation loss: 0.11652100, Gradient norm: 1.96127189
INFO:root:[   20] Training loss: 0.11401235, Validation loss: 0.11896494, Gradient norm: 2.05572185
INFO:root:[   21] Training loss: 0.11107082, Validation loss: 0.11118582, Gradient norm: 1.84882411
INFO:root:[   22] Training loss: 0.11071585, Validation loss: 0.10701988, Gradient norm: 1.94295448
INFO:root:[   23] Training loss: 0.11021022, Validation loss: 0.10910202, Gradient norm: 1.99987352
INFO:root:[   24] Training loss: 0.10688679, Validation loss: 0.10029033, Gradient norm: 1.86484125
INFO:root:[   25] Training loss: 0.10515855, Validation loss: 0.10803294, Gradient norm: 1.80343088
INFO:root:[   26] Training loss: 0.10550752, Validation loss: 0.10115184, Gradient norm: 1.96965679
INFO:root:[   27] Training loss: 0.10614443, Validation loss: 0.10628094, Gradient norm: 1.77306949
INFO:root:[   28] Training loss: 0.10312838, Validation loss: 0.09850626, Gradient norm: 1.77423266
INFO:root:[   29] Training loss: 0.10060991, Validation loss: 0.09455608, Gradient norm: 1.66297322
INFO:root:[   30] Training loss: 0.09910351, Validation loss: 0.09828212, Gradient norm: 1.77596408
INFO:root:[   31] Training loss: 0.09717577, Validation loss: 0.10119485, Gradient norm: 1.71531952
INFO:root:[   32] Training loss: 0.09717590, Validation loss: 0.09861433, Gradient norm: 1.91736181
INFO:root:[   33] Training loss: 0.09622813, Validation loss: 0.09991653, Gradient norm: 1.92923766
INFO:root:[   34] Training loss: 0.09613158, Validation loss: 0.10292991, Gradient norm: 1.98108008
INFO:root:[   35] Training loss: 0.09497210, Validation loss: 0.09209595, Gradient norm: 1.89842725
INFO:root:[   36] Training loss: 0.09249865, Validation loss: 0.08699361, Gradient norm: 1.80648048
INFO:root:[   37] Training loss: 0.09273733, Validation loss: 0.08602649, Gradient norm: 1.97566743
INFO:root:[   38] Training loss: 0.09210016, Validation loss: 0.09716984, Gradient norm: 1.97695451
INFO:root:[   39] Training loss: 0.09171387, Validation loss: 0.09284275, Gradient norm: 1.84610934
INFO:root:[   40] Training loss: 0.08884857, Validation loss: 0.08352911, Gradient norm: 1.79365561
INFO:root:[   41] Training loss: 0.08888497, Validation loss: 0.08334177, Gradient norm: 2.01686722
INFO:root:[   42] Training loss: 0.08790678, Validation loss: 0.08444077, Gradient norm: 2.04137270
INFO:root:[   43] Training loss: 0.08725227, Validation loss: 0.08088192, Gradient norm: 2.04934119
INFO:root:[   44] Training loss: 0.08543223, Validation loss: 0.08347876, Gradient norm: 1.91956295
INFO:root:[   45] Training loss: 0.08599743, Validation loss: 0.07946292, Gradient norm: 2.08398255
INFO:root:[   46] Training loss: 0.08338690, Validation loss: 0.08559436, Gradient norm: 1.77186101
INFO:root:[   47] Training loss: 0.08387844, Validation loss: 0.08918649, Gradient norm: 1.91088087
INFO:root:[   48] Training loss: 0.08420199, Validation loss: 0.07646031, Gradient norm: 2.24173705
INFO:root:[   49] Training loss: 0.08229641, Validation loss: 0.07790717, Gradient norm: 1.94438435
INFO:root:[   50] Training loss: 0.08154349, Validation loss: 0.08150889, Gradient norm: 2.00240389
INFO:root:[   51] Training loss: 0.08145170, Validation loss: 0.08062495, Gradient norm: 2.12705049
INFO:root:[   52] Training loss: 0.07917822, Validation loss: 0.07992012, Gradient norm: 2.00738186
INFO:root:[   53] Training loss: 0.07876404, Validation loss: 0.08089900, Gradient norm: 1.82573172
INFO:root:[   54] Training loss: 0.07849468, Validation loss: 0.07883707, Gradient norm: 2.05178603
INFO:root:[   55] Training loss: 0.07624875, Validation loss: 0.07844830, Gradient norm: 1.82750824
INFO:root:[   56] Training loss: 0.07528640, Validation loss: 0.07216445, Gradient norm: 1.87693111
INFO:root:[   57] Training loss: 0.07510906, Validation loss: 0.07574102, Gradient norm: 2.02604018
INFO:root:[   58] Training loss: 0.07508257, Validation loss: 0.07960460, Gradient norm: 1.93234971
INFO:root:[   59] Training loss: 0.07418044, Validation loss: 0.07398095, Gradient norm: 1.96911477
INFO:root:[   60] Training loss: 0.07339762, Validation loss: 0.07500563, Gradient norm: 2.20562735
INFO:root:[   61] Training loss: 0.07012342, Validation loss: 0.07289031, Gradient norm: 1.74200700
INFO:root:[   62] Training loss: 0.07055667, Validation loss: 0.06510136, Gradient norm: 1.99107023
INFO:root:[   63] Training loss: 0.06994650, Validation loss: 0.07122071, Gradient norm: 1.90871443
INFO:root:[   64] Training loss: 0.07082431, Validation loss: 0.07160752, Gradient norm: 2.02183735
INFO:root:[   65] Training loss: 0.06979038, Validation loss: 0.06972687, Gradient norm: 1.96407612
INFO:root:[   66] Training loss: 0.06905666, Validation loss: 0.06576918, Gradient norm: 2.09836884
INFO:root:[   67] Training loss: 0.06769443, Validation loss: 0.06169922, Gradient norm: 2.10086208
INFO:root:[   68] Training loss: 0.06564069, Validation loss: 0.06829490, Gradient norm: 1.86178105
INFO:root:[   69] Training loss: 0.06708573, Validation loss: 0.06181329, Gradient norm: 1.83750383
INFO:root:[   70] Training loss: 0.06549188, Validation loss: 0.06626455, Gradient norm: 1.88554114
INFO:root:[   71] Training loss: 0.06472508, Validation loss: 0.06152517, Gradient norm: 2.04234229
INFO:root:[   72] Training loss: 0.06356560, Validation loss: 0.06388365, Gradient norm: 1.95792901
INFO:root:[   73] Training loss: 0.06181689, Validation loss: 0.06556295, Gradient norm: 1.85461150
INFO:root:[   74] Training loss: 0.06235489, Validation loss: 0.05870616, Gradient norm: 2.04555430
INFO:root:[   75] Training loss: 0.06307536, Validation loss: 0.05719638, Gradient norm: 2.12845332
INFO:root:[   76] Training loss: 0.06150447, Validation loss: 0.06545483, Gradient norm: 1.98816807
INFO:root:[   77] Training loss: 0.06125134, Validation loss: 0.06345589, Gradient norm: 2.09659617
INFO:root:[   78] Training loss: 0.06084645, Validation loss: 0.05736904, Gradient norm: 2.00082036
INFO:root:[   79] Training loss: 0.06024976, Validation loss: 0.05435873, Gradient norm: 2.08858199
INFO:root:[   80] Training loss: 0.05911629, Validation loss: 0.06170588, Gradient norm: 2.00302622
INFO:root:[   81] Training loss: 0.05962144, Validation loss: 0.05619236, Gradient norm: 2.20462183
INFO:root:[   82] Training loss: 0.05733552, Validation loss: 0.05899857, Gradient norm: 2.09005719
INFO:root:[   83] Training loss: 0.05669903, Validation loss: 0.05746611, Gradient norm: 2.01024321
INFO:root:[   84] Training loss: 0.05679616, Validation loss: 0.05868811, Gradient norm: 1.87621745
INFO:root:[   85] Training loss: 0.05659304, Validation loss: 0.05197976, Gradient norm: 2.18860185
INFO:root:[   86] Training loss: 0.05522772, Validation loss: 0.05564665, Gradient norm: 2.02307458
INFO:root:[   87] Training loss: 0.05534987, Validation loss: 0.05905569, Gradient norm: 2.11094142
INFO:root:[   88] Training loss: 0.05483440, Validation loss: 0.05868252, Gradient norm: 2.05898990
INFO:root:[   89] Training loss: 0.05383913, Validation loss: 0.05023673, Gradient norm: 2.13037122
INFO:root:[   90] Training loss: 0.05241242, Validation loss: 0.05516121, Gradient norm: 1.98844226
INFO:root:[   91] Training loss: 0.05216602, Validation loss: 0.05306228, Gradient norm: 2.09624418
INFO:root:[   92] Training loss: 0.05211722, Validation loss: 0.05051071, Gradient norm: 1.91247495
INFO:root:[   93] Training loss: 0.05265576, Validation loss: 0.05266338, Gradient norm: 1.84317195
INFO:root:[   94] Training loss: 0.05126381, Validation loss: 0.05146508, Gradient norm: 2.03272267
INFO:root:[   95] Training loss: 0.04947874, Validation loss: 0.04766529, Gradient norm: 1.97094708
INFO:root:[   96] Training loss: 0.05044952, Validation loss: 0.04553959, Gradient norm: 2.05146445
INFO:root:[   97] Training loss: 0.04979667, Validation loss: 0.05136317, Gradient norm: 2.20732838
INFO:root:[   98] Training loss: 0.04761104, Validation loss: 0.04372971, Gradient norm: 1.93295236
INFO:root:[   99] Training loss: 0.04683863, Validation loss: 0.04381024, Gradient norm: 1.83013621
INFO:root:[  100] Training loss: 0.04678803, Validation loss: 0.04705998, Gradient norm: 1.87835531
INFO:root:[  101] Training loss: 0.04811612, Validation loss: 0.04934297, Gradient norm: 1.83447505
INFO:root:[  102] Training loss: 0.04689564, Validation loss: 0.04770335, Gradient norm: 2.04150716
INFO:root:[  103] Training loss: 0.04595796, Validation loss: 0.04786544, Gradient norm: 1.78986896
INFO:root:[  104] Training loss: 0.04560663, Validation loss: 0.04222368, Gradient norm: 1.94955970
INFO:root:[  105] Training loss: 0.04530744, Validation loss: 0.04542120, Gradient norm: 2.07283949
INFO:root:[  106] Training loss: 0.04436852, Validation loss: 0.04708190, Gradient norm: 2.07011358
INFO:root:[  107] Training loss: 0.04481982, Validation loss: 0.04543370, Gradient norm: 2.21160489
INFO:root:[  108] Training loss: 0.04364242, Validation loss: 0.04838084, Gradient norm: 1.91904737
INFO:root:[  109] Training loss: 0.04373974, Validation loss: 0.03938810, Gradient norm: 1.94022562
INFO:root:[  110] Training loss: 0.04281588, Validation loss: 0.04231646, Gradient norm: 2.00211418
INFO:root:[  111] Training loss: 0.04275436, Validation loss: 0.04135009, Gradient norm: 1.94445959
INFO:root:[  112] Training loss: 0.04098145, Validation loss: 0.03768809, Gradient norm: 1.88624981
INFO:root:[  113] Training loss: 0.04029489, Validation loss: 0.04346119, Gradient norm: 1.84942544
INFO:root:[  114] Training loss: 0.04117274, Validation loss: 0.04007291, Gradient norm: 2.05613481
INFO:root:[  115] Training loss: 0.04151961, Validation loss: 0.04190239, Gradient norm: 2.04479183
INFO:root:[  116] Training loss: 0.04015382, Validation loss: 0.04201976, Gradient norm: 2.05296841
INFO:root:[  117] Training loss: 0.03984563, Validation loss: 0.04260149, Gradient norm: 1.96720679
INFO:root:[  118] Training loss: 0.04120824, Validation loss: 0.03775068, Gradient norm: 2.14749248
INFO:root:[  119] Training loss: 0.03906556, Validation loss: 0.04012355, Gradient norm: 1.94117804
INFO:root:[  120] Training loss: 0.03868403, Validation loss: 0.04189126, Gradient norm: 1.90775812
INFO:root:[  121] Training loss: 0.03982151, Validation loss: 0.03939661, Gradient norm: 2.18787865
INFO:root:EP 121: Early stopping
INFO:root:Training the model took 1588.04s.
INFO:root:Emptying the cuda cache took 0.034s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01417
INFO:root:EnergyScoreTrain: 0.01282
INFO:root:CoverageTrain: 0.99957
INFO:root:IntervalWidthTrain: 0.03932
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01416
INFO:root:EnergyScoreValidation: 0.01281
INFO:root:CoverageValidation: 0.99956
INFO:root:IntervalWidthValidation: 0.03929
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01395
INFO:root:EnergyScoreTest: 0.01266
INFO:root:CoverageTest: 0.9995
INFO:root:IntervalWidthTest: 0.03899
INFO:root:###22 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.92415316, Validation loss: 0.27412063, Gradient norm: 3.51657148
INFO:root:[    2] Training loss: 0.24324273, Validation loss: 0.21466766, Gradient norm: 1.27309435
INFO:root:[    3] Training loss: 0.20987785, Validation loss: 0.19634341, Gradient norm: 1.80656255
INFO:root:[    4] Training loss: 0.18287737, Validation loss: 0.17282940, Gradient norm: 1.11026334
INFO:root:[    5] Training loss: 0.17541892, Validation loss: 0.18434875, Gradient norm: 1.98052907
INFO:root:[    6] Training loss: 0.16406386, Validation loss: 0.15746624, Gradient norm: 1.61155025
INFO:root:[    7] Training loss: 0.15827844, Validation loss: 0.15557865, Gradient norm: 1.64555556
INFO:root:[    8] Training loss: 0.15372965, Validation loss: 0.14746656, Gradient norm: 1.51411871
INFO:root:[    9] Training loss: 0.15023525, Validation loss: 0.14312194, Gradient norm: 1.54976723
INFO:root:[   10] Training loss: 0.14632514, Validation loss: 0.14900104, Gradient norm: 1.82527122
INFO:root:[   11] Training loss: 0.14199228, Validation loss: 0.14786251, Gradient norm: 1.57434552
INFO:root:[   12] Training loss: 0.14070506, Validation loss: 0.14132710, Gradient norm: 1.75623784
INFO:root:[   13] Training loss: 0.13689882, Validation loss: 0.14542141, Gradient norm: 1.59759590
INFO:root:[   14] Training loss: 0.13687540, Validation loss: 0.12902647, Gradient norm: 1.93572380
INFO:root:[   15] Training loss: 0.13321758, Validation loss: 0.13657468, Gradient norm: 1.73285335
INFO:root:[   16] Training loss: 0.13002457, Validation loss: 0.12735160, Gradient norm: 1.52690383
INFO:root:[   17] Training loss: 0.12967203, Validation loss: 0.12768987, Gradient norm: 1.73687677
INFO:root:[   18] Training loss: 0.12575631, Validation loss: 0.12067796, Gradient norm: 1.61725248
INFO:root:[   19] Training loss: 0.12418160, Validation loss: 0.12027094, Gradient norm: 1.70925446
INFO:root:[   20] Training loss: 0.12311666, Validation loss: 0.12389424, Gradient norm: 1.79920829
INFO:root:[   21] Training loss: 0.11985928, Validation loss: 0.11957221, Gradient norm: 1.53584975
INFO:root:[   22] Training loss: 0.11906499, Validation loss: 0.12090500, Gradient norm: 1.65584392
INFO:root:[   23] Training loss: 0.11651161, Validation loss: 0.11114317, Gradient norm: 1.62916566
INFO:root:[   24] Training loss: 0.11672684, Validation loss: 0.11243388, Gradient norm: 1.92374667
INFO:root:[   25] Training loss: 0.11460547, Validation loss: 0.11085022, Gradient norm: 1.75215400
INFO:root:[   26] Training loss: 0.11221603, Validation loss: 0.11507598, Gradient norm: 1.60771477
INFO:root:[   27] Training loss: 0.11064679, Validation loss: 0.11411946, Gradient norm: 1.73962457
INFO:root:[   28] Training loss: 0.10768841, Validation loss: 0.11362223, Gradient norm: 1.50839131
INFO:root:[   29] Training loss: 0.10851929, Validation loss: 0.10785451, Gradient norm: 1.86623655
INFO:root:[   30] Training loss: 0.10571310, Validation loss: 0.10137017, Gradient norm: 1.68275212
INFO:root:[   31] Training loss: 0.10389692, Validation loss: 0.10811916, Gradient norm: 1.68481762
INFO:root:[   32] Training loss: 0.10315291, Validation loss: 0.10485229, Gradient norm: 1.81575078
INFO:root:[   33] Training loss: 0.10107551, Validation loss: 0.09827154, Gradient norm: 1.83085919
INFO:root:[   34] Training loss: 0.09865021, Validation loss: 0.10478205, Gradient norm: 1.67612678
INFO:root:[   35] Training loss: 0.09768445, Validation loss: 0.09148729, Gradient norm: 1.81531438
INFO:root:[   36] Training loss: 0.09655494, Validation loss: 0.09432442, Gradient norm: 1.83655492
INFO:root:[   37] Training loss: 0.09473320, Validation loss: 0.09012818, Gradient norm: 1.71599970
INFO:root:[   38] Training loss: 0.09255014, Validation loss: 0.08892831, Gradient norm: 1.70154998
INFO:root:[   39] Training loss: 0.09083727, Validation loss: 0.08698091, Gradient norm: 1.69475505
INFO:root:[   40] Training loss: 0.09109662, Validation loss: 0.09181044, Gradient norm: 1.93223270
INFO:root:[   41] Training loss: 0.08910813, Validation loss: 0.08511141, Gradient norm: 1.85695956
INFO:root:[   42] Training loss: 0.08809726, Validation loss: 0.08888217, Gradient norm: 1.92573684
INFO:root:[   43] Training loss: 0.08665355, Validation loss: 0.08419411, Gradient norm: 1.81750965
INFO:root:[   44] Training loss: 0.08650749, Validation loss: 0.08992068, Gradient norm: 1.90761354
INFO:root:[   45] Training loss: 0.08634089, Validation loss: 0.08655344, Gradient norm: 2.08044678
INFO:root:[   46] Training loss: 0.08272319, Validation loss: 0.08156168, Gradient norm: 1.74162722
INFO:root:[   47] Training loss: 0.08131741, Validation loss: 0.08534223, Gradient norm: 1.78476174
INFO:root:[   48] Training loss: 0.07978349, Validation loss: 0.08873148, Gradient norm: 1.70569350
INFO:root:[   49] Training loss: 0.08034267, Validation loss: 0.08222008, Gradient norm: 2.15271106
INFO:root:[   50] Training loss: 0.07680209, Validation loss: 0.07488647, Gradient norm: 1.80366551
INFO:root:[   51] Training loss: 0.07659834, Validation loss: 0.07586721, Gradient norm: 1.86667702
INFO:root:[   52] Training loss: 0.07409457, Validation loss: 0.07847325, Gradient norm: 1.81666571
INFO:root:[   53] Training loss: 0.07569874, Validation loss: 0.07010654, Gradient norm: 2.13518114
INFO:root:[   54] Training loss: 0.07456677, Validation loss: 0.07261242, Gradient norm: 2.03149873
INFO:root:[   55] Training loss: 0.07342346, Validation loss: 0.07368481, Gradient norm: 2.00270908
INFO:root:[   56] Training loss: 0.07160172, Validation loss: 0.07830059, Gradient norm: 2.00914565
INFO:root:[   57] Training loss: 0.07005564, Validation loss: 0.06521114, Gradient norm: 1.95105922
INFO:root:[   58] Training loss: 0.07012439, Validation loss: 0.06355845, Gradient norm: 1.79360889
INFO:root:[   59] Training loss: 0.06862251, Validation loss: 0.06963950, Gradient norm: 1.86112189
INFO:root:[   60] Training loss: 0.06702303, Validation loss: 0.07311481, Gradient norm: 1.93810558
INFO:root:[   61] Training loss: 0.06609636, Validation loss: 0.06019364, Gradient norm: 1.97574012
INFO:root:[   62] Training loss: 0.06374705, Validation loss: 0.05822895, Gradient norm: 1.86616629
INFO:root:[   63] Training loss: 0.06274744, Validation loss: 0.06080196, Gradient norm: 1.85217138
INFO:root:[   64] Training loss: 0.06232258, Validation loss: 0.05744279, Gradient norm: 1.98973142
INFO:root:[   65] Training loss: 0.06170748, Validation loss: 0.05758315, Gradient norm: 2.05647879
INFO:root:[   66] Training loss: 0.06153453, Validation loss: 0.05597801, Gradient norm: 1.98721424
INFO:root:[   67] Training loss: 0.06288196, Validation loss: 0.05431055, Gradient norm: 2.19531981
INFO:root:[   68] Training loss: 0.06021793, Validation loss: 0.06160450, Gradient norm: 2.07240389
INFO:root:[   69] Training loss: 0.05995289, Validation loss: 0.06263582, Gradient norm: 2.07968957
INFO:root:[   70] Training loss: 0.05807223, Validation loss: 0.05430760, Gradient norm: 1.92101383
INFO:root:[   71] Training loss: 0.05556166, Validation loss: 0.05050659, Gradient norm: 1.89272027
INFO:root:[   72] Training loss: 0.05583161, Validation loss: 0.05802013, Gradient norm: 2.02282677
INFO:root:[   73] Training loss: 0.05373394, Validation loss: 0.05924584, Gradient norm: 1.82255800
INFO:root:[   74] Training loss: 0.05399365, Validation loss: 0.05709703, Gradient norm: 2.05817475
INFO:root:[   75] Training loss: 0.05211324, Validation loss: 0.05439809, Gradient norm: 1.92075772
INFO:root:[   76] Training loss: 0.05245536, Validation loss: 0.04991901, Gradient norm: 2.03481460
INFO:root:[   77] Training loss: 0.05301928, Validation loss: 0.05618658, Gradient norm: 2.00539806
INFO:root:[   78] Training loss: 0.05427081, Validation loss: 0.05152478, Gradient norm: 2.05189907
INFO:root:[   79] Training loss: 0.05077790, Validation loss: 0.05152799, Gradient norm: 1.88851155
INFO:root:[   80] Training loss: 0.04802867, Validation loss: 0.04462493, Gradient norm: 1.83927027
INFO:root:[   81] Training loss: 0.04785719, Validation loss: 0.04295618, Gradient norm: 1.92303816
INFO:root:[   82] Training loss: 0.04864802, Validation loss: 0.05360401, Gradient norm: 1.96300604
INFO:root:[   83] Training loss: 0.04902606, Validation loss: 0.04892824, Gradient norm: 2.18247022
INFO:root:[   84] Training loss: 0.04657407, Validation loss: 0.04684377, Gradient norm: 2.04504872
INFO:root:[   85] Training loss: 0.04536069, Validation loss: 0.04053423, Gradient norm: 1.90846862
INFO:root:[   86] Training loss: 0.04392028, Validation loss: 0.04298134, Gradient norm: 1.81228794
INFO:root:[   87] Training loss: 0.04478901, Validation loss: 0.04927175, Gradient norm: 1.78282251
INFO:root:[   88] Training loss: 0.04333491, Validation loss: 0.05066898, Gradient norm: 1.78481934
INFO:root:[   89] Training loss: 0.04654430, Validation loss: 0.04644131, Gradient norm: 2.16937768
INFO:root:[   90] Training loss: 0.04302983, Validation loss: 0.04865655, Gradient norm: 1.81118664
INFO:root:[   91] Training loss: 0.04273744, Validation loss: 0.04591665, Gradient norm: 1.85906635
INFO:root:[   92] Training loss: 0.04144881, Validation loss: 0.04224915, Gradient norm: 1.86556248
INFO:root:[   93] Training loss: 0.03932071, Validation loss: 0.03574233, Gradient norm: 1.73345812
INFO:root:[   94] Training loss: 0.04011210, Validation loss: 0.03726614, Gradient norm: 1.83366741
INFO:root:[   95] Training loss: 0.03942852, Validation loss: 0.04653005, Gradient norm: 1.71413590
INFO:root:[   96] Training loss: 0.04001212, Validation loss: 0.04165375, Gradient norm: 1.90565819
INFO:root:[   97] Training loss: 0.03983986, Validation loss: 0.04808697, Gradient norm: 1.70592529
INFO:root:[   98] Training loss: 0.04161150, Validation loss: 0.04750908, Gradient norm: 1.95035929
INFO:root:[   99] Training loss: 0.03909778, Validation loss: 0.03465132, Gradient norm: 1.91994884
INFO:root:[  100] Training loss: 0.03779604, Validation loss: 0.03799569, Gradient norm: 1.58181475
INFO:root:[  101] Training loss: 0.03858859, Validation loss: 0.03653353, Gradient norm: 1.79246555
INFO:root:[  102] Training loss: 0.03729599, Validation loss: 0.03593033, Gradient norm: 1.81642052
INFO:root:[  103] Training loss: 0.03650566, Validation loss: 0.03391388, Gradient norm: 1.75019463
INFO:root:[  104] Training loss: 0.03740831, Validation loss: 0.03195100, Gradient norm: 1.79223991
INFO:root:[  105] Training loss: 0.03702109, Validation loss: 0.04039419, Gradient norm: 1.84780598
INFO:root:[  106] Training loss: 0.03725486, Validation loss: 0.03454462, Gradient norm: 1.64354854
INFO:root:[  107] Training loss: 0.03635112, Validation loss: 0.03218046, Gradient norm: 1.80970270
INFO:root:[  108] Training loss: 0.03615933, Validation loss: 0.03911499, Gradient norm: 1.67614555
INFO:root:[  109] Training loss: 0.03612317, Validation loss: 0.03172957, Gradient norm: 1.80263392
INFO:root:[  110] Training loss: 0.03495013, Validation loss: 0.02952467, Gradient norm: 1.72137041
INFO:root:[  111] Training loss: 0.03490210, Validation loss: 0.03098515, Gradient norm: 1.63127429
INFO:root:[  112] Training loss: 0.03604463, Validation loss: 0.03936139, Gradient norm: 1.56475710
INFO:root:[  113] Training loss: 0.03556638, Validation loss: 0.04010592, Gradient norm: 1.70727305
INFO:root:[  114] Training loss: 0.03407025, Validation loss: 0.03865688, Gradient norm: 1.52288419
INFO:root:[  115] Training loss: 0.03490601, Validation loss: 0.03114784, Gradient norm: 1.70541180
INFO:root:[  116] Training loss: 0.03454872, Validation loss: 0.03201019, Gradient norm: 1.51571654
INFO:root:[  117] Training loss: 0.03662912, Validation loss: 0.03615521, Gradient norm: 1.71075178
INFO:root:[  118] Training loss: 0.03651042, Validation loss: 0.03460991, Gradient norm: 1.69951026
INFO:root:[  119] Training loss: 0.03248230, Validation loss: 0.03591977, Gradient norm: 1.50585848
INFO:root:EP 119: Early stopping
INFO:root:Training the model took 1559.064s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01244
INFO:root:EnergyScoreTrain: 0.01356
INFO:root:CoverageTrain: 0.99944
INFO:root:IntervalWidthTrain: 0.0261
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01232
INFO:root:EnergyScoreValidation: 0.01336
INFO:root:CoverageValidation: 0.99951
INFO:root:IntervalWidthValidation: 0.02612
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01213
INFO:root:EnergyScoreTest: 0.01358
INFO:root:CoverageTest: 0.99945
INFO:root:IntervalWidthTest: 0.02553
INFO:root:###23 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.79331399, Validation loss: 0.29681651, Gradient norm: 3.19537489
INFO:root:[    2] Training loss: 0.25672009, Validation loss: 0.23184036, Gradient norm: 1.33789717
INFO:root:[    3] Training loss: 0.22299443, Validation loss: 0.20529332, Gradient norm: 1.37076220
INFO:root:[    4] Training loss: 0.20178057, Validation loss: 0.19644155, Gradient norm: 1.16096030
INFO:root:[    5] Training loss: 0.18918552, Validation loss: 0.19601028, Gradient norm: 1.25644486
INFO:root:[    6] Training loss: 0.18540525, Validation loss: 0.17695350, Gradient norm: 1.24305289
INFO:root:[    7] Training loss: 0.17474796, Validation loss: 0.17271469, Gradient norm: 1.35720021
INFO:root:[    8] Training loss: 0.17420781, Validation loss: 0.16893065, Gradient norm: 1.82261460
INFO:root:[    9] Training loss: 0.16944189, Validation loss: 0.16158666, Gradient norm: 1.91255460
INFO:root:[   10] Training loss: 0.16430523, Validation loss: 0.15709289, Gradient norm: 1.79535690
INFO:root:[   11] Training loss: 0.16019988, Validation loss: 0.15200673, Gradient norm: 1.57619850
INFO:root:[   12] Training loss: 0.15510944, Validation loss: 0.16096561, Gradient norm: 1.46500636
INFO:root:[   13] Training loss: 0.15038015, Validation loss: 0.14850902, Gradient norm: 1.32298712
INFO:root:[   14] Training loss: 0.14928300, Validation loss: 0.15144279, Gradient norm: 1.55227719
INFO:root:[   15] Training loss: 0.14592398, Validation loss: 0.14530083, Gradient norm: 1.47023444
INFO:root:[   16] Training loss: 0.14583071, Validation loss: 0.14803712, Gradient norm: 1.73223452
INFO:root:[   17] Training loss: 0.14213492, Validation loss: 0.14156674, Gradient norm: 1.56355583
INFO:root:[   18] Training loss: 0.13906616, Validation loss: 0.13914109, Gradient norm: 1.50245104
INFO:root:[   19] Training loss: 0.13666720, Validation loss: 0.14146595, Gradient norm: 1.53922741
INFO:root:[   20] Training loss: 0.13470721, Validation loss: 0.13877957, Gradient norm: 1.70845046
INFO:root:[   21] Training loss: 0.13393809, Validation loss: 0.12846000, Gradient norm: 1.82144849
INFO:root:[   22] Training loss: 0.13034671, Validation loss: 0.13136811, Gradient norm: 1.62258569
INFO:root:[   23] Training loss: 0.12671191, Validation loss: 0.12195835, Gradient norm: 1.45477069
INFO:root:[   24] Training loss: 0.12642623, Validation loss: 0.12797346, Gradient norm: 1.70315966
INFO:root:[   25] Training loss: 0.12295318, Validation loss: 0.12622043, Gradient norm: 1.53598510
INFO:root:[   26] Training loss: 0.12007906, Validation loss: 0.11600395, Gradient norm: 1.48934245
INFO:root:[   27] Training loss: 0.11956738, Validation loss: 0.12210989, Gradient norm: 1.59807852
INFO:root:[   28] Training loss: 0.11974172, Validation loss: 0.11572464, Gradient norm: 1.84085132
INFO:root:[   29] Training loss: 0.11502783, Validation loss: 0.10820519, Gradient norm: 1.62083089
INFO:root:[   30] Training loss: 0.11218419, Validation loss: 0.10587024, Gradient norm: 1.57800184
INFO:root:[   31] Training loss: 0.11058108, Validation loss: 0.11384095, Gradient norm: 1.61642241
INFO:root:[   32] Training loss: 0.10842626, Validation loss: 0.11061971, Gradient norm: 1.60954238
INFO:root:[   33] Training loss: 0.10646655, Validation loss: 0.10240756, Gradient norm: 1.57175512
INFO:root:[   34] Training loss: 0.10399655, Validation loss: 0.10687951, Gradient norm: 1.54331149
INFO:root:[   35] Training loss: 0.10275613, Validation loss: 0.10137330, Gradient norm: 1.56741168
INFO:root:[   36] Training loss: 0.10151582, Validation loss: 0.10328525, Gradient norm: 1.77751001
INFO:root:[   37] Training loss: 0.09805816, Validation loss: 0.09510072, Gradient norm: 1.69860636
INFO:root:[   38] Training loss: 0.09440818, Validation loss: 0.09157749, Gradient norm: 1.46320121
INFO:root:[   39] Training loss: 0.09506099, Validation loss: 0.09235156, Gradient norm: 1.80815546
INFO:root:[   40] Training loss: 0.09188564, Validation loss: 0.09018803, Gradient norm: 1.67337773
INFO:root:[   41] Training loss: 0.09215914, Validation loss: 0.08971264, Gradient norm: 1.78865358
INFO:root:[   42] Training loss: 0.08824419, Validation loss: 0.08432374, Gradient norm: 1.64208616
INFO:root:[   43] Training loss: 0.08502174, Validation loss: 0.08832690, Gradient norm: 1.50596383
INFO:root:[   44] Training loss: 0.08558875, Validation loss: 0.07934909, Gradient norm: 1.65363017
INFO:root:[   45] Training loss: 0.08254034, Validation loss: 0.07672438, Gradient norm: 1.69106711
INFO:root:[   46] Training loss: 0.08000789, Validation loss: 0.07831043, Gradient norm: 1.61102452
INFO:root:[   47] Training loss: 0.07916266, Validation loss: 0.07318180, Gradient norm: 1.66661971
INFO:root:[   48] Training loss: 0.07694274, Validation loss: 0.07207620, Gradient norm: 1.58788346
INFO:root:[   49] Training loss: 0.07583991, Validation loss: 0.07068992, Gradient norm: 1.74363725
INFO:root:[   50] Training loss: 0.07432997, Validation loss: 0.07197632, Gradient norm: 1.68868035
INFO:root:[   51] Training loss: 0.07479527, Validation loss: 0.06735918, Gradient norm: 1.82248353
INFO:root:[   52] Training loss: 0.07193166, Validation loss: 0.07236045, Gradient norm: 1.70889457
INFO:root:[   53] Training loss: 0.06890558, Validation loss: 0.06489082, Gradient norm: 1.61396352
INFO:root:[   54] Training loss: 0.06876986, Validation loss: 0.06229311, Gradient norm: 1.76522783
INFO:root:[   55] Training loss: 0.06712847, Validation loss: 0.06428582, Gradient norm: 1.68052798
INFO:root:[   56] Training loss: 0.06666002, Validation loss: 0.06595230, Gradient norm: 1.70783431
INFO:root:[   57] Training loss: 0.06419834, Validation loss: 0.06131836, Gradient norm: 1.64777695
INFO:root:[   58] Training loss: 0.06398999, Validation loss: 0.05815201, Gradient norm: 1.80616106
INFO:root:[   59] Training loss: 0.06576299, Validation loss: 0.06657304, Gradient norm: 2.11919212
INFO:root:[   60] Training loss: 0.06245331, Validation loss: 0.05742785, Gradient norm: 1.80248307
INFO:root:[   61] Training loss: 0.05926679, Validation loss: 0.05460803, Gradient norm: 1.54045158
INFO:root:[   62] Training loss: 0.05760626, Validation loss: 0.06021638, Gradient norm: 1.56987127
INFO:root:[   63] Training loss: 0.05710307, Validation loss: 0.06295327, Gradient norm: 1.65468212
INFO:root:[   64] Training loss: 0.05781925, Validation loss: 0.05876305, Gradient norm: 1.89024952
INFO:root:[   65] Training loss: 0.05385194, Validation loss: 0.05489096, Gradient norm: 1.47242338
INFO:root:[   66] Training loss: 0.05463211, Validation loss: 0.05504902, Gradient norm: 1.68094603
INFO:root:[   67] Training loss: 0.05247254, Validation loss: 0.05430236, Gradient norm: 1.44838945
INFO:root:[   68] Training loss: 0.05255557, Validation loss: 0.05373858, Gradient norm: 1.73766973
INFO:root:[   69] Training loss: 0.05013783, Validation loss: 0.05056216, Gradient norm: 1.54295043
INFO:root:[   70] Training loss: 0.05036560, Validation loss: 0.05420322, Gradient norm: 1.59916802
INFO:root:[   71] Training loss: 0.05023885, Validation loss: 0.04819815, Gradient norm: 1.69722235
INFO:root:[   72] Training loss: 0.04926130, Validation loss: 0.04652669, Gradient norm: 1.63231980
INFO:root:[   73] Training loss: 0.04816496, Validation loss: 0.04466805, Gradient norm: 1.65259542
INFO:root:[   74] Training loss: 0.04733483, Validation loss: 0.05424263, Gradient norm: 1.57724183
INFO:root:[   75] Training loss: 0.04786622, Validation loss: 0.04248580, Gradient norm: 1.76848084
INFO:root:[   76] Training loss: 0.04426204, Validation loss: 0.04814935, Gradient norm: 1.44642582
INFO:root:[   77] Training loss: 0.04506485, Validation loss: 0.04090177, Gradient norm: 1.62579841
INFO:root:[   78] Training loss: 0.04665659, Validation loss: 0.04230605, Gradient norm: 1.80164563
INFO:root:[   79] Training loss: 0.04548413, Validation loss: 0.05158839, Gradient norm: 1.68357139
INFO:root:[   80] Training loss: 0.04643361, Validation loss: 0.05580314, Gradient norm: 1.75742076
INFO:root:[   81] Training loss: 0.04505979, Validation loss: 0.03834069, Gradient norm: 1.86899178
INFO:root:[   82] Training loss: 0.04375596, Validation loss: 0.04204168, Gradient norm: 1.71715587
INFO:root:[   83] Training loss: 0.04442916, Validation loss: 0.03987054, Gradient norm: 1.66325829
INFO:root:[   84] Training loss: 0.04182248, Validation loss: 0.04442781, Gradient norm: 1.58821665
INFO:root:[   85] Training loss: 0.04198374, Validation loss: 0.04269139, Gradient norm: 1.53037532
INFO:root:[   86] Training loss: 0.04253855, Validation loss: 0.04166405, Gradient norm: 1.60123906
INFO:root:[   87] Training loss: 0.04141701, Validation loss: 0.03974169, Gradient norm: 1.54503529
INFO:root:[   88] Training loss: 0.04037537, Validation loss: 0.03808979, Gradient norm: 1.45344330
INFO:root:[   89] Training loss: 0.03914227, Validation loss: 0.03957639, Gradient norm: 1.25621164
INFO:root:[   90] Training loss: 0.03763499, Validation loss: 0.04300248, Gradient norm: 1.21095032
INFO:root:[   91] Training loss: 0.03916486, Validation loss: 0.03681450, Gradient norm: 1.08771054
INFO:root:[   92] Training loss: 0.03915725, Validation loss: 0.04432730, Gradient norm: 1.38937851
INFO:root:[   93] Training loss: 0.03962029, Validation loss: 0.03595257, Gradient norm: 1.41905504
INFO:root:[   94] Training loss: 0.04019505, Validation loss: 0.03943247, Gradient norm: 1.44245886
INFO:root:[   95] Training loss: 0.03964003, Validation loss: 0.03444782, Gradient norm: 1.46499949
INFO:root:[   96] Training loss: 0.03889630, Validation loss: 0.03621747, Gradient norm: 1.42203746
INFO:root:[   97] Training loss: 0.03846996, Validation loss: 0.03651101, Gradient norm: 1.34207356
INFO:root:[   98] Training loss: 0.03789388, Validation loss: 0.03994177, Gradient norm: 1.35091779
INFO:root:[   99] Training loss: 0.03896785, Validation loss: 0.03953586, Gradient norm: 1.27160828
INFO:root:[  100] Training loss: 0.03988155, Validation loss: 0.03413005, Gradient norm: 1.31079998
INFO:root:[  101] Training loss: 0.03894098, Validation loss: 0.03451453, Gradient norm: 1.42399742
INFO:root:[  102] Training loss: 0.03641966, Validation loss: 0.03546847, Gradient norm: 1.14334030
INFO:root:[  103] Training loss: 0.03857615, Validation loss: 0.03514469, Gradient norm: 1.25800888
INFO:root:[  104] Training loss: 0.03727169, Validation loss: 0.03967365, Gradient norm: 1.15265215
INFO:root:[  105] Training loss: 0.03808585, Validation loss: 0.03448127, Gradient norm: 1.28657297
INFO:root:[  106] Training loss: 0.03857613, Validation loss: 0.03976966, Gradient norm: 1.29420564
INFO:root:[  107] Training loss: 0.03767142, Validation loss: 0.04329515, Gradient norm: 1.25687526
INFO:root:[  108] Training loss: 0.03967838, Validation loss: 0.03356680, Gradient norm: 1.43809007
INFO:root:[  109] Training loss: 0.03839127, Validation loss: 0.03859431, Gradient norm: 1.24539416
INFO:root:[  110] Training loss: 0.03773389, Validation loss: 0.03875260, Gradient norm: 1.36888145
INFO:root:[  111] Training loss: 0.03646228, Validation loss: 0.04471387, Gradient norm: 1.30471385
INFO:root:[  112] Training loss: 0.03783801, Validation loss: 0.03526669, Gradient norm: 1.27346547
INFO:root:[  113] Training loss: 0.03841682, Validation loss: 0.03865506, Gradient norm: 1.29885909
INFO:root:[  114] Training loss: 0.03759844, Validation loss: 0.03583593, Gradient norm: 1.31696530
INFO:root:[  115] Training loss: 0.03584824, Validation loss: 0.03734248, Gradient norm: 1.21533246
INFO:root:[  116] Training loss: 0.03779249, Validation loss: 0.03333990, Gradient norm: 1.32376047
INFO:root:[  117] Training loss: 0.03780228, Validation loss: 0.03699345, Gradient norm: 1.27784678
INFO:root:[  118] Training loss: 0.03706280, Validation loss: 0.03204364, Gradient norm: 1.17745016
INFO:root:[  119] Training loss: 0.03832798, Validation loss: 0.04028064, Gradient norm: 1.42603065
INFO:root:[  120] Training loss: 0.03825501, Validation loss: 0.03591776, Gradient norm: 1.26833466
INFO:root:[  121] Training loss: 0.03672237, Validation loss: 0.03571382, Gradient norm: 1.37633609
INFO:root:[  122] Training loss: 0.03642853, Validation loss: 0.03929492, Gradient norm: 1.27018803
INFO:root:[  123] Training loss: 0.03656723, Validation loss: 0.03511736, Gradient norm: 1.10677507
INFO:root:[  124] Training loss: 0.03757857, Validation loss: 0.03848196, Gradient norm: 1.30854578
INFO:root:[  125] Training loss: 0.03662770, Validation loss: 0.04021223, Gradient norm: 1.23458007
INFO:root:[  126] Training loss: 0.03781090, Validation loss: 0.03514881, Gradient norm: 1.33688011
INFO:root:[  127] Training loss: 0.03758379, Validation loss: 0.03583732, Gradient norm: 1.44478375
INFO:root:EP 127: Early stopping
INFO:root:Training the model took 1662.367s.
INFO:root:Emptying the cuda cache took 0.034s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0147
INFO:root:EnergyScoreTrain: 0.01456
INFO:root:CoverageTrain: 0.99955
INFO:root:IntervalWidthTrain: 0.02606
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01469
INFO:root:EnergyScoreValidation: 0.01468
INFO:root:CoverageValidation: 0.99956
INFO:root:IntervalWidthValidation: 0.02601
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01441
INFO:root:EnergyScoreTest: 0.01445
INFO:root:CoverageTest: 0.99947
INFO:root:IntervalWidthTest: 0.0256
INFO:root:###24 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.05848805, Validation loss: 0.39300474, Gradient norm: 3.74199570
INFO:root:[    2] Training loss: 0.33454947, Validation loss: 0.29735278, Gradient norm: 1.24573657
INFO:root:[    3] Training loss: 0.28412555, Validation loss: 0.27311933, Gradient norm: 1.26359429
INFO:root:[    4] Training loss: 0.25417878, Validation loss: 0.24472163, Gradient norm: 0.71037414
INFO:root:[    5] Training loss: 0.24064000, Validation loss: 0.22858787, Gradient norm: 1.19717795
INFO:root:[    6] Training loss: 0.22648506, Validation loss: 0.21654271, Gradient norm: 1.10253459
INFO:root:[    7] Training loss: 0.22009788, Validation loss: 0.22604173, Gradient norm: 1.45395194
INFO:root:[    8] Training loss: 0.21140579, Validation loss: 0.20700827, Gradient norm: 1.21653709
INFO:root:[    9] Training loss: 0.20418213, Validation loss: 0.20126311, Gradient norm: 1.12362037
INFO:root:[   10] Training loss: 0.19927303, Validation loss: 0.19354355, Gradient norm: 1.38564328
INFO:root:[   11] Training loss: 0.19347682, Validation loss: 0.19898696, Gradient norm: 1.36710922
INFO:root:[   12] Training loss: 0.18780980, Validation loss: 0.18300270, Gradient norm: 1.33529702
INFO:root:[   13] Training loss: 0.17999584, Validation loss: 0.18818308, Gradient norm: 1.15873199
INFO:root:[   14] Training loss: 0.17821203, Validation loss: 0.18296103, Gradient norm: 1.71361297
INFO:root:[   15] Training loss: 0.17157279, Validation loss: 0.16391687, Gradient norm: 1.35290719
INFO:root:[   16] Training loss: 0.16731689, Validation loss: 0.16329152, Gradient norm: 1.50359298
INFO:root:[   17] Training loss: 0.16101668, Validation loss: 0.15483568, Gradient norm: 1.32475713
INFO:root:[   18] Training loss: 0.15607728, Validation loss: 0.15686745, Gradient norm: 1.23707436
INFO:root:[   19] Training loss: 0.15536844, Validation loss: 0.15353577, Gradient norm: 1.67832185
INFO:root:[   20] Training loss: 0.14853621, Validation loss: 0.14180960, Gradient norm: 1.45875586
INFO:root:[   21] Training loss: 0.14384198, Validation loss: 0.13881955, Gradient norm: 1.40231008
INFO:root:[   22] Training loss: 0.14007416, Validation loss: 0.13462359, Gradient norm: 1.45232497
INFO:root:[   23] Training loss: 0.13590718, Validation loss: 0.12947719, Gradient norm: 1.46969216
INFO:root:[   24] Training loss: 0.13212192, Validation loss: 0.12590012, Gradient norm: 1.51313782
INFO:root:[   25] Training loss: 0.12794655, Validation loss: 0.12690482, Gradient norm: 1.44343093
INFO:root:[   26] Training loss: 0.12536126, Validation loss: 0.12539889, Gradient norm: 1.58094505
INFO:root:[   27] Training loss: 0.12100964, Validation loss: 0.12224303, Gradient norm: 1.55316953
INFO:root:[   28] Training loss: 0.11665532, Validation loss: 0.11334279, Gradient norm: 1.46617116
INFO:root:[   29] Training loss: 0.11408290, Validation loss: 0.11318416, Gradient norm: 1.57235646
INFO:root:[   30] Training loss: 0.11076845, Validation loss: 0.10442782, Gradient norm: 1.66880398
INFO:root:[   31] Training loss: 0.10799965, Validation loss: 0.10271806, Gradient norm: 1.65603261
INFO:root:[   32] Training loss: 0.10343383, Validation loss: 0.10548624, Gradient norm: 1.58049538
INFO:root:[   33] Training loss: 0.10011517, Validation loss: 0.10013139, Gradient norm: 1.58556095
INFO:root:[   34] Training loss: 0.09768811, Validation loss: 0.09549633, Gradient norm: 1.68421220
INFO:root:[   35] Training loss: 0.09385346, Validation loss: 0.09524217, Gradient norm: 1.50298443
INFO:root:[   36] Training loss: 0.09085896, Validation loss: 0.09361533, Gradient norm: 1.56106164
INFO:root:[   37] Training loss: 0.08774541, Validation loss: 0.09404045, Gradient norm: 1.62194434
INFO:root:[   38] Training loss: 0.08589314, Validation loss: 0.08729441, Gradient norm: 1.73049806
INFO:root:[   39] Training loss: 0.08271011, Validation loss: 0.08365419, Gradient norm: 1.62928660
INFO:root:[   40] Training loss: 0.08017442, Validation loss: 0.07723556, Gradient norm: 1.67671852
INFO:root:[   41] Training loss: 0.07768877, Validation loss: 0.07218229, Gradient norm: 1.66237688
INFO:root:[   42] Training loss: 0.07591698, Validation loss: 0.07585167, Gradient norm: 1.75722366
INFO:root:[   43] Training loss: 0.07178607, Validation loss: 0.07066556, Gradient norm: 1.43774976
INFO:root:[   44] Training loss: 0.06990725, Validation loss: 0.06508382, Gradient norm: 1.49637660
INFO:root:[   45] Training loss: 0.06858102, Validation loss: 0.06544500, Gradient norm: 1.59179094
INFO:root:[   46] Training loss: 0.06507346, Validation loss: 0.06539173, Gradient norm: 1.40850237
INFO:root:[   47] Training loss: 0.06399192, Validation loss: 0.06261305, Gradient norm: 1.63984350
INFO:root:[   48] Training loss: 0.06222684, Validation loss: 0.06307743, Gradient norm: 1.74106319
INFO:root:[   49] Training loss: 0.06121019, Validation loss: 0.06382892, Gradient norm: 1.68073471
INFO:root:[   50] Training loss: 0.06125325, Validation loss: 0.06188159, Gradient norm: 1.72254161
INFO:root:[   51] Training loss: 0.05968764, Validation loss: 0.05478398, Gradient norm: 1.67971452
INFO:root:[   52] Training loss: 0.05773236, Validation loss: 0.05588224, Gradient norm: 1.67790824
INFO:root:[   53] Training loss: 0.05771630, Validation loss: 0.05386858, Gradient norm: 1.75071169
INFO:root:[   54] Training loss: 0.05689341, Validation loss: 0.06074197, Gradient norm: 1.77558233
INFO:root:[   55] Training loss: 0.05556493, Validation loss: 0.06007677, Gradient norm: 1.69896166
INFO:root:[   56] Training loss: 0.05586322, Validation loss: 0.05019122, Gradient norm: 1.89493299
INFO:root:[   57] Training loss: 0.05530743, Validation loss: 0.05619412, Gradient norm: 1.68602306
INFO:root:[   58] Training loss: 0.05259204, Validation loss: 0.04968015, Gradient norm: 1.62955108
INFO:root:[   59] Training loss: 0.05323241, Validation loss: 0.05580708, Gradient norm: 1.78600668
INFO:root:[   60] Training loss: 0.05230395, Validation loss: 0.05372118, Gradient norm: 1.73121278
INFO:root:[   61] Training loss: 0.05249395, Validation loss: 0.04975165, Gradient norm: 1.72341721
INFO:root:[   62] Training loss: 0.05277701, Validation loss: 0.05801504, Gradient norm: 1.91343752
INFO:root:[   63] Training loss: 0.05025909, Validation loss: 0.05188198, Gradient norm: 1.63130320
INFO:root:[   64] Training loss: 0.05037913, Validation loss: 0.04547431, Gradient norm: 1.75849674
INFO:root:[   65] Training loss: 0.05145695, Validation loss: 0.04567949, Gradient norm: 1.88138756
INFO:root:[   66] Training loss: 0.05118088, Validation loss: 0.05163812, Gradient norm: 1.82733382
INFO:root:[   67] Training loss: 0.05013549, Validation loss: 0.04613733, Gradient norm: 1.75210692
INFO:root:[   68] Training loss: 0.04937077, Validation loss: 0.04771471, Gradient norm: 1.67509425
INFO:root:[   69] Training loss: 0.04958828, Validation loss: 0.04421148, Gradient norm: 1.80271843
INFO:root:[   70] Training loss: 0.04973999, Validation loss: 0.04368472, Gradient norm: 1.74666744
INFO:root:[   71] Training loss: 0.05016203, Validation loss: 0.05351168, Gradient norm: 1.80877762
INFO:root:[   72] Training loss: 0.05023974, Validation loss: 0.05024462, Gradient norm: 1.74730328
INFO:root:[   73] Training loss: 0.05031968, Validation loss: 0.05165335, Gradient norm: 1.73452276
INFO:root:[   74] Training loss: 0.04973045, Validation loss: 0.04852890, Gradient norm: 1.66313190
INFO:root:[   75] Training loss: 0.04833133, Validation loss: 0.05038335, Gradient norm: 1.59912149
INFO:root:[   76] Training loss: 0.05069791, Validation loss: 0.05192899, Gradient norm: 1.83285458
INFO:root:[   77] Training loss: 0.04840291, Validation loss: 0.04638561, Gradient norm: 1.50144368
INFO:root:[   78] Training loss: 0.04784519, Validation loss: 0.04949098, Gradient norm: 1.55056328
INFO:root:[   79] Training loss: 0.04783166, Validation loss: 0.04563443, Gradient norm: 1.65597589
INFO:root:EP 79: Early stopping
INFO:root:Training the model took 1040.576s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01877
INFO:root:EnergyScoreTrain: 0.01657
INFO:root:CoverageTrain: 0.9998
INFO:root:IntervalWidthTrain: 0.03353
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01846
INFO:root:EnergyScoreValidation: 0.01649
INFO:root:CoverageValidation: 0.99982
INFO:root:IntervalWidthValidation: 0.03349
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01854
INFO:root:EnergyScoreTest: 0.01635
INFO:root:CoverageTest: 0.99976
INFO:root:IntervalWidthTest: 0.03297
