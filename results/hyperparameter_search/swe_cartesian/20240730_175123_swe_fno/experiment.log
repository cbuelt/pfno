INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10, 'ood': True}
INFO:root:###1 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.69563750, Validation loss: 0.08266953, Gradient norm: 4.84942164
INFO:root:[    2] Training loss: 0.09151648, Validation loss: 0.07528793, Gradient norm: 3.09510835
INFO:root:[    3] Training loss: 0.06903952, Validation loss: 0.05740350, Gradient norm: 2.17525858
INFO:root:[    4] Training loss: 0.05948543, Validation loss: 0.05640643, Gradient norm: 3.03881531
INFO:root:[    5] Training loss: 0.05361147, Validation loss: 0.05398561, Gradient norm: 2.74444514
INFO:root:[    6] Training loss: 0.05374077, Validation loss: 0.05044205, Gradient norm: 3.14328251
INFO:root:[    7] Training loss: 0.05027420, Validation loss: 0.04297948, Gradient norm: 2.68776235
INFO:root:[    8] Training loss: 0.04579748, Validation loss: 0.04072147, Gradient norm: 2.25478693
INFO:root:[    9] Training loss: 0.04510178, Validation loss: 0.05054938, Gradient norm: 2.30862055
INFO:root:[   10] Training loss: 0.04379979, Validation loss: 0.05086186, Gradient norm: 1.77749055
INFO:root:[   11] Training loss: 0.04641162, Validation loss: 0.04013436, Gradient norm: 2.66956322
INFO:root:[   12] Training loss: 0.04296334, Validation loss: 0.03570445, Gradient norm: 2.48169230
INFO:root:[   13] Training loss: 0.04214404, Validation loss: 0.04583035, Gradient norm: 2.38217792
INFO:root:[   14] Training loss: 0.04109373, Validation loss: 0.03794871, Gradient norm: 2.13306950
INFO:root:[   15] Training loss: 0.04031874, Validation loss: 0.05180568, Gradient norm: 2.20033816
INFO:root:[   16] Training loss: 0.04063560, Validation loss: 0.03361042, Gradient norm: 2.36555631
INFO:root:[   17] Training loss: 0.03811477, Validation loss: 0.04377197, Gradient norm: 2.20945560
INFO:root:[   18] Training loss: 0.03993300, Validation loss: 0.04648003, Gradient norm: 2.59028880
INFO:root:[   19] Training loss: 0.03771619, Validation loss: 0.03156855, Gradient norm: 2.35367363
INFO:root:[   20] Training loss: 0.03643794, Validation loss: 0.03448791, Gradient norm: 2.22067196
INFO:root:[   21] Training loss: 0.03513661, Validation loss: 0.04019506, Gradient norm: 1.88608041
INFO:root:[   22] Training loss: 0.03599339, Validation loss: 0.03578225, Gradient norm: 1.85736820
INFO:root:[   23] Training loss: 0.03412451, Validation loss: 0.03562805, Gradient norm: 1.92632781
INFO:root:[   24] Training loss: 0.03391609, Validation loss: 0.03228747, Gradient norm: 1.83010150
INFO:root:[   25] Training loss: 0.03364618, Validation loss: 0.02771001, Gradient norm: 2.16980927
INFO:root:[   26] Training loss: 0.03289046, Validation loss: 0.03709016, Gradient norm: 2.28617253
INFO:root:[   27] Training loss: 0.03465564, Validation loss: 0.03969402, Gradient norm: 2.44691824
INFO:root:[   28] Training loss: 0.03231168, Validation loss: 0.03920546, Gradient norm: 2.08540353
INFO:root:[   29] Training loss: 0.03284959, Validation loss: 0.03003968, Gradient norm: 2.11971600
INFO:root:[   30] Training loss: 0.03129486, Validation loss: 0.02838260, Gradient norm: 1.98704244
INFO:root:[   31] Training loss: 0.03067225, Validation loss: 0.03395031, Gradient norm: 1.87241551
INFO:root:[   32] Training loss: 0.03038244, Validation loss: 0.03528693, Gradient norm: 1.85661362
INFO:root:[   33] Training loss: 0.03028703, Validation loss: 0.03061146, Gradient norm: 2.17145673
INFO:root:[   34] Training loss: 0.02985963, Validation loss: 0.02409537, Gradient norm: 2.25423601
INFO:root:[   35] Training loss: 0.02828747, Validation loss: 0.02312415, Gradient norm: 2.07059439
INFO:root:[   36] Training loss: 0.02819507, Validation loss: 0.02496550, Gradient norm: 2.05193682
INFO:root:[   37] Training loss: 0.02759860, Validation loss: 0.03676749, Gradient norm: 1.87382335
INFO:root:[   38] Training loss: 0.02825943, Validation loss: 0.02660655, Gradient norm: 1.91061389
INFO:root:[   39] Training loss: 0.02669686, Validation loss: 0.02144757, Gradient norm: 1.86931088
INFO:root:[   40] Training loss: 0.02628073, Validation loss: 0.03122919, Gradient norm: 1.89770976
INFO:root:[   41] Training loss: 0.02864019, Validation loss: 0.02383263, Gradient norm: 2.28280410
INFO:root:[   42] Training loss: 0.02554315, Validation loss: 0.02339902, Gradient norm: 1.98562194
INFO:root:[   43] Training loss: 0.02473068, Validation loss: 0.02415405, Gradient norm: 1.86791891
INFO:root:[   44] Training loss: 0.02455495, Validation loss: 0.02002100, Gradient norm: 1.85119244
INFO:root:[   45] Training loss: 0.02572799, Validation loss: 0.02443551, Gradient norm: 1.98189287
INFO:root:[   46] Training loss: 0.02465296, Validation loss: 0.01941319, Gradient norm: 1.82321831
INFO:root:[   47] Training loss: 0.02570389, Validation loss: 0.02981470, Gradient norm: 1.89540267
INFO:root:[   48] Training loss: 0.02647648, Validation loss: 0.02230870, Gradient norm: 2.12214032
INFO:root:[   49] Training loss: 0.02261664, Validation loss: 0.01986740, Gradient norm: 1.75855699
INFO:root:[   50] Training loss: 0.02271349, Validation loss: 0.02039232, Gradient norm: 1.75496307
INFO:root:[   51] Training loss: 0.02213629, Validation loss: 0.02456410, Gradient norm: 1.70819938
INFO:root:[   52] Training loss: 0.02380857, Validation loss: 0.02393710, Gradient norm: 1.83649017
INFO:root:[   53] Training loss: 0.02331117, Validation loss: 0.02621528, Gradient norm: 1.84186085
INFO:root:[   54] Training loss: 0.02266406, Validation loss: 0.02063088, Gradient norm: 1.74728108
INFO:root:[   55] Training loss: 0.02161601, Validation loss: 0.01731496, Gradient norm: 1.76818532
INFO:root:[   56] Training loss: 0.02316950, Validation loss: 0.02629960, Gradient norm: 2.01289599
INFO:root:[   57] Training loss: 0.02309870, Validation loss: 0.02075628, Gradient norm: 1.88260240
INFO:root:[   58] Training loss: 0.02244384, Validation loss: 0.02188296, Gradient norm: 1.71889635
INFO:root:[   59] Training loss: 0.02214962, Validation loss: 0.02314727, Gradient norm: 1.92923801
INFO:root:[   60] Training loss: 0.02171127, Validation loss: 0.01941167, Gradient norm: 1.73941481
INFO:root:[   61] Training loss: 0.02074978, Validation loss: 0.02438670, Gradient norm: 1.56162459
INFO:root:[   62] Training loss: 0.02101294, Validation loss: 0.02356931, Gradient norm: 1.70951935
INFO:root:[   63] Training loss: 0.02037585, Validation loss: 0.02101984, Gradient norm: 1.80041012
INFO:root:[   64] Training loss: 0.02088592, Validation loss: 0.02021922, Gradient norm: 1.88454472
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 2441.917s.
INFO:root:Emptying the cuda cache took 0.082s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0178
INFO:root:EnergyScoreTrain: 0.01746
INFO:root:CoverageTrain: 0.99868
INFO:root:IntervalWidthTrain: 0.0588
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01761
INFO:root:EnergyScoreValidation: 0.01732
INFO:root:CoverageValidation: 0.99865
INFO:root:IntervalWidthValidation: 0.05839
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02724
INFO:root:EnergyScoreTest: 0.02137
INFO:root:CoverageTest: 0.99144
INFO:root:IntervalWidthTest: 0.05834
INFO:root:###2 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.59811341, Validation loss: 0.10438046, Gradient norm: 4.70617077
INFO:root:[    2] Training loss: 0.10591575, Validation loss: 0.10618857, Gradient norm: 1.87708234
INFO:root:[    3] Training loss: 0.08912180, Validation loss: 0.07649162, Gradient norm: 1.89468012
INFO:root:[    4] Training loss: 0.07469957, Validation loss: 0.06634287, Gradient norm: 2.02169591
INFO:root:[    5] Training loss: 0.06867578, Validation loss: 0.05757006, Gradient norm: 2.36331472
INFO:root:[    6] Training loss: 0.06265562, Validation loss: 0.05451338, Gradient norm: 2.21918192
INFO:root:[    7] Training loss: 0.05735975, Validation loss: 0.05709452, Gradient norm: 1.82733186
INFO:root:[    8] Training loss: 0.05488233, Validation loss: 0.04914481, Gradient norm: 1.92889216
INFO:root:[    9] Training loss: 0.05212431, Validation loss: 0.04603223, Gradient norm: 1.73942898
INFO:root:[   10] Training loss: 0.05108025, Validation loss: 0.05705961, Gradient norm: 1.85622114
INFO:root:[   11] Training loss: 0.05008975, Validation loss: 0.05239704, Gradient norm: 1.97904754
INFO:root:[   12] Training loss: 0.04846835, Validation loss: 0.04869105, Gradient norm: 1.57694500
INFO:root:[   13] Training loss: 0.05132536, Validation loss: 0.06916959, Gradient norm: 2.06232628
INFO:root:[   14] Training loss: 0.04876766, Validation loss: 0.04049827, Gradient norm: 2.02743324
INFO:root:[   15] Training loss: 0.04440082, Validation loss: 0.04190763, Gradient norm: 1.64298127
INFO:root:[   16] Training loss: 0.04354790, Validation loss: 0.04672733, Gradient norm: 1.48236297
INFO:root:[   17] Training loss: 0.04548689, Validation loss: 0.04746250, Gradient norm: 1.46224443
INFO:root:[   18] Training loss: 0.04549591, Validation loss: 0.03871318, Gradient norm: 2.02259676
INFO:root:[   19] Training loss: 0.04280876, Validation loss: 0.03758217, Gradient norm: 1.75032099
INFO:root:[   20] Training loss: 0.04184496, Validation loss: 0.03902260, Gradient norm: 1.63337857
INFO:root:[   21] Training loss: 0.04103862, Validation loss: 0.04524891, Gradient norm: 1.61386381
INFO:root:[   22] Training loss: 0.04238246, Validation loss: 0.04049151, Gradient norm: 1.59943284
INFO:root:[   23] Training loss: 0.04281977, Validation loss: 0.04501004, Gradient norm: 1.80675642
INFO:root:[   24] Training loss: 0.04079153, Validation loss: 0.04068144, Gradient norm: 1.69214203
INFO:root:[   25] Training loss: 0.04004894, Validation loss: 0.04223169, Gradient norm: 1.59669248
INFO:root:[   26] Training loss: 0.04033002, Validation loss: 0.03881923, Gradient norm: 1.55428669
INFO:root:[   27] Training loss: 0.03821189, Validation loss: 0.04622729, Gradient norm: 1.45944514
INFO:root:[   28] Training loss: 0.03937812, Validation loss: 0.03677855, Gradient norm: 1.78647771
INFO:root:[   29] Training loss: 0.03870396, Validation loss: 0.03822821, Gradient norm: 1.68663025
INFO:root:[   30] Training loss: 0.03978385, Validation loss: 0.04269411, Gradient norm: 1.67233751
INFO:root:[   31] Training loss: 0.03832377, Validation loss: 0.03272805, Gradient norm: 1.61289370
INFO:root:[   32] Training loss: 0.03830897, Validation loss: 0.03632606, Gradient norm: 1.68865846
INFO:root:[   33] Training loss: 0.03681010, Validation loss: 0.03881360, Gradient norm: 1.42305226
INFO:root:[   34] Training loss: 0.03598420, Validation loss: 0.03911001, Gradient norm: 1.43982558
INFO:root:[   35] Training loss: 0.03705567, Validation loss: 0.03917837, Gradient norm: 1.63990290
INFO:root:[   36] Training loss: 0.03731475, Validation loss: 0.03230695, Gradient norm: 1.63645597
INFO:root:[   37] Training loss: 0.03656622, Validation loss: 0.04010121, Gradient norm: 1.56353828
INFO:root:[   38] Training loss: 0.03622823, Validation loss: 0.03371842, Gradient norm: 1.59973661
INFO:root:[   39] Training loss: 0.03524346, Validation loss: 0.03953031, Gradient norm: 1.49420139
INFO:root:[   40] Training loss: 0.03546704, Validation loss: 0.03026033, Gradient norm: 1.58060962
INFO:root:[   41] Training loss: 0.03632807, Validation loss: 0.03305034, Gradient norm: 1.69061487
INFO:root:[   42] Training loss: 0.03451911, Validation loss: 0.03758605, Gradient norm: 1.53575939
INFO:root:[   43] Training loss: 0.03487448, Validation loss: 0.03535478, Gradient norm: 1.59508304
INFO:root:[   44] Training loss: 0.03494271, Validation loss: 0.03022489, Gradient norm: 1.60723633
INFO:root:[   45] Training loss: 0.03522587, Validation loss: 0.03486343, Gradient norm: 1.70593990
INFO:root:[   46] Training loss: 0.03298466, Validation loss: 0.03143446, Gradient norm: 1.45722628
INFO:root:[   47] Training loss: 0.03330434, Validation loss: 0.03723441, Gradient norm: 1.54340362
INFO:root:[   48] Training loss: 0.03275934, Validation loss: 0.03012616, Gradient norm: 1.50367161
INFO:root:[   49] Training loss: 0.03262678, Validation loss: 0.03714368, Gradient norm: 1.47184001
INFO:root:[   50] Training loss: 0.03355201, Validation loss: 0.03726717, Gradient norm: 1.61468908
INFO:root:[   51] Training loss: 0.03298952, Validation loss: 0.03421215, Gradient norm: 1.51529499
INFO:root:[   52] Training loss: 0.03262312, Validation loss: 0.03306798, Gradient norm: 1.48070700
INFO:root:[   53] Training loss: 0.03247324, Validation loss: 0.03359212, Gradient norm: 1.44604330
INFO:root:[   54] Training loss: 0.03161464, Validation loss: 0.03136586, Gradient norm: 1.46370647
INFO:root:[   55] Training loss: 0.03311233, Validation loss: 0.03149128, Gradient norm: 1.46683113
INFO:root:[   56] Training loss: 0.03387585, Validation loss: 0.03006798, Gradient norm: 1.60861723
INFO:root:[   57] Training loss: 0.03257893, Validation loss: 0.03374601, Gradient norm: 1.56729694
INFO:root:[   58] Training loss: 0.03148818, Validation loss: 0.02973658, Gradient norm: 1.47632152
INFO:root:[   59] Training loss: 0.03044344, Validation loss: 0.03167902, Gradient norm: 1.38314528
INFO:root:[   60] Training loss: 0.03045762, Validation loss: 0.02745287, Gradient norm: 1.40983680
INFO:root:[   61] Training loss: 0.03057424, Validation loss: 0.03289771, Gradient norm: 1.47299556
INFO:root:[   62] Training loss: 0.03033484, Validation loss: 0.03141174, Gradient norm: 1.45350455
INFO:root:[   63] Training loss: 0.03166324, Validation loss: 0.03324002, Gradient norm: 1.59161764
INFO:root:[   64] Training loss: 0.03157418, Validation loss: 0.02877067, Gradient norm: 1.52787702
INFO:root:[   65] Training loss: 0.03081473, Validation loss: 0.03139580, Gradient norm: 1.44712007
INFO:root:[   66] Training loss: 0.03107975, Validation loss: 0.03296118, Gradient norm: 1.49316203
INFO:root:[   67] Training loss: 0.03005408, Validation loss: 0.03556588, Gradient norm: 1.38919906
INFO:root:[   68] Training loss: 0.03219311, Validation loss: 0.03341528, Gradient norm: 1.59442707
INFO:root:[   69] Training loss: 0.03029386, Validation loss: 0.02689983, Gradient norm: 1.48192150
INFO:root:[   70] Training loss: 0.02974805, Validation loss: 0.03218750, Gradient norm: 1.45470970
INFO:root:[   71] Training loss: 0.02919015, Validation loss: 0.02645548, Gradient norm: 1.39546340
INFO:root:[   72] Training loss: 0.02990817, Validation loss: 0.02956214, Gradient norm: 1.47816587
INFO:root:[   73] Training loss: 0.03154782, Validation loss: 0.03457014, Gradient norm: 1.56142097
INFO:root:[   74] Training loss: 0.02993515, Validation loss: 0.02939451, Gradient norm: 1.50664655
INFO:root:[   75] Training loss: 0.03003821, Validation loss: 0.02812787, Gradient norm: 1.50014558
INFO:root:[   76] Training loss: 0.02862164, Validation loss: 0.03207262, Gradient norm: 1.34629714
INFO:root:[   77] Training loss: 0.02910587, Validation loss: 0.03016407, Gradient norm: 1.50189532
INFO:root:[   78] Training loss: 0.02857995, Validation loss: 0.03185166, Gradient norm: 1.39276513
INFO:root:[   79] Training loss: 0.02879583, Validation loss: 0.02903832, Gradient norm: 1.46531611
INFO:root:[   80] Training loss: 0.02883786, Validation loss: 0.02510403, Gradient norm: 1.51749077
INFO:root:[   81] Training loss: 0.02879669, Validation loss: 0.02590399, Gradient norm: 1.50585549
INFO:root:[   82] Training loss: 0.02955523, Validation loss: 0.02973596, Gradient norm: 1.59666554
INFO:root:[   83] Training loss: 0.02894267, Validation loss: 0.03110114, Gradient norm: 1.47417794
INFO:root:[   84] Training loss: 0.02888745, Validation loss: 0.02918544, Gradient norm: 1.42776484
INFO:root:[   85] Training loss: 0.02879520, Validation loss: 0.02628239, Gradient norm: 1.45484233
INFO:root:[   86] Training loss: 0.02878820, Validation loss: 0.02811673, Gradient norm: 1.54099744
INFO:root:[   87] Training loss: 0.02853840, Validation loss: 0.02885028, Gradient norm: 1.38490304
INFO:root:[   88] Training loss: 0.02812400, Validation loss: 0.02435654, Gradient norm: 1.42517426
INFO:root:[   89] Training loss: 0.02857420, Validation loss: 0.02809254, Gradient norm: 1.44317838
INFO:root:[   90] Training loss: 0.02778973, Validation loss: 0.03172558, Gradient norm: 1.47269004
INFO:root:[   91] Training loss: 0.02766136, Validation loss: 0.02625347, Gradient norm: 1.41414598
INFO:root:[   92] Training loss: 0.02821413, Validation loss: 0.02957194, Gradient norm: 1.39095519
INFO:root:[   93] Training loss: 0.02782815, Validation loss: 0.02581635, Gradient norm: 1.37181165
INFO:root:[   94] Training loss: 0.02792933, Validation loss: 0.03030652, Gradient norm: 1.45007681
INFO:root:[   95] Training loss: 0.02800642, Validation loss: 0.02915851, Gradient norm: 1.57347452
INFO:root:[   96] Training loss: 0.02693596, Validation loss: 0.02573509, Gradient norm: 1.41831564
INFO:root:[   97] Training loss: 0.02758343, Validation loss: 0.02509772, Gradient norm: 1.46688521
INFO:root:EP 97: Early stopping
INFO:root:Training the model took 3330.303s.
INFO:root:Emptying the cuda cache took 0.086s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01644
INFO:root:EnergyScoreTrain: 0.02446
INFO:root:CoverageTrain: 0.99997
INFO:root:IntervalWidthTrain: 0.09313
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01627
INFO:root:EnergyScoreValidation: 0.02435
INFO:root:CoverageValidation: 0.99998
INFO:root:IntervalWidthValidation: 0.09284
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02286
INFO:root:EnergyScoreTest: 0.02598
INFO:root:CoverageTest: 0.99948
INFO:root:IntervalWidthTest: 0.09287
INFO:root:###3 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.59140613, Validation loss: 0.15076112, Gradient norm: 4.66460668
INFO:root:[    2] Training loss: 0.13255291, Validation loss: 0.13273677, Gradient norm: 2.20298075
INFO:root:[    3] Training loss: 0.10487920, Validation loss: 0.08957317, Gradient norm: 2.13744715
INFO:root:[    4] Training loss: 0.08059665, Validation loss: 0.07203115, Gradient norm: 1.79053569
INFO:root:[    5] Training loss: 0.07079418, Validation loss: 0.06328407, Gradient norm: 1.69973012
INFO:root:[    6] Training loss: 0.06827757, Validation loss: 0.08270080, Gradient norm: 1.89986567
INFO:root:[    7] Training loss: 0.06576067, Validation loss: 0.07960762, Gradient norm: 2.03874174
INFO:root:[    8] Training loss: 0.06272724, Validation loss: 0.06013437, Gradient norm: 1.66498404
INFO:root:[    9] Training loss: 0.05713330, Validation loss: 0.05130435, Gradient norm: 1.27201861
INFO:root:[   10] Training loss: 0.05505827, Validation loss: 0.05088730, Gradient norm: 1.26400155
INFO:root:[   11] Training loss: 0.05552919, Validation loss: 0.04997722, Gradient norm: 1.42727050
INFO:root:[   12] Training loss: 0.05402174, Validation loss: 0.06047196, Gradient norm: 1.37049817
INFO:root:[   13] Training loss: 0.05504318, Validation loss: 0.05806543, Gradient norm: 1.62185752
INFO:root:[   14] Training loss: 0.05230003, Validation loss: 0.05611423, Gradient norm: 1.32808893
INFO:root:[   15] Training loss: 0.05507665, Validation loss: 0.05612854, Gradient norm: 1.66753993
INFO:root:[   16] Training loss: 0.05229206, Validation loss: 0.04989573, Gradient norm: 1.31383071
INFO:root:[   17] Training loss: 0.05088289, Validation loss: 0.05390847, Gradient norm: 1.25058082
INFO:root:[   18] Training loss: 0.05043052, Validation loss: 0.05647645, Gradient norm: 1.38061467
INFO:root:[   19] Training loss: 0.05144524, Validation loss: 0.05490610, Gradient norm: 1.67687077
INFO:root:[   20] Training loss: 0.05004327, Validation loss: 0.05400581, Gradient norm: 1.51657714
INFO:root:[   21] Training loss: 0.04987891, Validation loss: 0.05455103, Gradient norm: 1.54346941
INFO:root:[   22] Training loss: 0.04766137, Validation loss: 0.04717787, Gradient norm: 1.35799812
INFO:root:[   23] Training loss: 0.04695775, Validation loss: 0.04211469, Gradient norm: 1.21034262
INFO:root:[   24] Training loss: 0.04684039, Validation loss: 0.04973628, Gradient norm: 1.29270505
INFO:root:[   25] Training loss: 0.04631941, Validation loss: 0.04996273, Gradient norm: 1.25125916
INFO:root:[   26] Training loss: 0.04804775, Validation loss: 0.04849513, Gradient norm: 1.63037397
INFO:root:[   27] Training loss: 0.04620724, Validation loss: 0.04684263, Gradient norm: 1.48389992
INFO:root:[   28] Training loss: 0.04492327, Validation loss: 0.04051208, Gradient norm: 1.33728621
INFO:root:[   29] Training loss: 0.04435901, Validation loss: 0.05342985, Gradient norm: 1.32655953
INFO:root:[   30] Training loss: 0.04580981, Validation loss: 0.04982060, Gradient norm: 1.65523859
INFO:root:[   31] Training loss: 0.04432309, Validation loss: 0.04419095, Gradient norm: 1.28944698
INFO:root:[   32] Training loss: 0.04395449, Validation loss: 0.03918624, Gradient norm: 1.28256543
INFO:root:[   33] Training loss: 0.04401933, Validation loss: 0.04805599, Gradient norm: 1.39687119
INFO:root:[   34] Training loss: 0.04429579, Validation loss: 0.04479698, Gradient norm: 1.59153933
INFO:root:[   35] Training loss: 0.04145536, Validation loss: 0.04250521, Gradient norm: 1.26257523
INFO:root:[   36] Training loss: 0.04106338, Validation loss: 0.03844109, Gradient norm: 1.29567986
INFO:root:[   37] Training loss: 0.04172590, Validation loss: 0.03703676, Gradient norm: 1.40047049
INFO:root:[   38] Training loss: 0.04076577, Validation loss: 0.03851016, Gradient norm: 1.31658052
INFO:root:[   39] Training loss: 0.04176227, Validation loss: 0.04584301, Gradient norm: 1.45173332
INFO:root:[   40] Training loss: 0.04253913, Validation loss: 0.03690881, Gradient norm: 1.54937568
INFO:root:[   41] Training loss: 0.04051978, Validation loss: 0.04305101, Gradient norm: 1.34346251
INFO:root:[   42] Training loss: 0.03996179, Validation loss: 0.03979041, Gradient norm: 1.38953346
INFO:root:[   43] Training loss: 0.03995164, Validation loss: 0.03499236, Gradient norm: 1.45861844
INFO:root:[   44] Training loss: 0.04152511, Validation loss: 0.04135638, Gradient norm: 1.71684087
INFO:root:[   45] Training loss: 0.04091991, Validation loss: 0.03828635, Gradient norm: 1.64867165
INFO:root:[   46] Training loss: 0.04082467, Validation loss: 0.03693658, Gradient norm: 1.63755896
INFO:root:[   47] Training loss: 0.04011492, Validation loss: 0.04173014, Gradient norm: 1.58839985
INFO:root:[   48] Training loss: 0.03869694, Validation loss: 0.03681349, Gradient norm: 1.29083759
INFO:root:[   49] Training loss: 0.03811902, Validation loss: 0.03450465, Gradient norm: 1.22493191
INFO:root:[   50] Training loss: 0.03770434, Validation loss: 0.03576248, Gradient norm: 1.26426207
INFO:root:[   51] Training loss: 0.03747376, Validation loss: 0.03558551, Gradient norm: 1.32192599
INFO:root:[   52] Training loss: 0.03949249, Validation loss: 0.03434350, Gradient norm: 1.67585109
INFO:root:[   53] Training loss: 0.03894953, Validation loss: 0.03432568, Gradient norm: 1.68945451
INFO:root:[   54] Training loss: 0.03872408, Validation loss: 0.03446316, Gradient norm: 1.63803892
INFO:root:[   55] Training loss: 0.03763466, Validation loss: 0.03485701, Gradient norm: 1.42149058
INFO:root:[   56] Training loss: 0.03667023, Validation loss: 0.03666976, Gradient norm: 1.22435714
INFO:root:[   57] Training loss: 0.03624640, Validation loss: 0.03966587, Gradient norm: 1.30710698
INFO:root:[   58] Training loss: 0.03615036, Validation loss: 0.03864311, Gradient norm: 1.37796906
INFO:root:[   59] Training loss: 0.03661737, Validation loss: 0.03312869, Gradient norm: 1.45045169
INFO:root:[   60] Training loss: 0.03525705, Validation loss: 0.03276761, Gradient norm: 1.30912194
INFO:root:[   61] Training loss: 0.03598439, Validation loss: 0.03428405, Gradient norm: 1.40015806
INFO:root:[   62] Training loss: 0.03551905, Validation loss: 0.03847022, Gradient norm: 1.41007924
INFO:root:[   63] Training loss: 0.03570441, Validation loss: 0.03700152, Gradient norm: 1.48180223
INFO:root:[   64] Training loss: 0.03455629, Validation loss: 0.03720157, Gradient norm: 1.35777772
INFO:root:[   65] Training loss: 0.03421625, Validation loss: 0.03865213, Gradient norm: 1.31316150
INFO:root:[   66] Training loss: 0.03510076, Validation loss: 0.03423531, Gradient norm: 1.48858899
INFO:root:[   67] Training loss: 0.03449467, Validation loss: 0.03871279, Gradient norm: 1.39805448
INFO:root:[   68] Training loss: 0.03438964, Validation loss: 0.03515614, Gradient norm: 1.36533586
INFO:root:[   69] Training loss: 0.03473425, Validation loss: 0.03590450, Gradient norm: 1.45965692
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 2377.784s.
INFO:root:Emptying the cuda cache took 0.086s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.027
INFO:root:EnergyScoreTrain: 0.03297
INFO:root:CoverageTrain: 0.99996
INFO:root:IntervalWidthTrain: 0.12008
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02655
INFO:root:EnergyScoreValidation: 0.03273
INFO:root:CoverageValidation: 0.99995
INFO:root:IntervalWidthValidation: 0.11958
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.03304
INFO:root:EnergyScoreTest: 0.03461
INFO:root:CoverageTest: 0.99966
INFO:root:IntervalWidthTest: 0.11981
INFO:root:###4 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.59036857, Validation loss: 0.14161413, Gradient norm: 4.15918656
INFO:root:[    2] Training loss: 0.13364637, Validation loss: 0.11310105, Gradient norm: 1.75754437
INFO:root:[    3] Training loss: 0.10406974, Validation loss: 0.09065698, Gradient norm: 1.16825380
INFO:root:[    4] Training loss: 0.08820177, Validation loss: 0.08278667, Gradient norm: 1.18065716
INFO:root:[    5] Training loss: 0.08188994, Validation loss: 0.08080369, Gradient norm: 1.64507195
INFO:root:[    6] Training loss: 0.07857550, Validation loss: 0.07259254, Gradient norm: 1.93167056
INFO:root:[    7] Training loss: 0.07136431, Validation loss: 0.06552017, Gradient norm: 1.54281278
INFO:root:[    8] Training loss: 0.06787443, Validation loss: 0.07186945, Gradient norm: 1.31721056
INFO:root:[    9] Training loss: 0.06804674, Validation loss: 0.06709178, Gradient norm: 1.59007585
INFO:root:[   10] Training loss: 0.06394629, Validation loss: 0.06132070, Gradient norm: 1.30905235
INFO:root:[   11] Training loss: 0.06280315, Validation loss: 0.06334371, Gradient norm: 1.26080581
INFO:root:[   12] Training loss: 0.06411164, Validation loss: 0.06459883, Gradient norm: 1.67719189
INFO:root:[   13] Training loss: 0.06120490, Validation loss: 0.06050855, Gradient norm: 1.40995841
INFO:root:[   14] Training loss: 0.06007824, Validation loss: 0.05979802, Gradient norm: 1.30861908
INFO:root:[   15] Training loss: 0.06110915, Validation loss: 0.05792939, Gradient norm: 1.47081989
INFO:root:[   16] Training loss: 0.05915013, Validation loss: 0.06032679, Gradient norm: 1.49070529
INFO:root:[   17] Training loss: 0.05820954, Validation loss: 0.06232926, Gradient norm: 1.33408516
INFO:root:[   18] Training loss: 0.05663958, Validation loss: 0.05137787, Gradient norm: 1.32785871
INFO:root:[   19] Training loss: 0.05556390, Validation loss: 0.06004782, Gradient norm: 1.24786878
INFO:root:[   20] Training loss: 0.05511262, Validation loss: 0.05661451, Gradient norm: 1.24130054
INFO:root:[   21] Training loss: 0.05633789, Validation loss: 0.05123202, Gradient norm: 1.42452302
INFO:root:[   22] Training loss: 0.05630845, Validation loss: 0.05030602, Gradient norm: 1.54891734
INFO:root:[   23] Training loss: 0.05346771, Validation loss: 0.05873861, Gradient norm: 1.29580639
INFO:root:[   24] Training loss: 0.05316937, Validation loss: 0.04980789, Gradient norm: 1.32955251
INFO:root:[   25] Training loss: 0.05555751, Validation loss: 0.05137585, Gradient norm: 1.60038489
INFO:root:[   26] Training loss: 0.05321657, Validation loss: 0.05050159, Gradient norm: 1.25866013
INFO:root:[   27] Training loss: 0.05151092, Validation loss: 0.05345843, Gradient norm: 1.24753930
INFO:root:[   28] Training loss: 0.05059532, Validation loss: 0.04987094, Gradient norm: 1.28677609
INFO:root:[   29] Training loss: 0.04964168, Validation loss: 0.05421048, Gradient norm: 1.23687400
INFO:root:[   30] Training loss: 0.05226488, Validation loss: 0.05123685, Gradient norm: 1.58423757
INFO:root:[   31] Training loss: 0.04973222, Validation loss: 0.05097822, Gradient norm: 1.37737548
INFO:root:[   32] Training loss: 0.05036346, Validation loss: 0.05764876, Gradient norm: 1.47756583
INFO:root:[   33] Training loss: 0.05056487, Validation loss: 0.05539580, Gradient norm: 1.57161981
INFO:root:[   34] Training loss: 0.05060942, Validation loss: 0.04626861, Gradient norm: 1.51223997
INFO:root:[   35] Training loss: 0.04827724, Validation loss: 0.05002594, Gradient norm: 1.19378759
INFO:root:[   36] Training loss: 0.04699500, Validation loss: 0.04870519, Gradient norm: 1.25101920
INFO:root:[   37] Training loss: 0.04634588, Validation loss: 0.05027052, Gradient norm: 1.19340570
INFO:root:[   38] Training loss: 0.04715825, Validation loss: 0.04364713, Gradient norm: 1.41456151
INFO:root:[   39] Training loss: 0.04632297, Validation loss: 0.04220845, Gradient norm: 1.35094258
INFO:root:[   40] Training loss: 0.04872993, Validation loss: 0.04337296, Gradient norm: 1.70714140
INFO:root:[   41] Training loss: 0.04779038, Validation loss: 0.04492874, Gradient norm: 1.67050740
INFO:root:[   42] Training loss: 0.04659289, Validation loss: 0.04287876, Gradient norm: 1.51345418
INFO:root:[   43] Training loss: 0.04580650, Validation loss: 0.04070510, Gradient norm: 1.49682314
INFO:root:[   44] Training loss: 0.04439656, Validation loss: 0.04328098, Gradient norm: 1.26751252
INFO:root:[   45] Training loss: 0.04425305, Validation loss: 0.04234399, Gradient norm: 1.27704261
INFO:root:[   46] Training loss: 0.04437393, Validation loss: 0.05055503, Gradient norm: 1.33006631
INFO:root:[   47] Training loss: 0.04633443, Validation loss: 0.05109984, Gradient norm: 1.70084707
INFO:root:[   48] Training loss: 0.04496618, Validation loss: 0.04784662, Gradient norm: 1.52450682
INFO:root:[   49] Training loss: 0.04434854, Validation loss: 0.04889054, Gradient norm: 1.51237321
INFO:root:[   50] Training loss: 0.04397749, Validation loss: 0.05068966, Gradient norm: 1.55088474
INFO:root:[   51] Training loss: 0.04389269, Validation loss: 0.04767867, Gradient norm: 1.56182519
INFO:root:[   52] Training loss: 0.04326667, Validation loss: 0.04654245, Gradient norm: 1.40659837
INFO:root:[   53] Training loss: 0.04259413, Validation loss: 0.03793407, Gradient norm: 1.28400073
INFO:root:[   54] Training loss: 0.04177219, Validation loss: 0.04722629, Gradient norm: 1.32754003
INFO:root:[   55] Training loss: 0.04324252, Validation loss: 0.04671600, Gradient norm: 1.60209806
INFO:root:[   56] Training loss: 0.04239809, Validation loss: 0.04849951, Gradient norm: 1.55250225
INFO:root:[   57] Training loss: 0.04250556, Validation loss: 0.04481752, Gradient norm: 1.55103016
INFO:root:[   58] Training loss: 0.04142718, Validation loss: 0.03744973, Gradient norm: 1.30312315
INFO:root:[   59] Training loss: 0.04022261, Validation loss: 0.04311689, Gradient norm: 1.26431087
INFO:root:[   60] Training loss: 0.04028908, Validation loss: 0.03734809, Gradient norm: 1.34859085
INFO:root:[   61] Training loss: 0.04049545, Validation loss: 0.04178071, Gradient norm: 1.41210449
INFO:root:[   62] Training loss: 0.04038855, Validation loss: 0.04397608, Gradient norm: 1.40656712
INFO:root:[   63] Training loss: 0.04052157, Validation loss: 0.03791235, Gradient norm: 1.49248532
INFO:root:[   64] Training loss: 0.04067798, Validation loss: 0.03747081, Gradient norm: 1.58159687
INFO:root:[   65] Training loss: 0.04031127, Validation loss: 0.03591195, Gradient norm: 1.49036724
INFO:root:[   66] Training loss: 0.03870375, Validation loss: 0.04157442, Gradient norm: 1.23336854
INFO:root:[   67] Training loss: 0.03909515, Validation loss: 0.03546232, Gradient norm: 1.33585139
INFO:root:[   68] Training loss: 0.03879376, Validation loss: 0.03488230, Gradient norm: 1.26420863
INFO:root:[   69] Training loss: 0.03846622, Validation loss: 0.04143573, Gradient norm: 1.32119331
INFO:root:[   70] Training loss: 0.03827294, Validation loss: 0.04181039, Gradient norm: 1.37121153
INFO:root:[   71] Training loss: 0.03975366, Validation loss: 0.03891982, Gradient norm: 1.60758270
INFO:root:[   72] Training loss: 0.03912097, Validation loss: 0.03581208, Gradient norm: 1.57852945
INFO:root:[   73] Training loss: 0.03744492, Validation loss: 0.03751157, Gradient norm: 1.27442824
INFO:root:[   74] Training loss: 0.03745822, Validation loss: 0.03669947, Gradient norm: 1.33717897
INFO:root:[   75] Training loss: 0.03774490, Validation loss: 0.03577078, Gradient norm: 1.37224487
INFO:root:[   76] Training loss: 0.03673792, Validation loss: 0.04038146, Gradient norm: 1.31187991
INFO:root:[   77] Training loss: 0.03716106, Validation loss: 0.03242112, Gradient norm: 1.41993804
INFO:root:[   78] Training loss: 0.03666222, Validation loss: 0.04107282, Gradient norm: 1.32274570
INFO:root:[   79] Training loss: 0.03705093, Validation loss: 0.03847079, Gradient norm: 1.32561006
INFO:root:[   80] Training loss: 0.03639230, Validation loss: 0.03168947, Gradient norm: 1.34746810
INFO:root:[   81] Training loss: 0.03603709, Validation loss: 0.03997103, Gradient norm: 1.35719734
INFO:root:[   82] Training loss: 0.03585746, Validation loss: 0.03856348, Gradient norm: 1.33916725
INFO:root:[   83] Training loss: 0.03617285, Validation loss: 0.03254062, Gradient norm: 1.43747102
INFO:root:[   84] Training loss: 0.03514758, Validation loss: 0.03710753, Gradient norm: 1.31644775
INFO:root:[   85] Training loss: 0.03492882, Validation loss: 0.03818640, Gradient norm: 1.32125454
INFO:root:[   86] Training loss: 0.03539749, Validation loss: 0.03534216, Gradient norm: 1.40546445
INFO:root:[   87] Training loss: 0.03445977, Validation loss: 0.03427488, Gradient norm: 1.31286339
INFO:root:[   88] Training loss: 0.03480608, Validation loss: 0.03086790, Gradient norm: 1.37465061
INFO:root:[   89] Training loss: 0.03468552, Validation loss: 0.03032316, Gradient norm: 1.32906738
INFO:root:[   90] Training loss: 0.03431061, Validation loss: 0.03104889, Gradient norm: 1.34524427
INFO:root:[   91] Training loss: 0.03413965, Validation loss: 0.03609801, Gradient norm: 1.35675723
INFO:root:[   92] Training loss: 0.03403198, Validation loss: 0.03750760, Gradient norm: 1.36379189
INFO:root:[   93] Training loss: 0.03407395, Validation loss: 0.03760611, Gradient norm: 1.33240227
INFO:root:[   94] Training loss: 0.03378795, Validation loss: 0.03559801, Gradient norm: 1.33914338
INFO:root:[   95] Training loss: 0.03469311, Validation loss: 0.03035405, Gradient norm: 1.52458324
INFO:root:[   96] Training loss: 0.03571297, Validation loss: 0.03218892, Gradient norm: 1.66455976
INFO:root:[   97] Training loss: 0.03408327, Validation loss: 0.02944159, Gradient norm: 1.40364411
INFO:root:[   98] Training loss: 0.03311091, Validation loss: 0.03894290, Gradient norm: 1.36677183
INFO:root:[   99] Training loss: 0.03495124, Validation loss: 0.03961030, Gradient norm: 1.64659791
INFO:root:[  100] Training loss: 0.03415117, Validation loss: 0.03879466, Gradient norm: 1.60316374
INFO:root:[  101] Training loss: 0.03368632, Validation loss: 0.03683991, Gradient norm: 1.55672559
INFO:root:[  102] Training loss: 0.03363015, Validation loss: 0.03722991, Gradient norm: 1.54713756
INFO:root:[  103] Training loss: 0.03325116, Validation loss: 0.03736995, Gradient norm: 1.55596583
INFO:root:[  104] Training loss: 0.03292843, Validation loss: 0.03730993, Gradient norm: 1.49570275
INFO:root:[  105] Training loss: 0.03285160, Validation loss: 0.03499466, Gradient norm: 1.51432976
INFO:root:[  106] Training loss: 0.03246923, Validation loss: 0.03000116, Gradient norm: 1.35921336
INFO:root:[  107] Training loss: 0.03261178, Validation loss: 0.02810832, Gradient norm: 1.41404550
INFO:root:[  108] Training loss: 0.03105766, Validation loss: 0.03553094, Gradient norm: 1.22739734
INFO:root:[  109] Training loss: 0.03216384, Validation loss: 0.03431303, Gradient norm: 1.45814468
INFO:root:[  110] Training loss: 0.03073053, Validation loss: 0.03202841, Gradient norm: 1.25539156
INFO:root:[  111] Training loss: 0.03079189, Validation loss: 0.02726497, Gradient norm: 1.32761322
INFO:root:[  112] Training loss: 0.03041154, Validation loss: 0.02830047, Gradient norm: 1.26652689
INFO:root:[  113] Training loss: 0.03054376, Validation loss: 0.03125161, Gradient norm: 1.30848457
INFO:root:[  114] Training loss: 0.03050746, Validation loss: 0.03332042, Gradient norm: 1.33954628
INFO:root:[  115] Training loss: 0.03100512, Validation loss: 0.02814186, Gradient norm: 1.37327241
INFO:root:[  116] Training loss: 0.03104862, Validation loss: 0.03555097, Gradient norm: 1.39354025
INFO:root:[  117] Training loss: 0.03223944, Validation loss: 0.03633795, Gradient norm: 1.65918011
INFO:root:[  118] Training loss: 0.03087725, Validation loss: 0.03034650, Gradient norm: 1.41117937
INFO:root:[  119] Training loss: 0.02927916, Validation loss: 0.03318764, Gradient norm: 1.25774939
INFO:root:[  120] Training loss: 0.02966661, Validation loss: 0.03183485, Gradient norm: 1.30589071
INFO:root:EP 120: Early stopping
INFO:root:Training the model took 4109.84s.
INFO:root:Emptying the cuda cache took 0.088s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.0177
INFO:root:EnergyScoreTrain: 0.02737
INFO:root:CoverageTrain: 0.99999
INFO:root:IntervalWidthTrain: 0.10503
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01743
INFO:root:EnergyScoreValidation: 0.02724
INFO:root:CoverageValidation: 0.99999
INFO:root:IntervalWidthValidation: 0.10474
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02381
INFO:root:EnergyScoreTest: 0.02877
INFO:root:CoverageTest: 0.99989
INFO:root:IntervalWidthTest: 0.1048
INFO:root:###5 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.81244373, Validation loss: 0.18191310, Gradient norm: 3.82784146
INFO:root:[    2] Training loss: 0.15437501, Validation loss: 0.12637067, Gradient norm: 1.26128885
INFO:root:[    3] Training loss: 0.11533333, Validation loss: 0.10007849, Gradient norm: 0.94922018
INFO:root:[    4] Training loss: 0.09356575, Validation loss: 0.08385968, Gradient norm: 0.96829564
INFO:root:[    5] Training loss: 0.08628752, Validation loss: 0.07843879, Gradient norm: 1.34155954
INFO:root:[    6] Training loss: 0.08299135, Validation loss: 0.07367632, Gradient norm: 1.53994000
INFO:root:[    7] Training loss: 0.08173780, Validation loss: 0.07507137, Gradient norm: 1.68303795
INFO:root:[    8] Training loss: 0.07701706, Validation loss: 0.07427090, Gradient norm: 1.35862807
INFO:root:[    9] Training loss: 0.07373606, Validation loss: 0.06814818, Gradient norm: 1.33643839
INFO:root:[   10] Training loss: 0.07375563, Validation loss: 0.06799691, Gradient norm: 1.48109870
INFO:root:[   11] Training loss: 0.07172507, Validation loss: 0.06859327, Gradient norm: 1.34805276
INFO:root:[   12] Training loss: 0.07086102, Validation loss: 0.07284795, Gradient norm: 1.37247675
INFO:root:[   13] Training loss: 0.06936174, Validation loss: 0.06403068, Gradient norm: 1.35797843
INFO:root:[   14] Training loss: 0.06705598, Validation loss: 0.07540441, Gradient norm: 1.21072673
INFO:root:[   15] Training loss: 0.06740182, Validation loss: 0.07009338, Gradient norm: 1.33723797
INFO:root:[   16] Training loss: 0.06648484, Validation loss: 0.06882415, Gradient norm: 1.37189906
INFO:root:[   17] Training loss: 0.06577254, Validation loss: 0.06497896, Gradient norm: 1.34151878
INFO:root:[   18] Training loss: 0.06450577, Validation loss: 0.06173060, Gradient norm: 1.28642193
INFO:root:[   19] Training loss: 0.06385342, Validation loss: 0.06606849, Gradient norm: 1.31672030
INFO:root:[   20] Training loss: 0.06345924, Validation loss: 0.05923241, Gradient norm: 1.28834578
INFO:root:[   21] Training loss: 0.06443968, Validation loss: 0.06147185, Gradient norm: 1.57438796
INFO:root:[   22] Training loss: 0.06229466, Validation loss: 0.06072992, Gradient norm: 1.36913439
INFO:root:[   23] Training loss: 0.06109977, Validation loss: 0.05589673, Gradient norm: 1.27701850
INFO:root:[   24] Training loss: 0.06143675, Validation loss: 0.05922376, Gradient norm: 1.34168231
INFO:root:[   25] Training loss: 0.06091641, Validation loss: 0.06746886, Gradient norm: 1.32433074
INFO:root:[   26] Training loss: 0.05987813, Validation loss: 0.05811103, Gradient norm: 1.31751988
INFO:root:[   27] Training loss: 0.05994185, Validation loss: 0.05532541, Gradient norm: 1.46643963
INFO:root:[   28] Training loss: 0.05961268, Validation loss: 0.05645688, Gradient norm: 1.49058987
INFO:root:[   29] Training loss: 0.05753761, Validation loss: 0.05638981, Gradient norm: 1.25063250
INFO:root:[   30] Training loss: 0.05883870, Validation loss: 0.05614019, Gradient norm: 1.52874236
INFO:root:[   31] Training loss: 0.05824878, Validation loss: 0.05493736, Gradient norm: 1.51449805
INFO:root:[   32] Training loss: 0.05792569, Validation loss: 0.06156595, Gradient norm: 1.43924994
INFO:root:[   33] Training loss: 0.05670048, Validation loss: 0.06251407, Gradient norm: 1.26125333
INFO:root:[   34] Training loss: 0.05615272, Validation loss: 0.05311099, Gradient norm: 1.25978391
INFO:root:[   35] Training loss: 0.05534663, Validation loss: 0.05801352, Gradient norm: 1.20268689
INFO:root:[   36] Training loss: 0.05493527, Validation loss: 0.04976402, Gradient norm: 1.28971904
INFO:root:[   37] Training loss: 0.05444211, Validation loss: 0.05582178, Gradient norm: 1.33621604
INFO:root:[   38] Training loss: 0.05458839, Validation loss: 0.06168855, Gradient norm: 1.47567803
INFO:root:[   39] Training loss: 0.05477661, Validation loss: 0.05837175, Gradient norm: 1.56842802
INFO:root:[   40] Training loss: 0.05431445, Validation loss: 0.05732565, Gradient norm: 1.42169891
INFO:root:[   41] Training loss: 0.05376020, Validation loss: 0.05067778, Gradient norm: 1.29813072
INFO:root:[   42] Training loss: 0.05289461, Validation loss: 0.05347864, Gradient norm: 1.32752064
INFO:root:[   43] Training loss: 0.05333052, Validation loss: 0.05042345, Gradient norm: 1.54230172
INFO:root:[   44] Training loss: 0.05318781, Validation loss: 0.05242437, Gradient norm: 1.48459771
INFO:root:[   45] Training loss: 0.05130366, Validation loss: 0.05329332, Gradient norm: 1.29017692
INFO:root:[   46] Training loss: 0.05167531, Validation loss: 0.05892664, Gradient norm: 1.47985849
INFO:root:[   47] Training loss: 0.05004126, Validation loss: 0.04732971, Gradient norm: 1.27953289
INFO:root:[   48] Training loss: 0.04888532, Validation loss: 0.05176531, Gradient norm: 1.16786091
INFO:root:[   49] Training loss: 0.04920289, Validation loss: 0.04752679, Gradient norm: 1.27732816
INFO:root:[   50] Training loss: 0.04840391, Validation loss: 0.04480955, Gradient norm: 1.23031638
INFO:root:[   51] Training loss: 0.04798521, Validation loss: 0.04641593, Gradient norm: 1.19649501
INFO:root:[   52] Training loss: 0.04835267, Validation loss: 0.04387179, Gradient norm: 1.34151593
INFO:root:[   53] Training loss: 0.04750964, Validation loss: 0.04510618, Gradient norm: 1.29542994
INFO:root:[   54] Training loss: 0.04709671, Validation loss: 0.04535322, Gradient norm: 1.31223979
INFO:root:[   55] Training loss: 0.04683782, Validation loss: 0.04833437, Gradient norm: 1.30335732
INFO:root:[   56] Training loss: 0.04693231, Validation loss: 0.04633151, Gradient norm: 1.28553781
INFO:root:[   57] Training loss: 0.04716486, Validation loss: 0.04795916, Gradient norm: 1.33312275
INFO:root:[   58] Training loss: 0.04621978, Validation loss: 0.04777053, Gradient norm: 1.32091984
INFO:root:[   59] Training loss: 0.04521674, Validation loss: 0.04189183, Gradient norm: 1.25398230
INFO:root:[   60] Training loss: 0.04518898, Validation loss: 0.04038032, Gradient norm: 1.32410207
INFO:root:[   61] Training loss: 0.04519531, Validation loss: 0.04434258, Gradient norm: 1.38584428
INFO:root:[   62] Training loss: 0.04551007, Validation loss: 0.04810615, Gradient norm: 1.35479213
INFO:root:[   63] Training loss: 0.04423628, Validation loss: 0.04330925, Gradient norm: 1.26627740
INFO:root:[   64] Training loss: 0.04464001, Validation loss: 0.04951126, Gradient norm: 1.42128446
INFO:root:[   65] Training loss: 0.04452851, Validation loss: 0.03915266, Gradient norm: 1.44418217
INFO:root:[   66] Training loss: 0.04372519, Validation loss: 0.04945476, Gradient norm: 1.40869382
INFO:root:[   67] Training loss: 0.04399840, Validation loss: 0.03990319, Gradient norm: 1.45696417
INFO:root:[   68] Training loss: 0.04339059, Validation loss: 0.04498990, Gradient norm: 1.39971970
INFO:root:[   69] Training loss: 0.04290699, Validation loss: 0.04263240, Gradient norm: 1.36492947
INFO:root:[   70] Training loss: 0.04192515, Validation loss: 0.04553455, Gradient norm: 1.27171982
INFO:root:[   71] Training loss: 0.04204758, Validation loss: 0.04498930, Gradient norm: 1.31604241
INFO:root:[   72] Training loss: 0.04129613, Validation loss: 0.04164389, Gradient norm: 1.35500595
INFO:root:[   73] Training loss: 0.04134582, Validation loss: 0.03695944, Gradient norm: 1.32223718
INFO:root:[   74] Training loss: 0.04092873, Validation loss: 0.04258163, Gradient norm: 1.33522210
INFO:root:[   75] Training loss: 0.04111157, Validation loss: 0.04440977, Gradient norm: 1.31920953
INFO:root:[   76] Training loss: 0.04040840, Validation loss: 0.04622189, Gradient norm: 1.29791797
INFO:root:[   77] Training loss: 0.04077424, Validation loss: 0.03671875, Gradient norm: 1.41616728
INFO:root:[   78] Training loss: 0.03951728, Validation loss: 0.03596338, Gradient norm: 1.31933053
INFO:root:[   79] Training loss: 0.03955332, Validation loss: 0.03496866, Gradient norm: 1.34089559
INFO:root:[   80] Training loss: 0.03909784, Validation loss: 0.03691650, Gradient norm: 1.32368262
INFO:root:[   81] Training loss: 0.03925111, Validation loss: 0.03858132, Gradient norm: 1.29264886
INFO:root:[   82] Training loss: 0.03904952, Validation loss: 0.03963461, Gradient norm: 1.34953832
INFO:root:[   83] Training loss: 0.03871309, Validation loss: 0.04129225, Gradient norm: 1.29761725
INFO:root:[   84] Training loss: 0.03809348, Validation loss: 0.04137469, Gradient norm: 1.31899309
INFO:root:[   85] Training loss: 0.03884480, Validation loss: 0.03999739, Gradient norm: 1.37861550
INFO:root:[   86] Training loss: 0.03793780, Validation loss: 0.03295439, Gradient norm: 1.40922545
INFO:root:[   87] Training loss: 0.03736555, Validation loss: 0.03641328, Gradient norm: 1.34711651
INFO:root:[   88] Training loss: 0.03753237, Validation loss: 0.03994984, Gradient norm: 1.39358720
INFO:root:[   89] Training loss: 0.03724747, Validation loss: 0.03245945, Gradient norm: 1.39429589
INFO:root:[   90] Training loss: 0.03668182, Validation loss: 0.03402144, Gradient norm: 1.28710805
INFO:root:[   91] Training loss: 0.03689505, Validation loss: 0.03926580, Gradient norm: 1.36212868
INFO:root:[   92] Training loss: 0.03654229, Validation loss: 0.03179370, Gradient norm: 1.35338007
INFO:root:[   93] Training loss: 0.03591860, Validation loss: 0.03307723, Gradient norm: 1.32428114
INFO:root:[   94] Training loss: 0.03532314, Validation loss: 0.04016342, Gradient norm: 1.31935636
INFO:root:[   95] Training loss: 0.03549438, Validation loss: 0.03367234, Gradient norm: 1.37450974
INFO:root:[   96] Training loss: 0.03565536, Validation loss: 0.03447469, Gradient norm: 1.38696778
INFO:root:[   97] Training loss: 0.03510174, Validation loss: 0.03954449, Gradient norm: 1.30711232
INFO:root:[   98] Training loss: 0.03504586, Validation loss: 0.03679144, Gradient norm: 1.34037650
INFO:root:[   99] Training loss: 0.03465133, Validation loss: 0.03074008, Gradient norm: 1.37380664
INFO:root:[  100] Training loss: 0.03421591, Validation loss: 0.03687695, Gradient norm: 1.32803246
INFO:root:[  101] Training loss: 0.03411516, Validation loss: 0.03132728, Gradient norm: 1.34883208
INFO:root:[  102] Training loss: 0.03371790, Validation loss: 0.03257907, Gradient norm: 1.29011168
INFO:root:[  103] Training loss: 0.03475580, Validation loss: 0.03110858, Gradient norm: 1.30043965
INFO:root:[  104] Training loss: 0.03366228, Validation loss: 0.03382580, Gradient norm: 1.27451959
INFO:root:[  105] Training loss: 0.03348649, Validation loss: 0.03512874, Gradient norm: 1.31291313
INFO:root:[  106] Training loss: 0.03274347, Validation loss: 0.03286827, Gradient norm: 1.33970059
INFO:root:[  107] Training loss: 0.03411401, Validation loss: 0.04051629, Gradient norm: 1.55374303
INFO:root:[  108] Training loss: 0.03426016, Validation loss: 0.03894983, Gradient norm: 1.61851982
INFO:root:EP 108: Early stopping
INFO:root:Training the model took 3700.87s.
INFO:root:Emptying the cuda cache took 0.087s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02202
INFO:root:EnergyScoreTrain: 0.03089
INFO:root:CoverageTrain: 0.99999
INFO:root:IntervalWidthTrain: 0.11631
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0217
INFO:root:EnergyScoreValidation: 0.03073
INFO:root:CoverageValidation: 0.99999
INFO:root:IntervalWidthValidation: 0.11596
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02824
INFO:root:EnergyScoreTest: 0.03247
INFO:root:CoverageTest: 0.99981
INFO:root:IntervalWidthTest: 0.11601
INFO:root:###6 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.75946069, Validation loss: 0.20906231, Gradient norm: 3.23998681
INFO:root:[    2] Training loss: 0.17008777, Validation loss: 0.14735907, Gradient norm: 1.10769595
INFO:root:[    3] Training loss: 0.12746612, Validation loss: 0.11286321, Gradient norm: 1.12586991
INFO:root:[    4] Training loss: 0.10881969, Validation loss: 0.09929564, Gradient norm: 0.93563183
INFO:root:[    5] Training loss: 0.09840627, Validation loss: 0.09351101, Gradient norm: 1.04392094
INFO:root:[    6] Training loss: 0.09112449, Validation loss: 0.08614901, Gradient norm: 0.88879140
INFO:root:[    7] Training loss: 0.08801453, Validation loss: 0.09057657, Gradient norm: 1.03347209
INFO:root:[    8] Training loss: 0.08505185, Validation loss: 0.08709162, Gradient norm: 0.98965802
INFO:root:[    9] Training loss: 0.08357831, Validation loss: 0.08162741, Gradient norm: 1.10496545
INFO:root:[   10] Training loss: 0.07987262, Validation loss: 0.07756266, Gradient norm: 0.99600295
INFO:root:[   11] Training loss: 0.07889634, Validation loss: 0.07452673, Gradient norm: 1.05756124
INFO:root:[   12] Training loss: 0.07564535, Validation loss: 0.07441163, Gradient norm: 0.86406035
INFO:root:[   13] Training loss: 0.07668016, Validation loss: 0.08293850, Gradient norm: 1.14565400
INFO:root:[   14] Training loss: 0.07432029, Validation loss: 0.07572402, Gradient norm: 1.03330348
INFO:root:[   15] Training loss: 0.07329855, Validation loss: 0.07962700, Gradient norm: 1.00167256
INFO:root:[   16] Training loss: 0.07337867, Validation loss: 0.07198072, Gradient norm: 1.11615540
INFO:root:[   17] Training loss: 0.07147159, Validation loss: 0.06582545, Gradient norm: 1.06446897
INFO:root:[   18] Training loss: 0.07022309, Validation loss: 0.06945871, Gradient norm: 0.99593068
INFO:root:[   19] Training loss: 0.06928049, Validation loss: 0.07011298, Gradient norm: 0.96243843
INFO:root:[   20] Training loss: 0.06794938, Validation loss: 0.07042784, Gradient norm: 0.94180379
INFO:root:[   21] Training loss: 0.06785678, Validation loss: 0.06334479, Gradient norm: 1.01086749
INFO:root:[   22] Training loss: 0.06769456, Validation loss: 0.07135759, Gradient norm: 1.08274323
INFO:root:[   23] Training loss: 0.06704597, Validation loss: 0.06934501, Gradient norm: 1.04901393
INFO:root:[   24] Training loss: 0.06556615, Validation loss: 0.06248722, Gradient norm: 0.99101679
INFO:root:[   25] Training loss: 0.06479323, Validation loss: 0.05952631, Gradient norm: 1.10103951
INFO:root:[   26] Training loss: 0.06396335, Validation loss: 0.06097925, Gradient norm: 1.00308002
INFO:root:[   27] Training loss: 0.06324194, Validation loss: 0.06235665, Gradient norm: 1.04232529
INFO:root:[   28] Training loss: 0.06369945, Validation loss: 0.06449838, Gradient norm: 1.14150379
INFO:root:[   29] Training loss: 0.06092654, Validation loss: 0.06238950, Gradient norm: 0.95678666
INFO:root:[   30] Training loss: 0.06063738, Validation loss: 0.06148291, Gradient norm: 1.03321474
INFO:root:[   31] Training loss: 0.06135733, Validation loss: 0.06325062, Gradient norm: 1.12958107
INFO:root:[   32] Training loss: 0.06120899, Validation loss: 0.06646122, Gradient norm: 1.00556366
INFO:root:[   33] Training loss: 0.06126007, Validation loss: 0.06121701, Gradient norm: 1.21180904
INFO:root:[   34] Training loss: 0.05871044, Validation loss: 0.06073935, Gradient norm: 1.05070252
INFO:root:[   35] Training loss: 0.05795207, Validation loss: 0.05632529, Gradient norm: 1.04501863
INFO:root:[   36] Training loss: 0.05795055, Validation loss: 0.05260270, Gradient norm: 1.19705940
INFO:root:[   37] Training loss: 0.05673117, Validation loss: 0.05361640, Gradient norm: 1.01404341
INFO:root:[   38] Training loss: 0.05714509, Validation loss: 0.05510207, Gradient norm: 1.16466456
INFO:root:[   39] Training loss: 0.05552771, Validation loss: 0.05438574, Gradient norm: 1.03820673
INFO:root:[   40] Training loss: 0.05528716, Validation loss: 0.05813643, Gradient norm: 1.13705624
INFO:root:[   41] Training loss: 0.05446708, Validation loss: 0.05961447, Gradient norm: 1.13501416
INFO:root:[   42] Training loss: 0.05441333, Validation loss: 0.05238023, Gradient norm: 1.15496785
INFO:root:[   43] Training loss: 0.05334291, Validation loss: 0.05310780, Gradient norm: 1.01283260
INFO:root:[   44] Training loss: 0.05264054, Validation loss: 0.05494138, Gradient norm: 1.03664227
INFO:root:[   45] Training loss: 0.05275482, Validation loss: 0.05003398, Gradient norm: 1.06680949
INFO:root:[   46] Training loss: 0.05186772, Validation loss: 0.05433836, Gradient norm: 1.06921026
INFO:root:[   47] Training loss: 0.05181557, Validation loss: 0.05423292, Gradient norm: 1.18105537
INFO:root:[   48] Training loss: 0.05099057, Validation loss: 0.05622691, Gradient norm: 1.26993254
INFO:root:[   49] Training loss: 0.05047939, Validation loss: 0.04882729, Gradient norm: 1.21945886
INFO:root:[   50] Training loss: 0.04953740, Validation loss: 0.04666814, Gradient norm: 1.15708305
INFO:root:[   51] Training loss: 0.04913161, Validation loss: 0.04661645, Gradient norm: 1.20373046
INFO:root:[   52] Training loss: 0.04956417, Validation loss: 0.04925954, Gradient norm: 1.26359941
INFO:root:[   53] Training loss: 0.04776891, Validation loss: 0.05265733, Gradient norm: 1.10069708
INFO:root:[   54] Training loss: 0.04795084, Validation loss: 0.04562251, Gradient norm: 1.22279335
INFO:root:[   55] Training loss: 0.04723054, Validation loss: 0.04672389, Gradient norm: 1.02189516
INFO:root:[   56] Training loss: 0.04694466, Validation loss: 0.05047916, Gradient norm: 1.09349272
INFO:root:[   57] Training loss: 0.04726715, Validation loss: 0.04327062, Gradient norm: 1.04825881
INFO:root:[   58] Training loss: 0.04628716, Validation loss: 0.04450494, Gradient norm: 1.22629757
INFO:root:[   59] Training loss: 0.04526735, Validation loss: 0.04309483, Gradient norm: 1.24329684
INFO:root:[   60] Training loss: 0.04452108, Validation loss: 0.04103764, Gradient norm: 1.19677607
INFO:root:[   61] Training loss: 0.04351296, Validation loss: 0.03943527, Gradient norm: 1.11119388
INFO:root:[   62] Training loss: 0.04489561, Validation loss: 0.04299339, Gradient norm: 1.30088741
INFO:root:[   63] Training loss: 0.04348087, Validation loss: 0.04444868, Gradient norm: 1.23120543
INFO:root:[   64] Training loss: 0.04334351, Validation loss: 0.04472337, Gradient norm: 1.24498247
INFO:root:[   65] Training loss: 0.04249333, Validation loss: 0.03996877, Gradient norm: 1.07553064
INFO:root:[   66] Training loss: 0.04256206, Validation loss: 0.04137261, Gradient norm: 1.26125345
INFO:root:[   67] Training loss: 0.04164443, Validation loss: 0.03901790, Gradient norm: 1.14017048
INFO:root:[   68] Training loss: 0.04153009, Validation loss: 0.03745767, Gradient norm: 1.24697314
INFO:root:[   69] Training loss: 0.04101806, Validation loss: 0.03778977, Gradient norm: 1.12773999
INFO:root:[   70] Training loss: 0.04040358, Validation loss: 0.03672939, Gradient norm: 1.09832121
INFO:root:[   71] Training loss: 0.04021045, Validation loss: 0.04269909, Gradient norm: 1.09761016
INFO:root:[   72] Training loss: 0.03926655, Validation loss: 0.03505956, Gradient norm: 1.10674188
INFO:root:[   73] Training loss: 0.03951908, Validation loss: 0.04042773, Gradient norm: 1.20155388
INFO:root:[   74] Training loss: 0.03946676, Validation loss: 0.03906875, Gradient norm: 1.29817158
INFO:root:[   75] Training loss: 0.03838521, Validation loss: 0.03835169, Gradient norm: 1.12483533
INFO:root:[   76] Training loss: 0.03772904, Validation loss: 0.03860694, Gradient norm: 1.11172656
INFO:root:[   77] Training loss: 0.03725493, Validation loss: 0.03600753, Gradient norm: 1.13332481
INFO:root:[   78] Training loss: 0.03749763, Validation loss: 0.03445095, Gradient norm: 1.15599826
INFO:root:[   79] Training loss: 0.03686180, Validation loss: 0.03999909, Gradient norm: 1.13785257
INFO:root:[   80] Training loss: 0.03671950, Validation loss: 0.03975802, Gradient norm: 1.27142682
INFO:root:[   81] Training loss: 0.03667987, Validation loss: 0.03729604, Gradient norm: 1.11698778
INFO:root:[   82] Training loss: 0.03636894, Validation loss: 0.03555779, Gradient norm: 1.21149178
INFO:root:[   83] Training loss: 0.03601603, Validation loss: 0.03436714, Gradient norm: 1.20707506
INFO:root:[   84] Training loss: 0.03576110, Validation loss: 0.03103365, Gradient norm: 1.27316381
INFO:root:[   85] Training loss: 0.03439521, Validation loss: 0.03252239, Gradient norm: 1.08237025
INFO:root:[   86] Training loss: 0.03524893, Validation loss: 0.03533819, Gradient norm: 1.16661801
INFO:root:[   87] Training loss: 0.03426631, Validation loss: 0.02983427, Gradient norm: 1.09962356
INFO:root:[   88] Training loss: 0.03380323, Validation loss: 0.03019032, Gradient norm: 1.20176508
INFO:root:[   89] Training loss: 0.03391876, Validation loss: 0.02997678, Gradient norm: 1.15965978
INFO:root:[   90] Training loss: 0.03390100, Validation loss: 0.03121196, Gradient norm: 1.22485356
INFO:root:[   91] Training loss: 0.03365055, Validation loss: 0.02959561, Gradient norm: 1.29969164
INFO:root:[   92] Training loss: 0.03229996, Validation loss: 0.03185723, Gradient norm: 1.17054788
INFO:root:[   93] Training loss: 0.03249541, Validation loss: 0.03687831, Gradient norm: 1.19419423
INFO:root:[   94] Training loss: 0.03229485, Validation loss: 0.03544691, Gradient norm: 1.19530097
INFO:root:[   95] Training loss: 0.03183001, Validation loss: 0.03313360, Gradient norm: 1.20452482
INFO:root:[   96] Training loss: 0.03190222, Validation loss: 0.03475978, Gradient norm: 1.24180221
INFO:root:[   97] Training loss: 0.03092938, Validation loss: 0.03467810, Gradient norm: 1.17800015
INFO:root:[   98] Training loss: 0.03140231, Validation loss: 0.03282591, Gradient norm: 1.26373298
INFO:root:[   99] Training loss: 0.03089768, Validation loss: 0.03339365, Gradient norm: 1.26588370
INFO:root:[  100] Training loss: 0.02974587, Validation loss: 0.03069971, Gradient norm: 1.12231621
INFO:root:[  101] Training loss: 0.03003360, Validation loss: 0.02803291, Gradient norm: 1.19227089
INFO:root:[  102] Training loss: 0.03075663, Validation loss: 0.02643511, Gradient norm: 1.15511616
INFO:root:[  103] Training loss: 0.03010650, Validation loss: 0.02547669, Gradient norm: 1.25960104
INFO:root:[  104] Training loss: 0.02910546, Validation loss: 0.02782585, Gradient norm: 1.15131350
INFO:root:[  105] Training loss: 0.02910271, Validation loss: 0.02554126, Gradient norm: 1.17349474
INFO:root:[  106] Training loss: 0.02825274, Validation loss: 0.02468840, Gradient norm: 1.08226227
INFO:root:[  107] Training loss: 0.02821937, Validation loss: 0.02476717, Gradient norm: 1.13814470
INFO:root:[  108] Training loss: 0.02844793, Validation loss: 0.02590436, Gradient norm: 1.29720853
INFO:root:[  109] Training loss: 0.02799911, Validation loss: 0.02444262, Gradient norm: 1.08730313
INFO:root:[  110] Training loss: 0.02776472, Validation loss: 0.02704329, Gradient norm: 1.21810287
INFO:root:[  111] Training loss: 0.02770486, Validation loss: 0.02453111, Gradient norm: 1.20567244
INFO:root:[  112] Training loss: 0.02766678, Validation loss: 0.02622119, Gradient norm: 1.15323040
INFO:root:[  113] Training loss: 0.02654012, Validation loss: 0.02817177, Gradient norm: 1.04567465
INFO:root:[  114] Training loss: 0.02729171, Validation loss: 0.02915340, Gradient norm: 1.09469586
INFO:root:[  115] Training loss: 0.02654503, Validation loss: 0.02613590, Gradient norm: 1.15718863
INFO:root:[  116] Training loss: 0.02590791, Validation loss: 0.02490239, Gradient norm: 1.14017434
INFO:root:[  117] Training loss: 0.02526977, Validation loss: 0.02900003, Gradient norm: 1.08953014
INFO:root:[  118] Training loss: 0.02588968, Validation loss: 0.02819715, Gradient norm: 1.27898446
INFO:root:EP 118: Early stopping
INFO:root:Training the model took 4042.54s.
INFO:root:Emptying the cuda cache took 0.087s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01904
INFO:root:EnergyScoreTrain: 0.02459
INFO:root:CoverageTrain: 0.99999
INFO:root:IntervalWidthTrain: 0.08862
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01894
INFO:root:EnergyScoreValidation: 0.02443
INFO:root:CoverageValidation: 0.99999
INFO:root:IntervalWidthValidation: 0.0881
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02933
INFO:root:EnergyScoreTest: 0.02777
INFO:root:CoverageTest: 0.99977
INFO:root:IntervalWidthTest: 0.08885
INFO:root:###7 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.64933900, Validation loss: 0.13947284, Gradient norm: 3.56578609
INFO:root:[    2] Training loss: 0.12115649, Validation loss: 0.12041404, Gradient norm: 2.67582064
INFO:root:[    3] Training loss: 0.09991578, Validation loss: 0.07477300, Gradient norm: 2.66004561
INFO:root:[    4] Training loss: 0.08388578, Validation loss: 0.05553095, Gradient norm: 2.48016682
INFO:root:[    5] Training loss: 0.07259481, Validation loss: 0.04501490, Gradient norm: 1.92062529
INFO:root:[    6] Training loss: 0.06867660, Validation loss: 0.05319111, Gradient norm: 2.16748176
INFO:root:[    7] Training loss: 0.06375008, Validation loss: 0.04398347, Gradient norm: 1.77944357
INFO:root:[    8] Training loss: 0.06345285, Validation loss: 0.03940820, Gradient norm: 2.48406616
INFO:root:[    9] Training loss: 0.05914140, Validation loss: 0.05081300, Gradient norm: 2.25208266
INFO:root:[   10] Training loss: 0.05764073, Validation loss: 0.04123777, Gradient norm: 2.38758870
INFO:root:[   11] Training loss: 0.05512586, Validation loss: 0.04312934, Gradient norm: 2.16128779
INFO:root:[   12] Training loss: 0.05375174, Validation loss: 0.04171588, Gradient norm: 2.00311236
INFO:root:[   13] Training loss: 0.05219206, Validation loss: 0.03857165, Gradient norm: 2.23827390
INFO:root:[   14] Training loss: 0.04924033, Validation loss: 0.03109039, Gradient norm: 2.02533767
INFO:root:[   15] Training loss: 0.05071965, Validation loss: 0.03716213, Gradient norm: 2.27654836
INFO:root:[   16] Training loss: 0.04984927, Validation loss: 0.02711571, Gradient norm: 2.32941540
INFO:root:[   17] Training loss: 0.04798501, Validation loss: 0.04022108, Gradient norm: 2.16656516
INFO:root:[   18] Training loss: 0.04528124, Validation loss: 0.02515184, Gradient norm: 1.98839275
INFO:root:[   19] Training loss: 0.04528808, Validation loss: 0.02705733, Gradient norm: 1.90842734
INFO:root:[   20] Training loss: 0.04454094, Validation loss: 0.02726554, Gradient norm: 1.94144349
INFO:root:[   21] Training loss: 0.04316746, Validation loss: 0.05135754, Gradient norm: 2.03481579
INFO:root:[   22] Training loss: 0.04220237, Validation loss: 0.02937217, Gradient norm: 1.90565588
INFO:root:[   23] Training loss: 0.04381435, Validation loss: 0.02838088, Gradient norm: 1.90940675
INFO:root:[   24] Training loss: 0.04263546, Validation loss: 0.02577841, Gradient norm: 2.03187316
INFO:root:[   25] Training loss: 0.04055795, Validation loss: 0.02895006, Gradient norm: 1.90018322
INFO:root:[   26] Training loss: 0.04087594, Validation loss: 0.02241204, Gradient norm: 2.00670324
INFO:root:[   27] Training loss: 0.03974571, Validation loss: 0.02694506, Gradient norm: 1.93167954
INFO:root:[   28] Training loss: 0.04030048, Validation loss: 0.02020780, Gradient norm: 2.09075216
INFO:root:[   29] Training loss: 0.03824944, Validation loss: 0.02731007, Gradient norm: 1.94179953
INFO:root:[   30] Training loss: 0.03784984, Validation loss: 0.03507360, Gradient norm: 1.91181961
INFO:root:[   31] Training loss: 0.03835530, Validation loss: 0.04006968, Gradient norm: 2.04895514
INFO:root:[   32] Training loss: 0.03746536, Validation loss: 0.04211465, Gradient norm: 1.88126677
INFO:root:[   33] Training loss: 0.03686007, Validation loss: 0.02941370, Gradient norm: 1.88333320
INFO:root:[   34] Training loss: 0.03902527, Validation loss: 0.04308754, Gradient norm: 2.02563048
INFO:root:[   35] Training loss: 0.03866617, Validation loss: 0.02212201, Gradient norm: 1.93443296
INFO:root:[   36] Training loss: 0.03660343, Validation loss: 0.02154533, Gradient norm: 1.76436329
INFO:root:[   37] Training loss: 0.03691156, Validation loss: 0.02252481, Gradient norm: 1.89858124
INFO:root:[   38] Training loss: 0.03790578, Validation loss: 0.01988048, Gradient norm: 1.85722444
INFO:root:[   39] Training loss: 0.03679254, Validation loss: 0.01969925, Gradient norm: 1.87576826
INFO:root:[   40] Training loss: 0.03579165, Validation loss: 0.01717221, Gradient norm: 1.92685825
INFO:root:[   41] Training loss: 0.03513678, Validation loss: 0.01958300, Gradient norm: 1.85774715
INFO:root:[   42] Training loss: 0.03594341, Validation loss: 0.02164730, Gradient norm: 2.05168701
INFO:root:[   43] Training loss: 0.03560141, Validation loss: 0.04170309, Gradient norm: 1.96102207
INFO:root:[   44] Training loss: 0.03391628, Validation loss: 0.03332206, Gradient norm: 1.76398213
INFO:root:[   45] Training loss: 0.03549427, Validation loss: 0.04111445, Gradient norm: 1.87511093
INFO:root:[   46] Training loss: 0.03491054, Validation loss: 0.03262541, Gradient norm: 1.68499043
INFO:root:[   47] Training loss: 0.03437047, Validation loss: 0.01988400, Gradient norm: 1.78831047
INFO:root:[   48] Training loss: 0.03506746, Validation loss: 0.04611910, Gradient norm: 1.95587022
INFO:root:[   49] Training loss: 0.03458701, Validation loss: 0.04382742, Gradient norm: 1.90543940
INFO:root:[   50] Training loss: 0.03416516, Validation loss: 0.03542155, Gradient norm: 1.80869651
INFO:root:[   51] Training loss: 0.03363596, Validation loss: 0.02806527, Gradient norm: 1.88141562
INFO:root:[   52] Training loss: 0.03271807, Validation loss: 0.02909117, Gradient norm: 1.63819370
INFO:root:[   53] Training loss: 0.03341087, Validation loss: 0.03054408, Gradient norm: 1.88603700
INFO:root:[   54] Training loss: 0.03247647, Validation loss: 0.01559880, Gradient norm: 1.81038032
INFO:root:[   55] Training loss: 0.03351242, Validation loss: 0.01666798, Gradient norm: 1.89405250
INFO:root:[   56] Training loss: 0.03276399, Validation loss: 0.02068402, Gradient norm: 1.75755128
INFO:root:[   57] Training loss: 0.03408250, Validation loss: 0.01942920, Gradient norm: 1.73983894
INFO:root:[   58] Training loss: 0.03290380, Validation loss: 0.01615678, Gradient norm: 1.78757160
INFO:root:[   59] Training loss: 0.03247378, Validation loss: 0.01515972, Gradient norm: 1.89328718
INFO:root:[   60] Training loss: 0.03245927, Validation loss: 0.01513703, Gradient norm: 1.86874428
INFO:root:[   61] Training loss: 0.03210079, Validation loss: 0.02312580, Gradient norm: 1.77569845
INFO:root:[   62] Training loss: 0.03243502, Validation loss: 0.01769568, Gradient norm: 1.88327425
INFO:root:[   63] Training loss: 0.03215576, Validation loss: 0.01544357, Gradient norm: 1.87961447
INFO:root:[   64] Training loss: 0.03274826, Validation loss: 0.01701625, Gradient norm: 1.97362955
INFO:root:[   65] Training loss: 0.03278978, Validation loss: 0.02353252, Gradient norm: 1.86137640
INFO:root:[   66] Training loss: 0.03199568, Validation loss: 0.03745570, Gradient norm: 1.87713239
INFO:root:[   67] Training loss: 0.03255976, Validation loss: 0.03054524, Gradient norm: 1.86905083
INFO:root:[   68] Training loss: 0.03187219, Validation loss: 0.02096949, Gradient norm: 1.76426622
INFO:root:[   69] Training loss: 0.03211581, Validation loss: 0.01562752, Gradient norm: 1.82869608
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 1036.116s.
INFO:root:Emptying the cuda cache took 0.046s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01435
INFO:root:EnergyScoreTrain: 0.01518
INFO:root:CoverageTrain: 0.92769
INFO:root:IntervalWidthTrain: 0.04748
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0142
INFO:root:EnergyScoreValidation: 0.01511
INFO:root:CoverageValidation: 0.92993
INFO:root:IntervalWidthValidation: 0.04756
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02018
INFO:root:EnergyScoreTest: 0.01756
INFO:root:CoverageTest: 0.91939
INFO:root:IntervalWidthTest: 0.04909
INFO:root:###8 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.61116007, Validation loss: 0.11835080, Gradient norm: 2.54880063
INFO:root:[    2] Training loss: 0.14121813, Validation loss: 0.09378924, Gradient norm: 2.19235456
INFO:root:[    3] Training loss: 0.11190495, Validation loss: 0.09627392, Gradient norm: 1.30858460
INFO:root:[    4] Training loss: 0.09996482, Validation loss: 0.07744431, Gradient norm: 1.77295216
INFO:root:[    5] Training loss: 0.09029483, Validation loss: 0.09369033, Gradient norm: 1.75058683
INFO:root:[    6] Training loss: 0.08576284, Validation loss: 0.04950744, Gradient norm: 1.78171641
INFO:root:[    7] Training loss: 0.08098014, Validation loss: 0.07956604, Gradient norm: 1.71509313
INFO:root:[    8] Training loss: 0.08154026, Validation loss: 0.05511622, Gradient norm: 1.87248333
INFO:root:[    9] Training loss: 0.07732592, Validation loss: 0.10067445, Gradient norm: 1.80126959
INFO:root:[   10] Training loss: 0.07383527, Validation loss: 0.10025920, Gradient norm: 1.57096528
INFO:root:[   11] Training loss: 0.07428914, Validation loss: 0.05942309, Gradient norm: 1.82354750
INFO:root:[   12] Training loss: 0.07298534, Validation loss: 0.07092253, Gradient norm: 1.83323132
INFO:root:[   13] Training loss: 0.07040474, Validation loss: 0.11007543, Gradient norm: 1.57479222
INFO:root:[   14] Training loss: 0.07038337, Validation loss: 0.06938379, Gradient norm: 1.86732848
INFO:root:[   15] Training loss: 0.06999405, Validation loss: 0.06381102, Gradient norm: 1.83285202
INFO:root:[   16] Training loss: 0.06968891, Validation loss: 0.08660542, Gradient norm: 1.87634811
INFO:root:[   17] Training loss: 0.06625988, Validation loss: 0.04658611, Gradient norm: 1.64452793
INFO:root:[   18] Training loss: 0.06503773, Validation loss: 0.04525865, Gradient norm: 1.61960091
INFO:root:[   19] Training loss: 0.06555425, Validation loss: 0.07281608, Gradient norm: 1.85970742
INFO:root:[   20] Training loss: 0.06311218, Validation loss: 0.06905804, Gradient norm: 1.70735206
INFO:root:[   21] Training loss: 0.06257317, Validation loss: 0.06574134, Gradient norm: 1.70525093
INFO:root:[   22] Training loss: 0.06349909, Validation loss: 0.10263190, Gradient norm: 1.87877825
INFO:root:[   23] Training loss: 0.06091155, Validation loss: 0.11214908, Gradient norm: 1.68980381
INFO:root:[   24] Training loss: 0.06130314, Validation loss: 0.11304055, Gradient norm: 1.78503937
INFO:root:[   25] Training loss: 0.06188772, Validation loss: 0.05793967, Gradient norm: 1.82637592
INFO:root:[   26] Training loss: 0.06234413, Validation loss: 0.04009138, Gradient norm: 1.68574783
INFO:root:[   27] Training loss: 0.06024806, Validation loss: 0.05627141, Gradient norm: 1.80322800
INFO:root:[   28] Training loss: 0.05913723, Validation loss: 0.11186016, Gradient norm: 1.83401617
INFO:root:[   29] Training loss: 0.05763023, Validation loss: 0.09823742, Gradient norm: 1.67173394
INFO:root:[   30] Training loss: 0.05909729, Validation loss: 0.08391504, Gradient norm: 1.79332524
INFO:root:[   31] Training loss: 0.05800423, Validation loss: 0.09254766, Gradient norm: 1.56956084
INFO:root:[   32] Training loss: 0.05697116, Validation loss: 0.10853012, Gradient norm: 1.83941236
INFO:root:[   33] Training loss: 0.05605917, Validation loss: 0.10963375, Gradient norm: 1.82830982
INFO:root:[   34] Training loss: 0.05733719, Validation loss: 0.04158234, Gradient norm: 2.00120668
INFO:root:[   35] Training loss: 0.05528238, Validation loss: 0.05189777, Gradient norm: 1.73898357
INFO:root:[   36] Training loss: 0.05516179, Validation loss: 0.06448376, Gradient norm: 1.76178123
INFO:root:[   37] Training loss: 0.05513654, Validation loss: 0.09782420, Gradient norm: 1.84754133
INFO:root:[   38] Training loss: 0.05378961, Validation loss: 0.11165182, Gradient norm: 1.69511030
INFO:root:[   39] Training loss: 0.05282632, Validation loss: 0.10370928, Gradient norm: 1.72986453
INFO:root:[   40] Training loss: 0.05496355, Validation loss: 0.06144895, Gradient norm: 1.94628901
INFO:root:[   41] Training loss: 0.05357403, Validation loss: 0.04447488, Gradient norm: 1.78696953
INFO:root:[   42] Training loss: 0.05271207, Validation loss: 0.03657716, Gradient norm: 1.64044754
INFO:root:[   43] Training loss: 0.05271639, Validation loss: 0.05682224, Gradient norm: 1.86050192
INFO:root:[   44] Training loss: 0.05206339, Validation loss: 0.09826098, Gradient norm: 1.90950858
INFO:root:[   45] Training loss: 0.05095920, Validation loss: 0.10491293, Gradient norm: 1.75046401
INFO:root:[   46] Training loss: 0.05081830, Validation loss: 0.09281255, Gradient norm: 1.68919506
INFO:root:[   47] Training loss: 0.05225172, Validation loss: 0.08340887, Gradient norm: 1.81158685
INFO:root:[   48] Training loss: 0.05107121, Validation loss: 0.06979175, Gradient norm: 1.75935518
INFO:root:[   49] Training loss: 0.05102591, Validation loss: 0.04289373, Gradient norm: 1.85914004
INFO:root:[   50] Training loss: 0.04858299, Validation loss: 0.04165757, Gradient norm: 1.74053482
INFO:root:[   51] Training loss: 0.04842776, Validation loss: 0.03714404, Gradient norm: 1.76733906
INFO:root:[   52] Training loss: 0.04889252, Validation loss: 0.03964731, Gradient norm: 1.86886966
INFO:root:[   53] Training loss: 0.04989767, Validation loss: 0.07240737, Gradient norm: 1.86720740
INFO:root:[   54] Training loss: 0.04805450, Validation loss: 0.09416052, Gradient norm: 1.80073628
INFO:root:[   55] Training loss: 0.04749032, Validation loss: 0.09705997, Gradient norm: 1.78081708
INFO:root:[   56] Training loss: 0.04743358, Validation loss: 0.10907608, Gradient norm: 1.86130446
INFO:root:[   57] Training loss: 0.04934134, Validation loss: 0.04113606, Gradient norm: 2.01796207
INFO:root:[   58] Training loss: 0.04711337, Validation loss: 0.05538057, Gradient norm: 1.82253565
INFO:root:[   59] Training loss: 0.04683944, Validation loss: 0.09526896, Gradient norm: 1.91441286
INFO:root:[   60] Training loss: 0.04521724, Validation loss: 0.09818676, Gradient norm: 1.74402521
INFO:root:[   61] Training loss: 0.04635230, Validation loss: 0.05910336, Gradient norm: 1.95875226
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 916.414s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04873
INFO:root:EnergyScoreTrain: 0.03652
INFO:root:CoverageTrain: 0.91169
INFO:root:IntervalWidthTrain: 0.07838
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.04889
INFO:root:EnergyScoreValidation: 0.03655
INFO:root:CoverageValidation: 0.91402
INFO:root:IntervalWidthValidation: 0.0786
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06047
INFO:root:EnergyScoreTest: 0.04373
INFO:root:CoverageTest: 0.89537
INFO:root:IntervalWidthTest: 0.07971
INFO:root:###9 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.63028675, Validation loss: 0.19244969, Gradient norm: 2.29244908
INFO:root:[    2] Training loss: 0.16236924, Validation loss: 0.12986384, Gradient norm: 1.65858148
INFO:root:[    3] Training loss: 0.12528551, Validation loss: 0.16009461, Gradient norm: 1.45720385
INFO:root:[    4] Training loss: 0.10982906, Validation loss: 0.10658296, Gradient norm: 1.52106984
INFO:root:[    5] Training loss: 0.10433700, Validation loss: 0.20601105, Gradient norm: 1.71385534
INFO:root:[    6] Training loss: 0.10184594, Validation loss: 0.18905747, Gradient norm: 1.77412489
INFO:root:[    7] Training loss: 0.09779588, Validation loss: 0.15369959, Gradient norm: 1.62157977
INFO:root:[    8] Training loss: 0.09273452, Validation loss: 0.15024856, Gradient norm: 1.39222383
INFO:root:[    9] Training loss: 0.09108254, Validation loss: 0.07937025, Gradient norm: 1.52150221
INFO:root:[   10] Training loss: 0.09107614, Validation loss: 0.14946680, Gradient norm: 1.68048132
INFO:root:[   11] Training loss: 0.08858513, Validation loss: 0.11376515, Gradient norm: 1.45841878
INFO:root:[   12] Training loss: 0.08731671, Validation loss: 0.13606508, Gradient norm: 1.71093041
INFO:root:[   13] Training loss: 0.08477559, Validation loss: 0.19873987, Gradient norm: 1.63367675
INFO:root:[   14] Training loss: 0.08321447, Validation loss: 0.14822087, Gradient norm: 1.52110713
INFO:root:[   15] Training loss: 0.08308000, Validation loss: 0.18389980, Gradient norm: 1.55467316
INFO:root:[   16] Training loss: 0.08100538, Validation loss: 0.10091616, Gradient norm: 1.60653183
INFO:root:[   17] Training loss: 0.07981333, Validation loss: 0.19016877, Gradient norm: 1.57168410
INFO:root:[   18] Training loss: 0.07950329, Validation loss: 0.12998789, Gradient norm: 1.61714540
INFO:root:[   19] Training loss: 0.07804110, Validation loss: 0.14859990, Gradient norm: 1.63551460
INFO:root:[   20] Training loss: 0.07671099, Validation loss: 0.14248154, Gradient norm: 1.50775986
INFO:root:[   21] Training loss: 0.07688497, Validation loss: 0.09403536, Gradient norm: 1.67270402
INFO:root:[   22] Training loss: 0.07650898, Validation loss: 0.15431219, Gradient norm: 1.70770646
INFO:root:[   23] Training loss: 0.07491300, Validation loss: 0.17263264, Gradient norm: 1.63631651
INFO:root:[   24] Training loss: 0.07236262, Validation loss: 0.11448465, Gradient norm: 1.57488812
INFO:root:[   25] Training loss: 0.07191433, Validation loss: 0.18097878, Gradient norm: 1.64832434
INFO:root:[   26] Training loss: 0.07144457, Validation loss: 0.12326398, Gradient norm: 1.63824623
INFO:root:[   27] Training loss: 0.07063178, Validation loss: 0.18486176, Gradient norm: 1.62199564
INFO:root:[   28] Training loss: 0.06944678, Validation loss: 0.09789691, Gradient norm: 1.68950232
INFO:root:[   29] Training loss: 0.07003515, Validation loss: 0.14484201, Gradient norm: 1.80132295
INFO:root:[   30] Training loss: 0.06805882, Validation loss: 0.17575134, Gradient norm: 1.68573890
INFO:root:[   31] Training loss: 0.06742019, Validation loss: 0.11129533, Gradient norm: 1.70229629
INFO:root:[   32] Training loss: 0.06555418, Validation loss: 0.17766289, Gradient norm: 1.57999337
INFO:root:[   33] Training loss: 0.06632725, Validation loss: 0.10439389, Gradient norm: 1.54166763
INFO:root:[   34] Training loss: 0.06462217, Validation loss: 0.12421415, Gradient norm: 1.47236759
INFO:root:[   35] Training loss: 0.06365235, Validation loss: 0.18429883, Gradient norm: 1.59949106
INFO:root:[   36] Training loss: 0.06393876, Validation loss: 0.10088391, Gradient norm: 1.73464619
INFO:root:[   37] Training loss: 0.06189181, Validation loss: 0.15994201, Gradient norm: 1.64395480
INFO:root:[   38] Training loss: 0.06146372, Validation loss: 0.14581664, Gradient norm: 1.66553009
INFO:root:[   39] Training loss: 0.06042397, Validation loss: 0.09271710, Gradient norm: 1.60270993
INFO:root:[   40] Training loss: 0.06011144, Validation loss: 0.17862534, Gradient norm: 1.72104791
INFO:root:[   41] Training loss: 0.05950655, Validation loss: 0.10335962, Gradient norm: 1.73244352
INFO:root:[   42] Training loss: 0.05945785, Validation loss: 0.14272371, Gradient norm: 1.67099237
INFO:root:[   43] Training loss: 0.05766746, Validation loss: 0.16774332, Gradient norm: 1.60743946
INFO:root:[   44] Training loss: 0.05761970, Validation loss: 0.09458042, Gradient norm: 1.74885360
INFO:root:[   45] Training loss: 0.05743463, Validation loss: 0.15284441, Gradient norm: 1.76478517
INFO:root:[   46] Training loss: 0.05619856, Validation loss: 0.13331671, Gradient norm: 1.70249965
INFO:root:[   47] Training loss: 0.05523397, Validation loss: 0.08625339, Gradient norm: 1.60670980
INFO:root:[   48] Training loss: 0.05641755, Validation loss: 0.15895799, Gradient norm: 1.70228346
INFO:root:[   49] Training loss: 0.05512275, Validation loss: 0.12758009, Gradient norm: 1.62598544
INFO:root:[   50] Training loss: 0.05391134, Validation loss: 0.08053394, Gradient norm: 1.70383723
INFO:root:[   51] Training loss: 0.05372595, Validation loss: 0.15673652, Gradient norm: 1.77393724
INFO:root:[   52] Training loss: 0.05217044, Validation loss: 0.08769573, Gradient norm: 1.65936046
INFO:root:[   53] Training loss: 0.05172188, Validation loss: 0.10743446, Gradient norm: 1.73917283
INFO:root:[   54] Training loss: 0.05166258, Validation loss: 0.14145924, Gradient norm: 1.76577581
INFO:root:[   55] Training loss: 0.05028244, Validation loss: 0.07889019, Gradient norm: 1.64920636
INFO:root:[   56] Training loss: 0.04997042, Validation loss: 0.11009549, Gradient norm: 1.71042194
INFO:root:[   57] Training loss: 0.04930225, Validation loss: 0.14462108, Gradient norm: 1.64189560
INFO:root:[   58] Training loss: 0.04919407, Validation loss: 0.08884237, Gradient norm: 1.67086211
INFO:root:[   59] Training loss: 0.04902334, Validation loss: 0.08941594, Gradient norm: 1.60522276
INFO:root:[   60] Training loss: 0.04810646, Validation loss: 0.14265514, Gradient norm: 1.67345138
INFO:root:[   61] Training loss: 0.04753007, Validation loss: 0.07208179, Gradient norm: 1.73185884
INFO:root:[   62] Training loss: 0.04697472, Validation loss: 0.10603467, Gradient norm: 1.69952161
INFO:root:[   63] Training loss: 0.04717800, Validation loss: 0.11552546, Gradient norm: 1.70470507
INFO:root:[   64] Training loss: 0.04632300, Validation loss: 0.07233295, Gradient norm: 1.68198875
INFO:root:[   65] Training loss: 0.04578164, Validation loss: 0.13509183, Gradient norm: 1.63841357
INFO:root:[   66] Training loss: 0.04533531, Validation loss: 0.07647085, Gradient norm: 1.63121389
INFO:root:[   67] Training loss: 0.04426695, Validation loss: 0.08362843, Gradient norm: 1.63160003
INFO:root:[   68] Training loss: 0.04401367, Validation loss: 0.13037476, Gradient norm: 1.60521570
INFO:root:[   69] Training loss: 0.04411300, Validation loss: 0.07570500, Gradient norm: 1.70923843
INFO:root:[   70] Training loss: 0.04346482, Validation loss: 0.10130307, Gradient norm: 1.69704784
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 1048.9s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.09863
INFO:root:EnergyScoreTrain: 0.07211
INFO:root:CoverageTrain: 0.75384
INFO:root:IntervalWidthTrain: 0.06981
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.09849
INFO:root:EnergyScoreValidation: 0.07204
INFO:root:CoverageValidation: 0.75499
INFO:root:IntervalWidthValidation: 0.06973
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.10246
INFO:root:EnergyScoreTest: 0.07554
INFO:root:CoverageTest: 0.68932
INFO:root:IntervalWidthTest: 0.06997
INFO:root:###10 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.71724466, Validation loss: 0.13556484, Gradient norm: 2.47794239
INFO:root:[    2] Training loss: 0.21229160, Validation loss: 0.15292121, Gradient norm: 2.01996938
INFO:root:[    3] Training loss: 0.15955783, Validation loss: 0.16464747, Gradient norm: 0.92645489
INFO:root:[    4] Training loss: 0.14470666, Validation loss: 0.27518120, Gradient norm: 1.52422590
INFO:root:[    5] Training loss: 0.13382483, Validation loss: 0.26043601, Gradient norm: 1.25563110
INFO:root:[    6] Training loss: 0.12619958, Validation loss: 0.29815322, Gradient norm: 1.29626015
INFO:root:[    7] Training loss: 0.12505181, Validation loss: 0.26916454, Gradient norm: 1.62618201
INFO:root:[    8] Training loss: 0.12005009, Validation loss: 0.25267220, Gradient norm: 1.47699605
INFO:root:[    9] Training loss: 0.11788153, Validation loss: 0.25509072, Gradient norm: 1.53067801
INFO:root:[   10] Training loss: 0.11727538, Validation loss: 0.35392137, Gradient norm: 1.76780913
INFO:root:[   11] Training loss: 0.11349570, Validation loss: 0.34006793, Gradient norm: 1.53891068
INFO:root:[   12] Training loss: 0.11160881, Validation loss: 0.29116964, Gradient norm: 1.50777501
INFO:root:[   13] Training loss: 0.11099971, Validation loss: 0.26186197, Gradient norm: 1.63464472
INFO:root:[   14] Training loss: 0.10769455, Validation loss: 0.34805732, Gradient norm: 1.48736234
INFO:root:[   15] Training loss: 0.10771398, Validation loss: 0.27822004, Gradient norm: 1.77746082
INFO:root:[   16] Training loss: 0.10363323, Validation loss: 0.27852971, Gradient norm: 1.37368531
INFO:root:[   17] Training loss: 0.10365753, Validation loss: 0.31119160, Gradient norm: 1.61415963
INFO:root:[   18] Training loss: 0.10092710, Validation loss: 0.30529785, Gradient norm: 1.42188467
INFO:root:[   19] Training loss: 0.09960665, Validation loss: 0.32228770, Gradient norm: 1.48336556
INFO:root:[   20] Training loss: 0.09713944, Validation loss: 0.30815881, Gradient norm: 1.38411018
INFO:root:[   21] Training loss: 0.09656220, Validation loss: 0.28495163, Gradient norm: 1.45009463
INFO:root:[   22] Training loss: 0.09551017, Validation loss: 0.29860494, Gradient norm: 1.53177554
INFO:root:[   23] Training loss: 0.09366753, Validation loss: 0.26235504, Gradient norm: 1.44512276
INFO:root:[   24] Training loss: 0.09262822, Validation loss: 0.25341942, Gradient norm: 1.46978663
INFO:root:[   25] Training loss: 0.09129437, Validation loss: 0.26175147, Gradient norm: 1.49585753
INFO:root:[   26] Training loss: 0.09048872, Validation loss: 0.30814111, Gradient norm: 1.50602801
INFO:root:[   27] Training loss: 0.08818683, Validation loss: 0.34202300, Gradient norm: 1.42199388
INFO:root:[   28] Training loss: 0.08855512, Validation loss: 0.34098132, Gradient norm: 1.58061662
INFO:root:[   29] Training loss: 0.08672448, Validation loss: 0.28614785, Gradient norm: 1.52561028
INFO:root:[   30] Training loss: 0.08586772, Validation loss: 0.22602860, Gradient norm: 1.54190887
INFO:root:[   31] Training loss: 0.08443374, Validation loss: 0.24234080, Gradient norm: 1.59565991
INFO:root:[   32] Training loss: 0.08316539, Validation loss: 0.28023405, Gradient norm: 1.60325690
INFO:root:[   33] Training loss: 0.08085108, Validation loss: 0.32670912, Gradient norm: 1.45888263
INFO:root:[   34] Training loss: 0.08010102, Validation loss: 0.25416844, Gradient norm: 1.57177829
INFO:root:[   35] Training loss: 0.07908798, Validation loss: 0.22768951, Gradient norm: 1.58308652
INFO:root:[   36] Training loss: 0.07727482, Validation loss: 0.31347157, Gradient norm: 1.51295739
INFO:root:[   37] Training loss: 0.07661093, Validation loss: 0.22406695, Gradient norm: 1.52528993
INFO:root:[   38] Training loss: 0.07529283, Validation loss: 0.29300605, Gradient norm: 1.55575221
INFO:root:[   39] Training loss: 0.07524454, Validation loss: 0.29520723, Gradient norm: 1.66713664
INFO:root:[   40] Training loss: 0.07287889, Validation loss: 0.21754648, Gradient norm: 1.54395088
INFO:root:[   41] Training loss: 0.07212523, Validation loss: 0.30258450, Gradient norm: 1.58809065
INFO:root:[   42] Training loss: 0.07094029, Validation loss: 0.22109383, Gradient norm: 1.59353754
INFO:root:[   43] Training loss: 0.07056403, Validation loss: 0.22442597, Gradient norm: 1.63731844
INFO:root:[   44] Training loss: 0.06910729, Validation loss: 0.28932156, Gradient norm: 1.61395827
INFO:root:[   45] Training loss: 0.06795489, Validation loss: 0.21875108, Gradient norm: 1.59187627
INFO:root:[   46] Training loss: 0.06696872, Validation loss: 0.23797568, Gradient norm: 1.61791383
INFO:root:[   47] Training loss: 0.06612458, Validation loss: 0.28435199, Gradient norm: 1.61750137
INFO:root:[   48] Training loss: 0.06609734, Validation loss: 0.22330650, Gradient norm: 1.67994896
INFO:root:[   49] Training loss: 0.06405161, Validation loss: 0.20667966, Gradient norm: 1.56131968
INFO:root:[   50] Training loss: 0.06371465, Validation loss: 0.27480054, Gradient norm: 1.57950751
INFO:root:[   51] Training loss: 0.06326736, Validation loss: 0.23597534, Gradient norm: 1.62010490
INFO:root:[   52] Training loss: 0.06155756, Validation loss: 0.18096950, Gradient norm: 1.54709709
INFO:root:[   53] Training loss: 0.06123510, Validation loss: 0.23019071, Gradient norm: 1.64834606
INFO:root:[   54] Training loss: 0.06009511, Validation loss: 0.23930312, Gradient norm: 1.52423868
INFO:root:[   55] Training loss: 0.05942769, Validation loss: 0.17558868, Gradient norm: 1.58219367
INFO:root:[   56] Training loss: 0.05838177, Validation loss: 0.19426233, Gradient norm: 1.58265337
INFO:root:[   57] Training loss: 0.05718292, Validation loss: 0.25014919, Gradient norm: 1.49436499
INFO:root:[   58] Training loss: 0.05646770, Validation loss: 0.16650991, Gradient norm: 1.49563518
INFO:root:[   59] Training loss: 0.05551879, Validation loss: 0.23978319, Gradient norm: 1.51153161
INFO:root:[   60] Training loss: 0.05566827, Validation loss: 0.23463236, Gradient norm: 1.59997316
INFO:root:[   61] Training loss: 0.05446952, Validation loss: 0.15884493, Gradient norm: 1.53900352
INFO:root:[   62] Training loss: 0.05430780, Validation loss: 0.15442906, Gradient norm: 1.64126900
INFO:root:[   63] Training loss: 0.05348250, Validation loss: 0.19406780, Gradient norm: 1.60234061
INFO:root:[   64] Training loss: 0.05302809, Validation loss: 0.20743251, Gradient norm: 1.60020960
INFO:root:[   65] Training loss: 0.05181207, Validation loss: 0.22689371, Gradient norm: 1.49658804
INFO:root:[   66] Training loss: 0.05126758, Validation loss: 0.18706319, Gradient norm: 1.55424261
INFO:root:[   67] Training loss: 0.05095221, Validation loss: 0.15747242, Gradient norm: 1.53178617
INFO:root:[   68] Training loss: 0.04957989, Validation loss: 0.14225682, Gradient norm: 1.53043261
INFO:root:[   69] Training loss: 0.04921686, Validation loss: 0.19770052, Gradient norm: 1.47237580
INFO:root:[   70] Training loss: 0.04862687, Validation loss: 0.20928884, Gradient norm: 1.55081671
INFO:root:[   71] Training loss: 0.04805446, Validation loss: 0.21101319, Gradient norm: 1.58645754
INFO:root:[   72] Training loss: 0.04693470, Validation loss: 0.16890546, Gradient norm: 1.52752031
INFO:root:[   73] Training loss: 0.04626831, Validation loss: 0.13764736, Gradient norm: 1.49703261
INFO:root:[   74] Training loss: 0.04533137, Validation loss: 0.13317967, Gradient norm: 1.48822305
INFO:root:[   75] Training loss: 0.04442572, Validation loss: 0.19546737, Gradient norm: 1.39706217
INFO:root:[   76] Training loss: 0.04521814, Validation loss: 0.17458229, Gradient norm: 1.50649018
INFO:root:[   77] Training loss: 0.04384747, Validation loss: 0.11861469, Gradient norm: 1.44083705
INFO:root:[   78] Training loss: 0.04332891, Validation loss: 0.11988058, Gradient norm: 1.53321503
INFO:root:[   79] Training loss: 0.04251201, Validation loss: 0.16082278, Gradient norm: 1.51287588
INFO:root:[   80] Training loss: 0.04218342, Validation loss: 0.17792290, Gradient norm: 1.51949735
INFO:root:[   81] Training loss: 0.04125830, Validation loss: 0.16715370, Gradient norm: 1.50586746
INFO:root:[   82] Training loss: 0.04099623, Validation loss: 0.11556746, Gradient norm: 1.42904594
INFO:root:[   83] Training loss: 0.04017929, Validation loss: 0.13889793, Gradient norm: 1.46736942
INFO:root:[   84] Training loss: 0.03952723, Validation loss: 0.17120982, Gradient norm: 1.44670765
INFO:root:[   85] Training loss: 0.04026325, Validation loss: 0.13864271, Gradient norm: 1.64303491
INFO:root:[   86] Training loss: 0.03825827, Validation loss: 0.16461409, Gradient norm: 1.39463116
INFO:root:[   87] Training loss: 0.03815378, Validation loss: 0.15222663, Gradient norm: 1.50323345
INFO:root:[   88] Training loss: 0.03710114, Validation loss: 0.09945349, Gradient norm: 1.39481314
INFO:root:[   89] Training loss: 0.03756790, Validation loss: 0.09515844, Gradient norm: 1.50043997
INFO:root:[   90] Training loss: 0.03639437, Validation loss: 0.11416228, Gradient norm: 1.45413933
INFO:root:[   91] Training loss: 0.03576601, Validation loss: 0.14863054, Gradient norm: 1.40140702
INFO:root:[   92] Training loss: 0.03527235, Validation loss: 0.14016936, Gradient norm: 1.45259326
INFO:root:[   93] Training loss: 0.03471538, Validation loss: 0.11103238, Gradient norm: 1.41318571
INFO:root:[   94] Training loss: 0.03447686, Validation loss: 0.08475043, Gradient norm: 1.34713926
INFO:root:[   95] Training loss: 0.03412810, Validation loss: 0.11099478, Gradient norm: 1.43558834
INFO:root:[   96] Training loss: 0.03354712, Validation loss: 0.13862343, Gradient norm: 1.37195323
INFO:root:[   97] Training loss: 0.03399358, Validation loss: 0.13022961, Gradient norm: 1.52793606
INFO:root:[   98] Training loss: 0.03264849, Validation loss: 0.12983105, Gradient norm: 1.36057362
INFO:root:[   99] Training loss: 0.03199454, Validation loss: 0.10027257, Gradient norm: 1.36310334
INFO:root:[  100] Training loss: 0.03247488, Validation loss: 0.07038404, Gradient norm: 1.33223196
INFO:root:[  101] Training loss: 0.03165435, Validation loss: 0.10082278, Gradient norm: 1.31086332
INFO:root:[  102] Training loss: 0.03108792, Validation loss: 0.11109095, Gradient norm: 1.39939394
INFO:root:[  103] Training loss: 0.03015267, Validation loss: 0.12283525, Gradient norm: 1.31514083
INFO:root:[  104] Training loss: 0.03221495, Validation loss: 0.05829962, Gradient norm: 1.72098227
INFO:root:[  105] Training loss: 0.03161035, Validation loss: 0.12327348, Gradient norm: 1.68818233
INFO:root:[  106] Training loss: 0.02979499, Validation loss: 0.10848227, Gradient norm: 1.40673023
INFO:root:[  107] Training loss: 0.02936488, Validation loss: 0.08570625, Gradient norm: 1.34022188
INFO:root:[  108] Training loss: 0.02898934, Validation loss: 0.05149289, Gradient norm: 1.35375877
INFO:root:[  109] Training loss: 0.03000169, Validation loss: 0.11340303, Gradient norm: 1.62407366
INFO:root:[  110] Training loss: 0.02939609, Validation loss: 0.08295179, Gradient norm: 1.36317726
INFO:root:[  111] Training loss: 0.02754574, Validation loss: 0.06338311, Gradient norm: 1.26434030
INFO:root:[  112] Training loss: 0.02742648, Validation loss: 0.04677733, Gradient norm: 1.36907871
INFO:root:[  113] Training loss: 0.02730196, Validation loss: 0.07886950, Gradient norm: 1.37355516
INFO:root:[  114] Training loss: 0.02615351, Validation loss: 0.09463868, Gradient norm: 1.24351196
INFO:root:[  115] Training loss: 0.02594153, Validation loss: 0.09193758, Gradient norm: 1.32176350
INFO:root:[  116] Training loss: 0.02593436, Validation loss: 0.08033254, Gradient norm: 1.24429833
INFO:root:[  117] Training loss: 0.02705367, Validation loss: 0.05420227, Gradient norm: 1.43178031
INFO:root:[  118] Training loss: 0.02597272, Validation loss: 0.08585372, Gradient norm: 1.34753199
INFO:root:[  119] Training loss: 0.02620348, Validation loss: 0.08991395, Gradient norm: 1.25102104
INFO:root:[  120] Training loss: 0.02442945, Validation loss: 0.08549938, Gradient norm: 1.10010807
INFO:root:[  121] Training loss: 0.02449361, Validation loss: 0.06220553, Gradient norm: 1.25245922
INFO:root:[  122] Training loss: 0.02514817, Validation loss: 0.04291354, Gradient norm: 1.15620453
INFO:root:[  123] Training loss: 0.02446542, Validation loss: 0.04003278, Gradient norm: 1.13129619
INFO:root:[  124] Training loss: 0.02369440, Validation loss: 0.06595882, Gradient norm: 1.22299640
INFO:root:[  125] Training loss: 0.02309662, Validation loss: 0.07737047, Gradient norm: 1.13064523
INFO:root:[  126] Training loss: 0.02399764, Validation loss: 0.03129600, Gradient norm: 1.20478618
INFO:root:[  127] Training loss: 0.02493088, Validation loss: 0.07524928, Gradient norm: 1.24450424
INFO:root:[  128] Training loss: 0.02317692, Validation loss: 0.03657054, Gradient norm: 1.27954318
INFO:root:[  129] Training loss: 0.02359032, Validation loss: 0.05255097, Gradient norm: 0.99242860
INFO:root:[  130] Training loss: 0.02260414, Validation loss: 0.04988723, Gradient norm: 1.23449548
INFO:root:[  131] Training loss: 0.02364100, Validation loss: 0.02822270, Gradient norm: 1.07747417
INFO:root:[  132] Training loss: 0.02283033, Validation loss: 0.03263362, Gradient norm: 1.24577218
INFO:root:[  133] Training loss: 0.02265445, Validation loss: 0.03134324, Gradient norm: 1.01807994
INFO:root:[  134] Training loss: 0.02263445, Validation loss: 0.02704120, Gradient norm: 1.21621723
INFO:root:[  135] Training loss: 0.02265931, Validation loss: 0.06160069, Gradient norm: 0.97526197
INFO:root:[  136] Training loss: 0.02271069, Validation loss: 0.04252191, Gradient norm: 1.13184839
INFO:root:[  137] Training loss: 0.02144589, Validation loss: 0.05116615, Gradient norm: 0.94379495
INFO:root:[  138] Training loss: 0.02131831, Validation loss: 0.04356821, Gradient norm: 1.07309875
INFO:root:[  139] Training loss: 0.02141851, Validation loss: 0.02583541, Gradient norm: 1.00875188
INFO:root:[  140] Training loss: 0.02250035, Validation loss: 0.05299463, Gradient norm: 1.02813994
INFO:root:[  141] Training loss: 0.02314913, Validation loss: 0.05966455, Gradient norm: 1.12332836
INFO:root:[  142] Training loss: 0.02186432, Validation loss: 0.05690350, Gradient norm: 1.04855376
INFO:root:[  143] Training loss: 0.02110451, Validation loss: 0.02637012, Gradient norm: 0.90403965
INFO:root:[  144] Training loss: 0.02297306, Validation loss: 0.03101182, Gradient norm: 0.97569205
INFO:root:[  145] Training loss: 0.02211344, Validation loss: 0.02166241, Gradient norm: 1.09566382
INFO:root:[  146] Training loss: 0.02162629, Validation loss: 0.02469722, Gradient norm: 1.03782308
INFO:root:[  147] Training loss: 0.02161872, Validation loss: 0.02093001, Gradient norm: 1.03125137
INFO:root:[  148] Training loss: 0.02066142, Validation loss: 0.02702777, Gradient norm: 0.85307459
INFO:root:[  149] Training loss: 0.02205537, Validation loss: 0.02914653, Gradient norm: 0.74425167
INFO:root:[  150] Training loss: 0.02135912, Validation loss: 0.02357479, Gradient norm: 0.87286580
INFO:root:[  151] Training loss: 0.02094428, Validation loss: 0.02732002, Gradient norm: 0.97608242
INFO:root:[  152] Training loss: 0.02039387, Validation loss: 0.03982493, Gradient norm: 0.93381116
INFO:root:[  153] Training loss: 0.02102178, Validation loss: 0.02846267, Gradient norm: 0.83727355
INFO:root:[  154] Training loss: 0.01980106, Validation loss: 0.04603282, Gradient norm: 0.88889609
INFO:root:[  155] Training loss: 0.01984132, Validation loss: 0.03102339, Gradient norm: 0.95834583
INFO:root:[  156] Training loss: 0.02127369, Validation loss: 0.02659854, Gradient norm: 0.98830074
INFO:root:EP 156: Early stopping
INFO:root:Training the model took 2316.205s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02898
INFO:root:EnergyScoreTrain: 0.01774
INFO:root:CoverageTrain: 0.89615
INFO:root:IntervalWidthTrain: 0.01976
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02889
INFO:root:EnergyScoreValidation: 0.01763
INFO:root:CoverageValidation: 0.89789
INFO:root:IntervalWidthValidation: 0.01961
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.0349
INFO:root:EnergyScoreTest: 0.02215
INFO:root:CoverageTest: 0.86106
INFO:root:IntervalWidthTest: 0.01906
INFO:root:###11 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.74226960, Validation loss: 0.16926681, Gradient norm: 2.92883686
INFO:root:[    2] Training loss: 0.22284660, Validation loss: 0.38292987, Gradient norm: 0.84790104
INFO:root:[    3] Training loss: 0.17692705, Validation loss: 0.35279865, Gradient norm: 1.27523608
INFO:root:[    4] Training loss: 0.15585628, Validation loss: 0.43838165, Gradient norm: 0.74079685
INFO:root:[    5] Training loss: 0.14252820, Validation loss: 0.47658271, Gradient norm: 0.64777758
INFO:root:[    6] Training loss: 0.13989207, Validation loss: 0.55468026, Gradient norm: 1.37673058
INFO:root:[    7] Training loss: 0.13379409, Validation loss: 0.43942726, Gradient norm: 1.34554036
INFO:root:[    8] Training loss: 0.12951594, Validation loss: 0.52068995, Gradient norm: 1.34085186
INFO:root:[    9] Training loss: 0.12611358, Validation loss: 0.42747060, Gradient norm: 1.34351959
INFO:root:[   10] Training loss: 0.12357302, Validation loss: 0.55068059, Gradient norm: 1.36066751
INFO:root:[   11] Training loss: 0.12202064, Validation loss: 0.42681609, Gradient norm: 1.36758471
INFO:root:[   12] Training loss: 0.11840910, Validation loss: 0.49120643, Gradient norm: 1.32706752
INFO:root:[   13] Training loss: 0.11500286, Validation loss: 0.54014545, Gradient norm: 1.11723665
INFO:root:[   14] Training loss: 0.11388087, Validation loss: 0.42618573, Gradient norm: 1.38802543
INFO:root:[   15] Training loss: 0.11225316, Validation loss: 0.49853267, Gradient norm: 1.53149187
INFO:root:[   16] Training loss: 0.10955857, Validation loss: 0.47093176, Gradient norm: 1.33871456
INFO:root:[   17] Training loss: 0.10791185, Validation loss: 0.42405581, Gradient norm: 1.44185013
INFO:root:[   18] Training loss: 0.10617016, Validation loss: 0.49228158, Gradient norm: 1.50329089
INFO:root:[   19] Training loss: 0.10413120, Validation loss: 0.40962123, Gradient norm: 1.49701933
INFO:root:[   20] Training loss: 0.10230035, Validation loss: 0.48319537, Gradient norm: 1.34701452
INFO:root:[   21] Training loss: 0.09983735, Validation loss: 0.40924516, Gradient norm: 1.45784934
INFO:root:[   22] Training loss: 0.09809753, Validation loss: 0.44366168, Gradient norm: 1.46325507
INFO:root:[   23] Training loss: 0.09417372, Validation loss: 0.41178897, Gradient norm: 1.15740166
INFO:root:[   24] Training loss: 0.09345743, Validation loss: 0.40133184, Gradient norm: 1.25625303
INFO:root:[   25] Training loss: 0.09380979, Validation loss: 0.48457462, Gradient norm: 1.55033919
INFO:root:[   26] Training loss: 0.09113500, Validation loss: 0.46960647, Gradient norm: 1.32501805
INFO:root:[   27] Training loss: 0.08924059, Validation loss: 0.42072596, Gradient norm: 1.44430032
INFO:root:[   28] Training loss: 0.08723295, Validation loss: 0.35170283, Gradient norm: 1.42863839
INFO:root:[   29] Training loss: 0.08770272, Validation loss: 0.45714955, Gradient norm: 1.76518402
INFO:root:[   30] Training loss: 0.08569189, Validation loss: 0.34083324, Gradient norm: 1.65669670
INFO:root:[   31] Training loss: 0.08317726, Validation loss: 0.42259732, Gradient norm: 1.50071249
INFO:root:[   32] Training loss: 0.08046553, Validation loss: 0.43290784, Gradient norm: 1.28595343
INFO:root:[   33] Training loss: 0.07901510, Validation loss: 0.43373182, Gradient norm: 1.35830359
INFO:root:[   34] Training loss: 0.07780783, Validation loss: 0.40355312, Gradient norm: 1.40489084
INFO:root:[   35] Training loss: 0.07695286, Validation loss: 0.35753248, Gradient norm: 1.40168151
INFO:root:[   36] Training loss: 0.07460775, Validation loss: 0.34246121, Gradient norm: 1.36232326
INFO:root:[   37] Training loss: 0.07376366, Validation loss: 0.30493946, Gradient norm: 1.44599957
INFO:root:[   38] Training loss: 0.07249701, Validation loss: 0.31114770, Gradient norm: 1.40678567
INFO:root:[   39] Training loss: 0.07093865, Validation loss: 0.31842989, Gradient norm: 1.42754438
INFO:root:[   40] Training loss: 0.06933188, Validation loss: 0.30113164, Gradient norm: 1.32293803
INFO:root:[   41] Training loss: 0.06920034, Validation loss: 0.27957367, Gradient norm: 1.46402210
INFO:root:[   42] Training loss: 0.07030189, Validation loss: 0.36403022, Gradient norm: 1.79434266
INFO:root:[   43] Training loss: 0.06783872, Validation loss: 0.29467232, Gradient norm: 1.71112600
INFO:root:[   44] Training loss: 0.06577673, Validation loss: 0.36211130, Gradient norm: 1.48567129
INFO:root:[   45] Training loss: 0.06398766, Validation loss: 0.31107340, Gradient norm: 1.41629979
INFO:root:[   46] Training loss: 0.06239505, Validation loss: 0.29082469, Gradient norm: 1.28332178
INFO:root:[   47] Training loss: 0.06169982, Validation loss: 0.26422373, Gradient norm: 1.36863338
INFO:root:[   48] Training loss: 0.06045194, Validation loss: 0.26656722, Gradient norm: 1.36694906
INFO:root:[   49] Training loss: 0.05974739, Validation loss: 0.25002826, Gradient norm: 1.30335676
INFO:root:[   50] Training loss: 0.05944041, Validation loss: 0.33027626, Gradient norm: 1.48597686
INFO:root:[   51] Training loss: 0.05706289, Validation loss: 0.32328008, Gradient norm: 1.34259878
INFO:root:[   52] Training loss: 0.05603909, Validation loss: 0.32053760, Gradient norm: 1.36558763
INFO:root:[   53] Training loss: 0.05496493, Validation loss: 0.31023840, Gradient norm: 1.38273255
INFO:root:[   54] Training loss: 0.05373279, Validation loss: 0.30363346, Gradient norm: 1.35276523
INFO:root:[   55] Training loss: 0.05278287, Validation loss: 0.30361259, Gradient norm: 1.31659267
INFO:root:[   56] Training loss: 0.05236388, Validation loss: 0.28195521, Gradient norm: 1.25118868
INFO:root:[   57] Training loss: 0.05155819, Validation loss: 0.27908626, Gradient norm: 1.36954934
INFO:root:[   58] Training loss: 0.05063617, Validation loss: 0.29131591, Gradient norm: 1.39166554
INFO:root:[   59] Training loss: 0.04980933, Validation loss: 0.27128907, Gradient norm: 1.43178555
INFO:root:[   60] Training loss: 0.04835295, Validation loss: 0.23794130, Gradient norm: 1.39857414
INFO:root:[   61] Training loss: 0.04671062, Validation loss: 0.26126567, Gradient norm: 1.26171945
INFO:root:[   62] Training loss: 0.04645684, Validation loss: 0.24886345, Gradient norm: 1.36121077
INFO:root:[   63] Training loss: 0.04632092, Validation loss: 0.22322536, Gradient norm: 1.37824278
INFO:root:[   64] Training loss: 0.04570489, Validation loss: 0.17087567, Gradient norm: 1.43821601
INFO:root:[   65] Training loss: 0.04480054, Validation loss: 0.18098832, Gradient norm: 1.31973739
INFO:root:[   66] Training loss: 0.04320252, Validation loss: 0.17355095, Gradient norm: 1.24175423
INFO:root:[   67] Training loss: 0.04274574, Validation loss: 0.17089948, Gradient norm: 1.40551256
INFO:root:[   68] Training loss: 0.04232827, Validation loss: 0.20760703, Gradient norm: 1.44548196
INFO:root:[   69] Training loss: 0.04295084, Validation loss: 0.16775100, Gradient norm: 1.61759534
INFO:root:[   70] Training loss: 0.04163423, Validation loss: 0.20312891, Gradient norm: 1.59366526
INFO:root:[   71] Training loss: 0.03998034, Validation loss: 0.20978230, Gradient norm: 1.33511580
INFO:root:[   72] Training loss: 0.03968113, Validation loss: 0.14982824, Gradient norm: 1.38618580
INFO:root:[   73] Training loss: 0.03895152, Validation loss: 0.18227846, Gradient norm: 1.32249931
INFO:root:[   74] Training loss: 0.03855511, Validation loss: 0.15086235, Gradient norm: 1.50993831
INFO:root:[   75] Training loss: 0.03737743, Validation loss: 0.16200793, Gradient norm: 1.30101182
INFO:root:[   76] Training loss: 0.03562193, Validation loss: 0.18626001, Gradient norm: 1.23145603
INFO:root:[   77] Training loss: 0.03542217, Validation loss: 0.17910745, Gradient norm: 1.29456875
INFO:root:[   78] Training loss: 0.03606823, Validation loss: 0.12687268, Gradient norm: 1.50218004
INFO:root:[   79] Training loss: 0.03470904, Validation loss: 0.17163658, Gradient norm: 1.28269484
INFO:root:[   80] Training loss: 0.03413531, Validation loss: 0.14565353, Gradient norm: 1.26168721
INFO:root:[   81] Training loss: 0.03331728, Validation loss: 0.10757380, Gradient norm: 1.14547181
INFO:root:[   82] Training loss: 0.03316871, Validation loss: 0.13425399, Gradient norm: 1.28737582
INFO:root:[   83] Training loss: 0.03275200, Validation loss: 0.15151381, Gradient norm: 1.19048934
INFO:root:[   84] Training loss: 0.03222702, Validation loss: 0.14579865, Gradient norm: 1.15663475
INFO:root:[   85] Training loss: 0.03174049, Validation loss: 0.09390660, Gradient norm: 1.35787603
INFO:root:[   86] Training loss: 0.03135844, Validation loss: 0.14343037, Gradient norm: 1.33850493
INFO:root:[   87] Training loss: 0.02980913, Validation loss: 0.14353593, Gradient norm: 1.03766096
INFO:root:[   88] Training loss: 0.03020107, Validation loss: 0.10298236, Gradient norm: 1.20586523
INFO:root:[   89] Training loss: 0.02977258, Validation loss: 0.10638226, Gradient norm: 1.25198302
INFO:root:[   90] Training loss: 0.02911019, Validation loss: 0.12749450, Gradient norm: 1.06432471
INFO:root:[   91] Training loss: 0.02940035, Validation loss: 0.08019142, Gradient norm: 1.18425128
INFO:root:[   92] Training loss: 0.02826186, Validation loss: 0.08800322, Gradient norm: 1.10014134
INFO:root:[   93] Training loss: 0.02788366, Validation loss: 0.12264038, Gradient norm: 1.14437246
INFO:root:[   94] Training loss: 0.02789885, Validation loss: 0.07066458, Gradient norm: 1.22062131
INFO:root:[   95] Training loss: 0.02815316, Validation loss: 0.10327098, Gradient norm: 1.18956020
INFO:root:[   96] Training loss: 0.02848210, Validation loss: 0.06256621, Gradient norm: 1.03835067
INFO:root:[   97] Training loss: 0.02681588, Validation loss: 0.08233901, Gradient norm: 1.04959482
INFO:root:[   98] Training loss: 0.02714409, Validation loss: 0.09903659, Gradient norm: 0.98956828
INFO:root:[   99] Training loss: 0.02650916, Validation loss: 0.04881908, Gradient norm: 1.05577212
INFO:root:[  100] Training loss: 0.02619186, Validation loss: 0.08725210, Gradient norm: 1.14246379
INFO:root:[  101] Training loss: 0.02781824, Validation loss: 0.05871009, Gradient norm: 1.05602119
INFO:root:[  102] Training loss: 0.02677830, Validation loss: 0.09857415, Gradient norm: 1.00950934
INFO:root:[  103] Training loss: 0.02667678, Validation loss: 0.08410149, Gradient norm: 1.14976353
INFO:root:[  104] Training loss: 0.02591628, Validation loss: 0.07520604, Gradient norm: 1.04177373
INFO:root:[  105] Training loss: 0.02622543, Validation loss: 0.05284527, Gradient norm: 1.06785124
INFO:root:[  106] Training loss: 0.02481822, Validation loss: 0.06452685, Gradient norm: 0.96561849
INFO:root:[  107] Training loss: 0.02529984, Validation loss: 0.08231847, Gradient norm: 0.98727477
INFO:root:[  108] Training loss: 0.02648351, Validation loss: 0.06280010, Gradient norm: 0.94233903
INFO:root:EP 108: Early stopping
INFO:root:Training the model took 1602.271s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06293
INFO:root:EnergyScoreTrain: 0.04938
INFO:root:CoverageTrain: 0.1821
INFO:root:IntervalWidthTrain: 0.0269
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06259
INFO:root:EnergyScoreValidation: 0.04918
INFO:root:CoverageValidation: 0.17647
INFO:root:IntervalWidthValidation: 0.02668
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06935
INFO:root:EnergyScoreTest: 0.05566
INFO:root:CoverageTest: 0.17435
INFO:root:IntervalWidthTest: 0.0258
INFO:root:###12 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2698534
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.86861817, Validation loss: 0.25757159, Gradient norm: 2.60390263
INFO:root:[    2] Training loss: 0.23589624, Validation loss: 0.60837759, Gradient norm: 0.84586478
INFO:root:[    3] Training loss: 0.19286942, Validation loss: 0.60744170, Gradient norm: 0.82033788
INFO:root:[    4] Training loss: 0.17826503, Validation loss: 0.70422577, Gradient norm: 0.88591129
INFO:root:[    5] Training loss: 0.16717499, Validation loss: 0.75037155, Gradient norm: 0.65840069
INFO:root:[    6] Training loss: 0.15894791, Validation loss: 0.79943233, Gradient norm: 0.85573803
INFO:root:[    7] Training loss: 0.15466344, Validation loss: 0.75831093, Gradient norm: 0.99261850
INFO:root:[    8] Training loss: 0.14836144, Validation loss: 0.70817175, Gradient norm: 0.81839390
INFO:root:[    9] Training loss: 0.14401271, Validation loss: 0.67150588, Gradient norm: 1.02616633
INFO:root:[   10] Training loss: 0.14110126, Validation loss: 0.76370582, Gradient norm: 1.07304878
INFO:root:[   11] Training loss: 0.13505558, Validation loss: 0.71484959, Gradient norm: 0.79850099
INFO:root:[   12] Training loss: 0.13210144, Validation loss: 0.72471439, Gradient norm: 0.99221660
INFO:root:[   13] Training loss: 0.12890301, Validation loss: 0.69023662, Gradient norm: 1.01365648
INFO:root:[   14] Training loss: 0.12482561, Validation loss: 0.60341128, Gradient norm: 0.94136875
INFO:root:[   15] Training loss: 0.12439280, Validation loss: 0.63350542, Gradient norm: 1.00065535
INFO:root:[   16] Training loss: 0.11902161, Validation loss: 0.65936931, Gradient norm: 1.02601641
INFO:root:[   17] Training loss: 0.11614177, Validation loss: 0.52771116, Gradient norm: 1.16860464
INFO:root:[   18] Training loss: 0.11275721, Validation loss: 0.61719631, Gradient norm: 1.12913592
INFO:root:[   19] Training loss: 0.10990111, Validation loss: 0.55213134, Gradient norm: 1.19918175
INFO:root:[   20] Training loss: 0.10589528, Validation loss: 0.56481729, Gradient norm: 1.06675299
INFO:root:[   21] Training loss: 0.10392964, Validation loss: 0.55183317, Gradient norm: 1.15938260
INFO:root:[   22] Training loss: 0.10168000, Validation loss: 0.50243687, Gradient norm: 1.30225398
INFO:root:[   23] Training loss: 0.09884472, Validation loss: 0.58312989, Gradient norm: 1.30194721
INFO:root:[   24] Training loss: 0.09557222, Validation loss: 0.46844179, Gradient norm: 1.19711624
INFO:root:[   25] Training loss: 0.09328781, Validation loss: 0.56504246, Gradient norm: 1.27122544
INFO:root:[   26] Training loss: 0.09010001, Validation loss: 0.48792578, Gradient norm: 1.20083794
INFO:root:[   27] Training loss: 0.08728714, Validation loss: 0.44893535, Gradient norm: 1.23178360
INFO:root:[   28] Training loss: 0.08455330, Validation loss: 0.50019652, Gradient norm: 1.28976336
INFO:root:[   29] Training loss: 0.08242997, Validation loss: 0.51609915, Gradient norm: 1.31969452
INFO:root:[   30] Training loss: 0.07983783, Validation loss: 0.45935430, Gradient norm: 1.31938226
INFO:root:[   31] Training loss: 0.07799807, Validation loss: 0.39748347, Gradient norm: 1.44806057
INFO:root:[   32] Training loss: 0.07479296, Validation loss: 0.46725760, Gradient norm: 1.32972580
INFO:root:[   33] Training loss: 0.07068742, Validation loss: 0.39848115, Gradient norm: 1.09772802
INFO:root:[   34] Training loss: 0.07004555, Validation loss: 0.40359278, Gradient norm: 1.38910328
INFO:root:[   35] Training loss: 0.06897816, Validation loss: 0.44580034, Gradient norm: 1.44473228
INFO:root:[   36] Training loss: 0.06522234, Validation loss: 0.39692254, Gradient norm: 1.26458892
INFO:root:[   37] Training loss: 0.06425725, Validation loss: 0.40264163, Gradient norm: 1.40944665
INFO:root:[   38] Training loss: 0.06125873, Validation loss: 0.39531029, Gradient norm: 1.31027459
INFO:root:[   39] Training loss: 0.06014619, Validation loss: 0.34511416, Gradient norm: 1.31519072
INFO:root:[   40] Training loss: 0.05833656, Validation loss: 0.32566262, Gradient norm: 1.26132080
INFO:root:[   41] Training loss: 0.05684381, Validation loss: 0.34317903, Gradient norm: 1.38581339
INFO:root:[   42] Training loss: 0.05444838, Validation loss: 0.33718504, Gradient norm: 1.32988465
INFO:root:[   43] Training loss: 0.05326665, Validation loss: 0.32880689, Gradient norm: 1.31112901
INFO:root:[   44] Training loss: 0.05189249, Validation loss: 0.32918067, Gradient norm: 1.30048569
INFO:root:[   45] Training loss: 0.05194058, Validation loss: 0.23714131, Gradient norm: 1.55852882
INFO:root:[   46] Training loss: 0.04875752, Validation loss: 0.25093461, Gradient norm: 1.21198093
INFO:root:[   47] Training loss: 0.04732324, Validation loss: 0.27055085, Gradient norm: 1.13952656
INFO:root:[   48] Training loss: 0.04697133, Validation loss: 0.22967628, Gradient norm: 1.29648626
INFO:root:[   49] Training loss: 0.04497199, Validation loss: 0.20704401, Gradient norm: 1.24262267
INFO:root:[   50] Training loss: 0.04441605, Validation loss: 0.19788419, Gradient norm: 1.22109882
INFO:root:[   51] Training loss: 0.04330740, Validation loss: 0.18670711, Gradient norm: 1.31167171
INFO:root:[   52] Training loss: 0.04177857, Validation loss: 0.17630245, Gradient norm: 1.15288434
INFO:root:[   53] Training loss: 0.04208655, Validation loss: 0.19093305, Gradient norm: 1.26679595
INFO:root:[   54] Training loss: 0.04031536, Validation loss: 0.20471787, Gradient norm: 1.17594095
INFO:root:[   55] Training loss: 0.03987015, Validation loss: 0.18617011, Gradient norm: 1.12506588
INFO:root:[   56] Training loss: 0.03871639, Validation loss: 0.17587397, Gradient norm: 1.09017559
INFO:root:[   57] Training loss: 0.03892606, Validation loss: 0.13675662, Gradient norm: 1.17808986
INFO:root:[   58] Training loss: 0.03789284, Validation loss: 0.11369142, Gradient norm: 1.15891336
INFO:root:[   59] Training loss: 0.03722438, Validation loss: 0.12414041, Gradient norm: 1.04880083
INFO:root:[   60] Training loss: 0.03832836, Validation loss: 0.10340649, Gradient norm: 1.15886641
INFO:root:[   61] Training loss: 0.03701689, Validation loss: 0.13963254, Gradient norm: 1.16660305
INFO:root:[   62] Training loss: 0.03601449, Validation loss: 0.14715722, Gradient norm: 1.05292547
INFO:root:[   63] Training loss: 0.03681773, Validation loss: 0.10756063, Gradient norm: 1.18752138
INFO:root:[   64] Training loss: 0.03638106, Validation loss: 0.13163942, Gradient norm: 1.11952458
INFO:root:[   65] Training loss: 0.03586077, Validation loss: 0.08004004, Gradient norm: 1.07728095
INFO:root:[   66] Training loss: 0.03605707, Validation loss: 0.13162716, Gradient norm: 1.09826850
INFO:root:[   67] Training loss: 0.03571073, Validation loss: 0.07293973, Gradient norm: 1.10779108
INFO:root:[   68] Training loss: 0.03517804, Validation loss: 0.08021677, Gradient norm: 1.02289127
INFO:root:[   69] Training loss: 0.03558739, Validation loss: 0.11839868, Gradient norm: 1.15384118
INFO:root:[   70] Training loss: 0.03484116, Validation loss: 0.07420529, Gradient norm: 1.09921335
INFO:root:[   71] Training loss: 0.03525316, Validation loss: 0.09132868, Gradient norm: 1.05391732
INFO:root:[   72] Training loss: 0.03551607, Validation loss: 0.09031478, Gradient norm: 1.08079660
INFO:root:[   73] Training loss: 0.03566123, Validation loss: 0.10005952, Gradient norm: 1.09682747
INFO:root:[   74] Training loss: 0.03497244, Validation loss: 0.07973303, Gradient norm: 1.08337464
INFO:root:[   75] Training loss: 0.03433310, Validation loss: 0.06872270, Gradient norm: 1.06314542
INFO:root:[   76] Training loss: 0.03462317, Validation loss: 0.10404901, Gradient norm: 1.02469249
INFO:root:[   77] Training loss: 0.03424230, Validation loss: 0.08403454, Gradient norm: 1.00619267
INFO:root:[   78] Training loss: 0.03516816, Validation loss: 0.07531977, Gradient norm: 1.09996844
INFO:root:[   79] Training loss: 0.03456343, Validation loss: 0.08065845, Gradient norm: 1.06200585
INFO:root:[   80] Training loss: 0.03523039, Validation loss: 0.09243847, Gradient norm: 1.05839895
INFO:root:[   81] Training loss: 0.03566920, Validation loss: 0.10152061, Gradient norm: 1.16854063
INFO:root:[   82] Training loss: 0.03559796, Validation loss: 0.07825768, Gradient norm: 1.10358073
INFO:root:[   83] Training loss: 0.03512638, Validation loss: 0.08279963, Gradient norm: 1.17042363
INFO:root:[   84] Training loss: 0.03417911, Validation loss: 0.08379612, Gradient norm: 1.08431063
INFO:root:EP 84: Early stopping
INFO:root:Training the model took 1248.304s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.08487
INFO:root:EnergyScoreTrain: 0.07054
INFO:root:CoverageTrain: 0.36399
INFO:root:IntervalWidthTrain: 0.02055
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0845
INFO:root:EnergyScoreValidation: 0.07036
INFO:root:CoverageValidation: 0.3566
INFO:root:IntervalWidthValidation: 0.02037
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.09355
INFO:root:EnergyScoreTest: 0.07941
INFO:root:CoverageTest: 0.29711
INFO:root:IntervalWidthTest: 0.02026
INFO:root:###13 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.85107782, Validation loss: 0.13138398, Gradient norm: 5.02671076
INFO:root:[    2] Training loss: 0.14907858, Validation loss: 0.11164517, Gradient norm: 2.26208379
INFO:root:[    3] Training loss: 0.13270826, Validation loss: 0.14777625, Gradient norm: 2.54599662
INFO:root:[    4] Training loss: 0.10549032, Validation loss: 0.11398161, Gradient norm: 2.21290087
INFO:root:[    5] Training loss: 0.09760909, Validation loss: 0.05831561, Gradient norm: 2.89234205
INFO:root:[    6] Training loss: 0.08814038, Validation loss: 0.06631560, Gradient norm: 2.65499610
INFO:root:[    7] Training loss: 0.07923866, Validation loss: 0.05583931, Gradient norm: 1.90297468
INFO:root:[    8] Training loss: 0.07933127, Validation loss: 0.05127813, Gradient norm: 2.13961907
INFO:root:[    9] Training loss: 0.07458160, Validation loss: 0.06362380, Gradient norm: 2.19348658
INFO:root:[   10] Training loss: 0.07356138, Validation loss: 0.05855580, Gradient norm: 2.51146201
INFO:root:[   11] Training loss: 0.07053369, Validation loss: 0.05104552, Gradient norm: 1.89421621
INFO:root:[   12] Training loss: 0.06929391, Validation loss: 0.04330207, Gradient norm: 2.09267924
INFO:root:[   13] Training loss: 0.06845842, Validation loss: 0.04300088, Gradient norm: 2.19690294
INFO:root:[   14] Training loss: 0.06534563, Validation loss: 0.03897590, Gradient norm: 2.00675041
INFO:root:[   15] Training loss: 0.06558145, Validation loss: 0.03733917, Gradient norm: 2.49978140
INFO:root:[   16] Training loss: 0.06358323, Validation loss: 0.03963461, Gradient norm: 2.45143825
INFO:root:[   17] Training loss: 0.06125611, Validation loss: 0.03327079, Gradient norm: 2.11638775
INFO:root:[   18] Training loss: 0.05903625, Validation loss: 0.03255322, Gradient norm: 2.03547218
INFO:root:[   19] Training loss: 0.05883465, Validation loss: 0.03063656, Gradient norm: 2.37015091
INFO:root:[   20] Training loss: 0.05523213, Validation loss: 0.04375542, Gradient norm: 2.28329650
INFO:root:[   21] Training loss: 0.05622073, Validation loss: 0.04577090, Gradient norm: 2.11402967
INFO:root:[   22] Training loss: 0.05581217, Validation loss: 0.04578049, Gradient norm: 2.07637647
INFO:root:[   23] Training loss: 0.05715792, Validation loss: 0.05838285, Gradient norm: 2.42309143
INFO:root:[   24] Training loss: 0.05193880, Validation loss: 0.02531476, Gradient norm: 2.46483993
INFO:root:[   25] Training loss: 0.05122275, Validation loss: 0.03072647, Gradient norm: 2.47666149
INFO:root:[   26] Training loss: 0.05102429, Validation loss: 0.04063266, Gradient norm: 2.41698215
INFO:root:[   27] Training loss: 0.05134030, Validation loss: 0.04707024, Gradient norm: 2.43513631
INFO:root:[   28] Training loss: 0.05095663, Validation loss: 0.02279308, Gradient norm: 2.38487942
INFO:root:[   29] Training loss: 0.04959879, Validation loss: 0.02163035, Gradient norm: 2.52967636
INFO:root:[   30] Training loss: 0.04874629, Validation loss: 0.02339423, Gradient norm: 2.39418660
INFO:root:[   31] Training loss: 0.04797783, Validation loss: 0.04039181, Gradient norm: 2.45393348
INFO:root:[   32] Training loss: 0.04764718, Validation loss: 0.04398775, Gradient norm: 2.54178866
INFO:root:[   33] Training loss: 0.04727199, Validation loss: 0.02369024, Gradient norm: 2.62497851
INFO:root:[   34] Training loss: 0.04803042, Validation loss: 0.01972624, Gradient norm: 2.43030049
INFO:root:[   35] Training loss: 0.04650914, Validation loss: 0.04211310, Gradient norm: 2.72827160
INFO:root:[   36] Training loss: 0.04646510, Validation loss: 0.04696821, Gradient norm: 2.55518999
INFO:root:[   37] Training loss: 0.04643098, Validation loss: 0.02459967, Gradient norm: 2.50239436
INFO:root:[   38] Training loss: 0.04667581, Validation loss: 0.03478655, Gradient norm: 2.46109572
INFO:root:[   39] Training loss: 0.04673950, Validation loss: 0.03839598, Gradient norm: 2.41990469
INFO:root:[   40] Training loss: 0.04470106, Validation loss: 0.01752364, Gradient norm: 2.73524615
INFO:root:[   41] Training loss: 0.04429034, Validation loss: 0.01644990, Gradient norm: 2.80398632
INFO:root:[   42] Training loss: 0.04357968, Validation loss: 0.04051682, Gradient norm: 2.68997110
INFO:root:[   43] Training loss: 0.04337908, Validation loss: 0.04288730, Gradient norm: 2.52958695
INFO:root:[   44] Training loss: 0.04412504, Validation loss: 0.02965166, Gradient norm: 2.52750644
INFO:root:[   45] Training loss: 0.04355412, Validation loss: 0.01980550, Gradient norm: 2.42568304
INFO:root:[   46] Training loss: 0.04294233, Validation loss: 0.03975600, Gradient norm: 2.75351319
INFO:root:[   47] Training loss: 0.04269662, Validation loss: 0.04953580, Gradient norm: 2.68974590
INFO:root:[   48] Training loss: 0.04412386, Validation loss: 0.03847443, Gradient norm: 2.40866774
INFO:root:[   49] Training loss: 0.04288297, Validation loss: 0.04265175, Gradient norm: 2.44886320
INFO:root:[   50] Training loss: 0.04439512, Validation loss: 0.02800658, Gradient norm: 2.08432325
INFO:root:[   51] Training loss: 0.04296967, Validation loss: 0.01795017, Gradient norm: 2.57212230
INFO:root:[   52] Training loss: 0.04198324, Validation loss: 0.04010811, Gradient norm: 2.91192674
INFO:root:[   53] Training loss: 0.04125773, Validation loss: 0.04062509, Gradient norm: 2.80610036
INFO:root:[   54] Training loss: 0.04111142, Validation loss: 0.01417511, Gradient norm: 2.82052460
INFO:root:[   55] Training loss: 0.04122137, Validation loss: 0.01527592, Gradient norm: 2.75258334
INFO:root:[   56] Training loss: 0.04111043, Validation loss: 0.04543225, Gradient norm: 2.56698777
INFO:root:[   57] Training loss: 0.04071885, Validation loss: 0.04465324, Gradient norm: 2.82739579
INFO:root:[   58] Training loss: 0.04021531, Validation loss: 0.01658504, Gradient norm: 2.72273913
INFO:root:[   59] Training loss: 0.03998599, Validation loss: 0.01529952, Gradient norm: 2.62959036
INFO:root:[   60] Training loss: 0.04002258, Validation loss: 0.03081111, Gradient norm: 2.78011748
INFO:root:[   61] Training loss: 0.04228586, Validation loss: 0.04027122, Gradient norm: 2.03705428
INFO:root:[   62] Training loss: 0.04123479, Validation loss: 0.02234191, Gradient norm: 2.45083146
INFO:root:[   63] Training loss: 0.04034138, Validation loss: 0.02646223, Gradient norm: 2.46416307
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 807.564s.
INFO:root:Emptying the cuda cache took 0.034s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04981
INFO:root:EnergyScoreTrain: 0.02472
INFO:root:CoverageTrain: 0.72434
INFO:root:IntervalWidthTrain: 0.04393
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05459
INFO:root:EnergyScoreValidation: 0.02503
INFO:root:CoverageValidation: 0.74516
INFO:root:IntervalWidthValidation: 0.04825
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.06584
INFO:root:EnergyScoreTest: 0.03433
INFO:root:CoverageTest: 0.52888
INFO:root:IntervalWidthTest: 0.0393
INFO:root:###14 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1855979520
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.82374385, Validation loss: 0.12754307, Gradient norm: 4.86794099
INFO:root:[    2] Training loss: 0.21240499, Validation loss: 0.16201629, Gradient norm: 1.95836370
INFO:root:[    3] Training loss: 0.16737462, Validation loss: 0.17658151, Gradient norm: 1.96210645
INFO:root:[    4] Training loss: 0.14115759, Validation loss: 0.12307596, Gradient norm: 1.82128454
INFO:root:[    5] Training loss: 0.12686634, Validation loss: 0.16716465, Gradient norm: 1.10788742
INFO:root:[    6] Training loss: 0.11779404, Validation loss: 0.17556025, Gradient norm: 1.57452660
INFO:root:[    7] Training loss: 0.11543753, Validation loss: 0.11290599, Gradient norm: 2.03704441
INFO:root:[    8] Training loss: 0.11178798, Validation loss: 0.07652048, Gradient norm: 2.11700715
INFO:root:[    9] Training loss: 0.10986634, Validation loss: 0.11461426, Gradient norm: 2.26712208
INFO:root:[   10] Training loss: 0.10649244, Validation loss: 0.12648173, Gradient norm: 2.09689225
INFO:root:[   11] Training loss: 0.10252125, Validation loss: 0.12499933, Gradient norm: 1.83549310
INFO:root:[   12] Training loss: 0.10308875, Validation loss: 0.10874499, Gradient norm: 2.07830369
INFO:root:[   13] Training loss: 0.09996577, Validation loss: 0.16133944, Gradient norm: 2.08846867
INFO:root:[   14] Training loss: 0.09739822, Validation loss: 0.13759980, Gradient norm: 1.97856878
INFO:root:[   15] Training loss: 0.09756815, Validation loss: 0.09555751, Gradient norm: 2.20386606
INFO:root:[   16] Training loss: 0.09470522, Validation loss: 0.15380495, Gradient norm: 2.20760715
INFO:root:[   17] Training loss: 0.09293304, Validation loss: 0.11449768, Gradient norm: 2.14303203
INFO:root:[   18] Training loss: 0.09295017, Validation loss: 0.11573687, Gradient norm: 2.16189438
INFO:root:[   19] Training loss: 0.09143024, Validation loss: 0.17548503, Gradient norm: 2.11882207
INFO:root:[   20] Training loss: 0.08892218, Validation loss: 0.14087099, Gradient norm: 1.95470401
INFO:root:[   21] Training loss: 0.08785367, Validation loss: 0.10405219, Gradient norm: 2.07725077
INFO:root:[   22] Training loss: 0.08745904, Validation loss: 0.12618192, Gradient norm: 2.29256633
INFO:root:[   23] Training loss: 0.08719528, Validation loss: 0.18168485, Gradient norm: 2.34484171
INFO:root:[   24] Training loss: 0.08648174, Validation loss: 0.12411362, Gradient norm: 2.19417400
INFO:root:[   25] Training loss: 0.08395371, Validation loss: 0.14435024, Gradient norm: 1.88948323
INFO:root:[   26] Training loss: 0.08390201, Validation loss: 0.09344956, Gradient norm: 2.01665442
INFO:root:[   27] Training loss: 0.08257038, Validation loss: 0.14306582, Gradient norm: 2.20629589
INFO:root:[   28] Training loss: 0.08124320, Validation loss: 0.14766757, Gradient norm: 2.20328470
INFO:root:[   29] Training loss: 0.08009767, Validation loss: 0.09532620, Gradient norm: 2.10333641
INFO:root:[   30] Training loss: 0.08021194, Validation loss: 0.08702172, Gradient norm: 2.04098259
INFO:root:[   31] Training loss: 0.08047080, Validation loss: 0.11752581, Gradient norm: 2.11824491
INFO:root:[   32] Training loss: 0.07957512, Validation loss: 0.16684426, Gradient norm: 2.17204364
INFO:root:[   33] Training loss: 0.07794217, Validation loss: 0.12177397, Gradient norm: 2.22788575
INFO:root:[   34] Training loss: 0.07735613, Validation loss: 0.08998653, Gradient norm: 2.27374374
INFO:root:[   35] Training loss: 0.07902940, Validation loss: 0.15522283, Gradient norm: 2.41193821
INFO:root:[   36] Training loss: 0.07576101, Validation loss: 0.12247747, Gradient norm: 2.13360805
INFO:root:[   37] Training loss: 0.07534145, Validation loss: 0.14608983, Gradient norm: 1.93975572
INFO:root:[   38] Training loss: 0.07601293, Validation loss: 0.09543273, Gradient norm: 2.35944754
INFO:root:[   39] Training loss: 0.07434748, Validation loss: 0.14861064, Gradient norm: 2.31767739
INFO:root:[   40] Training loss: 0.07295360, Validation loss: 0.15853797, Gradient norm: 2.16106179
INFO:root:[   41] Training loss: 0.07301464, Validation loss: 0.14830819, Gradient norm: 2.10852116
INFO:root:[   42] Training loss: 0.07344775, Validation loss: 0.15789646, Gradient norm: 2.30225206
INFO:root:[   43] Training loss: 0.07317500, Validation loss: 0.11146480, Gradient norm: 2.39522772
INFO:root:[   44] Training loss: 0.07316780, Validation loss: 0.09009123, Gradient norm: 2.26061261
INFO:root:[   45] Training loss: 0.07202379, Validation loss: 0.11405555, Gradient norm: 2.20021466
INFO:root:[   46] Training loss: 0.07047419, Validation loss: 0.11573724, Gradient norm: 2.17667783
INFO:root:[   47] Training loss: 0.07022558, Validation loss: 0.13671057, Gradient norm: 2.34149240
INFO:root:[   48] Training loss: 0.07027843, Validation loss: 0.13708728, Gradient norm: 2.51935207
INFO:root:[   49] Training loss: 0.06901271, Validation loss: 0.10669515, Gradient norm: 2.28245984
INFO:root:[   50] Training loss: 0.06804725, Validation loss: 0.11239431, Gradient norm: 2.14716526
INFO:root:[   51] Training loss: 0.06722011, Validation loss: 0.14884779, Gradient norm: 2.11821326
INFO:root:[   52] Training loss: 0.06768463, Validation loss: 0.15170333, Gradient norm: 2.23950483
INFO:root:[   53] Training loss: 0.06670213, Validation loss: 0.11677722, Gradient norm: 2.22377134
INFO:root:[   54] Training loss: 0.06763530, Validation loss: 0.10311217, Gradient norm: 2.38885660
INFO:root:[   55] Training loss: 0.06650244, Validation loss: 0.10813208, Gradient norm: 2.37471672
INFO:root:[   56] Training loss: 0.06537772, Validation loss: 0.09107092, Gradient norm: 2.26684511
INFO:root:[   57] Training loss: 0.06459249, Validation loss: 0.11790648, Gradient norm: 2.23711837
INFO:root:[   58] Training loss: 0.06518262, Validation loss: 0.12643299, Gradient norm: 2.42927739
INFO:root:[   59] Training loss: 0.06315688, Validation loss: 0.11074463, Gradient norm: 2.11403524
INFO:root:[   60] Training loss: 0.06448249, Validation loss: 0.07560821, Gradient norm: 2.30195574
INFO:root:[   61] Training loss: 0.06437707, Validation loss: 0.08189225, Gradient norm: 2.48236491
INFO:root:[   62] Training loss: 0.06270493, Validation loss: 0.11397301, Gradient norm: 2.37886344
INFO:root:[   63] Training loss: 0.06154002, Validation loss: 0.13704418, Gradient norm: 2.20419575
INFO:root:[   64] Training loss: 0.06232591, Validation loss: 0.09191404, Gradient norm: 2.32594220
INFO:root:[   65] Training loss: 0.06290814, Validation loss: 0.08129802, Gradient norm: 2.37132733
INFO:root:[   66] Training loss: 0.06166586, Validation loss: 0.09097138, Gradient norm: 2.28180043
INFO:root:[   67] Training loss: 0.06044515, Validation loss: 0.13141134, Gradient norm: 2.22599607
INFO:root:[   68] Training loss: 0.05937885, Validation loss: 0.08705239, Gradient norm: 2.20496315
INFO:root:[   69] Training loss: 0.05776396, Validation loss: 0.08000668, Gradient norm: 2.09381369
INFO:root:EP 69: Early stopping
INFO:root:Training the model took 871.315s.
INFO:root:Emptying the cuda cache took 0.038s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.11672
INFO:root:EnergyScoreTrain: 0.05953
INFO:root:CoverageTrain: 0.6417
INFO:root:IntervalWidthTrain: 0.09454
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.10969
INFO:root:EnergyScoreValidation: 0.06008
INFO:root:CoverageValidation: 0.64896
INFO:root:IntervalWidthValidation: 0.09911
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.09602
INFO:root:EnergyScoreTest: 0.04975
INFO:root:CoverageTest: 0.7766
INFO:root:IntervalWidthTest: 0.08397
INFO:root:###15 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1801453568
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.93205569, Validation loss: 0.12413376, Gradient norm: 4.96306471
INFO:root:[    2] Training loss: 0.24921805, Validation loss: 0.28137906, Gradient norm: 1.60602446
INFO:root:[    3] Training loss: 0.19753444, Validation loss: 0.19504891, Gradient norm: 1.93954709
INFO:root:[    4] Training loss: 0.17409668, Validation loss: 0.25134995, Gradient norm: 2.13489180
INFO:root:[    5] Training loss: 0.15814278, Validation loss: 0.31672309, Gradient norm: 1.94475604
INFO:root:[    6] Training loss: 0.14957634, Validation loss: 0.27654196, Gradient norm: 1.85061516
INFO:root:[    7] Training loss: 0.14152991, Validation loss: 0.23575302, Gradient norm: 1.50364874
INFO:root:[    8] Training loss: 0.13881701, Validation loss: 0.28063927, Gradient norm: 1.87863309
INFO:root:[    9] Training loss: 0.13260863, Validation loss: 0.21718683, Gradient norm: 1.52436847
INFO:root:[   10] Training loss: 0.13057201, Validation loss: 0.22937220, Gradient norm: 1.91833028
INFO:root:[   11] Training loss: 0.12795813, Validation loss: 0.25980313, Gradient norm: 1.98655782
INFO:root:[   12] Training loss: 0.12575880, Validation loss: 0.22532617, Gradient norm: 1.99656735
INFO:root:[   13] Training loss: 0.12386975, Validation loss: 0.27289889, Gradient norm: 1.98641713
INFO:root:[   14] Training loss: 0.12104987, Validation loss: 0.30608198, Gradient norm: 1.96145615
INFO:root:[   15] Training loss: 0.12044195, Validation loss: 0.33352706, Gradient norm: 2.14020467
INFO:root:[   16] Training loss: 0.11749502, Validation loss: 0.31764056, Gradient norm: 2.20701030
INFO:root:[   17] Training loss: 0.11500708, Validation loss: 0.29499773, Gradient norm: 2.06025891
INFO:root:[   18] Training loss: 0.11437408, Validation loss: 0.28153583, Gradient norm: 2.13512935
INFO:root:[   19] Training loss: 0.11038446, Validation loss: 0.23001469, Gradient norm: 1.96802482
INFO:root:[   20] Training loss: 0.10902529, Validation loss: 0.29505581, Gradient norm: 2.11078712
INFO:root:[   21] Training loss: 0.10571790, Validation loss: 0.26533352, Gradient norm: 1.84826606
INFO:root:[   22] Training loss: 0.10371128, Validation loss: 0.25454015, Gradient norm: 1.88863297
INFO:root:[   23] Training loss: 0.10281935, Validation loss: 0.25183390, Gradient norm: 2.03136914
INFO:root:[   24] Training loss: 0.10202526, Validation loss: 0.21446439, Gradient norm: 2.29755437
INFO:root:[   25] Training loss: 0.10087717, Validation loss: 0.29175005, Gradient norm: 2.34721961
INFO:root:[   26] Training loss: 0.09771209, Validation loss: 0.28744359, Gradient norm: 2.08210780
INFO:root:[   27] Training loss: 0.09725676, Validation loss: 0.21035830, Gradient norm: 2.26507118
INFO:root:[   28] Training loss: 0.09628895, Validation loss: 0.29079204, Gradient norm: 2.30101242
INFO:root:[   29] Training loss: 0.09556774, Validation loss: 0.25599520, Gradient norm: 2.39802234
INFO:root:[   30] Training loss: 0.09369571, Validation loss: 0.20741316, Gradient norm: 2.29646133
INFO:root:[   31] Training loss: 0.09244206, Validation loss: 0.27206241, Gradient norm: 2.25413393
INFO:root:[   32] Training loss: 0.08924912, Validation loss: 0.23238856, Gradient norm: 1.98357171
INFO:root:[   33] Training loss: 0.08917096, Validation loss: 0.23054223, Gradient norm: 2.16305064
INFO:root:[   34] Training loss: 0.08776231, Validation loss: 0.21343929, Gradient norm: 2.15637732
INFO:root:[   35] Training loss: 0.08797214, Validation loss: 0.27807992, Gradient norm: 2.45247103
INFO:root:[   36] Training loss: 0.08715327, Validation loss: 0.22802629, Gradient norm: 2.40600144
INFO:root:[   37] Training loss: 0.08578421, Validation loss: 0.19183061, Gradient norm: 2.22906305
INFO:root:[   38] Training loss: 0.08386708, Validation loss: 0.21806800, Gradient norm: 2.25931941
INFO:root:[   39] Training loss: 0.08378203, Validation loss: 0.26570090, Gradient norm: 2.36398588
INFO:root:[   40] Training loss: 0.08276942, Validation loss: 0.20283974, Gradient norm: 2.39871277
INFO:root:[   41] Training loss: 0.08108643, Validation loss: 0.18538297, Gradient norm: 2.23381908
INFO:root:[   42] Training loss: 0.08022128, Validation loss: 0.17762662, Gradient norm: 2.21616106
INFO:root:[   43] Training loss: 0.08015362, Validation loss: 0.24037286, Gradient norm: 2.43439837
INFO:root:[   44] Training loss: 0.07801380, Validation loss: 0.24089523, Gradient norm: 2.16229453
INFO:root:[   45] Training loss: 0.07714144, Validation loss: 0.24359004, Gradient norm: 2.19049965
INFO:root:[   46] Training loss: 0.07638021, Validation loss: 0.23058576, Gradient norm: 2.13294189
INFO:root:[   47] Training loss: 0.07508072, Validation loss: 0.22785133, Gradient norm: 2.14395136
INFO:root:[   48] Training loss: 0.07727134, Validation loss: 0.21006173, Gradient norm: 2.36545295
INFO:root:[   49] Training loss: 0.07369224, Validation loss: 0.20043163, Gradient norm: 2.24457875
INFO:root:[   50] Training loss: 0.07272488, Validation loss: 0.20606736, Gradient norm: 2.21904642
INFO:root:[   51] Training loss: 0.07237616, Validation loss: 0.18736660, Gradient norm: 2.24510570
INFO:root:[   52] Training loss: 0.07155723, Validation loss: 0.15735702, Gradient norm: 2.32813195
INFO:root:[   53] Training loss: 0.07119490, Validation loss: 0.16164863, Gradient norm: 2.26321859
INFO:root:[   54] Training loss: 0.07069512, Validation loss: 0.19935587, Gradient norm: 2.30593456
INFO:root:[   55] Training loss: 0.06877463, Validation loss: 0.19372045, Gradient norm: 2.23604376
INFO:root:[   56] Training loss: 0.06809048, Validation loss: 0.18681292, Gradient norm: 2.16830495
INFO:root:[   57] Training loss: 0.06752166, Validation loss: 0.18097531, Gradient norm: 2.16226138
INFO:root:[   58] Training loss: 0.06609323, Validation loss: 0.14441061, Gradient norm: 2.04419256
INFO:root:[   59] Training loss: 0.06582725, Validation loss: 0.14609558, Gradient norm: 2.15435217
INFO:root:[   60] Training loss: 0.06587566, Validation loss: 0.16621861, Gradient norm: 2.37306699
INFO:root:[   61] Training loss: 0.06379139, Validation loss: 0.14655787, Gradient norm: 2.07427657
INFO:root:[   62] Training loss: 0.06471889, Validation loss: 0.16489783, Gradient norm: 2.31015705
INFO:root:[   63] Training loss: 0.06354103, Validation loss: 0.18442819, Gradient norm: 2.18129380
INFO:root:[   64] Training loss: 0.06263045, Validation loss: 0.19316250, Gradient norm: 2.20418049
INFO:root:[   65] Training loss: 0.06180886, Validation loss: 0.19289479, Gradient norm: 2.26987731
INFO:root:[   66] Training loss: 0.06217835, Validation loss: 0.14999236, Gradient norm: 2.32880054
INFO:root:[   67] Training loss: 0.06054155, Validation loss: 0.13444630, Gradient norm: 2.20489600
INFO:root:[   68] Training loss: 0.05923588, Validation loss: 0.12834479, Gradient norm: 2.18957813
INFO:root:[   69] Training loss: 0.05922134, Validation loss: 0.16959547, Gradient norm: 2.30156903
INFO:root:[   70] Training loss: 0.05834970, Validation loss: 0.18900030, Gradient norm: 2.21977033
INFO:root:[   71] Training loss: 0.05838826, Validation loss: 0.11995471, Gradient norm: 2.34400905
INFO:root:[   72] Training loss: 0.05746274, Validation loss: 0.16263947, Gradient norm: 2.31738785
INFO:root:[   73] Training loss: 0.05613253, Validation loss: 0.17508206, Gradient norm: 2.18741442
INFO:root:[   74] Training loss: 0.05635187, Validation loss: 0.13627537, Gradient norm: 2.17418214
INFO:root:[   75] Training loss: 0.05569826, Validation loss: 0.11599236, Gradient norm: 2.20537517
INFO:root:[   76] Training loss: 0.05454100, Validation loss: 0.15355730, Gradient norm: 2.06451629
INFO:root:[   77] Training loss: 0.05383797, Validation loss: 0.16834089, Gradient norm: 2.16453465
INFO:root:[   78] Training loss: 0.05389085, Validation loss: 0.10819482, Gradient norm: 2.29063619
INFO:root:[   79] Training loss: 0.05319001, Validation loss: 0.16600982, Gradient norm: 2.18860946
INFO:root:[   80] Training loss: 0.05271756, Validation loss: 0.11399683, Gradient norm: 2.12633967
INFO:root:[   81] Training loss: 0.05219773, Validation loss: 0.12069048, Gradient norm: 2.12771427
INFO:root:[   82] Training loss: 0.05046669, Validation loss: 0.15607598, Gradient norm: 2.09829246
INFO:root:[   83] Training loss: 0.04949907, Validation loss: 0.15428467, Gradient norm: 2.04871248
INFO:root:[   84] Training loss: 0.05034843, Validation loss: 0.11508833, Gradient norm: 2.17698340
INFO:root:[   85] Training loss: 0.04921767, Validation loss: 0.10668035, Gradient norm: 2.22830851
INFO:root:[   86] Training loss: 0.04904633, Validation loss: 0.14655985, Gradient norm: 2.21365307
INFO:root:[   87] Training loss: 0.04798954, Validation loss: 0.11307245, Gradient norm: 2.01149824
INFO:root:[   88] Training loss: 0.04749703, Validation loss: 0.08896384, Gradient norm: 2.03660703
INFO:root:[   89] Training loss: 0.04765684, Validation loss: 0.14607284, Gradient norm: 2.21107946
INFO:root:[   90] Training loss: 0.04652402, Validation loss: 0.08826629, Gradient norm: 2.13989118
INFO:root:[   91] Training loss: 0.04668696, Validation loss: 0.14099591, Gradient norm: 2.22521721
INFO:root:[   92] Training loss: 0.04542948, Validation loss: 0.08471643, Gradient norm: 2.12299758
INFO:root:[   93] Training loss: 0.04518379, Validation loss: 0.13638792, Gradient norm: 2.15368265
INFO:root:[   94] Training loss: 0.04513949, Validation loss: 0.09111176, Gradient norm: 2.06829725
INFO:root:[   95] Training loss: 0.04428834, Validation loss: 0.13225880, Gradient norm: 2.12048531
INFO:root:[   96] Training loss: 0.04319952, Validation loss: 0.10436293, Gradient norm: 1.94263543
INFO:root:[   97] Training loss: 0.04421792, Validation loss: 0.10292073, Gradient norm: 2.01094382
INFO:root:[   98] Training loss: 0.04256045, Validation loss: 0.10757431, Gradient norm: 2.09308749
INFO:root:[   99] Training loss: 0.04133425, Validation loss: 0.07413442, Gradient norm: 2.00504603
INFO:root:[  100] Training loss: 0.04242435, Validation loss: 0.12815421, Gradient norm: 2.05966342
INFO:root:[  101] Training loss: 0.04227186, Validation loss: 0.09753167, Gradient norm: 2.17518002
INFO:root:[  102] Training loss: 0.04062243, Validation loss: 0.09415864, Gradient norm: 2.07864925
INFO:root:[  103] Training loss: 0.03991144, Validation loss: 0.08324539, Gradient norm: 1.96402347
INFO:root:[  104] Training loss: 0.04044534, Validation loss: 0.10727662, Gradient norm: 1.97077766
INFO:root:[  105] Training loss: 0.03900627, Validation loss: 0.07490232, Gradient norm: 1.96713130
INFO:root:[  106] Training loss: 0.03853592, Validation loss: 0.08130476, Gradient norm: 1.87934622
INFO:root:[  107] Training loss: 0.03898173, Validation loss: 0.07312144, Gradient norm: 2.12655998
INFO:root:[  108] Training loss: 0.03775472, Validation loss: 0.08691001, Gradient norm: 2.04775977
INFO:root:[  109] Training loss: 0.03804803, Validation loss: 0.06296634, Gradient norm: 2.10335975
INFO:root:[  110] Training loss: 0.03769977, Validation loss: 0.11344509, Gradient norm: 2.01604134
INFO:root:[  111] Training loss: 0.03720602, Validation loss: 0.06742070, Gradient norm: 1.96887329
INFO:root:[  112] Training loss: 0.03659605, Validation loss: 0.10206391, Gradient norm: 1.93683787
INFO:root:[  113] Training loss: 0.03653163, Validation loss: 0.08452411, Gradient norm: 2.10387399
INFO:root:[  114] Training loss: 0.03603571, Validation loss: 0.05665756, Gradient norm: 2.06048974
INFO:root:[  115] Training loss: 0.03620780, Validation loss: 0.05329630, Gradient norm: 2.16756997
INFO:root:[  116] Training loss: 0.03547353, Validation loss: 0.05437076, Gradient norm: 2.09192190
INFO:root:[  117] Training loss: 0.03535479, Validation loss: 0.05373204, Gradient norm: 2.11627172
INFO:root:[  118] Training loss: 0.03448147, Validation loss: 0.06932427, Gradient norm: 1.96094078
INFO:root:[  119] Training loss: 0.03427132, Validation loss: 0.06789809, Gradient norm: 2.04160413
INFO:root:[  120] Training loss: 0.03431257, Validation loss: 0.06989586, Gradient norm: 2.06297357
INFO:root:[  121] Training loss: 0.03434008, Validation loss: 0.06032789, Gradient norm: 2.00157051
INFO:root:[  122] Training loss: 0.03364675, Validation loss: 0.06853915, Gradient norm: 1.95457530
INFO:root:[  123] Training loss: 0.03377537, Validation loss: 0.05943138, Gradient norm: 2.02461468
INFO:root:[  124] Training loss: 0.03293948, Validation loss: 0.06474251, Gradient norm: 2.00826119
INFO:root:EP 124: Early stopping
INFO:root:Training the model took 1579.584s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.10383
INFO:root:EnergyScoreTrain: 0.05267
INFO:root:CoverageTrain: 0.65993
INFO:root:IntervalWidthTrain: 0.09184
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.11601
INFO:root:EnergyScoreValidation: 0.07404
INFO:root:CoverageValidation: 0.57345
INFO:root:IntervalWidthValidation: 0.08502
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07529
INFO:root:EnergyScoreTest: 0.04094
INFO:root:CoverageTest: 0.75729
INFO:root:IntervalWidthTest: 0.089
INFO:root:###16 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1801453568
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.93544796, Validation loss: 0.12614694, Gradient norm: 3.72682670
INFO:root:[    2] Training loss: 0.25568323, Validation loss: 0.16343738, Gradient norm: 1.52323977
INFO:root:[    3] Training loss: 0.21658993, Validation loss: 0.11437460, Gradient norm: 1.66408122
INFO:root:[    4] Training loss: 0.19343984, Validation loss: 0.23668977, Gradient norm: 1.61697643
INFO:root:[    5] Training loss: 0.17719864, Validation loss: 0.26570644, Gradient norm: 1.22676163
INFO:root:[    6] Training loss: 0.17180513, Validation loss: 0.20179546, Gradient norm: 1.72855509
INFO:root:[    7] Training loss: 0.16318993, Validation loss: 0.28805349, Gradient norm: 1.57987633
INFO:root:[    8] Training loss: 0.15849737, Validation loss: 0.30005032, Gradient norm: 1.66591330
INFO:root:[    9] Training loss: 0.15272589, Validation loss: 0.32392593, Gradient norm: 1.36352180
INFO:root:[   10] Training loss: 0.15060806, Validation loss: 0.27481668, Gradient norm: 1.70501093
INFO:root:[   11] Training loss: 0.14826397, Validation loss: 0.25877903, Gradient norm: 1.72125726
INFO:root:[   12] Training loss: 0.14625689, Validation loss: 0.29615957, Gradient norm: 1.88849446
INFO:root:[   13] Training loss: 0.14392378, Validation loss: 0.31338507, Gradient norm: 1.89114530
INFO:root:[   14] Training loss: 0.13959667, Validation loss: 0.27190794, Gradient norm: 1.61722949
INFO:root:[   15] Training loss: 0.13723239, Validation loss: 0.20896973, Gradient norm: 1.66800292
INFO:root:[   16] Training loss: 0.13771030, Validation loss: 0.26687960, Gradient norm: 1.94799567
INFO:root:[   17] Training loss: 0.13319408, Validation loss: 0.27693917, Gradient norm: 1.70159436
INFO:root:[   18] Training loss: 0.13023813, Validation loss: 0.30918724, Gradient norm: 1.60086414
INFO:root:[   19] Training loss: 0.12911913, Validation loss: 0.24453484, Gradient norm: 1.70962124
INFO:root:[   20] Training loss: 0.12674082, Validation loss: 0.22058723, Gradient norm: 1.67154233
INFO:root:[   21] Training loss: 0.12546185, Validation loss: 0.23942896, Gradient norm: 1.69563187
INFO:root:[   22] Training loss: 0.12373813, Validation loss: 0.31099744, Gradient norm: 1.81054960
INFO:root:[   23] Training loss: 0.12118915, Validation loss: 0.24898700, Gradient norm: 1.76875982
INFO:root:[   24] Training loss: 0.11857398, Validation loss: 0.22633135, Gradient norm: 1.67061625
INFO:root:[   25] Training loss: 0.11780494, Validation loss: 0.20596855, Gradient norm: 1.74225548
INFO:root:[   26] Training loss: 0.11453594, Validation loss: 0.20599393, Gradient norm: 1.63532356
INFO:root:[   27] Training loss: 0.11345797, Validation loss: 0.20359486, Gradient norm: 1.72270588
INFO:root:[   28] Training loss: 0.11235923, Validation loss: 0.19511511, Gradient norm: 1.82512788
INFO:root:[   29] Training loss: 0.11175157, Validation loss: 0.23845002, Gradient norm: 1.94451595
INFO:root:[   30] Training loss: 0.10834311, Validation loss: 0.25965986, Gradient norm: 1.67068006
INFO:root:[   31] Training loss: 0.10693799, Validation loss: 0.28933182, Gradient norm: 1.84431563
INFO:root:[   32] Training loss: 0.10693279, Validation loss: 0.22386483, Gradient norm: 2.06127248
INFO:root:[   33] Training loss: 0.10409336, Validation loss: 0.22874813, Gradient norm: 1.75044810
INFO:root:[   34] Training loss: 0.10264000, Validation loss: 0.20598526, Gradient norm: 1.77178459
INFO:root:[   35] Training loss: 0.10119464, Validation loss: 0.23086652, Gradient norm: 1.77040786
INFO:root:[   36] Training loss: 0.09888998, Validation loss: 0.21512652, Gradient norm: 1.86353882
INFO:root:[   37] Training loss: 0.09873595, Validation loss: 0.17363573, Gradient norm: 2.01874935
INFO:root:[   38] Training loss: 0.09693003, Validation loss: 0.18462714, Gradient norm: 1.89413510
INFO:root:[   39] Training loss: 0.09503181, Validation loss: 0.18898990, Gradient norm: 1.79059330
INFO:root:[   40] Training loss: 0.09273852, Validation loss: 0.23063515, Gradient norm: 1.78777942
INFO:root:[   41] Training loss: 0.09226757, Validation loss: 0.25678935, Gradient norm: 1.97349107
INFO:root:[   42] Training loss: 0.09171648, Validation loss: 0.19621665, Gradient norm: 2.09544802
INFO:root:[   43] Training loss: 0.08968133, Validation loss: 0.18236221, Gradient norm: 1.95721868
INFO:root:[   44] Training loss: 0.08736191, Validation loss: 0.20972361, Gradient norm: 1.81619202
INFO:root:[   45] Training loss: 0.08681587, Validation loss: 0.20702979, Gradient norm: 1.97511954
INFO:root:[   46] Training loss: 0.08492803, Validation loss: 0.17835539, Gradient norm: 1.94113531
INFO:root:[   47] Training loss: 0.08342490, Validation loss: 0.18563589, Gradient norm: 1.98352426
INFO:root:[   48] Training loss: 0.08261627, Validation loss: 0.18093380, Gradient norm: 1.98190524
INFO:root:[   49] Training loss: 0.08052737, Validation loss: 0.21272964, Gradient norm: 1.80461940
INFO:root:[   50] Training loss: 0.08046392, Validation loss: 0.22561364, Gradient norm: 1.95611006
INFO:root:[   51] Training loss: 0.07935622, Validation loss: 0.21602019, Gradient norm: 2.15212686
INFO:root:[   52] Training loss: 0.07618473, Validation loss: 0.21512034, Gradient norm: 1.83499531
INFO:root:[   53] Training loss: 0.07489028, Validation loss: 0.16066744, Gradient norm: 1.85597675
INFO:root:[   54] Training loss: 0.07418340, Validation loss: 0.17024717, Gradient norm: 1.87795046
INFO:root:[   55] Training loss: 0.07432419, Validation loss: 0.18165569, Gradient norm: 2.20004909
INFO:root:[   56] Training loss: 0.07372771, Validation loss: 0.18773648, Gradient norm: 2.06816187
INFO:root:[   57] Training loss: 0.07193058, Validation loss: 0.15410256, Gradient norm: 2.05277017
INFO:root:[   58] Training loss: 0.06973391, Validation loss: 0.19283320, Gradient norm: 2.02239702
INFO:root:[   59] Training loss: 0.06877148, Validation loss: 0.16988941, Gradient norm: 1.88438739
INFO:root:[   60] Training loss: 0.06769082, Validation loss: 0.13135133, Gradient norm: 1.97938628
INFO:root:[   61] Training loss: 0.06803945, Validation loss: 0.13048658, Gradient norm: 2.19093115
INFO:root:[   62] Training loss: 0.06611616, Validation loss: 0.16893530, Gradient norm: 2.00834145
INFO:root:[   63] Training loss: 0.06374947, Validation loss: 0.12993556, Gradient norm: 1.80505332
INFO:root:[   64] Training loss: 0.06535412, Validation loss: 0.12147754, Gradient norm: 2.22667585
INFO:root:[   65] Training loss: 0.06306940, Validation loss: 0.12433549, Gradient norm: 2.03221023
INFO:root:[   66] Training loss: 0.06183237, Validation loss: 0.18467648, Gradient norm: 1.88053479
INFO:root:[   67] Training loss: 0.06198372, Validation loss: 0.16666229, Gradient norm: 2.23298488
INFO:root:[   68] Training loss: 0.05904843, Validation loss: 0.13768455, Gradient norm: 1.97825386
INFO:root:[   69] Training loss: 0.05896580, Validation loss: 0.12461982, Gradient norm: 2.05131201
INFO:root:[   70] Training loss: 0.05778740, Validation loss: 0.13363821, Gradient norm: 1.73648428
INFO:root:[   71] Training loss: 0.05751240, Validation loss: 0.14731519, Gradient norm: 1.87324059
INFO:root:[   72] Training loss: 0.05617857, Validation loss: 0.15284727, Gradient norm: 2.01953107
INFO:root:[   73] Training loss: 0.05496702, Validation loss: 0.09930374, Gradient norm: 1.92746683
INFO:root:[   74] Training loss: 0.05478188, Validation loss: 0.12843750, Gradient norm: 2.08612183
INFO:root:[   75] Training loss: 0.05363130, Validation loss: 0.13783617, Gradient norm: 2.05560721
INFO:root:[   76] Training loss: 0.05242690, Validation loss: 0.08860920, Gradient norm: 1.93456933
INFO:root:[   77] Training loss: 0.05301449, Validation loss: 0.09905806, Gradient norm: 2.24853272
INFO:root:[   78] Training loss: 0.05063520, Validation loss: 0.12837364, Gradient norm: 1.94102406
INFO:root:[   79] Training loss: 0.04958256, Validation loss: 0.10210411, Gradient norm: 1.97281227
INFO:root:[   80] Training loss: 0.04910291, Validation loss: 0.10391510, Gradient norm: 1.98301358
INFO:root:[   81] Training loss: 0.04823577, Validation loss: 0.12979034, Gradient norm: 1.97383349
INFO:root:[   82] Training loss: 0.04765575, Validation loss: 0.09972025, Gradient norm: 1.83417018
INFO:root:[   83] Training loss: 0.04632556, Validation loss: 0.08279954, Gradient norm: 1.89433080
INFO:root:[   84] Training loss: 0.04938876, Validation loss: 0.11892126, Gradient norm: 2.35310198
INFO:root:[   85] Training loss: 0.04679489, Validation loss: 0.09332390, Gradient norm: 2.13842733
INFO:root:[   86] Training loss: 0.04523083, Validation loss: 0.07251147, Gradient norm: 1.98129018
INFO:root:[   87] Training loss: 0.04478072, Validation loss: 0.08905448, Gradient norm: 2.03917585
INFO:root:[   88] Training loss: 0.04320630, Validation loss: 0.06406655, Gradient norm: 1.69961635
INFO:root:[   89] Training loss: 0.04299910, Validation loss: 0.09619378, Gradient norm: 1.96816781
INFO:root:[   90] Training loss: 0.04245985, Validation loss: 0.05190891, Gradient norm: 1.87077183
INFO:root:[   91] Training loss: 0.04211201, Validation loss: 0.07896880, Gradient norm: 1.97262594
INFO:root:[   92] Training loss: 0.04094330, Validation loss: 0.09556483, Gradient norm: 1.86884952
INFO:root:[   93] Training loss: 0.04200448, Validation loss: 0.07926599, Gradient norm: 2.11974656
INFO:root:[   94] Training loss: 0.04102960, Validation loss: 0.06524364, Gradient norm: 1.90040406
INFO:root:[   95] Training loss: 0.03992003, Validation loss: 0.07810193, Gradient norm: 1.91134004
INFO:root:[   96] Training loss: 0.04001046, Validation loss: 0.06601892, Gradient norm: 2.05516114
INFO:root:[   97] Training loss: 0.03950237, Validation loss: 0.06330050, Gradient norm: 1.97346726
INFO:root:[   98] Training loss: 0.03819187, Validation loss: 0.06508961, Gradient norm: 1.80977370
INFO:root:[   99] Training loss: 0.03850899, Validation loss: 0.08326609, Gradient norm: 1.78430815
INFO:root:EP 99: Early stopping
INFO:root:Training the model took 1258.296s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.10636
INFO:root:EnergyScoreTrain: 0.05779
INFO:root:CoverageTrain: 0.74555
INFO:root:IntervalWidthTrain: 0.09946
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.08863
INFO:root:EnergyScoreValidation: 0.04928
INFO:root:CoverageValidation: 0.78788
INFO:root:IntervalWidthValidation: 0.08085
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.12994
INFO:root:EnergyScoreTest: 0.06471
INFO:root:CoverageTest: 0.5117
INFO:root:IntervalWidthTest: 0.08066
INFO:root:###17 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1929379840
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.84408269, Validation loss: 0.13773632, Gradient norm: 3.10151572
INFO:root:[    2] Training loss: 0.25146137, Validation loss: 0.23773367, Gradient norm: 1.25737764
INFO:root:[    3] Training loss: 0.22091536, Validation loss: 0.17340221, Gradient norm: 1.44310133
INFO:root:[    4] Training loss: 0.19317993, Validation loss: 0.27287287, Gradient norm: 1.04366977
INFO:root:[    5] Training loss: 0.17953097, Validation loss: 0.19742872, Gradient norm: 1.23561904
INFO:root:[    6] Training loss: 0.16933074, Validation loss: 0.20138044, Gradient norm: 1.28396492
INFO:root:[    7] Training loss: 0.16475999, Validation loss: 0.22815967, Gradient norm: 1.20118216
INFO:root:[    8] Training loss: 0.16119080, Validation loss: 0.23629743, Gradient norm: 1.52855620
INFO:root:[    9] Training loss: 0.15511862, Validation loss: 0.29962217, Gradient norm: 1.54399238
INFO:root:[   10] Training loss: 0.15343533, Validation loss: 0.20132430, Gradient norm: 1.68187168
INFO:root:[   11] Training loss: 0.14832046, Validation loss: 0.27692547, Gradient norm: 1.51039536
INFO:root:[   12] Training loss: 0.14369162, Validation loss: 0.31237176, Gradient norm: 1.43825269
INFO:root:[   13] Training loss: 0.14317440, Validation loss: 0.24755044, Gradient norm: 1.52368560
INFO:root:[   14] Training loss: 0.13981069, Validation loss: 0.20162717, Gradient norm: 1.40530069
INFO:root:[   15] Training loss: 0.13742329, Validation loss: 0.30905216, Gradient norm: 1.60109206
INFO:root:[   16] Training loss: 0.13519837, Validation loss: 0.22256879, Gradient norm: 1.66563798
INFO:root:[   17] Training loss: 0.13095086, Validation loss: 0.22856623, Gradient norm: 1.47203616
INFO:root:[   18] Training loss: 0.12982093, Validation loss: 0.27456786, Gradient norm: 1.56209272
INFO:root:[   19] Training loss: 0.12700298, Validation loss: 0.26080875, Gradient norm: 1.61172891
INFO:root:[   20] Training loss: 0.12583304, Validation loss: 0.21574000, Gradient norm: 1.70374638
INFO:root:[   21] Training loss: 0.12086568, Validation loss: 0.19659031, Gradient norm: 1.30378663
INFO:root:[   22] Training loss: 0.12046015, Validation loss: 0.24319934, Gradient norm: 1.50882252
INFO:root:[   23] Training loss: 0.11889830, Validation loss: 0.28435671, Gradient norm: 1.59275350
INFO:root:[   24] Training loss: 0.11505891, Validation loss: 0.25227048, Gradient norm: 1.38589444
INFO:root:[   25] Training loss: 0.11298650, Validation loss: 0.25428592, Gradient norm: 1.32535390
INFO:root:[   26] Training loss: 0.11178428, Validation loss: 0.26554548, Gradient norm: 1.44418991
INFO:root:[   27] Training loss: 0.10960354, Validation loss: 0.25030233, Gradient norm: 1.51762026
INFO:root:[   28] Training loss: 0.10860069, Validation loss: 0.20109990, Gradient norm: 1.56404709
INFO:root:[   29] Training loss: 0.10524661, Validation loss: 0.18922710, Gradient norm: 1.47204380
INFO:root:[   30] Training loss: 0.10318804, Validation loss: 0.19860145, Gradient norm: 1.48181862
INFO:root:[   31] Training loss: 0.10028564, Validation loss: 0.24452960, Gradient norm: 1.32041363
INFO:root:[   32] Training loss: 0.09864543, Validation loss: 0.21360085, Gradient norm: 1.49309093
INFO:root:[   33] Training loss: 0.09942239, Validation loss: 0.21284570, Gradient norm: 1.81335058
INFO:root:[   34] Training loss: 0.09626926, Validation loss: 0.22996618, Gradient norm: 1.64602780
INFO:root:[   35] Training loss: 0.09552317, Validation loss: 0.17569005, Gradient norm: 1.70675130
INFO:root:[   36] Training loss: 0.09364169, Validation loss: 0.15676913, Gradient norm: 1.85181887
INFO:root:[   37] Training loss: 0.09265182, Validation loss: 0.21821334, Gradient norm: 1.92968110
INFO:root:[   38] Training loss: 0.08903518, Validation loss: 0.15501561, Gradient norm: 1.75442344
INFO:root:[   39] Training loss: 0.08798506, Validation loss: 0.20110183, Gradient norm: 1.85787409
INFO:root:[   40] Training loss: 0.08433659, Validation loss: 0.22054746, Gradient norm: 1.56080249
INFO:root:[   41] Training loss: 0.08315339, Validation loss: 0.19807257, Gradient norm: 1.41094573
INFO:root:[   42] Training loss: 0.08110936, Validation loss: 0.16484911, Gradient norm: 1.62225054
INFO:root:[   43] Training loss: 0.07840031, Validation loss: 0.15008163, Gradient norm: 1.56542455
INFO:root:[   44] Training loss: 0.07689647, Validation loss: 0.19141741, Gradient norm: 1.59222336
INFO:root:[   45] Training loss: 0.07584488, Validation loss: 0.18339405, Gradient norm: 1.69938107
INFO:root:[   46] Training loss: 0.07614107, Validation loss: 0.16078523, Gradient norm: 1.75855644
INFO:root:[   47] Training loss: 0.07295373, Validation loss: 0.12551890, Gradient norm: 1.60441032
INFO:root:[   48] Training loss: 0.07093655, Validation loss: 0.17031094, Gradient norm: 1.71524933
INFO:root:[   49] Training loss: 0.06849363, Validation loss: 0.13011762, Gradient norm: 1.61870848
INFO:root:[   50] Training loss: 0.06880471, Validation loss: 0.14725732, Gradient norm: 1.83391988
INFO:root:[   51] Training loss: 0.06661672, Validation loss: 0.10412720, Gradient norm: 1.66014420
INFO:root:[   52] Training loss: 0.06617588, Validation loss: 0.12304084, Gradient norm: 1.85266855
INFO:root:[   53] Training loss: 0.06423815, Validation loss: 0.16101450, Gradient norm: 1.84448916
INFO:root:[   54] Training loss: 0.06171663, Validation loss: 0.15057781, Gradient norm: 1.59630181
INFO:root:[   55] Training loss: 0.06144854, Validation loss: 0.10212872, Gradient norm: 1.75982095
INFO:root:[   56] Training loss: 0.06010019, Validation loss: 0.09424553, Gradient norm: 1.77802794
INFO:root:[   57] Training loss: 0.06002042, Validation loss: 0.13886431, Gradient norm: 1.98111272
INFO:root:[   58] Training loss: 0.05772229, Validation loss: 0.08734270, Gradient norm: 1.78816881
INFO:root:[   59] Training loss: 0.05695012, Validation loss: 0.10034306, Gradient norm: 1.85635971
INFO:root:[   60] Training loss: 0.05635960, Validation loss: 0.12680567, Gradient norm: 1.84197938
INFO:root:[   61] Training loss: 0.05314807, Validation loss: 0.09955163, Gradient norm: 1.67004571
INFO:root:[   62] Training loss: 0.05419266, Validation loss: 0.10989115, Gradient norm: 1.96308861
INFO:root:[   63] Training loss: 0.05454350, Validation loss: 0.07880268, Gradient norm: 2.04390168
INFO:root:[   64] Training loss: 0.05327305, Validation loss: 0.12253458, Gradient norm: 1.87879802
INFO:root:[   65] Training loss: 0.05073367, Validation loss: 0.12061356, Gradient norm: 1.75251232
INFO:root:[   66] Training loss: 0.04922719, Validation loss: 0.08736509, Gradient norm: 1.83776047
INFO:root:[   67] Training loss: 0.04757433, Validation loss: 0.10409570, Gradient norm: 1.50339279
INFO:root:[   68] Training loss: 0.05054203, Validation loss: 0.08712072, Gradient norm: 1.97934239
INFO:root:[   69] Training loss: 0.04729436, Validation loss: 0.05425623, Gradient norm: 1.92916310
INFO:root:[   70] Training loss: 0.04662891, Validation loss: 0.04397667, Gradient norm: 1.75650661
INFO:root:[   71] Training loss: 0.04744510, Validation loss: 0.10265587, Gradient norm: 1.96118455
INFO:root:[   72] Training loss: 0.04762785, Validation loss: 0.04860919, Gradient norm: 1.94557035
INFO:root:[   73] Training loss: 0.04716359, Validation loss: 0.04207848, Gradient norm: 2.00562810
INFO:root:[   74] Training loss: 0.04902897, Validation loss: 0.09292689, Gradient norm: 1.98913953
INFO:root:[   75] Training loss: 0.04559246, Validation loss: 0.07071695, Gradient norm: 1.87886320
INFO:root:[   76] Training loss: 0.04421140, Validation loss: 0.05024946, Gradient norm: 1.86193039
INFO:root:[   77] Training loss: 0.04290116, Validation loss: 0.07168467, Gradient norm: 1.63837604
INFO:root:[   78] Training loss: 0.04159164, Validation loss: 0.04016309, Gradient norm: 1.65071645
INFO:root:[   79] Training loss: 0.04428750, Validation loss: 0.06924312, Gradient norm: 2.01911639
INFO:root:[   80] Training loss: 0.04207664, Validation loss: 0.04916805, Gradient norm: 1.55984454
INFO:root:[   81] Training loss: 0.04201544, Validation loss: 0.05347524, Gradient norm: 1.75434176
INFO:root:[   82] Training loss: 0.04237160, Validation loss: 0.06447023, Gradient norm: 1.73527230
INFO:root:[   83] Training loss: 0.04128072, Validation loss: 0.06721191, Gradient norm: 1.63688821
INFO:root:[   84] Training loss: 0.03970833, Validation loss: 0.03845592, Gradient norm: 1.61113483
INFO:root:[   85] Training loss: 0.04054435, Validation loss: 0.05659712, Gradient norm: 1.65060308
INFO:root:[   86] Training loss: 0.04030370, Validation loss: 0.05815602, Gradient norm: 1.67960735
INFO:root:[   87] Training loss: 0.03972549, Validation loss: 0.05999407, Gradient norm: 1.51373173
INFO:root:[   88] Training loss: 0.04024707, Validation loss: 0.06231416, Gradient norm: 1.64488466
INFO:root:[   89] Training loss: 0.03930462, Validation loss: 0.05009234, Gradient norm: 1.56845865
INFO:root:[   90] Training loss: 0.03884493, Validation loss: 0.04011341, Gradient norm: 1.63324349
INFO:root:[   91] Training loss: 0.03945084, Validation loss: 0.03652395, Gradient norm: 1.53162201
INFO:root:[   92] Training loss: 0.03841266, Validation loss: 0.02268043, Gradient norm: 1.39487979
INFO:root:[   93] Training loss: 0.03787071, Validation loss: 0.05613187, Gradient norm: 1.55985724
INFO:root:[   94] Training loss: 0.04020627, Validation loss: 0.03805630, Gradient norm: 1.52766288
INFO:root:[   95] Training loss: 0.03973672, Validation loss: 0.06866652, Gradient norm: 1.57240722
INFO:root:[   96] Training loss: 0.04038896, Validation loss: 0.03730992, Gradient norm: 1.44422547
INFO:root:[   97] Training loss: 0.03718390, Validation loss: 0.03266248, Gradient norm: 1.44205028
INFO:root:[   98] Training loss: 0.03659053, Validation loss: 0.03521268, Gradient norm: 1.38013063
INFO:root:[   99] Training loss: 0.03755738, Validation loss: 0.03665284, Gradient norm: 1.48052465
INFO:root:[  100] Training loss: 0.03707761, Validation loss: 0.03665672, Gradient norm: 1.31859004
INFO:root:[  101] Training loss: 0.03744427, Validation loss: 0.05967060, Gradient norm: 1.42131758
INFO:root:EP 101: Early stopping
INFO:root:Training the model took 1286.285s.
INFO:root:Emptying the cuda cache took 0.041s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.06972
INFO:root:EnergyScoreTrain: 0.03685
INFO:root:CoverageTrain: 0.70012
INFO:root:IntervalWidthTrain: 0.03747
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.05973
INFO:root:EnergyScoreValidation: 0.02908
INFO:root:CoverageValidation: 0.74503
INFO:root:IntervalWidthValidation: 0.03751
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.08039
INFO:root:EnergyScoreTest: 0.04319
INFO:root:CoverageTest: 0.71756
INFO:root:IntervalWidthTest: 0.04519
INFO:root:###18 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1929379840
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 1.09225207, Validation loss: 0.17950653, Gradient norm: 3.54104804
INFO:root:[    2] Training loss: 0.34446340, Validation loss: 0.13272638, Gradient norm: 1.31323998
INFO:root:[    3] Training loss: 0.28746856, Validation loss: 0.13585790, Gradient norm: 0.93889246
INFO:root:[    4] Training loss: 0.26351893, Validation loss: 0.18107805, Gradient norm: 0.93010891
INFO:root:[    5] Training loss: 0.24674473, Validation loss: 0.28786304, Gradient norm: 1.21475843
INFO:root:[    6] Training loss: 0.23532215, Validation loss: 0.27415320, Gradient norm: 1.38220557
INFO:root:[    7] Training loss: 0.22285409, Validation loss: 0.29146190, Gradient norm: 0.90632245
INFO:root:[    8] Training loss: 0.21574411, Validation loss: 0.25223898, Gradient norm: 1.34959362
INFO:root:[    9] Training loss: 0.20760159, Validation loss: 0.29803205, Gradient norm: 1.10502182
INFO:root:[   10] Training loss: 0.20241203, Validation loss: 0.36762665, Gradient norm: 1.28715026
INFO:root:[   11] Training loss: 0.19782505, Validation loss: 0.26055966, Gradient norm: 1.38449701
INFO:root:[   12] Training loss: 0.19086923, Validation loss: 0.26972379, Gradient norm: 1.30341261
INFO:root:[   13] Training loss: 0.18550945, Validation loss: 0.32635354, Gradient norm: 1.25095115
INFO:root:[   14] Training loss: 0.18251175, Validation loss: 0.26340323, Gradient norm: 1.58332158
INFO:root:[   15] Training loss: 0.17633512, Validation loss: 0.26505280, Gradient norm: 1.36353477
INFO:root:[   16] Training loss: 0.17137433, Validation loss: 0.33052156, Gradient norm: 1.22092994
INFO:root:[   17] Training loss: 0.16889260, Validation loss: 0.30873732, Gradient norm: 1.42477464
INFO:root:[   18] Training loss: 0.16293284, Validation loss: 0.24750000, Gradient norm: 1.25370776
INFO:root:[   19] Training loss: 0.15899922, Validation loss: 0.26730744, Gradient norm: 1.50278104
INFO:root:[   20] Training loss: 0.15529873, Validation loss: 0.27410290, Gradient norm: 1.47654601
INFO:root:[   21] Training loss: 0.14975811, Validation loss: 0.22127014, Gradient norm: 1.38557920
INFO:root:[   22] Training loss: 0.14761600, Validation loss: 0.32093106, Gradient norm: 1.61665838
INFO:root:[   23] Training loss: 0.14390266, Validation loss: 0.22872686, Gradient norm: 1.50015412
INFO:root:[   24] Training loss: 0.13749252, Validation loss: 0.22708405, Gradient norm: 1.28291290
INFO:root:[   25] Training loss: 0.13409759, Validation loss: 0.20685129, Gradient norm: 1.32052390
INFO:root:[   26] Training loss: 0.13048987, Validation loss: 0.24522368, Gradient norm: 1.26408770
INFO:root:[   27] Training loss: 0.12970078, Validation loss: 0.24270834, Gradient norm: 1.68088777
INFO:root:[   28] Training loss: 0.12716883, Validation loss: 0.26915189, Gradient norm: 1.81402786
INFO:root:[   29] Training loss: 0.12091267, Validation loss: 0.21516460, Gradient norm: 1.49261029
INFO:root:[   30] Training loss: 0.11955902, Validation loss: 0.24775367, Gradient norm: 1.81263984
INFO:root:[   31] Training loss: 0.11306933, Validation loss: 0.25113069, Gradient norm: 1.34562866
INFO:root:[   32] Training loss: 0.10996284, Validation loss: 0.19578269, Gradient norm: 1.41832073
INFO:root:[   33] Training loss: 0.10806778, Validation loss: 0.22604000, Gradient norm: 1.66868400
INFO:root:[   34] Training loss: 0.10452736, Validation loss: 0.20288135, Gradient norm: 1.58000808
INFO:root:[   35] Training loss: 0.09975302, Validation loss: 0.18487499, Gradient norm: 1.42032013
INFO:root:[   36] Training loss: 0.09646624, Validation loss: 0.20095609, Gradient norm: 1.37621602
INFO:root:[   37] Training loss: 0.09417539, Validation loss: 0.16912787, Gradient norm: 1.60241142
INFO:root:[   38] Training loss: 0.09144200, Validation loss: 0.21866653, Gradient norm: 1.52841302
INFO:root:[   39] Training loss: 0.08890501, Validation loss: 0.16871094, Gradient norm: 1.74335011
INFO:root:[   40] Training loss: 0.08649873, Validation loss: 0.22265245, Gradient norm: 1.80253594
INFO:root:[   41] Training loss: 0.08408341, Validation loss: 0.15213299, Gradient norm: 1.79665908
INFO:root:[   42] Training loss: 0.08043787, Validation loss: 0.16405381, Gradient norm: 1.67658631
INFO:root:[   43] Training loss: 0.07918261, Validation loss: 0.20100739, Gradient norm: 1.89521591
INFO:root:[   44] Training loss: 0.07777179, Validation loss: 0.13269635, Gradient norm: 1.94932673
INFO:root:[   45] Training loss: 0.07523925, Validation loss: 0.15423213, Gradient norm: 1.94258310
INFO:root:[   46] Training loss: 0.07095116, Validation loss: 0.12705867, Gradient norm: 1.67284736
INFO:root:[   47] Training loss: 0.07229455, Validation loss: 0.13365140, Gradient norm: 2.12339039
INFO:root:[   48] Training loss: 0.07012022, Validation loss: 0.14111887, Gradient norm: 2.01854105
INFO:root:[   49] Training loss: 0.06746735, Validation loss: 0.15659736, Gradient norm: 1.95613345
INFO:root:[   50] Training loss: 0.06883793, Validation loss: 0.14213587, Gradient norm: 2.33104062
INFO:root:[   51] Training loss: 0.06650884, Validation loss: 0.14049919, Gradient norm: 2.18611366
INFO:root:[   52] Training loss: 0.06554711, Validation loss: 0.11120970, Gradient norm: 2.28691747
INFO:root:[   53] Training loss: 0.06239057, Validation loss: 0.13238632, Gradient norm: 2.07330905
INFO:root:[   54] Training loss: 0.06255446, Validation loss: 0.09565414, Gradient norm: 2.41241228
INFO:root:[   55] Training loss: 0.06483119, Validation loss: 0.09181289, Gradient norm: 2.84881720
INFO:root:[   56] Training loss: 0.06202594, Validation loss: 0.09346811, Gradient norm: 2.57303109
INFO:root:[   57] Training loss: 0.06335945, Validation loss: 0.09590806, Gradient norm: 2.96481807
INFO:root:[   58] Training loss: 0.06157473, Validation loss: 0.08980750, Gradient norm: 2.78225089
INFO:root:[   59] Training loss: 0.05899699, Validation loss: 0.05885354, Gradient norm: 2.58162083
INFO:root:[   60] Training loss: 0.05883764, Validation loss: 0.05967379, Gradient norm: 3.05111213
INFO:root:[   61] Training loss: 0.05699822, Validation loss: 0.05160189, Gradient norm: 3.43134871
INFO:root:[   62] Training loss: 0.05668850, Validation loss: 0.04827767, Gradient norm: 3.54479342
INFO:root:[   63] Training loss: 0.05794178, Validation loss: 0.02831361, Gradient norm: 3.61621104
INFO:root:[   64] Training loss: 0.05824339, Validation loss: 0.02296895, Gradient norm: 3.68603659
INFO:root:[   65] Training loss: 0.05897858, Validation loss: 0.02186093, Gradient norm: 3.92657172
INFO:root:[   66] Training loss: 0.05814179, Validation loss: 0.05823167, Gradient norm: 4.03015126
INFO:root:[   67] Training loss: 0.05779048, Validation loss: 0.02585942, Gradient norm: 3.78803444
INFO:root:[   68] Training loss: 0.05516396, Validation loss: 0.02252472, Gradient norm: 3.08057491
INFO:root:[   69] Training loss: 0.05669840, Validation loss: 0.04231864, Gradient norm: 4.35651473
INFO:root:[   70] Training loss: 0.06281616, Validation loss: 0.05534493, Gradient norm: 5.74077070
INFO:root:[   71] Training loss: 0.06029943, Validation loss: 0.03401663, Gradient norm: 4.53738457
INFO:root:[   72] Training loss: 0.06135812, Validation loss: 0.06956431, Gradient norm: 5.16125404
INFO:root:[   73] Training loss: 0.06040875, Validation loss: 0.07572758, Gradient norm: 4.87296423
INFO:root:[   74] Training loss: 0.05616111, Validation loss: 0.05310936, Gradient norm: 4.11342537
INFO:root:EP 74: Early stopping
INFO:root:Training the model took 944.731s.
INFO:root:Emptying the cuda cache took 0.04s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.04975
INFO:root:EnergyScoreTrain: 0.02513
INFO:root:CoverageTrain: 0.78817
INFO:root:IntervalWidthTrain: 0.03906
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.06095
INFO:root:EnergyScoreValidation: 0.03296
INFO:root:CoverageValidation: 0.71599
INFO:root:IntervalWidthValidation: 0.03971
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.07381
INFO:root:EnergyScoreTest: 0.03295
INFO:root:CoverageTest: 0.60155
INFO:root:IntervalWidthTest: 0.04123
INFO:root:###19 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1914699776
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.70798737, Validation loss: 0.15209674, Gradient norm: 5.46893308
INFO:root:[    2] Training loss: 0.14831454, Validation loss: 0.12239892, Gradient norm: 3.28128183
INFO:root:[    3] Training loss: 0.11831136, Validation loss: 0.10595647, Gradient norm: 2.40988236
INFO:root:[    4] Training loss: 0.10037584, Validation loss: 0.08661509, Gradient norm: 2.56028400
INFO:root:[    5] Training loss: 0.09060015, Validation loss: 0.08839705, Gradient norm: 2.77894611
INFO:root:[    6] Training loss: 0.08559587, Validation loss: 0.07675570, Gradient norm: 2.79812093
INFO:root:[    7] Training loss: 0.08621525, Validation loss: 0.07296221, Gradient norm: 3.03119255
INFO:root:[    8] Training loss: 0.07814395, Validation loss: 0.07414049, Gradient norm: 2.62092125
INFO:root:[    9] Training loss: 0.07532673, Validation loss: 0.07238388, Gradient norm: 2.14460746
INFO:root:[   10] Training loss: 0.07487436, Validation loss: 0.06987471, Gradient norm: 2.49639148
INFO:root:[   11] Training loss: 0.06958178, Validation loss: 0.06671456, Gradient norm: 2.30503435
INFO:root:[   12] Training loss: 0.06779796, Validation loss: 0.06588993, Gradient norm: 2.12274900
INFO:root:[   13] Training loss: 0.06702030, Validation loss: 0.06757333, Gradient norm: 2.53987066
INFO:root:[   14] Training loss: 0.06566154, Validation loss: 0.06465759, Gradient norm: 2.49484764
INFO:root:[   15] Training loss: 0.06708106, Validation loss: 0.06530018, Gradient norm: 2.49117166
INFO:root:[   16] Training loss: 0.06447114, Validation loss: 0.06083487, Gradient norm: 2.59122443
INFO:root:[   17] Training loss: 0.06195821, Validation loss: 0.05967228, Gradient norm: 2.77185642
INFO:root:[   18] Training loss: 0.06071668, Validation loss: 0.06194451, Gradient norm: 2.75118351
INFO:root:[   19] Training loss: 0.05981168, Validation loss: 0.05911955, Gradient norm: 2.79155410
INFO:root:[   20] Training loss: 0.05949411, Validation loss: 0.06050683, Gradient norm: 2.76597105
INFO:root:[   21] Training loss: 0.05951966, Validation loss: 0.06126841, Gradient norm: 2.53578124
INFO:root:[   22] Training loss: 0.05859547, Validation loss: 0.05938412, Gradient norm: 2.49992226
INFO:root:[   23] Training loss: 0.05701433, Validation loss: 0.05699416, Gradient norm: 2.60870289
INFO:root:[   24] Training loss: 0.05576374, Validation loss: 0.05816366, Gradient norm: 2.82678861
INFO:root:[   25] Training loss: 0.05559631, Validation loss: 0.05396358, Gradient norm: 2.89302804
INFO:root:[   26] Training loss: 0.05442088, Validation loss: 0.06603075, Gradient norm: 2.72191110
INFO:root:[   27] Training loss: 0.05543532, Validation loss: 0.05560215, Gradient norm: 2.68254195
INFO:root:[   28] Training loss: 0.05561231, Validation loss: 0.04862957, Gradient norm: 2.56115303
INFO:root:[   29] Training loss: 0.05482313, Validation loss: 0.05827660, Gradient norm: 2.66488524
INFO:root:[   30] Training loss: 0.05429086, Validation loss: 0.05226172, Gradient norm: 2.71160047
INFO:root:[   31] Training loss: 0.05188407, Validation loss: 0.05190931, Gradient norm: 2.94920163
INFO:root:[   32] Training loss: 0.05113760, Validation loss: 0.04837709, Gradient norm: 2.83998738
INFO:root:[   33] Training loss: 0.05072497, Validation loss: 0.05620112, Gradient norm: 2.83489835
INFO:root:[   34] Training loss: 0.05018572, Validation loss: 0.04936055, Gradient norm: 2.75150826
INFO:root:[   35] Training loss: 0.04975144, Validation loss: 0.05100990, Gradient norm: 3.00982603
INFO:root:[   36] Training loss: 0.04892773, Validation loss: 0.04640375, Gradient norm: 2.84392452
INFO:root:[   37] Training loss: 0.04884747, Validation loss: 0.04544989, Gradient norm: 2.91170199
INFO:root:[   38] Training loss: 0.04865184, Validation loss: 0.05468934, Gradient norm: 2.51220637
INFO:root:[   39] Training loss: 0.05135737, Validation loss: 0.04904625, Gradient norm: 2.53286257
INFO:root:[   40] Training loss: 0.04868906, Validation loss: 0.04827738, Gradient norm: 2.78097285
INFO:root:[   41] Training loss: 0.04717103, Validation loss: 0.05085261, Gradient norm: 3.00184381
INFO:root:[   42] Training loss: 0.04696834, Validation loss: 0.04249075, Gradient norm: 2.82436963
INFO:root:[   43] Training loss: 0.04651115, Validation loss: 0.04670893, Gradient norm: 2.81153905
INFO:root:[   44] Training loss: 0.04620912, Validation loss: 0.04677598, Gradient norm: 2.96265199
INFO:root:[   45] Training loss: 0.04594707, Validation loss: 0.04922708, Gradient norm: 2.87844018
INFO:root:[   46] Training loss: 0.04580097, Validation loss: 0.04059809, Gradient norm: 2.81877657
INFO:root:[   47] Training loss: 0.04532650, Validation loss: 0.04558744, Gradient norm: 2.82570055
INFO:root:[   48] Training loss: 0.04516118, Validation loss: 0.04569303, Gradient norm: 2.85206742
INFO:root:[   49] Training loss: 0.04529724, Validation loss: 0.04435619, Gradient norm: 2.96062626
INFO:root:[   50] Training loss: 0.04483477, Validation loss: 0.04420106, Gradient norm: 2.66555562
INFO:root:[   51] Training loss: 0.04616267, Validation loss: 0.04421243, Gradient norm: 2.57029103
INFO:root:[   52] Training loss: 0.04548536, Validation loss: 0.04769263, Gradient norm: 2.60368931
INFO:root:[   53] Training loss: 0.04476849, Validation loss: 0.04420902, Gradient norm: 2.70099264
INFO:root:[   54] Training loss: 0.04388317, Validation loss: 0.04334458, Gradient norm: 3.02366856
INFO:root:[   55] Training loss: 0.04347632, Validation loss: 0.04387902, Gradient norm: 3.05156260
INFO:root:[   56] Training loss: 0.04340568, Validation loss: 0.04303434, Gradient norm: 2.94482125
INFO:root:[   57] Training loss: 0.04332693, Validation loss: 0.04406562, Gradient norm: 2.81262615
INFO:root:[   58] Training loss: 0.04285130, Validation loss: 0.04102622, Gradient norm: 2.96406068
INFO:root:[   59] Training loss: 0.04240237, Validation loss: 0.04370597, Gradient norm: 2.99449705
INFO:root:[   60] Training loss: 0.04255553, Validation loss: 0.04390615, Gradient norm: 2.79100614
INFO:root:[   61] Training loss: 0.04230406, Validation loss: 0.04422672, Gradient norm: 2.99566663
INFO:root:[   62] Training loss: 0.04178974, Validation loss: 0.04361164, Gradient norm: 2.75242664
INFO:root:[   63] Training loss: 0.04272737, Validation loss: 0.04114363, Gradient norm: 2.75564800
INFO:root:[   64] Training loss: 0.04198644, Validation loss: 0.04405394, Gradient norm: 2.99588310
INFO:root:[   65] Training loss: 0.04194814, Validation loss: 0.04282785, Gradient norm: 2.93567228
INFO:root:[   66] Training loss: 0.04140956, Validation loss: 0.03959395, Gradient norm: 2.81398749
INFO:root:[   67] Training loss: 0.04154487, Validation loss: 0.04286224, Gradient norm: 2.95200240
INFO:root:[   68] Training loss: 0.04109278, Validation loss: 0.03885411, Gradient norm: 2.92067947
INFO:root:[   69] Training loss: 0.04049104, Validation loss: 0.03892035, Gradient norm: 2.88440031
INFO:root:[   70] Training loss: 0.04052659, Validation loss: 0.03968481, Gradient norm: 2.94447685
INFO:root:[   71] Training loss: 0.04034764, Validation loss: 0.04124871, Gradient norm: 2.89892011
INFO:root:[   72] Training loss: 0.04068630, Validation loss: 0.03646363, Gradient norm: 2.87659305
INFO:root:[   73] Training loss: 0.04022232, Validation loss: 0.04134579, Gradient norm: 2.81761218
INFO:root:[   74] Training loss: 0.04000098, Validation loss: 0.04173097, Gradient norm: 2.92952429
INFO:root:[   75] Training loss: 0.03980820, Validation loss: 0.03891988, Gradient norm: 2.86385313
INFO:root:[   76] Training loss: 0.03977644, Validation loss: 0.03847141, Gradient norm: 3.01905176
INFO:root:[   77] Training loss: 0.03916709, Validation loss: 0.04052421, Gradient norm: 2.82788649
INFO:root:[   78] Training loss: 0.03973259, Validation loss: 0.03853617, Gradient norm: 2.63510744
INFO:root:[   79] Training loss: 0.03931964, Validation loss: 0.04182960, Gradient norm: 2.99528971
INFO:root:[   80] Training loss: 0.03974687, Validation loss: 0.03875375, Gradient norm: 2.73356726
INFO:root:[   81] Training loss: 0.03872394, Validation loss: 0.03800028, Gradient norm: 2.87895115
INFO:root:EP 81: Early stopping
INFO:root:Training the model took 1047.418s.
INFO:root:Emptying the cuda cache took 0.039s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01707
INFO:root:EnergyScoreTrain: 0.01463
INFO:root:CoverageTrain: 0.99479
INFO:root:IntervalWidthTrain: 0.03677
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01704
INFO:root:EnergyScoreValidation: 0.0146
INFO:root:CoverageValidation: 0.99475
INFO:root:IntervalWidthValidation: 0.0366
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02055
INFO:root:EnergyScoreTest: 0.01592
INFO:root:CoverageTest: 0.9874
INFO:root:IntervalWidthTest: 0.03667
INFO:root:###20 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.62019169, Validation loss: 0.22924386, Gradient norm: 4.25737472
INFO:root:[    2] Training loss: 0.20204710, Validation loss: 0.17930616, Gradient norm: 2.39938974
INFO:root:[    3] Training loss: 0.17209525, Validation loss: 0.15061699, Gradient norm: 2.39672224
INFO:root:[    4] Training loss: 0.14952870, Validation loss: 0.12988437, Gradient norm: 1.94212375
INFO:root:[    5] Training loss: 0.12969127, Validation loss: 0.13662109, Gradient norm: 1.47540948
INFO:root:[    6] Training loss: 0.12291463, Validation loss: 0.11291376, Gradient norm: 2.06576821
INFO:root:[    7] Training loss: 0.11342037, Validation loss: 0.10642769, Gradient norm: 1.80041066
INFO:root:[    8] Training loss: 0.11523852, Validation loss: 0.12228891, Gradient norm: 2.50096208
INFO:root:[    9] Training loss: 0.10749445, Validation loss: 0.11725541, Gradient norm: 2.11125627
INFO:root:[   10] Training loss: 0.10860525, Validation loss: 0.10518617, Gradient norm: 2.46394767
INFO:root:[   11] Training loss: 0.10310104, Validation loss: 0.10290262, Gradient norm: 1.93035858
INFO:root:[   12] Training loss: 0.10510837, Validation loss: 0.09392168, Gradient norm: 2.56227568
INFO:root:[   13] Training loss: 0.10435937, Validation loss: 0.09686392, Gradient norm: 2.54465940
INFO:root:[   14] Training loss: 0.09630409, Validation loss: 0.09334272, Gradient norm: 1.89258415
INFO:root:[   15] Training loss: 0.09722014, Validation loss: 0.09331801, Gradient norm: 2.31088705
INFO:root:[   16] Training loss: 0.09435749, Validation loss: 0.10805517, Gradient norm: 1.95605726
INFO:root:[   17] Training loss: 0.09545642, Validation loss: 0.08497226, Gradient norm: 2.45767708
INFO:root:[   18] Training loss: 0.09178424, Validation loss: 0.08630995, Gradient norm: 2.14144446
INFO:root:[   19] Training loss: 0.08952889, Validation loss: 0.09027031, Gradient norm: 1.91116076
INFO:root:[   20] Training loss: 0.09062978, Validation loss: 0.08187904, Gradient norm: 2.37100825
INFO:root:[   21] Training loss: 0.08947560, Validation loss: 0.09019389, Gradient norm: 2.36650842
INFO:root:[   22] Training loss: 0.08735836, Validation loss: 0.09048917, Gradient norm: 2.33487829
INFO:root:[   23] Training loss: 0.08592252, Validation loss: 0.08312080, Gradient norm: 2.23051311
INFO:root:[   24] Training loss: 0.08382052, Validation loss: 0.08678537, Gradient norm: 1.92893043
INFO:root:[   25] Training loss: 0.08507480, Validation loss: 0.08875158, Gradient norm: 2.32541529
INFO:root:[   26] Training loss: 0.08273806, Validation loss: 0.07855117, Gradient norm: 2.25013068
INFO:root:[   27] Training loss: 0.08040051, Validation loss: 0.08135163, Gradient norm: 2.12824643
INFO:root:[   28] Training loss: 0.08055419, Validation loss: 0.08138583, Gradient norm: 2.18987291
INFO:root:[   29] Training loss: 0.08025168, Validation loss: 0.08104593, Gradient norm: 2.29495260
INFO:root:[   30] Training loss: 0.07880949, Validation loss: 0.07435123, Gradient norm: 2.30479833
INFO:root:[   31] Training loss: 0.07772718, Validation loss: 0.07463623, Gradient norm: 2.16212097
INFO:root:[   32] Training loss: 0.07752736, Validation loss: 0.07291062, Gradient norm: 2.32185111
INFO:root:[   33] Training loss: 0.07728058, Validation loss: 0.07581875, Gradient norm: 2.34929811
INFO:root:[   34] Training loss: 0.07628855, Validation loss: 0.07642469, Gradient norm: 2.25663319
INFO:root:[   35] Training loss: 0.07474514, Validation loss: 0.06925162, Gradient norm: 2.25643219
INFO:root:[   36] Training loss: 0.07652008, Validation loss: 0.07471695, Gradient norm: 2.49783612
INFO:root:[   37] Training loss: 0.07385289, Validation loss: 0.07755133, Gradient norm: 2.22146788
INFO:root:[   38] Training loss: 0.07251053, Validation loss: 0.07678373, Gradient norm: 2.17470291
INFO:root:[   39] Training loss: 0.07449599, Validation loss: 0.06712750, Gradient norm: 2.57114056
INFO:root:[   40] Training loss: 0.07217212, Validation loss: 0.07408202, Gradient norm: 2.21207487
INFO:root:[   41] Training loss: 0.07167656, Validation loss: 0.07018665, Gradient norm: 2.25900261
INFO:root:[   42] Training loss: 0.07074280, Validation loss: 0.07590940, Gradient norm: 2.23711990
INFO:root:[   43] Training loss: 0.07000269, Validation loss: 0.06510593, Gradient norm: 2.21720560
INFO:root:[   44] Training loss: 0.07148912, Validation loss: 0.06595923, Gradient norm: 2.52606407
INFO:root:[   45] Training loss: 0.07100499, Validation loss: 0.07705368, Gradient norm: 2.41615404
INFO:root:[   46] Training loss: 0.07023431, Validation loss: 0.07562528, Gradient norm: 2.47832348
INFO:root:[   47] Training loss: 0.06916733, Validation loss: 0.06919295, Gradient norm: 2.37265321
INFO:root:[   48] Training loss: 0.06794878, Validation loss: 0.06511106, Gradient norm: 2.24571294
INFO:root:[   49] Training loss: 0.06849820, Validation loss: 0.06315167, Gradient norm: 2.43793994
INFO:root:[   50] Training loss: 0.06755456, Validation loss: 0.06185113, Gradient norm: 2.42180544
INFO:root:[   51] Training loss: 0.06837251, Validation loss: 0.07454773, Gradient norm: 2.53634962
INFO:root:[   52] Training loss: 0.06788252, Validation loss: 0.06052581, Gradient norm: 2.56403855
INFO:root:[   53] Training loss: 0.06558716, Validation loss: 0.06093227, Gradient norm: 2.24179288
INFO:root:[   54] Training loss: 0.06456892, Validation loss: 0.06540218, Gradient norm: 2.09433914
INFO:root:[   55] Training loss: 0.06478378, Validation loss: 0.06236845, Gradient norm: 2.18113075
INFO:root:[   56] Training loss: 0.06285952, Validation loss: 0.05983192, Gradient norm: 1.88413070
INFO:root:[   57] Training loss: 0.06309059, Validation loss: 0.06816614, Gradient norm: 2.17982270
INFO:root:[   58] Training loss: 0.06381258, Validation loss: 0.06209398, Gradient norm: 2.39709869
INFO:root:[   59] Training loss: 0.06135725, Validation loss: 0.06206296, Gradient norm: 2.14821991
INFO:root:[   60] Training loss: 0.06118576, Validation loss: 0.05957729, Gradient norm: 2.24466451
INFO:root:[   61] Training loss: 0.06067375, Validation loss: 0.06343651, Gradient norm: 2.29436488
INFO:root:[   62] Training loss: 0.06029299, Validation loss: 0.06206511, Gradient norm: 2.29876889
INFO:root:[   63] Training loss: 0.06148442, Validation loss: 0.05778402, Gradient norm: 2.28920540
INFO:root:[   64] Training loss: 0.06007016, Validation loss: 0.06071902, Gradient norm: 2.33632099
INFO:root:[   65] Training loss: 0.06063810, Validation loss: 0.06126388, Gradient norm: 2.22425309
INFO:root:[   66] Training loss: 0.06004699, Validation loss: 0.06397815, Gradient norm: 2.43695349
INFO:root:[   67] Training loss: 0.05860207, Validation loss: 0.05540003, Gradient norm: 2.27850987
INFO:root:[   68] Training loss: 0.05809293, Validation loss: 0.06079289, Gradient norm: 2.37631322
INFO:root:[   69] Training loss: 0.05800299, Validation loss: 0.05914388, Gradient norm: 2.36275080
INFO:root:[   70] Training loss: 0.05760255, Validation loss: 0.05737589, Gradient norm: 2.41950828
INFO:root:[   71] Training loss: 0.05697262, Validation loss: 0.06014797, Gradient norm: 2.41273444
INFO:root:[   72] Training loss: 0.05720774, Validation loss: 0.05587532, Gradient norm: 2.45846907
INFO:root:[   73] Training loss: 0.05692189, Validation loss: 0.05942336, Gradient norm: 2.23688708
INFO:root:[   74] Training loss: 0.05810416, Validation loss: 0.06363034, Gradient norm: 2.43489243
INFO:root:[   75] Training loss: 0.05701644, Validation loss: 0.05315711, Gradient norm: 2.44763619
INFO:root:[   76] Training loss: 0.05645144, Validation loss: 0.05297463, Gradient norm: 2.39322343
INFO:root:[   77] Training loss: 0.05436144, Validation loss: 0.05368088, Gradient norm: 2.39563963
INFO:root:[   78] Training loss: 0.05466739, Validation loss: 0.05463499, Gradient norm: 2.38598205
INFO:root:[   79] Training loss: 0.05398899, Validation loss: 0.05478032, Gradient norm: 2.40266402
INFO:root:[   80] Training loss: 0.05358404, Validation loss: 0.05286289, Gradient norm: 2.49858126
INFO:root:[   81] Training loss: 0.05339672, Validation loss: 0.05351029, Gradient norm: 2.48536636
INFO:root:[   82] Training loss: 0.05307712, Validation loss: 0.05191148, Gradient norm: 2.52748404
INFO:root:[   83] Training loss: 0.05294133, Validation loss: 0.05571208, Gradient norm: 2.50150251
INFO:root:[   84] Training loss: 0.05393395, Validation loss: 0.04924387, Gradient norm: 2.62905964
INFO:root:[   85] Training loss: 0.05353926, Validation loss: 0.05255168, Gradient norm: 2.49308822
INFO:root:[   86] Training loss: 0.05527557, Validation loss: 0.05525879, Gradient norm: 2.60941468
INFO:root:[   87] Training loss: 0.05265486, Validation loss: 0.04856781, Gradient norm: 2.12675130
INFO:root:[   88] Training loss: 0.05218057, Validation loss: 0.05025068, Gradient norm: 2.25597431
INFO:root:[   89] Training loss: 0.05073938, Validation loss: 0.04650173, Gradient norm: 2.10984167
INFO:root:[   90] Training loss: 0.05220576, Validation loss: 0.04879590, Gradient norm: 2.36458644
INFO:root:[   91] Training loss: 0.05255096, Validation loss: 0.05233755, Gradient norm: 2.47028578
INFO:root:[   92] Training loss: 0.04931507, Validation loss: 0.04682261, Gradient norm: 2.27207999
INFO:root:[   93] Training loss: 0.04872692, Validation loss: 0.04955386, Gradient norm: 2.20561430
INFO:root:[   94] Training loss: 0.04884181, Validation loss: 0.04765698, Gradient norm: 2.45845493
INFO:root:[   95] Training loss: 0.04866129, Validation loss: 0.04715111, Gradient norm: 2.50950082
INFO:root:[   96] Training loss: 0.04807184, Validation loss: 0.04515157, Gradient norm: 2.44880874
INFO:root:[   97] Training loss: 0.04797792, Validation loss: 0.05166267, Gradient norm: 2.47264278
INFO:root:[   98] Training loss: 0.04785779, Validation loss: 0.04546153, Gradient norm: 2.33250660
INFO:root:[   99] Training loss: 0.04735295, Validation loss: 0.05001569, Gradient norm: 2.42318634
INFO:root:[  100] Training loss: 0.04660274, Validation loss: 0.04431968, Gradient norm: 2.29325388
INFO:root:[  101] Training loss: 0.04705520, Validation loss: 0.04595475, Gradient norm: 2.49094674
INFO:root:[  102] Training loss: 0.04660487, Validation loss: 0.04828765, Gradient norm: 2.51442187
INFO:root:[  103] Training loss: 0.04627864, Validation loss: 0.04757780, Gradient norm: 2.38830886
INFO:root:[  104] Training loss: 0.04663611, Validation loss: 0.04804869, Gradient norm: 2.37583137
INFO:root:[  105] Training loss: 0.04576642, Validation loss: 0.04670198, Gradient norm: 2.41942856
INFO:root:[  106] Training loss: 0.04536650, Validation loss: 0.04182437, Gradient norm: 2.53310029
INFO:root:[  107] Training loss: 0.04491730, Validation loss: 0.04703707, Gradient norm: 2.43322140
INFO:root:[  108] Training loss: 0.04462223, Validation loss: 0.04431699, Gradient norm: 2.35226335
INFO:root:[  109] Training loss: 0.04413772, Validation loss: 0.04377225, Gradient norm: 2.38222589
INFO:root:[  110] Training loss: 0.04366913, Validation loss: 0.04311317, Gradient norm: 2.43255319
INFO:root:[  111] Training loss: 0.04417082, Validation loss: 0.04441469, Gradient norm: 2.52469581
INFO:root:[  112] Training loss: 0.04354057, Validation loss: 0.04083714, Gradient norm: 2.42917114
INFO:root:[  113] Training loss: 0.04413886, Validation loss: 0.04494262, Gradient norm: 2.18984891
INFO:root:[  114] Training loss: 0.04396501, Validation loss: 0.04272292, Gradient norm: 2.28959088
INFO:root:[  115] Training loss: 0.04270337, Validation loss: 0.04396031, Gradient norm: 2.41779816
INFO:root:[  116] Training loss: 0.04299597, Validation loss: 0.04272805, Gradient norm: 2.35255781
INFO:root:[  117] Training loss: 0.04200682, Validation loss: 0.04201087, Gradient norm: 2.20119337
INFO:root:[  118] Training loss: 0.04273220, Validation loss: 0.04011955, Gradient norm: 2.30988756
INFO:root:[  119] Training loss: 0.04167692, Validation loss: 0.04278274, Gradient norm: 2.35161431
INFO:root:[  120] Training loss: 0.04113160, Validation loss: 0.04002067, Gradient norm: 2.43597644
INFO:root:[  121] Training loss: 0.04069370, Validation loss: 0.04176801, Gradient norm: 2.42607557
INFO:root:[  122] Training loss: 0.04068048, Validation loss: 0.03905167, Gradient norm: 2.45528093
INFO:root:[  123] Training loss: 0.04009918, Validation loss: 0.04598040, Gradient norm: 2.38878478
INFO:root:[  124] Training loss: 0.04182485, Validation loss: 0.03991249, Gradient norm: 2.28943431
INFO:root:[  125] Training loss: 0.03961417, Validation loss: 0.03873942, Gradient norm: 2.36061704
INFO:root:[  126] Training loss: 0.04023279, Validation loss: 0.04310571, Gradient norm: 2.15675333
INFO:root:[  127] Training loss: 0.04014246, Validation loss: 0.03582003, Gradient norm: 2.33540820
INFO:root:[  128] Training loss: 0.03937524, Validation loss: 0.03895678, Gradient norm: 2.22530589
INFO:root:[  129] Training loss: 0.03888661, Validation loss: 0.04246317, Gradient norm: 2.26473328
INFO:root:[  130] Training loss: 0.03869937, Validation loss: 0.03884668, Gradient norm: 2.38241964
INFO:root:[  131] Training loss: 0.03806656, Validation loss: 0.03674812, Gradient norm: 2.43409150
INFO:root:[  132] Training loss: 0.03789227, Validation loss: 0.03771387, Gradient norm: 2.33407900
INFO:root:[  133] Training loss: 0.03742139, Validation loss: 0.03561603, Gradient norm: 2.36134481
INFO:root:[  134] Training loss: 0.03729026, Validation loss: 0.03971267, Gradient norm: 2.32553312
INFO:root:[  135] Training loss: 0.03748432, Validation loss: 0.03701544, Gradient norm: 2.29504626
INFO:root:[  136] Training loss: 0.03665897, Validation loss: 0.03374165, Gradient norm: 2.42137087
INFO:root:[  137] Training loss: 0.03607380, Validation loss: 0.03709884, Gradient norm: 2.27615022
INFO:root:[  138] Training loss: 0.03636397, Validation loss: 0.03697144, Gradient norm: 2.39105121
INFO:root:[  139] Training loss: 0.03621376, Validation loss: 0.03643577, Gradient norm: 2.24521030
INFO:root:[  140] Training loss: 0.03564567, Validation loss: 0.03446238, Gradient norm: 2.31450080
INFO:root:[  141] Training loss: 0.03565844, Validation loss: 0.03464032, Gradient norm: 2.27652906
INFO:root:[  142] Training loss: 0.03597211, Validation loss: 0.03308720, Gradient norm: 2.06754106
INFO:root:[  143] Training loss: 0.03548842, Validation loss: 0.03466227, Gradient norm: 2.18732855
INFO:root:[  144] Training loss: 0.03493100, Validation loss: 0.03504870, Gradient norm: 2.35660686
INFO:root:[  145] Training loss: 0.03525728, Validation loss: 0.03478053, Gradient norm: 2.12591079
INFO:root:[  146] Training loss: 0.03446414, Validation loss: 0.03388970, Gradient norm: 2.15609807
INFO:root:[  147] Training loss: 0.03452962, Validation loss: 0.03218472, Gradient norm: 2.22126014
INFO:root:[  148] Training loss: 0.03383571, Validation loss: 0.03496691, Gradient norm: 2.30998333
INFO:root:[  149] Training loss: 0.03335180, Validation loss: 0.03195907, Gradient norm: 2.30745933
INFO:root:[  150] Training loss: 0.03347669, Validation loss: 0.03305234, Gradient norm: 2.28329512
INFO:root:[  151] Training loss: 0.03306415, Validation loss: 0.03292858, Gradient norm: 2.21363934
INFO:root:[  152] Training loss: 0.03300473, Validation loss: 0.03557215, Gradient norm: 2.30446017
INFO:root:[  153] Training loss: 0.03252027, Validation loss: 0.03305389, Gradient norm: 2.18017450
INFO:root:[  154] Training loss: 0.03226030, Validation loss: 0.03350529, Gradient norm: 2.23805232
INFO:root:[  155] Training loss: 0.03209583, Validation loss: 0.03350117, Gradient norm: 2.25220466
INFO:root:[  156] Training loss: 0.03295954, Validation loss: 0.03025646, Gradient norm: 2.19267098
INFO:root:[  157] Training loss: 0.03226554, Validation loss: 0.03424048, Gradient norm: 2.11918876
INFO:root:[  158] Training loss: 0.03240761, Validation loss: 0.03380509, Gradient norm: 2.06030139
INFO:root:[  159] Training loss: 0.03197251, Validation loss: 0.03816470, Gradient norm: 2.01971296
INFO:root:[  160] Training loss: 0.03415090, Validation loss: 0.03633734, Gradient norm: 2.43748505
INFO:root:[  161] Training loss: 0.03119326, Validation loss: 0.03149628, Gradient norm: 2.01310691
INFO:root:[  162] Training loss: 0.03030367, Validation loss: 0.03127461, Gradient norm: 2.23184758
INFO:root:[  163] Training loss: 0.02990225, Validation loss: 0.02897321, Gradient norm: 2.15969503
INFO:root:[  164] Training loss: 0.03005028, Validation loss: 0.03122224, Gradient norm: 2.15661745
INFO:root:[  165] Training loss: 0.03027446, Validation loss: 0.03750839, Gradient norm: 2.16333601
INFO:root:[  166] Training loss: 0.03107349, Validation loss: 0.02887566, Gradient norm: 1.92314643
INFO:root:[  167] Training loss: 0.03059765, Validation loss: 0.03029851, Gradient norm: 1.98092612
INFO:root:[  168] Training loss: 0.02947972, Validation loss: 0.02819790, Gradient norm: 2.08998138
INFO:root:[  169] Training loss: 0.02907821, Validation loss: 0.02973376, Gradient norm: 2.23084409
INFO:root:[  170] Training loss: 0.02878521, Validation loss: 0.02752887, Gradient norm: 2.26621616
INFO:root:[  171] Training loss: 0.02834782, Validation loss: 0.03161931, Gradient norm: 2.15242270
INFO:root:[  172] Training loss: 0.02931151, Validation loss: 0.03172197, Gradient norm: 2.08624907
INFO:root:[  173] Training loss: 0.02858029, Validation loss: 0.02846551, Gradient norm: 2.16135203
INFO:root:[  174] Training loss: 0.02848833, Validation loss: 0.03062823, Gradient norm: 2.22342198
INFO:root:[  175] Training loss: 0.02814740, Validation loss: 0.02894753, Gradient norm: 2.09649394
INFO:root:[  176] Training loss: 0.02808223, Validation loss: 0.03340597, Gradient norm: 2.06228675
INFO:root:[  177] Training loss: 0.03087265, Validation loss: 0.02542140, Gradient norm: 2.30968740
INFO:root:[  178] Training loss: 0.02832397, Validation loss: 0.03179932, Gradient norm: 1.93290150
INFO:root:[  179] Training loss: 0.02936825, Validation loss: 0.03229281, Gradient norm: 2.09565653
INFO:root:[  180] Training loss: 0.03071288, Validation loss: 0.03689576, Gradient norm: 2.36223336
INFO:root:[  181] Training loss: 0.02898070, Validation loss: 0.02860812, Gradient norm: 2.29438828
INFO:root:[  182] Training loss: 0.02780642, Validation loss: 0.03071873, Gradient norm: 2.15293920
INFO:root:[  183] Training loss: 0.02835748, Validation loss: 0.02923790, Gradient norm: 2.18114580
INFO:root:[  184] Training loss: 0.02728926, Validation loss: 0.03032722, Gradient norm: 1.98515781
INFO:root:[  185] Training loss: 0.02865336, Validation loss: 0.02993415, Gradient norm: 2.21230831
INFO:root:[  186] Training loss: 0.02794982, Validation loss: 0.02919813, Gradient norm: 2.24180238
INFO:root:[  187] Training loss: 0.02779136, Validation loss: 0.02314585, Gradient norm: 2.21986171
INFO:root:[  188] Training loss: 0.02743771, Validation loss: 0.02258810, Gradient norm: 2.21985492
INFO:root:[  189] Training loss: 0.02763731, Validation loss: 0.02305821, Gradient norm: 2.16805594
INFO:root:[  190] Training loss: 0.02688877, Validation loss: 0.02543153, Gradient norm: 2.10475495
INFO:root:[  191] Training loss: 0.02668938, Validation loss: 0.02679424, Gradient norm: 2.07740016
INFO:root:[  192] Training loss: 0.02645304, Validation loss: 0.02776610, Gradient norm: 2.07396359
INFO:root:[  193] Training loss: 0.02712635, Validation loss: 0.02844630, Gradient norm: 2.09740836
INFO:root:[  194] Training loss: 0.02664883, Validation loss: 0.02866673, Gradient norm: 2.11295584
INFO:root:[  195] Training loss: 0.02659164, Validation loss: 0.02788456, Gradient norm: 2.15683497
INFO:root:[  196] Training loss: 0.02559610, Validation loss: 0.02424771, Gradient norm: 1.98452837
INFO:root:[  197] Training loss: 0.02578231, Validation loss: 0.02133246, Gradient norm: 2.08056357
INFO:root:[  198] Training loss: 0.02588669, Validation loss: 0.02368213, Gradient norm: 2.00831655
INFO:root:[  199] Training loss: 0.02539703, Validation loss: 0.02656124, Gradient norm: 2.10299474
INFO:root:[  200] Training loss: 0.02530549, Validation loss: 0.02658485, Gradient norm: 2.02837311
INFO:root:[  201] Training loss: 0.02604462, Validation loss: 0.02129617, Gradient norm: 2.20691214
INFO:root:[  202] Training loss: 0.02493705, Validation loss: 0.02162983, Gradient norm: 2.07187090
INFO:root:[  203] Training loss: 0.02456866, Validation loss: 0.02362571, Gradient norm: 1.89123070
INFO:root:[  204] Training loss: 0.02483569, Validation loss: 0.02398887, Gradient norm: 1.93852260
INFO:root:[  205] Training loss: 0.02418349, Validation loss: 0.02280698, Gradient norm: 1.88145760
INFO:root:[  206] Training loss: 0.02512770, Validation loss: 0.02843269, Gradient norm: 2.05610586
INFO:root:[  207] Training loss: 0.02498005, Validation loss: 0.02799897, Gradient norm: 2.03231692
INFO:root:[  208] Training loss: 0.02455931, Validation loss: 0.02141906, Gradient norm: 2.04875585
INFO:root:[  209] Training loss: 0.02424347, Validation loss: 0.02608834, Gradient norm: 1.97160524
INFO:root:[  210] Training loss: 0.02503361, Validation loss: 0.02751226, Gradient norm: 2.19849489
INFO:root:EP 210: Early stopping
INFO:root:Training the model took 2691.851s.
INFO:root:Emptying the cuda cache took 0.036s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01052
INFO:root:EnergyScoreTrain: 0.02135
INFO:root:CoverageTrain: 0.99819
INFO:root:IntervalWidthTrain: 0.01997
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01046
INFO:root:EnergyScoreValidation: 0.02117
INFO:root:CoverageValidation: 0.99826
INFO:root:IntervalWidthValidation: 0.01982
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01511
INFO:root:EnergyScoreTest: 0.02256
INFO:root:CoverageTest: 0.9913
INFO:root:IntervalWidthTest: 0.01997
INFO:root:###21 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.86438739, Validation loss: 0.26261269, Gradient norm: 3.87240130
INFO:root:[    2] Training loss: 0.23514607, Validation loss: 0.19425172, Gradient norm: 1.69968465
INFO:root:[    3] Training loss: 0.18598450, Validation loss: 0.16559462, Gradient norm: 1.49056688
INFO:root:[    4] Training loss: 0.16146258, Validation loss: 0.14861288, Gradient norm: 1.21276075
INFO:root:[    5] Training loss: 0.15138608, Validation loss: 0.14127333, Gradient norm: 1.48166832
INFO:root:[    6] Training loss: 0.14341672, Validation loss: 0.14234080, Gradient norm: 1.82135245
INFO:root:[    7] Training loss: 0.13807936, Validation loss: 0.13071521, Gradient norm: 1.74037233
INFO:root:[    8] Training loss: 0.13666693, Validation loss: 0.13735685, Gradient norm: 1.99651340
INFO:root:[    9] Training loss: 0.13266251, Validation loss: 0.12438271, Gradient norm: 1.95956097
INFO:root:[   10] Training loss: 0.12766193, Validation loss: 0.13049956, Gradient norm: 1.71429482
INFO:root:[   11] Training loss: 0.12390709, Validation loss: 0.12979417, Gradient norm: 1.59999049
INFO:root:[   12] Training loss: 0.12405802, Validation loss: 0.12820042, Gradient norm: 1.93928850
INFO:root:[   13] Training loss: 0.12137017, Validation loss: 0.11693879, Gradient norm: 1.88838835
INFO:root:[   14] Training loss: 0.12088868, Validation loss: 0.12098089, Gradient norm: 2.02966480
INFO:root:[   15] Training loss: 0.11826190, Validation loss: 0.11838931, Gradient norm: 1.86422176
INFO:root:[   16] Training loss: 0.11719737, Validation loss: 0.13016758, Gradient norm: 1.88388698
INFO:root:[   17] Training loss: 0.11732113, Validation loss: 0.10721117, Gradient norm: 2.25284595
INFO:root:[   18] Training loss: 0.11176472, Validation loss: 0.11326191, Gradient norm: 1.74478429
INFO:root:[   19] Training loss: 0.11038364, Validation loss: 0.11359931, Gradient norm: 1.83774459
INFO:root:[   20] Training loss: 0.11020456, Validation loss: 0.10544775, Gradient norm: 2.03178193
INFO:root:[   21] Training loss: 0.10785940, Validation loss: 0.10169431, Gradient norm: 1.85108835
INFO:root:[   22] Training loss: 0.10697366, Validation loss: 0.10509985, Gradient norm: 1.92473957
INFO:root:[   23] Training loss: 0.10458654, Validation loss: 0.10577452, Gradient norm: 1.79836454
INFO:root:[   24] Training loss: 0.10495431, Validation loss: 0.10598204, Gradient norm: 2.05953372
INFO:root:[   25] Training loss: 0.10083972, Validation loss: 0.10340433, Gradient norm: 1.82126443
INFO:root:[   26] Training loss: 0.10117928, Validation loss: 0.09967972, Gradient norm: 2.18856209
INFO:root:[   27] Training loss: 0.09984192, Validation loss: 0.09507345, Gradient norm: 2.17964976
INFO:root:[   28] Training loss: 0.09660749, Validation loss: 0.09503078, Gradient norm: 1.93109860
INFO:root:[   29] Training loss: 0.09482068, Validation loss: 0.09566056, Gradient norm: 1.92188956
INFO:root:[   30] Training loss: 0.09434084, Validation loss: 0.09548269, Gradient norm: 2.07747155
INFO:root:[   31] Training loss: 0.09326351, Validation loss: 0.08871949, Gradient norm: 2.04957480
INFO:root:[   32] Training loss: 0.09055907, Validation loss: 0.08622617, Gradient norm: 2.00908292
INFO:root:[   33] Training loss: 0.08991554, Validation loss: 0.08860663, Gradient norm: 2.17011938
INFO:root:[   34] Training loss: 0.08921362, Validation loss: 0.08472656, Gradient norm: 2.30676394
INFO:root:[   35] Training loss: 0.08749615, Validation loss: 0.08840634, Gradient norm: 2.21705985
INFO:root:[   36] Training loss: 0.08542839, Validation loss: 0.09125375, Gradient norm: 2.09764635
INFO:root:[   37] Training loss: 0.08487090, Validation loss: 0.07869144, Gradient norm: 2.31670445
INFO:root:[   38] Training loss: 0.08285174, Validation loss: 0.07824213, Gradient norm: 2.11530652
INFO:root:[   39] Training loss: 0.08209409, Validation loss: 0.07597222, Gradient norm: 2.25226410
INFO:root:[   40] Training loss: 0.08053949, Validation loss: 0.07536828, Gradient norm: 2.16543584
INFO:root:[   41] Training loss: 0.07931668, Validation loss: 0.07895739, Gradient norm: 2.11352174
INFO:root:[   42] Training loss: 0.07773996, Validation loss: 0.08081402, Gradient norm: 2.15941546
INFO:root:[   43] Training loss: 0.07757537, Validation loss: 0.07759171, Gradient norm: 2.21478050
INFO:root:[   44] Training loss: 0.07622656, Validation loss: 0.07041791, Gradient norm: 2.19344115
INFO:root:[   45] Training loss: 0.07437736, Validation loss: 0.07886843, Gradient norm: 2.05777310
INFO:root:[   46] Training loss: 0.07414867, Validation loss: 0.06943612, Gradient norm: 2.22704213
INFO:root:[   47] Training loss: 0.07260548, Validation loss: 0.07664911, Gradient norm: 2.13130589
INFO:root:[   48] Training loss: 0.07283433, Validation loss: 0.07768532, Gradient norm: 2.34822219
INFO:root:[   49] Training loss: 0.07136271, Validation loss: 0.06642426, Gradient norm: 2.26200169
INFO:root:[   50] Training loss: 0.06982788, Validation loss: 0.07220004, Gradient norm: 2.18088938
INFO:root:[   51] Training loss: 0.06938192, Validation loss: 0.07012637, Gradient norm: 2.25089635
INFO:root:[   52] Training loss: 0.06837867, Validation loss: 0.06571778, Gradient norm: 2.25409546
INFO:root:[   53] Training loss: 0.06782148, Validation loss: 0.06938583, Gradient norm: 2.21455718
INFO:root:[   54] Training loss: 0.06668705, Validation loss: 0.06430969, Gradient norm: 2.25199071
INFO:root:[   55] Training loss: 0.06635810, Validation loss: 0.06399825, Gradient norm: 2.28072754
INFO:root:[   56] Training loss: 0.06565525, Validation loss: 0.06719284, Gradient norm: 2.11113211
INFO:root:[   57] Training loss: 0.06450721, Validation loss: 0.05997716, Gradient norm: 2.15923492
INFO:root:[   58] Training loss: 0.06305108, Validation loss: 0.05939428, Gradient norm: 2.00219599
INFO:root:[   59] Training loss: 0.06203179, Validation loss: 0.06278549, Gradient norm: 2.10826483
INFO:root:[   60] Training loss: 0.06151533, Validation loss: 0.06417881, Gradient norm: 2.12801540
INFO:root:[   61] Training loss: 0.06141973, Validation loss: 0.05586019, Gradient norm: 2.33721422
INFO:root:[   62] Training loss: 0.06104331, Validation loss: 0.06087607, Gradient norm: 2.22966164
INFO:root:[   63] Training loss: 0.05982377, Validation loss: 0.05502932, Gradient norm: 2.14699081
INFO:root:[   64] Training loss: 0.05870589, Validation loss: 0.05793807, Gradient norm: 2.09266411
INFO:root:[   65] Training loss: 0.05774419, Validation loss: 0.05970296, Gradient norm: 2.07095752
INFO:root:[   66] Training loss: 0.05674346, Validation loss: 0.05659270, Gradient norm: 2.03163919
INFO:root:[   67] Training loss: 0.05651313, Validation loss: 0.05829180, Gradient norm: 2.15679236
INFO:root:[   68] Training loss: 0.05590906, Validation loss: 0.05683562, Gradient norm: 2.18327372
INFO:root:[   69] Training loss: 0.05556432, Validation loss: 0.05602029, Gradient norm: 2.22509258
INFO:root:[   70] Training loss: 0.05610330, Validation loss: 0.05718303, Gradient norm: 2.31352622
INFO:root:[   71] Training loss: 0.05499912, Validation loss: 0.04886615, Gradient norm: 2.16638321
INFO:root:[   72] Training loss: 0.05381137, Validation loss: 0.05020071, Gradient norm: 2.27179406
INFO:root:[   73] Training loss: 0.05306177, Validation loss: 0.05428390, Gradient norm: 2.16112785
INFO:root:[   74] Training loss: 0.05279305, Validation loss: 0.05478645, Gradient norm: 2.18067666
INFO:root:[   75] Training loss: 0.05215652, Validation loss: 0.05352854, Gradient norm: 2.28432768
INFO:root:[   76] Training loss: 0.05087978, Validation loss: 0.04711023, Gradient norm: 2.11522458
INFO:root:[   77] Training loss: 0.05080515, Validation loss: 0.04544226, Gradient norm: 2.17689246
INFO:root:[   78] Training loss: 0.04993982, Validation loss: 0.04569766, Gradient norm: 2.13447177
INFO:root:[   79] Training loss: 0.04846453, Validation loss: 0.04698114, Gradient norm: 1.93981496
INFO:root:[   80] Training loss: 0.04845988, Validation loss: 0.04890692, Gradient norm: 2.10579075
INFO:root:[   81] Training loss: 0.04834074, Validation loss: 0.05082601, Gradient norm: 2.15477610
INFO:root:[   82] Training loss: 0.04794406, Validation loss: 0.04858896, Gradient norm: 2.05606245
INFO:root:[   83] Training loss: 0.04655062, Validation loss: 0.04791210, Gradient norm: 1.99138688
INFO:root:[   84] Training loss: 0.04649226, Validation loss: 0.04265259, Gradient norm: 2.06250797
INFO:root:[   85] Training loss: 0.04553623, Validation loss: 0.04856755, Gradient norm: 2.13572652
INFO:root:[   86] Training loss: 0.04573459, Validation loss: 0.04665716, Gradient norm: 2.13139624
INFO:root:[   87] Training loss: 0.04538145, Validation loss: 0.04539048, Gradient norm: 2.10000961
INFO:root:[   88] Training loss: 0.04420663, Validation loss: 0.04043191, Gradient norm: 2.04652156
INFO:root:[   89] Training loss: 0.04309386, Validation loss: 0.04510603, Gradient norm: 2.02572359
INFO:root:[   90] Training loss: 0.04329570, Validation loss: 0.04521587, Gradient norm: 2.19749929
INFO:root:[   91] Training loss: 0.04278586, Validation loss: 0.04513312, Gradient norm: 2.05475593
INFO:root:[   92] Training loss: 0.04218568, Validation loss: 0.04001376, Gradient norm: 2.02394490
INFO:root:[   93] Training loss: 0.04185814, Validation loss: 0.04091340, Gradient norm: 2.09166648
INFO:root:[   94] Training loss: 0.04119315, Validation loss: 0.03690559, Gradient norm: 2.06758606
INFO:root:[   95] Training loss: 0.04052947, Validation loss: 0.03749529, Gradient norm: 2.08117600
INFO:root:[   96] Training loss: 0.04075960, Validation loss: 0.03854573, Gradient norm: 2.16474796
INFO:root:[   97] Training loss: 0.04050997, Validation loss: 0.04223910, Gradient norm: 2.09122987
INFO:root:[   98] Training loss: 0.03913698, Validation loss: 0.04229174, Gradient norm: 2.03550594
INFO:root:[   99] Training loss: 0.03968055, Validation loss: 0.03563257, Gradient norm: 2.15804126
INFO:root:[  100] Training loss: 0.03930964, Validation loss: 0.04090792, Gradient norm: 2.13837505
INFO:root:[  101] Training loss: 0.03858607, Validation loss: 0.03384412, Gradient norm: 2.07400626
INFO:root:[  102] Training loss: 0.03799281, Validation loss: 0.03818748, Gradient norm: 2.09685134
INFO:root:[  103] Training loss: 0.03681492, Validation loss: 0.03440507, Gradient norm: 1.99083878
INFO:root:[  104] Training loss: 0.03749951, Validation loss: 0.03676893, Gradient norm: 2.03347516
INFO:root:[  105] Training loss: 0.03635410, Validation loss: 0.03483054, Gradient norm: 2.02383837
INFO:root:[  106] Training loss: 0.03601831, Validation loss: 0.03290858, Gradient norm: 2.05859144
INFO:root:[  107] Training loss: 0.03568197, Validation loss: 0.03761815, Gradient norm: 1.97169412
INFO:root:[  108] Training loss: 0.03502723, Validation loss: 0.03419600, Gradient norm: 1.81910072
INFO:root:[  109] Training loss: 0.03607078, Validation loss: 0.03234674, Gradient norm: 1.95472788
INFO:root:[  110] Training loss: 0.03507976, Validation loss: 0.03045617, Gradient norm: 1.86825278
INFO:root:[  111] Training loss: 0.03450448, Validation loss: 0.02926672, Gradient norm: 2.12768537
INFO:root:[  112] Training loss: 0.03346333, Validation loss: 0.03634809, Gradient norm: 1.96904960
INFO:root:[  113] Training loss: 0.03415148, Validation loss: 0.03547630, Gradient norm: 2.03549138
INFO:root:[  114] Training loss: 0.03383862, Validation loss: 0.03307477, Gradient norm: 1.94278233
INFO:root:[  115] Training loss: 0.03298247, Validation loss: 0.02899189, Gradient norm: 1.88712619
INFO:root:[  116] Training loss: 0.03220318, Validation loss: 0.03289997, Gradient norm: 1.90963917
INFO:root:[  117] Training loss: 0.03249433, Validation loss: 0.03570795, Gradient norm: 1.92842606
INFO:root:[  118] Training loss: 0.03215309, Validation loss: 0.03477452, Gradient norm: 2.01055064
INFO:root:[  119] Training loss: 0.03149192, Validation loss: 0.03421489, Gradient norm: 2.00052117
INFO:root:[  120] Training loss: 0.03076230, Validation loss: 0.03193484, Gradient norm: 1.89292303
INFO:root:[  121] Training loss: 0.03058579, Validation loss: 0.02921895, Gradient norm: 1.77737706
INFO:root:[  122] Training loss: 0.03054921, Validation loss: 0.02589758, Gradient norm: 1.88369845
INFO:root:[  123] Training loss: 0.03074784, Validation loss: 0.03468203, Gradient norm: 1.89295659
INFO:root:[  124] Training loss: 0.03069895, Validation loss: 0.03265469, Gradient norm: 1.83660171
INFO:root:[  125] Training loss: 0.03039876, Validation loss: 0.03384120, Gradient norm: 1.74284450
INFO:root:[  126] Training loss: 0.03043586, Validation loss: 0.02662974, Gradient norm: 1.85831682
INFO:root:[  127] Training loss: 0.02891732, Validation loss: 0.02711344, Gradient norm: 1.71439380
INFO:root:[  128] Training loss: 0.02966584, Validation loss: 0.03407038, Gradient norm: 1.88211414
INFO:root:[  129] Training loss: 0.02942903, Validation loss: 0.02803156, Gradient norm: 1.87752830
INFO:root:[  130] Training loss: 0.02839999, Validation loss: 0.02924297, Gradient norm: 1.80090738
INFO:root:[  131] Training loss: 0.02906568, Validation loss: 0.02863199, Gradient norm: 1.76593269
INFO:root:EP 131: Early stopping
INFO:root:Training the model took 1684.529s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01083
INFO:root:EnergyScoreTrain: 0.00574
INFO:root:CoverageTrain: 0.99951
INFO:root:IntervalWidthTrain: 0.02455
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01055
INFO:root:EnergyScoreValidation: 0.00551
INFO:root:CoverageValidation: 0.99955
INFO:root:IntervalWidthValidation: 0.02434
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01738
INFO:root:EnergyScoreTest: 0.00945
INFO:root:CoverageTest: 0.99455
INFO:root:IntervalWidthTest: 0.0245
INFO:root:###22 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.83637586, Validation loss: 0.25916472, Gradient norm: 3.24507005
INFO:root:[    2] Training loss: 0.22636609, Validation loss: 0.23212731, Gradient norm: 1.07604777
INFO:root:[    3] Training loss: 0.20003695, Validation loss: 0.18147111, Gradient norm: 1.48024259
INFO:root:[    4] Training loss: 0.18084361, Validation loss: 0.17374946, Gradient norm: 0.92618691
INFO:root:[    5] Training loss: 0.16765508, Validation loss: 0.16114303, Gradient norm: 1.22271663
INFO:root:[    6] Training loss: 0.16100890, Validation loss: 0.15093617, Gradient norm: 1.50557751
INFO:root:[    7] Training loss: 0.15293068, Validation loss: 0.15185924, Gradient norm: 1.27800287
INFO:root:[    8] Training loss: 0.15064859, Validation loss: 0.14551097, Gradient norm: 1.47420855
INFO:root:[    9] Training loss: 0.14416886, Validation loss: 0.14348473, Gradient norm: 1.51265389
INFO:root:[   10] Training loss: 0.14205203, Validation loss: 0.14114703, Gradient norm: 1.66597586
INFO:root:[   11] Training loss: 0.13922242, Validation loss: 0.13154235, Gradient norm: 1.70734644
INFO:root:[   12] Training loss: 0.13967641, Validation loss: 0.13900457, Gradient norm: 1.79646311
INFO:root:[   13] Training loss: 0.13688917, Validation loss: 0.13049610, Gradient norm: 1.90947506
INFO:root:[   14] Training loss: 0.13119673, Validation loss: 0.12357796, Gradient norm: 1.65422465
INFO:root:[   15] Training loss: 0.12751536, Validation loss: 0.13098114, Gradient norm: 1.39823327
INFO:root:[   16] Training loss: 0.12586774, Validation loss: 0.12806044, Gradient norm: 1.39085713
INFO:root:[   17] Training loss: 0.12527087, Validation loss: 0.11847203, Gradient norm: 1.61396930
INFO:root:[   18] Training loss: 0.12371465, Validation loss: 0.12500230, Gradient norm: 1.53860105
INFO:root:[   19] Training loss: 0.12048263, Validation loss: 0.11447307, Gradient norm: 1.52078326
INFO:root:[   20] Training loss: 0.11760391, Validation loss: 0.11154063, Gradient norm: 1.43602175
INFO:root:[   21] Training loss: 0.11729547, Validation loss: 0.11551424, Gradient norm: 1.61493581
INFO:root:[   22] Training loss: 0.11580795, Validation loss: 0.11136806, Gradient norm: 1.67520506
INFO:root:[   23] Training loss: 0.11386447, Validation loss: 0.11130424, Gradient norm: 1.37032478
INFO:root:[   24] Training loss: 0.11414581, Validation loss: 0.10687719, Gradient norm: 1.59825672
INFO:root:[   25] Training loss: 0.10905882, Validation loss: 0.10316688, Gradient norm: 1.43239147
INFO:root:[   26] Training loss: 0.10907784, Validation loss: 0.11569133, Gradient norm: 1.68336853
INFO:root:[   27] Training loss: 0.10915031, Validation loss: 0.10284163, Gradient norm: 1.68731394
INFO:root:[   28] Training loss: 0.10622827, Validation loss: 0.10638055, Gradient norm: 1.61432268
INFO:root:[   29] Training loss: 0.10472525, Validation loss: 0.10836050, Gradient norm: 1.75014412
INFO:root:[   30] Training loss: 0.10196686, Validation loss: 0.10511255, Gradient norm: 1.53497383
INFO:root:[   31] Training loss: 0.10093256, Validation loss: 0.09993385, Gradient norm: 1.55126241
INFO:root:[   32] Training loss: 0.09888769, Validation loss: 0.09485568, Gradient norm: 1.57145767
INFO:root:[   33] Training loss: 0.09778496, Validation loss: 0.09310551, Gradient norm: 1.59370083
INFO:root:[   34] Training loss: 0.09735681, Validation loss: 0.09074323, Gradient norm: 1.60059724
INFO:root:[   35] Training loss: 0.09478003, Validation loss: 0.09546422, Gradient norm: 1.64511521
INFO:root:[   36] Training loss: 0.09406486, Validation loss: 0.09424355, Gradient norm: 1.63691914
INFO:root:[   37] Training loss: 0.09264598, Validation loss: 0.09797851, Gradient norm: 1.63189595
INFO:root:[   38] Training loss: 0.09178386, Validation loss: 0.09077454, Gradient norm: 1.67334759
INFO:root:[   39] Training loss: 0.08922998, Validation loss: 0.08585450, Gradient norm: 1.68377712
INFO:root:[   40] Training loss: 0.08825192, Validation loss: 0.08956391, Gradient norm: 1.76209504
INFO:root:[   41] Training loss: 0.08707707, Validation loss: 0.08030599, Gradient norm: 1.78445741
INFO:root:[   42] Training loss: 0.08426700, Validation loss: 0.08623459, Gradient norm: 1.63097690
INFO:root:[   43] Training loss: 0.08459614, Validation loss: 0.08496218, Gradient norm: 1.75653357
INFO:root:[   44] Training loss: 0.08387490, Validation loss: 0.07851169, Gradient norm: 1.73523964
INFO:root:[   45] Training loss: 0.08127149, Validation loss: 0.07566328, Gradient norm: 1.62417266
INFO:root:[   46] Training loss: 0.07831681, Validation loss: 0.07933867, Gradient norm: 1.48691450
INFO:root:[   47] Training loss: 0.07854354, Validation loss: 0.08212538, Gradient norm: 1.71949848
INFO:root:[   48] Training loss: 0.07788336, Validation loss: 0.07147489, Gradient norm: 1.79724539
INFO:root:[   49] Training loss: 0.07970131, Validation loss: 0.07948158, Gradient norm: 2.12597013
INFO:root:[   50] Training loss: 0.07432642, Validation loss: 0.07150522, Gradient norm: 1.67229840
INFO:root:[   51] Training loss: 0.07198692, Validation loss: 0.07701114, Gradient norm: 1.43462411
INFO:root:[   52] Training loss: 0.07680599, Validation loss: 0.07965480, Gradient norm: 1.82273275
INFO:root:[   53] Training loss: 0.07079225, Validation loss: 0.07002173, Gradient norm: 1.73161829
INFO:root:[   54] Training loss: 0.06900976, Validation loss: 0.06929835, Gradient norm: 1.63683104
INFO:root:[   55] Training loss: 0.06722170, Validation loss: 0.06476211, Gradient norm: 1.56009532
INFO:root:[   56] Training loss: 0.06842562, Validation loss: 0.06230420, Gradient norm: 1.74440416
INFO:root:[   57] Training loss: 0.06569759, Validation loss: 0.06567010, Gradient norm: 1.68716089
INFO:root:[   58] Training loss: 0.06638526, Validation loss: 0.06951300, Gradient norm: 1.78810951
INFO:root:[   59] Training loss: 0.06384104, Validation loss: 0.06434694, Gradient norm: 1.70832567
INFO:root:[   60] Training loss: 0.06377110, Validation loss: 0.06372471, Gradient norm: 1.84502119
INFO:root:[   61] Training loss: 0.06149937, Validation loss: 0.05697633, Gradient norm: 1.61047555
INFO:root:[   62] Training loss: 0.06471573, Validation loss: 0.05984639, Gradient norm: 1.87308786
INFO:root:[   63] Training loss: 0.06165612, Validation loss: 0.06358055, Gradient norm: 1.83879800
INFO:root:[   64] Training loss: 0.06106838, Validation loss: 0.05418193, Gradient norm: 1.91189716
INFO:root:[   65] Training loss: 0.05905015, Validation loss: 0.06198983, Gradient norm: 1.81446027
INFO:root:[   66] Training loss: 0.05591627, Validation loss: 0.05295558, Gradient norm: 1.55545518
INFO:root:[   67] Training loss: 0.05815953, Validation loss: 0.05145849, Gradient norm: 1.81401614
INFO:root:[   68] Training loss: 0.05842731, Validation loss: 0.05411333, Gradient norm: 2.05265127
INFO:root:[   69] Training loss: 0.05575443, Validation loss: 0.05703598, Gradient norm: 1.82619957
INFO:root:[   70] Training loss: 0.05465042, Validation loss: 0.05643511, Gradient norm: 1.73493531
INFO:root:[   71] Training loss: 0.05605402, Validation loss: 0.05997320, Gradient norm: 1.80601440
INFO:root:[   72] Training loss: 0.05640616, Validation loss: 0.04722033, Gradient norm: 2.11610896
INFO:root:[   73] Training loss: 0.05206200, Validation loss: 0.05387478, Gradient norm: 1.70865025
INFO:root:[   74] Training loss: 0.04976466, Validation loss: 0.05168496, Gradient norm: 1.63311058
INFO:root:[   75] Training loss: 0.04903042, Validation loss: 0.04478535, Gradient norm: 1.60953571
INFO:root:[   76] Training loss: 0.05048286, Validation loss: 0.05243789, Gradient norm: 1.78570863
INFO:root:[   77] Training loss: 0.04818491, Validation loss: 0.04935589, Gradient norm: 1.66586922
INFO:root:[   78] Training loss: 0.04697689, Validation loss: 0.04227179, Gradient norm: 1.70800243
INFO:root:[   79] Training loss: 0.04666325, Validation loss: 0.04156528, Gradient norm: 1.73915727
INFO:root:[   80] Training loss: 0.04677097, Validation loss: 0.05149015, Gradient norm: 1.74854081
INFO:root:[   81] Training loss: 0.04982804, Validation loss: 0.04341208, Gradient norm: 2.31688249
INFO:root:[   82] Training loss: 0.04942035, Validation loss: 0.04406172, Gradient norm: 2.15901909
INFO:root:[   83] Training loss: 0.04520024, Validation loss: 0.04399903, Gradient norm: 1.79927294
INFO:root:[   84] Training loss: 0.04369571, Validation loss: 0.04127056, Gradient norm: 1.61496795
INFO:root:[   85] Training loss: 0.04381209, Validation loss: 0.04583230, Gradient norm: 1.57578445
INFO:root:[   86] Training loss: 0.04449468, Validation loss: 0.04728406, Gradient norm: 1.81538922
INFO:root:[   87] Training loss: 0.04199070, Validation loss: 0.04173856, Gradient norm: 1.66942887
INFO:root:[   88] Training loss: 0.04082152, Validation loss: 0.03864262, Gradient norm: 1.68380715
INFO:root:[   89] Training loss: 0.04060035, Validation loss: 0.04568514, Gradient norm: 1.68290102
INFO:root:[   90] Training loss: 0.04302481, Validation loss: 0.04109299, Gradient norm: 2.04955716
INFO:root:[   91] Training loss: 0.04222264, Validation loss: 0.03571930, Gradient norm: 1.70929185
INFO:root:[   92] Training loss: 0.04123791, Validation loss: 0.03639125, Gradient norm: 1.86397241
INFO:root:[   93] Training loss: 0.04020158, Validation loss: 0.04615925, Gradient norm: 1.82538222
INFO:root:[   94] Training loss: 0.03802390, Validation loss: 0.03517608, Gradient norm: 1.54124015
INFO:root:[   95] Training loss: 0.03804101, Validation loss: 0.04522301, Gradient norm: 1.54389057
INFO:root:[   96] Training loss: 0.03841672, Validation loss: 0.03989084, Gradient norm: 1.80243687
INFO:root:[   97] Training loss: 0.03817065, Validation loss: 0.03824044, Gradient norm: 1.74316317
INFO:root:[   98] Training loss: 0.03818889, Validation loss: 0.03237809, Gradient norm: 1.85997118
INFO:root:[   99] Training loss: 0.03763943, Validation loss: 0.03412446, Gradient norm: 1.53924824
INFO:root:[  100] Training loss: 0.03735242, Validation loss: 0.03240116, Gradient norm: 1.78476764
INFO:root:[  101] Training loss: 0.03574588, Validation loss: 0.03189148, Gradient norm: 1.55810480
INFO:root:[  102] Training loss: 0.03602987, Validation loss: 0.03163961, Gradient norm: 1.64201963
INFO:root:[  103] Training loss: 0.03580863, Validation loss: 0.03710642, Gradient norm: 1.62424436
INFO:root:[  104] Training loss: 0.03627731, Validation loss: 0.04280295, Gradient norm: 1.65264890
INFO:root:[  105] Training loss: 0.03601199, Validation loss: 0.04026984, Gradient norm: 1.85639322
INFO:root:[  106] Training loss: 0.03460244, Validation loss: 0.03219945, Gradient norm: 1.59610311
INFO:root:[  107] Training loss: 0.03526648, Validation loss: 0.03394643, Gradient norm: 1.69023151
INFO:root:[  108] Training loss: 0.03476807, Validation loss: 0.03033926, Gradient norm: 1.72647941
INFO:root:[  109] Training loss: 0.03266170, Validation loss: 0.02924394, Gradient norm: 1.48941908
INFO:root:[  110] Training loss: 0.03382546, Validation loss: 0.03377885, Gradient norm: 1.52454851
INFO:root:[  111] Training loss: 0.03389478, Validation loss: 0.03148448, Gradient norm: 1.48024855
INFO:root:[  112] Training loss: 0.03302323, Validation loss: 0.03265926, Gradient norm: 1.36610946
INFO:root:[  113] Training loss: 0.03251519, Validation loss: 0.03461139, Gradient norm: 1.46822572
INFO:root:[  114] Training loss: 0.03244814, Validation loss: 0.03024814, Gradient norm: 1.43978749
INFO:root:[  115] Training loss: 0.03238638, Validation loss: 0.03479344, Gradient norm: 1.31033771
INFO:root:[  116] Training loss: 0.03308363, Validation loss: 0.03358210, Gradient norm: 1.56472981
INFO:root:[  117] Training loss: 0.03325417, Validation loss: 0.03024604, Gradient norm: 1.35749204
INFO:root:[  118] Training loss: 0.03140612, Validation loss: 0.03537381, Gradient norm: 1.31408601
INFO:root:EP 118: Early stopping
INFO:root:Training the model took 1519.245s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01516
INFO:root:EnergyScoreTrain: 0.0148
INFO:root:CoverageTrain: 0.99908
INFO:root:IntervalWidthTrain: 0.02551
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01505
INFO:root:EnergyScoreValidation: 0.01493
INFO:root:CoverageValidation: 0.99911
INFO:root:IntervalWidthValidation: 0.02516
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.01919
INFO:root:EnergyScoreTest: 0.01752
INFO:root:CoverageTest: 0.99319
INFO:root:IntervalWidthTest: 0.0256
INFO:root:###23 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.77585247, Validation loss: 0.29776508, Gradient norm: 3.09507325
INFO:root:[    2] Training loss: 0.26322921, Validation loss: 0.24262942, Gradient norm: 1.23438162
INFO:root:[    3] Training loss: 0.22886604, Validation loss: 0.22709172, Gradient norm: 1.10547825
INFO:root:[    4] Training loss: 0.20982275, Validation loss: 0.19748662, Gradient norm: 1.25367828
INFO:root:[    5] Training loss: 0.19362284, Validation loss: 0.18423172, Gradient norm: 1.15512112
INFO:root:[    6] Training loss: 0.18523765, Validation loss: 0.17600850, Gradient norm: 1.22645746
INFO:root:[    7] Training loss: 0.17547029, Validation loss: 0.16982876, Gradient norm: 1.11502156
INFO:root:[    8] Training loss: 0.17236645, Validation loss: 0.16473540, Gradient norm: 1.54075178
INFO:root:[    9] Training loss: 0.16809729, Validation loss: 0.17026797, Gradient norm: 1.59413360
INFO:root:[   10] Training loss: 0.16296653, Validation loss: 0.15413730, Gradient norm: 1.52373291
INFO:root:[   11] Training loss: 0.15864457, Validation loss: 0.15510044, Gradient norm: 1.51806140
INFO:root:[   12] Training loss: 0.15621479, Validation loss: 0.15279065, Gradient norm: 1.55860917
INFO:root:[   13] Training loss: 0.15214326, Validation loss: 0.15420022, Gradient norm: 1.38344765
INFO:root:[   14] Training loss: 0.14828121, Validation loss: 0.14359128, Gradient norm: 1.42892528
INFO:root:[   15] Training loss: 0.14524124, Validation loss: 0.14393003, Gradient norm: 1.43087950
INFO:root:[   16] Training loss: 0.14274160, Validation loss: 0.13670245, Gradient norm: 1.44233371
INFO:root:[   17] Training loss: 0.13905683, Validation loss: 0.13277796, Gradient norm: 1.40672920
INFO:root:[   18] Training loss: 0.13822633, Validation loss: 0.13121688, Gradient norm: 1.62455600
INFO:root:[   19] Training loss: 0.13515776, Validation loss: 0.12965724, Gradient norm: 1.45023391
INFO:root:[   20] Training loss: 0.13225250, Validation loss: 0.13060010, Gradient norm: 1.41287083
INFO:root:[   21] Training loss: 0.12971526, Validation loss: 0.13285620, Gradient norm: 1.52113229
INFO:root:[   22] Training loss: 0.12702916, Validation loss: 0.12017818, Gradient norm: 1.68892796
INFO:root:[   23] Training loss: 0.12405572, Validation loss: 0.12695921, Gradient norm: 1.58548756
INFO:root:[   24] Training loss: 0.12159570, Validation loss: 0.12631632, Gradient norm: 1.56481630
INFO:root:[   25] Training loss: 0.12123272, Validation loss: 0.11484750, Gradient norm: 1.79368506
INFO:root:[   26] Training loss: 0.11879438, Validation loss: 0.11166914, Gradient norm: 1.68423109
INFO:root:[   27] Training loss: 0.11427368, Validation loss: 0.11946961, Gradient norm: 1.73673470
INFO:root:[   28] Training loss: 0.11086061, Validation loss: 0.11423736, Gradient norm: 1.60389309
INFO:root:[   29] Training loss: 0.11032197, Validation loss: 0.10486692, Gradient norm: 1.84244013
INFO:root:[   30] Training loss: 0.10602784, Validation loss: 0.10262537, Gradient norm: 1.53270750
INFO:root:[   31] Training loss: 0.10449971, Validation loss: 0.10320596, Gradient norm: 1.75259887
INFO:root:[   32] Training loss: 0.10290208, Validation loss: 0.10306400, Gradient norm: 1.73363570
INFO:root:[   33] Training loss: 0.10059027, Validation loss: 0.10154806, Gradient norm: 1.80796600
INFO:root:[   34] Training loss: 0.09751582, Validation loss: 0.09691058, Gradient norm: 1.80172215
INFO:root:[   35] Training loss: 0.09473608, Validation loss: 0.10043340, Gradient norm: 1.65853177
INFO:root:[   36] Training loss: 0.09455766, Validation loss: 0.10105450, Gradient norm: 1.83655334
INFO:root:[   37] Training loss: 0.09271770, Validation loss: 0.08500026, Gradient norm: 1.95170636
INFO:root:[   38] Training loss: 0.08889864, Validation loss: 0.08303189, Gradient norm: 1.77569911
INFO:root:[   39] Training loss: 0.08577019, Validation loss: 0.08805024, Gradient norm: 1.55467387
INFO:root:[   40] Training loss: 0.08464697, Validation loss: 0.07890493, Gradient norm: 1.71904400
INFO:root:[   41] Training loss: 0.08241921, Validation loss: 0.08383536, Gradient norm: 1.72171338
INFO:root:[   42] Training loss: 0.08057544, Validation loss: 0.08441666, Gradient norm: 1.64644343
INFO:root:[   43] Training loss: 0.08010374, Validation loss: 0.08161043, Gradient norm: 1.84053461
INFO:root:[   44] Training loss: 0.07786069, Validation loss: 0.07125401, Gradient norm: 1.89600987
INFO:root:[   45] Training loss: 0.07525043, Validation loss: 0.07911412, Gradient norm: 1.74000906
INFO:root:[   46] Training loss: 0.07481714, Validation loss: 0.06890421, Gradient norm: 1.87411583
INFO:root:[   47] Training loss: 0.07349656, Validation loss: 0.07179151, Gradient norm: 1.87021766
INFO:root:[   48] Training loss: 0.07097552, Validation loss: 0.07190993, Gradient norm: 1.82337013
INFO:root:[   49] Training loss: 0.07020892, Validation loss: 0.06331717, Gradient norm: 1.95630191
INFO:root:[   50] Training loss: 0.06793090, Validation loss: 0.06421584, Gradient norm: 1.86636374
INFO:root:[   51] Training loss: 0.06643521, Validation loss: 0.06415171, Gradient norm: 1.74745268
INFO:root:[   52] Training loss: 0.06383117, Validation loss: 0.06542596, Gradient norm: 1.61616488
INFO:root:[   53] Training loss: 0.06366790, Validation loss: 0.05889970, Gradient norm: 1.89967577
INFO:root:[   54] Training loss: 0.06277362, Validation loss: 0.05745659, Gradient norm: 1.91265958
INFO:root:[   55] Training loss: 0.05990811, Validation loss: 0.05674661, Gradient norm: 1.76864402
INFO:root:[   56] Training loss: 0.05807775, Validation loss: 0.05425612, Gradient norm: 1.67410064
INFO:root:[   57] Training loss: 0.05721023, Validation loss: 0.05513100, Gradient norm: 1.61539580
INFO:root:[   58] Training loss: 0.05743461, Validation loss: 0.05285692, Gradient norm: 1.81735312
INFO:root:[   59] Training loss: 0.05592597, Validation loss: 0.06139058, Gradient norm: 1.81237685
INFO:root:[   60] Training loss: 0.05408427, Validation loss: 0.05684429, Gradient norm: 1.76317458
INFO:root:[   61] Training loss: 0.05299108, Validation loss: 0.05802320, Gradient norm: 1.69983844
INFO:root:[   62] Training loss: 0.05293988, Validation loss: 0.05638369, Gradient norm: 1.92842789
INFO:root:[   63] Training loss: 0.05053265, Validation loss: 0.05057632, Gradient norm: 1.77451574
INFO:root:[   64] Training loss: 0.04996148, Validation loss: 0.05223915, Gradient norm: 1.74140059
INFO:root:[   65] Training loss: 0.04928786, Validation loss: 0.04571835, Gradient norm: 1.88222045
INFO:root:[   66] Training loss: 0.04812804, Validation loss: 0.05142857, Gradient norm: 1.78241648
INFO:root:[   67] Training loss: 0.04716930, Validation loss: 0.04338447, Gradient norm: 1.85082051
INFO:root:[   68] Training loss: 0.04653332, Validation loss: 0.04608479, Gradient norm: 1.74119767
INFO:root:[   69] Training loss: 0.04617597, Validation loss: 0.04090796, Gradient norm: 1.84746819
INFO:root:[   70] Training loss: 0.04682524, Validation loss: 0.04986072, Gradient norm: 1.91175356
INFO:root:[   71] Training loss: 0.04583955, Validation loss: 0.04516237, Gradient norm: 1.76777016
INFO:root:[   72] Training loss: 0.04476322, Validation loss: 0.04804315, Gradient norm: 1.68951105
INFO:root:[   73] Training loss: 0.04276889, Validation loss: 0.03944818, Gradient norm: 1.73308882
INFO:root:[   74] Training loss: 0.04363319, Validation loss: 0.04182980, Gradient norm: 1.79469553
INFO:root:[   75] Training loss: 0.04337853, Validation loss: 0.04107058, Gradient norm: 1.68425199
INFO:root:[   76] Training loss: 0.04220523, Validation loss: 0.04200233, Gradient norm: 1.69167452
INFO:root:[   77] Training loss: 0.04046046, Validation loss: 0.03614611, Gradient norm: 1.58920630
INFO:root:[   78] Training loss: 0.04071546, Validation loss: 0.04356278, Gradient norm: 1.66304983
INFO:root:[   79] Training loss: 0.04228461, Validation loss: 0.04063778, Gradient norm: 1.85362787
INFO:root:[   80] Training loss: 0.04004597, Validation loss: 0.04422229, Gradient norm: 1.65470446
INFO:root:[   81] Training loss: 0.04120032, Validation loss: 0.03682561, Gradient norm: 1.72216667
INFO:root:[   82] Training loss: 0.04007628, Validation loss: 0.04021827, Gradient norm: 1.67804492
INFO:root:[   83] Training loss: 0.03939317, Validation loss: 0.03777410, Gradient norm: 1.63598643
INFO:root:[   84] Training loss: 0.03791471, Validation loss: 0.03414969, Gradient norm: 1.55823694
INFO:root:[   85] Training loss: 0.04045218, Validation loss: 0.03621762, Gradient norm: 1.68690936
INFO:root:[   86] Training loss: 0.04022096, Validation loss: 0.04434905, Gradient norm: 1.80209011
INFO:root:[   87] Training loss: 0.03929795, Validation loss: 0.03432110, Gradient norm: 1.60329698
INFO:root:[   88] Training loss: 0.03798990, Validation loss: 0.04391923, Gradient norm: 1.46982108
INFO:root:[   89] Training loss: 0.03900904, Validation loss: 0.03934367, Gradient norm: 1.52007270
INFO:root:[   90] Training loss: 0.03952451, Validation loss: 0.03907317, Gradient norm: 1.66559318
INFO:root:[   91] Training loss: 0.03944505, Validation loss: 0.03827612, Gradient norm: 1.69416087
INFO:root:[   92] Training loss: 0.03821594, Validation loss: 0.03809982, Gradient norm: 1.56042034
INFO:root:[   93] Training loss: 0.03755011, Validation loss: 0.03468914, Gradient norm: 1.56366887
INFO:root:EP 93: Early stopping
INFO:root:Training the model took 1201.515s.
INFO:root:Emptying the cuda cache took 0.036s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.01695
INFO:root:EnergyScoreTrain: 0.01062
INFO:root:CoverageTrain: 0.99934
INFO:root:IntervalWidthTrain: 0.02882
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.01643
INFO:root:EnergyScoreValidation: 0.01023
INFO:root:CoverageValidation: 0.99939
INFO:root:IntervalWidthValidation: 0.02848
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02511
INFO:root:EnergyScoreTest: 0.0152
INFO:root:CoverageTest: 0.99314
INFO:root:IntervalWidthTest: 0.0286
INFO:root:###24 out of 24 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.82242450, Validation loss: 0.36699451, Gradient norm: 3.37576936
INFO:root:[    2] Training loss: 0.30693518, Validation loss: 0.27010823, Gradient norm: 0.85649482
INFO:root:[    3] Training loss: 0.25560228, Validation loss: 0.24001584, Gradient norm: 0.75909307
INFO:root:[    4] Training loss: 0.23788086, Validation loss: 0.22367980, Gradient norm: 1.45246644
INFO:root:[    5] Training loss: 0.22346995, Validation loss: 0.21144762, Gradient norm: 1.17840348
INFO:root:[    6] Training loss: 0.21022660, Validation loss: 0.20109274, Gradient norm: 1.16103173
INFO:root:[    7] Training loss: 0.20185063, Validation loss: 0.20328366, Gradient norm: 1.15149047
INFO:root:[    8] Training loss: 0.19391623, Validation loss: 0.18561114, Gradient norm: 1.09074482
INFO:root:[    9] Training loss: 0.18954760, Validation loss: 0.18756650, Gradient norm: 1.19246420
INFO:root:[   10] Training loss: 0.18484862, Validation loss: 0.18302976, Gradient norm: 1.38065008
INFO:root:[   11] Training loss: 0.18032449, Validation loss: 0.17350217, Gradient norm: 1.51463126
INFO:root:[   12] Training loss: 0.17533630, Validation loss: 0.16876387, Gradient norm: 1.60503213
INFO:root:[   13] Training loss: 0.16980202, Validation loss: 0.16283571, Gradient norm: 1.47624866
INFO:root:[   14] Training loss: 0.16480790, Validation loss: 0.17100368, Gradient norm: 1.43832894
INFO:root:[   15] Training loss: 0.16108156, Validation loss: 0.15911474, Gradient norm: 1.61089149
INFO:root:[   16] Training loss: 0.15345884, Validation loss: 0.15172408, Gradient norm: 1.30836314
INFO:root:[   17] Training loss: 0.14931616, Validation loss: 0.14407581, Gradient norm: 1.33773279
INFO:root:[   18] Training loss: 0.14503208, Validation loss: 0.13931011, Gradient norm: 1.35524547
INFO:root:[   19] Training loss: 0.14047849, Validation loss: 0.14501494, Gradient norm: 1.43513276
INFO:root:[   20] Training loss: 0.13970501, Validation loss: 0.13457208, Gradient norm: 1.93884413
INFO:root:[   21] Training loss: 0.13190928, Validation loss: 0.12652049, Gradient norm: 1.35409867
INFO:root:[   22] Training loss: 0.12722980, Validation loss: 0.13596956, Gradient norm: 1.41809959
INFO:root:[   23] Training loss: 0.12725829, Validation loss: 0.12767902, Gradient norm: 1.78202181
INFO:root:[   24] Training loss: 0.12021883, Validation loss: 0.11957891, Gradient norm: 1.59820448
INFO:root:[   25] Training loss: 0.11557233, Validation loss: 0.11671872, Gradient norm: 1.45839315
INFO:root:[   26] Training loss: 0.11268375, Validation loss: 0.10518109, Gradient norm: 1.55484640
INFO:root:[   27] Training loss: 0.10875673, Validation loss: 0.10232933, Gradient norm: 1.59112571
INFO:root:[   28] Training loss: 0.10564267, Validation loss: 0.10746457, Gradient norm: 1.56018168
INFO:root:[   29] Training loss: 0.10298743, Validation loss: 0.10028417, Gradient norm: 1.65283130
INFO:root:[   30] Training loss: 0.09864018, Validation loss: 0.09114420, Gradient norm: 1.68960923
INFO:root:[   31] Training loss: 0.09442468, Validation loss: 0.08998361, Gradient norm: 1.49704835
INFO:root:[   32] Training loss: 0.09142895, Validation loss: 0.08973644, Gradient norm: 1.53503810
INFO:root:[   33] Training loss: 0.08864943, Validation loss: 0.08399968, Gradient norm: 1.57301721
INFO:root:[   34] Training loss: 0.08667333, Validation loss: 0.08183300, Gradient norm: 1.72628192
INFO:root:[   35] Training loss: 0.08337580, Validation loss: 0.08504897, Gradient norm: 1.59320198
INFO:root:[   36] Training loss: 0.08157480, Validation loss: 0.08504292, Gradient norm: 1.76284573
INFO:root:[   37] Training loss: 0.07833140, Validation loss: 0.08079110, Gradient norm: 1.74390683
INFO:root:[   38] Training loss: 0.07600523, Validation loss: 0.07911243, Gradient norm: 1.73595914
INFO:root:[   39] Training loss: 0.07449022, Validation loss: 0.07409331, Gradient norm: 1.77654531
INFO:root:[   40] Training loss: 0.07412533, Validation loss: 0.07593555, Gradient norm: 1.89796513
INFO:root:[   41] Training loss: 0.07143541, Validation loss: 0.06997274, Gradient norm: 1.79155802
INFO:root:[   42] Training loss: 0.06912286, Validation loss: 0.06295272, Gradient norm: 1.79319372
INFO:root:[   43] Training loss: 0.06663357, Validation loss: 0.06684965, Gradient norm: 1.77757270
INFO:root:[   44] Training loss: 0.06452282, Validation loss: 0.05924891, Gradient norm: 1.63449875
INFO:root:[   45] Training loss: 0.06276782, Validation loss: 0.06045224, Gradient norm: 1.66536858
INFO:root:[   46] Training loss: 0.06323424, Validation loss: 0.05865030, Gradient norm: 1.91867729
INFO:root:[   47] Training loss: 0.06098612, Validation loss: 0.06299896, Gradient norm: 1.82665718
INFO:root:[   48] Training loss: 0.05990255, Validation loss: 0.05428861, Gradient norm: 1.82357824
INFO:root:[   49] Training loss: 0.05887339, Validation loss: 0.05728477, Gradient norm: 1.72125050
INFO:root:[   50] Training loss: 0.05765414, Validation loss: 0.05993274, Gradient norm: 1.70896522
INFO:root:[   51] Training loss: 0.05795543, Validation loss: 0.05613437, Gradient norm: 1.89027699
INFO:root:[   52] Training loss: 0.05861058, Validation loss: 0.05355263, Gradient norm: 1.97461639
INFO:root:[   53] Training loss: 0.05414195, Validation loss: 0.04986691, Gradient norm: 1.62958530
INFO:root:[   54] Training loss: 0.05580801, Validation loss: 0.04973664, Gradient norm: 1.87670657
INFO:root:[   55] Training loss: 0.05576659, Validation loss: 0.05843489, Gradient norm: 1.86642992
INFO:root:[   56] Training loss: 0.05602276, Validation loss: 0.05102794, Gradient norm: 2.02467050
INFO:root:[   57] Training loss: 0.05339526, Validation loss: 0.05358909, Gradient norm: 1.78369095
INFO:root:[   58] Training loss: 0.05298428, Validation loss: 0.05314110, Gradient norm: 2.06425453
INFO:root:[   59] Training loss: 0.05587788, Validation loss: 0.05385033, Gradient norm: 2.31317295
INFO:root:[   60] Training loss: 0.05537444, Validation loss: 0.04977602, Gradient norm: 2.16508280
INFO:root:[   61] Training loss: 0.05442492, Validation loss: 0.05819538, Gradient norm: 2.22080735
INFO:root:[   62] Training loss: 0.05446761, Validation loss: 0.05761961, Gradient norm: 2.22941727
INFO:root:[   63] Training loss: 0.05308921, Validation loss: 0.05019299, Gradient norm: 2.13573971
INFO:root:EP 63: Early stopping
INFO:root:Training the model took 817.883s.
INFO:root:Emptying the cuda cache took 0.036s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.02244
INFO:root:EnergyScoreTrain: 0.0183
INFO:root:CoverageTrain: 0.99985
INFO:root:IntervalWidthTrain: 0.04411
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.02214
INFO:root:EnergyScoreValidation: 0.01807
INFO:root:CoverageValidation: 0.99985
INFO:root:IntervalWidthValidation: 0.0435
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.02906
INFO:root:EnergyScoreTest: 0.02142
INFO:root:CoverageTest: 0.99735
INFO:root:IntervalWidthTest: 0.04331
