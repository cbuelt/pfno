INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file ks/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'KS', 'max_training_set_size': 10000, 'downscaling_factor': 1, 'temporal_downscaling': 2, 'init_steps': 20, 't_start': 0, 'pred_horizon': 20}
INFO:root:###1 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 2097152
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99398727, Validation loss: 0.98848113, Gradient norm: 0.02319189
INFO:root:[    2] Training loss: 0.97858290, Validation loss: 0.96840295, Gradient norm: 0.09880822
INFO:root:[    3] Training loss: 0.95777744, Validation loss: 0.94640321, Gradient norm: 0.16065255
INFO:root:[    4] Training loss: 0.93893651, Validation loss: 0.92997267, Gradient norm: 0.17028342
INFO:root:[    5] Training loss: 0.92571058, Validation loss: 0.92142230, Gradient norm: 0.24593804
INFO:root:[    6] Training loss: 0.91439383, Validation loss: 0.91001956, Gradient norm: 0.26527513
INFO:root:[    7] Training loss: 0.90627426, Validation loss: 0.90258947, Gradient norm: 0.21805860
INFO:root:[    8] Training loss: 0.90042152, Validation loss: 0.90040742, Gradient norm: 0.26437901
INFO:root:[    9] Training loss: 0.89546302, Validation loss: 0.89501867, Gradient norm: 0.25948355
INFO:root:[   10] Training loss: 0.89135775, Validation loss: 0.89479855, Gradient norm: 0.24962021
INFO:root:[   11] Training loss: 0.88822148, Validation loss: 0.88884707, Gradient norm: 0.23988767
INFO:root:[   12] Training loss: 0.88446929, Validation loss: 0.89312271, Gradient norm: 0.25317375
INFO:root:[   13] Training loss: 0.88267741, Validation loss: 0.88632927, Gradient norm: 0.28143533
INFO:root:[   14] Training loss: 0.87813707, Validation loss: 0.88412009, Gradient norm: 0.20502607
INFO:root:[   15] Training loss: 0.87707021, Validation loss: 0.88149639, Gradient norm: 0.28278991
INFO:root:[   16] Training loss: 0.87345713, Validation loss: 0.87790485, Gradient norm: 0.24744656
INFO:root:[   17] Training loss: 0.87185388, Validation loss: 0.87950762, Gradient norm: 0.27376742
INFO:root:[   18] Training loss: 0.87000357, Validation loss: 0.87793821, Gradient norm: 0.29179555
INFO:root:[   19] Training loss: 0.86805495, Validation loss: 0.87540511, Gradient norm: 0.25132757
INFO:root:[   20] Training loss: 0.86437786, Validation loss: 0.87378503, Gradient norm: 0.15215745
INFO:root:[   21] Training loss: 0.86495472, Validation loss: 0.87283548, Gradient norm: 0.28676077
INFO:root:[   22] Training loss: 0.86354446, Validation loss: 0.87514118, Gradient norm: 0.33778910
INFO:root:[   23] Training loss: 0.86118600, Validation loss: 0.87130177, Gradient norm: 0.24473857
INFO:root:[   24] Training loss: 0.85908967, Validation loss: 0.87309617, Gradient norm: 0.27867994
INFO:root:[   25] Training loss: 0.85841365, Validation loss: 0.87053629, Gradient norm: 0.33666207
INFO:root:[   26] Training loss: 0.85641869, Validation loss: 0.86907015, Gradient norm: 0.25537853
INFO:root:[   27] Training loss: 0.85533319, Validation loss: 0.86879895, Gradient norm: 0.30985959
INFO:root:[   28] Training loss: 0.85291573, Validation loss: 0.86846558, Gradient norm: 0.20011102
INFO:root:[   29] Training loss: 0.85224917, Validation loss: 0.86862101, Gradient norm: 0.29034151
INFO:root:[   30] Training loss: 0.85145506, Validation loss: 0.86587566, Gradient norm: 0.31023679
INFO:root:[   31] Training loss: 0.84974482, Validation loss: 0.86624483, Gradient norm: 0.27744777
INFO:root:[   32] Training loss: 0.84890512, Validation loss: 0.86754282, Gradient norm: 0.29702403
INFO:root:[   33] Training loss: 0.84728524, Validation loss: 0.86579354, Gradient norm: 0.29135480
INFO:root:[   34] Training loss: 0.84630049, Validation loss: 0.86798452, Gradient norm: 0.29537790
INFO:root:[   35] Training loss: 0.84498575, Validation loss: 0.86747519, Gradient norm: 0.27318275
INFO:root:[   36] Training loss: 0.84459529, Validation loss: 0.86757717, Gradient norm: 0.31105518
INFO:root:[   37] Training loss: 0.84320684, Validation loss: 0.86740306, Gradient norm: 0.32912740
INFO:root:[   38] Training loss: 0.84143811, Validation loss: 0.86298852, Gradient norm: 0.24814577
INFO:root:[   39] Training loss: 0.84111155, Validation loss: 0.86381430, Gradient norm: 0.30353711
INFO:root:[   40] Training loss: 0.84146787, Validation loss: 0.86332795, Gradient norm: 0.38493257
INFO:root:[   41] Training loss: 0.83789581, Validation loss: 0.86239176, Gradient norm: 0.30002062
INFO:root:[   42] Training loss: 0.83863709, Validation loss: 0.86408299, Gradient norm: 0.33747193
INFO:root:[   43] Training loss: 0.83684876, Validation loss: 0.86313188, Gradient norm: 0.31364870
INFO:root:[   44] Training loss: 0.83644304, Validation loss: 0.86353228, Gradient norm: 0.28954803
INFO:root:[   45] Training loss: 0.83470194, Validation loss: 0.86383934, Gradient norm: 0.28762871
INFO:root:[   46] Training loss: 0.83389733, Validation loss: 0.86619323, Gradient norm: 0.33460270
INFO:root:[   47] Training loss: 0.83388817, Validation loss: 0.86474843, Gradient norm: 0.28320182
INFO:root:[   48] Training loss: 0.83254969, Validation loss: 0.86930845, Gradient norm: 0.37693407
INFO:root:[   49] Training loss: 0.83216370, Validation loss: 0.86870985, Gradient norm: 0.36573931
INFO:root:[   50] Training loss: 0.83067659, Validation loss: 0.86694057, Gradient norm: 0.33232784
INFO:root:[   51] Training loss: 0.82954825, Validation loss: 0.86577825, Gradient norm: 0.30952280
INFO:root:[   52] Training loss: 0.82841178, Validation loss: 0.86662325, Gradient norm: 0.31437116
INFO:root:[   53] Training loss: 0.82900168, Validation loss: 0.86444486, Gradient norm: 0.39323648
INFO:root:[   54] Training loss: 0.82732910, Validation loss: 0.86687669, Gradient norm: 0.33765566
INFO:root:[   55] Training loss: 0.82590932, Validation loss: 0.86865815, Gradient norm: 0.32719685
INFO:root:[   56] Training loss: 0.82437803, Validation loss: 0.86733493, Gradient norm: 0.30151951
INFO:root:[   57] Training loss: 0.82499966, Validation loss: 0.86827790, Gradient norm: 0.35387185
INFO:root:[   58] Training loss: 0.82431063, Validation loss: 0.86667887, Gradient norm: 0.36002207
INFO:root:[   59] Training loss: 0.82249526, Validation loss: 0.86738397, Gradient norm: 0.24682502
INFO:root:[   60] Training loss: 0.82295447, Validation loss: 0.87286876, Gradient norm: 0.41834706
INFO:root:[   61] Training loss: 0.82180912, Validation loss: 0.86593314, Gradient norm: 0.35656187
INFO:root:[   62] Training loss: 0.82077406, Validation loss: 0.87136193, Gradient norm: 0.31751872
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 2043.25s.
INFO:root:Emptying the cuda cache took 0.021s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.83139
INFO:root:EnergyScoreTrain: 0.81694
INFO:root:CRPSTrain: 0.63813
INFO:root:Gaussian NLLTrain: 1292.49093
INFO:root:CoverageTrain: 0.12545
INFO:root:IntervalWidthTrain: 0.18445
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86075
INFO:root:EnergyScoreValidation: 0.84718
INFO:root:CRPSValidation: 0.66359
INFO:root:Gaussian NLLValidation: 1310.54322
INFO:root:CoverageValidation: 0.11965
INFO:root:IntervalWidthValidation: 0.18421
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86127
INFO:root:EnergyScoreTest: 0.8478
INFO:root:CRPSTest: 0.6647
INFO:root:Gaussian NLLTest: 1286.20577
INFO:root:CoverageTest: 0.11922
INFO:root:IntervalWidthTest: 0.18499
INFO:root:###2 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99403043, Validation loss: 0.98858013, Gradient norm: 0.02256553
INFO:root:[    2] Training loss: 0.97871515, Validation loss: 0.96851154, Gradient norm: 0.09442785
INFO:root:[    3] Training loss: 0.95860876, Validation loss: 0.94790986, Gradient norm: 0.14085634
INFO:root:[    4] Training loss: 0.93993926, Validation loss: 0.93191469, Gradient norm: 0.17799894
INFO:root:[    5] Training loss: 0.92591651, Validation loss: 0.92375341, Gradient norm: 0.18081386
INFO:root:[    6] Training loss: 0.91554837, Validation loss: 0.91320341, Gradient norm: 0.23776149
INFO:root:[    7] Training loss: 0.90749568, Validation loss: 0.90428505, Gradient norm: 0.20294269
INFO:root:[    8] Training loss: 0.90117128, Validation loss: 0.90037471, Gradient norm: 0.18729710
INFO:root:[    9] Training loss: 0.89658373, Validation loss: 0.89566018, Gradient norm: 0.22237619
INFO:root:[   10] Training loss: 0.89271207, Validation loss: 0.89459914, Gradient norm: 0.20746443
INFO:root:[   11] Training loss: 0.88842512, Validation loss: 0.89118189, Gradient norm: 0.19415481
INFO:root:[   12] Training loss: 0.88514341, Validation loss: 0.88973176, Gradient norm: 0.18891326
INFO:root:[   13] Training loss: 0.88224021, Validation loss: 0.88905148, Gradient norm: 0.16475874
INFO:root:[   14] Training loss: 0.88003526, Validation loss: 0.88278419, Gradient norm: 0.22907539
INFO:root:[   15] Training loss: 0.87731979, Validation loss: 0.88217520, Gradient norm: 0.22725582
INFO:root:[   16] Training loss: 0.87415635, Validation loss: 0.88207485, Gradient norm: 0.19520716
INFO:root:[   17] Training loss: 0.87277797, Validation loss: 0.87797601, Gradient norm: 0.23906163
INFO:root:[   18] Training loss: 0.87025896, Validation loss: 0.87790192, Gradient norm: 0.20051362
INFO:root:[   19] Training loss: 0.86760783, Validation loss: 0.87557879, Gradient norm: 0.17936245
INFO:root:[   20] Training loss: 0.86686062, Validation loss: 0.87525299, Gradient norm: 0.24281288
INFO:root:[   21] Training loss: 0.86497199, Validation loss: 0.87310328, Gradient norm: 0.21645823
INFO:root:[   22] Training loss: 0.86273725, Validation loss: 0.87785189, Gradient norm: 0.21212739
INFO:root:[   23] Training loss: 0.86065798, Validation loss: 0.87208669, Gradient norm: 0.16965596
INFO:root:[   24] Training loss: 0.85885061, Validation loss: 0.87230886, Gradient norm: 0.17856942
INFO:root:[   25] Training loss: 0.85789408, Validation loss: 0.86886032, Gradient norm: 0.21821219
INFO:root:[   26] Training loss: 0.85689838, Validation loss: 0.86808875, Gradient norm: 0.21837339
INFO:root:[   27] Training loss: 0.85467081, Validation loss: 0.86805110, Gradient norm: 0.18830958
INFO:root:[   28] Training loss: 0.85344225, Validation loss: 0.86893169, Gradient norm: 0.16685048
INFO:root:[   29] Training loss: 0.85347994, Validation loss: 0.86585931, Gradient norm: 0.25225103
INFO:root:[   30] Training loss: 0.85090740, Validation loss: 0.87042454, Gradient norm: 0.23860266
INFO:root:[   31] Training loss: 0.84909470, Validation loss: 0.86543098, Gradient norm: 0.18210659
INFO:root:[   32] Training loss: 0.84915779, Validation loss: 0.86655079, Gradient norm: 0.23460697
INFO:root:[   33] Training loss: 0.84746671, Validation loss: 0.86460916, Gradient norm: 0.25647743
INFO:root:[   34] Training loss: 0.84689144, Validation loss: 0.86363678, Gradient norm: 0.23700910
INFO:root:[   35] Training loss: 0.84445988, Validation loss: 0.86427316, Gradient norm: 0.19011274
INFO:root:[   36] Training loss: 0.84246720, Validation loss: 0.86677335, Gradient norm: 0.17189742
INFO:root:[   37] Training loss: 0.84371188, Validation loss: 0.86350344, Gradient norm: 0.27581197
INFO:root:[   38] Training loss: 0.84168826, Validation loss: 0.86218987, Gradient norm: 0.18819441
INFO:root:[   39] Training loss: 0.83917145, Validation loss: 0.86202492, Gradient norm: 0.15395842
INFO:root:[   40] Training loss: 0.83834800, Validation loss: 0.86542553, Gradient norm: 0.18997352
INFO:root:[   41] Training loss: 0.83866066, Validation loss: 0.86688341, Gradient norm: 0.26631804
INFO:root:[   42] Training loss: 0.83862136, Validation loss: 0.86402755, Gradient norm: 0.23778515
INFO:root:[   43] Training loss: 0.83603304, Validation loss: 0.86384095, Gradient norm: 0.21496542
INFO:root:[   44] Training loss: 0.83562806, Validation loss: 0.86330396, Gradient norm: 0.22936775
INFO:root:[   45] Training loss: 0.83411922, Validation loss: 0.86469292, Gradient norm: 0.21110462
INFO:root:[   46] Training loss: 0.83326349, Validation loss: 0.86819682, Gradient norm: 0.24353726
INFO:root:[   47] Training loss: 0.83394025, Validation loss: 0.86190040, Gradient norm: 0.24135762
INFO:root:[   48] Training loss: 0.83099997, Validation loss: 0.86383746, Gradient norm: 0.21621461
INFO:root:[   49] Training loss: 0.83002632, Validation loss: 0.86411592, Gradient norm: 0.22657366
INFO:root:[   50] Training loss: 0.83111189, Validation loss: 0.86623128, Gradient norm: 0.28258842
INFO:root:[   51] Training loss: 0.82771078, Validation loss: 0.86394840, Gradient norm: 0.20689875
INFO:root:[   52] Training loss: 0.82969444, Validation loss: 0.86342999, Gradient norm: 0.24980995
INFO:root:[   53] Training loss: 0.82623430, Validation loss: 0.86517441, Gradient norm: 0.21480972
INFO:root:[   54] Training loss: 0.82604025, Validation loss: 0.86460610, Gradient norm: 0.19356834
INFO:root:[   55] Training loss: 0.82628892, Validation loss: 0.87052876, Gradient norm: 0.29929350
INFO:root:[   56] Training loss: 0.82478997, Validation loss: 0.86550104, Gradient norm: 0.26826308
INFO:root:[   57] Training loss: 0.82389804, Validation loss: 0.86845683, Gradient norm: 0.25762853
INFO:root:[   58] Training loss: 0.82216928, Validation loss: 0.86684369, Gradient norm: 0.21688258
INFO:root:[   59] Training loss: 0.82188620, Validation loss: 0.86629514, Gradient norm: 0.20528411
INFO:root:[   60] Training loss: 0.82040558, Validation loss: 0.86891349, Gradient norm: 0.22309317
INFO:root:[   61] Training loss: 0.82036663, Validation loss: 0.86750490, Gradient norm: 0.27902050
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1954.358s.
INFO:root:Emptying the cuda cache took 0.02s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.821
INFO:root:EnergyScoreTrain: 0.77521
INFO:root:CRPSTrain: 0.60613
INFO:root:Gaussian NLLTrain: 278.1654
INFO:root:CoverageTrain: 0.24437
INFO:root:IntervalWidthTrain: 0.36495
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.85586
INFO:root:EnergyScoreValidation: 0.81093
INFO:root:CRPSValidation: 0.63615
INFO:root:Gaussian NLLValidation: 284.24492
INFO:root:CoverageValidation: 0.23227
INFO:root:IntervalWidthValidation: 0.36458
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.85636
INFO:root:EnergyScoreTest: 0.81157
INFO:root:CRPSTest: 0.6372
INFO:root:Gaussian NLLTest: 281.90947
INFO:root:CoverageTest: 0.23116
INFO:root:IntervalWidthTest: 0.36449
INFO:root:###3 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99408244, Validation loss: 0.98871320, Gradient norm: 0.02149530
INFO:root:[    2] Training loss: 0.97928252, Validation loss: 0.96922697, Gradient norm: 0.08798905
INFO:root:[    3] Training loss: 0.95977539, Validation loss: 0.94971074, Gradient norm: 0.12592237
INFO:root:[    4] Training loss: 0.94147744, Validation loss: 0.93316149, Gradient norm: 0.14453153
INFO:root:[    5] Training loss: 0.92852058, Validation loss: 0.92408708, Gradient norm: 0.17329863
INFO:root:[    6] Training loss: 0.91756237, Validation loss: 0.91300257, Gradient norm: 0.19450715
INFO:root:[    7] Training loss: 0.91008705, Validation loss: 0.90716926, Gradient norm: 0.18424090
INFO:root:[    8] Training loss: 0.90382775, Validation loss: 0.90231324, Gradient norm: 0.16238180
INFO:root:[    9] Training loss: 0.89847417, Validation loss: 0.90129302, Gradient norm: 0.14946999
INFO:root:[   10] Training loss: 0.89575975, Validation loss: 0.89847107, Gradient norm: 0.20716595
INFO:root:[   11] Training loss: 0.89102844, Validation loss: 0.89127627, Gradient norm: 0.17567985
INFO:root:[   12] Training loss: 0.88825489, Validation loss: 0.89162368, Gradient norm: 0.17393543
INFO:root:[   13] Training loss: 0.88577610, Validation loss: 0.88796095, Gradient norm: 0.19495248
INFO:root:[   14] Training loss: 0.88200159, Validation loss: 0.88485456, Gradient norm: 0.17201211
INFO:root:[   15] Training loss: 0.87901642, Validation loss: 0.88475587, Gradient norm: 0.14608061
INFO:root:[   16] Training loss: 0.87666812, Validation loss: 0.88239799, Gradient norm: 0.14911843
INFO:root:[   17] Training loss: 0.87646047, Validation loss: 0.88007476, Gradient norm: 0.21613316
INFO:root:[   18] Training loss: 0.87244513, Validation loss: 0.87944960, Gradient norm: 0.14997623
INFO:root:[   19] Training loss: 0.87022413, Validation loss: 0.87848778, Gradient norm: 0.15149346
INFO:root:[   20] Training loss: 0.86914280, Validation loss: 0.88097407, Gradient norm: 0.19606750
INFO:root:[   21] Training loss: 0.86709558, Validation loss: 0.87501072, Gradient norm: 0.16106729
INFO:root:[   22] Training loss: 0.86486279, Validation loss: 0.87602778, Gradient norm: 0.17686822
INFO:root:[   23] Training loss: 0.86374814, Validation loss: 0.87195377, Gradient norm: 0.15270297
INFO:root:[   24] Training loss: 0.86217197, Validation loss: 0.87396299, Gradient norm: 0.17665117
INFO:root:[   25] Training loss: 0.86032861, Validation loss: 0.87069836, Gradient norm: 0.17016221
INFO:root:[   26] Training loss: 0.85866177, Validation loss: 0.87059661, Gradient norm: 0.16620862
INFO:root:[   27] Training loss: 0.85734565, Validation loss: 0.86955977, Gradient norm: 0.16603569
INFO:root:[   28] Training loss: 0.85570493, Validation loss: 0.87053727, Gradient norm: 0.17717181
INFO:root:[   29] Training loss: 0.85457463, Validation loss: 0.87182877, Gradient norm: 0.15346680
INFO:root:[   30] Training loss: 0.85393457, Validation loss: 0.86802681, Gradient norm: 0.22081045
INFO:root:[   31] Training loss: 0.85259351, Validation loss: 0.86893101, Gradient norm: 0.16740756
INFO:root:[   32] Training loss: 0.85064668, Validation loss: 0.86768183, Gradient norm: 0.15092715
INFO:root:[   33] Training loss: 0.85140799, Validation loss: 0.86893943, Gradient norm: 0.22509621
INFO:root:[   34] Training loss: 0.84850642, Validation loss: 0.86569567, Gradient norm: 0.17259563
INFO:root:[   35] Training loss: 0.84829573, Validation loss: 0.86661443, Gradient norm: 0.20725802
INFO:root:[   36] Training loss: 0.84564750, Validation loss: 0.86668893, Gradient norm: 0.13535865
INFO:root:[   37] Training loss: 0.84620508, Validation loss: 0.86543600, Gradient norm: 0.20479712
INFO:root:[   38] Training loss: 0.84488979, Validation loss: 0.86432651, Gradient norm: 0.15550269
INFO:root:[   39] Training loss: 0.84279580, Validation loss: 0.86850026, Gradient norm: 0.15563122
INFO:root:[   40] Training loss: 0.84307873, Validation loss: 0.86568678, Gradient norm: 0.19848517
INFO:root:[   41] Training loss: 0.84037201, Validation loss: 0.86478173, Gradient norm: 0.16481385
INFO:root:[   42] Training loss: 0.84112469, Validation loss: 0.86365764, Gradient norm: 0.17013677
INFO:root:[   43] Training loss: 0.83859345, Validation loss: 0.86508683, Gradient norm: 0.13799651
INFO:root:[   44] Training loss: 0.83948405, Validation loss: 0.86448311, Gradient norm: 0.18736764
INFO:root:[   45] Training loss: 0.83750660, Validation loss: 0.86540469, Gradient norm: 0.16124778
INFO:root:[   46] Training loss: 0.83767010, Validation loss: 0.86896485, Gradient norm: 0.22698531
INFO:root:[   47] Training loss: 0.83698406, Validation loss: 0.86426832, Gradient norm: 0.19520482
INFO:root:[   48] Training loss: 0.83438369, Validation loss: 0.86380179, Gradient norm: 0.15270614
INFO:root:[   49] Training loss: 0.83468477, Validation loss: 0.86539825, Gradient norm: 0.18865496
INFO:root:[   50] Training loss: 0.83432045, Validation loss: 0.86780239, Gradient norm: 0.19496934
INFO:root:[   51] Training loss: 0.83246275, Validation loss: 0.86489372, Gradient norm: 0.19661024
INFO:root:[   52] Training loss: 0.83247496, Validation loss: 0.86377026, Gradient norm: 0.20942474
INFO:root:[   53] Training loss: 0.83165127, Validation loss: 0.86565893, Gradient norm: 0.18896307
INFO:root:[   54] Training loss: 0.83057563, Validation loss: 0.86556284, Gradient norm: 0.20268690
INFO:root:[   55] Training loss: 0.82854037, Validation loss: 0.86673282, Gradient norm: 0.17604902
INFO:root:[   56] Training loss: 0.82729666, Validation loss: 0.86966893, Gradient norm: 0.15245494
INFO:root:[   57] Training loss: 0.83058850, Validation loss: 0.86905178, Gradient norm: 0.22643633
INFO:root:[   58] Training loss: 0.82710869, Validation loss: 0.86600360, Gradient norm: 0.19369845
INFO:root:[   59] Training loss: 0.82659094, Validation loss: 0.86748079, Gradient norm: 0.18812404
INFO:root:[   60] Training loss: 0.82502430, Validation loss: 0.86693790, Gradient norm: 0.16543737
INFO:root:[   61] Training loss: 0.82536242, Validation loss: 0.86649820, Gradient norm: 0.20124991
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1942.51s.
INFO:root:Emptying the cuda cache took 0.023s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82686
INFO:root:EnergyScoreTrain: 0.76822
INFO:root:CRPSTrain: 0.60458
INFO:root:Gaussian NLLTrain: 207.58193
INFO:root:CoverageTrain: 0.28037
INFO:root:IntervalWidthTrain: 0.43187
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.85519
INFO:root:EnergyScoreValidation: 0.79721
INFO:root:CRPSValidation: 0.62902
INFO:root:Gaussian NLLValidation: 210.26375
INFO:root:CoverageValidation: 0.26862
INFO:root:IntervalWidthValidation: 0.43151
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.85662
INFO:root:EnergyScoreTest: 0.79895
INFO:root:CRPSTest: 0.63109
INFO:root:Gaussian NLLTest: 210.77353
INFO:root:CoverageTest: 0.26649
INFO:root:IntervalWidthTest: 0.43034
INFO:root:###4 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99410686, Validation loss: 0.98870726, Gradient norm: 0.02123430
INFO:root:[    2] Training loss: 0.97957118, Validation loss: 0.97061122, Gradient norm: 0.08435169
INFO:root:[    3] Training loss: 0.96058091, Validation loss: 0.95033835, Gradient norm: 0.11818703
INFO:root:[    4] Training loss: 0.94220532, Validation loss: 0.93417513, Gradient norm: 0.11100846
INFO:root:[    5] Training loss: 0.92984637, Validation loss: 0.92405876, Gradient norm: 0.15367739
INFO:root:[    6] Training loss: 0.91997483, Validation loss: 0.91738540, Gradient norm: 0.17731261
INFO:root:[    7] Training loss: 0.91262840, Validation loss: 0.91013754, Gradient norm: 0.14076610
INFO:root:[    8] Training loss: 0.90714578, Validation loss: 0.90551446, Gradient norm: 0.16715746
INFO:root:[    9] Training loss: 0.90235584, Validation loss: 0.90309671, Gradient norm: 0.16594801
INFO:root:[   10] Training loss: 0.89858457, Validation loss: 0.90201583, Gradient norm: 0.14464274
INFO:root:[   11] Training loss: 0.89470940, Validation loss: 0.89633531, Gradient norm: 0.14718756
INFO:root:[   12] Training loss: 0.89188912, Validation loss: 0.89384725, Gradient norm: 0.14272796
INFO:root:[   13] Training loss: 0.88940956, Validation loss: 0.89270669, Gradient norm: 0.14982504
INFO:root:[   14] Training loss: 0.88604114, Validation loss: 0.88852256, Gradient norm: 0.13611612
INFO:root:[   15] Training loss: 0.88406038, Validation loss: 0.89049583, Gradient norm: 0.15971908
INFO:root:[   16] Training loss: 0.88138525, Validation loss: 0.88558643, Gradient norm: 0.12478229
INFO:root:[   17] Training loss: 0.87983236, Validation loss: 0.88368779, Gradient norm: 0.16539681
INFO:root:[   18] Training loss: 0.87768534, Validation loss: 0.88227498, Gradient norm: 0.16086171
INFO:root:[   19] Training loss: 0.87547062, Validation loss: 0.88074107, Gradient norm: 0.12177546
INFO:root:[   20] Training loss: 0.87342018, Validation loss: 0.88115484, Gradient norm: 0.12109021
INFO:root:[   21] Training loss: 0.87239841, Validation loss: 0.87774279, Gradient norm: 0.14133137
INFO:root:[   22] Training loss: 0.86978604, Validation loss: 0.87681562, Gradient norm: 0.14659579
INFO:root:[   23] Training loss: 0.86980091, Validation loss: 0.87547351, Gradient norm: 0.17694702
INFO:root:[   24] Training loss: 0.86714080, Validation loss: 0.87956959, Gradient norm: 0.13792257
INFO:root:[   25] Training loss: 0.86595320, Validation loss: 0.87398364, Gradient norm: 0.14870515
INFO:root:[   26] Training loss: 0.86524899, Validation loss: 0.87441304, Gradient norm: 0.17798937
INFO:root:[   27] Training loss: 0.86232946, Validation loss: 0.87334481, Gradient norm: 0.12898640
INFO:root:[   28] Training loss: 0.86205588, Validation loss: 0.87381528, Gradient norm: 0.15243966
INFO:root:[   29] Training loss: 0.86115375, Validation loss: 0.87320547, Gradient norm: 0.17131318
INFO:root:[   30] Training loss: 0.85960626, Validation loss: 0.87013027, Gradient norm: 0.15870934
INFO:root:[   31] Training loss: 0.85787384, Validation loss: 0.86977547, Gradient norm: 0.12566582
INFO:root:[   32] Training loss: 0.85754718, Validation loss: 0.87283507, Gradient norm: 0.14439967
INFO:root:[   33] Training loss: 0.85731604, Validation loss: 0.86893087, Gradient norm: 0.17905051
INFO:root:[   34] Training loss: 0.85532422, Validation loss: 0.86759038, Gradient norm: 0.17205975
INFO:root:[   35] Training loss: 0.85348701, Validation loss: 0.86987676, Gradient norm: 0.12048829
INFO:root:[   36] Training loss: 0.85323956, Validation loss: 0.86735853, Gradient norm: 0.15720870
INFO:root:[   37] Training loss: 0.85172407, Validation loss: 0.86787105, Gradient norm: 0.12910001
INFO:root:[   38] Training loss: 0.85140212, Validation loss: 0.86641770, Gradient norm: 0.17619005
INFO:root:[   39] Training loss: 0.85031691, Validation loss: 0.86782549, Gradient norm: 0.15126098
INFO:root:[   40] Training loss: 0.84995440, Validation loss: 0.86475856, Gradient norm: 0.15891809
INFO:root:[   41] Training loss: 0.84800014, Validation loss: 0.86959957, Gradient norm: 0.13046395
INFO:root:[   42] Training loss: 0.84794001, Validation loss: 0.86450828, Gradient norm: 0.15633123
INFO:root:[   43] Training loss: 0.84701989, Validation loss: 0.86687926, Gradient norm: 0.16161583
INFO:root:[   44] Training loss: 0.84518373, Validation loss: 0.86440691, Gradient norm: 0.12529396
INFO:root:[   45] Training loss: 0.84599523, Validation loss: 0.86508796, Gradient norm: 0.16871101
INFO:root:[   46] Training loss: 0.84516345, Validation loss: 0.86452630, Gradient norm: 0.18831422
INFO:root:[   47] Training loss: 0.84309026, Validation loss: 0.86445960, Gradient norm: 0.13341902
INFO:root:[   48] Training loss: 0.84224757, Validation loss: 0.86507717, Gradient norm: 0.14802909
INFO:root:[   49] Training loss: 0.84256699, Validation loss: 0.86642167, Gradient norm: 0.16010609
INFO:root:[   50] Training loss: 0.84156439, Validation loss: 0.86261993, Gradient norm: 0.15054057
INFO:root:[   51] Training loss: 0.84059976, Validation loss: 0.86275087, Gradient norm: 0.16625196
INFO:root:[   52] Training loss: 0.83910548, Validation loss: 0.86520144, Gradient norm: 0.11453030
INFO:root:[   53] Training loss: 0.84109252, Validation loss: 0.86345067, Gradient norm: 0.19921622
INFO:root:[   54] Training loss: 0.83921676, Validation loss: 0.86329866, Gradient norm: 0.17267238
INFO:root:[   55] Training loss: 0.83803534, Validation loss: 0.86439382, Gradient norm: 0.15802120
INFO:root:[   56] Training loss: 0.83647935, Validation loss: 0.86290642, Gradient norm: 0.14481207
INFO:root:[   57] Training loss: 0.83772659, Validation loss: 0.86402425, Gradient norm: 0.16161828
INFO:root:[   58] Training loss: 0.83564169, Validation loss: 0.86224930, Gradient norm: 0.14908015
INFO:root:[   59] Training loss: 0.83561865, Validation loss: 0.86710061, Gradient norm: 0.15793589
INFO:root:[   60] Training loss: 0.83465145, Validation loss: 0.86370160, Gradient norm: 0.14806917
INFO:root:[   61] Training loss: 0.83509514, Validation loss: 0.86359023, Gradient norm: 0.18668570
INFO:root:[   62] Training loss: 0.83451518, Validation loss: 0.86333429, Gradient norm: 0.17737119
INFO:root:[   63] Training loss: 0.83269817, Validation loss: 0.86208992, Gradient norm: 0.15950266
INFO:root:[   64] Training loss: 0.83337977, Validation loss: 0.86273593, Gradient norm: 0.18383824
INFO:root:[   65] Training loss: 0.83184088, Validation loss: 0.86320240, Gradient norm: 0.15301739
INFO:root:[   66] Training loss: 0.83111234, Validation loss: 0.86609994, Gradient norm: 0.15915160
INFO:root:[   67] Training loss: 0.83088414, Validation loss: 0.86331476, Gradient norm: 0.17141048
INFO:root:[   68] Training loss: 0.83084845, Validation loss: 0.86338163, Gradient norm: 0.17385714
INFO:root:[   69] Training loss: 0.82948724, Validation loss: 0.86424752, Gradient norm: 0.16392710
INFO:root:[   70] Training loss: 0.82869439, Validation loss: 0.86592026, Gradient norm: 0.15440216
INFO:root:[   71] Training loss: 0.82918166, Validation loss: 0.86227731, Gradient norm: 0.17593612
INFO:root:[   72] Training loss: 0.82731520, Validation loss: 0.86278577, Gradient norm: 0.14666355
INFO:root:EP 72: Early stopping
INFO:root:Training the model took 2288.396s.
INFO:root:Emptying the cuda cache took 0.023s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.8143
INFO:root:EnergyScoreTrain: 0.73966
INFO:root:CRPSTrain: 0.58169
INFO:root:Gaussian NLLTrain: 120.17191
INFO:root:CoverageTrain: 0.33077
INFO:root:IntervalWidthTrain: 0.53379
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.84832
INFO:root:EnergyScoreValidation: 0.77435
INFO:root:CRPSValidation: 0.61078
INFO:root:Gaussian NLLValidation: 122.66608
INFO:root:CoverageValidation: 0.31513
INFO:root:IntervalWidthValidation: 0.53241
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.84987
INFO:root:EnergyScoreTest: 0.77623
INFO:root:CRPSTest: 0.61265
INFO:root:Gaussian NLLTest: 122.51044
INFO:root:CoverageTest: 0.31337
INFO:root:IntervalWidthTest: 0.53104
INFO:root:###5 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99434437, Validation loss: 0.98912454, Gradient norm: 0.01992011
INFO:root:[    2] Training loss: 0.98073433, Validation loss: 0.97164494, Gradient norm: 0.05880182
INFO:root:[    3] Training loss: 0.96456516, Validation loss: 0.95702703, Gradient norm: 0.09724340
INFO:root:[    4] Training loss: 0.94901432, Validation loss: 0.94369233, Gradient norm: 0.09773291
INFO:root:[    5] Training loss: 0.93710207, Validation loss: 0.93208067, Gradient norm: 0.11392327
INFO:root:[    6] Training loss: 0.92817137, Validation loss: 0.92487358, Gradient norm: 0.10044914
INFO:root:[    7] Training loss: 0.92141871, Validation loss: 0.91800708, Gradient norm: 0.10866393
INFO:root:[    8] Training loss: 0.91559832, Validation loss: 0.91331118, Gradient norm: 0.11431906
INFO:root:[    9] Training loss: 0.91039918, Validation loss: 0.90890348, Gradient norm: 0.08553951
INFO:root:[   10] Training loss: 0.90733845, Validation loss: 0.90674874, Gradient norm: 0.11256312
INFO:root:[   11] Training loss: 0.90349640, Validation loss: 0.90374072, Gradient norm: 0.09721919
INFO:root:[   12] Training loss: 0.90096155, Validation loss: 0.90136861, Gradient norm: 0.10344229
INFO:root:[   13] Training loss: 0.89827052, Validation loss: 0.89953624, Gradient norm: 0.10002050
INFO:root:[   14] Training loss: 0.89593369, Validation loss: 0.89723959, Gradient norm: 0.10785581
INFO:root:[   15] Training loss: 0.89355158, Validation loss: 0.89709352, Gradient norm: 0.09602876
INFO:root:[   16] Training loss: 0.89145172, Validation loss: 0.89552821, Gradient norm: 0.08615216
INFO:root:[   17] Training loss: 0.88987849, Validation loss: 0.89195356, Gradient norm: 0.10321618
INFO:root:[   18] Training loss: 0.88773036, Validation loss: 0.89109103, Gradient norm: 0.10592338
INFO:root:[   19] Training loss: 0.88692939, Validation loss: 0.88986199, Gradient norm: 0.10723912
INFO:root:[   20] Training loss: 0.88408099, Validation loss: 0.88816988, Gradient norm: 0.07949095
INFO:root:[   21] Training loss: 0.88309977, Validation loss: 0.88739124, Gradient norm: 0.09890255
INFO:root:[   22] Training loss: 0.88207560, Validation loss: 0.88832941, Gradient norm: 0.10611473
INFO:root:[   23] Training loss: 0.88062470, Validation loss: 0.88619112, Gradient norm: 0.11548502
INFO:root:[   24] Training loss: 0.87855143, Validation loss: 0.88599992, Gradient norm: 0.08736275
INFO:root:[   25] Training loss: 0.87825133, Validation loss: 0.88212661, Gradient norm: 0.10260014
INFO:root:[   26] Training loss: 0.87713903, Validation loss: 0.88198058, Gradient norm: 0.10485026
INFO:root:[   27] Training loss: 0.87529933, Validation loss: 0.88442573, Gradient norm: 0.09911344
INFO:root:[   28] Training loss: 0.87435537, Validation loss: 0.88135925, Gradient norm: 0.09530020
INFO:root:[   29] Training loss: 0.87345939, Validation loss: 0.87982544, Gradient norm: 0.09478634
INFO:root:[   30] Training loss: 0.87150678, Validation loss: 0.87969507, Gradient norm: 0.08660671
INFO:root:[   31] Training loss: 0.87126287, Validation loss: 0.87828010, Gradient norm: 0.09335103
INFO:root:[   32] Training loss: 0.87135891, Validation loss: 0.87867405, Gradient norm: 0.11406388
INFO:root:[   33] Training loss: 0.86924240, Validation loss: 0.87718807, Gradient norm: 0.10474696
INFO:root:[   34] Training loss: 0.86860125, Validation loss: 0.87661578, Gradient norm: 0.10428215
INFO:root:[   35] Training loss: 0.86783771, Validation loss: 0.87732321, Gradient norm: 0.10390559
INFO:root:[   36] Training loss: 0.86664093, Validation loss: 0.87710142, Gradient norm: 0.09673763
INFO:root:[   37] Training loss: 0.86619194, Validation loss: 0.87475216, Gradient norm: 0.10762737
INFO:root:[   38] Training loss: 0.86668369, Validation loss: 0.87435010, Gradient norm: 0.13479310
INFO:root:[   39] Training loss: 0.86408542, Validation loss: 0.87268714, Gradient norm: 0.10386912
INFO:root:[   40] Training loss: 0.86326322, Validation loss: 0.87695754, Gradient norm: 0.09312039
INFO:root:[   41] Training loss: 0.86289364, Validation loss: 0.87382185, Gradient norm: 0.10086434
INFO:root:[   42] Training loss: 0.86397626, Validation loss: 0.87384372, Gradient norm: 0.12257747
INFO:root:[   43] Training loss: 0.86151803, Validation loss: 0.87307407, Gradient norm: 0.09574162
INFO:root:[   44] Training loss: 0.86217244, Validation loss: 0.87172672, Gradient norm: 0.12266373
INFO:root:[   45] Training loss: 0.86027376, Validation loss: 0.87101973, Gradient norm: 0.10127295
INFO:root:[   46] Training loss: 0.86040801, Validation loss: 0.87060997, Gradient norm: 0.11809321
INFO:root:[   47] Training loss: 0.85956911, Validation loss: 0.87054824, Gradient norm: 0.10762132
INFO:root:[   48] Training loss: 0.85856573, Validation loss: 0.86992479, Gradient norm: 0.10710681
INFO:root:[   49] Training loss: 0.85879452, Validation loss: 0.86997234, Gradient norm: 0.12559838
INFO:root:[   50] Training loss: 0.85753793, Validation loss: 0.87121812, Gradient norm: 0.09890642
INFO:root:[   51] Training loss: 0.85779984, Validation loss: 0.87018836, Gradient norm: 0.12222696
INFO:root:[   52] Training loss: 0.85765842, Validation loss: 0.87025826, Gradient norm: 0.13445494
INFO:root:[   53] Training loss: 0.85562553, Validation loss: 0.86840159, Gradient norm: 0.10240939
INFO:root:[   54] Training loss: 0.85698523, Validation loss: 0.86860901, Gradient norm: 0.12487240
INFO:root:[   55] Training loss: 0.85513557, Validation loss: 0.86972766, Gradient norm: 0.11453875
INFO:root:[   56] Training loss: 0.85403000, Validation loss: 0.86718402, Gradient norm: 0.10036596
INFO:root:[   57] Training loss: 0.85446959, Validation loss: 0.86885282, Gradient norm: 0.12112130
INFO:root:[   58] Training loss: 0.85459150, Validation loss: 0.86775797, Gradient norm: 0.12999155
INFO:root:[   59] Training loss: 0.85292951, Validation loss: 0.86758009, Gradient norm: 0.09929083
INFO:root:[   60] Training loss: 0.85316296, Validation loss: 0.86786407, Gradient norm: 0.11958074
INFO:root:[   61] Training loss: 0.85242436, Validation loss: 0.86754019, Gradient norm: 0.11865128
INFO:root:[   62] Training loss: 0.85268231, Validation loss: 0.86823458, Gradient norm: 0.13324039
INFO:root:[   63] Training loss: 0.85189869, Validation loss: 0.86576908, Gradient norm: 0.11763853
INFO:root:[   64] Training loss: 0.85234301, Validation loss: 0.86567632, Gradient norm: 0.13083131
INFO:root:[   65] Training loss: 0.85055418, Validation loss: 0.86682269, Gradient norm: 0.11011531
INFO:root:[   66] Training loss: 0.85107733, Validation loss: 0.86579485, Gradient norm: 0.14064714
INFO:root:[   67] Training loss: 0.85045642, Validation loss: 0.86778579, Gradient norm: 0.12809064
INFO:root:[   68] Training loss: 0.84965105, Validation loss: 0.86512318, Gradient norm: 0.11871351
INFO:root:[   69] Training loss: 0.85016255, Validation loss: 0.86632961, Gradient norm: 0.12669152
INFO:root:[   70] Training loss: 0.84869651, Validation loss: 0.86687775, Gradient norm: 0.10891067
INFO:root:[   71] Training loss: 0.84777768, Validation loss: 0.86535186, Gradient norm: 0.11176759
INFO:root:[   72] Training loss: 0.84864652, Validation loss: 0.87073928, Gradient norm: 0.13047297
INFO:root:[   73] Training loss: 0.84907158, Validation loss: 0.86554357, Gradient norm: 0.14492037
INFO:root:[   74] Training loss: 0.84772103, Validation loss: 0.86727066, Gradient norm: 0.11994974
INFO:root:[   75] Training loss: 0.84741052, Validation loss: 0.86502241, Gradient norm: 0.13255080
INFO:root:[   76] Training loss: 0.84770375, Validation loss: 0.86693949, Gradient norm: 0.14706360
INFO:root:[   77] Training loss: 0.84652096, Validation loss: 0.86557426, Gradient norm: 0.12089301
INFO:root:[   78] Training loss: 0.84654408, Validation loss: 0.86530935, Gradient norm: 0.12875715
INFO:root:[   79] Training loss: 0.84634083, Validation loss: 0.86469328, Gradient norm: 0.13412179
INFO:root:[   80] Training loss: 0.84630951, Validation loss: 0.87120726, Gradient norm: 0.14542360
INFO:root:[   81] Training loss: 0.84560796, Validation loss: 0.86405497, Gradient norm: 0.12058300
INFO:root:[   82] Training loss: 0.84436081, Validation loss: 0.86577298, Gradient norm: 0.10635689
INFO:root:[   83] Training loss: 0.84580760, Validation loss: 0.86681546, Gradient norm: 0.15226859
INFO:root:[   84] Training loss: 0.84525806, Validation loss: 0.86478892, Gradient norm: 0.13992605
INFO:root:[   85] Training loss: 0.84458971, Validation loss: 0.86402562, Gradient norm: 0.13837643
INFO:root:[   86] Training loss: 0.84324657, Validation loss: 0.86819494, Gradient norm: 0.11642248
INFO:root:[   87] Training loss: 0.84568331, Validation loss: 0.86701082, Gradient norm: 0.16762926
INFO:root:[   88] Training loss: 0.84290003, Validation loss: 0.86535900, Gradient norm: 0.11770026
INFO:root:[   89] Training loss: 0.84302512, Validation loss: 0.86567643, Gradient norm: 0.13362640
INFO:root:[   90] Training loss: 0.84257312, Validation loss: 0.86369116, Gradient norm: 0.12595902
INFO:root:[   91] Training loss: 0.84321975, Validation loss: 0.86692151, Gradient norm: 0.14247594
INFO:root:[   92] Training loss: 0.84203967, Validation loss: 0.86377367, Gradient norm: 0.12639481
INFO:root:[   93] Training loss: 0.84266525, Validation loss: 0.86623553, Gradient norm: 0.14998670
INFO:root:[   94] Training loss: 0.84354796, Validation loss: 0.86412741, Gradient norm: 0.16472179
INFO:root:[   95] Training loss: 0.84149229, Validation loss: 0.86483025, Gradient norm: 0.13289837
INFO:root:[   96] Training loss: 0.84074008, Validation loss: 0.86573822, Gradient norm: 0.11825862
INFO:root:[   97] Training loss: 0.84088549, Validation loss: 0.86310109, Gradient norm: 0.13740123
INFO:root:[   98] Training loss: 0.84142039, Validation loss: 0.86462714, Gradient norm: 0.14721011
INFO:root:[   99] Training loss: 0.84112261, Validation loss: 0.86351487, Gradient norm: 0.14970605
INFO:root:[  100] Training loss: 0.84058000, Validation loss: 0.87124384, Gradient norm: 0.15036481
INFO:root:[  101] Training loss: 0.84082272, Validation loss: 0.86364444, Gradient norm: 0.14358257
INFO:root:[  102] Training loss: 0.84022099, Validation loss: 0.86671775, Gradient norm: 0.14544445
INFO:root:[  103] Training loss: 0.84008368, Validation loss: 0.86575235, Gradient norm: 0.14952641
INFO:root:[  104] Training loss: 0.83969943, Validation loss: 0.86483583, Gradient norm: 0.13687046
INFO:root:[  105] Training loss: 0.83954777, Validation loss: 0.86519711, Gradient norm: 0.15332476
INFO:root:[  106] Training loss: 0.83962898, Validation loss: 0.86395995, Gradient norm: 0.15259532
INFO:root:EP 106: Early stopping
INFO:root:Training the model took 3358.257s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.8157
INFO:root:EnergyScoreTrain: 0.72034
INFO:root:CRPSTrain: 0.5735
INFO:root:Gaussian NLLTrain: 143.64868
INFO:root:CoverageTrain: 0.38568
INFO:root:IntervalWidthTrain: 0.65401
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.84266
INFO:root:EnergyScoreValidation: 0.74761
INFO:root:CRPSValidation: 0.59623
INFO:root:Gaussian NLLValidation: 145.37178
INFO:root:CoverageValidation: 0.37167
INFO:root:IntervalWidthValidation: 0.65268
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.84353
INFO:root:EnergyScoreTest: 0.7487
INFO:root:CRPSTest: 0.59724
INFO:root:Gaussian NLLTest: 144.38053
INFO:root:CoverageTest: 0.37081
INFO:root:IntervalWidthTest: 0.6522
INFO:root:###6 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99467106, Validation loss: 0.99014887, Gradient norm: 0.01680704
INFO:root:[    2] Training loss: 0.98291907, Validation loss: 0.97452742, Gradient norm: 0.04913905
INFO:root:[    3] Training loss: 0.96800654, Validation loss: 0.96132792, Gradient norm: 0.08013537
INFO:root:[    4] Training loss: 0.95552531, Validation loss: 0.95153460, Gradient norm: 0.07751387
INFO:root:[    5] Training loss: 0.94561748, Validation loss: 0.94104192, Gradient norm: 0.07888756
INFO:root:[    6] Training loss: 0.93723410, Validation loss: 0.93492433, Gradient norm: 0.08413876
INFO:root:[    7] Training loss: 0.93126148, Validation loss: 0.92888089, Gradient norm: 0.08961553
INFO:root:[    8] Training loss: 0.92501108, Validation loss: 0.92205944, Gradient norm: 0.07903543
INFO:root:[    9] Training loss: 0.91974614, Validation loss: 0.91790491, Gradient norm: 0.07321932
INFO:root:[   10] Training loss: 0.91600296, Validation loss: 0.91373354, Gradient norm: 0.07557644
INFO:root:[   11] Training loss: 0.91219080, Validation loss: 0.91122783, Gradient norm: 0.07608607
INFO:root:[   12] Training loss: 0.90924991, Validation loss: 0.90894037, Gradient norm: 0.07332192
INFO:root:[   13] Training loss: 0.90648065, Validation loss: 0.90746563, Gradient norm: 0.07326556
INFO:root:[   14] Training loss: 0.90408409, Validation loss: 0.90379876, Gradient norm: 0.08751995
INFO:root:[   15] Training loss: 0.90155279, Validation loss: 0.90345339, Gradient norm: 0.07458135
INFO:root:[   16] Training loss: 0.89990159, Validation loss: 0.90100333, Gradient norm: 0.08132901
INFO:root:[   17] Training loss: 0.89838751, Validation loss: 0.89898866, Gradient norm: 0.09544266
INFO:root:[   18] Training loss: 0.89686597, Validation loss: 0.89903918, Gradient norm: 0.10390969
INFO:root:[   19] Training loss: 0.89553467, Validation loss: 0.89687450, Gradient norm: 0.08722691
INFO:root:[   20] Training loss: 0.89337498, Validation loss: 0.89503227, Gradient norm: 0.08440826
INFO:root:[   21] Training loss: 0.89233141, Validation loss: 0.89396597, Gradient norm: 0.08588212
INFO:root:[   22] Training loss: 0.89123389, Validation loss: 0.89438342, Gradient norm: 0.09157089
INFO:root:[   23] Training loss: 0.88956018, Validation loss: 0.89149438, Gradient norm: 0.09475436
INFO:root:[   24] Training loss: 0.88810956, Validation loss: 0.89110451, Gradient norm: 0.08536764
INFO:root:[   25] Training loss: 0.88753252, Validation loss: 0.88967281, Gradient norm: 0.08716293
INFO:root:[   26] Training loss: 0.88657343, Validation loss: 0.88869454, Gradient norm: 0.09980889
INFO:root:[   27] Training loss: 0.88531846, Validation loss: 0.88923382, Gradient norm: 0.09110548
INFO:root:[   28] Training loss: 0.88473546, Validation loss: 0.88925631, Gradient norm: 0.09092117
INFO:root:[   29] Training loss: 0.88367121, Validation loss: 0.88658406, Gradient norm: 0.08863031
INFO:root:[   30] Training loss: 0.88318427, Validation loss: 0.88700644, Gradient norm: 0.11784026
INFO:root:[   31] Training loss: 0.88235733, Validation loss: 0.88764803, Gradient norm: 0.10112837
INFO:root:[   32] Training loss: 0.88153236, Validation loss: 0.88473249, Gradient norm: 0.10013501
INFO:root:[   33] Training loss: 0.88071272, Validation loss: 0.88478617, Gradient norm: 0.10705414
INFO:root:[   34] Training loss: 0.87995985, Validation loss: 0.88501880, Gradient norm: 0.10801947
INFO:root:[   35] Training loss: 0.87941492, Validation loss: 0.88497437, Gradient norm: 0.11439768
INFO:root:[   36] Training loss: 0.87853574, Validation loss: 0.88340733, Gradient norm: 0.11606675
INFO:root:[   37] Training loss: 0.87799547, Validation loss: 0.88123057, Gradient norm: 0.10165295
INFO:root:[   38] Training loss: 0.87742694, Validation loss: 0.88062273, Gradient norm: 0.11577629
INFO:root:[   39] Training loss: 0.87689884, Validation loss: 0.88238071, Gradient norm: 0.12407339
INFO:root:[   40] Training loss: 0.87654933, Validation loss: 0.88053587, Gradient norm: 0.11289719
INFO:root:[   41] Training loss: 0.87651691, Validation loss: 0.88046433, Gradient norm: 0.13279004
INFO:root:[   42] Training loss: 0.87582518, Validation loss: 0.88135878, Gradient norm: 0.12330021
INFO:root:[   43] Training loss: 0.87539530, Validation loss: 0.88099217, Gradient norm: 0.13730442
INFO:root:[   44] Training loss: 0.87518816, Validation loss: 0.88018399, Gradient norm: 0.14140526
INFO:root:[   45] Training loss: 0.87445910, Validation loss: 0.88048048, Gradient norm: 0.14398230
INFO:root:[   46] Training loss: 0.87430161, Validation loss: 0.88018046, Gradient norm: 0.14803039
INFO:root:[   47] Training loss: 0.87286427, Validation loss: 0.87793541, Gradient norm: 0.13298471
INFO:root:[   48] Training loss: 0.87282631, Validation loss: 0.87835106, Gradient norm: 0.14337419
INFO:root:[   49] Training loss: 0.87293390, Validation loss: 0.87871116, Gradient norm: 0.14560347
INFO:root:[   50] Training loss: 0.87263355, Validation loss: 0.87733815, Gradient norm: 0.13609874
INFO:root:[   51] Training loss: 0.87112710, Validation loss: 0.87608718, Gradient norm: 0.12822186
INFO:root:[   52] Training loss: 0.87201057, Validation loss: 0.87959856, Gradient norm: 0.15335214
INFO:root:[   53] Training loss: 0.87102794, Validation loss: 0.87733348, Gradient norm: 0.15574582
INFO:root:[   54] Training loss: 0.87066708, Validation loss: 0.87674034, Gradient norm: 0.16577385
INFO:root:[   55] Training loss: 0.86959955, Validation loss: 0.87601615, Gradient norm: 0.12727532
INFO:root:[   56] Training loss: 0.87047275, Validation loss: 0.87605935, Gradient norm: 0.15240573
INFO:root:[   57] Training loss: 0.86990546, Validation loss: 0.87711104, Gradient norm: 0.16081765
INFO:root:[   58] Training loss: 0.86941961, Validation loss: 0.87518869, Gradient norm: 0.14894786
INFO:root:[   59] Training loss: 0.86868185, Validation loss: 0.87552919, Gradient norm: 0.12276102
INFO:root:[   60] Training loss: 0.86866219, Validation loss: 0.87421277, Gradient norm: 0.15177898
INFO:root:[   61] Training loss: 0.86906330, Validation loss: 0.87458453, Gradient norm: 0.18277505
INFO:root:[   62] Training loss: 0.86897345, Validation loss: 0.87539050, Gradient norm: 0.15894827
INFO:root:[   63] Training loss: 0.86890471, Validation loss: 0.87482797, Gradient norm: 0.19079046
INFO:root:[   64] Training loss: 0.86793826, Validation loss: 0.87560052, Gradient norm: 0.17553690
INFO:root:[   65] Training loss: 0.86746542, Validation loss: 0.87660041, Gradient norm: 0.15113030
INFO:root:[   66] Training loss: 0.86773337, Validation loss: 0.87508227, Gradient norm: 0.19328401
INFO:root:[   67] Training loss: 0.86713111, Validation loss: 0.87350479, Gradient norm: 0.17095761
INFO:root:[   68] Training loss: 0.86695479, Validation loss: 0.87393042, Gradient norm: 0.17827181
INFO:root:[   69] Training loss: 0.86709341, Validation loss: 0.87549591, Gradient norm: 0.19666769
INFO:root:[   70] Training loss: 0.86599166, Validation loss: 0.87254860, Gradient norm: 0.17645934
INFO:root:[   71] Training loss: 0.86620327, Validation loss: 0.87497950, Gradient norm: 0.19014324
INFO:root:[   72] Training loss: 0.86588433, Validation loss: 0.87428227, Gradient norm: 0.18335700
INFO:root:[   73] Training loss: 0.86577362, Validation loss: 0.87546826, Gradient norm: 0.18937353
INFO:root:[   74] Training loss: 0.86597433, Validation loss: 0.87476683, Gradient norm: 0.17992954
INFO:root:[   75] Training loss: 0.86626577, Validation loss: 0.87335525, Gradient norm: 0.22255177
INFO:root:[   76] Training loss: 0.86520258, Validation loss: 0.87457397, Gradient norm: 0.18703373
INFO:root:[   77] Training loss: 0.86479499, Validation loss: 0.87376538, Gradient norm: 0.18507038
INFO:root:[   78] Training loss: 0.86499342, Validation loss: 0.87283267, Gradient norm: 0.22287548
INFO:root:[   79] Training loss: 0.86502383, Validation loss: 0.87721572, Gradient norm: 0.20826427
INFO:root:[   80] Training loss: 0.86426667, Validation loss: 0.87155346, Gradient norm: 0.19985773
INFO:root:[   81] Training loss: 0.86383078, Validation loss: 0.87269881, Gradient norm: 0.18352716
INFO:root:[   82] Training loss: 0.86325171, Validation loss: 0.87251144, Gradient norm: 0.18113968
INFO:root:[   83] Training loss: 0.86376299, Validation loss: 0.87362171, Gradient norm: 0.21643845
INFO:root:[   84] Training loss: 0.86341054, Validation loss: 0.87323500, Gradient norm: 0.19603388
INFO:root:[   85] Training loss: 0.86498111, Validation loss: 0.87587385, Gradient norm: 0.22823532
INFO:root:[   86] Training loss: 0.86340670, Validation loss: 0.87263860, Gradient norm: 0.20391578
INFO:root:[   87] Training loss: 0.86344083, Validation loss: 0.87636623, Gradient norm: 0.23048929
INFO:root:[   88] Training loss: 0.86241437, Validation loss: 0.87147001, Gradient norm: 0.19002619
INFO:root:[   89] Training loss: 0.86265196, Validation loss: 0.87498875, Gradient norm: 0.22248980
INFO:root:[   90] Training loss: 0.86315488, Validation loss: 0.87335490, Gradient norm: 0.19886570
INFO:root:[   91] Training loss: 0.86212263, Validation loss: 0.87306614, Gradient norm: 0.19281091
INFO:root:[   92] Training loss: 0.86216836, Validation loss: 0.87697732, Gradient norm: 0.21979553
INFO:root:[   93] Training loss: 0.86291520, Validation loss: 0.87122205, Gradient norm: 0.23138383
INFO:root:[   94] Training loss: 0.86266187, Validation loss: 0.87321458, Gradient norm: 0.22374802
INFO:root:[   95] Training loss: 0.86165193, Validation loss: 0.87325642, Gradient norm: 0.20916317
INFO:root:[   96] Training loss: 0.86209677, Validation loss: 0.87224290, Gradient norm: 0.22318161
INFO:root:[   97] Training loss: 0.86171348, Validation loss: 0.87242802, Gradient norm: 0.24596855
INFO:root:[   98] Training loss: 0.86138178, Validation loss: 0.87210096, Gradient norm: 0.21215282
INFO:root:[   99] Training loss: 0.86144508, Validation loss: 0.86955586, Gradient norm: 0.23829744
INFO:root:[  100] Training loss: 0.86060850, Validation loss: 0.87045379, Gradient norm: 0.21968072
INFO:root:[  101] Training loss: 0.86133045, Validation loss: 0.87097230, Gradient norm: 0.20318918
INFO:root:[  102] Training loss: 0.86062212, Validation loss: 0.87156722, Gradient norm: 0.20889477
INFO:root:[  103] Training loss: 0.86161748, Validation loss: 0.87275221, Gradient norm: 0.25845258
INFO:root:[  104] Training loss: 0.86098634, Validation loss: 0.87074462, Gradient norm: 0.23285754
INFO:root:[  105] Training loss: 0.86001825, Validation loss: 0.86888638, Gradient norm: 0.20875905
INFO:root:[  106] Training loss: 0.86153079, Validation loss: 0.87106423, Gradient norm: 0.26300023
INFO:root:[  107] Training loss: 0.86091500, Validation loss: 0.87184937, Gradient norm: 0.24787123
INFO:root:[  108] Training loss: 0.86020743, Validation loss: 0.87382165, Gradient norm: 0.24010254
INFO:root:[  109] Training loss: 0.86050415, Validation loss: 0.87004667, Gradient norm: 0.25263925
INFO:root:[  110] Training loss: 0.85975934, Validation loss: 0.87048121, Gradient norm: 0.21431475
INFO:root:[  111] Training loss: 0.85982172, Validation loss: 0.87114157, Gradient norm: 0.24459830
INFO:root:[  112] Training loss: 0.85958266, Validation loss: 0.87016217, Gradient norm: 0.22377306
INFO:root:[  113] Training loss: 0.86055372, Validation loss: 0.87073358, Gradient norm: 0.27057824
INFO:root:[  114] Training loss: 0.85966778, Validation loss: 0.87344507, Gradient norm: 0.23116876
INFO:root:EP 114: Early stopping
INFO:root:Training the model took 3603.269s.
INFO:root:Emptying the cuda cache took 0.024s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.8352
INFO:root:EnergyScoreTrain: 0.73714
INFO:root:CRPSTrain: 0.59755
INFO:root:Gaussian NLLTrain: 484299.47973
INFO:root:CoverageTrain: 0.36743
INFO:root:IntervalWidthTrain: 0.6383
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.84719
INFO:root:EnergyScoreValidation: 0.74942
INFO:root:CRPSValidation: 0.60799
INFO:root:Gaussian NLLValidation: 218081.98018
INFO:root:CoverageValidation: 0.36027
INFO:root:IntervalWidthValidation: 0.63735
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.84915
INFO:root:EnergyScoreTest: 0.75173
INFO:root:CRPSTest: 0.61012
INFO:root:Gaussian NLLTest: 697685.90435
INFO:root:CoverageTest: 0.35812
INFO:root:IntervalWidthTest: 0.63572
INFO:root:###7 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99496417, Validation loss: 0.99075741, Gradient norm: 0.01546231
INFO:root:[    2] Training loss: 0.98477894, Validation loss: 0.97640419, Gradient norm: 0.03483989
INFO:root:[    3] Training loss: 0.97042614, Validation loss: 0.96405338, Gradient norm: 0.08926011
INFO:root:[    4] Training loss: 0.95908059, Validation loss: 0.95614520, Gradient norm: 0.06223788
INFO:root:[    5] Training loss: 0.95149589, Validation loss: 0.94903005, Gradient norm: 0.07633618
INFO:root:[    6] Training loss: 0.94521343, Validation loss: 0.94424979, Gradient norm: 0.06918984
INFO:root:[    7] Training loss: 0.94005100, Validation loss: 0.93765893, Gradient norm: 0.07327014
INFO:root:[    8] Training loss: 0.93516118, Validation loss: 0.93269768, Gradient norm: 0.07986549
INFO:root:[    9] Training loss: 0.93055732, Validation loss: 0.92850200, Gradient norm: 0.08372069
INFO:root:[   10] Training loss: 0.92685424, Validation loss: 0.92453631, Gradient norm: 0.09368384
INFO:root:[   11] Training loss: 0.92262989, Validation loss: 0.92135047, Gradient norm: 0.06586801
INFO:root:[   12] Training loss: 0.92006366, Validation loss: 0.91978809, Gradient norm: 0.09487263
INFO:root:[   13] Training loss: 0.91698400, Validation loss: 0.91686998, Gradient norm: 0.10017553
INFO:root:[   14] Training loss: 0.91456951, Validation loss: 0.91256703, Gradient norm: 0.09817314
INFO:root:[   15] Training loss: 0.91244771, Validation loss: 0.91423554, Gradient norm: 0.12140354
INFO:root:[   16] Training loss: 0.91061096, Validation loss: 0.90995379, Gradient norm: 0.11143731
INFO:root:[   17] Training loss: 0.90817783, Validation loss: 0.90878771, Gradient norm: 0.11302869
INFO:root:[   18] Training loss: 0.90729759, Validation loss: 0.90846820, Gradient norm: 0.13586799
INFO:root:[   19] Training loss: 0.90625789, Validation loss: 0.90610254, Gradient norm: 0.14121228
INFO:root:[   20] Training loss: 0.90414970, Validation loss: 0.90469860, Gradient norm: 0.14029348
INFO:root:[   21] Training loss: 0.90300277, Validation loss: 0.90423971, Gradient norm: 0.13325147
INFO:root:[   22] Training loss: 0.90319418, Validation loss: 0.90468093, Gradient norm: 0.22055319
INFO:root:[   23] Training loss: 0.90160393, Validation loss: 0.90214480, Gradient norm: 0.20234857
INFO:root:[   24] Training loss: 0.90035655, Validation loss: 0.90142683, Gradient norm: 0.16929744
INFO:root:[   25] Training loss: 0.90122902, Validation loss: 0.90265191, Gradient norm: 0.28316305
INFO:root:[   26] Training loss: 0.89933407, Validation loss: 0.90094856, Gradient norm: 0.23185177
INFO:root:[   27] Training loss: 0.89928150, Validation loss: 0.89961391, Gradient norm: 0.28318687
INFO:root:[   28] Training loss: 0.89925248, Validation loss: 0.90169351, Gradient norm: 0.34243990
INFO:root:[   29] Training loss: 0.89744987, Validation loss: 0.90017200, Gradient norm: 0.29683377
INFO:root:[   30] Training loss: 0.89683878, Validation loss: 0.89990588, Gradient norm: 0.32361674
INFO:root:[   31] Training loss: 0.89761341, Validation loss: 0.89777813, Gradient norm: 0.43403341
INFO:root:[   32] Training loss: 0.89664784, Validation loss: 0.89688438, Gradient norm: 0.42820842
INFO:root:[   33] Training loss: 0.89711244, Validation loss: 0.89641014, Gradient norm: 0.49182083
INFO:root:[   34] Training loss: 0.89594243, Validation loss: 0.89684891, Gradient norm: 0.44320446
INFO:root:[   35] Training loss: 0.89492029, Validation loss: 0.89648673, Gradient norm: 0.45372009
INFO:root:[   36] Training loss: 0.89623226, Validation loss: 0.89935862, Gradient norm: 0.59795690
INFO:root:[   37] Training loss: 0.89518432, Validation loss: 0.89514027, Gradient norm: 0.55706859
INFO:root:[   38] Training loss: 0.89531297, Validation loss: 0.89650091, Gradient norm: 0.60673720
INFO:root:[   39] Training loss: 0.89448052, Validation loss: 0.89520818, Gradient norm: 0.60732734
INFO:root:[   40] Training loss: 0.89324772, Validation loss: 0.89595161, Gradient norm: 0.52705798
INFO:root:[   41] Training loss: 0.89446318, Validation loss: 0.89535531, Gradient norm: 0.65645736
INFO:root:[   42] Training loss: 0.89370204, Validation loss: 0.89639034, Gradient norm: 0.68660117
INFO:root:[   43] Training loss: 0.89339646, Validation loss: 0.89629972, Gradient norm: 0.68953552
INFO:root:[   44] Training loss: 0.89320064, Validation loss: 0.89988023, Gradient norm: 0.71422844
INFO:root:[   45] Training loss: 0.89357649, Validation loss: 0.89636208, Gradient norm: 0.75385507
INFO:root:[   46] Training loss: 0.89285253, Validation loss: 0.89731141, Gradient norm: 0.76237272
INFO:root:[   47] Training loss: 0.89179062, Validation loss: 0.89387878, Gradient norm: 0.72679308
INFO:root:[   48] Training loss: 0.89249450, Validation loss: 0.89574808, Gradient norm: 0.81579284
INFO:root:[   49] Training loss: 0.89179042, Validation loss: 0.89469815, Gradient norm: 0.84243334
INFO:root:[   50] Training loss: 0.89150254, Validation loss: 0.89920626, Gradient norm: 0.82991406
INFO:root:[   51] Training loss: 0.89123666, Validation loss: 0.89558639, Gradient norm: 0.85227800
INFO:root:[   52] Training loss: 0.89176414, Validation loss: 0.89469620, Gradient norm: 0.90921374
INFO:root:[   53] Training loss: 0.89096354, Validation loss: 0.89188947, Gradient norm: 0.85296628
INFO:root:[   54] Training loss: 0.89048813, Validation loss: 0.89550436, Gradient norm: 0.91405375
INFO:root:[   55] Training loss: 0.89091789, Validation loss: 0.89620754, Gradient norm: 0.94811042
INFO:root:[   56] Training loss: 0.89055507, Validation loss: 0.89587846, Gradient norm: 0.87861353
INFO:root:[   57] Training loss: 0.89081540, Validation loss: 0.89426531, Gradient norm: 1.00321364
INFO:root:[   58] Training loss: 0.89015557, Validation loss: 0.89424580, Gradient norm: 1.02531301
INFO:root:[   59] Training loss: 0.89026299, Validation loss: 0.89399158, Gradient norm: 1.05575817
INFO:root:[   60] Training loss: 0.89006070, Validation loss: 0.89353623, Gradient norm: 1.05774874
INFO:root:[   61] Training loss: 0.89028961, Validation loss: 0.89222940, Gradient norm: 1.16434709
INFO:root:[   62] Training loss: 0.88997581, Validation loss: 0.89049937, Gradient norm: 1.11857283
INFO:root:[   63] Training loss: 0.89004855, Validation loss: 0.89693510, Gradient norm: 1.11860012
INFO:root:[   64] Training loss: 0.88972355, Validation loss: 0.89017958, Gradient norm: 1.14530795
INFO:root:[   65] Training loss: 0.88939041, Validation loss: 0.89322098, Gradient norm: 1.18406314
INFO:root:[   66] Training loss: 0.88900561, Validation loss: 0.89132579, Gradient norm: 1.15338827
INFO:root:[   67] Training loss: 0.88888276, Validation loss: 0.89156265, Gradient norm: 1.20705900
INFO:root:[   68] Training loss: 0.88923818, Validation loss: 0.89420093, Gradient norm: 1.23804637
INFO:root:[   69] Training loss: 0.88893419, Validation loss: 0.89549963, Gradient norm: 1.29995365
INFO:root:[   70] Training loss: 0.88905456, Validation loss: 0.89441192, Gradient norm: 1.29795461
INFO:root:[   71] Training loss: 0.88897987, Validation loss: 0.89331888, Gradient norm: 1.34219018
INFO:root:[   72] Training loss: 0.88818219, Validation loss: 0.89228578, Gradient norm: 1.28190925
INFO:root:[   73] Training loss: 0.88873441, Validation loss: 0.89433315, Gradient norm: 1.26093505
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 2302.161s.
INFO:root:Emptying the cuda cache took 0.024s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.86752
INFO:root:EnergyScoreTrain: 0.77604
INFO:root:CRPSTrain: 0.64304
INFO:root:Gaussian NLLTrain: 1134996672.85333
INFO:root:CoverageTrain: 0.31001
INFO:root:IntervalWidthTrain: 0.55981
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.87134
INFO:root:EnergyScoreValidation: 0.78014
INFO:root:CRPSValidation: 0.64661
INFO:root:Gaussian NLLValidation: 1128334067.48444
INFO:root:CoverageValidation: 0.30737
INFO:root:IntervalWidthValidation: 0.55954
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.87219
INFO:root:EnergyScoreTest: 0.78126
INFO:root:CRPSTest: 0.64775
INFO:root:Gaussian NLLTest: 1130127633.408
INFO:root:CoverageTest: 0.3061
INFO:root:IntervalWidthTest: 0.5584
INFO:root:###8 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99529707, Validation loss: 0.99158147, Gradient norm: 0.01402344
INFO:root:[    2] Training loss: 0.98796199, Validation loss: 0.98138312, Gradient norm: 0.03296694
INFO:root:[    3] Training loss: 0.97458721, Validation loss: 0.96732757, Gradient norm: 0.05664073
INFO:root:[    4] Training loss: 0.96246715, Validation loss: 0.95803174, Gradient norm: 0.05828957
INFO:root:[    5] Training loss: 0.95287489, Validation loss: 0.94821275, Gradient norm: 0.05925208
INFO:root:[    6] Training loss: 0.94578013, Validation loss: 0.94251612, Gradient norm: 0.07051342
INFO:root:[    7] Training loss: 0.94015266, Validation loss: 0.93737350, Gradient norm: 0.06880455
INFO:root:[    8] Training loss: 0.93543382, Validation loss: 0.93302072, Gradient norm: 0.08286869
INFO:root:[    9] Training loss: 0.93168671, Validation loss: 0.92962457, Gradient norm: 0.08572927
INFO:root:[   10] Training loss: 0.92864566, Validation loss: 0.92604183, Gradient norm: 0.07617666
INFO:root:[   11] Training loss: 0.92578526, Validation loss: 0.92533240, Gradient norm: 0.07874808
INFO:root:[   12] Training loss: 0.92390123, Validation loss: 0.92246903, Gradient norm: 0.09939306
INFO:root:[   13] Training loss: 0.92194764, Validation loss: 0.92177565, Gradient norm: 0.11675927
INFO:root:[   14] Training loss: 0.91998935, Validation loss: 0.91788526, Gradient norm: 0.13696774
INFO:root:[   15] Training loss: 0.91850567, Validation loss: 0.91767477, Gradient norm: 0.13871126
INFO:root:[   16] Training loss: 0.91683335, Validation loss: 0.91624424, Gradient norm: 0.15239321
INFO:root:[   17] Training loss: 0.91552533, Validation loss: 0.91334335, Gradient norm: 0.16886232
INFO:root:[   18] Training loss: 0.91427483, Validation loss: 0.91454044, Gradient norm: 0.19024527
INFO:root:[   19] Training loss: 0.91311618, Validation loss: 0.91413212, Gradient norm: 0.20305696
INFO:root:[   20] Training loss: 0.91212497, Validation loss: 0.91139346, Gradient norm: 0.21570691
INFO:root:[   21] Training loss: 0.91098944, Validation loss: 0.91056573, Gradient norm: 0.19430028
INFO:root:[   22] Training loss: 0.91065584, Validation loss: 0.91083815, Gradient norm: 0.25704518
INFO:root:[   23] Training loss: 0.91071955, Validation loss: 0.91218723, Gradient norm: 0.32530026
INFO:root:[   24] Training loss: 0.90855069, Validation loss: 0.91364356, Gradient norm: 0.27300849
INFO:root:[   25] Training loss: 0.90842119, Validation loss: 0.90739608, Gradient norm: 0.29371618
INFO:root:[   26] Training loss: 0.90884663, Validation loss: 0.90801917, Gradient norm: 0.39373805
INFO:root:[   27] Training loss: 0.90769419, Validation loss: 0.90632835, Gradient norm: 0.37019036
INFO:root:[   28] Training loss: 0.90642765, Validation loss: 0.90773246, Gradient norm: 0.34831353
INFO:root:[   29] Training loss: 0.90746498, Validation loss: 0.90885812, Gradient norm: 0.44454620
INFO:root:[   30] Training loss: 0.90714302, Validation loss: 0.90614515, Gradient norm: 0.48196473
INFO:root:[   31] Training loss: 0.90609928, Validation loss: 0.90499698, Gradient norm: 0.46300039
INFO:root:[   32] Training loss: 0.90575135, Validation loss: 0.91220436, Gradient norm: 0.50855911
INFO:root:[   33] Training loss: 0.90522284, Validation loss: 0.90580435, Gradient norm: 0.48458373
INFO:root:[   34] Training loss: 0.90541789, Validation loss: 0.90490560, Gradient norm: 0.55092947
INFO:root:[   35] Training loss: 0.90416523, Validation loss: 0.90625424, Gradient norm: 0.51561250
INFO:root:[   36] Training loss: 0.90443317, Validation loss: 0.90740540, Gradient norm: 0.59705344
INFO:root:[   37] Training loss: 0.90528336, Validation loss: 0.90549438, Gradient norm: 0.65434088
INFO:root:[   38] Training loss: 0.90395693, Validation loss: 0.90778008, Gradient norm: 0.64839442
INFO:root:[   39] Training loss: 0.90344460, Validation loss: 0.90401006, Gradient norm: 0.64844654
INFO:root:[   40] Training loss: 0.90386574, Validation loss: 0.90704001, Gradient norm: 0.71117251
INFO:root:[   41] Training loss: 0.90313920, Validation loss: 0.90151181, Gradient norm: 0.68288201
INFO:root:[   42] Training loss: 0.90286023, Validation loss: 0.90249826, Gradient norm: 0.74172135
INFO:root:[   43] Training loss: 0.90213327, Validation loss: 0.90197673, Gradient norm: 0.70023678
INFO:root:[   44] Training loss: 0.90239523, Validation loss: 0.90145654, Gradient norm: 0.73056464
INFO:root:[   45] Training loss: 0.90287053, Validation loss: 0.90149364, Gradient norm: 0.86110932
INFO:root:[   46] Training loss: 0.90187846, Validation loss: 0.90183066, Gradient norm: 0.73579838
INFO:root:[   47] Training loss: 0.90215987, Validation loss: 0.90375238, Gradient norm: 0.85047611
INFO:root:[   48] Training loss: 0.90177473, Validation loss: 0.90116791, Gradient norm: 0.87640585
INFO:root:[   49] Training loss: 0.90133839, Validation loss: 0.90503723, Gradient norm: 0.84845993
INFO:root:[   50] Training loss: 0.90208820, Validation loss: 0.90439784, Gradient norm: 0.96071147
INFO:root:[   51] Training loss: 0.90144650, Validation loss: 0.89995695, Gradient norm: 0.93773734
INFO:root:[   52] Training loss: 0.90162070, Validation loss: 0.90346912, Gradient norm: 1.01729313
INFO:root:[   53] Training loss: 0.90099533, Validation loss: 0.90306382, Gradient norm: 0.97699874
INFO:root:[   54] Training loss: 0.90110594, Validation loss: 0.90539040, Gradient norm: 1.05945876
INFO:root:[   55] Training loss: 0.90067992, Validation loss: 0.90445649, Gradient norm: 1.01189764
INFO:root:[   56] Training loss: 0.90117897, Validation loss: 0.90022161, Gradient norm: 1.05124842
INFO:root:[   57] Training loss: 0.90037345, Validation loss: 0.90049458, Gradient norm: 1.06171657
INFO:root:[   58] Training loss: 0.90028565, Validation loss: 0.90030917, Gradient norm: 1.09173729
INFO:root:[   59] Training loss: 0.90058640, Validation loss: 0.90007316, Gradient norm: 1.16098481
INFO:root:[   60] Training loss: 0.90031091, Validation loss: 0.90332069, Gradient norm: 1.11983554
INFO:root:[   61] Training loss: 0.90022533, Validation loss: 0.90268061, Gradient norm: 1.19597375
INFO:root:[   62] Training loss: 0.90032614, Validation loss: 0.90162625, Gradient norm: 1.28627972
INFO:root:[   63] Training loss: 0.89993979, Validation loss: 0.90180766, Gradient norm: 1.19080150
INFO:root:[   64] Training loss: 0.90003023, Validation loss: 0.90100529, Gradient norm: 1.26653629
INFO:root:[   65] Training loss: 0.89963121, Validation loss: 0.90345132, Gradient norm: 1.27386459
INFO:root:[   66] Training loss: 0.89982729, Validation loss: 0.90505868, Gradient norm: 1.36751425
INFO:root:[   67] Training loss: 0.89971181, Validation loss: 0.90213087, Gradient norm: 1.41405537
INFO:root:[   68] Training loss: 0.89960949, Validation loss: 0.90188712, Gradient norm: 1.39823759
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 2145.415s.
INFO:root:Emptying the cuda cache took 0.025s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.87445
INFO:root:EnergyScoreTrain: 0.77256
INFO:root:CRPSTrain: 0.64873
INFO:root:Gaussian NLLTrain: 857060686.43556
INFO:root:CoverageTrain: 0.31848
INFO:root:IntervalWidthTrain: 0.60661
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.87677
INFO:root:EnergyScoreValidation: 0.77512
INFO:root:CRPSValidation: 0.65089
INFO:root:Gaussian NLLValidation: 870100781.51111
INFO:root:CoverageValidation: 0.317
INFO:root:IntervalWidthValidation: 0.60671
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.87742
INFO:root:EnergyScoreTest: 0.77596
INFO:root:CRPSTest: 0.65192
INFO:root:Gaussian NLLTest: 870195863.552
INFO:root:CoverageTest: 0.3154
INFO:root:IntervalWidthTest: 0.60545
INFO:root:###9 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.99592193, Validation loss: 0.99235116, Gradient norm: 0.01322509
INFO:root:[    2] Training loss: 0.99076635, Validation loss: 0.98718963, Gradient norm: 0.02748280
INFO:root:[    3] Training loss: 0.98164384, Validation loss: 0.97576315, Gradient norm: 0.05587912
INFO:root:[    4] Training loss: 0.97127378, Validation loss: 0.96788795, Gradient norm: 0.05413404
INFO:root:[    5] Training loss: 0.96431523, Validation loss: 0.96170353, Gradient norm: 0.06729197
INFO:root:[    6] Training loss: 0.95898394, Validation loss: 0.95789698, Gradient norm: 0.06330447
INFO:root:[    7] Training loss: 0.95546552, Validation loss: 0.95361819, Gradient norm: 0.08045856
INFO:root:[    8] Training loss: 0.95242347, Validation loss: 0.95202868, Gradient norm: 0.09548096
INFO:root:[    9] Training loss: 0.95022047, Validation loss: 0.94862837, Gradient norm: 0.12639895
INFO:root:[   10] Training loss: 0.94788756, Validation loss: 0.94621045, Gradient norm: 0.13031206
INFO:root:[   11] Training loss: 0.94626999, Validation loss: 0.94621475, Gradient norm: 0.14899248
INFO:root:[   12] Training loss: 0.94489139, Validation loss: 0.94643954, Gradient norm: 0.19190628
INFO:root:[   13] Training loss: 0.94337268, Validation loss: 0.94323138, Gradient norm: 0.22109995
INFO:root:[   14] Training loss: 0.94213009, Validation loss: 0.94061613, Gradient norm: 0.24333560
INFO:root:[   15] Training loss: 0.94185922, Validation loss: 0.94136780, Gradient norm: 0.31904231
INFO:root:[   16] Training loss: 0.94031034, Validation loss: 0.93935132, Gradient norm: 0.33134017
INFO:root:[   17] Training loss: 0.93926925, Validation loss: 0.93834659, Gradient norm: 0.36534153
INFO:root:[   18] Training loss: 0.93973054, Validation loss: 0.93785658, Gradient norm: 0.47618395
INFO:root:[   19] Training loss: 0.93747766, Validation loss: 0.93714727, Gradient norm: 0.46022179
INFO:root:[   20] Training loss: 0.93692093, Validation loss: 0.93646120, Gradient norm: 0.50090612
INFO:root:[   21] Training loss: 0.93720916, Validation loss: 0.93711676, Gradient norm: 0.59372201
INFO:root:[   22] Training loss: 0.93668138, Validation loss: 0.93951588, Gradient norm: 0.67801767
INFO:root:[   23] Training loss: 0.93625785, Validation loss: 0.93298476, Gradient norm: 0.68661834
INFO:root:[   24] Training loss: 0.93517700, Validation loss: 0.93539699, Gradient norm: 0.72163437
INFO:root:[   25] Training loss: 0.93471425, Validation loss: 0.93811668, Gradient norm: 0.78775748
INFO:root:[   26] Training loss: 0.93427172, Validation loss: 0.93632434, Gradient norm: 0.80782743
INFO:root:[   27] Training loss: 0.93467667, Validation loss: 0.93070487, Gradient norm: 0.95278749
INFO:root:[   28] Training loss: 0.93402263, Validation loss: 0.93278397, Gradient norm: 0.99837019
INFO:root:[   29] Training loss: 0.93317592, Validation loss: 0.93564329, Gradient norm: 0.97596271
INFO:root:[   30] Training loss: 0.93365983, Validation loss: 0.93219980, Gradient norm: 1.10111203
INFO:root:[   31] Training loss: 0.93352424, Validation loss: 0.93323347, Gradient norm: 1.20163346
INFO:root:[   32] Training loss: 0.93250436, Validation loss: 0.93315247, Gradient norm: 1.21720436
INFO:root:[   33] Training loss: 0.93265465, Validation loss: 0.93219541, Gradient norm: 1.30001525
INFO:root:[   34] Training loss: 0.93254411, Validation loss: 0.92981811, Gradient norm: 1.29460787
INFO:root:[   35] Training loss: 0.93209753, Validation loss: 0.93229084, Gradient norm: 1.37420284
INFO:root:[   36] Training loss: 0.93204272, Validation loss: 0.93417869, Gradient norm: 1.42881087
INFO:root:[   37] Training loss: 0.93280831, Validation loss: 0.93362003, Gradient norm: 1.58636252
INFO:root:[   38] Training loss: 0.93221156, Validation loss: 0.93454020, Gradient norm: 1.62449959
INFO:root:[   39] Training loss: 0.93202456, Validation loss: 0.92884853, Gradient norm: 1.77674789
INFO:root:[   40] Training loss: 0.93142253, Validation loss: 0.93762740, Gradient norm: 1.73026961
INFO:root:[   41] Training loss: 0.93235419, Validation loss: 0.93379807, Gradient norm: 1.94655092
INFO:root:[   42] Training loss: 0.93229526, Validation loss: 0.92913665, Gradient norm: 2.00626294
INFO:root:[   43] Training loss: 0.93145214, Validation loss: 0.93268621, Gradient norm: 2.05679414
INFO:root:[   44] Training loss: 0.93157843, Validation loss: 0.93160770, Gradient norm: 2.20654361
INFO:root:[   45] Training loss: 0.93135507, Validation loss: 0.93090592, Gradient norm: 2.22502930
INFO:root:[   46] Training loss: 0.93130152, Validation loss: 0.93295078, Gradient norm: 2.31376398
INFO:root:[   47] Training loss: 0.93164324, Validation loss: 0.93248962, Gradient norm: 2.39847979
INFO:root:[   48] Training loss: 0.93255375, Validation loss: 0.93098632, Gradient norm: 2.73631624
INFO:root:[   49] Training loss: 0.93159831, Validation loss: 0.93287067, Gradient norm: 2.60779961
INFO:root:[   50] Training loss: 0.93202999, Validation loss: 0.92796922, Gradient norm: 2.82071268
INFO:root:[   51] Training loss: 0.93087002, Validation loss: 0.93047561, Gradient norm: 2.81857802
INFO:root:[   52] Training loss: 0.93180840, Validation loss: 0.92888468, Gradient norm: 3.06924407
INFO:root:[   53] Training loss: 0.93147479, Validation loss: 0.93924430, Gradient norm: 3.08023939
INFO:root:[   54] Training loss: 0.93220765, Validation loss: 0.93734508, Gradient norm: 3.33406091
INFO:root:[   55] Training loss: 0.93171151, Validation loss: 0.93346374, Gradient norm: 3.48323346
INFO:root:[   56] Training loss: 0.93210148, Validation loss: 0.93261958, Gradient norm: 3.64225973
INFO:root:[   57] Training loss: 0.93307571, Validation loss: 0.93231344, Gradient norm: 3.82003979
INFO:root:[   58] Training loss: 0.93166659, Validation loss: 0.92876469, Gradient norm: 3.82133578
INFO:root:[   59] Training loss: 0.93341751, Validation loss: 0.93003735, Gradient norm: 4.18578021
INFO:root:[   60] Training loss: 0.93237992, Validation loss: 0.93494414, Gradient norm: 4.23192198
INFO:root:[   61] Training loss: 0.93271013, Validation loss: 0.93156562, Gradient norm: 4.21022798
INFO:root:[   62] Training loss: 0.93205091, Validation loss: 0.93128056, Gradient norm: 4.28510965
INFO:root:[   63] Training loss: 0.93324956, Validation loss: 0.93535622, Gradient norm: 4.57672961
INFO:root:[   64] Training loss: 0.93369384, Validation loss: 0.93198507, Gradient norm: 4.79631808
INFO:root:[   65] Training loss: 0.93468584, Validation loss: 0.93355780, Gradient norm: 4.95498035
INFO:root:[   66] Training loss: 0.93503165, Validation loss: 0.93243389, Gradient norm: 5.18826421
INFO:root:[   67] Training loss: 0.93357325, Validation loss: 0.93477163, Gradient norm: 5.30508071
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 2118.51s.
INFO:root:Emptying the cuda cache took 0.023s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.90983
INFO:root:EnergyScoreTrain: 0.81805
INFO:root:CRPSTrain: 0.70119
INFO:root:Gaussian NLLTrain: 5097836851.2
INFO:root:CoverageTrain: 0.24698
INFO:root:IntervalWidthTrain: 0.51326
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.91015
INFO:root:EnergyScoreValidation: 0.81838
INFO:root:CRPSValidation: 0.70154
INFO:root:Gaussian NLLValidation: 5114276818.48889
INFO:root:CoverageValidation: 0.24697
INFO:root:IntervalWidthValidation: 0.51464
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.91133
INFO:root:EnergyScoreTest: 0.82017
INFO:root:CRPSTest: 0.7033
INFO:root:Gaussian NLLTest: 5119055114.24
INFO:root:CoverageTest: 0.24483
INFO:root:IntervalWidthTest: 0.51159
INFO:root:###10 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.98561440, Validation loss: 1.12869504, Gradient norm: 7.62758216
INFO:root:[    2] Training loss: 0.84734199, Validation loss: 0.81382089, Gradient norm: 52.46150950
INFO:root:[    3] Training loss: 0.78688717, Validation loss: 0.73376609, Gradient norm: 64.99455064
INFO:root:[    4] Training loss: 0.76773158, Validation loss: 0.75299788, Gradient norm: 65.07335606
INFO:root:[    5] Training loss: 0.75508048, Validation loss: 0.76144353, Gradient norm: 54.71475334
INFO:root:[    6] Training loss: 0.75579724, Validation loss: 0.74701976, Gradient norm: 59.14458675
INFO:root:[    7] Training loss: 0.74921209, Validation loss: 0.72993333, Gradient norm: 56.30824518
INFO:root:[    8] Training loss: 0.74392956, Validation loss: 0.72314725, Gradient norm: 47.68768764
INFO:root:[    9] Training loss: 0.74493021, Validation loss: 0.72689467, Gradient norm: 49.96698516
INFO:root:[   10] Training loss: 0.74027909, Validation loss: 0.73029542, Gradient norm: 51.09633125
INFO:root:[   11] Training loss: 0.73501989, Validation loss: 0.81257064, Gradient norm: 48.49656788
INFO:root:[   12] Training loss: 0.73220192, Validation loss: 0.75017459, Gradient norm: 43.09951518
INFO:root:[   13] Training loss: 0.72517982, Validation loss: 0.71184610, Gradient norm: 47.64609406
INFO:root:[   14] Training loss: 0.72624862, Validation loss: 0.73838242, Gradient norm: 41.48461771
INFO:root:[   15] Training loss: 0.72096331, Validation loss: 0.73827398, Gradient norm: 46.96020949
INFO:root:[   16] Training loss: 0.71954987, Validation loss: 0.71241131, Gradient norm: 42.58813337
INFO:root:[   17] Training loss: 0.71420465, Validation loss: 0.69823761, Gradient norm: 41.77494950
INFO:root:[   18] Training loss: 0.71428835, Validation loss: 0.70351447, Gradient norm: 41.06895329
INFO:root:[   19] Training loss: 0.71059696, Validation loss: 0.69965984, Gradient norm: 42.92257554
INFO:root:[   20] Training loss: 0.70761201, Validation loss: 0.73376164, Gradient norm: 38.78561863
INFO:root:[   21] Training loss: 0.70429401, Validation loss: 0.68949117, Gradient norm: 41.59192658
INFO:root:[   22] Training loss: 0.70295608, Validation loss: 0.68906326, Gradient norm: 39.07423905
INFO:root:[   23] Training loss: 0.70167952, Validation loss: 0.69325834, Gradient norm: 39.25526472
INFO:root:[   24] Training loss: 0.69750429, Validation loss: 0.69252677, Gradient norm: 37.34833497
INFO:root:[   25] Training loss: 0.69369882, Validation loss: 0.69041205, Gradient norm: 34.79685532
INFO:root:[   26] Training loss: 0.69541581, Validation loss: 0.70101534, Gradient norm: 40.69494369
INFO:root:[   27] Training loss: 0.69082336, Validation loss: 0.72807764, Gradient norm: 34.49787359
INFO:root:[   28] Training loss: 0.69099284, Validation loss: 0.68991952, Gradient norm: 37.32312748
INFO:root:[   29] Training loss: 0.68824767, Validation loss: 0.72151612, Gradient norm: 35.62649688
INFO:root:[   30] Training loss: 0.68933354, Validation loss: 0.67939400, Gradient norm: 36.64739033
INFO:root:[   31] Training loss: 0.68403687, Validation loss: 0.67662053, Gradient norm: 33.54538667
INFO:root:[   32] Training loss: 0.68629209, Validation loss: 0.68180160, Gradient norm: 32.31750292
INFO:root:[   33] Training loss: 0.68091224, Validation loss: 0.67830491, Gradient norm: 34.45592149
INFO:root:[   34] Training loss: 0.68387593, Validation loss: 0.72145317, Gradient norm: 33.85404135
INFO:root:[   35] Training loss: 0.68103339, Validation loss: 0.69289959, Gradient norm: 34.16445106
INFO:root:[   36] Training loss: 0.67993763, Validation loss: 0.68094102, Gradient norm: 32.20106815
INFO:root:[   37] Training loss: 0.67816677, Validation loss: 0.66940633, Gradient norm: 31.49305352
INFO:root:[   38] Training loss: 0.67668747, Validation loss: 0.66936706, Gradient norm: 29.73374862
INFO:root:[   39] Training loss: 0.67554261, Validation loss: 0.67133143, Gradient norm: 30.25601957
INFO:root:[   40] Training loss: 0.67385648, Validation loss: 0.66991546, Gradient norm: 31.27416563
INFO:root:[   41] Training loss: 0.67510776, Validation loss: 0.68793170, Gradient norm: 31.06810941
INFO:root:[   42] Training loss: 0.66950707, Validation loss: 0.66954942, Gradient norm: 27.42048433
INFO:root:[   43] Training loss: 0.67170656, Validation loss: 0.66677230, Gradient norm: 29.11670723
INFO:root:[   44] Training loss: 0.67225754, Validation loss: 0.68881091, Gradient norm: 30.57788499
INFO:root:[   45] Training loss: 0.66719787, Validation loss: 0.71115133, Gradient norm: 26.14768057
INFO:root:[   46] Training loss: 0.67042792, Validation loss: 0.66520968, Gradient norm: 30.11100629
INFO:root:[   47] Training loss: 0.66654567, Validation loss: 0.66299063, Gradient norm: 27.42073530
INFO:root:[   48] Training loss: 0.66643638, Validation loss: 0.69780580, Gradient norm: 27.41874857
INFO:root:[   49] Training loss: 0.66639843, Validation loss: 0.66158868, Gradient norm: 26.47353335
INFO:root:[   50] Training loss: 0.66498767, Validation loss: 0.66213107, Gradient norm: 26.72978381
INFO:root:[   51] Training loss: 0.66353131, Validation loss: 0.68754114, Gradient norm: 25.59851589
INFO:root:[   52] Training loss: 0.66387724, Validation loss: 0.66148477, Gradient norm: 25.54345901
INFO:root:[   53] Training loss: 0.66179476, Validation loss: 0.67785807, Gradient norm: 24.46594721
INFO:root:[   54] Training loss: 0.66246660, Validation loss: 0.66757827, Gradient norm: 26.67473441
INFO:root:[   55] Training loss: 0.66116202, Validation loss: 0.68522436, Gradient norm: 24.33472140
INFO:root:[   56] Training loss: 0.66015328, Validation loss: 0.65893372, Gradient norm: 24.22702509
INFO:root:[   57] Training loss: 0.66000842, Validation loss: 0.67007512, Gradient norm: 25.72914202
INFO:root:[   58] Training loss: 0.65896036, Validation loss: 0.65905053, Gradient norm: 23.88586124
INFO:root:[   59] Training loss: 0.65679278, Validation loss: 0.65919740, Gradient norm: 22.23474020
INFO:root:[   60] Training loss: 0.65767414, Validation loss: 0.68066890, Gradient norm: 24.24057925
INFO:root:[   61] Training loss: 0.65684464, Validation loss: 0.69148895, Gradient norm: 22.79208445
INFO:root:[   62] Training loss: 0.65618691, Validation loss: 0.66185667, Gradient norm: 24.63411136
INFO:root:[   63] Training loss: 0.65488331, Validation loss: 0.67920886, Gradient norm: 22.69025521
INFO:root:[   64] Training loss: 0.65497034, Validation loss: 0.65554105, Gradient norm: 24.13641910
INFO:root:[   65] Training loss: 0.65365982, Validation loss: 0.66728901, Gradient norm: 23.19900600
INFO:root:[   66] Training loss: 0.65373136, Validation loss: 0.66206544, Gradient norm: 23.27026384
INFO:root:[   67] Training loss: 0.65249410, Validation loss: 0.65228729, Gradient norm: 21.04863295
INFO:root:[   68] Training loss: 0.65277754, Validation loss: 0.65843729, Gradient norm: 22.16938548
INFO:root:[   69] Training loss: 0.65304332, Validation loss: 0.66045817, Gradient norm: 22.77948101
INFO:root:[   70] Training loss: 0.65026803, Validation loss: 0.65221405, Gradient norm: 20.37844745
INFO:root:[   71] Training loss: 0.65035998, Validation loss: 0.65527998, Gradient norm: 21.47814307
INFO:root:[   72] Training loss: 0.65035589, Validation loss: 0.65296352, Gradient norm: 21.36410879
INFO:root:[   73] Training loss: 0.64826180, Validation loss: 0.65159130, Gradient norm: 20.52098258
INFO:root:[   74] Training loss: 0.64974650, Validation loss: 0.65486185, Gradient norm: 21.05814506
INFO:root:[   75] Training loss: 0.64917921, Validation loss: 0.65821262, Gradient norm: 21.52537508
INFO:root:[   76] Training loss: 0.64575326, Validation loss: 0.65470975, Gradient norm: 17.67841219
INFO:root:[   77] Training loss: 0.64953281, Validation loss: 0.66289698, Gradient norm: 23.04537370
INFO:root:[   78] Training loss: 0.64577575, Validation loss: 0.65226032, Gradient norm: 19.36843691
INFO:root:[   79] Training loss: 0.64607553, Validation loss: 0.65999766, Gradient norm: 20.00468173
INFO:root:[   80] Training loss: 0.64560525, Validation loss: 0.64935080, Gradient norm: 19.56158434
INFO:root:[   81] Training loss: 0.64497315, Validation loss: 0.64975303, Gradient norm: 19.52646538
INFO:root:[   82] Training loss: 0.64400156, Validation loss: 0.64971195, Gradient norm: 19.52499266
INFO:root:[   83] Training loss: 0.64362378, Validation loss: 0.64745731, Gradient norm: 18.98278785
INFO:root:[   84] Training loss: 0.64413884, Validation loss: 0.65089032, Gradient norm: 20.67898602
INFO:root:[   85] Training loss: 0.64273349, Validation loss: 0.65324225, Gradient norm: 17.54567782
INFO:root:[   86] Training loss: 0.64220938, Validation loss: 0.65987573, Gradient norm: 19.08855663
INFO:root:[   87] Training loss: 0.64191432, Validation loss: 0.65540007, Gradient norm: 19.41324473
INFO:root:[   88] Training loss: 0.64089168, Validation loss: 0.64826453, Gradient norm: 18.27838318
INFO:root:[   89] Training loss: 0.64090398, Validation loss: 0.66312594, Gradient norm: 18.27958809
INFO:root:[   90] Training loss: 0.64032973, Validation loss: 0.65320225, Gradient norm: 18.10836711
INFO:root:[   91] Training loss: 0.63929424, Validation loss: 0.64726941, Gradient norm: 17.70673383
INFO:root:[   92] Training loss: 0.63954598, Validation loss: 0.64400054, Gradient norm: 17.34240239
INFO:root:[   93] Training loss: 0.63858763, Validation loss: 0.66765345, Gradient norm: 17.02895912
INFO:root:[   94] Training loss: 0.64014833, Validation loss: 0.64730474, Gradient norm: 18.15283367
INFO:root:[   95] Training loss: 0.63762370, Validation loss: 0.64413350, Gradient norm: 16.16546190
INFO:root:[   96] Training loss: 0.63768973, Validation loss: 0.64618446, Gradient norm: 17.43330049
INFO:root:[   97] Training loss: 0.63706062, Validation loss: 0.64540229, Gradient norm: 17.00521533
INFO:root:[   98] Training loss: 0.63718340, Validation loss: 0.64617875, Gradient norm: 17.33275812
INFO:root:[   99] Training loss: 0.63653930, Validation loss: 0.65123161, Gradient norm: 16.39942566
INFO:root:[  100] Training loss: 0.63670794, Validation loss: 0.66046980, Gradient norm: 16.59939753
INFO:root:[  101] Training loss: 0.63474797, Validation loss: 0.64727848, Gradient norm: 16.18315289
INFO:root:EP 101: Early stopping
INFO:root:Training the model took 3687.469s.
INFO:root:Emptying the cuda cache took 0.053s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.87626
INFO:root:EnergyScoreTrain: 0.6322
INFO:root:CRPSTrain: 0.66603
INFO:root:Gaussian NLLTrain: 69.13382
INFO:root:CoverageTrain: 0.22054
INFO:root:IntervalWidthTrain: 0.43403
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.88919
INFO:root:EnergyScoreValidation: 0.64189
INFO:root:CRPSValidation: 0.67807
INFO:root:Gaussian NLLValidation: 69.55263
INFO:root:CoverageValidation: 0.21545
INFO:root:IntervalWidthValidation: 0.43366
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.89085
INFO:root:EnergyScoreTest: 0.64322
INFO:root:CRPSTest: 0.68009
INFO:root:Gaussian NLLTest: 70.20014
INFO:root:CoverageTest: 0.21352
INFO:root:IntervalWidthTest: 0.43332
INFO:root:###11 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.87008141, Validation loss: 0.75065516, Gradient norm: 14.71675092
INFO:root:[    2] Training loss: 0.74460889, Validation loss: 0.73171994, Gradient norm: 22.56770005
INFO:root:[    3] Training loss: 0.73430946, Validation loss: 0.72641007, Gradient norm: 19.20593832
INFO:root:[    4] Training loss: 0.73315776, Validation loss: 0.74853266, Gradient norm: 18.60810597
INFO:root:[    5] Training loss: 0.73039062, Validation loss: 0.76889294, Gradient norm: 16.62586324
INFO:root:[    6] Training loss: 0.73104198, Validation loss: 0.73504504, Gradient norm: 16.89921498
INFO:root:[    7] Training loss: 0.72975364, Validation loss: 0.72013181, Gradient norm: 16.52559483
INFO:root:[    8] Training loss: 0.72745531, Validation loss: 0.71954103, Gradient norm: 17.03258059
INFO:root:[    9] Training loss: 0.71923282, Validation loss: 0.71515553, Gradient norm: 15.39471050
INFO:root:[   10] Training loss: 0.71599390, Validation loss: 0.70371246, Gradient norm: 16.04748977
INFO:root:[   11] Training loss: 0.70842843, Validation loss: 0.71042593, Gradient norm: 15.01413403
INFO:root:[   12] Training loss: 0.70184278, Validation loss: 0.72003882, Gradient norm: 14.27651584
INFO:root:[   13] Training loss: 0.69692178, Validation loss: 0.69851509, Gradient norm: 13.56467206
INFO:root:[   14] Training loss: 0.69269506, Validation loss: 0.68885682, Gradient norm: 14.02836467
INFO:root:[   15] Training loss: 0.68583077, Validation loss: 0.70433223, Gradient norm: 10.78912195
INFO:root:[   16] Training loss: 0.68629890, Validation loss: 0.68116465, Gradient norm: 13.48981740
INFO:root:[   17] Training loss: 0.68292887, Validation loss: 0.68086330, Gradient norm: 12.91337301
INFO:root:[   18] Training loss: 0.67996560, Validation loss: 0.69440172, Gradient norm: 12.22880839
INFO:root:[   19] Training loss: 0.67689379, Validation loss: 0.67891430, Gradient norm: 11.74402742
INFO:root:[   20] Training loss: 0.67502088, Validation loss: 0.67482027, Gradient norm: 11.42740354
INFO:root:[   21] Training loss: 0.67204740, Validation loss: 0.67257679, Gradient norm: 10.26102705
INFO:root:[   22] Training loss: 0.67316080, Validation loss: 0.67511887, Gradient norm: 12.40826476
INFO:root:[   23] Training loss: 0.66985886, Validation loss: 0.67430149, Gradient norm: 10.96681613
INFO:root:[   24] Training loss: 0.66929235, Validation loss: 0.67238025, Gradient norm: 10.81304975
INFO:root:[   25] Training loss: 0.66839416, Validation loss: 0.67044497, Gradient norm: 11.22788611
INFO:root:[   26] Training loss: 0.66757982, Validation loss: 0.67156185, Gradient norm: 11.27997183
INFO:root:[   27] Training loss: 0.66517979, Validation loss: 0.67738386, Gradient norm: 9.90002618
INFO:root:[   28] Training loss: 0.66432390, Validation loss: 0.67376350, Gradient norm: 10.69303065
INFO:root:[   29] Training loss: 0.66177883, Validation loss: 0.66444334, Gradient norm: 8.50841773
INFO:root:[   30] Training loss: 0.66214824, Validation loss: 0.66317119, Gradient norm: 9.40908292
INFO:root:[   31] Training loss: 0.66100231, Validation loss: 0.66167595, Gradient norm: 9.25900223
INFO:root:[   32] Training loss: 0.65869963, Validation loss: 0.65971541, Gradient norm: 8.31426216
INFO:root:[   33] Training loss: 0.66006574, Validation loss: 0.66441611, Gradient norm: 10.26601097
INFO:root:[   34] Training loss: 0.65924216, Validation loss: 0.66013995, Gradient norm: 10.59290365
INFO:root:[   35] Training loss: 0.65789337, Validation loss: 0.65873679, Gradient norm: 8.75165462
INFO:root:[   36] Training loss: 0.65674662, Validation loss: 0.66458045, Gradient norm: 9.17740945
INFO:root:[   37] Training loss: 0.65599783, Validation loss: 0.66467013, Gradient norm: 8.76114325
INFO:root:[   38] Training loss: 0.65470619, Validation loss: 0.65751011, Gradient norm: 8.55771231
INFO:root:[   39] Training loss: 0.65451067, Validation loss: 0.65610391, Gradient norm: 9.35382384
INFO:root:[   40] Training loss: 0.65268014, Validation loss: 0.65575684, Gradient norm: 7.55250474
INFO:root:[   41] Training loss: 0.65330640, Validation loss: 0.66058695, Gradient norm: 8.96858387
INFO:root:[   42] Training loss: 0.65232298, Validation loss: 0.65374786, Gradient norm: 8.67071522
INFO:root:[   43] Training loss: 0.65016792, Validation loss: 0.65352207, Gradient norm: 7.21584549
INFO:root:[   44] Training loss: 0.65019928, Validation loss: 0.66631223, Gradient norm: 7.74618126
INFO:root:[   45] Training loss: 0.65059316, Validation loss: 0.65126018, Gradient norm: 9.15856374
INFO:root:[   46] Training loss: 0.64760349, Validation loss: 0.65173971, Gradient norm: 6.70042929
INFO:root:[   47] Training loss: 0.64853270, Validation loss: 0.65265676, Gradient norm: 8.13811178
INFO:root:[   48] Training loss: 0.64676305, Validation loss: 0.65690131, Gradient norm: 7.67265979
INFO:root:[   49] Training loss: 0.64629297, Validation loss: 0.64902948, Gradient norm: 7.68625709
INFO:root:[   50] Training loss: 0.64655611, Validation loss: 0.65386947, Gradient norm: 7.96121009
INFO:root:[   51] Training loss: 0.64520545, Validation loss: 0.66073930, Gradient norm: 7.41023830
INFO:root:[   52] Training loss: 0.64362425, Validation loss: 0.65177364, Gradient norm: 7.03584920
INFO:root:[   53] Training loss: 0.64384123, Validation loss: 0.64799947, Gradient norm: 7.35004427
INFO:root:[   54] Training loss: 0.64255568, Validation loss: 0.65030329, Gradient norm: 6.49307470
INFO:root:[   55] Training loss: 0.64217050, Validation loss: 0.65381479, Gradient norm: 6.95368273
INFO:root:[   56] Training loss: 0.64114588, Validation loss: 0.64874205, Gradient norm: 6.29613990
INFO:root:[   57] Training loss: 0.64127898, Validation loss: 0.64539374, Gradient norm: 6.97019169
INFO:root:[   58] Training loss: 0.64065035, Validation loss: 0.65309104, Gradient norm: 5.69227714
INFO:root:[   59] Training loss: 0.64042497, Validation loss: 0.65347702, Gradient norm: 7.88115974
INFO:root:[   60] Training loss: 0.63977677, Validation loss: 0.64380495, Gradient norm: 7.87206706
INFO:root:[   61] Training loss: 0.63847079, Validation loss: 0.64848431, Gradient norm: 5.86635952
INFO:root:[   62] Training loss: 0.63830556, Validation loss: 0.65416833, Gradient norm: 6.83628388
INFO:root:[   63] Training loss: 0.63745092, Validation loss: 0.65018675, Gradient norm: 6.77122535
INFO:root:[   64] Training loss: 0.63758967, Validation loss: 0.65523521, Gradient norm: 7.06127774
INFO:root:[   65] Training loss: 0.63653096, Validation loss: 0.65214193, Gradient norm: 6.29406485
INFO:root:[   66] Training loss: 0.63593517, Validation loss: 0.64580472, Gradient norm: 5.73497131
INFO:root:[   67] Training loss: 0.63555716, Validation loss: 0.65294965, Gradient norm: 6.61176533
INFO:root:[   68] Training loss: 0.63515872, Validation loss: 0.64940265, Gradient norm: 7.20786106
INFO:root:[   69] Training loss: 0.63439111, Validation loss: 0.64166354, Gradient norm: 6.57707576
INFO:root:[   70] Training loss: 0.63487060, Validation loss: 0.64423262, Gradient norm: 6.47167867
INFO:root:[   71] Training loss: 0.63309285, Validation loss: 0.64144154, Gradient norm: 5.58168610
INFO:root:[   72] Training loss: 0.63319040, Validation loss: 0.64077798, Gradient norm: 6.48045148
INFO:root:[   73] Training loss: 0.63187166, Validation loss: 0.64014352, Gradient norm: 6.01883753
INFO:root:[   74] Training loss: 0.63171785, Validation loss: 0.64388125, Gradient norm: 5.43551734
INFO:root:[   75] Training loss: 0.63266750, Validation loss: 0.65004601, Gradient norm: 6.58510751
INFO:root:[   76] Training loss: 0.63110044, Validation loss: 0.64035702, Gradient norm: 6.31596356
INFO:root:[   77] Training loss: 0.63164045, Validation loss: 0.64704029, Gradient norm: 5.94006322
INFO:root:[   78] Training loss: 0.63126319, Validation loss: 0.64373466, Gradient norm: 5.87659255
INFO:root:[   79] Training loss: 0.63007808, Validation loss: 0.64399390, Gradient norm: 5.97754272
INFO:root:[   80] Training loss: 0.62900490, Validation loss: 0.64199587, Gradient norm: 5.76799243
INFO:root:[   81] Training loss: 0.62833962, Validation loss: 0.64475060, Gradient norm: 5.60849353
INFO:root:[   82] Training loss: 0.62854231, Validation loss: 0.64028653, Gradient norm: 5.71667750
INFO:root:EP 82: Early stopping
INFO:root:Training the model took 3004.744s.
INFO:root:Emptying the cuda cache took 0.054s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.87029
INFO:root:EnergyScoreTrain: 0.62837
INFO:root:CRPSTrain: 0.59866
INFO:root:Gaussian NLLTrain: 1.85558
INFO:root:CoverageTrain: 0.6565
INFO:root:IntervalWidthTrain: 2.29263
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.88375
INFO:root:EnergyScoreValidation: 0.63851
INFO:root:CRPSValidation: 0.60996
INFO:root:Gaussian NLLValidation: 1.90122
INFO:root:CoverageValidation: 0.65046
INFO:root:IntervalWidthValidation: 2.28961
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.88524
INFO:root:EnergyScoreTest: 0.63975
INFO:root:CRPSTest: 0.61164
INFO:root:Gaussian NLLTest: 1.91339
INFO:root:CoverageTest: 0.6487
INFO:root:IntervalWidthTest: 2.28444
INFO:root:###12 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.83117643, Validation loss: 0.77589916, Gradient norm: 10.24201639
INFO:root:[    2] Training loss: 0.73426945, Validation loss: 0.73450597, Gradient norm: 11.58545425
INFO:root:[    3] Training loss: 0.73421347, Validation loss: 0.73418651, Gradient norm: 12.79374431
INFO:root:[    4] Training loss: 0.72970326, Validation loss: 0.73084195, Gradient norm: 10.08786925
INFO:root:[    5] Training loss: 0.72733061, Validation loss: 0.73083614, Gradient norm: 9.43428007
INFO:root:[    6] Training loss: 0.73058492, Validation loss: 0.72641808, Gradient norm: 10.54898276
INFO:root:[    7] Training loss: 0.72793724, Validation loss: 0.72745256, Gradient norm: 9.76641268
INFO:root:[    8] Training loss: 0.72351584, Validation loss: 0.71791044, Gradient norm: 7.44383635
INFO:root:[    9] Training loss: 0.72470645, Validation loss: 0.71737158, Gradient norm: 10.59151687
INFO:root:[   10] Training loss: 0.71417929, Validation loss: 0.72537369, Gradient norm: 8.62563945
INFO:root:[   11] Training loss: 0.71062595, Validation loss: 0.70469049, Gradient norm: 9.00552537
INFO:root:[   12] Training loss: 0.70307461, Validation loss: 0.70307626, Gradient norm: 7.64637190
INFO:root:[   13] Training loss: 0.69781300, Validation loss: 0.69642810, Gradient norm: 6.72441973
INFO:root:[   14] Training loss: 0.69564530, Validation loss: 0.71463844, Gradient norm: 8.33671659
INFO:root:[   15] Training loss: 0.69085650, Validation loss: 0.68842177, Gradient norm: 7.20666871
INFO:root:[   16] Training loss: 0.68959130, Validation loss: 0.68650755, Gradient norm: 8.33003660
INFO:root:[   17] Training loss: 0.68420822, Validation loss: 0.68657458, Gradient norm: 6.12024265
INFO:root:[   18] Training loss: 0.68281076, Validation loss: 0.68323871, Gradient norm: 7.33527038
INFO:root:[   19] Training loss: 0.67876195, Validation loss: 0.68392517, Gradient norm: 6.97636450
INFO:root:[   20] Training loss: 0.67642152, Validation loss: 0.67613178, Gradient norm: 6.36123673
INFO:root:[   21] Training loss: 0.67430717, Validation loss: 0.68972442, Gradient norm: 6.37699633
INFO:root:[   22] Training loss: 0.67274386, Validation loss: 0.67861262, Gradient norm: 7.02551796
INFO:root:[   23] Training loss: 0.67119853, Validation loss: 0.66915140, Gradient norm: 6.33869808
INFO:root:[   24] Training loss: 0.66905563, Validation loss: 0.66986692, Gradient norm: 6.39461799
INFO:root:[   25] Training loss: 0.66814971, Validation loss: 0.66597999, Gradient norm: 7.19340647
INFO:root:[   26] Training loss: 0.66524676, Validation loss: 0.66554836, Gradient norm: 5.93189431
INFO:root:[   27] Training loss: 0.66478364, Validation loss: 0.66423267, Gradient norm: 6.70531406
INFO:root:[   28] Training loss: 0.66276204, Validation loss: 0.66363263, Gradient norm: 5.94364943
INFO:root:[   29] Training loss: 0.66043302, Validation loss: 0.66590529, Gradient norm: 5.19437600
INFO:root:[   30] Training loss: 0.66031787, Validation loss: 0.66232968, Gradient norm: 6.35402678
INFO:root:[   31] Training loss: 0.65833535, Validation loss: 0.65858873, Gradient norm: 5.15373487
INFO:root:[   32] Training loss: 0.65690059, Validation loss: 0.65753416, Gradient norm: 5.30940409
INFO:root:[   33] Training loss: 0.65668594, Validation loss: 0.65854336, Gradient norm: 6.10987648
INFO:root:[   34] Training loss: 0.65563473, Validation loss: 0.65857335, Gradient norm: 5.69397346
INFO:root:[   35] Training loss: 0.65356740, Validation loss: 0.65467090, Gradient norm: 5.32402576
INFO:root:[   36] Training loss: 0.65281785, Validation loss: 0.65501888, Gradient norm: 5.93458692
INFO:root:[   37] Training loss: 0.65226921, Validation loss: 0.66141012, Gradient norm: 5.75321275
INFO:root:[   38] Training loss: 0.65124932, Validation loss: 0.66097895, Gradient norm: 5.35193576
INFO:root:[   39] Training loss: 0.65083732, Validation loss: 0.65404881, Gradient norm: 5.72218594
INFO:root:[   40] Training loss: 0.64955373, Validation loss: 0.65115730, Gradient norm: 4.86822631
INFO:root:[   41] Training loss: 0.64870862, Validation loss: 0.65179465, Gradient norm: 5.21094769
INFO:root:[   42] Training loss: 0.64726989, Validation loss: 0.64986226, Gradient norm: 4.78364146
INFO:root:[   43] Training loss: 0.64741155, Validation loss: 0.65359103, Gradient norm: 5.15015067
INFO:root:[   44] Training loss: 0.64701164, Validation loss: 0.65268135, Gradient norm: 5.11749861
INFO:root:[   45] Training loss: 0.64650628, Validation loss: 0.65252944, Gradient norm: 4.91990094
INFO:root:[   46] Training loss: 0.64551996, Validation loss: 0.64803901, Gradient norm: 5.20358928
INFO:root:[   47] Training loss: 0.64490775, Validation loss: 0.64860825, Gradient norm: 5.06539885
INFO:root:[   48] Training loss: 0.64432961, Validation loss: 0.64903949, Gradient norm: 4.92031989
INFO:root:[   49] Training loss: 0.64267085, Validation loss: 0.64737240, Gradient norm: 4.41653263
INFO:root:[   50] Training loss: 0.64298095, Validation loss: 0.64688955, Gradient norm: 4.79464560
INFO:root:[   51] Training loss: 0.64163873, Validation loss: 0.64617420, Gradient norm: 4.88071502
INFO:root:[   52] Training loss: 0.64100259, Validation loss: 0.64808176, Gradient norm: 4.82422538
INFO:root:[   53] Training loss: 0.64112007, Validation loss: 0.65093431, Gradient norm: 4.59196861
INFO:root:[   54] Training loss: 0.64000532, Validation loss: 0.64648256, Gradient norm: 4.75485597
INFO:root:[   55] Training loss: 0.63991308, Validation loss: 0.64962034, Gradient norm: 4.32266269
INFO:root:[   56] Training loss: 0.63888321, Validation loss: 0.64460043, Gradient norm: 4.15835406
INFO:root:[   57] Training loss: 0.63822596, Validation loss: 0.64248932, Gradient norm: 4.73494972
INFO:root:[   58] Training loss: 0.63848332, Validation loss: 0.64553329, Gradient norm: 4.92269910
INFO:root:[   59] Training loss: 0.63728852, Validation loss: 0.64123928, Gradient norm: 4.69209688
INFO:root:[   60] Training loss: 0.63612626, Validation loss: 0.63994533, Gradient norm: 4.45200417
INFO:root:[   61] Training loss: 0.63609089, Validation loss: 0.64395185, Gradient norm: 4.43294454
INFO:root:[   62] Training loss: 0.63575089, Validation loss: 0.64282788, Gradient norm: 3.92263601
INFO:root:[   63] Training loss: 0.63505933, Validation loss: 0.64597490, Gradient norm: 4.19408166
INFO:root:[   64] Training loss: 0.63473787, Validation loss: 0.64081640, Gradient norm: 4.70187700
INFO:root:[   65] Training loss: 0.63345386, Validation loss: 0.64282768, Gradient norm: 4.24407465
INFO:root:[   66] Training loss: 0.63412796, Validation loss: 0.64237394, Gradient norm: 4.60062221
INFO:root:[   67] Training loss: 0.63320949, Validation loss: 0.64241728, Gradient norm: 4.19173274
INFO:root:[   68] Training loss: 0.63258612, Validation loss: 0.63869821, Gradient norm: 4.61857121
INFO:root:[   69] Training loss: 0.63215588, Validation loss: 0.63798870, Gradient norm: 4.20626976
INFO:root:[   70] Training loss: 0.63128515, Validation loss: 0.63632255, Gradient norm: 4.29839128
INFO:root:[   71] Training loss: 0.63047105, Validation loss: 0.63700118, Gradient norm: 4.11023862
INFO:root:[   72] Training loss: 0.63035958, Validation loss: 0.63739357, Gradient norm: 4.02807062
INFO:root:[   73] Training loss: 0.63001454, Validation loss: 0.63773810, Gradient norm: 4.09372657
INFO:root:[   74] Training loss: 0.62965015, Validation loss: 0.64170104, Gradient norm: 4.26328871
INFO:root:[   75] Training loss: 0.62926204, Validation loss: 0.64104643, Gradient norm: 4.21686480
INFO:root:[   76] Training loss: 0.62819322, Validation loss: 0.63713534, Gradient norm: 4.21101685
INFO:root:[   77] Training loss: 0.62824859, Validation loss: 0.63748338, Gradient norm: 4.04831968
INFO:root:[   78] Training loss: 0.62818035, Validation loss: 0.63821488, Gradient norm: 3.84061358
INFO:root:[   79] Training loss: 0.62732276, Validation loss: 0.63968226, Gradient norm: 4.23637443
INFO:root:EP 79: Early stopping
INFO:root:Training the model took 2892.216s.
INFO:root:Emptying the cuda cache took 0.054s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.86745
INFO:root:EnergyScoreTrain: 0.62612
INFO:root:CRPSTrain: 0.56809
INFO:root:Gaussian NLLTrain: 1.50342
INFO:root:CoverageTrain: 0.80215
INFO:root:IntervalWidthTrain: 3.34612
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.87979
INFO:root:EnergyScoreValidation: 0.63529
INFO:root:CRPSValidation: 0.57793
INFO:root:Gaussian NLLValidation: 1.53404
INFO:root:CoverageValidation: 0.79744
INFO:root:IntervalWidthValidation: 3.34384
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.88102
INFO:root:EnergyScoreTest: 0.63645
INFO:root:CRPSTest: 0.5795
INFO:root:Gaussian NLLTest: 1.53869
INFO:root:CoverageTest: 0.79627
INFO:root:IntervalWidthTest: 3.3407
INFO:root:###13 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.80219576, Validation loss: 0.74866476, Gradient norm: 6.47304761
INFO:root:[    2] Training loss: 0.73491587, Validation loss: 0.74072017, Gradient norm: 6.37402793
INFO:root:[    3] Training loss: 0.73251584, Validation loss: 0.73841028, Gradient norm: 7.06014877
INFO:root:[    4] Training loss: 0.72854093, Validation loss: 0.72894226, Gradient norm: 5.95267028
INFO:root:[    5] Training loss: 0.72829848, Validation loss: 0.72723321, Gradient norm: 6.71099155
INFO:root:[    6] Training loss: 0.72796729, Validation loss: 0.73245838, Gradient norm: 6.28331826
INFO:root:[    7] Training loss: 0.72519551, Validation loss: 0.72465171, Gradient norm: 5.59640764
INFO:root:[    8] Training loss: 0.72452559, Validation loss: 0.72506126, Gradient norm: 4.71577701
INFO:root:[    9] Training loss: 0.72456306, Validation loss: 0.72730650, Gradient norm: 5.11854277
INFO:root:[   10] Training loss: 0.72359034, Validation loss: 0.72534708, Gradient norm: 5.37158195
INFO:root:[   11] Training loss: 0.71880775, Validation loss: 0.73392963, Gradient norm: 5.15315647
INFO:root:[   12] Training loss: 0.71105753, Validation loss: 0.70726519, Gradient norm: 4.80529349
INFO:root:[   13] Training loss: 0.70570921, Validation loss: 0.70107615, Gradient norm: 5.20764228
INFO:root:[   14] Training loss: 0.69782049, Validation loss: 0.69856447, Gradient norm: 4.35130793
INFO:root:[   15] Training loss: 0.69334944, Validation loss: 0.69533578, Gradient norm: 4.93725625
INFO:root:[   16] Training loss: 0.68774433, Validation loss: 0.68739443, Gradient norm: 4.90768992
INFO:root:[   17] Training loss: 0.68262720, Validation loss: 0.69660677, Gradient norm: 4.65243459
INFO:root:[   18] Training loss: 0.67773613, Validation loss: 0.67528423, Gradient norm: 4.24823943
INFO:root:[   19] Training loss: 0.67318486, Validation loss: 0.67514648, Gradient norm: 3.68598527
INFO:root:[   20] Training loss: 0.67179512, Validation loss: 0.66982691, Gradient norm: 4.44299299
INFO:root:[   21] Training loss: 0.66908787, Validation loss: 0.66638487, Gradient norm: 4.31716341
INFO:root:[   22] Training loss: 0.66627626, Validation loss: 0.67049289, Gradient norm: 3.92837930
INFO:root:[   23] Training loss: 0.66489117, Validation loss: 0.67278290, Gradient norm: 3.93934126
INFO:root:[   24] Training loss: 0.66373765, Validation loss: 0.66075395, Gradient norm: 4.19915039
INFO:root:[   25] Training loss: 0.66142593, Validation loss: 0.66902407, Gradient norm: 3.90110911
INFO:root:[   26] Training loss: 0.66048096, Validation loss: 0.65977900, Gradient norm: 4.09805560
INFO:root:[   27] Training loss: 0.65842537, Validation loss: 0.65715399, Gradient norm: 3.73127174
INFO:root:[   28] Training loss: 0.65816974, Validation loss: 0.66321623, Gradient norm: 4.74159743
INFO:root:[   29] Training loss: 0.65591147, Validation loss: 0.65592400, Gradient norm: 4.62892445
INFO:root:[   30] Training loss: 0.65449378, Validation loss: 0.65913594, Gradient norm: 4.39160329
INFO:root:[   31] Training loss: 0.65314001, Validation loss: 0.65512006, Gradient norm: 4.14964431
INFO:root:[   32] Training loss: 0.65202867, Validation loss: 0.65428806, Gradient norm: 4.25768706
INFO:root:[   33] Training loss: 0.65067368, Validation loss: 0.65304072, Gradient norm: 3.91755881
INFO:root:[   34] Training loss: 0.64992176, Validation loss: 0.65288463, Gradient norm: 3.97637783
INFO:root:[   35] Training loss: 0.64865900, Validation loss: 0.65433356, Gradient norm: 3.84808185
INFO:root:[   36] Training loss: 0.64786447, Validation loss: 0.65116719, Gradient norm: 3.84699711
INFO:root:[   37] Training loss: 0.64705087, Validation loss: 0.65246957, Gradient norm: 3.79449257
INFO:root:[   38] Training loss: 0.64557615, Validation loss: 0.64839944, Gradient norm: 3.56735198
INFO:root:[   39] Training loss: 0.64501997, Validation loss: 0.64859405, Gradient norm: 3.56150140
INFO:root:[   40] Training loss: 0.64395081, Validation loss: 0.64804744, Gradient norm: 3.46558880
INFO:root:[   41] Training loss: 0.64322949, Validation loss: 0.64581585, Gradient norm: 3.44045141
INFO:root:[   42] Training loss: 0.64267333, Validation loss: 0.64702626, Gradient norm: 3.34737037
INFO:root:[   43] Training loss: 0.64148913, Validation loss: 0.64771274, Gradient norm: 3.26106555
INFO:root:[   44] Training loss: 0.64052855, Validation loss: 0.64692444, Gradient norm: 3.24821465
INFO:root:[   45] Training loss: 0.64054275, Validation loss: 0.64620492, Gradient norm: 3.04912242
INFO:root:[   46] Training loss: 0.63888742, Validation loss: 0.64505897, Gradient norm: 3.09255669
INFO:root:[   47] Training loss: 0.63868237, Validation loss: 0.64439848, Gradient norm: 3.00931656
INFO:root:[   48] Training loss: 0.63763709, Validation loss: 0.64297780, Gradient norm: 2.99245523
INFO:root:[   49] Training loss: 0.63667069, Validation loss: 0.64321077, Gradient norm: 2.98745767
INFO:root:[   50] Training loss: 0.63652881, Validation loss: 0.63906478, Gradient norm: 2.88895724
INFO:root:[   51] Training loss: 0.63581151, Validation loss: 0.64217199, Gradient norm: 2.84859075
INFO:root:[   52] Training loss: 0.63475492, Validation loss: 0.64011830, Gradient norm: 2.81099962
INFO:root:[   53] Training loss: 0.63413361, Validation loss: 0.64098500, Gradient norm: 2.63455549
INFO:root:[   54] Training loss: 0.63389050, Validation loss: 0.64128815, Gradient norm: 2.59583423
INFO:root:[   55] Training loss: 0.63234826, Validation loss: 0.64062514, Gradient norm: 2.60567246
INFO:root:[   56] Training loss: 0.63252239, Validation loss: 0.63889376, Gradient norm: 2.56365068
INFO:root:[   57] Training loss: 0.63231968, Validation loss: 0.63974678, Gradient norm: 2.46621043
INFO:root:[   58] Training loss: 0.63071631, Validation loss: 0.63654139, Gradient norm: 2.43875340
INFO:root:[   59] Training loss: 0.63073002, Validation loss: 0.64041381, Gradient norm: 2.39841466
INFO:root:[   60] Training loss: 0.62951221, Validation loss: 0.63524840, Gradient norm: 2.49477190
INFO:root:[   61] Training loss: 0.62899540, Validation loss: 0.63963601, Gradient norm: 2.33941776
INFO:root:[   62] Training loss: 0.62992720, Validation loss: 0.63941512, Gradient norm: 2.20554830
INFO:root:[   63] Training loss: 0.62795375, Validation loss: 0.64274217, Gradient norm: 2.19867986
INFO:root:[   64] Training loss: 0.62871094, Validation loss: 0.64396788, Gradient norm: 2.41871183
INFO:root:[   65] Training loss: 0.62743786, Validation loss: 0.63600402, Gradient norm: 1.98060274
INFO:root:[   66] Training loss: 0.62702210, Validation loss: 0.63534438, Gradient norm: 2.42762144
INFO:root:[   67] Training loss: 0.62620312, Validation loss: 0.63866481, Gradient norm: 2.26263761
INFO:root:[   68] Training loss: 0.62567067, Validation loss: 0.63791554, Gradient norm: 2.30555326
INFO:root:[   69] Training loss: 0.62625281, Validation loss: 0.63538430, Gradient norm: 2.33991424
INFO:root:[   70] Training loss: 0.62456397, Validation loss: 0.63084224, Gradient norm: 2.19673020
INFO:root:[   71] Training loss: 0.62471254, Validation loss: 0.63700693, Gradient norm: 2.23510155
INFO:root:[   72] Training loss: 0.62404389, Validation loss: 0.63545981, Gradient norm: 2.20656085
INFO:root:[   73] Training loss: 0.62303606, Validation loss: 0.63883383, Gradient norm: 2.09524953
INFO:root:[   74] Training loss: 0.62316074, Validation loss: 0.63520635, Gradient norm: 2.22542819
INFO:root:[   75] Training loss: 0.62292907, Validation loss: 0.63349964, Gradient norm: 2.24452540
INFO:root:[   76] Training loss: 0.62171370, Validation loss: 0.63744511, Gradient norm: 2.22496344
INFO:root:[   77] Training loss: 0.62172593, Validation loss: 0.63577469, Gradient norm: 2.15327174
INFO:root:[   78] Training loss: 0.62083761, Validation loss: 0.63817473, Gradient norm: 2.10056417
INFO:root:[   79] Training loss: 0.62034263, Validation loss: 0.63661962, Gradient norm: 2.15795205
INFO:root:EP 79: Early stopping
INFO:root:Training the model took 2898.17s.
INFO:root:Emptying the cuda cache took 0.054s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.85932
INFO:root:EnergyScoreTrain: 0.6203
INFO:root:CRPSTrain: 0.53675
INFO:root:Gaussian NLLTrain: 1.45752
INFO:root:CoverageTrain: 0.87957
INFO:root:IntervalWidthTrain: 3.60894
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.8741
INFO:root:EnergyScoreValidation: 0.63131
INFO:root:CRPSValidation: 0.54766
INFO:root:Gaussian NLLValidation: 1.49167
INFO:root:CoverageValidation: 0.87399
INFO:root:IntervalWidthValidation: 3.60614
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.87641
INFO:root:EnergyScoreTest: 0.63327
INFO:root:CRPSTest: 0.54991
INFO:root:Gaussian NLLTest: 1.50081
INFO:root:CoverageTest: 0.8723
INFO:root:IntervalWidthTest: 3.60254
INFO:root:###14 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.77462874, Validation loss: 0.74291329, Gradient norm: 3.84175863
INFO:root:[    2] Training loss: 0.73195888, Validation loss: 0.72289528, Gradient norm: 4.08784067
INFO:root:[    3] Training loss: 0.72628874, Validation loss: 0.73200836, Gradient norm: 3.39761226
INFO:root:[    4] Training loss: 0.72480594, Validation loss: 0.72220041, Gradient norm: 3.43001486
INFO:root:[    5] Training loss: 0.72420582, Validation loss: 0.72205667, Gradient norm: 3.10252017
INFO:root:[    6] Training loss: 0.72340834, Validation loss: 0.72387226, Gradient norm: 3.02559347
INFO:root:[    7] Training loss: 0.72379590, Validation loss: 0.72800076, Gradient norm: 3.38386369
INFO:root:[    8] Training loss: 0.72336276, Validation loss: 0.72107702, Gradient norm: 3.27310976
INFO:root:[    9] Training loss: 0.72125210, Validation loss: 0.72414350, Gradient norm: 2.92253901
INFO:root:[   10] Training loss: 0.71448937, Validation loss: 0.70904693, Gradient norm: 2.74803126
INFO:root:[   11] Training loss: 0.70553520, Validation loss: 0.70250055, Gradient norm: 2.68767315
INFO:root:[   12] Training loss: 0.69574675, Validation loss: 0.69343776, Gradient norm: 2.55963910
INFO:root:[   13] Training loss: 0.68739027, Validation loss: 0.68819354, Gradient norm: 2.59298628
INFO:root:[   14] Training loss: 0.68098336, Validation loss: 0.67513644, Gradient norm: 2.73502414
INFO:root:[   15] Training loss: 0.67469450, Validation loss: 0.67232399, Gradient norm: 2.32016073
INFO:root:[   16] Training loss: 0.67003676, Validation loss: 0.66988868, Gradient norm: 2.07915836
INFO:root:[   17] Training loss: 0.66678442, Validation loss: 0.66527468, Gradient norm: 2.09002690
INFO:root:[   18] Training loss: 0.66444905, Validation loss: 0.66695813, Gradient norm: 2.15773866
INFO:root:[   19] Training loss: 0.66173908, Validation loss: 0.66252973, Gradient norm: 2.12764780
INFO:root:[   20] Training loss: 0.65972650, Validation loss: 0.66550911, Gradient norm: 2.00353738
INFO:root:[   21] Training loss: 0.65727722, Validation loss: 0.65699697, Gradient norm: 1.93072077
INFO:root:[   22] Training loss: 0.65557343, Validation loss: 0.65687745, Gradient norm: 1.91653243
INFO:root:[   23] Training loss: 0.65378879, Validation loss: 0.65786284, Gradient norm: 1.78172493
INFO:root:[   24] Training loss: 0.65289438, Validation loss: 0.65825820, Gradient norm: 1.95214663
INFO:root:[   25] Training loss: 0.65133796, Validation loss: 0.65276151, Gradient norm: 1.84008965
INFO:root:[   26] Training loss: 0.64983219, Validation loss: 0.65418966, Gradient norm: 1.81404746
INFO:root:[   27] Training loss: 0.64802609, Validation loss: 0.64947577, Gradient norm: 1.72012944
INFO:root:[   28] Training loss: 0.64734138, Validation loss: 0.64829514, Gradient norm: 1.65612738
INFO:root:[   29] Training loss: 0.64594348, Validation loss: 0.65012485, Gradient norm: 1.58630629
INFO:root:[   30] Training loss: 0.64523897, Validation loss: 0.65011618, Gradient norm: 1.66565424
INFO:root:[   31] Training loss: 0.64398850, Validation loss: 0.64424542, Gradient norm: 1.54881841
INFO:root:[   32] Training loss: 0.64270056, Validation loss: 0.65011188, Gradient norm: 1.55793444
INFO:root:[   33] Training loss: 0.64184195, Validation loss: 0.64445344, Gradient norm: 1.51684513
INFO:root:[   34] Training loss: 0.64090033, Validation loss: 0.64708901, Gradient norm: 1.50139906
INFO:root:[   35] Training loss: 0.63973199, Validation loss: 0.64077508, Gradient norm: 1.44811959
INFO:root:[   36] Training loss: 0.63841352, Validation loss: 0.64157523, Gradient norm: 1.47531761
INFO:root:[   37] Training loss: 0.63836092, Validation loss: 0.64602237, Gradient norm: 1.40827021
INFO:root:[   38] Training loss: 0.63712296, Validation loss: 0.63827845, Gradient norm: 1.40806687
INFO:root:[   39] Training loss: 0.63688642, Validation loss: 0.64197818, Gradient norm: 1.42066458
INFO:root:[   40] Training loss: 0.63580059, Validation loss: 0.64256411, Gradient norm: 1.32818586
INFO:root:[   41] Training loss: 0.63466293, Validation loss: 0.63855485, Gradient norm: 1.33654777
INFO:root:[   42] Training loss: 0.63385441, Validation loss: 0.63854341, Gradient norm: 1.30959147
INFO:root:[   43] Training loss: 0.63337377, Validation loss: 0.64063770, Gradient norm: 1.31057185
INFO:root:[   44] Training loss: 0.63267669, Validation loss: 0.64035029, Gradient norm: 1.26173816
INFO:root:[   45] Training loss: 0.63170108, Validation loss: 0.63954712, Gradient norm: 1.21600735
INFO:root:[   46] Training loss: 0.63144970, Validation loss: 0.64051542, Gradient norm: 1.27587575
INFO:root:[   47] Training loss: 0.63022795, Validation loss: 0.63856283, Gradient norm: 1.21424314
INFO:root:[   48] Training loss: 0.63001847, Validation loss: 0.64041743, Gradient norm: 1.21166725
INFO:root:[   49] Training loss: 0.62892373, Validation loss: 0.64289852, Gradient norm: 1.11185779
INFO:root:[   50] Training loss: 0.62895664, Validation loss: 0.64034654, Gradient norm: 1.19097435
INFO:root:[   51] Training loss: 0.62732493, Validation loss: 0.63318983, Gradient norm: 1.17200177
INFO:root:[   52] Training loss: 0.62695179, Validation loss: 0.63573387, Gradient norm: 1.14001856
INFO:root:[   53] Training loss: 0.62672421, Validation loss: 0.63527566, Gradient norm: 1.06061954
INFO:root:[   54] Training loss: 0.62624030, Validation loss: 0.63381668, Gradient norm: 1.12420373
INFO:root:[   55] Training loss: 0.62535487, Validation loss: 0.63786554, Gradient norm: 1.17246311
INFO:root:[   56] Training loss: 0.62426755, Validation loss: 0.63040939, Gradient norm: 1.15188284
INFO:root:[   57] Training loss: 0.62446880, Validation loss: 0.63997783, Gradient norm: 1.08278901
INFO:root:[   58] Training loss: 0.62429569, Validation loss: 0.62858874, Gradient norm: 1.08000485
INFO:root:[   59] Training loss: 0.62221974, Validation loss: 0.63468152, Gradient norm: 1.00879647
INFO:root:[   60] Training loss: 0.62326170, Validation loss: 0.63216536, Gradient norm: 1.00289881
INFO:root:[   61] Training loss: 0.62148586, Validation loss: 0.63167482, Gradient norm: 0.98994346
INFO:root:[   62] Training loss: 0.62121030, Validation loss: 0.63371246, Gradient norm: 0.98437288
INFO:root:[   63] Training loss: 0.62032571, Validation loss: 0.63427948, Gradient norm: 0.98412451
INFO:root:[   64] Training loss: 0.62031526, Validation loss: 0.63365975, Gradient norm: 1.02126547
INFO:root:[   65] Training loss: 0.61967280, Validation loss: 0.63347511, Gradient norm: 1.03868777
INFO:root:[   66] Training loss: 0.61868432, Validation loss: 0.63162105, Gradient norm: 0.91484119
INFO:root:[   67] Training loss: 0.61791645, Validation loss: 0.63129837, Gradient norm: 0.94988249
INFO:root:EP 67: Early stopping
INFO:root:Training the model took 2458.108s.
INFO:root:Emptying the cuda cache took 0.056s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.85556
INFO:root:EnergyScoreTrain: 0.61778
INFO:root:CRPSTrain: 0.51998
INFO:root:Gaussian NLLTrain: 1.45145
INFO:root:CoverageTrain: 0.88761
INFO:root:IntervalWidthTrain: 3.47706
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.87094
INFO:root:EnergyScoreValidation: 0.62923
INFO:root:CRPSValidation: 0.53014
INFO:root:Gaussian NLLValidation: 1.4778
INFO:root:CoverageValidation: 0.88232
INFO:root:IntervalWidthValidation: 3.47257
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.87169
INFO:root:EnergyScoreTest: 0.62991
INFO:root:CRPSTest: 0.53098
INFO:root:Gaussian NLLTest: 1.4808
INFO:root:CoverageTest: 0.88145
INFO:root:IntervalWidthTest: 3.46951
INFO:root:###15 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.76764542, Validation loss: 0.72371056, Gradient norm: 2.40000686
INFO:root:[    2] Training loss: 0.72672302, Validation loss: 0.72197650, Gradient norm: 2.22851773
INFO:root:[    3] Training loss: 0.72688339, Validation loss: 0.73223592, Gradient norm: 2.67064836
INFO:root:[    4] Training loss: 0.72404060, Validation loss: 0.73191423, Gradient norm: 2.17465946
INFO:root:[    5] Training loss: 0.72453359, Validation loss: 0.72021255, Gradient norm: 2.24916006
INFO:root:[    6] Training loss: 0.72166688, Validation loss: 0.72190801, Gradient norm: 1.52436178
INFO:root:[    7] Training loss: 0.72097817, Validation loss: 0.71778424, Gradient norm: 1.64761355
INFO:root:[    8] Training loss: 0.71743794, Validation loss: 0.71432709, Gradient norm: 1.81129723
INFO:root:[    9] Training loss: 0.70807585, Validation loss: 0.70432803, Gradient norm: 1.63508632
INFO:root:[   10] Training loss: 0.69869728, Validation loss: 0.69610600, Gradient norm: 1.45206654
INFO:root:[   11] Training loss: 0.69018270, Validation loss: 0.68412315, Gradient norm: 1.45070211
INFO:root:[   12] Training loss: 0.68347691, Validation loss: 0.68494634, Gradient norm: 1.38464297
INFO:root:[   13] Training loss: 0.67856597, Validation loss: 0.67930040, Gradient norm: 1.29991245
INFO:root:[   14] Training loss: 0.67441679, Validation loss: 0.66998019, Gradient norm: 1.24750620
INFO:root:[   15] Training loss: 0.67078874, Validation loss: 0.66909062, Gradient norm: 1.18813664
INFO:root:[   16] Training loss: 0.66840021, Validation loss: 0.67157527, Gradient norm: 1.10963835
INFO:root:[   17] Training loss: 0.66560903, Validation loss: 0.66848581, Gradient norm: 1.12433473
INFO:root:[   18] Training loss: 0.66333633, Validation loss: 0.66257964, Gradient norm: 1.07057071
INFO:root:[   19] Training loss: 0.66114265, Validation loss: 0.66502528, Gradient norm: 0.93254878
INFO:root:[   20] Training loss: 0.65948396, Validation loss: 0.66306736, Gradient norm: 1.05045311
INFO:root:[   21] Training loss: 0.65770768, Validation loss: 0.66147352, Gradient norm: 0.92233732
INFO:root:[   22] Training loss: 0.65604030, Validation loss: 0.65704781, Gradient norm: 0.92590972
INFO:root:[   23] Training loss: 0.65481219, Validation loss: 0.65909529, Gradient norm: 0.88020491
INFO:root:[   24] Training loss: 0.65289539, Validation loss: 0.65469047, Gradient norm: 0.90911922
INFO:root:[   25] Training loss: 0.65175337, Validation loss: 0.65634280, Gradient norm: 0.87834882
INFO:root:[   26] Training loss: 0.65027867, Validation loss: 0.65083594, Gradient norm: 0.83041758
INFO:root:[   27] Training loss: 0.64880199, Validation loss: 0.65541699, Gradient norm: 0.81322091
INFO:root:[   28] Training loss: 0.64767602, Validation loss: 0.65138006, Gradient norm: 0.82891915
INFO:root:[   29] Training loss: 0.64679744, Validation loss: 0.65107611, Gradient norm: 0.69183070
INFO:root:[   30] Training loss: 0.64499971, Validation loss: 0.64984247, Gradient norm: 0.79459425
INFO:root:[   31] Training loss: 0.64418939, Validation loss: 0.64546904, Gradient norm: 0.75020669
INFO:root:[   32] Training loss: 0.64283295, Validation loss: 0.64548674, Gradient norm: 0.68384895
INFO:root:[   33] Training loss: 0.64216176, Validation loss: 0.64926744, Gradient norm: 0.69848437
INFO:root:[   34] Training loss: 0.64161833, Validation loss: 0.64532287, Gradient norm: 0.67742456
INFO:root:[   35] Training loss: 0.64005915, Validation loss: 0.64592704, Gradient norm: 0.66382357
INFO:root:[   36] Training loss: 0.63959224, Validation loss: 0.64495891, Gradient norm: 0.70759469
INFO:root:[   37] Training loss: 0.63796218, Validation loss: 0.64228080, Gradient norm: 0.65011681
INFO:root:[   38] Training loss: 0.63712198, Validation loss: 0.64382633, Gradient norm: 0.59613178
INFO:root:[   39] Training loss: 0.63598614, Validation loss: 0.64420083, Gradient norm: 0.61767851
INFO:root:[   40] Training loss: 0.63655490, Validation loss: 0.64263293, Gradient norm: 0.68099254
INFO:root:[   41] Training loss: 0.63434506, Validation loss: 0.64584033, Gradient norm: 0.54283315
INFO:root:[   42] Training loss: 0.63477202, Validation loss: 0.64371219, Gradient norm: 0.67556048
INFO:root:[   43] Training loss: 0.63242837, Validation loss: 0.63837805, Gradient norm: 0.55546876
INFO:root:[   44] Training loss: 0.63228673, Validation loss: 0.63987095, Gradient norm: 0.60311531
INFO:root:[   45] Training loss: 0.63146672, Validation loss: 0.63519567, Gradient norm: 0.62507032
INFO:root:[   46] Training loss: 0.63038088, Validation loss: 0.64106444, Gradient norm: 0.56050719
INFO:root:[   47] Training loss: 0.62922187, Validation loss: 0.63972809, Gradient norm: 0.51944534
INFO:root:[   48] Training loss: 0.62989888, Validation loss: 0.63845565, Gradient norm: 0.65161898
INFO:root:[   49] Training loss: 0.62962604, Validation loss: 0.64013891, Gradient norm: 0.62021674
INFO:root:[   50] Training loss: 0.62820431, Validation loss: 0.63802591, Gradient norm: 0.54877921
INFO:root:[   51] Training loss: 0.62725460, Validation loss: 0.63519967, Gradient norm: 0.49809975
INFO:root:[   52] Training loss: 0.62701355, Validation loss: 0.63693555, Gradient norm: 0.58406766
INFO:root:[   53] Training loss: 0.62548003, Validation loss: 0.63208801, Gradient norm: 0.56082611
INFO:root:[   54] Training loss: 0.62500698, Validation loss: 0.63760032, Gradient norm: 0.54200166
INFO:root:[   55] Training loss: 0.62503966, Validation loss: 0.63348109, Gradient norm: 0.56146707
INFO:root:[   56] Training loss: 0.62484454, Validation loss: 0.63178604, Gradient norm: 0.59804855
INFO:root:[   57] Training loss: 0.62239005, Validation loss: 0.63229488, Gradient norm: 0.51228859
INFO:root:[   58] Training loss: 0.62265758, Validation loss: 0.63084031, Gradient norm: 0.52292801
INFO:root:[   59] Training loss: 0.62173712, Validation loss: 0.63260734, Gradient norm: 0.45476631
INFO:root:[   60] Training loss: 0.62260959, Validation loss: 0.63704162, Gradient norm: 0.62992316
INFO:root:[   61] Training loss: 0.62145703, Validation loss: 0.63159576, Gradient norm: 0.58886841
INFO:root:[   62] Training loss: 0.62068607, Validation loss: 0.63362161, Gradient norm: 0.48278480
INFO:root:[   63] Training loss: 0.61999591, Validation loss: 0.63235367, Gradient norm: 0.48668996
INFO:root:[   64] Training loss: 0.61898979, Validation loss: 0.62945233, Gradient norm: 0.51680594
INFO:root:[   65] Training loss: 0.61894253, Validation loss: 0.63044734, Gradient norm: 0.58084474
INFO:root:[   66] Training loss: 0.61825639, Validation loss: 0.62811220, Gradient norm: 0.50016168
INFO:root:[   67] Training loss: 0.61722363, Validation loss: 0.63208741, Gradient norm: 0.47933817
INFO:root:[   68] Training loss: 0.61775588, Validation loss: 0.63108365, Gradient norm: 0.51837190
INFO:root:[   69] Training loss: 0.61689749, Validation loss: 0.62754848, Gradient norm: 0.48760005
INFO:root:[   70] Training loss: 0.61583909, Validation loss: 0.63174537, Gradient norm: 0.47721745
INFO:root:[   71] Training loss: 0.61778425, Validation loss: 0.63202627, Gradient norm: 0.62114952
INFO:root:[   72] Training loss: 0.61450334, Validation loss: 0.63048256, Gradient norm: 0.46978017
INFO:root:[   73] Training loss: 0.61468965, Validation loss: 0.63173898, Gradient norm: 0.45432088
INFO:root:[   74] Training loss: 0.61457377, Validation loss: 0.62955413, Gradient norm: 0.41974339
INFO:root:[   75] Training loss: 0.61486618, Validation loss: 0.63471531, Gradient norm: 0.55890576
INFO:root:[   76] Training loss: 0.61400493, Validation loss: 0.62909667, Gradient norm: 0.52631935
INFO:root:[   77] Training loss: 0.61354554, Validation loss: 0.62820328, Gradient norm: 0.51979170
INFO:root:[   78] Training loss: 0.61221578, Validation loss: 0.62904468, Gradient norm: 0.41076234
INFO:root:EP 78: Early stopping
INFO:root:Training the model took 2861.969s.
INFO:root:Emptying the cuda cache took 0.057s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.84839
INFO:root:EnergyScoreTrain: 0.61271
INFO:root:CRPSTrain: 0.524
INFO:root:Gaussian NLLTrain: 1.60409
INFO:root:CoverageTrain: 0.84132
INFO:root:IntervalWidthTrain: 3.31289
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86869
INFO:root:EnergyScoreValidation: 0.62746
INFO:root:CRPSValidation: 0.53639
INFO:root:Gaussian NLLValidation: 1.63633
INFO:root:CoverageValidation: 0.83425
INFO:root:IntervalWidthValidation: 3.30893
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.87009
INFO:root:EnergyScoreTest: 0.62864
INFO:root:CRPSTest: 0.53757
INFO:root:Gaussian NLLTest: 1.63886
INFO:root:CoverageTest: 0.83342
INFO:root:IntervalWidthTest: 3.30322
INFO:root:###16 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.76232896, Validation loss: 0.72540963, Gradient norm: 1.72593930
INFO:root:[    2] Training loss: 0.72742554, Validation loss: 0.72802837, Gradient norm: 1.85679248
INFO:root:[    3] Training loss: 0.72452100, Validation loss: 0.72289427, Gradient norm: 1.68366417
INFO:root:[    4] Training loss: 0.72237374, Validation loss: 0.72361754, Gradient norm: 1.32830958
INFO:root:[    5] Training loss: 0.72197182, Validation loss: 0.72140137, Gradient norm: 1.31158494
INFO:root:[    6] Training loss: 0.72106232, Validation loss: 0.71736450, Gradient norm: 1.40262886
INFO:root:[    7] Training loss: 0.71731302, Validation loss: 0.71376185, Gradient norm: 1.49293605
INFO:root:[    8] Training loss: 0.70915811, Validation loss: 0.70390656, Gradient norm: 1.18407688
INFO:root:[    9] Training loss: 0.70252046, Validation loss: 0.70254515, Gradient norm: 1.07875484
INFO:root:[   10] Training loss: 0.69730328, Validation loss: 0.69192210, Gradient norm: 1.07755233
INFO:root:[   11] Training loss: 0.69146777, Validation loss: 0.68805643, Gradient norm: 0.91218395
INFO:root:[   12] Training loss: 0.68779413, Validation loss: 0.68715988, Gradient norm: 0.88014865
INFO:root:[   13] Training loss: 0.68378549, Validation loss: 0.68674977, Gradient norm: 0.87499020
INFO:root:[   14] Training loss: 0.68021119, Validation loss: 0.67940478, Gradient norm: 0.80786042
INFO:root:[   15] Training loss: 0.67709225, Validation loss: 0.67875650, Gradient norm: 0.75630072
INFO:root:[   16] Training loss: 0.67416941, Validation loss: 0.67493283, Gradient norm: 0.67663974
INFO:root:[   17] Training loss: 0.67149734, Validation loss: 0.67323126, Gradient norm: 0.70137107
INFO:root:[   18] Training loss: 0.66899790, Validation loss: 0.66772662, Gradient norm: 0.57461153
INFO:root:[   19] Training loss: 0.66686362, Validation loss: 0.66630922, Gradient norm: 0.52984062
INFO:root:[   20] Training loss: 0.66355823, Validation loss: 0.66598599, Gradient norm: 0.44144139
INFO:root:[   21] Training loss: 0.66334803, Validation loss: 0.66706541, Gradient norm: 0.62208014
INFO:root:[   22] Training loss: 0.66042934, Validation loss: 0.66131365, Gradient norm: 0.47852814
INFO:root:[   23] Training loss: 0.65873976, Validation loss: 0.65885425, Gradient norm: 0.43838292
INFO:root:[   24] Training loss: 0.65688659, Validation loss: 0.65888308, Gradient norm: 0.44247864
INFO:root:[   25] Training loss: 0.65568357, Validation loss: 0.65747113, Gradient norm: 0.40201655
INFO:root:[   26] Training loss: 0.65328187, Validation loss: 0.65900960, Gradient norm: 0.34684195
INFO:root:[   27] Training loss: 0.65351347, Validation loss: 0.65941585, Gradient norm: 0.48901317
INFO:root:[   28] Training loss: 0.65218846, Validation loss: 0.65256216, Gradient norm: 0.41799398
INFO:root:[   29] Training loss: 0.65052075, Validation loss: 0.65346381, Gradient norm: 0.42696999
INFO:root:[   30] Training loss: 0.64879648, Validation loss: 0.65300335, Gradient norm: 0.35464526
INFO:root:[   31] Training loss: 0.64746092, Validation loss: 0.65240087, Gradient norm: 0.28638353
INFO:root:[   32] Training loss: 0.64707767, Validation loss: 0.64982874, Gradient norm: 0.38788927
INFO:root:[   33] Training loss: 0.64575744, Validation loss: 0.64766711, Gradient norm: 0.37868760
INFO:root:[   34] Training loss: 0.64488835, Validation loss: 0.65142626, Gradient norm: 0.38189023
INFO:root:[   35] Training loss: 0.64450423, Validation loss: 0.64630001, Gradient norm: 0.41619762
INFO:root:[   36] Training loss: 0.64231343, Validation loss: 0.64560973, Gradient norm: 0.29514350
INFO:root:[   37] Training loss: 0.64117629, Validation loss: 0.64429262, Gradient norm: 0.23493389
INFO:root:[   38] Training loss: 0.64103939, Validation loss: 0.64280426, Gradient norm: 0.31427384
INFO:root:[   39] Training loss: 0.64002921, Validation loss: 0.64513170, Gradient norm: 0.26574979
INFO:root:[   40] Training loss: 0.63897689, Validation loss: 0.64590998, Gradient norm: 0.29716464
INFO:root:[   41] Training loss: 0.64056582, Validation loss: 0.64385422, Gradient norm: 0.43069127
INFO:root:[   42] Training loss: 0.63710231, Validation loss: 0.64136667, Gradient norm: 0.20468085
INFO:root:[   43] Training loss: 0.63554147, Validation loss: 0.63963159, Gradient norm: 0.19531243
INFO:root:[   44] Training loss: 0.63649821, Validation loss: 0.63996493, Gradient norm: 0.29428826
INFO:root:[   45] Training loss: 0.63679993, Validation loss: 0.63945483, Gradient norm: 0.37238616
INFO:root:[   46] Training loss: 0.63449770, Validation loss: 0.64031232, Gradient norm: 0.25348516
INFO:root:[   47] Training loss: 0.63310726, Validation loss: 0.63910695, Gradient norm: 0.22821244
INFO:root:[   48] Training loss: 0.63353867, Validation loss: 0.63812983, Gradient norm: 0.30726194
INFO:root:[   49] Training loss: 0.63239222, Validation loss: 0.63729754, Gradient norm: 0.21600054
INFO:root:[   50] Training loss: 0.63243133, Validation loss: 0.63578977, Gradient norm: 0.28007480
INFO:root:[   51] Training loss: 0.63153763, Validation loss: 0.63547557, Gradient norm: 0.28159757
INFO:root:[   52] Training loss: 0.63068186, Validation loss: 0.63483435, Gradient norm: 0.23355218
INFO:root:[   53] Training loss: 0.63113434, Validation loss: 0.63468374, Gradient norm: 0.29958530
INFO:root:[   54] Training loss: 0.62871585, Validation loss: 0.63559237, Gradient norm: 0.17637434
INFO:root:[   55] Training loss: 0.62918117, Validation loss: 0.63491691, Gradient norm: 0.22947901
INFO:root:[   56] Training loss: 0.62930824, Validation loss: 0.63357873, Gradient norm: 0.28094854
INFO:root:[   57] Training loss: 0.62742975, Validation loss: 0.63346517, Gradient norm: 0.20875228
INFO:root:[   58] Training loss: 0.62758240, Validation loss: 0.63484636, Gradient norm: 0.25494334
INFO:root:[   59] Training loss: 0.62856939, Validation loss: 0.63456848, Gradient norm: 0.29076644
INFO:root:[   60] Training loss: 0.62573795, Validation loss: 0.63227313, Gradient norm: 0.24745858
INFO:root:[   61] Training loss: 0.62482438, Validation loss: 0.63055498, Gradient norm: 0.16539705
INFO:root:[   62] Training loss: 0.62597603, Validation loss: 0.63167116, Gradient norm: 0.22482575
INFO:root:[   63] Training loss: 0.62581778, Validation loss: 0.63334243, Gradient norm: 0.30224982
INFO:root:[   64] Training loss: 0.62388222, Validation loss: 0.63013446, Gradient norm: 0.17552229
INFO:root:[   65] Training loss: 0.62306431, Validation loss: 0.63007906, Gradient norm: 0.14862359
INFO:root:[   66] Training loss: 0.62395027, Validation loss: 0.63220190, Gradient norm: 0.22552632
INFO:root:[   67] Training loss: 0.62240418, Validation loss: 0.62939257, Gradient norm: 0.22307110
INFO:root:[   68] Training loss: 0.62243142, Validation loss: 0.63231918, Gradient norm: 0.21527221
INFO:root:[   69] Training loss: 0.62326621, Validation loss: 0.63163900, Gradient norm: 0.28348821
INFO:root:[   70] Training loss: 0.62215538, Validation loss: 0.62887303, Gradient norm: 0.20463556
INFO:root:[   71] Training loss: 0.61999267, Validation loss: 0.62993758, Gradient norm: 0.14391022
INFO:root:[   72] Training loss: 0.62063557, Validation loss: 0.62824450, Gradient norm: 0.21321747
INFO:root:[   73] Training loss: 0.62032970, Validation loss: 0.62873428, Gradient norm: 0.23031325
INFO:root:[   74] Training loss: 0.61937889, Validation loss: 0.62673031, Gradient norm: 0.18888187
INFO:root:[   75] Training loss: 0.61979525, Validation loss: 0.62872566, Gradient norm: 0.20718147
INFO:root:[   76] Training loss: 0.61861085, Validation loss: 0.62800142, Gradient norm: 0.21009574
INFO:root:[   77] Training loss: 0.61865872, Validation loss: 0.63118854, Gradient norm: 0.20089823
INFO:root:[   78] Training loss: 0.61798352, Validation loss: 0.62513171, Gradient norm: 0.18688456
INFO:root:[   79] Training loss: 0.61737408, Validation loss: 0.62800563, Gradient norm: 0.15375397
INFO:root:[   80] Training loss: 0.61820728, Validation loss: 0.62691242, Gradient norm: 0.24802568
INFO:root:[   81] Training loss: 0.61736250, Validation loss: 0.62662783, Gradient norm: 0.19620370
INFO:root:[   82] Training loss: 0.61637826, Validation loss: 0.62999930, Gradient norm: 0.17229653
INFO:root:[   83] Training loss: 0.61649579, Validation loss: 0.62527629, Gradient norm: 0.20838463
INFO:root:[   84] Training loss: 0.61662892, Validation loss: 0.62417439, Gradient norm: 0.22634696
INFO:root:[   85] Training loss: 0.61645812, Validation loss: 0.62559737, Gradient norm: 0.18955941
INFO:root:[   86] Training loss: 0.61552333, Validation loss: 0.62839682, Gradient norm: 0.16707942
INFO:root:[   87] Training loss: 0.61448157, Validation loss: 0.62386765, Gradient norm: 0.19000836
INFO:root:[   88] Training loss: 0.61503543, Validation loss: 0.62574834, Gradient norm: 0.20282605
INFO:root:[   89] Training loss: 0.61553182, Validation loss: 0.62641121, Gradient norm: 0.22877687
INFO:root:[   90] Training loss: 0.61353407, Validation loss: 0.62321404, Gradient norm: 0.18384047
INFO:root:[   91] Training loss: 0.61259089, Validation loss: 0.62689040, Gradient norm: 0.17343130
INFO:root:[   92] Training loss: 0.61347470, Validation loss: 0.62331128, Gradient norm: 0.18982534
INFO:root:[   93] Training loss: 0.61151387, Validation loss: 0.62301123, Gradient norm: 0.16131082
INFO:root:[   94] Training loss: 0.61172048, Validation loss: 0.62316466, Gradient norm: 0.15077785
INFO:root:[   95] Training loss: 0.61217565, Validation loss: 0.62316719, Gradient norm: 0.18317446
INFO:root:[   96] Training loss: 0.61252062, Validation loss: 0.62493785, Gradient norm: 0.22602688
INFO:root:[   97] Training loss: 0.61147418, Validation loss: 0.62211373, Gradient norm: 0.16409552
INFO:root:[   98] Training loss: 0.61037699, Validation loss: 0.62146250, Gradient norm: 0.11595381
INFO:root:[   99] Training loss: 0.61237258, Validation loss: 0.62544579, Gradient norm: 0.22870411
INFO:root:[  100] Training loss: 0.61150071, Validation loss: 0.61993331, Gradient norm: 0.19298680
INFO:root:[  101] Training loss: 0.61013197, Validation loss: 0.62333507, Gradient norm: 0.15516909
INFO:root:[  102] Training loss: 0.60990510, Validation loss: 0.62174651, Gradient norm: 0.19671377
INFO:root:[  103] Training loss: 0.60886654, Validation loss: 0.62406920, Gradient norm: 0.15218185
INFO:root:[  104] Training loss: 0.60965175, Validation loss: 0.62536937, Gradient norm: 0.18481526
INFO:root:[  105] Training loss: 0.60958173, Validation loss: 0.62310553, Gradient norm: 0.19334584
INFO:root:[  106] Training loss: 0.60923828, Validation loss: 0.61958870, Gradient norm: 0.18559469
INFO:root:[  107] Training loss: 0.60805158, Validation loss: 0.62044548, Gradient norm: 0.14117260
INFO:root:[  108] Training loss: 0.60871234, Validation loss: 0.62370024, Gradient norm: 0.17150928
INFO:root:[  109] Training loss: 0.60966635, Validation loss: 0.62433110, Gradient norm: 0.22173940
INFO:root:[  110] Training loss: 0.60666230, Validation loss: 0.61999782, Gradient norm: 0.15720201
INFO:root:[  111] Training loss: 0.60777463, Validation loss: 0.62075115, Gradient norm: 0.18134637
INFO:root:[  112] Training loss: 0.60772453, Validation loss: 0.61951845, Gradient norm: 0.17892614
INFO:root:[  113] Training loss: 0.60565838, Validation loss: 0.62322386, Gradient norm: 0.12327894
INFO:root:[  114] Training loss: 0.60804828, Validation loss: 0.62202052, Gradient norm: 0.20458376
INFO:root:[  115] Training loss: 0.60600164, Validation loss: 0.62149046, Gradient norm: 0.16803041
INFO:root:[  116] Training loss: 0.60649490, Validation loss: 0.61954900, Gradient norm: 0.18153775
INFO:root:[  117] Training loss: 0.60522338, Validation loss: 0.62100319, Gradient norm: 0.12975204
INFO:root:[  118] Training loss: 0.60482303, Validation loss: 0.61822469, Gradient norm: 0.16644074
INFO:root:[  119] Training loss: 0.60531227, Validation loss: 0.62007850, Gradient norm: 0.15826291
INFO:root:[  120] Training loss: 0.60462308, Validation loss: 0.62086862, Gradient norm: 0.13644920
INFO:root:[  121] Training loss: 0.60513475, Validation loss: 0.61922214, Gradient norm: 0.18591932
INFO:root:[  122] Training loss: 0.60550361, Validation loss: 0.61970834, Gradient norm: 0.20750171
INFO:root:[  123] Training loss: 0.60417557, Validation loss: 0.61892623, Gradient norm: 0.14109508
INFO:root:[  124] Training loss: 0.60445211, Validation loss: 0.61837207, Gradient norm: 0.16699304
INFO:root:[  125] Training loss: 0.60493418, Validation loss: 0.61878861, Gradient norm: 0.18444796
INFO:root:[  126] Training loss: 0.60359387, Validation loss: 0.62100436, Gradient norm: 0.15651008
INFO:root:[  127] Training loss: 0.60366153, Validation loss: 0.62045422, Gradient norm: 0.17583688
INFO:root:[  128] Training loss: 0.60317049, Validation loss: 0.61780025, Gradient norm: 0.16238617
INFO:root:[  129] Training loss: 0.60250501, Validation loss: 0.61814560, Gradient norm: 0.14481837
INFO:root:[  130] Training loss: 0.60200361, Validation loss: 0.61700975, Gradient norm: 0.11015614
INFO:root:[  131] Training loss: 0.60298026, Validation loss: 0.62024204, Gradient norm: 0.18692381
INFO:root:[  132] Training loss: 0.60313357, Validation loss: 0.61579532, Gradient norm: 0.17894848
INFO:root:[  133] Training loss: 0.60197421, Validation loss: 0.61769478, Gradient norm: 0.17568514
INFO:root:[  134] Training loss: 0.60067500, Validation loss: 0.61645362, Gradient norm: 0.11887975
INFO:root:[  135] Training loss: 0.60097076, Validation loss: 0.61730353, Gradient norm: 0.13863377
INFO:root:[  136] Training loss: 0.60153506, Validation loss: 0.61677646, Gradient norm: 0.18703134
INFO:root:[  137] Training loss: 0.60134292, Validation loss: 0.61938509, Gradient norm: 0.18722201
INFO:root:[  138] Training loss: 0.59997635, Validation loss: 0.61679271, Gradient norm: 0.10631530
INFO:root:[  139] Training loss: 0.60040360, Validation loss: 0.62272243, Gradient norm: 0.15273596
INFO:root:[  140] Training loss: 0.60023477, Validation loss: 0.61885004, Gradient norm: 0.13479201
INFO:root:[  141] Training loss: 0.59965784, Validation loss: 0.62083765, Gradient norm: 0.14294144
INFO:root:EP 141: Early stopping
INFO:root:Training the model took 5146.133s.
INFO:root:Emptying the cuda cache took 0.056s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82843
INFO:root:EnergyScoreTrain: 0.59919
INFO:root:CRPSTrain: 0.53082
INFO:root:Gaussian NLLTrain: 4.08711
INFO:root:CoverageTrain: 0.79133
INFO:root:IntervalWidthTrain: 3.19101
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.85302
INFO:root:EnergyScoreValidation: 0.61679
INFO:root:CRPSValidation: 0.54517
INFO:root:Gaussian NLLValidation: 4.13836
INFO:root:CoverageValidation: 0.78291
INFO:root:IntervalWidthValidation: 3.18669
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.85384
INFO:root:EnergyScoreTest: 0.61753
INFO:root:CRPSTest: 0.54598
INFO:root:Gaussian NLLTest: 4.14973
INFO:root:CoverageTest: 0.78114
INFO:root:IntervalWidthTest: 3.17631
INFO:root:###17 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.75973567, Validation loss: 0.73311483, Gradient norm: 1.54169437
INFO:root:[    2] Training loss: 0.72547597, Validation loss: 0.73305612, Gradient norm: 1.22033857
INFO:root:[    3] Training loss: 0.72460361, Validation loss: 0.72282130, Gradient norm: 1.46279788
INFO:root:[    4] Training loss: 0.72254569, Validation loss: 0.72301237, Gradient norm: 1.12980829
INFO:root:[    5] Training loss: 0.72172189, Validation loss: 0.71938501, Gradient norm: 1.13259713
INFO:root:[    6] Training loss: 0.71963627, Validation loss: 0.71909831, Gradient norm: 1.00279409
INFO:root:[    7] Training loss: 0.71484651, Validation loss: 0.71072181, Gradient norm: 0.86969117
INFO:root:[    8] Training loss: 0.70862738, Validation loss: 0.70312647, Gradient norm: 0.86749981
INFO:root:[    9] Training loss: 0.70183008, Validation loss: 0.69654789, Gradient norm: 0.81250257
INFO:root:[   10] Training loss: 0.69542397, Validation loss: 0.69157306, Gradient norm: 0.70204532
INFO:root:[   11] Training loss: 0.68945223, Validation loss: 0.68778300, Gradient norm: 0.73091081
INFO:root:[   12] Training loss: 0.68335217, Validation loss: 0.68322346, Gradient norm: 0.48631651
INFO:root:[   13] Training loss: 0.67965775, Validation loss: 0.67838684, Gradient norm: 0.56563705
INFO:root:[   14] Training loss: 0.67625047, Validation loss: 0.67322206, Gradient norm: 0.57800283
INFO:root:[   15] Training loss: 0.67250115, Validation loss: 0.67162071, Gradient norm: 0.45391621
INFO:root:[   16] Training loss: 0.66947485, Validation loss: 0.66798254, Gradient norm: 0.35088017
INFO:root:[   17] Training loss: 0.66728685, Validation loss: 0.66973267, Gradient norm: 0.39679165
INFO:root:[   18] Training loss: 0.66573264, Validation loss: 0.66848178, Gradient norm: 0.44348975
INFO:root:[   19] Training loss: 0.66372030, Validation loss: 0.66435664, Gradient norm: 0.42122923
INFO:root:[   20] Training loss: 0.66004663, Validation loss: 0.66189042, Gradient norm: 0.26096689
INFO:root:[   21] Training loss: 0.66035237, Validation loss: 0.65980916, Gradient norm: 0.40564033
INFO:root:[   22] Training loss: 0.65788518, Validation loss: 0.65951143, Gradient norm: 0.35683523
INFO:root:[   23] Training loss: 0.65551368, Validation loss: 0.65654856, Gradient norm: 0.25066898
INFO:root:[   24] Training loss: 0.65392631, Validation loss: 0.65734511, Gradient norm: 0.19274142
INFO:root:[   25] Training loss: 0.65477956, Validation loss: 0.65651047, Gradient norm: 0.42118966
INFO:root:[   26] Training loss: 0.65298337, Validation loss: 0.65298626, Gradient norm: 0.33266106
INFO:root:[   27] Training loss: 0.65193718, Validation loss: 0.65968684, Gradient norm: 0.33386320
INFO:root:[   28] Training loss: 0.64982205, Validation loss: 0.65116423, Gradient norm: 0.27426382
INFO:root:[   29] Training loss: 0.64827719, Validation loss: 0.64979647, Gradient norm: 0.14295512
INFO:root:[   30] Training loss: 0.64834315, Validation loss: 0.65036957, Gradient norm: 0.29480718
INFO:root:[   31] Training loss: 0.64829496, Validation loss: 0.64935785, Gradient norm: 0.36759263
INFO:root:[   32] Training loss: 0.64512018, Validation loss: 0.64754191, Gradient norm: 0.15402067
INFO:root:[   33] Training loss: 0.64362449, Validation loss: 0.64680414, Gradient norm: 0.13638708
INFO:root:[   34] Training loss: 0.64577173, Validation loss: 0.64718832, Gradient norm: 0.38584374
INFO:root:[   35] Training loss: 0.64202395, Validation loss: 0.64447225, Gradient norm: 0.22537976
INFO:root:[   36] Training loss: 0.64120532, Validation loss: 0.64327906, Gradient norm: 0.13683074
INFO:root:[   37] Training loss: 0.64051065, Validation loss: 0.64503353, Gradient norm: 0.20441623
INFO:root:[   38] Training loss: 0.64183998, Validation loss: 0.64286891, Gradient norm: 0.34792029
INFO:root:[   39] Training loss: 0.63900309, Validation loss: 0.64052059, Gradient norm: 0.25876930
INFO:root:[   40] Training loss: 0.63790726, Validation loss: 0.64234167, Gradient norm: 0.19329874
INFO:root:[   41] Training loss: 0.63761400, Validation loss: 0.64283036, Gradient norm: 0.22196692
INFO:root:[   42] Training loss: 0.63768964, Validation loss: 0.63958534, Gradient norm: 0.26663684
INFO:root:[   43] Training loss: 0.63639224, Validation loss: 0.63796275, Gradient norm: 0.23883407
INFO:root:[   44] Training loss: 0.63497258, Validation loss: 0.63742138, Gradient norm: 0.16258261
INFO:root:[   45] Training loss: 0.63440078, Validation loss: 0.63671816, Gradient norm: 0.14129747
INFO:root:[   46] Training loss: 0.63489998, Validation loss: 0.64164488, Gradient norm: 0.26664835
INFO:root:[   47] Training loss: 0.63397939, Validation loss: 0.63660057, Gradient norm: 0.24722652
INFO:root:[   48] Training loss: 0.63296510, Validation loss: 0.63643132, Gradient norm: 0.24695900
INFO:root:[   49] Training loss: 0.63280802, Validation loss: 0.63441427, Gradient norm: 0.21118163
INFO:root:[   50] Training loss: 0.63082437, Validation loss: 0.63610207, Gradient norm: 0.22304421
INFO:root:[   51] Training loss: 0.63108920, Validation loss: 0.63466909, Gradient norm: 0.18470965
INFO:root:[   52] Training loss: 0.63057516, Validation loss: 0.63381329, Gradient norm: 0.22152172
INFO:root:[   53] Training loss: 0.63016848, Validation loss: 0.63282303, Gradient norm: 0.24503203
INFO:root:[   54] Training loss: 0.62784971, Validation loss: 0.63262601, Gradient norm: 0.11375273
INFO:root:[   55] Training loss: 0.62941313, Validation loss: 0.63460957, Gradient norm: 0.24270242
INFO:root:[   56] Training loss: 0.62816263, Validation loss: 0.63456419, Gradient norm: 0.22511673
INFO:root:[   57] Training loss: 0.62766425, Validation loss: 0.63057905, Gradient norm: 0.20684043
INFO:root:[   58] Training loss: 0.62691425, Validation loss: 0.62908085, Gradient norm: 0.21760211
INFO:root:[   59] Training loss: 0.62569836, Validation loss: 0.63126090, Gradient norm: 0.18076578
INFO:root:[   60] Training loss: 0.62576069, Validation loss: 0.62934288, Gradient norm: 0.20039555
INFO:root:[   61] Training loss: 0.62557137, Validation loss: 0.63071663, Gradient norm: 0.23231249
INFO:root:[   62] Training loss: 0.62461721, Validation loss: 0.63219890, Gradient norm: 0.19115294
INFO:root:[   63] Training loss: 0.62423753, Validation loss: 0.62993993, Gradient norm: 0.17931706
INFO:root:[   64] Training loss: 0.62421978, Validation loss: 0.62862018, Gradient norm: 0.23973434
INFO:root:[   65] Training loss: 0.62316185, Validation loss: 0.62788879, Gradient norm: 0.18152360
INFO:root:[   66] Training loss: 0.62167868, Validation loss: 0.62594796, Gradient norm: 0.14323731
INFO:root:[   67] Training loss: 0.62373997, Validation loss: 0.62818178, Gradient norm: 0.25406491
INFO:root:[   68] Training loss: 0.62183688, Validation loss: 0.62630046, Gradient norm: 0.18035278
INFO:root:[   69] Training loss: 0.62193242, Validation loss: 0.62449962, Gradient norm: 0.19402221
INFO:root:[   70] Training loss: 0.62174877, Validation loss: 0.62544826, Gradient norm: 0.22640808
INFO:root:[   71] Training loss: 0.62018161, Validation loss: 0.62497955, Gradient norm: 0.16276400
INFO:root:[   72] Training loss: 0.61966143, Validation loss: 0.62481922, Gradient norm: 0.16305459
INFO:root:[   73] Training loss: 0.61974692, Validation loss: 0.62793625, Gradient norm: 0.18527356
INFO:root:[   74] Training loss: 0.61954482, Validation loss: 0.62375899, Gradient norm: 0.18243095
INFO:root:[   75] Training loss: 0.61978437, Validation loss: 0.62545515, Gradient norm: 0.22102564
INFO:root:[   76] Training loss: 0.61871216, Validation loss: 0.62280448, Gradient norm: 0.20192632
INFO:root:[   77] Training loss: 0.61811712, Validation loss: 0.62559183, Gradient norm: 0.15375821
INFO:root:[   78] Training loss: 0.61917883, Validation loss: 0.62420696, Gradient norm: 0.23570437
INFO:root:[   79] Training loss: 0.61717712, Validation loss: 0.62185346, Gradient norm: 0.19786141
INFO:root:[   80] Training loss: 0.61645468, Validation loss: 0.62091900, Gradient norm: 0.17153453
INFO:root:[   81] Training loss: 0.61753297, Validation loss: 0.62929596, Gradient norm: 0.21256220
INFO:root:[   82] Training loss: 0.61632218, Validation loss: 0.62117166, Gradient norm: 0.15469803
INFO:root:[   83] Training loss: 0.61554645, Validation loss: 0.62127528, Gradient norm: 0.18012745
INFO:root:[   84] Training loss: 0.61583746, Validation loss: 0.62176350, Gradient norm: 0.20994818
INFO:root:[   85] Training loss: 0.61515555, Validation loss: 0.62245618, Gradient norm: 0.17861372
INFO:root:[   86] Training loss: 0.61413385, Validation loss: 0.62430775, Gradient norm: 0.13077021
INFO:root:[   87] Training loss: 0.61635499, Validation loss: 0.62292644, Gradient norm: 0.24642184
INFO:root:[   88] Training loss: 0.61577563, Validation loss: 0.62051395, Gradient norm: 0.21428446
INFO:root:[   89] Training loss: 0.61402291, Validation loss: 0.61849868, Gradient norm: 0.15670809
INFO:root:[   90] Training loss: 0.61252385, Validation loss: 0.62049857, Gradient norm: 0.15309770
INFO:root:[   91] Training loss: 0.61434414, Validation loss: 0.62102182, Gradient norm: 0.21253386
INFO:root:[   92] Training loss: 0.61353588, Validation loss: 0.62155442, Gradient norm: 0.19996508
INFO:root:[   93] Training loss: 0.61176600, Validation loss: 0.62228359, Gradient norm: 0.14289519
INFO:root:[   94] Training loss: 0.61250047, Validation loss: 0.62240931, Gradient norm: 0.17499831
INFO:root:[   95] Training loss: 0.61288010, Validation loss: 0.62031077, Gradient norm: 0.20934321
INFO:root:[   96] Training loss: 0.61199299, Validation loss: 0.61862256, Gradient norm: 0.17115137
INFO:root:[   97] Training loss: 0.61173027, Validation loss: 0.61829061, Gradient norm: 0.17819645
INFO:root:[   98] Training loss: 0.61145872, Validation loss: 0.61746296, Gradient norm: 0.18072943
INFO:root:[   99] Training loss: 0.61201865, Validation loss: 0.62062462, Gradient norm: 0.22849642
INFO:root:[  100] Training loss: 0.61052816, Validation loss: 0.61609864, Gradient norm: 0.16064359
INFO:root:[  101] Training loss: 0.61073700, Validation loss: 0.61855562, Gradient norm: 0.16153343
INFO:root:[  102] Training loss: 0.61061791, Validation loss: 0.61529173, Gradient norm: 0.22104994
INFO:root:[  103] Training loss: 0.61056087, Validation loss: 0.62042682, Gradient norm: 0.20204077
INFO:root:[  104] Training loss: 0.61071679, Validation loss: 0.61678642, Gradient norm: 0.18467887
INFO:root:[  105] Training loss: 0.60913291, Validation loss: 0.61587228, Gradient norm: 0.15273667
INFO:root:[  106] Training loss: 0.60978841, Validation loss: 0.61399532, Gradient norm: 0.20488489
INFO:root:[  107] Training loss: 0.60830624, Validation loss: 0.61543196, Gradient norm: 0.16080151
INFO:root:[  108] Training loss: 0.60992521, Validation loss: 0.62445393, Gradient norm: 0.21937736
INFO:root:[  109] Training loss: 0.60979851, Validation loss: 0.61469275, Gradient norm: 0.17448484
INFO:root:[  110] Training loss: 0.60835791, Validation loss: 0.61680306, Gradient norm: 0.14071676
INFO:root:[  111] Training loss: 0.60828989, Validation loss: 0.61769214, Gradient norm: 0.21743464
INFO:root:[  112] Training loss: 0.60719762, Validation loss: 0.61520537, Gradient norm: 0.14671365
INFO:root:[  113] Training loss: 0.60719414, Validation loss: 0.61681301, Gradient norm: 0.18030611
INFO:root:[  114] Training loss: 0.60753436, Validation loss: 0.61775133, Gradient norm: 0.20602275
INFO:root:[  115] Training loss: 0.60790912, Validation loss: 0.61361605, Gradient norm: 0.20776143
INFO:root:[  116] Training loss: 0.60665448, Validation loss: 0.61368966, Gradient norm: 0.17588731
INFO:root:[  117] Training loss: 0.60597807, Validation loss: 0.61586719, Gradient norm: 0.17426647
INFO:root:[  118] Training loss: 0.60714351, Validation loss: 0.61266994, Gradient norm: 0.17612117
INFO:root:[  119] Training loss: 0.60593026, Validation loss: 0.61479056, Gradient norm: 0.19100745
INFO:root:[  120] Training loss: 0.60597415, Validation loss: 0.61214755, Gradient norm: 0.16843541
INFO:root:[  121] Training loss: 0.60603079, Validation loss: 0.61223979, Gradient norm: 0.21761468
INFO:root:[  122] Training loss: 0.60610244, Validation loss: 0.61310876, Gradient norm: 0.19927199
INFO:root:[  123] Training loss: 0.60540556, Validation loss: 0.61088739, Gradient norm: 0.18226344
INFO:root:[  124] Training loss: 0.60378552, Validation loss: 0.61169701, Gradient norm: 0.14942577
INFO:root:[  125] Training loss: 0.60485720, Validation loss: 0.61235800, Gradient norm: 0.16772772
INFO:root:[  126] Training loss: 0.60479290, Validation loss: 0.61430671, Gradient norm: 0.18910435
INFO:root:[  127] Training loss: 0.60511804, Validation loss: 0.61348146, Gradient norm: 0.23363825
INFO:root:[  128] Training loss: 0.60349350, Validation loss: 0.61064842, Gradient norm: 0.16735632
INFO:root:[  129] Training loss: 0.60323378, Validation loss: 0.61322930, Gradient norm: 0.16465555
INFO:root:[  130] Training loss: 0.60463957, Validation loss: 0.61277911, Gradient norm: 0.20434104
INFO:root:[  131] Training loss: 0.60486906, Validation loss: 0.61259025, Gradient norm: 0.20664149
INFO:root:[  132] Training loss: 0.60250496, Validation loss: 0.61450987, Gradient norm: 0.14913565
INFO:root:[  133] Training loss: 0.60254631, Validation loss: 0.61043325, Gradient norm: 0.15086811
INFO:root:[  134] Training loss: 0.60459162, Validation loss: 0.61120294, Gradient norm: 0.23554054
INFO:root:[  135] Training loss: 0.60181245, Validation loss: 0.61020240, Gradient norm: 0.14230221
INFO:root:[  136] Training loss: 0.60277716, Validation loss: 0.60968543, Gradient norm: 0.19070253
INFO:root:[  137] Training loss: 0.60367661, Validation loss: 0.61055711, Gradient norm: 0.22282356
INFO:root:[  138] Training loss: 0.60242510, Validation loss: 0.61136655, Gradient norm: 0.17563752
INFO:root:[  139] Training loss: 0.60214800, Validation loss: 0.61186119, Gradient norm: 0.16324456
INFO:root:[  140] Training loss: 0.60227613, Validation loss: 0.61308612, Gradient norm: 0.19784259
INFO:root:[  141] Training loss: 0.60147103, Validation loss: 0.60916169, Gradient norm: 0.17158129
INFO:root:[  142] Training loss: 0.60214736, Validation loss: 0.61316391, Gradient norm: 0.21065725
INFO:root:[  143] Training loss: 0.60158002, Validation loss: 0.60940588, Gradient norm: 0.19423008
INFO:root:[  144] Training loss: 0.60067350, Validation loss: 0.61072923, Gradient norm: 0.15527857
INFO:root:[  145] Training loss: 0.60141411, Validation loss: 0.60861524, Gradient norm: 0.20223102
INFO:root:[  146] Training loss: 0.60098921, Validation loss: 0.60833472, Gradient norm: 0.19966447
INFO:root:[  147] Training loss: 0.60052906, Validation loss: 0.60872854, Gradient norm: 0.15430018
INFO:root:[  148] Training loss: 0.60024838, Validation loss: 0.60927462, Gradient norm: 0.14909022
INFO:root:[  149] Training loss: 0.60236080, Validation loss: 0.60957922, Gradient norm: 0.26076646
INFO:root:[  150] Training loss: 0.59989326, Validation loss: 0.60891735, Gradient norm: 0.16803465
INFO:root:[  151] Training loss: 0.60003765, Validation loss: 0.61251331, Gradient norm: 0.19547779
INFO:root:[  152] Training loss: 0.59985954, Validation loss: 0.60995129, Gradient norm: 0.20804970
INFO:root:[  153] Training loss: 0.59891655, Validation loss: 0.60587211, Gradient norm: 0.15132858
INFO:root:[  154] Training loss: 0.60084711, Validation loss: 0.61027168, Gradient norm: 0.23947836
INFO:root:[  155] Training loss: 0.59961056, Validation loss: 0.60589304, Gradient norm: 0.19054509
INFO:root:[  156] Training loss: 0.59917855, Validation loss: 0.60813198, Gradient norm: 0.16403966
INFO:root:[  157] Training loss: 0.59971105, Validation loss: 0.60939760, Gradient norm: 0.20941589
INFO:root:[  158] Training loss: 0.59992354, Validation loss: 0.60509334, Gradient norm: 0.21512990
INFO:root:[  159] Training loss: 0.59926807, Validation loss: 0.60635946, Gradient norm: 0.21507674
INFO:root:[  160] Training loss: 0.59901375, Validation loss: 0.60737304, Gradient norm: 0.18223331
INFO:root:[  161] Training loss: 0.60051695, Validation loss: 0.60968926, Gradient norm: 0.25265149
INFO:root:[  162] Training loss: 0.59827868, Validation loss: 0.60777281, Gradient norm: 0.14275184
INFO:root:[  163] Training loss: 0.59793732, Validation loss: 0.60489016, Gradient norm: 0.18196257
INFO:root:[  164] Training loss: 0.59905858, Validation loss: 0.60929788, Gradient norm: 0.21367096
INFO:root:[  165] Training loss: 0.59821253, Validation loss: 0.60680358, Gradient norm: 0.16669450
INFO:root:[  166] Training loss: 0.59940244, Validation loss: 0.60434169, Gradient norm: 0.22954110
INFO:root:[  167] Training loss: 0.59821777, Validation loss: 0.60683042, Gradient norm: 0.21155296
INFO:root:[  168] Training loss: 0.59854351, Validation loss: 0.60581167, Gradient norm: 0.20946055
INFO:root:[  169] Training loss: 0.59893424, Validation loss: 0.60672832, Gradient norm: 0.21852992
INFO:root:[  170] Training loss: 0.59822584, Validation loss: 0.60674805, Gradient norm: 0.19989231
INFO:root:[  171] Training loss: 0.59861807, Validation loss: 0.60729107, Gradient norm: 0.23197954
INFO:root:[  172] Training loss: 0.59788021, Validation loss: 0.60645596, Gradient norm: 0.22032361
INFO:root:[  173] Training loss: 0.59797513, Validation loss: 0.60707868, Gradient norm: 0.21097224
INFO:root:[  174] Training loss: 0.59775160, Validation loss: 0.60836483, Gradient norm: 0.21065202
INFO:root:[  175] Training loss: 0.59689659, Validation loss: 0.60410670, Gradient norm: 0.20016808
INFO:root:[  176] Training loss: 0.59761489, Validation loss: 0.60807674, Gradient norm: 0.23313654
INFO:root:[  177] Training loss: 0.59783788, Validation loss: 0.60671242, Gradient norm: 0.22879637
INFO:root:[  178] Training loss: 0.59691999, Validation loss: 0.60458920, Gradient norm: 0.20328602
INFO:root:[  179] Training loss: 0.59618519, Validation loss: 0.60766769, Gradient norm: 0.17566075
INFO:root:[  180] Training loss: 0.59690888, Validation loss: 0.60577530, Gradient norm: 0.22816256
INFO:root:[  181] Training loss: 0.59628802, Validation loss: 0.60780306, Gradient norm: 0.21322153
INFO:root:[  182] Training loss: 0.59591964, Validation loss: 0.60622436, Gradient norm: 0.19448643
INFO:root:[  183] Training loss: 0.59729779, Validation loss: 0.60717380, Gradient norm: 0.24204130
INFO:root:[  184] Training loss: 0.59720063, Validation loss: 0.60405695, Gradient norm: 0.25815008
INFO:root:[  185] Training loss: 0.59622540, Validation loss: 0.60595393, Gradient norm: 0.21010398
INFO:root:[  186] Training loss: 0.59637324, Validation loss: 0.60764795, Gradient norm: 0.21678871
INFO:root:[  187] Training loss: 0.59607603, Validation loss: 0.60610924, Gradient norm: 0.20915945
INFO:root:[  188] Training loss: 0.59676184, Validation loss: 0.60759137, Gradient norm: 0.26333322
INFO:root:[  189] Training loss: 0.59671547, Validation loss: 0.60570295, Gradient norm: 0.23475538
INFO:root:[  190] Training loss: 0.59538104, Validation loss: 0.60505854, Gradient norm: 0.17702703
INFO:root:[  191] Training loss: 0.59609852, Validation loss: 0.60845040, Gradient norm: 0.21679574
INFO:root:[  192] Training loss: 0.59707842, Validation loss: 0.60644393, Gradient norm: 0.28060513
INFO:root:[  193] Training loss: 0.59610225, Validation loss: 0.60611584, Gradient norm: 0.23164506
INFO:root:EP 193: Early stopping
INFO:root:Training the model took 7037.916s.
INFO:root:Emptying the cuda cache took 0.059s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82161
INFO:root:EnergyScoreTrain: 0.59465
INFO:root:CRPSTrain: 0.53667
INFO:root:Gaussian NLLTrain: 3.91393
INFO:root:CoverageTrain: 0.78619
INFO:root:IntervalWidthTrain: 3.2643
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.83514
INFO:root:EnergyScoreValidation: 0.60416
INFO:root:CRPSValidation: 0.54389
INFO:root:Gaussian NLLValidation: 4.03026
INFO:root:CoverageValidation: 0.78151
INFO:root:IntervalWidthValidation: 3.25882
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.83663
INFO:root:EnergyScoreTest: 0.60535
INFO:root:CRPSTest: 0.54511
INFO:root:Gaussian NLLTest: 3.94263
INFO:root:CoverageTest: 0.7802
INFO:root:IntervalWidthTest: 3.2544
INFO:root:###18 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 231589
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.75299321, Validation loss: 0.73202139, Gradient norm: 1.00585541
INFO:root:[    2] Training loss: 0.72453422, Validation loss: 0.72283821, Gradient norm: 0.96774016
INFO:root:[    3] Training loss: 0.72296313, Validation loss: 0.72393530, Gradient norm: 0.83954951
INFO:root:[    4] Training loss: 0.72170024, Validation loss: 0.72234262, Gradient norm: 0.73272557
INFO:root:[    5] Training loss: 0.72027807, Validation loss: 0.71949766, Gradient norm: 0.63734939
INFO:root:[    6] Training loss: 0.71698819, Validation loss: 0.71656945, Gradient norm: 0.36208924
INFO:root:[    7] Training loss: 0.71315485, Validation loss: 0.71009390, Gradient norm: 0.56720730
INFO:root:[    8] Training loss: 0.70579332, Validation loss: 0.70166009, Gradient norm: 0.52283168
INFO:root:[    9] Training loss: 0.69814650, Validation loss: 0.69410268, Gradient norm: 0.21552257
INFO:root:[   10] Training loss: 0.69333423, Validation loss: 0.69266857, Gradient norm: 0.41165464
INFO:root:[   11] Training loss: 0.68799531, Validation loss: 0.68732352, Gradient norm: 0.23555517
INFO:root:[   12] Training loss: 0.68393117, Validation loss: 0.68165525, Gradient norm: 0.23841599
INFO:root:[   13] Training loss: 0.68136541, Validation loss: 0.68149212, Gradient norm: 0.32579591
INFO:root:[   14] Training loss: 0.67770212, Validation loss: 0.67745760, Gradient norm: 0.22647369
INFO:root:[   15] Training loss: 0.67610902, Validation loss: 0.67556767, Gradient norm: 0.29163965
INFO:root:[   16] Training loss: 0.67265806, Validation loss: 0.67329964, Gradient norm: 0.22901158
INFO:root:[   17] Training loss: 0.66994753, Validation loss: 0.67070624, Gradient norm: 0.17163908
INFO:root:[   18] Training loss: 0.66821673, Validation loss: 0.67211446, Gradient norm: 0.21136918
INFO:root:[   19] Training loss: 0.66766757, Validation loss: 0.66767336, Gradient norm: 0.25880106
INFO:root:[   20] Training loss: 0.66605240, Validation loss: 0.66622563, Gradient norm: 0.23828101
INFO:root:[   21] Training loss: 0.66259009, Validation loss: 0.66398309, Gradient norm: 0.13989885
INFO:root:[   22] Training loss: 0.66244164, Validation loss: 0.66290987, Gradient norm: 0.24244215
INFO:root:[   23] Training loss: 0.65988830, Validation loss: 0.66064301, Gradient norm: 0.16654583
INFO:root:[   24] Training loss: 0.65904744, Validation loss: 0.65937206, Gradient norm: 0.20612744
INFO:root:[   25] Training loss: 0.65710206, Validation loss: 0.65806320, Gradient norm: 0.13468140
INFO:root:[   26] Training loss: 0.65756574, Validation loss: 0.65807444, Gradient norm: 0.25707952
INFO:root:[   27] Training loss: 0.65536673, Validation loss: 0.65711766, Gradient norm: 0.15958031
INFO:root:[   28] Training loss: 0.65351826, Validation loss: 0.65256085, Gradient norm: 0.10834450
INFO:root:[   29] Training loss: 0.65282638, Validation loss: 0.65303856, Gradient norm: 0.11281314
INFO:root:[   30] Training loss: 0.65311987, Validation loss: 0.65522598, Gradient norm: 0.22270469
INFO:root:[   31] Training loss: 0.65148441, Validation loss: 0.65372202, Gradient norm: 0.17849967
INFO:root:[   32] Training loss: 0.64923069, Validation loss: 0.65117284, Gradient norm: 0.11503304
INFO:root:[   33] Training loss: 0.64861157, Validation loss: 0.65134554, Gradient norm: 0.12576549
INFO:root:[   34] Training loss: 0.64890082, Validation loss: 0.65007706, Gradient norm: 0.22111447
INFO:root:[   35] Training loss: 0.64705803, Validation loss: 0.64865231, Gradient norm: 0.12620447
INFO:root:[   36] Training loss: 0.64596502, Validation loss: 0.65016711, Gradient norm: 0.13525247
INFO:root:[   37] Training loss: 0.64610469, Validation loss: 0.64796606, Gradient norm: 0.17289251
INFO:root:[   38] Training loss: 0.64589513, Validation loss: 0.64722545, Gradient norm: 0.17751800
INFO:root:[   39] Training loss: 0.64402749, Validation loss: 0.64511326, Gradient norm: 0.12428401
INFO:root:[   40] Training loss: 0.64305579, Validation loss: 0.64476608, Gradient norm: 0.09607908
INFO:root:[   41] Training loss: 0.64350797, Validation loss: 0.64634449, Gradient norm: 0.16786312
INFO:root:[   42] Training loss: 0.64231066, Validation loss: 0.64320400, Gradient norm: 0.12927577
INFO:root:[   43] Training loss: 0.64174737, Validation loss: 0.64563022, Gradient norm: 0.15961153
INFO:root:[   44] Training loss: 0.64124325, Validation loss: 0.64254330, Gradient norm: 0.12677426
INFO:root:[   45] Training loss: 0.63955446, Validation loss: 0.64326409, Gradient norm: 0.10993074
INFO:root:[   46] Training loss: 0.64096061, Validation loss: 0.64162703, Gradient norm: 0.19512722
INFO:root:[   47] Training loss: 0.63909555, Validation loss: 0.64103319, Gradient norm: 0.11325948
INFO:root:[   48] Training loss: 0.63800603, Validation loss: 0.63928148, Gradient norm: 0.10923295
INFO:root:[   49] Training loss: 0.63721070, Validation loss: 0.64026316, Gradient norm: 0.10938841
INFO:root:[   50] Training loss: 0.63726836, Validation loss: 0.63901221, Gradient norm: 0.16480538
INFO:root:[   51] Training loss: 0.63764282, Validation loss: 0.63931904, Gradient norm: 0.16455836
INFO:root:[   52] Training loss: 0.63621025, Validation loss: 0.64078252, Gradient norm: 0.13333451
INFO:root:[   53] Training loss: 0.63535763, Validation loss: 0.63943122, Gradient norm: 0.08509180
INFO:root:[   54] Training loss: 0.63609782, Validation loss: 0.63640237, Gradient norm: 0.17580570
INFO:root:[   55] Training loss: 0.63437368, Validation loss: 0.63534740, Gradient norm: 0.11377004
INFO:root:[   56] Training loss: 0.63443921, Validation loss: 0.64072762, Gradient norm: 0.12820226
INFO:root:[   57] Training loss: 0.63401603, Validation loss: 0.63780596, Gradient norm: 0.13206670
INFO:root:[   58] Training loss: 0.63325116, Validation loss: 0.63477970, Gradient norm: 0.13540376
INFO:root:[   59] Training loss: 0.63275435, Validation loss: 0.63377138, Gradient norm: 0.15438748
INFO:root:[   60] Training loss: 0.63273153, Validation loss: 0.63661592, Gradient norm: 0.14852642
INFO:root:[   61] Training loss: 0.63200717, Validation loss: 0.63522687, Gradient norm: 0.13201324
INFO:root:[   62] Training loss: 0.63159135, Validation loss: 0.63564608, Gradient norm: 0.10751304
INFO:root:[   63] Training loss: 0.63142281, Validation loss: 0.63349520, Gradient norm: 0.11126048
INFO:root:[   64] Training loss: 0.63137812, Validation loss: 0.63511655, Gradient norm: 0.13781663
INFO:root:[   65] Training loss: 0.63068542, Validation loss: 0.63313884, Gradient norm: 0.14503815
INFO:root:[   66] Training loss: 0.63014741, Validation loss: 0.63243540, Gradient norm: 0.12843921
INFO:root:[   67] Training loss: 0.62947527, Validation loss: 0.63126629, Gradient norm: 0.12136282
INFO:root:[   68] Training loss: 0.62984943, Validation loss: 0.63133779, Gradient norm: 0.14277302
INFO:root:[   69] Training loss: 0.62842210, Validation loss: 0.63168470, Gradient norm: 0.08866886
INFO:root:[   70] Training loss: 0.62833495, Validation loss: 0.63159311, Gradient norm: 0.11505283
INFO:root:[   71] Training loss: 0.62741707, Validation loss: 0.62986944, Gradient norm: 0.11133839
INFO:root:[   72] Training loss: 0.62900033, Validation loss: 0.63119573, Gradient norm: 0.16956845
INFO:root:[   73] Training loss: 0.62736531, Validation loss: 0.63052403, Gradient norm: 0.11649516
INFO:root:[   74] Training loss: 0.62701802, Validation loss: 0.63168999, Gradient norm: 0.10922510
INFO:root:[   75] Training loss: 0.62657644, Validation loss: 0.63215113, Gradient norm: 0.14411781
INFO:root:[   76] Training loss: 0.62564631, Validation loss: 0.62863921, Gradient norm: 0.10999221
INFO:root:[   77] Training loss: 0.62597578, Validation loss: 0.62964470, Gradient norm: 0.12355570
INFO:root:[   78] Training loss: 0.62562496, Validation loss: 0.62910940, Gradient norm: 0.10556961
INFO:root:[   79] Training loss: 0.62517370, Validation loss: 0.62810253, Gradient norm: 0.14763954
INFO:root:[   80] Training loss: 0.62417684, Validation loss: 0.62883852, Gradient norm: 0.10464359
INFO:root:[   81] Training loss: 0.62494096, Validation loss: 0.62787628, Gradient norm: 0.14253919
INFO:root:[   82] Training loss: 0.62398063, Validation loss: 0.62715413, Gradient norm: 0.11932603
INFO:root:[   83] Training loss: 0.62406495, Validation loss: 0.62747902, Gradient norm: 0.11316716
INFO:root:[   84] Training loss: 0.62352775, Validation loss: 0.62601093, Gradient norm: 0.10372377
INFO:root:[   85] Training loss: 0.62375475, Validation loss: 0.62689014, Gradient norm: 0.14401877
INFO:root:[   86] Training loss: 0.62393884, Validation loss: 0.62575212, Gradient norm: 0.11663613
INFO:root:[   87] Training loss: 0.62268877, Validation loss: 0.62793932, Gradient norm: 0.10148653
INFO:root:[   88] Training loss: 0.62360829, Validation loss: 0.62650929, Gradient norm: 0.13197625
INFO:root:[   89] Training loss: 0.62294305, Validation loss: 0.62581547, Gradient norm: 0.13574009
INFO:root:[   90] Training loss: 0.62217839, Validation loss: 0.62435488, Gradient norm: 0.11390255
INFO:root:[   91] Training loss: 0.62083671, Validation loss: 0.62623031, Gradient norm: 0.09436336
INFO:root:[   92] Training loss: 0.62167987, Validation loss: 0.62286904, Gradient norm: 0.12234016
INFO:root:[   93] Training loss: 0.62206458, Validation loss: 0.62615217, Gradient norm: 0.15103549
INFO:root:[   94] Training loss: 0.62140339, Validation loss: 0.62703740, Gradient norm: 0.12413293
INFO:root:[   95] Training loss: 0.62113746, Validation loss: 0.62345902, Gradient norm: 0.11687270
INFO:root:[   96] Training loss: 0.62013270, Validation loss: 0.62851052, Gradient norm: 0.10777698
INFO:root:[   97] Training loss: 0.62186667, Validation loss: 0.62275259, Gradient norm: 0.13597514
INFO:root:[   98] Training loss: 0.61900728, Validation loss: 0.62569608, Gradient norm: 0.10274565
INFO:root:[   99] Training loss: 0.62071056, Validation loss: 0.62248303, Gradient norm: 0.13857161
INFO:root:[  100] Training loss: 0.61992048, Validation loss: 0.62202253, Gradient norm: 0.12916127
INFO:root:[  101] Training loss: 0.61984913, Validation loss: 0.62299051, Gradient norm: 0.13537249
INFO:root:[  102] Training loss: 0.61867826, Validation loss: 0.62459292, Gradient norm: 0.10617128
INFO:root:[  103] Training loss: 0.62002524, Validation loss: 0.62371387, Gradient norm: 0.14952593
INFO:root:[  104] Training loss: 0.61880729, Validation loss: 0.62311812, Gradient norm: 0.10543709
INFO:root:[  105] Training loss: 0.61945640, Validation loss: 0.62215852, Gradient norm: 0.12191280
INFO:root:[  106] Training loss: 0.61809752, Validation loss: 0.62071094, Gradient norm: 0.10106463
INFO:root:[  107] Training loss: 0.61901830, Validation loss: 0.62090727, Gradient norm: 0.14773700
INFO:root:[  108] Training loss: 0.61795815, Validation loss: 0.62169242, Gradient norm: 0.14292826
INFO:root:[  109] Training loss: 0.61739104, Validation loss: 0.62033892, Gradient norm: 0.11029635
INFO:root:[  110] Training loss: 0.61701815, Validation loss: 0.62231649, Gradient norm: 0.09263480
INFO:root:[  111] Training loss: 0.61716074, Validation loss: 0.62063830, Gradient norm: 0.09683818
INFO:root:[  112] Training loss: 0.61686049, Validation loss: 0.62226403, Gradient norm: 0.10920431
INFO:root:[  113] Training loss: 0.61863078, Validation loss: 0.62629695, Gradient norm: 0.17711465
INFO:root:[  114] Training loss: 0.61694623, Validation loss: 0.62053574, Gradient norm: 0.11851442
INFO:root:[  115] Training loss: 0.61626135, Validation loss: 0.62132734, Gradient norm: 0.12846607
INFO:root:[  116] Training loss: 0.61702451, Validation loss: 0.62176922, Gradient norm: 0.11839540
INFO:root:[  117] Training loss: 0.61640844, Validation loss: 0.62478502, Gradient norm: 0.12637029
INFO:root:[  118] Training loss: 0.61700911, Validation loss: 0.62034831, Gradient norm: 0.15062119
INFO:root:[  119] Training loss: 0.61574604, Validation loss: 0.61993827, Gradient norm: 0.12535675
INFO:root:[  120] Training loss: 0.61516667, Validation loss: 0.62207703, Gradient norm: 0.10520209
INFO:root:[  121] Training loss: 0.61588534, Validation loss: 0.61880273, Gradient norm: 0.13493841
INFO:root:[  122] Training loss: 0.61485677, Validation loss: 0.62199021, Gradient norm: 0.12315497
INFO:root:[  123] Training loss: 0.61574953, Validation loss: 0.61683051, Gradient norm: 0.15166929
INFO:root:[  124] Training loss: 0.61495020, Validation loss: 0.62082395, Gradient norm: 0.11804806
INFO:root:[  125] Training loss: 0.61360490, Validation loss: 0.61802912, Gradient norm: 0.10675424
INFO:root:[  126] Training loss: 0.61404429, Validation loss: 0.61733139, Gradient norm: 0.10088611
INFO:root:[  127] Training loss: 0.61377912, Validation loss: 0.62061416, Gradient norm: 0.11330580
INFO:root:[  128] Training loss: 0.61598787, Validation loss: 0.61773961, Gradient norm: 0.17751655
INFO:root:[  129] Training loss: 0.61433037, Validation loss: 0.61871882, Gradient norm: 0.13325196
INFO:root:[  130] Training loss: 0.61370456, Validation loss: 0.61719024, Gradient norm: 0.11319270
INFO:root:[  131] Training loss: 0.61287525, Validation loss: 0.61996561, Gradient norm: 0.10242254
INFO:root:[  132] Training loss: 0.61445387, Validation loss: 0.61760985, Gradient norm: 0.14718299
INFO:root:EP 132: Early stopping
INFO:root:Training the model took 4824.723s.
INFO:root:Emptying the cuda cache took 0.06s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.84799
INFO:root:EnergyScoreTrain: 0.6135
INFO:root:CRPSTrain: 0.55928
INFO:root:Gaussian NLLTrain: 3.46732
INFO:root:CoverageTrain: 0.77217
INFO:root:IntervalWidthTrain: 3.32519
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.85485
INFO:root:EnergyScoreValidation: 0.61836
INFO:root:CRPSValidation: 0.5631
INFO:root:Gaussian NLLValidation: 3.47931
INFO:root:CoverageValidation: 0.77005
INFO:root:IntervalWidthValidation: 3.32499
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.85569
INFO:root:EnergyScoreTest: 0.61913
INFO:root:CRPSTest: 0.56376
INFO:root:Gaussian NLLTest: 3.48629
INFO:root:CoverageTest: 0.7691
INFO:root:IntervalWidthTest: 3.3186
INFO:root:###19 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.001, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 56623104
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72211814, Validation loss: 0.71914737, Gradient norm: 0.01958249
INFO:root:[    2] Training loss: 0.71510614, Validation loss: 0.70760931, Gradient norm: 0.04861663
INFO:root:[    3] Training loss: 0.70157077, Validation loss: 0.69618765, Gradient norm: 0.09907666
INFO:root:[    4] Training loss: 0.69061613, Validation loss: 0.68643744, Gradient norm: 0.10647875
INFO:root:[    5] Training loss: 0.67878994, Validation loss: 0.67312771, Gradient norm: 0.13747410
INFO:root:[    6] Training loss: 0.66951307, Validation loss: 0.66419384, Gradient norm: 0.15519045
INFO:root:[    7] Training loss: 0.66145755, Validation loss: 0.66047945, Gradient norm: 0.14569512
INFO:root:[    8] Training loss: 0.65629162, Validation loss: 0.65562762, Gradient norm: 0.15302464
INFO:root:[    9] Training loss: 0.65259423, Validation loss: 0.65236617, Gradient norm: 0.18149032
INFO:root:[   10] Training loss: 0.64824306, Validation loss: 0.64958929, Gradient norm: 0.17841516
INFO:root:[   11] Training loss: 0.64517024, Validation loss: 0.64537239, Gradient norm: 0.16569743
INFO:root:[   12] Training loss: 0.64242167, Validation loss: 0.64432185, Gradient norm: 0.17386943
INFO:root:[   13] Training loss: 0.64048238, Validation loss: 0.64431323, Gradient norm: 0.20635970
INFO:root:[   14] Training loss: 0.63735524, Validation loss: 0.63925044, Gradient norm: 0.17157854
INFO:root:[   15] Training loss: 0.63635861, Validation loss: 0.64157025, Gradient norm: 0.17560610
INFO:root:[   16] Training loss: 0.63384067, Validation loss: 0.63888154, Gradient norm: 0.20584610
INFO:root:[   17] Training loss: 0.63245811, Validation loss: 0.63900289, Gradient norm: 0.19864598
INFO:root:[   18] Training loss: 0.63037248, Validation loss: 0.63488410, Gradient norm: 0.19269709
INFO:root:[   19] Training loss: 0.62849054, Validation loss: 0.63482787, Gradient norm: 0.20059831
INFO:root:[   20] Training loss: 0.62747306, Validation loss: 0.63320075, Gradient norm: 0.20643808
INFO:root:[   21] Training loss: 0.62512731, Validation loss: 0.63002491, Gradient norm: 0.17596663
INFO:root:[   22] Training loss: 0.62445588, Validation loss: 0.63164458, Gradient norm: 0.20104874
INFO:root:[   23] Training loss: 0.62281619, Validation loss: 0.63006698, Gradient norm: 0.19975834
INFO:root:[   24] Training loss: 0.62146265, Validation loss: 0.63138928, Gradient norm: 0.19155413
INFO:root:[   25] Training loss: 0.62082594, Validation loss: 0.62897456, Gradient norm: 0.22411948
INFO:root:[   26] Training loss: 0.61921202, Validation loss: 0.62773079, Gradient norm: 0.18270672
INFO:root:[   27] Training loss: 0.61802594, Validation loss: 0.62721172, Gradient norm: 0.19847198
INFO:root:[   28] Training loss: 0.61700471, Validation loss: 0.63026768, Gradient norm: 0.22911008
INFO:root:[   29] Training loss: 0.61672518, Validation loss: 0.62763225, Gradient norm: 0.23037903
INFO:root:[   30] Training loss: 0.61475279, Validation loss: 0.62742985, Gradient norm: 0.16783914
INFO:root:[   31] Training loss: 0.61428775, Validation loss: 0.62575811, Gradient norm: 0.21990104
INFO:root:[   32] Training loss: 0.61297605, Validation loss: 0.62521332, Gradient norm: 0.21278697
INFO:root:[   33] Training loss: 0.61190398, Validation loss: 0.62830956, Gradient norm: 0.21454337
INFO:root:[   34] Training loss: 0.61171741, Validation loss: 0.62361086, Gradient norm: 0.25342433
INFO:root:[   35] Training loss: 0.60996543, Validation loss: 0.62871907, Gradient norm: 0.21016902
INFO:root:[   36] Training loss: 0.61039860, Validation loss: 0.62407080, Gradient norm: 0.21573433
INFO:root:[   37] Training loss: 0.60800181, Validation loss: 0.62593391, Gradient norm: 0.21483134
INFO:root:[   38] Training loss: 0.60806773, Validation loss: 0.62368016, Gradient norm: 0.21812476
INFO:root:[   39] Training loss: 0.60703889, Validation loss: 0.62308177, Gradient norm: 0.18086689
INFO:root:[   40] Training loss: 0.60702793, Validation loss: 0.62663283, Gradient norm: 0.24794463
INFO:root:[   41] Training loss: 0.60542485, Validation loss: 0.62327671, Gradient norm: 0.20339325
INFO:root:[   42] Training loss: 0.60475150, Validation loss: 0.62340762, Gradient norm: 0.21328963
INFO:root:[   43] Training loss: 0.60550276, Validation loss: 0.62468597, Gradient norm: 0.27485257
INFO:root:[   44] Training loss: 0.60392446, Validation loss: 0.62322038, Gradient norm: 0.26863294
INFO:root:[   45] Training loss: 0.60261636, Validation loss: 0.62303395, Gradient norm: 0.22372723
INFO:root:[   46] Training loss: 0.60158838, Validation loss: 0.62483223, Gradient norm: 0.21594164
INFO:root:[   47] Training loss: 0.60090218, Validation loss: 0.62544769, Gradient norm: 0.22711780
INFO:root:[   48] Training loss: 0.60066630, Validation loss: 0.62323022, Gradient norm: 0.26079713
INFO:root:[   49] Training loss: 0.59993283, Validation loss: 0.62311823, Gradient norm: 0.25886454
INFO:root:[   50] Training loss: 0.59897535, Validation loss: 0.62570534, Gradient norm: 0.24433512
INFO:root:[   51] Training loss: 0.59853124, Validation loss: 0.62535584, Gradient norm: 0.26393696
INFO:root:[   52] Training loss: 0.59701377, Validation loss: 0.62521313, Gradient norm: 0.15535127
INFO:root:[   53] Training loss: 0.59681869, Validation loss: 0.62396591, Gradient norm: 0.22003496
INFO:root:[   54] Training loss: 0.59779739, Validation loss: 0.62552643, Gradient norm: 0.24885952
INFO:root:[   55] Training loss: 0.59565102, Validation loss: 0.62354769, Gradient norm: 0.25595221
INFO:root:[   56] Training loss: 0.59521048, Validation loss: 0.62341626, Gradient norm: 0.25373806
INFO:root:[   57] Training loss: 0.59505875, Validation loss: 0.62458223, Gradient norm: 0.25753644
INFO:root:[   58] Training loss: 0.59444196, Validation loss: 0.62763583, Gradient norm: 0.27558040
INFO:root:[   59] Training loss: 0.59288939, Validation loss: 0.62550207, Gradient norm: 0.22457810
INFO:root:[   60] Training loss: 0.59366209, Validation loss: 0.62690940, Gradient norm: 0.30501849
INFO:root:[   61] Training loss: 0.59309739, Validation loss: 0.62628450, Gradient norm: 0.26539090
INFO:root:[   62] Training loss: 0.59053199, Validation loss: 0.62633811, Gradient norm: 0.22025308
INFO:root:[   63] Training loss: 0.59181015, Validation loss: 0.62775334, Gradient norm: 0.27408265
INFO:root:[   64] Training loss: 0.59005423, Validation loss: 0.62701384, Gradient norm: 0.24275308
INFO:root:[   65] Training loss: 0.58984877, Validation loss: 0.62763477, Gradient norm: 0.29089961
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 2128.317s.
INFO:root:Emptying the cuda cache took 0.033s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82536
INFO:root:EnergyScoreTrain: 0.59567
INFO:root:CRPSTrain: 0.48372
INFO:root:Gaussian NLLTrain: 14162428.85417
INFO:root:CoverageTrain: 0.7776
INFO:root:IntervalWidthTrain: 2.75297
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86138
INFO:root:EnergyScoreValidation: 0.62245
INFO:root:CRPSValidation: 0.50844
INFO:root:Gaussian NLLValidation: 17735824.32
INFO:root:CoverageValidation: 0.76545
INFO:root:IntervalWidthValidation: 2.75436
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86291
INFO:root:EnergyScoreTest: 0.62365
INFO:root:CRPSTest: 0.50958
INFO:root:Gaussian NLLTest: 16873431.334
INFO:root:CoverageTest: 0.76661
INFO:root:IntervalWidthTest: 2.76198
INFO:root:###20 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.005, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72213145, Validation loss: 0.71904808, Gradient norm: 0.01868834
INFO:root:[    2] Training loss: 0.71517009, Validation loss: 0.70773637, Gradient norm: 0.04795549
INFO:root:[    3] Training loss: 0.70196646, Validation loss: 0.69704362, Gradient norm: 0.08863565
INFO:root:[    4] Training loss: 0.69201994, Validation loss: 0.68766383, Gradient norm: 0.10456322
INFO:root:[    5] Training loss: 0.68115450, Validation loss: 0.67556875, Gradient norm: 0.09174192
INFO:root:[    6] Training loss: 0.67257512, Validation loss: 0.66880662, Gradient norm: 0.12706591
INFO:root:[    7] Training loss: 0.66494957, Validation loss: 0.66075509, Gradient norm: 0.11314827
INFO:root:[    8] Training loss: 0.65964496, Validation loss: 0.65640974, Gradient norm: 0.15279232
INFO:root:[    9] Training loss: 0.65535037, Validation loss: 0.65272879, Gradient norm: 0.15555882
INFO:root:[   10] Training loss: 0.65101787, Validation loss: 0.65097629, Gradient norm: 0.11260012
INFO:root:[   11] Training loss: 0.64924454, Validation loss: 0.64775377, Gradient norm: 0.17026690
INFO:root:[   12] Training loss: 0.64587955, Validation loss: 0.64456166, Gradient norm: 0.14365017
INFO:root:[   13] Training loss: 0.64353745, Validation loss: 0.64467419, Gradient norm: 0.15309407
INFO:root:[   14] Training loss: 0.64093542, Validation loss: 0.64012351, Gradient norm: 0.14302388
INFO:root:[   15] Training loss: 0.63959754, Validation loss: 0.63897061, Gradient norm: 0.14234103
INFO:root:[   16] Training loss: 0.63747670, Validation loss: 0.63805949, Gradient norm: 0.18661903
INFO:root:[   17] Training loss: 0.63515248, Validation loss: 0.63566348, Gradient norm: 0.10777499
INFO:root:[   18] Training loss: 0.63418612, Validation loss: 0.63693540, Gradient norm: 0.14589682
INFO:root:[   19] Training loss: 0.63255318, Validation loss: 0.63521337, Gradient norm: 0.16876692
INFO:root:[   20] Training loss: 0.63121046, Validation loss: 0.63455874, Gradient norm: 0.14373627
INFO:root:[   21] Training loss: 0.62896785, Validation loss: 0.63261451, Gradient norm: 0.11986889
INFO:root:[   22] Training loss: 0.62832559, Validation loss: 0.63094479, Gradient norm: 0.17694908
INFO:root:[   23] Training loss: 0.62726508, Validation loss: 0.63313846, Gradient norm: 0.16405407
INFO:root:[   24] Training loss: 0.62527350, Validation loss: 0.63204907, Gradient norm: 0.14881309
INFO:root:[   25] Training loss: 0.62514806, Validation loss: 0.62977306, Gradient norm: 0.15390080
INFO:root:[   26] Training loss: 0.62298702, Validation loss: 0.62687154, Gradient norm: 0.14389942
INFO:root:[   27] Training loss: 0.62238538, Validation loss: 0.62709457, Gradient norm: 0.16267084
INFO:root:[   28] Training loss: 0.62109421, Validation loss: 0.62992421, Gradient norm: 0.17334512
INFO:root:[   29] Training loss: 0.62066868, Validation loss: 0.62749524, Gradient norm: 0.15874739
INFO:root:[   30] Training loss: 0.61982821, Validation loss: 0.62755926, Gradient norm: 0.17877035
INFO:root:[   31] Training loss: 0.61782696, Validation loss: 0.62440428, Gradient norm: 0.13555582
INFO:root:[   32] Training loss: 0.61707245, Validation loss: 0.62728164, Gradient norm: 0.14193232
INFO:root:[   33] Training loss: 0.61689925, Validation loss: 0.62770689, Gradient norm: 0.16649993
INFO:root:[   34] Training loss: 0.61660338, Validation loss: 0.62613673, Gradient norm: 0.18713645
INFO:root:[   35] Training loss: 0.61543735, Validation loss: 0.62622049, Gradient norm: 0.16428761
INFO:root:[   36] Training loss: 0.61359540, Validation loss: 0.62437543, Gradient norm: 0.11068500
INFO:root:[   37] Training loss: 0.61290738, Validation loss: 0.62671560, Gradient norm: 0.12295843
INFO:root:[   38] Training loss: 0.61380439, Validation loss: 0.62584583, Gradient norm: 0.21009539
INFO:root:[   39] Training loss: 0.61217075, Validation loss: 0.62750147, Gradient norm: 0.17032628
INFO:root:[   40] Training loss: 0.61168513, Validation loss: 0.62541508, Gradient norm: 0.17129027
INFO:root:[   41] Training loss: 0.61045916, Validation loss: 0.62418058, Gradient norm: 0.15317744
INFO:root:[   42] Training loss: 0.61070141, Validation loss: 0.62474172, Gradient norm: 0.18055671
INFO:root:[   43] Training loss: 0.61012554, Validation loss: 0.62444907, Gradient norm: 0.20316140
INFO:root:[   44] Training loss: 0.60825422, Validation loss: 0.62325976, Gradient norm: 0.15574124
INFO:root:[   45] Training loss: 0.60794617, Validation loss: 0.62606469, Gradient norm: 0.16623890
INFO:root:[   46] Training loss: 0.60755552, Validation loss: 0.62367316, Gradient norm: 0.18671685
INFO:root:[   47] Training loss: 0.60604670, Validation loss: 0.62306223, Gradient norm: 0.14367136
INFO:root:[   48] Training loss: 0.60583428, Validation loss: 0.62363082, Gradient norm: 0.18788493
INFO:root:[   49] Training loss: 0.60604650, Validation loss: 0.62385272, Gradient norm: 0.16887266
INFO:root:[   50] Training loss: 0.60447620, Validation loss: 0.62463374, Gradient norm: 0.15079921
INFO:root:[   51] Training loss: 0.60412694, Validation loss: 0.62255962, Gradient norm: 0.14863474
INFO:root:[   52] Training loss: 0.60318916, Validation loss: 0.62465721, Gradient norm: 0.14799720
INFO:root:[   53] Training loss: 0.60343970, Validation loss: 0.62457381, Gradient norm: 0.17681353
INFO:root:[   54] Training loss: 0.60297623, Validation loss: 0.62439380, Gradient norm: 0.19719803
INFO:root:[   55] Training loss: 0.60235513, Validation loss: 0.62598307, Gradient norm: 0.20226289
INFO:root:[   56] Training loss: 0.60085384, Validation loss: 0.62289642, Gradient norm: 0.16644135
INFO:root:[   57] Training loss: 0.60181320, Validation loss: 0.62742437, Gradient norm: 0.19694358
INFO:root:[   58] Training loss: 0.60030112, Validation loss: 0.62496131, Gradient norm: 0.16166040
INFO:root:[   59] Training loss: 0.59975942, Validation loss: 0.62688760, Gradient norm: 0.17978446
INFO:root:[   60] Training loss: 0.59940150, Validation loss: 0.62479187, Gradient norm: 0.19249422
INFO:root:[   61] Training loss: 0.59764293, Validation loss: 0.62668701, Gradient norm: 0.12927174
INFO:root:[   62] Training loss: 0.59913427, Validation loss: 0.62724366, Gradient norm: 0.21656020
INFO:root:[   63] Training loss: 0.59672119, Validation loss: 0.62472468, Gradient norm: 0.12911351
INFO:root:[   64] Training loss: 0.59709139, Validation loss: 0.62584549, Gradient norm: 0.18280050
INFO:root:[   65] Training loss: 0.59744116, Validation loss: 0.62614577, Gradient norm: 0.17091852
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 2128.087s.
INFO:root:Emptying the cuda cache took 0.027s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82411
INFO:root:EnergyScoreTrain: 0.5947
INFO:root:CRPSTrain: 0.47356
INFO:root:Gaussian NLLTrain: 5635.60983
INFO:root:CoverageTrain: 0.85582
INFO:root:IntervalWidthTrain: 2.96154
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86227
INFO:root:EnergyScoreValidation: 0.6228
INFO:root:CRPSValidation: 0.49801
INFO:root:Gaussian NLLValidation: 4329.9023
INFO:root:CoverageValidation: 0.84227
INFO:root:IntervalWidthValidation: 2.96454
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86399
INFO:root:EnergyScoreTest: 0.62416
INFO:root:CRPSTest: 0.49903
INFO:root:Gaussian NLLTest: 7507.52893
INFO:root:CoverageTest: 0.84295
INFO:root:IntervalWidthTest: 2.96853
INFO:root:###21 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72216327, Validation loss: 0.71905785, Gradient norm: 0.01837224
INFO:root:[    2] Training loss: 0.71529076, Validation loss: 0.70788493, Gradient norm: 0.04579343
INFO:root:[    3] Training loss: 0.70251682, Validation loss: 0.69694117, Gradient norm: 0.08266962
INFO:root:[    4] Training loss: 0.69295847, Validation loss: 0.68918849, Gradient norm: 0.09602264
INFO:root:[    5] Training loss: 0.68281067, Validation loss: 0.67842547, Gradient norm: 0.10102809
INFO:root:[    6] Training loss: 0.67469230, Validation loss: 0.67000527, Gradient norm: 0.11095114
INFO:root:[    7] Training loss: 0.66695966, Validation loss: 0.66322057, Gradient norm: 0.09560340
INFO:root:[    8] Training loss: 0.66189209, Validation loss: 0.65659207, Gradient norm: 0.14247576
INFO:root:[    9] Training loss: 0.65687747, Validation loss: 0.65289435, Gradient norm: 0.13502786
INFO:root:[   10] Training loss: 0.65326185, Validation loss: 0.65284087, Gradient norm: 0.10216523
INFO:root:[   11] Training loss: 0.65073123, Validation loss: 0.64696662, Gradient norm: 0.15342061
INFO:root:[   12] Training loss: 0.64782273, Validation loss: 0.64556899, Gradient norm: 0.10033800
INFO:root:[   13] Training loss: 0.64552361, Validation loss: 0.64449065, Gradient norm: 0.13586877
INFO:root:[   14] Training loss: 0.64295801, Validation loss: 0.64054921, Gradient norm: 0.10915925
INFO:root:[   15] Training loss: 0.64090464, Validation loss: 0.63927992, Gradient norm: 0.10091151
INFO:root:[   16] Training loss: 0.63892787, Validation loss: 0.64006222, Gradient norm: 0.11868193
INFO:root:[   17] Training loss: 0.63816992, Validation loss: 0.63772001, Gradient norm: 0.15921795
INFO:root:[   18] Training loss: 0.63599610, Validation loss: 0.63593211, Gradient norm: 0.14647492
INFO:root:[   19] Training loss: 0.63397336, Validation loss: 0.63533074, Gradient norm: 0.11931770
INFO:root:[   20] Training loss: 0.63284747, Validation loss: 0.63428189, Gradient norm: 0.11423847
INFO:root:[   21] Training loss: 0.63169072, Validation loss: 0.63143210, Gradient norm: 0.14735098
INFO:root:[   22] Training loss: 0.62997454, Validation loss: 0.63292824, Gradient norm: 0.12001432
INFO:root:[   23] Training loss: 0.62881959, Validation loss: 0.63051498, Gradient norm: 0.12793083
INFO:root:[   24] Training loss: 0.62776268, Validation loss: 0.63419567, Gradient norm: 0.12795844
INFO:root:[   25] Training loss: 0.62691022, Validation loss: 0.62987606, Gradient norm: 0.12813495
INFO:root:[   26] Training loss: 0.62517702, Validation loss: 0.62814550, Gradient norm: 0.12246397
INFO:root:[   27] Training loss: 0.62477633, Validation loss: 0.62793021, Gradient norm: 0.14739206
INFO:root:[   28] Training loss: 0.62381083, Validation loss: 0.62655360, Gradient norm: 0.12638895
INFO:root:[   29] Training loss: 0.62223986, Validation loss: 0.63029449, Gradient norm: 0.11902505
INFO:root:[   30] Training loss: 0.62196307, Validation loss: 0.62656796, Gradient norm: 0.11864194
INFO:root:[   31] Training loss: 0.62031810, Validation loss: 0.62544856, Gradient norm: 0.13402754
INFO:root:[   32] Training loss: 0.62011175, Validation loss: 0.62594719, Gradient norm: 0.14290615
INFO:root:[   33] Training loss: 0.61889159, Validation loss: 0.62491635, Gradient norm: 0.11982427
INFO:root:[   34] Training loss: 0.61760360, Validation loss: 0.63059635, Gradient norm: 0.09552911
INFO:root:[   35] Training loss: 0.61858469, Validation loss: 0.62700121, Gradient norm: 0.17709222
INFO:root:[   36] Training loss: 0.61733637, Validation loss: 0.62429875, Gradient norm: 0.13242973
INFO:root:[   37] Training loss: 0.61499228, Validation loss: 0.62408387, Gradient norm: 0.08432267
INFO:root:[   38] Training loss: 0.61549750, Validation loss: 0.62716152, Gradient norm: 0.14564467
INFO:root:[   39] Training loss: 0.61472494, Validation loss: 0.62294962, Gradient norm: 0.14903160
INFO:root:[   40] Training loss: 0.61362283, Validation loss: 0.62661194, Gradient norm: 0.11914111
INFO:root:[   41] Training loss: 0.61337673, Validation loss: 0.62566987, Gradient norm: 0.12968351
INFO:root:[   42] Training loss: 0.61234288, Validation loss: 0.62257583, Gradient norm: 0.12512679
INFO:root:[   43] Training loss: 0.61158320, Validation loss: 0.62258240, Gradient norm: 0.11065796
INFO:root:[   44] Training loss: 0.61166039, Validation loss: 0.62169225, Gradient norm: 0.15514632
INFO:root:[   45] Training loss: 0.61035650, Validation loss: 0.62255871, Gradient norm: 0.12116466
INFO:root:[   46] Training loss: 0.61012687, Validation loss: 0.62176802, Gradient norm: 0.15474986
INFO:root:[   47] Training loss: 0.61023768, Validation loss: 0.62274751, Gradient norm: 0.15781172
INFO:root:[   48] Training loss: 0.60819545, Validation loss: 0.62466043, Gradient norm: 0.11923260
INFO:root:[   49] Training loss: 0.60876343, Validation loss: 0.62339208, Gradient norm: 0.14936157
INFO:root:[   50] Training loss: 0.60714236, Validation loss: 0.62304945, Gradient norm: 0.12513987
INFO:root:[   51] Training loss: 0.60705837, Validation loss: 0.62114213, Gradient norm: 0.14916353
INFO:root:[   52] Training loss: 0.60640153, Validation loss: 0.62144143, Gradient norm: 0.11448734
INFO:root:[   53] Training loss: 0.60552477, Validation loss: 0.62258837, Gradient norm: 0.12869022
INFO:root:[   54] Training loss: 0.60646980, Validation loss: 0.62522332, Gradient norm: 0.15944658
INFO:root:[   55] Training loss: 0.60407197, Validation loss: 0.62152836, Gradient norm: 0.12534162
INFO:root:[   56] Training loss: 0.60417650, Validation loss: 0.62334371, Gradient norm: 0.14849923
INFO:root:[   57] Training loss: 0.60366317, Validation loss: 0.62278239, Gradient norm: 0.13147310
INFO:root:[   58] Training loss: 0.60453103, Validation loss: 0.62358834, Gradient norm: 0.17528744
INFO:root:[   59] Training loss: 0.60244267, Validation loss: 0.62548246, Gradient norm: 0.11785162
INFO:root:[   60] Training loss: 0.60236937, Validation loss: 0.62429879, Gradient norm: 0.15353038
INFO:root:[   61] Training loss: 0.60235024, Validation loss: 0.62469712, Gradient norm: 0.15934437
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 1998.416s.
INFO:root:Emptying the cuda cache took 0.028s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82374
INFO:root:EnergyScoreTrain: 0.59443
INFO:root:CRPSTrain: 0.4759
INFO:root:Gaussian NLLTrain: 119.38917
INFO:root:CoverageTrain: 0.85498
INFO:root:IntervalWidthTrain: 2.97817
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.86016
INFO:root:EnergyScoreValidation: 0.62128
INFO:root:CRPSValidation: 0.49901
INFO:root:Gaussian NLLValidation: 121.24176
INFO:root:CoverageValidation: 0.84259
INFO:root:IntervalWidthValidation: 2.98133
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.86168
INFO:root:EnergyScoreTest: 0.62247
INFO:root:CRPSTest: 0.50001
INFO:root:Gaussian NLLTest: 159.98097
INFO:root:CoverageTest: 0.84329
INFO:root:IntervalWidthTest: 2.98611
INFO:root:###22 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.02, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72219288, Validation loss: 0.71914376, Gradient norm: 0.01759060
INFO:root:[    2] Training loss: 0.71569460, Validation loss: 0.70913189, Gradient norm: 0.03979231
INFO:root:[    3] Training loss: 0.70354927, Validation loss: 0.69755066, Gradient norm: 0.07023938
INFO:root:[    4] Training loss: 0.69408724, Validation loss: 0.68793870, Gradient norm: 0.08409262
INFO:root:[    5] Training loss: 0.68442548, Validation loss: 0.67856019, Gradient norm: 0.07993228
INFO:root:[    6] Training loss: 0.67637536, Validation loss: 0.67018226, Gradient norm: 0.09682102
INFO:root:[    7] Training loss: 0.66896428, Validation loss: 0.66212061, Gradient norm: 0.09970450
INFO:root:[    8] Training loss: 0.66283246, Validation loss: 0.65726339, Gradient norm: 0.09488377
INFO:root:[    9] Training loss: 0.65909502, Validation loss: 0.65336917, Gradient norm: 0.13715730
INFO:root:[   10] Training loss: 0.65533968, Validation loss: 0.65070158, Gradient norm: 0.09701436
INFO:root:[   11] Training loss: 0.65228162, Validation loss: 0.64677152, Gradient norm: 0.11403752
INFO:root:[   12] Training loss: 0.64996684, Validation loss: 0.64408189, Gradient norm: 0.12359958
INFO:root:[   13] Training loss: 0.64722251, Validation loss: 0.64318913, Gradient norm: 0.10150365
INFO:root:[   14] Training loss: 0.64445463, Validation loss: 0.63914207, Gradient norm: 0.10106020
INFO:root:[   15] Training loss: 0.64370242, Validation loss: 0.63848142, Gradient norm: 0.12401218
INFO:root:[   16] Training loss: 0.64078049, Validation loss: 0.63899911, Gradient norm: 0.10923660
INFO:root:[   17] Training loss: 0.63953767, Validation loss: 0.63620569, Gradient norm: 0.11888431
INFO:root:[   18] Training loss: 0.63818697, Validation loss: 0.63526847, Gradient norm: 0.11077905
INFO:root:[   19] Training loss: 0.63615824, Validation loss: 0.63682501, Gradient norm: 0.10146639
INFO:root:[   20] Training loss: 0.63530038, Validation loss: 0.63352650, Gradient norm: 0.10848434
INFO:root:[   21] Training loss: 0.63358381, Validation loss: 0.63133190, Gradient norm: 0.09884763
INFO:root:[   22] Training loss: 0.63225467, Validation loss: 0.63022736, Gradient norm: 0.09505567
INFO:root:[   23] Training loss: 0.63187752, Validation loss: 0.63091440, Gradient norm: 0.13707316
INFO:root:[   24] Training loss: 0.63015770, Validation loss: 0.63063216, Gradient norm: 0.10851466
INFO:root:[   25] Training loss: 0.62903033, Validation loss: 0.63116663, Gradient norm: 0.09878555
INFO:root:[   26] Training loss: 0.62912935, Validation loss: 0.62790810, Gradient norm: 0.14437207
INFO:root:[   27] Training loss: 0.62702349, Validation loss: 0.62808364, Gradient norm: 0.10167416
INFO:root:[   28] Training loss: 0.62669934, Validation loss: 0.62542664, Gradient norm: 0.12079495
INFO:root:[   29] Training loss: 0.62467782, Validation loss: 0.62743024, Gradient norm: 0.08478950
INFO:root:[   30] Training loss: 0.62476558, Validation loss: 0.62572979, Gradient norm: 0.11115949
INFO:root:[   31] Training loss: 0.62375810, Validation loss: 0.62566000, Gradient norm: 0.11553200
INFO:root:[   32] Training loss: 0.62302309, Validation loss: 0.62422518, Gradient norm: 0.12312381
INFO:root:[   33] Training loss: 0.62200478, Validation loss: 0.62453634, Gradient norm: 0.11762549
INFO:root:[   34] Training loss: 0.62084540, Validation loss: 0.62263390, Gradient norm: 0.07518590
INFO:root:[   35] Training loss: 0.62142843, Validation loss: 0.62454290, Gradient norm: 0.13112818
INFO:root:[   36] Training loss: 0.61982946, Validation loss: 0.62451878, Gradient norm: 0.10648570
INFO:root:[   37] Training loss: 0.61960842, Validation loss: 0.62767699, Gradient norm: 0.12201476
INFO:root:[   38] Training loss: 0.61883865, Validation loss: 0.62178461, Gradient norm: 0.10936590
INFO:root:[   39] Training loss: 0.61785678, Validation loss: 0.62291929, Gradient norm: 0.10813217
INFO:root:[   40] Training loss: 0.61790476, Validation loss: 0.62454069, Gradient norm: 0.13588837
INFO:root:[   41] Training loss: 0.61624318, Validation loss: 0.62181710, Gradient norm: 0.09398084
INFO:root:[   42] Training loss: 0.61682834, Validation loss: 0.62494135, Gradient norm: 0.13971757
INFO:root:[   43] Training loss: 0.61555967, Validation loss: 0.62076339, Gradient norm: 0.11048663
INFO:root:[   44] Training loss: 0.61506551, Validation loss: 0.62019918, Gradient norm: 0.11786192
INFO:root:[   45] Training loss: 0.61409194, Validation loss: 0.62041066, Gradient norm: 0.11419110
INFO:root:[   46] Training loss: 0.61451646, Validation loss: 0.62017572, Gradient norm: 0.12579541
INFO:root:[   47] Training loss: 0.61344838, Validation loss: 0.62016704, Gradient norm: 0.12712168
INFO:root:[   48] Training loss: 0.61318394, Validation loss: 0.62126847, Gradient norm: 0.12750127
INFO:root:[   49] Training loss: 0.61206588, Validation loss: 0.62054340, Gradient norm: 0.12376554
INFO:root:[   50] Training loss: 0.61197150, Validation loss: 0.62032234, Gradient norm: 0.12115475
INFO:root:[   51] Training loss: 0.61135134, Validation loss: 0.61929820, Gradient norm: 0.11909269
INFO:root:[   52] Training loss: 0.61062593, Validation loss: 0.61961423, Gradient norm: 0.10233106
INFO:root:[   53] Training loss: 0.61028416, Validation loss: 0.62070517, Gradient norm: 0.12583928
INFO:root:[   54] Training loss: 0.61048903, Validation loss: 0.62065746, Gradient norm: 0.13179192
INFO:root:[   55] Training loss: 0.60855141, Validation loss: 0.61936692, Gradient norm: 0.10715680
INFO:root:[   56] Training loss: 0.60896740, Validation loss: 0.61987408, Gradient norm: 0.12214675
INFO:root:[   57] Training loss: 0.60836523, Validation loss: 0.61929950, Gradient norm: 0.11789605
INFO:root:[   58] Training loss: 0.60829419, Validation loss: 0.62045759, Gradient norm: 0.12254550
INFO:root:[   59] Training loss: 0.60784875, Validation loss: 0.62116453, Gradient norm: 0.12514096
INFO:root:[   60] Training loss: 0.60685794, Validation loss: 0.62028908, Gradient norm: 0.10665953
INFO:root:[   61] Training loss: 0.60683394, Validation loss: 0.62046208, Gradient norm: 0.12354186
INFO:root:[   62] Training loss: 0.60558631, Validation loss: 0.62164474, Gradient norm: 0.11806056
INFO:root:[   63] Training loss: 0.60554415, Validation loss: 0.62068637, Gradient norm: 0.11580770
INFO:root:[   64] Training loss: 0.60531811, Validation loss: 0.62060163, Gradient norm: 0.11786851
INFO:root:[   65] Training loss: 0.60517816, Validation loss: 0.62156107, Gradient norm: 0.13441955
INFO:root:[   66] Training loss: 0.60485490, Validation loss: 0.62113254, Gradient norm: 0.14789322
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 2174.207s.
INFO:root:Emptying the cuda cache took 0.033s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-reparam
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.82642
INFO:root:EnergyScoreTrain: 0.59637
INFO:root:CRPSTrain: 0.47352
INFO:root:Gaussian NLLTrain: 2.08172
INFO:root:CoverageTrain: 0.89289
INFO:root:IntervalWidthTrain: 3.07563
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.85812
INFO:root:EnergyScoreValidation: 0.61962
INFO:root:CRPSValidation: 0.49328
INFO:root:Gaussian NLLValidation: 2.30885
INFO:root:CoverageValidation: 0.88193
INFO:root:IntervalWidthValidation: 3.07819
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.85943
INFO:root:EnergyScoreTest: 0.62068
INFO:root:CRPSTest: 0.49422
INFO:root:Gaussian NLLTest: 2.32258
INFO:root:CoverageTest: 0.88218
INFO:root:IntervalWidthTest: 3.08338
INFO:root:###23 out of 27 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-reparam', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12), 'n_samples': 3}
INFO:root:NumberParameters: 234406
INFO:root:Memory allocated: 54525952
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.72231396, Validation loss: 0.71930834, Gradient norm: 0.01621215
INFO:root:[    2] Training loss: 0.71650242, Validation loss: 0.70856475, Gradient norm: 0.03081974
INFO:root:[    3] Training loss: 0.70452725, Validation loss: 0.69692687, Gradient norm: 0.05737288
INFO:root:[    4] Training loss: 0.69317024, Validation loss: 0.68219760, Gradient norm: 0.07201726
INFO:root:[    5] Training loss: 0.68161037, Validation loss: 0.67129002, Gradient norm: 0.06444892
INFO:root:[    6] Training loss: 0.67390166, Validation loss: 0.66348887, Gradient norm: 0.07376671
INFO:root:[    7] Training loss: 0.66830995, Validation loss: 0.65800315, Gradient norm: 0.08843863
INFO:root:[    8] Training loss: 0.66335447, Validation loss: 0.65447708, Gradient norm: 0.08266518
INFO:root:[    9] Training loss: 0.65942780, Validation loss: 0.65091006, Gradient norm: 0.06350495
INFO:root:[   10] Training loss: 0.65583014, Validation loss: 0.64683507, Gradient norm: 0.06335361
INFO:root:[   11] Training loss: 0.65374178, Validation loss: 0.64499970, Gradient norm: 0.09017830
INFO:root:[   12] Training loss: 0.65105545, Validation loss: 0.64170625, Gradient norm: 0.06953368
INFO:root:[   13] Training loss: 0.64906629, Validation loss: 0.64181006, Gradient norm: 0.08282514
INFO:root:[   14] Training loss: 0.64699482, Validation loss: 0.63757468, Gradient norm: 0.06697279
INFO:root:[   15] Training loss: 0.64537818, Validation loss: 0.63635495, Gradient norm: 0.07193251
INFO:root:[   16] Training loss: 0.64335613, Validation loss: 0.63425826, Gradient norm: 0.06070161
INFO:root:[   17] Training loss: 0.64257549, Validation loss: 0.63486378, Gradient norm: 0.08892933
INFO:root:[   18] Training loss: 0.64122004, Validation loss: 0.63344592, Gradient norm: 0.09300336
INFO:root:[   19] Training loss: 0.63944501, Validation loss: 0.63193641, Gradient norm: 0.06418651
INFO:root:[   20] Training loss: 0.63839060, Validation loss: 0.62989237, Gradient norm: 0.07124874
INFO:root:[   21] Training loss: 0.63768080, Validation loss: 0.62994787, Gradient norm: 0.08519332
INFO:root:[   22] Training loss: 0.63614631, Validation loss: 0.62931811, Gradient norm: 0.07480366
INFO:root:[   23] Training loss: 0.63518107, Validation loss: 0.62740276, Gradient norm: 0.06349273
INFO:root:[   24] Training loss: 0.63413851, Validation loss: 0.62968177, Gradient norm: 0.06507595
INFO:root:[   25] Training loss: 0.63340876, Validation loss: 0.62692513, Gradient norm: 0.06899279
INFO:root:[   26] Training loss: 0.63242768, Validation loss: 0.62437473, Gradient norm: 0.07335996
INFO:root:[   27] Training loss: 0.63180282, Validation loss: 0.62433886, Gradient norm: 0.07207051
INFO:root:[   28] Training loss: 0.63104953, Validation loss: 0.62374878, Gradient norm: 0.07260311
INFO:root:[   29] Training loss: 0.62969847, Validation loss: 0.62297158, Gradient norm: 0.05859366
INFO:root:[   30] Training loss: 0.62960721, Validation loss: 0.62328835, Gradient norm: 0.08202934
INFO:root:[   31] Training loss: 0.62880095, Validation loss: 0.62105235, Gradient norm: 0.07515082
INFO:root:[   32] Training loss: 0.62847824, Validation loss: 0.62308698, Gradient norm: 0.08112180
INFO:root:[   33] Training loss: 0.62756412, Validation loss: 0.62141977, Gradient norm: 0.07174209
INFO:root:[   34] Training loss: 0.62694562, Validation loss: 0.61966848, Gradient norm: 0.06265378
INFO:root:[   35] Training loss: 0.62580260, Validation loss: 0.62011036, Gradient norm: 0.06044365
INFO:root:[   36] Training loss: 0.62581342, Validation loss: 0.61916116, Gradient norm: 0.07854559
INFO:root:[   37] Training loss: 0.62473317, Validation loss: 0.62000272, Gradient norm: 0.07209916
INFO:root:[   38] Training loss: 0.62531320, Validation loss: 0.61728495, Gradient norm: 0.08614359
INFO:root:[   39] Training loss: 0.62402956, Validation loss: 0.61750778, Gradient norm: 0.06585605
INFO:root:[   40] Training loss: 0.62405156, Validation loss: 0.62003813, Gradient norm: 0.08333548
INFO:root:[   41] Training loss: 0.62348657, Validation loss: 0.61639855, Gradient norm: 0.07828484
INFO:root:[   42] Training loss: 0.62232850, Validation loss: 0.61723307, Gradient norm: 0.06679117
INFO:root:[   43] Training loss: 0.62265976, Validation loss: 0.61881141, Gradient norm: 0.08109847
INFO:root:[   44] Training loss: 0.62244103, Validation loss: 0.61517262, Gradient norm: 0.08842528
INFO:root:[   45] Training loss: 0.62081710, Validation loss: 0.61549407, Gradient norm: 0.06612517
INFO:root:[   46] Training loss: 0.62034688, Validation loss: 0.61499451, Gradient norm: 0.05802589
INFO:root:[   47] Training loss: 0.62015327, Validation loss: 0.61644745, Gradient norm: 0.07117646
INFO:root:[   48] Training loss: 0.62067666, Validation loss: 0.61576858, Gradient norm: 0.08925018
INFO:root:[   49] Training loss: 0.62044157, Validation loss: 0.61579155, Gradient norm: 0.08587670
INFO:root:[   50] Training loss: 0.61958730, Validation loss: 0.61404536, Gradient norm: 0.07895042
INFO:root:[   51] Training loss: 0.61843659, Validation loss: 0.61403025, Gradient norm: 0.06341620
INFO:root:[   52] Training loss: 0.61944594, Validation loss: 0.61538344, Gradient norm: 0.09884360
INFO:root:[   53] Training loss: 0.61858460, Validation loss: 0.61463174, Gradient norm: 0.08006208
INFO:root:[   54] Training loss: 0.61788412, Validation loss: 0.61448273, Gradient norm: 0.07971536
INFO:root:[   55] Training loss: 0.61652194, Validation loss: 0.61351137, Gradient norm: 0.06476771
INFO:root:[   56] Training loss: 0.61664997, Validation loss: 0.61220066, Gradient norm: 0.07011952
INFO:root:[   57] Training loss: 0.61795782, Validation loss: 0.61203301, Gradient norm: 0.09973466
INFO:root:[   58] Training loss: 0.61650734, Validation loss: 0.61294439, Gradient norm: 0.08090148
INFO:root:[   59] Training loss: 0.61656284, Validation loss: 0.61397415, Gradient norm: 0.09257082
