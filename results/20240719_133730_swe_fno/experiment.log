INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.61170973, Validation loss: 1.96073084, Gradient norm: 6.35245510
INFO:root:[    2] Training loss: 0.80398045, Validation loss: 0.13400634, Gradient norm: 6.43139960
INFO:root:[    3] Training loss: 0.10905426, Validation loss: 0.09579606, Gradient norm: 1.08727779
INFO:root:[    4] Training loss: 0.08991052, Validation loss: 0.09073512, Gradient norm: 1.50633782
INFO:root:[    5] Training loss: 0.08624829, Validation loss: 0.08299672, Gradient norm: 1.92572748
INFO:root:[    6] Training loss: 0.08278517, Validation loss: 0.08190240, Gradient norm: 1.62809257
INFO:root:[    7] Training loss: 0.07882252, Validation loss: 0.07662490, Gradient norm: 1.05772518
INFO:root:[    8] Training loss: 0.07654099, Validation loss: 0.07361920, Gradient norm: 1.60039219
INFO:root:[    9] Training loss: 0.07271505, Validation loss: 0.06925948, Gradient norm: 1.47313504
INFO:root:[   10] Training loss: 0.06842325, Validation loss: 0.06587330, Gradient norm: 1.44839708
INFO:root:[   11] Training loss: 0.06429094, Validation loss: 0.06235125, Gradient norm: 1.90404658
INFO:root:[   12] Training loss: 0.06167348, Validation loss: 0.05918236, Gradient norm: 1.90332545
INFO:root:[   13] Training loss: 0.05774707, Validation loss: 0.05521644, Gradient norm: 1.27123075
INFO:root:[   14] Training loss: 0.05671586, Validation loss: 0.05766713, Gradient norm: 1.84454001
INFO:root:[   15] Training loss: 0.05418087, Validation loss: 0.05303170, Gradient norm: 1.47523895
INFO:root:[   16] Training loss: 0.05508374, Validation loss: 0.05288258, Gradient norm: 2.07945205
INFO:root:[   17] Training loss: 0.05274405, Validation loss: 0.05302768, Gradient norm: 1.91962470
INFO:root:[   18] Training loss: 0.05135781, Validation loss: 0.05148681, Gradient norm: 1.54493904
INFO:root:[   19] Training loss: 0.05010629, Validation loss: 0.04952190, Gradient norm: 1.16845261
INFO:root:[   20] Training loss: 0.04958139, Validation loss: 0.04890175, Gradient norm: 1.40068113
INFO:root:[   21] Training loss: 0.04832082, Validation loss: 0.04819650, Gradient norm: 1.26996573
INFO:root:[   22] Training loss: 0.04837574, Validation loss: 0.04951747, Gradient norm: 1.39149958
INFO:root:[   23] Training loss: 0.04913769, Validation loss: 0.04805038, Gradient norm: 2.09734241
INFO:root:[   24] Training loss: 0.04737631, Validation loss: 0.04743578, Gradient norm: 1.32625182
INFO:root:[   25] Training loss: 0.04831163, Validation loss: 0.04896164, Gradient norm: 2.33645941
INFO:root:[   26] Training loss: 0.04652559, Validation loss: 0.04449623, Gradient norm: 2.04525130
INFO:root:[   27] Training loss: 0.04552679, Validation loss: 0.04819973, Gradient norm: 1.61563959
INFO:root:[   28] Training loss: 0.04542911, Validation loss: 0.04457792, Gradient norm: 1.75210209
INFO:root:[   29] Training loss: 0.04495589, Validation loss: 0.04474524, Gradient norm: 1.45707931
INFO:root:[   30] Training loss: 0.04430025, Validation loss: 0.04634298, Gradient norm: 1.49331683
INFO:root:[   31] Training loss: 0.04513584, Validation loss: 0.04256293, Gradient norm: 2.27258435
INFO:root:[   32] Training loss: 0.04343238, Validation loss: 0.04263321, Gradient norm: 1.60394780
INFO:root:[   33] Training loss: 0.04290167, Validation loss: 0.04319905, Gradient norm: 1.35190702
INFO:root:[   34] Training loss: 0.04349125, Validation loss: 0.04148522, Gradient norm: 2.05699228
INFO:root:[   35] Training loss: 0.04153909, Validation loss: 0.04520080, Gradient norm: 1.56793701
INFO:root:[   36] Training loss: 0.04188912, Validation loss: 0.04205436, Gradient norm: 1.81633139
INFO:root:[   37] Training loss: 0.04227216, Validation loss: 0.04114817, Gradient norm: 2.24976516
INFO:root:[   38] Training loss: 0.04138238, Validation loss: 0.04046135, Gradient norm: 1.65301437
INFO:root:[   39] Training loss: 0.04061851, Validation loss: 0.03985867, Gradient norm: 1.41839552
INFO:root:[   40] Training loss: 0.03989692, Validation loss: 0.03950260, Gradient norm: 1.06833833
INFO:root:[   41] Training loss: 0.03954237, Validation loss: 0.04137027, Gradient norm: 1.24660113
INFO:root:[   42] Training loss: 0.03990812, Validation loss: 0.04025260, Gradient norm: 1.60135047
INFO:root:[   43] Training loss: 0.03997628, Validation loss: 0.03941561, Gradient norm: 1.74535933
INFO:root:[   44] Training loss: 0.04010520, Validation loss: 0.03802569, Gradient norm: 1.82149978
INFO:root:[   45] Training loss: 0.03865718, Validation loss: 0.03731323, Gradient norm: 1.37185726
INFO:root:[   46] Training loss: 0.03910738, Validation loss: 0.04176685, Gradient norm: 1.75039527
INFO:root:[   47] Training loss: 0.03840854, Validation loss: 0.03768922, Gradient norm: 1.70011880
INFO:root:[   48] Training loss: 0.03810881, Validation loss: 0.03829551, Gradient norm: 1.63194696
INFO:root:[   49] Training loss: 0.03739949, Validation loss: 0.03641637, Gradient norm: 1.45961909
INFO:root:[   50] Training loss: 0.03798205, Validation loss: 0.03721137, Gradient norm: 1.82888654
INFO:root:[   51] Training loss: 0.03709256, Validation loss: 0.04035427, Gradient norm: 1.38764939
INFO:root:[   52] Training loss: 0.03743744, Validation loss: 0.03585983, Gradient norm: 1.64885431
INFO:root:[   53] Training loss: 0.03728165, Validation loss: 0.03603259, Gradient norm: 1.70501822
INFO:root:[   54] Training loss: 0.03648504, Validation loss: 0.03740466, Gradient norm: 1.57942086
INFO:root:[   55] Training loss: 0.03647351, Validation loss: 0.03775376, Gradient norm: 1.33245636
INFO:root:[   56] Training loss: 0.03632714, Validation loss: 0.03662505, Gradient norm: 1.38304206
INFO:root:[   57] Training loss: 0.03560099, Validation loss: 0.03542562, Gradient norm: 1.36028117
INFO:root:[   58] Training loss: 0.03577040, Validation loss: 0.03591886, Gradient norm: 1.58629169
INFO:root:[   59] Training loss: 0.03622397, Validation loss: 0.03700094, Gradient norm: 1.88443357
INFO:root:[   60] Training loss: 0.03455645, Validation loss: 0.03335074, Gradient norm: 0.99452049
INFO:root:[   61] Training loss: 0.03454038, Validation loss: 0.03783219, Gradient norm: 1.07168375
INFO:root:[   62] Training loss: 0.03543581, Validation loss: 0.03585303, Gradient norm: 1.80527182
INFO:root:[   63] Training loss: 0.03552773, Validation loss: 0.03411687, Gradient norm: 1.93085912
INFO:root:[   64] Training loss: 0.03362278, Validation loss: 0.03366535, Gradient norm: 0.96563121
INFO:root:[   65] Training loss: 0.03400628, Validation loss: 0.03351327, Gradient norm: 1.47718951
INFO:root:[   66] Training loss: 0.03350562, Validation loss: 0.03280178, Gradient norm: 1.16030064
INFO:root:[   67] Training loss: 0.03342401, Validation loss: 0.03412322, Gradient norm: 1.18800619
INFO:root:[   68] Training loss: 0.03346959, Validation loss: 0.03296660, Gradient norm: 1.27514696
INFO:root:[   69] Training loss: 0.03345217, Validation loss: 0.03390123, Gradient norm: 1.40997546
INFO:root:[   70] Training loss: 0.03372513, Validation loss: 0.03272459, Gradient norm: 2.02178826
INFO:root:[   71] Training loss: 0.03294487, Validation loss: 0.03436091, Gradient norm: 1.38451697
INFO:root:[   72] Training loss: 0.03294130, Validation loss: 0.03155042, Gradient norm: 1.64118504
INFO:root:[   73] Training loss: 0.03259420, Validation loss: 0.03268163, Gradient norm: 1.61672050
INFO:root:[   74] Training loss: 0.03200693, Validation loss: 0.03123034, Gradient norm: 1.29708270
INFO:root:[   75] Training loss: 0.03131257, Validation loss: 0.03136608, Gradient norm: 0.85735915
INFO:root:[   76] Training loss: 0.03222779, Validation loss: 0.03139053, Gradient norm: 1.68064986
INFO:root:[   77] Training loss: 0.03158082, Validation loss: 0.03261868, Gradient norm: 1.39380681
INFO:root:[   78] Training loss: 0.03170526, Validation loss: 0.03250264, Gradient norm: 1.63265265
INFO:root:[   79] Training loss: 0.03118352, Validation loss: 0.03351185, Gradient norm: 1.36732311
INFO:root:[   80] Training loss: 0.03148297, Validation loss: 0.03115096, Gradient norm: 1.74356035
INFO:root:[   81] Training loss: 0.03095280, Validation loss: 0.03181722, Gradient norm: 1.42176178
INFO:root:[   82] Training loss: 0.03079899, Validation loss: 0.03421052, Gradient norm: 1.45752122
INFO:root:[   83] Training loss: 0.03035215, Validation loss: 0.02976508, Gradient norm: 1.48892453
INFO:root:[   84] Training loss: 0.02994409, Validation loss: 0.02954358, Gradient norm: 1.31796522
INFO:root:[   85] Training loss: 0.03047924, Validation loss: 0.03132595, Gradient norm: 1.56152418
INFO:root:[   86] Training loss: 0.02944848, Validation loss: 0.02958796, Gradient norm: 1.04534456
INFO:root:[   87] Training loss: 0.02922950, Validation loss: 0.02903708, Gradient norm: 1.06098615
INFO:root:[   88] Training loss: 0.02954111, Validation loss: 0.02903656, Gradient norm: 1.44073116
INFO:root:[   89] Training loss: 0.02958791, Validation loss: 0.02860781, Gradient norm: 1.40778929
INFO:root:[   90] Training loss: 0.02864049, Validation loss: 0.03012607, Gradient norm: 0.98466713
INFO:root:[   91] Training loss: 0.02947358, Validation loss: 0.02879342, Gradient norm: 1.46707559
INFO:root:[   92] Training loss: 0.02861875, Validation loss: 0.02944029, Gradient norm: 1.18616616
INFO:root:[   93] Training loss: 0.02866558, Validation loss: 0.03007387, Gradient norm: 1.38445543
INFO:root:[   94] Training loss: 0.02865532, Validation loss: 0.02770116, Gradient norm: 1.56220935
INFO:root:[   95] Training loss: 0.02813894, Validation loss: 0.02773501, Gradient norm: 1.07712283
INFO:root:[   96] Training loss: 0.02777224, Validation loss: 0.02831772, Gradient norm: 0.99858682
INFO:root:[   97] Training loss: 0.02819410, Validation loss: 0.02748888, Gradient norm: 1.37810023
INFO:root:[   98] Training loss: 0.02769998, Validation loss: 0.02704107, Gradient norm: 1.30807354
INFO:root:[   99] Training loss: 0.02726641, Validation loss: 0.02680654, Gradient norm: 0.79351944
INFO:root:[  100] Training loss: 0.02839091, Validation loss: 0.02760815, Gradient norm: 1.86699769
INFO:root:[  101] Training loss: 0.02703150, Validation loss: 0.02674957, Gradient norm: 1.16234031
INFO:root:[  102] Training loss: 0.02700749, Validation loss: 0.02780709, Gradient norm: 1.24871933
INFO:root:[  103] Training loss: 0.02694543, Validation loss: 0.02655058, Gradient norm: 1.24828615
INFO:root:[  104] Training loss: 0.02662131, Validation loss: 0.02678780, Gradient norm: 1.15574856
INFO:root:[  105] Training loss: 0.02634287, Validation loss: 0.02786708, Gradient norm: 1.08588397
INFO:root:[  106] Training loss: 0.02725803, Validation loss: 0.02581768, Gradient norm: 1.74813452
INFO:root:[  107] Training loss: 0.02677011, Validation loss: 0.02603829, Gradient norm: 1.65233028
INFO:root:[  108] Training loss: 0.02573255, Validation loss: 0.02585722, Gradient norm: 0.54345301
INFO:root:[  109] Training loss: 0.02577751, Validation loss: 0.02723712, Gradient norm: 0.97812950
INFO:root:[  110] Training loss: 0.02592385, Validation loss: 0.02613215, Gradient norm: 1.27901401
INFO:root:[  111] Training loss: 0.02609516, Validation loss: 0.02586191, Gradient norm: 1.59117026
INFO:root:[  112] Training loss: 0.02554427, Validation loss: 0.02483041, Gradient norm: 1.28903309
INFO:root:[  113] Training loss: 0.02492074, Validation loss: 0.02475690, Gradient norm: 0.57596574
INFO:root:[  114] Training loss: 0.02537689, Validation loss: 0.02459034, Gradient norm: 1.36921604
INFO:root:[  115] Training loss: 0.02551506, Validation loss: 0.02571505, Gradient norm: 1.61409588
INFO:root:[  116] Training loss: 0.02523497, Validation loss: 0.02455263, Gradient norm: 1.36139596
INFO:root:[  117] Training loss: 0.02441012, Validation loss: 0.02439072, Gradient norm: 0.75305690
INFO:root:[  118] Training loss: 0.02464920, Validation loss: 0.02515446, Gradient norm: 1.08944284
INFO:root:[  119] Training loss: 0.02437339, Validation loss: 0.02423334, Gradient norm: 0.96946484
INFO:root:[  120] Training loss: 0.02515476, Validation loss: 0.02474658, Gradient norm: 1.74245884
INFO:root:[  121] Training loss: 0.02459044, Validation loss: 0.02542499, Gradient norm: 1.40454307
INFO:root:[  122] Training loss: 0.02437568, Validation loss: 0.02469924, Gradient norm: 1.35109703
INFO:root:[  123] Training loss: 0.02426991, Validation loss: 0.02452512, Gradient norm: 1.31748749
INFO:root:[  124] Training loss: 0.02411855, Validation loss: 0.02394810, Gradient norm: 1.10206835
INFO:root:[  125] Training loss: 0.02394342, Validation loss: 0.02383616, Gradient norm: 1.32239550
INFO:root:[  126] Training loss: 0.02420740, Validation loss: 0.02441627, Gradient norm: 1.43872343
INFO:root:[  127] Training loss: 0.02393375, Validation loss: 0.02388031, Gradient norm: 1.33658301
INFO:root:[  128] Training loss: 0.02388543, Validation loss: 0.02362775, Gradient norm: 1.63450790
INFO:root:[  129] Training loss: 0.02399393, Validation loss: 0.02645150, Gradient norm: 1.59554183
INFO:root:[  130] Training loss: 0.02335144, Validation loss: 0.02276384, Gradient norm: 1.24942849
INFO:root:[  131] Training loss: 0.02319909, Validation loss: 0.02316910, Gradient norm: 1.18590457
INFO:root:[  132] Training loss: 0.02329314, Validation loss: 0.02366860, Gradient norm: 1.40067799
INFO:root:[  133] Training loss: 0.02336106, Validation loss: 0.02260856, Gradient norm: 1.44985477
INFO:root:[  134] Training loss: 0.02325088, Validation loss: 0.02465061, Gradient norm: 1.34927652
INFO:root:[  135] Training loss: 0.02297357, Validation loss: 0.02328416, Gradient norm: 1.32677974
INFO:root:[  136] Training loss: 0.02290457, Validation loss: 0.02329166, Gradient norm: 1.33975121
INFO:root:[  137] Training loss: 0.02251416, Validation loss: 0.02208508, Gradient norm: 0.85108300
INFO:root:[  138] Training loss: 0.02242994, Validation loss: 0.02231569, Gradient norm: 1.09750084
INFO:root:[  139] Training loss: 0.02265508, Validation loss: 0.02292756, Gradient norm: 1.53832605
INFO:root:[  140] Training loss: 0.02248809, Validation loss: 0.02230941, Gradient norm: 1.35605630
INFO:root:[  141] Training loss: 0.02278599, Validation loss: 0.02178751, Gradient norm: 1.57049642
INFO:root:[  142] Training loss: 0.02176489, Validation loss: 0.02205634, Gradient norm: 0.78005943
INFO:root:[  143] Training loss: 0.02158040, Validation loss: 0.02161635, Gradient norm: 0.60161502
INFO:root:[  144] Training loss: 0.02187263, Validation loss: 0.02432756, Gradient norm: 1.13746200
INFO:root:[  145] Training loss: 0.02209095, Validation loss: 0.02121152, Gradient norm: 1.36698070
INFO:root:[  146] Training loss: 0.02197880, Validation loss: 0.02159899, Gradient norm: 1.44818959
INFO:root:[  147] Training loss: 0.02183244, Validation loss: 0.02113160, Gradient norm: 1.30574299
INFO:root:[  148] Training loss: 0.02165017, Validation loss: 0.02096104, Gradient norm: 1.23884371
INFO:root:[  149] Training loss: 0.02164097, Validation loss: 0.02392303, Gradient norm: 1.21804814
INFO:root:[  150] Training loss: 0.02163688, Validation loss: 0.02167416, Gradient norm: 1.42655933
INFO:root:[  151] Training loss: 0.02149386, Validation loss: 0.02297939, Gradient norm: 1.35269664
INFO:root:[  152] Training loss: 0.02129236, Validation loss: 0.02189629, Gradient norm: 1.26654688
INFO:root:[  153] Training loss: 0.02163141, Validation loss: 0.02152980, Gradient norm: 1.61744738
INFO:root:[  154] Training loss: 0.02097784, Validation loss: 0.02033259, Gradient norm: 0.97511551
INFO:root:[  155] Training loss: 0.02104583, Validation loss: 0.02110167, Gradient norm: 1.17955711
INFO:root:[  156] Training loss: 0.02098389, Validation loss: 0.02050494, Gradient norm: 1.26926241
INFO:root:[  157] Training loss: 0.02097380, Validation loss: 0.02084943, Gradient norm: 1.39686198
INFO:root:[  158] Training loss: 0.02045174, Validation loss: 0.02003492, Gradient norm: 0.91639945
INFO:root:[  159] Training loss: 0.02067259, Validation loss: 0.02000970, Gradient norm: 1.16588747
INFO:root:[  160] Training loss: 0.02071233, Validation loss: 0.02075716, Gradient norm: 1.34213445
INFO:root:[  161] Training loss: 0.02062955, Validation loss: 0.02116065, Gradient norm: 1.34205672
INFO:root:[  162] Training loss: 0.02058445, Validation loss: 0.02062920, Gradient norm: 1.32035125
INFO:root:[  163] Training loss: 0.02044875, Validation loss: 0.02003984, Gradient norm: 1.10123888
INFO:root:[  164] Training loss: 0.02016614, Validation loss: 0.02198063, Gradient norm: 1.14301386
INFO:root:[  165] Training loss: 0.02027416, Validation loss: 0.02037627, Gradient norm: 1.32726846
INFO:root:[  166] Training loss: 0.02014071, Validation loss: 0.02002867, Gradient norm: 1.21509934
INFO:root:[  167] Training loss: 0.02000298, Validation loss: 0.01964274, Gradient norm: 1.13989420
INFO:root:[  168] Training loss: 0.02016766, Validation loss: 0.01944785, Gradient norm: 1.37138886
INFO:root:[  169] Training loss: 0.01981742, Validation loss: 0.02241318, Gradient norm: 1.12346717
INFO:root:[  170] Training loss: 0.01984065, Validation loss: 0.01934766, Gradient norm: 1.17689565
INFO:root:[  171] Training loss: 0.01979491, Validation loss: 0.01981217, Gradient norm: 1.19397220
INFO:root:[  172] Training loss: 0.01969803, Validation loss: 0.01941475, Gradient norm: 1.31481391
INFO:root:[  173] Training loss: 0.01926900, Validation loss: 0.01954538, Gradient norm: 0.91539081
INFO:root:[  174] Training loss: 0.01965639, Validation loss: 0.01957203, Gradient norm: 1.25253840
INFO:root:[  175] Training loss: 0.01995634, Validation loss: 0.02074213, Gradient norm: 1.59505820
INFO:root:[  176] Training loss: 0.01953508, Validation loss: 0.02018557, Gradient norm: 1.29300646
INFO:root:[  177] Training loss: 0.01926542, Validation loss: 0.01923795, Gradient norm: 1.19246935
INFO:root:[  178] Training loss: 0.01927835, Validation loss: 0.01880712, Gradient norm: 1.19978483
INFO:root:[  179] Training loss: 0.01916900, Validation loss: 0.01969901, Gradient norm: 0.93872941
INFO:root:[  180] Training loss: 0.01961055, Validation loss: 0.01881508, Gradient norm: 1.54709669
INFO:root:[  181] Training loss: 0.01946487, Validation loss: 0.02030742, Gradient norm: 1.56283210
INFO:root:[  182] Training loss: 0.01938889, Validation loss: 0.01888502, Gradient norm: 1.50626597
INFO:root:[  183] Training loss: 0.01901314, Validation loss: 0.01913117, Gradient norm: 1.09031858
INFO:root:[  184] Training loss: 0.01928098, Validation loss: 0.01845133, Gradient norm: 1.36031077
INFO:root:[  185] Training loss: 0.01884565, Validation loss: 0.01830344, Gradient norm: 1.18436549
INFO:root:[  186] Training loss: 0.01869981, Validation loss: 0.01864804, Gradient norm: 1.18228941
INFO:root:[  187] Training loss: 0.01857039, Validation loss: 0.01808558, Gradient norm: 0.98537939
INFO:root:[  188] Training loss: 0.01840579, Validation loss: 0.01875617, Gradient norm: 0.80165648
INFO:root:[  189] Training loss: 0.01854309, Validation loss: 0.01813082, Gradient norm: 1.15336690
INFO:root:[  190] Training loss: 0.01875437, Validation loss: 0.01817684, Gradient norm: 1.41505319
INFO:root:[  191] Training loss: 0.01844726, Validation loss: 0.01873986, Gradient norm: 1.09228706
INFO:root:[  192] Training loss: 0.01866746, Validation loss: 0.02039399, Gradient norm: 1.34461620
INFO:root:[  193] Training loss: 0.01851006, Validation loss: 0.01799624, Gradient norm: 1.29547589
INFO:root:[  194] Training loss: 0.01862846, Validation loss: 0.01823947, Gradient norm: 1.40795105
INFO:root:[  195] Training loss: 0.01838432, Validation loss: 0.01788059, Gradient norm: 1.33896221
INFO:root:[  196] Training loss: 0.01840137, Validation loss: 0.01775128, Gradient norm: 1.25049336
INFO:root:[  197] Training loss: 0.01816412, Validation loss: 0.01760000, Gradient norm: 1.14389429
INFO:root:[  198] Training loss: 0.01824013, Validation loss: 0.01777013, Gradient norm: 1.19920499
INFO:root:[  199] Training loss: 0.01817627, Validation loss: 0.01768196, Gradient norm: 1.40380952
INFO:root:[  200] Training loss: 0.01811348, Validation loss: 0.01740780, Gradient norm: 1.24083848
INFO:root:[  201] Training loss: 0.01787273, Validation loss: 0.01916689, Gradient norm: 0.98997684
INFO:root:[  202] Training loss: 0.01776047, Validation loss: 0.01763382, Gradient norm: 0.97899830
INFO:root:[  203] Training loss: 0.01791430, Validation loss: 0.01814822, Gradient norm: 1.19842187
INFO:root:[  204] Training loss: 0.01798584, Validation loss: 0.01790702, Gradient norm: 1.33738914
INFO:root:[  205] Training loss: 0.01789728, Validation loss: 0.01772644, Gradient norm: 1.31363144
INFO:root:[  206] Training loss: 0.01806129, Validation loss: 0.01721799, Gradient norm: 1.39568469
INFO:root:[  207] Training loss: 0.01770764, Validation loss: 0.01715715, Gradient norm: 1.18295955
INFO:root:[  208] Training loss: 0.01786652, Validation loss: 0.01685015, Gradient norm: 1.27094374
INFO:root:[  209] Training loss: 0.01775132, Validation loss: 0.01826181, Gradient norm: 1.31227388
INFO:root:[  210] Training loss: 0.01747843, Validation loss: 0.01698090, Gradient norm: 1.14205062
INFO:root:[  211] Training loss: 0.01775177, Validation loss: 0.01814700, Gradient norm: 1.40753218
INFO:root:[  212] Training loss: 0.01747195, Validation loss: 0.01679444, Gradient norm: 1.11225701
INFO:root:[  213] Training loss: 0.01739137, Validation loss: 0.01708090, Gradient norm: 1.12980225
INFO:root:[  214] Training loss: 0.01759180, Validation loss: 0.01699023, Gradient norm: 1.36451056
INFO:root:[  215] Training loss: 0.01722216, Validation loss: 0.01783099, Gradient norm: 1.02785500
INFO:root:[  216] Training loss: 0.01729228, Validation loss: 0.01836507, Gradient norm: 1.11769860
INFO:root:[  217] Training loss: 0.01733201, Validation loss: 0.01771573, Gradient norm: 1.28247315
INFO:root:[  218] Training loss: 0.01725711, Validation loss: 0.01776139, Gradient norm: 1.29529764
INFO:root:[  219] Training loss: 0.01715100, Validation loss: 0.01691265, Gradient norm: 1.19744629
INFO:root:[  220] Training loss: 0.01708784, Validation loss: 0.01727004, Gradient norm: 1.16414751
INFO:root:[  221] Training loss: 0.01698668, Validation loss: 0.01702569, Gradient norm: 1.05785653
INFO:root:EP 221: Early stopping
INFO:root:Training the model took 7398.085s.
INFO:root:Emptying the cuda cache took 0.081s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.36701
INFO:root:EnergyScoreTrain: 0.27013
INFO:root:CoverageTrain: 0.98123
INFO:root:IntervalWidthTrain: 0.0403
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.34476
INFO:root:EnergyScoreValidation: 0.25392
INFO:root:CoverageValidation: 0.98158
INFO:root:IntervalWidthValidation: 0.04057
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.32611
INFO:root:EnergyScoreTest: 0.23947
INFO:root:CoverageTest: 0.98108
INFO:root:IntervalWidthTest: 0.04009
INFO:root:###2 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 457179136
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.52195959, Validation loss: 1.68412332, Gradient norm: 5.50628012
INFO:root:[    2] Training loss: 0.50628799, Validation loss: 0.17011911, Gradient norm: 4.50682540
INFO:root:[    3] Training loss: 0.15189929, Validation loss: 0.14140327, Gradient norm: 1.37281278
INFO:root:[    4] Training loss: 0.13299892, Validation loss: 0.13746182, Gradient norm: 1.23940539
INFO:root:[    5] Training loss: 0.12739028, Validation loss: 0.13046120, Gradient norm: 1.24308513
INFO:root:[    6] Training loss: 0.12356537, Validation loss: 0.11844082, Gradient norm: 1.23946220
INFO:root:[    7] Training loss: 0.12197461, Validation loss: 0.11484663, Gradient norm: 1.25174367
INFO:root:[    8] Training loss: 0.11413097, Validation loss: 0.11663124, Gradient norm: 1.26870724
INFO:root:[    9] Training loss: 0.11141896, Validation loss: 0.11101971, Gradient norm: 1.48710425
INFO:root:[   10] Training loss: 0.10937889, Validation loss: 0.10842298, Gradient norm: 1.00851685
INFO:root:[   11] Training loss: 0.10606335, Validation loss: 0.10495288, Gradient norm: 1.16830202
INFO:root:[   12] Training loss: 0.10222450, Validation loss: 0.09882136, Gradient norm: 1.31269914
INFO:root:[   13] Training loss: 0.09989468, Validation loss: 0.09895966, Gradient norm: 1.42549438
INFO:root:[   14] Training loss: 0.09323352, Validation loss: 0.09100243, Gradient norm: 1.10623382
INFO:root:[   15] Training loss: 0.08829591, Validation loss: 0.09032605, Gradient norm: 1.27178111
INFO:root:[   16] Training loss: 0.08766245, Validation loss: 0.08393117, Gradient norm: 1.66560846
INFO:root:[   17] Training loss: 0.08183645, Validation loss: 0.08053192, Gradient norm: 1.02179351
INFO:root:[   18] Training loss: 0.07781201, Validation loss: 0.07616455, Gradient norm: 0.87406750
INFO:root:[   19] Training loss: 0.07718562, Validation loss: 0.07643142, Gradient norm: 1.17855641
INFO:root:[   20] Training loss: 0.07425433, Validation loss: 0.07411687, Gradient norm: 1.13506088
INFO:root:[   21] Training loss: 0.07250823, Validation loss: 0.07006685, Gradient norm: 1.50050539
INFO:root:[   22] Training loss: 0.06967109, Validation loss: 0.06937173, Gradient norm: 1.00974208
INFO:root:[   23] Training loss: 0.06906538, Validation loss: 0.06528318, Gradient norm: 1.51864605
INFO:root:[   24] Training loss: 0.06627054, Validation loss: 0.06343924, Gradient norm: 1.25299233
INFO:root:[   25] Training loss: 0.06410634, Validation loss: 0.06326602, Gradient norm: 0.95643624
INFO:root:[   26] Training loss: 0.06245633, Validation loss: 0.06071850, Gradient norm: 0.93922162
INFO:root:[   27] Training loss: 0.06031003, Validation loss: 0.06139405, Gradient norm: 0.94056700
INFO:root:[   28] Training loss: 0.06034583, Validation loss: 0.05820638, Gradient norm: 1.15586171
INFO:root:[   29] Training loss: 0.05842669, Validation loss: 0.05800954, Gradient norm: 0.95785616
INFO:root:[   30] Training loss: 0.05774917, Validation loss: 0.05741199, Gradient norm: 1.04240110
INFO:root:[   31] Training loss: 0.05619707, Validation loss: 0.05553413, Gradient norm: 0.92818049
INFO:root:[   32] Training loss: 0.05439942, Validation loss: 0.05456834, Gradient norm: 0.57986536
INFO:root:[   33] Training loss: 0.05438068, Validation loss: 0.05591092, Gradient norm: 1.02330319
INFO:root:[   34] Training loss: 0.05346015, Validation loss: 0.05396879, Gradient norm: 0.99306540
INFO:root:[   35] Training loss: 0.05265357, Validation loss: 0.05218725, Gradient norm: 0.97565028
INFO:root:[   36] Training loss: 0.05122849, Validation loss: 0.05122016, Gradient norm: 0.61350596
INFO:root:[   37] Training loss: 0.05133112, Validation loss: 0.05082586, Gradient norm: 1.01475738
INFO:root:[   38] Training loss: 0.04992447, Validation loss: 0.04958223, Gradient norm: 0.79696867
INFO:root:[   39] Training loss: 0.04913878, Validation loss: 0.04902736, Gradient norm: 0.58110318
INFO:root:[   40] Training loss: 0.04877259, Validation loss: 0.04830444, Gradient norm: 0.72600897
INFO:root:[   41] Training loss: 0.04795046, Validation loss: 0.04844200, Gradient norm: 0.52335871
INFO:root:[   42] Training loss: 0.04741216, Validation loss: 0.04686007, Gradient norm: 0.70873271
INFO:root:[   43] Training loss: 0.04681807, Validation loss: 0.04631551, Gradient norm: 0.45367035
INFO:root:[   44] Training loss: 0.04681769, Validation loss: 0.04594995, Gradient norm: 0.95013922
INFO:root:[   45] Training loss: 0.04667538, Validation loss: 0.04550455, Gradient norm: 1.08106869
INFO:root:[   46] Training loss: 0.04540551, Validation loss: 0.04656335, Gradient norm: 0.70811667
INFO:root:[   47] Training loss: 0.04486053, Validation loss: 0.04477179, Gradient norm: 0.60932915
INFO:root:[   48] Training loss: 0.04483605, Validation loss: 0.04667077, Gradient norm: 0.80138179
INFO:root:[   49] Training loss: 0.04444063, Validation loss: 0.04465615, Gradient norm: 0.70727578
INFO:root:[   50] Training loss: 0.04416848, Validation loss: 0.04356550, Gradient norm: 0.86607619
INFO:root:[   51] Training loss: 0.04398921, Validation loss: 0.04290360, Gradient norm: 0.89310485
INFO:root:[   52] Training loss: 0.04317707, Validation loss: 0.04326798, Gradient norm: 0.58168366
INFO:root:[   53] Training loss: 0.04289323, Validation loss: 0.04307109, Gradient norm: 0.58923853
INFO:root:[   54] Training loss: 0.04294473, Validation loss: 0.04369044, Gradient norm: 0.87357299
INFO:root:[   55] Training loss: 0.04287095, Validation loss: 0.04205040, Gradient norm: 1.01678687
INFO:root:[   56] Training loss: 0.04228094, Validation loss: 0.04168060, Gradient norm: 0.83555963
INFO:root:[   57] Training loss: 0.04187234, Validation loss: 0.04328547, Gradient norm: 0.73551204
INFO:root:[   58] Training loss: 0.04159565, Validation loss: 0.04178545, Gradient norm: 0.73890524
INFO:root:[   59] Training loss: 0.04114940, Validation loss: 0.04153395, Gradient norm: 0.55665155
INFO:root:[   60] Training loss: 0.04078805, Validation loss: 0.04077379, Gradient norm: 0.40613400
INFO:root:[   61] Training loss: 0.04138776, Validation loss: 0.04324781, Gradient norm: 0.91225350
INFO:root:[   62] Training loss: 0.04079297, Validation loss: 0.04045654, Gradient norm: 0.82799075
INFO:root:[   63] Training loss: 0.04036847, Validation loss: 0.04022555, Gradient norm: 0.78098812
INFO:root:[   64] Training loss: 0.03982509, Validation loss: 0.03964051, Gradient norm: 0.30744190
INFO:root:[   65] Training loss: 0.03960868, Validation loss: 0.03970043, Gradient norm: 0.37976068
INFO:root:[   66] Training loss: 0.03974934, Validation loss: 0.04029552, Gradient norm: 0.71187084
INFO:root:[   67] Training loss: 0.03940413, Validation loss: 0.03923582, Gradient norm: 0.68669311
INFO:root:[   68] Training loss: 0.03929912, Validation loss: 0.03938513, Gradient norm: 0.69893927
INFO:root:[   69] Training loss: 0.03924120, Validation loss: 0.03934831, Gradient norm: 0.75208390
INFO:root:[   70] Training loss: 0.03900113, Validation loss: 0.03839022, Gradient norm: 0.77992686
INFO:root:[   71] Training loss: 0.03906345, Validation loss: 0.03866777, Gradient norm: 0.86831052
INFO:root:[   72] Training loss: 0.03841161, Validation loss: 0.03893932, Gradient norm: 0.63941912
INFO:root:[   73] Training loss: 0.03812453, Validation loss: 0.03876016, Gradient norm: 0.59018233
INFO:root:[   74] Training loss: 0.03828695, Validation loss: 0.03805794, Gradient norm: 0.85576422
INFO:root:[   75] Training loss: 0.03814947, Validation loss: 0.03720375, Gradient norm: 0.82622101
INFO:root:[   76] Training loss: 0.03796060, Validation loss: 0.03764967, Gradient norm: 0.85990418
INFO:root:[   77] Training loss: 0.03748020, Validation loss: 0.03707543, Gradient norm: 0.65849317
INFO:root:[   78] Training loss: 0.03691842, Validation loss: 0.03714765, Gradient norm: 0.37700811
INFO:root:[   79] Training loss: 0.03696395, Validation loss: 0.03717483, Gradient norm: 0.46425803
INFO:root:[   80] Training loss: 0.03699565, Validation loss: 0.03664945, Gradient norm: 0.70908109
INFO:root:[   81] Training loss: 0.03664210, Validation loss: 0.03756330, Gradient norm: 0.61521017
INFO:root:[   82] Training loss: 0.03671141, Validation loss: 0.03649576, Gradient norm: 0.77919543
INFO:root:[   83] Training loss: 0.03660745, Validation loss: 0.03674778, Gradient norm: 0.83727315
INFO:root:[   84] Training loss: 0.03671947, Validation loss: 0.03598611, Gradient norm: 1.00985247
INFO:root:[   85] Training loss: 0.03625204, Validation loss: 0.03577348, Gradient norm: 0.90141485
INFO:root:[   86] Training loss: 0.03561194, Validation loss: 0.03584830, Gradient norm: 0.45181826
INFO:root:[   87] Training loss: 0.03596203, Validation loss: 0.03515094, Gradient norm: 0.86563417
INFO:root:[   88] Training loss: 0.03586329, Validation loss: 0.03514178, Gradient norm: 0.88705753
INFO:root:[   89] Training loss: 0.03542955, Validation loss: 0.03544022, Gradient norm: 0.71577584
INFO:root:[   90] Training loss: 0.03521670, Validation loss: 0.03484096, Gradient norm: 0.70201181
INFO:root:[   91] Training loss: 0.03481009, Validation loss: 0.03482557, Gradient norm: 0.50262076
INFO:root:[   92] Training loss: 0.03483465, Validation loss: 0.03525005, Gradient norm: 0.53856544
INFO:root:[   93] Training loss: 0.03490425, Validation loss: 0.03470092, Gradient norm: 0.86942261
INFO:root:[   94] Training loss: 0.03469355, Validation loss: 0.03428081, Gradient norm: 0.89412582
INFO:root:[   95] Training loss: 0.03432333, Validation loss: 0.03444194, Gradient norm: 0.73414722
INFO:root:[   96] Training loss: 0.03422037, Validation loss: 0.03413700, Gradient norm: 0.64597761
INFO:root:[   97] Training loss: 0.03409053, Validation loss: 0.03404153, Gradient norm: 0.73042438
INFO:root:[   98] Training loss: 0.03404125, Validation loss: 0.03393591, Gradient norm: 0.84791727
INFO:root:[   99] Training loss: 0.03380821, Validation loss: 0.03454966, Gradient norm: 0.75482374
INFO:root:[  100] Training loss: 0.03348221, Validation loss: 0.03377569, Gradient norm: 0.66923198
INFO:root:[  101] Training loss: 0.03333164, Validation loss: 0.03307246, Gradient norm: 0.65469353
INFO:root:[  102] Training loss: 0.03348281, Validation loss: 0.03451999, Gradient norm: 0.83326679
INFO:root:[  103] Training loss: 0.03331924, Validation loss: 0.03282103, Gradient norm: 0.90757058
INFO:root:[  104] Training loss: 0.03306140, Validation loss: 0.03308930, Gradient norm: 0.82621556
INFO:root:[  105] Training loss: 0.03291610, Validation loss: 0.03267324, Gradient norm: 0.84210306
INFO:root:[  106] Training loss: 0.03279811, Validation loss: 0.03362809, Gradient norm: 0.85955202
INFO:root:[  107] Training loss: 0.03243079, Validation loss: 0.03221358, Gradient norm: 0.70491817
INFO:root:[  108] Training loss: 0.03241044, Validation loss: 0.03247717, Gradient norm: 0.80988345
INFO:root:[  109] Training loss: 0.03227159, Validation loss: 0.03285223, Gradient norm: 0.76330932
INFO:root:[  110] Training loss: 0.03217341, Validation loss: 0.03208957, Gradient norm: 0.79452972
INFO:root:[  111] Training loss: 0.03200787, Validation loss: 0.03238403, Gradient norm: 0.81512514
INFO:root:[  112] Training loss: 0.03205752, Validation loss: 0.03172077, Gradient norm: 0.95826507
INFO:root:[  113] Training loss: 0.03162893, Validation loss: 0.03169002, Gradient norm: 0.74701744
INFO:root:[  114] Training loss: 0.03170480, Validation loss: 0.03152642, Gradient norm: 0.94182809
INFO:root:[  115] Training loss: 0.03154213, Validation loss: 0.03127508, Gradient norm: 0.87840848
INFO:root:[  116] Training loss: 0.03087075, Validation loss: 0.03085451, Gradient norm: 0.38447633
INFO:root:[  117] Training loss: 0.03133947, Validation loss: 0.03054186, Gradient norm: 0.92184982
INFO:root:[  118] Training loss: 0.03100817, Validation loss: 0.03041065, Gradient norm: 0.85280140
INFO:root:[  119] Training loss: 0.03077190, Validation loss: 0.03096555, Gradient norm: 0.73425380
INFO:root:[  120] Training loss: 0.03062017, Validation loss: 0.03074342, Gradient norm: 0.70391697
INFO:root:[  121] Training loss: 0.03054094, Validation loss: 0.03100111, Gradient norm: 0.76614088
INFO:root:[  122] Training loss: 0.03058747, Validation loss: 0.03130375, Gradient norm: 0.91910816
INFO:root:[  123] Training loss: 0.03055809, Validation loss: 0.03069003, Gradient norm: 1.03203273
INFO:root:[  124] Training loss: 0.03013263, Validation loss: 0.03013467, Gradient norm: 0.76307031
INFO:root:[  125] Training loss: 0.03008892, Validation loss: 0.03008462, Gradient norm: 0.86091281
INFO:root:[  126] Training loss: 0.02998003, Validation loss: 0.02962508, Gradient norm: 0.83918478
INFO:root:[  127] Training loss: 0.02959003, Validation loss: 0.02975084, Gradient norm: 0.62586239
INFO:root:[  128] Training loss: 0.02977036, Validation loss: 0.02996229, Gradient norm: 0.91984330
INFO:root:[  129] Training loss: 0.02974674, Validation loss: 0.03031850, Gradient norm: 0.99935944
INFO:root:[  130] Training loss: 0.02956714, Validation loss: 0.02985249, Gradient norm: 0.99003067
INFO:root:[  131] Training loss: 0.02920068, Validation loss: 0.02900600, Gradient norm: 0.74722603
INFO:root:[  132] Training loss: 0.02922891, Validation loss: 0.02952297, Gradient norm: 0.85495666
INFO:root:[  133] Training loss: 0.02928902, Validation loss: 0.02880935, Gradient norm: 1.01642375
INFO:root:[  134] Training loss: 0.02883949, Validation loss: 0.02927966, Gradient norm: 0.76574883
INFO:root:[  135] Training loss: 0.02879792, Validation loss: 0.02898983, Gradient norm: 0.81671351
INFO:root:[  136] Training loss: 0.02843038, Validation loss: 0.02849576, Gradient norm: 0.52943535
INFO:root:[  137] Training loss: 0.02849243, Validation loss: 0.02824355, Gradient norm: 0.73719231
INFO:root:[  138] Training loss: 0.02877847, Validation loss: 0.02835284, Gradient norm: 1.11762229
INFO:root:[  139] Training loss: 0.02852157, Validation loss: 0.02800014, Gradient norm: 1.00718029
INFO:root:[  140] Training loss: 0.02836521, Validation loss: 0.02895769, Gradient norm: 0.95801545
INFO:root:[  141] Training loss: 0.02844935, Validation loss: 0.02852911, Gradient norm: 1.05726125
INFO:root:[  142] Training loss: 0.02811433, Validation loss: 0.02876307, Gradient norm: 0.93234662
INFO:root:[  143] Training loss: 0.02795462, Validation loss: 0.02757759, Gradient norm: 0.84668717
INFO:root:[  144] Training loss: 0.02805310, Validation loss: 0.02768744, Gradient norm: 1.03884814
INFO:root:[  145] Training loss: 0.02770109, Validation loss: 0.02786233, Gradient norm: 0.86963381
INFO:root:[  146] Training loss: 0.02783263, Validation loss: 0.02745290, Gradient norm: 1.11737693
INFO:root:[  147] Training loss: 0.02795489, Validation loss: 0.02791360, Gradient norm: 1.17221154
INFO:root:[  148] Training loss: 0.02762553, Validation loss: 0.02757629, Gradient norm: 1.01296116
INFO:root:[  149] Training loss: 0.02733509, Validation loss: 0.02700203, Gradient norm: 0.85962730
INFO:root:[  150] Training loss: 0.02720947, Validation loss: 0.02762203, Gradient norm: 0.84524010
INFO:root:[  151] Training loss: 0.02735629, Validation loss: 0.02758993, Gradient norm: 1.08693362
INFO:root:[  152] Training loss: 0.02726442, Validation loss: 0.02719561, Gradient norm: 1.06981410
INFO:root:[  153] Training loss: 0.02690123, Validation loss: 0.02687397, Gradient norm: 0.69609537
INFO:root:[  154] Training loss: 0.02684201, Validation loss: 0.02734982, Gradient norm: 0.84324603
INFO:root:[  155] Training loss: 0.02694992, Validation loss: 0.02649556, Gradient norm: 0.98704062
INFO:root:[  156] Training loss: 0.02677001, Validation loss: 0.02773914, Gradient norm: 0.88960383
INFO:root:[  157] Training loss: 0.02665164, Validation loss: 0.02631297, Gradient norm: 0.93304664
INFO:root:[  158] Training loss: 0.02657458, Validation loss: 0.02713032, Gradient norm: 0.96808767
INFO:root:[  159] Training loss: 0.02652422, Validation loss: 0.02623110, Gradient norm: 0.90819273
INFO:root:[  160] Training loss: 0.02645593, Validation loss: 0.02676945, Gradient norm: 0.93355633
INFO:root:[  161] Training loss: 0.02648861, Validation loss: 0.02651291, Gradient norm: 1.08534167
INFO:root:[  162] Training loss: 0.02621831, Validation loss: 0.02664135, Gradient norm: 0.87148881
INFO:root:[  163] Training loss: 0.02632065, Validation loss: 0.02651454, Gradient norm: 1.10214989
INFO:root:[  164] Training loss: 0.02618661, Validation loss: 0.02569462, Gradient norm: 1.02229679
INFO:root:[  165] Training loss: 0.02599064, Validation loss: 0.02613407, Gradient norm: 0.91051488
INFO:root:[  166] Training loss: 0.02591369, Validation loss: 0.02568469, Gradient norm: 0.96013480
INFO:root:[  167] Training loss: 0.02570304, Validation loss: 0.02566727, Gradient norm: 0.77485595
INFO:root:[  168] Training loss: 0.02585938, Validation loss: 0.02590236, Gradient norm: 1.05602642
INFO:root:[  169] Training loss: 0.02576730, Validation loss: 0.02568102, Gradient norm: 0.98679933
INFO:root:[  170] Training loss: 0.02577728, Validation loss: 0.02511869, Gradient norm: 1.11526511
INFO:root:[  171] Training loss: 0.02555983, Validation loss: 0.02554643, Gradient norm: 0.96006222
INFO:root:[  172] Training loss: 0.02546221, Validation loss: 0.02547099, Gradient norm: 0.92863594
INFO:root:[  173] Training loss: 0.02558864, Validation loss: 0.02499446, Gradient norm: 1.11199606
INFO:root:[  174] Training loss: 0.02561849, Validation loss: 0.02498922, Gradient norm: 1.21018381
INFO:root:[  175] Training loss: 0.02535101, Validation loss: 0.02566176, Gradient norm: 1.04172713
INFO:root:[  176] Training loss: 0.02533502, Validation loss: 0.02528663, Gradient norm: 1.10483446
INFO:root:[  177] Training loss: 0.02525074, Validation loss: 0.02534491, Gradient norm: 1.00967960
INFO:root:[  178] Training loss: 0.02504775, Validation loss: 0.02481189, Gradient norm: 0.94651877
INFO:root:[  179] Training loss: 0.02497550, Validation loss: 0.02504490, Gradient norm: 0.95080562
INFO:root:[  180] Training loss: 0.02504913, Validation loss: 0.02471206, Gradient norm: 1.04194238
INFO:root:[  181] Training loss: 0.02488744, Validation loss: 0.02498327, Gradient norm: 1.01866397
INFO:root:[  182] Training loss: 0.02481872, Validation loss: 0.02445066, Gradient norm: 1.07722740
INFO:root:[  183] Training loss: 0.02485856, Validation loss: 0.02497566, Gradient norm: 1.11341780
INFO:root:[  184] Training loss: 0.02475107, Validation loss: 0.02501223, Gradient norm: 1.09297311
INFO:root:[  185] Training loss: 0.02477395, Validation loss: 0.02435541, Gradient norm: 1.19787344
INFO:root:[  186] Training loss: 0.02474397, Validation loss: 0.02482980, Gradient norm: 1.14660383
INFO:root:[  187] Training loss: 0.02469799, Validation loss: 0.02464099, Gradient norm: 1.25123830
INFO:root:[  188] Training loss: 0.02436338, Validation loss: 0.02393455, Gradient norm: 0.92357041
INFO:root:[  189] Training loss: 0.02431879, Validation loss: 0.02440539, Gradient norm: 0.89140285
INFO:root:[  190] Training loss: 0.02436990, Validation loss: 0.02439897, Gradient norm: 1.05042594
INFO:root:[  191] Training loss: 0.02422558, Validation loss: 0.02394400, Gradient norm: 0.99914037
INFO:root:[  192] Training loss: 0.02412586, Validation loss: 0.02397736, Gradient norm: 0.92736923
INFO:root:[  193] Training loss: 0.02402518, Validation loss: 0.02413477, Gradient norm: 0.83808292
INFO:root:[  194] Training loss: 0.02416145, Validation loss: 0.02497597, Gradient norm: 1.11831408
INFO:root:[  195] Training loss: 0.02413672, Validation loss: 0.02403277, Gradient norm: 1.12416217
INFO:root:[  196] Training loss: 0.02411865, Validation loss: 0.02449387, Gradient norm: 1.16029508
INFO:root:[  197] Training loss: 0.02408658, Validation loss: 0.02370950, Gradient norm: 1.17649655
INFO:root:[  198] Training loss: 0.02414347, Validation loss: 0.02364036, Gradient norm: 1.30170570
INFO:root:[  199] Training loss: 0.02406951, Validation loss: 0.02478933, Gradient norm: 1.27221263
INFO:root:[  200] Training loss: 0.02392948, Validation loss: 0.02376425, Gradient norm: 1.19089836
INFO:root:[  201] Training loss: 0.02393482, Validation loss: 0.02389132, Gradient norm: 1.25411856
INFO:root:[  202] Training loss: 0.02386275, Validation loss: 0.02377005, Gradient norm: 1.26330712
INFO:root:[  203] Training loss: 0.02357008, Validation loss: 0.02454011, Gradient norm: 0.98346404
INFO:root:[  204] Training loss: 0.02360502, Validation loss: 0.02339629, Gradient norm: 1.09645630
INFO:root:[  205] Training loss: 0.02353420, Validation loss: 0.02368939, Gradient norm: 1.06720725
INFO:root:[  206] Training loss: 0.02338769, Validation loss: 0.02418334, Gradient norm: 0.97522887
INFO:root:[  207] Training loss: 0.02340990, Validation loss: 0.02357243, Gradient norm: 1.01588962
INFO:root:[  208] Training loss: 0.02327444, Validation loss: 0.02380226, Gradient norm: 0.89147682
INFO:root:[  209] Training loss: 0.02340443, Validation loss: 0.02314344, Gradient norm: 1.19236282
INFO:root:[  210] Training loss: 0.02323937, Validation loss: 0.02354726, Gradient norm: 0.91177973
INFO:root:[  211] Training loss: 0.02331347, Validation loss: 0.02316025, Gradient norm: 1.20475342
INFO:root:[  212] Training loss: 0.02353409, Validation loss: 0.02333165, Gradient norm: 1.40201287
INFO:root:[  213] Training loss: 0.02327395, Validation loss: 0.02284637, Gradient norm: 1.20968888
INFO:root:[  214] Training loss: 0.02304682, Validation loss: 0.02342622, Gradient norm: 1.05617075
INFO:root:[  215] Training loss: 0.02328411, Validation loss: 0.02299035, Gradient norm: 1.35069991
INFO:root:[  216] Training loss: 0.02291578, Validation loss: 0.02273915, Gradient norm: 0.92418911
INFO:root:[  217] Training loss: 0.02294281, Validation loss: 0.02297720, Gradient norm: 1.00497223
INFO:root:[  218] Training loss: 0.02300405, Validation loss: 0.02281606, Gradient norm: 1.19441381
INFO:root:[  219] Training loss: 0.02285838, Validation loss: 0.02291555, Gradient norm: 1.03029013
INFO:root:[  220] Training loss: 0.02281445, Validation loss: 0.02356154, Gradient norm: 1.08395538
INFO:root:[  221] Training loss: 0.02287113, Validation loss: 0.02323668, Gradient norm: 1.14001589
INFO:root:[  222] Training loss: 0.02270921, Validation loss: 0.02289968, Gradient norm: 1.04768352
INFO:root:[  223] Training loss: 0.02288237, Validation loss: 0.02247965, Gradient norm: 1.28878758
INFO:root:[  224] Training loss: 0.02295362, Validation loss: 0.02294492, Gradient norm: 1.38503299
INFO:root:[  225] Training loss: 0.02260970, Validation loss: 0.02295956, Gradient norm: 1.07138427
INFO:root:[  226] Training loss: 0.02272667, Validation loss: 0.02262985, Gradient norm: 1.26018420
INFO:root:[  227] Training loss: 0.02256252, Validation loss: 0.02222807, Gradient norm: 1.13716056
INFO:root:[  228] Training loss: 0.02263524, Validation loss: 0.02265035, Gradient norm: 1.24004897
INFO:root:[  229] Training loss: 0.02258604, Validation loss: 0.02295114, Gradient norm: 1.23025784
INFO:root:[  230] Training loss: 0.02271243, Validation loss: 0.02213238, Gradient norm: 1.45236263
INFO:root:[  231] Training loss: 0.02232998, Validation loss: 0.02233819, Gradient norm: 1.05003881
INFO:root:[  232] Training loss: 0.02252665, Validation loss: 0.02256584, Gradient norm: 1.31955359
INFO:root:[  233] Training loss: 0.02246630, Validation loss: 0.02250694, Gradient norm: 1.29892554
INFO:root:[  234] Training loss: 0.02235268, Validation loss: 0.02199939, Gradient norm: 1.24438125
INFO:root:[  235] Training loss: 0.02219787, Validation loss: 0.02259743, Gradient norm: 1.03368988
INFO:root:[  236] Training loss: 0.02226143, Validation loss: 0.02189201, Gradient norm: 1.18530296
INFO:root:[  237] Training loss: 0.02204190, Validation loss: 0.02263932, Gradient norm: 0.89444646
INFO:root:[  238] Training loss: 0.02228972, Validation loss: 0.02213142, Gradient norm: 1.29846825
INFO:root:[  239] Training loss: 0.02208229, Validation loss: 0.02251104, Gradient norm: 1.11944154
INFO:root:[  240] Training loss: 0.02215203, Validation loss: 0.02198713, Gradient norm: 1.26803434
INFO:root:[  241] Training loss: 0.02206343, Validation loss: 0.02182994, Gradient norm: 1.25394673
INFO:root:[  242] Training loss: 0.02225912, Validation loss: 0.02229047, Gradient norm: 1.42867021
INFO:root:[  243] Training loss: 0.02197874, Validation loss: 0.02228288, Gradient norm: 1.11345546
INFO:root:[  244] Training loss: 0.02200050, Validation loss: 0.02202216, Gradient norm: 1.20160400
INFO:root:[  245] Training loss: 0.02190832, Validation loss: 0.02186415, Gradient norm: 1.20590826
INFO:root:[  246] Training loss: 0.02201506, Validation loss: 0.02144651, Gradient norm: 1.30496052
INFO:root:[  247] Training loss: 0.02173081, Validation loss: 0.02194442, Gradient norm: 0.98641621
INFO:root:[  248] Training loss: 0.02177765, Validation loss: 0.02217019, Gradient norm: 1.19026821
INFO:root:[  249] Training loss: 0.02184037, Validation loss: 0.02211860, Gradient norm: 1.22478749
INFO:root:[  250] Training loss: 0.02187284, Validation loss: 0.02242234, Gradient norm: 1.32684183
INFO:root:[  251] Training loss: 0.02189877, Validation loss: 0.02139089, Gradient norm: 1.37716381
INFO:root:[  252] Training loss: 0.02176814, Validation loss: 0.02173027, Gradient norm: 1.34457315
INFO:root:[  253] Training loss: 0.02164197, Validation loss: 0.02137712, Gradient norm: 1.15377481
INFO:root:[  254] Training loss: 0.02150312, Validation loss: 0.02155966, Gradient norm: 1.05390805
INFO:root:[  255] Training loss: 0.02161951, Validation loss: 0.02185666, Gradient norm: 1.20145705
INFO:root:[  256] Training loss: 0.02156385, Validation loss: 0.02182567, Gradient norm: 1.12025131
INFO:root:[  257] Training loss: 0.02153538, Validation loss: 0.02194957, Gradient norm: 1.18181619
INFO:root:[  258] Training loss: 0.02166682, Validation loss: 0.02209721, Gradient norm: 1.41233698
INFO:root:[  259] Training loss: 0.02128632, Validation loss: 0.02124175, Gradient norm: 0.84234891
INFO:root:[  260] Training loss: 0.02147929, Validation loss: 0.02196203, Gradient norm: 1.30034062
INFO:root:[  261] Training loss: 0.02145771, Validation loss: 0.02149211, Gradient norm: 1.30380682
INFO:root:[  262] Training loss: 0.02123566, Validation loss: 0.02166440, Gradient norm: 0.97695879
INFO:root:[  263] Training loss: 0.02140283, Validation loss: 0.02166473, Gradient norm: 1.24581695
INFO:root:[  264] Training loss: 0.02140141, Validation loss: 0.02136321, Gradient norm: 1.28729241
INFO:root:[  265] Training loss: 0.02129636, Validation loss: 0.02083298, Gradient norm: 1.23351354
INFO:root:[  266] Training loss: 0.02123890, Validation loss: 0.02147282, Gradient norm: 1.13887581
