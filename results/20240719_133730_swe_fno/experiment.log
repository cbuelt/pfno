INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file swe/fno.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'SWE', 'max_training_set_size': 1000, 'downscaling_factor': 1, 'temporal_downscaling': 4, 'init_steps': 10, 't_start': 0, 'pred_horizon': 10}
INFO:root:###1 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 23068672
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.61170973, Validation loss: 1.96073084, Gradient norm: 6.35245510
INFO:root:[    2] Training loss: 0.80398045, Validation loss: 0.13400634, Gradient norm: 6.43139960
INFO:root:[    3] Training loss: 0.10905426, Validation loss: 0.09579606, Gradient norm: 1.08727779
INFO:root:[    4] Training loss: 0.08991052, Validation loss: 0.09073512, Gradient norm: 1.50633782
INFO:root:[    5] Training loss: 0.08624829, Validation loss: 0.08299672, Gradient norm: 1.92572748
INFO:root:[    6] Training loss: 0.08278517, Validation loss: 0.08190240, Gradient norm: 1.62809257
INFO:root:[    7] Training loss: 0.07882252, Validation loss: 0.07662490, Gradient norm: 1.05772518
INFO:root:[    8] Training loss: 0.07654099, Validation loss: 0.07361920, Gradient norm: 1.60039219
INFO:root:[    9] Training loss: 0.07271505, Validation loss: 0.06925948, Gradient norm: 1.47313504
INFO:root:[   10] Training loss: 0.06842325, Validation loss: 0.06587330, Gradient norm: 1.44839708
INFO:root:[   11] Training loss: 0.06429094, Validation loss: 0.06235125, Gradient norm: 1.90404658
INFO:root:[   12] Training loss: 0.06167348, Validation loss: 0.05918236, Gradient norm: 1.90332545
INFO:root:[   13] Training loss: 0.05774707, Validation loss: 0.05521644, Gradient norm: 1.27123075
INFO:root:[   14] Training loss: 0.05671586, Validation loss: 0.05766713, Gradient norm: 1.84454001
INFO:root:[   15] Training loss: 0.05418087, Validation loss: 0.05303170, Gradient norm: 1.47523895
INFO:root:[   16] Training loss: 0.05508374, Validation loss: 0.05288258, Gradient norm: 2.07945205
INFO:root:[   17] Training loss: 0.05274405, Validation loss: 0.05302768, Gradient norm: 1.91962470
INFO:root:[   18] Training loss: 0.05135781, Validation loss: 0.05148681, Gradient norm: 1.54493904
INFO:root:[   19] Training loss: 0.05010629, Validation loss: 0.04952190, Gradient norm: 1.16845261
INFO:root:[   20] Training loss: 0.04958139, Validation loss: 0.04890175, Gradient norm: 1.40068113
INFO:root:[   21] Training loss: 0.04832082, Validation loss: 0.04819650, Gradient norm: 1.26996573
INFO:root:[   22] Training loss: 0.04837574, Validation loss: 0.04951747, Gradient norm: 1.39149958
INFO:root:[   23] Training loss: 0.04913769, Validation loss: 0.04805038, Gradient norm: 2.09734241
INFO:root:[   24] Training loss: 0.04737631, Validation loss: 0.04743578, Gradient norm: 1.32625182
INFO:root:[   25] Training loss: 0.04831163, Validation loss: 0.04896164, Gradient norm: 2.33645941
INFO:root:[   26] Training loss: 0.04652559, Validation loss: 0.04449623, Gradient norm: 2.04525130
INFO:root:[   27] Training loss: 0.04552679, Validation loss: 0.04819973, Gradient norm: 1.61563959
INFO:root:[   28] Training loss: 0.04542911, Validation loss: 0.04457792, Gradient norm: 1.75210209
INFO:root:[   29] Training loss: 0.04495589, Validation loss: 0.04474524, Gradient norm: 1.45707931
INFO:root:[   30] Training loss: 0.04430025, Validation loss: 0.04634298, Gradient norm: 1.49331683
INFO:root:[   31] Training loss: 0.04513584, Validation loss: 0.04256293, Gradient norm: 2.27258435
INFO:root:[   32] Training loss: 0.04343238, Validation loss: 0.04263321, Gradient norm: 1.60394780
INFO:root:[   33] Training loss: 0.04290167, Validation loss: 0.04319905, Gradient norm: 1.35190702
INFO:root:[   34] Training loss: 0.04349125, Validation loss: 0.04148522, Gradient norm: 2.05699228
INFO:root:[   35] Training loss: 0.04153909, Validation loss: 0.04520080, Gradient norm: 1.56793701
INFO:root:[   36] Training loss: 0.04188912, Validation loss: 0.04205436, Gradient norm: 1.81633139
INFO:root:[   37] Training loss: 0.04227216, Validation loss: 0.04114817, Gradient norm: 2.24976516
INFO:root:[   38] Training loss: 0.04138238, Validation loss: 0.04046135, Gradient norm: 1.65301437
INFO:root:[   39] Training loss: 0.04061851, Validation loss: 0.03985867, Gradient norm: 1.41839552
INFO:root:[   40] Training loss: 0.03989692, Validation loss: 0.03950260, Gradient norm: 1.06833833
INFO:root:[   41] Training loss: 0.03954237, Validation loss: 0.04137027, Gradient norm: 1.24660113
INFO:root:[   42] Training loss: 0.03990812, Validation loss: 0.04025260, Gradient norm: 1.60135047
INFO:root:[   43] Training loss: 0.03997628, Validation loss: 0.03941561, Gradient norm: 1.74535933
INFO:root:[   44] Training loss: 0.04010520, Validation loss: 0.03802569, Gradient norm: 1.82149978
INFO:root:[   45] Training loss: 0.03865718, Validation loss: 0.03731323, Gradient norm: 1.37185726
INFO:root:[   46] Training loss: 0.03910738, Validation loss: 0.04176685, Gradient norm: 1.75039527
INFO:root:[   47] Training loss: 0.03840854, Validation loss: 0.03768922, Gradient norm: 1.70011880
INFO:root:[   48] Training loss: 0.03810881, Validation loss: 0.03829551, Gradient norm: 1.63194696
INFO:root:[   49] Training loss: 0.03739949, Validation loss: 0.03641637, Gradient norm: 1.45961909
INFO:root:[   50] Training loss: 0.03798205, Validation loss: 0.03721137, Gradient norm: 1.82888654
INFO:root:[   51] Training loss: 0.03709256, Validation loss: 0.04035427, Gradient norm: 1.38764939
INFO:root:[   52] Training loss: 0.03743744, Validation loss: 0.03585983, Gradient norm: 1.64885431
INFO:root:[   53] Training loss: 0.03728165, Validation loss: 0.03603259, Gradient norm: 1.70501822
INFO:root:[   54] Training loss: 0.03648504, Validation loss: 0.03740466, Gradient norm: 1.57942086
INFO:root:[   55] Training loss: 0.03647351, Validation loss: 0.03775376, Gradient norm: 1.33245636
INFO:root:[   56] Training loss: 0.03632714, Validation loss: 0.03662505, Gradient norm: 1.38304206
INFO:root:[   57] Training loss: 0.03560099, Validation loss: 0.03542562, Gradient norm: 1.36028117
INFO:root:[   58] Training loss: 0.03577040, Validation loss: 0.03591886, Gradient norm: 1.58629169
INFO:root:[   59] Training loss: 0.03622397, Validation loss: 0.03700094, Gradient norm: 1.88443357
INFO:root:[   60] Training loss: 0.03455645, Validation loss: 0.03335074, Gradient norm: 0.99452049
INFO:root:[   61] Training loss: 0.03454038, Validation loss: 0.03783219, Gradient norm: 1.07168375
INFO:root:[   62] Training loss: 0.03543581, Validation loss: 0.03585303, Gradient norm: 1.80527182
INFO:root:[   63] Training loss: 0.03552773, Validation loss: 0.03411687, Gradient norm: 1.93085912
INFO:root:[   64] Training loss: 0.03362278, Validation loss: 0.03366535, Gradient norm: 0.96563121
INFO:root:[   65] Training loss: 0.03400628, Validation loss: 0.03351327, Gradient norm: 1.47718951
INFO:root:[   66] Training loss: 0.03350562, Validation loss: 0.03280178, Gradient norm: 1.16030064
INFO:root:[   67] Training loss: 0.03342401, Validation loss: 0.03412322, Gradient norm: 1.18800619
INFO:root:[   68] Training loss: 0.03346959, Validation loss: 0.03296660, Gradient norm: 1.27514696
INFO:root:[   69] Training loss: 0.03345217, Validation loss: 0.03390123, Gradient norm: 1.40997546
INFO:root:[   70] Training loss: 0.03372513, Validation loss: 0.03272459, Gradient norm: 2.02178826
INFO:root:[   71] Training loss: 0.03294487, Validation loss: 0.03436091, Gradient norm: 1.38451697
INFO:root:[   72] Training loss: 0.03294130, Validation loss: 0.03155042, Gradient norm: 1.64118504
INFO:root:[   73] Training loss: 0.03259420, Validation loss: 0.03268163, Gradient norm: 1.61672050
INFO:root:[   74] Training loss: 0.03200693, Validation loss: 0.03123034, Gradient norm: 1.29708270
INFO:root:[   75] Training loss: 0.03131257, Validation loss: 0.03136608, Gradient norm: 0.85735915
INFO:root:[   76] Training loss: 0.03222779, Validation loss: 0.03139053, Gradient norm: 1.68064986
INFO:root:[   77] Training loss: 0.03158082, Validation loss: 0.03261868, Gradient norm: 1.39380681
INFO:root:[   78] Training loss: 0.03170526, Validation loss: 0.03250264, Gradient norm: 1.63265265
INFO:root:[   79] Training loss: 0.03118352, Validation loss: 0.03351185, Gradient norm: 1.36732311
INFO:root:[   80] Training loss: 0.03148297, Validation loss: 0.03115096, Gradient norm: 1.74356035
INFO:root:[   81] Training loss: 0.03095280, Validation loss: 0.03181722, Gradient norm: 1.42176178
INFO:root:[   82] Training loss: 0.03079899, Validation loss: 0.03421052, Gradient norm: 1.45752122
INFO:root:[   83] Training loss: 0.03035215, Validation loss: 0.02976508, Gradient norm: 1.48892453
INFO:root:[   84] Training loss: 0.02994409, Validation loss: 0.02954358, Gradient norm: 1.31796522
INFO:root:[   85] Training loss: 0.03047924, Validation loss: 0.03132595, Gradient norm: 1.56152418
INFO:root:[   86] Training loss: 0.02944848, Validation loss: 0.02958796, Gradient norm: 1.04534456
INFO:root:[   87] Training loss: 0.02922950, Validation loss: 0.02903708, Gradient norm: 1.06098615
INFO:root:[   88] Training loss: 0.02954111, Validation loss: 0.02903656, Gradient norm: 1.44073116
INFO:root:[   89] Training loss: 0.02958791, Validation loss: 0.02860781, Gradient norm: 1.40778929
INFO:root:[   90] Training loss: 0.02864049, Validation loss: 0.03012607, Gradient norm: 0.98466713
INFO:root:[   91] Training loss: 0.02947358, Validation loss: 0.02879342, Gradient norm: 1.46707559
INFO:root:[   92] Training loss: 0.02861875, Validation loss: 0.02944029, Gradient norm: 1.18616616
INFO:root:[   93] Training loss: 0.02866558, Validation loss: 0.03007387, Gradient norm: 1.38445543
INFO:root:[   94] Training loss: 0.02865532, Validation loss: 0.02770116, Gradient norm: 1.56220935
INFO:root:[   95] Training loss: 0.02813894, Validation loss: 0.02773501, Gradient norm: 1.07712283
INFO:root:[   96] Training loss: 0.02777224, Validation loss: 0.02831772, Gradient norm: 0.99858682
INFO:root:[   97] Training loss: 0.02819410, Validation loss: 0.02748888, Gradient norm: 1.37810023
INFO:root:[   98] Training loss: 0.02769998, Validation loss: 0.02704107, Gradient norm: 1.30807354
INFO:root:[   99] Training loss: 0.02726641, Validation loss: 0.02680654, Gradient norm: 0.79351944
INFO:root:[  100] Training loss: 0.02839091, Validation loss: 0.02760815, Gradient norm: 1.86699769
INFO:root:[  101] Training loss: 0.02703150, Validation loss: 0.02674957, Gradient norm: 1.16234031
INFO:root:[  102] Training loss: 0.02700749, Validation loss: 0.02780709, Gradient norm: 1.24871933
INFO:root:[  103] Training loss: 0.02694543, Validation loss: 0.02655058, Gradient norm: 1.24828615
INFO:root:[  104] Training loss: 0.02662131, Validation loss: 0.02678780, Gradient norm: 1.15574856
INFO:root:[  105] Training loss: 0.02634287, Validation loss: 0.02786708, Gradient norm: 1.08588397
INFO:root:[  106] Training loss: 0.02725803, Validation loss: 0.02581768, Gradient norm: 1.74813452
INFO:root:[  107] Training loss: 0.02677011, Validation loss: 0.02603829, Gradient norm: 1.65233028
INFO:root:[  108] Training loss: 0.02573255, Validation loss: 0.02585722, Gradient norm: 0.54345301
INFO:root:[  109] Training loss: 0.02577751, Validation loss: 0.02723712, Gradient norm: 0.97812950
INFO:root:[  110] Training loss: 0.02592385, Validation loss: 0.02613215, Gradient norm: 1.27901401
INFO:root:[  111] Training loss: 0.02609516, Validation loss: 0.02586191, Gradient norm: 1.59117026
INFO:root:[  112] Training loss: 0.02554427, Validation loss: 0.02483041, Gradient norm: 1.28903309
INFO:root:[  113] Training loss: 0.02492074, Validation loss: 0.02475690, Gradient norm: 0.57596574
INFO:root:[  114] Training loss: 0.02537689, Validation loss: 0.02459034, Gradient norm: 1.36921604
INFO:root:[  115] Training loss: 0.02551506, Validation loss: 0.02571505, Gradient norm: 1.61409588
INFO:root:[  116] Training loss: 0.02523497, Validation loss: 0.02455263, Gradient norm: 1.36139596
INFO:root:[  117] Training loss: 0.02441012, Validation loss: 0.02439072, Gradient norm: 0.75305690
INFO:root:[  118] Training loss: 0.02464920, Validation loss: 0.02515446, Gradient norm: 1.08944284
INFO:root:[  119] Training loss: 0.02437339, Validation loss: 0.02423334, Gradient norm: 0.96946484
INFO:root:[  120] Training loss: 0.02515476, Validation loss: 0.02474658, Gradient norm: 1.74245884
INFO:root:[  121] Training loss: 0.02459044, Validation loss: 0.02542499, Gradient norm: 1.40454307
INFO:root:[  122] Training loss: 0.02437568, Validation loss: 0.02469924, Gradient norm: 1.35109703
INFO:root:[  123] Training loss: 0.02426991, Validation loss: 0.02452512, Gradient norm: 1.31748749
INFO:root:[  124] Training loss: 0.02411855, Validation loss: 0.02394810, Gradient norm: 1.10206835
INFO:root:[  125] Training loss: 0.02394342, Validation loss: 0.02383616, Gradient norm: 1.32239550
INFO:root:[  126] Training loss: 0.02420740, Validation loss: 0.02441627, Gradient norm: 1.43872343
INFO:root:[  127] Training loss: 0.02393375, Validation loss: 0.02388031, Gradient norm: 1.33658301
INFO:root:[  128] Training loss: 0.02388543, Validation loss: 0.02362775, Gradient norm: 1.63450790
INFO:root:[  129] Training loss: 0.02399393, Validation loss: 0.02645150, Gradient norm: 1.59554183
INFO:root:[  130] Training loss: 0.02335144, Validation loss: 0.02276384, Gradient norm: 1.24942849
INFO:root:[  131] Training loss: 0.02319909, Validation loss: 0.02316910, Gradient norm: 1.18590457
INFO:root:[  132] Training loss: 0.02329314, Validation loss: 0.02366860, Gradient norm: 1.40067799
INFO:root:[  133] Training loss: 0.02336106, Validation loss: 0.02260856, Gradient norm: 1.44985477
INFO:root:[  134] Training loss: 0.02325088, Validation loss: 0.02465061, Gradient norm: 1.34927652
INFO:root:[  135] Training loss: 0.02297357, Validation loss: 0.02328416, Gradient norm: 1.32677974
INFO:root:[  136] Training loss: 0.02290457, Validation loss: 0.02329166, Gradient norm: 1.33975121
INFO:root:[  137] Training loss: 0.02251416, Validation loss: 0.02208508, Gradient norm: 0.85108300
INFO:root:[  138] Training loss: 0.02242994, Validation loss: 0.02231569, Gradient norm: 1.09750084
INFO:root:[  139] Training loss: 0.02265508, Validation loss: 0.02292756, Gradient norm: 1.53832605
INFO:root:[  140] Training loss: 0.02248809, Validation loss: 0.02230941, Gradient norm: 1.35605630
INFO:root:[  141] Training loss: 0.02278599, Validation loss: 0.02178751, Gradient norm: 1.57049642
INFO:root:[  142] Training loss: 0.02176489, Validation loss: 0.02205634, Gradient norm: 0.78005943
INFO:root:[  143] Training loss: 0.02158040, Validation loss: 0.02161635, Gradient norm: 0.60161502
INFO:root:[  144] Training loss: 0.02187263, Validation loss: 0.02432756, Gradient norm: 1.13746200
INFO:root:[  145] Training loss: 0.02209095, Validation loss: 0.02121152, Gradient norm: 1.36698070
INFO:root:[  146] Training loss: 0.02197880, Validation loss: 0.02159899, Gradient norm: 1.44818959
INFO:root:[  147] Training loss: 0.02183244, Validation loss: 0.02113160, Gradient norm: 1.30574299
INFO:root:[  148] Training loss: 0.02165017, Validation loss: 0.02096104, Gradient norm: 1.23884371
INFO:root:[  149] Training loss: 0.02164097, Validation loss: 0.02392303, Gradient norm: 1.21804814
INFO:root:[  150] Training loss: 0.02163688, Validation loss: 0.02167416, Gradient norm: 1.42655933
INFO:root:[  151] Training loss: 0.02149386, Validation loss: 0.02297939, Gradient norm: 1.35269664
INFO:root:[  152] Training loss: 0.02129236, Validation loss: 0.02189629, Gradient norm: 1.26654688
INFO:root:[  153] Training loss: 0.02163141, Validation loss: 0.02152980, Gradient norm: 1.61744738
INFO:root:[  154] Training loss: 0.02097784, Validation loss: 0.02033259, Gradient norm: 0.97511551
INFO:root:[  155] Training loss: 0.02104583, Validation loss: 0.02110167, Gradient norm: 1.17955711
INFO:root:[  156] Training loss: 0.02098389, Validation loss: 0.02050494, Gradient norm: 1.26926241
INFO:root:[  157] Training loss: 0.02097380, Validation loss: 0.02084943, Gradient norm: 1.39686198
INFO:root:[  158] Training loss: 0.02045174, Validation loss: 0.02003492, Gradient norm: 0.91639945
INFO:root:[  159] Training loss: 0.02067259, Validation loss: 0.02000970, Gradient norm: 1.16588747
INFO:root:[  160] Training loss: 0.02071233, Validation loss: 0.02075716, Gradient norm: 1.34213445
INFO:root:[  161] Training loss: 0.02062955, Validation loss: 0.02116065, Gradient norm: 1.34205672
INFO:root:[  162] Training loss: 0.02058445, Validation loss: 0.02062920, Gradient norm: 1.32035125
INFO:root:[  163] Training loss: 0.02044875, Validation loss: 0.02003984, Gradient norm: 1.10123888
INFO:root:[  164] Training loss: 0.02016614, Validation loss: 0.02198063, Gradient norm: 1.14301386
INFO:root:[  165] Training loss: 0.02027416, Validation loss: 0.02037627, Gradient norm: 1.32726846
INFO:root:[  166] Training loss: 0.02014071, Validation loss: 0.02002867, Gradient norm: 1.21509934
INFO:root:[  167] Training loss: 0.02000298, Validation loss: 0.01964274, Gradient norm: 1.13989420
INFO:root:[  168] Training loss: 0.02016766, Validation loss: 0.01944785, Gradient norm: 1.37138886
INFO:root:[  169] Training loss: 0.01981742, Validation loss: 0.02241318, Gradient norm: 1.12346717
INFO:root:[  170] Training loss: 0.01984065, Validation loss: 0.01934766, Gradient norm: 1.17689565
INFO:root:[  171] Training loss: 0.01979491, Validation loss: 0.01981217, Gradient norm: 1.19397220
INFO:root:[  172] Training loss: 0.01969803, Validation loss: 0.01941475, Gradient norm: 1.31481391
INFO:root:[  173] Training loss: 0.01926900, Validation loss: 0.01954538, Gradient norm: 0.91539081
INFO:root:[  174] Training loss: 0.01965639, Validation loss: 0.01957203, Gradient norm: 1.25253840
INFO:root:[  175] Training loss: 0.01995634, Validation loss: 0.02074213, Gradient norm: 1.59505820
INFO:root:[  176] Training loss: 0.01953508, Validation loss: 0.02018557, Gradient norm: 1.29300646
INFO:root:[  177] Training loss: 0.01926542, Validation loss: 0.01923795, Gradient norm: 1.19246935
INFO:root:[  178] Training loss: 0.01927835, Validation loss: 0.01880712, Gradient norm: 1.19978483
INFO:root:[  179] Training loss: 0.01916900, Validation loss: 0.01969901, Gradient norm: 0.93872941
INFO:root:[  180] Training loss: 0.01961055, Validation loss: 0.01881508, Gradient norm: 1.54709669
INFO:root:[  181] Training loss: 0.01946487, Validation loss: 0.02030742, Gradient norm: 1.56283210
INFO:root:[  182] Training loss: 0.01938889, Validation loss: 0.01888502, Gradient norm: 1.50626597
INFO:root:[  183] Training loss: 0.01901314, Validation loss: 0.01913117, Gradient norm: 1.09031858
INFO:root:[  184] Training loss: 0.01928098, Validation loss: 0.01845133, Gradient norm: 1.36031077
INFO:root:[  185] Training loss: 0.01884565, Validation loss: 0.01830344, Gradient norm: 1.18436549
INFO:root:[  186] Training loss: 0.01869981, Validation loss: 0.01864804, Gradient norm: 1.18228941
INFO:root:[  187] Training loss: 0.01857039, Validation loss: 0.01808558, Gradient norm: 0.98537939
INFO:root:[  188] Training loss: 0.01840579, Validation loss: 0.01875617, Gradient norm: 0.80165648
INFO:root:[  189] Training loss: 0.01854309, Validation loss: 0.01813082, Gradient norm: 1.15336690
INFO:root:[  190] Training loss: 0.01875437, Validation loss: 0.01817684, Gradient norm: 1.41505319
INFO:root:[  191] Training loss: 0.01844726, Validation loss: 0.01873986, Gradient norm: 1.09228706
INFO:root:[  192] Training loss: 0.01866746, Validation loss: 0.02039399, Gradient norm: 1.34461620
INFO:root:[  193] Training loss: 0.01851006, Validation loss: 0.01799624, Gradient norm: 1.29547589
INFO:root:[  194] Training loss: 0.01862846, Validation loss: 0.01823947, Gradient norm: 1.40795105
INFO:root:[  195] Training loss: 0.01838432, Validation loss: 0.01788059, Gradient norm: 1.33896221
INFO:root:[  196] Training loss: 0.01840137, Validation loss: 0.01775128, Gradient norm: 1.25049336
INFO:root:[  197] Training loss: 0.01816412, Validation loss: 0.01760000, Gradient norm: 1.14389429
INFO:root:[  198] Training loss: 0.01824013, Validation loss: 0.01777013, Gradient norm: 1.19920499
INFO:root:[  199] Training loss: 0.01817627, Validation loss: 0.01768196, Gradient norm: 1.40380952
INFO:root:[  200] Training loss: 0.01811348, Validation loss: 0.01740780, Gradient norm: 1.24083848
INFO:root:[  201] Training loss: 0.01787273, Validation loss: 0.01916689, Gradient norm: 0.98997684
INFO:root:[  202] Training loss: 0.01776047, Validation loss: 0.01763382, Gradient norm: 0.97899830
INFO:root:[  203] Training loss: 0.01791430, Validation loss: 0.01814822, Gradient norm: 1.19842187
INFO:root:[  204] Training loss: 0.01798584, Validation loss: 0.01790702, Gradient norm: 1.33738914
INFO:root:[  205] Training loss: 0.01789728, Validation loss: 0.01772644, Gradient norm: 1.31363144
INFO:root:[  206] Training loss: 0.01806129, Validation loss: 0.01721799, Gradient norm: 1.39568469
INFO:root:[  207] Training loss: 0.01770764, Validation loss: 0.01715715, Gradient norm: 1.18295955
INFO:root:[  208] Training loss: 0.01786652, Validation loss: 0.01685015, Gradient norm: 1.27094374
INFO:root:[  209] Training loss: 0.01775132, Validation loss: 0.01826181, Gradient norm: 1.31227388
INFO:root:[  210] Training loss: 0.01747843, Validation loss: 0.01698090, Gradient norm: 1.14205062
INFO:root:[  211] Training loss: 0.01775177, Validation loss: 0.01814700, Gradient norm: 1.40753218
INFO:root:[  212] Training loss: 0.01747195, Validation loss: 0.01679444, Gradient norm: 1.11225701
INFO:root:[  213] Training loss: 0.01739137, Validation loss: 0.01708090, Gradient norm: 1.12980225
INFO:root:[  214] Training loss: 0.01759180, Validation loss: 0.01699023, Gradient norm: 1.36451056
INFO:root:[  215] Training loss: 0.01722216, Validation loss: 0.01783099, Gradient norm: 1.02785500
INFO:root:[  216] Training loss: 0.01729228, Validation loss: 0.01836507, Gradient norm: 1.11769860
INFO:root:[  217] Training loss: 0.01733201, Validation loss: 0.01771573, Gradient norm: 1.28247315
INFO:root:[  218] Training loss: 0.01725711, Validation loss: 0.01776139, Gradient norm: 1.29529764
INFO:root:[  219] Training loss: 0.01715100, Validation loss: 0.01691265, Gradient norm: 1.19744629
INFO:root:[  220] Training loss: 0.01708784, Validation loss: 0.01727004, Gradient norm: 1.16414751
INFO:root:[  221] Training loss: 0.01698668, Validation loss: 0.01702569, Gradient norm: 1.05785653
INFO:root:EP 221: Early stopping
INFO:root:Training the model took 7398.085s.
INFO:root:Emptying the cuda cache took 0.081s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.36701
INFO:root:EnergyScoreTrain: 0.27013
INFO:root:CoverageTrain: 0.98123
INFO:root:IntervalWidthTrain: 0.0403
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.34476
INFO:root:EnergyScoreValidation: 0.25392
INFO:root:CoverageValidation: 0.98158
INFO:root:IntervalWidthValidation: 0.04057
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.32611
INFO:root:EnergyScoreTest: 0.23947
INFO:root:CoverageTest: 0.98108
INFO:root:IntervalWidthTest: 0.04009
INFO:root:###2 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 457179136
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.52195959, Validation loss: 1.68412332, Gradient norm: 5.50628012
INFO:root:[    2] Training loss: 0.50628799, Validation loss: 0.17011911, Gradient norm: 4.50682540
INFO:root:[    3] Training loss: 0.15189929, Validation loss: 0.14140327, Gradient norm: 1.37281278
INFO:root:[    4] Training loss: 0.13299892, Validation loss: 0.13746182, Gradient norm: 1.23940539
INFO:root:[    5] Training loss: 0.12739028, Validation loss: 0.13046120, Gradient norm: 1.24308513
INFO:root:[    6] Training loss: 0.12356537, Validation loss: 0.11844082, Gradient norm: 1.23946220
INFO:root:[    7] Training loss: 0.12197461, Validation loss: 0.11484663, Gradient norm: 1.25174367
INFO:root:[    8] Training loss: 0.11413097, Validation loss: 0.11663124, Gradient norm: 1.26870724
INFO:root:[    9] Training loss: 0.11141896, Validation loss: 0.11101971, Gradient norm: 1.48710425
INFO:root:[   10] Training loss: 0.10937889, Validation loss: 0.10842298, Gradient norm: 1.00851685
INFO:root:[   11] Training loss: 0.10606335, Validation loss: 0.10495288, Gradient norm: 1.16830202
INFO:root:[   12] Training loss: 0.10222450, Validation loss: 0.09882136, Gradient norm: 1.31269914
INFO:root:[   13] Training loss: 0.09989468, Validation loss: 0.09895966, Gradient norm: 1.42549438
INFO:root:[   14] Training loss: 0.09323352, Validation loss: 0.09100243, Gradient norm: 1.10623382
INFO:root:[   15] Training loss: 0.08829591, Validation loss: 0.09032605, Gradient norm: 1.27178111
INFO:root:[   16] Training loss: 0.08766245, Validation loss: 0.08393117, Gradient norm: 1.66560846
INFO:root:[   17] Training loss: 0.08183645, Validation loss: 0.08053192, Gradient norm: 1.02179351
INFO:root:[   18] Training loss: 0.07781201, Validation loss: 0.07616455, Gradient norm: 0.87406750
INFO:root:[   19] Training loss: 0.07718562, Validation loss: 0.07643142, Gradient norm: 1.17855641
INFO:root:[   20] Training loss: 0.07425433, Validation loss: 0.07411687, Gradient norm: 1.13506088
INFO:root:[   21] Training loss: 0.07250823, Validation loss: 0.07006685, Gradient norm: 1.50050539
INFO:root:[   22] Training loss: 0.06967109, Validation loss: 0.06937173, Gradient norm: 1.00974208
INFO:root:[   23] Training loss: 0.06906538, Validation loss: 0.06528318, Gradient norm: 1.51864605
INFO:root:[   24] Training loss: 0.06627054, Validation loss: 0.06343924, Gradient norm: 1.25299233
INFO:root:[   25] Training loss: 0.06410634, Validation loss: 0.06326602, Gradient norm: 0.95643624
INFO:root:[   26] Training loss: 0.06245633, Validation loss: 0.06071850, Gradient norm: 0.93922162
INFO:root:[   27] Training loss: 0.06031003, Validation loss: 0.06139405, Gradient norm: 0.94056700
INFO:root:[   28] Training loss: 0.06034583, Validation loss: 0.05820638, Gradient norm: 1.15586171
INFO:root:[   29] Training loss: 0.05842669, Validation loss: 0.05800954, Gradient norm: 0.95785616
INFO:root:[   30] Training loss: 0.05774917, Validation loss: 0.05741199, Gradient norm: 1.04240110
INFO:root:[   31] Training loss: 0.05619707, Validation loss: 0.05553413, Gradient norm: 0.92818049
INFO:root:[   32] Training loss: 0.05439942, Validation loss: 0.05456834, Gradient norm: 0.57986536
INFO:root:[   33] Training loss: 0.05438068, Validation loss: 0.05591092, Gradient norm: 1.02330319
INFO:root:[   34] Training loss: 0.05346015, Validation loss: 0.05396879, Gradient norm: 0.99306540
INFO:root:[   35] Training loss: 0.05265357, Validation loss: 0.05218725, Gradient norm: 0.97565028
INFO:root:[   36] Training loss: 0.05122849, Validation loss: 0.05122016, Gradient norm: 0.61350596
INFO:root:[   37] Training loss: 0.05133112, Validation loss: 0.05082586, Gradient norm: 1.01475738
INFO:root:[   38] Training loss: 0.04992447, Validation loss: 0.04958223, Gradient norm: 0.79696867
INFO:root:[   39] Training loss: 0.04913878, Validation loss: 0.04902736, Gradient norm: 0.58110318
INFO:root:[   40] Training loss: 0.04877259, Validation loss: 0.04830444, Gradient norm: 0.72600897
INFO:root:[   41] Training loss: 0.04795046, Validation loss: 0.04844200, Gradient norm: 0.52335871
INFO:root:[   42] Training loss: 0.04741216, Validation loss: 0.04686007, Gradient norm: 0.70873271
INFO:root:[   43] Training loss: 0.04681807, Validation loss: 0.04631551, Gradient norm: 0.45367035
INFO:root:[   44] Training loss: 0.04681769, Validation loss: 0.04594995, Gradient norm: 0.95013922
INFO:root:[   45] Training loss: 0.04667538, Validation loss: 0.04550455, Gradient norm: 1.08106869
INFO:root:[   46] Training loss: 0.04540551, Validation loss: 0.04656335, Gradient norm: 0.70811667
INFO:root:[   47] Training loss: 0.04486053, Validation loss: 0.04477179, Gradient norm: 0.60932915
INFO:root:[   48] Training loss: 0.04483605, Validation loss: 0.04667077, Gradient norm: 0.80138179
INFO:root:[   49] Training loss: 0.04444063, Validation loss: 0.04465615, Gradient norm: 0.70727578
INFO:root:[   50] Training loss: 0.04416848, Validation loss: 0.04356550, Gradient norm: 0.86607619
INFO:root:[   51] Training loss: 0.04398921, Validation loss: 0.04290360, Gradient norm: 0.89310485
INFO:root:[   52] Training loss: 0.04317707, Validation loss: 0.04326798, Gradient norm: 0.58168366
INFO:root:[   53] Training loss: 0.04289323, Validation loss: 0.04307109, Gradient norm: 0.58923853
INFO:root:[   54] Training loss: 0.04294473, Validation loss: 0.04369044, Gradient norm: 0.87357299
INFO:root:[   55] Training loss: 0.04287095, Validation loss: 0.04205040, Gradient norm: 1.01678687
INFO:root:[   56] Training loss: 0.04228094, Validation loss: 0.04168060, Gradient norm: 0.83555963
INFO:root:[   57] Training loss: 0.04187234, Validation loss: 0.04328547, Gradient norm: 0.73551204
INFO:root:[   58] Training loss: 0.04159565, Validation loss: 0.04178545, Gradient norm: 0.73890524
INFO:root:[   59] Training loss: 0.04114940, Validation loss: 0.04153395, Gradient norm: 0.55665155
INFO:root:[   60] Training loss: 0.04078805, Validation loss: 0.04077379, Gradient norm: 0.40613400
INFO:root:[   61] Training loss: 0.04138776, Validation loss: 0.04324781, Gradient norm: 0.91225350
INFO:root:[   62] Training loss: 0.04079297, Validation loss: 0.04045654, Gradient norm: 0.82799075
INFO:root:[   63] Training loss: 0.04036847, Validation loss: 0.04022555, Gradient norm: 0.78098812
INFO:root:[   64] Training loss: 0.03982509, Validation loss: 0.03964051, Gradient norm: 0.30744190
INFO:root:[   65] Training loss: 0.03960868, Validation loss: 0.03970043, Gradient norm: 0.37976068
INFO:root:[   66] Training loss: 0.03974934, Validation loss: 0.04029552, Gradient norm: 0.71187084
INFO:root:[   67] Training loss: 0.03940413, Validation loss: 0.03923582, Gradient norm: 0.68669311
INFO:root:[   68] Training loss: 0.03929912, Validation loss: 0.03938513, Gradient norm: 0.69893927
INFO:root:[   69] Training loss: 0.03924120, Validation loss: 0.03934831, Gradient norm: 0.75208390
INFO:root:[   70] Training loss: 0.03900113, Validation loss: 0.03839022, Gradient norm: 0.77992686
INFO:root:[   71] Training loss: 0.03906345, Validation loss: 0.03866777, Gradient norm: 0.86831052
INFO:root:[   72] Training loss: 0.03841161, Validation loss: 0.03893932, Gradient norm: 0.63941912
INFO:root:[   73] Training loss: 0.03812453, Validation loss: 0.03876016, Gradient norm: 0.59018233
INFO:root:[   74] Training loss: 0.03828695, Validation loss: 0.03805794, Gradient norm: 0.85576422
INFO:root:[   75] Training loss: 0.03814947, Validation loss: 0.03720375, Gradient norm: 0.82622101
INFO:root:[   76] Training loss: 0.03796060, Validation loss: 0.03764967, Gradient norm: 0.85990418
INFO:root:[   77] Training loss: 0.03748020, Validation loss: 0.03707543, Gradient norm: 0.65849317
INFO:root:[   78] Training loss: 0.03691842, Validation loss: 0.03714765, Gradient norm: 0.37700811
INFO:root:[   79] Training loss: 0.03696395, Validation loss: 0.03717483, Gradient norm: 0.46425803
INFO:root:[   80] Training loss: 0.03699565, Validation loss: 0.03664945, Gradient norm: 0.70908109
INFO:root:[   81] Training loss: 0.03664210, Validation loss: 0.03756330, Gradient norm: 0.61521017
INFO:root:[   82] Training loss: 0.03671141, Validation loss: 0.03649576, Gradient norm: 0.77919543
INFO:root:[   83] Training loss: 0.03660745, Validation loss: 0.03674778, Gradient norm: 0.83727315
INFO:root:[   84] Training loss: 0.03671947, Validation loss: 0.03598611, Gradient norm: 1.00985247
INFO:root:[   85] Training loss: 0.03625204, Validation loss: 0.03577348, Gradient norm: 0.90141485
INFO:root:[   86] Training loss: 0.03561194, Validation loss: 0.03584830, Gradient norm: 0.45181826
INFO:root:[   87] Training loss: 0.03596203, Validation loss: 0.03515094, Gradient norm: 0.86563417
INFO:root:[   88] Training loss: 0.03586329, Validation loss: 0.03514178, Gradient norm: 0.88705753
INFO:root:[   89] Training loss: 0.03542955, Validation loss: 0.03544022, Gradient norm: 0.71577584
INFO:root:[   90] Training loss: 0.03521670, Validation loss: 0.03484096, Gradient norm: 0.70201181
INFO:root:[   91] Training loss: 0.03481009, Validation loss: 0.03482557, Gradient norm: 0.50262076
INFO:root:[   92] Training loss: 0.03483465, Validation loss: 0.03525005, Gradient norm: 0.53856544
INFO:root:[   93] Training loss: 0.03490425, Validation loss: 0.03470092, Gradient norm: 0.86942261
INFO:root:[   94] Training loss: 0.03469355, Validation loss: 0.03428081, Gradient norm: 0.89412582
INFO:root:[   95] Training loss: 0.03432333, Validation loss: 0.03444194, Gradient norm: 0.73414722
INFO:root:[   96] Training loss: 0.03422037, Validation loss: 0.03413700, Gradient norm: 0.64597761
INFO:root:[   97] Training loss: 0.03409053, Validation loss: 0.03404153, Gradient norm: 0.73042438
INFO:root:[   98] Training loss: 0.03404125, Validation loss: 0.03393591, Gradient norm: 0.84791727
INFO:root:[   99] Training loss: 0.03380821, Validation loss: 0.03454966, Gradient norm: 0.75482374
INFO:root:[  100] Training loss: 0.03348221, Validation loss: 0.03377569, Gradient norm: 0.66923198
INFO:root:[  101] Training loss: 0.03333164, Validation loss: 0.03307246, Gradient norm: 0.65469353
INFO:root:[  102] Training loss: 0.03348281, Validation loss: 0.03451999, Gradient norm: 0.83326679
INFO:root:[  103] Training loss: 0.03331924, Validation loss: 0.03282103, Gradient norm: 0.90757058
INFO:root:[  104] Training loss: 0.03306140, Validation loss: 0.03308930, Gradient norm: 0.82621556
INFO:root:[  105] Training loss: 0.03291610, Validation loss: 0.03267324, Gradient norm: 0.84210306
INFO:root:[  106] Training loss: 0.03279811, Validation loss: 0.03362809, Gradient norm: 0.85955202
INFO:root:[  107] Training loss: 0.03243079, Validation loss: 0.03221358, Gradient norm: 0.70491817
INFO:root:[  108] Training loss: 0.03241044, Validation loss: 0.03247717, Gradient norm: 0.80988345
INFO:root:[  109] Training loss: 0.03227159, Validation loss: 0.03285223, Gradient norm: 0.76330932
INFO:root:[  110] Training loss: 0.03217341, Validation loss: 0.03208957, Gradient norm: 0.79452972
INFO:root:[  111] Training loss: 0.03200787, Validation loss: 0.03238403, Gradient norm: 0.81512514
INFO:root:[  112] Training loss: 0.03205752, Validation loss: 0.03172077, Gradient norm: 0.95826507
INFO:root:[  113] Training loss: 0.03162893, Validation loss: 0.03169002, Gradient norm: 0.74701744
INFO:root:[  114] Training loss: 0.03170480, Validation loss: 0.03152642, Gradient norm: 0.94182809
INFO:root:[  115] Training loss: 0.03154213, Validation loss: 0.03127508, Gradient norm: 0.87840848
INFO:root:[  116] Training loss: 0.03087075, Validation loss: 0.03085451, Gradient norm: 0.38447633
INFO:root:[  117] Training loss: 0.03133947, Validation loss: 0.03054186, Gradient norm: 0.92184982
INFO:root:[  118] Training loss: 0.03100817, Validation loss: 0.03041065, Gradient norm: 0.85280140
INFO:root:[  119] Training loss: 0.03077190, Validation loss: 0.03096555, Gradient norm: 0.73425380
INFO:root:[  120] Training loss: 0.03062017, Validation loss: 0.03074342, Gradient norm: 0.70391697
INFO:root:[  121] Training loss: 0.03054094, Validation loss: 0.03100111, Gradient norm: 0.76614088
INFO:root:[  122] Training loss: 0.03058747, Validation loss: 0.03130375, Gradient norm: 0.91910816
INFO:root:[  123] Training loss: 0.03055809, Validation loss: 0.03069003, Gradient norm: 1.03203273
INFO:root:[  124] Training loss: 0.03013263, Validation loss: 0.03013467, Gradient norm: 0.76307031
INFO:root:[  125] Training loss: 0.03008892, Validation loss: 0.03008462, Gradient norm: 0.86091281
INFO:root:[  126] Training loss: 0.02998003, Validation loss: 0.02962508, Gradient norm: 0.83918478
INFO:root:[  127] Training loss: 0.02959003, Validation loss: 0.02975084, Gradient norm: 0.62586239
INFO:root:[  128] Training loss: 0.02977036, Validation loss: 0.02996229, Gradient norm: 0.91984330
INFO:root:[  129] Training loss: 0.02974674, Validation loss: 0.03031850, Gradient norm: 0.99935944
INFO:root:[  130] Training loss: 0.02956714, Validation loss: 0.02985249, Gradient norm: 0.99003067
INFO:root:[  131] Training loss: 0.02920068, Validation loss: 0.02900600, Gradient norm: 0.74722603
INFO:root:[  132] Training loss: 0.02922891, Validation loss: 0.02952297, Gradient norm: 0.85495666
INFO:root:[  133] Training loss: 0.02928902, Validation loss: 0.02880935, Gradient norm: 1.01642375
INFO:root:[  134] Training loss: 0.02883949, Validation loss: 0.02927966, Gradient norm: 0.76574883
INFO:root:[  135] Training loss: 0.02879792, Validation loss: 0.02898983, Gradient norm: 0.81671351
INFO:root:[  136] Training loss: 0.02843038, Validation loss: 0.02849576, Gradient norm: 0.52943535
INFO:root:[  137] Training loss: 0.02849243, Validation loss: 0.02824355, Gradient norm: 0.73719231
INFO:root:[  138] Training loss: 0.02877847, Validation loss: 0.02835284, Gradient norm: 1.11762229
INFO:root:[  139] Training loss: 0.02852157, Validation loss: 0.02800014, Gradient norm: 1.00718029
INFO:root:[  140] Training loss: 0.02836521, Validation loss: 0.02895769, Gradient norm: 0.95801545
INFO:root:[  141] Training loss: 0.02844935, Validation loss: 0.02852911, Gradient norm: 1.05726125
INFO:root:[  142] Training loss: 0.02811433, Validation loss: 0.02876307, Gradient norm: 0.93234662
INFO:root:[  143] Training loss: 0.02795462, Validation loss: 0.02757759, Gradient norm: 0.84668717
INFO:root:[  144] Training loss: 0.02805310, Validation loss: 0.02768744, Gradient norm: 1.03884814
INFO:root:[  145] Training loss: 0.02770109, Validation loss: 0.02786233, Gradient norm: 0.86963381
INFO:root:[  146] Training loss: 0.02783263, Validation loss: 0.02745290, Gradient norm: 1.11737693
INFO:root:[  147] Training loss: 0.02795489, Validation loss: 0.02791360, Gradient norm: 1.17221154
INFO:root:[  148] Training loss: 0.02762553, Validation loss: 0.02757629, Gradient norm: 1.01296116
INFO:root:[  149] Training loss: 0.02733509, Validation loss: 0.02700203, Gradient norm: 0.85962730
INFO:root:[  150] Training loss: 0.02720947, Validation loss: 0.02762203, Gradient norm: 0.84524010
INFO:root:[  151] Training loss: 0.02735629, Validation loss: 0.02758993, Gradient norm: 1.08693362
INFO:root:[  152] Training loss: 0.02726442, Validation loss: 0.02719561, Gradient norm: 1.06981410
INFO:root:[  153] Training loss: 0.02690123, Validation loss: 0.02687397, Gradient norm: 0.69609537
INFO:root:[  154] Training loss: 0.02684201, Validation loss: 0.02734982, Gradient norm: 0.84324603
INFO:root:[  155] Training loss: 0.02694992, Validation loss: 0.02649556, Gradient norm: 0.98704062
INFO:root:[  156] Training loss: 0.02677001, Validation loss: 0.02773914, Gradient norm: 0.88960383
INFO:root:[  157] Training loss: 0.02665164, Validation loss: 0.02631297, Gradient norm: 0.93304664
INFO:root:[  158] Training loss: 0.02657458, Validation loss: 0.02713032, Gradient norm: 0.96808767
INFO:root:[  159] Training loss: 0.02652422, Validation loss: 0.02623110, Gradient norm: 0.90819273
INFO:root:[  160] Training loss: 0.02645593, Validation loss: 0.02676945, Gradient norm: 0.93355633
INFO:root:[  161] Training loss: 0.02648861, Validation loss: 0.02651291, Gradient norm: 1.08534167
INFO:root:[  162] Training loss: 0.02621831, Validation loss: 0.02664135, Gradient norm: 0.87148881
INFO:root:[  163] Training loss: 0.02632065, Validation loss: 0.02651454, Gradient norm: 1.10214989
INFO:root:[  164] Training loss: 0.02618661, Validation loss: 0.02569462, Gradient norm: 1.02229679
INFO:root:[  165] Training loss: 0.02599064, Validation loss: 0.02613407, Gradient norm: 0.91051488
INFO:root:[  166] Training loss: 0.02591369, Validation loss: 0.02568469, Gradient norm: 0.96013480
INFO:root:[  167] Training loss: 0.02570304, Validation loss: 0.02566727, Gradient norm: 0.77485595
INFO:root:[  168] Training loss: 0.02585938, Validation loss: 0.02590236, Gradient norm: 1.05602642
INFO:root:[  169] Training loss: 0.02576730, Validation loss: 0.02568102, Gradient norm: 0.98679933
INFO:root:[  170] Training loss: 0.02577728, Validation loss: 0.02511869, Gradient norm: 1.11526511
INFO:root:[  171] Training loss: 0.02555983, Validation loss: 0.02554643, Gradient norm: 0.96006222
INFO:root:[  172] Training loss: 0.02546221, Validation loss: 0.02547099, Gradient norm: 0.92863594
INFO:root:[  173] Training loss: 0.02558864, Validation loss: 0.02499446, Gradient norm: 1.11199606
INFO:root:[  174] Training loss: 0.02561849, Validation loss: 0.02498922, Gradient norm: 1.21018381
INFO:root:[  175] Training loss: 0.02535101, Validation loss: 0.02566176, Gradient norm: 1.04172713
INFO:root:[  176] Training loss: 0.02533502, Validation loss: 0.02528663, Gradient norm: 1.10483446
INFO:root:[  177] Training loss: 0.02525074, Validation loss: 0.02534491, Gradient norm: 1.00967960
INFO:root:[  178] Training loss: 0.02504775, Validation loss: 0.02481189, Gradient norm: 0.94651877
INFO:root:[  179] Training loss: 0.02497550, Validation loss: 0.02504490, Gradient norm: 0.95080562
INFO:root:[  180] Training loss: 0.02504913, Validation loss: 0.02471206, Gradient norm: 1.04194238
INFO:root:[  181] Training loss: 0.02488744, Validation loss: 0.02498327, Gradient norm: 1.01866397
INFO:root:[  182] Training loss: 0.02481872, Validation loss: 0.02445066, Gradient norm: 1.07722740
INFO:root:[  183] Training loss: 0.02485856, Validation loss: 0.02497566, Gradient norm: 1.11341780
INFO:root:[  184] Training loss: 0.02475107, Validation loss: 0.02501223, Gradient norm: 1.09297311
INFO:root:[  185] Training loss: 0.02477395, Validation loss: 0.02435541, Gradient norm: 1.19787344
INFO:root:[  186] Training loss: 0.02474397, Validation loss: 0.02482980, Gradient norm: 1.14660383
INFO:root:[  187] Training loss: 0.02469799, Validation loss: 0.02464099, Gradient norm: 1.25123830
INFO:root:[  188] Training loss: 0.02436338, Validation loss: 0.02393455, Gradient norm: 0.92357041
INFO:root:[  189] Training loss: 0.02431879, Validation loss: 0.02440539, Gradient norm: 0.89140285
INFO:root:[  190] Training loss: 0.02436990, Validation loss: 0.02439897, Gradient norm: 1.05042594
INFO:root:[  191] Training loss: 0.02422558, Validation loss: 0.02394400, Gradient norm: 0.99914037
INFO:root:[  192] Training loss: 0.02412586, Validation loss: 0.02397736, Gradient norm: 0.92736923
INFO:root:[  193] Training loss: 0.02402518, Validation loss: 0.02413477, Gradient norm: 0.83808292
INFO:root:[  194] Training loss: 0.02416145, Validation loss: 0.02497597, Gradient norm: 1.11831408
INFO:root:[  195] Training loss: 0.02413672, Validation loss: 0.02403277, Gradient norm: 1.12416217
INFO:root:[  196] Training loss: 0.02411865, Validation loss: 0.02449387, Gradient norm: 1.16029508
INFO:root:[  197] Training loss: 0.02408658, Validation loss: 0.02370950, Gradient norm: 1.17649655
INFO:root:[  198] Training loss: 0.02414347, Validation loss: 0.02364036, Gradient norm: 1.30170570
INFO:root:[  199] Training loss: 0.02406951, Validation loss: 0.02478933, Gradient norm: 1.27221263
INFO:root:[  200] Training loss: 0.02392948, Validation loss: 0.02376425, Gradient norm: 1.19089836
INFO:root:[  201] Training loss: 0.02393482, Validation loss: 0.02389132, Gradient norm: 1.25411856
INFO:root:[  202] Training loss: 0.02386275, Validation loss: 0.02377005, Gradient norm: 1.26330712
INFO:root:[  203] Training loss: 0.02357008, Validation loss: 0.02454011, Gradient norm: 0.98346404
INFO:root:[  204] Training loss: 0.02360502, Validation loss: 0.02339629, Gradient norm: 1.09645630
INFO:root:[  205] Training loss: 0.02353420, Validation loss: 0.02368939, Gradient norm: 1.06720725
INFO:root:[  206] Training loss: 0.02338769, Validation loss: 0.02418334, Gradient norm: 0.97522887
INFO:root:[  207] Training loss: 0.02340990, Validation loss: 0.02357243, Gradient norm: 1.01588962
INFO:root:[  208] Training loss: 0.02327444, Validation loss: 0.02380226, Gradient norm: 0.89147682
INFO:root:[  209] Training loss: 0.02340443, Validation loss: 0.02314344, Gradient norm: 1.19236282
INFO:root:[  210] Training loss: 0.02323937, Validation loss: 0.02354726, Gradient norm: 0.91177973
INFO:root:[  211] Training loss: 0.02331347, Validation loss: 0.02316025, Gradient norm: 1.20475342
INFO:root:[  212] Training loss: 0.02353409, Validation loss: 0.02333165, Gradient norm: 1.40201287
INFO:root:[  213] Training loss: 0.02327395, Validation loss: 0.02284637, Gradient norm: 1.20968888
INFO:root:[  214] Training loss: 0.02304682, Validation loss: 0.02342622, Gradient norm: 1.05617075
INFO:root:[  215] Training loss: 0.02328411, Validation loss: 0.02299035, Gradient norm: 1.35069991
INFO:root:[  216] Training loss: 0.02291578, Validation loss: 0.02273915, Gradient norm: 0.92418911
INFO:root:[  217] Training loss: 0.02294281, Validation loss: 0.02297720, Gradient norm: 1.00497223
INFO:root:[  218] Training loss: 0.02300405, Validation loss: 0.02281606, Gradient norm: 1.19441381
INFO:root:[  219] Training loss: 0.02285838, Validation loss: 0.02291555, Gradient norm: 1.03029013
INFO:root:[  220] Training loss: 0.02281445, Validation loss: 0.02356154, Gradient norm: 1.08395538
INFO:root:[  221] Training loss: 0.02287113, Validation loss: 0.02323668, Gradient norm: 1.14001589
INFO:root:[  222] Training loss: 0.02270921, Validation loss: 0.02289968, Gradient norm: 1.04768352
INFO:root:[  223] Training loss: 0.02288237, Validation loss: 0.02247965, Gradient norm: 1.28878758
INFO:root:[  224] Training loss: 0.02295362, Validation loss: 0.02294492, Gradient norm: 1.38503299
INFO:root:[  225] Training loss: 0.02260970, Validation loss: 0.02295956, Gradient norm: 1.07138427
INFO:root:[  226] Training loss: 0.02272667, Validation loss: 0.02262985, Gradient norm: 1.26018420
INFO:root:[  227] Training loss: 0.02256252, Validation loss: 0.02222807, Gradient norm: 1.13716056
INFO:root:[  228] Training loss: 0.02263524, Validation loss: 0.02265035, Gradient norm: 1.24004897
INFO:root:[  229] Training loss: 0.02258604, Validation loss: 0.02295114, Gradient norm: 1.23025784
INFO:root:[  230] Training loss: 0.02271243, Validation loss: 0.02213238, Gradient norm: 1.45236263
INFO:root:[  231] Training loss: 0.02232998, Validation loss: 0.02233819, Gradient norm: 1.05003881
INFO:root:[  232] Training loss: 0.02252665, Validation loss: 0.02256584, Gradient norm: 1.31955359
INFO:root:[  233] Training loss: 0.02246630, Validation loss: 0.02250694, Gradient norm: 1.29892554
INFO:root:[  234] Training loss: 0.02235268, Validation loss: 0.02199939, Gradient norm: 1.24438125
INFO:root:[  235] Training loss: 0.02219787, Validation loss: 0.02259743, Gradient norm: 1.03368988
INFO:root:[  236] Training loss: 0.02226143, Validation loss: 0.02189201, Gradient norm: 1.18530296
INFO:root:[  237] Training loss: 0.02204190, Validation loss: 0.02263932, Gradient norm: 0.89444646
INFO:root:[  238] Training loss: 0.02228972, Validation loss: 0.02213142, Gradient norm: 1.29846825
INFO:root:[  239] Training loss: 0.02208229, Validation loss: 0.02251104, Gradient norm: 1.11944154
INFO:root:[  240] Training loss: 0.02215203, Validation loss: 0.02198713, Gradient norm: 1.26803434
INFO:root:[  241] Training loss: 0.02206343, Validation loss: 0.02182994, Gradient norm: 1.25394673
INFO:root:[  242] Training loss: 0.02225912, Validation loss: 0.02229047, Gradient norm: 1.42867021
INFO:root:[  243] Training loss: 0.02197874, Validation loss: 0.02228288, Gradient norm: 1.11345546
INFO:root:[  244] Training loss: 0.02200050, Validation loss: 0.02202216, Gradient norm: 1.20160400
INFO:root:[  245] Training loss: 0.02190832, Validation loss: 0.02186415, Gradient norm: 1.20590826
INFO:root:[  246] Training loss: 0.02201506, Validation loss: 0.02144651, Gradient norm: 1.30496052
INFO:root:[  247] Training loss: 0.02173081, Validation loss: 0.02194442, Gradient norm: 0.98641621
INFO:root:[  248] Training loss: 0.02177765, Validation loss: 0.02217019, Gradient norm: 1.19026821
INFO:root:[  249] Training loss: 0.02184037, Validation loss: 0.02211860, Gradient norm: 1.22478749
INFO:root:[  250] Training loss: 0.02187284, Validation loss: 0.02242234, Gradient norm: 1.32684183
INFO:root:[  251] Training loss: 0.02189877, Validation loss: 0.02139089, Gradient norm: 1.37716381
INFO:root:[  252] Training loss: 0.02176814, Validation loss: 0.02173027, Gradient norm: 1.34457315
INFO:root:[  253] Training loss: 0.02164197, Validation loss: 0.02137712, Gradient norm: 1.15377481
INFO:root:[  254] Training loss: 0.02150312, Validation loss: 0.02155966, Gradient norm: 1.05390805
INFO:root:[  255] Training loss: 0.02161951, Validation loss: 0.02185666, Gradient norm: 1.20145705
INFO:root:[  256] Training loss: 0.02156385, Validation loss: 0.02182567, Gradient norm: 1.12025131
INFO:root:[  257] Training loss: 0.02153538, Validation loss: 0.02194957, Gradient norm: 1.18181619
INFO:root:[  258] Training loss: 0.02166682, Validation loss: 0.02209721, Gradient norm: 1.41233698
INFO:root:[  259] Training loss: 0.02128632, Validation loss: 0.02124175, Gradient norm: 0.84234891
INFO:root:[  260] Training loss: 0.02147929, Validation loss: 0.02196203, Gradient norm: 1.30034062
INFO:root:[  261] Training loss: 0.02145771, Validation loss: 0.02149211, Gradient norm: 1.30380682
INFO:root:[  262] Training loss: 0.02123566, Validation loss: 0.02166440, Gradient norm: 0.97695879
INFO:root:[  263] Training loss: 0.02140283, Validation loss: 0.02166473, Gradient norm: 1.24581695
INFO:root:[  264] Training loss: 0.02140141, Validation loss: 0.02136321, Gradient norm: 1.28729241
INFO:root:[  265] Training loss: 0.02129636, Validation loss: 0.02083298, Gradient norm: 1.23351354
INFO:root:[  266] Training loss: 0.02123890, Validation loss: 0.02147282, Gradient norm: 1.13887581
INFO:root:[  267] Training loss: 0.02113147, Validation loss: 0.02130689, Gradient norm: 1.09145464
INFO:root:[  268] Training loss: 0.02138181, Validation loss: 0.02107390, Gradient norm: 1.44270916
INFO:root:[  269] Training loss: 0.02132127, Validation loss: 0.02114346, Gradient norm: 1.40333946
INFO:root:[  270] Training loss: 0.02135408, Validation loss: 0.02094837, Gradient norm: 1.52117194
INFO:root:[  271] Training loss: 0.02107369, Validation loss: 0.02153889, Gradient norm: 1.09821368
INFO:root:[  272] Training loss: 0.02112844, Validation loss: 0.02089899, Gradient norm: 1.29377320
INFO:root:[  273] Training loss: 0.02120664, Validation loss: 0.02065581, Gradient norm: 1.46333320
INFO:root:[  274] Training loss: 0.02106389, Validation loss: 0.02070182, Gradient norm: 1.29982168
INFO:root:[  275] Training loss: 0.02103092, Validation loss: 0.02085201, Gradient norm: 1.16714985
INFO:root:[  276] Training loss: 0.02099288, Validation loss: 0.02105922, Gradient norm: 1.21536195
INFO:root:[  277] Training loss: 0.02113070, Validation loss: 0.02086204, Gradient norm: 1.52180534
INFO:root:[  278] Training loss: 0.02087729, Validation loss: 0.02135998, Gradient norm: 1.21490805
INFO:root:[  279] Training loss: 0.02094938, Validation loss: 0.02076697, Gradient norm: 1.21687445
INFO:root:[  280] Training loss: 0.02094852, Validation loss: 0.02105744, Gradient norm: 1.44375093
INFO:root:[  281] Training loss: 0.02093094, Validation loss: 0.02208897, Gradient norm: 1.39201707
INFO:root:[  282] Training loss: 0.02101217, Validation loss: 0.02093556, Gradient norm: 1.53293795
INFO:root:EP 282: Early stopping
INFO:root:Training the model took 9416.33s.
INFO:root:Emptying the cuda cache took 0.082s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.38942
INFO:root:EnergyScoreTrain: 0.33086
INFO:root:CoverageTrain: 0.9957
INFO:root:IntervalWidthTrain: 0.05824
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.36662
INFO:root:EnergyScoreValidation: 0.31105
INFO:root:CoverageValidation: 0.99563
INFO:root:IntervalWidthValidation: 0.05832
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.3449
INFO:root:EnergyScoreTest: 0.29299
INFO:root:CoverageTest: 0.99566
INFO:root:IntervalWidthTest: 0.05783
INFO:root:###3 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.27682879, Validation loss: 1.30851265, Gradient norm: 5.98167943
INFO:root:[    2] Training loss: 0.44808788, Validation loss: 0.18541076, Gradient norm: 4.45268604
INFO:root:[    3] Training loss: 0.17474354, Validation loss: 0.16430324, Gradient norm: 1.20801458
INFO:root:[    4] Training loss: 0.16036414, Validation loss: 0.16456220, Gradient norm: 1.33455138
INFO:root:[    5] Training loss: 0.15324551, Validation loss: 0.15513260, Gradient norm: 1.36429733
INFO:root:[    6] Training loss: 0.14725986, Validation loss: 0.14198929, Gradient norm: 1.28408853
INFO:root:[    7] Training loss: 0.13763648, Validation loss: 0.13462837, Gradient norm: 1.37623538
INFO:root:[    8] Training loss: 0.13274217, Validation loss: 0.13358900, Gradient norm: 1.41023334
INFO:root:[    9] Training loss: 0.12437828, Validation loss: 0.12255673, Gradient norm: 1.13910874
INFO:root:[   10] Training loss: 0.11968881, Validation loss: 0.11809297, Gradient norm: 1.23533740
INFO:root:[   11] Training loss: 0.11377667, Validation loss: 0.11053789, Gradient norm: 1.12193943
INFO:root:[   12] Training loss: 0.10947851, Validation loss: 0.10947808, Gradient norm: 1.05886066
INFO:root:[   13] Training loss: 0.10626973, Validation loss: 0.10472855, Gradient norm: 1.38814509
INFO:root:[   14] Training loss: 0.10106071, Validation loss: 0.09787137, Gradient norm: 1.27659205
INFO:root:[   15] Training loss: 0.09723400, Validation loss: 0.09507828, Gradient norm: 1.18675446
INFO:root:[   16] Training loss: 0.09326105, Validation loss: 0.08931600, Gradient norm: 1.04324145
INFO:root:[   17] Training loss: 0.09006954, Validation loss: 0.08838176, Gradient norm: 1.21702566
INFO:root:[   18] Training loss: 0.08496107, Validation loss: 0.08328545, Gradient norm: 0.97755356
INFO:root:[   19] Training loss: 0.08257305, Validation loss: 0.08159050, Gradient norm: 1.11606403
INFO:root:[   20] Training loss: 0.07861322, Validation loss: 0.07824040, Gradient norm: 0.76704868
INFO:root:[   21] Training loss: 0.07716826, Validation loss: 0.07544006, Gradient norm: 0.94007697
INFO:root:[   22] Training loss: 0.07400433, Validation loss: 0.07323352, Gradient norm: 0.87441790
INFO:root:[   23] Training loss: 0.07248389, Validation loss: 0.07152046, Gradient norm: 0.99403674
INFO:root:[   24] Training loss: 0.07098864, Validation loss: 0.06913018, Gradient norm: 0.94373981
INFO:root:[   25] Training loss: 0.06894477, Validation loss: 0.06748313, Gradient norm: 0.83987953
INFO:root:[   26] Training loss: 0.06665152, Validation loss: 0.06560875, Gradient norm: 0.51481507
INFO:root:[   27] Training loss: 0.06537265, Validation loss: 0.06471367, Gradient norm: 0.69053722
INFO:root:[   28] Training loss: 0.06452159, Validation loss: 0.06342056, Gradient norm: 0.91020028
INFO:root:[   29] Training loss: 0.06249302, Validation loss: 0.06263721, Gradient norm: 0.48094940
INFO:root:[   30] Training loss: 0.06139372, Validation loss: 0.06160483, Gradient norm: 0.63509152
INFO:root:[   31] Training loss: 0.06047589, Validation loss: 0.05994610, Gradient norm: 0.68111402
INFO:root:[   32] Training loss: 0.05928396, Validation loss: 0.05907691, Gradient norm: 0.50221782
INFO:root:[   33] Training loss: 0.05843764, Validation loss: 0.05810967, Gradient norm: 0.56179311
INFO:root:[   34] Training loss: 0.05761384, Validation loss: 0.05730605, Gradient norm: 0.56500369
INFO:root:[   35] Training loss: 0.05691477, Validation loss: 0.05770042, Gradient norm: 0.67241864
INFO:root:[   36] Training loss: 0.05651516, Validation loss: 0.05584860, Gradient norm: 0.83897033
INFO:root:[   37] Training loss: 0.05530044, Validation loss: 0.05592851, Gradient norm: 0.50094822
INFO:root:[   38] Training loss: 0.05510644, Validation loss: 0.05507187, Gradient norm: 0.79775809
INFO:root:[   39] Training loss: 0.05433569, Validation loss: 0.05359084, Gradient norm: 0.65170062
INFO:root:[   40] Training loss: 0.05378277, Validation loss: 0.05335036, Gradient norm: 0.57071810
INFO:root:[   41] Training loss: 0.05285688, Validation loss: 0.05327776, Gradient norm: 0.48673005
INFO:root:[   42] Training loss: 0.05277221, Validation loss: 0.05242164, Gradient norm: 0.69968972
INFO:root:[   43] Training loss: 0.05201681, Validation loss: 0.05170865, Gradient norm: 0.43181925
INFO:root:[   44] Training loss: 0.05154419, Validation loss: 0.05096128, Gradient norm: 0.35850619
INFO:root:[   45] Training loss: 0.05115992, Validation loss: 0.05103088, Gradient norm: 0.46418117
INFO:root:[   46] Training loss: 0.05088219, Validation loss: 0.05076246, Gradient norm: 0.53105533
INFO:root:[   47] Training loss: 0.05045690, Validation loss: 0.04995211, Gradient norm: 0.57304957
INFO:root:[   48] Training loss: 0.04996874, Validation loss: 0.04971869, Gradient norm: 0.48080271
INFO:root:[   49] Training loss: 0.04994478, Validation loss: 0.04965682, Gradient norm: 0.76913740
INFO:root:[   50] Training loss: 0.04927007, Validation loss: 0.04922221, Gradient norm: 0.65476928
INFO:root:[   51] Training loss: 0.04914417, Validation loss: 0.04944930, Gradient norm: 0.59373076
INFO:root:[   52] Training loss: 0.04916262, Validation loss: 0.04868295, Gradient norm: 0.94293569
INFO:root:[   53] Training loss: 0.04825980, Validation loss: 0.04842114, Gradient norm: 0.40462149
INFO:root:[   54] Training loss: 0.04806254, Validation loss: 0.04774603, Gradient norm: 0.44996654
INFO:root:[   55] Training loss: 0.04801793, Validation loss: 0.04793547, Gradient norm: 0.81186828
INFO:root:[   56] Training loss: 0.04753416, Validation loss: 0.04796565, Gradient norm: 0.70477819
INFO:root:[   57] Training loss: 0.04754477, Validation loss: 0.04694406, Gradient norm: 0.78851593
INFO:root:[   58] Training loss: 0.04724437, Validation loss: 0.04694925, Gradient norm: 0.84517609
INFO:root:[   59] Training loss: 0.04676124, Validation loss: 0.04654897, Gradient norm: 0.61958306
INFO:root:[   60] Training loss: 0.04643371, Validation loss: 0.04713378, Gradient norm: 0.53005204
INFO:root:[   61] Training loss: 0.04631000, Validation loss: 0.04605823, Gradient norm: 0.66161843
INFO:root:[   62] Training loss: 0.04578263, Validation loss: 0.04648699, Gradient norm: 0.41739700
INFO:root:[   63] Training loss: 0.04565732, Validation loss: 0.04541748, Gradient norm: 0.56992482
INFO:root:[   64] Training loss: 0.04549032, Validation loss: 0.04598694, Gradient norm: 0.73621210
INFO:root:[   65] Training loss: 0.04506532, Validation loss: 0.04549392, Gradient norm: 0.55649264
INFO:root:[   66] Training loss: 0.04480871, Validation loss: 0.04473851, Gradient norm: 0.43062159
INFO:root:[   67] Training loss: 0.04480620, Validation loss: 0.04527597, Gradient norm: 0.61217742
INFO:root:[   68] Training loss: 0.04473764, Validation loss: 0.04520176, Gradient norm: 0.94593405
INFO:root:[   69] Training loss: 0.04430231, Validation loss: 0.04436247, Gradient norm: 0.70639078
INFO:root:[   70] Training loss: 0.04407576, Validation loss: 0.04396457, Gradient norm: 0.60201997
INFO:root:[   71] Training loss: 0.04379962, Validation loss: 0.04374596, Gradient norm: 0.63082399
INFO:root:[   72] Training loss: 0.04373553, Validation loss: 0.04429257, Gradient norm: 0.71838866
INFO:root:[   73] Training loss: 0.04352642, Validation loss: 0.04332884, Gradient norm: 0.86511602
INFO:root:[   74] Training loss: 0.04310139, Validation loss: 0.04356820, Gradient norm: 0.63085202
INFO:root:[   75] Training loss: 0.04328413, Validation loss: 0.04274457, Gradient norm: 0.97648069
INFO:root:[   76] Training loss: 0.04293988, Validation loss: 0.04352253, Gradient norm: 0.83320361
INFO:root:[   77] Training loss: 0.04279516, Validation loss: 0.04255899, Gradient norm: 0.92465629
INFO:root:[   78] Training loss: 0.04230033, Validation loss: 0.04230230, Gradient norm: 0.58864628
INFO:root:[   79] Training loss: 0.04223402, Validation loss: 0.04202181, Gradient norm: 0.73036687
INFO:root:[   80] Training loss: 0.04214125, Validation loss: 0.04212963, Gradient norm: 0.71806517
INFO:root:[   81] Training loss: 0.04165746, Validation loss: 0.04199179, Gradient norm: 0.62377825
INFO:root:[   82] Training loss: 0.04190876, Validation loss: 0.04149798, Gradient norm: 0.99397242
INFO:root:[   83] Training loss: 0.04168861, Validation loss: 0.04134370, Gradient norm: 0.93675325
INFO:root:[   84] Training loss: 0.04149904, Validation loss: 0.04215419, Gradient norm: 0.97286145
INFO:root:[   85] Training loss: 0.04132284, Validation loss: 0.04135468, Gradient norm: 1.00375116
INFO:root:[   86] Training loss: 0.04083028, Validation loss: 0.04100134, Gradient norm: 0.76560296
INFO:root:[   87] Training loss: 0.04092107, Validation loss: 0.04040949, Gradient norm: 0.92674634
INFO:root:[   88] Training loss: 0.04072928, Validation loss: 0.04065028, Gradient norm: 0.99765953
INFO:root:[   89] Training loss: 0.04042658, Validation loss: 0.04004968, Gradient norm: 0.83570463
INFO:root:[   90] Training loss: 0.04022593, Validation loss: 0.03991105, Gradient norm: 0.78244294
INFO:root:[   91] Training loss: 0.03997949, Validation loss: 0.04058938, Gradient norm: 0.56885151
INFO:root:[   92] Training loss: 0.03993402, Validation loss: 0.04049699, Gradient norm: 0.88720755
INFO:root:[   93] Training loss: 0.03992919, Validation loss: 0.03954374, Gradient norm: 1.04163795
INFO:root:[   94] Training loss: 0.03952684, Validation loss: 0.04008311, Gradient norm: 0.81136720
INFO:root:[   95] Training loss: 0.03950299, Validation loss: 0.03929000, Gradient norm: 0.90766274
INFO:root:[   96] Training loss: 0.03906871, Validation loss: 0.03986352, Gradient norm: 0.43746713
INFO:root:[   97] Training loss: 0.03901719, Validation loss: 0.03901658, Gradient norm: 0.75198369
INFO:root:[   98] Training loss: 0.03917644, Validation loss: 0.03907897, Gradient norm: 1.08856676
INFO:root:[   99] Training loss: 0.03898805, Validation loss: 0.03850085, Gradient norm: 1.07792715
INFO:root:[  100] Training loss: 0.03867505, Validation loss: 0.03834684, Gradient norm: 0.80243068
INFO:root:[  101] Training loss: 0.03870755, Validation loss: 0.03884386, Gradient norm: 1.00543267
INFO:root:[  102] Training loss: 0.03839078, Validation loss: 0.03896043, Gradient norm: 0.92821804
INFO:root:[  103] Training loss: 0.03815705, Validation loss: 0.03798818, Gradient norm: 0.72707037
INFO:root:[  104] Training loss: 0.03816613, Validation loss: 0.03776980, Gradient norm: 0.90840134
INFO:root:[  105] Training loss: 0.03813180, Validation loss: 0.03760390, Gradient norm: 1.09174717
INFO:root:[  106] Training loss: 0.03801184, Validation loss: 0.03848070, Gradient norm: 1.11765579
INFO:root:[  107] Training loss: 0.03769239, Validation loss: 0.03738234, Gradient norm: 0.87705297
INFO:root:[  108] Training loss: 0.03755527, Validation loss: 0.03735970, Gradient norm: 0.84302345
INFO:root:[  109] Training loss: 0.03744473, Validation loss: 0.03784673, Gradient norm: 0.97331042
INFO:root:[  110] Training loss: 0.03724641, Validation loss: 0.03709076, Gradient norm: 0.94826501
INFO:root:[  111] Training loss: 0.03711053, Validation loss: 0.03720006, Gradient norm: 0.88219051
INFO:root:[  112] Training loss: 0.03702753, Validation loss: 0.03674296, Gradient norm: 0.99009702
INFO:root:[  113] Training loss: 0.03689770, Validation loss: 0.03659989, Gradient norm: 1.01525815
INFO:root:[  114] Training loss: 0.03679928, Validation loss: 0.03648902, Gradient norm: 0.97231003
INFO:root:[  115] Training loss: 0.03659904, Validation loss: 0.03668505, Gradient norm: 0.96051017
INFO:root:[  116] Training loss: 0.03661697, Validation loss: 0.03604367, Gradient norm: 1.10105167
INFO:root:[  117] Training loss: 0.03644200, Validation loss: 0.03622159, Gradient norm: 1.00823145
INFO:root:[  118] Training loss: 0.03630761, Validation loss: 0.03608024, Gradient norm: 1.09769578
INFO:root:[  119] Training loss: 0.03616427, Validation loss: 0.03587089, Gradient norm: 1.12956353
INFO:root:[  120] Training loss: 0.03610729, Validation loss: 0.03581336, Gradient norm: 1.16665901
INFO:root:[  121] Training loss: 0.03597166, Validation loss: 0.03640743, Gradient norm: 0.96526877
INFO:root:[  122] Training loss: 0.03579375, Validation loss: 0.03575088, Gradient norm: 1.02834528
INFO:root:[  123] Training loss: 0.03559445, Validation loss: 0.03582396, Gradient norm: 0.98545576
INFO:root:[  124] Training loss: 0.03551900, Validation loss: 0.03517172, Gradient norm: 0.95233124
INFO:root:[  125] Training loss: 0.03543759, Validation loss: 0.03524114, Gradient norm: 1.08914017
INFO:root:[  126] Training loss: 0.03531299, Validation loss: 0.03584464, Gradient norm: 1.08482454
INFO:root:[  127] Training loss: 0.03528668, Validation loss: 0.03508592, Gradient norm: 1.10434662
INFO:root:[  128] Training loss: 0.03507746, Validation loss: 0.03543916, Gradient norm: 1.07833402
INFO:root:[  129] Training loss: 0.03500925, Validation loss: 0.03476933, Gradient norm: 1.16921520
INFO:root:[  130] Training loss: 0.03493655, Validation loss: 0.03509903, Gradient norm: 1.22059449
INFO:root:[  131] Training loss: 0.03464856, Validation loss: 0.03457356, Gradient norm: 1.02156369
INFO:root:[  132] Training loss: 0.03458951, Validation loss: 0.03417890, Gradient norm: 1.08106153
INFO:root:[  133] Training loss: 0.03440496, Validation loss: 0.03406686, Gradient norm: 0.94106682
INFO:root:[  134] Training loss: 0.03441352, Validation loss: 0.03419142, Gradient norm: 1.10779025
INFO:root:[  135] Training loss: 0.03409128, Validation loss: 0.03386415, Gradient norm: 0.82857226
INFO:root:[  136] Training loss: 0.03421619, Validation loss: 0.03388451, Gradient norm: 1.31329804
INFO:root:[  137] Training loss: 0.03388370, Validation loss: 0.03393912, Gradient norm: 0.86043605
INFO:root:[  138] Training loss: 0.03389058, Validation loss: 0.03351305, Gradient norm: 1.08046707
INFO:root:[  139] Training loss: 0.03367181, Validation loss: 0.03364955, Gradient norm: 0.94201305
INFO:root:[  140] Training loss: 0.03351694, Validation loss: 0.03384874, Gradient norm: 0.94778512
INFO:root:[  141] Training loss: 0.03351713, Validation loss: 0.03317048, Gradient norm: 1.03113127
INFO:root:[  142] Training loss: 0.03372229, Validation loss: 0.03375300, Gradient norm: 1.43043620
INFO:root:[  143] Training loss: 0.03327758, Validation loss: 0.03389203, Gradient norm: 1.26411650
INFO:root:[  144] Training loss: 0.03313288, Validation loss: 0.03330011, Gradient norm: 1.17083820
INFO:root:[  145] Training loss: 0.03318883, Validation loss: 0.03299106, Gradient norm: 1.29080311
INFO:root:[  146] Training loss: 0.03298825, Validation loss: 0.03291430, Gradient norm: 1.22625642
INFO:root:[  147] Training loss: 0.03290104, Validation loss: 0.03320080, Gradient norm: 1.26282298
INFO:root:[  148] Training loss: 0.03270837, Validation loss: 0.03293127, Gradient norm: 1.13897415
INFO:root:[  149] Training loss: 0.03250888, Validation loss: 0.03270477, Gradient norm: 1.14041018
INFO:root:[  150] Training loss: 0.03256149, Validation loss: 0.03256294, Gradient norm: 1.20147103
INFO:root:[  151] Training loss: 0.03248045, Validation loss: 0.03358742, Gradient norm: 1.27433394
INFO:root:[  152] Training loss: 0.03249155, Validation loss: 0.03267700, Gradient norm: 1.50064486
INFO:root:[  153] Training loss: 0.03230778, Validation loss: 0.03219675, Gradient norm: 1.42448546
INFO:root:[  154] Training loss: 0.03217826, Validation loss: 0.03207012, Gradient norm: 1.11115587
INFO:root:[  155] Training loss: 0.03192759, Validation loss: 0.03222665, Gradient norm: 0.94096764
INFO:root:[  156] Training loss: 0.03205155, Validation loss: 0.03210419, Gradient norm: 1.46326353
INFO:root:[  157] Training loss: 0.03178595, Validation loss: 0.03158343, Gradient norm: 1.07743592
INFO:root:[  158] Training loss: 0.03170301, Validation loss: 0.03210900, Gradient norm: 1.16350008
INFO:root:[  159] Training loss: 0.03169116, Validation loss: 0.03194068, Gradient norm: 1.30030776
INFO:root:[  160] Training loss: 0.03168515, Validation loss: 0.03145970, Gradient norm: 1.37976439
INFO:root:[  161] Training loss: 0.03137790, Validation loss: 0.03147344, Gradient norm: 1.14428112
INFO:root:[  162] Training loss: 0.03132561, Validation loss: 0.03096647, Gradient norm: 1.08987235
INFO:root:[  163] Training loss: 0.03125104, Validation loss: 0.03120024, Gradient norm: 1.20022062
INFO:root:[  164] Training loss: 0.03123179, Validation loss: 0.03073093, Gradient norm: 1.23331697
INFO:root:[  165] Training loss: 0.03124041, Validation loss: 0.03097266, Gradient norm: 1.30904338
INFO:root:[  166] Training loss: 0.03104749, Validation loss: 0.03095850, Gradient norm: 1.15636306
INFO:root:[  167] Training loss: 0.03092708, Validation loss: 0.03090475, Gradient norm: 1.15046327
INFO:root:[  168] Training loss: 0.03094955, Validation loss: 0.03103689, Gradient norm: 1.40881585
INFO:root:[  169] Training loss: 0.03086457, Validation loss: 0.03145806, Gradient norm: 1.36539416
INFO:root:[  170] Training loss: 0.03075436, Validation loss: 0.03106661, Gradient norm: 1.38087786
INFO:root:[  171] Training loss: 0.03053555, Validation loss: 0.03072487, Gradient norm: 1.02814376
INFO:root:[  172] Training loss: 0.03049925, Validation loss: 0.03053293, Gradient norm: 1.12073535
INFO:root:[  173] Training loss: 0.03044840, Validation loss: 0.03043417, Gradient norm: 1.14820889
INFO:root:[  174] Training loss: 0.03040922, Validation loss: 0.03064878, Gradient norm: 1.25785328
INFO:root:[  175] Training loss: 0.03037039, Validation loss: 0.02986960, Gradient norm: 1.34787891
INFO:root:[  176] Training loss: 0.03020299, Validation loss: 0.03009462, Gradient norm: 1.23225791
INFO:root:[  177] Training loss: 0.03023369, Validation loss: 0.03079241, Gradient norm: 1.39176022
INFO:root:[  178] Training loss: 0.03032076, Validation loss: 0.03059713, Gradient norm: 1.59972320
INFO:root:[  179] Training loss: 0.03019282, Validation loss: 0.03031129, Gradient norm: 1.47586054
INFO:root:[  180] Training loss: 0.03018043, Validation loss: 0.02979188, Gradient norm: 1.74989973
INFO:root:[  181] Training loss: 0.02989114, Validation loss: 0.02983767, Gradient norm: 1.17997798
INFO:root:[  182] Training loss: 0.02987545, Validation loss: 0.03040613, Gradient norm: 1.30077178
INFO:root:[  183] Training loss: 0.02977438, Validation loss: 0.02980260, Gradient norm: 1.16518508
INFO:root:[  184] Training loss: 0.02970783, Validation loss: 0.02944608, Gradient norm: 1.28425323
INFO:root:[  185] Training loss: 0.02982913, Validation loss: 0.03017497, Gradient norm: 1.71622846
INFO:root:[  186] Training loss: 0.02970747, Validation loss: 0.03058954, Gradient norm: 1.50430331
INFO:root:[  187] Training loss: 0.02973842, Validation loss: 0.03058851, Gradient norm: 1.75913447
INFO:root:[  188] Training loss: 0.02951039, Validation loss: 0.02943692, Gradient norm: 1.38684435
INFO:root:[  189] Training loss: 0.02934476, Validation loss: 0.02962069, Gradient norm: 1.18423646
INFO:root:[  190] Training loss: 0.02941420, Validation loss: 0.02983967, Gradient norm: 1.48565104
INFO:root:[  191] Training loss: 0.02930438, Validation loss: 0.02905171, Gradient norm: 1.33857727
INFO:root:[  192] Training loss: 0.02934228, Validation loss: 0.03012188, Gradient norm: 1.55737625
INFO:root:[  193] Training loss: 0.02924008, Validation loss: 0.02933270, Gradient norm: 1.46077631
INFO:root:[  194] Training loss: 0.02916590, Validation loss: 0.02893302, Gradient norm: 1.20766462
INFO:root:[  195] Training loss: 0.02894421, Validation loss: 0.02903872, Gradient norm: 1.01613926
INFO:root:[  196] Training loss: 0.02912393, Validation loss: 0.02902858, Gradient norm: 1.44098571
INFO:root:[  197] Training loss: 0.02911677, Validation loss: 0.02985681, Gradient norm: 1.71245754
INFO:root:[  198] Training loss: 0.02919424, Validation loss: 0.02909775, Gradient norm: 1.96289690
INFO:root:[  199] Training loss: 0.02888509, Validation loss: 0.02898307, Gradient norm: 1.45620196
INFO:root:[  200] Training loss: 0.02878822, Validation loss: 0.02838985, Gradient norm: 1.34410748
INFO:root:[  201] Training loss: 0.02879598, Validation loss: 0.02847097, Gradient norm: 1.41809603
INFO:root:[  202] Training loss: 0.02875496, Validation loss: 0.02894061, Gradient norm: 1.37699912
INFO:root:[  203] Training loss: 0.02868488, Validation loss: 0.02857178, Gradient norm: 1.46687626
INFO:root:[  204] Training loss: 0.02874857, Validation loss: 0.02905328, Gradient norm: 1.56644738
INFO:root:[  205] Training loss: 0.02863593, Validation loss: 0.02838471, Gradient norm: 1.54958321
INFO:root:[  206] Training loss: 0.02860736, Validation loss: 0.02863909, Gradient norm: 1.50122807
INFO:root:[  207] Training loss: 0.02866749, Validation loss: 0.02819500, Gradient norm: 1.79444173
INFO:root:[  208] Training loss: 0.02850664, Validation loss: 0.02838775, Gradient norm: 1.46789515
INFO:root:[  209] Training loss: 0.02838841, Validation loss: 0.02814480, Gradient norm: 1.35372631
INFO:root:[  210] Training loss: 0.02841151, Validation loss: 0.02850722, Gradient norm: 1.53126320
INFO:root:[  211] Training loss: 0.02836901, Validation loss: 0.02824326, Gradient norm: 1.58251491
INFO:root:[  212] Training loss: 0.02843244, Validation loss: 0.02852525, Gradient norm: 1.74291339
INFO:root:[  213] Training loss: 0.02841636, Validation loss: 0.02857755, Gradient norm: 1.77966101
INFO:root:[  214] Training loss: 0.02814629, Validation loss: 0.02835651, Gradient norm: 1.51015435
INFO:root:[  215] Training loss: 0.02821446, Validation loss: 0.02894041, Gradient norm: 1.69247303
INFO:root:[  216] Training loss: 0.02811256, Validation loss: 0.02789734, Gradient norm: 1.64850470
INFO:root:[  217] Training loss: 0.02812571, Validation loss: 0.02779004, Gradient norm: 1.66681608
INFO:root:[  218] Training loss: 0.02793994, Validation loss: 0.02802062, Gradient norm: 1.34915409
INFO:root:[  219] Training loss: 0.02781177, Validation loss: 0.02783533, Gradient norm: 0.99705402
INFO:root:[  220] Training loss: 0.02788803, Validation loss: 0.02794475, Gradient norm: 1.46030419
INFO:root:[  221] Training loss: 0.02781137, Validation loss: 0.02772650, Gradient norm: 1.26749071
INFO:root:[  222] Training loss: 0.02779214, Validation loss: 0.02783148, Gradient norm: 1.46097409
INFO:root:[  223] Training loss: 0.02780768, Validation loss: 0.02782924, Gradient norm: 1.58085969
INFO:root:[  224] Training loss: 0.02778783, Validation loss: 0.02783107, Gradient norm: 1.55354462
INFO:root:[  225] Training loss: 0.02774246, Validation loss: 0.02761741, Gradient norm: 1.67042930
INFO:root:[  226] Training loss: 0.02767910, Validation loss: 0.02752364, Gradient norm: 1.57457969
INFO:root:[  227] Training loss: 0.02752364, Validation loss: 0.02772806, Gradient norm: 1.37550769
INFO:root:[  228] Training loss: 0.02758848, Validation loss: 0.02741928, Gradient norm: 1.45216813
INFO:root:[  229] Training loss: 0.02756427, Validation loss: 0.02732356, Gradient norm: 1.53908874
INFO:root:[  230] Training loss: 0.02750671, Validation loss: 0.02734677, Gradient norm: 1.53505401
INFO:root:[  231] Training loss: 0.02736255, Validation loss: 0.02718606, Gradient norm: 1.35133729
INFO:root:[  232] Training loss: 0.02740897, Validation loss: 0.02732468, Gradient norm: 1.45789998
INFO:root:[  233] Training loss: 0.02732731, Validation loss: 0.02715800, Gradient norm: 1.52410340
INFO:root:[  234] Training loss: 0.02735421, Validation loss: 0.02758827, Gradient norm: 1.63046904
INFO:root:[  235] Training loss: 0.02739904, Validation loss: 0.02752451, Gradient norm: 1.68839447
INFO:root:[  236] Training loss: 0.02729475, Validation loss: 0.02840931, Gradient norm: 1.55544079
INFO:root:[  237] Training loss: 0.02723310, Validation loss: 0.02716539, Gradient norm: 1.52069013
INFO:root:[  238] Training loss: 0.02719668, Validation loss: 0.02733168, Gradient norm: 1.59432534
INFO:root:[  239] Training loss: 0.02710224, Validation loss: 0.02765147, Gradient norm: 1.31407344
INFO:root:[  240] Training loss: 0.02714040, Validation loss: 0.02718527, Gradient norm: 1.67655970
INFO:root:[  241] Training loss: 0.02703389, Validation loss: 0.02725176, Gradient norm: 1.75747161
INFO:root:[  242] Training loss: 0.02706589, Validation loss: 0.02712391, Gradient norm: 1.68769095
INFO:root:[  243] Training loss: 0.02697730, Validation loss: 0.02692761, Gradient norm: 1.52799872
INFO:root:[  244] Training loss: 0.02676283, Validation loss: 0.02666530, Gradient norm: 1.06167794
INFO:root:[  245] Training loss: 0.02677381, Validation loss: 0.02718253, Gradient norm: 1.09312134
INFO:root:[  246] Training loss: 0.02703276, Validation loss: 0.02718255, Gradient norm: 1.93197187
INFO:root:[  247] Training loss: 0.02691955, Validation loss: 0.02715681, Gradient norm: 1.85015182
INFO:root:[  248] Training loss: 0.02678539, Validation loss: 0.02653087, Gradient norm: 1.68619889
INFO:root:[  249] Training loss: 0.02681331, Validation loss: 0.02663601, Gradient norm: 1.82322642
INFO:root:[  250] Training loss: 0.02685293, Validation loss: 0.02732181, Gradient norm: 1.85677451
INFO:root:[  251] Training loss: 0.02679547, Validation loss: 0.02683285, Gradient norm: 1.92401823
INFO:root:[  252] Training loss: 0.02671730, Validation loss: 0.02662307, Gradient norm: 1.73394808
INFO:root:[  253] Training loss: 0.02656370, Validation loss: 0.02650032, Gradient norm: 1.62958707
INFO:root:[  254] Training loss: 0.02651471, Validation loss: 0.02655133, Gradient norm: 1.40421120
INFO:root:[  255] Training loss: 0.02665544, Validation loss: 0.02678106, Gradient norm: 1.82658757
INFO:root:[  256] Training loss: 0.02661276, Validation loss: 0.02662027, Gradient norm: 1.76419157
INFO:root:[  257] Training loss: 0.02639616, Validation loss: 0.02684605, Gradient norm: 1.58900857
INFO:root:[  258] Training loss: 0.02650068, Validation loss: 0.02617518, Gradient norm: 1.64274091
INFO:root:[  259] Training loss: 0.02623566, Validation loss: 0.02644082, Gradient norm: 1.30278457
INFO:root:[  260] Training loss: 0.02636075, Validation loss: 0.02638547, Gradient norm: 1.60470845
INFO:root:[  261] Training loss: 0.02636906, Validation loss: 0.02598689, Gradient norm: 1.95294231
INFO:root:[  262] Training loss: 0.02647311, Validation loss: 0.02607524, Gradient norm: 2.00025749
INFO:root:[  263] Training loss: 0.02623978, Validation loss: 0.02591918, Gradient norm: 1.71499364
INFO:root:[  264] Training loss: 0.02618173, Validation loss: 0.02589742, Gradient norm: 1.61995357
INFO:root:[  265] Training loss: 0.02621572, Validation loss: 0.02609510, Gradient norm: 1.74258329
INFO:root:[  266] Training loss: 0.02611059, Validation loss: 0.02586281, Gradient norm: 1.50060632
INFO:root:[  267] Training loss: 0.02618918, Validation loss: 0.02582743, Gradient norm: 1.96457536
INFO:root:[  268] Training loss: 0.02608948, Validation loss: 0.02576989, Gradient norm: 1.72997219
INFO:root:[  269] Training loss: 0.02602130, Validation loss: 0.02607978, Gradient norm: 1.61466764
INFO:root:[  270] Training loss: 0.02595595, Validation loss: 0.02583594, Gradient norm: 1.61979471
INFO:root:[  271] Training loss: 0.02600606, Validation loss: 0.02612859, Gradient norm: 1.72973253
INFO:root:[  272] Training loss: 0.02597349, Validation loss: 0.02638759, Gradient norm: 1.88414781
INFO:root:[  273] Training loss: 0.02595246, Validation loss: 0.02586603, Gradient norm: 1.85105863
INFO:root:[  274] Training loss: 0.02592058, Validation loss: 0.02673583, Gradient norm: 1.76725227
INFO:root:[  275] Training loss: 0.02593326, Validation loss: 0.02563896, Gradient norm: 1.93908561
INFO:root:[  276] Training loss: 0.02577283, Validation loss: 0.02618140, Gradient norm: 1.43343986
INFO:root:[  277] Training loss: 0.02588481, Validation loss: 0.02563508, Gradient norm: 2.08551452
INFO:root:[  278] Training loss: 0.02566428, Validation loss: 0.02556105, Gradient norm: 1.56222527
INFO:root:[  279] Training loss: 0.02575847, Validation loss: 0.02593401, Gradient norm: 1.76572352
INFO:root:[  280] Training loss: 0.02572798, Validation loss: 0.02617399, Gradient norm: 1.85454518
INFO:root:[  281] Training loss: 0.02570080, Validation loss: 0.02578242, Gradient norm: 1.98209035
INFO:root:[  282] Training loss: 0.02567764, Validation loss: 0.02565466, Gradient norm: 1.96163615
INFO:root:[  283] Training loss: 0.02549140, Validation loss: 0.02545275, Gradient norm: 1.52628979
INFO:root:[  284] Training loss: 0.02563557, Validation loss: 0.02531698, Gradient norm: 2.07924662
INFO:root:[  285] Training loss: 0.02557048, Validation loss: 0.02596489, Gradient norm: 1.82908883
INFO:root:[  286] Training loss: 0.02556260, Validation loss: 0.02547651, Gradient norm: 1.87059323
INFO:root:[  287] Training loss: 0.02547266, Validation loss: 0.02576694, Gradient norm: 1.75319477
INFO:root:[  288] Training loss: 0.02547433, Validation loss: 0.02529706, Gradient norm: 2.07873675
INFO:root:[  289] Training loss: 0.02530306, Validation loss: 0.02518773, Gradient norm: 1.49978733
INFO:root:[  290] Training loss: 0.02542163, Validation loss: 0.02549757, Gradient norm: 1.99650242
INFO:root:[  291] Training loss: 0.02536587, Validation loss: 0.02574105, Gradient norm: 1.85448944
INFO:root:[  292] Training loss: 0.02529585, Validation loss: 0.02591034, Gradient norm: 1.90434871
INFO:root:[  293] Training loss: 0.02534383, Validation loss: 0.02501223, Gradient norm: 2.00594445
INFO:root:[  294] Training loss: 0.02530925, Validation loss: 0.02505997, Gradient norm: 1.89137848
INFO:root:[  295] Training loss: 0.02524106, Validation loss: 0.02552831, Gradient norm: 1.69084860
INFO:root:[  296] Training loss: 0.02533927, Validation loss: 0.02577487, Gradient norm: 2.27689716
INFO:root:[  297] Training loss: 0.02513774, Validation loss: 0.02517981, Gradient norm: 1.75499618
INFO:root:[  298] Training loss: 0.02506827, Validation loss: 0.02495830, Gradient norm: 1.73098617
INFO:root:[  299] Training loss: 0.02498921, Validation loss: 0.02485249, Gradient norm: 1.42544053
INFO:root:[  300] Training loss: 0.02508832, Validation loss: 0.02552115, Gradient norm: 1.69604321
INFO:root:[  301] Training loss: 0.02500307, Validation loss: 0.02483070, Gradient norm: 1.70789500
INFO:root:[  302] Training loss: 0.02515272, Validation loss: 0.02485692, Gradient norm: 2.07277870
INFO:root:[  303] Training loss: 0.02509403, Validation loss: 0.02492168, Gradient norm: 2.15757646
INFO:root:[  304] Training loss: 0.02510766, Validation loss: 0.02488719, Gradient norm: 2.28963433
INFO:root:[  305] Training loss: 0.02502059, Validation loss: 0.02451363, Gradient norm: 2.24568729
INFO:root:[  306] Training loss: 0.02480307, Validation loss: 0.02471875, Gradient norm: 1.64323254
INFO:root:[  307] Training loss: 0.02505991, Validation loss: 0.02544646, Gradient norm: 2.38022193
INFO:root:[  308] Training loss: 0.02494447, Validation loss: 0.02488718, Gradient norm: 2.25466394
INFO:root:[  309] Training loss: 0.02486750, Validation loss: 0.02492811, Gradient norm: 2.07399815
INFO:root:[  310] Training loss: 0.02469210, Validation loss: 0.02478475, Gradient norm: 1.48008899
INFO:root:[  311] Training loss: 0.02474885, Validation loss: 0.02485447, Gradient norm: 1.85354773
INFO:root:[  312] Training loss: 0.02470094, Validation loss: 0.02474363, Gradient norm: 1.85787300
INFO:root:[  313] Training loss: 0.02466554, Validation loss: 0.02436963, Gradient norm: 1.81327991
INFO:root:[  314] Training loss: 0.02467574, Validation loss: 0.02496336, Gradient norm: 1.90895257
INFO:root:[  315] Training loss: 0.02462373, Validation loss: 0.02496583, Gradient norm: 1.89137124
INFO:root:[  316] Training loss: 0.02480157, Validation loss: 0.02442906, Gradient norm: 2.14218307
INFO:root:[  317] Training loss: 0.02465724, Validation loss: 0.02455503, Gradient norm: 1.97523139
INFO:root:[  318] Training loss: 0.02461302, Validation loss: 0.02427595, Gradient norm: 2.01491576
INFO:root:[  319] Training loss: 0.02455640, Validation loss: 0.02460747, Gradient norm: 1.96873792
INFO:root:[  320] Training loss: 0.02449616, Validation loss: 0.02499385, Gradient norm: 1.84306795
INFO:root:[  321] Training loss: 0.02457407, Validation loss: 0.02451243, Gradient norm: 2.24507460
INFO:root:[  322] Training loss: 0.02445599, Validation loss: 0.02429456, Gradient norm: 2.10502399
INFO:root:[  323] Training loss: 0.02437018, Validation loss: 0.02444911, Gradient norm: 1.69176637
INFO:root:[  324] Training loss: 0.02437369, Validation loss: 0.02451139, Gradient norm: 1.89049881
INFO:root:[  325] Training loss: 0.02446556, Validation loss: 0.02486667, Gradient norm: 2.22439485
INFO:root:[  326] Training loss: 0.02431146, Validation loss: 0.02411885, Gradient norm: 1.74754316
INFO:root:[  327] Training loss: 0.02441320, Validation loss: 0.02470296, Gradient norm: 2.34807113
INFO:root:[  328] Training loss: 0.02436809, Validation loss: 0.02430743, Gradient norm: 2.29228134
INFO:root:[  329] Training loss: 0.02425804, Validation loss: 0.02422075, Gradient norm: 1.93240938
INFO:root:[  330] Training loss: 0.02430461, Validation loss: 0.02428395, Gradient norm: 2.14833129
INFO:root:[  331] Training loss: 0.02412224, Validation loss: 0.02412728, Gradient norm: 1.63293743
INFO:root:[  332] Training loss: 0.02409866, Validation loss: 0.02463457, Gradient norm: 1.81897367
INFO:root:[  333] Training loss: 0.02438354, Validation loss: 0.02430835, Gradient norm: 2.71867294
INFO:root:[  334] Training loss: 0.02412850, Validation loss: 0.02431019, Gradient norm: 2.13403860
INFO:root:[  335] Training loss: 0.02415954, Validation loss: 0.02436635, Gradient norm: 2.39336811
INFO:root:[  336] Training loss: 0.02409122, Validation loss: 0.02380960, Gradient norm: 2.24947712
INFO:root:[  337] Training loss: 0.02412888, Validation loss: 0.02385103, Gradient norm: 2.44059187
INFO:root:[  338] Training loss: 0.02394637, Validation loss: 0.02423969, Gradient norm: 1.79873288
INFO:root:[  339] Training loss: 0.02389051, Validation loss: 0.02416541, Gradient norm: 1.89875124
INFO:root:[  340] Training loss: 0.02404650, Validation loss: 0.02402859, Gradient norm: 2.30678915
INFO:root:[  341] Training loss: 0.02396610, Validation loss: 0.02451053, Gradient norm: 2.30428822
INFO:root:[  342] Training loss: 0.02403724, Validation loss: 0.02384895, Gradient norm: 2.47958395
INFO:root:[  343] Training loss: 0.02379594, Validation loss: 0.02362367, Gradient norm: 1.66736738
INFO:root:[  344] Training loss: 0.02389583, Validation loss: 0.02349745, Gradient norm: 2.31456552
INFO:root:[  345] Training loss: 0.02392290, Validation loss: 0.02359351, Gradient norm: 2.50188005
INFO:root:[  346] Training loss: 0.02379055, Validation loss: 0.02350607, Gradient norm: 2.20475411
INFO:root:[  347] Training loss: 0.02376172, Validation loss: 0.02390049, Gradient norm: 1.97917755
INFO:root:[  348] Training loss: 0.02376339, Validation loss: 0.02377562, Gradient norm: 2.09471238
INFO:root:[  349] Training loss: 0.02382260, Validation loss: 0.02360144, Gradient norm: 2.29560414
INFO:root:[  350] Training loss: 0.02365751, Validation loss: 0.02368885, Gradient norm: 1.96097184
INFO:root:[  351] Training loss: 0.02359434, Validation loss: 0.02347665, Gradient norm: 2.02844618
INFO:root:[  352] Training loss: 0.02363822, Validation loss: 0.02324708, Gradient norm: 2.03847383
INFO:root:[  353] Training loss: 0.02363425, Validation loss: 0.02387962, Gradient norm: 2.29082674
INFO:root:[  354] Training loss: 0.02353866, Validation loss: 0.02336997, Gradient norm: 2.16364287
INFO:root:[  355] Training loss: 0.02360947, Validation loss: 0.02340829, Gradient norm: 2.18101921
INFO:root:[  356] Training loss: 0.02359405, Validation loss: 0.02391154, Gradient norm: 2.46081439
INFO:root:[  357] Training loss: 0.02359678, Validation loss: 0.02375301, Gradient norm: 2.41475764
INFO:root:[  358] Training loss: 0.02366121, Validation loss: 0.02356217, Gradient norm: 2.69781801
INFO:root:[  359] Training loss: 0.02349466, Validation loss: 0.02385632, Gradient norm: 1.92206546
INFO:root:[  360] Training loss: 0.02344924, Validation loss: 0.02353453, Gradient norm: 1.97817101
INFO:root:[  361] Training loss: 0.02348159, Validation loss: 0.02314883, Gradient norm: 2.42247176
INFO:root:[  362] Training loss: 0.02340290, Validation loss: 0.02384220, Gradient norm: 2.26429288
INFO:root:[  363] Training loss: 0.02343148, Validation loss: 0.02374074, Gradient norm: 2.37831476
INFO:root:[  364] Training loss: 0.02357439, Validation loss: 0.02342324, Gradient norm: 3.00795335
INFO:root:[  365] Training loss: 0.02358991, Validation loss: 0.02389859, Gradient norm: 3.13441993
INFO:root:[  366] Training loss: 0.02347314, Validation loss: 0.02335771, Gradient norm: 2.77450869
INFO:root:[  367] Training loss: 0.02337588, Validation loss: 0.02339530, Gradient norm: 2.49340723
INFO:root:[  368] Training loss: 0.02339116, Validation loss: 0.02341215, Gradient norm: 2.63466130
INFO:root:[  369] Training loss: 0.02339897, Validation loss: 0.02385927, Gradient norm: 2.85619361
INFO:root:[  370] Training loss: 0.02331887, Validation loss: 0.02311422, Gradient norm: 2.66097698
INFO:root:[  371] Training loss: 0.02313434, Validation loss: 0.02328994, Gradient norm: 2.09474812
INFO:root:[  372] Training loss: 0.02324093, Validation loss: 0.02328877, Gradient norm: 2.72973956
INFO:root:[  373] Training loss: 0.02326109, Validation loss: 0.02320753, Gradient norm: 2.66116379
INFO:root:[  374] Training loss: 0.02325074, Validation loss: 0.02298507, Gradient norm: 2.61417462
INFO:root:[  375] Training loss: 0.02310308, Validation loss: 0.02305748, Gradient norm: 2.11467538
INFO:root:[  376] Training loss: 0.02309673, Validation loss: 0.02362290, Gradient norm: 2.37102824
INFO:root:[  377] Training loss: 0.02333976, Validation loss: 0.02298786, Gradient norm: 3.19868827
INFO:root:[  378] Training loss: 0.02317122, Validation loss: 0.02328383, Gradient norm: 2.89829217
INFO:root:[  379] Training loss: 0.02295699, Validation loss: 0.02268396, Gradient norm: 2.17741399
INFO:root:[  380] Training loss: 0.02307827, Validation loss: 0.02285518, Gradient norm: 2.42819590
INFO:root:[  381] Training loss: 0.02287013, Validation loss: 0.02335932, Gradient norm: 2.21177372
INFO:root:[  382] Training loss: 0.02309167, Validation loss: 0.02349372, Gradient norm: 2.96775967
INFO:root:[  383] Training loss: 0.02297592, Validation loss: 0.02299454, Gradient norm: 2.37550030
INFO:root:[  384] Training loss: 0.02290621, Validation loss: 0.02290047, Gradient norm: 2.42787930
INFO:root:[  385] Training loss: 0.02293187, Validation loss: 0.02317142, Gradient norm: 2.55040478
INFO:root:[  386] Training loss: 0.02293180, Validation loss: 0.02295111, Gradient norm: 2.73207145
INFO:root:[  387] Training loss: 0.02288224, Validation loss: 0.02275957, Gradient norm: 2.58201289
INFO:root:[  388] Training loss: 0.02294657, Validation loss: 0.02273799, Gradient norm: 2.90258948
INFO:root:EP 388: Early stopping
INFO:root:Training the model took 12935.012s.
INFO:root:Emptying the cuda cache took 0.08s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.41654
INFO:root:EnergyScoreTrain: 0.36455
INFO:root:CoverageTrain: 0.99623
INFO:root:IntervalWidthTrain: 0.06609
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.39165
INFO:root:EnergyScoreValidation: 0.34238
INFO:root:CoverageValidation: 0.99622
INFO:root:IntervalWidthValidation: 0.06616
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.36799
INFO:root:EnergyScoreTest: 0.32294
INFO:root:CoverageTest: 0.99631
INFO:root:IntervalWidthTest: 0.06576
INFO:root:###4 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.52788902, Validation loss: 1.49276764, Gradient norm: 5.90031511
INFO:root:[    2] Training loss: 0.43417724, Validation loss: 0.22931706, Gradient norm: 3.80919066
INFO:root:[    3] Training loss: 0.21108744, Validation loss: 0.19579304, Gradient norm: 1.70286422
INFO:root:[    4] Training loss: 0.19657089, Validation loss: 0.18143613, Gradient norm: 1.44793352
INFO:root:[    5] Training loss: 0.18435502, Validation loss: 0.16100072, Gradient norm: 1.41915848
INFO:root:[    6] Training loss: 0.16626575, Validation loss: 0.15372870, Gradient norm: 1.10919829
INFO:root:[    7] Training loss: 0.15880600, Validation loss: 0.15195239, Gradient norm: 1.34334345
INFO:root:[    8] Training loss: 0.14504418, Validation loss: 0.13569899, Gradient norm: 1.26999729
INFO:root:[    9] Training loss: 0.14149483, Validation loss: 0.13111872, Gradient norm: 1.60452503
INFO:root:[   10] Training loss: 0.13485576, Validation loss: 0.12705785, Gradient norm: 1.25940733
INFO:root:[   11] Training loss: 0.12488090, Validation loss: 0.11959933, Gradient norm: 0.87723314
INFO:root:[   12] Training loss: 0.12014887, Validation loss: 0.11927662, Gradient norm: 1.11115368
INFO:root:[   13] Training loss: 0.11435776, Validation loss: 0.10614236, Gradient norm: 0.89674011
INFO:root:[   14] Training loss: 0.10926572, Validation loss: 0.10819031, Gradient norm: 0.75624946
INFO:root:[   15] Training loss: 0.10414759, Validation loss: 0.10339969, Gradient norm: 0.76877640
INFO:root:[   16] Training loss: 0.10006587, Validation loss: 0.09854915, Gradient norm: 0.94005901
INFO:root:[   17] Training loss: 0.09790474, Validation loss: 0.09333224, Gradient norm: 0.96390211
INFO:root:[   18] Training loss: 0.09355553, Validation loss: 0.08999323, Gradient norm: 0.70052145
INFO:root:[   19] Training loss: 0.09019790, Validation loss: 0.08912671, Gradient norm: 0.85576888
INFO:root:[   20] Training loss: 0.08727539, Validation loss: 0.08641683, Gradient norm: 0.65123292
INFO:root:[   21] Training loss: 0.08549834, Validation loss: 0.08280089, Gradient norm: 0.99756535
INFO:root:[   22] Training loss: 0.08123755, Validation loss: 0.07915139, Gradient norm: 0.51716562
INFO:root:[   23] Training loss: 0.07910592, Validation loss: 0.07749825, Gradient norm: 0.41840898
INFO:root:[   24] Training loss: 0.07796076, Validation loss: 0.07715062, Gradient norm: 0.53264410
INFO:root:[   25] Training loss: 0.07549531, Validation loss: 0.07388511, Gradient norm: 0.38781149
INFO:root:[   26] Training loss: 0.07399800, Validation loss: 0.07340127, Gradient norm: 0.58329137
INFO:root:[   27] Training loss: 0.07304605, Validation loss: 0.07273500, Gradient norm: 0.63363674
INFO:root:[   28] Training loss: 0.07108395, Validation loss: 0.07125544, Gradient norm: 0.47898695
INFO:root:[   29] Training loss: 0.06996437, Validation loss: 0.07077336, Gradient norm: 0.36272018
INFO:root:[   30] Training loss: 0.06929262, Validation loss: 0.06792286, Gradient norm: 0.60825557
INFO:root:[   31] Training loss: 0.06787731, Validation loss: 0.06683798, Gradient norm: 0.47978010
INFO:root:[   32] Training loss: 0.06700757, Validation loss: 0.06644664, Gradient norm: 0.49857313
INFO:root:[   33] Training loss: 0.06594466, Validation loss: 0.06534721, Gradient norm: 0.49482824
INFO:root:[   34] Training loss: 0.06490441, Validation loss: 0.06450380, Gradient norm: 0.38834499
INFO:root:[   35] Training loss: 0.06436244, Validation loss: 0.06432724, Gradient norm: 0.34321648
INFO:root:[   36] Training loss: 0.06358342, Validation loss: 0.06356491, Gradient norm: 0.39304103
INFO:root:[   37] Training loss: 0.06292389, Validation loss: 0.06232278, Gradient norm: 0.53662688
INFO:root:[   38] Training loss: 0.06238152, Validation loss: 0.06238946, Gradient norm: 0.42555589
INFO:root:[   39] Training loss: 0.06132184, Validation loss: 0.06154113, Gradient norm: 0.36471599
INFO:root:[   40] Training loss: 0.06077774, Validation loss: 0.06037629, Gradient norm: 0.36000513
INFO:root:[   41] Training loss: 0.06033037, Validation loss: 0.06059991, Gradient norm: 0.44215135
INFO:root:[   42] Training loss: 0.05977455, Validation loss: 0.05937948, Gradient norm: 0.39122842
INFO:root:[   43] Training loss: 0.05924593, Validation loss: 0.05865924, Gradient norm: 0.35363932
INFO:root:[   44] Training loss: 0.05909844, Validation loss: 0.06015666, Gradient norm: 0.48125686
INFO:root:[   45] Training loss: 0.05865264, Validation loss: 0.05855218, Gradient norm: 0.52440935
INFO:root:[   46] Training loss: 0.05787002, Validation loss: 0.05750750, Gradient norm: 0.41903036
INFO:root:[   47] Training loss: 0.05742309, Validation loss: 0.05757764, Gradient norm: 0.41351338
INFO:root:[   48] Training loss: 0.05686301, Validation loss: 0.05672868, Gradient norm: 0.29089619
INFO:root:[   49] Training loss: 0.05645196, Validation loss: 0.05631997, Gradient norm: 0.24412399
INFO:root:[   50] Training loss: 0.05622200, Validation loss: 0.05709538, Gradient norm: 0.34623246
INFO:root:[   51] Training loss: 0.05668879, Validation loss: 0.05648750, Gradient norm: 0.80187407
INFO:root:[   52] Training loss: 0.05558541, Validation loss: 0.05528848, Gradient norm: 0.47742240
INFO:root:[   53] Training loss: 0.05580172, Validation loss: 0.05458957, Gradient norm: 0.68497312
INFO:root:[   54] Training loss: 0.05508938, Validation loss: 0.05456406, Gradient norm: 0.53015064
INFO:root:[   55] Training loss: 0.05459862, Validation loss: 0.05448495, Gradient norm: 0.41003250
INFO:root:[   56] Training loss: 0.05415896, Validation loss: 0.05404791, Gradient norm: 0.34290673
INFO:root:[   57] Training loss: 0.05383931, Validation loss: 0.05357253, Gradient norm: 0.30202084
INFO:root:[   58] Training loss: 0.05377523, Validation loss: 0.05355986, Gradient norm: 0.39769333
INFO:root:[   59] Training loss: 0.05347163, Validation loss: 0.05318277, Gradient norm: 0.51962993
INFO:root:[   60] Training loss: 0.05312107, Validation loss: 0.05329693, Gradient norm: 0.36667689
INFO:root:[   61] Training loss: 0.05285112, Validation loss: 0.05269083, Gradient norm: 0.40845179
INFO:root:[   62] Training loss: 0.05227725, Validation loss: 0.05236895, Gradient norm: 0.20584638
INFO:root:[   63] Training loss: 0.05243403, Validation loss: 0.05211985, Gradient norm: 0.46746810
INFO:root:[   64] Training loss: 0.05238559, Validation loss: 0.05216712, Gradient norm: 0.59282966
INFO:root:[   65] Training loss: 0.05209434, Validation loss: 0.05278335, Gradient norm: 0.56269165
INFO:root:[   66] Training loss: 0.05197276, Validation loss: 0.05144852, Gradient norm: 0.65702502
INFO:root:[   67] Training loss: 0.05153234, Validation loss: 0.05167875, Gradient norm: 0.49634502
INFO:root:[   68] Training loss: 0.05154852, Validation loss: 0.05140933, Gradient norm: 0.60810671
INFO:root:[   69] Training loss: 0.05120106, Validation loss: 0.05157917, Gradient norm: 0.54066875
INFO:root:[   70] Training loss: 0.05103966, Validation loss: 0.05124247, Gradient norm: 0.58852505
INFO:root:[   71] Training loss: 0.05074412, Validation loss: 0.05023778, Gradient norm: 0.52889381
INFO:root:[   72] Training loss: 0.05037716, Validation loss: 0.05024085, Gradient norm: 0.48716062
INFO:root:[   73] Training loss: 0.05008857, Validation loss: 0.05011216, Gradient norm: 0.39362203
INFO:root:[   74] Training loss: 0.04988500, Validation loss: 0.04971288, Gradient norm: 0.30572324
INFO:root:[   75] Training loss: 0.04976876, Validation loss: 0.05031101, Gradient norm: 0.42092421
INFO:root:[   76] Training loss: 0.04995701, Validation loss: 0.04975386, Gradient norm: 0.60021405
INFO:root:[   77] Training loss: 0.04953242, Validation loss: 0.04970109, Gradient norm: 0.45829334
INFO:root:[   78] Training loss: 0.04941531, Validation loss: 0.04976372, Gradient norm: 0.49104616
INFO:root:[   79] Training loss: 0.04942795, Validation loss: 0.04994412, Gradient norm: 0.63905598
INFO:root:[   80] Training loss: 0.04892503, Validation loss: 0.04866825, Gradient norm: 0.36217731
INFO:root:[   81] Training loss: 0.04887449, Validation loss: 0.04883097, Gradient norm: 0.52247378
INFO:root:[   82] Training loss: 0.04863826, Validation loss: 0.04841843, Gradient norm: 0.45643962
INFO:root:[   83] Training loss: 0.04874384, Validation loss: 0.04839519, Gradient norm: 0.61947040
INFO:root:[   84] Training loss: 0.04853493, Validation loss: 0.04845308, Gradient norm: 0.65624768
INFO:root:[   85] Training loss: 0.04810362, Validation loss: 0.04777107, Gradient norm: 0.35645286
INFO:root:[   86] Training loss: 0.04795856, Validation loss: 0.04950522, Gradient norm: 0.40864672
INFO:root:[   87] Training loss: 0.04809154, Validation loss: 0.04861038, Gradient norm: 0.67176587
INFO:root:[   88] Training loss: 0.04785280, Validation loss: 0.04737227, Gradient norm: 0.53538797
INFO:root:[   89] Training loss: 0.04731339, Validation loss: 0.04732319, Gradient norm: 0.24733572
INFO:root:[   90] Training loss: 0.04722027, Validation loss: 0.04713640, Gradient norm: 0.31380250
INFO:root:[   91] Training loss: 0.04717061, Validation loss: 0.04706409, Gradient norm: 0.44131974
INFO:root:[   92] Training loss: 0.04756106, Validation loss: 0.04800497, Gradient norm: 0.79714409
INFO:root:[   93] Training loss: 0.04718662, Validation loss: 0.04722865, Gradient norm: 0.67170400
INFO:root:[   94] Training loss: 0.04671949, Validation loss: 0.04635487, Gradient norm: 0.46938594
INFO:root:[   95] Training loss: 0.04648534, Validation loss: 0.04648614, Gradient norm: 0.33663312
INFO:root:[   96] Training loss: 0.04634975, Validation loss: 0.04646471, Gradient norm: 0.29099579
INFO:root:[   97] Training loss: 0.04669313, Validation loss: 0.04652012, Gradient norm: 0.69932182
INFO:root:[   98] Training loss: 0.04641990, Validation loss: 0.04644682, Gradient norm: 0.62324454
INFO:root:[   99] Training loss: 0.04599880, Validation loss: 0.04576804, Gradient norm: 0.42119541
INFO:root:[  100] Training loss: 0.04604003, Validation loss: 0.04594939, Gradient norm: 0.57759946
INFO:root:[  101] Training loss: 0.04596471, Validation loss: 0.04587665, Gradient norm: 0.65243237
INFO:root:[  102] Training loss: 0.04592702, Validation loss: 0.04661253, Gradient norm: 0.66771932
INFO:root:[  103] Training loss: 0.04587989, Validation loss: 0.04647971, Gradient norm: 0.73772426
INFO:root:[  104] Training loss: 0.04580222, Validation loss: 0.04600465, Gradient norm: 0.76157129
INFO:root:[  105] Training loss: 0.04545352, Validation loss: 0.04562521, Gradient norm: 0.59964100
INFO:root:[  106] Training loss: 0.04542220, Validation loss: 0.04491257, Gradient norm: 0.68427137
INFO:root:[  107] Training loss: 0.04508965, Validation loss: 0.04499934, Gradient norm: 0.53227922
INFO:root:[  108] Training loss: 0.04491864, Validation loss: 0.04476041, Gradient norm: 0.49546206
INFO:root:[  109] Training loss: 0.04509840, Validation loss: 0.04572122, Gradient norm: 0.69619426
INFO:root:[  110] Training loss: 0.04484166, Validation loss: 0.04462105, Gradient norm: 0.60704741
INFO:root:[  111] Training loss: 0.04438307, Validation loss: 0.04439034, Gradient norm: 0.29830583
INFO:root:[  112] Training loss: 0.04434630, Validation loss: 0.04436714, Gradient norm: 0.32460584
INFO:root:[  113] Training loss: 0.04452252, Validation loss: 0.04424787, Gradient norm: 0.64129785
INFO:root:[  114] Training loss: 0.04427579, Validation loss: 0.04417485, Gradient norm: 0.55001579
INFO:root:[  115] Training loss: 0.04451109, Validation loss: 0.04544478, Gradient norm: 0.78658600
INFO:root:[  116] Training loss: 0.04430868, Validation loss: 0.04419852, Gradient norm: 0.75327007
INFO:root:[  117] Training loss: 0.04398131, Validation loss: 0.04385469, Gradient norm: 0.59373980
INFO:root:[  118] Training loss: 0.04355769, Validation loss: 0.04357170, Gradient norm: 0.21517707
INFO:root:[  119] Training loss: 0.04363199, Validation loss: 0.04341542, Gradient norm: 0.41228949
INFO:root:[  120] Training loss: 0.04352638, Validation loss: 0.04346786, Gradient norm: 0.50423254
INFO:root:[  121] Training loss: 0.04351871, Validation loss: 0.04335611, Gradient norm: 0.59636699
INFO:root:[  122] Training loss: 0.04373840, Validation loss: 0.04380772, Gradient norm: 0.81952316
INFO:root:[  123] Training loss: 0.04333513, Validation loss: 0.04292429, Gradient norm: 0.61320752
INFO:root:[  124] Training loss: 0.04290700, Validation loss: 0.04276483, Gradient norm: 0.33535197
INFO:root:[  125] Training loss: 0.04280283, Validation loss: 0.04292083, Gradient norm: 0.28946756
INFO:root:[  126] Training loss: 0.04326715, Validation loss: 0.04285654, Gradient norm: 0.78457205
INFO:root:[  127] Training loss: 0.04278149, Validation loss: 0.04261087, Gradient norm: 0.44031664
INFO:root:[  128] Training loss: 0.04295380, Validation loss: 0.04317723, Gradient norm: 0.69359591
INFO:root:[  129] Training loss: 0.04294884, Validation loss: 0.04258341, Gradient norm: 0.80724403
INFO:root:[  130] Training loss: 0.04241096, Validation loss: 0.04279205, Gradient norm: 0.47405755
INFO:root:[  131] Training loss: 0.04239226, Validation loss: 0.04196193, Gradient norm: 0.49380600
INFO:root:[  132] Training loss: 0.04224238, Validation loss: 0.04251387, Gradient norm: 0.53535001
INFO:root:[  133] Training loss: 0.04251113, Validation loss: 0.04243885, Gradient norm: 0.81167137
INFO:root:[  134] Training loss: 0.04205097, Validation loss: 0.04194691, Gradient norm: 0.53266704
INFO:root:[  135] Training loss: 0.04174054, Validation loss: 0.04175738, Gradient norm: 0.23459301
INFO:root:[  136] Training loss: 0.04187340, Validation loss: 0.04210644, Gradient norm: 0.49900087
INFO:root:[  137] Training loss: 0.04217023, Validation loss: 0.04258122, Gradient norm: 0.84417050
INFO:root:[  138] Training loss: 0.04206759, Validation loss: 0.04181010, Gradient norm: 0.83387104
INFO:root:[  139] Training loss: 0.04168206, Validation loss: 0.04148222, Gradient norm: 0.65116466
INFO:root:[  140] Training loss: 0.04162001, Validation loss: 0.04174959, Gradient norm: 0.71516121
INFO:root:[  141] Training loss: 0.04152768, Validation loss: 0.04141482, Gradient norm: 0.71524042
INFO:root:[  142] Training loss: 0.04135760, Validation loss: 0.04132180, Gradient norm: 0.64370772
INFO:root:[  143] Training loss: 0.04130242, Validation loss: 0.04134424, Gradient norm: 0.66287317
INFO:root:[  144] Training loss: 0.04104344, Validation loss: 0.04111368, Gradient norm: 0.51444484
INFO:root:[  145] Training loss: 0.04082478, Validation loss: 0.04087435, Gradient norm: 0.33834497
INFO:root:[  146] Training loss: 0.04086422, Validation loss: 0.04153847, Gradient norm: 0.40603936
INFO:root:[  147] Training loss: 0.04103568, Validation loss: 0.04061513, Gradient norm: 0.73974859
INFO:root:[  148] Training loss: 0.04083171, Validation loss: 0.04083393, Gradient norm: 0.70807791
INFO:root:[  149] Training loss: 0.04068302, Validation loss: 0.04034416, Gradient norm: 0.63767402
INFO:root:[  150] Training loss: 0.04065006, Validation loss: 0.04134658, Gradient norm: 0.69295638
INFO:root:[  151] Training loss: 0.04078205, Validation loss: 0.04163256, Gradient norm: 0.88596666
INFO:root:[  152] Training loss: 0.04058793, Validation loss: 0.04109377, Gradient norm: 0.78284311
INFO:root:[  153] Training loss: 0.04030944, Validation loss: 0.04028133, Gradient norm: 0.58321756
INFO:root:[  154] Training loss: 0.04012909, Validation loss: 0.04069772, Gradient norm: 0.56829093
INFO:root:[  155] Training loss: 0.04012941, Validation loss: 0.04132723, Gradient norm: 0.63887703
INFO:root:[  156] Training loss: 0.04031818, Validation loss: 0.04068159, Gradient norm: 0.85842216
INFO:root:[  157] Training loss: 0.03984577, Validation loss: 0.03955194, Gradient norm: 0.53964499
INFO:root:[  158] Training loss: 0.03990847, Validation loss: 0.03967347, Gradient norm: 0.67074149
INFO:root:[  159] Training loss: 0.03974336, Validation loss: 0.03990835, Gradient norm: 0.65360449
INFO:root:[  160] Training loss: 0.03947305, Validation loss: 0.04082577, Gradient norm: 0.42019998
INFO:root:[  161] Training loss: 0.03968863, Validation loss: 0.04017479, Gradient norm: 0.78868820
INFO:root:[  162] Training loss: 0.03980711, Validation loss: 0.03979329, Gradient norm: 0.89806822
INFO:root:[  163] Training loss: 0.03951067, Validation loss: 0.03929397, Gradient norm: 0.72845772
INFO:root:[  164] Training loss: 0.03933021, Validation loss: 0.03910148, Gradient norm: 0.61991198
INFO:root:[  165] Training loss: 0.03919998, Validation loss: 0.03955243, Gradient norm: 0.62784435
INFO:root:[  166] Training loss: 0.03931700, Validation loss: 0.03896291, Gradient norm: 0.80371296
INFO:root:[  167] Training loss: 0.03914565, Validation loss: 0.03963282, Gradient norm: 0.74403686
INFO:root:[  168] Training loss: 0.03913052, Validation loss: 0.03879301, Gradient norm: 0.79099961
INFO:root:[  169] Training loss: 0.03871755, Validation loss: 0.03849153, Gradient norm: 0.46219282
INFO:root:[  170] Training loss: 0.03859454, Validation loss: 0.03885035, Gradient norm: 0.39541209
INFO:root:[  171] Training loss: 0.03882506, Validation loss: 0.03853110, Gradient norm: 0.74166042
INFO:root:[  172] Training loss: 0.03860240, Validation loss: 0.03835586, Gradient norm: 0.65883959
INFO:root:[  173] Training loss: 0.03852105, Validation loss: 0.03890281, Gradient norm: 0.64084125
INFO:root:[  174] Training loss: 0.03844274, Validation loss: 0.03839821, Gradient norm: 0.67126120
INFO:root:[  175] Training loss: 0.03848908, Validation loss: 0.03876272, Gradient norm: 0.83825090
INFO:root:[  176] Training loss: 0.03825187, Validation loss: 0.03845989, Gradient norm: 0.63466594
INFO:root:[  177] Training loss: 0.03827909, Validation loss: 0.03787333, Gradient norm: 0.77059233
INFO:root:[  178] Training loss: 0.03825639, Validation loss: 0.03792215, Gradient norm: 0.76260414
INFO:root:[  179] Training loss: 0.03777868, Validation loss: 0.03768556, Gradient norm: 0.28808555
INFO:root:[  180] Training loss: 0.03767914, Validation loss: 0.03755419, Gradient norm: 0.33001128
INFO:root:[  181] Training loss: 0.03772729, Validation loss: 0.03757496, Gradient norm: 0.54870034
INFO:root:[  182] Training loss: 0.03791834, Validation loss: 0.03756123, Gradient norm: 0.87174983
INFO:root:[  183] Training loss: 0.03792051, Validation loss: 0.03818564, Gradient norm: 0.93262397
INFO:root:[  184] Training loss: 0.03770550, Validation loss: 0.03768130, Gradient norm: 0.85990330
INFO:root:[  185] Training loss: 0.03751606, Validation loss: 0.03776333, Gradient norm: 0.69527223
INFO:root:[  186] Training loss: 0.03744997, Validation loss: 0.03760591, Gradient norm: 0.78548135
INFO:root:[  187] Training loss: 0.03726025, Validation loss: 0.03699965, Gradient norm: 0.63301119
INFO:root:[  188] Training loss: 0.03728146, Validation loss: 0.03818349, Gradient norm: 0.65622427
INFO:root:[  189] Training loss: 0.03707579, Validation loss: 0.03712286, Gradient norm: 0.54271719
INFO:root:[  190] Training loss: 0.03701301, Validation loss: 0.03739321, Gradient norm: 0.64808609
INFO:root:[  191] Training loss: 0.03698470, Validation loss: 0.03744935, Gradient norm: 0.74387750
INFO:root:[  192] Training loss: 0.03687698, Validation loss: 0.03734638, Gradient norm: 0.71560664
INFO:root:[  193] Training loss: 0.03691770, Validation loss: 0.03678736, Gradient norm: 0.85812981
INFO:root:[  194] Training loss: 0.03683141, Validation loss: 0.03686701, Gradient norm: 0.78122454
INFO:root:[  195] Training loss: 0.03685814, Validation loss: 0.03649846, Gradient norm: 0.93195830
INFO:root:[  196] Training loss: 0.03663969, Validation loss: 0.03695027, Gradient norm: 0.83754678
INFO:root:[  197] Training loss: 0.03667064, Validation loss: 0.03683251, Gradient norm: 0.96645435
INFO:root:[  198] Training loss: 0.03618042, Validation loss: 0.03602574, Gradient norm: 0.40506974
INFO:root:[  199] Training loss: 0.03605454, Validation loss: 0.03620569, Gradient norm: 0.35184749
INFO:root:[  200] Training loss: 0.03590761, Validation loss: 0.03609904, Gradient norm: 0.30835585
INFO:root:[  201] Training loss: 0.03593720, Validation loss: 0.03653802, Gradient norm: 0.37728776
INFO:root:[  202] Training loss: 0.03616719, Validation loss: 0.03595128, Gradient norm: 0.86064014
INFO:root:[  203] Training loss: 0.03631146, Validation loss: 0.03654260, Gradient norm: 1.03989345
INFO:root:[  204] Training loss: 0.03601124, Validation loss: 0.03605666, Gradient norm: 0.90391152
INFO:root:[  205] Training loss: 0.03583146, Validation loss: 0.03557847, Gradient norm: 0.77262840
INFO:root:[  206] Training loss: 0.03586641, Validation loss: 0.03649306, Gradient norm: 0.92160757
INFO:root:[  207] Training loss: 0.03580155, Validation loss: 0.03530771, Gradient norm: 0.94212011
INFO:root:[  208] Training loss: 0.03550308, Validation loss: 0.03527829, Gradient norm: 0.71492607
INFO:root:[  209] Training loss: 0.03538530, Validation loss: 0.03566349, Gradient norm: 0.62726310
INFO:root:[  210] Training loss: 0.03548545, Validation loss: 0.03509590, Gradient norm: 0.89038107
INFO:root:[  211] Training loss: 0.03550671, Validation loss: 0.03648909, Gradient norm: 0.93846533
INFO:root:[  212] Training loss: 0.03543652, Validation loss: 0.03500077, Gradient norm: 0.92409245
INFO:root:[  213] Training loss: 0.03512353, Validation loss: 0.03515739, Gradient norm: 0.71281751
INFO:root:[  214] Training loss: 0.03511790, Validation loss: 0.03553776, Gradient norm: 0.82570819
INFO:root:[  215] Training loss: 0.03497655, Validation loss: 0.03535305, Gradient norm: 0.69385060
INFO:root:[  216] Training loss: 0.03508062, Validation loss: 0.03512904, Gradient norm: 0.92819396
INFO:root:[  217] Training loss: 0.03495777, Validation loss: 0.03459309, Gradient norm: 0.95564311
INFO:root:[  218] Training loss: 0.03483080, Validation loss: 0.03494971, Gradient norm: 0.84212311
INFO:root:[  219] Training loss: 0.03470790, Validation loss: 0.03480846, Gradient norm: 0.84534762
INFO:root:[  220] Training loss: 0.03441463, Validation loss: 0.03439890, Gradient norm: 0.48635880
INFO:root:[  221] Training loss: 0.03445734, Validation loss: 0.03447765, Gradient norm: 0.70144665
INFO:root:[  222] Training loss: 0.03454508, Validation loss: 0.03413179, Gradient norm: 0.90390715
INFO:root:[  223] Training loss: 0.03440277, Validation loss: 0.03408170, Gradient norm: 0.81118471
INFO:root:[  224] Training loss: 0.03441959, Validation loss: 0.03426409, Gradient norm: 0.92331159
INFO:root:[  225] Training loss: 0.03431247, Validation loss: 0.03465794, Gradient norm: 0.94974041
INFO:root:[  226] Training loss: 0.03423460, Validation loss: 0.03425327, Gradient norm: 0.94470547
INFO:root:[  227] Training loss: 0.03412583, Validation loss: 0.03408210, Gradient norm: 0.87193642
INFO:root:[  228] Training loss: 0.03390231, Validation loss: 0.03381547, Gradient norm: 0.75270418
INFO:root:[  229] Training loss: 0.03399368, Validation loss: 0.03388886, Gradient norm: 0.92872821
INFO:root:[  230] Training loss: 0.03390082, Validation loss: 0.03441064, Gradient norm: 0.89638378
INFO:root:[  231] Training loss: 0.03386239, Validation loss: 0.03380805, Gradient norm: 0.93203130
INFO:root:[  232] Training loss: 0.03362068, Validation loss: 0.03365093, Gradient norm: 0.71483649
INFO:root:[  233] Training loss: 0.03365651, Validation loss: 0.03403000, Gradient norm: 0.89072554
INFO:root:[  234] Training loss: 0.03367497, Validation loss: 0.03338990, Gradient norm: 0.99359633
INFO:root:[  235] Training loss: 0.03359201, Validation loss: 0.03314519, Gradient norm: 0.96068410
INFO:root:[  236] Training loss: 0.03333544, Validation loss: 0.03304979, Gradient norm: 0.72750424
INFO:root:[  237] Training loss: 0.03326087, Validation loss: 0.03355733, Gradient norm: 0.72396564
INFO:root:[  238] Training loss: 0.03319635, Validation loss: 0.03298628, Gradient norm: 0.79695374
INFO:root:[  239] Training loss: 0.03329721, Validation loss: 0.03375900, Gradient norm: 0.98298694
INFO:root:[  240] Training loss: 0.03317935, Validation loss: 0.03332598, Gradient norm: 0.92917159
INFO:root:[  241] Training loss: 0.03292331, Validation loss: 0.03277004, Gradient norm: 0.75455372
INFO:root:[  242] Training loss: 0.03288805, Validation loss: 0.03310718, Gradient norm: 0.76973402
INFO:root:[  243] Training loss: 0.03285062, Validation loss: 0.03288048, Gradient norm: 0.81368481
INFO:root:[  244] Training loss: 0.03290141, Validation loss: 0.03250791, Gradient norm: 0.96869531
INFO:root:[  245] Training loss: 0.03288998, Validation loss: 0.03247886, Gradient norm: 1.02991122
INFO:root:[  246] Training loss: 0.03275389, Validation loss: 0.03263990, Gradient norm: 0.94532539
INFO:root:[  247] Training loss: 0.03259377, Validation loss: 0.03232773, Gradient norm: 0.88826564
INFO:root:[  248] Training loss: 0.03252492, Validation loss: 0.03241744, Gradient norm: 0.85818135
INFO:root:[  249] Training loss: 0.03257189, Validation loss: 0.03246611, Gradient norm: 0.98599686
INFO:root:[  250] Training loss: 0.03228607, Validation loss: 0.03245591, Gradient norm: 0.72506980
INFO:root:[  251] Training loss: 0.03238746, Validation loss: 0.03213064, Gradient norm: 0.98488107
INFO:root:[  252] Training loss: 0.03217918, Validation loss: 0.03239134, Gradient norm: 0.79989585
INFO:root:[  253] Training loss: 0.03224254, Validation loss: 0.03225786, Gradient norm: 1.01734960
INFO:root:[  254] Training loss: 0.03219827, Validation loss: 0.03185687, Gradient norm: 1.03342684
INFO:root:[  255] Training loss: 0.03215445, Validation loss: 0.03166523, Gradient norm: 1.01560738
INFO:root:[  256] Training loss: 0.03206085, Validation loss: 0.03185796, Gradient norm: 1.05149172
INFO:root:[  257] Training loss: 0.03201072, Validation loss: 0.03239404, Gradient norm: 1.04140058
INFO:root:[  258] Training loss: 0.03192446, Validation loss: 0.03244327, Gradient norm: 1.00622669
INFO:root:[  259] Training loss: 0.03183064, Validation loss: 0.03188063, Gradient norm: 1.02515815
INFO:root:[  260] Training loss: 0.03177989, Validation loss: 0.03167412, Gradient norm: 1.03794171
INFO:root:[  261] Training loss: 0.03165267, Validation loss: 0.03214291, Gradient norm: 0.93864730
INFO:root:[  262] Training loss: 0.03158698, Validation loss: 0.03163218, Gradient norm: 0.93685185
INFO:root:[  263] Training loss: 0.03160903, Validation loss: 0.03143931, Gradient norm: 1.05648412
INFO:root:[  264] Training loss: 0.03151961, Validation loss: 0.03110869, Gradient norm: 1.04462900
INFO:root:[  265] Training loss: 0.03146931, Validation loss: 0.03123464, Gradient norm: 1.02732991
INFO:root:[  266] Training loss: 0.03123734, Validation loss: 0.03102857, Gradient norm: 0.87255340
INFO:root:[  267] Training loss: 0.03116516, Validation loss: 0.03103644, Gradient norm: 0.84306273
INFO:root:[  268] Training loss: 0.03120364, Validation loss: 0.03114274, Gradient norm: 0.97953358
INFO:root:[  269] Training loss: 0.03102159, Validation loss: 0.03105391, Gradient norm: 0.80377068
INFO:root:[  270] Training loss: 0.03088625, Validation loss: 0.03118777, Gradient norm: 0.72667482
INFO:root:[  271] Training loss: 0.03088340, Validation loss: 0.03053317, Gradient norm: 0.89223067
INFO:root:[  272] Training loss: 0.03080771, Validation loss: 0.03065847, Gradient norm: 0.85086981
INFO:root:[  273] Training loss: 0.03085081, Validation loss: 0.03082269, Gradient norm: 0.97899540
INFO:root:[  274] Training loss: 0.03071724, Validation loss: 0.03078567, Gradient norm: 0.92090859
INFO:root:[  275] Training loss: 0.03062790, Validation loss: 0.03091343, Gradient norm: 0.86206874
INFO:root:[  276] Training loss: 0.03070182, Validation loss: 0.03054032, Gradient norm: 1.06238995
INFO:root:[  277] Training loss: 0.03049658, Validation loss: 0.03082902, Gradient norm: 0.92403706
INFO:root:[  278] Training loss: 0.03037596, Validation loss: 0.03076247, Gradient norm: 0.86907062
INFO:root:[  279] Training loss: 0.03036540, Validation loss: 0.03018474, Gradient norm: 0.87856603
INFO:root:[  280] Training loss: 0.03033024, Validation loss: 0.03033311, Gradient norm: 0.94652902
INFO:root:[  281] Training loss: 0.03039337, Validation loss: 0.03012856, Gradient norm: 1.13960152
INFO:root:[  282] Training loss: 0.03027235, Validation loss: 0.03058306, Gradient norm: 1.13131341
INFO:root:[  283] Training loss: 0.03010000, Validation loss: 0.02978815, Gradient norm: 0.98055907
INFO:root:[  284] Training loss: 0.02999116, Validation loss: 0.03031450, Gradient norm: 0.87440605
INFO:root:[  285] Training loss: 0.03006377, Validation loss: 0.02971070, Gradient norm: 1.10244261
INFO:root:[  286] Training loss: 0.03007006, Validation loss: 0.02955276, Gradient norm: 1.21660254
INFO:root:[  287] Training loss: 0.02991342, Validation loss: 0.02975814, Gradient norm: 1.05552583
INFO:root:[  288] Training loss: 0.02990806, Validation loss: 0.03039124, Gradient norm: 1.17126610
INFO:root:[  289] Training loss: 0.02976021, Validation loss: 0.02983768, Gradient norm: 1.05554367
INFO:root:[  290] Training loss: 0.02955798, Validation loss: 0.02951667, Gradient norm: 0.72082098
INFO:root:[  291] Training loss: 0.02948218, Validation loss: 0.02993375, Gradient norm: 0.79899840
INFO:root:[  292] Training loss: 0.02964202, Validation loss: 0.02991964, Gradient norm: 1.23450259
INFO:root:[  293] Training loss: 0.02952257, Validation loss: 0.02957225, Gradient norm: 1.02790665
INFO:root:[  294] Training loss: 0.02935866, Validation loss: 0.02925212, Gradient norm: 0.93854102
INFO:root:[  295] Training loss: 0.02930542, Validation loss: 0.02985765, Gradient norm: 0.98662528
INFO:root:[  296] Training loss: 0.02931370, Validation loss: 0.02956416, Gradient norm: 1.00215939
INFO:root:[  297] Training loss: 0.02937967, Validation loss: 0.02972360, Gradient norm: 1.26418029
INFO:root:[  298] Training loss: 0.02924736, Validation loss: 0.02914975, Gradient norm: 1.19284826
INFO:root:[  299] Training loss: 0.02912541, Validation loss: 0.02880704, Gradient norm: 1.09845344
INFO:root:[  300] Training loss: 0.02919572, Validation loss: 0.02897403, Gradient norm: 1.20357550
INFO:root:[  301] Training loss: 0.02898819, Validation loss: 0.02916679, Gradient norm: 1.11800044
INFO:root:[  302] Training loss: 0.02892907, Validation loss: 0.02872468, Gradient norm: 1.08217288
INFO:root:[  303] Training loss: 0.02888508, Validation loss: 0.02858272, Gradient norm: 1.08429098
INFO:root:[  304] Training loss: 0.02893118, Validation loss: 0.02867229, Gradient norm: 1.23586211
INFO:root:[  305] Training loss: 0.02870226, Validation loss: 0.02857899, Gradient norm: 1.00439559
INFO:root:[  306] Training loss: 0.02867269, Validation loss: 0.02944722, Gradient norm: 1.09338601
INFO:root:[  307] Training loss: 0.02856595, Validation loss: 0.02837324, Gradient norm: 0.97866720
INFO:root:[  308] Training loss: 0.02841145, Validation loss: 0.02878603, Gradient norm: 0.83981924
INFO:root:[  309] Training loss: 0.02847089, Validation loss: 0.02867309, Gradient norm: 1.00310729
INFO:root:[  310] Training loss: 0.02850548, Validation loss: 0.02826113, Gradient norm: 1.14137662
INFO:root:[  311] Training loss: 0.02840407, Validation loss: 0.02872195, Gradient norm: 1.16408382
INFO:root:[  312] Training loss: 0.02828063, Validation loss: 0.02839308, Gradient norm: 0.95290010
INFO:root:[  313] Training loss: 0.02820560, Validation loss: 0.02850391, Gradient norm: 1.05593917
INFO:root:[  314] Training loss: 0.02813142, Validation loss: 0.02831396, Gradient norm: 0.98950431
INFO:root:[  315] Training loss: 0.02813273, Validation loss: 0.02826201, Gradient norm: 1.08175042
INFO:root:[  316] Training loss: 0.02810672, Validation loss: 0.02810027, Gradient norm: 1.15817929
INFO:root:[  317] Training loss: 0.02801296, Validation loss: 0.02824911, Gradient norm: 1.07472512
INFO:root:[  318] Training loss: 0.02793577, Validation loss: 0.02797872, Gradient norm: 0.95047997
INFO:root:[  319] Training loss: 0.02787128, Validation loss: 0.02783013, Gradient norm: 0.97164442
INFO:root:[  320] Training loss: 0.02778157, Validation loss: 0.02845929, Gradient norm: 0.95974704
INFO:root:[  321] Training loss: 0.02787343, Validation loss: 0.02764049, Gradient norm: 1.24858761
INFO:root:[  322] Training loss: 0.02770678, Validation loss: 0.02744777, Gradient norm: 1.07991326
INFO:root:[  323] Training loss: 0.02772158, Validation loss: 0.02809311, Gradient norm: 1.20870642
INFO:root:[  324] Training loss: 0.02755757, Validation loss: 0.02742338, Gradient norm: 1.05010609
INFO:root:[  325] Training loss: 0.02752959, Validation loss: 0.02768244, Gradient norm: 1.10017214
INFO:root:[  326] Training loss: 0.02754962, Validation loss: 0.02751682, Gradient norm: 1.23533095
INFO:root:[  327] Training loss: 0.02738564, Validation loss: 0.02732600, Gradient norm: 1.10085526
INFO:root:[  328] Training loss: 0.02741454, Validation loss: 0.02735146, Gradient norm: 1.10638150
INFO:root:[  329] Training loss: 0.02736363, Validation loss: 0.02768926, Gradient norm: 1.18879887
INFO:root:[  330] Training loss: 0.02728951, Validation loss: 0.02717599, Gradient norm: 1.09265695
INFO:root:[  331] Training loss: 0.02717976, Validation loss: 0.02699916, Gradient norm: 0.99984608
INFO:root:[  332] Training loss: 0.02718841, Validation loss: 0.02714107, Gradient norm: 1.23316223
INFO:root:[  333] Training loss: 0.02714537, Validation loss: 0.02720661, Gradient norm: 1.20705722
INFO:root:[  334] Training loss: 0.02708331, Validation loss: 0.02708664, Gradient norm: 1.15555499
INFO:root:[  335] Training loss: 0.02700160, Validation loss: 0.02669425, Gradient norm: 1.07628949
INFO:root:[  336] Training loss: 0.02685075, Validation loss: 0.02755184, Gradient norm: 0.84371481
INFO:root:[  337] Training loss: 0.02702019, Validation loss: 0.02668901, Gradient norm: 1.34528776
INFO:root:[  338] Training loss: 0.02684570, Validation loss: 0.02649279, Gradient norm: 1.07696898
INFO:root:[  339] Training loss: 0.02672701, Validation loss: 0.02662986, Gradient norm: 0.98492651
INFO:root:[  340] Training loss: 0.02677119, Validation loss: 0.02673915, Gradient norm: 1.22629543
INFO:root:[  341] Training loss: 0.02677809, Validation loss: 0.02667758, Gradient norm: 1.21514663
INFO:root:[  342] Training loss: 0.02669823, Validation loss: 0.02687377, Gradient norm: 1.28611860
INFO:root:[  343] Training loss: 0.02660955, Validation loss: 0.02675475, Gradient norm: 1.20488241
INFO:root:[  344] Training loss: 0.02651956, Validation loss: 0.02674441, Gradient norm: 1.04829054
INFO:root:[  345] Training loss: 0.02640614, Validation loss: 0.02635841, Gradient norm: 1.01958432
INFO:root:[  346] Training loss: 0.02637978, Validation loss: 0.02603148, Gradient norm: 1.06055157
INFO:root:[  347] Training loss: 0.02637791, Validation loss: 0.02620414, Gradient norm: 1.16103486
INFO:root:[  348] Training loss: 0.02649005, Validation loss: 0.02647697, Gradient norm: 1.40222605
INFO:root:[  349] Training loss: 0.02628311, Validation loss: 0.02647870, Gradient norm: 1.16109290
INFO:root:[  350] Training loss: 0.02634486, Validation loss: 0.02652795, Gradient norm: 1.30954690
INFO:root:[  351] Training loss: 0.02609064, Validation loss: 0.02640584, Gradient norm: 1.04429929
INFO:root:[  352] Training loss: 0.02616932, Validation loss: 0.02606127, Gradient norm: 1.25298119
INFO:root:[  353] Training loss: 0.02613164, Validation loss: 0.02618412, Gradient norm: 1.26208475
INFO:root:[  354] Training loss: 0.02597938, Validation loss: 0.02572397, Gradient norm: 1.05270033
INFO:root:[  355] Training loss: 0.02595845, Validation loss: 0.02582145, Gradient norm: 1.17232467
INFO:root:[  356] Training loss: 0.02591349, Validation loss: 0.02611595, Gradient norm: 1.13927042
INFO:root:[  357] Training loss: 0.02592331, Validation loss: 0.02562053, Gradient norm: 1.18844952
INFO:root:[  358] Training loss: 0.02594630, Validation loss: 0.02599944, Gradient norm: 1.34589133
INFO:root:[  359] Training loss: 0.02580513, Validation loss: 0.02584062, Gradient norm: 1.20663619
INFO:root:[  360] Training loss: 0.02576181, Validation loss: 0.02555041, Gradient norm: 1.27317399
INFO:root:[  361] Training loss: 0.02561983, Validation loss: 0.02557019, Gradient norm: 1.06275833
INFO:root:[  362] Training loss: 0.02551626, Validation loss: 0.02538239, Gradient norm: 0.95492496
INFO:root:[  363] Training loss: 0.02574985, Validation loss: 0.02535026, Gradient norm: 1.50083979
INFO:root:[  364] Training loss: 0.02553795, Validation loss: 0.02556296, Gradient norm: 1.08173408
INFO:root:[  365] Training loss: 0.02542128, Validation loss: 0.02545368, Gradient norm: 1.04241795
INFO:root:[  366] Training loss: 0.02543397, Validation loss: 0.02554367, Gradient norm: 1.18741679
INFO:root:[  367] Training loss: 0.02541960, Validation loss: 0.02551480, Gradient norm: 1.18931618
INFO:root:[  368] Training loss: 0.02524627, Validation loss: 0.02568388, Gradient norm: 0.95144188
INFO:root:[  369] Training loss: 0.02531481, Validation loss: 0.02570404, Gradient norm: 1.30992192
INFO:root:[  370] Training loss: 0.02541366, Validation loss: 0.02579501, Gradient norm: 1.50256043
INFO:root:[  371] Training loss: 0.02526465, Validation loss: 0.02548831, Gradient norm: 1.26684699
INFO:root:[  372] Training loss: 0.02518187, Validation loss: 0.02546652, Gradient norm: 1.22924303
INFO:root:EP 372: Early stopping
INFO:root:Training the model took 12391.641s.
INFO:root:Emptying the cuda cache took 0.08s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.46166
INFO:root:EnergyScoreTrain: 0.40432
INFO:root:CoverageTrain: 0.99661
INFO:root:IntervalWidthTrain: 0.07335
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.43457
INFO:root:EnergyScoreValidation: 0.38011
INFO:root:CoverageValidation: 0.99659
INFO:root:IntervalWidthValidation: 0.07348
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.40861
INFO:root:EnergyScoreTest: 0.35852
INFO:root:CoverageTest: 0.99666
INFO:root:IntervalWidthTest: 0.073
INFO:root:###5 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.84473707, Validation loss: 2.25548104, Gradient norm: 4.95306490
INFO:root:[    2] Training loss: 0.93754616, Validation loss: 0.29090128, Gradient norm: 4.76486733
INFO:root:[    3] Training loss: 0.25431513, Validation loss: 0.23660822, Gradient norm: 1.10084411
INFO:root:[    4] Training loss: 0.23823086, Validation loss: 0.21629803, Gradient norm: 1.19220437
INFO:root:[    5] Training loss: 0.21025536, Validation loss: 0.20258162, Gradient norm: 1.16592286
INFO:root:[    6] Training loss: 0.19417440, Validation loss: 0.18901541, Gradient norm: 1.12460135
INFO:root:[    7] Training loss: 0.18462538, Validation loss: 0.17444152, Gradient norm: 1.25212029
INFO:root:[    8] Training loss: 0.17638687, Validation loss: 0.17945361, Gradient norm: 1.05682951
INFO:root:[    9] Training loss: 0.16336373, Validation loss: 0.15464790, Gradient norm: 1.02629953
INFO:root:[   10] Training loss: 0.15615976, Validation loss: 0.14678723, Gradient norm: 1.07994899
INFO:root:[   11] Training loss: 0.14772465, Validation loss: 0.14429356, Gradient norm: 0.79747660
INFO:root:[   12] Training loss: 0.14494376, Validation loss: 0.14216563, Gradient norm: 0.88440370
INFO:root:[   13] Training loss: 0.13686866, Validation loss: 0.13440362, Gradient norm: 0.95684036
INFO:root:[   14] Training loss: 0.12985591, Validation loss: 0.12853701, Gradient norm: 0.71031678
INFO:root:[   15] Training loss: 0.12702562, Validation loss: 0.11977658, Gradient norm: 1.16894630
INFO:root:[   16] Training loss: 0.12064055, Validation loss: 0.11456126, Gradient norm: 0.89031517
INFO:root:[   17] Training loss: 0.11429720, Validation loss: 0.11327943, Gradient norm: 0.89089484
INFO:root:[   18] Training loss: 0.10976366, Validation loss: 0.10435713, Gradient norm: 0.98748072
INFO:root:[   19] Training loss: 0.10338798, Validation loss: 0.10134370, Gradient norm: 0.82959071
INFO:root:[   20] Training loss: 0.09854248, Validation loss: 0.09718547, Gradient norm: 0.74039662
INFO:root:[   21] Training loss: 0.09414781, Validation loss: 0.09314716, Gradient norm: 0.63441645
INFO:root:[   22] Training loss: 0.09151438, Validation loss: 0.09036142, Gradient norm: 0.72080439
INFO:root:[   23] Training loss: 0.08830782, Validation loss: 0.08551434, Gradient norm: 0.69833721
INFO:root:[   24] Training loss: 0.08515107, Validation loss: 0.08499198, Gradient norm: 0.50227919
INFO:root:[   25] Training loss: 0.08295082, Validation loss: 0.08276654, Gradient norm: 0.65753054
INFO:root:[   26] Training loss: 0.08145036, Validation loss: 0.07971799, Gradient norm: 0.84872634
INFO:root:[   27] Training loss: 0.07890585, Validation loss: 0.07844515, Gradient norm: 0.53564953
INFO:root:[   28] Training loss: 0.07757800, Validation loss: 0.07730842, Gradient norm: 0.61367745
INFO:root:[   29] Training loss: 0.07587140, Validation loss: 0.07505323, Gradient norm: 0.44420746
INFO:root:[   30] Training loss: 0.07475198, Validation loss: 0.07571028, Gradient norm: 0.51111640
INFO:root:[   31] Training loss: 0.07386000, Validation loss: 0.07454060, Gradient norm: 0.67581645
INFO:root:[   32] Training loss: 0.07287767, Validation loss: 0.07188750, Gradient norm: 0.79623096
INFO:root:[   33] Training loss: 0.07169607, Validation loss: 0.07080907, Gradient norm: 0.62975676
INFO:root:[   34] Training loss: 0.07079723, Validation loss: 0.07029164, Gradient norm: 0.54951260
INFO:root:[   35] Training loss: 0.06965560, Validation loss: 0.06920805, Gradient norm: 0.42822828
INFO:root:[   36] Training loss: 0.06871907, Validation loss: 0.06880256, Gradient norm: 0.44425097
INFO:root:[   37] Training loss: 0.06811152, Validation loss: 0.06800062, Gradient norm: 0.41416955
INFO:root:[   38] Training loss: 0.06806087, Validation loss: 0.06669508, Gradient norm: 0.70558255
INFO:root:[   39] Training loss: 0.06681899, Validation loss: 0.06673901, Gradient norm: 0.41405542
INFO:root:[   40] Training loss: 0.06633931, Validation loss: 0.06657310, Gradient norm: 0.31432772
INFO:root:[   41] Training loss: 0.06591529, Validation loss: 0.06532831, Gradient norm: 0.57325276
INFO:root:[   42] Training loss: 0.06545307, Validation loss: 0.06448791, Gradient norm: 0.63014281
INFO:root:[   43] Training loss: 0.06451866, Validation loss: 0.06459543, Gradient norm: 0.33858770
INFO:root:[   44] Training loss: 0.06419418, Validation loss: 0.06451937, Gradient norm: 0.37002584
INFO:root:[   45] Training loss: 0.06360768, Validation loss: 0.06362765, Gradient norm: 0.45799603
INFO:root:[   46] Training loss: 0.06311879, Validation loss: 0.06307544, Gradient norm: 0.41031060
INFO:root:[   47] Training loss: 0.06273535, Validation loss: 0.06286081, Gradient norm: 0.38351410
INFO:root:[   48] Training loss: 0.06275424, Validation loss: 0.06193694, Gradient norm: 0.71044248
INFO:root:[   49] Training loss: 0.06192100, Validation loss: 0.06283830, Gradient norm: 0.52375919
INFO:root:[   50] Training loss: 0.06166116, Validation loss: 0.06176371, Gradient norm: 0.59350724
INFO:root:[   51] Training loss: 0.06124548, Validation loss: 0.06140706, Gradient norm: 0.41140621
INFO:root:[   52] Training loss: 0.06099831, Validation loss: 0.06078400, Gradient norm: 0.53927572
INFO:root:[   53] Training loss: 0.06071226, Validation loss: 0.06072573, Gradient norm: 0.69951485
INFO:root:[   54] Training loss: 0.05999200, Validation loss: 0.05980168, Gradient norm: 0.33786003
INFO:root:[   55] Training loss: 0.05975153, Validation loss: 0.05964848, Gradient norm: 0.39658774
INFO:root:[   56] Training loss: 0.06012179, Validation loss: 0.05925879, Gradient norm: 0.92520881
INFO:root:[   57] Training loss: 0.05915997, Validation loss: 0.05907787, Gradient norm: 0.50507745
INFO:root:[   58] Training loss: 0.05889513, Validation loss: 0.05888823, Gradient norm: 0.40669329
INFO:root:[   59] Training loss: 0.05863806, Validation loss: 0.05966474, Gradient norm: 0.49977049
INFO:root:[   60] Training loss: 0.05893439, Validation loss: 0.05800965, Gradient norm: 0.92958146
INFO:root:[   61] Training loss: 0.05832847, Validation loss: 0.05788374, Gradient norm: 0.74260407
INFO:root:[   62] Training loss: 0.05771813, Validation loss: 0.05785596, Gradient norm: 0.42252139
INFO:root:[   63] Training loss: 0.05759905, Validation loss: 0.05721400, Gradient norm: 0.60970677
INFO:root:[   64] Training loss: 0.05726355, Validation loss: 0.05769330, Gradient norm: 0.58428519
INFO:root:[   65] Training loss: 0.05710914, Validation loss: 0.05783231, Gradient norm: 0.55416446
INFO:root:[   66] Training loss: 0.05691481, Validation loss: 0.05635017, Gradient norm: 0.68536801
INFO:root:[   67] Training loss: 0.05655083, Validation loss: 0.05642975, Gradient norm: 0.57960237
INFO:root:[   68] Training loss: 0.05638140, Validation loss: 0.05665338, Gradient norm: 0.64311411
INFO:root:[   69] Training loss: 0.05608498, Validation loss: 0.05647291, Gradient norm: 0.61593283
INFO:root:[   70] Training loss: 0.05645909, Validation loss: 0.05633631, Gradient norm: 1.03193191
INFO:root:[   71] Training loss: 0.05572630, Validation loss: 0.05541901, Gradient norm: 0.74200901
INFO:root:[   72] Training loss: 0.05535581, Validation loss: 0.05530166, Gradient norm: 0.53357777
INFO:root:[   73] Training loss: 0.05538917, Validation loss: 0.05520598, Gradient norm: 0.77978349
INFO:root:[   74] Training loss: 0.05510331, Validation loss: 0.05466098, Gradient norm: 0.68893344
INFO:root:[   75] Training loss: 0.05463197, Validation loss: 0.05510149, Gradient norm: 0.36416028
INFO:root:[   76] Training loss: 0.05463967, Validation loss: 0.05509960, Gradient norm: 0.65765906
INFO:root:[   77] Training loss: 0.05454068, Validation loss: 0.05456163, Gradient norm: 0.77797244
INFO:root:[   78] Training loss: 0.05428910, Validation loss: 0.05433597, Gradient norm: 0.63909749
INFO:root:[   79] Training loss: 0.05420793, Validation loss: 0.05419428, Gradient norm: 0.75181334
INFO:root:[   80] Training loss: 0.05415963, Validation loss: 0.05446409, Gradient norm: 0.90395157
INFO:root:[   81] Training loss: 0.05411523, Validation loss: 0.05341504, Gradient norm: 1.01525537
INFO:root:[   82] Training loss: 0.05345800, Validation loss: 0.05339506, Gradient norm: 0.48672983
INFO:root:[   83] Training loss: 0.05336122, Validation loss: 0.05291158, Gradient norm: 0.58580720
INFO:root:[   84] Training loss: 0.05324724, Validation loss: 0.05306658, Gradient norm: 0.72325055
INFO:root:[   85] Training loss: 0.05298069, Validation loss: 0.05263419, Gradient norm: 0.62848873
INFO:root:[   86] Training loss: 0.05305838, Validation loss: 0.05275541, Gradient norm: 0.88576751
INFO:root:[   87] Training loss: 0.05260585, Validation loss: 0.05274382, Gradient norm: 0.61396490
INFO:root:[   88] Training loss: 0.05263837, Validation loss: 0.05411867, Gradient norm: 0.80260354
INFO:root:[   89] Training loss: 0.05257650, Validation loss: 0.05252250, Gradient norm: 0.86836191
INFO:root:[   90] Training loss: 0.05226603, Validation loss: 0.05214917, Gradient norm: 0.73771884
INFO:root:[   91] Training loss: 0.05181191, Validation loss: 0.05193676, Gradient norm: 0.51787594
INFO:root:[   92] Training loss: 0.05170891, Validation loss: 0.05165655, Gradient norm: 0.56902646
INFO:root:[   93] Training loss: 0.05143311, Validation loss: 0.05179010, Gradient norm: 0.39494675
INFO:root:[   94] Training loss: 0.05141475, Validation loss: 0.05184839, Gradient norm: 0.66987889
INFO:root:[   95] Training loss: 0.05138459, Validation loss: 0.05142240, Gradient norm: 0.76870557
INFO:root:[   96] Training loss: 0.05108574, Validation loss: 0.05102413, Gradient norm: 0.54503320
INFO:root:[   97] Training loss: 0.05081282, Validation loss: 0.05120223, Gradient norm: 0.30225438
INFO:root:[   98] Training loss: 0.05088930, Validation loss: 0.05142185, Gradient norm: 0.74281114
INFO:root:[   99] Training loss: 0.05084027, Validation loss: 0.05092350, Gradient norm: 0.89964114
INFO:root:[  100] Training loss: 0.05074602, Validation loss: 0.05055575, Gradient norm: 0.96369455
INFO:root:[  101] Training loss: 0.05030221, Validation loss: 0.05016006, Gradient norm: 0.63302803
INFO:root:[  102] Training loss: 0.05016134, Validation loss: 0.04990447, Gradient norm: 0.67126773
INFO:root:[  103] Training loss: 0.05013358, Validation loss: 0.04968189, Gradient norm: 0.83299361
INFO:root:[  104] Training loss: 0.04974097, Validation loss: 0.04956822, Gradient norm: 0.54750693
INFO:root:[  105] Training loss: 0.04979490, Validation loss: 0.04983620, Gradient norm: 0.75513647
INFO:root:[  106] Training loss: 0.04956495, Validation loss: 0.04944717, Gradient norm: 0.86210271
INFO:root:[  107] Training loss: 0.04934086, Validation loss: 0.04932133, Gradient norm: 0.71012461
INFO:root:[  108] Training loss: 0.04915126, Validation loss: 0.04948992, Gradient norm: 0.58761023
INFO:root:[  109] Training loss: 0.04926729, Validation loss: 0.04908687, Gradient norm: 1.00971724
INFO:root:[  110] Training loss: 0.04899534, Validation loss: 0.04878934, Gradient norm: 0.73911371
INFO:root:[  111] Training loss: 0.04888163, Validation loss: 0.04958271, Gradient norm: 0.77508215
INFO:root:[  112] Training loss: 0.04880753, Validation loss: 0.04844591, Gradient norm: 0.92560238
INFO:root:[  113] Training loss: 0.04858775, Validation loss: 0.04872051, Gradient norm: 0.92556117
INFO:root:[  114] Training loss: 0.04836648, Validation loss: 0.04842561, Gradient norm: 0.85127491
INFO:root:[  115] Training loss: 0.04820342, Validation loss: 0.04827625, Gradient norm: 0.84781300
INFO:root:[  116] Training loss: 0.04820140, Validation loss: 0.04810848, Gradient norm: 0.91165858
INFO:root:[  117] Training loss: 0.04803647, Validation loss: 0.04796465, Gradient norm: 0.97949607
INFO:root:[  118] Training loss: 0.04779605, Validation loss: 0.04762689, Gradient norm: 0.84343680
INFO:root:[  119] Training loss: 0.04768263, Validation loss: 0.04755432, Gradient norm: 0.84036632
INFO:root:[  120] Training loss: 0.04744388, Validation loss: 0.04730932, Gradient norm: 0.77376171
INFO:root:[  121] Training loss: 0.04735679, Validation loss: 0.04697920, Gradient norm: 0.78356798
INFO:root:[  122] Training loss: 0.04710205, Validation loss: 0.04711004, Gradient norm: 0.70441468
INFO:root:[  123] Training loss: 0.04716917, Validation loss: 0.04739090, Gradient norm: 0.93978365
INFO:root:[  124] Training loss: 0.04710033, Validation loss: 0.04701884, Gradient norm: 1.05037865
INFO:root:[  125] Training loss: 0.04678019, Validation loss: 0.04662511, Gradient norm: 0.90529158
INFO:root:[  126] Training loss: 0.04665018, Validation loss: 0.04684608, Gradient norm: 0.87000253
INFO:root:[  127] Training loss: 0.04650856, Validation loss: 0.04673377, Gradient norm: 0.85856578
INFO:root:[  128] Training loss: 0.04630212, Validation loss: 0.04593558, Gradient norm: 0.74956559
INFO:root:[  129] Training loss: 0.04608262, Validation loss: 0.04651338, Gradient norm: 0.62678104
INFO:root:[  130] Training loss: 0.04602689, Validation loss: 0.04575495, Gradient norm: 0.82713269
INFO:root:[  131] Training loss: 0.04602269, Validation loss: 0.04543760, Gradient norm: 1.01047875
INFO:root:[  132] Training loss: 0.04572842, Validation loss: 0.04606973, Gradient norm: 0.83008005
INFO:root:[  133] Training loss: 0.04575827, Validation loss: 0.04591290, Gradient norm: 1.08858999
INFO:root:[  134] Training loss: 0.04555710, Validation loss: 0.04624816, Gradient norm: 0.92919253
INFO:root:[  135] Training loss: 0.04547697, Validation loss: 0.04532337, Gradient norm: 1.05109678
INFO:root:[  136] Training loss: 0.04530974, Validation loss: 0.04541946, Gradient norm: 1.08619862
INFO:root:[  137] Training loss: 0.04503961, Validation loss: 0.04479640, Gradient norm: 0.75967945
INFO:root:[  138] Training loss: 0.04468160, Validation loss: 0.04444069, Gradient norm: 0.37046360
INFO:root:[  139] Training loss: 0.04463488, Validation loss: 0.04450643, Gradient norm: 0.66463613
INFO:root:[  140] Training loss: 0.04473001, Validation loss: 0.04521797, Gradient norm: 1.00098311
INFO:root:[  141] Training loss: 0.04474140, Validation loss: 0.04452528, Gradient norm: 1.14015933
INFO:root:[  142] Training loss: 0.04454887, Validation loss: 0.04430829, Gradient norm: 1.12827049
INFO:root:[  143] Training loss: 0.04431384, Validation loss: 0.04413894, Gradient norm: 1.07404908
INFO:root:[  144] Training loss: 0.04424169, Validation loss: 0.04479662, Gradient norm: 1.00900872
INFO:root:[  145] Training loss: 0.04406675, Validation loss: 0.04408227, Gradient norm: 1.02913014
INFO:root:[  146] Training loss: 0.04380587, Validation loss: 0.04372859, Gradient norm: 0.88539664
INFO:root:[  147] Training loss: 0.04387780, Validation loss: 0.04343564, Gradient norm: 1.16487875
INFO:root:[  148] Training loss: 0.04357537, Validation loss: 0.04416281, Gradient norm: 0.92425315
INFO:root:[  149] Training loss: 0.04357709, Validation loss: 0.04350535, Gradient norm: 1.09628836
INFO:root:[  150] Training loss: 0.04320298, Validation loss: 0.04361818, Gradient norm: 0.60784938
INFO:root:[  151] Training loss: 0.04314932, Validation loss: 0.04280807, Gradient norm: 0.95746185
INFO:root:[  152] Training loss: 0.04323897, Validation loss: 0.04343781, Gradient norm: 1.19921653
INFO:root:[  153] Training loss: 0.04298195, Validation loss: 0.04292032, Gradient norm: 1.05649151
INFO:root:[  154] Training loss: 0.04312200, Validation loss: 0.04305904, Gradient norm: 1.35739892
INFO:root:[  155] Training loss: 0.04262294, Validation loss: 0.04226301, Gradient norm: 0.85730645
INFO:root:[  156] Training loss: 0.04239561, Validation loss: 0.04294565, Gradient norm: 0.64199649
INFO:root:[  157] Training loss: 0.04242166, Validation loss: 0.04209931, Gradient norm: 0.94159336
INFO:root:[  158] Training loss: 0.04237944, Validation loss: 0.04266085, Gradient norm: 1.16564364
INFO:root:[  159] Training loss: 0.04224467, Validation loss: 0.04266138, Gradient norm: 1.20269816
INFO:root:[  160] Training loss: 0.04198077, Validation loss: 0.04185078, Gradient norm: 0.91001119
INFO:root:[  161] Training loss: 0.04176229, Validation loss: 0.04207841, Gradient norm: 0.76020653
INFO:root:[  162] Training loss: 0.04191793, Validation loss: 0.04207883, Gradient norm: 1.25154979
INFO:root:[  163] Training loss: 0.04165656, Validation loss: 0.04178183, Gradient norm: 0.99998074
INFO:root:[  164] Training loss: 0.04148954, Validation loss: 0.04165148, Gradient norm: 1.02944188
INFO:root:[  165] Training loss: 0.04161046, Validation loss: 0.04162085, Gradient norm: 1.24985273
INFO:root:[  166] Training loss: 0.04127098, Validation loss: 0.04097808, Gradient norm: 0.91257357
INFO:root:[  167] Training loss: 0.04103787, Validation loss: 0.04088328, Gradient norm: 0.85144492
INFO:root:[  168] Training loss: 0.04090289, Validation loss: 0.04080601, Gradient norm: 0.89124600
INFO:root:[  169] Training loss: 0.04083782, Validation loss: 0.04073850, Gradient norm: 1.00704063
INFO:root:[  170] Training loss: 0.04083220, Validation loss: 0.04119085, Gradient norm: 1.16805704
INFO:root:[  171] Training loss: 0.04075004, Validation loss: 0.04088580, Gradient norm: 1.23466446
INFO:root:[  172] Training loss: 0.04062890, Validation loss: 0.04017642, Gradient norm: 1.15451813
INFO:root:[  173] Training loss: 0.04037707, Validation loss: 0.04032631, Gradient norm: 0.97382388
INFO:root:[  174] Training loss: 0.04037039, Validation loss: 0.04041744, Gradient norm: 1.16001883
INFO:root:[  175] Training loss: 0.04029742, Validation loss: 0.04026999, Gradient norm: 1.28035949
INFO:root:[  176] Training loss: 0.04002363, Validation loss: 0.04017142, Gradient norm: 1.03176921
INFO:root:[  177] Training loss: 0.04008240, Validation loss: 0.03979623, Gradient norm: 1.36694104
INFO:root:[  178] Training loss: 0.03990631, Validation loss: 0.04063136, Gradient norm: 1.13298713
INFO:root:[  179] Training loss: 0.03983779, Validation loss: 0.03972941, Gradient norm: 1.27714300
INFO:root:[  180] Training loss: 0.03978467, Validation loss: 0.03985583, Gradient norm: 1.39088290
INFO:root:[  181] Training loss: 0.03954898, Validation loss: 0.03967746, Gradient norm: 1.19932466
INFO:root:[  182] Training loss: 0.03953440, Validation loss: 0.03940535, Gradient norm: 1.34288892
INFO:root:[  183] Training loss: 0.03929848, Validation loss: 0.03946439, Gradient norm: 1.18997365
INFO:root:[  184] Training loss: 0.03913175, Validation loss: 0.03929680, Gradient norm: 1.01210856
INFO:root:[  185] Training loss: 0.03907885, Validation loss: 0.03932030, Gradient norm: 1.10657891
INFO:root:[  186] Training loss: 0.03892944, Validation loss: 0.03898235, Gradient norm: 1.07694725
INFO:root:[  187] Training loss: 0.03880744, Validation loss: 0.03884008, Gradient norm: 0.98422805
INFO:root:[  188] Training loss: 0.03878151, Validation loss: 0.03855477, Gradient norm: 1.36096993
INFO:root:[  189] Training loss: 0.03860964, Validation loss: 0.03857159, Gradient norm: 1.20618628
INFO:root:[  190] Training loss: 0.03866483, Validation loss: 0.03827652, Gradient norm: 1.39398843
INFO:root:[  191] Training loss: 0.03857718, Validation loss: 0.03872082, Gradient norm: 1.46157652
INFO:root:[  192] Training loss: 0.03819474, Validation loss: 0.03804632, Gradient norm: 0.94793536
INFO:root:[  193] Training loss: 0.03825731, Validation loss: 0.03804997, Gradient norm: 1.25946681
INFO:root:[  194] Training loss: 0.03820085, Validation loss: 0.03768165, Gradient norm: 1.46444733
INFO:root:[  195] Training loss: 0.03808545, Validation loss: 0.03775305, Gradient norm: 1.35552507
INFO:root:[  196] Training loss: 0.03790725, Validation loss: 0.03750816, Gradient norm: 1.26323836
INFO:root:[  197] Training loss: 0.03784212, Validation loss: 0.03764802, Gradient norm: 1.38811572
INFO:root:[  198] Training loss: 0.03763819, Validation loss: 0.03753910, Gradient norm: 1.18331193
INFO:root:[  199] Training loss: 0.03746617, Validation loss: 0.03749421, Gradient norm: 1.05813651
INFO:root:[  200] Training loss: 0.03748165, Validation loss: 0.03746125, Gradient norm: 1.21132883
INFO:root:[  201] Training loss: 0.03739738, Validation loss: 0.03763421, Gradient norm: 1.28356068
INFO:root:[  202] Training loss: 0.03734189, Validation loss: 0.03773137, Gradient norm: 1.32275081
INFO:root:[  203] Training loss: 0.03738928, Validation loss: 0.03785993, Gradient norm: 1.66794342
INFO:root:[  204] Training loss: 0.03701805, Validation loss: 0.03724802, Gradient norm: 1.20712214
INFO:root:[  205] Training loss: 0.03700949, Validation loss: 0.03740385, Gradient norm: 1.38670563
INFO:root:[  206] Training loss: 0.03699108, Validation loss: 0.03730735, Gradient norm: 1.60859095
INFO:root:[  207] Training loss: 0.03688461, Validation loss: 0.03702038, Gradient norm: 1.56407198
INFO:root:[  208] Training loss: 0.03666332, Validation loss: 0.03693817, Gradient norm: 1.25299545
INFO:root:[  209] Training loss: 0.03654890, Validation loss: 0.03682399, Gradient norm: 1.28116525
INFO:root:[  210] Training loss: 0.03666263, Validation loss: 0.03652626, Gradient norm: 1.68700414
INFO:root:[  211] Training loss: 0.03634601, Validation loss: 0.03624394, Gradient norm: 1.15339451
INFO:root:[  212] Training loss: 0.03637843, Validation loss: 0.03595999, Gradient norm: 1.47835260
INFO:root:[  213] Training loss: 0.03607606, Validation loss: 0.03606111, Gradient norm: 1.05319655
INFO:root:[  214] Training loss: 0.03619492, Validation loss: 0.03597212, Gradient norm: 1.39520082
INFO:root:[  215] Training loss: 0.03608123, Validation loss: 0.03639595, Gradient norm: 1.47624667
INFO:root:[  216] Training loss: 0.03601879, Validation loss: 0.03604908, Gradient norm: 1.45622457
INFO:root:[  217] Training loss: 0.03593487, Validation loss: 0.03548554, Gradient norm: 1.51886727
INFO:root:[  218] Training loss: 0.03567038, Validation loss: 0.03570023, Gradient norm: 1.15851041
INFO:root:[  219] Training loss: 0.03579446, Validation loss: 0.03552016, Gradient norm: 1.61948579
INFO:root:[  220] Training loss: 0.03565448, Validation loss: 0.03566039, Gradient norm: 1.37973977
INFO:root:[  221] Training loss: 0.03550777, Validation loss: 0.03575646, Gradient norm: 1.40326138
INFO:root:[  222] Training loss: 0.03563801, Validation loss: 0.03536133, Gradient norm: 1.94181199
INFO:root:[  223] Training loss: 0.03529558, Validation loss: 0.03520784, Gradient norm: 1.22196823
INFO:root:[  224] Training loss: 0.03512541, Validation loss: 0.03504673, Gradient norm: 1.07313790
INFO:root:[  225] Training loss: 0.03511581, Validation loss: 0.03575708, Gradient norm: 1.40033026
INFO:root:[  226] Training loss: 0.03509015, Validation loss: 0.03467158, Gradient norm: 1.47127784
INFO:root:[  227] Training loss: 0.03488464, Validation loss: 0.03491682, Gradient norm: 1.32759444
INFO:root:[  228] Training loss: 0.03497800, Validation loss: 0.03535094, Gradient norm: 1.67430640
INFO:root:[  229] Training loss: 0.03488336, Validation loss: 0.03459539, Gradient norm: 1.62314765
INFO:root:[  230] Training loss: 0.03479072, Validation loss: 0.03508343, Gradient norm: 1.60059459
INFO:root:[  231] Training loss: 0.03470757, Validation loss: 0.03429464, Gradient norm: 1.63754744
INFO:root:[  232] Training loss: 0.03475507, Validation loss: 0.03442756, Gradient norm: 1.99739619
INFO:root:[  233] Training loss: 0.03457087, Validation loss: 0.03458750, Gradient norm: 1.75302957
INFO:root:[  234] Training loss: 0.03434327, Validation loss: 0.03450201, Gradient norm: 1.35003309
INFO:root:[  235] Training loss: 0.03444685, Validation loss: 0.03397838, Gradient norm: 1.77541274
INFO:root:[  236] Training loss: 0.03450443, Validation loss: 0.03418195, Gradient norm: 2.03582875
INFO:root:[  237] Training loss: 0.03421897, Validation loss: 0.03394363, Gradient norm: 1.72280418
INFO:root:[  238] Training loss: 0.03391108, Validation loss: 0.03383192, Gradient norm: 1.22731117
INFO:root:[  239] Training loss: 0.03388481, Validation loss: 0.03374558, Gradient norm: 1.20162358
INFO:root:[  240] Training loss: 0.03390796, Validation loss: 0.03398682, Gradient norm: 1.44497810
INFO:root:[  241] Training loss: 0.03371511, Validation loss: 0.03356833, Gradient norm: 1.28723421
INFO:root:[  242] Training loss: 0.03374021, Validation loss: 0.03345107, Gradient norm: 1.44796226
INFO:root:[  243] Training loss: 0.03366380, Validation loss: 0.03438185, Gradient norm: 1.54325408
INFO:root:[  244] Training loss: 0.03361367, Validation loss: 0.03321578, Gradient norm: 1.62564642
INFO:root:[  245] Training loss: 0.03346471, Validation loss: 0.03376755, Gradient norm: 1.43578019
INFO:root:[  246] Training loss: 0.03330815, Validation loss: 0.03315905, Gradient norm: 1.36123468
INFO:root:[  247] Training loss: 0.03337665, Validation loss: 0.03327213, Gradient norm: 1.66532602
INFO:root:[  248] Training loss: 0.03332394, Validation loss: 0.03361447, Gradient norm: 1.64370888
INFO:root:[  249] Training loss: 0.03311280, Validation loss: 0.03307779, Gradient norm: 1.41099268
INFO:root:[  250] Training loss: 0.03318884, Validation loss: 0.03306643, Gradient norm: 1.76028067
INFO:root:[  251] Training loss: 0.03298209, Validation loss: 0.03300739, Gradient norm: 1.55615057
INFO:root:[  252] Training loss: 0.03287235, Validation loss: 0.03332722, Gradient norm: 1.51571863
INFO:root:[  253] Training loss: 0.03300650, Validation loss: 0.03309421, Gradient norm: 1.91791455
INFO:root:[  254] Training loss: 0.03270998, Validation loss: 0.03314043, Gradient norm: 1.43830575
INFO:root:[  255] Training loss: 0.03273289, Validation loss: 0.03299756, Gradient norm: 1.87861834
INFO:root:[  256] Training loss: 0.03270574, Validation loss: 0.03236677, Gradient norm: 1.84910294
INFO:root:[  257] Training loss: 0.03244569, Validation loss: 0.03278755, Gradient norm: 1.54101779
INFO:root:[  258] Training loss: 0.03238264, Validation loss: 0.03276590, Gradient norm: 1.46976345
INFO:root:[  259] Training loss: 0.03247646, Validation loss: 0.03273928, Gradient norm: 1.87414156
INFO:root:[  260] Training loss: 0.03225889, Validation loss: 0.03226081, Gradient norm: 1.55343694
INFO:root:[  261] Training loss: 0.03207523, Validation loss: 0.03233793, Gradient norm: 1.38448555
INFO:root:[  262] Training loss: 0.03218213, Validation loss: 0.03196925, Gradient norm: 1.75254095
INFO:root:[  263] Training loss: 0.03212895, Validation loss: 0.03211084, Gradient norm: 1.83599284
INFO:root:[  264] Training loss: 0.03202381, Validation loss: 0.03227850, Gradient norm: 1.79151376
INFO:root:[  265] Training loss: 0.03190887, Validation loss: 0.03175894, Gradient norm: 1.70066983
INFO:root:[  266] Training loss: 0.03183935, Validation loss: 0.03201285, Gradient norm: 1.69259657
INFO:root:[  267] Training loss: 0.03179142, Validation loss: 0.03208207, Gradient norm: 1.70762853
INFO:root:[  268] Training loss: 0.03169872, Validation loss: 0.03175479, Gradient norm: 1.73808882
INFO:root:[  269] Training loss: 0.03162097, Validation loss: 0.03125528, Gradient norm: 1.64818635
INFO:root:[  270] Training loss: 0.03141656, Validation loss: 0.03121876, Gradient norm: 1.47155591
INFO:root:[  271] Training loss: 0.03136722, Validation loss: 0.03111254, Gradient norm: 1.35711378
INFO:root:[  272] Training loss: 0.03148616, Validation loss: 0.03091578, Gradient norm: 1.95712918
INFO:root:[  273] Training loss: 0.03135370, Validation loss: 0.03117924, Gradient norm: 1.88283845
INFO:root:[  274] Training loss: 0.03125832, Validation loss: 0.03184115, Gradient norm: 1.78670926
INFO:root:[  275] Training loss: 0.03115881, Validation loss: 0.03097354, Gradient norm: 1.69186303
INFO:root:[  276] Training loss: 0.03108352, Validation loss: 0.03137676, Gradient norm: 1.82293802
INFO:root:[  277] Training loss: 0.03120016, Validation loss: 0.03128385, Gradient norm: 2.16314739
INFO:root:[  278] Training loss: 0.03093626, Validation loss: 0.03074271, Gradient norm: 1.69500993
INFO:root:[  279] Training loss: 0.03089020, Validation loss: 0.03048708, Gradient norm: 1.82568803
INFO:root:[  280] Training loss: 0.03070440, Validation loss: 0.03093855, Gradient norm: 1.41637128
INFO:root:[  281] Training loss: 0.03069122, Validation loss: 0.03057951, Gradient norm: 1.75324721
INFO:root:[  282] Training loss: 0.03062571, Validation loss: 0.03078633, Gradient norm: 1.77214423
INFO:root:[  283] Training loss: 0.03072058, Validation loss: 0.03086479, Gradient norm: 2.05464894
INFO:root:[  284] Training loss: 0.03061898, Validation loss: 0.03068817, Gradient norm: 2.10283714
INFO:root:[  285] Training loss: 0.03043021, Validation loss: 0.03088087, Gradient norm: 1.74275722
INFO:root:[  286] Training loss: 0.03043934, Validation loss: 0.03009204, Gradient norm: 1.95305945
INFO:root:[  287] Training loss: 0.03043154, Validation loss: 0.03021620, Gradient norm: 2.12738380
INFO:root:[  288] Training loss: 0.03041952, Validation loss: 0.03061199, Gradient norm: 2.10577044
INFO:root:[  289] Training loss: 0.03020342, Validation loss: 0.03029823, Gradient norm: 1.76080172
INFO:root:[  290] Training loss: 0.03030263, Validation loss: 0.02978127, Gradient norm: 2.28196103
INFO:root:[  291] Training loss: 0.03000955, Validation loss: 0.03030109, Gradient norm: 1.68326940
INFO:root:[  292] Training loss: 0.02993651, Validation loss: 0.03032130, Gradient norm: 1.69971002
INFO:root:[  293] Training loss: 0.02984601, Validation loss: 0.02973164, Gradient norm: 1.57651633
INFO:root:[  294] Training loss: 0.02974706, Validation loss: 0.02985741, Gradient norm: 1.40428344
INFO:root:[  295] Training loss: 0.02983482, Validation loss: 0.02950278, Gradient norm: 1.90087547
INFO:root:[  296] Training loss: 0.02983152, Validation loss: 0.03039067, Gradient norm: 2.12927433
INFO:root:[  297] Training loss: 0.02977136, Validation loss: 0.03003128, Gradient norm: 2.22936779
INFO:root:[  298] Training loss: 0.02969436, Validation loss: 0.02923624, Gradient norm: 2.23969424
INFO:root:[  299] Training loss: 0.02960432, Validation loss: 0.02922320, Gradient norm: 2.10230079
INFO:root:[  300] Training loss: 0.02949860, Validation loss: 0.02924917, Gradient norm: 2.05168605
INFO:root:[  301] Training loss: 0.02942826, Validation loss: 0.02960621, Gradient norm: 1.99225454
INFO:root:[  302] Training loss: 0.02941626, Validation loss: 0.03010562, Gradient norm: 2.13528256
INFO:root:[  303] Training loss: 0.02944406, Validation loss: 0.03006003, Gradient norm: 2.36456851
INFO:root:[  304] Training loss: 0.02932423, Validation loss: 0.02934239, Gradient norm: 2.16313679
INFO:root:[  305] Training loss: 0.02913207, Validation loss: 0.02919510, Gradient norm: 1.85919730
INFO:root:[  306] Training loss: 0.02911721, Validation loss: 0.02879066, Gradient norm: 2.12645480
INFO:root:[  307] Training loss: 0.02896673, Validation loss: 0.02900738, Gradient norm: 1.81654220
INFO:root:[  308] Training loss: 0.02900982, Validation loss: 0.02862760, Gradient norm: 2.09392151
INFO:root:[  309] Training loss: 0.02903726, Validation loss: 0.02880214, Gradient norm: 2.25651108
INFO:root:[  310] Training loss: 0.02894123, Validation loss: 0.02847799, Gradient norm: 2.17591917
INFO:root:[  311] Training loss: 0.02873370, Validation loss: 0.02906855, Gradient norm: 1.83057829
INFO:root:[  312] Training loss: 0.02871575, Validation loss: 0.02921683, Gradient norm: 1.97523550
INFO:root:[  313] Training loss: 0.02874770, Validation loss: 0.02861614, Gradient norm: 1.84107222
INFO:root:[  314] Training loss: 0.02866393, Validation loss: 0.02830400, Gradient norm: 2.06050520
INFO:root:[  315] Training loss: 0.02858080, Validation loss: 0.02866770, Gradient norm: 1.96861242
INFO:root:[  316] Training loss: 0.02853862, Validation loss: 0.02863732, Gradient norm: 2.27810242
INFO:root:[  317] Training loss: 0.02837837, Validation loss: 0.02810407, Gradient norm: 1.71642902
INFO:root:[  318] Training loss: 0.02841467, Validation loss: 0.02801738, Gradient norm: 2.33888618
INFO:root:[  319] Training loss: 0.02845154, Validation loss: 0.02842209, Gradient norm: 2.53977411
INFO:root:[  320] Training loss: 0.02833850, Validation loss: 0.02887082, Gradient norm: 2.27892933
INFO:root:[  321] Training loss: 0.02829089, Validation loss: 0.02798944, Gradient norm: 2.47222973
INFO:root:[  322] Training loss: 0.02818714, Validation loss: 0.02870086, Gradient norm: 2.17500729
INFO:root:[  323] Training loss: 0.02815882, Validation loss: 0.02837971, Gradient norm: 2.25329731
INFO:root:[  324] Training loss: 0.02794777, Validation loss: 0.02793575, Gradient norm: 1.88952275
INFO:root:[  325] Training loss: 0.02808504, Validation loss: 0.02772165, Gradient norm: 2.34556450
INFO:root:[  326] Training loss: 0.02794529, Validation loss: 0.02787047, Gradient norm: 2.28886096
INFO:root:[  327] Training loss: 0.02801755, Validation loss: 0.02816382, Gradient norm: 2.64272463
INFO:root:[  328] Training loss: 0.02783225, Validation loss: 0.02801920, Gradient norm: 2.32357902
INFO:root:[  329] Training loss: 0.02781725, Validation loss: 0.02774708, Gradient norm: 2.35427996
INFO:root:[  330] Training loss: 0.02771042, Validation loss: 0.02764157, Gradient norm: 2.31819485
INFO:root:[  331] Training loss: 0.02765702, Validation loss: 0.02740560, Gradient norm: 2.23584278
INFO:root:[  332] Training loss: 0.02765708, Validation loss: 0.02736023, Gradient norm: 2.41130263
INFO:root:[  333] Training loss: 0.02749456, Validation loss: 0.02729626, Gradient norm: 1.96081910
INFO:root:[  334] Training loss: 0.02758889, Validation loss: 0.02743806, Gradient norm: 2.61912414
INFO:root:[  335] Training loss: 0.02761255, Validation loss: 0.02807405, Gradient norm: 2.60688300
INFO:root:[  336] Training loss: 0.02743041, Validation loss: 0.02714784, Gradient norm: 2.39219017
INFO:root:[  337] Training loss: 0.02740628, Validation loss: 0.02751540, Gradient norm: 2.33595550
INFO:root:[  338] Training loss: 0.02737942, Validation loss: 0.02695936, Gradient norm: 2.51655610
INFO:root:[  339] Training loss: 0.02723106, Validation loss: 0.02788568, Gradient norm: 2.23978310
INFO:root:[  340] Training loss: 0.02720771, Validation loss: 0.02698627, Gradient norm: 2.52948333
INFO:root:[  341] Training loss: 0.02719007, Validation loss: 0.02753221, Gradient norm: 2.49286522
INFO:root:[  342] Training loss: 0.02707313, Validation loss: 0.02694609, Gradient norm: 2.37208254
INFO:root:[  343] Training loss: 0.02692027, Validation loss: 0.02675334, Gradient norm: 1.86899425
INFO:root:[  344] Training loss: 0.02695668, Validation loss: 0.02697364, Gradient norm: 2.44864715
INFO:root:[  345] Training loss: 0.02706025, Validation loss: 0.02703050, Gradient norm: 2.82484742
INFO:root:[  346] Training loss: 0.02686329, Validation loss: 0.02696385, Gradient norm: 2.38253223
INFO:root:[  347] Training loss: 0.02687176, Validation loss: 0.02715970, Gradient norm: 2.60103601
INFO:root:[  348] Training loss: 0.02678403, Validation loss: 0.02681405, Gradient norm: 2.41778861
INFO:root:[  349] Training loss: 0.02656369, Validation loss: 0.02671378, Gradient norm: 1.70847255
INFO:root:[  350] Training loss: 0.02674067, Validation loss: 0.02737419, Gradient norm: 2.52227063
INFO:root:[  351] Training loss: 0.02679652, Validation loss: 0.02709776, Gradient norm: 3.01204278
INFO:root:[  352] Training loss: 0.02649446, Validation loss: 0.02672049, Gradient norm: 2.51656234
INFO:root:[  353] Training loss: 0.02654383, Validation loss: 0.02660151, Gradient norm: 2.57477021
INFO:root:[  354] Training loss: 0.02651177, Validation loss: 0.02601348, Gradient norm: 2.76398844
INFO:root:[  355] Training loss: 0.02658046, Validation loss: 0.02629975, Gradient norm: 3.02896448
INFO:root:[  356] Training loss: 0.02635674, Validation loss: 0.02632710, Gradient norm: 2.46164285
INFO:root:[  357] Training loss: 0.02628148, Validation loss: 0.02700359, Gradient norm: 2.36442655
INFO:root:[  358] Training loss: 0.02627646, Validation loss: 0.02627667, Gradient norm: 2.58513430
INFO:root:[  359] Training loss: 0.02612555, Validation loss: 0.02620167, Gradient norm: 2.16441084
INFO:root:[  360] Training loss: 0.02627625, Validation loss: 0.02593540, Gradient norm: 3.03669577
INFO:root:[  361] Training loss: 0.02604821, Validation loss: 0.02598369, Gradient norm: 2.27528205
INFO:root:[  362] Training loss: 0.02612785, Validation loss: 0.02685751, Gradient norm: 2.76565767
INFO:root:[  363] Training loss: 0.02614351, Validation loss: 0.02585980, Gradient norm: 2.96439530
INFO:root:[  364] Training loss: 0.02603452, Validation loss: 0.02575399, Gradient norm: 2.74913490
INFO:root:[  365] Training loss: 0.02608763, Validation loss: 0.02632982, Gradient norm: 3.17232941
INFO:root:[  366] Training loss: 0.02579660, Validation loss: 0.02571884, Gradient norm: 2.19010072
INFO:root:[  367] Training loss: 0.02571604, Validation loss: 0.02606882, Gradient norm: 2.15046182
INFO:root:[  368] Training loss: 0.02595029, Validation loss: 0.02553422, Gradient norm: 3.31785991
INFO:root:[  369] Training loss: 0.02576216, Validation loss: 0.02582815, Gradient norm: 2.59646893
INFO:root:[  370] Training loss: 0.02563061, Validation loss: 0.02560727, Gradient norm: 2.47282728
INFO:root:[  371] Training loss: 0.02558538, Validation loss: 0.02598703, Gradient norm: 2.61976269
INFO:root:[  372] Training loss: 0.02547965, Validation loss: 0.02537981, Gradient norm: 2.30675601
INFO:root:[  373] Training loss: 0.02566381, Validation loss: 0.02547154, Gradient norm: 3.34721651
INFO:root:[  374] Training loss: 0.02547558, Validation loss: 0.02531583, Gradient norm: 2.55771268
INFO:root:[  375] Training loss: 0.02547004, Validation loss: 0.02533476, Gradient norm: 2.83992889
INFO:root:[  376] Training loss: 0.02540372, Validation loss: 0.02505814, Gradient norm: 2.92153364
INFO:root:[  377] Training loss: 0.02544889, Validation loss: 0.02521185, Gradient norm: 3.43981136
INFO:root:[  378] Training loss: 0.02527131, Validation loss: 0.02557000, Gradient norm: 2.79415772
INFO:root:[  379] Training loss: 0.02525654, Validation loss: 0.02528240, Gradient norm: 2.89717600
INFO:root:[  380] Training loss: 0.02529315, Validation loss: 0.02556427, Gradient norm: 3.02081139
INFO:root:[  381] Training loss: 0.02527922, Validation loss: 0.02579807, Gradient norm: 3.39584845
INFO:root:[  382] Training loss: 0.02518125, Validation loss: 0.02520290, Gradient norm: 3.12858397
INFO:root:[  383] Training loss: 0.02504722, Validation loss: 0.02499348, Gradient norm: 2.99077482
INFO:root:[  384] Training loss: 0.02499153, Validation loss: 0.02504677, Gradient norm: 2.97894047
INFO:root:[  385] Training loss: 0.02498350, Validation loss: 0.02498445, Gradient norm: 2.98661337
INFO:root:[  386] Training loss: 0.02506290, Validation loss: 0.02558905, Gradient norm: 3.50221125
INFO:root:[  387] Training loss: 0.02505783, Validation loss: 0.02517348, Gradient norm: 3.73953493
INFO:root:[  388] Training loss: 0.02480971, Validation loss: 0.02512455, Gradient norm: 3.13156313
INFO:root:[  389] Training loss: 0.02482189, Validation loss: 0.02519386, Gradient norm: 3.05099782
INFO:root:[  390] Training loss: 0.02477217, Validation loss: 0.02486913, Gradient norm: 3.31418065
INFO:root:[  391] Training loss: 0.02469104, Validation loss: 0.02493665, Gradient norm: 3.03689439
INFO:root:[  392] Training loss: 0.02472839, Validation loss: 0.02465744, Gradient norm: 3.26474225
INFO:root:[  393] Training loss: 0.02468011, Validation loss: 0.02468661, Gradient norm: 3.19238222
INFO:root:[  394] Training loss: 0.02472798, Validation loss: 0.02495292, Gradient norm: 3.46612193
INFO:root:[  395] Training loss: 0.02463618, Validation loss: 0.02450346, Gradient norm: 3.99956556
INFO:root:[  396] Training loss: 0.02459150, Validation loss: 0.02437581, Gradient norm: 3.50398458
INFO:root:[  397] Training loss: 0.02448764, Validation loss: 0.02474145, Gradient norm: 3.27566674
INFO:root:[  398] Training loss: 0.02451361, Validation loss: 0.02427835, Gradient norm: 3.80015528
INFO:root:[  399] Training loss: 0.02453013, Validation loss: 0.02493846, Gradient norm: 3.78764255
INFO:root:[  400] Training loss: 0.02437235, Validation loss: 0.02441818, Gradient norm: 3.12425848
INFO:root:[  401] Training loss: 0.02430210, Validation loss: 0.02444062, Gradient norm: 3.35905090
INFO:root:[  402] Training loss: 0.02458249, Validation loss: 0.02472824, Gradient norm: 4.50478111
INFO:root:[  403] Training loss: 0.02432837, Validation loss: 0.02458474, Gradient norm: 3.89540216
INFO:root:[  404] Training loss: 0.02431067, Validation loss: 0.02435392, Gradient norm: 3.71923679
INFO:root:[  405] Training loss: 0.02415854, Validation loss: 0.02435872, Gradient norm: 3.49589143
INFO:root:[  406] Training loss: 0.02423429, Validation loss: 0.02428442, Gradient norm: 3.63844758
INFO:root:[  407] Training loss: 0.02414552, Validation loss: 0.02401166, Gradient norm: 3.37964321
INFO:root:[  408] Training loss: 0.02418870, Validation loss: 0.02436647, Gradient norm: 4.19313113
INFO:root:[  409] Training loss: 0.02409013, Validation loss: 0.02385369, Gradient norm: 3.90708266
INFO:root:[  410] Training loss: 0.02402172, Validation loss: 0.02398797, Gradient norm: 3.39245490
INFO:root:[  411] Training loss: 0.02405284, Validation loss: 0.02399119, Gradient norm: 4.20735520
INFO:root:[  412] Training loss: 0.02406328, Validation loss: 0.02453158, Gradient norm: 4.38375849
INFO:root:[  413] Training loss: 0.02398887, Validation loss: 0.02389691, Gradient norm: 4.22913048
INFO:root:[  414] Training loss: 0.02381856, Validation loss: 0.02430174, Gradient norm: 3.70476819
INFO:root:[  415] Training loss: 0.02387820, Validation loss: 0.02402402, Gradient norm: 4.28329932
INFO:root:[  416] Training loss: 0.02384129, Validation loss: 0.02349819, Gradient norm: 4.13818701
INFO:root:[  417] Training loss: 0.02376526, Validation loss: 0.02376440, Gradient norm: 3.71303050
INFO:root:[  418] Training loss: 0.02373872, Validation loss: 0.02380716, Gradient norm: 3.90798702
INFO:root:[  419] Training loss: 0.02374117, Validation loss: 0.02368630, Gradient norm: 4.04465407
INFO:root:[  420] Training loss: 0.02372409, Validation loss: 0.02371122, Gradient norm: 4.06423969
INFO:root:[  421] Training loss: 0.02366593, Validation loss: 0.02380825, Gradient norm: 4.42361062
INFO:root:[  422] Training loss: 0.02357550, Validation loss: 0.02329762, Gradient norm: 4.01022335
INFO:root:[  423] Training loss: 0.02350159, Validation loss: 0.02332405, Gradient norm: 3.35078656
INFO:root:[  424] Training loss: 0.02368111, Validation loss: 0.02338461, Gradient norm: 4.38204809
INFO:root:[  425] Training loss: 0.02365664, Validation loss: 0.02390420, Gradient norm: 4.47123901
INFO:root:[  426] Training loss: 0.02350171, Validation loss: 0.02366891, Gradient norm: 4.03263712
INFO:root:[  427] Training loss: 0.02346256, Validation loss: 0.02324897, Gradient norm: 3.95213107
INFO:root:[  428] Training loss: 0.02349351, Validation loss: 0.02328609, Gradient norm: 4.22006719
INFO:root:[  429] Training loss: 0.02342501, Validation loss: 0.02337322, Gradient norm: 4.59416876
INFO:root:[  430] Training loss: 0.02342927, Validation loss: 0.02347915, Gradient norm: 4.71616437
INFO:root:[  431] Training loss: 0.02329514, Validation loss: 0.02370317, Gradient norm: 4.17555583
INFO:root:[  432] Training loss: 0.02332294, Validation loss: 0.02311574, Gradient norm: 4.49283628
INFO:root:[  433] Training loss: 0.02323056, Validation loss: 0.02374049, Gradient norm: 3.54256108
INFO:root:[  434] Training loss: 0.02339006, Validation loss: 0.02312760, Gradient norm: 5.09384196
INFO:root:[  435] Training loss: 0.02328852, Validation loss: 0.02301516, Gradient norm: 4.74885478
INFO:root:[  436] Training loss: 0.02321990, Validation loss: 0.02296915, Gradient norm: 4.43105660
INFO:root:[  437] Training loss: 0.02304437, Validation loss: 0.02288914, Gradient norm: 4.17257533
INFO:root:[  438] Training loss: 0.02330210, Validation loss: 0.02308189, Gradient norm: 5.59321909
INFO:root:[  439] Training loss: 0.02297697, Validation loss: 0.02267557, Gradient norm: 3.70980665
INFO:root:[  440] Training loss: 0.02290539, Validation loss: 0.02309628, Gradient norm: 3.12891757
INFO:root:[  441] Training loss: 0.02312961, Validation loss: 0.02475961, Gradient norm: 4.85229184
INFO:root:[  442] Training loss: 0.02309823, Validation loss: 0.02290763, Gradient norm: 5.05892917
INFO:root:[  443] Training loss: 0.02306692, Validation loss: 0.02373563, Gradient norm: 4.76313294
INFO:root:[  444] Training loss: 0.02299305, Validation loss: 0.02315692, Gradient norm: 5.48763569
INFO:root:[  445] Training loss: 0.02281766, Validation loss: 0.02309097, Gradient norm: 4.36402809
INFO:root:[  446] Training loss: 0.02275741, Validation loss: 0.02288813, Gradient norm: 3.99882631
INFO:root:[  447] Training loss: 0.02276445, Validation loss: 0.02264919, Gradient norm: 4.09860391
INFO:root:[  448] Training loss: 0.02308393, Validation loss: 0.02311494, Gradient norm: 6.40508348
INFO:root:[  449] Training loss: 0.02280121, Validation loss: 0.02306656, Gradient norm: 5.48203988
INFO:root:[  450] Training loss: 0.02271136, Validation loss: 0.02303242, Gradient norm: 4.87938700
INFO:root:[  451] Training loss: 0.02263293, Validation loss: 0.02227869, Gradient norm: 4.73047418
INFO:root:[  452] Training loss: 0.02275389, Validation loss: 0.02272556, Gradient norm: 5.43951983
INFO:root:[  453] Training loss: 0.02246748, Validation loss: 0.02326128, Gradient norm: 3.69554700
INFO:root:[  454] Training loss: 0.02257701, Validation loss: 0.02258009, Gradient norm: 4.24146911
INFO:root:[  455] Training loss: 0.02264130, Validation loss: 0.02286612, Gradient norm: 5.50016487
INFO:root:[  456] Training loss: 0.02249258, Validation loss: 0.02239447, Gradient norm: 4.82853729
INFO:root:[  457] Training loss: 0.02235221, Validation loss: 0.02307685, Gradient norm: 3.64092962
INFO:root:[  458] Training loss: 0.02260808, Validation loss: 0.02370993, Gradient norm: 5.27064861
INFO:root:[  459] Training loss: 0.02254011, Validation loss: 0.02207937, Gradient norm: 5.45868615
INFO:root:[  460] Training loss: 0.02250617, Validation loss: 0.02282943, Gradient norm: 5.87416532
INFO:root:[  461] Training loss: 0.02239445, Validation loss: 0.02214886, Gradient norm: 5.38239338
INFO:root:[  462] Training loss: 0.02227186, Validation loss: 0.02204529, Gradient norm: 4.26243580
INFO:root:[  463] Training loss: 0.02246783, Validation loss: 0.02239936, Gradient norm: 5.44595402
INFO:root:[  464] Training loss: 0.02247327, Validation loss: 0.02257461, Gradient norm: 6.19262901
INFO:root:[  465] Training loss: 0.02232072, Validation loss: 0.02207867, Gradient norm: 5.66112980
INFO:root:[  466] Training loss: 0.02219262, Validation loss: 0.02205402, Gradient norm: 4.75540197
INFO:root:[  467] Training loss: 0.02214977, Validation loss: 0.02313611, Gradient norm: 4.74237264
INFO:root:[  468] Training loss: 0.02243501, Validation loss: 0.02242812, Gradient norm: 6.30635679
INFO:root:[  469] Training loss: 0.02223469, Validation loss: 0.02323004, Gradient norm: 5.83088188
INFO:root:[  470] Training loss: 0.02220225, Validation loss: 0.02206068, Gradient norm: 5.41214254
INFO:root:[  471] Training loss: 0.02204157, Validation loss: 0.02256398, Gradient norm: 4.91046057
INFO:root:[  472] Training loss: 0.02211285, Validation loss: 0.02169459, Gradient norm: 5.87042473
INFO:root:[  473] Training loss: 0.02193527, Validation loss: 0.02179958, Gradient norm: 4.50754247
INFO:root:[  474] Training loss: 0.02197326, Validation loss: 0.02239656, Gradient norm: 4.67062521
INFO:root:[  475] Training loss: 0.02192711, Validation loss: 0.02260141, Gradient norm: 4.62835466
INFO:root:[  476] Training loss: 0.02194639, Validation loss: 0.02188343, Gradient norm: 4.73266660
INFO:root:[  477] Training loss: 0.02182859, Validation loss: 0.02192828, Gradient norm: 4.44509842
INFO:root:[  478] Training loss: 0.02190636, Validation loss: 0.02213214, Gradient norm: 5.09122795
INFO:root:[  479] Training loss: 0.02179461, Validation loss: 0.02164765, Gradient norm: 4.71422725
INFO:root:[  480] Training loss: 0.02204276, Validation loss: 0.02226787, Gradient norm: 6.03525004
INFO:root:[  481] Training loss: 0.02189382, Validation loss: 0.02336821, Gradient norm: 5.77184092
INFO:root:[  482] Training loss: 0.02178309, Validation loss: 0.02327422, Gradient norm: 5.06527307
INFO:root:[  483] Training loss: 0.02180498, Validation loss: 0.02128164, Gradient norm: 5.61525447
INFO:root:[  484] Training loss: 0.02154946, Validation loss: 0.02127554, Gradient norm: 3.92033025
INFO:root:[  485] Training loss: 0.02194283, Validation loss: 0.02140073, Gradient norm: 6.42194511
INFO:root:[  486] Training loss: 0.02151224, Validation loss: 0.02249705, Gradient norm: 4.17847743
INFO:root:[  487] Training loss: 0.02153200, Validation loss: 0.02165654, Gradient norm: 4.39796325
INFO:root:[  488] Training loss: 0.02142224, Validation loss: 0.02127493, Gradient norm: 3.84981912
INFO:root:[  489] Training loss: 0.02130744, Validation loss: 0.02165027, Gradient norm: 2.83738502
INFO:root:[  490] Training loss: 0.02144397, Validation loss: 0.02168575, Gradient norm: 4.13403465
INFO:root:[  491] Training loss: 0.02150348, Validation loss: 0.02132611, Gradient norm: 5.11842204
INFO:root:[  492] Training loss: 0.02139940, Validation loss: 0.02132669, Gradient norm: 5.08396974
INFO:root:[  493] Training loss: 0.02126403, Validation loss: 0.02145438, Gradient norm: 3.76751247
INFO:root:[  494] Training loss: 0.02126958, Validation loss: 0.02174097, Gradient norm: 4.09826219
INFO:root:[  495] Training loss: 0.02124865, Validation loss: 0.02165971, Gradient norm: 4.10478441
INFO:root:[  496] Training loss: 0.02125896, Validation loss: 0.02102531, Gradient norm: 4.97516469
INFO:root:[  497] Training loss: 0.02120138, Validation loss: 0.02094880, Gradient norm: 4.33945420
INFO:root:[  498] Training loss: 0.02147992, Validation loss: 0.02096177, Gradient norm: 6.63229223
INFO:root:[  499] Training loss: 0.02117658, Validation loss: 0.02108467, Gradient norm: 4.40378183
INFO:root:[  500] Training loss: 0.02138604, Validation loss: 0.02118088, Gradient norm: 6.41648009
INFO:root:[  501] Training loss: 0.02128845, Validation loss: 0.02088202, Gradient norm: 5.73703343
INFO:root:[  502] Training loss: 0.02099191, Validation loss: 0.02106439, Gradient norm: 4.09965119
INFO:root:[  503] Training loss: 0.02121317, Validation loss: 0.02126011, Gradient norm: 5.59282420
INFO:root:[  504] Training loss: 0.02121685, Validation loss: 0.02124801, Gradient norm: 6.01078382
INFO:root:[  505] Training loss: 0.02097820, Validation loss: 0.02133339, Gradient norm: 5.41610687
INFO:root:[  506] Training loss: 0.02109961, Validation loss: 0.02098588, Gradient norm: 6.12308549
INFO:root:[  507] Training loss: 0.02088191, Validation loss: 0.02081024, Gradient norm: 4.81612194
INFO:root:[  508] Training loss: 0.02082989, Validation loss: 0.02163750, Gradient norm: 4.62726345
INFO:root:[  509] Training loss: 0.02103198, Validation loss: 0.02049894, Gradient norm: 6.12753086
INFO:root:[  510] Training loss: 0.02082305, Validation loss: 0.02086897, Gradient norm: 4.99180911
INFO:root:[  511] Training loss: 0.02076172, Validation loss: 0.02189875, Gradient norm: 4.66245900
INFO:root:[  512] Training loss: 0.02076541, Validation loss: 0.02102152, Gradient norm: 4.56717116
INFO:root:[  513] Training loss: 0.02077689, Validation loss: 0.02183332, Gradient norm: 5.12530575
INFO:root:[  514] Training loss: 0.02077620, Validation loss: 0.02209809, Gradient norm: 5.60869346
INFO:root:[  515] Training loss: 0.02096244, Validation loss: 0.02111805, Gradient norm: 7.35235905
INFO:root:[  516] Training loss: 0.02055133, Validation loss: 0.02061236, Gradient norm: 4.67661213
INFO:root:[  517] Training loss: 0.02074599, Validation loss: 0.02105924, Gradient norm: 6.10742775
INFO:root:[  518] Training loss: 0.02093327, Validation loss: 0.02087509, Gradient norm: 7.85860414
INFO:root:EP 518: Early stopping
INFO:root:Training the model took 17241.246s.
INFO:root:Emptying the cuda cache took 0.08s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.38281
INFO:root:EnergyScoreTrain: 0.32818
INFO:root:CoverageTrain: 0.99653
INFO:root:IntervalWidthTrain: 0.05629
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.35963
INFO:root:EnergyScoreValidation: 0.30875
INFO:root:CoverageValidation: 0.99655
INFO:root:IntervalWidthValidation: 0.05653
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.33713
INFO:root:EnergyScoreTest: 0.29011
INFO:root:CoverageTest: 0.99663
INFO:root:IntervalWidthTest: 0.05596
INFO:root:###6 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.75175755, Validation loss: 1.74775599, Gradient norm: 6.39827845
INFO:root:[    2] Training loss: 0.67950755, Validation loss: 0.36438045, Gradient norm: 4.34478452
INFO:root:[    3] Training loss: 0.34475905, Validation loss: 0.32115263, Gradient norm: 1.19236441
INFO:root:[    4] Training loss: 0.30707002, Validation loss: 0.29615307, Gradient norm: 1.24884577
INFO:root:[    5] Training loss: 0.28138827, Validation loss: 0.26914775, Gradient norm: 1.33970069
INFO:root:[    6] Training loss: 0.26121744, Validation loss: 0.24483346, Gradient norm: 1.10652368
INFO:root:[    7] Training loss: 0.23913690, Validation loss: 0.22837009, Gradient norm: 0.96448518
INFO:root:[    8] Training loss: 0.22218060, Validation loss: 0.21027858, Gradient norm: 0.97589233
INFO:root:[    9] Training loss: 0.20687002, Validation loss: 0.19712827, Gradient norm: 1.17706647
INFO:root:[   10] Training loss: 0.18960993, Validation loss: 0.18560811, Gradient norm: 1.01438648
INFO:root:[   11] Training loss: 0.18252547, Validation loss: 0.17059669, Gradient norm: 1.14505238
INFO:root:[   12] Training loss: 0.17295944, Validation loss: 0.17068895, Gradient norm: 0.97113111
INFO:root:[   13] Training loss: 0.15974301, Validation loss: 0.15618958, Gradient norm: 0.73570704
INFO:root:[   14] Training loss: 0.15351129, Validation loss: 0.15283146, Gradient norm: 0.79947934
INFO:root:[   15] Training loss: 0.15045087, Validation loss: 0.14613900, Gradient norm: 1.06918713
INFO:root:[   16] Training loss: 0.14407220, Validation loss: 0.14291039, Gradient norm: 0.75403587
INFO:root:[   17] Training loss: 0.13936525, Validation loss: 0.13278694, Gradient norm: 0.64479078
INFO:root:[   18] Training loss: 0.13275331, Validation loss: 0.13344503, Gradient norm: 0.75808386
INFO:root:[   19] Training loss: 0.12886833, Validation loss: 0.12554378, Gradient norm: 0.81720523
INFO:root:[   20] Training loss: 0.12559146, Validation loss: 0.12577481, Gradient norm: 0.70688193
INFO:root:[   21] Training loss: 0.12120532, Validation loss: 0.11928984, Gradient norm: 0.61219577
INFO:root:[   22] Training loss: 0.11742548, Validation loss: 0.11717275, Gradient norm: 0.72717857
INFO:root:[   23] Training loss: 0.11371389, Validation loss: 0.11170781, Gradient norm: 0.61567370
INFO:root:[   24] Training loss: 0.11097044, Validation loss: 0.10938273, Gradient norm: 0.48652199
INFO:root:[   25] Training loss: 0.10871109, Validation loss: 0.10566958, Gradient norm: 0.52442121
INFO:root:[   26] Training loss: 0.10517716, Validation loss: 0.10480727, Gradient norm: 0.53893739
INFO:root:[   27] Training loss: 0.10314528, Validation loss: 0.10263895, Gradient norm: 0.37078595
INFO:root:[   28] Training loss: 0.09968861, Validation loss: 0.09984659, Gradient norm: 0.33445487
INFO:root:[   29] Training loss: 0.09823165, Validation loss: 0.09718985, Gradient norm: 0.44237287
INFO:root:[   30] Training loss: 0.09664461, Validation loss: 0.09537749, Gradient norm: 0.40989707
INFO:root:[   31] Training loss: 0.09506196, Validation loss: 0.09399101, Gradient norm: 0.43331854
INFO:root:[   32] Training loss: 0.09346689, Validation loss: 0.09255895, Gradient norm: 0.45346761
INFO:root:[   33] Training loss: 0.09188803, Validation loss: 0.09149716, Gradient norm: 0.43025099
INFO:root:[   34] Training loss: 0.09054559, Validation loss: 0.08906242, Gradient norm: 0.40147834
INFO:root:[   35] Training loss: 0.08859999, Validation loss: 0.08903749, Gradient norm: 0.31570807
INFO:root:[   36] Training loss: 0.08797944, Validation loss: 0.08797639, Gradient norm: 0.34849768
INFO:root:[   37] Training loss: 0.08666063, Validation loss: 0.08685708, Gradient norm: 0.28353843
INFO:root:[   38] Training loss: 0.08639686, Validation loss: 0.08587663, Gradient norm: 0.44319222
INFO:root:[   39] Training loss: 0.08504371, Validation loss: 0.08468994, Gradient norm: 0.41754747
INFO:root:[   40] Training loss: 0.08375983, Validation loss: 0.08303638, Gradient norm: 0.24546590
INFO:root:[   41] Training loss: 0.08322644, Validation loss: 0.08303107, Gradient norm: 0.31598518
INFO:root:[   42] Training loss: 0.08245420, Validation loss: 0.08238084, Gradient norm: 0.27910049
INFO:root:[   43] Training loss: 0.08175915, Validation loss: 0.08120377, Gradient norm: 0.31289453
INFO:root:[   44] Training loss: 0.08100222, Validation loss: 0.08105898, Gradient norm: 0.30100297
INFO:root:[   45] Training loss: 0.08064855, Validation loss: 0.08057309, Gradient norm: 0.35548511
INFO:root:[   46] Training loss: 0.07965914, Validation loss: 0.08003014, Gradient norm: 0.28146540
INFO:root:[   47] Training loss: 0.07922465, Validation loss: 0.07934599, Gradient norm: 0.36125663
INFO:root:[   48] Training loss: 0.07869126, Validation loss: 0.07839235, Gradient norm: 0.29751280
INFO:root:[   49] Training loss: 0.07808503, Validation loss: 0.07811176, Gradient norm: 0.27163560
INFO:root:[   50] Training loss: 0.07754197, Validation loss: 0.07783192, Gradient norm: 0.19769990
INFO:root:[   51] Training loss: 0.07702092, Validation loss: 0.07648232, Gradient norm: 0.27066383
INFO:root:[   52] Training loss: 0.07638902, Validation loss: 0.07653169, Gradient norm: 0.25856687
INFO:root:[   53] Training loss: 0.07601391, Validation loss: 0.07608720, Gradient norm: 0.25094445
INFO:root:[   54] Training loss: 0.07536155, Validation loss: 0.07516265, Gradient norm: 0.17585760
INFO:root:[   55] Training loss: 0.07512054, Validation loss: 0.07488952, Gradient norm: 0.25049819
INFO:root:[   56] Training loss: 0.07467428, Validation loss: 0.07577559, Gradient norm: 0.35250463
INFO:root:[   57] Training loss: 0.07505562, Validation loss: 0.07427447, Gradient norm: 0.67521294
INFO:root:[   58] Training loss: 0.07410350, Validation loss: 0.07361303, Gradient norm: 0.38639914
INFO:root:[   59] Training loss: 0.07332746, Validation loss: 0.07330065, Gradient norm: 0.20663089
INFO:root:[   60] Training loss: 0.07306319, Validation loss: 0.07333050, Gradient norm: 0.23407056
INFO:root:[   61] Training loss: 0.07254749, Validation loss: 0.07273697, Gradient norm: 0.17670013
INFO:root:[   62] Training loss: 0.07221521, Validation loss: 0.07206894, Gradient norm: 0.23133231
INFO:root:[   63] Training loss: 0.07180457, Validation loss: 0.07177248, Gradient norm: 0.20029633
INFO:root:[   64] Training loss: 0.07151303, Validation loss: 0.07142120, Gradient norm: 0.27738574
INFO:root:[   65] Training loss: 0.07113642, Validation loss: 0.07159128, Gradient norm: 0.26099249
INFO:root:[   66] Training loss: 0.07097442, Validation loss: 0.07168496, Gradient norm: 0.39850753
INFO:root:[   67] Training loss: 0.07055230, Validation loss: 0.07065879, Gradient norm: 0.29524483
INFO:root:[   68] Training loss: 0.07009669, Validation loss: 0.07012165, Gradient norm: 0.25971959
INFO:root:[   69] Training loss: 0.06987527, Validation loss: 0.06993743, Gradient norm: 0.29844347
INFO:root:[   70] Training loss: 0.06974473, Validation loss: 0.06990539, Gradient norm: 0.47486271
INFO:root:[   71] Training loss: 0.06940014, Validation loss: 0.06899954, Gradient norm: 0.43595728
INFO:root:[   72] Training loss: 0.06886827, Validation loss: 0.06891586, Gradient norm: 0.25022223
INFO:root:[   73] Training loss: 0.06894547, Validation loss: 0.06928747, Gradient norm: 0.45045389
INFO:root:[   74] Training loss: 0.06854036, Validation loss: 0.06788192, Gradient norm: 0.42096639
INFO:root:[   75] Training loss: 0.06807502, Validation loss: 0.06801204, Gradient norm: 0.22007115
INFO:root:[   76] Training loss: 0.06767732, Validation loss: 0.06768141, Gradient norm: 0.27335607
INFO:root:[   77] Training loss: 0.06738435, Validation loss: 0.06717428, Gradient norm: 0.32941447
INFO:root:[   78] Training loss: 0.06742175, Validation loss: 0.06719308, Gradient norm: 0.46267249
INFO:root:[   79] Training loss: 0.06685185, Validation loss: 0.06669413, Gradient norm: 0.22251950
INFO:root:[   80] Training loss: 0.06675126, Validation loss: 0.06692106, Gradient norm: 0.31175195
INFO:root:[   81] Training loss: 0.06667279, Validation loss: 0.06619095, Gradient norm: 0.44950778
INFO:root:[   82] Training loss: 0.06608119, Validation loss: 0.06601716, Gradient norm: 0.29700416
INFO:root:[   83] Training loss: 0.06587233, Validation loss: 0.06593605, Gradient norm: 0.32603836
INFO:root:[   84] Training loss: 0.06555780, Validation loss: 0.06591625, Gradient norm: 0.27112527
INFO:root:[   85] Training loss: 0.06559073, Validation loss: 0.06607263, Gradient norm: 0.49557850
INFO:root:[   86] Training loss: 0.06543549, Validation loss: 0.06492906, Gradient norm: 0.56772030
INFO:root:[   87] Training loss: 0.06487533, Validation loss: 0.06495457, Gradient norm: 0.31687384
INFO:root:[   88] Training loss: 0.06447629, Validation loss: 0.06534105, Gradient norm: 0.17813791
INFO:root:[   89] Training loss: 0.06438269, Validation loss: 0.06430534, Gradient norm: 0.32634306
INFO:root:[   90] Training loss: 0.06420170, Validation loss: 0.06468713, Gradient norm: 0.36870283
INFO:root:[   91] Training loss: 0.06422884, Validation loss: 0.06443748, Gradient norm: 0.55737184
INFO:root:[   92] Training loss: 0.06403295, Validation loss: 0.06383184, Gradient norm: 0.61034915
INFO:root:[   93] Training loss: 0.06331158, Validation loss: 0.06342332, Gradient norm: 0.24300917
INFO:root:[   94] Training loss: 0.06315220, Validation loss: 0.06330236, Gradient norm: 0.21891295
INFO:root:[   95] Training loss: 0.06296129, Validation loss: 0.06280011, Gradient norm: 0.31842217
INFO:root:[   96] Training loss: 0.06275168, Validation loss: 0.06248930, Gradient norm: 0.29323427
INFO:root:[   97] Training loss: 0.06262081, Validation loss: 0.06277417, Gradient norm: 0.42353185
INFO:root:[   98] Training loss: 0.06230789, Validation loss: 0.06223121, Gradient norm: 0.32169564
INFO:root:[   99] Training loss: 0.06195923, Validation loss: 0.06226575, Gradient norm: 0.25430201
INFO:root:[  100] Training loss: 0.06195797, Validation loss: 0.06182774, Gradient norm: 0.48356684
INFO:root:[  101] Training loss: 0.06143391, Validation loss: 0.06151068, Gradient norm: 0.17224599
INFO:root:[  102] Training loss: 0.06124654, Validation loss: 0.06121448, Gradient norm: 0.23070883
INFO:root:[  103] Training loss: 0.06103332, Validation loss: 0.06098625, Gradient norm: 0.30032467
INFO:root:[  104] Training loss: 0.06100688, Validation loss: 0.06131982, Gradient norm: 0.45834925
INFO:root:[  105] Training loss: 0.06087443, Validation loss: 0.06068160, Gradient norm: 0.49969551
INFO:root:[  106] Training loss: 0.06052152, Validation loss: 0.06016373, Gradient norm: 0.50142985
INFO:root:[  107] Training loss: 0.06041572, Validation loss: 0.06047057, Gradient norm: 0.56864722
INFO:root:[  108] Training loss: 0.05990132, Validation loss: 0.05980827, Gradient norm: 0.29645623
INFO:root:[  109] Training loss: 0.05966398, Validation loss: 0.05994380, Gradient norm: 0.32881858
INFO:root:[  110] Training loss: 0.05964001, Validation loss: 0.05956367, Gradient norm: 0.47408232
INFO:root:[  111] Training loss: 0.05978348, Validation loss: 0.06024929, Gradient norm: 0.72106887
INFO:root:[  112] Training loss: 0.05937779, Validation loss: 0.05954859, Gradient norm: 0.63301437
INFO:root:[  113] Training loss: 0.05926958, Validation loss: 0.05899688, Gradient norm: 0.69106709
INFO:root:[  114] Training loss: 0.05909738, Validation loss: 0.05863734, Gradient norm: 0.74906094
INFO:root:[  115] Training loss: 0.05854070, Validation loss: 0.05838393, Gradient norm: 0.38842230
INFO:root:[  116] Training loss: 0.05827032, Validation loss: 0.05827263, Gradient norm: 0.37400989
INFO:root:[  117] Training loss: 0.05812810, Validation loss: 0.05883335, Gradient norm: 0.36934522
INFO:root:[  118] Training loss: 0.05807660, Validation loss: 0.05787982, Gradient norm: 0.64195276
INFO:root:[  119] Training loss: 0.05786465, Validation loss: 0.05810772, Gradient norm: 0.59222919
INFO:root:[  120] Training loss: 0.05790673, Validation loss: 0.05762911, Gradient norm: 0.68433764
INFO:root:[  121] Training loss: 0.05759606, Validation loss: 0.05737155, Gradient norm: 0.64610804
INFO:root:[  122] Training loss: 0.05739489, Validation loss: 0.05740078, Gradient norm: 0.62832326
INFO:root:[  123] Training loss: 0.05690007, Validation loss: 0.05668667, Gradient norm: 0.36865975
INFO:root:[  124] Training loss: 0.05665041, Validation loss: 0.05668345, Gradient norm: 0.36086744
INFO:root:[  125] Training loss: 0.05680573, Validation loss: 0.05680696, Gradient norm: 0.61722621
INFO:root:[  126] Training loss: 0.05665634, Validation loss: 0.05681068, Gradient norm: 0.67401290
INFO:root:[  127] Training loss: 0.05606394, Validation loss: 0.05620934, Gradient norm: 0.26876931
INFO:root:[  128] Training loss: 0.05600437, Validation loss: 0.05648259, Gradient norm: 0.34750215
INFO:root:[  129] Training loss: 0.05606680, Validation loss: 0.05656110, Gradient norm: 0.68223782
INFO:root:[  130] Training loss: 0.05605127, Validation loss: 0.05589216, Gradient norm: 0.74783869
INFO:root:[  131] Training loss: 0.05547801, Validation loss: 0.05542182, Gradient norm: 0.51832252
INFO:root:[  132] Training loss: 0.05551282, Validation loss: 0.05520610, Gradient norm: 0.65572045
INFO:root:[  133] Training loss: 0.05519727, Validation loss: 0.05519913, Gradient norm: 0.54499121
INFO:root:[  134] Training loss: 0.05505066, Validation loss: 0.05484075, Gradient norm: 0.66710505
INFO:root:[  135] Training loss: 0.05475648, Validation loss: 0.05494713, Gradient norm: 0.51719606
INFO:root:[  136] Training loss: 0.05455669, Validation loss: 0.05418468, Gradient norm: 0.51649285
INFO:root:[  137] Training loss: 0.05436363, Validation loss: 0.05437480, Gradient norm: 0.44322189
INFO:root:[  138] Training loss: 0.05408000, Validation loss: 0.05399690, Gradient norm: 0.29895411
INFO:root:[  139] Training loss: 0.05391011, Validation loss: 0.05424846, Gradient norm: 0.43125944
INFO:root:[  140] Training loss: 0.05390019, Validation loss: 0.05346180, Gradient norm: 0.63396056
INFO:root:[  141] Training loss: 0.05389148, Validation loss: 0.05394362, Gradient norm: 0.79682117
INFO:root:[  142] Training loss: 0.05375693, Validation loss: 0.05355980, Gradient norm: 0.82076771
INFO:root:[  143] Training loss: 0.05330085, Validation loss: 0.05329325, Gradient norm: 0.56600318
INFO:root:[  144] Training loss: 0.05302752, Validation loss: 0.05317647, Gradient norm: 0.46688881
INFO:root:[  145] Training loss: 0.05270353, Validation loss: 0.05269995, Gradient norm: 0.23750852
INFO:root:[  146] Training loss: 0.05258109, Validation loss: 0.05265137, Gradient norm: 0.35029060
INFO:root:[  147] Training loss: 0.05249789, Validation loss: 0.05252212, Gradient norm: 0.57044379
INFO:root:[  148] Training loss: 0.05255611, Validation loss: 0.05303342, Gradient norm: 0.78445654
INFO:root:[  149] Training loss: 0.05250457, Validation loss: 0.05194410, Gradient norm: 0.89206204
INFO:root:[  150] Training loss: 0.05193246, Validation loss: 0.05168033, Gradient norm: 0.57356557
INFO:root:[  151] Training loss: 0.05170272, Validation loss: 0.05170267, Gradient norm: 0.57015354
INFO:root:[  152] Training loss: 0.05189551, Validation loss: 0.05181029, Gradient norm: 0.84816634
INFO:root:[  153] Training loss: 0.05150697, Validation loss: 0.05163919, Gradient norm: 0.64382049
INFO:root:[  154] Training loss: 0.05131684, Validation loss: 0.05095219, Gradient norm: 0.67522781
INFO:root:[  155] Training loss: 0.05089913, Validation loss: 0.05114104, Gradient norm: 0.42640563
INFO:root:[  156] Training loss: 0.05099627, Validation loss: 0.05111120, Gradient norm: 0.70306014
INFO:root:[  157] Training loss: 0.05078531, Validation loss: 0.05094171, Gradient norm: 0.67178725
INFO:root:[  158] Training loss: 0.05058414, Validation loss: 0.05092910, Gradient norm: 0.74300777
INFO:root:[  159] Training loss: 0.05034002, Validation loss: 0.05048579, Gradient norm: 0.63733062
INFO:root:[  160] Training loss: 0.05026147, Validation loss: 0.05077979, Gradient norm: 0.74605526
INFO:root:[  161] Training loss: 0.05010456, Validation loss: 0.05016057, Gradient norm: 0.76338918
INFO:root:[  162] Training loss: 0.04970496, Validation loss: 0.04983615, Gradient norm: 0.53053881
INFO:root:[  163] Training loss: 0.04979018, Validation loss: 0.04966701, Gradient norm: 0.77928494
INFO:root:[  164] Training loss: 0.04958145, Validation loss: 0.04949971, Gradient norm: 0.76901778
INFO:root:[  165] Training loss: 0.04924067, Validation loss: 0.04905940, Gradient norm: 0.57610208
INFO:root:[  166] Training loss: 0.04895920, Validation loss: 0.04900790, Gradient norm: 0.35818710
INFO:root:[  167] Training loss: 0.04880973, Validation loss: 0.04920599, Gradient norm: 0.44615740
INFO:root:[  168] Training loss: 0.04860829, Validation loss: 0.04868806, Gradient norm: 0.54153021
INFO:root:[  169] Training loss: 0.04843854, Validation loss: 0.04906673, Gradient norm: 0.40871261
INFO:root:[  170] Training loss: 0.04842056, Validation loss: 0.04846831, Gradient norm: 0.78152769
INFO:root:[  171] Training loss: 0.04813997, Validation loss: 0.04814524, Gradient norm: 0.66145024
INFO:root:[  172] Training loss: 0.04813151, Validation loss: 0.04784996, Gradient norm: 0.85294209
INFO:root:[  173] Training loss: 0.04805722, Validation loss: 0.04748209, Gradient norm: 0.90731577
INFO:root:[  174] Training loss: 0.04771117, Validation loss: 0.04758383, Gradient norm: 0.80545848
INFO:root:[  175] Training loss: 0.04761374, Validation loss: 0.04723981, Gradient norm: 0.82390753
INFO:root:[  176] Training loss: 0.04736830, Validation loss: 0.04740093, Gradient norm: 0.83481081
INFO:root:[  177] Training loss: 0.04711781, Validation loss: 0.04708717, Gradient norm: 0.68043771
INFO:root:[  178] Training loss: 0.04683189, Validation loss: 0.04698722, Gradient norm: 0.57731810
INFO:root:[  179] Training loss: 0.04664162, Validation loss: 0.04693329, Gradient norm: 0.45802990
INFO:root:[  180] Training loss: 0.04667739, Validation loss: 0.04691250, Gradient norm: 0.78260734
INFO:root:[  181] Training loss: 0.04645075, Validation loss: 0.04607021, Gradient norm: 0.76797247
INFO:root:[  182] Training loss: 0.04630963, Validation loss: 0.04635548, Gradient norm: 0.87002186
INFO:root:[  183] Training loss: 0.04605996, Validation loss: 0.04610302, Gradient norm: 0.74111281
INFO:root:[  184] Training loss: 0.04583520, Validation loss: 0.04611763, Gradient norm: 0.70241135
INFO:root:[  185] Training loss: 0.04573661, Validation loss: 0.04550157, Gradient norm: 0.75466724
INFO:root:[  186] Training loss: 0.04559564, Validation loss: 0.04604227, Gradient norm: 0.79294475
INFO:root:[  187] Training loss: 0.04541744, Validation loss: 0.04496930, Gradient norm: 0.83816199
INFO:root:[  188] Training loss: 0.04531291, Validation loss: 0.04490065, Gradient norm: 0.92744172
INFO:root:[  189] Training loss: 0.04529214, Validation loss: 0.04539647, Gradient norm: 1.01754777
INFO:root:[  190] Training loss: 0.04492473, Validation loss: 0.04456441, Gradient norm: 0.77090535
INFO:root:[  191] Training loss: 0.04447738, Validation loss: 0.04441291, Gradient norm: 0.36998921
INFO:root:[  192] Training loss: 0.04429297, Validation loss: 0.04424274, Gradient norm: 0.36682506
INFO:root:[  193] Training loss: 0.04446418, Validation loss: 0.04528982, Gradient norm: 0.82750470
INFO:root:[  194] Training loss: 0.04443205, Validation loss: 0.04439590, Gradient norm: 1.05995409
INFO:root:[  195] Training loss: 0.04406212, Validation loss: 0.04403464, Gradient norm: 0.82181599
INFO:root:[  196] Training loss: 0.04384762, Validation loss: 0.04363468, Gradient norm: 0.77633191
INFO:root:[  197] Training loss: 0.04360048, Validation loss: 0.04333212, Gradient norm: 0.62675081
INFO:root:[  198] Training loss: 0.04357110, Validation loss: 0.04328850, Gradient norm: 0.90988974
INFO:root:[  199] Training loss: 0.04337289, Validation loss: 0.04312264, Gradient norm: 0.88366234
INFO:root:[  200] Training loss: 0.04317694, Validation loss: 0.04329063, Gradient norm: 0.82427617
INFO:root:[  201] Training loss: 0.04299576, Validation loss: 0.04271449, Gradient norm: 0.73459309
INFO:root:[  202] Training loss: 0.04287427, Validation loss: 0.04293744, Gradient norm: 0.83426751
INFO:root:[  203] Training loss: 0.04278020, Validation loss: 0.04339389, Gradient norm: 0.93046988
INFO:root:[  204] Training loss: 0.04286792, Validation loss: 0.04264672, Gradient norm: 1.22820281
INFO:root:[  205] Training loss: 0.04251062, Validation loss: 0.04211483, Gradient norm: 1.00709286
INFO:root:[  206] Training loss: 0.04217231, Validation loss: 0.04240852, Gradient norm: 0.77171018
INFO:root:[  207] Training loss: 0.04209018, Validation loss: 0.04168676, Gradient norm: 0.89313654
INFO:root:[  208] Training loss: 0.04187902, Validation loss: 0.04179134, Gradient norm: 0.77525903
INFO:root:[  209] Training loss: 0.04165916, Validation loss: 0.04152044, Gradient norm: 0.74712174
INFO:root:[  210] Training loss: 0.04167519, Validation loss: 0.04192761, Gradient norm: 0.96980600
INFO:root:[  211] Training loss: 0.04149899, Validation loss: 0.04177494, Gradient norm: 0.96165238
INFO:root:[  212] Training loss: 0.04139084, Validation loss: 0.04144476, Gradient norm: 0.97578497
INFO:root:[  213] Training loss: 0.04131701, Validation loss: 0.04085769, Gradient norm: 1.13870655
INFO:root:[  214] Training loss: 0.04100507, Validation loss: 0.04078292, Gradient norm: 0.95469710
INFO:root:[  215] Training loss: 0.04076546, Validation loss: 0.04114657, Gradient norm: 0.70992432
INFO:root:[  216] Training loss: 0.04090709, Validation loss: 0.04090848, Gradient norm: 1.09900017
INFO:root:[  217] Training loss: 0.04067785, Validation loss: 0.04041067, Gradient norm: 1.08719430
INFO:root:[  218] Training loss: 0.04026965, Validation loss: 0.04023390, Gradient norm: 0.59241629
INFO:root:[  219] Training loss: 0.04046968, Validation loss: 0.04071245, Gradient norm: 1.07235192
INFO:root:[  220] Training loss: 0.04008820, Validation loss: 0.04019898, Gradient norm: 0.79676228
INFO:root:[  221] Training loss: 0.03993389, Validation loss: 0.03998958, Gradient norm: 0.80449273
INFO:root:[  222] Training loss: 0.03976858, Validation loss: 0.03957012, Gradient norm: 0.81238550
INFO:root:[  223] Training loss: 0.03980771, Validation loss: 0.04032248, Gradient norm: 1.03812493
INFO:root:[  224] Training loss: 0.03970426, Validation loss: 0.03971035, Gradient norm: 1.14156202
INFO:root:[  225] Training loss: 0.03948390, Validation loss: 0.04044613, Gradient norm: 1.05445604
INFO:root:[  226] Training loss: 0.03927541, Validation loss: 0.03947723, Gradient norm: 0.89027383
INFO:root:[  227] Training loss: 0.03918250, Validation loss: 0.03922654, Gradient norm: 0.95516563
INFO:root:[  228] Training loss: 0.03910369, Validation loss: 0.03878712, Gradient norm: 1.01928208
INFO:root:[  229] Training loss: 0.03893615, Validation loss: 0.03863724, Gradient norm: 1.06501553
INFO:root:[  230] Training loss: 0.03852835, Validation loss: 0.03868471, Gradient norm: 0.35013819
INFO:root:[  231] Training loss: 0.03853865, Validation loss: 0.03860554, Gradient norm: 0.92589288
INFO:root:[  232] Training loss: 0.03849920, Validation loss: 0.03856761, Gradient norm: 1.07028667
INFO:root:[  233] Training loss: 0.03850588, Validation loss: 0.03807967, Gradient norm: 1.20964337
INFO:root:[  234] Training loss: 0.03824010, Validation loss: 0.03803839, Gradient norm: 1.08744644
INFO:root:[  235] Training loss: 0.03809957, Validation loss: 0.03819476, Gradient norm: 1.02714315
INFO:root:[  236] Training loss: 0.03801542, Validation loss: 0.03782135, Gradient norm: 1.10925031
INFO:root:[  237] Training loss: 0.03799957, Validation loss: 0.03737937, Gradient norm: 1.21093422
INFO:root:[  238] Training loss: 0.03782607, Validation loss: 0.03767795, Gradient norm: 1.29559062
INFO:root:[  239] Training loss: 0.03753340, Validation loss: 0.03738423, Gradient norm: 0.97999335
INFO:root:[  240] Training loss: 0.03755911, Validation loss: 0.03729053, Gradient norm: 1.25117360
INFO:root:[  241] Training loss: 0.03721468, Validation loss: 0.03719765, Gradient norm: 0.89569094
INFO:root:[  242] Training loss: 0.03716946, Validation loss: 0.03696281, Gradient norm: 1.09500550
INFO:root:[  243] Training loss: 0.03704757, Validation loss: 0.03685040, Gradient norm: 1.03603752
INFO:root:[  244] Training loss: 0.03698823, Validation loss: 0.03694318, Gradient norm: 1.13824336
INFO:root:[  245] Training loss: 0.03694455, Validation loss: 0.03667680, Gradient norm: 1.35443983
INFO:root:[  246] Training loss: 0.03683379, Validation loss: 0.03663809, Gradient norm: 1.38158941
INFO:root:[  247] Training loss: 0.03653753, Validation loss: 0.03683617, Gradient norm: 1.07376225
INFO:root:[  248] Training loss: 0.03645864, Validation loss: 0.03674933, Gradient norm: 0.97979103
INFO:root:[  249] Training loss: 0.03628397, Validation loss: 0.03620670, Gradient norm: 1.00400431
INFO:root:[  250] Training loss: 0.03619150, Validation loss: 0.03613452, Gradient norm: 1.02722678
INFO:root:[  251] Training loss: 0.03615346, Validation loss: 0.03617357, Gradient norm: 1.16964385
INFO:root:[  252] Training loss: 0.03612328, Validation loss: 0.03680161, Gradient norm: 1.31621653
INFO:root:[  253] Training loss: 0.03591770, Validation loss: 0.03608562, Gradient norm: 1.21241530
INFO:root:[  254] Training loss: 0.03573965, Validation loss: 0.03605948, Gradient norm: 1.16230063
INFO:root:[  255] Training loss: 0.03564709, Validation loss: 0.03551065, Gradient norm: 1.23195800
INFO:root:[  256] Training loss: 0.03548353, Validation loss: 0.03580812, Gradient norm: 1.09636787
INFO:root:[  257] Training loss: 0.03553765, Validation loss: 0.03563326, Gradient norm: 1.27920900
INFO:root:[  258] Training loss: 0.03534381, Validation loss: 0.03549528, Gradient norm: 1.28316313
INFO:root:[  259] Training loss: 0.03530374, Validation loss: 0.03571308, Gradient norm: 1.37541910
INFO:root:[  260] Training loss: 0.03515112, Validation loss: 0.03538382, Gradient norm: 1.31937568
INFO:root:[  261] Training loss: 0.03503791, Validation loss: 0.03496107, Gradient norm: 1.36460269
INFO:root:[  262] Training loss: 0.03486426, Validation loss: 0.03493365, Gradient norm: 1.19237698
INFO:root:[  263] Training loss: 0.03481244, Validation loss: 0.03505897, Gradient norm: 1.34205988
INFO:root:[  264] Training loss: 0.03486832, Validation loss: 0.03484092, Gradient norm: 1.59316421
INFO:root:[  265] Training loss: 0.03462234, Validation loss: 0.03488041, Gradient norm: 1.24234052
INFO:root:[  266] Training loss: 0.03448283, Validation loss: 0.03431274, Gradient norm: 1.09234365
INFO:root:[  267] Training loss: 0.03441289, Validation loss: 0.03410090, Gradient norm: 1.32994597
INFO:root:[  268] Training loss: 0.03430854, Validation loss: 0.03447963, Gradient norm: 1.36125288
INFO:root:[  269] Training loss: 0.03420277, Validation loss: 0.03441362, Gradient norm: 1.47232850
INFO:root:[  270] Training loss: 0.03423609, Validation loss: 0.03386691, Gradient norm: 1.59748211
INFO:root:[  271] Training loss: 0.03402718, Validation loss: 0.03404363, Gradient norm: 1.36069032
INFO:root:[  272] Training loss: 0.03389496, Validation loss: 0.03404534, Gradient norm: 1.37676239
INFO:root:[  273] Training loss: 0.03389845, Validation loss: 0.03361525, Gradient norm: 1.50181394
INFO:root:[  274] Training loss: 0.03386517, Validation loss: 0.03352241, Gradient norm: 1.78039290
INFO:root:[  275] Training loss: 0.03360489, Validation loss: 0.03358519, Gradient norm: 1.30614783
INFO:root:[  276] Training loss: 0.03349193, Validation loss: 0.03358222, Gradient norm: 1.35460897
INFO:root:[  277] Training loss: 0.03366643, Validation loss: 0.03370393, Gradient norm: 1.72884671
INFO:root:[  278] Training loss: 0.03323086, Validation loss: 0.03339333, Gradient norm: 0.97799631
INFO:root:[  279] Training loss: 0.03329850, Validation loss: 0.03357292, Gradient norm: 1.28828477
INFO:root:[  280] Training loss: 0.03323202, Validation loss: 0.03323907, Gradient norm: 1.49844479
INFO:root:[  281] Training loss: 0.03311145, Validation loss: 0.03282706, Gradient norm: 1.51708554
INFO:root:[  282] Training loss: 0.03308245, Validation loss: 0.03290801, Gradient norm: 1.52653322
INFO:root:[  283] Training loss: 0.03291232, Validation loss: 0.03322756, Gradient norm: 1.41147912
INFO:root:[  284] Training loss: 0.03292822, Validation loss: 0.03308037, Gradient norm: 1.50927290
INFO:root:[  285] Training loss: 0.03282310, Validation loss: 0.03305130, Gradient norm: 1.58611079
INFO:root:[  286] Training loss: 0.03263149, Validation loss: 0.03300680, Gradient norm: 1.31953029
INFO:root:[  287] Training loss: 0.03255231, Validation loss: 0.03289152, Gradient norm: 1.34581150
INFO:root:[  288] Training loss: 0.03269680, Validation loss: 0.03265678, Gradient norm: 1.95029776
INFO:root:[  289] Training loss: 0.03240812, Validation loss: 0.03222006, Gradient norm: 1.48659240
INFO:root:[  290] Training loss: 0.03221901, Validation loss: 0.03233666, Gradient norm: 1.31106395
INFO:root:[  291] Training loss: 0.03223248, Validation loss: 0.03205114, Gradient norm: 1.42193382
INFO:root:[  292] Training loss: 0.03221653, Validation loss: 0.03210875, Gradient norm: 1.64785185
INFO:root:[  293] Training loss: 0.03209213, Validation loss: 0.03199575, Gradient norm: 1.74769324
INFO:root:[  294] Training loss: 0.03203765, Validation loss: 0.03232961, Gradient norm: 1.80655379
INFO:root:[  295] Training loss: 0.03198912, Validation loss: 0.03213978, Gradient norm: 1.81366382
INFO:root:[  296] Training loss: 0.03194611, Validation loss: 0.03162446, Gradient norm: 2.04398340
INFO:root:[  297] Training loss: 0.03181871, Validation loss: 0.03205399, Gradient norm: 1.79856311
INFO:root:[  298] Training loss: 0.03178001, Validation loss: 0.03146785, Gradient norm: 2.03052692
INFO:root:[  299] Training loss: 0.03173829, Validation loss: 0.03187932, Gradient norm: 2.06288874
INFO:root:[  300] Training loss: 0.03142473, Validation loss: 0.03145928, Gradient norm: 1.35152962
INFO:root:[  301] Training loss: 0.03128590, Validation loss: 0.03205684, Gradient norm: 1.28344137
INFO:root:[  302] Training loss: 0.03156393, Validation loss: 0.03159898, Gradient norm: 2.21794298
INFO:root:[  303] Training loss: 0.03119162, Validation loss: 0.03138253, Gradient norm: 1.52497019
INFO:root:[  304] Training loss: 0.03117299, Validation loss: 0.03111927, Gradient norm: 1.66716596
INFO:root:[  305] Training loss: 0.03116072, Validation loss: 0.03172581, Gradient norm: 2.15101272
INFO:root:[  306] Training loss: 0.03112830, Validation loss: 0.03079034, Gradient norm: 2.21740158
INFO:root:[  307] Training loss: 0.03104592, Validation loss: 0.03094796, Gradient norm: 2.00672706
INFO:root:[  308] Training loss: 0.03090821, Validation loss: 0.03126551, Gradient norm: 1.96425329
INFO:root:[  309] Training loss: 0.03080816, Validation loss: 0.03115635, Gradient norm: 1.95898436
INFO:root:[  310] Training loss: 0.03066345, Validation loss: 0.03056680, Gradient norm: 1.75760482
INFO:root:[  311] Training loss: 0.03082797, Validation loss: 0.03047112, Gradient norm: 2.33409090
INFO:root:[  312] Training loss: 0.03058300, Validation loss: 0.03048170, Gradient norm: 2.03108057
INFO:root:[  313] Training loss: 0.03059484, Validation loss: 0.03146822, Gradient norm: 2.13405058
INFO:root:[  314] Training loss: 0.03046724, Validation loss: 0.03050908, Gradient norm: 2.05840238
INFO:root:[  315] Training loss: 0.03023123, Validation loss: 0.03053540, Gradient norm: 1.49783728
INFO:root:[  316] Training loss: 0.03038585, Validation loss: 0.03060051, Gradient norm: 2.13056028
INFO:root:[  317] Training loss: 0.03021490, Validation loss: 0.03062525, Gradient norm: 2.16451711
INFO:root:[  318] Training loss: 0.03028533, Validation loss: 0.02996955, Gradient norm: 2.35304781
INFO:root:[  319] Training loss: 0.03003916, Validation loss: 0.02996101, Gradient norm: 1.86227291
INFO:root:[  320] Training loss: 0.03000246, Validation loss: 0.03011217, Gradient norm: 2.10012271
INFO:root:[  321] Training loss: 0.03000081, Validation loss: 0.03014891, Gradient norm: 2.37940418
INFO:root:[  322] Training loss: 0.02990852, Validation loss: 0.02955910, Gradient norm: 2.27939087
INFO:root:[  323] Training loss: 0.03002369, Validation loss: 0.02961349, Gradient norm: 2.70202972
INFO:root:[  324] Training loss: 0.02997513, Validation loss: 0.02975078, Gradient norm: 3.05965022
INFO:root:[  325] Training loss: 0.02965634, Validation loss: 0.02939130, Gradient norm: 2.25495482
INFO:root:[  326] Training loss: 0.02966007, Validation loss: 0.03003062, Gradient norm: 2.45861159
INFO:root:[  327] Training loss: 0.02952139, Validation loss: 0.02935359, Gradient norm: 2.10326570
INFO:root:[  328] Training loss: 0.02952443, Validation loss: 0.02982293, Gradient norm: 2.49162694
INFO:root:[  329] Training loss: 0.02959016, Validation loss: 0.02959470, Gradient norm: 3.18393053
INFO:root:[  330] Training loss: 0.02936379, Validation loss: 0.03003454, Gradient norm: 2.44461196
INFO:root:[  331] Training loss: 0.02943147, Validation loss: 0.02964993, Gradient norm: 3.02665139
INFO:root:[  332] Training loss: 0.02906455, Validation loss: 0.02923587, Gradient norm: 2.12209642
INFO:root:[  333] Training loss: 0.02903572, Validation loss: 0.02920679, Gradient norm: 2.53878508
INFO:root:[  334] Training loss: 0.02898909, Validation loss: 0.02882971, Gradient norm: 2.25375114
INFO:root:[  335] Training loss: 0.02899332, Validation loss: 0.02944701, Gradient norm: 2.85231920
INFO:root:[  336] Training loss: 0.02909867, Validation loss: 0.02860746, Gradient norm: 3.62086739
INFO:root:[  337] Training loss: 0.02885508, Validation loss: 0.02934130, Gradient norm: 2.71284455
INFO:root:[  338] Training loss: 0.02886273, Validation loss: 0.02855733, Gradient norm: 2.99564940
INFO:root:[  339] Training loss: 0.02865542, Validation loss: 0.02913775, Gradient norm: 2.04155887
INFO:root:[  340] Training loss: 0.02868132, Validation loss: 0.02880540, Gradient norm: 3.29674207
INFO:root:[  341] Training loss: 0.02853070, Validation loss: 0.02846966, Gradient norm: 2.50333423
INFO:root:[  342] Training loss: 0.02854734, Validation loss: 0.02843007, Gradient norm: 2.82641057
INFO:root:[  343] Training loss: 0.02857811, Validation loss: 0.02831380, Gradient norm: 3.59698452
INFO:root:[  344] Training loss: 0.02849985, Validation loss: 0.02869649, Gradient norm: 3.88054993
INFO:root:[  345] Training loss: 0.02844183, Validation loss: 0.02893339, Gradient norm: 3.94663256
INFO:root:[  346] Training loss: 0.02813462, Validation loss: 0.02815549, Gradient norm: 2.70586552
INFO:root:[  347] Training loss: 0.02819489, Validation loss: 0.02810045, Gradient norm: 3.52130737
INFO:root:[  348] Training loss: 0.02806435, Validation loss: 0.02799609, Gradient norm: 2.79810271
INFO:root:[  349] Training loss: 0.02798592, Validation loss: 0.02788749, Gradient norm: 3.18670091
INFO:root:[  350] Training loss: 0.02800156, Validation loss: 0.02843264, Gradient norm: 3.40281458
INFO:root:[  351] Training loss: 0.02805821, Validation loss: 0.02798392, Gradient norm: 4.08051148
INFO:root:[  352] Training loss: 0.02799961, Validation loss: 0.02778627, Gradient norm: 3.89585378
INFO:root:[  353] Training loss: 0.02770611, Validation loss: 0.02790397, Gradient norm: 3.08102011
INFO:root:[  354] Training loss: 0.02767857, Validation loss: 0.02742317, Gradient norm: 3.55200659
INFO:root:[  355] Training loss: 0.02775254, Validation loss: 0.02738909, Gradient norm: 3.83976589
INFO:root:[  356] Training loss: 0.02785843, Validation loss: 0.02751302, Gradient norm: 5.16818827
INFO:root:[  357] Training loss: 0.02747013, Validation loss: 0.02788570, Gradient norm: 3.96285454
INFO:root:[  358] Training loss: 0.02730624, Validation loss: 0.02757254, Gradient norm: 2.89093533
INFO:root:[  359] Training loss: 0.02741864, Validation loss: 0.02813512, Gradient norm: 4.26790683
INFO:root:[  360] Training loss: 0.02736568, Validation loss: 0.02769739, Gradient norm: 3.94877826
INFO:root:[  361] Training loss: 0.02719185, Validation loss: 0.02708635, Gradient norm: 2.99530507
INFO:root:[  362] Training loss: 0.02736758, Validation loss: 0.02769557, Gradient norm: 4.81262068
INFO:root:[  363] Training loss: 0.02726381, Validation loss: 0.02759796, Gradient norm: 4.20285352
INFO:root:[  364] Training loss: 0.02710959, Validation loss: 0.02702743, Gradient norm: 4.04662127
INFO:root:[  365] Training loss: 0.02703173, Validation loss: 0.02674528, Gradient norm: 4.18993552
INFO:root:[  366] Training loss: 0.02697789, Validation loss: 0.02710170, Gradient norm: 4.26122087
INFO:root:[  367] Training loss: 0.02705121, Validation loss: 0.02693382, Gradient norm: 4.94923652
INFO:root:[  368] Training loss: 0.02677800, Validation loss: 0.02685897, Gradient norm: 3.80288563
INFO:root:[  369] Training loss: 0.02703582, Validation loss: 0.02702870, Gradient norm: 5.41167312
INFO:root:[  370] Training loss: 0.02673723, Validation loss: 0.02739919, Gradient norm: 4.05829588
INFO:root:[  371] Training loss: 0.02674966, Validation loss: 0.02703235, Gradient norm: 4.24021511
INFO:root:[  372] Training loss: 0.02676302, Validation loss: 0.02676505, Gradient norm: 5.44555171
INFO:root:[  373] Training loss: 0.02682978, Validation loss: 0.02678993, Gradient norm: 5.90437235
INFO:root:[  374] Training loss: 0.02670779, Validation loss: 0.02641452, Gradient norm: 5.31994533
INFO:root:[  375] Training loss: 0.02646908, Validation loss: 0.02603611, Gradient norm: 4.30946021
INFO:root:[  376] Training loss: 0.02636459, Validation loss: 0.02631210, Gradient norm: 3.85698881
INFO:root:[  377] Training loss: 0.02672724, Validation loss: 0.02702209, Gradient norm: 6.53412486
INFO:root:[  378] Training loss: 0.02652465, Validation loss: 0.02635194, Gradient norm: 5.66860304
INFO:root:[  379] Training loss: 0.02626545, Validation loss: 0.02602225, Gradient norm: 4.80269153
INFO:root:[  380] Training loss: 0.02637561, Validation loss: 0.02668152, Gradient norm: 5.59956454
INFO:root:[  381] Training loss: 0.02624444, Validation loss: 0.02618705, Gradient norm: 5.82519380
INFO:root:[  382] Training loss: 0.02613696, Validation loss: 0.02682619, Gradient norm: 5.40973461
INFO:root:[  383] Training loss: 0.02620012, Validation loss: 0.02573936, Gradient norm: 5.89078808
INFO:root:[  384] Training loss: 0.02615150, Validation loss: 0.02584994, Gradient norm: 5.29888320
INFO:root:[  385] Training loss: 0.02578995, Validation loss: 0.02566594, Gradient norm: 4.10886875
INFO:root:[  386] Training loss: 0.02591759, Validation loss: 0.02607553, Gradient norm: 4.57151201
INFO:root:[  387] Training loss: 0.02581918, Validation loss: 0.02593430, Gradient norm: 4.38148953
INFO:root:[  388] Training loss: 0.02569444, Validation loss: 0.02573245, Gradient norm: 3.87030882
INFO:root:[  389] Training loss: 0.02580109, Validation loss: 0.02659629, Gradient norm: 3.90108306
INFO:root:[  390] Training loss: 0.02573220, Validation loss: 0.02557013, Gradient norm: 5.10112443
INFO:root:[  391] Training loss: 0.02569676, Validation loss: 0.02524461, Gradient norm: 4.91026974
INFO:root:[  392] Training loss: 0.02563658, Validation loss: 0.02532077, Gradient norm: 5.23760802
INFO:root:[  393] Training loss: 0.02548190, Validation loss: 0.02566600, Gradient norm: 4.82186041
INFO:root:[  394] Training loss: 0.02569582, Validation loss: 0.02527174, Gradient norm: 6.15250151
INFO:root:[  395] Training loss: 0.02543758, Validation loss: 0.02532719, Gradient norm: 5.25312162
INFO:root:[  396] Training loss: 0.02544683, Validation loss: 0.02498540, Gradient norm: 5.87137953
INFO:root:[  397] Training loss: 0.02543496, Validation loss: 0.02700089, Gradient norm: 6.15278240
INFO:root:[  398] Training loss: 0.02550723, Validation loss: 0.02577884, Gradient norm: 7.27189623
INFO:root:[  399] Training loss: 0.02535234, Validation loss: 0.02499700, Gradient norm: 5.90792265
INFO:root:[  400] Training loss: 0.02517160, Validation loss: 0.02524116, Gradient norm: 5.60473554
INFO:root:[  401] Training loss: 0.02519335, Validation loss: 0.02566869, Gradient norm: 6.00841449
INFO:root:[  402] Training loss: 0.02506957, Validation loss: 0.02504866, Gradient norm: 5.71385849
INFO:root:[  403] Training loss: 0.02513674, Validation loss: 0.02490051, Gradient norm: 6.34572066
INFO:root:[  404] Training loss: 0.02504565, Validation loss: 0.02596776, Gradient norm: 6.89669614
INFO:root:[  405] Training loss: 0.02525436, Validation loss: 0.02472653, Gradient norm: 8.40851290
INFO:root:[  406] Training loss: 0.02462804, Validation loss: 0.02482116, Gradient norm: 3.60351845
INFO:root:[  407] Training loss: 0.02479866, Validation loss: 0.02468667, Gradient norm: 6.37955803
INFO:root:[  408] Training loss: 0.02466580, Validation loss: 0.02490867, Gradient norm: 4.84109064
INFO:root:[  409] Training loss: 0.02497271, Validation loss: 0.02717441, Gradient norm: 8.34946588
INFO:root:[  410] Training loss: 0.02451038, Validation loss: 0.02556324, Gradient norm: 4.14955227
INFO:root:[  411] Training loss: 0.02481657, Validation loss: 0.02439785, Gradient norm: 7.94735437
INFO:root:[  412] Training loss: 0.02477397, Validation loss: 0.02468547, Gradient norm: 8.27811693
INFO:root:[  413] Training loss: 0.02477222, Validation loss: 0.02460718, Gradient norm: 8.04539193
INFO:root:[  414] Training loss: 0.02428120, Validation loss: 0.02475683, Gradient norm: 5.16788894
INFO:root:[  415] Training loss: 0.02441946, Validation loss: 0.02434525, Gradient norm: 7.15439324
INFO:root:[  416] Training loss: 0.02483438, Validation loss: 0.02575098, Gradient norm: 9.74981624
INFO:root:[  417] Training loss: 0.02447713, Validation loss: 0.02388246, Gradient norm: 7.50396669
INFO:root:[  418] Training loss: 0.02413980, Validation loss: 0.02384977, Gradient norm: 5.49044399
INFO:root:[  419] Training loss: 0.02414770, Validation loss: 0.02485427, Gradient norm: 7.00074533
INFO:root:[  420] Training loss: 0.02444352, Validation loss: 0.02409771, Gradient norm: 9.62652961
INFO:root:[  421] Training loss: 0.02410949, Validation loss: 0.02439479, Gradient norm: 7.50858342
INFO:root:[  422] Training loss: 0.02417239, Validation loss: 0.02359390, Gradient norm: 8.24938472
INFO:root:[  423] Training loss: 0.02393142, Validation loss: 0.02385235, Gradient norm: 6.77328224
INFO:root:[  424] Training loss: 0.02392281, Validation loss: 0.02359686, Gradient norm: 7.07049851
INFO:root:[  425] Training loss: 0.02389022, Validation loss: 0.02511435, Gradient norm: 8.09892611
INFO:root:[  426] Training loss: 0.02376154, Validation loss: 0.02348767, Gradient norm: 6.37897866
INFO:root:[  427] Training loss: 0.02364068, Validation loss: 0.02363644, Gradient norm: 5.49506255
INFO:root:[  428] Training loss: 0.02407002, Validation loss: 0.02354974, Gradient norm: 10.36750197
INFO:root:[  429] Training loss: 0.02376805, Validation loss: 0.02386431, Gradient norm: 8.66519785
INFO:root:[  430] Training loss: 0.02363000, Validation loss: 0.02474546, Gradient norm: 7.43620566
INFO:root:[  431] Training loss: 0.02381765, Validation loss: 0.02402413, Gradient norm: 9.47292751
INFO:root:[  432] Training loss: 0.02341207, Validation loss: 0.02342542, Gradient norm: 5.94826144
INFO:root:[  433] Training loss: 0.02363665, Validation loss: 0.02621982, Gradient norm: 9.06357407
INFO:root:[  434] Training loss: 0.02468115, Validation loss: 0.02311160, Gradient norm: 16.33340521
INFO:root:[  435] Training loss: 0.02342892, Validation loss: 0.02347998, Gradient norm: 8.88947930
INFO:root:[  436] Training loss: 0.02319528, Validation loss: 0.02374125, Gradient norm: 6.02256985
INFO:root:[  437] Training loss: 0.02320731, Validation loss: 0.02411725, Gradient norm: 6.80866971
INFO:root:[  438] Training loss: 0.02356080, Validation loss: 0.02279800, Gradient norm: 11.10718987
INFO:root:[  439] Training loss: 0.02312916, Validation loss: 0.02376967, Gradient norm: 7.22251431
INFO:root:[  440] Training loss: 0.02329398, Validation loss: 0.02312992, Gradient norm: 9.23008130
INFO:root:[  441] Training loss: 0.02308319, Validation loss: 0.02291642, Gradient norm: 8.37216167
INFO:root:[  442] Training loss: 0.02324704, Validation loss: 0.02398981, Gradient norm: 10.02521596
INFO:root:[  443] Training loss: 0.02329694, Validation loss: 0.02403833, Gradient norm: 10.74357621
INFO:root:[  444] Training loss: 0.02294579, Validation loss: 0.02273509, Gradient norm: 8.69898938
INFO:root:[  445] Training loss: 0.02304108, Validation loss: 0.02251438, Gradient norm: 9.43650674
INFO:root:[  446] Training loss: 0.02302111, Validation loss: 0.02249668, Gradient norm: 9.71946452
INFO:root:[  447] Training loss: 0.02303828, Validation loss: 0.02360577, Gradient norm: 9.80085260
INFO:root:[  448] Training loss: 0.02311807, Validation loss: 0.02541459, Gradient norm: 12.23463907
INFO:root:[  449] Training loss: 0.02284039, Validation loss: 0.02273482, Gradient norm: 9.73365777
INFO:root:[  450] Training loss: 0.02335331, Validation loss: 0.02256734, Gradient norm: 13.61734060
INFO:root:[  451] Training loss: 0.02237704, Validation loss: 0.02238255, Gradient norm: 6.20063445
INFO:root:[  452] Training loss: 0.02260740, Validation loss: 0.02276575, Gradient norm: 8.96204791
INFO:root:[  453] Training loss: 0.02241702, Validation loss: 0.02330696, Gradient norm: 8.05951558
INFO:root:[  454] Training loss: 0.02281274, Validation loss: 0.02238219, Gradient norm: 12.12727529
INFO:root:[  455] Training loss: 0.02256536, Validation loss: 0.02269415, Gradient norm: 10.39243370
INFO:root:[  456] Training loss: 0.02270823, Validation loss: 0.02216914, Gradient norm: 11.17841897
INFO:root:[  457] Training loss: 0.02252275, Validation loss: 0.02199331, Gradient norm: 10.62549280
INFO:root:[  458] Training loss: 0.02275054, Validation loss: 0.02298183, Gradient norm: 12.44547707
INFO:root:[  459] Training loss: 0.02270520, Validation loss: 0.02364657, Gradient norm: 13.18850870
INFO:root:[  460] Training loss: 0.02230144, Validation loss: 0.02181326, Gradient norm: 9.76794789
INFO:root:[  461] Training loss: 0.02220300, Validation loss: 0.02239309, Gradient norm: 8.64359556
INFO:root:[  462] Training loss: 0.02240805, Validation loss: 0.02267322, Gradient norm: 11.74476084
INFO:root:[  463] Training loss: 0.02275393, Validation loss: 0.02317761, Gradient norm: 15.06562200
INFO:root:[  464] Training loss: 0.02207661, Validation loss: 0.02288160, Gradient norm: 9.38905600
INFO:root:[  465] Training loss: 0.02203249, Validation loss: 0.02263126, Gradient norm: 9.35008528
INFO:root:[  466] Training loss: 0.02218365, Validation loss: 0.02202471, Gradient norm: 9.77677696
INFO:root:[  467] Training loss: 0.02204329, Validation loss: 0.02195536, Gradient norm: 10.04821776
INFO:root:[  468] Training loss: 0.02252701, Validation loss: 0.02207823, Gradient norm: 14.63906194
INFO:root:[  469] Training loss: 0.02188652, Validation loss: 0.02338500, Gradient norm: 10.41946799
INFO:root:EP 469: Early stopping
INFO:root:Training the model took 15613.313s.
INFO:root:Emptying the cuda cache took 0.08s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.41689
INFO:root:EnergyScoreTrain: 0.3503
INFO:root:CoverageTrain: 0.99637
INFO:root:IntervalWidthTrain: 0.05507
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.39248
INFO:root:EnergyScoreValidation: 0.32958
INFO:root:CoverageValidation: 0.99636
INFO:root:IntervalWidthValidation: 0.05524
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.36622
INFO:root:EnergyScoreTest: 0.30842
INFO:root:CoverageTest: 0.99639
INFO:root:IntervalWidthTest: 0.05447
INFO:root:###7 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'scoring-rule-dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.35048628, Validation loss: 0.90662000, Gradient norm: 6.70694566
INFO:root:[    2] Training loss: 0.62273520, Validation loss: 0.53018154, Gradient norm: 2.50912505
INFO:root:[    3] Training loss: 0.51294961, Validation loss: 0.48591126, Gradient norm: 1.53880960
INFO:root:[    4] Training loss: 0.45694042, Validation loss: 0.44109738, Gradient norm: 1.54902196
INFO:root:[    5] Training loss: 0.41185870, Validation loss: 0.38012305, Gradient norm: 1.42668103
INFO:root:[    6] Training loss: 0.37320703, Validation loss: 0.35074487, Gradient norm: 1.27805915
INFO:root:[    7] Training loss: 0.33923275, Validation loss: 0.30598033, Gradient norm: 1.15114734
INFO:root:[    8] Training loss: 0.31206193, Validation loss: 0.28723065, Gradient norm: 1.00624153
INFO:root:[    9] Training loss: 0.28068340, Validation loss: 0.27318312, Gradient norm: 0.99764775
INFO:root:[   10] Training loss: 0.26273959, Validation loss: 0.25053562, Gradient norm: 0.83442866
INFO:root:[   11] Training loss: 0.24087665, Validation loss: 0.23325914, Gradient norm: 0.88954351
INFO:root:[   12] Training loss: 0.22348177, Validation loss: 0.21504740, Gradient norm: 0.71744014
INFO:root:[   13] Training loss: 0.20926386, Validation loss: 0.20848168, Gradient norm: 0.72905120
INFO:root:[   14] Training loss: 0.19751420, Validation loss: 0.19002485, Gradient norm: 0.64939859
INFO:root:[   15] Training loss: 0.18533949, Validation loss: 0.18004422, Gradient norm: 0.64968003
INFO:root:[   16] Training loss: 0.17718673, Validation loss: 0.17375362, Gradient norm: 0.52448290
INFO:root:[   17] Training loss: 0.16956090, Validation loss: 0.16543384, Gradient norm: 0.45422738
INFO:root:[   18] Training loss: 0.16374185, Validation loss: 0.16333787, Gradient norm: 0.60640019
INFO:root:[   19] Training loss: 0.15897841, Validation loss: 0.15739702, Gradient norm: 0.50697572
INFO:root:[   20] Training loss: 0.15426080, Validation loss: 0.15100876, Gradient norm: 0.39797459
INFO:root:[   21] Training loss: 0.14993945, Validation loss: 0.14981404, Gradient norm: 0.33095974
INFO:root:[   22] Training loss: 0.14771097, Validation loss: 0.14500052, Gradient norm: 0.47651058
INFO:root:[   23] Training loss: 0.14352706, Validation loss: 0.14273373, Gradient norm: 0.32918028
INFO:root:[   24] Training loss: 0.14051623, Validation loss: 0.13900395, Gradient norm: 0.38803767
INFO:root:[   25] Training loss: 0.13825347, Validation loss: 0.13662852, Gradient norm: 0.39618891
INFO:root:[   26] Training loss: 0.13525444, Validation loss: 0.13462570, Gradient norm: 0.32885185
INFO:root:[   27] Training loss: 0.13358140, Validation loss: 0.13208790, Gradient norm: 0.33759747
INFO:root:[   28] Training loss: 0.13070685, Validation loss: 0.12955736, Gradient norm: 0.29517464
INFO:root:[   29] Training loss: 0.12901614, Validation loss: 0.12865568, Gradient norm: 0.34671574
INFO:root:[   30] Training loss: 0.12689177, Validation loss: 0.12670582, Gradient norm: 0.35622977
INFO:root:[   31] Training loss: 0.12501385, Validation loss: 0.12472299, Gradient norm: 0.30657801
INFO:root:[   32] Training loss: 0.12258659, Validation loss: 0.12220644, Gradient norm: 0.32946151
INFO:root:[   33] Training loss: 0.12097154, Validation loss: 0.12090199, Gradient norm: 0.29951044
INFO:root:[   34] Training loss: 0.11889976, Validation loss: 0.11888697, Gradient norm: 0.27389559
INFO:root:[   35] Training loss: 0.11775165, Validation loss: 0.11633997, Gradient norm: 0.36579938
INFO:root:[   36] Training loss: 0.11569194, Validation loss: 0.11438370, Gradient norm: 0.23696334
INFO:root:[   37] Training loss: 0.11402556, Validation loss: 0.11365762, Gradient norm: 0.29505187
INFO:root:[   38] Training loss: 0.11247546, Validation loss: 0.11208328, Gradient norm: 0.25666853
INFO:root:[   39] Training loss: 0.11088255, Validation loss: 0.11036791, Gradient norm: 0.26164652
INFO:root:[   40] Training loss: 0.10989839, Validation loss: 0.10933803, Gradient norm: 0.37891994
INFO:root:[   41] Training loss: 0.10837921, Validation loss: 0.10801254, Gradient norm: 0.20583798
INFO:root:[   42] Training loss: 0.10735769, Validation loss: 0.10668949, Gradient norm: 0.21697266
INFO:root:[   43] Training loss: 0.10644503, Validation loss: 0.10613641, Gradient norm: 0.23391890
INFO:root:[   44] Training loss: 0.10510450, Validation loss: 0.10488164, Gradient norm: 0.18519812
INFO:root:[   45] Training loss: 0.10399103, Validation loss: 0.10385922, Gradient norm: 0.20909159
INFO:root:[   46] Training loss: 0.10307969, Validation loss: 0.10314417, Gradient norm: 0.21094886
INFO:root:[   47] Training loss: 0.10240928, Validation loss: 0.10197807, Gradient norm: 0.30592440
INFO:root:[   48] Training loss: 0.10150899, Validation loss: 0.10229817, Gradient norm: 0.28241889
INFO:root:[   49] Training loss: 0.10056855, Validation loss: 0.10057962, Gradient norm: 0.21837186
INFO:root:[   50] Training loss: 0.09975250, Validation loss: 0.09967201, Gradient norm: 0.20984676
INFO:root:[   51] Training loss: 0.09887272, Validation loss: 0.09889318, Gradient norm: 0.18687817
INFO:root:[   52] Training loss: 0.09818852, Validation loss: 0.09775678, Gradient norm: 0.19627585
INFO:root:[   53] Training loss: 0.09748494, Validation loss: 0.09753027, Gradient norm: 0.20170405
INFO:root:[   54] Training loss: 0.09662910, Validation loss: 0.09726613, Gradient norm: 0.25769329
INFO:root:[   55] Training loss: 0.09593697, Validation loss: 0.09570569, Gradient norm: 0.16890134
INFO:root:[   56] Training loss: 0.09545068, Validation loss: 0.09546338, Gradient norm: 0.25565651
INFO:root:[   57] Training loss: 0.09462855, Validation loss: 0.09479077, Gradient norm: 0.21045929
INFO:root:[   58] Training loss: 0.09371278, Validation loss: 0.09423353, Gradient norm: 0.19735648
INFO:root:[   59] Training loss: 0.09332338, Validation loss: 0.09342648, Gradient norm: 0.25099703
INFO:root:[   60] Training loss: 0.09271468, Validation loss: 0.09289798, Gradient norm: 0.24927945
INFO:root:[   61] Training loss: 0.09198467, Validation loss: 0.09176530, Gradient norm: 0.28897374
INFO:root:[   62] Training loss: 0.09112887, Validation loss: 0.09106571, Gradient norm: 0.21598875
INFO:root:[   63] Training loss: 0.09061575, Validation loss: 0.09026414, Gradient norm: 0.22104088
INFO:root:[   64] Training loss: 0.08987400, Validation loss: 0.09004641, Gradient norm: 0.16220665
INFO:root:[   65] Training loss: 0.08951889, Validation loss: 0.08904404, Gradient norm: 0.33923856
INFO:root:[   66] Training loss: 0.08852633, Validation loss: 0.08871532, Gradient norm: 0.16212138
INFO:root:[   67] Training loss: 0.08811128, Validation loss: 0.08811935, Gradient norm: 0.30987581
INFO:root:[   68] Training loss: 0.08735249, Validation loss: 0.08717498, Gradient norm: 0.23194431
INFO:root:[   69] Training loss: 0.08681662, Validation loss: 0.08620175, Gradient norm: 0.34501035
INFO:root:[   70] Training loss: 0.08602489, Validation loss: 0.08576901, Gradient norm: 0.24558277
INFO:root:[   71] Training loss: 0.08551282, Validation loss: 0.08620224, Gradient norm: 0.23162543
INFO:root:[   72] Training loss: 0.08523431, Validation loss: 0.08533056, Gradient norm: 0.38620781
INFO:root:[   73] Training loss: 0.08450513, Validation loss: 0.08400853, Gradient norm: 0.39502371
INFO:root:[   74] Training loss: 0.08367262, Validation loss: 0.08347503, Gradient norm: 0.17967443
INFO:root:[   75] Training loss: 0.08326608, Validation loss: 0.08315874, Gradient norm: 0.29450077
INFO:root:[   76] Training loss: 0.08264234, Validation loss: 0.08238849, Gradient norm: 0.26803944
INFO:root:[   77] Training loss: 0.08206280, Validation loss: 0.08217552, Gradient norm: 0.21043320
INFO:root:[   78] Training loss: 0.08158865, Validation loss: 0.08276671, Gradient norm: 0.23559379
INFO:root:[   79] Training loss: 0.08131181, Validation loss: 0.08163708, Gradient norm: 0.44021262
INFO:root:[   80] Training loss: 0.08052253, Validation loss: 0.08041399, Gradient norm: 0.26194056
INFO:root:[   81] Training loss: 0.08008429, Validation loss: 0.07966029, Gradient norm: 0.33237072
INFO:root:[   82] Training loss: 0.07970882, Validation loss: 0.07936275, Gradient norm: 0.45718889
INFO:root:[   83] Training loss: 0.07919291, Validation loss: 0.07884090, Gradient norm: 0.33056384
INFO:root:[   84] Training loss: 0.07834005, Validation loss: 0.07883316, Gradient norm: 0.23491135
INFO:root:[   85] Training loss: 0.07795828, Validation loss: 0.07810296, Gradient norm: 0.24841573
INFO:root:[   86] Training loss: 0.07758052, Validation loss: 0.07750581, Gradient norm: 0.32888192
INFO:root:[   87] Training loss: 0.07713798, Validation loss: 0.07708983, Gradient norm: 0.36907111
INFO:root:[   88] Training loss: 0.07642307, Validation loss: 0.07670269, Gradient norm: 0.22642753
INFO:root:[   89] Training loss: 0.07609981, Validation loss: 0.07636582, Gradient norm: 0.36478418
INFO:root:[   90] Training loss: 0.07560103, Validation loss: 0.07558917, Gradient norm: 0.42063768
INFO:root:[   91] Training loss: 0.07515694, Validation loss: 0.07485773, Gradient norm: 0.31256082
INFO:root:[   92] Training loss: 0.07474097, Validation loss: 0.07526397, Gradient norm: 0.42264685
INFO:root:[   93] Training loss: 0.07432300, Validation loss: 0.07426598, Gradient norm: 0.48794928
INFO:root:[   94] Training loss: 0.07375146, Validation loss: 0.07364976, Gradient norm: 0.35849394
INFO:root:[   95] Training loss: 0.07327909, Validation loss: 0.07326124, Gradient norm: 0.38500061
INFO:root:[   96] Training loss: 0.07286902, Validation loss: 0.07301076, Gradient norm: 0.33082343
INFO:root:[   97] Training loss: 0.07239521, Validation loss: 0.07230242, Gradient norm: 0.34645295
INFO:root:[   98] Training loss: 0.07222511, Validation loss: 0.07222259, Gradient norm: 0.49296002
INFO:root:[   99] Training loss: 0.07155008, Validation loss: 0.07139295, Gradient norm: 0.35844030
INFO:root:[  100] Training loss: 0.07118258, Validation loss: 0.07109070, Gradient norm: 0.39527153
INFO:root:[  101] Training loss: 0.07068763, Validation loss: 0.07029198, Gradient norm: 0.36196916
INFO:root:[  102] Training loss: 0.07023433, Validation loss: 0.07017566, Gradient norm: 0.31657109
INFO:root:[  103] Training loss: 0.06971560, Validation loss: 0.06973818, Gradient norm: 0.21127299
INFO:root:[  104] Training loss: 0.06939141, Validation loss: 0.06924717, Gradient norm: 0.31548245
INFO:root:[  105] Training loss: 0.06889325, Validation loss: 0.06914734, Gradient norm: 0.39870805
INFO:root:[  106] Training loss: 0.06882871, Validation loss: 0.06859391, Gradient norm: 0.60990616
INFO:root:[  107] Training loss: 0.06805560, Validation loss: 0.06770220, Gradient norm: 0.26577127
INFO:root:[  108] Training loss: 0.06785679, Validation loss: 0.06852051, Gradient norm: 0.49626318
INFO:root:[  109] Training loss: 0.06768797, Validation loss: 0.06767126, Gradient norm: 0.74895548
INFO:root:[  110] Training loss: 0.06699194, Validation loss: 0.06750431, Gradient norm: 0.45600294
INFO:root:[  111] Training loss: 0.06685625, Validation loss: 0.06646067, Gradient norm: 0.67433233
INFO:root:[  112] Training loss: 0.06621177, Validation loss: 0.06584520, Gradient norm: 0.49836246
INFO:root:[  113] Training loss: 0.06595795, Validation loss: 0.06620913, Gradient norm: 0.55372925
INFO:root:[  114] Training loss: 0.06558049, Validation loss: 0.06561281, Gradient norm: 0.58426913
INFO:root:[  115] Training loss: 0.06518788, Validation loss: 0.06482607, Gradient norm: 0.55485133
INFO:root:[  116] Training loss: 0.06473406, Validation loss: 0.06454374, Gradient norm: 0.43625463
INFO:root:[  117] Training loss: 0.06435396, Validation loss: 0.06446622, Gradient norm: 0.48076627
INFO:root:[  118] Training loss: 0.06388335, Validation loss: 0.06353225, Gradient norm: 0.31957788
INFO:root:[  119] Training loss: 0.06358019, Validation loss: 0.06407755, Gradient norm: 0.42712595
INFO:root:[  120] Training loss: 0.06327426, Validation loss: 0.06333996, Gradient norm: 0.43658723
INFO:root:[  121] Training loss: 0.06312643, Validation loss: 0.06271806, Gradient norm: 0.65206573
INFO:root:[  122] Training loss: 0.06238962, Validation loss: 0.06238208, Gradient norm: 0.32087331
INFO:root:[  123] Training loss: 0.06213794, Validation loss: 0.06214381, Gradient norm: 0.36847781
INFO:root:[  124] Training loss: 0.06198230, Validation loss: 0.06240983, Gradient norm: 0.64983329
INFO:root:[  125] Training loss: 0.06160717, Validation loss: 0.06153838, Gradient norm: 0.56693140
INFO:root:[  126] Training loss: 0.06146100, Validation loss: 0.06097390, Gradient norm: 0.75829407
INFO:root:[  127] Training loss: 0.06084970, Validation loss: 0.06082980, Gradient norm: 0.57314629
INFO:root:[  128] Training loss: 0.06077636, Validation loss: 0.06051040, Gradient norm: 0.80838585
INFO:root:[  129] Training loss: 0.06018391, Validation loss: 0.06026418, Gradient norm: 0.51964299
INFO:root:[  130] Training loss: 0.06020494, Validation loss: 0.06042945, Gradient norm: 0.88955450
INFO:root:[  131] Training loss: 0.05974789, Validation loss: 0.05933716, Gradient norm: 0.75215849
INFO:root:[  132] Training loss: 0.05928226, Validation loss: 0.05982782, Gradient norm: 0.69392041
INFO:root:[  133] Training loss: 0.05899593, Validation loss: 0.05897513, Gradient norm: 0.60545606
INFO:root:[  134] Training loss: 0.05844251, Validation loss: 0.05851562, Gradient norm: 0.24797065
INFO:root:[  135] Training loss: 0.05815576, Validation loss: 0.05777186, Gradient norm: 0.27902488
INFO:root:[  136] Training loss: 0.05797784, Validation loss: 0.05763737, Gradient norm: 0.65817754
INFO:root:[  137] Training loss: 0.05761009, Validation loss: 0.05820785, Gradient norm: 0.54911866
INFO:root:[  138] Training loss: 0.05727128, Validation loss: 0.05708143, Gradient norm: 0.41489632
INFO:root:[  139] Training loss: 0.05699802, Validation loss: 0.05697181, Gradient norm: 0.56275449
INFO:root:[  140] Training loss: 0.05681225, Validation loss: 0.05720451, Gradient norm: 0.63017976
INFO:root:[  141] Training loss: 0.05651911, Validation loss: 0.05654476, Gradient norm: 0.82881346
INFO:root:[  142] Training loss: 0.05617449, Validation loss: 0.05601383, Gradient norm: 0.76982079
INFO:root:[  143] Training loss: 0.05583556, Validation loss: 0.05590448, Gradient norm: 0.74203536
INFO:root:[  144] Training loss: 0.05558205, Validation loss: 0.05521007, Gradient norm: 0.76587843
INFO:root:[  145] Training loss: 0.05545705, Validation loss: 0.05494093, Gradient norm: 0.97632194
INFO:root:[  146] Training loss: 0.05473251, Validation loss: 0.05498520, Gradient norm: 0.58862996
INFO:root:[  147] Training loss: 0.05447065, Validation loss: 0.05467621, Gradient norm: 0.48199395
INFO:root:[  148] Training loss: 0.05429447, Validation loss: 0.05474446, Gradient norm: 0.53117403
INFO:root:[  149] Training loss: 0.05407790, Validation loss: 0.05366828, Gradient norm: 0.71549020
INFO:root:[  150] Training loss: 0.05375410, Validation loss: 0.05328045, Gradient norm: 0.71923692
INFO:root:[  151] Training loss: 0.05342694, Validation loss: 0.05321142, Gradient norm: 0.68210293
INFO:root:[  152] Training loss: 0.05326874, Validation loss: 0.05298532, Gradient norm: 0.90694351
INFO:root:[  153] Training loss: 0.05289274, Validation loss: 0.05276557, Gradient norm: 0.77841285
INFO:root:[  154] Training loss: 0.05266554, Validation loss: 0.05256941, Gradient norm: 0.80078343
INFO:root:[  155] Training loss: 0.05231015, Validation loss: 0.05270755, Gradient norm: 0.68030313
INFO:root:[  156] Training loss: 0.05221691, Validation loss: 0.05243169, Gradient norm: 0.92359052
INFO:root:[  157] Training loss: 0.05172279, Validation loss: 0.05173453, Gradient norm: 0.66157018
INFO:root:[  158] Training loss: 0.05150284, Validation loss: 0.05122384, Gradient norm: 0.77807173
INFO:root:[  159] Training loss: 0.05126810, Validation loss: 0.05099487, Gradient norm: 0.85539504
INFO:root:[  160] Training loss: 0.05098668, Validation loss: 0.05104132, Gradient norm: 0.78744049
INFO:root:[  161] Training loss: 0.05076584, Validation loss: 0.05137291, Gradient norm: 0.92173312
INFO:root:[  162] Training loss: 0.05041975, Validation loss: 0.05066579, Gradient norm: 0.80439705
INFO:root:[  163] Training loss: 0.05027795, Validation loss: 0.05027011, Gradient norm: 0.94371854
INFO:root:[  164] Training loss: 0.04970959, Validation loss: 0.04964860, Gradient norm: 0.45079790
INFO:root:[  165] Training loss: 0.04954277, Validation loss: 0.04931045, Gradient norm: 0.64855096
INFO:root:[  166] Training loss: 0.04956889, Validation loss: 0.04946832, Gradient norm: 1.13498395
INFO:root:[  167] Training loss: 0.04892563, Validation loss: 0.04909886, Gradient norm: 0.75073345
INFO:root:[  168] Training loss: 0.04890915, Validation loss: 0.04936574, Gradient norm: 0.88344437
INFO:root:[  169] Training loss: 0.04847615, Validation loss: 0.04853013, Gradient norm: 0.75606045
INFO:root:[  170] Training loss: 0.04835798, Validation loss: 0.04862819, Gradient norm: 0.94800187
INFO:root:[  171] Training loss: 0.04806240, Validation loss: 0.04813883, Gradient norm: 0.93912742
INFO:root:[  172] Training loss: 0.04801843, Validation loss: 0.04785488, Gradient norm: 1.14100008
INFO:root:[  173] Training loss: 0.04748675, Validation loss: 0.04783694, Gradient norm: 0.60096523
INFO:root:[  174] Training loss: 0.04756520, Validation loss: 0.04795340, Gradient norm: 1.25650159
INFO:root:[  175] Training loss: 0.04709230, Validation loss: 0.04743221, Gradient norm: 0.97224192
INFO:root:[  176] Training loss: 0.04702990, Validation loss: 0.04724489, Gradient norm: 1.18347850
INFO:root:[  177] Training loss: 0.04666658, Validation loss: 0.04660119, Gradient norm: 0.99244266
INFO:root:[  178] Training loss: 0.04653706, Validation loss: 0.04629477, Gradient norm: 1.11710768
INFO:root:[  179] Training loss: 0.04606413, Validation loss: 0.04609319, Gradient norm: 0.75725794
INFO:root:[  180] Training loss: 0.04622967, Validation loss: 0.04577764, Gradient norm: 1.35930617
INFO:root:[  181] Training loss: 0.04589471, Validation loss: 0.04561207, Gradient norm: 1.14714840
INFO:root:[  182] Training loss: 0.04532420, Validation loss: 0.04584611, Gradient norm: 0.72722205
INFO:root:[  183] Training loss: 0.04539713, Validation loss: 0.04551270, Gradient norm: 1.14352534
INFO:root:[  184] Training loss: 0.04499338, Validation loss: 0.04449643, Gradient norm: 0.86838397
INFO:root:[  185] Training loss: 0.04479768, Validation loss: 0.04460038, Gradient norm: 0.99396403
INFO:root:[  186] Training loss: 0.04453874, Validation loss: 0.04505644, Gradient norm: 0.88158003
INFO:root:[  187] Training loss: 0.04431175, Validation loss: 0.04457602, Gradient norm: 1.01818876
INFO:root:[  188] Training loss: 0.04416778, Validation loss: 0.04419747, Gradient norm: 1.11804386
INFO:root:[  189] Training loss: 0.04404536, Validation loss: 0.04362198, Gradient norm: 1.31654759
INFO:root:[  190] Training loss: 0.04386273, Validation loss: 0.04530914, Gradient norm: 1.46283608
INFO:root:[  191] Training loss: 0.04357930, Validation loss: 0.04316202, Gradient norm: 1.32319118
INFO:root:[  192] Training loss: 0.04322964, Validation loss: 0.04317994, Gradient norm: 1.08857529
INFO:root:[  193] Training loss: 0.04308878, Validation loss: 0.04302666, Gradient norm: 1.22222926
INFO:root:[  194] Training loss: 0.04286690, Validation loss: 0.04285659, Gradient norm: 1.32344287
INFO:root:[  195] Training loss: 0.04266720, Validation loss: 0.04296843, Gradient norm: 1.21737273
INFO:root:[  196] Training loss: 0.04231499, Validation loss: 0.04237797, Gradient norm: 0.99065174
INFO:root:[  197] Training loss: 0.04228885, Validation loss: 0.04248678, Gradient norm: 1.30138590
INFO:root:[  198] Training loss: 0.04189854, Validation loss: 0.04159270, Gradient norm: 1.12604460
INFO:root:[  199] Training loss: 0.04192193, Validation loss: 0.04160786, Gradient norm: 1.51022301
INFO:root:[  200] Training loss: 0.04157738, Validation loss: 0.04128067, Gradient norm: 1.21100484
INFO:root:[  201] Training loss: 0.04126259, Validation loss: 0.04084658, Gradient norm: 0.96143035
INFO:root:[  202] Training loss: 0.04093158, Validation loss: 0.04138004, Gradient norm: 1.00996226
INFO:root:[  203] Training loss: 0.04089277, Validation loss: 0.04048561, Gradient norm: 1.40908916
INFO:root:[  204] Training loss: 0.04047268, Validation loss: 0.04093634, Gradient norm: 0.93569484
INFO:root:[  205] Training loss: 0.04047732, Validation loss: 0.04069048, Gradient norm: 1.24298335
INFO:root:[  206] Training loss: 0.04028549, Validation loss: 0.04020905, Gradient norm: 1.22601359
INFO:root:[  207] Training loss: 0.04010090, Validation loss: 0.03970624, Gradient norm: 1.47270348
INFO:root:[  208] Training loss: 0.03981867, Validation loss: 0.03968569, Gradient norm: 1.37800474
INFO:root:[  209] Training loss: 0.03943160, Validation loss: 0.03975537, Gradient norm: 0.84965805
INFO:root:[  210] Training loss: 0.03947123, Validation loss: 0.03912449, Gradient norm: 1.46564487
INFO:root:[  211] Training loss: 0.03903608, Validation loss: 0.03913239, Gradient norm: 0.95882894
INFO:root:[  212] Training loss: 0.03897740, Validation loss: 0.03910982, Gradient norm: 1.45071431
INFO:root:[  213] Training loss: 0.03863189, Validation loss: 0.03890536, Gradient norm: 1.24062353
INFO:root:[  214] Training loss: 0.03836836, Validation loss: 0.03854541, Gradient norm: 1.13884023
INFO:root:[  215] Training loss: 0.03831306, Validation loss: 0.03913407, Gradient norm: 1.50521575
INFO:root:[  216] Training loss: 0.03833453, Validation loss: 0.03876039, Gradient norm: 1.87760887
INFO:root:[  217] Training loss: 0.03833298, Validation loss: 0.03845754, Gradient norm: 2.20605396
INFO:root:[  218] Training loss: 0.03768154, Validation loss: 0.03832797, Gradient norm: 1.50604075
INFO:root:[  219] Training loss: 0.03771753, Validation loss: 0.03711289, Gradient norm: 1.64569216
INFO:root:[  220] Training loss: 0.03727953, Validation loss: 0.03730462, Gradient norm: 1.39881202
INFO:root:[  221] Training loss: 0.03729563, Validation loss: 0.03696370, Gradient norm: 1.72012007
INFO:root:[  222] Training loss: 0.03701633, Validation loss: 0.03669729, Gradient norm: 1.39889886
INFO:root:[  223] Training loss: 0.03669777, Validation loss: 0.03699561, Gradient norm: 1.16440733
INFO:root:[  224] Training loss: 0.03656921, Validation loss: 0.03757866, Gradient norm: 1.51478228
INFO:root:[  225] Training loss: 0.03625934, Validation loss: 0.03625994, Gradient norm: 1.07357453
INFO:root:[  226] Training loss: 0.03609441, Validation loss: 0.03610138, Gradient norm: 1.12803388
INFO:root:[  227] Training loss: 0.03601526, Validation loss: 0.03586086, Gradient norm: 1.33065790
INFO:root:[  228] Training loss: 0.03583876, Validation loss: 0.03561324, Gradient norm: 1.47788360
INFO:root:[  229] Training loss: 0.03554276, Validation loss: 0.03535964, Gradient norm: 1.14555120
INFO:root:[  230] Training loss: 0.03530490, Validation loss: 0.03570252, Gradient norm: 1.18349613
INFO:root:[  231] Training loss: 0.03544319, Validation loss: 0.03524835, Gradient norm: 1.64945563
INFO:root:[  232] Training loss: 0.03527270, Validation loss: 0.03512716, Gradient norm: 1.97930805
INFO:root:[  233] Training loss: 0.03523031, Validation loss: 0.03511564, Gradient norm: 2.19718263
INFO:root:[  234] Training loss: 0.03497370, Validation loss: 0.03494553, Gradient norm: 1.94821632
INFO:root:[  235] Training loss: 0.03480780, Validation loss: 0.03467896, Gradient norm: 1.89890713
INFO:root:[  236] Training loss: 0.03464476, Validation loss: 0.03447673, Gradient norm: 1.96952560
INFO:root:[  237] Training loss: 0.03427199, Validation loss: 0.03424658, Gradient norm: 1.26089520
INFO:root:[  238] Training loss: 0.03408387, Validation loss: 0.03442692, Gradient norm: 1.58377368
INFO:root:[  239] Training loss: 0.03390456, Validation loss: 0.03397975, Gradient norm: 1.62183731
INFO:root:[  240] Training loss: 0.03418633, Validation loss: 0.03501880, Gradient norm: 2.56296026
INFO:root:[  241] Training loss: 0.03398365, Validation loss: 0.03423968, Gradient norm: 2.67625398
INFO:root:[  242] Training loss: 0.03368290, Validation loss: 0.03400909, Gradient norm: 2.16071651
INFO:root:[  243] Training loss: 0.03333803, Validation loss: 0.03384046, Gradient norm: 1.37432279
INFO:root:[  244] Training loss: 0.03315906, Validation loss: 0.03321057, Gradient norm: 1.56372734
INFO:root:[  245] Training loss: 0.03298167, Validation loss: 0.03347526, Gradient norm: 1.58718862
INFO:root:[  246] Training loss: 0.03299409, Validation loss: 0.03291323, Gradient norm: 1.86153635
INFO:root:[  247] Training loss: 0.03285304, Validation loss: 0.03288051, Gradient norm: 1.91994096
INFO:root:[  248] Training loss: 0.03292368, Validation loss: 0.03296162, Gradient norm: 2.58889042
INFO:root:[  249] Training loss: 0.03262965, Validation loss: 0.03229295, Gradient norm: 2.31747315
INFO:root:[  250] Training loss: 0.03249482, Validation loss: 0.03236167, Gradient norm: 2.28405028
INFO:root:[  251] Training loss: 0.03227581, Validation loss: 0.03186997, Gradient norm: 2.01150752
INFO:root:[  252] Training loss: 0.03199419, Validation loss: 0.03198968, Gradient norm: 1.51195481
INFO:root:[  253] Training loss: 0.03192788, Validation loss: 0.03212834, Gradient norm: 1.84540231
INFO:root:[  254] Training loss: 0.03180923, Validation loss: 0.03153039, Gradient norm: 1.46658051
INFO:root:[  255] Training loss: 0.03154153, Validation loss: 0.03188478, Gradient norm: 1.80016077
INFO:root:[  256] Training loss: 0.03140204, Validation loss: 0.03130497, Gradient norm: 1.54890148
INFO:root:[  257] Training loss: 0.03147295, Validation loss: 0.03182503, Gradient norm: 2.48797709
INFO:root:[  258] Training loss: 0.03129183, Validation loss: 0.03130508, Gradient norm: 2.40533739
INFO:root:[  259] Training loss: 0.03108428, Validation loss: 0.03151360, Gradient norm: 2.36423596
INFO:root:[  260] Training loss: 0.03105080, Validation loss: 0.03121213, Gradient norm: 2.54677215
INFO:root:[  261] Training loss: 0.03091767, Validation loss: 0.03078187, Gradient norm: 2.29885325
INFO:root:[  262] Training loss: 0.03066724, Validation loss: 0.03078470, Gradient norm: 2.29680952
INFO:root:[  263] Training loss: 0.03049919, Validation loss: 0.03091044, Gradient norm: 1.83247525
INFO:root:[  264] Training loss: 0.03030689, Validation loss: 0.03028953, Gradient norm: 1.64703891
INFO:root:[  265] Training loss: 0.03018637, Validation loss: 0.03109260, Gradient norm: 1.95896454
INFO:root:[  266] Training loss: 0.03018582, Validation loss: 0.03000915, Gradient norm: 2.13580879
INFO:root:[  267] Training loss: 0.02996398, Validation loss: 0.03007972, Gradient norm: 1.84067664
INFO:root:[  268] Training loss: 0.03002080, Validation loss: 0.03071726, Gradient norm: 2.96022639
INFO:root:[  269] Training loss: 0.03021264, Validation loss: 0.02941762, Gradient norm: 3.94827740
INFO:root:[  270] Training loss: 0.02962823, Validation loss: 0.02930159, Gradient norm: 2.30532126
INFO:root:[  271] Training loss: 0.02974520, Validation loss: 0.02927713, Gradient norm: 2.85777731
INFO:root:[  272] Training loss: 0.02989009, Validation loss: 0.02990527, Gradient norm: 4.11097634
INFO:root:[  273] Training loss: 0.02926216, Validation loss: 0.02900869, Gradient norm: 2.11423751
INFO:root:[  274] Training loss: 0.02919106, Validation loss: 0.02955475, Gradient norm: 2.27747562
INFO:root:[  275] Training loss: 0.02901558, Validation loss: 0.02913139, Gradient norm: 2.34161374
INFO:root:[  276] Training loss: 0.02902243, Validation loss: 0.02922918, Gradient norm: 2.97732557
INFO:root:[  277] Training loss: 0.02894256, Validation loss: 0.02922723, Gradient norm: 3.46812908
INFO:root:[  278] Training loss: 0.02882294, Validation loss: 0.02871867, Gradient norm: 3.22415806
INFO:root:[  279] Training loss: 0.02876957, Validation loss: 0.02866381, Gradient norm: 2.75445094
INFO:root:[  280] Training loss: 0.02851820, Validation loss: 0.02906398, Gradient norm: 2.39528815
INFO:root:[  281] Training loss: 0.02857495, Validation loss: 0.02955029, Gradient norm: 2.66319180
INFO:root:[  282] Training loss: 0.02878343, Validation loss: 0.02933030, Gradient norm: 3.97318022
INFO:root:[  283] Training loss: 0.02838789, Validation loss: 0.02835800, Gradient norm: 3.45637563
INFO:root:[  284] Training loss: 0.02824399, Validation loss: 0.02830481, Gradient norm: 3.26002078
INFO:root:[  285] Training loss: 0.02816085, Validation loss: 0.02806022, Gradient norm: 3.14189421
INFO:root:[  286] Training loss: 0.02811863, Validation loss: 0.02764964, Gradient norm: 3.64155149
INFO:root:[  287] Training loss: 0.02804570, Validation loss: 0.02797169, Gradient norm: 3.75172005
INFO:root:[  288] Training loss: 0.02780738, Validation loss: 0.02823200, Gradient norm: 3.29920603
INFO:root:[  289] Training loss: 0.02812800, Validation loss: 0.02728756, Gradient norm: 5.19005130
INFO:root:[  290] Training loss: 0.02767062, Validation loss: 0.02738942, Gradient norm: 3.80511999
INFO:root:[  291] Training loss: 0.02753574, Validation loss: 0.02775370, Gradient norm: 3.87944006
INFO:root:[  292] Training loss: 0.02764296, Validation loss: 0.02762190, Gradient norm: 4.34452642
INFO:root:[  293] Training loss: 0.02737478, Validation loss: 0.02755984, Gradient norm: 3.26958588
INFO:root:[  294] Training loss: 0.02739140, Validation loss: 0.02758910, Gradient norm: 4.21153781
INFO:root:[  295] Training loss: 0.02741767, Validation loss: 0.02747114, Gradient norm: 4.52520304
INFO:root:[  296] Training loss: 0.02696847, Validation loss: 0.02762386, Gradient norm: 3.35482615
INFO:root:[  297] Training loss: 0.02690138, Validation loss: 0.02702129, Gradient norm: 3.12812648
INFO:root:[  298] Training loss: 0.02693209, Validation loss: 0.02671222, Gradient norm: 3.47724861
INFO:root:[  299] Training loss: 0.02704119, Validation loss: 0.02867242, Gradient norm: 4.67494196
INFO:root:[  300] Training loss: 0.02727560, Validation loss: 0.02750990, Gradient norm: 6.03281919
INFO:root:[  301] Training loss: 0.02681398, Validation loss: 0.02693262, Gradient norm: 4.81436486
INFO:root:[  302] Training loss: 0.02668113, Validation loss: 0.02744925, Gradient norm: 3.47720762
INFO:root:[  303] Training loss: 0.02671671, Validation loss: 0.02623203, Gradient norm: 4.82383517
INFO:root:[  304] Training loss: 0.02669707, Validation loss: 0.02669980, Gradient norm: 5.24716011
INFO:root:[  305] Training loss: 0.02642523, Validation loss: 0.02705629, Gradient norm: 3.63645900
INFO:root:[  306] Training loss: 0.02624865, Validation loss: 0.02629433, Gradient norm: 3.43467519
INFO:root:[  307] Training loss: 0.02656075, Validation loss: 0.02615298, Gradient norm: 5.97115680
INFO:root:[  308] Training loss: 0.02615597, Validation loss: 0.02647852, Gradient norm: 3.67472166
INFO:root:[  309] Training loss: 0.02628268, Validation loss: 0.02608687, Gradient norm: 5.47416161
INFO:root:[  310] Training loss: 0.02618574, Validation loss: 0.02650840, Gradient norm: 4.94660385
INFO:root:[  311] Training loss: 0.02594990, Validation loss: 0.02578954, Gradient norm: 4.09400465
INFO:root:[  312] Training loss: 0.02589760, Validation loss: 0.02664150, Gradient norm: 4.55098564
INFO:root:[  313] Training loss: 0.02621492, Validation loss: 0.02683423, Gradient norm: 7.21232542
INFO:root:[  314] Training loss: 0.02613795, Validation loss: 0.02592988, Gradient norm: 6.65136085
INFO:root:[  315] Training loss: 0.02560387, Validation loss: 0.02578317, Gradient norm: 3.39874658
INFO:root:[  316] Training loss: 0.02587998, Validation loss: 0.02545209, Gradient norm: 5.51202261
INFO:root:[  317] Training loss: 0.02593958, Validation loss: 0.02551525, Gradient norm: 6.38691744
INFO:root:[  318] Training loss: 0.02554081, Validation loss: 0.02718098, Gradient norm: 4.33790821
INFO:root:[  319] Training loss: 0.02576298, Validation loss: 0.02566077, Gradient norm: 6.74820714
INFO:root:[  320] Training loss: 0.02555413, Validation loss: 0.02617239, Gradient norm: 5.59664100
INFO:root:[  321] Training loss: 0.02585871, Validation loss: 0.02614151, Gradient norm: 7.67015124
INFO:root:[  322] Training loss: 0.02598612, Validation loss: 0.02538047, Gradient norm: 8.33172205
INFO:root:[  323] Training loss: 0.02562354, Validation loss: 0.02631410, Gradient norm: 6.86998804
INFO:root:[  324] Training loss: 0.02549215, Validation loss: 0.02595797, Gradient norm: 6.48002334
INFO:root:[  325] Training loss: 0.02531784, Validation loss: 0.02492315, Gradient norm: 6.40197073
INFO:root:[  326] Training loss: 0.02491146, Validation loss: 0.02489332, Gradient norm: 3.65078158
INFO:root:[  327] Training loss: 0.02504032, Validation loss: 0.02517628, Gradient norm: 4.84856358
INFO:root:[  328] Training loss: 0.02487396, Validation loss: 0.02485010, Gradient norm: 4.83247780
INFO:root:[  329] Training loss: 0.02498483, Validation loss: 0.02815642, Gradient norm: 6.01811267
INFO:root:[  330] Training loss: 0.02533583, Validation loss: 0.02448692, Gradient norm: 8.05986172
INFO:root:[  331] Training loss: 0.02476402, Validation loss: 0.02451407, Gradient norm: 5.85643015
INFO:root:[  332] Training loss: 0.02456416, Validation loss: 0.02447066, Gradient norm: 3.90629781
INFO:root:[  333] Training loss: 0.02507960, Validation loss: 0.02457352, Gradient norm: 8.12951567
INFO:root:[  334] Training loss: 0.02441866, Validation loss: 0.02424660, Gradient norm: 4.27163912
INFO:root:[  335] Training loss: 0.02481063, Validation loss: 0.02527079, Gradient norm: 7.33035622
INFO:root:[  336] Training loss: 0.02516000, Validation loss: 0.02501313, Gradient norm: 9.35170938
INFO:root:[  337] Training loss: 0.02458824, Validation loss: 0.02486446, Gradient norm: 6.32575933
INFO:root:[  338] Training loss: 0.02457922, Validation loss: 0.02511315, Gradient norm: 7.13416853
INFO:root:[  339] Training loss: 0.02421357, Validation loss: 0.02401306, Gradient norm: 4.90882945
INFO:root:[  340] Training loss: 0.02432062, Validation loss: 0.02525840, Gradient norm: 5.16283486
INFO:root:[  341] Training loss: 0.02433139, Validation loss: 0.02524473, Gradient norm: 6.98296977
INFO:root:[  342] Training loss: 0.02454082, Validation loss: 0.02464893, Gradient norm: 8.70879458
INFO:root:[  343] Training loss: 0.02458390, Validation loss: 0.02408779, Gradient norm: 8.85084972
INFO:root:[  344] Training loss: 0.02431157, Validation loss: 0.02387933, Gradient norm: 7.11409468
INFO:root:[  345] Training loss: 0.02442264, Validation loss: 0.02440771, Gradient norm: 8.97567194
INFO:root:[  346] Training loss: 0.02440789, Validation loss: 0.02365184, Gradient norm: 8.78564644
INFO:root:[  347] Training loss: 0.02406979, Validation loss: 0.02633615, Gradient norm: 7.23808972
INFO:root:[  348] Training loss: 0.02448117, Validation loss: 0.02897367, Gradient norm: 9.35753508
INFO:root:[  349] Training loss: 0.02425324, Validation loss: 0.02395161, Gradient norm: 9.02294230
INFO:root:[  350] Training loss: 0.02381850, Validation loss: 0.02369873, Gradient norm: 6.56871541
INFO:root:[  351] Training loss: 0.02419885, Validation loss: 0.02512882, Gradient norm: 9.93366060
INFO:root:[  352] Training loss: 0.02398357, Validation loss: 0.02439765, Gradient norm: 7.81867613
INFO:root:[  353] Training loss: 0.02382783, Validation loss: 0.02379473, Gradient norm: 7.37409379
INFO:root:[  354] Training loss: 0.02371513, Validation loss: 0.02387264, Gradient norm: 5.71142051
INFO:root:[  355] Training loss: 0.02402285, Validation loss: 0.02331221, Gradient norm: 9.58841471
INFO:root:[  356] Training loss: 0.02382897, Validation loss: 0.02385856, Gradient norm: 8.22680179
INFO:root:[  357] Training loss: 0.02419702, Validation loss: 0.02367523, Gradient norm: 10.78645049
INFO:root:[  358] Training loss: 0.02355024, Validation loss: 0.02298704, Gradient norm: 7.99821101
INFO:root:[  359] Training loss: 0.02416048, Validation loss: 0.02332920, Gradient norm: 10.93208677
INFO:root:[  360] Training loss: 0.02348809, Validation loss: 0.02426842, Gradient norm: 7.61924339
INFO:root:[  361] Training loss: 0.02411984, Validation loss: 0.02384318, Gradient norm: 12.13259255
INFO:root:[  362] Training loss: 0.02417384, Validation loss: 0.02346594, Gradient norm: 12.55830250
INFO:root:[  363] Training loss: 0.02393268, Validation loss: 0.02340433, Gradient norm: 10.58002839
INFO:root:[  364] Training loss: 0.02380142, Validation loss: 0.02400372, Gradient norm: 9.13012887
INFO:root:[  365] Training loss: 0.02365881, Validation loss: 0.02316522, Gradient norm: 9.70579462
INFO:root:[  366] Training loss: 0.02335893, Validation loss: 0.02373072, Gradient norm: 8.08819130
INFO:root:[  367] Training loss: 0.02329621, Validation loss: 0.02311163, Gradient norm: 6.32665047
INFO:root:EP 367: Early stopping
INFO:root:Training the model took 12224.762s.
INFO:root:Emptying the cuda cache took 0.08s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification scoring-rule-dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.47342
INFO:root:EnergyScoreTrain: 0.3706
INFO:root:CoverageTrain: 0.99515
INFO:root:IntervalWidthTrain: 0.04071
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.4429
INFO:root:EnergyScoreValidation: 0.34824
INFO:root:CoverageValidation: 0.99528
INFO:root:IntervalWidthValidation: 0.041
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.41393
INFO:root:EnergyScoreTest: 0.3245
INFO:root:CoverageTest: 0.99524
INFO:root:IntervalWidthTest: 0.03982
INFO:root:###8 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 446693376
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.56763118, Validation loss: 1.92857579, Gradient norm: 5.15174126
INFO:root:[    2] Training loss: 0.76034410, Validation loss: 0.18373880, Gradient norm: 5.47554683
INFO:root:[    3] Training loss: 0.20090683, Validation loss: 0.14747745, Gradient norm: 1.05173444
INFO:root:[    4] Training loss: 0.17435957, Validation loss: 0.12380534, Gradient norm: 1.38395511
INFO:root:[    5] Training loss: 0.16197968, Validation loss: 0.11898932, Gradient norm: 1.18075781
INFO:root:[    6] Training loss: 0.16323437, Validation loss: 0.12498471, Gradient norm: 1.72350026
INFO:root:[    7] Training loss: 0.15539585, Validation loss: 0.11518654, Gradient norm: 1.17713441
INFO:root:[    8] Training loss: 0.15246120, Validation loss: 0.11443931, Gradient norm: 1.25700990
INFO:root:[    9] Training loss: 0.14819728, Validation loss: 0.11585579, Gradient norm: 1.30614284
INFO:root:[   10] Training loss: 0.14375197, Validation loss: 0.10786381, Gradient norm: 1.72164722
INFO:root:[   11] Training loss: 0.13863847, Validation loss: 0.10262424, Gradient norm: 1.32663495
INFO:root:[   12] Training loss: 0.13518208, Validation loss: 0.09677758, Gradient norm: 1.76055271
INFO:root:[   13] Training loss: 0.12912941, Validation loss: 0.08975057, Gradient norm: 1.27977043
INFO:root:[   14] Training loss: 0.11948050, Validation loss: 0.08464981, Gradient norm: 1.42135036
INFO:root:[   15] Training loss: 0.11484793, Validation loss: 0.07511463, Gradient norm: 1.38362230
INFO:root:[   16] Training loss: 0.10982173, Validation loss: 0.07945484, Gradient norm: 1.47556480
INFO:root:[   17] Training loss: 0.10564845, Validation loss: 0.06744773, Gradient norm: 1.75995547
INFO:root:[   18] Training loss: 0.10067653, Validation loss: 0.06594298, Gradient norm: 0.96361045
INFO:root:[   19] Training loss: 0.09924370, Validation loss: 0.06451046, Gradient norm: 1.45396573
INFO:root:[   20] Training loss: 0.09466127, Validation loss: 0.06200696, Gradient norm: 1.17172070
INFO:root:[   21] Training loss: 0.09194920, Validation loss: 0.06129342, Gradient norm: 1.03192390
INFO:root:[   22] Training loss: 0.09060524, Validation loss: 0.06254367, Gradient norm: 1.21485707
INFO:root:[   23] Training loss: 0.08944814, Validation loss: 0.06519991, Gradient norm: 1.35087552
INFO:root:[   24] Training loss: 0.08503036, Validation loss: 0.05760617, Gradient norm: 1.00626902
INFO:root:[   25] Training loss: 0.08420675, Validation loss: 0.05708629, Gradient norm: 0.91698385
INFO:root:[   26] Training loss: 0.08179783, Validation loss: 0.05700670, Gradient norm: 0.99299709
INFO:root:[   27] Training loss: 0.08089529, Validation loss: 0.05469068, Gradient norm: 0.84232333
INFO:root:[   28] Training loss: 0.07883434, Validation loss: 0.05359623, Gradient norm: 0.79293828
INFO:root:[   29] Training loss: 0.07725185, Validation loss: 0.05328289, Gradient norm: 0.88527261
INFO:root:[   30] Training loss: 0.07554737, Validation loss: 0.05595159, Gradient norm: 0.78560100
INFO:root:[   31] Training loss: 0.07623781, Validation loss: 0.05247730, Gradient norm: 1.11335405
INFO:root:[   32] Training loss: 0.07436572, Validation loss: 0.05650141, Gradient norm: 1.40532362
INFO:root:[   33] Training loss: 0.07273198, Validation loss: 0.05139618, Gradient norm: 0.83453243
INFO:root:[   34] Training loss: 0.07125744, Validation loss: 0.06100034, Gradient norm: 0.66164647
INFO:root:[   35] Training loss: 0.07153898, Validation loss: 0.04928163, Gradient norm: 1.35027209
INFO:root:[   36] Training loss: 0.06999276, Validation loss: 0.05740618, Gradient norm: 0.76878388
INFO:root:[   37] Training loss: 0.06930334, Validation loss: 0.04856487, Gradient norm: 1.13750884
INFO:root:[   38] Training loss: 0.06852301, Validation loss: 0.04778153, Gradient norm: 0.83975261
INFO:root:[   39] Training loss: 0.06755106, Validation loss: 0.04764317, Gradient norm: 0.92026260
INFO:root:[   40] Training loss: 0.06723094, Validation loss: 0.04719163, Gradient norm: 1.17662752
INFO:root:[   41] Training loss: 0.06646508, Validation loss: 0.04640202, Gradient norm: 0.96122878
INFO:root:[   42] Training loss: 0.06529449, Validation loss: 0.04783033, Gradient norm: 0.59853915
INFO:root:[   43] Training loss: 0.06467571, Validation loss: 0.05085021, Gradient norm: 0.84732395
INFO:root:[   44] Training loss: 0.06408970, Validation loss: 0.05053889, Gradient norm: 0.69971827
INFO:root:[   45] Training loss: 0.06394962, Validation loss: 0.04975258, Gradient norm: 0.82205328
INFO:root:[   46] Training loss: 0.06346463, Validation loss: 0.05418384, Gradient norm: 0.87398698
INFO:root:[   47] Training loss: 0.06306312, Validation loss: 0.05287335, Gradient norm: 1.52989287
INFO:root:[   48] Training loss: 0.06182957, Validation loss: 0.04747264, Gradient norm: 0.90758340
INFO:root:[   49] Training loss: 0.06212967, Validation loss: 0.04731701, Gradient norm: 1.58907345
INFO:root:[   50] Training loss: 0.06117179, Validation loss: 0.04810406, Gradient norm: 0.96831005
INFO:root:[   51] Training loss: 0.06023504, Validation loss: 0.04057236, Gradient norm: 0.71593217
INFO:root:[   52] Training loss: 0.06005837, Validation loss: 0.04235968, Gradient norm: 1.04218423
INFO:root:[   53] Training loss: 0.05924921, Validation loss: 0.04599897, Gradient norm: 0.90555540
INFO:root:[   54] Training loss: 0.05844746, Validation loss: 0.04086402, Gradient norm: 0.59460048
INFO:root:[   55] Training loss: 0.05862485, Validation loss: 0.04268028, Gradient norm: 1.04202034
INFO:root:[   56] Training loss: 0.05805113, Validation loss: 0.04165609, Gradient norm: 0.90302129
INFO:root:[   57] Training loss: 0.05759546, Validation loss: 0.04022943, Gradient norm: 1.03414223
INFO:root:[   58] Training loss: 0.05680077, Validation loss: 0.04536491, Gradient norm: 0.68205724
INFO:root:[   59] Training loss: 0.05731534, Validation loss: 0.03691009, Gradient norm: 1.25230650
INFO:root:[   60] Training loss: 0.05702194, Validation loss: 0.05138968, Gradient norm: 1.37959610
INFO:root:[   61] Training loss: 0.05620986, Validation loss: 0.04623185, Gradient norm: 1.43042226
INFO:root:[   62] Training loss: 0.05569282, Validation loss: 0.04447227, Gradient norm: 1.16722716
INFO:root:[   63] Training loss: 0.05514345, Validation loss: 0.04495998, Gradient norm: 0.76950570
INFO:root:[   64] Training loss: 0.05568408, Validation loss: 0.04544225, Gradient norm: 1.43933020
INFO:root:[   65] Training loss: 0.05527255, Validation loss: 0.04922702, Gradient norm: 1.60948367
INFO:root:[   66] Training loss: 0.05417674, Validation loss: 0.03835124, Gradient norm: 1.18238219
INFO:root:[   67] Training loss: 0.05406503, Validation loss: 0.03712179, Gradient norm: 1.05076000
INFO:root:[   68] Training loss: 0.05390461, Validation loss: 0.04992612, Gradient norm: 1.14188513
INFO:root:EP 68: Early stopping
INFO:root:Training the model took 788.517s.
INFO:root:Emptying the cuda cache took 0.032s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.90083
INFO:root:EnergyScoreTrain: 0.48846
INFO:root:CoverageTrain: 0.67488
INFO:root:IntervalWidthTrain: 0.03908
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.7805
INFO:root:EnergyScoreValidation: 0.43818
INFO:root:CoverageValidation: 0.65921
INFO:root:IntervalWidthValidation: 0.03641
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.58663
INFO:root:EnergyScoreTest: 0.34527
INFO:root:CoverageTest: 0.68182
INFO:root:IntervalWidthTest: 0.02103
INFO:root:###9 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1855979520
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 3.03584011, Validation loss: 2.35037231, Gradient norm: 5.55570278
INFO:root:[    2] Training loss: 1.03809491, Validation loss: 0.20999380, Gradient norm: 6.20658231
INFO:root:[    3] Training loss: 0.35495674, Validation loss: 0.17062240, Gradient norm: 1.70953834
INFO:root:[    4] Training loss: 0.32398113, Validation loss: 0.15772434, Gradient norm: 1.90382965
INFO:root:[    5] Training loss: 0.29981005, Validation loss: 0.13978316, Gradient norm: 1.57542116
INFO:root:[    6] Training loss: 0.28038165, Validation loss: 0.12168125, Gradient norm: 1.54956446
INFO:root:[    7] Training loss: 0.26551719, Validation loss: 0.12365749, Gradient norm: 1.40798212
INFO:root:[    8] Training loss: 0.24883844, Validation loss: 0.12018260, Gradient norm: 1.54103273
INFO:root:[    9] Training loss: 0.23302698, Validation loss: 0.12421368, Gradient norm: 1.23333315
INFO:root:[   10] Training loss: 0.22322926, Validation loss: 0.11708684, Gradient norm: 1.36912921
INFO:root:[   11] Training loss: 0.21856251, Validation loss: 0.11810389, Gradient norm: 1.52293410
INFO:root:[   12] Training loss: 0.21026168, Validation loss: 0.11316281, Gradient norm: 1.32177214
INFO:root:[   13] Training loss: 0.19907696, Validation loss: 0.11166593, Gradient norm: 1.07702785
INFO:root:[   14] Training loss: 0.19196065, Validation loss: 0.10943859, Gradient norm: 1.23219509
INFO:root:[   15] Training loss: 0.18590489, Validation loss: 0.10611431, Gradient norm: 1.20182546
INFO:root:[   16] Training loss: 0.18149633, Validation loss: 0.10708950, Gradient norm: 1.22855753
INFO:root:[   17] Training loss: 0.17278478, Validation loss: 0.10353997, Gradient norm: 0.91135577
INFO:root:[   18] Training loss: 0.16758072, Validation loss: 0.10870167, Gradient norm: 1.12509782
INFO:root:[   19] Training loss: 0.16107664, Validation loss: 0.11041317, Gradient norm: 0.89295283
INFO:root:[   20] Training loss: 0.15587043, Validation loss: 0.12562039, Gradient norm: 0.93493509
INFO:root:[   21] Training loss: 0.15037323, Validation loss: 0.09843896, Gradient norm: 0.88531266
INFO:root:[   22] Training loss: 0.14528388, Validation loss: 0.09554543, Gradient norm: 0.97175071
INFO:root:[   23] Training loss: 0.14186789, Validation loss: 0.10708893, Gradient norm: 0.89842919
INFO:root:[   24] Training loss: 0.13690461, Validation loss: 0.12204903, Gradient norm: 0.69251788
INFO:root:[   25] Training loss: 0.13387031, Validation loss: 0.09468912, Gradient norm: 0.93431867
INFO:root:[   26] Training loss: 0.13036092, Validation loss: 0.11414732, Gradient norm: 0.76402262
INFO:root:[   27] Training loss: 0.12768322, Validation loss: 0.10980077, Gradient norm: 0.60991833
INFO:root:[   28] Training loss: 0.12526438, Validation loss: 0.12454535, Gradient norm: 0.66219578
INFO:root:[   29] Training loss: 0.12327608, Validation loss: 0.11771149, Gradient norm: 0.82828257
INFO:root:[   30] Training loss: 0.12065397, Validation loss: 0.13281919, Gradient norm: 0.64422405
INFO:root:[   31] Training loss: 0.11944830, Validation loss: 0.11666845, Gradient norm: 0.67287205
INFO:root:[   32] Training loss: 0.11746836, Validation loss: 0.12334119, Gradient norm: 0.57101744
INFO:root:[   33] Training loss: 0.11551789, Validation loss: 0.11897337, Gradient norm: 0.55546807
INFO:root:[   34] Training loss: 0.11409040, Validation loss: 0.12734138, Gradient norm: 0.55651902
INFO:root:[   35] Training loss: 0.11279934, Validation loss: 0.13732437, Gradient norm: 0.74972453
INFO:root:[   36] Training loss: 0.11141296, Validation loss: 0.12157936, Gradient norm: 0.56715049
INFO:root:[   37] Training loss: 0.11014450, Validation loss: 0.11850610, Gradient norm: 0.55900184
INFO:root:[   38] Training loss: 0.10926481, Validation loss: 0.12005017, Gradient norm: 0.70299396
INFO:root:[   39] Training loss: 0.10846484, Validation loss: 0.12477569, Gradient norm: 0.53166067
INFO:root:[   40] Training loss: 0.10739580, Validation loss: 0.13510675, Gradient norm: 0.44691877
INFO:root:[   41] Training loss: 0.10679221, Validation loss: 0.11954804, Gradient norm: 0.79693667
INFO:root:[   42] Training loss: 0.10598885, Validation loss: 0.13421268, Gradient norm: 0.74636165
INFO:root:[   43] Training loss: 0.10527560, Validation loss: 0.14282194, Gradient norm: 0.56061005
INFO:root:[   44] Training loss: 0.10484089, Validation loss: 0.14498850, Gradient norm: 0.98054766
INFO:root:[   45] Training loss: 0.10405177, Validation loss: 0.13298851, Gradient norm: 0.88979960
INFO:root:[   46] Training loss: 0.10351682, Validation loss: 0.12457355, Gradient norm: 0.94897867
INFO:root:[   47] Training loss: 0.10209837, Validation loss: 0.12961061, Gradient norm: 0.43011545
INFO:root:[   48] Training loss: 0.10174745, Validation loss: 0.14404693, Gradient norm: 0.51260643
INFO:root:[   49] Training loss: 0.10168517, Validation loss: 0.13493713, Gradient norm: 1.01709237
INFO:root:[   50] Training loss: 0.10078864, Validation loss: 0.13236437, Gradient norm: 0.75515479
INFO:root:[   51] Training loss: 0.10018577, Validation loss: 0.13971072, Gradient norm: 0.70269799
INFO:root:[   52] Training loss: 0.10017917, Validation loss: 0.12659933, Gradient norm: 0.90079372
INFO:root:[   53] Training loss: 0.09926822, Validation loss: 0.12947987, Gradient norm: 0.71920298
INFO:root:[   54] Training loss: 0.09902027, Validation loss: 0.14583913, Gradient norm: 0.81564333
INFO:root:[   55] Training loss: 0.09873952, Validation loss: 0.12079658, Gradient norm: 1.09922571
INFO:root:[   56] Training loss: 0.09769703, Validation loss: 0.14728659, Gradient norm: 1.01710448
INFO:root:[   57] Training loss: 0.09714478, Validation loss: 0.12122252, Gradient norm: 0.96449135
INFO:root:[   58] Training loss: 0.09696281, Validation loss: 0.13877158, Gradient norm: 0.88615944
INFO:root:[   59] Training loss: 0.09637635, Validation loss: 0.13677313, Gradient norm: 0.76468014
INFO:root:[   60] Training loss: 0.09585730, Validation loss: 0.13880818, Gradient norm: 0.71596472
INFO:root:[   61] Training loss: 0.09563000, Validation loss: 0.12918836, Gradient norm: 0.85825284
INFO:root:[   62] Training loss: 0.09541235, Validation loss: 0.14891484, Gradient norm: 1.03038272
INFO:root:[   63] Training loss: 0.09495402, Validation loss: 0.12664859, Gradient norm: 0.98401486
INFO:root:[   64] Training loss: 0.09484585, Validation loss: 0.12615470, Gradient norm: 1.11721198
INFO:root:EP 64: Early stopping
INFO:root:Training the model took 741.728s.
INFO:root:Emptying the cuda cache took 0.035s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 2.54282
INFO:root:EnergyScoreTrain: 1.43618
INFO:root:CoverageTrain: 0.61607
INFO:root:IntervalWidthTrain: 0.10278
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 2.2965
INFO:root:EnergyScoreValidation: 1.31448
INFO:root:CoverageValidation: 0.60298
INFO:root:IntervalWidthValidation: 0.10684
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.45518
INFO:root:EnergyScoreTest: 0.71426
INFO:root:CoverageTest: 0.88373
INFO:root:IntervalWidthTest: 0.12428
INFO:root:###10 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1870659584
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.83576467, Validation loss: 2.09216801, Gradient norm: 5.88168606
INFO:root:[    2] Training loss: 0.93823844, Validation loss: 0.23503902, Gradient norm: 6.08257440
INFO:root:[    3] Training loss: 0.51259922, Validation loss: 0.17841574, Gradient norm: 2.50939431
INFO:root:[    4] Training loss: 0.44603780, Validation loss: 0.18442174, Gradient norm: 2.25884405
INFO:root:[    5] Training loss: 0.39772115, Validation loss: 0.16262310, Gradient norm: 2.15272265
INFO:root:[    6] Training loss: 0.35847888, Validation loss: 0.13468363, Gradient norm: 1.77572027
INFO:root:[    7] Training loss: 0.32767843, Validation loss: 0.12554597, Gradient norm: 1.36164119
INFO:root:[    8] Training loss: 0.29893215, Validation loss: 0.12287611, Gradient norm: 1.41531722
INFO:root:[    9] Training loss: 0.27743362, Validation loss: 0.12502368, Gradient norm: 1.40096016
INFO:root:[   10] Training loss: 0.26110006, Validation loss: 0.13413176, Gradient norm: 1.34714710
INFO:root:[   11] Training loss: 0.24331247, Validation loss: 0.14888294, Gradient norm: 1.08512988
INFO:root:[   12] Training loss: 0.23277188, Validation loss: 0.15409733, Gradient norm: 1.13850134
INFO:root:[   13] Training loss: 0.22161542, Validation loss: 0.16515956, Gradient norm: 0.93771283
INFO:root:[   14] Training loss: 0.21073718, Validation loss: 0.16946253, Gradient norm: 1.05782804
INFO:root:[   15] Training loss: 0.20172625, Validation loss: 0.17817061, Gradient norm: 0.89811669
INFO:root:[   16] Training loss: 0.19643440, Validation loss: 0.19614649, Gradient norm: 1.03676450
INFO:root:[   17] Training loss: 0.18873346, Validation loss: 0.19567869, Gradient norm: 0.80039550
INFO:root:[   18] Training loss: 0.18275534, Validation loss: 0.19812372, Gradient norm: 1.12818312
INFO:root:[   19] Training loss: 0.17584327, Validation loss: 0.19263127, Gradient norm: 0.71553190
INFO:root:[   20] Training loss: 0.17016114, Validation loss: 0.19831674, Gradient norm: 0.61323012
INFO:root:[   21] Training loss: 0.16491341, Validation loss: 0.22854672, Gradient norm: 0.62410505
INFO:root:[   22] Training loss: 0.16167230, Validation loss: 0.21538417, Gradient norm: 0.61672874
INFO:root:[   23] Training loss: 0.15692858, Validation loss: 0.21079608, Gradient norm: 0.60320961
INFO:root:[   24] Training loss: 0.15525431, Validation loss: 0.22834900, Gradient norm: 1.03730235
INFO:root:[   25] Training loss: 0.15179785, Validation loss: 0.22567896, Gradient norm: 0.62797790
INFO:root:[   26] Training loss: 0.14942258, Validation loss: 0.23109773, Gradient norm: 0.68379636
INFO:root:[   27] Training loss: 0.14790776, Validation loss: 0.23459250, Gradient norm: 0.85414332
INFO:root:[   28] Training loss: 0.14612350, Validation loss: 0.23759762, Gradient norm: 0.80843492
INFO:root:[   29] Training loss: 0.14389019, Validation loss: 0.23102616, Gradient norm: 0.68114463
INFO:root:[   30] Training loss: 0.14217076, Validation loss: 0.24370427, Gradient norm: 0.59201813
INFO:root:[   31] Training loss: 0.14092797, Validation loss: 0.25181034, Gradient norm: 0.81982670
INFO:root:[   32] Training loss: 0.13920306, Validation loss: 0.24960400, Gradient norm: 0.57507366
INFO:root:[   33] Training loss: 0.13779098, Validation loss: 0.24628791, Gradient norm: 0.71336884
INFO:root:[   34] Training loss: 0.13692630, Validation loss: 0.25182677, Gradient norm: 0.89819378
INFO:root:[   35] Training loss: 0.13535629, Validation loss: 0.26306574, Gradient norm: 0.59058993
INFO:root:[   36] Training loss: 0.13432728, Validation loss: 0.26296348, Gradient norm: 0.57542213
INFO:root:[   37] Training loss: 0.13355966, Validation loss: 0.24988613, Gradient norm: 0.85000537
INFO:root:[   38] Training loss: 0.13181109, Validation loss: 0.25295184, Gradient norm: 0.50005048
INFO:root:[   39] Training loss: 0.13090230, Validation loss: 0.25545915, Gradient norm: 0.54244470
INFO:root:[   40] Training loss: 0.12977623, Validation loss: 0.25648171, Gradient norm: 0.54744970
INFO:root:[   41] Training loss: 0.12914356, Validation loss: 0.26225071, Gradient norm: 0.65800993
INFO:root:[   42] Training loss: 0.12776997, Validation loss: 0.25746463, Gradient norm: 0.45757233
INFO:root:[   43] Training loss: 0.12723236, Validation loss: 0.24872864, Gradient norm: 0.67358708
INFO:root:[   44] Training loss: 0.12667671, Validation loss: 0.23736835, Gradient norm: 0.96588097
INFO:root:[   45] Training loss: 0.12596203, Validation loss: 0.24556442, Gradient norm: 0.81692018
INFO:root:[   46] Training loss: 0.12513518, Validation loss: 0.25597619, Gradient norm: 1.02409428
INFO:root:[   47] Training loss: 0.12415020, Validation loss: 0.25053005, Gradient norm: 0.89482047
INFO:root:[   48] Training loss: 0.12332944, Validation loss: 0.24559583, Gradient norm: 0.72549795
INFO:root:[   49] Training loss: 0.12246201, Validation loss: 0.25236377, Gradient norm: 0.71418311
INFO:root:[   50] Training loss: 0.12162686, Validation loss: 0.26302980, Gradient norm: 0.50119369
INFO:root:[   51] Training loss: 0.12067162, Validation loss: 0.25644825, Gradient norm: 0.36100536
INFO:root:[   52] Training loss: 0.12045813, Validation loss: 0.26052838, Gradient norm: 0.65089636
INFO:root:[   53] Training loss: 0.11961604, Validation loss: 0.26528519, Gradient norm: 0.46548842
INFO:root:[   54] Training loss: 0.11890393, Validation loss: 0.25863461, Gradient norm: 0.50284602
INFO:root:[   55] Training loss: 0.11867082, Validation loss: 0.24725038, Gradient norm: 0.94045691
INFO:root:[   56] Training loss: 0.11811942, Validation loss: 0.26256679, Gradient norm: 0.88066901
INFO:root:[   57] Training loss: 0.11760736, Validation loss: 0.27652563, Gradient norm: 0.93500579
INFO:root:[   58] Training loss: 0.11684453, Validation loss: 0.26867391, Gradient norm: 0.86652946
INFO:root:[   59] Training loss: 0.11602078, Validation loss: 0.26462780, Gradient norm: 0.77205096
INFO:root:[   60] Training loss: 0.11505111, Validation loss: 0.26506295, Gradient norm: 0.45059823
INFO:root:[   61] Training loss: 0.11491045, Validation loss: 0.26073465, Gradient norm: 0.67345463
INFO:root:[   62] Training loss: 0.11441414, Validation loss: 0.25500869, Gradient norm: 0.74646203
INFO:root:[   63] Training loss: 0.11375652, Validation loss: 0.26858513, Gradient norm: 0.71668391
INFO:root:[   64] Training loss: 0.11314052, Validation loss: 0.24549769, Gradient norm: 0.74168450
INFO:root:[   65] Training loss: 0.11285079, Validation loss: 0.27201451, Gradient norm: 0.93189551
INFO:root:[   66] Training loss: 0.11197353, Validation loss: 0.26106185, Gradient norm: 0.69348594
INFO:root:[   67] Training loss: 0.11127824, Validation loss: 0.25805187, Gradient norm: 0.30878494
INFO:root:[   68] Training loss: 0.11120203, Validation loss: 0.26311724, Gradient norm: 0.95622368
INFO:root:[   69] Training loss: 0.11046588, Validation loss: 0.26283393, Gradient norm: 0.75154986
INFO:root:[   70] Training loss: 0.11001884, Validation loss: 0.25239752, Gradient norm: 0.84912692
INFO:root:[   71] Training loss: 0.10932281, Validation loss: 0.24853865, Gradient norm: 0.70664725
INFO:root:[   72] Training loss: 0.10873716, Validation loss: 0.24963396, Gradient norm: 0.73866678
INFO:root:[   73] Training loss: 0.10857496, Validation loss: 0.25554867, Gradient norm: 1.05934211
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 843.463s.
INFO:root:Emptying the cuda cache took 0.036s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 4.86692
INFO:root:EnergyScoreTrain: 2.58013
INFO:root:CoverageTrain: 0.67608
INFO:root:IntervalWidthTrain: 0.23983
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 4.26489
INFO:root:EnergyScoreValidation: 2.15175
INFO:root:CoverageValidation: 0.63304
INFO:root:IntervalWidthValidation: 0.2167
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 2.64445
INFO:root:EnergyScoreTest: 1.2999
INFO:root:CoverageTest: 0.8813
INFO:root:IntervalWidthTest: 0.26727
INFO:root:###11 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1830813696
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.62645639, Validation loss: 1.69976090, Gradient norm: 5.66511713
INFO:root:[    2] Training loss: 1.09730309, Validation loss: 0.30631658, Gradient norm: 5.35055337
INFO:root:[    3] Training loss: 0.68897197, Validation loss: 0.21521521, Gradient norm: 3.38211641
INFO:root:[    4] Training loss: 0.54957860, Validation loss: 0.16173650, Gradient norm: 2.53535034
INFO:root:[    5] Training loss: 0.46671942, Validation loss: 0.15759435, Gradient norm: 2.30384466
INFO:root:[    6] Training loss: 0.40881517, Validation loss: 0.19722282, Gradient norm: 2.19396892
INFO:root:[    7] Training loss: 0.35715359, Validation loss: 0.19296566, Gradient norm: 1.63624621
INFO:root:[    8] Training loss: 0.31177664, Validation loss: 0.28319970, Gradient norm: 1.45642181
INFO:root:[    9] Training loss: 0.28311568, Validation loss: 0.30095702, Gradient norm: 1.09112692
INFO:root:[   10] Training loss: 0.26198987, Validation loss: 0.32510617, Gradient norm: 1.21808601
INFO:root:[   11] Training loss: 0.24578230, Validation loss: 0.34824418, Gradient norm: 1.06802336
INFO:root:[   12] Training loss: 0.23403624, Validation loss: 0.36857592, Gradient norm: 0.96235627
INFO:root:[   13] Training loss: 0.22429283, Validation loss: 0.38445672, Gradient norm: 0.70039166
INFO:root:[   14] Training loss: 0.21394815, Validation loss: 0.39065996, Gradient norm: 0.67141011
INFO:root:[   15] Training loss: 0.20762083, Validation loss: 0.37820543, Gradient norm: 0.75427198
INFO:root:[   16] Training loss: 0.20195262, Validation loss: 0.40866372, Gradient norm: 0.71107599
INFO:root:[   17] Training loss: 0.19722459, Validation loss: 0.43037082, Gradient norm: 0.83583054
INFO:root:[   18] Training loss: 0.19259771, Validation loss: 0.42005620, Gradient norm: 0.70663154
INFO:root:[   19] Training loss: 0.18634809, Validation loss: 0.43372587, Gradient norm: 0.51544393
INFO:root:[   20] Training loss: 0.18206362, Validation loss: 0.43117687, Gradient norm: 0.51300740
INFO:root:[   21] Training loss: 0.17787279, Validation loss: 0.41636883, Gradient norm: 0.48418365
INFO:root:[   22] Training loss: 0.17494771, Validation loss: 0.43859284, Gradient norm: 0.57701521
INFO:root:[   23] Training loss: 0.17111338, Validation loss: 0.43341484, Gradient norm: 0.40118991
INFO:root:[   24] Training loss: 0.16776211, Validation loss: 0.43456016, Gradient norm: 0.55518118
INFO:root:[   25] Training loss: 0.16597156, Validation loss: 0.42179404, Gradient norm: 0.47875792
INFO:root:[   26] Training loss: 0.16310226, Validation loss: 0.43188970, Gradient norm: 0.55123928
INFO:root:[   27] Training loss: 0.16079835, Validation loss: 0.45188837, Gradient norm: 0.46003537
INFO:root:[   28] Training loss: 0.15938100, Validation loss: 0.43400755, Gradient norm: 0.68430641
INFO:root:[   29] Training loss: 0.15793443, Validation loss: 0.44941212, Gradient norm: 0.83606666
INFO:root:[   30] Training loss: 0.15554566, Validation loss: 0.44257773, Gradient norm: 0.53236879
INFO:root:[   31] Training loss: 0.15402823, Validation loss: 0.43721233, Gradient norm: 0.58742868
INFO:root:[   32] Training loss: 0.15224644, Validation loss: 0.44095759, Gradient norm: 0.56711994
INFO:root:[   33] Training loss: 0.15063632, Validation loss: 0.43844318, Gradient norm: 0.39285599
INFO:root:[   34] Training loss: 0.14908627, Validation loss: 0.45082776, Gradient norm: 0.39793515
INFO:root:[   35] Training loss: 0.14806667, Validation loss: 0.45152674, Gradient norm: 0.61791024
INFO:root:[   36] Training loss: 0.14693651, Validation loss: 0.44056800, Gradient norm: 0.77871431
INFO:root:[   37] Training loss: 0.14616433, Validation loss: 0.45038035, Gradient norm: 0.73609140
INFO:root:[   38] Training loss: 0.14436424, Validation loss: 0.44869944, Gradient norm: 0.54807190
INFO:root:[   39] Training loss: 0.14320943, Validation loss: 0.44426300, Gradient norm: 0.68545696
INFO:root:[   40] Training loss: 0.14190401, Validation loss: 0.43705794, Gradient norm: 0.51384855
INFO:root:[   41] Training loss: 0.14103362, Validation loss: 0.46101873, Gradient norm: 0.65859843
INFO:root:[   42] Training loss: 0.14026661, Validation loss: 0.45240472, Gradient norm: 0.85970510
INFO:root:[   43] Training loss: 0.13913212, Validation loss: 0.43433766, Gradient norm: 0.53494577
INFO:root:[   44] Training loss: 0.13805006, Validation loss: 0.45695243, Gradient norm: 0.68096490
INFO:root:[   45] Training loss: 0.13737997, Validation loss: 0.44634233, Gradient norm: 0.85854414
INFO:root:[   46] Training loss: 0.13640954, Validation loss: 0.45450749, Gradient norm: 0.81343816
INFO:root:[   47] Training loss: 0.13492521, Validation loss: 0.43751903, Gradient norm: 0.49503057
INFO:root:[   48] Training loss: 0.13430753, Validation loss: 0.43693430, Gradient norm: 0.69024492
INFO:root:[   49] Training loss: 0.13328115, Validation loss: 0.44546906, Gradient norm: 0.53026389
INFO:root:[   50] Training loss: 0.13232203, Validation loss: 0.43415245, Gradient norm: 0.54377603
INFO:root:[   51] Training loss: 0.13165166, Validation loss: 0.44994392, Gradient norm: 0.81192464
INFO:root:[   52] Training loss: 0.13068309, Validation loss: 0.43047816, Gradient norm: 0.76784748
INFO:root:[   53] Training loss: 0.13017591, Validation loss: 0.42234106, Gradient norm: 1.16428996
INFO:root:[   54] Training loss: 0.12921797, Validation loss: 0.43824967, Gradient norm: 1.07334529
INFO:root:[   55] Training loss: 0.12874060, Validation loss: 0.42393144, Gradient norm: 1.20807648
INFO:root:[   56] Training loss: 0.12736377, Validation loss: 0.43941679, Gradient norm: 0.76552473
INFO:root:[   57] Training loss: 0.12622507, Validation loss: 0.43839943, Gradient norm: 0.37067889
INFO:root:[   58] Training loss: 0.12544550, Validation loss: 0.42829133, Gradient norm: 0.50290306
INFO:root:[   59] Training loss: 0.12452432, Validation loss: 0.42948302, Gradient norm: 0.35041192
INFO:root:[   60] Training loss: 0.12373512, Validation loss: 0.44296562, Gradient norm: 0.75911695
INFO:root:[   61] Training loss: 0.12318626, Validation loss: 0.43543133, Gradient norm: 0.96980026
INFO:root:[   62] Training loss: 0.12234519, Validation loss: 0.43624313, Gradient norm: 0.81169775
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 718.053s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 4.22453
INFO:root:EnergyScoreTrain: 2.28984
INFO:root:CoverageTrain: 0.68498
INFO:root:IntervalWidthTrain: 0.2096
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 3.4124
INFO:root:EnergyScoreValidation: 1.96248
INFO:root:CoverageValidation: 0.73992
INFO:root:IntervalWidthValidation: 0.2073
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 3.63242
INFO:root:EnergyScoreTest: 1.58867
INFO:root:CoverageTest: 0.86438
INFO:root:IntervalWidthTest: 0.34003
INFO:root:###12 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1845493760
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 3.22944131, Validation loss: 2.83633749, Gradient norm: 4.69598449
INFO:root:[    2] Training loss: 1.95177050, Validation loss: 0.59281537, Gradient norm: 5.65098915
INFO:root:[    3] Training loss: 0.78485292, Validation loss: 0.30590574, Gradient norm: 2.46764154
INFO:root:[    4] Training loss: 0.66261065, Validation loss: 0.27922160, Gradient norm: 2.29315469
INFO:root:[    5] Training loss: 0.57579494, Validation loss: 0.16287435, Gradient norm: 1.69568482
INFO:root:[    6] Training loss: 0.51682004, Validation loss: 0.18594273, Gradient norm: 1.78606433
INFO:root:[    7] Training loss: 0.46706057, Validation loss: 0.13701149, Gradient norm: 1.77180640
INFO:root:[    8] Training loss: 0.42695650, Validation loss: 0.15204915, Gradient norm: 1.46708179
INFO:root:[    9] Training loss: 0.39421872, Validation loss: 0.17844063, Gradient norm: 1.23734010
INFO:root:[   10] Training loss: 0.35860301, Validation loss: 0.24149210, Gradient norm: 1.17405635
INFO:root:[   11] Training loss: 0.33260436, Validation loss: 0.23571185, Gradient norm: 0.91921168
INFO:root:[   12] Training loss: 0.30980180, Validation loss: 0.29650605, Gradient norm: 1.03617616
INFO:root:[   13] Training loss: 0.29245753, Validation loss: 0.31095941, Gradient norm: 0.89758517
INFO:root:[   14] Training loss: 0.27620836, Validation loss: 0.35183383, Gradient norm: 0.76677460
INFO:root:[   15] Training loss: 0.26511351, Validation loss: 0.40067777, Gradient norm: 0.75135304
INFO:root:[   16] Training loss: 0.25952114, Validation loss: 0.44375268, Gradient norm: 1.01054687
INFO:root:[   17] Training loss: 0.24770548, Validation loss: 0.43913820, Gradient norm: 0.71857647
INFO:root:[   18] Training loss: 0.24229217, Validation loss: 0.44789226, Gradient norm: 0.84731484
INFO:root:[   19] Training loss: 0.23732278, Validation loss: 0.51830024, Gradient norm: 1.01102159
INFO:root:[   20] Training loss: 0.23102951, Validation loss: 0.52520872, Gradient norm: 0.82324757
INFO:root:[   21] Training loss: 0.22503561, Validation loss: 0.51157031, Gradient norm: 0.63810556
INFO:root:[   22] Training loss: 0.22065172, Validation loss: 0.55364825, Gradient norm: 0.58972950
INFO:root:[   23] Training loss: 0.21652665, Validation loss: 0.54022819, Gradient norm: 0.53278068
INFO:root:[   24] Training loss: 0.21302363, Validation loss: 0.53806417, Gradient norm: 0.76168917
INFO:root:[   25] Training loss: 0.20952410, Validation loss: 0.55346339, Gradient norm: 0.75167488
INFO:root:[   26] Training loss: 0.20628975, Validation loss: 0.55651276, Gradient norm: 0.58824163
INFO:root:[   27] Training loss: 0.20399229, Validation loss: 0.59697597, Gradient norm: 0.67616429
INFO:root:[   28] Training loss: 0.20118340, Validation loss: 0.57439023, Gradient norm: 0.54564055
INFO:root:[   29] Training loss: 0.19884931, Validation loss: 0.58321604, Gradient norm: 0.52597308
INFO:root:[   30] Training loss: 0.19619514, Validation loss: 0.58720485, Gradient norm: 0.48166029
INFO:root:[   31] Training loss: 0.19491433, Validation loss: 0.59049884, Gradient norm: 0.58632355
INFO:root:[   32] Training loss: 0.19224023, Validation loss: 0.59598991, Gradient norm: 0.46026803
INFO:root:[   33] Training loss: 0.19032408, Validation loss: 0.60516410, Gradient norm: 0.50339087
INFO:root:[   34] Training loss: 0.18864485, Validation loss: 0.59945550, Gradient norm: 0.43682135
INFO:root:[   35] Training loss: 0.18736237, Validation loss: 0.61544804, Gradient norm: 0.37839416
INFO:root:[   36] Training loss: 0.18548857, Validation loss: 0.61425906, Gradient norm: 0.47093022
INFO:root:[   37] Training loss: 0.18394916, Validation loss: 0.61477570, Gradient norm: 0.48293708
INFO:root:[   38] Training loss: 0.18248693, Validation loss: 0.61866356, Gradient norm: 0.53701203
INFO:root:[   39] Training loss: 0.18142901, Validation loss: 0.62976367, Gradient norm: 0.75808694
INFO:root:[   40] Training loss: 0.17971434, Validation loss: 0.57827109, Gradient norm: 0.63358946
INFO:root:[   41] Training loss: 0.17860697, Validation loss: 0.61592385, Gradient norm: 0.76228397
INFO:root:[   42] Training loss: 0.17696041, Validation loss: 0.59204909, Gradient norm: 0.57602034
INFO:root:[   43] Training loss: 0.17617471, Validation loss: 0.60637226, Gradient norm: 0.76435705
INFO:root:[   44] Training loss: 0.17475497, Validation loss: 0.60700635, Gradient norm: 0.46314476
INFO:root:[   45] Training loss: 0.17361087, Validation loss: 0.61408006, Gradient norm: 0.63342029
INFO:root:[   46] Training loss: 0.17257648, Validation loss: 0.60569108, Gradient norm: 0.61884250
INFO:root:[   47] Training loss: 0.17112013, Validation loss: 0.59302531, Gradient norm: 0.49144937
INFO:root:[   48] Training loss: 0.17016794, Validation loss: 0.60209506, Gradient norm: 0.41357868
INFO:root:[   49] Training loss: 0.16990882, Validation loss: 0.61932561, Gradient norm: 0.82780039
INFO:root:[   50] Training loss: 0.16852680, Validation loss: 0.59510897, Gradient norm: 0.75026865
INFO:root:[   51] Training loss: 0.16692934, Validation loss: 0.60180587, Gradient norm: 0.53999955
INFO:root:[   52] Training loss: 0.16602424, Validation loss: 0.60894815, Gradient norm: 0.42045177
INFO:root:[   53] Training loss: 0.16534192, Validation loss: 0.59563535, Gradient norm: 0.64099712
INFO:root:[   54] Training loss: 0.16452268, Validation loss: 0.58842282, Gradient norm: 0.94216363
INFO:root:[   55] Training loss: 0.16316221, Validation loss: 0.60554822, Gradient norm: 0.71714985
INFO:root:[   56] Training loss: 0.16174198, Validation loss: 0.59172204, Gradient norm: 0.42216886
INFO:root:[   57] Training loss: 0.16105178, Validation loss: 0.60910243, Gradient norm: 0.49052154
INFO:root:[   58] Training loss: 0.16032575, Validation loss: 0.58536929, Gradient norm: 0.73975880
INFO:root:[   59] Training loss: 0.15911608, Validation loss: 0.58448465, Gradient norm: 0.43144515
INFO:root:[   60] Training loss: 0.15849580, Validation loss: 0.59120443, Gradient norm: 0.92470363
INFO:root:[   61] Training loss: 0.15723231, Validation loss: 0.58323843, Gradient norm: 0.44243191
INFO:root:[   62] Training loss: 0.15658723, Validation loss: 0.58455049, Gradient norm: 0.67301238
INFO:root:[   63] Training loss: 0.15527537, Validation loss: 0.59459272, Gradient norm: 0.38123630
INFO:root:[   64] Training loss: 0.15503013, Validation loss: 0.58206181, Gradient norm: 0.90293518
INFO:root:[   65] Training loss: 0.15386975, Validation loss: 0.58512660, Gradient norm: 0.63512366
INFO:root:[   66] Training loss: 0.15301077, Validation loss: 0.57538813, Gradient norm: 0.62944614
INFO:root:[   67] Training loss: 0.15212687, Validation loss: 0.57765971, Gradient norm: 0.66227735
INFO:root:[   68] Training loss: 0.15116985, Validation loss: 0.57789624, Gradient norm: 0.69377681
INFO:root:[   69] Training loss: 0.15041026, Validation loss: 0.57460499, Gradient norm: 0.71535845
INFO:root:[   70] Training loss: 0.14940816, Validation loss: 0.56085848, Gradient norm: 0.68275423
INFO:root:[   71] Training loss: 0.14847538, Validation loss: 0.56960398, Gradient norm: 0.66127404
INFO:root:[   72] Training loss: 0.14752491, Validation loss: 0.57857812, Gradient norm: 0.70680832
INFO:root:[   73] Training loss: 0.14692438, Validation loss: 0.57096587, Gradient norm: 0.77893549
INFO:root:[   74] Training loss: 0.14568275, Validation loss: 0.55945607, Gradient norm: 0.54391323
INFO:root:[   75] Training loss: 0.14512590, Validation loss: 0.55191057, Gradient norm: 0.80441018
INFO:root:[   76] Training loss: 0.14446021, Validation loss: 0.56296412, Gradient norm: 0.91994363
INFO:root:[   77] Training loss: 0.14331396, Validation loss: 0.54800731, Gradient norm: 0.65559721
INFO:root:[   78] Training loss: 0.14261648, Validation loss: 0.54173055, Gradient norm: 0.66085383
INFO:root:[   79] Training loss: 0.14215957, Validation loss: 0.52971578, Gradient norm: 0.99076048
INFO:root:[   80] Training loss: 0.14104378, Validation loss: 0.54231910, Gradient norm: 0.86323997
INFO:root:[   81] Training loss: 0.14000209, Validation loss: 0.54556165, Gradient norm: 0.63857739
INFO:root:[   82] Training loss: 0.13942045, Validation loss: 0.54644343, Gradient norm: 0.66456348
INFO:root:[   83] Training loss: 0.13861561, Validation loss: 0.54415680, Gradient norm: 0.73071107
INFO:root:[   84] Training loss: 0.13759989, Validation loss: 0.53362709, Gradient norm: 0.60533754
INFO:root:[   85] Training loss: 0.13679931, Validation loss: 0.53306951, Gradient norm: 0.72726612
INFO:root:[   86] Training loss: 0.13596712, Validation loss: 0.51826881, Gradient norm: 0.67979280
INFO:root:[   87] Training loss: 0.13540821, Validation loss: 0.54237253, Gradient norm: 0.93794473
INFO:root:[   88] Training loss: 0.13457235, Validation loss: 0.51714626, Gradient norm: 0.81196455
INFO:root:[   89] Training loss: 0.13368603, Validation loss: 0.52699006, Gradient norm: 0.76300635
INFO:root:[   90] Training loss: 0.13259061, Validation loss: 0.52811426, Gradient norm: 0.46353272
INFO:root:[   91] Training loss: 0.13204779, Validation loss: 0.50316386, Gradient norm: 0.82207694
INFO:root:[   92] Training loss: 0.13160007, Validation loss: 0.51914259, Gradient norm: 1.00995105
INFO:root:[   93] Training loss: 0.13051702, Validation loss: 0.51458385, Gradient norm: 0.72012043
INFO:root:[   94] Training loss: 0.12961782, Validation loss: 0.49875381, Gradient norm: 0.71233557
INFO:root:[   95] Training loss: 0.12887229, Validation loss: 0.49946337, Gradient norm: 0.81735083
INFO:root:[   96] Training loss: 0.12786600, Validation loss: 0.51410140, Gradient norm: 0.69670213
INFO:root:[   97] Training loss: 0.12735431, Validation loss: 0.50626487, Gradient norm: 0.78529549
INFO:root:[   98] Training loss: 0.12642435, Validation loss: 0.50563548, Gradient norm: 0.83279547
INFO:root:[   99] Training loss: 0.12581053, Validation loss: 0.48362121, Gradient norm: 0.91171715
INFO:root:[  100] Training loss: 0.12524679, Validation loss: 0.49945782, Gradient norm: 1.08318500
INFO:root:[  101] Training loss: 0.12408737, Validation loss: 0.50741632, Gradient norm: 0.80296432
INFO:root:[  102] Training loss: 0.12339523, Validation loss: 0.49335891, Gradient norm: 0.61677355
INFO:root:[  103] Training loss: 0.12298363, Validation loss: 0.49611407, Gradient norm: 1.06935610
INFO:root:[  104] Training loss: 0.12206051, Validation loss: 0.47706315, Gradient norm: 0.86793019
INFO:root:[  105] Training loss: 0.12118292, Validation loss: 0.47971505, Gradient norm: 0.72937185
INFO:root:[  106] Training loss: 0.12035650, Validation loss: 0.48813737, Gradient norm: 0.66566506
INFO:root:[  107] Training loss: 0.11983899, Validation loss: 0.48478940, Gradient norm: 0.97007941
INFO:root:[  108] Training loss: 0.11904305, Validation loss: 0.46181417, Gradient norm: 1.03928342
INFO:root:[  109] Training loss: 0.11817859, Validation loss: 0.46520084, Gradient norm: 0.75525435
INFO:root:[  110] Training loss: 0.11719289, Validation loss: 0.47547661, Gradient norm: 0.64460498
INFO:root:[  111] Training loss: 0.11643025, Validation loss: 0.46888968, Gradient norm: 0.70340238
INFO:root:[  112] Training loss: 0.11589880, Validation loss: 0.46399243, Gradient norm: 0.91219240
INFO:root:[  113] Training loss: 0.11479427, Validation loss: 0.46175638, Gradient norm: 0.43652801
INFO:root:[  114] Training loss: 0.11438151, Validation loss: 0.47118385, Gradient norm: 0.77026878
INFO:root:[  115] Training loss: 0.11333770, Validation loss: 0.45596498, Gradient norm: 0.60697450
INFO:root:[  116] Training loss: 0.11253023, Validation loss: 0.44311288, Gradient norm: 0.68331113
INFO:root:[  117] Training loss: 0.11194180, Validation loss: 0.44914283, Gradient norm: 0.77418884
INFO:root:[  118] Training loss: 0.11151059, Validation loss: 0.44061379, Gradient norm: 1.24295993
INFO:root:[  119] Training loss: 0.11044367, Validation loss: 0.44502522, Gradient norm: 0.82911811
INFO:root:[  120] Training loss: 0.10979547, Validation loss: 0.44436133, Gradient norm: 1.09556676
INFO:root:[  121] Training loss: 0.10923181, Validation loss: 0.43856140, Gradient norm: 1.12035499
INFO:root:[  122] Training loss: 0.10817420, Validation loss: 0.44763352, Gradient norm: 0.75196754
INFO:root:[  123] Training loss: 0.10785579, Validation loss: 0.43070457, Gradient norm: 1.18001578
INFO:root:[  124] Training loss: 0.10687338, Validation loss: 0.43461877, Gradient norm: 0.68719152
INFO:root:[  125] Training loss: 0.10634684, Validation loss: 0.43928017, Gradient norm: 1.12670524
INFO:root:[  126] Training loss: 0.10562239, Validation loss: 0.41085380, Gradient norm: 1.16325303
INFO:root:[  127] Training loss: 0.10516835, Validation loss: 0.42170264, Gradient norm: 1.32782975
INFO:root:[  128] Training loss: 0.10446912, Validation loss: 0.41486027, Gradient norm: 1.13402835
INFO:root:[  129] Training loss: 0.10366832, Validation loss: 0.41845580, Gradient norm: 1.13971514
INFO:root:[  130] Training loss: 0.10292018, Validation loss: 0.41962926, Gradient norm: 1.02445946
INFO:root:[  131] Training loss: 0.10238556, Validation loss: 0.41847664, Gradient norm: 1.10022929
INFO:root:[  132] Training loss: 0.10173086, Validation loss: 0.42248376, Gradient norm: 1.13345582
INFO:root:[  133] Training loss: 0.10116474, Validation loss: 0.40582214, Gradient norm: 1.26055460
INFO:root:[  134] Training loss: 0.10031237, Validation loss: 0.40477569, Gradient norm: 0.90248020
INFO:root:[  135] Training loss: 0.09946353, Validation loss: 0.39300117, Gradient norm: 0.67586490
INFO:root:[  136] Training loss: 0.09895015, Validation loss: 0.39853573, Gradient norm: 0.77245124
INFO:root:[  137] Training loss: 0.09839966, Validation loss: 0.39909853, Gradient norm: 1.04277926
INFO:root:[  138] Training loss: 0.09776901, Validation loss: 0.38292428, Gradient norm: 1.14112105
INFO:root:[  139] Training loss: 0.09740191, Validation loss: 0.38351786, Gradient norm: 1.37932828
INFO:root:[  140] Training loss: 0.09676365, Validation loss: 0.39422542, Gradient norm: 1.29225370
INFO:root:[  141] Training loss: 0.09610191, Validation loss: 0.38697779, Gradient norm: 1.37162963
INFO:root:[  142] Training loss: 0.09533809, Validation loss: 0.38694427, Gradient norm: 1.11271658
INFO:root:[  143] Training loss: 0.09501365, Validation loss: 0.36801748, Gradient norm: 1.48463323
INFO:root:[  144] Training loss: 0.09428756, Validation loss: 0.37625504, Gradient norm: 1.12600596
INFO:root:[  145] Training loss: 0.09404554, Validation loss: 0.37228853, Gradient norm: 1.61439517
INFO:root:[  146] Training loss: 0.09290111, Validation loss: 0.37430703, Gradient norm: 0.71748822
INFO:root:[  147] Training loss: 0.09231628, Validation loss: 0.37415073, Gradient norm: 0.88687144
INFO:root:[  148] Training loss: 0.09182147, Validation loss: 0.36650799, Gradient norm: 1.29673493
INFO:root:[  149] Training loss: 0.09131798, Validation loss: 0.35861609, Gradient norm: 1.32172908
INFO:root:[  150] Training loss: 0.09044933, Validation loss: 0.35197172, Gradient norm: 0.85418436
INFO:root:[  151] Training loss: 0.09000907, Validation loss: 0.36082765, Gradient norm: 0.94804311
INFO:root:[  152] Training loss: 0.08951598, Validation loss: 0.35642392, Gradient norm: 1.28166118
INFO:root:[  153] Training loss: 0.08879097, Validation loss: 0.35085739, Gradient norm: 0.79733665
INFO:root:[  154] Training loss: 0.08822513, Validation loss: 0.34070893, Gradient norm: 1.03215990
INFO:root:[  155] Training loss: 0.08799747, Validation loss: 0.35101885, Gradient norm: 1.57849270
INFO:root:[  156] Training loss: 0.08719611, Validation loss: 0.33314454, Gradient norm: 1.16478185
INFO:root:[  157] Training loss: 0.08684921, Validation loss: 0.33353119, Gradient norm: 1.54264036
INFO:root:[  158] Training loss: 0.08618704, Validation loss: 0.32566519, Gradient norm: 1.45300900
INFO:root:[  159] Training loss: 0.08587279, Validation loss: 0.32252527, Gradient norm: 1.64292653
INFO:root:[  160] Training loss: 0.08550234, Validation loss: 0.32550574, Gradient norm: 1.65946818
INFO:root:[  161] Training loss: 0.08457259, Validation loss: 0.32405653, Gradient norm: 1.37762054
INFO:root:[  162] Training loss: 0.08403748, Validation loss: 0.33010903, Gradient norm: 1.14622948
INFO:root:[  163] Training loss: 0.08341772, Validation loss: 0.32551036, Gradient norm: 1.11097286
INFO:root:[  164] Training loss: 0.08332555, Validation loss: 0.33308385, Gradient norm: 1.78250781
INFO:root:[  165] Training loss: 0.08310514, Validation loss: 0.31130753, Gradient norm: 2.06050644
INFO:root:[  166] Training loss: 0.08198141, Validation loss: 0.31430076, Gradient norm: 0.99301820
INFO:root:[  167] Training loss: 0.08134354, Validation loss: 0.30680763, Gradient norm: 0.93760513
INFO:root:[  168] Training loss: 0.08115771, Validation loss: 0.30390805, Gradient norm: 1.41495984
INFO:root:[  169] Training loss: 0.08075407, Validation loss: 0.29834530, Gradient norm: 1.67851702
INFO:root:[  170] Training loss: 0.07990995, Validation loss: 0.30670881, Gradient norm: 0.83581801
INFO:root:[  171] Training loss: 0.07959465, Validation loss: 0.29867109, Gradient norm: 1.18840550
INFO:root:[  172] Training loss: 0.07915634, Validation loss: 0.29880760, Gradient norm: 1.32677522
INFO:root:[  173] Training loss: 0.07879590, Validation loss: 0.30118503, Gradient norm: 1.64396443
INFO:root:[  174] Training loss: 0.07832590, Validation loss: 0.30381629, Gradient norm: 1.54441167
INFO:root:[  175] Training loss: 0.07803493, Validation loss: 0.29762714, Gradient norm: 1.61008590
INFO:root:[  176] Training loss: 0.07722373, Validation loss: 0.28801495, Gradient norm: 1.16876782
INFO:root:[  177] Training loss: 0.07702695, Validation loss: 0.29684550, Gradient norm: 1.69074298
INFO:root:[  178] Training loss: 0.07637290, Validation loss: 0.29032320, Gradient norm: 1.25084968
INFO:root:[  179] Training loss: 0.07580023, Validation loss: 0.27865987, Gradient norm: 1.25678774
INFO:root:[  180] Training loss: 0.07553584, Validation loss: 0.28801579, Gradient norm: 1.43485890
INFO:root:[  181] Training loss: 0.07482972, Validation loss: 0.27855912, Gradient norm: 1.06020229
INFO:root:[  182] Training loss: 0.07437117, Validation loss: 0.28121747, Gradient norm: 1.36055462
INFO:root:[  183] Training loss: 0.07407924, Validation loss: 0.27098873, Gradient norm: 1.61739229
INFO:root:[  184] Training loss: 0.07348009, Validation loss: 0.26826260, Gradient norm: 1.20397652
INFO:root:[  185] Training loss: 0.07301383, Validation loss: 0.26687422, Gradient norm: 1.39120046
INFO:root:[  186] Training loss: 0.07267712, Validation loss: 0.27400758, Gradient norm: 1.39149594
INFO:root:[  187] Training loss: 0.07200378, Validation loss: 0.25930222, Gradient norm: 1.01651784
INFO:root:[  188] Training loss: 0.07193585, Validation loss: 0.26296799, Gradient norm: 1.94411732
INFO:root:[  189] Training loss: 0.07106939, Validation loss: 0.27126063, Gradient norm: 0.96049416
INFO:root:[  190] Training loss: 0.07132518, Validation loss: 0.26319257, Gradient norm: 2.23129030
INFO:root:[  191] Training loss: 0.07040909, Validation loss: 0.26714945, Gradient norm: 1.73073629
INFO:root:[  192] Training loss: 0.06996439, Validation loss: 0.26063213, Gradient norm: 1.40373868
INFO:root:[  193] Training loss: 0.06946843, Validation loss: 0.25288777, Gradient norm: 1.09380859
INFO:root:[  194] Training loss: 0.06913867, Validation loss: 0.26338418, Gradient norm: 1.58442405
INFO:root:[  195] Training loss: 0.06912018, Validation loss: 0.24442932, Gradient norm: 2.05324449
INFO:root:[  196] Training loss: 0.06833547, Validation loss: 0.25090785, Gradient norm: 1.67178142
INFO:root:[  197] Training loss: 0.06803590, Validation loss: 0.24772201, Gradient norm: 1.55382999
INFO:root:[  198] Training loss: 0.06740858, Validation loss: 0.24062964, Gradient norm: 1.09905535
INFO:root:[  199] Training loss: 0.06700022, Validation loss: 0.23883867, Gradient norm: 1.65958788
INFO:root:[  200] Training loss: 0.06658882, Validation loss: 0.23702028, Gradient norm: 1.19104997
INFO:root:[  201] Training loss: 0.06614343, Validation loss: 0.22871283, Gradient norm: 1.37178139
INFO:root:[  202] Training loss: 0.06617227, Validation loss: 0.23587846, Gradient norm: 2.28314431
INFO:root:[  203] Training loss: 0.06571209, Validation loss: 0.22945624, Gradient norm: 1.99508389
INFO:root:[  204] Training loss: 0.06513964, Validation loss: 0.22628001, Gradient norm: 1.77536900
INFO:root:[  205] Training loss: 0.06477573, Validation loss: 0.22219349, Gradient norm: 1.79638876
INFO:root:[  206] Training loss: 0.06443754, Validation loss: 0.22082765, Gradient norm: 1.50845988
INFO:root:[  207] Training loss: 0.06409660, Validation loss: 0.21814132, Gradient norm: 1.76189908
INFO:root:[  208] Training loss: 0.06393968, Validation loss: 0.21756857, Gradient norm: 2.25305641
INFO:root:[  209] Training loss: 0.06334969, Validation loss: 0.21357536, Gradient norm: 1.65427822
INFO:root:[  210] Training loss: 0.06287830, Validation loss: 0.22083932, Gradient norm: 1.48874849
INFO:root:[  211] Training loss: 0.06247486, Validation loss: 0.21315672, Gradient norm: 1.36729524
INFO:root:[  212] Training loss: 0.06221614, Validation loss: 0.21204901, Gradient norm: 1.62646934
INFO:root:[  213] Training loss: 0.06217505, Validation loss: 0.20682604, Gradient norm: 2.24136270
INFO:root:[  214] Training loss: 0.06164507, Validation loss: 0.20395995, Gradient norm: 2.10407453
INFO:root:[  215] Training loss: 0.06133760, Validation loss: 0.20736052, Gradient norm: 1.99767345
INFO:root:[  216] Training loss: 0.06089693, Validation loss: 0.20564517, Gradient norm: 1.96041629
INFO:root:[  217] Training loss: 0.06060351, Validation loss: 0.19366479, Gradient norm: 1.99887024
INFO:root:[  218] Training loss: 0.06063065, Validation loss: 0.21274354, Gradient norm: 2.77982502
INFO:root:[  219] Training loss: 0.06002110, Validation loss: 0.18950300, Gradient norm: 2.03771495
INFO:root:[  220] Training loss: 0.05966842, Validation loss: 0.20443784, Gradient norm: 2.16343050
INFO:root:[  221] Training loss: 0.05943957, Validation loss: 0.20171264, Gradient norm: 2.41908601
INFO:root:[  222] Training loss: 0.05882747, Validation loss: 0.20157459, Gradient norm: 1.72190437
INFO:root:[  223] Training loss: 0.05863640, Validation loss: 0.18917987, Gradient norm: 1.99872091
INFO:root:[  224] Training loss: 0.05837644, Validation loss: 0.19373408, Gradient norm: 1.96317785
INFO:root:[  225] Training loss: 0.05794727, Validation loss: 0.19530841, Gradient norm: 1.89669177
INFO:root:[  226] Training loss: 0.05746486, Validation loss: 0.18861249, Gradient norm: 1.76960156
INFO:root:[  227] Training loss: 0.05741367, Validation loss: 0.19333503, Gradient norm: 2.30908176
INFO:root:[  228] Training loss: 0.05714242, Validation loss: 0.18725866, Gradient norm: 2.05014277
INFO:root:[  229] Training loss: 0.05693103, Validation loss: 0.18023548, Gradient norm: 2.18448836
INFO:root:[  230] Training loss: 0.05660409, Validation loss: 0.18374239, Gradient norm: 2.23446479
INFO:root:[  231] Training loss: 0.05643393, Validation loss: 0.17435374, Gradient norm: 2.47008734
INFO:root:[  232] Training loss: 0.05587835, Validation loss: 0.18214585, Gradient norm: 2.11608077
INFO:root:[  233] Training loss: 0.05578861, Validation loss: 0.17226972, Gradient norm: 2.60791616
INFO:root:[  234] Training loss: 0.05519245, Validation loss: 0.16831707, Gradient norm: 1.70389247
INFO:root:[  235] Training loss: 0.05543424, Validation loss: 0.17108609, Gradient norm: 2.33846631
INFO:root:[  236] Training loss: 0.05489680, Validation loss: 0.16495885, Gradient norm: 2.05794294
INFO:root:[  237] Training loss: 0.05465349, Validation loss: 0.17181496, Gradient norm: 2.15968955
INFO:root:[  238] Training loss: 0.05444281, Validation loss: 0.15596003, Gradient norm: 2.69270308
INFO:root:[  239] Training loss: 0.05426831, Validation loss: 0.16404655, Gradient norm: 2.56276106
INFO:root:[  240] Training loss: 0.05382283, Validation loss: 0.16167902, Gradient norm: 1.50645113
INFO:root:[  241] Training loss: 0.05341053, Validation loss: 0.15924671, Gradient norm: 2.25638626
INFO:root:[  242] Training loss: 0.05335420, Validation loss: 0.14865337, Gradient norm: 2.97200668
INFO:root:[  243] Training loss: 0.05304997, Validation loss: 0.14845144, Gradient norm: 2.67222163
INFO:root:[  244] Training loss: 0.05307781, Validation loss: 0.14841663, Gradient norm: 3.31067700
INFO:root:[  245] Training loss: 0.05247818, Validation loss: 0.15437186, Gradient norm: 2.09462995
INFO:root:[  246] Training loss: 0.05198289, Validation loss: 0.14725943, Gradient norm: 1.59929462
INFO:root:[  247] Training loss: 0.05205931, Validation loss: 0.15236835, Gradient norm: 2.73006556
INFO:root:[  248] Training loss: 0.05165004, Validation loss: 0.14042565, Gradient norm: 2.47792513
INFO:root:[  249] Training loss: 0.05162347, Validation loss: 0.14486795, Gradient norm: 2.79782109
INFO:root:[  250] Training loss: 0.05114843, Validation loss: 0.13927537, Gradient norm: 2.61131480
INFO:root:[  251] Training loss: 0.05120315, Validation loss: 0.13629426, Gradient norm: 2.77552083
INFO:root:[  252] Training loss: 0.05060322, Validation loss: 0.13706676, Gradient norm: 2.30983787
INFO:root:[  253] Training loss: 0.05118460, Validation loss: 0.13887450, Gradient norm: 4.50048489
INFO:root:[  254] Training loss: 0.05028886, Validation loss: 0.12996345, Gradient norm: 2.12673966
INFO:root:[  255] Training loss: 0.05024863, Validation loss: 0.13107668, Gradient norm: 3.08784690
INFO:root:[  256] Training loss: 0.05005457, Validation loss: 0.12208069, Gradient norm: 2.88380809
INFO:root:[  257] Training loss: 0.04964330, Validation loss: 0.12916321, Gradient norm: 2.59905932
INFO:root:[  258] Training loss: 0.04963545, Validation loss: 0.12745403, Gradient norm: 2.94796405
INFO:root:[  259] Training loss: 0.04922822, Validation loss: 0.12545367, Gradient norm: 3.09310830
INFO:root:[  260] Training loss: 0.04885531, Validation loss: 0.11584829, Gradient norm: 2.12084674
INFO:root:[  261] Training loss: 0.04916247, Validation loss: 0.12000343, Gradient norm: 3.75231333
INFO:root:[  262] Training loss: 0.04868130, Validation loss: 0.12374150, Gradient norm: 3.16577666
INFO:root:[  263] Training loss: 0.04857977, Validation loss: 0.11140562, Gradient norm: 3.28555263
INFO:root:[  264] Training loss: 0.04818994, Validation loss: 0.11685976, Gradient norm: 2.87722153
INFO:root:[  265] Training loss: 0.04825569, Validation loss: 0.10580162, Gradient norm: 3.41466238
INFO:root:[  266] Training loss: 0.04818446, Validation loss: 0.11256620, Gradient norm: 3.82234647
INFO:root:[  267] Training loss: 0.04812147, Validation loss: 0.10634586, Gradient norm: 3.98479428
INFO:root:[  268] Training loss: 0.04802869, Validation loss: 0.11110225, Gradient norm: 4.51798019
INFO:root:[  269] Training loss: 0.04743935, Validation loss: 0.10713889, Gradient norm: 3.58836223
INFO:root:[  270] Training loss: 0.04733015, Validation loss: 0.09855998, Gradient norm: 3.98134102
INFO:root:[  271] Training loss: 0.04728137, Validation loss: 0.10064317, Gradient norm: 3.48455362
INFO:root:[  272] Training loss: 0.04677417, Validation loss: 0.10342071, Gradient norm: 3.54944011
INFO:root:[  273] Training loss: 0.04667855, Validation loss: 0.10226291, Gradient norm: 2.72917599
INFO:root:[  274] Training loss: 0.04658128, Validation loss: 0.09937814, Gradient norm: 4.23078852
INFO:root:[  275] Training loss: 0.04651253, Validation loss: 0.09042424, Gradient norm: 3.70872454
INFO:root:[  276] Training loss: 0.04660778, Validation loss: 0.09875709, Gradient norm: 4.87235434
INFO:root:[  277] Training loss: 0.04619344, Validation loss: 0.09679203, Gradient norm: 4.23167897
INFO:root:[  278] Training loss: 0.04591733, Validation loss: 0.08236354, Gradient norm: 4.41399310
INFO:root:[  279] Training loss: 0.04578019, Validation loss: 0.09499420, Gradient norm: 4.38882461
INFO:root:[  280] Training loss: 0.04537971, Validation loss: 0.08646490, Gradient norm: 2.53384832
INFO:root:[  281] Training loss: 0.04532562, Validation loss: 0.08044387, Gradient norm: 3.22703209
INFO:root:[  282] Training loss: 0.04523897, Validation loss: 0.07934989, Gradient norm: 4.07426473
INFO:root:[  283] Training loss: 0.04517920, Validation loss: 0.08112359, Gradient norm: 4.36640736
INFO:root:[  284] Training loss: 0.04523225, Validation loss: 0.08698578, Gradient norm: 4.97527114
INFO:root:[  285] Training loss: 0.04495639, Validation loss: 0.08044551, Gradient norm: 4.87811615
INFO:root:[  286] Training loss: 0.04487980, Validation loss: 0.07242908, Gradient norm: 5.29583615
INFO:root:[  287] Training loss: 0.04452514, Validation loss: 0.07568579, Gradient norm: 4.51602313
INFO:root:[  288] Training loss: 0.04500733, Validation loss: 0.08043281, Gradient norm: 5.98417792
INFO:root:[  289] Training loss: 0.04421259, Validation loss: 0.06749681, Gradient norm: 4.25261159
INFO:root:[  290] Training loss: 0.04442345, Validation loss: 0.06430520, Gradient norm: 5.69863107
INFO:root:[  291] Training loss: 0.04456306, Validation loss: 0.06585790, Gradient norm: 6.78949327
INFO:root:[  292] Training loss: 0.04417129, Validation loss: 0.07207682, Gradient norm: 5.92001173
INFO:root:[  293] Training loss: 0.04374220, Validation loss: 0.06833322, Gradient norm: 3.99406534
INFO:root:[  294] Training loss: 0.04370525, Validation loss: 0.06894260, Gradient norm: 4.12830811
INFO:root:[  295] Training loss: 0.04408425, Validation loss: 0.06782514, Gradient norm: 7.56958990
INFO:root:[  296] Training loss: 0.04405415, Validation loss: 0.06463196, Gradient norm: 7.01024639
INFO:root:[  297] Training loss: 0.04332240, Validation loss: 0.06211219, Gradient norm: 4.59120755
INFO:root:[  298] Training loss: 0.04341761, Validation loss: 0.05838868, Gradient norm: 6.28375328
INFO:root:[  299] Training loss: 0.04321197, Validation loss: 0.06423868, Gradient norm: 6.55504828
INFO:root:[  300] Training loss: 0.04304467, Validation loss: 0.06377918, Gradient norm: 5.97130485
INFO:root:[  301] Training loss: 0.04312580, Validation loss: 0.06563190, Gradient norm: 6.05290041
INFO:root:[  302] Training loss: 0.04292846, Validation loss: 0.05634630, Gradient norm: 5.66669580
INFO:root:[  303] Training loss: 0.04291903, Validation loss: 0.05876820, Gradient norm: 5.89549631
INFO:root:[  304] Training loss: 0.04311395, Validation loss: 0.05752684, Gradient norm: 7.24796457
INFO:root:[  305] Training loss: 0.04308340, Validation loss: 0.05451445, Gradient norm: 7.90435130
INFO:root:[  306] Training loss: 0.04240882, Validation loss: 0.05268178, Gradient norm: 5.38932773
INFO:root:[  307] Training loss: 0.04238635, Validation loss: 0.04935674, Gradient norm: 5.27723337
INFO:root:[  308] Training loss: 0.04242489, Validation loss: 0.04339040, Gradient norm: 6.51270848
INFO:root:[  309] Training loss: 0.04251762, Validation loss: 0.04197247, Gradient norm: 7.70823469
INFO:root:[  310] Training loss: 0.04241190, Validation loss: 0.05227796, Gradient norm: 7.99386316
INFO:root:[  311] Training loss: 0.04223195, Validation loss: 0.04091823, Gradient norm: 7.76356109
INFO:root:[  312] Training loss: 0.04205054, Validation loss: 0.05170124, Gradient norm: 6.36503120
INFO:root:[  313] Training loss: 0.04238876, Validation loss: 0.05113179, Gradient norm: 9.40945759
INFO:root:[  314] Training loss: 0.04211922, Validation loss: 0.04539054, Gradient norm: 8.19783702
INFO:root:[  315] Training loss: 0.04165883, Validation loss: 0.04380294, Gradient norm: 5.41967150
INFO:root:[  316] Training loss: 0.04165221, Validation loss: 0.04432490, Gradient norm: 6.50436324
INFO:root:[  317] Training loss: 0.04165913, Validation loss: 0.03860473, Gradient norm: 6.76478536
INFO:root:[  318] Training loss: 0.04195543, Validation loss: 0.03912223, Gradient norm: 8.40425979
INFO:root:[  319] Training loss: 0.04202182, Validation loss: 0.04176769, Gradient norm: 9.66980694
INFO:root:[  320] Training loss: 0.04156177, Validation loss: 0.04343296, Gradient norm: 7.22157217
INFO:root:[  321] Training loss: 0.04157859, Validation loss: 0.03980792, Gradient norm: 8.47572973
INFO:root:[  322] Training loss: 0.04180664, Validation loss: 0.04178266, Gradient norm: 10.55381370
INFO:root:[  323] Training loss: 0.04120105, Validation loss: 0.03577070, Gradient norm: 7.71261326
INFO:root:[  324] Training loss: 0.04136988, Validation loss: 0.03442188, Gradient norm: 8.23081296
INFO:root:[  325] Training loss: 0.04079180, Validation loss: 0.03438439, Gradient norm: 5.97217467
INFO:root:[  326] Training loss: 0.04086904, Validation loss: 0.03005378, Gradient norm: 5.99386698
INFO:root:[  327] Training loss: 0.04119009, Validation loss: 0.03174466, Gradient norm: 9.73534050
INFO:root:[  328] Training loss: 0.04107815, Validation loss: 0.02968553, Gradient norm: 9.23296102
INFO:root:[  329] Training loss: 0.04102514, Validation loss: 0.02596407, Gradient norm: 9.16607307
INFO:root:[  330] Training loss: 0.04063203, Validation loss: 0.03397541, Gradient norm: 8.52993096
INFO:root:[  331] Training loss: 0.04104495, Validation loss: 0.03154496, Gradient norm: 11.00754383
INFO:root:[  332] Training loss: 0.04119028, Validation loss: 0.03612840, Gradient norm: 11.49793318
INFO:root:[  333] Training loss: 0.04117600, Validation loss: 0.02904848, Gradient norm: 12.71126042
INFO:root:[  334] Training loss: 0.04053945, Validation loss: 0.02626930, Gradient norm: 9.47754805
INFO:root:[  335] Training loss: 0.04033812, Validation loss: 0.02411930, Gradient norm: 8.85437441
INFO:root:[  336] Training loss: 0.04074917, Validation loss: 0.02598473, Gradient norm: 11.46636382
INFO:root:[  337] Training loss: 0.04069225, Validation loss: 0.02305679, Gradient norm: 12.35968748
INFO:root:[  338] Training loss: 0.04096677, Validation loss: 0.02723687, Gradient norm: 13.27027677
INFO:root:[  339] Training loss: 0.04049523, Validation loss: 0.02258638, Gradient norm: 11.53047659
INFO:root:[  340] Training loss: 0.03997637, Validation loss: 0.02235284, Gradient norm: 9.45434272
INFO:root:[  341] Training loss: 0.03971152, Validation loss: 0.02521915, Gradient norm: 6.89117751
INFO:root:[  342] Training loss: 0.04008391, Validation loss: 0.02156975, Gradient norm: 11.01682891
INFO:root:[  343] Training loss: 0.04060133, Validation loss: 0.02268119, Gradient norm: 14.79620732
INFO:root:[  344] Training loss: 0.03980631, Validation loss: 0.02130777, Gradient norm: 8.88884864
INFO:root:[  345] Training loss: 0.04037961, Validation loss: 0.02671927, Gradient norm: 14.88623186
INFO:root:[  346] Training loss: 0.03962450, Validation loss: 0.02005949, Gradient norm: 9.08053966
INFO:root:[  347] Training loss: 0.04013560, Validation loss: 0.02310440, Gradient norm: 13.47783048
INFO:root:[  348] Training loss: 0.04003937, Validation loss: 0.02046231, Gradient norm: 12.61363846
INFO:root:[  349] Training loss: 0.04101862, Validation loss: 0.02273759, Gradient norm: 19.87868242
INFO:root:[  350] Training loss: 0.04020240, Validation loss: 0.01918472, Gradient norm: 13.11415325
INFO:root:[  351] Training loss: 0.03936211, Validation loss: 0.02066317, Gradient norm: 9.12476850
INFO:root:[  352] Training loss: 0.03991011, Validation loss: 0.02134098, Gradient norm: 12.39953880
INFO:root:[  353] Training loss: 0.03945197, Validation loss: 0.02531009, Gradient norm: 11.08733511
INFO:root:[  354] Training loss: 0.03954519, Validation loss: 0.01828390, Gradient norm: 13.07490763
INFO:root:[  355] Training loss: 0.03926438, Validation loss: 0.02415392, Gradient norm: 9.47791830
INFO:root:[  356] Training loss: 0.03958745, Validation loss: 0.01940823, Gradient norm: 14.52888526
INFO:root:[  357] Training loss: 0.03967343, Validation loss: 0.02582248, Gradient norm: 14.18656101
INFO:root:[  358] Training loss: 0.04000118, Validation loss: 0.02191530, Gradient norm: 17.93004511
INFO:root:[  359] Training loss: 0.03948894, Validation loss: 0.01761862, Gradient norm: 14.52889516
INFO:root:[  360] Training loss: 0.03937510, Validation loss: 0.02466213, Gradient norm: 13.15681082
INFO:root:[  361] Training loss: 0.03909558, Validation loss: 0.02110192, Gradient norm: 9.93628295
INFO:root:[  362] Training loss: 0.04005487, Validation loss: 0.03194181, Gradient norm: 18.54712388
INFO:root:[  363] Training loss: 0.04035004, Validation loss: 0.02074549, Gradient norm: 18.91644628
INFO:root:[  364] Training loss: 0.03865909, Validation loss: 0.02143403, Gradient norm: 8.76820048
INFO:root:[  365] Training loss: 0.03971467, Validation loss: 0.01654103, Gradient norm: 17.60332075
INFO:root:[  366] Training loss: 0.03916491, Validation loss: 0.02215103, Gradient norm: 14.10623670
INFO:root:[  367] Training loss: 0.03881319, Validation loss: 0.01815405, Gradient norm: 13.02494392
INFO:root:[  368] Training loss: 0.03952366, Validation loss: 0.01704804, Gradient norm: 17.53646644
INFO:root:[  369] Training loss: 0.03877703, Validation loss: 0.01718536, Gradient norm: 11.31570042
INFO:root:[  370] Training loss: 0.03935367, Validation loss: 0.01888720, Gradient norm: 17.70010141
INFO:root:[  371] Training loss: 0.03920675, Validation loss: 0.01904076, Gradient norm: 17.44008451
INFO:root:[  372] Training loss: 0.03854080, Validation loss: 0.01631832, Gradient norm: 12.65679976
INFO:root:[  373] Training loss: 0.03998292, Validation loss: 0.02172987, Gradient norm: 21.78739672
INFO:root:[  374] Training loss: 0.03865887, Validation loss: 0.01973150, Gradient norm: 13.51205945
INFO:root:[  375] Training loss: 0.03960947, Validation loss: 0.01621533, Gradient norm: 19.11563009
INFO:root:[  376] Training loss: 0.03897535, Validation loss: 0.02856599, Gradient norm: 15.79244451
INFO:root:[  377] Training loss: 0.03944421, Validation loss: 0.01726278, Gradient norm: 18.70103635
INFO:root:[  378] Training loss: 0.03893610, Validation loss: 0.01554269, Gradient norm: 17.01072648
INFO:root:[  379] Training loss: 0.03937803, Validation loss: 0.01599648, Gradient norm: 19.36063225
INFO:root:[  380] Training loss: 0.03874107, Validation loss: 0.01598921, Gradient norm: 14.28784161
INFO:root:[  381] Training loss: 0.03926389, Validation loss: 0.01542726, Gradient norm: 20.62010047
INFO:root:[  382] Training loss: 0.03845591, Validation loss: 0.01652367, Gradient norm: 13.38750202
INFO:root:[  383] Training loss: 0.03855219, Validation loss: 0.02392085, Gradient norm: 14.82046708
INFO:root:[  384] Training loss: 0.03891212, Validation loss: 0.02423671, Gradient norm: 19.20359859
INFO:root:[  385] Training loss: 0.03935103, Validation loss: 0.01903622, Gradient norm: 22.14312656
INFO:root:[  386] Training loss: 0.03847426, Validation loss: 0.01762186, Gradient norm: 15.10695068
INFO:root:[  387] Training loss: 0.03812976, Validation loss: 0.01892936, Gradient norm: 13.81206316
INFO:root:[  388] Training loss: 0.03908193, Validation loss: 0.01847905, Gradient norm: 20.17431289
INFO:root:[  389] Training loss: 0.03845995, Validation loss: 0.01901177, Gradient norm: 17.48475021
INFO:root:[  390] Training loss: 0.03895170, Validation loss: 0.02204632, Gradient norm: 19.10145195
INFO:root:EP 390: Early stopping
INFO:root:Training the model took 4458.739s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 1.69055
INFO:root:EnergyScoreTrain: 0.82236
INFO:root:CoverageTrain: 0.67009
INFO:root:IntervalWidthTrain: 0.06406
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 1.77895
INFO:root:EnergyScoreValidation: 0.78154
INFO:root:CoverageValidation: 0.70218
INFO:root:IntervalWidthValidation: 0.07536
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 1.13263
INFO:root:EnergyScoreTest: 0.67232
INFO:root:CoverageTest: 0.7336
INFO:root:IntervalWidthTest: 0.05772
INFO:root:###13 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1860173824
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.97274578, Validation loss: 2.42322582, Gradient norm: 5.26865057
INFO:root:[    2] Training loss: 1.31680427, Validation loss: 0.75815389, Gradient norm: 4.83884899
INFO:root:[    3] Training loss: 0.80721486, Validation loss: 0.57808033, Gradient norm: 2.38868822
INFO:root:[    4] Training loss: 0.69059753, Validation loss: 0.44472104, Gradient norm: 2.34499475
INFO:root:[    5] Training loss: 0.58427222, Validation loss: 0.34197573, Gradient norm: 1.97782406
INFO:root:[    6] Training loss: 0.51249795, Validation loss: 0.26879923, Gradient norm: 1.49660185
INFO:root:[    7] Training loss: 0.45469175, Validation loss: 0.16234457, Gradient norm: 1.54788968
INFO:root:[    8] Training loss: 0.41439393, Validation loss: 0.15124419, Gradient norm: 1.38143757
INFO:root:[    9] Training loss: 0.38077337, Validation loss: 0.14499477, Gradient norm: 1.09151700
INFO:root:[   10] Training loss: 0.35795910, Validation loss: 0.14501875, Gradient norm: 1.14880281
INFO:root:[   11] Training loss: 0.33880296, Validation loss: 0.15573748, Gradient norm: 0.98566542
INFO:root:[   12] Training loss: 0.32814578, Validation loss: 0.16515559, Gradient norm: 0.93680714
INFO:root:[   13] Training loss: 0.31357403, Validation loss: 0.18027110, Gradient norm: 0.88221366
INFO:root:[   14] Training loss: 0.30174981, Validation loss: 0.23536414, Gradient norm: 0.91343768
INFO:root:[   15] Training loss: 0.29368043, Validation loss: 0.25787593, Gradient norm: 0.73235669
INFO:root:[   16] Training loss: 0.28724092, Validation loss: 0.25646134, Gradient norm: 0.67803195
INFO:root:[   17] Training loss: 0.28271366, Validation loss: 0.27888758, Gradient norm: 0.82599623
INFO:root:[   18] Training loss: 0.27567681, Validation loss: 0.32308876, Gradient norm: 0.49218005
INFO:root:[   19] Training loss: 0.27062861, Validation loss: 0.33500555, Gradient norm: 0.50712581
INFO:root:[   20] Training loss: 0.26573656, Validation loss: 0.37486285, Gradient norm: 0.77399381
INFO:root:[   21] Training loss: 0.26235628, Validation loss: 0.38083150, Gradient norm: 0.59086927
INFO:root:[   22] Training loss: 0.25711793, Validation loss: 0.37496109, Gradient norm: 0.67789211
INFO:root:[   23] Training loss: 0.25429658, Validation loss: 0.39509023, Gradient norm: 0.65877337
INFO:root:[   24] Training loss: 0.24945682, Validation loss: 0.39936729, Gradient norm: 0.55510534
INFO:root:[   25] Training loss: 0.24675289, Validation loss: 0.42441477, Gradient norm: 0.60250140
INFO:root:[   26] Training loss: 0.24390283, Validation loss: 0.43353838, Gradient norm: 0.61418442
INFO:root:[   27] Training loss: 0.23947338, Validation loss: 0.43464782, Gradient norm: 0.49688820
INFO:root:[   28] Training loss: 0.23697053, Validation loss: 0.45988215, Gradient norm: 0.49662247
INFO:root:[   29] Training loss: 0.23392935, Validation loss: 0.48349472, Gradient norm: 0.52193783
INFO:root:[   30] Training loss: 0.23147256, Validation loss: 0.48715870, Gradient norm: 0.37966169
INFO:root:[   31] Training loss: 0.22889248, Validation loss: 0.48249526, Gradient norm: 0.48914420
INFO:root:[   32] Training loss: 0.22609137, Validation loss: 0.49995006, Gradient norm: 0.47779000
INFO:root:[   33] Training loss: 0.22414452, Validation loss: 0.48257685, Gradient norm: 0.61058541
INFO:root:[   34] Training loss: 0.22143561, Validation loss: 0.48519462, Gradient norm: 0.45679760
INFO:root:[   35] Training loss: 0.22032374, Validation loss: 0.49927974, Gradient norm: 0.60186295
INFO:root:[   36] Training loss: 0.21785495, Validation loss: 0.50313927, Gradient norm: 0.46368772
INFO:root:[   37] Training loss: 0.21613635, Validation loss: 0.50843154, Gradient norm: 0.61607043
INFO:root:[   38] Training loss: 0.21388654, Validation loss: 0.51260459, Gradient norm: 0.41995679
INFO:root:[   39] Training loss: 0.21257174, Validation loss: 0.50034243, Gradient norm: 0.65319969
INFO:root:[   40] Training loss: 0.21081166, Validation loss: 0.52193582, Gradient norm: 0.48960074
INFO:root:[   41] Training loss: 0.20993411, Validation loss: 0.52182926, Gradient norm: 0.65184186
INFO:root:[   42] Training loss: 0.20784395, Validation loss: 0.50125356, Gradient norm: 0.52688062
INFO:root:[   43] Training loss: 0.20620177, Validation loss: 0.52734262, Gradient norm: 0.38185318
INFO:root:[   44] Training loss: 0.20446073, Validation loss: 0.49756950, Gradient norm: 0.38947788
INFO:root:[   45] Training loss: 0.20328210, Validation loss: 0.49627801, Gradient norm: 0.44224328
INFO:root:[   46] Training loss: 0.20162843, Validation loss: 0.50488255, Gradient norm: 0.49385858
INFO:root:[   47] Training loss: 0.19981672, Validation loss: 0.50606123, Gradient norm: 0.45795959
INFO:root:[   48] Training loss: 0.19872846, Validation loss: 0.50356651, Gradient norm: 0.40911808
INFO:root:[   49] Training loss: 0.19710979, Validation loss: 0.51281819, Gradient norm: 0.32042246
INFO:root:[   50] Training loss: 0.19549014, Validation loss: 0.50439230, Gradient norm: 0.38369318
INFO:root:[   51] Training loss: 0.19414917, Validation loss: 0.50488482, Gradient norm: 0.46151907
INFO:root:[   52] Training loss: 0.19253580, Validation loss: 0.50462730, Gradient norm: 0.49442212
INFO:root:[   53] Training loss: 0.19099768, Validation loss: 0.52359914, Gradient norm: 0.41826949
INFO:root:[   54] Training loss: 0.18973339, Validation loss: 0.51551124, Gradient norm: 0.43250511
INFO:root:[   55] Training loss: 0.18814765, Validation loss: 0.51534324, Gradient norm: 0.42412149
INFO:root:[   56] Training loss: 0.18676273, Validation loss: 0.52650744, Gradient norm: 0.39524254
INFO:root:[   57] Training loss: 0.18544387, Validation loss: 0.50021631, Gradient norm: 0.46065760
INFO:root:[   58] Training loss: 0.18429534, Validation loss: 0.52312670, Gradient norm: 0.62692120
INFO:root:[   59] Training loss: 0.18255290, Validation loss: 0.52894747, Gradient norm: 0.43914728
INFO:root:[   60] Training loss: 0.18099446, Validation loss: 0.53806485, Gradient norm: 0.41470072
INFO:root:[   61] Training loss: 0.17981031, Validation loss: 0.53980136, Gradient norm: 0.54039493
INFO:root:[   62] Training loss: 0.17834887, Validation loss: 0.53687782, Gradient norm: 0.40121958
INFO:root:[   63] Training loss: 0.17712246, Validation loss: 0.54522192, Gradient norm: 0.49836343
INFO:root:[   64] Training loss: 0.17590305, Validation loss: 0.52247675, Gradient norm: 0.64198619
INFO:root:[   65] Training loss: 0.17422208, Validation loss: 0.52087785, Gradient norm: 0.57239242
INFO:root:[   66] Training loss: 0.17252759, Validation loss: 0.52281330, Gradient norm: 0.32355330
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 763.509s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: nan
INFO:root:EnergyScoreTrain: nan
INFO:root:CoverageTrain: 0.0
INFO:root:IntervalWidthTrain: nan
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: nan
INFO:root:EnergyScoreValidation: nan
INFO:root:CoverageValidation: 0.0
INFO:root:IntervalWidthValidation: nan
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: nan
INFO:root:EnergyScoreTest: nan
INFO:root:CoverageTest: 0.0
INFO:root:IntervalWidthTest: nan
INFO:root:###14 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'laplace', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1845493760
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.70882157, Validation loss: 2.03750159, Gradient norm: 5.95593090
INFO:root:[    2] Training loss: 1.78582057, Validation loss: 1.22945281, Gradient norm: 5.02096825
INFO:root:[    3] Training loss: 1.40506410, Validation loss: 1.13933136, Gradient norm: 3.53228182
INFO:root:[    4] Training loss: 1.15742723, Validation loss: 0.88449996, Gradient norm: 3.34447026
INFO:root:[    5] Training loss: 0.96173506, Validation loss: 0.60339698, Gradient norm: 2.39447834
INFO:root:[    6] Training loss: 0.83503404, Validation loss: 0.50391782, Gradient norm: 2.26149811
INFO:root:[    7] Training loss: 0.70929344, Validation loss: 0.29292973, Gradient norm: 1.72402774
INFO:root:[    8] Training loss: 0.62616153, Validation loss: 0.19810626, Gradient norm: 1.53997557
INFO:root:[    9] Training loss: 0.56526308, Validation loss: 0.18812960, Gradient norm: 1.28678169
INFO:root:[   10] Training loss: 0.51508326, Validation loss: 0.22365056, Gradient norm: 1.15916688
INFO:root:[   11] Training loss: 0.47754751, Validation loss: 0.28883260, Gradient norm: 0.98206858
INFO:root:[   12] Training loss: 0.44827747, Validation loss: 0.39403443, Gradient norm: 0.94661019
INFO:root:[   13] Training loss: 0.42525090, Validation loss: 0.42250680, Gradient norm: 0.87897912
INFO:root:[   14] Training loss: 0.40697014, Validation loss: 0.46714893, Gradient norm: 0.73217619
INFO:root:[   15] Training loss: 0.39208063, Validation loss: 0.54171644, Gradient norm: 0.69940709
INFO:root:[   16] Training loss: 0.37947275, Validation loss: 0.59149490, Gradient norm: 0.69004380
INFO:root:[   17] Training loss: 0.36982716, Validation loss: 0.67051966, Gradient norm: 0.67937652
INFO:root:[   18] Training loss: 0.36077678, Validation loss: 0.71135368, Gradient norm: 0.68412198
INFO:root:[   19] Training loss: 0.35307404, Validation loss: 0.74898143, Gradient norm: 0.62354025
INFO:root:[   20] Training loss: 0.34582397, Validation loss: 0.76928635, Gradient norm: 0.61802488
INFO:root:[   21] Training loss: 0.33825671, Validation loss: 0.81986048, Gradient norm: 0.51618873
INFO:root:[   22] Training loss: 0.33389437, Validation loss: 0.87623975, Gradient norm: 0.57427588
INFO:root:[   23] Training loss: 0.32956929, Validation loss: 0.87427597, Gradient norm: 0.52833786
INFO:root:[   24] Training loss: 0.32405883, Validation loss: 0.89873121, Gradient norm: 0.49374496
INFO:root:[   25] Training loss: 0.32017792, Validation loss: 0.92871811, Gradient norm: 0.53425782
INFO:root:[   26] Training loss: 0.31620767, Validation loss: 0.95129995, Gradient norm: 0.55204308
INFO:root:[   27] Training loss: 0.31191139, Validation loss: 0.95752341, Gradient norm: 0.53101596
INFO:root:[   28] Training loss: 0.30882227, Validation loss: 0.98027485, Gradient norm: 0.48854519
INFO:root:[   29] Training loss: 0.30537694, Validation loss: 0.97964624, Gradient norm: 0.50194431
INFO:root:[   30] Training loss: 0.30272530, Validation loss: 0.99700367, Gradient norm: 0.54449096
INFO:root:[   31] Training loss: 0.29967487, Validation loss: 1.02332124, Gradient norm: 0.63092220
INFO:root:[   32] Training loss: 0.29519657, Validation loss: 1.03567967, Gradient norm: 0.60452828
INFO:root:[   33] Training loss: 0.29314141, Validation loss: 1.02453477, Gradient norm: 0.56904431
INFO:root:[   34] Training loss: 0.29024998, Validation loss: 1.05803965, Gradient norm: 0.74756233
INFO:root:[   35] Training loss: 0.28668488, Validation loss: 1.05455358, Gradient norm: 0.47837166
INFO:root:[   36] Training loss: 0.28372429, Validation loss: 1.08741014, Gradient norm: 0.49766088
INFO:root:[   37] Training loss: 0.28102032, Validation loss: 1.09160194, Gradient norm: 0.47264744
INFO:root:[   38] Training loss: 0.27809409, Validation loss: 1.09055610, Gradient norm: 0.55043698
INFO:root:[   39] Training loss: 0.27568909, Validation loss: 1.11646323, Gradient norm: 0.72146269
INFO:root:[   40] Training loss: 0.27285601, Validation loss: 1.10022228, Gradient norm: 0.68088776
INFO:root:[   41] Training loss: 0.27026989, Validation loss: 1.10119324, Gradient norm: 0.50102490
INFO:root:[   42] Training loss: 0.26667854, Validation loss: 1.09841036, Gradient norm: 0.48838876
INFO:root:[   43] Training loss: 0.26462730, Validation loss: 1.12292203, Gradient norm: 0.60543391
INFO:root:[   44] Training loss: 0.26177660, Validation loss: 1.12693340, Gradient norm: 0.54005010
INFO:root:[   45] Training loss: 0.25872464, Validation loss: 1.12126816, Gradient norm: 0.47664027
INFO:root:[   46] Training loss: 0.25682509, Validation loss: 1.12796435, Gradient norm: 0.72929942
INFO:root:[   47] Training loss: 0.25334029, Validation loss: 1.11241619, Gradient norm: 0.57256540
INFO:root:[   48] Training loss: 0.25144836, Validation loss: 1.12074899, Gradient norm: 0.62868789
INFO:root:[   49] Training loss: 0.24843052, Validation loss: 1.13040344, Gradient norm: 0.51695180
INFO:root:[   50] Training loss: 0.24546894, Validation loss: 1.12456298, Gradient norm: 0.52247367
INFO:root:[   51] Training loss: 0.24285295, Validation loss: 1.12539620, Gradient norm: 0.53456199
INFO:root:[   52] Training loss: 0.24044725, Validation loss: 1.15062056, Gradient norm: 0.59152288
INFO:root:[   53] Training loss: 0.23770584, Validation loss: 1.14889334, Gradient norm: 0.52856636
INFO:root:[   54] Training loss: 0.23494768, Validation loss: 1.14237233, Gradient norm: 0.48711326
INFO:root:[   55] Training loss: 0.23240423, Validation loss: 1.14125664, Gradient norm: 0.50321747
INFO:root:[   56] Training loss: 0.23016231, Validation loss: 1.14755049, Gradient norm: 0.48718216
INFO:root:[   57] Training loss: 0.22785629, Validation loss: 1.13035362, Gradient norm: 0.62568534
INFO:root:[   58] Training loss: 0.22564375, Validation loss: 1.15653200, Gradient norm: 0.92263854
INFO:root:[   59] Training loss: 0.22315933, Validation loss: 1.13207737, Gradient norm: 0.87033591
INFO:root:[   60] Training loss: 0.22082521, Validation loss: 1.12198568, Gradient norm: 0.73399439
INFO:root:[   61] Training loss: 0.21825255, Validation loss: 1.15448764, Gradient norm: 0.60531308
INFO:root:[   62] Training loss: 0.21553235, Validation loss: 1.15048731, Gradient norm: 0.48726706
INFO:root:[   63] Training loss: 0.21318050, Validation loss: 1.14135031, Gradient norm: 0.68086211
INFO:root:[   64] Training loss: 0.21135827, Validation loss: 1.13451171, Gradient norm: 0.91372157
INFO:root:[   65] Training loss: 0.20918178, Validation loss: 1.13031115, Gradient norm: 0.88773987
INFO:root:[   66] Training loss: 0.20639925, Validation loss: 1.12782026, Gradient norm: 0.58335633
INFO:root:[   67] Training loss: 0.20434011, Validation loss: 1.13390176, Gradient norm: 0.76294811
INFO:root:[   68] Training loss: 0.20179188, Validation loss: 1.12411550, Gradient norm: 0.73123286
INFO:root:[   69] Training loss: 0.19942485, Validation loss: 1.12700250, Gradient norm: 0.56557875
INFO:root:[   70] Training loss: 0.19778042, Validation loss: 1.10205369, Gradient norm: 1.02022425
INFO:root:[   71] Training loss: 0.19547177, Validation loss: 1.09055733, Gradient norm: 0.99976323
INFO:root:[   72] Training loss: 0.19277188, Validation loss: 1.10757982, Gradient norm: 0.65473727
INFO:root:[   73] Training loss: 0.19050328, Validation loss: 1.08898039, Gradient norm: 0.65503717
INFO:root:[   74] Training loss: 0.18844810, Validation loss: 1.08365007, Gradient norm: 0.85734754
INFO:root:[   75] Training loss: 0.18644815, Validation loss: 1.07388842, Gradient norm: 0.73989647
INFO:root:[   76] Training loss: 0.18419887, Validation loss: 1.07407804, Gradient norm: 0.96941009
INFO:root:[   77] Training loss: 0.18216273, Validation loss: 1.04899190, Gradient norm: 0.98593590
INFO:root:[   78] Training loss: 0.18018682, Validation loss: 1.03670569, Gradient norm: 0.92873237
INFO:root:[   79] Training loss: 0.17817793, Validation loss: 1.03316483, Gradient norm: 0.70794990
INFO:root:[   80] Training loss: 0.17628301, Validation loss: 1.01917996, Gradient norm: 1.12851109
INFO:root:[   81] Training loss: 0.17396309, Validation loss: 1.02158819, Gradient norm: 1.03157909
INFO:root:[   82] Training loss: 0.17212399, Validation loss: 1.00887776, Gradient norm: 1.14122709
INFO:root:[   83] Training loss: 0.16980999, Validation loss: 0.98646415, Gradient norm: 0.84522506
INFO:root:[   84] Training loss: 0.16775869, Validation loss: 0.97459681, Gradient norm: 1.06595863
INFO:root:[   85] Training loss: 0.16601747, Validation loss: 0.98060540, Gradient norm: 1.09873635
INFO:root:[   86] Training loss: 0.16430620, Validation loss: 0.96450048, Gradient norm: 1.03364895
INFO:root:[   87] Training loss: 0.16282790, Validation loss: 0.96651899, Gradient norm: 1.35387941
INFO:root:[   88] Training loss: 0.16026545, Validation loss: 0.93240109, Gradient norm: 0.86588499
INFO:root:[   89] Training loss: 0.15866868, Validation loss: 0.94549509, Gradient norm: 1.33130398
INFO:root:[   90] Training loss: 0.15675292, Validation loss: 0.91542258, Gradient norm: 1.40389378
INFO:root:[   91] Training loss: 0.15457156, Validation loss: 0.90961888, Gradient norm: 1.06452374
INFO:root:[   92] Training loss: 0.15267277, Validation loss: 0.89350846, Gradient norm: 0.77074030
INFO:root:[   93] Training loss: 0.15107828, Validation loss: 0.88545875, Gradient norm: 1.05022065
INFO:root:[   94] Training loss: 0.14942106, Validation loss: 0.87306461, Gradient norm: 1.70705437
INFO:root:[   95] Training loss: 0.14715669, Validation loss: 0.85240368, Gradient norm: 1.01701469
INFO:root:[   96] Training loss: 0.14540582, Validation loss: 0.83785152, Gradient norm: 0.97985535
INFO:root:[   97] Training loss: 0.14441143, Validation loss: 0.81306918, Gradient norm: 1.67667174
INFO:root:[   98] Training loss: 0.14250925, Validation loss: 0.82138451, Gradient norm: 1.72884087
INFO:root:[   99] Training loss: 0.14041718, Validation loss: 0.80468029, Gradient norm: 1.29398744
INFO:root:[  100] Training loss: 0.13873208, Validation loss: 0.78502579, Gradient norm: 1.10527184
INFO:root:[  101] Training loss: 0.13726543, Validation loss: 0.78355155, Gradient norm: 1.61403321
INFO:root:[  102] Training loss: 0.13524372, Validation loss: 0.76649396, Gradient norm: 1.09710284
INFO:root:[  103] Training loss: 0.13404480, Validation loss: 0.74915655, Gradient norm: 1.90115986
INFO:root:[  104] Training loss: 0.13206052, Validation loss: 0.75157161, Gradient norm: 1.54465886
INFO:root:[  105] Training loss: 0.13057709, Validation loss: 0.73207353, Gradient norm: 1.36054424
INFO:root:[  106] Training loss: 0.12903920, Validation loss: 0.73787978, Gradient norm: 1.37871487
INFO:root:[  107] Training loss: 0.12719385, Validation loss: 0.71441244, Gradient norm: 1.17456823
INFO:root:[  108] Training loss: 0.12578471, Validation loss: 0.71211397, Gradient norm: 1.79082862
INFO:root:[  109] Training loss: 0.12461866, Validation loss: 0.69723816, Gradient norm: 2.03221274
INFO:root:[  110] Training loss: 0.12292469, Validation loss: 0.69491356, Gradient norm: 1.64312484
INFO:root:[  111] Training loss: 0.12146701, Validation loss: 0.66652831, Gradient norm: 1.93888074
INFO:root:[  112] Training loss: 0.11974142, Validation loss: 0.66018685, Gradient norm: 1.12826223
INFO:root:[  113] Training loss: 0.11823251, Validation loss: 0.64518889, Gradient norm: 1.56428540
INFO:root:[  114] Training loss: 0.11692443, Validation loss: 0.63785540, Gradient norm: 1.84699458
INFO:root:[  115] Training loss: 0.11535951, Validation loss: 0.63771873, Gradient norm: 1.29732368
INFO:root:[  116] Training loss: 0.11406487, Validation loss: 0.62130859, Gradient norm: 1.91022825
INFO:root:[  117] Training loss: 0.11273046, Validation loss: 0.61359239, Gradient norm: 1.46664127
INFO:root:[  118] Training loss: 0.11140662, Validation loss: 0.58769354, Gradient norm: 1.87316748
INFO:root:[  119] Training loss: 0.11055664, Validation loss: 0.60079192, Gradient norm: 2.84166494
INFO:root:[  120] Training loss: 0.10883552, Validation loss: 0.58898503, Gradient norm: 1.76761679
INFO:root:[  121] Training loss: 0.10752480, Validation loss: 0.58212602, Gradient norm: 1.80604321
INFO:root:[  122] Training loss: 0.10634187, Validation loss: 0.55991048, Gradient norm: 2.08448462
INFO:root:[  123] Training loss: 0.10541532, Validation loss: 0.56751374, Gradient norm: 2.53014218
INFO:root:[  124] Training loss: 0.10425680, Validation loss: 0.55077651, Gradient norm: 2.35619373
INFO:root:[  125] Training loss: 0.10309307, Validation loss: 0.54166739, Gradient norm: 2.10056729
INFO:root:[  126] Training loss: 0.10160233, Validation loss: 0.53246459, Gradient norm: 1.44484313
INFO:root:[  127] Training loss: 0.10034084, Validation loss: 0.51875643, Gradient norm: 1.79142794
INFO:root:[  128] Training loss: 0.09972465, Validation loss: 0.51948418, Gradient norm: 2.41718349
INFO:root:[  129] Training loss: 0.09846065, Validation loss: 0.51681251, Gradient norm: 2.20973361
INFO:root:[  130] Training loss: 0.09732439, Validation loss: 0.49620417, Gradient norm: 1.80600846
INFO:root:[  131] Training loss: 0.09629191, Validation loss: 0.49761826, Gradient norm: 1.68732894
INFO:root:[  132] Training loss: 0.09566251, Validation loss: 0.48962506, Gradient norm: 2.76825138
INFO:root:[  133] Training loss: 0.09505383, Validation loss: 0.47172458, Gradient norm: 3.18106214
INFO:root:[  134] Training loss: 0.09386899, Validation loss: 0.46838895, Gradient norm: 3.04175616
INFO:root:[  135] Training loss: 0.09285993, Validation loss: 0.46753261, Gradient norm: 2.12988813
INFO:root:[  136] Training loss: 0.09190609, Validation loss: 0.45704206, Gradient norm: 2.32701878
INFO:root:[  137] Training loss: 0.09164589, Validation loss: 0.45632491, Gradient norm: 3.61821269
INFO:root:[  138] Training loss: 0.09071887, Validation loss: 0.43655657, Gradient norm: 2.70293616
INFO:root:[  139] Training loss: 0.08959257, Validation loss: 0.43063569, Gradient norm: 2.67835198
INFO:root:[  140] Training loss: 0.08917549, Validation loss: 0.42698705, Gradient norm: 2.40917960
INFO:root:[  141] Training loss: 0.08896398, Validation loss: 0.41798954, Gradient norm: 3.35168331
INFO:root:[  142] Training loss: 0.08805665, Validation loss: 0.41008534, Gradient norm: 2.47547179
INFO:root:[  143] Training loss: 0.08719823, Validation loss: 0.39776103, Gradient norm: 2.11566487
INFO:root:[  144] Training loss: 0.08683425, Validation loss: 0.38368789, Gradient norm: 4.13856132
INFO:root:[  145] Training loss: 0.08594551, Validation loss: 0.37715903, Gradient norm: 2.89733410
INFO:root:[  146] Training loss: 0.08568538, Validation loss: 0.36268536, Gradient norm: 3.72081116
INFO:root:[  147] Training loss: 0.08509031, Validation loss: 0.35898163, Gradient norm: 4.01997058
INFO:root:[  148] Training loss: 0.08485264, Validation loss: 0.34123607, Gradient norm: 4.97713944
INFO:root:[  149] Training loss: 0.08389040, Validation loss: 0.33305422, Gradient norm: 2.77092900
INFO:root:[  150] Training loss: 0.08346456, Validation loss: 0.32348566, Gradient norm: 3.73767687
INFO:root:[  151] Training loss: 0.08331111, Validation loss: 0.31425342, Gradient norm: 4.72509972
INFO:root:[  152] Training loss: 0.08240959, Validation loss: 0.30715856, Gradient norm: 3.51776135
INFO:root:[  153] Training loss: 0.08208373, Validation loss: 0.28407748, Gradient norm: 4.06150790
INFO:root:[  154] Training loss: 0.08167110, Validation loss: 0.28490462, Gradient norm: 5.82209144
INFO:root:[  155] Training loss: 0.08118946, Validation loss: 0.26460428, Gradient norm: 5.12949690
INFO:root:[  156] Training loss: 0.08108318, Validation loss: 0.24790094, Gradient norm: 6.19231726
INFO:root:[  157] Training loss: 0.08026701, Validation loss: 0.24427345, Gradient norm: 4.53651899
INFO:root:[  158] Training loss: 0.08112563, Validation loss: 0.23185709, Gradient norm: 8.89073555
INFO:root:[  159] Training loss: 0.07972692, Validation loss: 0.21968409, Gradient norm: 5.32571571
INFO:root:[  160] Training loss: 0.07886625, Validation loss: 0.20997089, Gradient norm: 4.09684432
INFO:root:[  161] Training loss: 0.07950032, Validation loss: 0.20177407, Gradient norm: 7.39314371
INFO:root:[  162] Training loss: 0.07875537, Validation loss: 0.19176265, Gradient norm: 6.35512059
INFO:root:[  163] Training loss: 0.07809595, Validation loss: 0.18596939, Gradient norm: 5.79581403
INFO:root:[  164] Training loss: 0.07875823, Validation loss: 0.16478435, Gradient norm: 9.76123681
INFO:root:[  165] Training loss: 0.07759697, Validation loss: 0.16250547, Gradient norm: 6.59641267
INFO:root:[  166] Training loss: 0.07822743, Validation loss: 0.15039612, Gradient norm: 9.65988469
INFO:root:[  167] Training loss: 0.07775105, Validation loss: 0.14916449, Gradient norm: 8.48502634
INFO:root:[  168] Training loss: 0.07734244, Validation loss: 0.13406971, Gradient norm: 8.03820847
INFO:root:[  169] Training loss: 0.07706843, Validation loss: 0.12587439, Gradient norm: 8.32056534
INFO:root:[  170] Training loss: 0.07680759, Validation loss: 0.12242033, Gradient norm: 6.78748674
INFO:root:[  171] Training loss: 0.07797288, Validation loss: 0.11854605, Gradient norm: 14.52294277
INFO:root:[  172] Training loss: 0.07677695, Validation loss: 0.10579723, Gradient norm: 9.97202915
INFO:root:[  173] Training loss: 0.07686736, Validation loss: 0.10575479, Gradient norm: 12.88702865
INFO:root:[  174] Training loss: 0.07594234, Validation loss: 0.10834010, Gradient norm: 8.88637957
INFO:root:[  175] Training loss: 0.07634697, Validation loss: 0.09239618, Gradient norm: 12.96425641
INFO:root:[  176] Training loss: 0.07587985, Validation loss: 0.09307991, Gradient norm: 9.97430576
INFO:root:[  177] Training loss: 0.07606653, Validation loss: 0.09113295, Gradient norm: 11.78073970
INFO:root:[  178] Training loss: 0.07603030, Validation loss: 0.09124259, Gradient norm: 16.39051975
INFO:root:[  179] Training loss: 0.07589321, Validation loss: 0.08128221, Gradient norm: 13.95532944
INFO:root:[  180] Training loss: 0.07613057, Validation loss: 0.07892129, Gradient norm: 14.48716557
INFO:root:[  181] Training loss: 0.07489651, Validation loss: 0.07615589, Gradient norm: 9.98194993
INFO:root:[  182] Training loss: 0.07455301, Validation loss: 0.07616902, Gradient norm: 10.30899394
INFO:root:[  183] Training loss: 0.07506482, Validation loss: 0.07419838, Gradient norm: 16.06359605
INFO:root:[  184] Training loss: 0.07447632, Validation loss: 0.07135782, Gradient norm: 13.57738404
INFO:root:[  185] Training loss: 0.07487436, Validation loss: 0.07196075, Gradient norm: 17.15676321
INFO:root:[  186] Training loss: 0.07575528, Validation loss: 0.07684404, Gradient norm: 20.09311273
INFO:root:[  187] Training loss: 0.07462738, Validation loss: 0.06791622, Gradient norm: 16.88228183
INFO:root:[  188] Training loss: 0.07338567, Validation loss: 0.06799386, Gradient norm: 12.81697700
INFO:root:[  189] Training loss: 0.07375096, Validation loss: 0.06845155, Gradient norm: 13.85698413
INFO:root:[  190] Training loss: 0.07467565, Validation loss: 0.06536670, Gradient norm: 19.40230079
INFO:root:[  191] Training loss: 0.07453003, Validation loss: 0.06791632, Gradient norm: 19.80668876
INFO:root:[  192] Training loss: 0.07434869, Validation loss: 0.06472495, Gradient norm: 20.31183491
INFO:root:[  193] Training loss: 0.07419316, Validation loss: 0.06463932, Gradient norm: 17.89235118
INFO:root:[  194] Training loss: 0.07274535, Validation loss: 0.06165569, Gradient norm: 12.59069444
INFO:root:[  195] Training loss: 0.07296362, Validation loss: 0.06327770, Gradient norm: 17.22244652
INFO:root:[  196] Training loss: 0.07427255, Validation loss: 0.06066959, Gradient norm: 24.64906126
INFO:root:[  197] Training loss: 0.07349202, Validation loss: 0.06383158, Gradient norm: 19.21018088
INFO:root:[  198] Training loss: 0.07429807, Validation loss: 0.06199795, Gradient norm: 24.09612614
INFO:root:[  199] Training loss: 0.07255295, Validation loss: 0.06009824, Gradient norm: 16.16592876
INFO:root:[  200] Training loss: 0.07417168, Validation loss: 0.05957686, Gradient norm: 27.68922627
INFO:root:[  201] Training loss: 0.07375026, Validation loss: 0.06701275, Gradient norm: 24.78393846
INFO:root:[  202] Training loss: 0.07303035, Validation loss: 0.05871731, Gradient norm: 23.37515490
INFO:root:[  203] Training loss: 0.07213487, Validation loss: 0.05747147, Gradient norm: 17.65935623
INFO:root:[  204] Training loss: 0.07221357, Validation loss: 0.05701597, Gradient norm: 18.18118382
INFO:root:[  205] Training loss: 0.07328812, Validation loss: 0.05781424, Gradient norm: 25.26293724
INFO:root:[  206] Training loss: 0.07268691, Validation loss: 0.05504966, Gradient norm: 24.53567661
INFO:root:[  207] Training loss: 0.07326337, Validation loss: 0.05528770, Gradient norm: 27.33136212
INFO:root:[  208] Training loss: 0.07133321, Validation loss: 0.05428592, Gradient norm: 15.97671660
INFO:root:[  209] Training loss: 0.07192673, Validation loss: 0.05539367, Gradient norm: 20.62705410
INFO:root:[  210] Training loss: 0.07237459, Validation loss: 0.05836233, Gradient norm: 28.07304755
INFO:root:[  211] Training loss: 0.07282716, Validation loss: 0.05764870, Gradient norm: 32.05644223
INFO:root:[  212] Training loss: 0.07215831, Validation loss: 0.05866087, Gradient norm: 28.05953227
INFO:root:[  213] Training loss: 0.07327866, Validation loss: 0.06416529, Gradient norm: 32.74294333
INFO:root:[  214] Training loss: 0.07255452, Validation loss: 0.05470258, Gradient norm: 29.74108971
INFO:root:[  215] Training loss: 0.07117496, Validation loss: 0.05135066, Gradient norm: 22.66816480
INFO:root:[  216] Training loss: 0.07061553, Validation loss: 0.05099230, Gradient norm: 17.36727515
INFO:root:[  217] Training loss: 0.07367392, Validation loss: 0.05756412, Gradient norm: 36.56604096
INFO:root:[  218] Training loss: 0.07136823, Validation loss: 0.05078874, Gradient norm: 24.75195133
INFO:root:[  219] Training loss: 0.07082332, Validation loss: 0.05447837, Gradient norm: 22.48854246
INFO:root:[  220] Training loss: 0.07316232, Validation loss: 0.06072215, Gradient norm: 34.89321998
INFO:root:[  221] Training loss: 0.07289687, Validation loss: 0.05642060, Gradient norm: 36.26262917
INFO:root:[  222] Training loss: 0.07085269, Validation loss: 0.05035401, Gradient norm: 21.89608820
INFO:root:[  223] Training loss: 0.07150082, Validation loss: 0.05524628, Gradient norm: 28.85663384
INFO:root:[  224] Training loss: 0.07237628, Validation loss: 0.05353545, Gradient norm: 37.55629535
INFO:root:[  225] Training loss: 0.07029493, Validation loss: 0.05222420, Gradient norm: 24.06663990
INFO:root:[  226] Training loss: 0.07164302, Validation loss: 0.05294832, Gradient norm: 32.46005814
INFO:root:[  227] Training loss: 0.07094876, Validation loss: 0.05727223, Gradient norm: 26.74750488
INFO:root:[  228] Training loss: 0.07131019, Validation loss: 0.05294145, Gradient norm: 28.59117716
INFO:root:[  229] Training loss: 0.07184926, Validation loss: 0.05315963, Gradient norm: 36.68910936
INFO:root:[  230] Training loss: 0.07101144, Validation loss: 0.05084777, Gradient norm: 32.92994839
INFO:root:[  231] Training loss: 0.07144711, Validation loss: 0.04938507, Gradient norm: 31.63508799
INFO:root:[  232] Training loss: 0.07085553, Validation loss: 0.05052892, Gradient norm: 29.45823858
INFO:root:[  233] Training loss: 0.07027441, Validation loss: 0.05200756, Gradient norm: 27.21643943
INFO:root:[  234] Training loss: 0.07143536, Validation loss: 0.05338042, Gradient norm: 35.81989521
INFO:root:[  235] Training loss: 0.06967375, Validation loss: 0.05411707, Gradient norm: 19.83398019
INFO:root:[  236] Training loss: 0.07138599, Validation loss: 0.05177541, Gradient norm: 36.86525897
INFO:root:[  237] Training loss: 0.07017716, Validation loss: 0.05735941, Gradient norm: 30.76079670
INFO:root:[  238] Training loss: 0.07221435, Validation loss: 0.05381355, Gradient norm: 45.72909539
INFO:root:[  239] Training loss: 0.07126986, Validation loss: 0.05167779, Gradient norm: 32.98333359
INFO:root:[  240] Training loss: 0.07334187, Validation loss: 0.06652553, Gradient norm: 47.40455264
INFO:root:EP 240: Early stopping
INFO:root:Training the model took 2745.388s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 4.27226
INFO:root:EnergyScoreTrain: 2.06738
INFO:root:CoverageTrain: 0.42418
INFO:root:IntervalWidthTrain: 0.12569
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 3.74013
INFO:root:EnergyScoreValidation: 1.72361
INFO:root:CoverageValidation: 0.37498
INFO:root:IntervalWidthValidation: 0.11166
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 3.05362
INFO:root:EnergyScoreTest: 1.68759
INFO:root:CoverageTest: 0.42198
INFO:root:IntervalWidthTest: 0.12756
INFO:root:###15 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 1860173824
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 3.30290305, Validation loss: 2.58533883, Gradient norm: 5.64311370
INFO:root:[    2] Training loss: 1.00810513, Validation loss: 0.26833860, Gradient norm: 7.54135808
INFO:root:[    3] Training loss: 0.22865508, Validation loss: 0.20201558, Gradient norm: 2.23590381
INFO:root:[    4] Training loss: 0.19437627, Validation loss: 0.19890402, Gradient norm: 1.91156364
INFO:root:[    5] Training loss: 0.19044939, Validation loss: 0.18942035, Gradient norm: 1.85461184
INFO:root:[    6] Training loss: 0.18656832, Validation loss: 0.20128954, Gradient norm: 1.56615024
INFO:root:[    7] Training loss: 0.18336774, Validation loss: 0.17514084, Gradient norm: 2.04786590
INFO:root:[    8] Training loss: 0.17898816, Validation loss: 0.16697868, Gradient norm: 2.63616317
INFO:root:[    9] Training loss: 0.16908482, Validation loss: 0.17392229, Gradient norm: 1.60832730
INFO:root:[   10] Training loss: 0.16901195, Validation loss: 0.16617482, Gradient norm: 2.04803767
INFO:root:[   11] Training loss: 0.15719482, Validation loss: 0.15468562, Gradient norm: 2.07080645
INFO:root:[   12] Training loss: 0.15817310, Validation loss: 0.14169018, Gradient norm: 2.26232095
INFO:root:[   13] Training loss: 0.14245376, Validation loss: 0.14346826, Gradient norm: 1.40301220
INFO:root:[   14] Training loss: 0.14162827, Validation loss: 0.14047432, Gradient norm: 1.79224753
INFO:root:[   15] Training loss: 0.13159263, Validation loss: 0.12560660, Gradient norm: 1.41290238
INFO:root:[   16] Training loss: 0.12675729, Validation loss: 0.12391032, Gradient norm: 1.84253085
INFO:root:[   17] Training loss: 0.12105366, Validation loss: 0.11794030, Gradient norm: 2.31133363
INFO:root:[   18] Training loss: 0.11570550, Validation loss: 0.12094714, Gradient norm: 1.83265580
INFO:root:[   19] Training loss: 0.11506859, Validation loss: 0.11268082, Gradient norm: 2.28589499
INFO:root:[   20] Training loss: 0.10904067, Validation loss: 0.10618200, Gradient norm: 1.68463613
INFO:root:[   21] Training loss: 0.10690697, Validation loss: 0.11041439, Gradient norm: 1.72182188
INFO:root:[   22] Training loss: 0.10203228, Validation loss: 0.09902798, Gradient norm: 1.37578551
INFO:root:[   23] Training loss: 0.10079081, Validation loss: 0.10047811, Gradient norm: 1.62656891
INFO:root:[   24] Training loss: 0.09677135, Validation loss: 0.09365455, Gradient norm: 1.62090382
INFO:root:[   25] Training loss: 0.09502190, Validation loss: 0.09250564, Gradient norm: 1.34803771
INFO:root:[   26] Training loss: 0.09329864, Validation loss: 0.09239013, Gradient norm: 1.73368227
INFO:root:[   27] Training loss: 0.08869634, Validation loss: 0.09113308, Gradient norm: 1.10743946
INFO:root:[   28] Training loss: 0.08858959, Validation loss: 0.08746572, Gradient norm: 1.51970908
INFO:root:[   29] Training loss: 0.08472587, Validation loss: 0.08860882, Gradient norm: 0.85311091
INFO:root:[   30] Training loss: 0.08373447, Validation loss: 0.08332888, Gradient norm: 1.44772633
INFO:root:[   31] Training loss: 0.08138119, Validation loss: 0.08115188, Gradient norm: 0.84711190
INFO:root:[   32] Training loss: 0.08043888, Validation loss: 0.07956533, Gradient norm: 1.03329546
INFO:root:[   33] Training loss: 0.07856814, Validation loss: 0.07738353, Gradient norm: 0.77119228
INFO:root:[   34] Training loss: 0.07725364, Validation loss: 0.07964271, Gradient norm: 1.43747924
INFO:root:[   35] Training loss: 0.07605580, Validation loss: 0.07453147, Gradient norm: 1.22755769
INFO:root:[   36] Training loss: 0.07534387, Validation loss: 0.07379800, Gradient norm: 1.30589032
INFO:root:[   37] Training loss: 0.07424545, Validation loss: 0.07446896, Gradient norm: 1.38549477
INFO:root:[   38] Training loss: 0.07301167, Validation loss: 0.07281031, Gradient norm: 1.52031409
INFO:root:[   39] Training loss: 0.07286604, Validation loss: 0.07273526, Gradient norm: 1.52885706
INFO:root:[   40] Training loss: 0.07121494, Validation loss: 0.07084520, Gradient norm: 1.54867522
INFO:root:[   41] Training loss: 0.06988708, Validation loss: 0.06944912, Gradient norm: 0.83492873
INFO:root:[   42] Training loss: 0.06956146, Validation loss: 0.06783004, Gradient norm: 1.37431711
INFO:root:[   43] Training loss: 0.06852771, Validation loss: 0.07020292, Gradient norm: 1.45169280
INFO:root:[   44] Training loss: 0.06711670, Validation loss: 0.06742568, Gradient norm: 0.93484265
INFO:root:[   45] Training loss: 0.06663809, Validation loss: 0.06601397, Gradient norm: 1.20030089
INFO:root:[   46] Training loss: 0.06595384, Validation loss: 0.06670954, Gradient norm: 1.07659758
INFO:root:[   47] Training loss: 0.06497823, Validation loss: 0.06596242, Gradient norm: 0.70458465
INFO:root:[   48] Training loss: 0.06475109, Validation loss: 0.06506881, Gradient norm: 1.06985851
INFO:root:[   49] Training loss: 0.06405102, Validation loss: 0.06457666, Gradient norm: 1.06167997
INFO:root:[   50] Training loss: 0.06347501, Validation loss: 0.06244094, Gradient norm: 0.96395014
INFO:root:[   51] Training loss: 0.06283684, Validation loss: 0.06210225, Gradient norm: 1.15591039
INFO:root:[   52] Training loss: 0.06221413, Validation loss: 0.06197572, Gradient norm: 0.96633060
INFO:root:[   53] Training loss: 0.06175436, Validation loss: 0.06174646, Gradient norm: 1.33746456
INFO:root:[   54] Training loss: 0.06175750, Validation loss: 0.06161482, Gradient norm: 1.25872132
INFO:root:[   55] Training loss: 0.06045250, Validation loss: 0.06070457, Gradient norm: 1.07236721
INFO:root:[   56] Training loss: 0.06068928, Validation loss: 0.05935656, Gradient norm: 1.38506016
INFO:root:[   57] Training loss: 0.06008961, Validation loss: 0.05975057, Gradient norm: 1.08681247
INFO:root:[   58] Training loss: 0.05910178, Validation loss: 0.05895573, Gradient norm: 0.75859759
INFO:root:[   59] Training loss: 0.05874161, Validation loss: 0.05845257, Gradient norm: 0.74265814
INFO:root:[   60] Training loss: 0.05837857, Validation loss: 0.05840587, Gradient norm: 1.18321348
INFO:root:[   61] Training loss: 0.05786419, Validation loss: 0.05814379, Gradient norm: 0.92762425
INFO:root:[   62] Training loss: 0.05749311, Validation loss: 0.05842743, Gradient norm: 0.87391105
INFO:root:[   63] Training loss: 0.05823987, Validation loss: 0.05687474, Gradient norm: 1.64897804
INFO:root:[   64] Training loss: 0.05729374, Validation loss: 0.05770060, Gradient norm: 1.40171602
INFO:root:[   65] Training loss: 0.05670843, Validation loss: 0.05621041, Gradient norm: 1.14814109
INFO:root:[   66] Training loss: 0.05622788, Validation loss: 0.05574752, Gradient norm: 0.94682221
INFO:root:[   67] Training loss: 0.05655611, Validation loss: 0.05525171, Gradient norm: 1.28514134
INFO:root:[   68] Training loss: 0.05578256, Validation loss: 0.05462397, Gradient norm: 1.18613696
INFO:root:[   69] Training loss: 0.05568649, Validation loss: 0.05603497, Gradient norm: 1.27127038
INFO:root:[   70] Training loss: 0.05482579, Validation loss: 0.05449009, Gradient norm: 0.96995917
INFO:root:[   71] Training loss: 0.05417594, Validation loss: 0.05449161, Gradient norm: 0.82118912
INFO:root:[   72] Training loss: 0.05492403, Validation loss: 0.05465588, Gradient norm: 1.50068775
INFO:root:[   73] Training loss: 0.05435176, Validation loss: 0.05419315, Gradient norm: 1.39900667
INFO:root:[   74] Training loss: 0.05382385, Validation loss: 0.05483439, Gradient norm: 1.36286447
INFO:root:[   75] Training loss: 0.05320438, Validation loss: 0.05266667, Gradient norm: 1.23605395
INFO:root:[   76] Training loss: 0.05347992, Validation loss: 0.05251000, Gradient norm: 1.32374521
INFO:root:[   77] Training loss: 0.05301800, Validation loss: 0.05338762, Gradient norm: 1.53255435
INFO:root:[   78] Training loss: 0.05261575, Validation loss: 0.05243990, Gradient norm: 1.16519728
INFO:root:[   79] Training loss: 0.05279461, Validation loss: 0.05271752, Gradient norm: 1.56053886
INFO:root:[   80] Training loss: 0.05231249, Validation loss: 0.05317442, Gradient norm: 1.36184134
INFO:root:[   81] Training loss: 0.05229515, Validation loss: 0.05223653, Gradient norm: 1.55150561
INFO:root:[   82] Training loss: 0.05209572, Validation loss: 0.05126812, Gradient norm: 1.27762647
INFO:root:[   83] Training loss: 0.05200086, Validation loss: 0.05093553, Gradient norm: 1.47227288
INFO:root:[   84] Training loss: 0.05157871, Validation loss: 0.05195289, Gradient norm: 1.47893930
INFO:root:[   85] Training loss: 0.05107605, Validation loss: 0.05119632, Gradient norm: 0.97861140
INFO:root:[   86] Training loss: 0.05086890, Validation loss: 0.05046175, Gradient norm: 0.99881853
INFO:root:[   87] Training loss: 0.05066044, Validation loss: 0.05120088, Gradient norm: 1.36290802
INFO:root:[   88] Training loss: 0.05093290, Validation loss: 0.05030063, Gradient norm: 1.73125443
INFO:root:[   89] Training loss: 0.05049213, Validation loss: 0.05012484, Gradient norm: 1.34694157
INFO:root:[   90] Training loss: 0.05081699, Validation loss: 0.04981046, Gradient norm: 1.81113168
INFO:root:[   91] Training loss: 0.04980480, Validation loss: 0.04987105, Gradient norm: 1.11596328
INFO:root:[   92] Training loss: 0.05003717, Validation loss: 0.04995060, Gradient norm: 1.46872187
INFO:root:[   93] Training loss: 0.04940241, Validation loss: 0.04914600, Gradient norm: 1.05536585
INFO:root:[   94] Training loss: 0.04941301, Validation loss: 0.04908765, Gradient norm: 1.56137509
INFO:root:[   95] Training loss: 0.04951989, Validation loss: 0.04882780, Gradient norm: 1.48102601
INFO:root:[   96] Training loss: 0.04884823, Validation loss: 0.04858422, Gradient norm: 1.32460807
INFO:root:[   97] Training loss: 0.04879155, Validation loss: 0.04849042, Gradient norm: 1.03490074
INFO:root:[   98] Training loss: 0.04826722, Validation loss: 0.04876658, Gradient norm: 0.98675837
INFO:root:[   99] Training loss: 0.04859202, Validation loss: 0.04928003, Gradient norm: 1.25279066
INFO:root:[  100] Training loss: 0.04820579, Validation loss: 0.04783564, Gradient norm: 1.33423367
INFO:root:[  101] Training loss: 0.04801807, Validation loss: 0.04847740, Gradient norm: 1.09601890
INFO:root:[  102] Training loss: 0.04801044, Validation loss: 0.04812308, Gradient norm: 1.49322508
INFO:root:[  103] Training loss: 0.04769276, Validation loss: 0.04755326, Gradient norm: 1.20930057
INFO:root:[  104] Training loss: 0.04744304, Validation loss: 0.04719606, Gradient norm: 1.10453580
INFO:root:[  105] Training loss: 0.04736576, Validation loss: 0.04732872, Gradient norm: 1.51533382
INFO:root:[  106] Training loss: 0.04708083, Validation loss: 0.04709649, Gradient norm: 1.25060344
INFO:root:[  107] Training loss: 0.04706495, Validation loss: 0.04703976, Gradient norm: 1.25672774
INFO:root:[  108] Training loss: 0.04692243, Validation loss: 0.04687876, Gradient norm: 1.59385456
INFO:root:[  109] Training loss: 0.04670936, Validation loss: 0.04736200, Gradient norm: 1.39382941
INFO:root:[  110] Training loss: 0.04660511, Validation loss: 0.04676745, Gradient norm: 1.39784907
INFO:root:[  111] Training loss: 0.04686490, Validation loss: 0.04599038, Gradient norm: 1.79006981
INFO:root:[  112] Training loss: 0.04675758, Validation loss: 0.04532491, Gradient norm: 1.77490378
INFO:root:[  113] Training loss: 0.04660829, Validation loss: 0.04526125, Gradient norm: 1.85900587
INFO:root:[  114] Training loss: 0.04641948, Validation loss: 0.04557706, Gradient norm: 1.76173494
INFO:root:[  115] Training loss: 0.04620474, Validation loss: 0.04550165, Gradient norm: 1.44287621
INFO:root:[  116] Training loss: 0.04582629, Validation loss: 0.04550908, Gradient norm: 1.53120826
INFO:root:[  117] Training loss: 0.04585489, Validation loss: 0.04677595, Gradient norm: 1.54347827
INFO:root:[  118] Training loss: 0.04570841, Validation loss: 0.04614812, Gradient norm: 1.48407752
INFO:root:[  119] Training loss: 0.04528977, Validation loss: 0.04597508, Gradient norm: 1.36628844
INFO:root:[  120] Training loss: 0.04547241, Validation loss: 0.04497869, Gradient norm: 1.72126875
INFO:root:[  121] Training loss: 0.04511647, Validation loss: 0.04423304, Gradient norm: 1.60106385
INFO:root:[  122] Training loss: 0.04490811, Validation loss: 0.04438241, Gradient norm: 1.20092904
INFO:root:[  123] Training loss: 0.04504586, Validation loss: 0.04458330, Gradient norm: 1.45819336
INFO:root:[  124] Training loss: 0.04503223, Validation loss: 0.04507744, Gradient norm: 1.61347636
INFO:root:[  125] Training loss: 0.04462968, Validation loss: 0.04400255, Gradient norm: 1.54934131
INFO:root:[  126] Training loss: 0.04449943, Validation loss: 0.04400160, Gradient norm: 1.50775920
INFO:root:[  127] Training loss: 0.04413092, Validation loss: 0.04569691, Gradient norm: 1.33357291
INFO:root:[  128] Training loss: 0.04419710, Validation loss: 0.04390899, Gradient norm: 1.23159850
INFO:root:[  129] Training loss: 0.04389285, Validation loss: 0.04454535, Gradient norm: 1.11995419
INFO:root:[  130] Training loss: 0.04414773, Validation loss: 0.04420755, Gradient norm: 1.50599859
INFO:root:[  131] Training loss: 0.04426172, Validation loss: 0.04393837, Gradient norm: 1.74095656
INFO:root:[  132] Training loss: 0.04372747, Validation loss: 0.04324529, Gradient norm: 1.35508974
INFO:root:[  133] Training loss: 0.04369252, Validation loss: 0.04584733, Gradient norm: 1.66954580
INFO:root:[  134] Training loss: 0.04387291, Validation loss: 0.04385714, Gradient norm: 1.54242125
INFO:root:[  135] Training loss: 0.04354477, Validation loss: 0.04329832, Gradient norm: 1.50448131
INFO:root:[  136] Training loss: 0.04387415, Validation loss: 0.04437128, Gradient norm: 1.97800166
INFO:root:[  137] Training loss: 0.04325763, Validation loss: 0.04278106, Gradient norm: 1.59658942
INFO:root:[  138] Training loss: 0.04329410, Validation loss: 0.04314071, Gradient norm: 1.31637103
INFO:root:[  139] Training loss: 0.04286032, Validation loss: 0.04394843, Gradient norm: 1.27997706
INFO:root:[  140] Training loss: 0.04289319, Validation loss: 0.04226493, Gradient norm: 1.38684130
INFO:root:[  141] Training loss: 0.04251095, Validation loss: 0.04353283, Gradient norm: 1.28774122
INFO:root:[  142] Training loss: 0.04295250, Validation loss: 0.04194595, Gradient norm: 1.68071272
INFO:root:[  143] Training loss: 0.04257317, Validation loss: 0.04235814, Gradient norm: 1.53617962
INFO:root:[  144] Training loss: 0.04250203, Validation loss: 0.04217406, Gradient norm: 1.55777105
INFO:root:[  145] Training loss: 0.04251262, Validation loss: 0.04198222, Gradient norm: 1.74617007
INFO:root:[  146] Training loss: 0.04237744, Validation loss: 0.04335184, Gradient norm: 1.53452508
INFO:root:[  147] Training loss: 0.04242617, Validation loss: 0.04174957, Gradient norm: 1.44943934
INFO:root:[  148] Training loss: 0.04216175, Validation loss: 0.04151227, Gradient norm: 1.52715008
INFO:root:[  149] Training loss: 0.04189520, Validation loss: 0.04289392, Gradient norm: 1.31136760
INFO:root:[  150] Training loss: 0.04240649, Validation loss: 0.04283640, Gradient norm: 1.87875816
INFO:root:[  151] Training loss: 0.04192218, Validation loss: 0.04139693, Gradient norm: 1.32098821
INFO:root:[  152] Training loss: 0.04192512, Validation loss: 0.04292648, Gradient norm: 1.54489908
INFO:root:[  153] Training loss: 0.04198023, Validation loss: 0.04262655, Gradient norm: 1.84986086
INFO:root:[  154] Training loss: 0.04193831, Validation loss: 0.04252161, Gradient norm: 1.81914121
INFO:root:[  155] Training loss: 0.04187223, Validation loss: 0.04108006, Gradient norm: 1.75217359
INFO:root:[  156] Training loss: 0.04117558, Validation loss: 0.04130231, Gradient norm: 1.05857284
INFO:root:[  157] Training loss: 0.04128777, Validation loss: 0.04111275, Gradient norm: 1.39752091
INFO:root:[  158] Training loss: 0.04128269, Validation loss: 0.04173724, Gradient norm: 1.34455505
INFO:root:[  159] Training loss: 0.04145228, Validation loss: 0.04079031, Gradient norm: 1.91385560
INFO:root:[  160] Training loss: 0.04131155, Validation loss: 0.04082364, Gradient norm: 1.71669735
INFO:root:[  161] Training loss: 0.04113944, Validation loss: 0.04046911, Gradient norm: 1.61778809
INFO:root:[  162] Training loss: 0.04115746, Validation loss: 0.04052606, Gradient norm: 1.52134030
INFO:root:[  163] Training loss: 0.04093985, Validation loss: 0.04195779, Gradient norm: 1.65672477
INFO:root:[  164] Training loss: 0.04088588, Validation loss: 0.04097310, Gradient norm: 1.60135899
INFO:root:[  165] Training loss: 0.04075965, Validation loss: 0.04077845, Gradient norm: 1.42480859
INFO:root:[  166] Training loss: 0.04065121, Validation loss: 0.04013723, Gradient norm: 1.56452251
INFO:root:[  167] Training loss: 0.04032661, Validation loss: 0.04044094, Gradient norm: 1.21186807
INFO:root:[  168] Training loss: 0.04038183, Validation loss: 0.04015609, Gradient norm: 1.39225048
INFO:root:[  169] Training loss: 0.04040776, Validation loss: 0.03972489, Gradient norm: 1.70420083
INFO:root:[  170] Training loss: 0.04030638, Validation loss: 0.04028545, Gradient norm: 1.58025822
INFO:root:[  171] Training loss: 0.04004284, Validation loss: 0.04015481, Gradient norm: 1.30926565
INFO:root:[  172] Training loss: 0.04020272, Validation loss: 0.04017241, Gradient norm: 1.66960450
INFO:root:[  173] Training loss: 0.04021031, Validation loss: 0.03972704, Gradient norm: 1.78296586
INFO:root:[  174] Training loss: 0.04023453, Validation loss: 0.04135649, Gradient norm: 1.92038752
INFO:root:[  175] Training loss: 0.04005415, Validation loss: 0.04094039, Gradient norm: 1.69178774
INFO:root:[  176] Training loss: 0.04016696, Validation loss: 0.03946806, Gradient norm: 1.79244052
INFO:root:[  177] Training loss: 0.03970675, Validation loss: 0.03971046, Gradient norm: 1.49222976
INFO:root:[  178] Training loss: 0.03959136, Validation loss: 0.03891818, Gradient norm: 1.34200737
INFO:root:[  179] Training loss: 0.03951482, Validation loss: 0.03935635, Gradient norm: 1.33146133
INFO:root:[  180] Training loss: 0.03959230, Validation loss: 0.03970526, Gradient norm: 1.50380641
INFO:root:[  181] Training loss: 0.03975328, Validation loss: 0.03920780, Gradient norm: 1.82070296
INFO:root:[  182] Training loss: 0.03943038, Validation loss: 0.03954431, Gradient norm: 1.49026959
INFO:root:[  183] Training loss: 0.03928229, Validation loss: 0.03886803, Gradient norm: 1.60019604
INFO:root:[  184] Training loss: 0.03933380, Validation loss: 0.03957940, Gradient norm: 1.63794661
INFO:root:[  185] Training loss: 0.03929837, Validation loss: 0.04130556, Gradient norm: 1.71577508
INFO:root:[  186] Training loss: 0.03943036, Validation loss: 0.03876126, Gradient norm: 1.89793290
INFO:root:[  187] Training loss: 0.03933523, Validation loss: 0.03948855, Gradient norm: 1.72418712
INFO:root:[  188] Training loss: 0.03899676, Validation loss: 0.03876283, Gradient norm: 1.58454067
INFO:root:[  189] Training loss: 0.03905633, Validation loss: 0.03897974, Gradient norm: 1.68780800
INFO:root:[  190] Training loss: 0.03891860, Validation loss: 0.03880585, Gradient norm: 1.72117194
INFO:root:[  191] Training loss: 0.03904321, Validation loss: 0.03936731, Gradient norm: 1.72101968
INFO:root:[  192] Training loss: 0.03858335, Validation loss: 0.03889528, Gradient norm: 1.22524208
INFO:root:[  193] Training loss: 0.03882449, Validation loss: 0.03973292, Gradient norm: 1.72503373
INFO:root:[  194] Training loss: 0.03878546, Validation loss: 0.03855849, Gradient norm: 1.46469566
INFO:root:[  195] Training loss: 0.03868665, Validation loss: 0.03867860, Gradient norm: 1.79677261
INFO:root:[  196] Training loss: 0.03875854, Validation loss: 0.03969600, Gradient norm: 1.79253746
INFO:root:[  197] Training loss: 0.03864263, Validation loss: 0.03815069, Gradient norm: 1.88501815
INFO:root:[  198] Training loss: 0.03843974, Validation loss: 0.03843156, Gradient norm: 1.58335280
INFO:root:[  199] Training loss: 0.03839225, Validation loss: 0.03773610, Gradient norm: 1.52127924
INFO:root:[  200] Training loss: 0.03811775, Validation loss: 0.03787292, Gradient norm: 1.34416561
INFO:root:[  201] Training loss: 0.03815552, Validation loss: 0.03856073, Gradient norm: 1.47016361
INFO:root:[  202] Training loss: 0.03817841, Validation loss: 0.03895340, Gradient norm: 1.67487421
INFO:root:[  203] Training loss: 0.03823046, Validation loss: 0.03762211, Gradient norm: 1.90938777
INFO:root:[  204] Training loss: 0.03809207, Validation loss: 0.03925801, Gradient norm: 1.79985643
INFO:root:[  205] Training loss: 0.03819484, Validation loss: 0.03771172, Gradient norm: 1.80116954
INFO:root:[  206] Training loss: 0.03800418, Validation loss: 0.03788928, Gradient norm: 1.63923329
INFO:root:[  207] Training loss: 0.03789337, Validation loss: 0.03816982, Gradient norm: 1.60463331
INFO:root:[  208] Training loss: 0.03780067, Validation loss: 0.03780835, Gradient norm: 1.50424551
INFO:root:[  209] Training loss: 0.03763555, Validation loss: 0.03925951, Gradient norm: 1.56584714
INFO:root:[  210] Training loss: 0.03801351, Validation loss: 0.03753124, Gradient norm: 1.86429081
INFO:root:[  211] Training loss: 0.03761933, Validation loss: 0.03729979, Gradient norm: 1.66570168
INFO:root:[  212] Training loss: 0.03740094, Validation loss: 0.03754150, Gradient norm: 1.51165336
INFO:root:[  213] Training loss: 0.03758027, Validation loss: 0.03726751, Gradient norm: 1.69258006
INFO:root:[  214] Training loss: 0.03738352, Validation loss: 0.03701743, Gradient norm: 1.45166821
INFO:root:[  215] Training loss: 0.03744516, Validation loss: 0.03696438, Gradient norm: 1.80970627
INFO:root:[  216] Training loss: 0.03732866, Validation loss: 0.03734189, Gradient norm: 1.59543360
INFO:root:[  217] Training loss: 0.03717760, Validation loss: 0.03680549, Gradient norm: 1.48514351
INFO:root:[  218] Training loss: 0.03725819, Validation loss: 0.03725259, Gradient norm: 1.73166364
INFO:root:[  219] Training loss: 0.03748420, Validation loss: 0.03740265, Gradient norm: 1.90001112
INFO:root:[  220] Training loss: 0.03729466, Validation loss: 0.03742146, Gradient norm: 1.73839910
INFO:root:[  221] Training loss: 0.03695610, Validation loss: 0.03704431, Gradient norm: 1.58881815
INFO:root:[  222] Training loss: 0.03671004, Validation loss: 0.03698134, Gradient norm: 1.28747717
INFO:root:[  223] Training loss: 0.03697669, Validation loss: 0.03662624, Gradient norm: 1.48564234
INFO:root:[  224] Training loss: 0.03673397, Validation loss: 0.03758327, Gradient norm: 1.22794207
INFO:root:[  225] Training loss: 0.03696544, Validation loss: 0.03645088, Gradient norm: 1.94994805
INFO:root:[  226] Training loss: 0.03673756, Validation loss: 0.03718828, Gradient norm: 1.64622231
INFO:root:[  227] Training loss: 0.03669865, Validation loss: 0.03685314, Gradient norm: 1.42557427
INFO:root:[  228] Training loss: 0.03666904, Validation loss: 0.03764029, Gradient norm: 1.61648227
INFO:root:[  229] Training loss: 0.03660953, Validation loss: 0.03631778, Gradient norm: 1.56525602
INFO:root:[  230] Training loss: 0.03670837, Validation loss: 0.03677424, Gradient norm: 1.84836430
INFO:root:[  231] Training loss: 0.03650781, Validation loss: 0.03650024, Gradient norm: 1.47394876
INFO:root:[  232] Training loss: 0.03669687, Validation loss: 0.03667386, Gradient norm: 1.79150059
INFO:root:[  233] Training loss: 0.03665020, Validation loss: 0.03728093, Gradient norm: 2.06324829
INFO:root:[  234] Training loss: 0.03649787, Validation loss: 0.03579355, Gradient norm: 1.79370742
INFO:root:[  235] Training loss: 0.03625896, Validation loss: 0.03667050, Gradient norm: 1.46391071
INFO:root:[  236] Training loss: 0.03619912, Validation loss: 0.03677872, Gradient norm: 1.45248576
INFO:root:[  237] Training loss: 0.03619602, Validation loss: 0.03667749, Gradient norm: 1.61591474
INFO:root:[  238] Training loss: 0.03618019, Validation loss: 0.03626776, Gradient norm: 1.66793705
INFO:root:[  239] Training loss: 0.03624813, Validation loss: 0.03652204, Gradient norm: 1.54615353
INFO:root:[  240] Training loss: 0.03605948, Validation loss: 0.03668191, Gradient norm: 1.53793740
INFO:root:[  241] Training loss: 0.03618134, Validation loss: 0.03589473, Gradient norm: 1.90531121
INFO:root:[  242] Training loss: 0.03596962, Validation loss: 0.03580451, Gradient norm: 1.31296507
INFO:root:[  243] Training loss: 0.03583589, Validation loss: 0.03662963, Gradient norm: 1.41662063
INFO:root:[  244] Training loss: 0.03578654, Validation loss: 0.03566137, Gradient norm: 1.56237857
INFO:root:[  245] Training loss: 0.03607002, Validation loss: 0.03533396, Gradient norm: 1.85009337
INFO:root:[  246] Training loss: 0.03576118, Validation loss: 0.03686968, Gradient norm: 1.80631023
INFO:root:[  247] Training loss: 0.03578836, Validation loss: 0.03582147, Gradient norm: 1.70028439
INFO:root:[  248] Training loss: 0.03557458, Validation loss: 0.03513990, Gradient norm: 1.45262211
INFO:root:[  249] Training loss: 0.03571227, Validation loss: 0.03543284, Gradient norm: 1.87854199
INFO:root:[  250] Training loss: 0.03581819, Validation loss: 0.03672788, Gradient norm: 1.82792347
INFO:root:[  251] Training loss: 0.03548853, Validation loss: 0.03583336, Gradient norm: 1.61071300
INFO:root:[  252] Training loss: 0.03552993, Validation loss: 0.03585952, Gradient norm: 1.68661512
INFO:root:[  253] Training loss: 0.03594192, Validation loss: 0.03648688, Gradient norm: 2.09577562
INFO:root:[  254] Training loss: 0.03544614, Validation loss: 0.03558412, Gradient norm: 1.71773419
INFO:root:[  255] Training loss: 0.03546020, Validation loss: 0.03611401, Gradient norm: 1.73878582
INFO:root:[  256] Training loss: 0.03554496, Validation loss: 0.03608322, Gradient norm: 1.79401098
INFO:root:[  257] Training loss: 0.03535124, Validation loss: 0.03545453, Gradient norm: 1.86243380
INFO:root:EP 257: Early stopping
INFO:root:Training the model took 2977.382s.
INFO:root:Emptying the cuda cache took 0.037s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.26079
INFO:root:EnergyScoreTrain: 0.19138
INFO:root:CoverageTrain: 0.978
INFO:root:IntervalWidthTrain: 0.02786
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.24591
INFO:root:EnergyScoreValidation: 0.18028
INFO:root:CoverageValidation: 0.97792
INFO:root:IntervalWidthValidation: 0.02796
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.23112
INFO:root:EnergyScoreTest: 0.16959
INFO:root:CoverageTest: 0.97783
INFO:root:IntervalWidthTest: 0.02767
INFO:root:###16 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.05, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 3.19606281, Validation loss: 2.75078414, Gradient norm: 4.67053047
INFO:root:[    2] Training loss: 1.62322890, Validation loss: 0.48538293, Gradient norm: 6.00333800
INFO:root:[    3] Training loss: 0.41767828, Validation loss: 0.37196590, Gradient norm: 1.86287231
INFO:root:[    4] Training loss: 0.36547979, Validation loss: 0.33371234, Gradient norm: 1.99100203
INFO:root:[    5] Training loss: 0.32345115, Validation loss: 0.31162229, Gradient norm: 1.86508652
INFO:root:[    6] Training loss: 0.29635368, Validation loss: 0.27589664, Gradient norm: 1.74533486
INFO:root:[    7] Training loss: 0.27356536, Validation loss: 0.25772987, Gradient norm: 1.78407457
INFO:root:[    8] Training loss: 0.24870604, Validation loss: 0.25281507, Gradient norm: 1.64370138
INFO:root:[    9] Training loss: 0.23664011, Validation loss: 0.22812790, Gradient norm: 1.79799699
INFO:root:[   10] Training loss: 0.22418156, Validation loss: 0.21462699, Gradient norm: 1.59940428
INFO:root:[   11] Training loss: 0.21548677, Validation loss: 0.20981875, Gradient norm: 1.56828039
INFO:root:[   12] Training loss: 0.20744534, Validation loss: 0.19605936, Gradient norm: 1.54365655
INFO:root:[   13] Training loss: 0.19372566, Validation loss: 0.18864160, Gradient norm: 1.25206782
INFO:root:[   14] Training loss: 0.18348788, Validation loss: 0.18204444, Gradient norm: 0.95055949
INFO:root:[   15] Training loss: 0.17612861, Validation loss: 0.17482252, Gradient norm: 1.11845808
INFO:root:[   16] Training loss: 0.16925761, Validation loss: 0.16760395, Gradient norm: 1.06845175
INFO:root:[   17] Training loss: 0.16198059, Validation loss: 0.15977629, Gradient norm: 1.00112556
INFO:root:[   18] Training loss: 0.15620832, Validation loss: 0.15430270, Gradient norm: 1.13937352
INFO:root:[   19] Training loss: 0.15042082, Validation loss: 0.14944020, Gradient norm: 1.06921336
INFO:root:[   20] Training loss: 0.14549835, Validation loss: 0.14082996, Gradient norm: 0.94877908
INFO:root:[   21] Training loss: 0.14059143, Validation loss: 0.13811174, Gradient norm: 0.73134797
INFO:root:[   22] Training loss: 0.13668388, Validation loss: 0.13439499, Gradient norm: 0.86121744
INFO:root:[   23] Training loss: 0.13417422, Validation loss: 0.13283681, Gradient norm: 1.04591104
INFO:root:[   24] Training loss: 0.12993637, Validation loss: 0.12828997, Gradient norm: 0.92328316
INFO:root:[   25] Training loss: 0.12713783, Validation loss: 0.12550505, Gradient norm: 0.77566307
INFO:root:[   26] Training loss: 0.12530851, Validation loss: 0.12366910, Gradient norm: 1.10026134
INFO:root:[   27] Training loss: 0.12217548, Validation loss: 0.12293253, Gradient norm: 0.66347222
INFO:root:[   28] Training loss: 0.12062844, Validation loss: 0.12106976, Gradient norm: 0.65515865
INFO:root:[   29] Training loss: 0.11974784, Validation loss: 0.11839628, Gradient norm: 0.81989770
INFO:root:[   30] Training loss: 0.11754904, Validation loss: 0.11795009, Gradient norm: 0.67129420
INFO:root:[   31] Training loss: 0.11619444, Validation loss: 0.11457090, Gradient norm: 0.92146252
INFO:root:[   32] Training loss: 0.11469363, Validation loss: 0.11444677, Gradient norm: 0.79043558
INFO:root:[   33] Training loss: 0.11340245, Validation loss: 0.11329139, Gradient norm: 0.64564371
INFO:root:[   34] Training loss: 0.11196405, Validation loss: 0.11256435, Gradient norm: 0.70615414
INFO:root:[   35] Training loss: 0.11116775, Validation loss: 0.11062518, Gradient norm: 0.68017968
INFO:root:[   36] Training loss: 0.10971722, Validation loss: 0.10981255, Gradient norm: 0.66417629
INFO:root:[   37] Training loss: 0.10989962, Validation loss: 0.10911433, Gradient norm: 1.23722094
INFO:root:[   38] Training loss: 0.10811648, Validation loss: 0.10877746, Gradient norm: 0.76014810
INFO:root:[   39] Training loss: 0.10728030, Validation loss: 0.10725396, Gradient norm: 0.74252244
INFO:root:[   40] Training loss: 0.10668280, Validation loss: 0.10802353, Gradient norm: 0.78455902
INFO:root:[   41] Training loss: 0.10581574, Validation loss: 0.10579690, Gradient norm: 1.00844264
INFO:root:[   42] Training loss: 0.10531751, Validation loss: 0.10464354, Gradient norm: 0.71024784
INFO:root:[   43] Training loss: 0.10459044, Validation loss: 0.10407739, Gradient norm: 0.85312856
INFO:root:[   44] Training loss: 0.10336987, Validation loss: 0.10257951, Gradient norm: 0.56365397
INFO:root:[   45] Training loss: 0.10281046, Validation loss: 0.10260872, Gradient norm: 0.86142481
INFO:root:[   46] Training loss: 0.10189577, Validation loss: 0.10187810, Gradient norm: 0.59721118
INFO:root:[   47] Training loss: 0.10147940, Validation loss: 0.10139181, Gradient norm: 0.71193480
INFO:root:[   48] Training loss: 0.10103559, Validation loss: 0.10100332, Gradient norm: 1.06962868
INFO:root:[   49] Training loss: 0.10028451, Validation loss: 0.10072902, Gradient norm: 0.69174467
INFO:root:[   50] Training loss: 0.09981901, Validation loss: 0.10057116, Gradient norm: 0.69979592
INFO:root:[   51] Training loss: 0.09933675, Validation loss: 0.09897848, Gradient norm: 0.94175703
INFO:root:[   52] Training loss: 0.09872407, Validation loss: 0.09763013, Gradient norm: 0.93604672
INFO:root:[   53] Training loss: 0.09845537, Validation loss: 0.09933213, Gradient norm: 1.09492191
INFO:root:[   54] Training loss: 0.09773437, Validation loss: 0.09734060, Gradient norm: 0.95340160
INFO:root:[   55] Training loss: 0.09761680, Validation loss: 0.09755526, Gradient norm: 1.19976166
INFO:root:[   56] Training loss: 0.09646996, Validation loss: 0.09688811, Gradient norm: 0.53478563
INFO:root:[   57] Training loss: 0.09637527, Validation loss: 0.09543024, Gradient norm: 0.99797400
INFO:root:[   58] Training loss: 0.09538969, Validation loss: 0.09521697, Gradient norm: 0.71796452
INFO:root:[   59] Training loss: 0.09552246, Validation loss: 0.09428883, Gradient norm: 0.99644916
INFO:root:[   60] Training loss: 0.09476288, Validation loss: 0.09473921, Gradient norm: 0.69305512
INFO:root:[   61] Training loss: 0.09443380, Validation loss: 0.09604846, Gradient norm: 1.02816758
INFO:root:[   62] Training loss: 0.09402603, Validation loss: 0.09338781, Gradient norm: 0.93535193
INFO:root:[   63] Training loss: 0.09368574, Validation loss: 0.09383362, Gradient norm: 1.11571983
INFO:root:[   64] Training loss: 0.09312414, Validation loss: 0.09352988, Gradient norm: 1.09729107
INFO:root:[   65] Training loss: 0.09270710, Validation loss: 0.09208590, Gradient norm: 0.95261750
INFO:root:[   66] Training loss: 0.09264324, Validation loss: 0.09261686, Gradient norm: 1.05245772
INFO:root:[   67] Training loss: 0.09221064, Validation loss: 0.09222023, Gradient norm: 1.07122089
INFO:root:[   68] Training loss: 0.09134975, Validation loss: 0.09136613, Gradient norm: 0.63012171
INFO:root:[   69] Training loss: 0.09107567, Validation loss: 0.09118718, Gradient norm: 0.90694393
INFO:root:[   70] Training loss: 0.09062783, Validation loss: 0.09043940, Gradient norm: 0.74234087
INFO:root:[   71] Training loss: 0.09027187, Validation loss: 0.09095886, Gradient norm: 0.49500252
INFO:root:[   72] Training loss: 0.09020787, Validation loss: 0.09030272, Gradient norm: 1.08196458
INFO:root:[   73] Training loss: 0.08970782, Validation loss: 0.08988477, Gradient norm: 1.00358706
INFO:root:[   74] Training loss: 0.08957427, Validation loss: 0.08933774, Gradient norm: 1.16839819
INFO:root:[   75] Training loss: 0.08972080, Validation loss: 0.08858633, Gradient norm: 1.28062782
INFO:root:[   76] Training loss: 0.08872569, Validation loss: 0.08916201, Gradient norm: 1.00717347
INFO:root:[   77] Training loss: 0.08814712, Validation loss: 0.08896568, Gradient norm: 0.85223362
INFO:root:[   78] Training loss: 0.08796483, Validation loss: 0.08895913, Gradient norm: 0.77480236
INFO:root:[   79] Training loss: 0.08798902, Validation loss: 0.08745389, Gradient norm: 1.24794869
INFO:root:[   80] Training loss: 0.08767636, Validation loss: 0.08744960, Gradient norm: 1.30861773
INFO:root:[   81] Training loss: 0.08722554, Validation loss: 0.08882141, Gradient norm: 0.83649949
INFO:root:[   82] Training loss: 0.08736214, Validation loss: 0.08776801, Gradient norm: 1.28195684
INFO:root:[   83] Training loss: 0.08643121, Validation loss: 0.08632629, Gradient norm: 0.72327329
INFO:root:[   84] Training loss: 0.08609998, Validation loss: 0.08626940, Gradient norm: 0.73554973
INFO:root:[   85] Training loss: 0.08616317, Validation loss: 0.08608125, Gradient norm: 1.24697456
INFO:root:[   86] Training loss: 0.08591316, Validation loss: 0.08537453, Gradient norm: 1.35692488
INFO:root:[   87] Training loss: 0.08565777, Validation loss: 0.08497953, Gradient norm: 1.15750798
INFO:root:[   88] Training loss: 0.08556237, Validation loss: 0.08606195, Gradient norm: 1.45359917
INFO:root:[   89] Training loss: 0.08474538, Validation loss: 0.08505119, Gradient norm: 1.00673342
INFO:root:[   90] Training loss: 0.08441644, Validation loss: 0.08432663, Gradient norm: 0.88918671
INFO:root:[   91] Training loss: 0.08421261, Validation loss: 0.08435471, Gradient norm: 0.99226295
INFO:root:[   92] Training loss: 0.08397634, Validation loss: 0.08380992, Gradient norm: 1.14601599
INFO:root:[   93] Training loss: 0.08356796, Validation loss: 0.08384236, Gradient norm: 1.01041671
INFO:root:[   94] Training loss: 0.08316959, Validation loss: 0.08346452, Gradient norm: 0.81611780
INFO:root:[   95] Training loss: 0.08302333, Validation loss: 0.08276163, Gradient norm: 0.91004234
INFO:root:[   96] Training loss: 0.08295067, Validation loss: 0.08307689, Gradient norm: 1.32369059
INFO:root:[   97] Training loss: 0.08271231, Validation loss: 0.08218413, Gradient norm: 1.37380541
INFO:root:[   98] Training loss: 0.08236675, Validation loss: 0.08212715, Gradient norm: 1.16439847
INFO:root:[   99] Training loss: 0.08194737, Validation loss: 0.08260982, Gradient norm: 1.01773510
INFO:root:[  100] Training loss: 0.08167357, Validation loss: 0.08186585, Gradient norm: 1.06198477
INFO:root:[  101] Training loss: 0.08146476, Validation loss: 0.08125734, Gradient norm: 1.11964584
INFO:root:[  102] Training loss: 0.08132131, Validation loss: 0.08263935, Gradient norm: 1.16013712
INFO:root:[  103] Training loss: 0.08081304, Validation loss: 0.08111815, Gradient norm: 0.84816444
INFO:root:[  104] Training loss: 0.08030563, Validation loss: 0.08005890, Gradient norm: 0.45944821
INFO:root:[  105] Training loss: 0.08029423, Validation loss: 0.08038027, Gradient norm: 1.06339525
INFO:root:[  106] Training loss: 0.07988388, Validation loss: 0.07983595, Gradient norm: 0.72691196
INFO:root:[  107] Training loss: 0.07993364, Validation loss: 0.07940487, Gradient norm: 1.21578500
INFO:root:[  108] Training loss: 0.07970409, Validation loss: 0.07938121, Gradient norm: 1.29075808
INFO:root:[  109] Training loss: 0.07911000, Validation loss: 0.07881434, Gradient norm: 0.92040336
INFO:root:[  110] Training loss: 0.07889272, Validation loss: 0.07880453, Gradient norm: 0.97851643
INFO:root:[  111] Training loss: 0.07867008, Validation loss: 0.07872878, Gradient norm: 1.08849558
INFO:root:[  112] Training loss: 0.07842198, Validation loss: 0.07818696, Gradient norm: 0.82058254
INFO:root:[  113] Training loss: 0.07779085, Validation loss: 0.07803192, Gradient norm: 0.61419361
INFO:root:[  114] Training loss: 0.07782746, Validation loss: 0.07754439, Gradient norm: 1.04970503
INFO:root:[  115] Training loss: 0.07786937, Validation loss: 0.07849135, Gradient norm: 1.43868337
INFO:root:[  116] Training loss: 0.07759381, Validation loss: 0.07814784, Gradient norm: 1.47160269
INFO:root:[  117] Training loss: 0.07698464, Validation loss: 0.07665036, Gradient norm: 0.90110511
INFO:root:[  118] Training loss: 0.07659020, Validation loss: 0.07652244, Gradient norm: 0.50973129
INFO:root:[  119] Training loss: 0.07646768, Validation loss: 0.07721905, Gradient norm: 1.05224257
INFO:root:[  120] Training loss: 0.07631546, Validation loss: 0.07655590, Gradient norm: 1.12726299
INFO:root:[  121] Training loss: 0.07628596, Validation loss: 0.07600913, Gradient norm: 1.42898490
INFO:root:[  122] Training loss: 0.07578646, Validation loss: 0.07678472, Gradient norm: 1.23890026
INFO:root:[  123] Training loss: 0.07555645, Validation loss: 0.07673675, Gradient norm: 1.36894875
INFO:root:[  124] Training loss: 0.07528861, Validation loss: 0.07570455, Gradient norm: 1.21122693
INFO:root:[  125] Training loss: 0.07476961, Validation loss: 0.07502958, Gradient norm: 0.91445630
INFO:root:[  126] Training loss: 0.07483238, Validation loss: 0.07482537, Gradient norm: 1.24713022
INFO:root:[  127] Training loss: 0.07462296, Validation loss: 0.07438084, Gradient norm: 1.20495030
INFO:root:[  128] Training loss: 0.07459511, Validation loss: 0.07402851, Gradient norm: 1.48386869
INFO:root:[  129] Training loss: 0.07401580, Validation loss: 0.07452595, Gradient norm: 1.22966944
INFO:root:[  130] Training loss: 0.07359310, Validation loss: 0.07351643, Gradient norm: 1.03314899
INFO:root:[  131] Training loss: 0.07353550, Validation loss: 0.07297237, Gradient norm: 1.05312655
INFO:root:[  132] Training loss: 0.07328272, Validation loss: 0.07332511, Gradient norm: 1.12634650
INFO:root:[  133] Training loss: 0.07302163, Validation loss: 0.07248171, Gradient norm: 1.22042727
INFO:root:[  134] Training loss: 0.07256294, Validation loss: 0.07272121, Gradient norm: 0.91819686
INFO:root:[  135] Training loss: 0.07244254, Validation loss: 0.07300313, Gradient norm: 1.19944427
INFO:root:[  136] Training loss: 0.07238722, Validation loss: 0.07250498, Gradient norm: 1.19883448
INFO:root:[  137] Training loss: 0.07205851, Validation loss: 0.07172181, Gradient norm: 1.07385167
INFO:root:[  138] Training loss: 0.07165965, Validation loss: 0.07219437, Gradient norm: 1.09070782
INFO:root:[  139] Training loss: 0.07139733, Validation loss: 0.07157378, Gradient norm: 1.16508257
INFO:root:[  140] Training loss: 0.07132930, Validation loss: 0.07146149, Gradient norm: 1.24949978
INFO:root:[  141] Training loss: 0.07091603, Validation loss: 0.07074128, Gradient norm: 1.18749675
INFO:root:[  142] Training loss: 0.07082687, Validation loss: 0.07070750, Gradient norm: 1.26486249
INFO:root:[  143] Training loss: 0.07058862, Validation loss: 0.07101921, Gradient norm: 1.26609737
INFO:root:[  144] Training loss: 0.07068444, Validation loss: 0.07107517, Gradient norm: 1.68937156
INFO:root:[  145] Training loss: 0.07022761, Validation loss: 0.07046432, Gradient norm: 1.26618658
INFO:root:[  146] Training loss: 0.06999281, Validation loss: 0.06961416, Gradient norm: 1.28977546
INFO:root:[  147] Training loss: 0.06973573, Validation loss: 0.06957129, Gradient norm: 1.36854902
INFO:root:[  148] Training loss: 0.06960182, Validation loss: 0.07013253, Gradient norm: 1.46845070
INFO:root:[  149] Training loss: 0.06923081, Validation loss: 0.06890039, Gradient norm: 1.18973018
INFO:root:[  150] Training loss: 0.06909312, Validation loss: 0.06965359, Gradient norm: 1.30625053
INFO:root:[  151] Training loss: 0.06911014, Validation loss: 0.06923692, Gradient norm: 1.65099076
INFO:root:[  152] Training loss: 0.06860932, Validation loss: 0.07000865, Gradient norm: 1.12780118
INFO:root:[  153] Training loss: 0.06844024, Validation loss: 0.06903179, Gradient norm: 1.30503245
INFO:root:[  154] Training loss: 0.06821554, Validation loss: 0.06841741, Gradient norm: 1.13207879
INFO:root:[  155] Training loss: 0.06775251, Validation loss: 0.06778310, Gradient norm: 0.77715943
INFO:root:[  156] Training loss: 0.06793033, Validation loss: 0.06808093, Gradient norm: 1.43220601
INFO:root:[  157] Training loss: 0.06761441, Validation loss: 0.06690105, Gradient norm: 1.25109372
INFO:root:[  158] Training loss: 0.06729554, Validation loss: 0.06737345, Gradient norm: 1.15367623
INFO:root:[  159] Training loss: 0.06735444, Validation loss: 0.06741253, Gradient norm: 1.52808882
INFO:root:[  160] Training loss: 0.06706733, Validation loss: 0.06660382, Gradient norm: 1.43517049
INFO:root:[  161] Training loss: 0.06677352, Validation loss: 0.06678318, Gradient norm: 1.08757287
INFO:root:[  162] Training loss: 0.06684932, Validation loss: 0.06668454, Gradient norm: 1.57900172
INFO:root:[  163] Training loss: 0.06655456, Validation loss: 0.06587452, Gradient norm: 1.59003193
INFO:root:[  164] Training loss: 0.06619319, Validation loss: 0.06629350, Gradient norm: 1.14420508
INFO:root:[  165] Training loss: 0.06609713, Validation loss: 0.06679990, Gradient norm: 1.52065917
INFO:root:[  166] Training loss: 0.06601695, Validation loss: 0.06564936, Gradient norm: 1.41404413
INFO:root:[  167] Training loss: 0.06566136, Validation loss: 0.06539847, Gradient norm: 1.25668462
INFO:root:[  168] Training loss: 0.06537719, Validation loss: 0.06495432, Gradient norm: 1.16884545
INFO:root:[  169] Training loss: 0.06517404, Validation loss: 0.06512392, Gradient norm: 0.82258273
INFO:root:[  170] Training loss: 0.06497983, Validation loss: 0.06472607, Gradient norm: 0.94600835
INFO:root:[  171] Training loss: 0.06498262, Validation loss: 0.06561773, Gradient norm: 1.46878639
INFO:root:[  172] Training loss: 0.06504991, Validation loss: 0.06486296, Gradient norm: 1.83247894
INFO:root:[  173] Training loss: 0.06448108, Validation loss: 0.06470621, Gradient norm: 1.17103591
INFO:root:[  174] Training loss: 0.06427941, Validation loss: 0.06433556, Gradient norm: 1.13642617
INFO:root:[  175] Training loss: 0.06452354, Validation loss: 0.06393120, Gradient norm: 1.79438554
INFO:root:[  176] Training loss: 0.06388180, Validation loss: 0.06370263, Gradient norm: 1.07731937
INFO:root:[  177] Training loss: 0.06376118, Validation loss: 0.06456603, Gradient norm: 1.09790958
INFO:root:[  178] Training loss: 0.06388289, Validation loss: 0.06410820, Gradient norm: 1.67446278
INFO:root:[  179] Training loss: 0.06348141, Validation loss: 0.06419806, Gradient norm: 1.21596171
INFO:root:[  180] Training loss: 0.06323925, Validation loss: 0.06325522, Gradient norm: 1.20485656
INFO:root:[  181] Training loss: 0.06318167, Validation loss: 0.06318609, Gradient norm: 1.21714516
INFO:root:[  182] Training loss: 0.06302922, Validation loss: 0.06341477, Gradient norm: 1.50307448
INFO:root:[  183] Training loss: 0.06310235, Validation loss: 0.06303768, Gradient norm: 1.91775946
INFO:root:[  184] Training loss: 0.06280120, Validation loss: 0.06287415, Gradient norm: 1.61422197
INFO:root:[  185] Training loss: 0.06251203, Validation loss: 0.06274419, Gradient norm: 1.28842504
INFO:root:[  186] Training loss: 0.06253604, Validation loss: 0.06280566, Gradient norm: 1.59721917
INFO:root:[  187] Training loss: 0.06240270, Validation loss: 0.06259585, Gradient norm: 1.77334455
INFO:root:[  188] Training loss: 0.06218527, Validation loss: 0.06185691, Gradient norm: 1.57132128
INFO:root:[  189] Training loss: 0.06181374, Validation loss: 0.06204920, Gradient norm: 1.22670710
INFO:root:[  190] Training loss: 0.06175011, Validation loss: 0.06145323, Gradient norm: 1.34616051
INFO:root:[  191] Training loss: 0.06157719, Validation loss: 0.06160886, Gradient norm: 1.21919317
INFO:root:[  192] Training loss: 0.06175685, Validation loss: 0.06120840, Gradient norm: 1.89034724
INFO:root:[  193] Training loss: 0.06150786, Validation loss: 0.06188526, Gradient norm: 1.73927974
INFO:root:[  194] Training loss: 0.06120350, Validation loss: 0.06095006, Gradient norm: 1.49781040
INFO:root:[  195] Training loss: 0.06115902, Validation loss: 0.06128658, Gradient norm: 1.67214600
INFO:root:[  196] Training loss: 0.06087220, Validation loss: 0.06064664, Gradient norm: 1.27252402
INFO:root:[  197] Training loss: 0.06067879, Validation loss: 0.06051366, Gradient norm: 1.11938410
INFO:root:[  198] Training loss: 0.06054935, Validation loss: 0.06114852, Gradient norm: 1.44089417
INFO:root:[  199] Training loss: 0.06064767, Validation loss: 0.06157286, Gradient norm: 1.74715820
INFO:root:[  200] Training loss: 0.06039307, Validation loss: 0.06053240, Gradient norm: 1.52503209
INFO:root:[  201] Training loss: 0.06047613, Validation loss: 0.05992337, Gradient norm: 1.89962803
INFO:root:[  202] Training loss: 0.05999468, Validation loss: 0.05976005, Gradient norm: 1.33572328
INFO:root:[  203] Training loss: 0.05964218, Validation loss: 0.06013249, Gradient norm: 0.95595959
INFO:root:[  204] Training loss: 0.05960319, Validation loss: 0.05913081, Gradient norm: 1.07022912
INFO:root:[  205] Training loss: 0.05961759, Validation loss: 0.05947411, Gradient norm: 1.51337651
INFO:root:[  206] Training loss: 0.05941289, Validation loss: 0.06004364, Gradient norm: 1.55400579
INFO:root:[  207] Training loss: 0.05924123, Validation loss: 0.05915179, Gradient norm: 1.43864823
INFO:root:[  208] Training loss: 0.05948012, Validation loss: 0.05937725, Gradient norm: 2.03634768
INFO:root:[  209] Training loss: 0.05918681, Validation loss: 0.05983433, Gradient norm: 1.65564695
INFO:root:[  210] Training loss: 0.05892102, Validation loss: 0.05930985, Gradient norm: 1.55501823
INFO:root:[  211] Training loss: 0.05878091, Validation loss: 0.05836788, Gradient norm: 1.49831298
INFO:root:[  212] Training loss: 0.05836867, Validation loss: 0.05858942, Gradient norm: 0.87589898
INFO:root:[  213] Training loss: 0.05845256, Validation loss: 0.05932058, Gradient norm: 1.33899270
INFO:root:[  214] Training loss: 0.05832364, Validation loss: 0.05915578, Gradient norm: 1.46552615
INFO:root:[  215] Training loss: 0.05836355, Validation loss: 0.05814108, Gradient norm: 1.78446058
INFO:root:[  216] Training loss: 0.05799095, Validation loss: 0.05757985, Gradient norm: 1.28724835
INFO:root:[  217] Training loss: 0.05783349, Validation loss: 0.05894388, Gradient norm: 1.21869851
INFO:root:[  218] Training loss: 0.05803296, Validation loss: 0.05745727, Gradient norm: 1.83844545
INFO:root:[  219] Training loss: 0.05777975, Validation loss: 0.05803700, Gradient norm: 1.71468776
INFO:root:[  220] Training loss: 0.05769933, Validation loss: 0.05710826, Gradient norm: 1.83269193
INFO:root:[  221] Training loss: 0.05724603, Validation loss: 0.05711162, Gradient norm: 1.28376976
INFO:root:[  222] Training loss: 0.05737144, Validation loss: 0.05711034, Gradient norm: 1.60306473
INFO:root:[  223] Training loss: 0.05745064, Validation loss: 0.05688428, Gradient norm: 2.04913628
INFO:root:[  224] Training loss: 0.05711427, Validation loss: 0.05786334, Gradient norm: 1.70921419
INFO:root:[  225] Training loss: 0.05711128, Validation loss: 0.05705930, Gradient norm: 1.99146702
INFO:root:[  226] Training loss: 0.05675319, Validation loss: 0.05671195, Gradient norm: 1.45137405
INFO:root:[  227] Training loss: 0.05648144, Validation loss: 0.05678895, Gradient norm: 1.29593608
INFO:root:[  228] Training loss: 0.05643025, Validation loss: 0.05663332, Gradient norm: 1.43020481
INFO:root:[  229] Training loss: 0.05646281, Validation loss: 0.05669019, Gradient norm: 1.54077691
INFO:root:[  230] Training loss: 0.05620275, Validation loss: 0.05695296, Gradient norm: 1.36687432
INFO:root:[  231] Training loss: 0.05607600, Validation loss: 0.05609200, Gradient norm: 1.43513820
INFO:root:[  232] Training loss: 0.05604355, Validation loss: 0.05651552, Gradient norm: 1.66782936
INFO:root:[  233] Training loss: 0.05597815, Validation loss: 0.05613363, Gradient norm: 1.78748344
INFO:root:[  234] Training loss: 0.05597413, Validation loss: 0.05547703, Gradient norm: 2.00325726
INFO:root:[  235] Training loss: 0.05561844, Validation loss: 0.05514171, Gradient norm: 1.52390010
INFO:root:[  236] Training loss: 0.05546750, Validation loss: 0.05548982, Gradient norm: 1.22609620
INFO:root:[  237] Training loss: 0.05527977, Validation loss: 0.05568482, Gradient norm: 1.39372273
INFO:root:[  238] Training loss: 0.05541168, Validation loss: 0.05510677, Gradient norm: 1.86921996
INFO:root:[  239] Training loss: 0.05534199, Validation loss: 0.05492354, Gradient norm: 1.97548412
INFO:root:[  240] Training loss: 0.05498876, Validation loss: 0.05529014, Gradient norm: 1.52492590
INFO:root:[  241] Training loss: 0.05523232, Validation loss: 0.05487163, Gradient norm: 2.08008673
INFO:root:[  242] Training loss: 0.05476597, Validation loss: 0.05503623, Gradient norm: 1.57645968
INFO:root:[  243] Training loss: 0.05482879, Validation loss: 0.05504809, Gradient norm: 1.88153556
INFO:root:[  244] Training loss: 0.05479840, Validation loss: 0.05426211, Gradient norm: 2.00624115
INFO:root:[  245] Training loss: 0.05463844, Validation loss: 0.05450145, Gradient norm: 2.00419102
INFO:root:[  246] Training loss: 0.05454003, Validation loss: 0.05421448, Gradient norm: 1.95110167
INFO:root:[  247] Training loss: 0.05430930, Validation loss: 0.05429170, Gradient norm: 1.65722649
INFO:root:[  248] Training loss: 0.05403241, Validation loss: 0.05455323, Gradient norm: 1.36452419
INFO:root:[  249] Training loss: 0.05415142, Validation loss: 0.05389563, Gradient norm: 1.86094398
INFO:root:[  250] Training loss: 0.05373345, Validation loss: 0.05391515, Gradient norm: 1.24118757
INFO:root:[  251] Training loss: 0.05386218, Validation loss: 0.05427762, Gradient norm: 1.70256540
INFO:root:[  252] Training loss: 0.05368230, Validation loss: 0.05343564, Gradient norm: 1.58509392
INFO:root:[  253] Training loss: 0.05339631, Validation loss: 0.05402484, Gradient norm: 1.22422003
INFO:root:[  254] Training loss: 0.05350137, Validation loss: 0.05374086, Gradient norm: 1.70587016
INFO:root:[  255] Training loss: 0.05326492, Validation loss: 0.05304274, Gradient norm: 1.58302352
INFO:root:[  256] Training loss: 0.05323480, Validation loss: 0.05393030, Gradient norm: 1.83949893
INFO:root:[  257] Training loss: 0.05310565, Validation loss: 0.05327371, Gradient norm: 1.65912096
INFO:root:[  258] Training loss: 0.05302150, Validation loss: 0.05323766, Gradient norm: 1.60103087
INFO:root:[  259] Training loss: 0.05290307, Validation loss: 0.05271044, Gradient norm: 1.67887796
INFO:root:[  260] Training loss: 0.05271603, Validation loss: 0.05261055, Gradient norm: 1.54541867
INFO:root:[  261] Training loss: 0.05255801, Validation loss: 0.05253265, Gradient norm: 1.38988250
INFO:root:[  262] Training loss: 0.05243387, Validation loss: 0.05281301, Gradient norm: 1.35669477
INFO:root:[  263] Training loss: 0.05249561, Validation loss: 0.05237130, Gradient norm: 1.85479540
INFO:root:[  264] Training loss: 0.05227843, Validation loss: 0.05268467, Gradient norm: 1.46657981
INFO:root:[  265] Training loss: 0.05242556, Validation loss: 0.05201756, Gradient norm: 1.94917581
INFO:root:[  266] Training loss: 0.05217559, Validation loss: 0.05197250, Gradient norm: 1.69086570
INFO:root:[  267] Training loss: 0.05188196, Validation loss: 0.05192234, Gradient norm: 1.56323448
INFO:root:[  268] Training loss: 0.05184238, Validation loss: 0.05139225, Gradient norm: 1.68933566
INFO:root:[  269] Training loss: 0.05174747, Validation loss: 0.05182939, Gradient norm: 1.73892814
INFO:root:[  270] Training loss: 0.05167225, Validation loss: 0.05182776, Gradient norm: 1.71120003
INFO:root:[  271] Training loss: 0.05156292, Validation loss: 0.05219634, Gradient norm: 1.81514053
INFO:root:[  272] Training loss: 0.05153418, Validation loss: 0.05127975, Gradient norm: 1.85673778
INFO:root:[  273] Training loss: 0.05138258, Validation loss: 0.05110950, Gradient norm: 1.80501023
INFO:root:[  274] Training loss: 0.05096415, Validation loss: 0.05087653, Gradient norm: 0.92818490
INFO:root:[  275] Training loss: 0.05105966, Validation loss: 0.05101436, Gradient norm: 1.74253921
INFO:root:[  276] Training loss: 0.05082620, Validation loss: 0.05139764, Gradient norm: 1.47424262
INFO:root:[  277] Training loss: 0.05090944, Validation loss: 0.05108823, Gradient norm: 1.82487144
INFO:root:[  278] Training loss: 0.05084863, Validation loss: 0.05042782, Gradient norm: 1.81574318
INFO:root:[  279] Training loss: 0.05064856, Validation loss: 0.05044321, Gradient norm: 1.76458842
INFO:root:[  280] Training loss: 0.05047267, Validation loss: 0.05042231, Gradient norm: 1.44517564
INFO:root:[  281] Training loss: 0.05031426, Validation loss: 0.05022758, Gradient norm: 1.64108942
INFO:root:[  282] Training loss: 0.05037385, Validation loss: 0.05034726, Gradient norm: 1.90436225
INFO:root:[  283] Training loss: 0.05017284, Validation loss: 0.05093719, Gradient norm: 1.73272131
INFO:root:[  284] Training loss: 0.05017028, Validation loss: 0.05020616, Gradient norm: 2.00731844
INFO:root:[  285] Training loss: 0.05002972, Validation loss: 0.05016295, Gradient norm: 1.80008528
INFO:root:[  286] Training loss: 0.04987056, Validation loss: 0.04963891, Gradient norm: 1.72816082
INFO:root:[  287] Training loss: 0.04980255, Validation loss: 0.04952781, Gradient norm: 1.89941066
INFO:root:[  288] Training loss: 0.04963885, Validation loss: 0.04983396, Gradient norm: 1.66828120
INFO:root:[  289] Training loss: 0.04948070, Validation loss: 0.04926230, Gradient norm: 1.56357031
INFO:root:[  290] Training loss: 0.04945257, Validation loss: 0.04932649, Gradient norm: 1.72404219
INFO:root:[  291] Training loss: 0.04934636, Validation loss: 0.04938103, Gradient norm: 1.82094585
INFO:root:[  292] Training loss: 0.04920136, Validation loss: 0.04949392, Gradient norm: 1.58108405
INFO:root:[  293] Training loss: 0.04906496, Validation loss: 0.04922393, Gradient norm: 1.69693563
INFO:root:[  294] Training loss: 0.04910467, Validation loss: 0.04885314, Gradient norm: 1.90267560
INFO:root:[  295] Training loss: 0.04912375, Validation loss: 0.04872035, Gradient norm: 2.12979055
INFO:root:[  296] Training loss: 0.04900559, Validation loss: 0.04934445, Gradient norm: 1.97384843
INFO:root:[  297] Training loss: 0.04861427, Validation loss: 0.04844714, Gradient norm: 1.48661822
INFO:root:[  298] Training loss: 0.04870107, Validation loss: 0.04861420, Gradient norm: 1.94754293
INFO:root:[  299] Training loss: 0.04848176, Validation loss: 0.04821032, Gradient norm: 1.58840551
INFO:root:[  300] Training loss: 0.04840529, Validation loss: 0.04819241, Gradient norm: 1.93444791
INFO:root:[  301] Training loss: 0.04830822, Validation loss: 0.04816894, Gradient norm: 1.66842777
INFO:root:[  302] Training loss: 0.04836358, Validation loss: 0.04875671, Gradient norm: 2.14313619
INFO:root:[  303] Training loss: 0.04812615, Validation loss: 0.04820322, Gradient norm: 1.59868077
INFO:root:[  304] Training loss: 0.04811421, Validation loss: 0.04834761, Gradient norm: 2.04903804
INFO:root:[  305] Training loss: 0.04793946, Validation loss: 0.04871314, Gradient norm: 1.81878450
INFO:root:[  306] Training loss: 0.04782230, Validation loss: 0.04744067, Gradient norm: 1.78181229
INFO:root:[  307] Training loss: 0.04762838, Validation loss: 0.04764677, Gradient norm: 1.64617900
INFO:root:[  308] Training loss: 0.04769234, Validation loss: 0.04741156, Gradient norm: 1.88285306
INFO:root:[  309] Training loss: 0.04737109, Validation loss: 0.04785215, Gradient norm: 1.40635915
INFO:root:[  310] Training loss: 0.04741349, Validation loss: 0.04777480, Gradient norm: 1.86184284
INFO:root:[  311] Training loss: 0.04738041, Validation loss: 0.04707327, Gradient norm: 2.01736413
INFO:root:[  312] Training loss: 0.04721744, Validation loss: 0.04784657, Gradient norm: 1.75002550
INFO:root:[  313] Training loss: 0.04703790, Validation loss: 0.04727442, Gradient norm: 1.66711501
INFO:root:[  314] Training loss: 0.04684606, Validation loss: 0.04680208, Gradient norm: 1.32521339
INFO:root:[  315] Training loss: 0.04686096, Validation loss: 0.04651575, Gradient norm: 1.73658210
INFO:root:[  316] Training loss: 0.04677267, Validation loss: 0.04666007, Gradient norm: 1.45178796
INFO:root:[  317] Training loss: 0.04660350, Validation loss: 0.04704624, Gradient norm: 1.65848313
INFO:root:[  318] Training loss: 0.04662169, Validation loss: 0.04709179, Gradient norm: 1.75728788
INFO:root:[  319] Training loss: 0.04669470, Validation loss: 0.04630959, Gradient norm: 2.22760830
INFO:root:[  320] Training loss: 0.04640856, Validation loss: 0.04603063, Gradient norm: 1.86176860
INFO:root:[  321] Training loss: 0.04627395, Validation loss: 0.04649546, Gradient norm: 1.93932378
INFO:root:[  322] Training loss: 0.04635737, Validation loss: 0.04625845, Gradient norm: 2.19992297
INFO:root:[  323] Training loss: 0.04606960, Validation loss: 0.04707978, Gradient norm: 1.73300153
INFO:root:[  324] Training loss: 0.04597782, Validation loss: 0.04578194, Gradient norm: 1.74161237
INFO:root:[  325] Training loss: 0.04585010, Validation loss: 0.04551505, Gradient norm: 1.70325201
INFO:root:[  326] Training loss: 0.04581169, Validation loss: 0.04558558, Gradient norm: 1.87090571
INFO:root:[  327] Training loss: 0.04564889, Validation loss: 0.04573375, Gradient norm: 1.58329654
INFO:root:[  328] Training loss: 0.04570070, Validation loss: 0.04591936, Gradient norm: 1.96066974
INFO:root:[  329] Training loss: 0.04560804, Validation loss: 0.04604006, Gradient norm: 1.97167594
INFO:root:[  330] Training loss: 0.04543278, Validation loss: 0.04516144, Gradient norm: 2.07118928
INFO:root:[  331] Training loss: 0.04535956, Validation loss: 0.04567111, Gradient norm: 2.06387987
INFO:root:[  332] Training loss: 0.04539780, Validation loss: 0.04501127, Gradient norm: 2.41553949
INFO:root:[  333] Training loss: 0.04513406, Validation loss: 0.04572427, Gradient norm: 1.81309643
INFO:root:[  334] Training loss: 0.04514951, Validation loss: 0.04575998, Gradient norm: 2.02457942
INFO:root:[  335] Training loss: 0.04496500, Validation loss: 0.04472772, Gradient norm: 2.03046801
INFO:root:[  336] Training loss: 0.04488193, Validation loss: 0.04537118, Gradient norm: 2.00190795
INFO:root:[  337] Training loss: 0.04477893, Validation loss: 0.04490506, Gradient norm: 1.77715092
INFO:root:[  338] Training loss: 0.04458033, Validation loss: 0.04466622, Gradient norm: 1.76496535
INFO:root:[  339] Training loss: 0.04453689, Validation loss: 0.04460747, Gradient norm: 1.79641506
INFO:root:[  340] Training loss: 0.04466649, Validation loss: 0.04476973, Gradient norm: 2.18909393
INFO:root:[  341] Training loss: 0.04444530, Validation loss: 0.04472959, Gradient norm: 2.05733922
INFO:root:[  342] Training loss: 0.04445057, Validation loss: 0.04418605, Gradient norm: 2.12595360
INFO:root:[  343] Training loss: 0.04420602, Validation loss: 0.04405742, Gradient norm: 1.98231619
INFO:root:[  344] Training loss: 0.04420138, Validation loss: 0.04394997, Gradient norm: 2.02206958
INFO:root:[  345] Training loss: 0.04409129, Validation loss: 0.04387011, Gradient norm: 1.84418363
INFO:root:[  346] Training loss: 0.04402547, Validation loss: 0.04385428, Gradient norm: 1.96799066
INFO:root:[  347] Training loss: 0.04397273, Validation loss: 0.04377540, Gradient norm: 2.12935294
INFO:root:[  348] Training loss: 0.04386444, Validation loss: 0.04390551, Gradient norm: 2.16407273
INFO:root:[  349] Training loss: 0.04371189, Validation loss: 0.04336670, Gradient norm: 1.81329351
INFO:root:[  350] Training loss: 0.04352596, Validation loss: 0.04341686, Gradient norm: 1.73808280
INFO:root:[  351] Training loss: 0.04365194, Validation loss: 0.04320688, Gradient norm: 2.07142715
INFO:root:[  352] Training loss: 0.04348057, Validation loss: 0.04383006, Gradient norm: 1.84435950
INFO:root:[  353] Training loss: 0.04349688, Validation loss: 0.04416915, Gradient norm: 2.22752092
INFO:root:[  354] Training loss: 0.04323363, Validation loss: 0.04330910, Gradient norm: 1.65779178
INFO:root:[  355] Training loss: 0.04326354, Validation loss: 0.04291616, Gradient norm: 2.16013810
INFO:root:[  356] Training loss: 0.04300259, Validation loss: 0.04313395, Gradient norm: 1.53103218
INFO:root:[  357] Training loss: 0.04297319, Validation loss: 0.04295655, Gradient norm: 1.73457971
INFO:root:[  358] Training loss: 0.04289908, Validation loss: 0.04257260, Gradient norm: 1.96761607
INFO:root:[  359] Training loss: 0.04292218, Validation loss: 0.04274593, Gradient norm: 2.09722163
INFO:root:[  360] Training loss: 0.04277173, Validation loss: 0.04316428, Gradient norm: 2.36166073
INFO:root:[  361] Training loss: 0.04267466, Validation loss: 0.04257381, Gradient norm: 1.87525235
INFO:root:[  362] Training loss: 0.04265395, Validation loss: 0.04254617, Gradient norm: 2.11667279
INFO:root:[  363] Training loss: 0.04245708, Validation loss: 0.04242336, Gradient norm: 1.88476270
INFO:root:[  364] Training loss: 0.04231394, Validation loss: 0.04262200, Gradient norm: 1.29514655
INFO:root:[  365] Training loss: 0.04237148, Validation loss: 0.04199019, Gradient norm: 2.07786747
INFO:root:[  366] Training loss: 0.04245623, Validation loss: 0.04173859, Gradient norm: 2.73462099
INFO:root:[  367] Training loss: 0.04219201, Validation loss: 0.04239872, Gradient norm: 2.19059338
INFO:root:[  368] Training loss: 0.04210878, Validation loss: 0.04189169, Gradient norm: 1.95877291
INFO:root:[  369] Training loss: 0.04195122, Validation loss: 0.04209361, Gradient norm: 1.89747582
INFO:root:[  370] Training loss: 0.04208366, Validation loss: 0.04188202, Gradient norm: 2.36632429
INFO:root:[  371] Training loss: 0.04178879, Validation loss: 0.04170497, Gradient norm: 1.91708669
INFO:root:[  372] Training loss: 0.04173162, Validation loss: 0.04235743, Gradient norm: 1.98764099
INFO:root:[  373] Training loss: 0.04186966, Validation loss: 0.04209153, Gradient norm: 2.47722799
INFO:root:[  374] Training loss: 0.04165644, Validation loss: 0.04141627, Gradient norm: 2.05805384
INFO:root:[  375] Training loss: 0.04148589, Validation loss: 0.04132502, Gradient norm: 1.75167896
INFO:root:[  376] Training loss: 0.04137680, Validation loss: 0.04096156, Gradient norm: 2.08175167
INFO:root:[  377] Training loss: 0.04126084, Validation loss: 0.04162477, Gradient norm: 2.21925053
INFO:root:[  378] Training loss: 0.04127499, Validation loss: 0.04096047, Gradient norm: 2.22531666
INFO:root:[  379] Training loss: 0.04111694, Validation loss: 0.04182901, Gradient norm: 2.25053242
INFO:root:[  380] Training loss: 0.04110479, Validation loss: 0.04077880, Gradient norm: 2.36600282
INFO:root:[  381] Training loss: 0.04088710, Validation loss: 0.04153532, Gradient norm: 1.80345297
INFO:root:[  382] Training loss: 0.04077201, Validation loss: 0.04110861, Gradient norm: 1.60998251
INFO:root:[  383] Training loss: 0.04068810, Validation loss: 0.04084645, Gradient norm: 1.81636132
INFO:root:[  384] Training loss: 0.04065876, Validation loss: 0.04056534, Gradient norm: 1.57928080
INFO:root:[  385] Training loss: 0.04068898, Validation loss: 0.04032356, Gradient norm: 2.37565254
INFO:root:[  386] Training loss: 0.04056499, Validation loss: 0.04144423, Gradient norm: 2.07393074
INFO:root:[  387] Training loss: 0.04058917, Validation loss: 0.04032428, Gradient norm: 2.63112592
INFO:root:[  388] Training loss: 0.04043033, Validation loss: 0.04089783, Gradient norm: 2.25004747
INFO:root:[  389] Training loss: 0.04037011, Validation loss: 0.04017006, Gradient norm: 2.30498710
INFO:root:[  390] Training loss: 0.04031259, Validation loss: 0.04032680, Gradient norm: 2.29858367
INFO:root:[  391] Training loss: 0.04010968, Validation loss: 0.04010410, Gradient norm: 1.80945762
INFO:root:[  392] Training loss: 0.04023124, Validation loss: 0.03989619, Gradient norm: 2.58453390
INFO:root:[  393] Training loss: 0.04011844, Validation loss: 0.04042926, Gradient norm: 2.57616793
INFO:root:[  394] Training loss: 0.04005299, Validation loss: 0.03988773, Gradient norm: 2.72959889
INFO:root:[  395] Training loss: 0.03989843, Validation loss: 0.04053060, Gradient norm: 2.48936022
INFO:root:[  396] Training loss: 0.03992310, Validation loss: 0.04005736, Gradient norm: 2.52752499
INFO:root:[  397] Training loss: 0.03975684, Validation loss: 0.04007887, Gradient norm: 2.17006464
INFO:root:[  398] Training loss: 0.03967320, Validation loss: 0.03923474, Gradient norm: 2.19742638
INFO:root:[  399] Training loss: 0.03967565, Validation loss: 0.03955768, Gradient norm: 2.25060261
INFO:root:[  400] Training loss: 0.03951213, Validation loss: 0.03987167, Gradient norm: 1.88227214
INFO:root:[  401] Training loss: 0.03942373, Validation loss: 0.03942115, Gradient norm: 2.37712929
INFO:root:[  402] Training loss: 0.03953019, Validation loss: 0.03961258, Gradient norm: 2.47033003
INFO:root:[  403] Training loss: 0.03931753, Validation loss: 0.03965001, Gradient norm: 2.18920761
INFO:root:[  404] Training loss: 0.03917112, Validation loss: 0.03889004, Gradient norm: 2.04962763
INFO:root:[  405] Training loss: 0.03927675, Validation loss: 0.03939949, Gradient norm: 2.70740878
INFO:root:[  406] Training loss: 0.03904911, Validation loss: 0.03903249, Gradient norm: 2.60084892
INFO:root:[  407] Training loss: 0.03902969, Validation loss: 0.03889775, Gradient norm: 2.27470564
INFO:root:[  408] Training loss: 0.03897435, Validation loss: 0.03883435, Gradient norm: 2.19129493
INFO:root:[  409] Training loss: 0.03873916, Validation loss: 0.03905409, Gradient norm: 1.73144926
INFO:root:[  410] Training loss: 0.03895869, Validation loss: 0.03836316, Gradient norm: 2.50842908
INFO:root:[  411] Training loss: 0.03877858, Validation loss: 0.03866992, Gradient norm: 2.55214261
INFO:root:[  412] Training loss: 0.03855253, Validation loss: 0.03827561, Gradient norm: 1.84745417
INFO:root:[  413] Training loss: 0.03839818, Validation loss: 0.03810158, Gradient norm: 1.78609892
INFO:root:[  414] Training loss: 0.03854489, Validation loss: 0.03866238, Gradient norm: 2.16777889
INFO:root:[  415] Training loss: 0.03838782, Validation loss: 0.03872359, Gradient norm: 2.09913516
INFO:root:[  416] Training loss: 0.03831285, Validation loss: 0.03807449, Gradient norm: 2.18335697
INFO:root:[  417] Training loss: 0.03827760, Validation loss: 0.03856859, Gradient norm: 2.12137119
INFO:root:[  418] Training loss: 0.03817644, Validation loss: 0.03833827, Gradient norm: 1.93512140
INFO:root:[  419] Training loss: 0.03800358, Validation loss: 0.03782639, Gradient norm: 1.83853310
INFO:root:[  420] Training loss: 0.03789073, Validation loss: 0.03751759, Gradient norm: 1.67490175
INFO:root:[  421] Training loss: 0.03789683, Validation loss: 0.03820978, Gradient norm: 2.40060305
INFO:root:[  422] Training loss: 0.03789410, Validation loss: 0.03802702, Gradient norm: 2.38394991
INFO:root:[  423] Training loss: 0.03781460, Validation loss: 0.03762158, Gradient norm: 2.44018154
INFO:root:[  424] Training loss: 0.03767179, Validation loss: 0.03786217, Gradient norm: 2.56431876
INFO:root:[  425] Training loss: 0.03758474, Validation loss: 0.03771986, Gradient norm: 2.45538423
INFO:root:[  426] Training loss: 0.03747307, Validation loss: 0.03738379, Gradient norm: 2.10390653
INFO:root:[  427] Training loss: 0.03757870, Validation loss: 0.03796810, Gradient norm: 2.38623742
INFO:root:[  428] Training loss: 0.03733371, Validation loss: 0.03769281, Gradient norm: 2.22027986
INFO:root:[  429] Training loss: 0.03753542, Validation loss: 0.03704618, Gradient norm: 2.49941901
INFO:root:[  430] Training loss: 0.03721740, Validation loss: 0.03763287, Gradient norm: 2.53599061
INFO:root:[  431] Training loss: 0.03739056, Validation loss: 0.03779207, Gradient norm: 3.29850476
INFO:root:[  432] Training loss: 0.03716991, Validation loss: 0.03695823, Gradient norm: 2.67808663
INFO:root:[  433] Training loss: 0.03721028, Validation loss: 0.03706983, Gradient norm: 2.48301703
INFO:root:[  434] Training loss: 0.03697593, Validation loss: 0.03745106, Gradient norm: 2.15630277
INFO:root:[  435] Training loss: 0.03706447, Validation loss: 0.03671843, Gradient norm: 2.91051832
INFO:root:[  436] Training loss: 0.03684681, Validation loss: 0.03665832, Gradient norm: 1.84633277
INFO:root:[  437] Training loss: 0.03671587, Validation loss: 0.03712988, Gradient norm: 1.86719792
INFO:root:[  438] Training loss: 0.03672997, Validation loss: 0.03624550, Gradient norm: 1.73350188
INFO:root:[  439] Training loss: 0.03674423, Validation loss: 0.03678652, Gradient norm: 2.66487389
INFO:root:[  440] Training loss: 0.03658514, Validation loss: 0.03697112, Gradient norm: 2.71684380
INFO:root:[  441] Training loss: 0.03652726, Validation loss: 0.03661080, Gradient norm: 2.27944892
INFO:root:[  442] Training loss: 0.03657876, Validation loss: 0.03690295, Gradient norm: 3.20635489
INFO:root:[  443] Training loss: 0.03648906, Validation loss: 0.03661455, Gradient norm: 3.14233523
INFO:root:[  444] Training loss: 0.03643672, Validation loss: 0.03623106, Gradient norm: 3.14921318
INFO:root:[  445] Training loss: 0.03615752, Validation loss: 0.03682325, Gradient norm: 2.30543408
INFO:root:[  446] Training loss: 0.03639519, Validation loss: 0.03591089, Gradient norm: 3.30359456
INFO:root:[  447] Training loss: 0.03611729, Validation loss: 0.03633799, Gradient norm: 2.45344726
INFO:root:[  448] Training loss: 0.03607677, Validation loss: 0.03608501, Gradient norm: 1.38749175
INFO:root:[  449] Training loss: 0.03606542, Validation loss: 0.03631260, Gradient norm: 2.62942472
INFO:root:[  450] Training loss: 0.03586329, Validation loss: 0.03636493, Gradient norm: 2.13946830
INFO:root:[  451] Training loss: 0.03580810, Validation loss: 0.03566605, Gradient norm: 2.68702754
INFO:root:[  452] Training loss: 0.03585573, Validation loss: 0.03583530, Gradient norm: 2.90628030
INFO:root:[  453] Training loss: 0.03595875, Validation loss: 0.03558192, Gradient norm: 2.75549627
INFO:root:[  454] Training loss: 0.03565587, Validation loss: 0.03573030, Gradient norm: 2.23141642
INFO:root:[  455] Training loss: 0.03550623, Validation loss: 0.03561968, Gradient norm: 2.14882577
INFO:root:[  456] Training loss: 0.03566486, Validation loss: 0.03551104, Gradient norm: 3.18220327
INFO:root:[  457] Training loss: 0.03527799, Validation loss: 0.03518276, Gradient norm: 2.18777977
INFO:root:[  458] Training loss: 0.03537638, Validation loss: 0.03540509, Gradient norm: 2.47253559
INFO:root:[  459] Training loss: 0.03540497, Validation loss: 0.03488753, Gradient norm: 2.17416421
INFO:root:[  460] Training loss: 0.03537431, Validation loss: 0.03507906, Gradient norm: 2.45033408
INFO:root:[  461] Training loss: 0.03526128, Validation loss: 0.03539240, Gradient norm: 2.35354826
INFO:root:[  462] Training loss: 0.03500321, Validation loss: 0.03565390, Gradient norm: 2.46451259
INFO:root:[  463] Training loss: 0.03508019, Validation loss: 0.03480252, Gradient norm: 3.12207993
INFO:root:[  464] Training loss: 0.03511081, Validation loss: 0.03467737, Gradient norm: 3.24772282
INFO:root:[  465] Training loss: 0.03502066, Validation loss: 0.03541177, Gradient norm: 2.80867298
INFO:root:[  466] Training loss: 0.03468774, Validation loss: 0.03470166, Gradient norm: 2.14476545
INFO:root:[  467] Training loss: 0.03521042, Validation loss: 0.03469409, Gradient norm: 3.54478437
INFO:root:[  468] Training loss: 0.03488096, Validation loss: 0.03484162, Gradient norm: 3.23177286
INFO:root:[  469] Training loss: 0.03475673, Validation loss: 0.03454902, Gradient norm: 2.47221413
INFO:root:[  470] Training loss: 0.03481860, Validation loss: 0.03474314, Gradient norm: 3.23703600
INFO:root:[  471] Training loss: 0.03450015, Validation loss: 0.03459045, Gradient norm: 2.41604774
INFO:root:[  472] Training loss: 0.03454216, Validation loss: 0.03454030, Gradient norm: 2.73185322
INFO:root:[  473] Training loss: 0.03444598, Validation loss: 0.03502207, Gradient norm: 2.64799307
INFO:root:[  474] Training loss: 0.03435719, Validation loss: 0.03440031, Gradient norm: 2.46580961
INFO:root:[  475] Training loss: 0.03432162, Validation loss: 0.03466177, Gradient norm: 2.63455798
INFO:root:[  476] Training loss: 0.03428264, Validation loss: 0.03438675, Gradient norm: 2.75108618
INFO:root:[  477] Training loss: 0.03447852, Validation loss: 0.03412597, Gradient norm: 3.05905515
INFO:root:[  478] Training loss: 0.03434780, Validation loss: 0.03397272, Gradient norm: 3.47809933
INFO:root:[  479] Training loss: 0.03409368, Validation loss: 0.03488712, Gradient norm: 2.70097642
INFO:root:[  480] Training loss: 0.03399471, Validation loss: 0.03420408, Gradient norm: 2.72110739
INFO:root:[  481] Training loss: 0.03404741, Validation loss: 0.03480760, Gradient norm: 2.92650239
INFO:root:[  482] Training loss: 0.03416419, Validation loss: 0.03444237, Gradient norm: 3.81450459
INFO:root:[  483] Training loss: 0.03420824, Validation loss: 0.03398291, Gradient norm: 3.32281781
INFO:root:[  484] Training loss: 0.03402058, Validation loss: 0.03390819, Gradient norm: 3.04649583
INFO:root:[  485] Training loss: 0.03385434, Validation loss: 0.03522789, Gradient norm: 3.10401074
INFO:root:[  486] Training loss: 0.03374157, Validation loss: 0.03416349, Gradient norm: 2.90278670
INFO:root:[  487] Training loss: 0.03361718, Validation loss: 0.03350910, Gradient norm: 2.97763631
INFO:root:[  488] Training loss: 0.03368748, Validation loss: 0.03410784, Gradient norm: 2.92329374
INFO:root:[  489] Training loss: 0.03351831, Validation loss: 0.03334575, Gradient norm: 2.72949970
INFO:root:[  490] Training loss: 0.03353398, Validation loss: 0.03408790, Gradient norm: 2.90439064
INFO:root:[  491] Training loss: 0.03353470, Validation loss: 0.03383377, Gradient norm: 3.12543686
INFO:root:[  492] Training loss: 0.03346101, Validation loss: 0.03363968, Gradient norm: 3.38866333
INFO:root:[  493] Training loss: 0.03339098, Validation loss: 0.03326552, Gradient norm: 2.88397033
INFO:root:[  494] Training loss: 0.03344404, Validation loss: 0.03349946, Gradient norm: 3.46982455
INFO:root:[  495] Training loss: 0.03327548, Validation loss: 0.03320705, Gradient norm: 2.79582814
INFO:root:[  496] Training loss: 0.03308031, Validation loss: 0.03326528, Gradient norm: 3.25044319
INFO:root:[  497] Training loss: 0.03314485, Validation loss: 0.03330890, Gradient norm: 3.23624232
INFO:root:[  498] Training loss: 0.03344514, Validation loss: 0.03329892, Gradient norm: 3.51182227
INFO:root:[  499] Training loss: 0.03307990, Validation loss: 0.03331565, Gradient norm: 2.66924961
INFO:root:[  500] Training loss: 0.03314790, Validation loss: 0.03424614, Gradient norm: 3.47238061
INFO:root:[  501] Training loss: 0.03292032, Validation loss: 0.03277852, Gradient norm: 2.92755682
INFO:root:[  502] Training loss: 0.03285803, Validation loss: 0.03342096, Gradient norm: 2.77684121
INFO:root:[  503] Training loss: 0.03285765, Validation loss: 0.03285802, Gradient norm: 3.12832703
INFO:root:[  504] Training loss: 0.03283915, Validation loss: 0.03356595, Gradient norm: 1.88380473
INFO:root:[  505] Training loss: 0.03265402, Validation loss: 0.03282047, Gradient norm: 3.16774631
INFO:root:[  506] Training loss: 0.03266843, Validation loss: 0.03260289, Gradient norm: 3.23855124
INFO:root:[  507] Training loss: 0.03274003, Validation loss: 0.03319542, Gradient norm: 3.58259469
INFO:root:[  508] Training loss: 0.03262338, Validation loss: 0.03243788, Gradient norm: 3.76397848
INFO:root:[  509] Training loss: 0.03250643, Validation loss: 0.03245919, Gradient norm: 3.06897508
INFO:root:[  510] Training loss: 0.03275633, Validation loss: 0.03270232, Gradient norm: 4.25339464
INFO:root:[  511] Training loss: 0.03255539, Validation loss: 0.03247415, Gradient norm: 2.31924965
INFO:root:[  512] Training loss: 0.03251990, Validation loss: 0.03257772, Gradient norm: 4.06229937
INFO:root:[  513] Training loss: 0.03238407, Validation loss: 0.03218635, Gradient norm: 3.27808385
INFO:root:[  514] Training loss: 0.03229105, Validation loss: 0.03200813, Gradient norm: 3.21947007
INFO:root:[  515] Training loss: 0.03239935, Validation loss: 0.03184795, Gradient norm: 3.50856906
INFO:root:[  516] Training loss: 0.03238171, Validation loss: 0.03308428, Gradient norm: 4.09020826
INFO:root:[  517] Training loss: 0.03234482, Validation loss: 0.03269278, Gradient norm: 3.37168246
INFO:root:[  518] Training loss: 0.03213023, Validation loss: 0.03317134, Gradient norm: 3.42844739
INFO:root:[  519] Training loss: 0.03220059, Validation loss: 0.03216873, Gradient norm: 3.75721400
INFO:root:[  520] Training loss: 0.03203169, Validation loss: 0.03320845, Gradient norm: 2.87815532
INFO:root:[  521] Training loss: 0.03183772, Validation loss: 0.03257804, Gradient norm: 2.69130099
INFO:root:[  522] Training loss: 0.03218994, Validation loss: 0.03164440, Gradient norm: 2.86270370
INFO:root:[  523] Training loss: 0.03222823, Validation loss: 0.03262367, Gradient norm: 4.79859446
INFO:root:[  524] Training loss: 0.03230016, Validation loss: 0.03185375, Gradient norm: 4.74579331
INFO:root:[  525] Training loss: 0.03209410, Validation loss: 0.03183826, Gradient norm: 4.20855173
INFO:root:[  526] Training loss: 0.03194022, Validation loss: 0.03285014, Gradient norm: 4.48136785
INFO:root:[  527] Training loss: 0.03190332, Validation loss: 0.03250269, Gradient norm: 3.53248481
INFO:root:[  528] Training loss: 0.03197354, Validation loss: 0.03139212, Gradient norm: 4.61551020
INFO:root:[  529] Training loss: 0.03168878, Validation loss: 0.03172179, Gradient norm: 3.88544530
INFO:root:[  530] Training loss: 0.03194131, Validation loss: 0.03191685, Gradient norm: 3.62626584
INFO:root:[  531] Training loss: 0.03156958, Validation loss: 0.03265018, Gradient norm: 3.79180594
INFO:root:[  532] Training loss: 0.03155788, Validation loss: 0.03175335, Gradient norm: 2.02224035
INFO:root:[  533] Training loss: 0.03183426, Validation loss: 0.03135491, Gradient norm: 3.88630490
INFO:root:[  534] Training loss: 0.03145773, Validation loss: 0.03188429, Gradient norm: 3.77368698
INFO:root:[  535] Training loss: 0.03184422, Validation loss: 0.03218613, Gradient norm: 5.43103996
INFO:root:[  536] Training loss: 0.03165795, Validation loss: 0.03188936, Gradient norm: 5.02274090
INFO:root:[  537] Training loss: 0.03163827, Validation loss: 0.03169203, Gradient norm: 4.37632370
INFO:root:[  538] Training loss: 0.03143312, Validation loss: 0.03168004, Gradient norm: 4.28973654
INFO:root:[  539] Training loss: 0.03178597, Validation loss: 0.03184796, Gradient norm: 4.82668629
INFO:root:[  540] Training loss: 0.03146394, Validation loss: 0.03202298, Gradient norm: 4.65113920
INFO:root:[  541] Training loss: 0.03145411, Validation loss: 0.03119925, Gradient norm: 3.12503640
INFO:root:[  542] Training loss: 0.03149813, Validation loss: 0.03160841, Gradient norm: 4.80840945
INFO:root:[  543] Training loss: 0.03140187, Validation loss: 0.03139100, Gradient norm: 5.15078000
INFO:root:[  544] Training loss: 0.03160976, Validation loss: 0.03101900, Gradient norm: 5.51631509
INFO:root:[  545] Training loss: 0.03143741, Validation loss: 0.03133764, Gradient norm: 3.92532009
INFO:root:[  546] Training loss: 0.03127973, Validation loss: 0.03153444, Gradient norm: 4.04281977
INFO:root:[  547] Training loss: 0.03144837, Validation loss: 0.03098594, Gradient norm: 4.45158299
INFO:root:[  548] Training loss: 0.03136135, Validation loss: 0.03181206, Gradient norm: 4.71311451
INFO:root:[  549] Training loss: 0.03129456, Validation loss: 0.03122677, Gradient norm: 5.93736081
INFO:root:[  550] Training loss: 0.03133454, Validation loss: 0.03145718, Gradient norm: 3.89096492
INFO:root:[  551] Training loss: 0.03124775, Validation loss: 0.03112060, Gradient norm: 4.66892196
INFO:root:[  552] Training loss: 0.03111577, Validation loss: 0.03242475, Gradient norm: 3.80038393
INFO:root:[  553] Training loss: 0.03165437, Validation loss: 0.03136566, Gradient norm: 5.77880262
INFO:root:[  554] Training loss: 0.03106843, Validation loss: 0.03077054, Gradient norm: 2.99207510
INFO:root:[  555] Training loss: 0.03139012, Validation loss: 0.03128586, Gradient norm: 5.13200386
INFO:root:[  556] Training loss: 0.03115914, Validation loss: 0.03209088, Gradient norm: 4.80713109
INFO:root:[  557] Training loss: 0.03128554, Validation loss: 0.03212206, Gradient norm: 6.86749729
INFO:root:[  558] Training loss: 0.03118543, Validation loss: 0.03144718, Gradient norm: 5.55390843
INFO:root:[  559] Training loss: 0.03091921, Validation loss: 0.03070332, Gradient norm: 4.75709121
INFO:root:[  560] Training loss: 0.03113572, Validation loss: 0.03156918, Gradient norm: 4.72588238
INFO:root:[  561] Training loss: 0.03110850, Validation loss: 0.03109878, Gradient norm: 4.73651063
INFO:root:[  562] Training loss: 0.03120433, Validation loss: 0.03063819, Gradient norm: 5.51041680
INFO:root:[  563] Training loss: 0.03101523, Validation loss: 0.03135058, Gradient norm: 4.41086307
INFO:root:[  564] Training loss: 0.03088413, Validation loss: 0.03089929, Gradient norm: 5.24566044
INFO:root:[  565] Training loss: 0.03086928, Validation loss: 0.03078556, Gradient norm: 3.80803447
INFO:root:[  566] Training loss: 0.03120045, Validation loss: 0.03186695, Gradient norm: 5.52104302
INFO:root:[  567] Training loss: 0.03113127, Validation loss: 0.03092111, Gradient norm: 6.14001600
INFO:root:[  568] Training loss: 0.03095880, Validation loss: 0.03185339, Gradient norm: 4.17872843
INFO:root:[  569] Training loss: 0.03095828, Validation loss: 0.03159662, Gradient norm: 5.42974195
INFO:root:[  570] Training loss: 0.03096829, Validation loss: 0.03088547, Gradient norm: 4.47953808
INFO:root:[  571] Training loss: 0.03117841, Validation loss: 0.03117503, Gradient norm: 6.22595677
INFO:root:EP 571: Early stopping
INFO:root:Training the model took 6600.364s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.20687
INFO:root:EnergyScoreTrain: 0.1586
INFO:root:CoverageTrain: 0.99097
INFO:root:IntervalWidthTrain: 0.02478
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.1951
INFO:root:EnergyScoreValidation: 0.14924
INFO:root:CoverageValidation: 0.9907
INFO:root:IntervalWidthValidation: 0.02472
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.18083
INFO:root:EnergyScoreTest: 0.13928
INFO:root:CoverageTest: 0.99137
INFO:root:IntervalWidthTest: 0.02448
INFO:root:###17 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.1, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.62465838, Validation loss: 1.91000243, Gradient norm: 6.20950481
INFO:root:[    2] Training loss: 0.90794537, Validation loss: 0.54985579, Gradient norm: 4.76410380
INFO:root:[    3] Training loss: 0.51359053, Validation loss: 0.45225091, Gradient norm: 2.05122827
INFO:root:[    4] Training loss: 0.41641806, Validation loss: 0.39324873, Gradient norm: 1.80048443
INFO:root:[    5] Training loss: 0.36145337, Validation loss: 0.35676309, Gradient norm: 1.55975280
INFO:root:[    6] Training loss: 0.32441837, Validation loss: 0.30489008, Gradient norm: 1.44529343
INFO:root:[    7] Training loss: 0.29400948, Validation loss: 0.27637570, Gradient norm: 1.44024882
INFO:root:[    8] Training loss: 0.26415341, Validation loss: 0.25183097, Gradient norm: 0.96477318
INFO:root:[    9] Training loss: 0.24629305, Validation loss: 0.24031485, Gradient norm: 1.05109761
INFO:root:[   10] Training loss: 0.23426943, Validation loss: 0.22722189, Gradient norm: 0.96682163
INFO:root:[   11] Training loss: 0.22362574, Validation loss: 0.21695658, Gradient norm: 0.71189892
INFO:root:[   12] Training loss: 0.21409150, Validation loss: 0.20843473, Gradient norm: 0.64632835
INFO:root:[   13] Training loss: 0.20615474, Validation loss: 0.20109510, Gradient norm: 0.67882219
INFO:root:[   14] Training loss: 0.19915584, Validation loss: 0.19679096, Gradient norm: 0.69821304
INFO:root:[   15] Training loss: 0.19311610, Validation loss: 0.19241849, Gradient norm: 0.76572950
INFO:root:[   16] Training loss: 0.18675453, Validation loss: 0.18263852, Gradient norm: 0.61425703
INFO:root:[   17] Training loss: 0.18003229, Validation loss: 0.17861703, Gradient norm: 0.59423143
INFO:root:[   18] Training loss: 0.17570041, Validation loss: 0.17603694, Gradient norm: 0.54553627
INFO:root:[   19] Training loss: 0.17263359, Validation loss: 0.16915413, Gradient norm: 0.77508071
INFO:root:[   20] Training loss: 0.16755557, Validation loss: 0.16573827, Gradient norm: 0.80935055
INFO:root:[   21] Training loss: 0.16370801, Validation loss: 0.16195828, Gradient norm: 0.56285122
INFO:root:[   22] Training loss: 0.15972647, Validation loss: 0.16155386, Gradient norm: 0.44270491
INFO:root:[   23] Training loss: 0.15628361, Validation loss: 0.15646182, Gradient norm: 0.48150953
INFO:root:[   24] Training loss: 0.15361683, Validation loss: 0.15286572, Gradient norm: 0.53108716
INFO:root:[   25] Training loss: 0.15075175, Validation loss: 0.15101233, Gradient norm: 0.51387020
INFO:root:[   26] Training loss: 0.14854837, Validation loss: 0.14806516, Gradient norm: 0.49919925
INFO:root:[   27] Training loss: 0.14633913, Validation loss: 0.14490157, Gradient norm: 0.42357249
INFO:root:[   28] Training loss: 0.14425478, Validation loss: 0.14264028, Gradient norm: 0.37457509
INFO:root:[   29] Training loss: 0.14217033, Validation loss: 0.14051268, Gradient norm: 0.47864975
INFO:root:[   30] Training loss: 0.14039704, Validation loss: 0.13995020, Gradient norm: 0.48847339
INFO:root:[   31] Training loss: 0.13813131, Validation loss: 0.13765880, Gradient norm: 0.41228501
INFO:root:[   32] Training loss: 0.13704831, Validation loss: 0.13702303, Gradient norm: 0.42692675
INFO:root:[   33] Training loss: 0.13560926, Validation loss: 0.13478742, Gradient norm: 0.58024757
INFO:root:[   34] Training loss: 0.13375048, Validation loss: 0.13382562, Gradient norm: 0.51334360
INFO:root:[   35] Training loss: 0.13272854, Validation loss: 0.13179063, Gradient norm: 0.54876570
INFO:root:[   36] Training loss: 0.13095261, Validation loss: 0.13104165, Gradient norm: 0.39670251
INFO:root:[   37] Training loss: 0.13020123, Validation loss: 0.13028453, Gradient norm: 0.53687240
INFO:root:[   38] Training loss: 0.12880206, Validation loss: 0.12789130, Gradient norm: 0.36071925
INFO:root:[   39] Training loss: 0.12770834, Validation loss: 0.12752086, Gradient norm: 0.41414535
INFO:root:[   40] Training loss: 0.12660507, Validation loss: 0.12661432, Gradient norm: 0.43386980
INFO:root:[   41] Training loss: 0.12596814, Validation loss: 0.12527527, Gradient norm: 0.51401233
INFO:root:[   42] Training loss: 0.12460798, Validation loss: 0.12406256, Gradient norm: 0.43036409
INFO:root:[   43] Training loss: 0.12383528, Validation loss: 0.12354460, Gradient norm: 0.33335551
INFO:root:[   44] Training loss: 0.12295095, Validation loss: 0.12334466, Gradient norm: 0.53637785
INFO:root:[   45] Training loss: 0.12189998, Validation loss: 0.12289975, Gradient norm: 0.39845551
INFO:root:[   46] Training loss: 0.12102437, Validation loss: 0.12042834, Gradient norm: 0.41236503
INFO:root:[   47] Training loss: 0.12024948, Validation loss: 0.12031261, Gradient norm: 0.36496647
INFO:root:[   48] Training loss: 0.11954718, Validation loss: 0.11877525, Gradient norm: 0.42344422
INFO:root:[   49] Training loss: 0.11917456, Validation loss: 0.11831287, Gradient norm: 0.57444068
INFO:root:[   50] Training loss: 0.11761866, Validation loss: 0.11724075, Gradient norm: 0.35995248
INFO:root:[   51] Training loss: 0.11738559, Validation loss: 0.11744394, Gradient norm: 0.61957939
INFO:root:[   52] Training loss: 0.11680838, Validation loss: 0.11614726, Gradient norm: 0.59031057
INFO:root:[   53] Training loss: 0.11587786, Validation loss: 0.11590871, Gradient norm: 0.58479990
INFO:root:[   54] Training loss: 0.11542238, Validation loss: 0.11627155, Gradient norm: 0.54058219
INFO:root:[   55] Training loss: 0.11442265, Validation loss: 0.11480143, Gradient norm: 0.41028899
INFO:root:[   56] Training loss: 0.11400857, Validation loss: 0.11382072, Gradient norm: 0.49378407
INFO:root:[   57] Training loss: 0.11413852, Validation loss: 0.11358402, Gradient norm: 0.86804410
INFO:root:[   58] Training loss: 0.11301764, Validation loss: 0.11269457, Gradient norm: 0.62830560
INFO:root:[   59] Training loss: 0.11239468, Validation loss: 0.11229896, Gradient norm: 0.51614833
INFO:root:[   60] Training loss: 0.11207231, Validation loss: 0.11101772, Gradient norm: 0.61347352
INFO:root:[   61] Training loss: 0.11170932, Validation loss: 0.11127230, Gradient norm: 0.63049597
INFO:root:[   62] Training loss: 0.11091846, Validation loss: 0.11188695, Gradient norm: 0.66135286
INFO:root:[   63] Training loss: 0.11051489, Validation loss: 0.11062417, Gradient norm: 0.58030604
INFO:root:[   64] Training loss: 0.10997297, Validation loss: 0.11057502, Gradient norm: 0.67107264
INFO:root:[   65] Training loss: 0.10958829, Validation loss: 0.10968981, Gradient norm: 0.73345979
INFO:root:[   66] Training loss: 0.10874614, Validation loss: 0.10889888, Gradient norm: 0.49740648
INFO:root:[   67] Training loss: 0.10820146, Validation loss: 0.10836199, Gradient norm: 0.45523779
INFO:root:[   68] Training loss: 0.10776915, Validation loss: 0.10871777, Gradient norm: 0.51345698
INFO:root:[   69] Training loss: 0.10745178, Validation loss: 0.10699638, Gradient norm: 0.59903060
INFO:root:[   70] Training loss: 0.10674957, Validation loss: 0.10673004, Gradient norm: 0.43619181
INFO:root:[   71] Training loss: 0.10640907, Validation loss: 0.10628253, Gradient norm: 0.57106746
INFO:root:[   72] Training loss: 0.10612092, Validation loss: 0.10756378, Gradient norm: 0.48193719
INFO:root:[   73] Training loss: 0.10607653, Validation loss: 0.10662750, Gradient norm: 0.78739578
INFO:root:[   74] Training loss: 0.10529146, Validation loss: 0.10502721, Gradient norm: 0.74357769
INFO:root:[   75] Training loss: 0.10474264, Validation loss: 0.10425712, Gradient norm: 0.58434996
INFO:root:[   76] Training loss: 0.10451800, Validation loss: 0.10466684, Gradient norm: 0.72252163
INFO:root:[   77] Training loss: 0.10381530, Validation loss: 0.10424815, Gradient norm: 0.63172167
INFO:root:[   78] Training loss: 0.10358819, Validation loss: 0.10297435, Gradient norm: 0.79606574
INFO:root:[   79] Training loss: 0.10289209, Validation loss: 0.10272416, Gradient norm: 0.46419165
INFO:root:[   80] Training loss: 0.10237047, Validation loss: 0.10351993, Gradient norm: 0.37425749
INFO:root:[   81] Training loss: 0.10233503, Validation loss: 0.10188229, Gradient norm: 0.78285363
INFO:root:[   82] Training loss: 0.10176126, Validation loss: 0.10205280, Gradient norm: 0.77218324
INFO:root:[   83] Training loss: 0.10109694, Validation loss: 0.10115290, Gradient norm: 0.51297262
INFO:root:[   84] Training loss: 0.10064448, Validation loss: 0.10067257, Gradient norm: 0.51723492
INFO:root:[   85] Training loss: 0.10034625, Validation loss: 0.10009659, Gradient norm: 0.56246960
INFO:root:[   86] Training loss: 0.10034439, Validation loss: 0.10012558, Gradient norm: 0.83932530
INFO:root:[   87] Training loss: 0.09968742, Validation loss: 0.09927184, Gradient norm: 0.69870039
INFO:root:[   88] Training loss: 0.09928046, Validation loss: 0.09943958, Gradient norm: 0.69820830
INFO:root:[   89] Training loss: 0.09901686, Validation loss: 0.09993654, Gradient norm: 0.78552728
INFO:root:[   90] Training loss: 0.09828691, Validation loss: 0.09807300, Gradient norm: 0.59129172
INFO:root:[   91] Training loss: 0.09784895, Validation loss: 0.09799898, Gradient norm: 0.51234264
INFO:root:[   92] Training loss: 0.09770135, Validation loss: 0.09837218, Gradient norm: 0.75525582
INFO:root:[   93] Training loss: 0.09737083, Validation loss: 0.09666906, Gradient norm: 0.82827061
INFO:root:[   94] Training loss: 0.09660510, Validation loss: 0.09689388, Gradient norm: 0.57150705
INFO:root:[   95] Training loss: 0.09631766, Validation loss: 0.09661113, Gradient norm: 0.46743717
INFO:root:[   96] Training loss: 0.09571080, Validation loss: 0.09576477, Gradient norm: 0.39324025
INFO:root:[   97] Training loss: 0.09553707, Validation loss: 0.09505345, Gradient norm: 0.59391774
INFO:root:[   98] Training loss: 0.09508878, Validation loss: 0.09487770, Gradient norm: 0.50057398
INFO:root:[   99] Training loss: 0.09496711, Validation loss: 0.09495972, Gradient norm: 0.81136733
INFO:root:[  100] Training loss: 0.09443338, Validation loss: 0.09454369, Gradient norm: 0.77651421
INFO:root:[  101] Training loss: 0.09395044, Validation loss: 0.09467484, Gradient norm: 0.67673345
INFO:root:[  102] Training loss: 0.09372940, Validation loss: 0.09492029, Gradient norm: 0.77148628
INFO:root:[  103] Training loss: 0.09339273, Validation loss: 0.09317233, Gradient norm: 0.89109758
INFO:root:[  104] Training loss: 0.09270171, Validation loss: 0.09260631, Gradient norm: 0.58319544
INFO:root:[  105] Training loss: 0.09232499, Validation loss: 0.09192190, Gradient norm: 0.60456970
INFO:root:[  106] Training loss: 0.09218388, Validation loss: 0.09209744, Gradient norm: 0.74293930
INFO:root:[  107] Training loss: 0.09189626, Validation loss: 0.09143280, Gradient norm: 0.84844588
INFO:root:[  108] Training loss: 0.09104442, Validation loss: 0.09114369, Gradient norm: 0.56221787
INFO:root:[  109] Training loss: 0.09077275, Validation loss: 0.09105164, Gradient norm: 0.64910369
INFO:root:[  110] Training loss: 0.09050125, Validation loss: 0.09004022, Gradient norm: 0.66604066
INFO:root:[  111] Training loss: 0.08995418, Validation loss: 0.09005821, Gradient norm: 0.49768498
INFO:root:[  112] Training loss: 0.08996644, Validation loss: 0.08946286, Gradient norm: 0.78804915
INFO:root:[  113] Training loss: 0.08960185, Validation loss: 0.09014683, Gradient norm: 0.89868838
INFO:root:[  114] Training loss: 0.08882623, Validation loss: 0.08917609, Gradient norm: 0.63473308
INFO:root:[  115] Training loss: 0.08838029, Validation loss: 0.08798040, Gradient norm: 0.43203619
INFO:root:[  116] Training loss: 0.08817293, Validation loss: 0.08822302, Gradient norm: 0.72356754
INFO:root:[  117] Training loss: 0.08778105, Validation loss: 0.08772917, Gradient norm: 0.66812734
INFO:root:[  118] Training loss: 0.08749491, Validation loss: 0.08692762, Gradient norm: 0.82273422
INFO:root:[  119] Training loss: 0.08692117, Validation loss: 0.08716900, Gradient norm: 0.63329709
INFO:root:[  120] Training loss: 0.08665830, Validation loss: 0.08710480, Gradient norm: 0.72629428
INFO:root:[  121] Training loss: 0.08656087, Validation loss: 0.08596752, Gradient norm: 0.96611697
INFO:root:[  122] Training loss: 0.08578717, Validation loss: 0.08597423, Gradient norm: 0.70291372
INFO:root:[  123] Training loss: 0.08575560, Validation loss: 0.08503397, Gradient norm: 0.97549782
INFO:root:[  124] Training loss: 0.08486346, Validation loss: 0.08470758, Gradient norm: 0.46100645
INFO:root:[  125] Training loss: 0.08511608, Validation loss: 0.08448007, Gradient norm: 0.93847521
INFO:root:[  126] Training loss: 0.08416550, Validation loss: 0.08386316, Gradient norm: 0.57904786
INFO:root:[  127] Training loss: 0.08398000, Validation loss: 0.08458941, Gradient norm: 0.64914185
INFO:root:[  128] Training loss: 0.08354879, Validation loss: 0.08355635, Gradient norm: 0.60272336
INFO:root:[  129] Training loss: 0.08306496, Validation loss: 0.08288826, Gradient norm: 0.52268612
INFO:root:[  130] Training loss: 0.08270086, Validation loss: 0.08300051, Gradient norm: 0.56909559
INFO:root:[  131] Training loss: 0.08228057, Validation loss: 0.08200546, Gradient norm: 0.52566136
INFO:root:[  132] Training loss: 0.08223981, Validation loss: 0.08170062, Gradient norm: 0.69090801
INFO:root:[  133] Training loss: 0.08230128, Validation loss: 0.08175606, Gradient norm: 1.15075882
INFO:root:[  134] Training loss: 0.08132998, Validation loss: 0.08095569, Gradient norm: 0.76211372
INFO:root:[  135] Training loss: 0.08098928, Validation loss: 0.08084861, Gradient norm: 0.71339821
INFO:root:[  136] Training loss: 0.08042140, Validation loss: 0.08025960, Gradient norm: 0.66937495
INFO:root:[  137] Training loss: 0.08015143, Validation loss: 0.08051431, Gradient norm: 0.72719107
INFO:root:[  138] Training loss: 0.07988703, Validation loss: 0.07987593, Gradient norm: 0.71113608
INFO:root:[  139] Training loss: 0.07959154, Validation loss: 0.08015962, Gradient norm: 0.90729420
INFO:root:[  140] Training loss: 0.07932862, Validation loss: 0.07925836, Gradient norm: 0.89261692
INFO:root:[  141] Training loss: 0.07894381, Validation loss: 0.07946136, Gradient norm: 0.89340546
INFO:root:[  142] Training loss: 0.07857078, Validation loss: 0.07854369, Gradient norm: 0.92165404
INFO:root:[  143] Training loss: 0.07814915, Validation loss: 0.07799317, Gradient norm: 0.81200725
INFO:root:[  144] Training loss: 0.07773496, Validation loss: 0.07745683, Gradient norm: 0.77297058
INFO:root:[  145] Training loss: 0.07750666, Validation loss: 0.07777605, Gradient norm: 0.97858574
INFO:root:[  146] Training loss: 0.07692404, Validation loss: 0.07664147, Gradient norm: 0.73535081
INFO:root:[  147] Training loss: 0.07654461, Validation loss: 0.07668814, Gradient norm: 0.61628983
INFO:root:[  148] Training loss: 0.07627334, Validation loss: 0.07583099, Gradient norm: 0.88012225
INFO:root:[  149] Training loss: 0.07582538, Validation loss: 0.07561467, Gradient norm: 0.75439826
INFO:root:[  150] Training loss: 0.07537615, Validation loss: 0.07495360, Gradient norm: 0.50865186
INFO:root:[  151] Training loss: 0.07534941, Validation loss: 0.07476960, Gradient norm: 0.89788728
INFO:root:[  152] Training loss: 0.07496144, Validation loss: 0.07470892, Gradient norm: 0.95640927
INFO:root:[  153] Training loss: 0.07468837, Validation loss: 0.07385729, Gradient norm: 0.95795915
INFO:root:[  154] Training loss: 0.07422572, Validation loss: 0.07353996, Gradient norm: 1.00104378
INFO:root:[  155] Training loss: 0.07405520, Validation loss: 0.07375305, Gradient norm: 1.06959573
INFO:root:[  156] Training loss: 0.07345614, Validation loss: 0.07385682, Gradient norm: 0.81358923
INFO:root:[  157] Training loss: 0.07310630, Validation loss: 0.07264997, Gradient norm: 0.92155290
INFO:root:[  158] Training loss: 0.07267858, Validation loss: 0.07303722, Gradient norm: 0.88492739
INFO:root:[  159] Training loss: 0.07248225, Validation loss: 0.07227107, Gradient norm: 0.90341797
INFO:root:[  160] Training loss: 0.07194678, Validation loss: 0.07182338, Gradient norm: 0.47452029
INFO:root:[  161] Training loss: 0.07173157, Validation loss: 0.07168397, Gradient norm: 0.79739595
INFO:root:[  162] Training loss: 0.07150568, Validation loss: 0.07171047, Gradient norm: 0.91460305
INFO:root:[  163] Training loss: 0.07108946, Validation loss: 0.07082228, Gradient norm: 0.88641488
INFO:root:[  164] Training loss: 0.07057316, Validation loss: 0.07038619, Gradient norm: 0.61381814
INFO:root:[  165] Training loss: 0.07060822, Validation loss: 0.07048700, Gradient norm: 1.02072292
INFO:root:[  166] Training loss: 0.07016162, Validation loss: 0.06997301, Gradient norm: 0.97638819
INFO:root:[  167] Training loss: 0.06980979, Validation loss: 0.06968690, Gradient norm: 0.92782363
INFO:root:[  168] Training loss: 0.06937476, Validation loss: 0.06913102, Gradient norm: 0.80299001
INFO:root:[  169] Training loss: 0.06918529, Validation loss: 0.06886634, Gradient norm: 1.01226763
INFO:root:[  170] Training loss: 0.06858992, Validation loss: 0.06897553, Gradient norm: 0.52093374
INFO:root:[  171] Training loss: 0.06853884, Validation loss: 0.06868120, Gradient norm: 0.93578220
INFO:root:[  172] Training loss: 0.06836577, Validation loss: 0.06801815, Gradient norm: 1.06597015
INFO:root:[  173] Training loss: 0.06793082, Validation loss: 0.06804013, Gradient norm: 0.76965477
INFO:root:[  174] Training loss: 0.06753432, Validation loss: 0.06827772, Gradient norm: 0.92379589
INFO:root:[  175] Training loss: 0.06744760, Validation loss: 0.06712104, Gradient norm: 1.04415674
INFO:root:[  176] Training loss: 0.06708085, Validation loss: 0.06785763, Gradient norm: 1.00125083
INFO:root:[  177] Training loss: 0.06666275, Validation loss: 0.06690258, Gradient norm: 0.94887436
INFO:root:[  178] Training loss: 0.06630551, Validation loss: 0.06645355, Gradient norm: 0.91086963
INFO:root:[  179] Training loss: 0.06606529, Validation loss: 0.06590699, Gradient norm: 0.79822059
INFO:root:[  180] Training loss: 0.06584517, Validation loss: 0.06650053, Gradient norm: 0.96204593
INFO:root:[  181] Training loss: 0.06549515, Validation loss: 0.06544112, Gradient norm: 0.90916755
INFO:root:[  182] Training loss: 0.06501949, Validation loss: 0.06562578, Gradient norm: 0.79875955
INFO:root:[  183] Training loss: 0.06495130, Validation loss: 0.06522736, Gradient norm: 0.92606947
INFO:root:[  184] Training loss: 0.06460033, Validation loss: 0.06517876, Gradient norm: 0.92768119
INFO:root:[  185] Training loss: 0.06419076, Validation loss: 0.06460776, Gradient norm: 0.84271209
INFO:root:[  186] Training loss: 0.06401823, Validation loss: 0.06432878, Gradient norm: 0.93964055
INFO:root:[  187] Training loss: 0.06380986, Validation loss: 0.06397759, Gradient norm: 1.05996818
INFO:root:[  188] Training loss: 0.06330814, Validation loss: 0.06298930, Gradient norm: 0.53614505
INFO:root:[  189] Training loss: 0.06314690, Validation loss: 0.06340869, Gradient norm: 0.78033883
INFO:root:[  190] Training loss: 0.06288900, Validation loss: 0.06284033, Gradient norm: 0.89911230
INFO:root:[  191] Training loss: 0.06270265, Validation loss: 0.06232564, Gradient norm: 1.03935943
INFO:root:[  192] Training loss: 0.06218536, Validation loss: 0.06193346, Gradient norm: 0.70322117
INFO:root:[  193] Training loss: 0.06185962, Validation loss: 0.06173766, Gradient norm: 0.56297598
INFO:root:[  194] Training loss: 0.06173468, Validation loss: 0.06211573, Gradient norm: 0.89260484
INFO:root:[  195] Training loss: 0.06157546, Validation loss: 0.06169302, Gradient norm: 0.96971446
INFO:root:[  196] Training loss: 0.06116100, Validation loss: 0.06125281, Gradient norm: 0.99030200
INFO:root:[  197] Training loss: 0.06125480, Validation loss: 0.06114855, Gradient norm: 1.27537533
INFO:root:[  198] Training loss: 0.06076173, Validation loss: 0.06034926, Gradient norm: 1.00283552
INFO:root:[  199] Training loss: 0.06022253, Validation loss: 0.06002109, Gradient norm: 0.56388579
INFO:root:[  200] Training loss: 0.06000665, Validation loss: 0.05990796, Gradient norm: 0.74465325
INFO:root:[  201] Training loss: 0.05981868, Validation loss: 0.05949951, Gradient norm: 0.81357680
INFO:root:[  202] Training loss: 0.05947419, Validation loss: 0.05954615, Gradient norm: 0.78018529
INFO:root:[  203] Training loss: 0.05945629, Validation loss: 0.05899831, Gradient norm: 1.06396711
INFO:root:[  204] Training loss: 0.05910065, Validation loss: 0.05919177, Gradient norm: 0.96277552
INFO:root:[  205] Training loss: 0.05890722, Validation loss: 0.05898157, Gradient norm: 0.94537220
INFO:root:[  206] Training loss: 0.05880163, Validation loss: 0.05866017, Gradient norm: 1.17342426
INFO:root:[  207] Training loss: 0.05844734, Validation loss: 0.05838629, Gradient norm: 0.99671822
INFO:root:[  208] Training loss: 0.05818011, Validation loss: 0.05813415, Gradient norm: 0.92545386
INFO:root:[  209] Training loss: 0.05780036, Validation loss: 0.05784473, Gradient norm: 0.93348489
INFO:root:[  210] Training loss: 0.05761048, Validation loss: 0.05778970, Gradient norm: 0.99304425
INFO:root:[  211] Training loss: 0.05774562, Validation loss: 0.05809194, Gradient norm: 1.36885433
INFO:root:[  212] Training loss: 0.05715897, Validation loss: 0.05771113, Gradient norm: 0.97045430
INFO:root:[  213] Training loss: 0.05700677, Validation loss: 0.05683198, Gradient norm: 0.99449457
INFO:root:[  214] Training loss: 0.05659081, Validation loss: 0.05687323, Gradient norm: 0.92307129
INFO:root:[  215] Training loss: 0.05633188, Validation loss: 0.05635411, Gradient norm: 0.85203476
INFO:root:[  216] Training loss: 0.05632587, Validation loss: 0.05564030, Gradient norm: 1.09193682
INFO:root:[  217] Training loss: 0.05594300, Validation loss: 0.05590306, Gradient norm: 0.88651697
INFO:root:[  218] Training loss: 0.05576169, Validation loss: 0.05569221, Gradient norm: 0.98070008
INFO:root:[  219] Training loss: 0.05546595, Validation loss: 0.05590276, Gradient norm: 0.95545448
INFO:root:[  220] Training loss: 0.05528704, Validation loss: 0.05566576, Gradient norm: 1.01569061
INFO:root:[  221] Training loss: 0.05480689, Validation loss: 0.05464514, Gradient norm: 0.63577128
INFO:root:[  222] Training loss: 0.05470929, Validation loss: 0.05481228, Gradient norm: 0.80363763
INFO:root:[  223] Training loss: 0.05476743, Validation loss: 0.05500617, Gradient norm: 1.18034057
INFO:root:[  224] Training loss: 0.05446934, Validation loss: 0.05422921, Gradient norm: 1.08494574
INFO:root:[  225] Training loss: 0.05398181, Validation loss: 0.05418536, Gradient norm: 0.71959065
INFO:root:[  226] Training loss: 0.05396953, Validation loss: 0.05332162, Gradient norm: 1.14262444
INFO:root:[  227] Training loss: 0.05386317, Validation loss: 0.05373701, Gradient norm: 1.24853080
INFO:root:[  228] Training loss: 0.05360884, Validation loss: 0.05355570, Gradient norm: 1.06908393
INFO:root:[  229] Training loss: 0.05344981, Validation loss: 0.05373826, Gradient norm: 1.10300620
INFO:root:[  230] Training loss: 0.05299987, Validation loss: 0.05310429, Gradient norm: 1.02471576
INFO:root:[  231] Training loss: 0.05298593, Validation loss: 0.05312901, Gradient norm: 1.23009420
INFO:root:[  232] Training loss: 0.05255953, Validation loss: 0.05276074, Gradient norm: 0.96592253
INFO:root:[  233] Training loss: 0.05236219, Validation loss: 0.05213685, Gradient norm: 0.97272739
INFO:root:[  234] Training loss: 0.05210258, Validation loss: 0.05193130, Gradient norm: 0.96699464
INFO:root:[  235] Training loss: 0.05199870, Validation loss: 0.05194672, Gradient norm: 1.09291101
INFO:root:[  236] Training loss: 0.05193635, Validation loss: 0.05255238, Gradient norm: 1.28346644
INFO:root:[  237] Training loss: 0.05158353, Validation loss: 0.05256441, Gradient norm: 1.04464398
INFO:root:[  238] Training loss: 0.05133779, Validation loss: 0.05119889, Gradient norm: 1.08166137
INFO:root:[  239] Training loss: 0.05122035, Validation loss: 0.05096252, Gradient norm: 1.12853770
INFO:root:[  240] Training loss: 0.05089133, Validation loss: 0.05111808, Gradient norm: 0.98236621
INFO:root:[  241] Training loss: 0.05065453, Validation loss: 0.05147037, Gradient norm: 0.91707474
INFO:root:[  242] Training loss: 0.05050675, Validation loss: 0.05024310, Gradient norm: 1.01710908
INFO:root:[  243] Training loss: 0.05033380, Validation loss: 0.05048435, Gradient norm: 1.13639821
INFO:root:[  244] Training loss: 0.05014564, Validation loss: 0.04991640, Gradient norm: 1.09056458
INFO:root:[  245] Training loss: 0.05008778, Validation loss: 0.05027638, Gradient norm: 1.18349138
INFO:root:[  246] Training loss: 0.04975494, Validation loss: 0.04929184, Gradient norm: 1.05817296
INFO:root:[  247] Training loss: 0.04955172, Validation loss: 0.04972151, Gradient norm: 1.05428000
INFO:root:[  248] Training loss: 0.04927970, Validation loss: 0.04940384, Gradient norm: 1.00793655
INFO:root:[  249] Training loss: 0.04947508, Validation loss: 0.04908301, Gradient norm: 1.47790458
INFO:root:[  250] Training loss: 0.04917266, Validation loss: 0.04850738, Gradient norm: 1.31592531
INFO:root:[  251] Training loss: 0.04875335, Validation loss: 0.04928469, Gradient norm: 1.05401477
INFO:root:[  252] Training loss: 0.04879389, Validation loss: 0.04882207, Gradient norm: 1.30014399
INFO:root:[  253] Training loss: 0.04836099, Validation loss: 0.04815425, Gradient norm: 1.03338859
INFO:root:[  254] Training loss: 0.04824640, Validation loss: 0.04850346, Gradient norm: 1.11456394
INFO:root:[  255] Training loss: 0.04797124, Validation loss: 0.04787492, Gradient norm: 1.02162573
INFO:root:[  256] Training loss: 0.04795292, Validation loss: 0.04846169, Gradient norm: 1.25450795
INFO:root:[  257] Training loss: 0.04774405, Validation loss: 0.04732642, Gradient norm: 1.05974949
INFO:root:[  258] Training loss: 0.04772623, Validation loss: 0.04809486, Gradient norm: 1.34698713
INFO:root:[  259] Training loss: 0.04752050, Validation loss: 0.04695967, Gradient norm: 1.43285354
INFO:root:[  260] Training loss: 0.04692364, Validation loss: 0.04679757, Gradient norm: 0.66644419
INFO:root:[  261] Training loss: 0.04673497, Validation loss: 0.04706330, Gradient norm: 0.65261806
INFO:root:[  262] Training loss: 0.04673041, Validation loss: 0.04641470, Gradient norm: 0.97569192
INFO:root:[  263] Training loss: 0.04654417, Validation loss: 0.04635312, Gradient norm: 1.09337803
INFO:root:[  264] Training loss: 0.04640675, Validation loss: 0.04665583, Gradient norm: 1.24954053
INFO:root:[  265] Training loss: 0.04616534, Validation loss: 0.04612725, Gradient norm: 1.08046817
INFO:root:[  266] Training loss: 0.04602718, Validation loss: 0.04590976, Gradient norm: 1.16399818
INFO:root:[  267] Training loss: 0.04589967, Validation loss: 0.04617083, Gradient norm: 1.18855260
INFO:root:[  268] Training loss: 0.04584004, Validation loss: 0.04557493, Gradient norm: 1.44585599
INFO:root:[  269] Training loss: 0.04529738, Validation loss: 0.04533989, Gradient norm: 0.82556462
INFO:root:[  270] Training loss: 0.04510821, Validation loss: 0.04506118, Gradient norm: 0.90706841
INFO:root:[  271] Training loss: 0.04504215, Validation loss: 0.04600589, Gradient norm: 1.13595904
INFO:root:[  272] Training loss: 0.04505242, Validation loss: 0.04443468, Gradient norm: 1.38051536
INFO:root:[  273] Training loss: 0.04452825, Validation loss: 0.04467799, Gradient norm: 0.63442927
INFO:root:[  274] Training loss: 0.04475286, Validation loss: 0.04497505, Gradient norm: 1.36745361
INFO:root:[  275] Training loss: 0.04436449, Validation loss: 0.04562699, Gradient norm: 1.14377594
INFO:root:[  276] Training loss: 0.04440561, Validation loss: 0.04481888, Gradient norm: 1.34949786
INFO:root:[  277] Training loss: 0.04408701, Validation loss: 0.04449637, Gradient norm: 1.15234696
INFO:root:[  278] Training loss: 0.04394585, Validation loss: 0.04444970, Gradient norm: 1.28062775
INFO:root:[  279] Training loss: 0.04381969, Validation loss: 0.04329839, Gradient norm: 1.31337079
INFO:root:[  280] Training loss: 0.04359782, Validation loss: 0.04348535, Gradient norm: 1.37302105
INFO:root:[  281] Training loss: 0.04345845, Validation loss: 0.04372503, Gradient norm: 1.31022978
INFO:root:[  282] Training loss: 0.04315231, Validation loss: 0.04353211, Gradient norm: 1.00125812
INFO:root:[  283] Training loss: 0.04307332, Validation loss: 0.04284255, Gradient norm: 1.19877453
INFO:root:[  284] Training loss: 0.04291461, Validation loss: 0.04284588, Gradient norm: 1.21032294
INFO:root:[  285] Training loss: 0.04288757, Validation loss: 0.04293714, Gradient norm: 1.28690830
INFO:root:[  286] Training loss: 0.04267275, Validation loss: 0.04257407, Gradient norm: 1.37203837
INFO:root:[  287] Training loss: 0.04229910, Validation loss: 0.04204557, Gradient norm: 1.03967018
INFO:root:[  288] Training loss: 0.04213861, Validation loss: 0.04193893, Gradient norm: 1.00114343
INFO:root:[  289] Training loss: 0.04194243, Validation loss: 0.04181157, Gradient norm: 0.85037500
INFO:root:[  290] Training loss: 0.04180846, Validation loss: 0.04271543, Gradient norm: 0.96933270
INFO:root:[  291] Training loss: 0.04184283, Validation loss: 0.04155970, Gradient norm: 1.34457849
INFO:root:[  292] Training loss: 0.04164288, Validation loss: 0.04150463, Gradient norm: 1.35774697
INFO:root:[  293] Training loss: 0.04138448, Validation loss: 0.04163353, Gradient norm: 1.06174117
INFO:root:[  294] Training loss: 0.04118044, Validation loss: 0.04143271, Gradient norm: 1.12044544
INFO:root:[  295] Training loss: 0.04109660, Validation loss: 0.04074632, Gradient norm: 1.13962185
INFO:root:[  296] Training loss: 0.04102292, Validation loss: 0.04051076, Gradient norm: 1.34660471
INFO:root:[  297] Training loss: 0.04084889, Validation loss: 0.04152372, Gradient norm: 1.27178291
INFO:root:[  298] Training loss: 0.04078139, Validation loss: 0.04094258, Gradient norm: 1.55978066
INFO:root:[  299] Training loss: 0.04040491, Validation loss: 0.04022664, Gradient norm: 0.98591285
INFO:root:[  300] Training loss: 0.04045247, Validation loss: 0.04047744, Gradient norm: 1.29778526
INFO:root:[  301] Training loss: 0.04019762, Validation loss: 0.04010020, Gradient norm: 1.26108314
INFO:root:[  302] Training loss: 0.03995968, Validation loss: 0.04058306, Gradient norm: 1.10414159
INFO:root:[  303] Training loss: 0.03996464, Validation loss: 0.04071801, Gradient norm: 1.45802096
INFO:root:[  304] Training loss: 0.03981719, Validation loss: 0.04004457, Gradient norm: 1.48662170
INFO:root:[  305] Training loss: 0.03942364, Validation loss: 0.04004965, Gradient norm: 1.14912002
INFO:root:[  306] Training loss: 0.03968570, Validation loss: 0.04000368, Gradient norm: 1.72376205
INFO:root:[  307] Training loss: 0.03931163, Validation loss: 0.03987903, Gradient norm: 1.18596288
INFO:root:[  308] Training loss: 0.03916424, Validation loss: 0.03905710, Gradient norm: 1.24155304
INFO:root:[  309] Training loss: 0.03901816, Validation loss: 0.03872873, Gradient norm: 1.26167119
INFO:root:[  310] Training loss: 0.03897977, Validation loss: 0.03904528, Gradient norm: 1.33333698
INFO:root:[  311] Training loss: 0.03885868, Validation loss: 0.03914431, Gradient norm: 1.51046842
INFO:root:[  312] Training loss: 0.03850765, Validation loss: 0.03798043, Gradient norm: 1.04755882
INFO:root:[  313] Training loss: 0.03868872, Validation loss: 0.03881580, Gradient norm: 1.69538584
INFO:root:[  314] Training loss: 0.03824963, Validation loss: 0.03861093, Gradient norm: 1.24203344
INFO:root:[  315] Training loss: 0.03829885, Validation loss: 0.03869709, Gradient norm: 1.42562728
INFO:root:[  316] Training loss: 0.03809880, Validation loss: 0.03866882, Gradient norm: 1.43756592
INFO:root:[  317] Training loss: 0.03797155, Validation loss: 0.03801186, Gradient norm: 1.39089195
INFO:root:[  318] Training loss: 0.03789331, Validation loss: 0.03785498, Gradient norm: 1.47484981
INFO:root:[  319] Training loss: 0.03780845, Validation loss: 0.03747003, Gradient norm: 1.41389190
INFO:root:[  320] Training loss: 0.03755169, Validation loss: 0.03778589, Gradient norm: 1.21770250
INFO:root:[  321] Training loss: 0.03739946, Validation loss: 0.03746123, Gradient norm: 1.27764504
INFO:root:[  322] Training loss: 0.03720929, Validation loss: 0.03704101, Gradient norm: 1.15512209
INFO:root:[  323] Training loss: 0.03713899, Validation loss: 0.03754109, Gradient norm: 1.42831799
INFO:root:[  324] Training loss: 0.03700003, Validation loss: 0.03688417, Gradient norm: 1.29397156
INFO:root:[  325] Training loss: 0.03696513, Validation loss: 0.03658921, Gradient norm: 1.60797923
INFO:root:[  326] Training loss: 0.03662961, Validation loss: 0.03644679, Gradient norm: 1.17073227
INFO:root:[  327] Training loss: 0.03662147, Validation loss: 0.03658250, Gradient norm: 1.34316619
INFO:root:[  328] Training loss: 0.03644263, Validation loss: 0.03657083, Gradient norm: 1.15289105
INFO:root:[  329] Training loss: 0.03656252, Validation loss: 0.03655217, Gradient norm: 1.67392700
INFO:root:[  330] Training loss: 0.03633348, Validation loss: 0.03651254, Gradient norm: 1.54952149
INFO:root:[  331] Training loss: 0.03631114, Validation loss: 0.03741882, Gradient norm: 1.67745898
INFO:root:[  332] Training loss: 0.03613139, Validation loss: 0.03555009, Gradient norm: 1.59866967
INFO:root:[  333] Training loss: 0.03590865, Validation loss: 0.03644358, Gradient norm: 1.34029242
INFO:root:[  334] Training loss: 0.03568154, Validation loss: 0.03579916, Gradient norm: 1.12765162
INFO:root:[  335] Training loss: 0.03591144, Validation loss: 0.03553539, Gradient norm: 1.78469491
INFO:root:[  336] Training loss: 0.03554634, Validation loss: 0.03562615, Gradient norm: 1.37439193
INFO:root:[  337] Training loss: 0.03553958, Validation loss: 0.03603537, Gradient norm: 1.56717981
INFO:root:[  338] Training loss: 0.03522202, Validation loss: 0.03515174, Gradient norm: 0.96970076
INFO:root:[  339] Training loss: 0.03542073, Validation loss: 0.03644165, Gradient norm: 1.58998259
INFO:root:[  340] Training loss: 0.03516037, Validation loss: 0.03492612, Gradient norm: 1.49895295
INFO:root:[  341] Training loss: 0.03505041, Validation loss: 0.03500894, Gradient norm: 1.53202400
INFO:root:[  342] Training loss: 0.03477628, Validation loss: 0.03513495, Gradient norm: 1.17254840
INFO:root:[  343] Training loss: 0.03494788, Validation loss: 0.03518044, Gradient norm: 1.58692633
INFO:root:[  344] Training loss: 0.03440524, Validation loss: 0.03486591, Gradient norm: 0.85586783
INFO:root:[  345] Training loss: 0.03459656, Validation loss: 0.03465605, Gradient norm: 1.39078131
INFO:root:[  346] Training loss: 0.03438590, Validation loss: 0.03446468, Gradient norm: 1.08379536
INFO:root:[  347] Training loss: 0.03424649, Validation loss: 0.03494502, Gradient norm: 1.16938721
INFO:root:[  348] Training loss: 0.03428167, Validation loss: 0.03432881, Gradient norm: 1.59147040
INFO:root:[  349] Training loss: 0.03399759, Validation loss: 0.03389181, Gradient norm: 1.42844232
INFO:root:[  350] Training loss: 0.03414402, Validation loss: 0.03369677, Gradient norm: 1.71124755
INFO:root:[  351] Training loss: 0.03388284, Validation loss: 0.03426514, Gradient norm: 1.52765312
INFO:root:[  352] Training loss: 0.03376916, Validation loss: 0.03379482, Gradient norm: 1.55846663
INFO:root:[  353] Training loss: 0.03358662, Validation loss: 0.03443119, Gradient norm: 1.21848150
INFO:root:[  354] Training loss: 0.03349225, Validation loss: 0.03343348, Gradient norm: 1.48989308
INFO:root:[  355] Training loss: 0.03350257, Validation loss: 0.03370926, Gradient norm: 1.44721719
INFO:root:[  356] Training loss: 0.03352975, Validation loss: 0.03340921, Gradient norm: 1.86677128
INFO:root:[  357] Training loss: 0.03304571, Validation loss: 0.03316880, Gradient norm: 0.86193002
INFO:root:[  358] Training loss: 0.03314482, Validation loss: 0.03348599, Gradient norm: 1.58252030
INFO:root:[  359] Training loss: 0.03335361, Validation loss: 0.03275706, Gradient norm: 2.16950127
INFO:root:[  360] Training loss: 0.03282896, Validation loss: 0.03307897, Gradient norm: 1.32828045
INFO:root:[  361] Training loss: 0.03306243, Validation loss: 0.03290517, Gradient norm: 1.92139898
INFO:root:[  362] Training loss: 0.03265462, Validation loss: 0.03272779, Gradient norm: 1.38847734
INFO:root:[  363] Training loss: 0.03306909, Validation loss: 0.03269852, Gradient norm: 2.22968634
INFO:root:[  364] Training loss: 0.03272634, Validation loss: 0.03329330, Gradient norm: 1.87462603
INFO:root:[  365] Training loss: 0.03255353, Validation loss: 0.03249442, Gradient norm: 1.68328465
INFO:root:[  366] Training loss: 0.03248116, Validation loss: 0.03244578, Gradient norm: 1.37158984
INFO:root:[  367] Training loss: 0.03222911, Validation loss: 0.03292745, Gradient norm: 1.34465020
INFO:root:[  368] Training loss: 0.03227926, Validation loss: 0.03211140, Gradient norm: 1.61225951
INFO:root:[  369] Training loss: 0.03229072, Validation loss: 0.03236626, Gradient norm: 1.94693418
INFO:root:[  370] Training loss: 0.03193242, Validation loss: 0.03249464, Gradient norm: 1.42203875
INFO:root:[  371] Training loss: 0.03188237, Validation loss: 0.03251279, Gradient norm: 1.45515021
INFO:root:[  372] Training loss: 0.03193718, Validation loss: 0.03212452, Gradient norm: 1.87849161
INFO:root:[  373] Training loss: 0.03205721, Validation loss: 0.03199134, Gradient norm: 2.03406259
INFO:root:[  374] Training loss: 0.03176382, Validation loss: 0.03174526, Gradient norm: 1.59429288
INFO:root:[  375] Training loss: 0.03177893, Validation loss: 0.03260723, Gradient norm: 1.74242136
INFO:root:[  376] Training loss: 0.03202915, Validation loss: 0.03156014, Gradient norm: 2.53272702
INFO:root:[  377] Training loss: 0.03129855, Validation loss: 0.03239823, Gradient norm: 1.15039336
INFO:root:[  378] Training loss: 0.03136764, Validation loss: 0.03175815, Gradient norm: 1.42358031
INFO:root:[  379] Training loss: 0.03136418, Validation loss: 0.03222506, Gradient norm: 1.57596094
INFO:root:[  380] Training loss: 0.03162400, Validation loss: 0.03218263, Gradient norm: 2.39778293
INFO:root:[  381] Training loss: 0.03133524, Validation loss: 0.03118306, Gradient norm: 1.98016389
INFO:root:[  382] Training loss: 0.03124184, Validation loss: 0.03176424, Gradient norm: 1.96591951
INFO:root:[  383] Training loss: 0.03126620, Validation loss: 0.03084326, Gradient norm: 1.89856232
INFO:root:[  384] Training loss: 0.03119840, Validation loss: 0.03110253, Gradient norm: 2.12092469
INFO:root:[  385] Training loss: 0.03077645, Validation loss: 0.03135481, Gradient norm: 1.48235841
INFO:root:[  386] Training loss: 0.03097919, Validation loss: 0.03046858, Gradient norm: 1.95493784
INFO:root:[  387] Training loss: 0.03092220, Validation loss: 0.03089115, Gradient norm: 1.94041194
INFO:root:[  388] Training loss: 0.03060761, Validation loss: 0.03149321, Gradient norm: 1.61818950
INFO:root:[  389] Training loss: 0.03068307, Validation loss: 0.03098826, Gradient norm: 1.42155948
INFO:root:[  390] Training loss: 0.03054921, Validation loss: 0.03141116, Gradient norm: 1.57970488
INFO:root:[  391] Training loss: 0.03073379, Validation loss: 0.03037559, Gradient norm: 2.02895069
INFO:root:[  392] Training loss: 0.03044132, Validation loss: 0.03038926, Gradient norm: 1.91146457
INFO:root:[  393] Training loss: 0.03042956, Validation loss: 0.03045223, Gradient norm: 1.96444129
INFO:root:[  394] Training loss: 0.03026864, Validation loss: 0.03011152, Gradient norm: 1.86854786
INFO:root:[  395] Training loss: 0.03050333, Validation loss: 0.03053210, Gradient norm: 2.25107652
INFO:root:[  396] Training loss: 0.03027058, Validation loss: 0.03001790, Gradient norm: 2.01313208
INFO:root:[  397] Training loss: 0.03015411, Validation loss: 0.03043681, Gradient norm: 1.83368183
INFO:root:[  398] Training loss: 0.03014109, Validation loss: 0.02978411, Gradient norm: 1.66993277
INFO:root:[  399] Training loss: 0.02992240, Validation loss: 0.03082556, Gradient norm: 1.72443662
INFO:root:[  400] Training loss: 0.02984590, Validation loss: 0.02987795, Gradient norm: 1.58144656
INFO:root:[  401] Training loss: 0.02988723, Validation loss: 0.02947949, Gradient norm: 1.57929551
INFO:root:[  402] Training loss: 0.03000086, Validation loss: 0.02966039, Gradient norm: 2.30557119
INFO:root:[  403] Training loss: 0.02985040, Validation loss: 0.02978074, Gradient norm: 2.52341737
INFO:root:[  404] Training loss: 0.02975394, Validation loss: 0.03005995, Gradient norm: 2.17424043
INFO:root:[  405] Training loss: 0.02973430, Validation loss: 0.03086930, Gradient norm: 2.12991645
INFO:root:[  406] Training loss: 0.02973318, Validation loss: 0.02986719, Gradient norm: 1.90819918
INFO:root:[  407] Training loss: 0.02962381, Validation loss: 0.02971331, Gradient norm: 1.93768642
INFO:root:[  408] Training loss: 0.02945034, Validation loss: 0.02963521, Gradient norm: 1.80414421
INFO:root:[  409] Training loss: 0.02924053, Validation loss: 0.02932018, Gradient norm: 1.11444083
INFO:root:[  410] Training loss: 0.02950755, Validation loss: 0.02929931, Gradient norm: 2.37480203
INFO:root:[  411] Training loss: 0.02946480, Validation loss: 0.02952069, Gradient norm: 2.39052703
INFO:root:[  412] Training loss: 0.02940825, Validation loss: 0.02930934, Gradient norm: 2.29506876
INFO:root:[  413] Training loss: 0.02928599, Validation loss: 0.02964338, Gradient norm: 2.22023593
INFO:root:[  414] Training loss: 0.02917893, Validation loss: 0.02918693, Gradient norm: 1.95792383
INFO:root:[  415] Training loss: 0.02919438, Validation loss: 0.02996631, Gradient norm: 1.70666027
INFO:root:[  416] Training loss: 0.02922147, Validation loss: 0.02915314, Gradient norm: 2.16805162
INFO:root:[  417] Training loss: 0.02905017, Validation loss: 0.02957631, Gradient norm: 1.59120719
INFO:root:[  418] Training loss: 0.02914305, Validation loss: 0.02934920, Gradient norm: 2.36054535
INFO:root:[  419] Training loss: 0.02927233, Validation loss: 0.02885740, Gradient norm: 2.68982031
INFO:root:[  420] Training loss: 0.02910728, Validation loss: 0.02998813, Gradient norm: 2.17532315
INFO:root:[  421] Training loss: 0.02908588, Validation loss: 0.02907295, Gradient norm: 2.53411220
INFO:root:[  422] Training loss: 0.02896273, Validation loss: 0.02869511, Gradient norm: 2.25498981
INFO:root:[  423] Training loss: 0.02905015, Validation loss: 0.02900921, Gradient norm: 2.53749415
INFO:root:[  424] Training loss: 0.02904240, Validation loss: 0.02882485, Gradient norm: 2.41377248
INFO:root:[  425] Training loss: 0.02889373, Validation loss: 0.02874470, Gradient norm: 2.05917675
INFO:root:[  426] Training loss: 0.02861932, Validation loss: 0.02871562, Gradient norm: 1.40154824
INFO:root:[  427] Training loss: 0.02879380, Validation loss: 0.02879609, Gradient norm: 1.53884084
INFO:root:[  428] Training loss: 0.02862309, Validation loss: 0.02915331, Gradient norm: 2.34999590
INFO:root:[  429] Training loss: 0.02894395, Validation loss: 0.02894350, Gradient norm: 3.03665328
INFO:root:[  430] Training loss: 0.02880211, Validation loss: 0.02861040, Gradient norm: 2.79885295
INFO:root:[  431] Training loss: 0.02870730, Validation loss: 0.02928668, Gradient norm: 2.92084627
INFO:root:[  432] Training loss: 0.02863103, Validation loss: 0.02872276, Gradient norm: 2.40783048
INFO:root:[  433] Training loss: 0.02864999, Validation loss: 0.02868512, Gradient norm: 2.25688641
INFO:root:[  434] Training loss: 0.02853171, Validation loss: 0.02914555, Gradient norm: 1.95052409
INFO:root:[  435] Training loss: 0.02874989, Validation loss: 0.02871813, Gradient norm: 3.05950802
INFO:root:[  436] Training loss: 0.02856396, Validation loss: 0.02941265, Gradient norm: 2.28076689
INFO:root:[  437] Training loss: 0.02848975, Validation loss: 0.02874321, Gradient norm: 2.34907944
INFO:root:[  438] Training loss: 0.02861475, Validation loss: 0.02895401, Gradient norm: 2.98319838
INFO:root:[  439] Training loss: 0.02861999, Validation loss: 0.02906997, Gradient norm: 3.23445469
INFO:root:EP 439: Early stopping
INFO:root:Training the model took 5080.085s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.2124
INFO:root:EnergyScoreTrain: 0.1549
INFO:root:CoverageTrain: 0.98987
INFO:root:IntervalWidthTrain: 0.01978
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.20133
INFO:root:EnergyScoreValidation: 0.14649
INFO:root:CoverageValidation: 0.98948
INFO:root:IntervalWidthValidation: 0.01988
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.18574
INFO:root:EnergyScoreTest: 0.13557
INFO:root:CoverageTest: 0.99018
INFO:root:IntervalWidthTest: 0.01941
INFO:root:###18 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.15, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.67753592, Validation loss: 1.68693619, Gradient norm: 5.58243330
INFO:root:[    2] Training loss: 0.81330046, Validation loss: 0.64342351, Gradient norm: 3.81810112
INFO:root:[    3] Training loss: 0.61650226, Validation loss: 0.58439269, Gradient norm: 2.34846881
INFO:root:[    4] Training loss: 0.53303974, Validation loss: 0.47914277, Gradient norm: 1.89526135
INFO:root:[    5] Training loss: 0.45350897, Validation loss: 0.42263908, Gradient norm: 1.74805454
INFO:root:[    6] Training loss: 0.39661296, Validation loss: 0.37733550, Gradient norm: 1.87651317
INFO:root:[    7] Training loss: 0.35240307, Validation loss: 0.32830088, Gradient norm: 1.57198759
INFO:root:[    8] Training loss: 0.31659169, Validation loss: 0.29676929, Gradient norm: 1.56556530
INFO:root:[    9] Training loss: 0.28800983, Validation loss: 0.28526776, Gradient norm: 1.14694875
INFO:root:[   10] Training loss: 0.27116497, Validation loss: 0.26056105, Gradient norm: 1.23887925
INFO:root:[   11] Training loss: 0.25174867, Validation loss: 0.24884979, Gradient norm: 0.85903118
INFO:root:[   12] Training loss: 0.24231417, Validation loss: 0.23685602, Gradient norm: 0.91435453
INFO:root:[   13] Training loss: 0.23209916, Validation loss: 0.22771357, Gradient norm: 0.94467468
INFO:root:[   14] Training loss: 0.22411055, Validation loss: 0.22017876, Gradient norm: 0.95871705
INFO:root:[   15] Training loss: 0.21618880, Validation loss: 0.21480457, Gradient norm: 0.71895074
INFO:root:[   16] Training loss: 0.20999295, Validation loss: 0.20618635, Gradient norm: 0.80360946
INFO:root:[   17] Training loss: 0.20373690, Validation loss: 0.20132382, Gradient norm: 0.64527952
INFO:root:[   18] Training loss: 0.19909226, Validation loss: 0.19961123, Gradient norm: 0.74451965
INFO:root:[   19] Training loss: 0.19385753, Validation loss: 0.19164823, Gradient norm: 0.82190120
INFO:root:[   20] Training loss: 0.18902188, Validation loss: 0.18755121, Gradient norm: 0.61561801
INFO:root:[   21] Training loss: 0.18572732, Validation loss: 0.18395941, Gradient norm: 0.73443768
INFO:root:[   22] Training loss: 0.18130979, Validation loss: 0.17998479, Gradient norm: 0.64915308
INFO:root:[   23] Training loss: 0.17819450, Validation loss: 0.17578429, Gradient norm: 0.58474520
INFO:root:[   24] Training loss: 0.17503384, Validation loss: 0.17311195, Gradient norm: 0.46151760
INFO:root:[   25] Training loss: 0.17189193, Validation loss: 0.17062467, Gradient norm: 0.44356096
INFO:root:[   26] Training loss: 0.16887078, Validation loss: 0.16842857, Gradient norm: 0.42858646
INFO:root:[   27] Training loss: 0.16718521, Validation loss: 0.16905676, Gradient norm: 0.66496504
INFO:root:[   28] Training loss: 0.16549565, Validation loss: 0.16549622, Gradient norm: 0.66406501
INFO:root:[   29] Training loss: 0.16296503, Validation loss: 0.16171449, Gradient norm: 0.57435679
INFO:root:[   30] Training loss: 0.16105028, Validation loss: 0.15999503, Gradient norm: 0.51745997
INFO:root:[   31] Training loss: 0.15937379, Validation loss: 0.15873154, Gradient norm: 0.50317173
INFO:root:[   32] Training loss: 0.15794358, Validation loss: 0.15658380, Gradient norm: 0.49233404
INFO:root:[   33] Training loss: 0.15647130, Validation loss: 0.15660891, Gradient norm: 0.64239878
INFO:root:[   34] Training loss: 0.15527151, Validation loss: 0.15520081, Gradient norm: 0.75103436
INFO:root:[   35] Training loss: 0.15380936, Validation loss: 0.15210884, Gradient norm: 0.74399218
INFO:root:[   36] Training loss: 0.15225841, Validation loss: 0.15247656, Gradient norm: 0.57757396
INFO:root:[   37] Training loss: 0.15119748, Validation loss: 0.15033245, Gradient norm: 0.44298646
INFO:root:[   38] Training loss: 0.15000575, Validation loss: 0.14925002, Gradient norm: 0.43892348
INFO:root:[   39] Training loss: 0.14869827, Validation loss: 0.14853728, Gradient norm: 0.47690661
INFO:root:[   40] Training loss: 0.14768934, Validation loss: 0.14715811, Gradient norm: 0.49700266
INFO:root:[   41] Training loss: 0.14653513, Validation loss: 0.14682175, Gradient norm: 0.63583565
INFO:root:[   42] Training loss: 0.14586577, Validation loss: 0.14482285, Gradient norm: 0.78289871
INFO:root:[   43] Training loss: 0.14464816, Validation loss: 0.14458625, Gradient norm: 0.51167797
INFO:root:[   44] Training loss: 0.14427200, Validation loss: 0.14518183, Gradient norm: 0.77113868
INFO:root:[   45] Training loss: 0.14307456, Validation loss: 0.14288108, Gradient norm: 0.74347476
INFO:root:[   46] Training loss: 0.14212562, Validation loss: 0.14306272, Gradient norm: 0.59424796
INFO:root:[   47] Training loss: 0.14127944, Validation loss: 0.14061032, Gradient norm: 0.70416681
INFO:root:[   48] Training loss: 0.14018379, Validation loss: 0.13952041, Gradient norm: 0.56205490
INFO:root:[   49] Training loss: 0.13922907, Validation loss: 0.14039876, Gradient norm: 0.56686139
INFO:root:[   50] Training loss: 0.13938282, Validation loss: 0.13856584, Gradient norm: 0.95433063
INFO:root:[   51] Training loss: 0.13797058, Validation loss: 0.13842951, Gradient norm: 0.75226547
INFO:root:[   52] Training loss: 0.13703656, Validation loss: 0.13772723, Gradient norm: 0.69113367
INFO:root:[   53] Training loss: 0.13644479, Validation loss: 0.13650341, Gradient norm: 0.47715274
INFO:root:[   54] Training loss: 0.13544716, Validation loss: 0.13515826, Gradient norm: 0.35577815
INFO:root:[   55] Training loss: 0.13462058, Validation loss: 0.13484232, Gradient norm: 0.42183705
INFO:root:[   56] Training loss: 0.13382356, Validation loss: 0.13472198, Gradient norm: 0.35167575
INFO:root:[   57] Training loss: 0.13410636, Validation loss: 0.13547411, Gradient norm: 1.03610051
INFO:root:[   58] Training loss: 0.13292643, Validation loss: 0.13166996, Gradient norm: 0.76760742
INFO:root:[   59] Training loss: 0.13203545, Validation loss: 0.13277716, Gradient norm: 0.61174894
INFO:root:[   60] Training loss: 0.13166674, Validation loss: 0.13209278, Gradient norm: 0.95914629
INFO:root:[   61] Training loss: 0.13044127, Validation loss: 0.13028472, Gradient norm: 0.41433140
INFO:root:[   62] Training loss: 0.12976019, Validation loss: 0.12955185, Gradient norm: 0.51855814
INFO:root:[   63] Training loss: 0.12923431, Validation loss: 0.12955768, Gradient norm: 0.57805693
INFO:root:[   64] Training loss: 0.12871456, Validation loss: 0.12773671, Gradient norm: 0.77747395
INFO:root:[   65] Training loss: 0.12788366, Validation loss: 0.12796721, Gradient norm: 0.68170403
INFO:root:[   66] Training loss: 0.12758460, Validation loss: 0.12809858, Gradient norm: 1.02307151
INFO:root:[   67] Training loss: 0.12647089, Validation loss: 0.12660086, Gradient norm: 0.55594659
INFO:root:[   68] Training loss: 0.12566009, Validation loss: 0.12520798, Gradient norm: 0.36828950
INFO:root:[   69] Training loss: 0.12532607, Validation loss: 0.12520390, Gradient norm: 0.65565798
INFO:root:[   70] Training loss: 0.12498658, Validation loss: 0.12515783, Gradient norm: 0.90389896
INFO:root:[   71] Training loss: 0.12437865, Validation loss: 0.12493094, Gradient norm: 0.97218178
INFO:root:[   72] Training loss: 0.12399857, Validation loss: 0.12452278, Gradient norm: 1.00380908
INFO:root:[   73] Training loss: 0.12298681, Validation loss: 0.12261569, Gradient norm: 0.80581689
INFO:root:[   74] Training loss: 0.12203110, Validation loss: 0.12193915, Gradient norm: 0.44904042
INFO:root:[   75] Training loss: 0.12183789, Validation loss: 0.12139110, Gradient norm: 0.80402681
INFO:root:[   76] Training loss: 0.12074187, Validation loss: 0.12069706, Gradient norm: 0.42350051
INFO:root:[   77] Training loss: 0.12032808, Validation loss: 0.12052382, Gradient norm: 0.58370111
INFO:root:[   78] Training loss: 0.12014370, Validation loss: 0.12123413, Gradient norm: 0.97569590
INFO:root:[   79] Training loss: 0.11912544, Validation loss: 0.11899889, Gradient norm: 0.59301910
INFO:root:[   80] Training loss: 0.11871519, Validation loss: 0.11863449, Gradient norm: 0.69304734
INFO:root:[   81] Training loss: 0.11800032, Validation loss: 0.11895853, Gradient norm: 0.69961639
INFO:root:[   82] Training loss: 0.11760010, Validation loss: 0.11772754, Gradient norm: 0.87627998
INFO:root:[   83] Training loss: 0.11696272, Validation loss: 0.11629134, Gradient norm: 0.92749779
INFO:root:[   84] Training loss: 0.11633357, Validation loss: 0.11643076, Gradient norm: 0.65711878
INFO:root:[   85] Training loss: 0.11564050, Validation loss: 0.11600364, Gradient norm: 0.78945077
INFO:root:[   86] Training loss: 0.11522507, Validation loss: 0.11516834, Gradient norm: 0.84292126
INFO:root:[   87] Training loss: 0.11468278, Validation loss: 0.11542200, Gradient norm: 1.00552801
INFO:root:[   88] Training loss: 0.11413009, Validation loss: 0.11415743, Gradient norm: 0.94064244
INFO:root:[   89] Training loss: 0.11357652, Validation loss: 0.11341389, Gradient norm: 0.87612998
INFO:root:[   90] Training loss: 0.11345845, Validation loss: 0.11490667, Gradient norm: 1.23575881
INFO:root:[   91] Training loss: 0.11280083, Validation loss: 0.11220000, Gradient norm: 1.14253286
INFO:root:[   92] Training loss: 0.11187513, Validation loss: 0.11237708, Gradient norm: 0.80536701
INFO:root:[   93] Training loss: 0.11123542, Validation loss: 0.11109939, Gradient norm: 0.61554708
INFO:root:[   94] Training loss: 0.11068822, Validation loss: 0.11120510, Gradient norm: 0.62255697
INFO:root:[   95] Training loss: 0.11013972, Validation loss: 0.11006713, Gradient norm: 0.67494523
INFO:root:[   96] Training loss: 0.11012771, Validation loss: 0.11072308, Gradient norm: 1.16728525
INFO:root:[   97] Training loss: 0.10924208, Validation loss: 0.10948120, Gradient norm: 0.92108881
INFO:root:[   98] Training loss: 0.10909684, Validation loss: 0.10883562, Gradient norm: 1.20624988
INFO:root:[   99] Training loss: 0.10844953, Validation loss: 0.10854631, Gradient norm: 1.23762869
INFO:root:[  100] Training loss: 0.10760265, Validation loss: 0.10709258, Gradient norm: 0.82733406
INFO:root:[  101] Training loss: 0.10680966, Validation loss: 0.10762874, Gradient norm: 0.46489077
INFO:root:[  102] Training loss: 0.10682663, Validation loss: 0.10646961, Gradient norm: 1.24204970
INFO:root:[  103] Training loss: 0.10601332, Validation loss: 0.10673281, Gradient norm: 0.98695023
INFO:root:[  104] Training loss: 0.10550532, Validation loss: 0.10510076, Gradient norm: 0.89543532
INFO:root:[  105] Training loss: 0.10508405, Validation loss: 0.10479891, Gradient norm: 1.04458979
INFO:root:[  106] Training loss: 0.10439881, Validation loss: 0.10468583, Gradient norm: 0.75518699
INFO:root:[  107] Training loss: 0.10388588, Validation loss: 0.10346899, Gradient norm: 0.84358695
INFO:root:[  108] Training loss: 0.10350273, Validation loss: 0.10296348, Gradient norm: 0.89414232
INFO:root:[  109] Training loss: 0.10304248, Validation loss: 0.10260253, Gradient norm: 1.08886436
INFO:root:[  110] Training loss: 0.10221554, Validation loss: 0.10202164, Gradient norm: 0.86804590
INFO:root:[  111] Training loss: 0.10206791, Validation loss: 0.10154822, Gradient norm: 1.12995493
INFO:root:[  112] Training loss: 0.10129986, Validation loss: 0.10051558, Gradient norm: 0.95516114
INFO:root:[  113] Training loss: 0.10111706, Validation loss: 0.10081402, Gradient norm: 1.27007352
INFO:root:[  114] Training loss: 0.10016502, Validation loss: 0.10021991, Gradient norm: 0.85244586
INFO:root:[  115] Training loss: 0.09980887, Validation loss: 0.10146144, Gradient norm: 1.00767719
INFO:root:[  116] Training loss: 0.09945811, Validation loss: 0.10033472, Gradient norm: 1.23445247
INFO:root:[  117] Training loss: 0.09880422, Validation loss: 0.09891657, Gradient norm: 1.08141894
INFO:root:[  118] Training loss: 0.09812346, Validation loss: 0.09814838, Gradient norm: 0.74185805
INFO:root:[  119] Training loss: 0.09764171, Validation loss: 0.09798256, Gradient norm: 1.05637977
INFO:root:[  120] Training loss: 0.09725517, Validation loss: 0.09670646, Gradient norm: 1.12424442
INFO:root:[  121] Training loss: 0.09651914, Validation loss: 0.09631864, Gradient norm: 0.99056641
INFO:root:[  122] Training loss: 0.09617171, Validation loss: 0.09568229, Gradient norm: 1.13567084
INFO:root:[  123] Training loss: 0.09557581, Validation loss: 0.09519512, Gradient norm: 1.10076040
INFO:root:[  124] Training loss: 0.09532146, Validation loss: 0.09559093, Gradient norm: 1.36071337
INFO:root:[  125] Training loss: 0.09465448, Validation loss: 0.09538861, Gradient norm: 1.26283772
INFO:root:[  126] Training loss: 0.09421881, Validation loss: 0.09385419, Gradient norm: 1.42333666
INFO:root:[  127] Training loss: 0.09368015, Validation loss: 0.09337291, Gradient norm: 1.16641479
INFO:root:[  128] Training loss: 0.09319102, Validation loss: 0.09334288, Gradient norm: 1.19431293
INFO:root:[  129] Training loss: 0.09267091, Validation loss: 0.09210477, Gradient norm: 1.32843756
INFO:root:[  130] Training loss: 0.09235495, Validation loss: 0.09163065, Gradient norm: 1.37919893
INFO:root:[  131] Training loss: 0.09157329, Validation loss: 0.09168655, Gradient norm: 0.91323268
INFO:root:[  132] Training loss: 0.09129682, Validation loss: 0.09144387, Gradient norm: 1.33660277
INFO:root:[  133] Training loss: 0.09075824, Validation loss: 0.09154036, Gradient norm: 1.42038734
INFO:root:[  134] Training loss: 0.09020433, Validation loss: 0.09075766, Gradient norm: 1.21864415
INFO:root:[  135] Training loss: 0.08991542, Validation loss: 0.09027300, Gradient norm: 1.58317544
INFO:root:[  136] Training loss: 0.08941675, Validation loss: 0.08921436, Gradient norm: 1.45183613
INFO:root:[  137] Training loss: 0.08879882, Validation loss: 0.08812212, Gradient norm: 1.24605105
INFO:root:[  138] Training loss: 0.08819826, Validation loss: 0.08890079, Gradient norm: 1.17392160
INFO:root:[  139] Training loss: 0.08832011, Validation loss: 0.08723698, Gradient norm: 1.76434594
INFO:root:[  140] Training loss: 0.08731916, Validation loss: 0.08750910, Gradient norm: 1.17794585
INFO:root:[  141] Training loss: 0.08709973, Validation loss: 0.08639540, Gradient norm: 1.56792164
INFO:root:[  142] Training loss: 0.08640108, Validation loss: 0.08581660, Gradient norm: 1.15799574
INFO:root:[  143] Training loss: 0.08597052, Validation loss: 0.08661292, Gradient norm: 1.33348560
INFO:root:[  144] Training loss: 0.08569380, Validation loss: 0.08526532, Gradient norm: 1.56152421
INFO:root:[  145] Training loss: 0.08512255, Validation loss: 0.08460530, Gradient norm: 1.22557670
INFO:root:[  146] Training loss: 0.08470391, Validation loss: 0.08397435, Gradient norm: 1.55050416
INFO:root:[  147] Training loss: 0.08409709, Validation loss: 0.08397939, Gradient norm: 1.30197557
INFO:root:[  148] Training loss: 0.08346327, Validation loss: 0.08313157, Gradient norm: 1.11947431
INFO:root:[  149] Training loss: 0.08352036, Validation loss: 0.08295477, Gradient norm: 1.97939942
INFO:root:[  150] Training loss: 0.08289717, Validation loss: 0.08512503, Gradient norm: 1.59175103
INFO:root:[  151] Training loss: 0.08260813, Validation loss: 0.08283415, Gradient norm: 2.07851567
INFO:root:[  152] Training loss: 0.08192287, Validation loss: 0.08183591, Gradient norm: 1.52686644
INFO:root:[  153] Training loss: 0.08179554, Validation loss: 0.08100765, Gradient norm: 1.83424964
INFO:root:[  154] Training loss: 0.08124743, Validation loss: 0.08107553, Gradient norm: 1.82550269
INFO:root:[  155] Training loss: 0.08082100, Validation loss: 0.08072351, Gradient norm: 1.81938010
INFO:root:[  156] Training loss: 0.08031499, Validation loss: 0.08010317, Gradient norm: 1.67184150
INFO:root:[  157] Training loss: 0.08004696, Validation loss: 0.07966615, Gradient norm: 1.89073004
INFO:root:[  158] Training loss: 0.07939388, Validation loss: 0.08014605, Gradient norm: 1.30366287
INFO:root:[  159] Training loss: 0.07897582, Validation loss: 0.07917327, Gradient norm: 1.66084266
INFO:root:[  160] Training loss: 0.07839791, Validation loss: 0.07844693, Gradient norm: 1.26259117
INFO:root:[  161] Training loss: 0.07808507, Validation loss: 0.07808405, Gradient norm: 1.46979037
INFO:root:[  162] Training loss: 0.07776524, Validation loss: 0.07776356, Gradient norm: 1.50735654
INFO:root:[  163] Training loss: 0.07723429, Validation loss: 0.07755142, Gradient norm: 1.41182817
INFO:root:[  164] Training loss: 0.07672448, Validation loss: 0.07631960, Gradient norm: 1.06906723
INFO:root:[  165] Training loss: 0.07636615, Validation loss: 0.07652259, Gradient norm: 1.35467846
INFO:root:[  166] Training loss: 0.07606884, Validation loss: 0.07601285, Gradient norm: 1.81912026
INFO:root:[  167] Training loss: 0.07549887, Validation loss: 0.07574104, Gradient norm: 1.41054959
INFO:root:[  168] Training loss: 0.07487075, Validation loss: 0.07575926, Gradient norm: 1.17249897
INFO:root:[  169] Training loss: 0.07495747, Validation loss: 0.07536133, Gradient norm: 2.35036994
INFO:root:[  170] Training loss: 0.07454431, Validation loss: 0.07414985, Gradient norm: 2.21498601
INFO:root:[  171] Training loss: 0.07409226, Validation loss: 0.07370028, Gradient norm: 1.95834883
INFO:root:[  172] Training loss: 0.07363185, Validation loss: 0.07394466, Gradient norm: 2.06753821
INFO:root:[  173] Training loss: 0.07333111, Validation loss: 0.07261352, Gradient norm: 1.84083231
INFO:root:[  174] Training loss: 0.07288648, Validation loss: 0.07225657, Gradient norm: 1.87372786
INFO:root:[  175] Training loss: 0.07256138, Validation loss: 0.07226764, Gradient norm: 2.23391910
INFO:root:[  176] Training loss: 0.07203827, Validation loss: 0.07194255, Gradient norm: 1.56756058
INFO:root:[  177] Training loss: 0.07155993, Validation loss: 0.07140418, Gradient norm: 1.60801886
INFO:root:[  178] Training loss: 0.07137594, Validation loss: 0.07112421, Gradient norm: 1.77653577
INFO:root:[  179] Training loss: 0.07124495, Validation loss: 0.07082410, Gradient norm: 2.61032025
INFO:root:[  180] Training loss: 0.07056207, Validation loss: 0.07086624, Gradient norm: 1.77455154
INFO:root:[  181] Training loss: 0.07041043, Validation loss: 0.07114323, Gradient norm: 2.13147185
INFO:root:[  182] Training loss: 0.07022801, Validation loss: 0.06992460, Gradient norm: 2.48622203
INFO:root:[  183] Training loss: 0.06964822, Validation loss: 0.06921208, Gradient norm: 2.14804883
INFO:root:[  184] Training loss: 0.06947942, Validation loss: 0.06899418, Gradient norm: 2.09895167
INFO:root:[  185] Training loss: 0.06934700, Validation loss: 0.06862297, Gradient norm: 2.90884228
INFO:root:[  186] Training loss: 0.06849447, Validation loss: 0.06879114, Gradient norm: 1.53149372
INFO:root:[  187] Training loss: 0.06828484, Validation loss: 0.06838333, Gradient norm: 2.19680805
INFO:root:[  188] Training loss: 0.06793277, Validation loss: 0.06838617, Gradient norm: 2.18998020
INFO:root:[  189] Training loss: 0.06810800, Validation loss: 0.06886019, Gradient norm: 2.97382239
INFO:root:[  190] Training loss: 0.06752709, Validation loss: 0.06753591, Gradient norm: 2.90468365
INFO:root:[  191] Training loss: 0.06703376, Validation loss: 0.06702071, Gradient norm: 2.10407169
INFO:root:[  192] Training loss: 0.06662788, Validation loss: 0.06605051, Gradient norm: 2.08886976
INFO:root:[  193] Training loss: 0.06629554, Validation loss: 0.06684366, Gradient norm: 2.21881616
INFO:root:[  194] Training loss: 0.06592704, Validation loss: 0.06610168, Gradient norm: 2.17902032
INFO:root:[  195] Training loss: 0.06568180, Validation loss: 0.06681095, Gradient norm: 2.09422599
INFO:root:[  196] Training loss: 0.06550974, Validation loss: 0.06520780, Gradient norm: 2.45037876
INFO:root:[  197] Training loss: 0.06490868, Validation loss: 0.06524556, Gradient norm: 1.63589722
INFO:root:[  198] Training loss: 0.06470079, Validation loss: 0.06508700, Gradient norm: 1.99267593
INFO:root:[  199] Training loss: 0.06432339, Validation loss: 0.06471110, Gradient norm: 2.11628640
INFO:root:[  200] Training loss: 0.06413984, Validation loss: 0.06432909, Gradient norm: 2.58177639
INFO:root:[  201] Training loss: 0.06393784, Validation loss: 0.06429192, Gradient norm: 2.69870188
INFO:root:[  202] Training loss: 0.06369559, Validation loss: 0.06329856, Gradient norm: 2.83666397
INFO:root:[  203] Training loss: 0.06342540, Validation loss: 0.06286955, Gradient norm: 2.95091558
INFO:root:[  204] Training loss: 0.06289955, Validation loss: 0.06309592, Gradient norm: 2.73110981
INFO:root:[  205] Training loss: 0.06263347, Validation loss: 0.06265821, Gradient norm: 2.44896624
INFO:root:[  206] Training loss: 0.06230197, Validation loss: 0.06193782, Gradient norm: 2.55409723
INFO:root:[  207] Training loss: 0.06212054, Validation loss: 0.06217394, Gradient norm: 2.25136525
INFO:root:[  208] Training loss: 0.06191144, Validation loss: 0.06226602, Gradient norm: 3.07239370
INFO:root:[  209] Training loss: 0.06168119, Validation loss: 0.06153745, Gradient norm: 2.97162958
INFO:root:[  210] Training loss: 0.06108852, Validation loss: 0.06062998, Gradient norm: 2.39403898
INFO:root:[  211] Training loss: 0.06115331, Validation loss: 0.06137767, Gradient norm: 3.66464834
INFO:root:[  212] Training loss: 0.06055162, Validation loss: 0.06102904, Gradient norm: 2.23687783
INFO:root:[  213] Training loss: 0.06035310, Validation loss: 0.06023580, Gradient norm: 2.62505704
INFO:root:[  214] Training loss: 0.06024714, Validation loss: 0.06010005, Gradient norm: 3.07735396
INFO:root:[  215] Training loss: 0.06005921, Validation loss: 0.06035401, Gradient norm: 2.97470641
INFO:root:[  216] Training loss: 0.05972174, Validation loss: 0.05917357, Gradient norm: 2.86098593
INFO:root:[  217] Training loss: 0.05955085, Validation loss: 0.05945745, Gradient norm: 3.71443293
INFO:root:[  218] Training loss: 0.05909749, Validation loss: 0.05895032, Gradient norm: 2.75008328
INFO:root:[  219] Training loss: 0.05900144, Validation loss: 0.05870335, Gradient norm: 3.42474993
INFO:root:[  220] Training loss: 0.05875805, Validation loss: 0.05908712, Gradient norm: 3.54987817
INFO:root:[  221] Training loss: 0.05879985, Validation loss: 0.05843631, Gradient norm: 4.36336450
INFO:root:[  222] Training loss: 0.05793882, Validation loss: 0.05806238, Gradient norm: 2.11355514
INFO:root:[  223] Training loss: 0.05802511, Validation loss: 0.05766012, Gradient norm: 3.49752624
INFO:root:[  224] Training loss: 0.05767657, Validation loss: 0.05778107, Gradient norm: 3.09248496
INFO:root:[  225] Training loss: 0.05742553, Validation loss: 0.05777881, Gradient norm: 3.34477369
INFO:root:[  226] Training loss: 0.05720069, Validation loss: 0.05766707, Gradient norm: 3.72416354
INFO:root:[  227] Training loss: 0.05684949, Validation loss: 0.05623473, Gradient norm: 2.69422783
INFO:root:[  228] Training loss: 0.05672371, Validation loss: 0.05654461, Gradient norm: 3.24615320
INFO:root:[  229] Training loss: 0.05655895, Validation loss: 0.05697309, Gradient norm: 3.74619366
INFO:root:[  230] Training loss: 0.05641234, Validation loss: 0.05703848, Gradient norm: 3.70486274
INFO:root:[  231] Training loss: 0.05621760, Validation loss: 0.05605935, Gradient norm: 4.09614667
INFO:root:[  232] Training loss: 0.05567414, Validation loss: 0.05585421, Gradient norm: 3.13585602
INFO:root:[  233] Training loss: 0.05587723, Validation loss: 0.05564007, Gradient norm: 4.78324267
INFO:root:[  234] Training loss: 0.05535808, Validation loss: 0.05531234, Gradient norm: 3.68595667
INFO:root:[  235] Training loss: 0.05523983, Validation loss: 0.05513137, Gradient norm: 4.14340583
INFO:root:[  236] Training loss: 0.05502504, Validation loss: 0.05434601, Gradient norm: 4.46392809
INFO:root:[  237] Training loss: 0.05476640, Validation loss: 0.05543838, Gradient norm: 3.78553236
INFO:root:[  238] Training loss: 0.05495148, Validation loss: 0.05558446, Gradient norm: 5.36267073
INFO:root:[  239] Training loss: 0.05472510, Validation loss: 0.05447849, Gradient norm: 5.06183297
INFO:root:[  240] Training loss: 0.05407209, Validation loss: 0.05390515, Gradient norm: 4.23234250
INFO:root:[  241] Training loss: 0.05383917, Validation loss: 0.05395662, Gradient norm: 3.64917290
INFO:root:[  242] Training loss: 0.05379375, Validation loss: 0.05362939, Gradient norm: 4.45552051
INFO:root:[  243] Training loss: 0.05352889, Validation loss: 0.05335391, Gradient norm: 3.50016514
INFO:root:[  244] Training loss: 0.05355090, Validation loss: 0.05356308, Gradient norm: 5.37653447
INFO:root:[  245] Training loss: 0.05296499, Validation loss: 0.05377077, Gradient norm: 3.47407080
INFO:root:[  246] Training loss: 0.05295776, Validation loss: 0.05315143, Gradient norm: 3.81562737
INFO:root:[  247] Training loss: 0.05261474, Validation loss: 0.05241950, Gradient norm: 3.98069770
INFO:root:[  248] Training loss: 0.05293874, Validation loss: 0.05309436, Gradient norm: 6.38528287
INFO:root:[  249] Training loss: 0.05243087, Validation loss: 0.05160824, Gradient norm: 4.69417768
INFO:root:[  250] Training loss: 0.05195255, Validation loss: 0.05246945, Gradient norm: 3.75833493
INFO:root:[  251] Training loss: 0.05226419, Validation loss: 0.05192757, Gradient norm: 5.90787639
INFO:root:[  252] Training loss: 0.05182290, Validation loss: 0.05177026, Gradient norm: 4.30429680
INFO:root:[  253] Training loss: 0.05170752, Validation loss: 0.05150643, Gradient norm: 5.75617340
INFO:root:[  254] Training loss: 0.05123871, Validation loss: 0.05119340, Gradient norm: 3.29160534
INFO:root:[  255] Training loss: 0.05134139, Validation loss: 0.05220535, Gradient norm: 4.80200029
INFO:root:[  256] Training loss: 0.05116381, Validation loss: 0.05064208, Gradient norm: 5.60029970
INFO:root:[  257] Training loss: 0.05096016, Validation loss: 0.05089469, Gradient norm: 5.40578795
INFO:root:[  258] Training loss: 0.05063364, Validation loss: 0.05038818, Gradient norm: 5.00350959
INFO:root:[  259] Training loss: 0.05081554, Validation loss: 0.05017616, Gradient norm: 6.46061357
INFO:root:[  260] Training loss: 0.05076892, Validation loss: 0.05005888, Gradient norm: 6.14404372
INFO:root:[  261] Training loss: 0.05009350, Validation loss: 0.04995966, Gradient norm: 4.06075478
INFO:root:[  262] Training loss: 0.04995000, Validation loss: 0.05084727, Gradient norm: 4.55479285
INFO:root:[  263] Training loss: 0.05022875, Validation loss: 0.04966113, Gradient norm: 6.90281304
INFO:root:[  264] Training loss: 0.04979935, Validation loss: 0.04995859, Gradient norm: 5.96296292
INFO:root:[  265] Training loss: 0.04981707, Validation loss: 0.04932057, Gradient norm: 6.34326277
INFO:root:[  266] Training loss: 0.04926071, Validation loss: 0.04921664, Gradient norm: 4.60018299
INFO:root:[  267] Training loss: 0.04920608, Validation loss: 0.04981176, Gradient norm: 4.48533099
INFO:root:[  268] Training loss: 0.04900623, Validation loss: 0.04881298, Gradient norm: 5.36766381
INFO:root:[  269] Training loss: 0.04907033, Validation loss: 0.05027682, Gradient norm: 5.63110217
INFO:root:[  270] Training loss: 0.04867989, Validation loss: 0.04943334, Gradient norm: 5.64296031
INFO:root:[  271] Training loss: 0.04869660, Validation loss: 0.05044401, Gradient norm: 5.87508587
INFO:root:[  272] Training loss: 0.04897361, Validation loss: 0.04844621, Gradient norm: 8.44868583
INFO:root:[  273] Training loss: 0.04834402, Validation loss: 0.04815488, Gradient norm: 5.91066047
INFO:root:[  274] Training loss: 0.04805443, Validation loss: 0.04805451, Gradient norm: 4.56292000
INFO:root:[  275] Training loss: 0.04800300, Validation loss: 0.04808088, Gradient norm: 5.30578481
INFO:root:[  276] Training loss: 0.04806582, Validation loss: 0.04792120, Gradient norm: 6.37680367
INFO:root:[  277] Training loss: 0.04822557, Validation loss: 0.04849784, Gradient norm: 9.46917440
INFO:root:[  278] Training loss: 0.04805991, Validation loss: 0.04769051, Gradient norm: 10.15204192
INFO:root:[  279] Training loss: 0.04728759, Validation loss: 0.04725092, Gradient norm: 5.81396333
INFO:root:[  280] Training loss: 0.04695279, Validation loss: 0.04672364, Gradient norm: 4.23962254
INFO:root:[  281] Training loss: 0.04697633, Validation loss: 0.04736466, Gradient norm: 5.39951281
INFO:root:[  282] Training loss: 0.04690040, Validation loss: 0.04683639, Gradient norm: 4.85669798
INFO:root:[  283] Training loss: 0.04692398, Validation loss: 0.04635716, Gradient norm: 6.55204230
INFO:root:[  284] Training loss: 0.04663817, Validation loss: 0.04705704, Gradient norm: 6.73042082
INFO:root:[  285] Training loss: 0.04653183, Validation loss: 0.04824050, Gradient norm: 6.26157096
INFO:root:[  286] Training loss: 0.04653402, Validation loss: 0.04593140, Gradient norm: 7.57075054
INFO:root:[  287] Training loss: 0.04637994, Validation loss: 0.04606986, Gradient norm: 6.97566515
INFO:root:[  288] Training loss: 0.04627328, Validation loss: 0.04520558, Gradient norm: 7.34573654
INFO:root:[  289] Training loss: 0.04583711, Validation loss: 0.04572020, Gradient norm: 5.92689766
INFO:root:[  290] Training loss: 0.04635605, Validation loss: 0.04540223, Gradient norm: 9.06523387
INFO:root:[  291] Training loss: 0.04605404, Validation loss: 0.04578325, Gradient norm: 8.43445545
INFO:root:[  292] Training loss: 0.04551614, Validation loss: 0.04577728, Gradient norm: 6.56204327
INFO:root:[  293] Training loss: 0.04521413, Validation loss: 0.04543337, Gradient norm: 6.18346004
INFO:root:[  294] Training loss: 0.04528482, Validation loss: 0.04587690, Gradient norm: 5.93013275
INFO:root:[  295] Training loss: 0.04537552, Validation loss: 0.04510011, Gradient norm: 8.04557977
INFO:root:[  296] Training loss: 0.04540005, Validation loss: 0.04474571, Gradient norm: 9.19762516
INFO:root:[  297] Training loss: 0.04507259, Validation loss: 0.04449568, Gradient norm: 7.92312191
INFO:root:[  298] Training loss: 0.04473008, Validation loss: 0.04470219, Gradient norm: 6.86273394
INFO:root:[  299] Training loss: 0.04492900, Validation loss: 0.04647810, Gradient norm: 9.11567507
INFO:root:[  300] Training loss: 0.04454672, Validation loss: 0.04453979, Gradient norm: 8.01543438
INFO:root:[  301] Training loss: 0.04480258, Validation loss: 0.04455458, Gradient norm: 8.51394538
INFO:root:[  302] Training loss: 0.04464832, Validation loss: 0.04487093, Gradient norm: 8.91617686
INFO:root:[  303] Training loss: 0.04432885, Validation loss: 0.04419491, Gradient norm: 8.41473052
INFO:root:[  304] Training loss: 0.04393015, Validation loss: 0.04365078, Gradient norm: 7.06319655
INFO:root:[  305] Training loss: 0.04402492, Validation loss: 0.04577240, Gradient norm: 9.05715816
INFO:root:[  306] Training loss: 0.04431678, Validation loss: 0.04352411, Gradient norm: 11.49049145
INFO:root:[  307] Training loss: 0.04357438, Validation loss: 0.04394664, Gradient norm: 6.21704538
INFO:root:[  308] Training loss: 0.04370649, Validation loss: 0.04393942, Gradient norm: 9.84400946
INFO:root:[  309] Training loss: 0.04404248, Validation loss: 0.04435974, Gradient norm: 10.67799116
INFO:root:[  310] Training loss: 0.04339302, Validation loss: 0.04373865, Gradient norm: 7.44151092
INFO:root:[  311] Training loss: 0.04373903, Validation loss: 0.04322563, Gradient norm: 10.17805012
INFO:root:[  312] Training loss: 0.04323505, Validation loss: 0.04272191, Gradient norm: 7.91628896
INFO:root:[  313] Training loss: 0.04284018, Validation loss: 0.04259433, Gradient norm: 6.84342996
INFO:root:[  314] Training loss: 0.04330831, Validation loss: 0.04280124, Gradient norm: 12.31624826
INFO:root:[  315] Training loss: 0.04311574, Validation loss: 0.04270912, Gradient norm: 10.92211979
INFO:root:[  316] Training loss: 0.04325933, Validation loss: 0.04243101, Gradient norm: 13.26054631
INFO:root:[  317] Training loss: 0.04305382, Validation loss: 0.04284106, Gradient norm: 10.58284252
INFO:root:[  318] Training loss: 0.04294797, Validation loss: 0.04343708, Gradient norm: 12.22700917
INFO:root:[  319] Training loss: 0.04243521, Validation loss: 0.04333583, Gradient norm: 7.47755935
INFO:root:[  320] Training loss: 0.04255522, Validation loss: 0.04246341, Gradient norm: 10.78902777
INFO:root:[  321] Training loss: 0.04244294, Validation loss: 0.04236067, Gradient norm: 9.51163308
INFO:root:[  322] Training loss: 0.04230188, Validation loss: 0.04238662, Gradient norm: 9.38806679
INFO:root:[  323] Training loss: 0.04246223, Validation loss: 0.04154393, Gradient norm: 12.73196013
INFO:root:[  324] Training loss: 0.04197949, Validation loss: 0.04253654, Gradient norm: 9.34982492
INFO:root:[  325] Training loss: 0.04208330, Validation loss: 0.04190698, Gradient norm: 11.35423033
INFO:root:[  326] Training loss: 0.04169977, Validation loss: 0.04222874, Gradient norm: 7.43931888
INFO:root:[  327] Training loss: 0.04198101, Validation loss: 0.04358332, Gradient norm: 12.92785909
INFO:root:[  328] Training loss: 0.04184636, Validation loss: 0.04242734, Gradient norm: 11.34765213
INFO:root:[  329] Training loss: 0.04175955, Validation loss: 0.04134372, Gradient norm: 11.67929000
INFO:root:[  330] Training loss: 0.04164904, Validation loss: 0.04108097, Gradient norm: 13.89000985
INFO:root:[  331] Training loss: 0.04133771, Validation loss: 0.04153554, Gradient norm: 10.11427075
INFO:root:[  332] Training loss: 0.04217248, Validation loss: 0.04116957, Gradient norm: 15.53563616
INFO:root:[  333] Training loss: 0.04126701, Validation loss: 0.04106772, Gradient norm: 11.00617473
INFO:root:[  334] Training loss: 0.04098013, Validation loss: 0.04106296, Gradient norm: 8.38150762
INFO:root:[  335] Training loss: 0.04102699, Validation loss: 0.04057318, Gradient norm: 9.55171141
INFO:root:[  336] Training loss: 0.04187466, Validation loss: 0.04306892, Gradient norm: 16.66961379
INFO:root:[  337] Training loss: 0.04110668, Validation loss: 0.04136812, Gradient norm: 13.29320241
INFO:root:[  338] Training loss: 0.04078238, Validation loss: 0.04065435, Gradient norm: 10.81699228
INFO:root:[  339] Training loss: 0.04134762, Validation loss: 0.04272731, Gradient norm: 14.74450419
INFO:root:[  340] Training loss: 0.04120646, Validation loss: 0.04029068, Gradient norm: 16.34289254
INFO:root:[  341] Training loss: 0.04058616, Validation loss: 0.04050052, Gradient norm: 10.67186946
INFO:root:[  342] Training loss: 0.04058646, Validation loss: 0.04202868, Gradient norm: 11.55957995
INFO:root:[  343] Training loss: 0.04067325, Validation loss: 0.04232326, Gradient norm: 14.03305757
INFO:root:[  344] Training loss: 0.04050155, Validation loss: 0.03968606, Gradient norm: 11.96891791
INFO:root:[  345] Training loss: 0.04054218, Validation loss: 0.03973195, Gradient norm: 15.50950738
INFO:root:[  346] Training loss: 0.04014046, Validation loss: 0.04079806, Gradient norm: 12.37940642
INFO:root:[  347] Training loss: 0.04017924, Validation loss: 0.04080066, Gradient norm: 13.95427543
INFO:root:[  348] Training loss: 0.04028348, Validation loss: 0.03950269, Gradient norm: 15.83691941
INFO:root:[  349] Training loss: 0.04052376, Validation loss: 0.03976006, Gradient norm: 17.21784436
INFO:root:[  350] Training loss: 0.03986057, Validation loss: 0.03981149, Gradient norm: 10.29439714
INFO:root:[  351] Training loss: 0.03964261, Validation loss: 0.04365530, Gradient norm: 10.97188934
INFO:root:[  352] Training loss: 0.04091760, Validation loss: 0.03924356, Gradient norm: 19.33848330
INFO:root:[  353] Training loss: 0.03948537, Validation loss: 0.03976765, Gradient norm: 10.81881014
INFO:root:[  354] Training loss: 0.04027549, Validation loss: 0.04219524, Gradient norm: 19.57260632
INFO:root:[  355] Training loss: 0.04007252, Validation loss: 0.04099601, Gradient norm: 17.81888281
INFO:root:[  356] Training loss: 0.03965863, Validation loss: 0.03995946, Gradient norm: 15.12430407
INFO:root:[  357] Training loss: 0.03988505, Validation loss: 0.04136747, Gradient norm: 16.58072273
INFO:root:[  358] Training loss: 0.03966880, Validation loss: 0.03855886, Gradient norm: 15.35168967
INFO:root:[  359] Training loss: 0.03930020, Validation loss: 0.03887593, Gradient norm: 12.85324586
INFO:root:[  360] Training loss: 0.03902387, Validation loss: 0.03871810, Gradient norm: 13.48081549
INFO:root:[  361] Training loss: 0.03897887, Validation loss: 0.04089132, Gradient norm: 12.30945199
INFO:root:[  362] Training loss: 0.03938668, Validation loss: 0.03930370, Gradient norm: 15.86332988
INFO:root:[  363] Training loss: 0.03883624, Validation loss: 0.03830216, Gradient norm: 13.54751392
INFO:root:[  364] Training loss: 0.03903005, Validation loss: 0.03907318, Gradient norm: 14.98936751
INFO:root:[  365] Training loss: 0.03898949, Validation loss: 0.03894872, Gradient norm: 17.52092555
INFO:root:[  366] Training loss: 0.03938589, Validation loss: 0.03987718, Gradient norm: 19.19279646
INFO:root:[  367] Training loss: 0.03989843, Validation loss: 0.03890202, Gradient norm: 21.86306808
INFO:root:[  368] Training loss: 0.03892161, Validation loss: 0.03888827, Gradient norm: 17.03472597
INFO:root:[  369] Training loss: 0.03854203, Validation loss: 0.03842916, Gradient norm: 14.18231721
INFO:root:[  370] Training loss: 0.03840729, Validation loss: 0.03837265, Gradient norm: 14.05356146
INFO:root:[  371] Training loss: 0.03827532, Validation loss: 0.03886589, Gradient norm: 14.07133215
INFO:root:[  372] Training loss: 0.03937020, Validation loss: 0.03972691, Gradient norm: 22.72716644
INFO:root:[  373] Training loss: 0.03838017, Validation loss: 0.03812563, Gradient norm: 15.82811918
INFO:root:[  374] Training loss: 0.03840419, Validation loss: 0.03954309, Gradient norm: 15.17327543
INFO:root:[  375] Training loss: 0.03903763, Validation loss: 0.04011957, Gradient norm: 20.49598948
INFO:root:[  376] Training loss: 0.03877872, Validation loss: 0.03945704, Gradient norm: 19.28796536
INFO:root:[  377] Training loss: 0.03824257, Validation loss: 0.03784837, Gradient norm: 16.50232850
INFO:root:[  378] Training loss: 0.03807145, Validation loss: 0.03905388, Gradient norm: 15.43049281
INFO:root:[  379] Training loss: 0.03815237, Validation loss: 0.03754292, Gradient norm: 18.08311535
INFO:root:[  380] Training loss: 0.03791461, Validation loss: 0.03744043, Gradient norm: 18.62937797
INFO:root:[  381] Training loss: 0.03815260, Validation loss: 0.03717326, Gradient norm: 16.61645524
INFO:root:[  382] Training loss: 0.03783840, Validation loss: 0.03854375, Gradient norm: 17.08778330
INFO:root:[  383] Training loss: 0.03765033, Validation loss: 0.03832965, Gradient norm: 13.89967590
INFO:root:[  384] Training loss: 0.03886995, Validation loss: 0.03784485, Gradient norm: 26.04357363
INFO:root:[  385] Training loss: 0.03793109, Validation loss: 0.03776301, Gradient norm: 17.64418986
INFO:root:[  386] Training loss: 0.03766730, Validation loss: 0.03961552, Gradient norm: 17.30420546
INFO:root:[  387] Training loss: 0.03784832, Validation loss: 0.03861154, Gradient norm: 20.99385802
INFO:root:[  388] Training loss: 0.03775069, Validation loss: 0.03806126, Gradient norm: 19.00486227
INFO:root:[  389] Training loss: 0.03806676, Validation loss: 0.03886519, Gradient norm: 22.66796905
INFO:root:[  390] Training loss: 0.03794641, Validation loss: 0.03628034, Gradient norm: 21.95061514
INFO:root:[  391] Training loss: 0.03708611, Validation loss: 0.03964302, Gradient norm: 15.81488716
INFO:root:[  392] Training loss: 0.03749706, Validation loss: 0.03684538, Gradient norm: 20.97345573
INFO:root:[  393] Training loss: 0.03739961, Validation loss: 0.03657013, Gradient norm: 19.04515938
INFO:root:[  394] Training loss: 0.03736252, Validation loss: 0.03762894, Gradient norm: 17.58467203
INFO:root:[  395] Training loss: 0.03758385, Validation loss: 0.03925271, Gradient norm: 22.55994444
INFO:root:[  396] Training loss: 0.03754298, Validation loss: 0.03916863, Gradient norm: 21.88154271
INFO:root:[  397] Training loss: 0.03709977, Validation loss: 0.03668113, Gradient norm: 18.55681091
INFO:root:[  398] Training loss: 0.03725181, Validation loss: 0.03901582, Gradient norm: 21.68151730
INFO:root:[  399] Training loss: 0.03757756, Validation loss: 0.03673528, Gradient norm: 23.72080739
INFO:root:EP 399: Early stopping
INFO:root:Training the model took 4621.236s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.28824
INFO:root:EnergyScoreTrain: 0.20556
INFO:root:CoverageTrain: 0.98989
INFO:root:IntervalWidthTrain: 0.02288
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.27157
INFO:root:EnergyScoreValidation: 0.19351
INFO:root:CoverageValidation: 0.98962
INFO:root:IntervalWidthValidation: 0.023
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.25389
INFO:root:EnergyScoreTest: 0.18085
INFO:root:CoverageTest: 0.99001
INFO:root:IntervalWidthTest: 0.02246
INFO:root:###19 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.2, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.84820392, Validation loss: 2.34882222, Gradient norm: 5.05900893
INFO:root:[    2] Training loss: 1.44563908, Validation loss: 0.84000226, Gradient norm: 5.04652124
INFO:root:[    3] Training loss: 0.72474875, Validation loss: 0.60367171, Gradient norm: 2.28126108
INFO:root:[    4] Training loss: 0.57271425, Validation loss: 0.49952176, Gradient norm: 1.98415497
INFO:root:[    5] Training loss: 0.46420225, Validation loss: 0.43177589, Gradient norm: 1.55196548
INFO:root:[    6] Training loss: 0.39743253, Validation loss: 0.36579703, Gradient norm: 1.50429298
INFO:root:[    7] Training loss: 0.35375421, Validation loss: 0.32529383, Gradient norm: 1.44062734
INFO:root:[    8] Training loss: 0.31584871, Validation loss: 0.29836267, Gradient norm: 1.01422240
INFO:root:[    9] Training loss: 0.28719887, Validation loss: 0.28280638, Gradient norm: 1.14048723
INFO:root:[   10] Training loss: 0.27121440, Validation loss: 0.26158390, Gradient norm: 1.08926722
INFO:root:[   11] Training loss: 0.25657714, Validation loss: 0.24919952, Gradient norm: 0.87690579
INFO:root:[   12] Training loss: 0.24417271, Validation loss: 0.23992644, Gradient norm: 0.79087613
INFO:root:[   13] Training loss: 0.23486684, Validation loss: 0.23387556, Gradient norm: 0.56993661
INFO:root:[   14] Training loss: 0.22775636, Validation loss: 0.22425495, Gradient norm: 0.56811850
INFO:root:[   15] Training loss: 0.22125456, Validation loss: 0.21900983, Gradient norm: 0.49467630
INFO:root:[   16] Training loss: 0.21532508, Validation loss: 0.21256178, Gradient norm: 0.53112392
INFO:root:[   17] Training loss: 0.21030773, Validation loss: 0.20967553, Gradient norm: 0.54355106
INFO:root:[   18] Training loss: 0.20600516, Validation loss: 0.20541252, Gradient norm: 0.63144775
INFO:root:[   19] Training loss: 0.20129459, Validation loss: 0.19978142, Gradient norm: 0.48074572
INFO:root:[   20] Training loss: 0.19809130, Validation loss: 0.19816050, Gradient norm: 0.47094114
INFO:root:[   21] Training loss: 0.19505216, Validation loss: 0.19200346, Gradient norm: 0.59803887
INFO:root:[   22] Training loss: 0.19139708, Validation loss: 0.18977602, Gradient norm: 0.48443211
INFO:root:[   23] Training loss: 0.18804441, Validation loss: 0.18960199, Gradient norm: 0.52590196
INFO:root:[   24] Training loss: 0.18602006, Validation loss: 0.18474893, Gradient norm: 0.55141875
INFO:root:[   25] Training loss: 0.18378082, Validation loss: 0.18312436, Gradient norm: 0.47122927
INFO:root:[   26] Training loss: 0.18166331, Validation loss: 0.17948896, Gradient norm: 0.42976113
INFO:root:[   27] Training loss: 0.17983914, Validation loss: 0.17971539, Gradient norm: 0.49543877
INFO:root:[   28] Training loss: 0.17764433, Validation loss: 0.17751856, Gradient norm: 0.36606386
INFO:root:[   29] Training loss: 0.17618457, Validation loss: 0.17606763, Gradient norm: 0.49421041
INFO:root:[   30] Training loss: 0.17432971, Validation loss: 0.17499292, Gradient norm: 0.45689603
INFO:root:[   31] Training loss: 0.17316524, Validation loss: 0.17194583, Gradient norm: 0.47171503
INFO:root:[   32] Training loss: 0.17174671, Validation loss: 0.17119004, Gradient norm: 0.48393454
INFO:root:[   33] Training loss: 0.17006991, Validation loss: 0.16937264, Gradient norm: 0.37080976
INFO:root:[   34] Training loss: 0.16823196, Validation loss: 0.16874368, Gradient norm: 0.36603520
INFO:root:[   35] Training loss: 0.16718973, Validation loss: 0.16810206, Gradient norm: 0.65815895
INFO:root:[   36] Training loss: 0.16557690, Validation loss: 0.16532715, Gradient norm: 0.34965574
INFO:root:[   37] Training loss: 0.16450830, Validation loss: 0.16584249, Gradient norm: 0.53301883
INFO:root:[   38] Training loss: 0.16353643, Validation loss: 0.16354214, Gradient norm: 0.66772450
INFO:root:[   39] Training loss: 0.16188476, Validation loss: 0.16133517, Gradient norm: 0.46061751
INFO:root:[   40] Training loss: 0.16075023, Validation loss: 0.16067400, Gradient norm: 0.48325619
INFO:root:[   41] Training loss: 0.15976694, Validation loss: 0.16013090, Gradient norm: 0.64168357
INFO:root:[   42] Training loss: 0.15840772, Validation loss: 0.15754024, Gradient norm: 0.71311745
INFO:root:[   43] Training loss: 0.15720055, Validation loss: 0.15556432, Gradient norm: 0.64073071
INFO:root:[   44] Training loss: 0.15627672, Validation loss: 0.15625435, Gradient norm: 0.64921743
INFO:root:[   45] Training loss: 0.15464388, Validation loss: 0.15436306, Gradient norm: 0.55340600
INFO:root:[   46] Training loss: 0.15378631, Validation loss: 0.15317055, Gradient norm: 0.62809513
INFO:root:[   47] Training loss: 0.15262464, Validation loss: 0.15186743, Gradient norm: 0.41427319
INFO:root:[   48] Training loss: 0.15180089, Validation loss: 0.15175030, Gradient norm: 0.69779714
INFO:root:[   49] Training loss: 0.15068447, Validation loss: 0.15017222, Gradient norm: 0.76204353
INFO:root:[   50] Training loss: 0.14992320, Validation loss: 0.15004436, Gradient norm: 0.78781391
INFO:root:[   51] Training loss: 0.14858944, Validation loss: 0.14894055, Gradient norm: 0.59869502
INFO:root:[   52] Training loss: 0.14769240, Validation loss: 0.14768345, Gradient norm: 0.74030020
INFO:root:[   53] Training loss: 0.14667677, Validation loss: 0.14627972, Gradient norm: 0.53680758
INFO:root:[   54] Training loss: 0.14626151, Validation loss: 0.14532568, Gradient norm: 0.78980843
INFO:root:[   55] Training loss: 0.14534761, Validation loss: 0.14404816, Gradient norm: 0.92581786
INFO:root:[   56] Training loss: 0.14357247, Validation loss: 0.14381191, Gradient norm: 0.45399583
INFO:root:[   57] Training loss: 0.14283027, Validation loss: 0.14389879, Gradient norm: 0.41983978
INFO:root:[   58] Training loss: 0.14256414, Validation loss: 0.14172437, Gradient norm: 0.95824232
INFO:root:[   59] Training loss: 0.14110739, Validation loss: 0.14034654, Gradient norm: 0.46667595
INFO:root:[   60] Training loss: 0.14046626, Validation loss: 0.14082765, Gradient norm: 0.71488483
INFO:root:[   61] Training loss: 0.13930477, Validation loss: 0.13960982, Gradient norm: 0.64821347
INFO:root:[   62] Training loss: 0.13835274, Validation loss: 0.13824805, Gradient norm: 0.55642658
INFO:root:[   63] Training loss: 0.13782257, Validation loss: 0.13795933, Gradient norm: 0.81903607
INFO:root:[   64] Training loss: 0.13709847, Validation loss: 0.13725312, Gradient norm: 0.76898554
INFO:root:[   65] Training loss: 0.13599720, Validation loss: 0.13545726, Gradient norm: 0.64662869
INFO:root:[   66] Training loss: 0.13493983, Validation loss: 0.13521629, Gradient norm: 0.47238348
INFO:root:[   67] Training loss: 0.13451875, Validation loss: 0.13429101, Gradient norm: 0.93411917
INFO:root:[   68] Training loss: 0.13361410, Validation loss: 0.13342640, Gradient norm: 0.75725860
INFO:root:[   69] Training loss: 0.13231039, Validation loss: 0.13190418, Gradient norm: 0.57397103
INFO:root:[   70] Training loss: 0.13167166, Validation loss: 0.13185742, Gradient norm: 0.40913415
INFO:root:[   71] Training loss: 0.13071351, Validation loss: 0.13030033, Gradient norm: 0.61207931
INFO:root:[   72] Training loss: 0.12975085, Validation loss: 0.12915252, Gradient norm: 0.51091850
INFO:root:[   73] Training loss: 0.12911167, Validation loss: 0.12959626, Gradient norm: 0.72321055
INFO:root:[   74] Training loss: 0.12833574, Validation loss: 0.12732977, Gradient norm: 0.86433632
INFO:root:[   75] Training loss: 0.12752064, Validation loss: 0.12634582, Gradient norm: 0.94832843
INFO:root:[   76] Training loss: 0.12634798, Validation loss: 0.12680395, Gradient norm: 0.56740112
INFO:root:[   77] Training loss: 0.12561863, Validation loss: 0.12649934, Gradient norm: 0.88428131
INFO:root:[   78] Training loss: 0.12485864, Validation loss: 0.12511604, Gradient norm: 0.83463006
INFO:root:[   79] Training loss: 0.12385208, Validation loss: 0.12419805, Gradient norm: 0.71182590
INFO:root:[   80] Training loss: 0.12283797, Validation loss: 0.12263475, Gradient norm: 0.56747950
INFO:root:[   81] Training loss: 0.12214156, Validation loss: 0.12203691, Gradient norm: 0.74403138
INFO:root:[   82] Training loss: 0.12126147, Validation loss: 0.12099886, Gradient norm: 0.66953012
INFO:root:[   83] Training loss: 0.12031082, Validation loss: 0.12074254, Gradient norm: 0.59341703
INFO:root:[   84] Training loss: 0.11968708, Validation loss: 0.11957371, Gradient norm: 0.88331017
INFO:root:[   85] Training loss: 0.11891949, Validation loss: 0.11910802, Gradient norm: 0.92900681
INFO:root:[   86] Training loss: 0.11793035, Validation loss: 0.11776283, Gradient norm: 0.73678047
INFO:root:[   87] Training loss: 0.11702656, Validation loss: 0.11696051, Gradient norm: 0.95238620
INFO:root:[   88] Training loss: 0.11627801, Validation loss: 0.11592520, Gradient norm: 0.58252203
INFO:root:[   89] Training loss: 0.11543123, Validation loss: 0.11617702, Gradient norm: 0.66298025
INFO:root:[   90] Training loss: 0.11475168, Validation loss: 0.11471607, Gradient norm: 1.09857652
INFO:root:[   91] Training loss: 0.11411294, Validation loss: 0.11344660, Gradient norm: 1.13285560
INFO:root:[   92] Training loss: 0.11307084, Validation loss: 0.11274248, Gradient norm: 1.09837583
INFO:root:[   93] Training loss: 0.11237321, Validation loss: 0.11219849, Gradient norm: 1.15221056
INFO:root:[   94] Training loss: 0.11153417, Validation loss: 0.11166279, Gradient norm: 0.74912675
INFO:root:[   95] Training loss: 0.11078551, Validation loss: 0.11030480, Gradient norm: 0.87138711
INFO:root:[   96] Training loss: 0.10999087, Validation loss: 0.11019246, Gradient norm: 0.98617509
INFO:root:[   97] Training loss: 0.10949889, Validation loss: 0.10894020, Gradient norm: 1.00552433
INFO:root:[   98] Training loss: 0.10891447, Validation loss: 0.10874909, Gradient norm: 1.35267475
INFO:root:[   99] Training loss: 0.10821597, Validation loss: 0.10826006, Gradient norm: 1.22845053
INFO:root:[  100] Training loss: 0.10736127, Validation loss: 0.10761906, Gradient norm: 0.98742548
INFO:root:[  101] Training loss: 0.10660094, Validation loss: 0.10641688, Gradient norm: 0.95476291
INFO:root:[  102] Training loss: 0.10606727, Validation loss: 0.10612769, Gradient norm: 0.83140013
INFO:root:[  103] Training loss: 0.10513447, Validation loss: 0.10516515, Gradient norm: 1.01690000
INFO:root:[  104] Training loss: 0.10463154, Validation loss: 0.10386656, Gradient norm: 1.29742842
INFO:root:[  105] Training loss: 0.10354092, Validation loss: 0.10367136, Gradient norm: 0.66905443
INFO:root:[  106] Training loss: 0.10311774, Validation loss: 0.10323062, Gradient norm: 1.12510953
INFO:root:[  107] Training loss: 0.10225596, Validation loss: 0.10211713, Gradient norm: 1.05748235
INFO:root:[  108] Training loss: 0.10177058, Validation loss: 0.10147243, Gradient norm: 1.31178063
INFO:root:[  109] Training loss: 0.10128738, Validation loss: 0.10053760, Gradient norm: 1.22494264
INFO:root:[  110] Training loss: 0.10067039, Validation loss: 0.10153452, Gradient norm: 1.56875638
INFO:root:[  111] Training loss: 0.09969102, Validation loss: 0.09900064, Gradient norm: 1.14853293
INFO:root:[  112] Training loss: 0.09900889, Validation loss: 0.09840255, Gradient norm: 0.87754506
INFO:root:[  113] Training loss: 0.09839218, Validation loss: 0.09866348, Gradient norm: 1.12009112
INFO:root:[  114] Training loss: 0.09810896, Validation loss: 0.09933822, Gradient norm: 1.52577608
INFO:root:[  115] Training loss: 0.09736561, Validation loss: 0.09696119, Gradient norm: 1.78587173
INFO:root:[  116] Training loss: 0.09684484, Validation loss: 0.09693045, Gradient norm: 1.52522131
INFO:root:[  117] Training loss: 0.09629770, Validation loss: 0.09650684, Gradient norm: 1.53153918
INFO:root:[  118] Training loss: 0.09576644, Validation loss: 0.09631108, Gradient norm: 1.74896507
INFO:root:[  119] Training loss: 0.09514607, Validation loss: 0.09439770, Gradient norm: 1.40610014
INFO:root:[  120] Training loss: 0.09434295, Validation loss: 0.09423089, Gradient norm: 1.53821726
INFO:root:[  121] Training loss: 0.09407439, Validation loss: 0.09369594, Gradient norm: 1.46272039
INFO:root:[  122] Training loss: 0.09325120, Validation loss: 0.09263781, Gradient norm: 1.40883018
INFO:root:[  123] Training loss: 0.09273547, Validation loss: 0.09343408, Gradient norm: 1.53230148
INFO:root:[  124] Training loss: 0.09252503, Validation loss: 0.09227163, Gradient norm: 2.26650890
INFO:root:[  125] Training loss: 0.09169629, Validation loss: 0.09133178, Gradient norm: 1.73314503
INFO:root:[  126] Training loss: 0.09157508, Validation loss: 0.09121466, Gradient norm: 1.89359169
INFO:root:[  127] Training loss: 0.09073077, Validation loss: 0.09103565, Gradient norm: 1.60204287
INFO:root:[  128] Training loss: 0.08999329, Validation loss: 0.08945732, Gradient norm: 1.34981132
INFO:root:[  129] Training loss: 0.08965991, Validation loss: 0.08955258, Gradient norm: 1.28701048
INFO:root:[  130] Training loss: 0.08938286, Validation loss: 0.08953250, Gradient norm: 2.08454197
INFO:root:[  131] Training loss: 0.08862985, Validation loss: 0.08844173, Gradient norm: 1.90716923
INFO:root:[  132] Training loss: 0.08789036, Validation loss: 0.08717652, Gradient norm: 1.40551388
INFO:root:[  133] Training loss: 0.08730027, Validation loss: 0.08774968, Gradient norm: 1.13085372
INFO:root:[  134] Training loss: 0.08684893, Validation loss: 0.08698022, Gradient norm: 1.57173768
INFO:root:[  135] Training loss: 0.08641335, Validation loss: 0.08625663, Gradient norm: 1.39809400
INFO:root:[  136] Training loss: 0.08596789, Validation loss: 0.08713016, Gradient norm: 1.92354035
INFO:root:[  137] Training loss: 0.08565372, Validation loss: 0.08514585, Gradient norm: 2.27837244
INFO:root:[  138] Training loss: 0.08492575, Validation loss: 0.08404757, Gradient norm: 1.61953053
INFO:root:[  139] Training loss: 0.08433766, Validation loss: 0.08433894, Gradient norm: 1.71155314
INFO:root:[  140] Training loss: 0.08378937, Validation loss: 0.08361625, Gradient norm: 2.03387130
INFO:root:[  141] Training loss: 0.08369970, Validation loss: 0.08296367, Gradient norm: 2.25974348
INFO:root:[  142] Training loss: 0.08308695, Validation loss: 0.08310520, Gradient norm: 1.86386585
INFO:root:[  143] Training loss: 0.08237652, Validation loss: 0.08283903, Gradient norm: 2.05207878
INFO:root:[  144] Training loss: 0.08209230, Validation loss: 0.08150991, Gradient norm: 2.88507937
INFO:root:[  145] Training loss: 0.08164781, Validation loss: 0.08198806, Gradient norm: 1.93982878
INFO:root:[  146] Training loss: 0.08135107, Validation loss: 0.08198220, Gradient norm: 3.06883229
INFO:root:[  147] Training loss: 0.08081448, Validation loss: 0.08094183, Gradient norm: 2.52699943
INFO:root:[  148] Training loss: 0.08025787, Validation loss: 0.08053874, Gradient norm: 2.12880425
INFO:root:[  149] Training loss: 0.07975221, Validation loss: 0.07952001, Gradient norm: 2.23990968
INFO:root:[  150] Training loss: 0.07947659, Validation loss: 0.08000172, Gradient norm: 2.83131673
INFO:root:[  151] Training loss: 0.07907514, Validation loss: 0.07917681, Gradient norm: 2.03041764
INFO:root:[  152] Training loss: 0.07854346, Validation loss: 0.07901668, Gradient norm: 2.94144680
INFO:root:[  153] Training loss: 0.07801225, Validation loss: 0.07737541, Gradient norm: 2.00479239
INFO:root:[  154] Training loss: 0.07791083, Validation loss: 0.07779722, Gradient norm: 2.87195679
INFO:root:[  155] Training loss: 0.07766140, Validation loss: 0.07765786, Gradient norm: 3.65166566
INFO:root:[  156] Training loss: 0.07743130, Validation loss: 0.07667354, Gradient norm: 3.85900884
INFO:root:[  157] Training loss: 0.07663919, Validation loss: 0.07604958, Gradient norm: 2.96303650
INFO:root:[  158] Training loss: 0.07605610, Validation loss: 0.07599465, Gradient norm: 2.31345731
INFO:root:[  159] Training loss: 0.07589198, Validation loss: 0.07616681, Gradient norm: 3.64781714
INFO:root:[  160] Training loss: 0.07551451, Validation loss: 0.07583832, Gradient norm: 3.56338931
INFO:root:[  161] Training loss: 0.07508516, Validation loss: 0.07501614, Gradient norm: 3.57904360
INFO:root:[  162] Training loss: 0.07472696, Validation loss: 0.07486936, Gradient norm: 3.41901349
INFO:root:[  163] Training loss: 0.07442191, Validation loss: 0.07379650, Gradient norm: 3.31105377
INFO:root:[  164] Training loss: 0.07369665, Validation loss: 0.07356406, Gradient norm: 2.51445884
INFO:root:[  165] Training loss: 0.07361695, Validation loss: 0.07405766, Gradient norm: 3.11200898
INFO:root:[  166] Training loss: 0.07347628, Validation loss: 0.07326991, Gradient norm: 3.96660797
INFO:root:[  167] Training loss: 0.07315979, Validation loss: 0.07305768, Gradient norm: 3.93592581
INFO:root:[  168] Training loss: 0.07246185, Validation loss: 0.07232921, Gradient norm: 2.68530209
INFO:root:[  169] Training loss: 0.07230148, Validation loss: 0.07215773, Gradient norm: 3.58603279
INFO:root:[  170] Training loss: 0.07212483, Validation loss: 0.07184093, Gradient norm: 4.19281606
INFO:root:[  171] Training loss: 0.07131695, Validation loss: 0.07215181, Gradient norm: 3.21263989
INFO:root:[  172] Training loss: 0.07126767, Validation loss: 0.07100895, Gradient norm: 3.83637465
INFO:root:[  173] Training loss: 0.07139051, Validation loss: 0.07033443, Gradient norm: 5.62692594
INFO:root:[  174] Training loss: 0.07064241, Validation loss: 0.07069681, Gradient norm: 5.36768565
INFO:root:[  175] Training loss: 0.07026068, Validation loss: 0.07000026, Gradient norm: 3.27535219
INFO:root:[  176] Training loss: 0.07000770, Validation loss: 0.06925243, Gradient norm: 4.59352503
INFO:root:[  177] Training loss: 0.06982700, Validation loss: 0.07004749, Gradient norm: 5.98663791
INFO:root:[  178] Training loss: 0.06925029, Validation loss: 0.06941630, Gradient norm: 4.30247122
INFO:root:[  179] Training loss: 0.06891568, Validation loss: 0.06864182, Gradient norm: 3.59077602
INFO:root:[  180] Training loss: 0.06853595, Validation loss: 0.06874353, Gradient norm: 3.68228271
INFO:root:[  181] Training loss: 0.06851337, Validation loss: 0.06846764, Gradient norm: 3.99194233
INFO:root:[  182] Training loss: 0.06801299, Validation loss: 0.06842767, Gradient norm: 5.86495939
INFO:root:[  183] Training loss: 0.06793016, Validation loss: 0.06789638, Gradient norm: 5.38096876
INFO:root:[  184] Training loss: 0.06760779, Validation loss: 0.06709724, Gradient norm: 5.91316408
INFO:root:[  185] Training loss: 0.06730279, Validation loss: 0.06793658, Gradient norm: 5.46465672
INFO:root:[  186] Training loss: 0.06746411, Validation loss: 0.06705053, Gradient norm: 6.13341214
INFO:root:[  187] Training loss: 0.06654938, Validation loss: 0.06684991, Gradient norm: 5.42530465
INFO:root:[  188] Training loss: 0.06655889, Validation loss: 0.06704155, Gradient norm: 6.03425851
INFO:root:[  189] Training loss: 0.06647808, Validation loss: 0.06615032, Gradient norm: 7.26578632
INFO:root:[  190] Training loss: 0.06628182, Validation loss: 0.06631694, Gradient norm: 6.96267195
INFO:root:[  191] Training loss: 0.06569013, Validation loss: 0.06547218, Gradient norm: 3.92435582
INFO:root:[  192] Training loss: 0.06550917, Validation loss: 0.06517692, Gradient norm: 4.55711508
INFO:root:[  193] Training loss: 0.06516946, Validation loss: 0.06602559, Gradient norm: 6.39553369
INFO:root:[  194] Training loss: 0.06532466, Validation loss: 0.06453957, Gradient norm: 5.90167841
INFO:root:[  195] Training loss: 0.06487565, Validation loss: 0.06390623, Gradient norm: 6.24916301
INFO:root:[  196] Training loss: 0.06450584, Validation loss: 0.06444776, Gradient norm: 6.93951776
INFO:root:[  197] Training loss: 0.06458593, Validation loss: 0.06519909, Gradient norm: 8.33459357
INFO:root:[  198] Training loss: 0.06401801, Validation loss: 0.06417954, Gradient norm: 6.05923274
INFO:root:[  199] Training loss: 0.06410898, Validation loss: 0.06343406, Gradient norm: 7.77099563
INFO:root:[  200] Training loss: 0.06364179, Validation loss: 0.06530797, Gradient norm: 6.51616004
INFO:root:[  201] Training loss: 0.06422201, Validation loss: 0.06337680, Gradient norm: 11.18024306
INFO:root:[  202] Training loss: 0.06290916, Validation loss: 0.06350032, Gradient norm: 5.19165041
INFO:root:[  203] Training loss: 0.06310650, Validation loss: 0.06222765, Gradient norm: 6.95931518
INFO:root:[  204] Training loss: 0.06269505, Validation loss: 0.06322767, Gradient norm: 5.99080726
INFO:root:[  205] Training loss: 0.06301290, Validation loss: 0.06250889, Gradient norm: 10.15060392
INFO:root:[  206] Training loss: 0.06254210, Validation loss: 0.06198239, Gradient norm: 7.49599309
INFO:root:[  207] Training loss: 0.06241516, Validation loss: 0.06255991, Gradient norm: 7.84226916
INFO:root:[  208] Training loss: 0.06225615, Validation loss: 0.06245933, Gradient norm: 8.76953320
INFO:root:[  209] Training loss: 0.06189617, Validation loss: 0.06121236, Gradient norm: 9.10100109
INFO:root:[  210] Training loss: 0.06173322, Validation loss: 0.06085406, Gradient norm: 9.38271496
INFO:root:[  211] Training loss: 0.06162222, Validation loss: 0.06066722, Gradient norm: 9.98611220
INFO:root:[  212] Training loss: 0.06119788, Validation loss: 0.06077241, Gradient norm: 10.65293028
INFO:root:[  213] Training loss: 0.06098390, Validation loss: 0.06225371, Gradient norm: 10.01117848
INFO:root:[  214] Training loss: 0.06057572, Validation loss: 0.06131346, Gradient norm: 7.58809179
INFO:root:[  215] Training loss: 0.06073469, Validation loss: 0.05965475, Gradient norm: 11.50190372
INFO:root:[  216] Training loss: 0.06016124, Validation loss: 0.06028228, Gradient norm: 8.33077531
INFO:root:[  217] Training loss: 0.05964127, Validation loss: 0.05958922, Gradient norm: 5.62595100
INFO:root:[  218] Training loss: 0.05979180, Validation loss: 0.05933291, Gradient norm: 10.35198715
INFO:root:[  219] Training loss: 0.05977422, Validation loss: 0.06388145, Gradient norm: 10.50558521
INFO:root:[  220] Training loss: 0.06004490, Validation loss: 0.05934885, Gradient norm: 11.53731701
INFO:root:[  221] Training loss: 0.05916046, Validation loss: 0.05891080, Gradient norm: 9.93599576
INFO:root:[  222] Training loss: 0.05961247, Validation loss: 0.05920959, Gradient norm: 13.57664298
INFO:root:[  223] Training loss: 0.05886878, Validation loss: 0.06013881, Gradient norm: 8.37542825
INFO:root:[  224] Training loss: 0.05906220, Validation loss: 0.05952937, Gradient norm: 13.63439747
INFO:root:[  225] Training loss: 0.05829922, Validation loss: 0.05814845, Gradient norm: 11.31303477
INFO:root:[  226] Training loss: 0.05825141, Validation loss: 0.05806921, Gradient norm: 10.02348971
INFO:root:[  227] Training loss: 0.05842740, Validation loss: 0.05800962, Gradient norm: 12.31806216
INFO:root:[  228] Training loss: 0.05742252, Validation loss: 0.05775139, Gradient norm: 7.97197574
INFO:root:[  229] Training loss: 0.05762910, Validation loss: 0.05756745, Gradient norm: 10.22717099
INFO:root:[  230] Training loss: 0.05779726, Validation loss: 0.05672466, Gradient norm: 13.60549986
INFO:root:[  231] Training loss: 0.05756397, Validation loss: 0.05731971, Gradient norm: 12.00693625
INFO:root:[  232] Training loss: 0.05728086, Validation loss: 0.05664264, Gradient norm: 12.93425157
INFO:root:[  233] Training loss: 0.05688376, Validation loss: 0.05592403, Gradient norm: 12.73195494
INFO:root:[  234] Training loss: 0.05661615, Validation loss: 0.05662821, Gradient norm: 10.47815391
INFO:root:[  235] Training loss: 0.05728793, Validation loss: 0.05668530, Gradient norm: 16.02546718
INFO:root:[  236] Training loss: 0.05681922, Validation loss: 0.05627104, Gradient norm: 13.53561567
INFO:root:[  237] Training loss: 0.05628424, Validation loss: 0.05597621, Gradient norm: 11.56350761
INFO:root:[  238] Training loss: 0.05620620, Validation loss: 0.05605431, Gradient norm: 12.07138725
INFO:root:[  239] Training loss: 0.05549328, Validation loss: 0.05585052, Gradient norm: 9.12449676
INFO:root:[  240] Training loss: 0.05594974, Validation loss: 0.05582745, Gradient norm: 14.93781401
INFO:root:[  241] Training loss: 0.05523344, Validation loss: 0.05555459, Gradient norm: 10.02021006
INFO:root:[  242] Training loss: 0.05568346, Validation loss: 0.05528559, Gradient norm: 13.90967623
INFO:root:[  243] Training loss: 0.05555220, Validation loss: 0.05703859, Gradient norm: 15.39506636
INFO:root:[  244] Training loss: 0.05621525, Validation loss: 0.05460257, Gradient norm: 20.11788632
INFO:root:[  245] Training loss: 0.05498255, Validation loss: 0.05495881, Gradient norm: 11.39987517
INFO:root:[  246] Training loss: 0.05430221, Validation loss: 0.05411044, Gradient norm: 10.12104141
INFO:root:[  247] Training loss: 0.05492998, Validation loss: 0.05427300, Gradient norm: 16.77767239
INFO:root:[  248] Training loss: 0.05490170, Validation loss: 0.05502971, Gradient norm: 16.87045318
INFO:root:[  249] Training loss: 0.05435009, Validation loss: 0.05396579, Gradient norm: 16.35434287
INFO:root:[  250] Training loss: 0.05458534, Validation loss: 0.05354879, Gradient norm: 17.50837162
INFO:root:[  251] Training loss: 0.05438152, Validation loss: 0.05396003, Gradient norm: 19.16173540
INFO:root:[  252] Training loss: 0.05451016, Validation loss: 0.05640081, Gradient norm: 20.00420234
INFO:root:[  253] Training loss: 0.05342767, Validation loss: 0.05725568, Gradient norm: 13.21755460
INFO:root:[  254] Training loss: 0.05329452, Validation loss: 0.05383387, Gradient norm: 15.55166532
INFO:root:[  255] Training loss: 0.05275094, Validation loss: 0.05271917, Gradient norm: 9.47479711
INFO:root:[  256] Training loss: 0.05348275, Validation loss: 0.05726032, Gradient norm: 20.01697441
INFO:root:[  257] Training loss: 0.05341164, Validation loss: 0.05750938, Gradient norm: 17.97133245
INFO:root:[  258] Training loss: 0.05325760, Validation loss: 0.05261883, Gradient norm: 17.15282466
INFO:root:[  259] Training loss: 0.05252886, Validation loss: 0.05209435, Gradient norm: 17.06847262
INFO:root:[  260] Training loss: 0.05248005, Validation loss: 0.05217415, Gradient norm: 15.16873772
INFO:root:[  261] Training loss: 0.05243651, Validation loss: 0.05137357, Gradient norm: 16.22708284
INFO:root:[  262] Training loss: 0.05239899, Validation loss: 0.05187962, Gradient norm: 19.75861510
INFO:root:[  263] Training loss: 0.05154677, Validation loss: 0.05138380, Gradient norm: 12.09097419
INFO:root:[  264] Training loss: 0.05205485, Validation loss: 0.05189799, Gradient norm: 16.59905666
INFO:root:[  265] Training loss: 0.05197860, Validation loss: 0.05185786, Gradient norm: 18.55868034
INFO:root:[  266] Training loss: 0.05323967, Validation loss: 0.05129305, Gradient norm: 27.45567982
INFO:root:[  267] Training loss: 0.05164893, Validation loss: 0.05061906, Gradient norm: 19.68990837
INFO:root:[  268] Training loss: 0.05059541, Validation loss: 0.05110157, Gradient norm: 8.84824936
INFO:root:[  269] Training loss: 0.05111956, Validation loss: 0.05048648, Gradient norm: 15.46686119
INFO:root:[  270] Training loss: 0.05110493, Validation loss: 0.05124393, Gradient norm: 17.09960480
INFO:root:[  271] Training loss: 0.05147629, Validation loss: 0.05137796, Gradient norm: 23.76264862
INFO:root:[  272] Training loss: 0.05086423, Validation loss: 0.05155940, Gradient norm: 20.04382620
INFO:root:[  273] Training loss: 0.05143021, Validation loss: 0.05534255, Gradient norm: 25.02174086
INFO:root:[  274] Training loss: 0.05109948, Validation loss: 0.04995183, Gradient norm: 23.44478778
INFO:root:[  275] Training loss: 0.04977100, Validation loss: 0.04995766, Gradient norm: 13.26142483
INFO:root:[  276] Training loss: 0.04980389, Validation loss: 0.04996894, Gradient norm: 16.52849243
INFO:root:[  277] Training loss: 0.04976748, Validation loss: 0.05212176, Gradient norm: 19.68617406
INFO:root:[  278] Training loss: 0.05080454, Validation loss: 0.04917853, Gradient norm: 28.32821736
INFO:root:[  279] Training loss: 0.04941070, Validation loss: 0.05220379, Gradient norm: 18.49057242
INFO:root:[  280] Training loss: 0.04966000, Validation loss: 0.04892945, Gradient norm: 21.14292887
INFO:root:[  281] Training loss: 0.04982413, Validation loss: 0.04980908, Gradient norm: 23.82592668
INFO:root:[  282] Training loss: 0.04908500, Validation loss: 0.05037029, Gradient norm: 16.29831883
INFO:root:[  283] Training loss: 0.04918521, Validation loss: 0.04924111, Gradient norm: 19.74663445
INFO:root:[  284] Training loss: 0.04864413, Validation loss: 0.05089216, Gradient norm: 15.83594503
INFO:root:[  285] Training loss: 0.04879896, Validation loss: 0.04820892, Gradient norm: 19.22424508
INFO:root:[  286] Training loss: 0.04822522, Validation loss: 0.04760913, Gradient norm: 17.55576236
INFO:root:[  287] Training loss: 0.04875242, Validation loss: 0.05053608, Gradient norm: 22.04904135
INFO:root:[  288] Training loss: 0.04950485, Validation loss: 0.04899370, Gradient norm: 28.76872406
INFO:root:[  289] Training loss: 0.04855177, Validation loss: 0.04778289, Gradient norm: 22.63052596
INFO:root:[  290] Training loss: 0.04796791, Validation loss: 0.04765406, Gradient norm: 18.89351541
INFO:root:[  291] Training loss: 0.04922146, Validation loss: 0.05223509, Gradient norm: 32.21253729
INFO:root:[  292] Training loss: 0.04853757, Validation loss: 0.04850222, Gradient norm: 28.80772362
INFO:root:[  293] Training loss: 0.04776464, Validation loss: 0.04713320, Gradient norm: 22.96582455
INFO:root:[  294] Training loss: 0.04786047, Validation loss: 0.04717553, Gradient norm: 23.56953651
INFO:root:[  295] Training loss: 0.04720945, Validation loss: 0.04803775, Gradient norm: 16.50077500
INFO:root:[  296] Training loss: 0.04708798, Validation loss: 0.04875214, Gradient norm: 19.80656260
INFO:root:[  297] Training loss: 0.04683687, Validation loss: 0.04795663, Gradient norm: 20.19665393
INFO:root:[  298] Training loss: 0.04681638, Validation loss: 0.04661805, Gradient norm: 19.43459618
INFO:root:[  299] Training loss: 0.04673795, Validation loss: 0.04678487, Gradient norm: 19.34534806
INFO:root:[  300] Training loss: 0.04806180, Validation loss: 0.04589620, Gradient norm: 27.54999720
INFO:root:[  301] Training loss: 0.04690942, Validation loss: 0.04682643, Gradient norm: 20.85734206
INFO:root:[  302] Training loss: 0.04661540, Validation loss: 0.04624729, Gradient norm: 22.10603851
INFO:root:[  303] Training loss: 0.04619508, Validation loss: 0.04561290, Gradient norm: 19.74354841
INFO:root:[  304] Training loss: 0.04644902, Validation loss: 0.04582762, Gradient norm: 23.87620705
INFO:root:[  305] Training loss: 0.04637410, Validation loss: 0.04536223, Gradient norm: 22.81220297
INFO:root:[  306] Training loss: 0.04622772, Validation loss: 0.04514649, Gradient norm: 22.01919315
INFO:root:[  307] Training loss: 0.04627476, Validation loss: 0.04552431, Gradient norm: 24.54297203
INFO:root:[  308] Training loss: 0.04617136, Validation loss: 0.04497323, Gradient norm: 23.44876412
INFO:root:[  309] Training loss: 0.04510342, Validation loss: 0.04434175, Gradient norm: 16.23984654
INFO:root:[  310] Training loss: 0.04628685, Validation loss: 0.04584578, Gradient norm: 30.81136546
INFO:root:[  311] Training loss: 0.04600789, Validation loss: 0.04457648, Gradient norm: 28.16067225
INFO:root:[  312] Training loss: 0.04547410, Validation loss: 0.04626515, Gradient norm: 24.50895850
INFO:root:[  313] Training loss: 0.04577518, Validation loss: 0.04753355, Gradient norm: 29.97215559
INFO:root:[  314] Training loss: 0.04467423, Validation loss: 0.04446830, Gradient norm: 16.54679356
INFO:root:[  315] Training loss: 0.04490533, Validation loss: 0.04425908, Gradient norm: 22.13743274
INFO:root:[  316] Training loss: 0.04489441, Validation loss: 0.04402801, Gradient norm: 23.27755612
INFO:root:[  317] Training loss: 0.04535765, Validation loss: 0.04398779, Gradient norm: 29.79978984
INFO:root:[  318] Training loss: 0.04469405, Validation loss: 0.04379501, Gradient norm: 23.53143333
INFO:root:[  319] Training loss: 0.04558229, Validation loss: 0.04458566, Gradient norm: 32.06347649
INFO:root:[  320] Training loss: 0.04480582, Validation loss: 0.04361553, Gradient norm: 26.39737916
INFO:root:[  321] Training loss: 0.04494366, Validation loss: 0.04444426, Gradient norm: 28.18062254
INFO:root:[  322] Training loss: 0.04416016, Validation loss: 0.04452564, Gradient norm: 21.63408498
INFO:root:[  323] Training loss: 0.04420295, Validation loss: 0.04437837, Gradient norm: 25.01628191
INFO:root:[  324] Training loss: 0.04415851, Validation loss: 0.04302683, Gradient norm: 24.03691707
INFO:root:[  325] Training loss: 0.04431852, Validation loss: 0.04306189, Gradient norm: 25.96419767
INFO:root:[  326] Training loss: 0.04392619, Validation loss: 0.04307147, Gradient norm: 25.85230376
INFO:root:[  327] Training loss: 0.04458467, Validation loss: 0.04505254, Gradient norm: 31.75568617
INFO:root:[  328] Training loss: 0.04404357, Validation loss: 0.04327417, Gradient norm: 29.48460921
INFO:root:[  329] Training loss: 0.04407641, Validation loss: 0.05243548, Gradient norm: 28.70409602
INFO:root:[  330] Training loss: 0.04374806, Validation loss: 0.04466684, Gradient norm: 24.94302905
INFO:root:[  331] Training loss: 0.04318004, Validation loss: 0.04281097, Gradient norm: 22.27548632
INFO:root:[  332] Training loss: 0.04287382, Validation loss: 0.04272269, Gradient norm: 18.30240262
INFO:root:[  333] Training loss: 0.04350551, Validation loss: 0.04249997, Gradient norm: 27.25091150
INFO:root:[  334] Training loss: 0.04334330, Validation loss: 0.04864932, Gradient norm: 25.62275861
INFO:root:[  335] Training loss: 0.04395793, Validation loss: 0.04232052, Gradient norm: 30.52334465
INFO:root:[  336] Training loss: 0.04383778, Validation loss: 0.04199817, Gradient norm: 32.32428433
INFO:root:[  337] Training loss: 0.04291036, Validation loss: 0.04168646, Gradient norm: 26.01541043
INFO:root:[  338] Training loss: 0.04268792, Validation loss: 0.04294065, Gradient norm: 24.97041009
INFO:root:[  339] Training loss: 0.04257460, Validation loss: 0.04153216, Gradient norm: 24.40114588
INFO:root:[  340] Training loss: 0.04294291, Validation loss: 0.04467869, Gradient norm: 29.67442653
INFO:root:[  341] Training loss: 0.04290255, Validation loss: 0.04777835, Gradient norm: 29.09421833
INFO:root:[  342] Training loss: 0.04265848, Validation loss: 0.04145562, Gradient norm: 23.31128281
INFO:root:[  343] Training loss: 0.04244152, Validation loss: 0.04196189, Gradient norm: 24.17040158
INFO:root:[  344] Training loss: 0.04202218, Validation loss: 0.04126143, Gradient norm: 23.05376998
INFO:root:[  345] Training loss: 0.04264405, Validation loss: 0.04229519, Gradient norm: 29.29437369
INFO:root:[  346] Training loss: 0.04150886, Validation loss: 0.04152389, Gradient norm: 21.56076649
INFO:root:[  347] Training loss: 0.04184023, Validation loss: 0.04155855, Gradient norm: 28.27563555
INFO:root:[  348] Training loss: 0.04235822, Validation loss: 0.04095376, Gradient norm: 28.33981013
INFO:root:[  349] Training loss: 0.04118718, Validation loss: 0.04319370, Gradient norm: 18.57356514
INFO:root:[  350] Training loss: 0.04264630, Validation loss: 0.05202413, Gradient norm: 34.03045409
INFO:root:[  351] Training loss: 0.04222333, Validation loss: 0.04234300, Gradient norm: 29.92198267
INFO:root:[  352] Training loss: 0.04325122, Validation loss: 0.04152079, Gradient norm: 39.20106276
INFO:root:[  353] Training loss: 0.04060834, Validation loss: 0.04027669, Gradient norm: 15.31968358
INFO:root:[  354] Training loss: 0.04158218, Validation loss: 0.04219210, Gradient norm: 27.63230293
INFO:root:[  355] Training loss: 0.04148705, Validation loss: 0.04156969, Gradient norm: 30.02679852
INFO:root:[  356] Training loss: 0.04076515, Validation loss: 0.04126158, Gradient norm: 21.36965444
INFO:root:[  357] Training loss: 0.04061724, Validation loss: 0.04185920, Gradient norm: 22.17148860
INFO:root:[  358] Training loss: 0.04089678, Validation loss: 0.04370405, Gradient norm: 24.78160161
INFO:root:[  359] Training loss: 0.04166740, Validation loss: 0.04217743, Gradient norm: 30.71574148
INFO:root:[  360] Training loss: 0.04109888, Validation loss: 0.04017645, Gradient norm: 29.02148186
INFO:root:[  361] Training loss: 0.04024049, Validation loss: 0.04137120, Gradient norm: 20.96418754
INFO:root:[  362] Training loss: 0.04109875, Validation loss: 0.03996537, Gradient norm: 31.12342420
INFO:root:[  363] Training loss: 0.04078583, Validation loss: 0.03996031, Gradient norm: 27.07438181
INFO:root:[  364] Training loss: 0.04136027, Validation loss: 0.04532643, Gradient norm: 33.81413152
INFO:root:[  365] Training loss: 0.04115901, Validation loss: 0.04609815, Gradient norm: 31.66167907
INFO:root:[  366] Training loss: 0.04018323, Validation loss: 0.03976509, Gradient norm: 18.42248866
INFO:root:[  367] Training loss: 0.04016296, Validation loss: 0.04242068, Gradient norm: 24.63057171
INFO:root:[  368] Training loss: 0.03975765, Validation loss: 0.03974746, Gradient norm: 21.73712951
INFO:root:[  369] Training loss: 0.04072189, Validation loss: 0.03980351, Gradient norm: 31.62136822
INFO:root:[  370] Training loss: 0.04024550, Validation loss: 0.03975626, Gradient norm: 30.95904081
INFO:root:[  371] Training loss: 0.04016309, Validation loss: 0.03937855, Gradient norm: 22.48567249
INFO:root:[  372] Training loss: 0.04093961, Validation loss: 0.04658159, Gradient norm: 34.28875241
INFO:root:[  373] Training loss: 0.04004059, Validation loss: 0.03941620, Gradient norm: 25.82902753
INFO:root:[  374] Training loss: 0.03989705, Validation loss: 0.03891692, Gradient norm: 25.31825539
INFO:root:[  375] Training loss: 0.04033442, Validation loss: 0.03902255, Gradient norm: 29.15811675
INFO:root:[  376] Training loss: 0.04541583, Validation loss: 0.04614724, Gradient norm: 55.82161948
INFO:root:[  377] Training loss: 0.03951123, Validation loss: 0.04259321, Gradient norm: 20.60530995
INFO:root:[  378] Training loss: 0.03927267, Validation loss: 0.03890862, Gradient norm: 18.88000503
INFO:root:[  379] Training loss: 0.03867220, Validation loss: 0.03962912, Gradient norm: 14.47203152
INFO:root:[  380] Training loss: 0.03912823, Validation loss: 0.03913845, Gradient norm: 20.95537987
INFO:root:[  381] Training loss: 0.03922764, Validation loss: 0.03858615, Gradient norm: 25.73958877
INFO:root:[  382] Training loss: 0.03955542, Validation loss: 0.03860554, Gradient norm: 24.60929979
INFO:root:[  383] Training loss: 0.03952561, Validation loss: 0.03945013, Gradient norm: 25.38725551
INFO:root:[  384] Training loss: 0.03941496, Validation loss: 0.03923027, Gradient norm: 26.82088733
INFO:root:[  385] Training loss: 0.03915802, Validation loss: 0.04305524, Gradient norm: 22.65519009
INFO:root:[  386] Training loss: 0.03983172, Validation loss: 0.03898504, Gradient norm: 30.14425330
INFO:root:[  387] Training loss: 0.03899045, Validation loss: 0.03839511, Gradient norm: 25.57925538
INFO:root:[  388] Training loss: 0.03918927, Validation loss: 0.03817280, Gradient norm: 25.64419690
INFO:root:[  389] Training loss: 0.03895309, Validation loss: 0.03787372, Gradient norm: 25.38369944
INFO:root:[  390] Training loss: 0.04021253, Validation loss: 0.03858822, Gradient norm: 35.74922130
INFO:root:[  391] Training loss: 0.03870994, Validation loss: 0.03784864, Gradient norm: 22.75923331
INFO:root:[  392] Training loss: 0.03929707, Validation loss: 0.04209387, Gradient norm: 30.52502680
INFO:root:[  393] Training loss: 0.03864652, Validation loss: 0.03827059, Gradient norm: 23.85361001
INFO:root:[  394] Training loss: 0.03882415, Validation loss: 0.03808857, Gradient norm: 25.38903347
INFO:root:[  395] Training loss: 0.04003637, Validation loss: 0.03824805, Gradient norm: 35.53932478
INFO:root:[  396] Training loss: 0.03955948, Validation loss: 0.03928514, Gradient norm: 31.50774867
INFO:root:[  397] Training loss: 0.03907023, Validation loss: 0.03754910, Gradient norm: 28.63636713
INFO:root:[  398] Training loss: 0.03955442, Validation loss: 0.03801692, Gradient norm: 31.74001461
INFO:root:[  399] Training loss: 0.03807468, Validation loss: 0.03819120, Gradient norm: 15.66741297
INFO:root:[  400] Training loss: 0.03911028, Validation loss: 0.03905089, Gradient norm: 30.37017013
INFO:root:[  401] Training loss: 0.03881539, Validation loss: 0.04176416, Gradient norm: 27.46861703
INFO:root:[  402] Training loss: 0.03837814, Validation loss: 0.03912467, Gradient norm: 21.23027669
INFO:root:[  403] Training loss: 0.03963663, Validation loss: 0.03808100, Gradient norm: 31.56325666
INFO:root:[  404] Training loss: 0.03861598, Validation loss: 0.04028221, Gradient norm: 24.72885187
INFO:root:[  405] Training loss: 0.03873780, Validation loss: 0.03826336, Gradient norm: 23.49444714
INFO:root:[  406] Training loss: 0.03896825, Validation loss: 0.03764163, Gradient norm: 29.81165204
INFO:root:EP 406: Early stopping
INFO:root:Training the model took 4701.409s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.30168
INFO:root:EnergyScoreTrain: 0.21424
INFO:root:CoverageTrain: 0.98697
INFO:root:IntervalWidthTrain: 0.01794
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.2848
INFO:root:EnergyScoreValidation: 0.20228
INFO:root:CoverageValidation: 0.98691
INFO:root:IntervalWidthValidation: 0.01815
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.26643
INFO:root:EnergyScoreTest: 0.1887
INFO:root:CoverageTest: 0.98664
INFO:root:IntervalWidthTest: 0.01749
INFO:root:###20 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.3, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.65210422, Validation loss: 2.25044986, Gradient norm: 5.10842476
INFO:root:[    2] Training loss: 1.63553329, Validation loss: 0.88024125, Gradient norm: 6.35967331
INFO:root:[    3] Training loss: 0.77043910, Validation loss: 0.68247071, Gradient norm: 2.43094239
INFO:root:[    4] Training loss: 0.62089636, Validation loss: 0.55300922, Gradient norm: 1.95670538
INFO:root:[    5] Training loss: 0.51297658, Validation loss: 0.45621166, Gradient norm: 1.36457197
INFO:root:[    6] Training loss: 0.44067134, Validation loss: 0.40898407, Gradient norm: 1.37916604
INFO:root:[    7] Training loss: 0.39024515, Validation loss: 0.37292487, Gradient norm: 1.02993347
INFO:root:[    8] Training loss: 0.35946341, Validation loss: 0.34438585, Gradient norm: 0.85905653
INFO:root:[    9] Training loss: 0.33757817, Validation loss: 0.33138752, Gradient norm: 0.83022032
INFO:root:[   10] Training loss: 0.32200931, Validation loss: 0.31649025, Gradient norm: 0.69379753
INFO:root:[   11] Training loss: 0.31007485, Validation loss: 0.30792795, Gradient norm: 0.72346908
INFO:root:[   12] Training loss: 0.29885154, Validation loss: 0.29633943, Gradient norm: 0.61226311
INFO:root:[   13] Training loss: 0.29179983, Validation loss: 0.28787906, Gradient norm: 0.60962485
INFO:root:[   14] Training loss: 0.28474774, Validation loss: 0.28248370, Gradient norm: 0.62203228
INFO:root:[   15] Training loss: 0.27729648, Validation loss: 0.27226041, Gradient norm: 0.65839607
INFO:root:[   16] Training loss: 0.27060455, Validation loss: 0.26624030, Gradient norm: 0.62032814
INFO:root:[   17] Training loss: 0.26473389, Validation loss: 0.26546487, Gradient norm: 0.48994381
INFO:root:[   18] Training loss: 0.26028082, Validation loss: 0.26173130, Gradient norm: 0.55558069
INFO:root:[   19] Training loss: 0.25575977, Validation loss: 0.25357413, Gradient norm: 0.53589048
INFO:root:[   20] Training loss: 0.25186356, Validation loss: 0.24953823, Gradient norm: 0.44252191
INFO:root:[   21] Training loss: 0.24851468, Validation loss: 0.24709306, Gradient norm: 0.52799856
INFO:root:[   22] Training loss: 0.24483615, Validation loss: 0.24505627, Gradient norm: 0.59421109
INFO:root:[   23] Training loss: 0.24014085, Validation loss: 0.23947018, Gradient norm: 0.44713108
INFO:root:[   24] Training loss: 0.23703035, Validation loss: 0.23531452, Gradient norm: 0.50664196
INFO:root:[   25] Training loss: 0.23363186, Validation loss: 0.23405277, Gradient norm: 0.51212684
INFO:root:[   26] Training loss: 0.23073783, Validation loss: 0.22939059, Gradient norm: 0.47551569
INFO:root:[   27] Training loss: 0.22771658, Validation loss: 0.22647003, Gradient norm: 0.49430185
INFO:root:[   28] Training loss: 0.22516729, Validation loss: 0.22420507, Gradient norm: 0.44503542
INFO:root:[   29] Training loss: 0.22275929, Validation loss: 0.22186965, Gradient norm: 0.49113601
INFO:root:[   30] Training loss: 0.22060628, Validation loss: 0.22043685, Gradient norm: 0.46446269
INFO:root:[   31] Training loss: 0.21844728, Validation loss: 0.21833337, Gradient norm: 0.43199313
INFO:root:[   32] Training loss: 0.21656967, Validation loss: 0.21519467, Gradient norm: 0.41972786
INFO:root:[   33] Training loss: 0.21448822, Validation loss: 0.21406408, Gradient norm: 0.48405783
INFO:root:[   34] Training loss: 0.21260425, Validation loss: 0.21274714, Gradient norm: 0.41184786
INFO:root:[   35] Training loss: 0.21067396, Validation loss: 0.20994642, Gradient norm: 0.39334528
INFO:root:[   36] Training loss: 0.20914403, Validation loss: 0.20821688, Gradient norm: 0.43655558
INFO:root:[   37] Training loss: 0.20718108, Validation loss: 0.20587750, Gradient norm: 0.39405586
INFO:root:[   38] Training loss: 0.20562562, Validation loss: 0.20605338, Gradient norm: 0.31979899
INFO:root:[   39] Training loss: 0.20401707, Validation loss: 0.20300471, Gradient norm: 0.40608897
INFO:root:[   40] Training loss: 0.20252788, Validation loss: 0.20275804, Gradient norm: 0.42704662
INFO:root:[   41] Training loss: 0.20139462, Validation loss: 0.20088343, Gradient norm: 0.47970696
INFO:root:[   42] Training loss: 0.19956700, Validation loss: 0.19923651, Gradient norm: 0.33773212
INFO:root:[   43] Training loss: 0.19832292, Validation loss: 0.19807574, Gradient norm: 0.33156392
INFO:root:[   44] Training loss: 0.19693438, Validation loss: 0.19630433, Gradient norm: 0.36836856
INFO:root:[   45] Training loss: 0.19568953, Validation loss: 0.19541325, Gradient norm: 0.31814905
INFO:root:[   46] Training loss: 0.19422308, Validation loss: 0.19404113, Gradient norm: 0.37484976
INFO:root:[   47] Training loss: 0.19249983, Validation loss: 0.19202895, Gradient norm: 0.34215518
INFO:root:[   48] Training loss: 0.19124075, Validation loss: 0.19094831, Gradient norm: 0.31719407
INFO:root:[   49] Training loss: 0.19006865, Validation loss: 0.18945470, Gradient norm: 0.35895146
INFO:root:[   50] Training loss: 0.18878722, Validation loss: 0.18958207, Gradient norm: 0.45389773
INFO:root:[   51] Training loss: 0.18739012, Validation loss: 0.18679721, Gradient norm: 0.52931418
INFO:root:[   52] Training loss: 0.18578147, Validation loss: 0.18464839, Gradient norm: 0.40533520
INFO:root:[   53] Training loss: 0.18472051, Validation loss: 0.18453801, Gradient norm: 0.48576510
INFO:root:[   54] Training loss: 0.18316366, Validation loss: 0.18242515, Gradient norm: 0.42780128
INFO:root:[   55] Training loss: 0.18162830, Validation loss: 0.18157496, Gradient norm: 0.27040291
INFO:root:[   56] Training loss: 0.18039199, Validation loss: 0.18051831, Gradient norm: 0.36951556
INFO:root:[   57] Training loss: 0.17937124, Validation loss: 0.17951732, Gradient norm: 0.49282851
INFO:root:[   58] Training loss: 0.17761144, Validation loss: 0.17854846, Gradient norm: 0.39451125
INFO:root:[   59] Training loss: 0.17648350, Validation loss: 0.17664335, Gradient norm: 0.50375020
INFO:root:[   60] Training loss: 0.17525716, Validation loss: 0.17505364, Gradient norm: 0.56891262
INFO:root:[   61] Training loss: 0.17348743, Validation loss: 0.17409495, Gradient norm: 0.40562079
INFO:root:[   62] Training loss: 0.17231179, Validation loss: 0.17186366, Gradient norm: 0.34025467
INFO:root:[   63] Training loss: 0.17141026, Validation loss: 0.17049648, Gradient norm: 0.56045786
INFO:root:[   64] Training loss: 0.16989730, Validation loss: 0.16911587, Gradient norm: 0.46708346
INFO:root:[   65] Training loss: 0.16843050, Validation loss: 0.16875701, Gradient norm: 0.41700828
INFO:root:[   66] Training loss: 0.16720134, Validation loss: 0.16660905, Gradient norm: 0.45569412
INFO:root:[   67] Training loss: 0.16634427, Validation loss: 0.16575479, Gradient norm: 0.69430994
INFO:root:[   68] Training loss: 0.16435338, Validation loss: 0.16397881, Gradient norm: 0.49885881
INFO:root:[   69] Training loss: 0.16322464, Validation loss: 0.16261977, Gradient norm: 0.49480436
INFO:root:[   70] Training loss: 0.16158419, Validation loss: 0.16189273, Gradient norm: 0.39220798
INFO:root:[   71] Training loss: 0.16065900, Validation loss: 0.16097138, Gradient norm: 0.53407425
INFO:root:[   72] Training loss: 0.15934404, Validation loss: 0.15856719, Gradient norm: 0.58546332
INFO:root:[   73] Training loss: 0.15820920, Validation loss: 0.15781107, Gradient norm: 0.63374909
INFO:root:[   74] Training loss: 0.15643922, Validation loss: 0.15750793, Gradient norm: 0.35466522
INFO:root:[   75] Training loss: 0.15537800, Validation loss: 0.15449301, Gradient norm: 0.34018086
INFO:root:[   76] Training loss: 0.15450186, Validation loss: 0.15388204, Gradient norm: 0.65250439
INFO:root:[   77] Training loss: 0.15273284, Validation loss: 0.15364894, Gradient norm: 0.43285751
INFO:root:[   78] Training loss: 0.15133788, Validation loss: 0.15140985, Gradient norm: 0.28993580
INFO:root:[   79] Training loss: 0.15028345, Validation loss: 0.15027517, Gradient norm: 0.44363856
INFO:root:[   80] Training loss: 0.14901340, Validation loss: 0.14888277, Gradient norm: 0.50956779
INFO:root:[   81] Training loss: 0.14760436, Validation loss: 0.14665938, Gradient norm: 0.50941848
INFO:root:[   82] Training loss: 0.14644937, Validation loss: 0.14543456, Gradient norm: 0.49939643
INFO:root:[   83] Training loss: 0.14488495, Validation loss: 0.14458468, Gradient norm: 0.37872702
INFO:root:[   84] Training loss: 0.14358903, Validation loss: 0.14305734, Gradient norm: 0.49603832
INFO:root:[   85] Training loss: 0.14252025, Validation loss: 0.14239219, Gradient norm: 0.67118196
INFO:root:[   86] Training loss: 0.14112186, Validation loss: 0.14062663, Gradient norm: 0.65333244
INFO:root:[   87] Training loss: 0.13999517, Validation loss: 0.14015557, Gradient norm: 0.58563617
INFO:root:[   88] Training loss: 0.13863600, Validation loss: 0.13810794, Gradient norm: 0.44440983
INFO:root:[   89] Training loss: 0.13758078, Validation loss: 0.13834887, Gradient norm: 0.78503071
INFO:root:[   90] Training loss: 0.13690289, Validation loss: 0.13616175, Gradient norm: 0.88819304
INFO:root:[   91] Training loss: 0.13495949, Validation loss: 0.13488643, Gradient norm: 0.44765073
INFO:root:[   92] Training loss: 0.13409720, Validation loss: 0.13439931, Gradient norm: 0.75648204
INFO:root:[   93] Training loss: 0.13294788, Validation loss: 0.13262677, Gradient norm: 0.66819901
INFO:root:[   94] Training loss: 0.13152434, Validation loss: 0.13119322, Gradient norm: 0.49393130
INFO:root:[   95] Training loss: 0.13041115, Validation loss: 0.13094920, Gradient norm: 0.46638359
INFO:root:[   96] Training loss: 0.12929922, Validation loss: 0.12885160, Gradient norm: 0.69706332
INFO:root:[   97] Training loss: 0.12792239, Validation loss: 0.12756177, Gradient norm: 0.44395140
INFO:root:[   98] Training loss: 0.12719297, Validation loss: 0.12622952, Gradient norm: 0.69748223
INFO:root:[   99] Training loss: 0.12619569, Validation loss: 0.12587715, Gradient norm: 0.69059569
INFO:root:[  100] Training loss: 0.12471371, Validation loss: 0.12402129, Gradient norm: 0.43281527
INFO:root:[  101] Training loss: 0.12349567, Validation loss: 0.12298155, Gradient norm: 0.48547739
INFO:root:[  102] Training loss: 0.12254103, Validation loss: 0.12204962, Gradient norm: 0.56085449
INFO:root:[  103] Training loss: 0.12126213, Validation loss: 0.12201942, Gradient norm: 0.65647845
INFO:root:[  104] Training loss: 0.12006339, Validation loss: 0.11995253, Gradient norm: 0.55168331
INFO:root:[  105] Training loss: 0.11883413, Validation loss: 0.11940513, Gradient norm: 0.61135842
INFO:root:[  106] Training loss: 0.11806879, Validation loss: 0.11776092, Gradient norm: 0.88499880
INFO:root:[  107] Training loss: 0.11710220, Validation loss: 0.11809301, Gradient norm: 0.99156916
INFO:root:[  108] Training loss: 0.11597504, Validation loss: 0.11470879, Gradient norm: 0.81612521
INFO:root:[  109] Training loss: 0.11438732, Validation loss: 0.11419328, Gradient norm: 0.43904702
INFO:root:[  110] Training loss: 0.11400808, Validation loss: 0.11311190, Gradient norm: 1.07623723
INFO:root:[  111] Training loss: 0.11264570, Validation loss: 0.11266593, Gradient norm: 0.86670792
INFO:root:[  112] Training loss: 0.11163732, Validation loss: 0.11104998, Gradient norm: 0.82097807
INFO:root:[  113] Training loss: 0.11030539, Validation loss: 0.11070702, Gradient norm: 0.52560685
INFO:root:[  114] Training loss: 0.10963542, Validation loss: 0.10982144, Gradient norm: 0.78645656
INFO:root:[  115] Training loss: 0.10854289, Validation loss: 0.10788611, Gradient norm: 0.90376661
INFO:root:[  116] Training loss: 0.10756540, Validation loss: 0.10775940, Gradient norm: 0.76855395
INFO:root:[  117] Training loss: 0.10679115, Validation loss: 0.10732031, Gradient norm: 1.02896214
INFO:root:[  118] Training loss: 0.10570698, Validation loss: 0.10559785, Gradient norm: 0.65436850
INFO:root:[  119] Training loss: 0.10446278, Validation loss: 0.10433850, Gradient norm: 0.63816035
INFO:root:[  120] Training loss: 0.10347471, Validation loss: 0.10290270, Gradient norm: 0.61002830
INFO:root:[  121] Training loss: 0.10251162, Validation loss: 0.10159811, Gradient norm: 0.71387583
INFO:root:[  122] Training loss: 0.10140469, Validation loss: 0.10125845, Gradient norm: 0.53198527
INFO:root:[  123] Training loss: 0.10070249, Validation loss: 0.10036436, Gradient norm: 0.85555548
INFO:root:[  124] Training loss: 0.09961422, Validation loss: 0.09985887, Gradient norm: 0.68268741
INFO:root:[  125] Training loss: 0.09875463, Validation loss: 0.09874139, Gradient norm: 1.06325201
INFO:root:[  126] Training loss: 0.09786418, Validation loss: 0.09771639, Gradient norm: 0.89065421
INFO:root:[  127] Training loss: 0.09663399, Validation loss: 0.09620953, Gradient norm: 0.63649478
INFO:root:[  128] Training loss: 0.09597769, Validation loss: 0.09623316, Gradient norm: 0.96453194
INFO:root:[  129] Training loss: 0.09520842, Validation loss: 0.09502061, Gradient norm: 1.06721311
INFO:root:[  130] Training loss: 0.09390496, Validation loss: 0.09450788, Gradient norm: 0.73857352
INFO:root:[  131] Training loss: 0.09345599, Validation loss: 0.09267765, Gradient norm: 1.16740878
INFO:root:[  132] Training loss: 0.09225547, Validation loss: 0.09257103, Gradient norm: 0.70693677
INFO:root:[  133] Training loss: 0.09187927, Validation loss: 0.09162120, Gradient norm: 1.20069680
INFO:root:[  134] Training loss: 0.09091781, Validation loss: 0.09171337, Gradient norm: 1.03838651
INFO:root:[  135] Training loss: 0.09000186, Validation loss: 0.08975122, Gradient norm: 1.07109952
INFO:root:[  136] Training loss: 0.08965786, Validation loss: 0.08958656, Gradient norm: 1.34360968
INFO:root:[  137] Training loss: 0.08872135, Validation loss: 0.08782500, Gradient norm: 1.28768203
INFO:root:[  138] Training loss: 0.08767565, Validation loss: 0.08724332, Gradient norm: 0.75747752
INFO:root:[  139] Training loss: 0.08698930, Validation loss: 0.08689720, Gradient norm: 0.76479315
INFO:root:[  140] Training loss: 0.08628508, Validation loss: 0.08685554, Gradient norm: 0.96542599
INFO:root:[  141] Training loss: 0.08589268, Validation loss: 0.08605559, Gradient norm: 1.63961725
INFO:root:[  142] Training loss: 0.08502863, Validation loss: 0.08494201, Gradient norm: 1.47678408
INFO:root:[  143] Training loss: 0.08451762, Validation loss: 0.08353409, Gradient norm: 1.43313710
INFO:root:[  144] Training loss: 0.08348751, Validation loss: 0.08447314, Gradient norm: 0.96611220
INFO:root:[  145] Training loss: 0.08298099, Validation loss: 0.08261638, Gradient norm: 1.21018306
INFO:root:[  146] Training loss: 0.08238755, Validation loss: 0.08353506, Gradient norm: 1.49623880
INFO:root:[  147] Training loss: 0.08162022, Validation loss: 0.08291481, Gradient norm: 1.37957331
INFO:root:[  148] Training loss: 0.08137747, Validation loss: 0.08188775, Gradient norm: 1.95339376
INFO:root:[  149] Training loss: 0.08031643, Validation loss: 0.07997563, Gradient norm: 1.17782299
INFO:root:[  150] Training loss: 0.07975302, Validation loss: 0.08029734, Gradient norm: 0.94717167
INFO:root:[  151] Training loss: 0.07927393, Validation loss: 0.07895467, Gradient norm: 1.57858159
INFO:root:[  152] Training loss: 0.07873146, Validation loss: 0.07866106, Gradient norm: 1.64653404
INFO:root:[  153] Training loss: 0.07774141, Validation loss: 0.07780115, Gradient norm: 1.23337075
INFO:root:[  154] Training loss: 0.07734592, Validation loss: 0.07708131, Gradient norm: 1.39885302
INFO:root:[  155] Training loss: 0.07678201, Validation loss: 0.07714796, Gradient norm: 1.91766655
INFO:root:[  156] Training loss: 0.07623805, Validation loss: 0.07726670, Gradient norm: 1.76012725
INFO:root:[  157] Training loss: 0.07573906, Validation loss: 0.07470485, Gradient norm: 2.03231694
INFO:root:[  158] Training loss: 0.07496847, Validation loss: 0.07508652, Gradient norm: 1.22483497
INFO:root:[  159] Training loss: 0.07440258, Validation loss: 0.07507007, Gradient norm: 1.38073229
INFO:root:[  160] Training loss: 0.07425362, Validation loss: 0.07516084, Gradient norm: 2.02838722
INFO:root:[  161] Training loss: 0.07365919, Validation loss: 0.07383709, Gradient norm: 1.45155742
INFO:root:[  162] Training loss: 0.07331293, Validation loss: 0.07349449, Gradient norm: 2.32091270
INFO:root:[  163] Training loss: 0.07291651, Validation loss: 0.07235673, Gradient norm: 2.07533903
INFO:root:[  164] Training loss: 0.07227175, Validation loss: 0.07224281, Gradient norm: 2.29300922
INFO:root:[  165] Training loss: 0.07176069, Validation loss: 0.07152589, Gradient norm: 1.43074529
INFO:root:[  166] Training loss: 0.07124490, Validation loss: 0.07077676, Gradient norm: 2.30205256
INFO:root:[  167] Training loss: 0.07089725, Validation loss: 0.07116312, Gradient norm: 1.70590907
INFO:root:[  168] Training loss: 0.07041276, Validation loss: 0.07038011, Gradient norm: 1.91521248
INFO:root:[  169] Training loss: 0.07028031, Validation loss: 0.06927273, Gradient norm: 2.76486761
INFO:root:[  170] Training loss: 0.06963178, Validation loss: 0.07026767, Gradient norm: 2.58011820
INFO:root:[  171] Training loss: 0.06940178, Validation loss: 0.06973512, Gradient norm: 2.19554328
INFO:root:[  172] Training loss: 0.06883164, Validation loss: 0.06866355, Gradient norm: 2.44190302
INFO:root:[  173] Training loss: 0.06874317, Validation loss: 0.06920943, Gradient norm: 3.31912293
INFO:root:[  174] Training loss: 0.06844841, Validation loss: 0.06956958, Gradient norm: 3.34811508
INFO:root:[  175] Training loss: 0.06803904, Validation loss: 0.06901480, Gradient norm: 3.14600887
INFO:root:[  176] Training loss: 0.06796224, Validation loss: 0.06805470, Gradient norm: 4.03993861
INFO:root:[  177] Training loss: 0.06706662, Validation loss: 0.06840174, Gradient norm: 1.87869081
INFO:root:[  178] Training loss: 0.06703982, Validation loss: 0.06784101, Gradient norm: 3.37703240
INFO:root:[  179] Training loss: 0.06677827, Validation loss: 0.06687343, Gradient norm: 3.14349802
INFO:root:[  180] Training loss: 0.06686767, Validation loss: 0.06860426, Gradient norm: 4.54737572
INFO:root:[  181] Training loss: 0.06634572, Validation loss: 0.06703252, Gradient norm: 3.33704611
INFO:root:[  182] Training loss: 0.06577336, Validation loss: 0.06710744, Gradient norm: 3.21265426
INFO:root:[  183] Training loss: 0.06562729, Validation loss: 0.06581309, Gradient norm: 2.71569014
INFO:root:[  184] Training loss: 0.06510971, Validation loss: 0.06570803, Gradient norm: 3.38430502
INFO:root:[  185] Training loss: 0.06503146, Validation loss: 0.06540870, Gradient norm: 4.11614696
INFO:root:[  186] Training loss: 0.06560876, Validation loss: 0.06552860, Gradient norm: 6.53661477
INFO:root:[  187] Training loss: 0.06485129, Validation loss: 0.06380006, Gradient norm: 6.22775294
INFO:root:[  188] Training loss: 0.06440331, Validation loss: 0.06478687, Gradient norm: 3.85879159
INFO:root:[  189] Training loss: 0.06439185, Validation loss: 0.06401578, Gradient norm: 5.63614030
INFO:root:[  190] Training loss: 0.06372812, Validation loss: 0.06378725, Gradient norm: 3.60197325
INFO:root:[  191] Training loss: 0.06335825, Validation loss: 0.06291178, Gradient norm: 3.96375551
INFO:root:[  192] Training loss: 0.06390137, Validation loss: 0.06392474, Gradient norm: 8.69747233
INFO:root:[  193] Training loss: 0.06291759, Validation loss: 0.06319419, Gradient norm: 5.69276934
INFO:root:[  194] Training loss: 0.06273883, Validation loss: 0.06248298, Gradient norm: 5.26087728
INFO:root:[  195] Training loss: 0.06276187, Validation loss: 0.06358556, Gradient norm: 7.34884916
INFO:root:[  196] Training loss: 0.06242272, Validation loss: 0.06367594, Gradient norm: 5.79130425
INFO:root:[  197] Training loss: 0.06226145, Validation loss: 0.06161577, Gradient norm: 6.07953312
INFO:root:[  198] Training loss: 0.06167796, Validation loss: 0.06188051, Gradient norm: 6.37959374
INFO:root:[  199] Training loss: 0.06186466, Validation loss: 0.06108347, Gradient norm: 8.81762747
INFO:root:[  200] Training loss: 0.06123874, Validation loss: 0.06160189, Gradient norm: 6.47000157
INFO:root:[  201] Training loss: 0.06154529, Validation loss: 0.06063139, Gradient norm: 8.29868754
INFO:root:[  202] Training loss: 0.06086302, Validation loss: 0.06326272, Gradient norm: 9.90597940
INFO:root:[  203] Training loss: 0.06075943, Validation loss: 0.06106362, Gradient norm: 9.82424185
INFO:root:[  204] Training loss: 0.06101795, Validation loss: 0.06081770, Gradient norm: 12.87208676
INFO:root:[  205] Training loss: 0.06019638, Validation loss: 0.06285643, Gradient norm: 10.28913537
INFO:root:[  206] Training loss: 0.06103798, Validation loss: 0.05975652, Gradient norm: 16.17236910
INFO:root:[  207] Training loss: 0.05957259, Validation loss: 0.05945424, Gradient norm: 7.50748004
INFO:root:[  208] Training loss: 0.05967604, Validation loss: 0.05968969, Gradient norm: 10.40759607
INFO:root:[  209] Training loss: 0.05967367, Validation loss: 0.06142391, Gradient norm: 10.55354124
INFO:root:[  210] Training loss: 0.06018667, Validation loss: 0.05961301, Gradient norm: 15.25436049
INFO:root:[  211] Training loss: 0.05885238, Validation loss: 0.05952266, Gradient norm: 8.44791601
INFO:root:[  212] Training loss: 0.05922270, Validation loss: 0.06496245, Gradient norm: 15.00303372
INFO:root:[  213] Training loss: 0.05826165, Validation loss: 0.05845547, Gradient norm: 11.48393329
INFO:root:[  214] Training loss: 0.05794631, Validation loss: 0.05965689, Gradient norm: 8.49745836
INFO:root:[  215] Training loss: 0.05937428, Validation loss: 0.05863059, Gradient norm: 19.02201385
INFO:root:[  216] Training loss: 0.05762653, Validation loss: 0.05700888, Gradient norm: 10.11507474
INFO:root:[  217] Training loss: 0.05786224, Validation loss: 0.05706322, Gradient norm: 15.85011784
INFO:root:[  218] Training loss: 0.05745580, Validation loss: 0.05661368, Gradient norm: 15.12202242
INFO:root:[  219] Training loss: 0.05752987, Validation loss: 0.05928312, Gradient norm: 17.34260213
INFO:root:[  220] Training loss: 0.05709821, Validation loss: 0.05725100, Gradient norm: 14.00498691
INFO:root:[  221] Training loss: 0.05669183, Validation loss: 0.05677270, Gradient norm: 16.16497312
INFO:root:[  222] Training loss: 0.05683853, Validation loss: 0.05662328, Gradient norm: 17.25136564
INFO:root:[  223] Training loss: 0.05683708, Validation loss: 0.05641982, Gradient norm: 19.40618302
INFO:root:[  224] Training loss: 0.05618949, Validation loss: 0.06111419, Gradient norm: 16.96165401
INFO:root:[  225] Training loss: 0.05634357, Validation loss: 0.05583918, Gradient norm: 19.27508764
INFO:root:[  226] Training loss: 0.05654584, Validation loss: 0.06027678, Gradient norm: 22.04218742
INFO:root:[  227] Training loss: 0.05548134, Validation loss: 0.05745268, Gradient norm: 15.17975093
INFO:root:[  228] Training loss: 0.05597774, Validation loss: 0.05498794, Gradient norm: 22.16579453
INFO:root:[  229] Training loss: 0.05637019, Validation loss: 0.05675078, Gradient norm: 23.43328374
INFO:root:[  230] Training loss: 0.05682396, Validation loss: 0.05468898, Gradient norm: 28.29092140
INFO:root:[  231] Training loss: 0.05505862, Validation loss: 0.05514978, Gradient norm: 19.94680318
INFO:root:[  232] Training loss: 0.05611056, Validation loss: 0.05336101, Gradient norm: 27.06351244
INFO:root:[  233] Training loss: 0.05489317, Validation loss: 0.05322746, Gradient norm: 21.97230379
INFO:root:[  234] Training loss: 0.05443340, Validation loss: 0.05467569, Gradient norm: 16.06221974
INFO:root:[  235] Training loss: 0.05521300, Validation loss: 0.05361772, Gradient norm: 26.53694571
INFO:root:[  236] Training loss: 0.05438111, Validation loss: 0.05775609, Gradient norm: 19.33267966
INFO:root:[  237] Training loss: 0.05661897, Validation loss: 0.05402380, Gradient norm: 35.65440106
INFO:root:[  238] Training loss: 0.05434894, Validation loss: 0.05451486, Gradient norm: 24.48173829
INFO:root:[  239] Training loss: 0.05413223, Validation loss: 0.05468996, Gradient norm: 22.76565361
INFO:root:[  240] Training loss: 0.05566104, Validation loss: 0.05324728, Gradient norm: 34.08585190
INFO:root:[  241] Training loss: 0.05454198, Validation loss: 0.05480845, Gradient norm: 28.41029396
INFO:root:[  242] Training loss: 0.05366456, Validation loss: 0.05389504, Gradient norm: 23.89702285
INFO:root:EP 242: Early stopping
INFO:root:Training the model took 2811.584s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.48724
INFO:root:EnergyScoreTrain: 0.33256
INFO:root:CoverageTrain: 0.98269
INFO:root:IntervalWidthTrain: 0.02686
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.45788
INFO:root:EnergyScoreValidation: 0.31274
INFO:root:CoverageValidation: 0.98272
INFO:root:IntervalWidthValidation: 0.02708
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.42853
INFO:root:EnergyScoreTest: 0.2927
INFO:root:CoverageTest: 0.98294
INFO:root:IntervalWidthTest: 0.02633
INFO:root:###21 out of 21 training parameter combinations ###
INFO:root:Training parameters: {'model': 'FNO', 'uncertainty_quantification': 'dropout', 'batch_size': 16, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'no', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 10, 'weight_decay': 0, 'dropout': 0.5, 'fourier_dropout': None, 'hidden_channels': 20, 'projection_channels': 128, 'lifting_channels': 128, 'n_modes': (10, 12, 12), 'n_samples': 3}
INFO:root:NumberParameters: 2695717
INFO:root:Memory allocated: 232783872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 2.75242946, Validation loss: 2.12715408, Gradient norm: 5.41859883
INFO:root:[    2] Training loss: 1.60423260, Validation loss: 1.38779242, Gradient norm: 4.20515132
INFO:root:[    3] Training loss: 1.22763184, Validation loss: 1.11195326, Gradient norm: 2.71715152
INFO:root:[    4] Training loss: 1.02298979, Validation loss: 0.93629639, Gradient norm: 2.12113158
INFO:root:[    5] Training loss: 0.85789772, Validation loss: 0.80540649, Gradient norm: 2.01416265
INFO:root:[    6] Training loss: 0.74702586, Validation loss: 0.70759803, Gradient norm: 1.75345239
INFO:root:[    7] Training loss: 0.66734950, Validation loss: 0.63473517, Gradient norm: 1.47705679
INFO:root:[    8] Training loss: 0.60454031, Validation loss: 0.57901946, Gradient norm: 1.32852114
INFO:root:[    9] Training loss: 0.56745887, Validation loss: 0.54478489, Gradient norm: 1.31591603
INFO:root:[   10] Training loss: 0.52399888, Validation loss: 0.50576121, Gradient norm: 1.08062968
INFO:root:[   11] Training loss: 0.48841455, Validation loss: 0.47079802, Gradient norm: 1.03973983
INFO:root:[   12] Training loss: 0.46418020, Validation loss: 0.45095135, Gradient norm: 0.99040987
INFO:root:[   13] Training loss: 0.44134386, Validation loss: 0.42911157, Gradient norm: 0.91672851
INFO:root:[   14] Training loss: 0.42192603, Validation loss: 0.41569613, Gradient norm: 0.90417725
INFO:root:[   15] Training loss: 0.40829820, Validation loss: 0.40156281, Gradient norm: 0.82096718
INFO:root:[   16] Training loss: 0.39447530, Validation loss: 0.39149577, Gradient norm: 0.73824455
INFO:root:[   17] Training loss: 0.38270762, Validation loss: 0.37805673, Gradient norm: 0.63156916
INFO:root:[   18] Training loss: 0.37474142, Validation loss: 0.37045957, Gradient norm: 0.61885214
INFO:root:[   19] Training loss: 0.36752512, Validation loss: 0.36457223, Gradient norm: 0.66432748
INFO:root:[   20] Training loss: 0.36026529, Validation loss: 0.35987544, Gradient norm: 0.75053102
INFO:root:[   21] Training loss: 0.35444160, Validation loss: 0.35308598, Gradient norm: 0.64432125
INFO:root:[   22] Training loss: 0.34948564, Validation loss: 0.34750172, Gradient norm: 0.64926407
INFO:root:[   23] Training loss: 0.34494352, Validation loss: 0.34233095, Gradient norm: 0.64085508
INFO:root:[   24] Training loss: 0.34028786, Validation loss: 0.33857014, Gradient norm: 0.63919268
INFO:root:[   25] Training loss: 0.33644166, Validation loss: 0.33526222, Gradient norm: 0.69413637
INFO:root:[   26] Training loss: 0.33205376, Validation loss: 0.33134592, Gradient norm: 0.79393587
INFO:root:[   27] Training loss: 0.32786274, Validation loss: 0.32690238, Gradient norm: 0.70709022
INFO:root:[   28] Training loss: 0.32460083, Validation loss: 0.32397752, Gradient norm: 0.81919105
INFO:root:[   29] Training loss: 0.32024015, Validation loss: 0.31943493, Gradient norm: 0.76143425
INFO:root:[   30] Training loss: 0.31647984, Validation loss: 0.31559155, Gradient norm: 0.70489828
INFO:root:[   31] Training loss: 0.31305362, Validation loss: 0.31253750, Gradient norm: 0.72955850
INFO:root:[   32] Training loss: 0.30970643, Validation loss: 0.31065211, Gradient norm: 0.60494697
INFO:root:[   33] Training loss: 0.30687090, Validation loss: 0.30585210, Gradient norm: 0.77312007
INFO:root:[   34] Training loss: 0.30365881, Validation loss: 0.30427462, Gradient norm: 0.94775396
INFO:root:[   35] Training loss: 0.30044178, Validation loss: 0.29817174, Gradient norm: 0.92301070
INFO:root:[   36] Training loss: 0.29723087, Validation loss: 0.29700317, Gradient norm: 0.91064768
INFO:root:[   37] Training loss: 0.29356697, Validation loss: 0.29266104, Gradient norm: 0.86756613
INFO:root:[   38] Training loss: 0.29097709, Validation loss: 0.28990831, Gradient norm: 0.79170071
INFO:root:[   39] Training loss: 0.28781063, Validation loss: 0.28947480, Gradient norm: 1.09568812
INFO:root:[   40] Training loss: 0.28487167, Validation loss: 0.28322270, Gradient norm: 0.84657607
INFO:root:[   41] Training loss: 0.28133446, Validation loss: 0.28107375, Gradient norm: 0.74614171
INFO:root:[   42] Training loss: 0.27816938, Validation loss: 0.27874723, Gradient norm: 0.95597686
INFO:root:[   43] Training loss: 0.27543910, Validation loss: 0.27453382, Gradient norm: 1.04802943
INFO:root:[   44] Training loss: 0.27269715, Validation loss: 0.27285738, Gradient norm: 0.90969667
INFO:root:[   45] Training loss: 0.26979586, Validation loss: 0.26769966, Gradient norm: 0.93201981
INFO:root:[   46] Training loss: 0.26714861, Validation loss: 0.26596255, Gradient norm: 1.03812223
INFO:root:[   47] Training loss: 0.26396696, Validation loss: 0.26318181, Gradient norm: 1.00769840
INFO:root:[   48] Training loss: 0.26093592, Validation loss: 0.25956895, Gradient norm: 1.20322389
INFO:root:[   49] Training loss: 0.25733513, Validation loss: 0.25677669, Gradient norm: 1.21715393
INFO:root:[   50] Training loss: 0.25505413, Validation loss: 0.25513695, Gradient norm: 1.06034018
INFO:root:[   51] Training loss: 0.25320962, Validation loss: 0.25218958, Gradient norm: 2.02206566
INFO:root:[   52] Training loss: 0.24932838, Validation loss: 0.24858139, Gradient norm: 1.05620957
INFO:root:[   53] Training loss: 0.24759545, Validation loss: 0.24598930, Gradient norm: 1.21532567
INFO:root:[   54] Training loss: 0.24466978, Validation loss: 0.24583655, Gradient norm: 1.34314285
INFO:root:[   55] Training loss: 0.24241645, Validation loss: 0.24121546, Gradient norm: 1.63361483
INFO:root:[   56] Training loss: 0.23979135, Validation loss: 0.23940491, Gradient norm: 1.29283101
INFO:root:[   57] Training loss: 0.23663827, Validation loss: 0.23600121, Gradient norm: 1.34692134
INFO:root:[   58] Training loss: 0.23424777, Validation loss: 0.23411252, Gradient norm: 1.23538896
INFO:root:[   59] Training loss: 0.23169567, Validation loss: 0.23142711, Gradient norm: 1.40917829
INFO:root:[   60] Training loss: 0.22958001, Validation loss: 0.22998674, Gradient norm: 1.14571760
INFO:root:[   61] Training loss: 0.22706148, Validation loss: 0.22484026, Gradient norm: 1.71750871
INFO:root:[   62] Training loss: 0.22456079, Validation loss: 0.22289449, Gradient norm: 1.49750997
INFO:root:[   63] Training loss: 0.22259107, Validation loss: 0.22283587, Gradient norm: 1.77272233
INFO:root:[   64] Training loss: 0.21992261, Validation loss: 0.21810234, Gradient norm: 1.34889011
INFO:root:[   65] Training loss: 0.21786896, Validation loss: 0.21687712, Gradient norm: 1.47065961
INFO:root:[   66] Training loss: 0.21550779, Validation loss: 0.21520183, Gradient norm: 1.55409651
INFO:root:[   67] Training loss: 0.21343066, Validation loss: 0.21346559, Gradient norm: 1.71652172
INFO:root:[   68] Training loss: 0.21098373, Validation loss: 0.21064840, Gradient norm: 1.54493394
INFO:root:[   69] Training loss: 0.20861026, Validation loss: 0.20880539, Gradient norm: 1.60583134
INFO:root:[   70] Training loss: 0.20678816, Validation loss: 0.20565282, Gradient norm: 1.94862377
INFO:root:[   71] Training loss: 0.20426756, Validation loss: 0.20367009, Gradient norm: 1.35785548
INFO:root:[   72] Training loss: 0.20180706, Validation loss: 0.20111394, Gradient norm: 1.33296459
INFO:root:[   73] Training loss: 0.19968866, Validation loss: 0.19923978, Gradient norm: 1.22496192
INFO:root:[   74] Training loss: 0.19751475, Validation loss: 0.19556664, Gradient norm: 1.68685379
INFO:root:[   75] Training loss: 0.19534623, Validation loss: 0.19508160, Gradient norm: 1.77035920
INFO:root:[   76] Training loss: 0.19336179, Validation loss: 0.19347843, Gradient norm: 1.90551260
INFO:root:[   77] Training loss: 0.19163212, Validation loss: 0.19078759, Gradient norm: 2.62589175
INFO:root:[   78] Training loss: 0.18958678, Validation loss: 0.18947108, Gradient norm: 1.84344847
INFO:root:[   79] Training loss: 0.18783600, Validation loss: 0.18792241, Gradient norm: 2.62792680
INFO:root:[   80] Training loss: 0.18567075, Validation loss: 0.18556422, Gradient norm: 2.21185458
INFO:root:[   81] Training loss: 0.18366582, Validation loss: 0.18443523, Gradient norm: 1.68079829
INFO:root:[   82] Training loss: 0.18223816, Validation loss: 0.18149325, Gradient norm: 2.45679594
INFO:root:[   83] Training loss: 0.17970559, Validation loss: 0.17803467, Gradient norm: 1.70403837
INFO:root:[   84] Training loss: 0.17815366, Validation loss: 0.17984016, Gradient norm: 2.35912972
INFO:root:[   85] Training loss: 0.17686407, Validation loss: 0.17670415, Gradient norm: 3.29771903
INFO:root:[   86] Training loss: 0.17432529, Validation loss: 0.17462904, Gradient norm: 2.61589003
INFO:root:[   87] Training loss: 0.17260246, Validation loss: 0.17160709, Gradient norm: 2.29808843
INFO:root:[   88] Training loss: 0.17063422, Validation loss: 0.16981805, Gradient norm: 2.21994753
INFO:root:[   89] Training loss: 0.16891905, Validation loss: 0.16931428, Gradient norm: 1.99929646
INFO:root:[   90] Training loss: 0.16685757, Validation loss: 0.16514902, Gradient norm: 1.51114304
INFO:root:[   91] Training loss: 0.16497530, Validation loss: 0.16482468, Gradient norm: 1.76236776
INFO:root:[   92] Training loss: 0.16325468, Validation loss: 0.16268987, Gradient norm: 2.70071559
INFO:root:[   93] Training loss: 0.16143938, Validation loss: 0.16015197, Gradient norm: 1.82563662
INFO:root:[   94] Training loss: 0.15941996, Validation loss: 0.15929302, Gradient norm: 1.78951589
INFO:root:[   95] Training loss: 0.15744254, Validation loss: 0.15562123, Gradient norm: 2.20540392
INFO:root:[   96] Training loss: 0.15591735, Validation loss: 0.15619566, Gradient norm: 3.24344107
INFO:root:[   97] Training loss: 0.15411956, Validation loss: 0.15408503, Gradient norm: 2.70883119
INFO:root:[   98] Training loss: 0.15247312, Validation loss: 0.15237435, Gradient norm: 2.21090341
INFO:root:[   99] Training loss: 0.15105150, Validation loss: 0.15005697, Gradient norm: 3.27175747
INFO:root:[  100] Training loss: 0.14972751, Validation loss: 0.14902466, Gradient norm: 4.09899493
INFO:root:[  101] Training loss: 0.14727299, Validation loss: 0.14695114, Gradient norm: 2.13927391
INFO:root:[  102] Training loss: 0.14583766, Validation loss: 0.14603717, Gradient norm: 2.41125435
INFO:root:[  103] Training loss: 0.14437342, Validation loss: 0.14353638, Gradient norm: 3.00565672
INFO:root:[  104] Training loss: 0.14266122, Validation loss: 0.14173514, Gradient norm: 3.39059133
INFO:root:[  105] Training loss: 0.14076249, Validation loss: 0.14037472, Gradient norm: 2.43459989
INFO:root:[  106] Training loss: 0.13939406, Validation loss: 0.13942365, Gradient norm: 3.24487369
INFO:root:[  107] Training loss: 0.13758836, Validation loss: 0.13788311, Gradient norm: 2.44160841
INFO:root:[  108] Training loss: 0.13641457, Validation loss: 0.13555073, Gradient norm: 3.57686570
INFO:root:[  109] Training loss: 0.13431233, Validation loss: 0.13493749, Gradient norm: 2.85885484
INFO:root:[  110] Training loss: 0.13349677, Validation loss: 0.13223807, Gradient norm: 4.02749183
INFO:root:[  111] Training loss: 0.13170551, Validation loss: 0.13128295, Gradient norm: 3.39167098
INFO:root:[  112] Training loss: 0.13056129, Validation loss: 0.13052923, Gradient norm: 3.99617792
INFO:root:[  113] Training loss: 0.12880634, Validation loss: 0.12794776, Gradient norm: 3.64781253
INFO:root:[  114] Training loss: 0.12785166, Validation loss: 0.12623748, Gradient norm: 4.31500598
INFO:root:[  115] Training loss: 0.12584066, Validation loss: 0.12473919, Gradient norm: 2.59364969
INFO:root:[  116] Training loss: 0.12449335, Validation loss: 0.12485548, Gradient norm: 3.46592242
INFO:root:[  117] Training loss: 0.12336482, Validation loss: 0.12284384, Gradient norm: 3.79751110
INFO:root:[  118] Training loss: 0.12234205, Validation loss: 0.12266319, Gradient norm: 3.98238666
INFO:root:[  119] Training loss: 0.12112712, Validation loss: 0.12239637, Gradient norm: 5.12618766
INFO:root:[  120] Training loss: 0.11997991, Validation loss: 0.12100670, Gradient norm: 5.73009876
INFO:root:[  121] Training loss: 0.11862652, Validation loss: 0.11808891, Gradient norm: 5.25083043
INFO:root:[  122] Training loss: 0.11800775, Validation loss: 0.11850328, Gradient norm: 6.64911137
INFO:root:[  123] Training loss: 0.11599697, Validation loss: 0.11635778, Gradient norm: 4.32501532
INFO:root:[  124] Training loss: 0.11579053, Validation loss: 0.11534230, Gradient norm: 5.80157587
INFO:root:[  125] Training loss: 0.11431828, Validation loss: 0.11517223, Gradient norm: 5.82984812
INFO:root:[  126] Training loss: 0.11319586, Validation loss: 0.11331602, Gradient norm: 5.21235139
INFO:root:[  127] Training loss: 0.11202961, Validation loss: 0.11084693, Gradient norm: 4.62717559
INFO:root:[  128] Training loss: 0.11107167, Validation loss: 0.11275792, Gradient norm: 5.47803656
INFO:root:[  129] Training loss: 0.10993723, Validation loss: 0.11055099, Gradient norm: 4.49073725
INFO:root:[  130] Training loss: 0.10881833, Validation loss: 0.10849631, Gradient norm: 4.42699090
INFO:root:[  131] Training loss: 0.10783886, Validation loss: 0.10908927, Gradient norm: 4.16003682
INFO:root:[  132] Training loss: 0.10767312, Validation loss: 0.10943420, Gradient norm: 7.19955247
INFO:root:[  133] Training loss: 0.10658225, Validation loss: 0.10585389, Gradient norm: 5.93296993
INFO:root:[  134] Training loss: 0.10552793, Validation loss: 0.10557900, Gradient norm: 7.18536235
INFO:root:[  135] Training loss: 0.10431070, Validation loss: 0.10499851, Gradient norm: 5.19825774
INFO:root:[  136] Training loss: 0.10333026, Validation loss: 0.10281541, Gradient norm: 7.05900210
INFO:root:[  137] Training loss: 0.10272294, Validation loss: 0.10205943, Gradient norm: 5.02495065
INFO:root:[  138] Training loss: 0.10195810, Validation loss: 0.10064095, Gradient norm: 6.21282476
INFO:root:[  139] Training loss: 0.10120410, Validation loss: 0.10212403, Gradient norm: 7.96752270
INFO:root:[  140] Training loss: 0.10014983, Validation loss: 0.10087061, Gradient norm: 7.58234577
INFO:root:[  141] Training loss: 0.09943391, Validation loss: 0.09990600, Gradient norm: 6.23328718
INFO:root:[  142] Training loss: 0.09894212, Validation loss: 0.09795013, Gradient norm: 8.21628265
INFO:root:[  143] Training loss: 0.09774697, Validation loss: 0.09978894, Gradient norm: 5.81305335
INFO:root:[  144] Training loss: 0.09712427, Validation loss: 0.09876140, Gradient norm: 6.30859298
INFO:root:[  145] Training loss: 0.09678148, Validation loss: 0.09863120, Gradient norm: 8.00884248
INFO:root:[  146] Training loss: 0.09625120, Validation loss: 0.09693435, Gradient norm: 9.91149810
INFO:root:[  147] Training loss: 0.09593786, Validation loss: 0.09527026, Gradient norm: 9.92054221
INFO:root:[  148] Training loss: 0.09542673, Validation loss: 0.09490967, Gradient norm: 12.38860051
INFO:root:[  149] Training loss: 0.09419742, Validation loss: 0.09536545, Gradient norm: 9.40066823
INFO:root:[  150] Training loss: 0.09404702, Validation loss: 0.09529466, Gradient norm: 11.44704347
INFO:root:[  151] Training loss: 0.09408758, Validation loss: 0.09382656, Gradient norm: 14.62586919
INFO:root:[  152] Training loss: 0.09260780, Validation loss: 0.09120256, Gradient norm: 11.12347402
INFO:root:[  153] Training loss: 0.09203655, Validation loss: 0.09122098, Gradient norm: 8.99526674
INFO:root:[  154] Training loss: 0.09113975, Validation loss: 0.08972486, Gradient norm: 7.84280265
INFO:root:[  155] Training loss: 0.09123491, Validation loss: 0.08959725, Gradient norm: 12.81620995
INFO:root:[  156] Training loss: 0.09042103, Validation loss: 0.08979553, Gradient norm: 10.66653785
INFO:root:[  157] Training loss: 0.09011596, Validation loss: 0.09031692, Gradient norm: 12.77374470
INFO:root:[  158] Training loss: 0.09232018, Validation loss: 0.09031449, Gradient norm: 24.01017158
INFO:root:[  159] Training loss: 0.08892857, Validation loss: 0.08807436, Gradient norm: 10.85562325
INFO:root:[  160] Training loss: 0.08914282, Validation loss: 0.08773442, Gradient norm: 13.74086408
INFO:root:[  161] Training loss: 0.08828084, Validation loss: 0.08814597, Gradient norm: 13.10362593
INFO:root:[  162] Training loss: 0.08898352, Validation loss: 0.09117268, Gradient norm: 18.90406480
INFO:root:[  163] Training loss: 0.08805278, Validation loss: 0.08737449, Gradient norm: 14.70246383
INFO:root:[  164] Training loss: 0.08721528, Validation loss: 0.08728085, Gradient norm: 13.93499351
INFO:root:[  165] Training loss: 0.08661559, Validation loss: 0.09235284, Gradient norm: 15.34286558
INFO:root:[  166] Training loss: 0.08641067, Validation loss: 0.08541105, Gradient norm: 16.60357616
INFO:root:[  167] Training loss: 0.08660494, Validation loss: 0.08583822, Gradient norm: 18.38261221
INFO:root:[  168] Training loss: 0.08524211, Validation loss: 0.08434466, Gradient norm: 13.62393480
INFO:root:[  169] Training loss: 0.08568689, Validation loss: 0.08432482, Gradient norm: 19.60531582
INFO:root:[  170] Training loss: 0.08493353, Validation loss: 0.08479570, Gradient norm: 16.39390952
INFO:root:[  171] Training loss: 0.08493897, Validation loss: 0.08688164, Gradient norm: 19.21624013
INFO:root:[  172] Training loss: 0.08493224, Validation loss: 0.08708525, Gradient norm: 21.54735166
INFO:root:[  173] Training loss: 0.08351772, Validation loss: 0.09391850, Gradient norm: 15.66349236
INFO:root:[  174] Training loss: 0.08451634, Validation loss: 0.08300891, Gradient norm: 22.41532461
INFO:root:[  175] Training loss: 0.08293354, Validation loss: 0.08222218, Gradient norm: 12.99472481
INFO:root:[  176] Training loss: 0.08371241, Validation loss: 0.08665331, Gradient norm: 22.08540885
INFO:root:[  177] Training loss: 0.08364169, Validation loss: 0.08222077, Gradient norm: 24.60994568
INFO:root:[  178] Training loss: 0.08263491, Validation loss: 0.08153393, Gradient norm: 21.44101893
INFO:root:[  179] Training loss: 0.08241571, Validation loss: 0.08306480, Gradient norm: 21.86336716
INFO:root:[  180] Training loss: 0.08304163, Validation loss: 0.08221356, Gradient norm: 26.42278025
INFO:root:[  181] Training loss: 0.08103528, Validation loss: 0.08036067, Gradient norm: 17.46890343
INFO:root:[  182] Training loss: 0.08203909, Validation loss: 0.08005772, Gradient norm: 26.97867250
INFO:root:[  183] Training loss: 0.07987660, Validation loss: 0.08095130, Gradient norm: 15.94696437
INFO:root:[  184] Training loss: 0.07955625, Validation loss: 0.07976530, Gradient norm: 13.81073531
INFO:root:[  185] Training loss: 0.08129947, Validation loss: 0.08036080, Gradient norm: 29.54372793
INFO:root:[  186] Training loss: 0.07959577, Validation loss: 0.07838792, Gradient norm: 19.46815626
INFO:root:[  187] Training loss: 0.08022542, Validation loss: 0.07861932, Gradient norm: 27.01727113
INFO:root:[  188] Training loss: 0.07878092, Validation loss: 0.07913591, Gradient norm: 19.21328592
INFO:root:[  189] Training loss: 0.07912100, Validation loss: 0.07802920, Gradient norm: 24.83606930
INFO:root:[  190] Training loss: 0.07978942, Validation loss: 0.08054516, Gradient norm: 29.23717121
INFO:root:[  191] Training loss: 0.07821675, Validation loss: 0.07794230, Gradient norm: 20.68908203
INFO:root:[  192] Training loss: 0.07948976, Validation loss: 0.08036930, Gradient norm: 29.56337358
INFO:root:[  193] Training loss: 0.07878732, Validation loss: 0.07832037, Gradient norm: 27.28096055
INFO:root:[  194] Training loss: 0.07784776, Validation loss: 0.08053292, Gradient norm: 25.12921749
INFO:root:[  195] Training loss: 0.07875701, Validation loss: 0.07699232, Gradient norm: 33.81343466
INFO:root:[  196] Training loss: 0.07676001, Validation loss: 0.07635822, Gradient norm: 22.46806343
INFO:root:[  197] Training loss: 0.07966437, Validation loss: 0.07701440, Gradient norm: 38.46188387
INFO:root:[  198] Training loss: 0.07697784, Validation loss: 0.07656784, Gradient norm: 24.65036255
INFO:root:[  199] Training loss: 0.07640095, Validation loss: 0.07480322, Gradient norm: 25.33684728
INFO:root:[  200] Training loss: 0.07506287, Validation loss: 0.07839428, Gradient norm: 16.68450677
INFO:root:[  201] Training loss: 0.07519096, Validation loss: 0.07616415, Gradient norm: 19.59935738
INFO:root:[  202] Training loss: 0.07583145, Validation loss: 0.08319369, Gradient norm: 27.65491425
INFO:root:[  203] Training loss: 0.07578435, Validation loss: 0.07814947, Gradient norm: 28.75610706
INFO:root:[  204] Training loss: 0.07616458, Validation loss: 0.08960414, Gradient norm: 25.80718440
INFO:root:[  205] Training loss: 0.07728134, Validation loss: 0.07955622, Gradient norm: 40.89190222
INFO:root:[  206] Training loss: 0.07513310, Validation loss: 0.07411219, Gradient norm: 26.73392832
INFO:root:[  207] Training loss: 0.07432747, Validation loss: 0.07735575, Gradient norm: 22.70263476
INFO:root:[  208] Training loss: 0.07441811, Validation loss: 0.07415458, Gradient norm: 27.66387704
INFO:root:[  209] Training loss: 0.07638444, Validation loss: 0.07325058, Gradient norm: 41.73480058
INFO:root:[  210] Training loss: 0.07305121, Validation loss: 0.07511414, Gradient norm: 19.68384939
INFO:root:[  211] Training loss: 0.07373872, Validation loss: 0.07357830, Gradient norm: 25.82249376
INFO:root:[  212] Training loss: 0.07406610, Validation loss: 0.07493724, Gradient norm: 32.65955031
INFO:root:[  213] Training loss: 0.07301918, Validation loss: 0.07420163, Gradient norm: 23.60244182
INFO:root:[  214] Training loss: 0.07311335, Validation loss: 0.07522454, Gradient norm: 30.07665709
INFO:root:[  215] Training loss: 0.07368356, Validation loss: 0.07509525, Gradient norm: 33.32664661
INFO:root:[  216] Training loss: 0.07226045, Validation loss: 0.07222634, Gradient norm: 23.73407747
INFO:root:[  217] Training loss: 0.07217956, Validation loss: 0.07572760, Gradient norm: 25.47465454
INFO:root:[  218] Training loss: 0.07222371, Validation loss: 0.07976476, Gradient norm: 28.63922368
INFO:root:[  219] Training loss: 0.07321266, Validation loss: 0.07328542, Gradient norm: 35.70726538
INFO:root:[  220] Training loss: 0.07292446, Validation loss: 0.07621641, Gradient norm: 33.24176906
INFO:root:[  221] Training loss: 0.07204033, Validation loss: 0.07121062, Gradient norm: 33.00109119
INFO:root:[  222] Training loss: 0.07155651, Validation loss: 0.07278887, Gradient norm: 26.15155053
INFO:root:[  223] Training loss: 0.07174780, Validation loss: 0.06993093, Gradient norm: 31.78402742
INFO:root:[  224] Training loss: 0.07154279, Validation loss: 0.07046446, Gradient norm: 29.26176445
INFO:root:[  225] Training loss: 0.07117983, Validation loss: 0.07094601, Gradient norm: 29.70801011
INFO:root:[  226] Training loss: 0.07144545, Validation loss: 0.07441010, Gradient norm: 32.38832479
INFO:root:[  227] Training loss: 0.07176159, Validation loss: 0.07046347, Gradient norm: 35.91518741
INFO:root:[  228] Training loss: 0.07236546, Validation loss: 0.07008112, Gradient norm: 40.12869739
INFO:root:[  229] Training loss: 0.07091113, Validation loss: 0.07151556, Gradient norm: 29.78141686
INFO:root:[  230] Training loss: 0.07042107, Validation loss: 0.06989556, Gradient norm: 29.21554388
INFO:root:[  231] Training loss: 0.07126638, Validation loss: 0.06879186, Gradient norm: 34.41202969
INFO:root:[  232] Training loss: 0.07089662, Validation loss: 0.06930377, Gradient norm: 36.99703453
INFO:root:[  233] Training loss: 0.07049945, Validation loss: 0.07086015, Gradient norm: 30.26948598
INFO:root:[  234] Training loss: 0.07090314, Validation loss: 0.06944570, Gradient norm: 36.88305308
INFO:root:[  235] Training loss: 0.07163031, Validation loss: 0.07111881, Gradient norm: 37.95978837
INFO:root:[  236] Training loss: 0.07138635, Validation loss: 0.06861720, Gradient norm: 40.50410304
INFO:root:[  237] Training loss: 0.07024786, Validation loss: 0.07179041, Gradient norm: 30.46741093
INFO:root:[  238] Training loss: 0.07098233, Validation loss: 0.07159906, Gradient norm: 38.32146501
INFO:root:[  239] Training loss: 0.07116887, Validation loss: 0.07085717, Gradient norm: 42.35522176
INFO:root:[  240] Training loss: 0.07075142, Validation loss: 0.06910169, Gradient norm: 35.95117122
INFO:root:[  241] Training loss: 0.06985169, Validation loss: 0.07547203, Gradient norm: 33.11488472
INFO:root:[  242] Training loss: 0.07030185, Validation loss: 0.06936345, Gradient norm: 34.26904341
INFO:root:[  243] Training loss: 0.06977024, Validation loss: 0.07018120, Gradient norm: 30.43953892
INFO:root:[  244] Training loss: 0.07010828, Validation loss: 0.08508234, Gradient norm: 31.19919705
INFO:root:[  245] Training loss: 0.07160678, Validation loss: 0.07040184, Gradient norm: 44.53867732
INFO:root:EP 245: Early stopping
INFO:root:Training the model took 2842.154s.
INFO:root:Emptying the cuda cache took 0.031s.
INFO:root:Starting evaluation: model FNO & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.68255
INFO:root:EnergyScoreTrain: 0.45764
INFO:root:CoverageTrain: 0.97261
INFO:root:IntervalWidthTrain: 0.03341
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.6398
INFO:root:EnergyScoreValidation: 0.42904
INFO:root:CoverageValidation: 0.97268
INFO:root:IntervalWidthValidation: 0.03374
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.59908
INFO:root:EnergyScoreTest: 0.40153
INFO:root:CoverageTest: 0.97255
INFO:root:IntervalWidthTest: 0.03258
