INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file darcy_flow/uno_laplace.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'DarcyFlow', 'max_training_set_size': 10000, 'downscaling_factor': 2}
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 44040192
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.38803506, Validation loss: 0.18295943, Gradient norm: 7.55143373
INFO:root:[    2] Training loss: 0.19270722, Validation loss: 0.21921921, Gradient norm: 4.44572515
INFO:root:[    3] Training loss: 0.16793940, Validation loss: 0.13075253, Gradient norm: 3.39397597
INFO:root:[    4] Training loss: 0.15993265, Validation loss: 0.14545724, Gradient norm: 3.95670601
INFO:root:[    5] Training loss: 0.15210655, Validation loss: 0.13881261, Gradient norm: 4.10667869
INFO:root:[    6] Training loss: 0.14428386, Validation loss: 0.17013057, Gradient norm: 3.69584733
INFO:root:[    7] Training loss: 0.14038677, Validation loss: 0.13420701, Gradient norm: 3.34366584
INFO:root:[    8] Training loss: 0.13742831, Validation loss: 0.13705925, Gradient norm: 3.45513078
INFO:root:[    9] Training loss: 0.13638778, Validation loss: 0.13256525, Gradient norm: 3.57052663
INFO:root:[   10] Training loss: 0.13032726, Validation loss: 0.14156728, Gradient norm: 3.48447673
INFO:root:[   11] Training loss: 0.12368278, Validation loss: 0.12697938, Gradient norm: 3.09028499
INFO:root:[   12] Training loss: 0.12422675, Validation loss: 0.15306287, Gradient norm: 3.51545885
INFO:root:[   13] Training loss: 0.12130067, Validation loss: 0.11786914, Gradient norm: 3.56328537
INFO:root:[   14] Training loss: 0.11438164, Validation loss: 0.12683957, Gradient norm: 3.33202801
INFO:root:[   15] Training loss: 0.11405423, Validation loss: 0.13141000, Gradient norm: 3.30033002
INFO:root:[   16] Training loss: 0.10756248, Validation loss: 0.11782168, Gradient norm: 3.38437159
INFO:root:[   17] Training loss: 0.10816864, Validation loss: 0.12636764, Gradient norm: 2.83754416
INFO:root:[   18] Training loss: 0.10548092, Validation loss: 0.12965069, Gradient norm: 3.42240484
INFO:root:[   19] Training loss: 0.10228114, Validation loss: 0.15309305, Gradient norm: 2.94192400
INFO:root:[   20] Training loss: 0.09879452, Validation loss: 0.16153150, Gradient norm: 3.17526154
INFO:root:[   21] Training loss: 0.09673810, Validation loss: 0.14348712, Gradient norm: 2.80074969
INFO:root:[   22] Training loss: 0.09445235, Validation loss: 0.14776461, Gradient norm: 2.88493013
INFO:root:[   23] Training loss: 0.09235306, Validation loss: 0.13026270, Gradient norm: 2.72540525
INFO:root:[   24] Training loss: 0.09213051, Validation loss: 0.16161643, Gradient norm: 2.96222925
INFO:root:[   25] Training loss: 0.09125860, Validation loss: 0.15283976, Gradient norm: 3.24340953
INFO:root:[   26] Training loss: 0.08931433, Validation loss: 0.15802272, Gradient norm: 3.19155373
INFO:root:[   27] Training loss: 0.08999378, Validation loss: 0.16288564, Gradient norm: 3.16519944
INFO:root:[   28] Training loss: 0.08526855, Validation loss: 0.14006583, Gradient norm: 2.81238670
INFO:root:[   29] Training loss: 0.08519160, Validation loss: 0.17480883, Gradient norm: 2.52700724
INFO:root:[   30] Training loss: 0.08451246, Validation loss: 0.14239587, Gradient norm: 3.20141226
INFO:root:[   31] Training loss: 0.08246593, Validation loss: 0.15226573, Gradient norm: 2.66893488
INFO:root:[   32] Training loss: 0.08257754, Validation loss: 0.17249991, Gradient norm: 2.93144256
INFO:root:[   33] Training loss: 0.08094945, Validation loss: 0.16704414, Gradient norm: 3.09110360
INFO:root:[   34] Training loss: 0.07930847, Validation loss: 0.17554009, Gradient norm: 2.56170849
INFO:root:[   35] Training loss: 0.08031097, Validation loss: 0.13671686, Gradient norm: 3.03320873
INFO:root:[   36] Training loss: 0.07994774, Validation loss: 0.14873897, Gradient norm: 2.82875821
INFO:root:[   37] Training loss: 0.07755320, Validation loss: 0.17275804, Gradient norm: 2.79126578
INFO:root:[   38] Training loss: 0.07685287, Validation loss: 0.15577768, Gradient norm: 2.34772413
INFO:root:[   39] Training loss: 0.07891693, Validation loss: 0.19090109, Gradient norm: 3.31675196
INFO:root:[   40] Training loss: 0.07707056, Validation loss: 0.14535943, Gradient norm: 2.77272850
INFO:root:[   41] Training loss: 0.07647368, Validation loss: 0.16476642, Gradient norm: 2.69604448
INFO:root:[   42] Training loss: 0.07503081, Validation loss: 0.17616867, Gradient norm: 2.38859965
INFO:root:[   43] Training loss: 0.07824552, Validation loss: 0.16650125, Gradient norm: 3.23552288
INFO:root:[   44] Training loss: 0.07286770, Validation loss: 0.15057083, Gradient norm: 2.62628567
INFO:root:[   45] Training loss: 0.07626646, Validation loss: 0.16554218, Gradient norm: 2.93671565
INFO:root:[   46] Training loss: 0.07387288, Validation loss: 0.18395864, Gradient norm: 2.89190521
INFO:root:[   47] Training loss: 0.07338084, Validation loss: 0.16947378, Gradient norm: 2.62845010
INFO:root:[   48] Training loss: 0.07150752, Validation loss: 0.15706055, Gradient norm: 2.58484900
INFO:root:[   49] Training loss: 0.07412971, Validation loss: 0.14279085, Gradient norm: 2.47224665
INFO:root:[   50] Training loss: 0.07150003, Validation loss: 0.16584598, Gradient norm: 2.80829595
INFO:root:[   51] Training loss: 0.07157074, Validation loss: 0.17120178, Gradient norm: 2.62479705
INFO:root:[   52] Training loss: 0.07141844, Validation loss: 0.14670787, Gradient norm: 2.59835974
INFO:root:[   53] Training loss: 0.07016729, Validation loss: 0.16920865, Gradient norm: 2.18553756
INFO:root:[   54] Training loss: 0.07045252, Validation loss: 0.17386906, Gradient norm: 2.81926137
INFO:root:[   55] Training loss: 0.07101699, Validation loss: 0.15455195, Gradient norm: 2.97256468
INFO:root:[   56] Training loss: 0.07208308, Validation loss: 0.15516150, Gradient norm: 2.79312029
INFO:root:[   57] Training loss: 0.06772599, Validation loss: 0.15956894, Gradient norm: 2.50156677
INFO:root:[   58] Training loss: 0.07000552, Validation loss: 0.15726175, Gradient norm: 2.96893049
INFO:root:[   59] Training loss: 0.06779373, Validation loss: 0.17783171, Gradient norm: 2.54699078
INFO:root:[   60] Training loss: 0.06919232, Validation loss: 0.15068031, Gradient norm: 2.42892250
INFO:root:[   61] Training loss: 0.06766222, Validation loss: 0.15379308, Gradient norm: 2.30437332
INFO:root:EP 61: Early stopping
INFO:root:Training the model took 3014.976s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.27883
INFO:root:EnergyScoreTrain: 0.14314
INFO:root:CoverageTrain: 0.71651
INFO:root:IntervalWidthTrain: 0.11916
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.27361
INFO:root:EnergyScoreValidation: 0.14452
INFO:root:CoverageValidation: 0.67221
INFO:root:IntervalWidthValidation: 0.10629
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.2788
INFO:root:EnergyScoreTest: 0.14221
INFO:root:CoverageTest: 0.69685
INFO:root:IntervalWidthTest: 0.11713
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 494927872
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.33932296, Validation loss: 0.18386498, Gradient norm: 6.35586507
INFO:root:[    2] Training loss: 0.18195445, Validation loss: 0.24102841, Gradient norm: 3.36020707
INFO:root:[    3] Training loss: 0.16665242, Validation loss: 0.13861461, Gradient norm: 3.91943856
INFO:root:[    4] Training loss: 0.15095347, Validation loss: 0.14248018, Gradient norm: 4.49717489
INFO:root:[    5] Training loss: 0.13998310, Validation loss: 0.14394225, Gradient norm: 3.82357506
INFO:root:[    6] Training loss: 0.13429683, Validation loss: 0.12398917, Gradient norm: 4.40880153
INFO:root:[    7] Training loss: 0.12823905, Validation loss: 0.12788787, Gradient norm: 3.74929441
INFO:root:[    8] Training loss: 0.12143317, Validation loss: 0.17097210, Gradient norm: 3.15386515
INFO:root:[    9] Training loss: 0.12314562, Validation loss: 0.16394281, Gradient norm: 3.80078125
INFO:root:[   10] Training loss: 0.12005157, Validation loss: 0.14808897, Gradient norm: 3.82866207
INFO:root:[   11] Training loss: 0.11523230, Validation loss: 0.11479210, Gradient norm: 3.66747119
INFO:root:[   12] Training loss: 0.11052335, Validation loss: 0.14033742, Gradient norm: 3.50038742
INFO:root:[   13] Training loss: 0.11016313, Validation loss: 0.14481511, Gradient norm: 2.99465757
INFO:root:[   14] Training loss: 0.10686229, Validation loss: 0.16453576, Gradient norm: 3.33834522
INFO:root:[   15] Training loss: 0.10646692, Validation loss: 0.17076207, Gradient norm: 3.20836431
INFO:root:[   16] Training loss: 0.10134987, Validation loss: 0.15110785, Gradient norm: 2.82792525
INFO:root:[   17] Training loss: 0.10108917, Validation loss: 0.14740596, Gradient norm: 2.95185172
INFO:root:[   18] Training loss: 0.10159266, Validation loss: 0.12736718, Gradient norm: 3.26603719
INFO:root:[   19] Training loss: 0.09622340, Validation loss: 0.16424168, Gradient norm: 3.39048757
INFO:root:[   20] Training loss: 0.09476768, Validation loss: 0.17570946, Gradient norm: 3.15792450
INFO:root:[   21] Training loss: 0.09393119, Validation loss: 0.17693224, Gradient norm: 3.16077206
INFO:root:[   22] Training loss: 0.09270721, Validation loss: 0.12900111, Gradient norm: 3.15963973
INFO:root:[   23] Training loss: 0.09302253, Validation loss: 0.17243586, Gradient norm: 2.80290230
INFO:root:[   24] Training loss: 0.08866144, Validation loss: 0.16616955, Gradient norm: 3.13056961
INFO:root:[   25] Training loss: 0.08652459, Validation loss: 0.15861399, Gradient norm: 2.85575313
INFO:root:[   26] Training loss: 0.08708028, Validation loss: 0.18987972, Gradient norm: 3.09407209
INFO:root:[   27] Training loss: 0.08352389, Validation loss: 0.13869357, Gradient norm: 2.75504536
INFO:root:[   28] Training loss: 0.08335028, Validation loss: 0.18557815, Gradient norm: 2.85628291
INFO:root:[   29] Training loss: 0.08176012, Validation loss: 0.13477921, Gradient norm: 2.93016115
INFO:root:[   30] Training loss: 0.08342431, Validation loss: 0.14346175, Gradient norm: 2.98602305
INFO:root:[   31] Training loss: 0.08072874, Validation loss: 0.17149012, Gradient norm: 3.03042167
INFO:root:[   32] Training loss: 0.08401777, Validation loss: 0.15434493, Gradient norm: 2.95088747
INFO:root:[   33] Training loss: 0.08007262, Validation loss: 0.15068411, Gradient norm: 2.71778448
INFO:root:[   34] Training loss: 0.07817134, Validation loss: 0.13068808, Gradient norm: 2.67873060
INFO:root:[   35] Training loss: 0.07754389, Validation loss: 0.16696991, Gradient norm: 2.99970984
INFO:root:[   36] Training loss: 0.08008717, Validation loss: 0.16062436, Gradient norm: 2.83617742
INFO:root:[   37] Training loss: 0.07705622, Validation loss: 0.15581598, Gradient norm: 2.99023180
INFO:root:[   38] Training loss: 0.07356979, Validation loss: 0.17348169, Gradient norm: 2.35934495
INFO:root:[   39] Training loss: 0.07782613, Validation loss: 0.17442372, Gradient norm: 2.91618465
INFO:root:[   40] Training loss: 0.07430980, Validation loss: 0.18888855, Gradient norm: 2.58175982
INFO:root:[   41] Training loss: 0.07587647, Validation loss: 0.14971944, Gradient norm: 2.61009879
INFO:root:[   42] Training loss: 0.07404097, Validation loss: 0.17094332, Gradient norm: 3.11409574
INFO:root:[   43] Training loss: 0.07330152, Validation loss: 0.16596122, Gradient norm: 2.51851449
INFO:root:[   44] Training loss: 0.07241634, Validation loss: 0.16499459, Gradient norm: 2.57032763
INFO:root:[   45] Training loss: 0.07367771, Validation loss: 0.15432735, Gradient norm: 2.82658395
INFO:root:[   46] Training loss: 0.07256598, Validation loss: 0.15979361, Gradient norm: 2.90476244
INFO:root:[   47] Training loss: 0.07082857, Validation loss: 0.15048346, Gradient norm: 2.70802971
INFO:root:[   48] Training loss: 0.07108366, Validation loss: 0.18118086, Gradient norm: 2.72181411
INFO:root:[   49] Training loss: 0.07138289, Validation loss: 0.14796826, Gradient norm: 2.69902480
INFO:root:[   50] Training loss: 0.06981008, Validation loss: 0.16818591, Gradient norm: 2.63063115
INFO:root:[   51] Training loss: 0.07133175, Validation loss: 0.17118149, Gradient norm: 2.68229439
INFO:root:[   52] Training loss: 0.06864481, Validation loss: 0.17044866, Gradient norm: 2.64521857
INFO:root:[   53] Training loss: 0.06883642, Validation loss: 0.14856201, Gradient norm: 2.66755022
INFO:root:[   54] Training loss: 0.06705150, Validation loss: 0.16032619, Gradient norm: 2.34343156
INFO:root:[   55] Training loss: 0.06978698, Validation loss: 0.15669168, Gradient norm: 2.45842759
INFO:root:[   56] Training loss: 0.06648346, Validation loss: 0.15976220, Gradient norm: 2.44610099
INFO:root:[   57] Training loss: 0.06830237, Validation loss: 0.16066640, Gradient norm: 2.83655125
INFO:root:[   58] Training loss: 0.06685680, Validation loss: 0.15407379, Gradient norm: 2.41535094
INFO:root:[   59] Training loss: 0.06866416, Validation loss: 0.16391062, Gradient norm: 2.20833518
INFO:root:[   60] Training loss: 0.06834228, Validation loss: 0.15510139, Gradient norm: 2.53685327
INFO:root:[   61] Training loss: 0.06769211, Validation loss: 0.16655762, Gradient norm: 2.33495106
INFO:root:[   62] Training loss: 0.06801567, Validation loss: 0.15757934, Gradient norm: 2.79404469
INFO:root:EP 62: Early stopping
INFO:root:Training the model took 2970.03s.
INFO:root:Emptying the cuda cache took 0.008s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.27217
INFO:root:EnergyScoreTrain: 0.13914
INFO:root:CoverageTrain: 0.69306
INFO:root:IntervalWidthTrain: 0.11118
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.27767
INFO:root:EnergyScoreValidation: 0.14128
INFO:root:CoverageValidation: 0.65796
INFO:root:IntervalWidthValidation: 0.10635
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.26369
INFO:root:EnergyScoreTest: 0.13877
INFO:root:CoverageTest: 0.65799
INFO:root:IntervalWidthTest: 0.09903
INFO:root:###3 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 268435456
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.29088749, Validation loss: 0.17537222, Gradient norm: 4.30315738
INFO:root:[    2] Training loss: 0.17723078, Validation loss: 0.15311033, Gradient norm: 4.03221800
INFO:root:[    3] Training loss: 0.15517289, Validation loss: 0.16623574, Gradient norm: 3.66484905
INFO:root:[    4] Training loss: 0.13882071, Validation loss: 0.12926234, Gradient norm: 3.83689134
INFO:root:[    5] Training loss: 0.13216641, Validation loss: 0.11882828, Gradient norm: 3.81898589
INFO:root:[    6] Training loss: 0.12719932, Validation loss: 0.12539880, Gradient norm: 3.47391703
INFO:root:[    7] Training loss: 0.12250401, Validation loss: 0.14170239, Gradient norm: 3.50535693
INFO:root:[    8] Training loss: 0.11872218, Validation loss: 0.12908482, Gradient norm: 2.88246673
INFO:root:[    9] Training loss: 0.11475697, Validation loss: 0.16220664, Gradient norm: 2.93350934
INFO:root:[   10] Training loss: 0.11061835, Validation loss: 0.16545294, Gradient norm: 3.05564572
INFO:root:[   11] Training loss: 0.10932471, Validation loss: 0.14393026, Gradient norm: 2.84817089
INFO:root:[   12] Training loss: 0.10997528, Validation loss: 0.12272886, Gradient norm: 3.28457860
INFO:root:[   13] Training loss: 0.10374355, Validation loss: 0.13117979, Gradient norm: 2.97012365
INFO:root:[   14] Training loss: 0.10316145, Validation loss: 0.11525533, Gradient norm: 3.28239600
INFO:root:[   15] Training loss: 0.10450816, Validation loss: 0.14625524, Gradient norm: 3.17986938
INFO:root:[   16] Training loss: 0.09770953, Validation loss: 0.11828575, Gradient norm: 2.90829768
INFO:root:[   17] Training loss: 0.09702052, Validation loss: 0.13329658, Gradient norm: 2.99074530
INFO:root:[   18] Training loss: 0.09499072, Validation loss: 0.14935249, Gradient norm: 3.15702486
INFO:root:[   19] Training loss: 0.09483369, Validation loss: 0.17746572, Gradient norm: 2.78169955
INFO:root:[   20] Training loss: 0.09353413, Validation loss: 0.18651624, Gradient norm: 3.02910735
INFO:root:[   21] Training loss: 0.09271660, Validation loss: 0.13259796, Gradient norm: 2.80242977
INFO:root:[   22] Training loss: 0.09159828, Validation loss: 0.17777189, Gradient norm: 2.94080864
INFO:root:[   23] Training loss: 0.08860232, Validation loss: 0.13023930, Gradient norm: 2.94289959
INFO:root:[   24] Training loss: 0.08921085, Validation loss: 0.16179474, Gradient norm: 2.94293339
INFO:root:[   25] Training loss: 0.08861489, Validation loss: 0.17799187, Gradient norm: 2.72836972
INFO:root:[   26] Training loss: 0.08658836, Validation loss: 0.16244532, Gradient norm: 2.82933624
INFO:root:[   27] Training loss: 0.08659177, Validation loss: 0.17578386, Gradient norm: 2.80444084
INFO:root:[   28] Training loss: 0.08649443, Validation loss: 0.16206995, Gradient norm: 3.24754436
INFO:root:[   29] Training loss: 0.08474209, Validation loss: 0.15701530, Gradient norm: 2.84311036
INFO:root:[   30] Training loss: 0.08253926, Validation loss: 0.16280223, Gradient norm: 2.77825319
INFO:root:[   31] Training loss: 0.08053977, Validation loss: 0.14862314, Gradient norm: 2.75281667
INFO:root:[   32] Training loss: 0.08367889, Validation loss: 0.15250620, Gradient norm: 3.08803589
INFO:root:[   33] Training loss: 0.08125898, Validation loss: 0.18709131, Gradient norm: 2.96910058
INFO:root:[   34] Training loss: 0.08107385, Validation loss: 0.15513652, Gradient norm: 2.69321242
INFO:root:[   35] Training loss: 0.08080903, Validation loss: 0.15797153, Gradient norm: 2.92664761
INFO:root:[   36] Training loss: 0.07610345, Validation loss: 0.17080165, Gradient norm: 2.36588931
INFO:root:[   37] Training loss: 0.07894880, Validation loss: 0.16390168, Gradient norm: 2.86688319
INFO:root:[   38] Training loss: 0.07891427, Validation loss: 0.16592354, Gradient norm: 2.92226708
INFO:root:[   39] Training loss: 0.07768666, Validation loss: 0.19067930, Gradient norm: 2.98048555
INFO:root:[   40] Training loss: 0.07508107, Validation loss: 0.14965190, Gradient norm: 2.64144290
INFO:root:[   41] Training loss: 0.07747111, Validation loss: 0.14044049, Gradient norm: 2.65931268
INFO:root:[   42] Training loss: 0.07792944, Validation loss: 0.15205230, Gradient norm: 3.24058584
INFO:root:[   43] Training loss: 0.07571242, Validation loss: 0.16069371, Gradient norm: 2.82890891
INFO:root:[   44] Training loss: 0.07545383, Validation loss: 0.16857427, Gradient norm: 2.68604822
INFO:root:[   45] Training loss: 0.07357685, Validation loss: 0.16001765, Gradient norm: 2.58987745
INFO:root:[   46] Training loss: 0.07560405, Validation loss: 0.17109916, Gradient norm: 3.10150230
INFO:root:[   47] Training loss: 0.07215808, Validation loss: 0.16309249, Gradient norm: 2.45997032
INFO:root:[   48] Training loss: 0.07495527, Validation loss: 0.16507777, Gradient norm: 3.17762298
INFO:root:[   49] Training loss: 0.07143393, Validation loss: 0.17556116, Gradient norm: 2.87475957
INFO:root:[   50] Training loss: 0.07151132, Validation loss: 0.16458051, Gradient norm: 2.78050635
INFO:root:[   51] Training loss: 0.07355820, Validation loss: 0.16467320, Gradient norm: 3.11178022
INFO:root:[   52] Training loss: 0.06906079, Validation loss: 0.17652305, Gradient norm: 2.71207734
INFO:root:[   53] Training loss: 0.07187369, Validation loss: 0.16665743, Gradient norm: 2.51018683
INFO:root:[   54] Training loss: 0.06895513, Validation loss: 0.16412338, Gradient norm: 2.75947149
INFO:root:[   55] Training loss: 0.06990527, Validation loss: 0.16225857, Gradient norm: 2.55881885
INFO:root:[   56] Training loss: 0.06914289, Validation loss: 0.15926857, Gradient norm: 2.78499412
INFO:root:[   57] Training loss: 0.06663308, Validation loss: 0.17338932, Gradient norm: 2.41102465
INFO:root:[   58] Training loss: 0.07030938, Validation loss: 0.15996812, Gradient norm: 2.97883164
INFO:root:[   59] Training loss: 0.06900673, Validation loss: 0.15933459, Gradient norm: 2.73483171
INFO:root:[   60] Training loss: 0.06795900, Validation loss: 0.16638175, Gradient norm: 2.79526261
INFO:root:[   61] Training loss: 0.06699142, Validation loss: 0.15794691, Gradient norm: 2.70888109
INFO:root:[   62] Training loss: 0.06545121, Validation loss: 0.15575695, Gradient norm: 2.38938016
INFO:root:[   63] Training loss: 0.06695404, Validation loss: 0.17461226, Gradient norm: 2.95798211
INFO:root:[   64] Training loss: 0.06879093, Validation loss: 0.15251068, Gradient norm: 2.34110517
INFO:root:[   65] Training loss: 0.06793920, Validation loss: 0.17023239, Gradient norm: 3.00995750
INFO:root:[   66] Training loss: 0.06590520, Validation loss: 0.17975091, Gradient norm: 2.70546682
INFO:root:[   67] Training loss: 0.06540011, Validation loss: 0.16667092, Gradient norm: 2.38808824
INFO:root:[   68] Training loss: 0.06586391, Validation loss: 0.16171324, Gradient norm: 2.61626370
INFO:root:[   69] Training loss: 0.06586042, Validation loss: 0.16801047, Gradient norm: 2.50032714
INFO:root:[   70] Training loss: 0.06737198, Validation loss: 0.16895049, Gradient norm: 2.54487227
INFO:root:[   71] Training loss: 0.06658678, Validation loss: 0.15775127, Gradient norm: 2.71374207
INFO:root:[   72] Training loss: 0.06486939, Validation loss: 0.15497235, Gradient norm: 2.57583573
INFO:root:[   73] Training loss: 0.06441907, Validation loss: 0.18040868, Gradient norm: 2.78815653
INFO:root:EP 73: Early stopping
INFO:root:Training the model took 3405.617s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.25871
INFO:root:EnergyScoreTrain: 0.13377
INFO:root:CoverageTrain: 0.69411
INFO:root:IntervalWidthTrain: 0.10214
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.23731
INFO:root:EnergyScoreValidation: 0.12427
INFO:root:CoverageValidation: 0.67255
INFO:root:IntervalWidthValidation: 0.08932
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.24971
INFO:root:EnergyScoreTest: 0.13412
INFO:root:CoverageTest: 0.63601
INFO:root:IntervalWidthTest: 0.08816
INFO:root:###4 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 574619648
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.32353047, Validation loss: 0.18571319, Gradient norm: 4.83242762
INFO:root:[    2] Training loss: 0.19225016, Validation loss: 0.15767714, Gradient norm: 4.13095292
INFO:root:[    3] Training loss: 0.16329450, Validation loss: 0.15961776, Gradient norm: 3.35370458
INFO:root:[    4] Training loss: 0.15223832, Validation loss: 0.14972908, Gradient norm: 4.00774311
INFO:root:[    5] Training loss: 0.13984994, Validation loss: 0.12733044, Gradient norm: 3.69709571
INFO:root:[    6] Training loss: 0.13665842, Validation loss: 0.13334628, Gradient norm: 3.72596029
INFO:root:[    7] Training loss: 0.12556812, Validation loss: 0.16402853, Gradient norm: 3.33831362
INFO:root:[    8] Training loss: 0.12191337, Validation loss: 0.12917929, Gradient norm: 3.23287820
INFO:root:[    9] Training loss: 0.11954057, Validation loss: 0.11986919, Gradient norm: 2.75964209
INFO:root:[   10] Training loss: 0.11624962, Validation loss: 0.11918037, Gradient norm: 3.33316029
INFO:root:[   11] Training loss: 0.11150389, Validation loss: 0.12714228, Gradient norm: 2.76139565
INFO:root:[   12] Training loss: 0.11064248, Validation loss: 0.13591528, Gradient norm: 3.30890083
INFO:root:[   13] Training loss: 0.10816082, Validation loss: 0.13331988, Gradient norm: 2.90393911
INFO:root:[   14] Training loss: 0.10457282, Validation loss: 0.14239348, Gradient norm: 2.89154869
INFO:root:[   15] Training loss: 0.10196415, Validation loss: 0.18136293, Gradient norm: 2.74382867
INFO:root:[   16] Training loss: 0.10353575, Validation loss: 0.14323233, Gradient norm: 3.12948086
INFO:root:[   17] Training loss: 0.10363269, Validation loss: 0.15125230, Gradient norm: 3.17404928
INFO:root:[   18] Training loss: 0.09536467, Validation loss: 0.18079760, Gradient norm: 2.47155587
INFO:root:[   19] Training loss: 0.09612842, Validation loss: 0.14946617, Gradient norm: 3.06295790
INFO:root:[   20] Training loss: 0.09430095, Validation loss: 0.14561195, Gradient norm: 2.78566125
INFO:root:[   21] Training loss: 0.09200271, Validation loss: 0.17424097, Gradient norm: 2.53671352
INFO:root:[   22] Training loss: 0.08981577, Validation loss: 0.16119093, Gradient norm: 2.76845730
INFO:root:[   23] Training loss: 0.09062829, Validation loss: 0.13954692, Gradient norm: 2.95159501
INFO:root:[   24] Training loss: 0.09051750, Validation loss: 0.14906347, Gradient norm: 3.10071544
INFO:root:[   25] Training loss: 0.08702428, Validation loss: 0.15109972, Gradient norm: 2.53017746
INFO:root:[   26] Training loss: 0.08608735, Validation loss: 0.16190754, Gradient norm: 2.90374692
INFO:root:[   27] Training loss: 0.08501325, Validation loss: 0.20171343, Gradient norm: 2.86365232
INFO:root:[   28] Training loss: 0.08529193, Validation loss: 0.19039852, Gradient norm: 2.85841386
INFO:root:[   29] Training loss: 0.08261145, Validation loss: 0.15010571, Gradient norm: 2.65846662
INFO:root:[   30] Training loss: 0.08380144, Validation loss: 0.16188875, Gradient norm: 2.46239399
INFO:root:[   31] Training loss: 0.07858151, Validation loss: 0.16809126, Gradient norm: 2.67991088
INFO:root:[   32] Training loss: 0.07857788, Validation loss: 0.15221411, Gradient norm: 2.92309367
INFO:root:[   33] Training loss: 0.07844167, Validation loss: 0.14377499, Gradient norm: 2.88005580
INFO:root:[   34] Training loss: 0.07682900, Validation loss: 0.17481200, Gradient norm: 2.62940614
INFO:root:[   35] Training loss: 0.07601194, Validation loss: 0.17635838, Gradient norm: 2.57653509
INFO:root:[   36] Training loss: 0.07513489, Validation loss: 0.16356963, Gradient norm: 2.84763255
INFO:root:[   37] Training loss: 0.07708726, Validation loss: 0.18972178, Gradient norm: 2.90300746
INFO:root:[   38] Training loss: 0.07497781, Validation loss: 0.15959638, Gradient norm: 2.81653748
INFO:root:[   39] Training loss: 0.07442563, Validation loss: 0.16659209, Gradient norm: 2.63798918
INFO:root:[   40] Training loss: 0.07463670, Validation loss: 0.15472194, Gradient norm: 3.03405789
INFO:root:[   41] Training loss: 0.07171866, Validation loss: 0.18209369, Gradient norm: 2.69119630
INFO:root:[   42] Training loss: 0.07369918, Validation loss: 0.15239531, Gradient norm: 2.66415248
INFO:root:[   43] Training loss: 0.07127082, Validation loss: 0.15575404, Gradient norm: 2.55115687
INFO:root:[   44] Training loss: 0.07082791, Validation loss: 0.16717676, Gradient norm: 2.62183788
INFO:root:[   45] Training loss: 0.07294849, Validation loss: 0.15613566, Gradient norm: 2.62737341
INFO:root:[   46] Training loss: 0.07054862, Validation loss: 0.17598466, Gradient norm: 2.45020908
INFO:root:[   47] Training loss: 0.07172455, Validation loss: 0.19362207, Gradient norm: 2.78768470
INFO:root:[   48] Training loss: 0.07081363, Validation loss: 0.15084150, Gradient norm: 2.71310731
INFO:root:[   49] Training loss: 0.06870905, Validation loss: 0.14953741, Gradient norm: 2.26307815
INFO:root:[   50] Training loss: 0.07074879, Validation loss: 0.14855098, Gradient norm: 2.94121948
INFO:root:[   51] Training loss: 0.07079780, Validation loss: 0.16054377, Gradient norm: 2.80868741
INFO:root:[   52] Training loss: 0.06951355, Validation loss: 0.15624532, Gradient norm: 2.94280033
INFO:root:[   53] Training loss: 0.06927724, Validation loss: 0.17453341, Gradient norm: 2.75182859
INFO:root:[   54] Training loss: 0.06783206, Validation loss: 0.15590358, Gradient norm: 2.63048984
INFO:root:[   55] Training loss: 0.06699472, Validation loss: 0.17708785, Gradient norm: 2.47539457
INFO:root:[   56] Training loss: 0.06594145, Validation loss: 0.15239067, Gradient norm: 2.49154385
INFO:root:[   57] Training loss: 0.06657447, Validation loss: 0.14816837, Gradient norm: 2.70389885
INFO:root:[   58] Training loss: 0.06733826, Validation loss: 0.15347006, Gradient norm: 2.32243018
INFO:root:[   59] Training loss: 0.06757155, Validation loss: 0.16988050, Gradient norm: 2.54366256
INFO:root:[   60] Training loss: 0.06646222, Validation loss: 0.15818847, Gradient norm: 2.30304018
INFO:root:[   61] Training loss: 0.06587756, Validation loss: 0.15957378, Gradient norm: 2.29557153
INFO:root:[   62] Training loss: 0.06599293, Validation loss: 0.14684328, Gradient norm: 2.54736411
INFO:root:[   63] Training loss: 0.06631222, Validation loss: 0.16016348, Gradient norm: 2.71516344
INFO:root:[   64] Training loss: 0.06588167, Validation loss: 0.18057749, Gradient norm: 2.48568683
INFO:root:[   65] Training loss: 0.06740663, Validation loss: 0.16877729, Gradient norm: 2.91071739
INFO:root:[   66] Training loss: 0.06451950, Validation loss: 0.16130231, Gradient norm: 2.23109064
INFO:root:[   67] Training loss: 0.06398102, Validation loss: 0.15890267, Gradient norm: 2.26872380
INFO:root:[   68] Training loss: 0.06615009, Validation loss: 0.15815589, Gradient norm: 2.68631968
INFO:root:[   69] Training loss: 0.06561711, Validation loss: 0.16503754, Gradient norm: 2.47235644
INFO:root:[   70] Training loss: 0.06504956, Validation loss: 0.15871575, Gradient norm: 2.42895899
INFO:root:[   71] Training loss: 0.06588038, Validation loss: 0.16194142, Gradient norm: 2.35751005
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 3315.085s.
INFO:root:Emptying the cuda cache took 0.005s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.24468
INFO:root:EnergyScoreTrain: 0.12838
INFO:root:CoverageTrain: 0.6794
INFO:root:IntervalWidthTrain: 0.0975
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.2257
INFO:root:EnergyScoreValidation: 0.11663
INFO:root:CoverageValidation: 0.69802
INFO:root:IntervalWidthValidation: 0.09744
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.24044
INFO:root:EnergyScoreTest: 0.12801
INFO:root:CoverageTest: 0.65314
INFO:root:IntervalWidthTest: 0.09105
INFO:root:###5 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 301989888
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.28306276, Validation loss: 0.18482531, Gradient norm: 3.49434873
INFO:root:[    2] Training loss: 0.17311524, Validation loss: 0.14089356, Gradient norm: 3.34932380
INFO:root:[    3] Training loss: 0.15184727, Validation loss: 0.17553371, Gradient norm: 3.34993717
INFO:root:[    4] Training loss: 0.13861724, Validation loss: 0.14267726, Gradient norm: 3.11162078
INFO:root:[    5] Training loss: 0.12883917, Validation loss: 0.18113176, Gradient norm: 3.06332588
INFO:root:[    6] Training loss: 0.12279997, Validation loss: 0.11868188, Gradient norm: 3.07020385
INFO:root:[    7] Training loss: 0.11675153, Validation loss: 0.11882220, Gradient norm: 2.99212550
INFO:root:[    8] Training loss: 0.11425602, Validation loss: 0.16491669, Gradient norm: 2.89214214
INFO:root:[    9] Training loss: 0.11356388, Validation loss: 0.14040576, Gradient norm: 3.19013301
INFO:root:[   10] Training loss: 0.10714228, Validation loss: 0.14216429, Gradient norm: 2.54066298
INFO:root:[   11] Training loss: 0.11184771, Validation loss: 0.15415097, Gradient norm: 3.22391189
INFO:root:[   12] Training loss: 0.10718272, Validation loss: 0.13707389, Gradient norm: 2.88291626
INFO:root:[   13] Training loss: 0.10304645, Validation loss: 0.12407842, Gradient norm: 2.81836692
INFO:root:[   14] Training loss: 0.09977236, Validation loss: 0.14654430, Gradient norm: 2.81865825
INFO:root:[   15] Training loss: 0.10039410, Validation loss: 0.15227425, Gradient norm: 2.55821523
INFO:root:[   16] Training loss: 0.09671445, Validation loss: 0.15487519, Gradient norm: 2.72494679
INFO:root:[   17] Training loss: 0.09813209, Validation loss: 0.14277376, Gradient norm: 2.63506242
INFO:root:[   18] Training loss: 0.09393193, Validation loss: 0.16473045, Gradient norm: 2.57723656
INFO:root:[   19] Training loss: 0.09457271, Validation loss: 0.14326311, Gradient norm: 2.78023863
INFO:root:[   20] Training loss: 0.09441077, Validation loss: 0.19708539, Gradient norm: 2.77386376
INFO:root:[   21] Training loss: 0.09462250, Validation loss: 0.15317635, Gradient norm: 2.94235145
INFO:root:[   22] Training loss: 0.09204263, Validation loss: 0.18661061, Gradient norm: 3.00159681
INFO:root:[   23] Training loss: 0.09032473, Validation loss: 0.15859562, Gradient norm: 2.85286476
INFO:root:[   24] Training loss: 0.08813591, Validation loss: 0.18242228, Gradient norm: 2.82241979
INFO:root:[   25] Training loss: 0.08562928, Validation loss: 0.18110459, Gradient norm: 2.66705575
INFO:root:[   26] Training loss: 0.08642733, Validation loss: 0.17864062, Gradient norm: 2.89725919
INFO:root:[   27] Training loss: 0.08490270, Validation loss: 0.15744028, Gradient norm: 2.90913031
INFO:root:[   28] Training loss: 0.08154967, Validation loss: 0.18550132, Gradient norm: 2.78426989
INFO:root:[   29] Training loss: 0.08110854, Validation loss: 0.17198225, Gradient norm: 2.74838333
INFO:root:[   30] Training loss: 0.08123149, Validation loss: 0.19016095, Gradient norm: 2.89588811
INFO:root:[   31] Training loss: 0.08097477, Validation loss: 0.16094111, Gradient norm: 2.84798114
INFO:root:[   32] Training loss: 0.07873010, Validation loss: 0.13510874, Gradient norm: 2.73730816
INFO:root:[   33] Training loss: 0.08083839, Validation loss: 0.17476880, Gradient norm: 3.06421033
INFO:root:[   34] Training loss: 0.07485271, Validation loss: 0.16916525, Gradient norm: 2.90205172
INFO:root:[   35] Training loss: 0.07544026, Validation loss: 0.14963823, Gradient norm: 2.75357984
INFO:root:[   36] Training loss: 0.07512453, Validation loss: 0.17038149, Gradient norm: 2.73624699
INFO:root:[   37] Training loss: 0.07578821, Validation loss: 0.16714043, Gradient norm: 2.91077744
INFO:root:[   38] Training loss: 0.07381972, Validation loss: 0.16182846, Gradient norm: 2.77688599
INFO:root:[   39] Training loss: 0.07367721, Validation loss: 0.15102877, Gradient norm: 2.82390999
INFO:root:[   40] Training loss: 0.07592817, Validation loss: 0.15722593, Gradient norm: 3.12253486
INFO:root:[   41] Training loss: 0.07155987, Validation loss: 0.15671811, Gradient norm: 2.64641621
INFO:root:[   42] Training loss: 0.07159263, Validation loss: 0.16496133, Gradient norm: 2.97699859
INFO:root:[   43] Training loss: 0.07494141, Validation loss: 0.16405347, Gradient norm: 2.90482487
INFO:root:[   44] Training loss: 0.07129876, Validation loss: 0.16539002, Gradient norm: 2.83948106
INFO:root:[   45] Training loss: 0.07216934, Validation loss: 0.16329218, Gradient norm: 3.11698690
INFO:root:[   46] Training loss: 0.07253408, Validation loss: 0.16804924, Gradient norm: 2.67474287
INFO:root:[   47] Training loss: 0.06954884, Validation loss: 0.16178004, Gradient norm: 2.78552452
INFO:root:[   48] Training loss: 0.07228104, Validation loss: 0.15869468, Gradient norm: 3.12344855
INFO:root:[   49] Training loss: 0.06909774, Validation loss: 0.15404637, Gradient norm: 2.86820429
INFO:root:[   50] Training loss: 0.07096968, Validation loss: 0.17868114, Gradient norm: 3.03170600
INFO:root:[   51] Training loss: 0.06980128, Validation loss: 0.16166996, Gradient norm: 2.96170220
INFO:root:[   52] Training loss: 0.06951463, Validation loss: 0.18327844, Gradient norm: 2.73864885
INFO:root:[   53] Training loss: 0.06862664, Validation loss: 0.15787647, Gradient norm: 2.61397935
INFO:root:[   54] Training loss: 0.07008044, Validation loss: 0.16622980, Gradient norm: 3.03203087
INFO:root:[   55] Training loss: 0.06834923, Validation loss: 0.15717699, Gradient norm: 2.73840149
INFO:root:[   56] Training loss: 0.06932585, Validation loss: 0.15974777, Gradient norm: 2.81278825
INFO:root:[   57] Training loss: 0.06837779, Validation loss: 0.18238350, Gradient norm: 2.65871711
INFO:root:[   58] Training loss: 0.06932130, Validation loss: 0.15975321, Gradient norm: 2.77222490
INFO:root:[   59] Training loss: 0.06686841, Validation loss: 0.17573811, Gradient norm: 2.39993415
INFO:root:[   60] Training loss: 0.06772379, Validation loss: 0.17964517, Gradient norm: 2.55062233
INFO:root:[   61] Training loss: 0.06648067, Validation loss: 0.15589803, Gradient norm: 2.73001135
INFO:root:[   62] Training loss: 0.06699549, Validation loss: 0.15704223, Gradient norm: 2.49123702
INFO:root:[   63] Training loss: 0.06514219, Validation loss: 0.16578456, Gradient norm: 2.74757468
INFO:root:[   64] Training loss: 0.06614979, Validation loss: 0.16447574, Gradient norm: 2.43939675
INFO:root:[   65] Training loss: 0.06611846, Validation loss: 0.15772435, Gradient norm: 2.61328225
INFO:root:[   66] Training loss: 0.06445766, Validation loss: 0.16969952, Gradient norm: 2.57290236
INFO:root:[   67] Training loss: 0.06549588, Validation loss: 0.15365923, Gradient norm: 2.63944621
INFO:root:[   68] Training loss: 0.06585246, Validation loss: 0.16197450, Gradient norm: 2.77498953
INFO:root:[   69] Training loss: 0.06450381, Validation loss: 0.16057472, Gradient norm: 2.68169886
INFO:root:[   70] Training loss: 0.06565441, Validation loss: 0.18200744, Gradient norm: 2.41859690
INFO:root:[   71] Training loss: 0.06633538, Validation loss: 0.16369561, Gradient norm: 2.90803516
INFO:root:[   72] Training loss: 0.06608001, Validation loss: 0.15547741, Gradient norm: 2.94636895
INFO:root:[   73] Training loss: 0.06419897, Validation loss: 0.17537499, Gradient norm: 2.45048576
INFO:root:[   74] Training loss: 0.06533853, Validation loss: 0.17343513, Gradient norm: 2.66870436
INFO:root:[   75] Training loss: 0.06445013, Validation loss: 0.15481479, Gradient norm: 2.67588884
INFO:root:[   76] Training loss: 0.06458218, Validation loss: 0.16672337, Gradient norm: 2.57257262
INFO:root:[   77] Training loss: 0.06327190, Validation loss: 0.14492572, Gradient norm: 2.54738111
INFO:root:[   78] Training loss: 0.06382770, Validation loss: 0.16550680, Gradient norm: 2.41821002
INFO:root:[   79] Training loss: 0.06354192, Validation loss: 0.16545288, Gradient norm: 2.47548180
INFO:root:[   80] Training loss: 0.06371832, Validation loss: 0.16631714, Gradient norm: 2.48125436
INFO:root:[   81] Training loss: 0.06249065, Validation loss: 0.15198844, Gradient norm: 2.36101484
INFO:root:[   82] Training loss: 0.06215353, Validation loss: 0.16343402, Gradient norm: 2.32345765
INFO:root:[   83] Training loss: 0.06403723, Validation loss: 0.16643125, Gradient norm: 2.88708006
INFO:root:[   84] Training loss: 0.06080305, Validation loss: 0.17429852, Gradient norm: 2.54556710
INFO:root:[   85] Training loss: 0.06423748, Validation loss: 0.16416288, Gradient norm: 2.79760576
INFO:root:[   86] Training loss: 0.06249911, Validation loss: 0.16205754, Gradient norm: 2.63175862
INFO:root:EP 86: Early stopping
INFO:root:Training the model took 4003.418s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.23334
INFO:root:EnergyScoreTrain: 0.12359
INFO:root:CoverageTrain: 0.67043
INFO:root:IntervalWidthTrain: 0.08781
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.23074
INFO:root:EnergyScoreValidation: 0.12632
INFO:root:CoverageValidation: 0.64187
INFO:root:IntervalWidthValidation: 0.07924
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21997
INFO:root:EnergyScoreTest: 0.1209
INFO:root:CoverageTest: 0.63515
INFO:root:IntervalWidthTest: 0.0764
INFO:root:###6 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 268435456
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.30329552, Validation loss: 0.18654893, Gradient norm: 4.89560696
INFO:root:[    2] Training loss: 0.17799839, Validation loss: 0.15023120, Gradient norm: 3.87555761
INFO:root:[    3] Training loss: 0.16134703, Validation loss: 0.14968427, Gradient norm: 3.63449135
INFO:root:[    4] Training loss: 0.15059276, Validation loss: 0.13973620, Gradient norm: 3.10538293
INFO:root:[    5] Training loss: 0.14217038, Validation loss: 0.14464174, Gradient norm: 3.45695614
INFO:root:[    6] Training loss: 0.13479839, Validation loss: 0.15423785, Gradient norm: 3.26604194
INFO:root:[    7] Training loss: 0.12857398, Validation loss: 0.14653365, Gradient norm: 3.42853249
INFO:root:[    8] Training loss: 0.12445458, Validation loss: 0.15873571, Gradient norm: 3.05154273
INFO:root:[    9] Training loss: 0.11814279, Validation loss: 0.13244075, Gradient norm: 3.23129541
INFO:root:[   10] Training loss: 0.11656143, Validation loss: 0.14303932, Gradient norm: 2.95775429
INFO:root:[   11] Training loss: 0.11239009, Validation loss: 0.11993132, Gradient norm: 3.26692199
INFO:root:[   12] Training loss: 0.10943694, Validation loss: 0.12069087, Gradient norm: 3.38420093
INFO:root:[   13] Training loss: 0.10775748, Validation loss: 0.15103990, Gradient norm: 3.00122933
INFO:root:[   14] Training loss: 0.10516935, Validation loss: 0.15408181, Gradient norm: 3.01674434
INFO:root:[   15] Training loss: 0.10301100, Validation loss: 0.14296598, Gradient norm: 2.49561940
INFO:root:[   16] Training loss: 0.10113742, Validation loss: 0.12717138, Gradient norm: 2.98816244
INFO:root:[   17] Training loss: 0.10077589, Validation loss: 0.12690548, Gradient norm: 2.80949353
INFO:root:[   18] Training loss: 0.09695163, Validation loss: 0.14367598, Gradient norm: 2.47011140
INFO:root:[   19] Training loss: 0.09493838, Validation loss: 0.13473842, Gradient norm: 2.94901493
INFO:root:[   20] Training loss: 0.09429830, Validation loss: 0.14653121, Gradient norm: 2.50840252
INFO:root:[   21] Training loss: 0.09761100, Validation loss: 0.13819580, Gradient norm: 3.16585311
INFO:root:[   22] Training loss: 0.09341961, Validation loss: 0.16617091, Gradient norm: 2.68901986
INFO:root:[   23] Training loss: 0.08747971, Validation loss: 0.14995576, Gradient norm: 2.60415743
INFO:root:[   24] Training loss: 0.08946333, Validation loss: 0.15002595, Gradient norm: 3.01890025
INFO:root:[   25] Training loss: 0.08792334, Validation loss: 0.14593735, Gradient norm: 2.91927090
INFO:root:[   26] Training loss: 0.08612977, Validation loss: 0.15501863, Gradient norm: 2.89717792
INFO:root:[   27] Training loss: 0.08439094, Validation loss: 0.15563808, Gradient norm: 2.54535343
INFO:root:[   28] Training loss: 0.08754298, Validation loss: 0.19295105, Gradient norm: 3.15700705
INFO:root:[   29] Training loss: 0.08531433, Validation loss: 0.17978184, Gradient norm: 2.65669298
INFO:root:[   30] Training loss: 0.08296553, Validation loss: 0.16712998, Gradient norm: 2.50926052
INFO:root:[   31] Training loss: 0.08028416, Validation loss: 0.17045522, Gradient norm: 2.85282816
INFO:root:[   32] Training loss: 0.08002082, Validation loss: 0.16583158, Gradient norm: 2.84704718
INFO:root:[   33] Training loss: 0.08071913, Validation loss: 0.15754418, Gradient norm: 2.88329586
INFO:root:[   34] Training loss: 0.07927695, Validation loss: 0.16598802, Gradient norm: 2.96251395
INFO:root:[   35] Training loss: 0.07995580, Validation loss: 0.17765168, Gradient norm: 2.37498862
INFO:root:[   36] Training loss: 0.07732614, Validation loss: 0.14949785, Gradient norm: 2.93881291
INFO:root:[   37] Training loss: 0.07467663, Validation loss: 0.16203985, Gradient norm: 2.59893169
INFO:root:[   38] Training loss: 0.07580735, Validation loss: 0.17511860, Gradient norm: 2.88102863
INFO:root:[   39] Training loss: 0.07306228, Validation loss: 0.16667970, Gradient norm: 2.59116045
INFO:root:[   40] Training loss: 0.07344439, Validation loss: 0.16602164, Gradient norm: 2.45406275
INFO:root:[   41] Training loss: 0.07346667, Validation loss: 0.14891431, Gradient norm: 2.54518619
INFO:root:[   42] Training loss: 0.07054280, Validation loss: 0.15373013, Gradient norm: 2.87944705
INFO:root:[   43] Training loss: 0.07229456, Validation loss: 0.16959353, Gradient norm: 2.31844936
INFO:root:[   44] Training loss: 0.07419539, Validation loss: 0.15488637, Gradient norm: 3.08616653
INFO:root:[   45] Training loss: 0.07267213, Validation loss: 0.15753454, Gradient norm: 3.00928792
INFO:root:[   46] Training loss: 0.07060576, Validation loss: 0.16485142, Gradient norm: 2.48578410
INFO:root:[   47] Training loss: 0.06971451, Validation loss: 0.15963647, Gradient norm: 2.33550904
INFO:root:[   48] Training loss: 0.07145792, Validation loss: 0.16819645, Gradient norm: 2.34062312
INFO:root:[   49] Training loss: 0.06883060, Validation loss: 0.16518353, Gradient norm: 2.45529910
INFO:root:[   50] Training loss: 0.07035861, Validation loss: 0.17659680, Gradient norm: 2.75606206
INFO:root:[   51] Training loss: 0.06726718, Validation loss: 0.16493825, Gradient norm: 2.31074769
INFO:root:[   52] Training loss: 0.06850766, Validation loss: 0.17579842, Gradient norm: 2.60353013
INFO:root:[   53] Training loss: 0.06892494, Validation loss: 0.16726698, Gradient norm: 2.82336508
INFO:root:[   54] Training loss: 0.06736806, Validation loss: 0.17408790, Gradient norm: 2.57270977
INFO:root:[   55] Training loss: 0.06670997, Validation loss: 0.16018467, Gradient norm: 2.43684982
INFO:root:[   56] Training loss: 0.06943320, Validation loss: 0.17206170, Gradient norm: 2.42554300
INFO:root:[   57] Training loss: 0.06577232, Validation loss: 0.15370759, Gradient norm: 2.52667570
INFO:root:[   58] Training loss: 0.06753690, Validation loss: 0.17210274, Gradient norm: 2.72643532
INFO:root:[   59] Training loss: 0.06679510, Validation loss: 0.19612539, Gradient norm: 2.57442937
INFO:root:[   60] Training loss: 0.06838955, Validation loss: 0.17372553, Gradient norm: 2.68143696
INFO:root:[   61] Training loss: 0.06649580, Validation loss: 0.16084033, Gradient norm: 2.55644714
INFO:root:[   62] Training loss: 0.06779924, Validation loss: 0.15392162, Gradient norm: 2.59148833
INFO:root:[   63] Training loss: 0.06861139, Validation loss: 0.17308533, Gradient norm: 3.02632190
INFO:root:[   64] Training loss: 0.06521008, Validation loss: 0.18041827, Gradient norm: 2.62541331
INFO:root:[   65] Training loss: 0.06584559, Validation loss: 0.16869890, Gradient norm: 2.67961032
INFO:root:[   66] Training loss: 0.06452900, Validation loss: 0.16616363, Gradient norm: 2.34349869
INFO:root:EP 66: Early stopping
INFO:root:Training the model took 3062.716s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.25336
INFO:root:EnergyScoreTrain: 0.1315
INFO:root:CoverageTrain: 0.70008
INFO:root:IntervalWidthTrain: 0.10542
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.2613
INFO:root:EnergyScoreValidation: 0.1384
INFO:root:CoverageValidation: 0.63777
INFO:root:IntervalWidthValidation: 0.09898
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.28012
INFO:root:EnergyScoreTest: 0.14608
INFO:root:CoverageTest: 0.62009
INFO:root:IntervalWidthTest: 0.10069
INFO:root:###7 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234567, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 574619648
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.27331215, Validation loss: 0.18063782, Gradient norm: 3.86355745
INFO:root:[    2] Training loss: 0.17629045, Validation loss: 0.17422876, Gradient norm: 2.91557602
INFO:root:[    3] Training loss: 0.15952474, Validation loss: 0.17111709, Gradient norm: 2.57842350
INFO:root:[    4] Training loss: 0.15174500, Validation loss: 0.16335143, Gradient norm: 2.93794704
INFO:root:[    5] Training loss: 0.14453195, Validation loss: 0.18478472, Gradient norm: 3.09246744
INFO:root:[    6] Training loss: 0.14097418, Validation loss: 0.13793470, Gradient norm: 2.91981287
INFO:root:[    7] Training loss: 0.13477146, Validation loss: 0.13493333, Gradient norm: 2.69566731
INFO:root:[    8] Training loss: 0.12714338, Validation loss: 0.14493408, Gradient norm: 3.28270407
INFO:root:[    9] Training loss: 0.11839085, Validation loss: 0.14369207, Gradient norm: 2.79497480
INFO:root:[   10] Training loss: 0.11295791, Validation loss: 0.14405588, Gradient norm: 2.80090020
INFO:root:[   11] Training loss: 0.11197113, Validation loss: 0.18843651, Gradient norm: 2.89713280
INFO:root:[   12] Training loss: 0.10912199, Validation loss: 0.15195584, Gradient norm: 2.64621369
INFO:root:[   13] Training loss: 0.10882392, Validation loss: 0.13828522, Gradient norm: 2.97098809
INFO:root:[   14] Training loss: 0.10588635, Validation loss: 0.18850408, Gradient norm: 2.90487458
INFO:root:[   15] Training loss: 0.10165480, Validation loss: 0.18123802, Gradient norm: 2.69088457
INFO:root:[   16] Training loss: 0.10133666, Validation loss: 0.13690172, Gradient norm: 2.86030939
INFO:root:[   17] Training loss: 0.09554860, Validation loss: 0.19401196, Gradient norm: 2.45411967
INFO:root:[   18] Training loss: 0.09819864, Validation loss: 0.17261339, Gradient norm: 2.83707464
INFO:root:[   19] Training loss: 0.09871026, Validation loss: 0.16630444, Gradient norm: 3.09836143
INFO:root:[   20] Training loss: 0.09610401, Validation loss: 0.14090172, Gradient norm: 2.81381927
INFO:root:[   21] Training loss: 0.09409183, Validation loss: 0.14288570, Gradient norm: 2.65720877
INFO:root:[   22] Training loss: 0.09256690, Validation loss: 0.14678760, Gradient norm: 2.96972560
INFO:root:[   23] Training loss: 0.08970809, Validation loss: 0.19252312, Gradient norm: 2.61495310
INFO:root:[   24] Training loss: 0.08936351, Validation loss: 0.18566724, Gradient norm: 2.59634206
INFO:root:[   25] Training loss: 0.08661329, Validation loss: 0.18378673, Gradient norm: 2.55578052
INFO:root:[   26] Training loss: 0.08724225, Validation loss: 0.17030208, Gradient norm: 2.64542604
INFO:root:[   27] Training loss: 0.08653038, Validation loss: 0.15640332, Gradient norm: 2.66007333
INFO:root:[   28] Training loss: 0.08727811, Validation loss: 0.20073763, Gradient norm: 2.53010613
INFO:root:[   29] Training loss: 0.08419812, Validation loss: 0.15538668, Gradient norm: 2.51872736
INFO:root:[   30] Training loss: 0.08374092, Validation loss: 0.18331517, Gradient norm: 2.79755239
INFO:root:[   31] Training loss: 0.08145896, Validation loss: 0.17829186, Gradient norm: 2.73538329
INFO:root:[   32] Training loss: 0.08437504, Validation loss: 0.17951013, Gradient norm: 3.04280752
INFO:root:[   33] Training loss: 0.08188116, Validation loss: 0.17865640, Gradient norm: 3.00683266
INFO:root:[   34] Training loss: 0.07888966, Validation loss: 0.18797752, Gradient norm: 2.68797102
INFO:root:[   35] Training loss: 0.08057071, Validation loss: 0.17337159, Gradient norm: 2.52258812
INFO:root:[   36] Training loss: 0.07804066, Validation loss: 0.17948352, Gradient norm: 2.66452889
INFO:root:[   37] Training loss: 0.07669421, Validation loss: 0.16416714, Gradient norm: 2.70765539
INFO:root:[   38] Training loss: 0.07642206, Validation loss: 0.17893928, Gradient norm: 2.70536982
INFO:root:[   39] Training loss: 0.07767753, Validation loss: 0.17488162, Gradient norm: 2.26840238
INFO:root:[   40] Training loss: 0.07707328, Validation loss: 0.18085083, Gradient norm: 3.13102823
INFO:root:[   41] Training loss: 0.07754182, Validation loss: 0.17070299, Gradient norm: 3.11343510
INFO:root:[   42] Training loss: 0.07414159, Validation loss: 0.18367629, Gradient norm: 2.66530587
INFO:root:[   43] Training loss: 0.07582236, Validation loss: 0.17689122, Gradient norm: 2.61696188
INFO:root:[   44] Training loss: 0.07482179, Validation loss: 0.16459146, Gradient norm: 2.83085580
INFO:root:[   45] Training loss: 0.07123805, Validation loss: 0.18205779, Gradient norm: 2.71354701
INFO:root:[   46] Training loss: 0.07244132, Validation loss: 0.17948351, Gradient norm: 2.52521945
INFO:root:[   47] Training loss: 0.07152333, Validation loss: 0.19644552, Gradient norm: 2.69908523
INFO:root:[   48] Training loss: 0.07336788, Validation loss: 0.16651838, Gradient norm: 2.85553457
INFO:root:[   49] Training loss: 0.07086213, Validation loss: 0.16141540, Gradient norm: 2.80248272
INFO:root:[   50] Training loss: 0.07215707, Validation loss: 0.17662356, Gradient norm: 2.72290969
INFO:root:[   51] Training loss: 0.07019162, Validation loss: 0.16661874, Gradient norm: 2.72653960
INFO:root:[   52] Training loss: 0.07207979, Validation loss: 0.18679347, Gradient norm: 2.84850457
INFO:root:[   53] Training loss: 0.07094829, Validation loss: 0.17772000, Gradient norm: 2.91582846
INFO:root:[   54] Training loss: 0.06821235, Validation loss: 0.18574547, Gradient norm: 2.51402095
INFO:root:[   55] Training loss: 0.07043458, Validation loss: 0.17219453, Gradient norm: 2.83637165
INFO:root:[   56] Training loss: 0.06960903, Validation loss: 0.17661167, Gradient norm: 3.06842923
INFO:root:[   57] Training loss: 0.06913190, Validation loss: 0.18613910, Gradient norm: 2.85405208
INFO:root:[   58] Training loss: 0.07037694, Validation loss: 0.18286377, Gradient norm: 3.12394158
INFO:root:[   59] Training loss: 0.06563674, Validation loss: 0.17937142, Gradient norm: 2.39550074
INFO:root:[   60] Training loss: 0.06939880, Validation loss: 0.17638806, Gradient norm: 2.59932896
INFO:root:[   61] Training loss: 0.06787065, Validation loss: 0.15992599, Gradient norm: 2.77481257
INFO:root:[   62] Training loss: 0.06694984, Validation loss: 0.16621530, Gradient norm: 2.66666247
INFO:root:[   63] Training loss: 0.06783829, Validation loss: 0.17656130, Gradient norm: 2.83055318
INFO:root:[   64] Training loss: 0.06815349, Validation loss: 0.18655708, Gradient norm: 2.83916633
INFO:root:[   65] Training loss: 0.06425767, Validation loss: 0.17817040, Gradient norm: 2.34574754
INFO:root:[   66] Training loss: 0.06827586, Validation loss: 0.17068425, Gradient norm: 3.15566518
INFO:root:[   67] Training loss: 0.06778305, Validation loss: 0.20085940, Gradient norm: 2.63206347
INFO:root:[   68] Training loss: 0.06685822, Validation loss: 0.18063597, Gradient norm: 2.68490007
INFO:root:[   69] Training loss: 0.06636816, Validation loss: 0.17832974, Gradient norm: 2.35842437
INFO:root:[   70] Training loss: 0.06543966, Validation loss: 0.18509659, Gradient norm: 2.53247577
INFO:root:EP 70: Early stopping
INFO:root:Training the model took 3247.952s.
INFO:root:Emptying the cuda cache took 0.005s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.23252
INFO:root:EnergyScoreTrain: 0.12443
INFO:root:CoverageTrain: 0.65149
INFO:root:IntervalWidthTrain: 0.08318
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.23435
INFO:root:EnergyScoreValidation: 0.12725
INFO:root:CoverageValidation: 0.64557
INFO:root:IntervalWidthValidation: 0.08272
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.22525
INFO:root:EnergyScoreTest: 0.12393
INFO:root:CoverageTest: 0.63582
INFO:root:IntervalWidthTest: 0.07775
INFO:root:###8 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 12345678, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 436207616
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.30361087, Validation loss: 0.21038175, Gradient norm: 4.13309012
INFO:root:[    2] Training loss: 0.18481502, Validation loss: 0.18016142, Gradient norm: 3.44262188
INFO:root:[    3] Training loss: 0.16373730, Validation loss: 0.14260665, Gradient norm: 3.52808594
INFO:root:[    4] Training loss: 0.15429150, Validation loss: 0.16735793, Gradient norm: 2.87016328
INFO:root:[    5] Training loss: 0.14425372, Validation loss: 0.14499911, Gradient norm: 3.52262659
INFO:root:[    6] Training loss: 0.13903533, Validation loss: 0.13543660, Gradient norm: 3.17532515
INFO:root:[    7] Training loss: 0.12907262, Validation loss: 0.16157915, Gradient norm: 3.06753620
INFO:root:[    8] Training loss: 0.12930730, Validation loss: 0.17143906, Gradient norm: 3.60499608
INFO:root:[    9] Training loss: 0.12301071, Validation loss: 0.17001639, Gradient norm: 3.33375485
INFO:root:[   10] Training loss: 0.11614217, Validation loss: 0.14664179, Gradient norm: 3.52652876
INFO:root:[   11] Training loss: 0.11151571, Validation loss: 0.15186440, Gradient norm: 3.10576879
INFO:root:[   12] Training loss: 0.10965124, Validation loss: 0.13575084, Gradient norm: 3.01977374
INFO:root:[   13] Training loss: 0.11140270, Validation loss: 0.15355577, Gradient norm: 3.49310468
INFO:root:[   14] Training loss: 0.10352836, Validation loss: 0.15992238, Gradient norm: 2.91012902
INFO:root:[   15] Training loss: 0.10553921, Validation loss: 0.13201375, Gradient norm: 3.18481251
INFO:root:[   16] Training loss: 0.10229057, Validation loss: 0.13739984, Gradient norm: 2.93628383
INFO:root:[   17] Training loss: 0.09724338, Validation loss: 0.15994537, Gradient norm: 2.90845145
INFO:root:[   18] Training loss: 0.09584302, Validation loss: 0.16011128, Gradient norm: 2.93482944
INFO:root:[   19] Training loss: 0.09632547, Validation loss: 0.16552308, Gradient norm: 3.08313626
INFO:root:[   20] Training loss: 0.09415913, Validation loss: 0.14841724, Gradient norm: 2.92676477
INFO:root:[   21] Training loss: 0.09279380, Validation loss: 0.14496933, Gradient norm: 2.61297813
INFO:root:[   22] Training loss: 0.09225402, Validation loss: 0.13797216, Gradient norm: 3.08571508
INFO:root:[   23] Training loss: 0.09321398, Validation loss: 0.15755336, Gradient norm: 3.20468570
INFO:root:[   24] Training loss: 0.08792950, Validation loss: 0.17287326, Gradient norm: 2.93697217
INFO:root:[   25] Training loss: 0.08808367, Validation loss: 0.13137546, Gradient norm: 3.14536868
INFO:root:[   26] Training loss: 0.09002135, Validation loss: 0.13921259, Gradient norm: 3.02973045
INFO:root:[   27] Training loss: 0.08428706, Validation loss: 0.16062739, Gradient norm: 2.83968677
INFO:root:[   28] Training loss: 0.08218563, Validation loss: 0.16955501, Gradient norm: 2.22076916
INFO:root:[   29] Training loss: 0.08352338, Validation loss: 0.16149805, Gradient norm: 2.58677849
INFO:root:[   30] Training loss: 0.08077085, Validation loss: 0.13797508, Gradient norm: 2.91507756
INFO:root:[   31] Training loss: 0.08104066, Validation loss: 0.13626396, Gradient norm: 3.07146588
INFO:root:[   32] Training loss: 0.08063815, Validation loss: 0.17303233, Gradient norm: 2.56068090
INFO:root:[   33] Training loss: 0.08075804, Validation loss: 0.15680103, Gradient norm: 3.03331147
INFO:root:[   34] Training loss: 0.08012860, Validation loss: 0.15847220, Gradient norm: 2.64870798
INFO:root:[   35] Training loss: 0.07760826, Validation loss: 0.15460122, Gradient norm: 2.43964315
INFO:root:[   36] Training loss: 0.07617188, Validation loss: 0.14547866, Gradient norm: 2.15115517
INFO:root:[   37] Training loss: 0.07666388, Validation loss: 0.17070762, Gradient norm: 2.85349323
INFO:root:[   38] Training loss: 0.07729874, Validation loss: 0.16301204, Gradient norm: 2.74844290
INFO:root:[   39] Training loss: 0.07621350, Validation loss: 0.16738759, Gradient norm: 2.59393324
INFO:root:[   40] Training loss: 0.07625762, Validation loss: 0.16508527, Gradient norm: 2.91166834
INFO:root:[   41] Training loss: 0.07509026, Validation loss: 0.16627098, Gradient norm: 2.69033214
INFO:root:[   42] Training loss: 0.07452106, Validation loss: 0.17212172, Gradient norm: 2.78650364
INFO:root:[   43] Training loss: 0.07407974, Validation loss: 0.15663888, Gradient norm: 2.35804182
INFO:root:[   44] Training loss: 0.07364877, Validation loss: 0.16464195, Gradient norm: 2.59347321
INFO:root:[   45] Training loss: 0.07113452, Validation loss: 0.16228077, Gradient norm: 2.56475347
INFO:root:[   46] Training loss: 0.07113549, Validation loss: 0.16191968, Gradient norm: 2.14265561
INFO:root:[   47] Training loss: 0.07121884, Validation loss: 0.16621713, Gradient norm: 2.85686792
INFO:root:[   48] Training loss: 0.06979500, Validation loss: 0.16181910, Gradient norm: 2.43502573
INFO:root:[   49] Training loss: 0.06905450, Validation loss: 0.15307071, Gradient norm: 2.71601003
INFO:root:[   50] Training loss: 0.06987521, Validation loss: 0.17797911, Gradient norm: 2.68257597
INFO:root:[   51] Training loss: 0.06907289, Validation loss: 0.16396831, Gradient norm: 2.50441549
INFO:root:[   52] Training loss: 0.07044407, Validation loss: 0.16268899, Gradient norm: 2.64057024
INFO:root:[   53] Training loss: 0.06918578, Validation loss: 0.15675954, Gradient norm: 2.72956963
INFO:root:[   54] Training loss: 0.06918536, Validation loss: 0.17406746, Gradient norm: 2.69341629
INFO:root:[   55] Training loss: 0.06750303, Validation loss: 0.17831127, Gradient norm: 2.48303411
INFO:root:[   56] Training loss: 0.07002913, Validation loss: 0.17716100, Gradient norm: 3.07264385
INFO:root:[   57] Training loss: 0.06753257, Validation loss: 0.17000651, Gradient norm: 2.76825933
INFO:root:[   58] Training loss: 0.06858105, Validation loss: 0.18672569, Gradient norm: 2.88397016
INFO:root:[   59] Training loss: 0.06886059, Validation loss: 0.17978526, Gradient norm: 3.02472789
INFO:root:[   60] Training loss: 0.06719362, Validation loss: 0.15934945, Gradient norm: 2.44620341
INFO:root:[   61] Training loss: 0.06864796, Validation loss: 0.16076918, Gradient norm: 2.81453830
INFO:root:[   62] Training loss: 0.11164035, Validation loss: 0.15144114, Gradient norm: 2.98396346
INFO:root:[   63] Training loss: 0.06968746, Validation loss: 0.16545217, Gradient norm: 2.64375655
INFO:root:[   64] Training loss: 0.06870070, Validation loss: 0.16318367, Gradient norm: 3.06697639
INFO:root:[   65] Training loss: 0.06439121, Validation loss: 0.16981006, Gradient norm: 2.78959591
INFO:root:[   66] Training loss: 0.06447810, Validation loss: 0.16651008, Gradient norm: 2.38527975
INFO:root:[   67] Training loss: 0.06411922, Validation loss: 0.17199287, Gradient norm: 2.67364691
INFO:root:[   68] Training loss: 0.06555968, Validation loss: 0.17875410, Gradient norm: 2.72594480
INFO:root:[   69] Training loss: 0.06288001, Validation loss: 0.17134162, Gradient norm: 2.29490152
INFO:root:[   70] Training loss: 0.06439358, Validation loss: 0.17427452, Gradient norm: 2.60472729
INFO:root:[   71] Training loss: 0.06243643, Validation loss: 0.16381211, Gradient norm: 2.29751814
INFO:root:EP 71: Early stopping
INFO:root:Training the model took 3290.831s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.21623
INFO:root:EnergyScoreTrain: 0.11514
INFO:root:CoverageTrain: 0.66863
INFO:root:IntervalWidthTrain: 0.08378
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.21661
INFO:root:EnergyScoreValidation: 0.11709
INFO:root:CoverageValidation: 0.61071
INFO:root:IntervalWidthValidation: 0.07263
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.21262
INFO:root:EnergyScoreTest: 0.11529
INFO:root:CoverageTest: 0.60862
INFO:root:IntervalWidthTest: 0.07401
INFO:root:###9 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 123456789, 'model': 'UNO', 'uncertainty_quantification': 'laplace', 'batch_size': 64, 'n_epochs': 1000, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'layer_normalization': True, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0, 'dropout': 0.01, 'fourier_dropout': None, 'hidden_channels': 32, 'projection_channels': 32, 'lifting_channels': 32, 'n_samples': 3, 'uno_out_channels': [64, 128, 128, 64, 32], 'uno_scalings': [[0.5, 0.5], [0.5, 0.5], [1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], 'uno_n_modes': [[18, 18], [8, 8], [8, 8], [8, 8], [18, 18]]}
INFO:root:NumberParameters: 5814913
INFO:root:Memory allocated: 469762048
INFO:root:Training starts now.
INFO:root:[    1] Training loss: 0.32245733, Validation loss: 0.16748901, Gradient norm: 5.12275214
INFO:root:[    2] Training loss: 0.18182466, Validation loss: 0.20663193, Gradient norm: 3.66642682
INFO:root:[    3] Training loss: 0.16066048, Validation loss: 0.18679963, Gradient norm: 3.51998853
INFO:root:[    4] Training loss: 0.14848081, Validation loss: 0.14244123, Gradient norm: 3.84641779
INFO:root:[    5] Training loss: 0.13483110, Validation loss: 0.12479366, Gradient norm: 3.36190470
INFO:root:[    6] Training loss: 0.13352104, Validation loss: 0.14124284, Gradient norm: 3.21670591
INFO:root:[    7] Training loss: 0.12317800, Validation loss: 0.15062342, Gradient norm: 3.44656887
INFO:root:[    8] Training loss: 0.12067781, Validation loss: 0.11837895, Gradient norm: 3.05558455
INFO:root:[    9] Training loss: 0.11411013, Validation loss: 0.13366537, Gradient norm: 3.28556684
INFO:root:[   10] Training loss: 0.11411175, Validation loss: 0.12117465, Gradient norm: 3.50115030
INFO:root:[   11] Training loss: 0.10977603, Validation loss: 0.13161189, Gradient norm: 3.00610461
INFO:root:[   12] Training loss: 0.10530527, Validation loss: 0.14335765, Gradient norm: 2.71718164
INFO:root:[   13] Training loss: 0.10531468, Validation loss: 0.12296714, Gradient norm: 3.12543472
INFO:root:[   14] Training loss: 0.10440999, Validation loss: 0.16823773, Gradient norm: 3.21648240
INFO:root:[   15] Training loss: 0.10226663, Validation loss: 0.13010010, Gradient norm: 2.72389538
INFO:root:[   16] Training loss: 0.10092590, Validation loss: 0.16058527, Gradient norm: 2.89816223
INFO:root:[   17] Training loss: 0.09666968, Validation loss: 0.16453688, Gradient norm: 2.92851238
INFO:root:[   18] Training loss: 0.09401775, Validation loss: 0.14243680, Gradient norm: 2.75642530
INFO:root:[   19] Training loss: 0.09535326, Validation loss: 0.13582247, Gradient norm: 2.97277743
INFO:root:[   20] Training loss: 0.09298286, Validation loss: 0.14105439, Gradient norm: 2.84792952
INFO:root:[   21] Training loss: 0.09043399, Validation loss: 0.13900790, Gradient norm: 2.77386797
INFO:root:[   22] Training loss: 0.09133811, Validation loss: 0.13937572, Gradient norm: 3.05425382
INFO:root:[   23] Training loss: 0.09054993, Validation loss: 0.15738857, Gradient norm: 2.88982225
INFO:root:[   24] Training loss: 0.08886636, Validation loss: 0.14576275, Gradient norm: 2.65724379
INFO:root:[   25] Training loss: 0.08739530, Validation loss: 0.16682413, Gradient norm: 2.53853524
INFO:root:[   26] Training loss: 0.08564874, Validation loss: 0.16942470, Gradient norm: 2.75912101
INFO:root:[   27] Training loss: 0.08470267, Validation loss: 0.15004598, Gradient norm: 2.40598539
INFO:root:[   28] Training loss: 0.08557082, Validation loss: 0.14934571, Gradient norm: 2.77504337
INFO:root:[   29] Training loss: 0.08263519, Validation loss: 0.16558431, Gradient norm: 3.03381425
INFO:root:[   30] Training loss: 0.08275237, Validation loss: 0.16489590, Gradient norm: 2.52783635
INFO:root:[   31] Training loss: 0.08221320, Validation loss: 0.15236156, Gradient norm: 2.73640128
INFO:root:[   32] Training loss: 0.08225076, Validation loss: 0.18006753, Gradient norm: 2.45555281
INFO:root:[   33] Training loss: 0.08055577, Validation loss: 0.14998427, Gradient norm: 3.17434990
INFO:root:[   34] Training loss: 0.07968362, Validation loss: 0.17939785, Gradient norm: 2.83779923
INFO:root:[   35] Training loss: 0.07916222, Validation loss: 0.14774821, Gradient norm: 2.93339245
INFO:root:[   36] Training loss: 0.07791551, Validation loss: 0.15692765, Gradient norm: 2.75560242
INFO:root:[   37] Training loss: 0.07830890, Validation loss: 0.15001844, Gradient norm: 2.55365858
INFO:root:[   38] Training loss: 0.07599977, Validation loss: 0.16389347, Gradient norm: 2.63334957
INFO:root:[   39] Training loss: 0.07651565, Validation loss: 0.15927876, Gradient norm: 2.60418370
INFO:root:[   40] Training loss: 0.07459364, Validation loss: 0.14871096, Gradient norm: 2.58500365
INFO:root:[   41] Training loss: 0.07393987, Validation loss: 0.15972724, Gradient norm: 2.74642287
INFO:root:[   42] Training loss: 0.07515327, Validation loss: 0.16280821, Gradient norm: 2.76322241
INFO:root:[   43] Training loss: 0.07387738, Validation loss: 0.15849964, Gradient norm: 2.57727215
INFO:root:[   44] Training loss: 0.07462243, Validation loss: 0.15613867, Gradient norm: 2.67152245
INFO:root:[   45] Training loss: 0.07228702, Validation loss: 0.16146494, Gradient norm: 3.03318728
INFO:root:[   46] Training loss: 0.07255283, Validation loss: 0.16629940, Gradient norm: 2.64049731
INFO:root:[   47] Training loss: 0.07219937, Validation loss: 0.17597021, Gradient norm: 2.76647037
INFO:root:[   48] Training loss: 0.07047870, Validation loss: 0.16656516, Gradient norm: 2.62167048
INFO:root:[   49] Training loss: 0.06870073, Validation loss: 0.16997847, Gradient norm: 2.39271949
INFO:root:[   50] Training loss: 0.07054100, Validation loss: 0.16435300, Gradient norm: 2.92157290
INFO:root:[   51] Training loss: 0.07050225, Validation loss: 0.17666518, Gradient norm: 2.67447697
INFO:root:[   52] Training loss: 0.06965641, Validation loss: 0.16563064, Gradient norm: 2.47787878
INFO:root:[   53] Training loss: 0.06806627, Validation loss: 0.15339774, Gradient norm: 2.50056914
INFO:root:[   54] Training loss: 0.06777775, Validation loss: 0.17445494, Gradient norm: 2.50005980
INFO:root:[   55] Training loss: 0.06863297, Validation loss: 0.15190465, Gradient norm: 2.92089128
INFO:root:[   56] Training loss: 0.06725507, Validation loss: 0.14475476, Gradient norm: 2.38152296
INFO:root:[   57] Training loss: 0.06800879, Validation loss: 0.15336170, Gradient norm: 2.91141949
INFO:root:[   58] Training loss: 0.06589869, Validation loss: 0.17279568, Gradient norm: 2.30925108
INFO:root:[   59] Training loss: 0.06677340, Validation loss: 0.16214721, Gradient norm: 2.56494220
INFO:root:[   60] Training loss: 0.06701360, Validation loss: 0.16693627, Gradient norm: 2.66086561
INFO:root:[   61] Training loss: 0.06810892, Validation loss: 0.15598065, Gradient norm: 2.66948184
INFO:root:[   62] Training loss: 0.06579936, Validation loss: 0.16129218, Gradient norm: 2.85381038
INFO:root:[   63] Training loss: 0.06644847, Validation loss: 0.15208499, Gradient norm: 2.55525588
INFO:root:[   64] Training loss: 0.06619709, Validation loss: 0.16916339, Gradient norm: 2.78537263
INFO:root:[   65] Training loss: 0.06556846, Validation loss: 0.17589520, Gradient norm: 2.48359575
INFO:root:EP 65: Early stopping
INFO:root:Training the model took 3012.727s.
INFO:root:Emptying the cuda cache took 0.006s.
INFO:root:Starting evaluation: model UNO & uncertainty quantification laplace
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.24659
INFO:root:EnergyScoreTrain: 0.12752
INFO:root:CoverageTrain: 0.69663
INFO:root:IntervalWidthTrain: 0.10452
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.24185
INFO:root:EnergyScoreValidation: 0.12735
INFO:root:CoverageValidation: 0.66857
INFO:root:IntervalWidthValidation: 0.09634
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.23489
INFO:root:EnergyScoreTest: 0.12707
INFO:root:CoverageTest: 0.66907
INFO:root:IntervalWidthTest: 0.0937
