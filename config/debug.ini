[META]
results_path = results/
data_path = data/
experiment_name = debug

[TRAININGPARAMETERS]
seed = [1, 12]
model = 'FNO' # 'UNO', 'FNO'
uncertainty_quantification = ['scoring-rule-dropout']  # 'laplace', 'dropout', 'scoring-rule-dropout'
batch_size =  [32]
n_epochs = 10
early_stopping = 10
dropout = 0.1
init = 'default' # he, xavier, default
learning_rate = 0.0001
lr_schedule = 'no' # 'no', 'step'
optimizer = 'adam'
gradient_clipping = 1
layer_normalization = True
data_loader_pin_memory = False 
data_loader_num_workers = [0]
distributed_training = False
fourier_dropout = 0.0
hidden_channels = 32
projection_channels = 64 
alpha = 0.05
n_samples_uq = 10
weight_decay = 0
### PFNO & PUNO
n_samples = 3
### FNO & PFNO
lifting_channels = 128
n_modes = (2, 2, 2)  # (16, 16) for Darcy Flow, (16, 16, 16) for SWE
### UNO & PUNO
uno_out_channels = [32,64,64,32]  # has to be a list (hyperparameter search does not iterate over it)
uno_scalings= [[1.0,1.0],[0.5,0.5],[1,1],[2,2]]  # has to be a list (hyperparameter search does not iterate over it)
uno_n_modes= [[16,16],[8,8],[8,8],[16,16]]  # has to be a list (hyperparameter search does not iterate over it)

[DATAPARAMETERS]
dataset_name = ['SWE'] # 'DarcyFlow', 'SWE'
max_training_set_size = 10000
downscaling_factor = 2  # int
### SWE
temporal_downscaling = 2
init_steps = 10 # Initial steps
t_start = 0 # Where to start input
pred_horizon = 10 # Prediction horizon across time domain