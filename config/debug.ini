[META]
results_path = results/
data_path = data/
experiment_name = debug

[TRAININGPARAMETERS]
; resume_training = 'trainingOnSyntheticData/results/20240507_104629_debug_model_parameters/Datetime_20240507_104629_Loss_training_set_size_946_batch_size_32_hidden_dim_32.pt'
resume_training = 'trainingOnSyntheticData/results/Flexible dimension/full_19_big_training_sets/20240427_070711_5M/Datetime_20240427_070717_Loss_training_set_size_797545_batch_size_341_hidden_dim_256.pt'
embedding = ['no']
; no, linear, linear-relu, linear-relu-linear 
dim_embedding = None # dimension of the embedding
dim_hidden_embedding = None # dimension of the hidden layer in the embedding network; will only be used if embedding='linear-relu-linear'
hidden_dim = [32]
batch_size =  [32]
num_inds = 32
num_layers_enc = 2
num_layers = 4
n_epochs = 100
loss = 'CE'
early_stopping = 100
gamma_neg = 1
p_target = 0
model = 'set-transformer'
;set-transformer, mlp
n_head = 4
dropout = 0.0
init = 'default' # he, xavier, default
num_layers_classifier = 2
sab_in_output = False
learning_rate = 0.0001
num_example_predictions = 3
balanced_loss = False
lr_schedule = 'no'
; 'no', 'warm-up', 'step'
warmup_steps = 20
one_hot = True
alpha_label_smoothing = 0.0
optimizer = 'adam'
gradient_clipping = 1
layer_normalization = True
plt_activations = False
curriculum_learning = False
data_device = ['cpu'] # cpu or cuda (if available)
data_loader_pin_memory = False  # False if 
data_loader_num_workers = [0]
distributed_training = False

[DATAPARAMETERS]
daset_name = ['darcy_flow']
; 'darcy_flow', 
max_training_set_size = 1000